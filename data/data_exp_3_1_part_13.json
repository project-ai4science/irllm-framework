[
  {
    "id":2411.16896,
    "research_type":"applied",
    "start_id":"b27",
    "start_title":"Fast fit-free analysis of fluorescence lifetime imaging via deep learning",
    "start_abstract":"Fluorescence lifetime imaging (FLI) provides unique quantitative information in biomedical and molecular biology studies but relies on complex data-fitting techniques to derive the quantities of interest. Herein, we propose a fit-free approach FLI image formation that is based deep learning (DL) quantify fluorescence decays simultaneously over whole at fast speeds. We report neural network (DNN) architecture, named (FLI-Net) designed trained for different classes experiments, including visible near-infrared (NIR) microscopy (FLIM) NIR gated macroscopy (MFLI). FLI-Net outputs quantitatively spatially resolved lifetime-based parameters are typically employed field. validate utility framework by performing microscopic preclinical across spectra, as well 2 main data acquisition technologies. These results demonstrate suited accurately lifetimes cells and, real time, intact animals without any parameter settings. Hence, paves way reproducible unprecedented speeds, improved dissemination impact many important applications ranging from fundamental discoveries cellular clinical translation.",
    "start_categories":[
      "cs.NE"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b16"
      ],
      "title":[
        "Characterization of fluorescence lifetime of organic fluorophores for molecular imaging in the shortwave infrared window"
      ],
      "abstract":[
        "SignificanceFluorescence lifetime imaging in the shortwave infrared (SWIR) is expected to enable high-resolution multiplexed molecular highly scattering tissue.AimTo characterize brightness and fluorescence of commercially available organic SWIR fluorophores benchmark them against tail emission conventional NIR-excited probes.ApproachCharacterization was performed through our established time-domain mesoscopic tomography system integrated around a time-correlated single-photon counting-single-photon avalanche diode array. Brightness were measured for NIR probes >1000 nm. Simultaneous probe then assess their potential studies.ResultsThe outperformed while mean lifetimes extremely short. The phantom study demonstrated feasibility multiplexing window with both probes.ConclusionsLong-tail Fluorescence readily detectable window, where showed shorter compared probes. We demonstrate which paves way vivo studies intact tissues at improved resolution."
      ],
      "categories":[
        "q-bio.BM"
      ]
    },
    "list":{
      "title":[
        "Short Paths in the Planar Graph Product Structure Theorem",
        "Measuring $A_\\text{FB}^b$ and $R_b$ with exclusive $b$-hadron decays at\n  the FCC-ee",
        "Monotonicity and convergence of two-relaxation-times lattice Boltzmann\n  schemes for a non-linear conservation law",
        "Coherent dynamics of flavor mode entangled neutrinos",
        "On the Use of WGANs for Super Resolution in Dark-Matter Simulations",
        "Relationship between the $\\gamma-$ray variability and the pc-scale jet\n  in the blazar 3C 454.3",
        "Gas Perturbations in Hot Smooth Atmospheres X-ray Surface Brightness\n  Fluctuations in Smooth Galaxy Cluster Atmospheres",
        "Causal Inference on Outcomes Learned from Text",
        "Propagation of extreme events in multiplex neuronal networks",
        "The Energy Cascade Rate in Supersonic Magnetohydrodynamic Turbulence",
        "Self-organized institutions in evolutionary dynamical-systems game",
        "Identifying Flare Locations Through Exoplanet Transit Occultations",
        "Effect of 3d Transition Metal Doping (Mn, Fe, Co, Ni) on the Electronic\n  and Magnetic Properties of Pd Alloys at Low Impurity Concentrations: An Ab\n  initio Study",
        "Resonant current from singlet-triplet state mixing in coupled quantum\n  dots",
        "Exchange Rate Sensitivity in Free Zone Trade: An Empirical Study of the\n  Istanbul Ataturk Airport Free Zone",
        "Phase evolution of strong-field ionization",
        "Fault tolerance for metric dimension and its variants",
        "On the viability of higher order theories",
        "PhysicsSolver: Transformer-Enhanced Physics-Informed Neural Networks for\n  Forward and Forecasting Problems in Partial Differential Equations",
        "Enhancement of Large Eddy Simulation for the prediction of an intake\n  flow rig using sequential Data Assimilation",
        "A network-driven framework for enhancing gene-disease association\n  studies in coronary artery disease",
        "The Jacobian of a regular orthogonal matroid and torsor structures on\n  spanning quasi-trees of ribbon graphs",
        "Prediction and observation of a stellar occultation by Haumea's\n  satellite Namaka",
        "Hausdorffness of certain nilpotent cohomology spaces",
        "The r-Dynamic Chromatic Number is Bounded in the Strong 2-Coloring\n  Number",
        "Non-orbital particle trapping in binary black holes through dynamic\n  stability",
        "Numerical security analysis for quantum key distribution with partial\n  state characterization",
        "Embedding 1D BDI topological models into continuous elastic plates",
        "On Exponents of Thickness in Geometry Rigidity Inequality for Shells"
      ],
      "abstract":[
        "The Planar Graph Product Structure Theorem of Dujmovi\\'c et al. [J. ACM '20]\nsays that every planar graph $G$ is contained in $H\\boxtimes P\\boxtimes K_3$\nfor some planar graph $H$ with treewidth at most 3 and some path $P$. This\nresult has been the key to solving several old open problems. Several people\nhave asked whether the Planar Graph Product Structure Theorem can be proved\nwith good upper bounds on the length of $P$. No $o(n)$ upper bound was\npreviously known for $n$-vertex planar graphs. We answer this question in the\naffirmative, by proving that for any $\\epsilon\\in (0,1)$ every $n$-vertex\nplanar graph is contained in $H\\boxtimes P\\boxtimes K_{O(1\/\\epsilon)}$, for\nsome planar graph $H$ with treewidth 3 and for some path $P$ of length\n$O(\\frac{1}{\\epsilon}n^{(1+\\epsilon)\/2})$. This bound is almost tight since\nthere is a lower bound of $\\Omega(n^{1\/2})$ for certain $n$-vertex planar\ngraphs. In fact, we prove a stronger result with $P$ of length\n$O(\\frac{1}{\\epsilon}\\,\\textrm{tw}(G)\\,n^{\\epsilon})$, which is tight up to the\n$O(\\frac{1}{\\epsilon}\\,n^{\\epsilon})$ factor for every $n$-vertex planar graph\n$G$. Finally, taking $\\epsilon=\\frac{1}{\\log n}$, we show that every $n$-vertex\nplanar graph $G$ is contained in $H\\boxtimes P\\boxtimes K_{O(\\log n)}$ for some\nplanar graph $H$ with treewidth at most 3 and some path $P$ of length\n$O(\\textrm{tw}(G)\\,\\log n)$. This result is particularly attractive since the\ntreewidth of the product $H\\boxtimes P\\boxtimes K_{O(\\log n)}$ is within a\n$O(\\log^2n)$ factor of the treewidth of $G$.",
        "This paper presents a novel tagging technique to measure the beauty-quark\npartial decay-width ratio $R_b$ and its forward-backward asymmetry\n$A_\\text{FB}^b$ at the FCC-ee, using $\\mathcal{O}(10^{12})$ $Z$-boson decays.\nThe method is based on the exclusive reconstruction of a selected list of\n$b$-hadron decay modes in $Z\\to b\\bar{b}$ events at the $Z$ pole, which can\nprovide the flavour and possibly the charge of the hemisphere. This approach\neffectively eliminates the contamination from light-quark physics events and\nreduces the leading systematic uncertainties arising from background\ncontamination, tagging-efficiency correlations, and gluon-radiation corrections\nby exploiting the geometric and kinematic properties of beauty hadrons. This\nresults in a total relative uncertainty of the order of $0.01\\,\\%$ for both\nobservables. Furthermore, this precision allows to obtain a commensurate\nprecision on the weak mixing angle $\\sin^2(\\theta_W^\\text{eff})$ compared to\nthe muon forward-backward asymmetry on the order of $0.002\\,\\%$.",
        "We address the convergence analysis of lattice Boltzmann methods for scalar\nnon-linear conservation laws, focusing on two-relaxation-times (TRT) schemes.\nUnlike Finite Difference\/Finite Volume methods, lattice Boltzmann schemes offer\nexceptional computational efficiency and parallelization capabilities. However,\ntheir monotonicity and $L^{\\infty}$-stability remain underexplored. Extending\nexisting results on simpler BGK schemes, we derive conditions ensuring that TRT\nschemes are monotone and stable by leveraging their unique relaxation\nstructure. Our analysis culminates in proving convergence of the numerical\nsolution to the weak entropy solution of the conservation law. Compared to BGK\nschemes, TRT schemes achieve reduced numerical diffusion while retaining\nprovable convergence. Numerical experiments validate and illustrate the\ntheoretical findings.",
        "As the lynchpin of all quantum correlations, quantum coherence is fundamental\nfor distinguishing quantum systems from classical ones and is essential for\nrealizing quantum advantages in areas such as computation, communication, and\nmetrology. In this study, we investigate the relationship between quantum\ncoherence and neutrino oscillations by mapping the neutrino state as a\nmulti-mode quantum system into qubit and qutrit frameworks. Our analysis\nextends beyond the commonly used $l_1$-norm and relative entropy of coherence\nto include all relevant measures of coherence such as robustness of coherence,\ncoherence concurrence, trace-norm distance measure of coherence, coherence of\nformation, Schatten-$p$-norm-based functionals, geometric coherence and\nlogarithmic coherence rank, each offering unique insights into the quantum\ncorrelations in these systems. Notably, while the $l_1$-norm and relative\nentropy-based measures apply to general quantum states, the other measures are\nparticularly relevant for entangled systems, highlighting the critical role of\nentanglement in neutrino oscillations. We present a detailed methodology for\ncalculating coherence measures in both two-flavor and three-flavor mixing\nscenarios, contributing to a deeper understanding of how quantum coherence\nmanifests and evolves in mode-entangled neutrino systems. Our findings\nemphasize the potential of these systems as robust candidates for quantum\ninformation tasks, facilitated by the weak interaction nature of neutrinos.",
        "Super-resolution techniques have the potential to reduce the computational\ncost of cosmological and astrophysical simulations. This can be achieved by\nenabling traditional simulation methods to run at lower resolution and then\nefficiently computing high-resolution data corresponding to the simulated\nlow-resolution data. In this work, we investigate the application of a\nWasserstein Generative Adversarial Network (WGAN) to increase the particle\nresolution of dark-matter-only simulations, reproducing and building on prior\nresults. Our WGAN models successfully generate high-resolution data with\nsummary statistics, including the power spectrum and halo mass function, that\nclosely match those of true high-resolution simulations. We also identify a\nlimitation of the WGAN model in the form of smeared features in the generated\nhigh-resolution snapshots, particularly in the shapes of dark-matter halos.",
        "3C 454.3 is a flat spectrum radio quasar (FSRQ) known for its high\nvariability across the electromagnetic spectrum, showing structural and flux\nvariability in its pc-scale jet, and correlated variability among frequency\nbands. This study aims to identify the structure, dynamics, and radiative\nprocesses common to the innermost regions of the blazar 3C 454.3. We\ninvestigate whether any jet component can be associated with $\\gamma-$ray\nemission and variability. We analyze the relationship between the variable\n$\\gamma-$ray emission and pc-scale jet properties in 3C 454.3 by combining\n$\\gamma-$ray data spanning twelve years with contemporaneous VLBA multi-epoch\nimages at 15 and 43 GHz. Spearman rank correlation tests are conducted to\ndetermine if the flux variability of any jet component is associated with\n$\\gamma-$ray variability. Core emission at 43 and 15 GHz strongly correlates\nwith $\\gamma-$ray emission. The 43 GHz core (Q0) contributes around 37$\\%$ of\nthe observed $\\gamma-$ray variability, while the 15 GHz core (K0) accounts for\n30$\\%$. A quasi-stationary component at 43 GHz, at a projected distance of 4.6\npc, correlates with the $\\gamma-$ray flux, accounting for 20$\\%$ of its\nemission between 2016 and 2021. We found a mobile component (Q3 between 2010.18\nand 2011.16) at 43 GHz with a projected distance between 0.8 and 2.3 pc and\napparent velocity of $\\beta_{app} = 9.9 \\pm 1.1$ c, accounting for\napproximately 28% of the $\\gamma-$ray emission. The observed simultaneous\nvariability in emission regions beyond the central parsec strongly suggests\nsynchrotron self-Compton (SSC) as the primary mechanism for $\\gamma-$ray\nproduction in these regions. Our findings demonstrate the existence of multiple\n$\\gamma-$ray emission regions within the blazar jet but also suggest that some\nof these regions are non-stationary over time.",
        "We measure surface brightness fluctuations in Chandra X-ray images of the\ncores of the galaxy clusters Abell 2029, Abell 2151, Abell 2107, RBS0533, and\nRBS0540. Their relatively structureless X-ray atmospheres exhibit the\nthermodynamic properties of cool cores including short central cooling times\nand low entropy. However, unlike typical cool-core clusters, molecular gas,\nstar formation, and bubbles associated with radio jets are faint or absent near\ntheir central galaxies. Four clusters show typical gas density fluctuation\namplitudes of $\\sim$ 10 per cent on the scales probed, apart from RBS0540,\nwhich exhibits lower amplitudes, suggesting that its gas is mildly disturbed.\nUnder the assumption that gas density fluctuations are indicative of random gas\nvelocities, we estimate scale-dependent velocity amplitudes of gas motions\nacross all studied clusters, which range from 100 km\/s to 200 km\/s in Abell\n2029, Abell 2151, and Abell 2107. These velocity estimates are comparable to\nthe atmospheric velocity dispersion in the Perseus cluster measured by the\nHitomi X-ray Observatory. The turbulent heating rates implied by our\nmeasurements are of the same order as the radiative cooling rates. Our results\nsuggest that atmospheric sloshing and perhaps turbulent motion may aid radio\njets in stabilizing atmospheric cooling.",
        "We propose a machine-learning tool that yields causal inference on text in\nrandomized trials. Based on a simple econometric framework in which text may\ncapture outcomes of interest, our procedure addresses three questions: First,\nis the text affected by the treatment? Second, which outcomes is the effect on?\nAnd third, how complete is our description of causal effects? To answer all\nthree questions, our approach uses large language models (LLMs) that suggest\nsystematic differences across two groups of text documents and then provides\nvalid inference based on costly validation. Specifically, we highlight the need\nfor sample splitting to allow for statistical validation of LLM outputs, as\nwell as the need for human labeling to validate substantive claims about how\ndocuments differ across groups. We illustrate the tool in a proof-of-concept\napplication using abstracts of academic manuscripts.",
        "In previous studies, the propagation of extreme events across nodes in\nmonolayer networks has been extensively studied. In this work, we extend this\ninvestigation to explore the propagation of extreme events between two distinct\nlayers in a multiplex network. We consider a two-layer network, where one layer\nis globally coupled and exhibits extreme events, while the second layer remains\nuncoupled. The interlayer connections between the layers are either\nunidirectional or bidirectional. We find that unidirectional coupling between\nthe layers can induce extreme events in the uncoupled layer, whereas\nbidirectional coupling tends to mitigate extreme events in the globally coupled\nlayer. To characterize extreme and non-extreme states, we use probability plots\nto identify distinct regions in the parameter space. Additionally, we study the\nrobustness of extreme events emergence by examining various network topologies\nin the uncoupled layer. The mechanism behind the occurrence of extreme events\nis explored, with a particular focus on the transition from asynchronous states\nto a fully synchronized excitable state. For numerical simulations, we use\nnonidentical FitzHugh-Nagumo neurons at each node, which captures the dynamical\nbehavior of both coupled and uncoupled layers. Our findings suggest that\nextreme events in the uncoupled layer emerge through the gradual disappearance\nof disorder, accompanied by occasional bursts of synchronized activity. Results\nobtained in this work will serve a starting point in understanding the dynamics\nbehind the propagation of extreme events in real-world networks.",
        "Three-dimensional direct numerical simulations (DNS) are implemented to\ninvestigate the energy cascade rate in compressible isothermal\nmagnetohydrodynamic (MHD) turbulence. Utilizing an exact law derived from the\nK\\'arm\\'an-Howarth equation, we examine the contributions of flux and non-flux\nterms to the cascade rate across a broad range of sonic and Alfv\\'enic Mach\nnumbers, from subsonic to supersonic regimes and varying mean magnetic fields.\nCascade rates are computed using on-grid 3-D decomposition and two plasma\nincrement approaches: signed and absolute values. Anisotropy induced by strong\nmagnetic fields is analyzed through angular-dependent scaling of the cascade\nterms. Moreover, the increment calculation method significantly influences the\nrelative contributions of flux and non-flux terms, with absolute methods\ntending to overestimate the latter. These findings extend current studies of\ncompressible turbulence and offer critical insights into energy transfer\nmechanisms relevant to many astrophysical phenomena.",
        "Social institutions are systems of shared norms and rules that regulate\npeople's behaviors, often emerging without external enforcement. They provide\ncriteria to distinguish cooperation from defection and establish rules to\nsustain cooperation, shaped through long-term trial and error. While principles\nfor successful institutions have been proposed, the mechanisms underlying their\nemergence remain poorly understood. Here, we introduce the evolutionary\ndynamical-systems game, a framework that couples game actions with\nenvironmental dynamics and explores the evolution of cognitive frameworks for\ndecision-making. We analyze a minimal model of common-pool resource management,\nwhere resources grow naturally and are harvested. Players use decision-making\nfunctions to determine whether to harvest at each step, based on environmental\nand peer monitoring. As these functions evolve, players detect selfish\nharvesting and punish it by degrading the environment through harvesting. This\nprocess leads to the self-organization of norms that classify harvesting\nactions as cooperative, defective, or punitive. The emergent norms for\n``cooperativeness'' and rules of punishment serve as institutions. The\nenvironmental and players' states converge to distinct modes characterized by\nlimit-cycles, representing temporal regularities in socio-ecological systems.\nThese modes remain stable despite slight variations in decision-making,\nillustrating the stability of institutions. The evolutionary robustness of\ndecision-making functions serves as a measure of the evolutionary favorability\nof institutions, highlighting the role of plasticity in responding to diverse\nopponents. This work introduces foundational concepts in evolutionary\ndynamical-systems games and elucidates the mechanisms underlying the\nself-organization of institutions by modeling the interplay between ecological\ndynamics and human decision-making.",
        "M dwarfs are the most common stars in the galaxy, with long lifespans, a high\noccurrence rate of rocky planets, and close-in habitable zones. However, high\nstellar activity in the form of frequent flaring and any associated coronal\nmass ejections may drive atmospheric escape with the bombardment of radiation\nand high-energy particles, drastically impacting the habitability of these\nsystems. The stellar latitude where flares and coronal mass ejections occur\ndetermines the space weather that exoplanets are subject to, with high-energy\nparticle events associated with equatorial flares producing significant\natmospheric erosion. However, the flaring latitudes for M dwarfs remain largely\nunconstrained. To aid in the effort to locate these flaring regions we explore\nthe applicability of flare occultations using optical photometry to identify\nthe latitudes of flares. As a planet transits in front of an ongoing flare the\ntiming and geometry of the transit can be used to constrain the latitude and\nlongitude of the flare. We predict the probability of detecting an occultation\nfor known transiting planets and eclipsing binaries. From this, we estimate\n3-22 detectable occultations exist within the TESS primary mission photometry,\nwith the majority occurring in eclipsing binary observations. To demonstrate\nthis technique, we analyze a candidate flare occultation event for the\neclipsing binary CM Draconis.",
        "The nature of low-impurity ferromagnetism remains a challenging problem in\nthe solid-state community. Despite initial experiments dating back to the\nmid-20th century, a comprehensive theoretical explanation and reliable ab\ninitio evaluations have remained elusive. The present research aims to bridge\nthis gap by refining first-principle calculations by elucidating the magnetic\nand electronic behavior of Pd1-xMx alloys (where M = Mn, Fe, Co, Ni). Our study\nincludes calculations of magnetic properties throughout the range of impurity\nconcentrations, from 1 to 100 atomic percent (at.%), where we estimate critical\nconcentrations and perform a comparative analysis for the listed alloys.\nFurthermore, electronic structure was analyzed, including the calculations of\natomic, spin, and orbital-resolved states density, and exploration of the\nspatial formation of magnetic clusters containing ferromagnetic impurities\nacross all concentration ranges.",
        "Electrically driven spin resonances in double quantum dots can lift the spin\nblockade and give rise to a resonant current. This current can probe the\nproperties of coupled two-spin states for different quantum dot configurations.\nUsing a Floquet-Markov quantum transport model we compute the resonant current\nfor different driving amplitudes and ac field frequencies in spin-orbit coupled\nquantum dots. We show that the resonant current has a very rich interference\npattern which can give valuable insight into the singlet-triplet state mixing.",
        "This study as part of an ongoing research effort, empirically examines the\nrelationship between foreign trade in the Istanbul Ataturk Airport Free Zone\nand exchange rate movements. Monthly data from 2003 to 2016 were analyzed\nthrough stationarity tests (Unit Root), followed by the Vector Autoregressive\n(VAR) model, Cointegration Analysis, and the Toda-Yamamoto Causality Test. The\nfindings indicate that the exchange rate does not significantly affect imports\nand exports in the free zone. This result suggests that free zones, due to\ntheir structural characteristics and operational framework, may be relatively\ninsulated from exchange rate fluctuations. The study contributes to the\nliterature by providing a focused analysis of a specific free zone in Turkiye,\nhighlighting the potential independence of free zone trade from exchange rate\nvolatility.",
        "We investigate the time-dependent evolution of the dipole phase shift induced\nby strong-field ionization (SFI) using attosecond transient absorption\nspectroscopy (ATAS) for time-delays where the pump-probe pulses overlap. We\nstudy measured and calculated time-dependent ATA spectra of the ionic 4d-5p\ntransition in xenon, and present the time-dependent line shape parameters in\nthe complex plane. We attribute the complex, attosecond-scale dynamics to the\ncontribution of three distinct processes: accumulation of ionization, transient\npopulation, and reversible population of excited states arising from\npolarization of the ground state.",
        "Hernando et al. (2008) introduced the fault-tolerant metric dimension\n$\\text{ftdim}(G)$, which is the size of the smallest resolving set $S$ of a\ngraph $G$ such that $S-\\left\\{s\\right\\}$ is also a resolving set of $G$ for\nevery $s \\in S$. They found an upper bound $\\text{ftdim}(G) \\le \\dim(G) (1+2\n\\cdot 5^{\\dim(G)-1})$, where $\\dim(G)$ denotes the standard metric dimension of\n$G$. It was unknown whether there exists a family of graphs where\n$\\text{ftdim}(G)$ grows exponentially in terms of $\\dim(G)$, until recently\nwhen Knor et al. (2024) found a family with $\\text{ftdim}(G) =\n\\dim(G)+2^{\\dim(G)-1}$ for any possible value of $\\dim(G)$. We improve the\nupper bound on fault-tolerant metric dimension by showing that $\\text{ftdim}(G)\n\\le \\dim(G)(1+3^{\\dim(G)-1})$ for every connected graph $G$. Moreover, we find\nan infinite family of connected graphs $J_k$ such that $\\dim(J_k) = k$ and\n$\\text{ftdim}(J_k) \\ge 3^{k-1}-k-1$ for each positive integer $k$. Together,\nour results show that \\[\\lim_{k \\rightarrow \\infty} \\left( \\max_{G: \\text{ }\n\\dim(G) = k} \\frac{\\log_3(\\text{ftdim}(G))}{k} \\right) = 1.\\] In addition, we\nconsider the fault-tolerant edge metric dimension $\\text{ftedim}(G)$ and bound\nit with respect to the edge metric dimension $\\text{edim}(G)$, showing that\n\\[\\lim_{k \\rightarrow \\infty} \\left( \\max_{G: \\text{ } \\text{edim}(G) = k}\n\\frac{\\log_2(\\text{ftedim}(G))}{k} \\right) = 1.\\] We also obtain sharp extremal\nbounds on fault-tolerance for adjacency dimension and $k$-truncated metric\ndimension. Furthermore, we obtain sharp bounds for some other extremal problems\nabout metric dimension and its variants. In particular, we prove an equivalence\nbetween an extremal problem about edge metric dimension and an open problem of\nErd\\H{o}s and Kleitman (1974) in extremal set theory.",
        "In physics, all dynamical equations that describe fundamental interactions\nare second order ordinary differential equations in the time derivatives. In\nthe literature, this property is traced back to a result obtained by\nOstrogradski in the mid 19th century, which is the technical basis of a 'no-go'\ntheorem for higher order theories. In this work, we review the connection of\nsymmetry properties with the order of dynamical equations, before reconsidering\nOstrogradski's result. Then, we show how Ostrogradski's conclusion is reached\nby applying to higher order theories concepts and method that have been\nspecifically developed for second order theories. We discuss a potential lack\nof consistency in this approach, to support the claim that Ostrogradski's\nresult applies to a class of higher order theories that is nowhere\nrepresentative of generic ones: we support this claim by giving an example of a\nhigher-order Lagrangian that is asymptotically stable, but that would be\nunstable under Ostrogradski's criterion. We also conclude that, when\nconsidering higher order theories as fundamental, we may need to reconsider and\nextend the conceptual framework on which our standard treatment of second order\ntheories is based.",
        "Time-dependent partial differential equations are a significant class of\nequations that describe the evolution of various physical phenomena over time.\nOne of the open problems in scientific computing is predicting the behaviour of\nthe solution outside the given temporal region. Most traditional numerical\nmethods are applied to a given time-space region and can only accurately\napproximate the solution of the given region. To address this problem, many\ndeep learning-based methods, basically data-driven and data-free approaches,\nhave been developed to solve these problems. However, most data-driven methods\nrequire a large amount of data, which consumes significant computational\nresources and fails to utilize all the necessary information embedded\nunderlying the partial differential equations (PDEs). Moreover, data-free\napproaches such as Physics-Informed Neural Networks (PINNs) may not be that\nideal in practice, as traditional PINNs, which primarily rely on multilayer\nperceptrons (MLPs) and convolutional neural networks (CNNs), tend to overlook\nthe crucial temporal dependencies inherent in real-world physical systems. We\npropose a method denoted as \\textbf{PhysicsSolver} that merges the strengths of\ntwo approaches: data-free methods that can learn the intrinsic properties of\nphysical systems without using data, and data-driven methods, which are\neffective at making predictions. Extensive numerical experiments have\ndemonstrated the efficiency and robustness of our proposed method. We provide\nthe code at\n\\href{https:\/\/github.com\/PhysicsSolver\/PhysicsSolver}{https:\/\/github.com\/PhysicsSolver}.",
        "A Data Assimilation (DA) strategy based on an ensemble Kalman filter (EnKF)\nis used to enhance the predictive capabilities of scale resolving numerical\ntools for the analysis of flows exhibiting cyclic behaviour. More precisely, an\nensemble of numerical runs using Large Eddy Simulation (LES) for the\ncompressible steady-state flow rig is augmented via the integration of\nhigh-fidelity data. This observation is in the form of instantaneous velocity\nmeasurements, which are sampled at localized sensors in the physical domain.\nTwo objectives are targeted. The first one is the calibration of an unsteady\ninlet condition suitable to capture the cyclic flow investigated. The second\none is the analysis of the synchronization of velocity field predicted by the\nLES with the available observation. In order to reduce the computational costs\nrequired for this analysis, a hyper-localization procedure (HLEnKF) is proposed\nand it is integrated in the library CONES, tailored to perform fast online DA.\nThe proposed strategy performs a satisfactory calibration of the inlet\nconditions, and its robustness is assessed using two different prior\ndistributions for the free parameters optimized in this task. DA state\nestimation is efficient in obtaining accurate local synchronization of the\ninferred velocity fields with the observed data. The modal analysis of the\nkinetic energy of the flow field provides additional information on the quality\nof the reconstruction of the velocity field, which shows improvements. Thus,\nthe HLEnKF shows promising features for the calibration and the synchronization\nof scale-resolved turbulent flows, opening perspectives of applications for\ncomplex phenomena using advanced tools such as digital twins.",
        "Over the last decade, genome-wide association studies (GWAS) have\nsuccessfully identified numerous genetic variants associated with complex\ndiseases. These associations have the potential to reveal the molecular\nmechanisms underlying complex diseases and lead to the identification of novel\ndrug targets. Despite these advancements, the biological pathways and\nmechanisms linking genetic variants to complex diseases are still not fully\nunderstood. Most trait-associated variants reside in non-coding regions and are\npresumed to influence phenotypes through regulatory effects on gene expression.\nYet, it is often unclear which genes they regulate and in which cell types this\nregulation occurs. Transcriptome-wide association studies (TWAS) aim to bridge\nthis gap by detecting trait-associated tissue gene expression regulated by GWAS\nvariants. However, traditional TWAS approaches frequently overlook the critical\ncontributions of trans-regulatory effects and fail to integrate comprehensive\nregulatory networks. Here, we present a novel framework that leverages\ntissue-specific gene regulatory networks (GRNs) to integrate cis- and\ntrans-genetic regulatory effects into the TWAS framework for complex diseases.\nWe validate our approach using coronary artery disease (CAD), utilizing data\nfrom the STARNET project, which provides multi-tissue gene expression and\ngenetic data from around 600 living patients with cardiovascular disease.\nPreliminary results demonstrate the potential of our GRN-driven framework to\nuncover more genes and pathways that may underlie CAD. This framework extends\ntraditional TWAS methodologies by utilizing tissue-specific regulatory insights\nand advancing the understanding of complex disease genetic architecture.",
        "Previous work of Chan--Church--Grochow and Baker--Wang shows that the set of\nspanning trees in a plane graph $G$ is naturally a torsor for the Jacobian\ngroup of $G$. Informally, this means that the set of spanning trees of $G$\nnaturally forms a group, except that there is no distinguished identity\nelement. We generalize this fact to graphs embedded on orientable surfaces of\narbitrary genus, which can be identified with ribbon graphs. In this\ngeneralization, the set of spanning trees of $G$ is replaced by the set of\nspanning quasi-trees of the ribbon graph, and the Jacobian group of $G$ is\nreplaced by the Jacobian group of the associated regular orthogonal matroid $M$\n(along with an associated regular representation of $M$). Our proof shows, more\ngenerally, that the family of \"BBY torsors\" constructed by Backman--Baker--Yuen\nand later generalized by Ding admit natural generalizations to (regular\nrepresentations of) regular orthogonal matroids. In addition to shedding light\non the role of planarity in the earlier work mentioned above, our results\nrepresent one of the first substantial applications of orthogonal matroids\n(also called \"even delta-matroids\" or \"Lagrangian orthogonal matroids\") to a\nnatural combinatorial problem about graphs.",
        "Stellar occultations are an ideal way to characterize the physical and\norbital properties of trans-Neptunian binary systems. In this research note, we\ndetail the prediction and observation of a stellar occultation observed with\nNASA's IRTF on March 16$^{\\mathrm{th}}$, 2025 (UT), with drop-outs from both\nthe dwarf planet Haumea and its smaller satellite Namaka. This occultation\nplaces a lower limit of 83 $\\pm$ 2 km on Namaka's diameter. We also discuss the\npossibility that this detection could help to constrain the orbit of Namaka,\nmeasure Haumea's gravitational harmonics, and provide a path to measuring the\ninternal structure of Haumea.",
        "Let $(\\pi,V)$ be a smooth representation of a compact Lie group $G$ on a\nquasi-complete locally convex complex topological vector space. We show that\nthe Lie algebra cohomology space $\\mathrm{H} ^\\bullet(\\mathfrak{u}, V)$ and the\nLie algebra homology space $\\mathrm{H}_\\bullet(\\mathfrak{u}, V)$ are both\nHausdorff, where $\\mathfrak{u}$ is the nilpotent radical of a parabolic\nsubalgebra of the complexified Lie algebra $\\mathfrak{g}$ of $G$.",
        "A proper vertex-coloring of a graph is $r$-dynamic if the neighbors of each\nvertex $v$ receive at least $\\min(r, \\mathrm{deg}(v))$ different colors. In\nthis note, we prove that if $G$ has a strong $2$-coloring number at most $k$,\nthen $G$ admits an $r$-dynamic coloring with no more than $(k-1)r+1$ colors. As\na consequence, for every class of graphs of bounded expansion, the $r$-dynamic\nchromatic number is bounded by a linear function in $r$. We give a concrete\nupper bound for graphs of bounded row-treewidth, which includes for example all\nplanar graphs.",
        "We present an interdisciplinary comparison between binary black hole systems\nand Radio Frequency (RF) Paul Traps, modeling the gravitational binary system\nas a rotating saddle near its center. This analogy connects these seemingly\nunrelated systems through the concept of dynamic stability. The rotating saddle\npotential is analytically tractable, allowing us to prove the existence of\nbounded charged particle trajectories under certain conditions. By focusing on\nstellar-mass black holes with a weak electric charge-a feature consistent with\nspecific astrophysical conditions that leaves the spacetime metric largely\nunaffected but can influence nearby particle interactions-we can neglect\ncomplicating factors such as magnetic fields from large accretion disks of\nheavier black holes or stellar winds. Our simulation results demonstrate that\ncharged particles can exhibit stable, non-orbital trajectories near the center\nof a binary system with charged stellar-mass black holes, providing unique\nthree-dimensional trapping primarily through gravity. This system is\ndistinctive in the literature for its non-orbital trapping mechanism. While\ntheoretically intriguing, this trapping relies on specific conditions,\nincluding nearly identical black hole masses. These types of non-orbital\ntrapping mechanisms could potentially allow for longer-lived plasma\nconfigurations, enhancing our ability to detect electromagnetic signatures from\nthese systems. The significance of this work lies in the novel comparison\nbetween a laboratory-scale quantum system and a larger astrophysical one,\nopening new avenues for exploring parallels between microscopic and cosmic\nphenomena across fourteen orders of magnitude in distance.",
        "Numerical security proofs offer a versatile approach for evaluating the\nsecret-key generation rate of quantum key distribution (QKD) protocols.\nHowever, existing methods typically require perfect source characterization,\nwhich is unrealistic in practice due to the presence of inevitable encoding\nimperfections and side channels. In this paper, we introduce a novel security\nproof technique based on semidefinite programming that can evaluate the\nsecret-key rate for both prepare-and-measure and measurement-device-independent\nQKD protocols when only partial information about the emitted states is\navailable, significantly improving the applicability and practical relevance\ncompared to existing numerical techniques. We demonstrate that our method can\noutperform current analytical approaches addressing partial state\ncharacterization in terms of achievable secret-key rates, particularly for\nprotocols with non-qubit encoding spaces. This represents a significant step\ntowards bridging the gap between theoretical security proofs and practical QKD\nimplementations.",
        "One-dimensional mechanical topological metamaterials belonging to the BDI\nsymmetry class (that is, preserving time-reversal, chiral, and particle-hole\nsymmetries) have been realized in discrete systems by exploiting arrangements\nof either masses and springs or acoustic resonators. This study presents an\napproach to embed one-dimensional BDI class metamaterials into fully continuous\nelastic two-dimensional waveguides. The design leverages the concept of\nevanescently coupled waveguides and defect resonances in order to reproduce the\nequivalent dynamics of prototypical BDI systems, such as the\nSu-Schrieffer-Heeger (SSH) model. Starting with a continuous plate waveguide\nwith a periodic distribution of pillars, resonant waveguides and local defects\nare created by either eliminating or by properly adjusting the height of\nselected pillars. The approach is validated by designing fully continuous\nelastic analogs of the SSH model and the dual SSH model. Numerical simulations\nconfirm the emergence of topological edge modes at the interface of\ntopologically distinct systems. In addition, edge modes in the elastic analog\nof the dual SSH model are shown to be Majorana-like modes.",
        "We study exponents of thickness in Frieseck-James-M\\\"uller's inequalities for\nshells. We derive the following results: (a) the exponent of thickness\n$\\mu(S)\\leq15\/8$ if the middle surface $S$ is parabolic; (b) the exponent of\nthickness $\\mu(S)\\leq11\/6$ if the middle surface $S$ is a minimal surface with\nnegative curvature; (c) the exponent of thickness $\\mu(S)\\leq11\/6$ if the\nmiddle surface $S$ is a ruled surface with negative curvature. The exponents of\nthickness in Frieseck-James-M\\\"uller's inequalities for thin shells represent\nthe relationship between rigidity and thickness $h$ of a shell when the large\ndeformations take place, i. e., the rigidity of the shell related to the\nthickness $h$ is $$Ch^{\\mu(S)}.$$ Thus the above results of $\\mu(S)<2$ show\nthat those shells are strictly more rigid than plates since $\\mu(S)=2$ for\nplates. Moreover, we present another result which shows that when $\\mu(S)<2,$\nany $W^{2,2}$ isometry of the middle surface is rigid."
      ]
    }
  },
  {
    "id":2411.16961,
    "research_type":"applied",
    "start_id":"b7",
    "start_title":"Pathology image analysis using segmentation deep learning algorithms",
    "start_abstract":"With the rapid development of image scanning techniques and visualization software, whole slide imaging (WSI) is becoming a routine diagnostic method. Accelerating clinical diagnosis from pathology images and automating image analysis efficiently and accurately remain significant challenges. Recently, deep learning algorithms have shown great promise in pathology image analysis, such as in tumor region identification, metastasis detection, and patient prognosis. Many machine learning algorithms, including convolutional neural networks, have been proposed to automatically segment pathology images. Among these algorithms, segmentation deep learning algorithms such as fully convolutional networks stand out for their accuracy, computational efficiency, and generalizability. Thus, deep learning\u2013based pathology image segmentation has become an important tool in WSI analysis. In this review, the pathology image segmentation process using deep learning algorithms is described in detail. The goals are to provide quick guidance for implementing deep learning into pathology image analysis and to provide some potential ways of further improving segmentation performance. Although there have been previous reviews on using machine learning methods in digital pathology image analysis, this is the first in-depth review of the applications of deep learning algorithms for segmentation in WSI analysis.",
    "start_categories":[
      "cs.NE"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b9"
      ],
      "title":[
        "Comparative gene expression profiles of intestinal transporters in mice, rats and humans"
      ],
      "abstract":[
        "We have studied gene expression profiles of intestinal transporters in model animals and humans. Total RNA was isolated from duodenum and the mRNA expression was measured using Affymetrix GeneChip oligonucleotide arrays. Detected genes from the intestine of mice, rats, and humans were about 60% of 22,690 sequences, 40% of 8739, and 47% of 12,559, respectively. A total of 86 genes involving transporters expressed in mice, 50 genes in rats, and 61 genes in humans were detected. Mice exhibited abundant mRNA expressions for peptide transporter HPT1, amino acid transporters CSNU3, CT1 and ASC1, nucleoside transporter CNT2, organic cation transporter SFXN1, organic anion transporter NBC3, glucose transporter SGLT1, and fatty acid transporters FABP1 and FABP2. Rats showed high expression profiles of peptide transporter PEPT1, amino acid transporters CSNU1 and 4F2HC, nucleoside transporter CNT2, organic cation transporter OCT5, organic anion transporter SDCT1, glucose transporter GLUT2 and GLUT5, and folate carrier FOLT. In humans, the highly expressed genes were peptide transporter HPT1, amino acid transporters LAT3, 4F2HC and PROT, nucleoside transporter CNT2, organic cation transporter OCTN2, organic anion transporters NADC1, NBC1 and SBC2, glucose transporters SGLT1 and GLUT5, multidrug resistance-associated protein RHO12, fatty acid transporters FABP1 and FABP2, and phosphate carrier PHC. Overall these data reveal diverse transcriptomic profiles for intestinal transporters among these species. Therefore, this transcriptional data may lead to more effective use of the laboratory animals as a model for oral drug development."
      ],
      "categories":[
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "Thermodynamic properties of fcc lead: A scalar and fully relativistic\n  first principle study",
        "QUOKKA-based understanding of outflows (QED) -- III. Outflow loading and\n  phase structure as a function of galactic environment",
        "Viscoelastic tensor and hydrodynamics of altermagnets",
        "Star Formation Rates, Metallicities, and Stellar Masses on kpc-scales in\n  TNG50",
        "Euclid Quick Data Release (Q1). Combined Euclid and Spitzer galaxy\n  density catalogues at $z>$ 1.3 and detection of significant Euclid passive\n  galaxy overdensities in Spitzer overdense regions",
        "Vacuum axisymmetric gravitational collapse revisited: preliminary\n  investigation",
        "Asymptotic integrability and its consequences",
        "Bidirectional controlled quantum state preparation in high-dimensional\n  quantum system",
        "Multiple orthogonal polynomial ensembles of derivative type",
        "Bayesian Hierarchical Emulators for Multi-Level Models: BayHEm",
        "Scaffold-Assisted Window Junctions for Superconducting Qubit Fabrication",
        "A benchmark analysis of saliency-based explainable deep learning methods\n  for the morphological classification of radio galaxies",
        "Simulations of Magnetic Monopole Collisions",
        "First-principles study of dielectric properties of ferroelectric\n  perovskite oxides with on-site and inter-site Hubbard interactions",
        "When Less is More: Evolutionary Dynamics of Deception in a\n  Sender-Receiver Game",
        "Continuous-Time Analysis of Federated Averaging",
        "PIETOOLS 2024: User Manual",
        "Loss Functions for Inventory Control",
        "Effect of thickness on the maximum potential drop of current collectors",
        "Optical absorption and luminescence of $\\alpha$-LiV$_2$O$_5$ from the\n  Bethe Salpeter Equation",
        "Around the topological classification problem of polynomial maps: A\n  survey",
        "Ghost Kohnert posets",
        "Modularity theorems for abelian surfaces",
        "Subgroups of Bestvina-Brady groups",
        "Hall Coefficient of the Intercalated Graphite CaC$_6$ in the Uniaxial\n  CDW Ground State",
        "Torsion models for tensor-triangulated categories",
        "On properties of eigenvalue regions for monotone stochastic matrices",
        "Infinitely many solutions for elliptic system with Hamiltonian type",
        "Solar oblateness & asphericities temporal variations: outstanding some\n  unsolved issues"
      ],
      "abstract":[
        "This study investigates the thermodynamic properties of face-centered cubic\nlead (fcc-Pb) using ab-initio methods within the quasi-harmonic approximation\n(QHA), examining the influence of spin-orbit coupling (SOC) and the\nexchange-correlation functionals. Two types of ultrasoft pseudopotential\n(US-PP) are considered: one that excludes (scalar relativistic PP) and one that\nincludes the SOC effects (fully relativistic PP). Further, for each PP, we test\nthe performance of three popular exchange-correlation functionals:\nPerdew-Burke-Ernzerhof generalized gradient approximation (PBE) (Perdew et al.\nPhys. Rev. Lett. 77, 3865 (1996)), PBE modified for dense solids (PBEsol)\n(Perdew et al. Phys. Rev. Lett. 100, 136406 (2008)), and local density\napproximation (LDA) (Perdew et al. Phys. Rev. B 23, 5048 (1981)). We calculate\nthe Helmholtz free energy, incorporating lattice vibrations (phonons) and\nelectronic excitation contributions. The estimated equation of state (at 4 K\nand 301 K), phonon dispersions (at 100 K and 300 K), mode-Gr\\\"uneisen\nparameters ({\\gamma}q{\\eta}) (at 100 K), volume thermal expansion coefficient\n(\\b{eta}), isobaric heat capacity (CP), bulk modulus (BS), and thermodynamic\naverage Gr\\\"uneisen parameter ({\\gamma}) are compared with the available\nexperimental and theoretical studies. Moreover, the 0 K pressure-dependent\nelastic constant-coefficient (Cij) of fcc lead and Pugh ratio, Debye\ntemperature, and longitudinal and transverse sound velocities for\npolycrystalline lead are presented. The contributions of electronic excitations\nin all the thermodynamic properties are found to be negligible. With increasing\npressure, the role of spin-orbit effects decreases but does not vanish. Our\nfindings demonstrate that SOC leads to results distinct from the SR approach,\nbut agreement with the experiment is not consistently improved by including\nSOC.",
        "We present results from a suite of 3D high-resolution hydrodynamic\nsimulations of supernova-driven outflows from galactic disc regions with a\nrange of gas surface density, metallicity, and supernova scale height. We use\nthis suite to quantify how outflow properties -- particularly the loading\nfactors for mass, metallicity, and energy -- vary with these parameters. We\nfind that the winds fall into three broad categories: steady and hot,\nmultiphase and moderately bursty, and cool and highly bursty. The first of\nthese is characterised by efficient metal and energy loading but weak mass\nloading, the second by moderate loading of mass, metals, and energy, and the\nthird by negligible metal and energy loading but substantial mass loading. The\nmost important factor in determining the kind of wind a galaxy will produce is\nthe ratio of supernova to gas gas scale heights, with the latter set by a\ncombination of supernova rate, metallicity-dependent cooling rate, and the\ngravitational potential. These often combine in counterintuitive ways -- for\nexample increased cooling causes cold clouds to sink into the galactic midplane\nmore rapidly, lowering the volume-filling factor of dense gas and making the\nenvironment more favourable for strong winds. Our findings suggest that the\nnature of galactic winds is likely highly sensitive to phenomena such as\nrunaway stars occuring at a large height and dense gas and are poorly captured\nin most simulations, and that metal loading factors for type Ia supernovae may\nbe substantially larger than those for type II, with important implications for\ngalactic chemical evolution.",
        "We calculate the viscoelasticity tensor for altermagnets and formulate the\ncorresponding hydrodynamic equations. The anisotropy of altermagnetic Fermi\nsurfaces allows for additional terms in the viscoelasticity tensor and is\nmanifested in transport properties including electron and spin flows in a\nchannel and nonlocal responses. In the channel geometry, the altermagnetic spin\nsplitting leads to nontrivial spin density and spin current. Like the electric\ncurrent, the spin current acquires a Poiseuille profile for no-slip boundary\nconditions. In nonlocal responses, the altermagnetic anisotropy affects current\nstreamlines and electric potential distributions in the viscous regime. Our\nresults provide signatures of the hydrodynamic transport regime in altermagnets\npotentially facilitating its experimental studies and discovery.",
        "Integral field units (IFU) have extended our knowledge of galactic properties\nto kpc (or, sometimes, even smaller) patches of galaxies. These scales are\nwhere the physics driving galaxy evolution (feedback, chemical enrichment,\netc.) take place. Quantifying the spatially-resolved properties of galaxies,\nboth observationally and theoretically, is therefore critical to our\nunderstanding of galaxy evolution. To this end, we investigate\nspatially-resolved scaling relations within central galaxies\n($M_\\star>10^{9.0}$) at $z=0$ in IllustrisTNG. We examine both the resolved\nstar-forming main sequence (rSFMS) and the resolved mass-metallicity relation\n(rMZR) using $1~{\\rm kpc}\\times1~{\\rm kpc}$ maps of galaxies. We find that the\nrSFMS in IllustrisTNG is well-described by a power-law, but has some dependence\non the host galaxy's mass. Conversely, the rMZR for IllustrisTNG can be\ndescribed by a single power-law at low stellar mass surface density that\nflattens at high surface densities and is independent of host galaxy mass. We\nfind quantitative agreement in both the rSFMS and rMZR with recent IFU\nobservational campaigns. Furthermore, we argue that the rSFMS is an indirect\nresult of the Schmidt-Kennicutt (SK) law and local gas fraction relation, which\nare both independent of host galaxy properties. Finally, we expand upon a\nlocalized leaky-box model to study the evolution of idealized spaxels and find\nthat it provides a good description of these resolved relations. The degree of\nagreement, however, between idealized spaxels and simulated spaxels depends on\nthe `net' outflow rate for the spaxel, and the observed scaling relations\nindicate a preference for a low net outflow rate.",
        "Euclid will detect tens of thousands of clusters and protoclusters at\n$z$>1.3. With a total coverage of 63.1deg$^2$, the Euclid Quick Data Release 1\n(Q1) is large enough to detect tens of clusters and hundreds of protoclusters\nat these early epochs. The Q1 photometric redshift catalogue enables us to\ndetect clusters out to $z$ < 1.5; however, infrared imaging from Spitzer\nextends this limit to higher redshifts by using high local projected densities\nof Spitzer-selected galaxies as signposts for cluster and protocluster\ncandidates. We use Spitzer imaging of the Euclid Deep Fields (EDFs) to derive\ndensities for a sample of Spitzer-selected galaxies at redshifts $z$ > 1.3,\nbuilding Spitzer IRAC1 and IRAC2 photometric catalogues that are 95% complete\nat a magnitude limit of IRAC2=22.2, 22.6, and 22.8 for the EDF-S, EDF-F, and\nEDF-N, respectively. We apply two complementary methods to calculate galaxy\ndensities: (1) aperture and surface density; and (2) the Nth-nearest-neighbour\nmethod. When considering a sample selected at a magnitude limit of IRAC2 <\n22.2, at which all three EDFs are 95% complete, our surface density\ndistributions are consistent among the three EDFs and with the SpUDS blank\nfield survey. We also considered a deeper sample (IRAC2 < 22.8), finding that\n2% and 3% of the surface densities in the North and Fornax fields are 3$\\sigma$\nhigher than the average field distribution and similar to densities found in\nthe CARLA cluster survey. Our surface densities are also consistent with\npredictions from the GAEA semi-analytical model. Using combined Euclid and\nground-based i-band photometry we show that our highest Spitzer-selected galaxy\noverdense regions, found at $z$~1.5, also host high densities of passive\ngalaxies. This means that we measure densities consistent with those found in\nclusters and protoclusters at $z$>1.3.",
        "Validating the results of [A.M. Abrahams and C.R. Evans, Phys. Rev. Lett. 70,\n2980] poses a numerical challenge and has been inspiring a lot of research. We\njoin these efforts and present our first steps to achieve this goal: we discuss\na formulation of Einstein equations for a vacuum axisymmetric spacetime with\nvanishing twist in spherical-polar coordinates, its linearised approximation,\nand identify some problems in achieving numerically stable evolution at the\nthreshold of a black hole formation.",
        "We give a brief review of the concept of asymptotic integrability, which\nmeans that the Hamilton equations for the propagation of short-wavelength\npackets along a smooth, large-scale background wave have an integral\nindependent of the initial conditions. The existence of such an integral leads\nto a number of important consequences, which include, besides the direct\napplication to the packets propagation problems, Hamiltonian theory of narrow\nsolitons motion and generalized Bohr-Sommerfeld rule for parameters of solitons\nproduced from an intensive initial pulse. We show that in the case of systems\nwith two wave variables and exact fulfillment of the asymptotic integrability\ncondition, the `quantization' of mechanical systems, associated with the\nadditional integrals, yields the Lax pairs for a number of typical completely\nintegrable equations, and this sheds new light on the origin of the complete\nintegrability in nonlinear wave physics.",
        "High-dimensional quantum system exhibits unique advantages over the qubit\nsystem in some quantum information processing tasks. We present a program for\nimplementing deterministic bidirectional controlled remote quantum state\npreparation (BCRSP) in arbitrary $N$-dimensional (quNit) system. By introducing\ntwo generalized Greenberger-Horne-Zeilinger (GHZ) states as quantum channels,\ntwo communication parties can simultaneously prepare a single-particle\nhigh-dimensional state at each other's site under the control of Charlie.\nCompared with the previous counterparts, the significant advantage of our\nscheme is that the high-dimensional CNOT operations are not required. Moreover,\nthe performance our scheme are evaluated. The evaluation of the performance\nshows that if the quNit is encoded in the spatial mode of single photons, our\nscheme can be accomplished solely using only linear optical elements.",
        "We characterize the biorthogonal ensembles that are both a multiple\northogonal polynomial ensemble and a polynomial ensemble of derivative type\n(also called a P\\'olya ensemble). We focus on the two notions of derivative\ntype that typically appear in connection with the squared singular values of\nproducts of invertible random matrices and the eigenvalues of sums of Hermitian\nrandom matrices. Essential in the characterization is the use of the Mellin and\nLaplace transform: we show that the derivative type structure, which is a\npriori analytic in nature, becomes algebraic after applying the appropriate\ntransform. Afterwards, we explain how these notions of derivative type can be\nused to provide a partial solution to an open problem related to orthogonality\nof the finite finite free multiplicative and additive convolution from finite\nfree probability. In particular, we obtain families of multiple orthogonal\npolynomials that (de)compose naturally using these convolutions.",
        "Decision making often uses complex computer codes run at the exa-scale (10e18\nflops). Such computer codes or models are often run in a hierarchy of different\nlevels of fidelity ranging from the basic to the very sophisticated. The top\nlevels in this hierarchy are expensive to run, limiting the number of possible\nruns. To make use of runs over all levels, and crucially improve emulation at\nthe top level, we use multi-level Gaussian process emulators (GPs). We will\npresent a new method of building GP emulators from hierarchies of models. In\norder to share information across the different levels, l=1,...,L, we define\nthe form of the prior of the l+1th level to be the posterior of the lth level,\nhence building a Bayesian hierarchical structure for the top Lth level. This\nenables us to not only learn about the GP hyperparameters as we move up the\nmulti-level hierarchy, but also allows us to limit the total number of\nparameters in the full model, whilst maintaining accuracy.",
        "The superconducting qubit is one of the promising directions in realizing\nfault-tolerant quantum computing (FTQC), which requires many high-quality\nqubits. To achieve this, it is desirable to leverage modern semiconductor\nindustry technology to ensure quality, uniformity, and reproducibility.\nHowever, conventional Josephson junction fabrication relies mainly on\nresist-assistant double-angle evaporation, posing integration challenges. Here,\nwe demonstrate a lift-off-free qubit fabrication that integrates seamlessly\nwith existing industrial technologies. This method employs a silicon oxide\n(SiO$_2$) scaffold to define an etched window with a well-controlled size to\nform a Josephson junction. The SiO$_2$, which has a large dielectric loss, is\netched away in the final step using vapor HF leaving little residue. This\nWindow junction (WJ) process mitigates the degradation of qubit quality during\nfabrication and allows clean removal of the scaffold. The WJ process is\nvalidated by inspection and Josephson junction measurement. The scaffold\nremoval process is verified by measuring the quality factor of the resonators.\nFurthermore, compared to scaffolds fabricated by plasma-enhanced chemical vapor\ndeposition (PECVD), qubits made by WJ through physical vapor deposition (PVD)\nachieve relaxation time up to $57\\,\\mu\\text{s}$. Our results pave the way for a\nlift-off-free qubit fabrication process, designed to be compatible with modern\nfoundry tools and capable of minimizing damage to the substrate and material\nsurfaces.",
        "This work proposes a saliency-based attribution framework to evaluate and\ncompare 10 state-of-the-art explainability methods for deep learning models in\nastronomy, focusing on the classification of radio galaxy images. While\nprevious work has primarily emphasized classification accuracy, we prioritize\nmodel interpretability. Qualitative assessments reveal that Score-CAM,\nGrad-CAM, and Grad-CAM++ consistently produce meaningful attribution maps,\nhighlighting the brightest regions of FRI and FRII galaxies in alignment with\nknown astrophysical features. In contrast, other methods often emphasize\nirrelevant or noisy areas, reducing their effectiveness.",
        "In this paper, we investigate the scattering of BPS magnetic monopoles\nthrough numerical simulations. We present an ansatz for various multi-monopole\nconfigurations suitable for analyzing monopole scattering processes. Our study\nincludes planar scattering scenarios involving two, three, and four monopoles,\nas well as non-planar processes where three and four monopoles form\nintermediate tetrahedral and cubic states, respectively. Our observations align\nwith the theoretical predictions of the moduli space approximation.\nFurthermore, we extend our analysis to relativistic velocities and explore\nparameters beyond the BPS limit.",
        "We study the atomic and electronic structures of ferroelectric perovskite\noxides, BaTiO$_3$, LiNbO$_3$, and PbTiO$_3$ using ab initio extended Hubbard\nfunctionals in which the on-site and inter-site Hubbard interactions are\ndetermined self-consistently, adapted from the pseudohybrid density functional\nproposed by Agapito-Curtarolo-Buongiorno Nardelli. Band structures,\nferroelectric distortions, polarization, Born effective charges, and switching\nbarriers are calculated with extended Hubbard functionals, that are compared\nwith those using local density approximation (LDA), generalized gradient\napproximation (GGA), and Hybrid (HSE06) functionals. The properties of all\nthree compounds calculated by extended Hubbard functionals are in good\nagreement with experimental data. We find a substantial increase in band gaps\ndue to the inter-site Coulomb interactions, which show better agreement with\n$GW$ results compared to those from LDA and GGA functionals. The crucial role\nof the inter-site Coulomb interactions in restoring the suppressed polar\ninstability, which is computed when only the on-site Hubbard interactions are\nconsidered, is also highlighted. Overall, we find that the properties\ncalculated using our extended Hubbard functionals exhibit trends similar to\nthose obtained with the HSE06 functional, while reducing computational costs by\nover an order of magnitude. Thus, we propose that the current method is\nwell-suited for high-throughput calculations for perovskite oxides, offering\nsignificantly improved accuracy in computing band gap and other related\nphysical properties such as the shift current photovoltaic effect and band\nalignments in ferroelectric heterostructures.",
        "The spread of disinformation poses a significant threat to societal\nwell-being. We analyze this phenomenon using an evolutionary game theory model\nof the sender-receiver game, where senders aim to mislead receivers and\nreceivers aim to discern the truth. Using a combination of replicator\nequations, finite-size scaling analysis, and extensive Monte Carlo simulations,\nwe investigate the long-term evolutionary dynamics of this game. Our central\nfinding is a counterintuitive threshold phenomenon: the role (sender or\nreceiver) with the larger difference in payoffs between successful and\nunsuccessful interactions is surprisingly more likely to lose in the long run.\nWe show that this effect is robust across different parameter values and arises\nfrom the interplay between the relative speeds of evolution of the two roles\nand the ability of the slower evolving role to exploit the fixed strategy of\nthe faster evolving role. Moreover, for finite populations we find that the\ninitially less frequent strategy of the slower role is more likely to fixate in\nthe population. The initially rarer strategy in the less-rewarded role is,\nparadoxically, more likely to prevail.",
        "Federated averaging (FedAvg) is a popular algorithm for horizontal federated\nlearning (FL), where samples are gathered across different clients and are not\nshared with each other or a central server. Extensive convergence analysis of\nFedAvg exists for the discrete iteration setting, guaranteeing convergence for\na range of loss functions and varying levels of data heterogeneity. We extend\nthis analysis to the continuous-time setting where the global weights evolve\naccording to a multivariate stochastic differential equation (SDE), which is\nthe first time FedAvg has been studied from the continuous-time perspective. We\nuse techniques from stochastic processes to establish convergence guarantees\nunder different loss functions, some of which are more general than existing\nwork in the discrete setting. We also provide conditions for which FedAvg\nupdates to the server weights can be approximated as normal random variables.\nFinally, we use the continuous-time formulation to reveal generalization\nproperties of FedAvg.",
        "The PIETOOLS 2024 User Manual describes all the features of version 2024 of\nthe MATLAB toolbox PIETOOLS for the analysis and control of Partial Integral\nEquations (PIEs). The manual is aimed to guide, with examples, first-time users\nto four fundamental features of PIETOOLS: converting coupled ODE-PDEs, DDEs,\nDDFs, etc., to PIE representation; analysis of stability and input-output\nproperties of PIEs; design of optimal observers and controllers for PIEs;\nsimulation of open- and closed-loop PIE systems. The use of PIETOOLS is not\nlimited to the features described above. However, the manual focuses on these\nfeatures to provide a holistic understanding of the workflow of PIETOOLS, which\nwill serve as a foundation to develop more complicated programs, for example,\nthe design of boundary feedback controllers, robust observers, robust\ncontrollers, etc..",
        "In this paper, we provide analytic expressions for the first-order loss\nfunction, the complementary loss function and the second-order loss function\nfor several probability distributions. These loss functions are important\nfunctions in inventory optimization and other quantitative fields. For several\nreasons, which will become apparent throughout this paper, the implementation\nof these loss functions prefers the use of an analytic expression, only using\nstandard probability functions. However, complete and consistent references of\nanalytic expressions for these loss functions are lacking in literature. This\npaper aims to close this gap and can serve as a reference for researchers,\nsoftware engineers and practitioners that are concerned with the optimization\nof a quantitative system. This should lead directly to easily using different\nprobability distributions in quantitive models which is at the core of\noptimization. Also, this paper serves as a broad introduction to loss functions\nand their use in inventory control.",
        "The basic principle for achieving high-power capability on an electrochemical\nenergy storage cell is minimizing the overall resistance. The resistance due to\ncurrent collecting systems has not received sufficient attention in the past,\npresumably because it was not considered of significance for low-power\nbatteries and supercapacitors. However, the necessity of high-power cells has\nreduced other sources of the inner resistance, and the current collector\npotential drop has become more important. Moreover, the miniaturization of\nenergy storage devices could increase the ohmic loses in current collectors. In\nthis work, we have developed an electrical model to assess the effect of the\ncurrent collector thickness on the maximum potential drop. We have found that\nthe thickness of current collectors is a critical parameter that can increase\nthe maximum potential drop drastically. Indeed, the maximum potential drop of\ncurrent collectors remains almost constant for thicknesses greater than 500 lm,\nbut below this value, there is an inverse relationship between the maximum\npotential drop and the thickness. We have also analyzed the effect of the\nmaterial and tab position in the maximum potential drop.",
        "$\\alpha$-Li$_x$V$_2$O$_5$ is obtained by intercalating Li between the layers\nof V$_2$O$_5$. The partial filling of the split-off conduction band by electron\ndonation from Li leads to significant changes in optical properties. Here we\nstudy the electronic band structure of $\\alpha$-LiV$_2$O$_5$ using\nquasiparticle self-consistent (QS) $GW$ calculations and the optical dielectric\nfunction by means of the Bethe Salpeter equation. We find a very strong optical\nabsorption band related to transitions between the filled V-$d_{xy}$ like\nstates to the empty ones with strong polarization along the $a$-direction. We\nrelate this to recent experimental observations of cathodoluminescence (CL) in\nwhich a supression of the CL was observed upon addition of Li.",
        "The study of the topology of polynomial maps originates from classical\nquestions in affine geometry, such as the Jacobian Conjecture, as well as from\nworks of Whitney, Thom, and Mather in the 1950-70s on diffeomorphism types of\nsmooth maps. During that period, Thom came up with a famous construction of a\none-dimensional family of real polynomial maps having fixed degree nine and\ninfinitely many topological types. In his convention, a topological type of a\nmap is preserved precisely when it is composed with a homeomorphism on the\nsource space, and one on the target space.\n  Thom also conjectured that for each pair $(n,d)$, any family of $n$--variate,\ndegree--$d$ (complex, or real) polynomial functions has at most finitely-many\ntopological types. Soon after, a collection of results by several\nmathematicians throughout the 1970s and 1980s settled this conjecture, and\nsolved its subsequent generalization to polynomial maps.\n  In this survey, we outline the historical context and highlight a range of\nsignificant works from the 1950s to the present day that lead to the current\nstate of the art in the study of polynomial maps' topology. The focal point of\nthis survey is to shed some light on the ensuing classification problem of\ntopological types of polynomial maps. The presentation is achieved by making a\ngentle introduction to several other prominent questions in affine geometry,\nall of which are recounted through the lens of the above classification\nproblem.",
        "Recently, Pan and Yu showed that Lascoux polynomials can be defined in terms\nof certain collections of diagrams consisting of unit cells arranged in the\nfirst quadrant. Starting from certain initial diagrams, one forms a finite set\nof diagrams by applying two types of moves: Kohnert and ghost moves. Both moves\ncause at most one cell to move to a lower row with ghost moves leaving a new\n\"ghost cell\" in its place. Each diagram formed in this way defines a monomial\nin the associated Lascoux polynomial. Restricting attention to diagrams formed\nby applying sequences of only Kohnert moves in the definition of Lascoux\npolynomials, one obtains the family of key polynomials. Recent articles have\nconsidered a poset structure on the collections of diagrams formed when one\nuses only Kohnert moves. In general, these posets are not \"well-behaved,\" not\nusually having desirable poset properties. Here, as an intermediate step to\nstudying the analogous posets associated with Lascoux polynomials, we consider\nthe posets formed by restricting attention to those diagrams formed by using\nonly ghost moves. Unlike in the case of Kohnert posets, we show that such\n\"ghost Kohnert posets\" are always ranked join semi-lattices. In addition, we\nestablish a necessary condition for when ghost Kohnert posets are bounded and,\nconsequently, lattices.",
        "We prove the modularity of a positive proportion of abelian surfaces over\n$\\mathbf{Q}$. More precisely, we prove the modularity of abelian surfaces which\nare ordinary at $3$ and are $3$-distinguished, subject to some assumptions on\nthe $3$-torsion representation (a \"big image\" hypothesis, and a technical\nhypothesis on the action of a decomposition group at $2$). We employ a 2-3\nswitch and a new classicality theorem (in the style of Lue Pan) for ordinary\n$p$-adic Siegel modular forms.",
        "In \"Subgroups of Graph Groups\", 1987, J. Alg., Droms proved that all the\nsubgroups of a right-angled Artin group (RAAG) defined by a finite simplicial\ngraph $\\Gamma$ are themselves RAAGs if, and only if, $\\Gamma$ has no induced\nsquare graph nor line-graph of length $3$. The present work provides a similar\nresult for specific normal subgroups of RAAGs, called Bestvina-Brady groups: We\ncharacterize those graphs in which every subgroup of such a group is itself a\nRAAG. In turn, we confirm several Galois theoretic conjectures for the pro-$p$\ncompletions of these groups.",
        "We evaluate the Hall coefficient characterising magnetotransport in an\nintercalated graphite CaC$_6$ with the Fermi surface reconstructed by an\nuniaxial charge density wave from closed pockets to open sheets. As the typical\norder parameter, corresponding to the pseudo-gap in electronic spectrum and\nconsequently to spacing between electron trajectories in reciprocal space, is\nof the order of $10^2$K, magnetic breakdown in strong experimentally achievable\nfields of the order of 10T is inevitable. The classical expressions for the\ncomponents of the magnetoconductivity tensor are strongly modified by magnetic\nfield-assisted over-gap tunneling causing quantum interference. Due to magnetic\nbreakdown, all magnetoconductivity components undergo strong quantum\noscillations reflected in the Hall coefficient. In their nature, these are\ndifferent than standard Shubnikov de Haas oscillations which would not appear\nin a system with an open Fermi surface.",
        "Given a rigidly-compactly generated tensor-triangulated category whose Balmer\nspectrum is finite dimensional and Noetherian, we construct a torsion model for\nit, which is equivalent to the original tensor-triangulated category. The\ntorsion model is determined in an adelic fashion by objects with singleton\nsupports. This categorifies the Cousin complex from algebra, and the process of\nreconstructing a spectrum from its monochromatic layers in chromatic stable\nhomotopy theory. This model is inspired by work of the second author in\nrational equivariant stable homotopy theory, and extends previous work of the\nauthors from the one-dimensional setting.",
        "Monotone stochastic matrices are stochastic matrices in which each row\nstochastically dominates the previous one. While the eigenvalue regions for\nstochastic matrices have been fully described by F.I. Karpelevich in 1951, this\nstudy focuses on the analysis of monotone matrices. This paper examines their\nspectral properties and establishes a reduction theorem stating that, for all n\nfrom 3 on, the eigenvalue region for the nxn monotone matrices is included in\nthose for the (n-1)x(n-1) stochastic matrices. Moreover, the eigenvalue region,\nalong with the corresponding realising matrices, is determined for monotone\nmatrices up till order 3.",
        "In this paper, we use Legendre-Fenchel transform and a space decomposition to\ncarry out Fountain theorem and dual Fountain theorem for the following elliptic\nsystem of Hamiltonian type: \\[ \\begin{cases} \\begin{aligned} -\\Delta u&=H_v(u,\nv) \\,\\quad&&\\text{in}~\\Omega,\\\\ -\\Delta v&=H_u(u, v)\n\\,\\quad&&\\text{in}~\\Omega,\\\\ u,\\,v&=0~~&&\\text{on} ~ \\partial\\Omega,\\\\\n\\end{aligned} \\end{cases} \\] where $N\\ge 1$, $\\Omega \\subset \\mathbb{R}^N$ is a\nbounded domain and $H\\in C^1( \\mathbb{R}^2)$ is strictly convex, even and\nsubcritical. We mainly present two results: (i) When $H$ is superlinear, the\nsystem has infinitely many solutions, whose energies tend to infinity. (ii)\nWhen $H$ is sublinear, the system has infinitely many solutions, whose energies\nare negative and tend to 0. As a byproduct, the Lane-Emden system under\nsubcritical growth has infinitely many solutions.",
        "Solar oblateness has been the subject of several studies dating back to the\nnineteenth century. Despite diffculties, both theoretical and observational,\ntangible results have been achieved. However, variability of the solar\noblateness with time is still poorly known. How the solar shape evolves with\nthe solar cycle has been a challenging problem. Analysis of the helioseismic\ndata, which are the most accurate measure of the solar structure up to now,\nleads to the determination of asphericity coeffcients which have been found to\nchange with time. We show here that by inverting even coeffcients of f-mode\noscillation frequency splitting to obtain the oblateness magnitude and its\ntemporal dependence can be inferred. It is found that the oblateness variations\nlag the solar activity cycles by about 3 years. A major change occurred between\nsolar cycles 23 and 24 is that the oblateness was greater in cycle 24 despite\nthe lower solar activity level. Such results may help to better understand the\nnear-subsurface layers as they strongly impacts the internal dynamics of the\nSun and may induce instabilities driving the transport of angular momentum."
      ]
    }
  },
  {
    "id":2411.16961,
    "research_type":"applied",
    "start_id":"b9",
    "start_title":"Comparative gene expression profiles of intestinal transporters in mice, rats and humans",
    "start_abstract":"We have studied gene expression profiles of intestinal transporters in model animals and humans. Total RNA was isolated from duodenum and the mRNA expression was measured using Affymetrix GeneChip oligonucleotide arrays. Detected genes from the intestine of mice, rats, and humans were about 60% of 22,690 sequences, 40% of 8739, and 47% of 12,559, respectively. A total of 86 genes involving transporters expressed in mice, 50 genes in rats, and 61 genes in humans were detected. Mice exhibited abundant mRNA expressions for peptide transporter HPT1, amino acid transporters CSNU3, CT1 and ASC1, nucleoside transporter CNT2, organic cation transporter SFXN1, organic anion transporter NBC3, glucose transporter SGLT1, and fatty acid transporters FABP1 and FABP2. Rats showed high expression profiles of peptide transporter PEPT1, amino acid transporters CSNU1 and 4F2HC, nucleoside transporter CNT2, organic cation transporter OCT5, organic anion transporter SDCT1, glucose transporter GLUT2 and GLUT5, and folate carrier FOLT. In humans, the highly expressed genes were peptide transporter HPT1, amino acid transporters LAT3, 4F2HC and PROT, nucleoside transporter CNT2, organic cation transporter OCTN2, organic anion transporters NADC1, NBC1 and SBC2, glucose transporters SGLT1 and GLUT5, multidrug resistance-associated protein RHO12, fatty acid transporters FABP1 and FABP2, and phosphate carrier PHC. Overall these data reveal diverse transcriptomic profiles for intestinal transporters among these species. Therefore, this transcriptional data may lead to more effective use of the laboratory animals as a model for oral drug development.",
    "start_categories":[
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b7"
      ],
      "title":[
        "Pathology image analysis using segmentation deep learning algorithms"
      ],
      "abstract":[
        "With the rapid development of image scanning techniques and visualization software, whole slide imaging (WSI) is becoming a routine diagnostic method. Accelerating clinical diagnosis from pathology images and automating image analysis efficiently and accurately remain significant challenges. Recently, deep learning algorithms have shown great promise in pathology image analysis, such as in tumor region identification, metastasis detection, and patient prognosis. Many machine learning algorithms, including convolutional neural networks, have been proposed to automatically segment pathology images. Among these algorithms, segmentation deep learning algorithms such as fully convolutional networks stand out for their accuracy, computational efficiency, and generalizability. Thus, deep learning\u2013based pathology image segmentation has become an important tool in WSI analysis. In this review, the pathology image segmentation process using deep learning algorithms is described in detail. The goals are to provide quick guidance for implementing deep learning into pathology image analysis and to provide some potential ways of further improving segmentation performance. Although there have been previous reviews on using machine learning methods in digital pathology image analysis, this is the first in-depth review of the applications of deep learning algorithms for segmentation in WSI analysis."
      ],
      "categories":[
        "cs.NE"
      ]
    },
    "list":{
      "title":[
        "Hotter and Colder: A New Approach to Annotating Sentiment, Emotions, and\n  Bias in Icelandic Blog Comments",
        "Global Hall-magnetohydrodynamic simulations of transition disks",
        "Vertical Federated Learning in Practice: The Good, the Bad, and the Ugly",
        "MIGHTEE: exploring the relationship between spectral index, redshift and\n  radio luminosity",
        "Storing quantum coherence in a quantum dot nuclear spin ensemble for\n  over 100 milliseconds",
        "Intrinsic charm and $D^+D^-$ asymmetry produced in proton-proton\n  collisions",
        "A note on partial polynomial functions, in memory of Marek Jarnicki",
        "Gaussian Universality of Products Over Split Reductive Groups and the\n  Satake Isomorphism",
        "Random Dynamical Systems on the circle without a finite orbit",
        "First photometric investigation of V517 Cam combined with ground-based\n  and TESS data",
        "Enhancing the charging performance of an atomic quantum battery",
        "BEARCUBS: A benchmark for computer-using web agents",
        "Interfacial spin-orbit coupling in superconducting hybrid systems",
        "Practical Spoofing Attacks on Galileo Open Service Navigation Message\n  Authentication",
        "Intelligent Gradient Boosting Algorithms for Estimating Strength of\n  Modified Subgrade Soil",
        "GIFT: Generated Indoor video frames for Texture-less point tracking",
        "Detecting Convolutional Codes: A Markovian Approach with LRT and DNN",
        "Interface reconstruction of adhering droplets for distortion correction\n  using glare points and deep learning",
        "Fine-tunings in nucleosynthesis and the emergence of life: Status and\n  perspectives",
        "STEMS: Spatial-Temporal Mapping Tool For Spiking Neural Networks",
        "Separate This, and All of these Things Around It: Music Source\n  Separation via Hyperellipsoidal Queries",
        "Robust Trajectory Generation and Control for Quadrotor Motion Planning\n  with Field-of-View Control Barrier Certification",
        "Iterative Feature Space Optimization through Incremental Adaptive\n  Evaluation",
        "Saint-Venant Estimates and Liouville-Type Theorems for the Stationary\n  MHD Equation in $\\mathbb{R}^3$",
        "Deep RC: A Scalable Data Engineering and Deep Learning Pipeline",
        "Hierarchies from deterministic non-locality in theory space Anderson\n  localisation",
        "To BEE or not to BEE: Estimating more than Entropy with Biased Entropy\n  Estimators",
        "Navigating Tomorrow: Reliably Assessing Large Language Models\n  Performance on Future Event Prediction",
        "Fits of $\\alpha_s$ from event-shapes in the three-jet region: extension\n  to all energies"
      ],
      "abstract":[
        "This paper presents Hotter and Colder, a dataset designed to analyze various\ntypes of online behavior in Icelandic blog comments. Building on previous work,\nwe used GPT-4o mini to annotate approximately 800,000 comments for 25 tasks,\nincluding sentiment analysis, emotion detection, hate speech, and group\ngeneralizations. Each comment was automatically labeled on a 5-point Likert\nscale. In a second annotation stage, comments with high or low probabilities of\ncontaining each examined behavior were subjected to manual revision. By\nleveraging crowdworkers to refine these automatically labeled comments, we\nensure the quality and accuracy of our dataset resulting in 12,232 uniquely\nannotated comments and 19,301 annotations. Hotter and Colder provides an\nessential resource for advancing research in content moderation and\nautomatically detectiong harmful online behaviors in Icelandic.",
        "Context. Transition disks (TDs) are a type of protoplanetary disk\ncharacterized by a central dust and gas cavity. The processes behind how these\ncavities are formed and maintained, along with their observed high accretion\nrates of $10^{-8} -10^{-7} \\, M_{\\odot} \\, \\mathrm{yr}^{-1}$, continue to be\nsubjects of active research. Aims. This work aims to investigate how the\ninclusion of the Hall effect (HE) alongside Ohmic resistivity (OR) and\nambipolar diffusion (AD) affects the structure of the TD. Of key interest is\nthe dynamical evolution of the cavity and whether it can indeed produce\ntransonic accretion, as predicted by theoretical models in order to account for\nthe observed high accretion rates despite the inner disk's low density.\nMethods. We present our results of 2D axisymmetric global radiation\nmagnetohydrodynamic (MHD) simulations of TDs for which all three non-ideal MHD\neffects are accounted. We used the NIRVANA-III fluid code and initialized our\nmodel with a disk cavity reaching up to $R=8~\\mathrm{au}$ with a density\ncontrast of $10^5$. We performed three runs, one with only OR and AD, and one\nfor each of the two configurations that arise when additionally including the\nHE, that is, with the field aligned (anti-aligned) with respect to the rotation\naxis. Results. For all three runs, our models maintain an intact inner cavity\nand an outer standard disk. MHD winds are launched both from the cavity and\nfrom the disk. Notably, when the HE is included, ring-like structures develop\nwithin the cavity. We moreover obtain accretion rates of $3 - 8 \\times 10^{-8}\n\\, M_{\\odot} \\, \\mathrm{yr}^{-1}$, comparable to typical values seen in full\ndisks. Importantly, we clearly observe transonic accretion ($v_{\\mathrm{acc}}\n\\gtrsim c_{s}$) in the cavity. Additionally, outward magnetic flux transport\noccurs in all three runs.",
        "Vertical Federated Learning (VFL) is a privacy-preserving collaborative\nlearning paradigm that enables multiple parties with distinct feature sets to\njointly train machine learning models without sharing their raw data. Despite\nits potential to facilitate cross-organizational collaborations, the deployment\nof VFL systems in real-world applications remains limited. To investigate the\ngap between existing VFL research and practical deployment, this survey\nanalyzes the real-world data distributions in potential VFL applications and\nidentifies four key findings that highlight this gap. We propose a novel\ndata-oriented taxonomy of VFL algorithms based on real VFL data distributions.\nOur comprehensive review of existing VFL algorithms reveals that some common\npractical VFL scenarios have few or no viable solutions. Based on these\nobservations, we outline key research directions aimed at bridging the gap\nbetween current VFL research and real-world applications.",
        "It has been known for many years that there is an apparent trend for the\nspectral index ({\\alpha}) of radio sources to steepen with redshift z, which\nhas led to attempts to select high-redshift objects by searching for radio\nsources with steep spectra. In this study we use data from the MeerKAT, LOFAR,\nGMRT, and uGMRT telescopes, particularly using the MIGHTEE and superMIGHTEE\nsurveys, to select compact sources over a wide range of redshifts and\nluminosities. We investigate the relationship between spectral index,\nluminosity and redshift and compare our results to those of previous studies.\nAlthough there is a correlation between {\\alpha} and z in our sample for some\ncombinations of frequency where good data are available, there is a clear\noffset between the {\\alpha}-z relations in our sample and those derived\npreviously from samples of more luminous objects; in other words, the\n{\\alpha}-z relation is different for low and high luminosity sources. The\nrelationships between {\\alpha} and luminosity are also weak in our sample but\nin general the most luminous sources are steeper-spectrum and this trend is\nextended by samples from previous studies. In detail, we argue that both a\n{\\alpha}-luminosity relation and an {\\alpha}-z relation can be found in the\ndata, but it is the former that drives the apparent {\\alpha}-z relation\nobserved in earlier work, which only appears because of the strong\nredshift-luminosity relation in bright, flux density-limited samples.\nSteep-spectrum selection should be applied with caution in searching for high-z\nsources in future deep surveys.",
        "States with long coherence are a crucial requirement for qubits and quantum\nmemories. Nuclear spins in epitaxial quantum dots are a great candidate,\noffering excellent isolation from external environments and on-demand coupling\nto optical flying qubits. However, coherence times are limited to $\\lesssim1$\nms by the dipole-dipole interactions between the nuclei and their quadrupolar\ncoupling to inhomogeneous crystal strain. Here, we combine strain engineering\nof the nuclear spin ensemble and tailored dynamical decoupling sequences to\nachieve nuclear spin coherence times exceeding 100 ms. Recently, a reversible\ntransfer of quantum information into nuclear spin ensembles has been\ndemonstrated in quantum dots. Our results provide a path to develop this\nconcept into a functioning solid-state quantum memory suitable for quantum\nrepeaters in optical quantum communication networks.",
        "We investigate the contribution of the charm-anticharrm ($c{\\bar c}$)\nasymmetry of the proton eigenstate obtained from QCD lattice gauge to the\nasymmetry of $D^+D^-$ and $D^0{\\bar D}^0$ mesons produced in $pp$ collisions at\nlarge Feynman variables $x$. It is shown that an important tool for the\nestablishing the intrinsic charm (IC) content of the proton is the charm\nhadron-antihadron asymmetry formed in $pp$ collisions. Predictions for the\nasymmetry as function of $x$ for different IC probabilities are presented. We\nshow that the interference of the intrinsic $|uud c{\\bar c}>$ Fock state with\nthe standard contribution from the PQCD evolution leads to a large $D^+D^-$\nasymmetry at large Feynman $x$.",
        "We present an extension theorem for a separately holomorphic function which\nis polynomial\/rational in some variables.",
        "We establish that the singular numbers (arising from Cartan decomposition)\nand corners (emerging from Iwasawa decomposition) in split reductive groups\nover non-archimedean fields are fundamentally determined by Hall-Littlewood\npolynomials. Through applications of the Satake isomorphism, we extend Van\nPeski's results (arXiv:2011.09356, Theorem 1.3) to encompass arbitrary root\nsystems. Leveraging this theoretical foundation, we further develop Shen's work\n(arXiv:2411.01104, Theorem 1.1) to demonstrate that both singular numbers and\ncorners of such products exhibit minimal separation. This characterization\nenables the derivation of asymptotic properties for singular numbers in matrix\nproducts, particularly establishing the strong law of large numbers and central\nlimit theorem for these quantities. Our results provide a unified framework\nconnecting algebraic decomposition structures with probabilistic limit theorems\nin non-archimedean settings.",
        "In this paper, we study Random Dynamical Systems (RDSs) of homeomorphisms on\nthe circle without a finite orbit. We characterize the topological dynamics of\nthe associated semigroup by identifying the existence of invariant sets which\nare finite unions of intervals. We describe the accumulation points of the\naverage orbit of the transfer operator. For each ergodic stationary measure, we\ndemonstrate interesting properties of its weight function on the circle.\nRelationships between the minimal sets of an RDS and its inverse RDS are also\nestablished.",
        "The observations of eclipsing binary systems are of great importance in\nastrophysics, as they allow direct measurements of fundamental stellar\nparameters. By analysing high-quality space-based observations with\nground-based photometric data, it becomes possible to detect these fundamental\nparameters with greater precision using multicolour photometry. Here, we report\nthe first photometric analysis results of the V517 Cam eclipsing binary system\nby combining the Transiting Exoplanet Survey Satellite (TESS) light curve and\nnew CCD observations in BVRI filters, obtained with a 60 cm robotic telescope\n(T60) at the T\\\"UB\\.ITAK National Observatory. By means of photometric\nanalyses, the masses and radii of the primary and secondary stars were\ncarefully determined to be $M_{1}= 1.47\\pm 0.06\\,M_\\odot$, $M_{2}=\n0.79\\pm0.05\\,M_\\odot$, and $R_{1}=1.43\\pm 0.03\\,R_\\odot$, $R_{2}= 0.75\\pm\n0.04\\,R_\\odot$, respectively. Furthermore, the distance to V517 Cam was\ncalculated to be $284\\pm20$ pc. The overall age of the system is estimated to\nbe around $63\\pm15$ Myr. At this age, the primary component stands near the\nonset of its main-sequence evolution, near the ZAMS, whereas the secondary\ncomponent remains in the pre-main-sequence evolutionary phase. To better\nunderstand the evolutionary status and nature of V517 Cam, the mass ratio and\ntemperature values, obtained with relatively low sensitivity by photometric\nmeasurements, need to be confirmed by spectral analysis.",
        "We study a quantum battery (QB) model composed of two atoms, where the\ncharger and battery elements are coupled to a multimode vacuum field that\nserves as a mediator for energy transfer. Different figures of merit such as\nergotropy, charging time, and charging efficiency are analyzed, putting\nemphasis on the role of various control parameters on the charging performance.\nIt is found that there is a range of angle between the transition dipole\nmoments and interatomic axis in which the QB can be charged. The optimal\ncharging performance is achieved if the atomic dipole moments are perpendicular\nor parallel to the interatomic axis. The charging performance also improves\nwith the decrease of the interatomic distance. Besides, the charged ergotropy\ncan be enhanced by increasing the initial ergotropy of the charger and it is\nbeneficial to charge the QB starting from a passive state.",
        "Modern web agents possess computer use abilities that allow them to interact\nwith webpages by sending commands to a virtual keyboard and mouse. While such\nagents have considerable potential to assist human users with complex tasks,\nevaluating their capabilities in real-world settings poses a major challenge.\nTo this end, we introduce BEARCUBS, a \"small but mighty\" benchmark of 111\ninformation-seeking questions designed to evaluate a web agent's ability to\nsearch, browse, and identify factual information from the web. Unlike prior web\nagent benchmarks, solving BEARCUBS requires (1) accessing live web content\nrather than synthetic or simulated pages, which captures the unpredictability\nof real-world web interactions; and (2) performing a broad range of multimodal\ninteractions (e.g., video understanding, 3D navigation) that cannot be bypassed\nvia text-based workarounds. Each question in BEARCUBS has a corresponding\nshort, unambiguous answer and a human-validated browsing trajectory, allowing\nfor transparent evaluation of agent performance and strategies. A human study\nconfirms that BEARCUBS questions are solvable but non-trivial (84.7% human\naccuracy), revealing search inefficiencies and domain knowledge gaps as common\nfailure points. By contrast, state-of-the-art computer-using agents\nunderperform, with the best-scoring system (OpenAI's Operator) reaching only\n24.3% accuracy. These results highlight critical areas for improvement,\nincluding reliable source selection and more powerful multimodal capabilities.\nTo facilitate future research, BEARCUBS will be updated periodically to replace\ninvalid or contaminated questions, keeping the benchmark fresh for future\ngenerations of web agents.",
        "We investigate the effects of interfacial spin-orbit coupling (ISOC) on\nsuperconductors, focusing on its impact on electronic transport and spin-charge\nconversion. Using a symmetry-based nonlinear sigma model, we derive effective\nboundary conditions for the Usadel and Maxwell equations that account for\nspin-galvanic effect, spin relaxation, and spin precession. This approach\nallows for the analysis of various interfaces without relying on specific\nmicroscopic models. We apply these boundary conditions to derive ISOC-induced\nterms in the Ginzburg-Landau functional, which is then used to compute the\ncritical temperature of superconducting films with ISOC subjected to an\nexternal magnetic field. Our findings show that, contrary to a recent\nprediction, the critical temperature of a film cannot be enhanced by an\nexternal magnetic field. Additionally, we demonstrate that the combination of\nISOC and an external magnetic field leads to a superconducting diode effect.\nIts efficiency strongly depends on the interplay between the spin-galvanic and\nthe spin relaxation terms. Our results provide a framework for understanding\nISOC in superconducting systems and highlight the potential for optimizing\ndiode efficiency through careful interface engineering",
        "This paper examines the Galileo Open Service Navigation Message\nAuthentication (OSNMA) and, for the first time, discovers two critical\nvulnerabilities, namely artificially-manipulated time synchronization (ATS) and\ninterruptible message authentication (IMA). ATS allows attackers falsify a\nreceiver's signals and\/or local reference time (LRT) while still fulfilling the\ntime synchronization (TS) requirement. IMA allows temporary interruption of the\nnavigation data authentication process due to the reception of a broken message\n(probably caused by spoofing attacks) and restores the authentication later. By\nexploiting the ATS vulnerability, we propose a TS-comply replay (TSR) attack\nwith two variants (real-time and non-real-time), where attackers replay signals\nto a victim receiver while strictly complying with the TS rule. We further\npropose a TS-comply forgery (TSF) attack, where attackers first use a\npreviously-disclosed key to forge a message based on the OSNMA protocol, then\ntamper with the vitcim receiver's LRT correspondingly to comply with the TS\nrule and finally transmit the forged message to the receiver. Finally, we\npropose a concatenating replay (CR) attack based on the IMA vulnerability,\nwhere attackers concatenate replayed signals to the victim receiver's signals\nin a way that still enables correct verification of the navigation data in the\nreplayed signals. To validate the effectiveness of the proposed attacks, we\nconduct real-world experiments with a commercial Galileo receiver manufactured\nby Septentrio, two software-defined radio (SDR) devices, open-source\nGalileo-SDR-SIM and OSNMAlib software. The results showed that all the attacks\ncan successfully pass the OSNMA scheme and the TSF attack can spoof receivers\nto arbitrary locations.",
        "The performance of pavement under loading depends on the strength of the\nsubgrade. However, experimental estimation of properties of pavement strengths\nsuch as California bearing ratio (CBR), unconfined compressive strength (UCS)\nand resistance value (R) are often tedious, time-consuming and costly, thereby\ninspiring a growing interest in machine learning based tools which are simple,\ncheap and fast alternatives. Thus, the potential application of two boosting\ntechniques; categorical boosting (CatBoost) and extreme gradient boosting\n(XGBoost) and support vector regression (SVR), is similarly explored in this\nstudy for estimation of properties of subgrade soil modified with hydrated lime\nactivated rice husk ash (HARSH). Using 121 experimental data samples of varying\nproportions of HARSH, plastic limit, liquid limit, plasticity index, clay\nactivity, optimum moisture content, and maximum dry density as input for CBR,\nUCS and R estimation, four evaluation metrics namely coefficient of\ndetermination (R2), root mean squared error (RMSE), mean absolute error (MAE)\nand mean absolute percentage error (MAPE) are used to evaluate the models'\nperformance. The results indicate that XGBoost outperformed CatBoost and SVR in\nestimating these properties, yielding R2 of 0.9994, 0.9995 and 0.9999 in\nestimating the CBR, UCS and R respectively. Also, SVR outperformed CatBoost in\nestimating the CBR and R with R2 of 0.9997 respectively. On the other hand,\nCatBoost outperformed SVR in estimating the UCS with R2 of 0.9994. Feature\nsensitivity analysis shows that the three machine learning techniques are\nunanimous that increasing HARSH proportion lead to values of the estimated\nproperties respectively. A comparison with previous results also shows\nsuperiority of XGBoost in estimating subgrade properties.",
        "Point tracking is becoming a powerful solver for motion estimation and video\nediting. Compared to classical feature matching, point tracking methods have\nthe key advantage of robustly tracking points under complex camera motion\ntrajectories and over extended periods. However, despite certain improvements\nin methodologies, current point tracking methods still struggle to track any\nposition in video frames, especially in areas that are texture-less or weakly\ntextured. In this work, we first introduce metrics for evaluating the texture\nintensity of a 3D object. Using these metrics, we classify the 3D models in\nShapeNet into three levels of texture intensity and create GIFT, a challenging\nsynthetic benchmark comprising 1800 indoor video sequences with rich\nannotations. Unlike existing datasets that assign ground truth points\narbitrarily, GIFT precisely anchors ground truth on classified target objects,\nensuring that each video corresponds to a specific texture intensity level.\nFurthermore, we comprehensively evaluate current methods on GIFT to assess\ntheir performance across different texture intensity levels and analyze the\nimpact of texture on point tracking.",
        "Identifying the unknown convolutional code corresponding to the given\nintercepted data is an important problem in military surveillance and in\nwireless communication. While a variety of code identification algorithms are\navailable in the literature, the key contribution of our work lies in the novel\nsolution and the corresponding analysis. In this paper, we focus on the\nsituation when the given data corresponds to either of the two potential\nconvolutional codes and the goal is to detect the correct code. We first\nprovide a new interpretation of the convolutional code as a Markov chain, which\nis more suitable for analyzing the code detection problem. Our problem then\ngets reduced to identifying between the two Markov chains. We provide the\nclosed-form expressions for the corresponding state transition matrices and\nestimate the error exponent for the underlying likelihood ratio test (LRT). We\nalso provide a computationally efficient BCJR-based method for computing the\nlikelihoods required for the LRT. We observe that BCJR-based likelihoods suffer\nfrom numerical issues for a longer data sequence, and hence, in this case, we\ndesign neural networks that have been found to achieve the optimal performance\nof the LRT.",
        "The flow within adhering droplets subjected to external shear flows has a\nsignificant influence on the stability and eventual detachment of the droplets\nfrom the surface. Most commonly, the velocity field inside adhering droplets is\nmeasured by means of particle image velocimetry (PIV), which requires a\ncorrection step for distortion caused by refraction of light at the gas-liquid\ninterface. Current methods for distortion correction based on ray tracing are\nlimited to low external flow velocities. However, the ray-tracing method can be\nextended to arbitrarily deformed droplet shapes if the instantaneous\nthree-dimensional droplet interface is availble. In the present work, a\npreviously introduced method for the image-based reconstruction of gas-liquid\ninterfaces by means of deep learning is adapted to determine the instantaneous\ninterface of adhering droplets in external shear flows. In this regard, a\npurposefully developed optical measurement technique based on the shadowgraphy\nmethod is employed that encodes additional three-dimensional (3D) information\nof the interface in the images via glare points from lateral light sources. On\nthe basis of the images recorded in the experiments, the volumetric shape of\nthe droplet is reconstructed by a neural network that was trained on the\nspatio-temporal dynamics of the gas-liquid interface from a synthetic dataset\nobtained by numerical simulation. The results for experiments with adhering\ndroplets at different velocities of external flow demonstrate that the\ncombination of the learned droplet geometry with the depth encoding through the\nglare points facilitates a robust and flexible reconstruction. The proposed\nmethod reconstructs the instantaneous three-dimensional interface of adhering\ndroplets at both high resolution and spatial accuracy and thereby enables the\ndistortion correction of PIV measurements at high external flow velocities.",
        "We discuss the fine-tunings of nuclear reactions in the Big Bang and in stars\nand draw some conclusions on the emergence of the light elements and the\nlife-relevant elements carbon and oxygen. We also stress how to improve these\ncalculations in the future. This requires a concerted effort of different\ncommunities, especially in nuclear reaction theory, lattice QCD for few-nucleon\nsystems, stellar evolution calculations, particle physics and philosophy.",
        "Spiking Neural Networks (SNNs) are promising bio-inspired third-generation\nneural networks. Recent research has trained deep SNN models with accuracy on\npar with Artificial Neural Networks (ANNs). Although the event-driven and\nsparse nature of SNNs show potential for more energy efficient computation than\nANNs, SNN neurons have internal states which evolve over time. Keeping track of\nSNN states can significantly increase data movement and storage requirements,\npotentially losing its advantages with respect to ANNs. This paper investigates\nthe energy effects of having neuron states, and how it is influenced by the\nchosen mapping to realistic hardware architectures with advanced memory\nhierarchies. Therefore, we develop STEMS, a mapping design space exploration\ntool for SNNs. STEMS models SNN's stateful behavior and explores intra-layer\nand inter-layer mapping optimizations to minimize data movement, considering\nboth spatial and temporal SNN dimensions. Using STEMS, we show up to 12x\nreduction in off-chip data movement and 5x reduction in energy (on top of\nintra-layer optimizations), on two event-based vision SNN benchmarks. Finally,\nneuron states may not be needed for all SNN layers. By optimizing neuron states\nfor one of our benchmarks, we show 20x reduction in neuron states and 1.4x\nbetter performance without accuracy loss.",
        "Music source separation is an audio-to-audio retrieval task of extracting one\nor more constituent components, or composites thereof, from a musical audio\nmixture. Each of these constituent components is often referred to as a \"stem\"\nin literature. Historically, music source separation has been dominated by a\nstem-based paradigm, leading to most state-of-the-art systems being either a\ncollection of single-stem extraction models, or a tightly coupled system with a\nfixed, difficult-to-modify, set of supported stems. Combined with the limited\ndata availability, advances in music source separation have thus been mostly\nlimited to the \"VDBO\" set of stems: \\textit{vocals}, \\textit{drum},\n\\textit{bass}, and the catch-all \\textit{others}. Recent work in music source\nseparation has begun to challenge the fixed-stem paradigm, moving towards\nmodels able to extract any musical sound as long as this target type of sound\ncould be specified to the model as an additional query input. We generalize\nthis idea to a \\textit{query-by-region} source separation system, specifying\nthe target based on the query regardless of how many sound sources or which\nsound classes are contained within it. To do so, we propose the use of\nhyperellipsoidal regions as queries to allow for an intuitive yet easily\nparametrizable approach to specifying both the target (location) as well as its\nspread. Evaluation of the proposed system on the MoisesDB dataset demonstrated\nstate-of-the-art performance of the proposed system both in terms of\nsignal-to-noise ratios and retrieval metrics.",
        "Many approaches to multi-robot coordination are susceptible to failure due to\ncommunication loss and uncertainty in estimation. We present a real-time\ncommunication-free distributed algorithm for navigating robots to their desired\ngoals certified by control barrier functions, that model and control the\nonboard sensing behavior to keep neighbors in the limited field of view for\nposition estimation. The approach is robust to temporary tracking loss and\ndirectly synthesizes control in real time to stabilize visual contact through\ncontrol Lyapunov-barrier functions. The main contributions of this paper are a\ncontinuous-time robust trajectory generation and control method certified by\ncontrol barrier functions for distributed multi-robot systems and a discrete\noptimization procedure, namely, MPC-CBF, to approximate the certified\ncontroller. In addition, we propose a linear surrogate of high-order control\nbarrier function constraints and use sequential quadratic programming to solve\nMPC-CBF efficiently. We demonstrate results in simulation with 10 robots and\nphysical experiments with 2 custom-built UAVs. To the best of our knowledge,\nthis work is the first of its kind to generate a robust continuous-time\ntrajectory and controller concurrently, certified by control barrier functions\nutilizing piecewise splines.",
        "Iterative feature space optimization involves systematically evaluating and\nadjusting the feature space to improve downstream task performance. However,\nexisting works suffer from three key limitations:1) overlooking differences\namong data samples leads to evaluation bias; 2) tailoring feature spaces to\nspecific machine learning models results in overfitting and poor\ngeneralization; 3) requiring the evaluator to be retrained from scratch during\neach optimization iteration significantly reduces the overall efficiency of the\noptimization process. To bridge these gaps, we propose a gEneralized Adaptive\nfeature Space Evaluator (EASE) to efficiently produce optimal and generalized\nfeature spaces. This framework consists of two key components: Feature-Sample\nSubspace Generator and Contextual Attention Evaluator. The first component aims\nto decouple the information distribution within the feature space to mitigate\nevaluation bias. To achieve this, we first identify features most relevant to\nprediction tasks and samples most challenging for evaluation based on feedback\nfrom the subsequent evaluator. This decoupling strategy makes the evaluator\nconsistently target the most challenging aspects of the feature space. The\nsecond component intends to incrementally capture evolving patterns of the\nfeature space for efficient evaluation. We propose a weighted-sharing\nmulti-head attention mechanism to encode key characteristics of the feature\nspace into an embedding vector for evaluation. Moreover, the evaluator is\nupdated incrementally, retaining prior evaluation knowledge while incorporating\nnew insights, as consecutive feature spaces during the optimization process\nshare partial information. Extensive experiments on fourteen real-world\ndatasets demonstrate the effectiveness of the proposed framework. Our code and\ndata are publicly available.",
        "In this paper, we investigate a Liouville-type theorem for the MHD equations\nusing Saint-Venant type estimates. We show that \\( (u, B) \\) is a trivial\nsolution if the growth of the \\( L^s \\) mean oscillation of the potential\nfunctions for both the velocity and magnetic fields are controlled. Our growth\nassumption is weaker than those previously known for similar results. The main\nidea is to refine the Saint-Venant type estimates using the Froullani integral.",
        "Significant obstacles exist in scientific domains including genetics, climate\nmodeling, and astronomy due to the management, preprocess, and training on\ncomplicated data for deep learning. Even while several large-scale solutions\noffer distributed execution environments, open-source alternatives that\nintegrate scalable runtime tools, deep learning and data frameworks on\nhigh-performance computing platforms remain crucial for accessibility and\nflexibility. In this paper, we introduce Deep Radical-Cylon(RC), a\nheterogeneous runtime system that combines data engineering, deep learning\nframeworks, and workflow engines across several HPC environments, including\ncloud and supercomputing infrastructures. Deep RC supports heterogeneous\nsystems with accelerators, allows the usage of communication libraries like\nMPI, GLOO and NCCL across multi-node setups, and facilitates parallel and\ndistributed deep learning pipelines by utilizing Radical Pilot as a task\nexecution framework. By attaining an end-to-end pipeline including\npreprocessing, model training, and postprocessing with 11 neural forecasting\nmodels (PyTorch) and hydrology models (TensorFlow) under identical resource\nconditions, the system reduces 3.28 and 75.9 seconds, respectively. The design\nof Deep RC guarantees the smooth integration of scalable data frameworks, such\nas Cylon, with deep learning processes, exhibiting strong performance on cloud\nplatforms and scientific HPC systems. By offering a flexible, high-performance\nsolution for resource-intensive applications, this method closes the gap\nbetween data preprocessing, model training, and postprocessing.",
        "The nearest-neighbour or local mass terms in theory space among quantum\nfields, with their generic disordered values, are known to lead to the\nlocalisation of mass eigenstates, analogous to Anderson localisation in a\none-dimensional lattice. This mechanism can be used to create an exponential\nhierarchy in the coupling between two fields by placing them at opposite ends\nof the lattice chain. Extending this mechanism, we show that when copies of\nsuch fields are appropriately attached to the lattice chain, it leads to the\nemergence of multiple massless modes. These vanishing masses are a direct\nconsequence of the locality of interactions in theory space. The latter may\nbreak down in an ordered and deterministic manner through quantum effects if\nadditional interactions exist among the chain fields. Such non-locality can\ninduce small masses for the otherwise massless modes without necessarily\ndelocalising the mass eigenstates. We provide examples of interactions that\npreserve or even enhance localisation. Applications to flavour hierarchies,\nneutrino mass, and the $\\mu$-problem in supersymmetric theories are discussed.",
        "Entropy estimation plays a significant role in biology, economics, physics,\ncommunication engineering and other disciplines. It is increasingly used in\nsoftware engineering, e.g. in software confidentiality, software testing,\npredictive analysis, machine learning, and software improvement. However\naccurate estimation is demonstrably expensive in many contexts, including\nsoftware. Statisticians have consequently developed biased estimators that aim\nto accurately estimate entropy on the basis of a sample. In this paper we apply\n18 widely employed entropy estimators to Shannon measures useful to the\nsoftware engineer: entropy, mutual information and conditional mutual\ninformation. Moreover, we investigate how the estimators are affected by two\nmain influential factors: sample size and domain size. Our experiments range\nover a large set of randomly generated joint probability distributions and\nvarying sample sizes, rather than choosing just one or two well known\nprobability distributions as in previous investigations.\n  Our most important result is identifying that the Chao-Shen and\nChao-Wang-Jost estimators stand out for consistently converging more quickly to\nthe ground truth, regardless of domain size and regardless of the measure used.\nThey also tend to outperform the others in terms of accuracy as sample sizes\nincrease. This discovery enables a significant reduction in data collection\neffort without compromising performance.",
        "Predicting future events is an important activity with applications across\nmultiple fields and domains. For example, the capacity to foresee stock market\ntrends, natural disasters, business developments, or political events can\nfacilitate early preventive measures and uncover new opportunities. Multiple\ndiverse computational methods for attempting future predictions, including\npredictive analysis, time series forecasting, and simulations have been\nproposed. This study evaluates the performance of several large language models\n(LLMs) in supporting future prediction tasks, an under-explored domain. We\nassess the models across three scenarios: Affirmative vs. Likelihood\nquestioning, Reasoning, and Counterfactual analysis. For this, we create a\ndataset1 by finding and categorizing news articles based on entity type and its\npopularity. We gather news articles before and after the LLMs training cutoff\ndate in order to thoroughly test and compare model performance. Our research\nhighlights LLMs potential and limitations in predictive modeling, providing a\nfoundation for future improvements.",
        "This work is an extension of a previous publication [1] where we fitted the\nstrong coupling $\\alpha_s$ together with the non-perturbative parameter\n$\\alpha_0$ from event-shape and jet-shape distributions using power corrections\ncomputed in the three-jet region. In ref. [1] only ALEPH data at the $Z$-pole\nwere used in the fit. Here, instead, we include a large data sample from\nvarious $e^+e^-$ experiments at energies ranging from 22 to 207 GeV and\nrevisited the treatment of theoretical uncertainties. We find that the\ninclusion of different energies, while not changing the central fit result\nconsiderably, helps to disentangle the dependence of perturbative and\nnon-perturbative corrections. Our best fit result is $\\alpha_s(M_Z) = 0.1181\n(+0.0002 -0.0005) (+0.0018 -0.0021)$, where the first error includes\nexperimental uncertianties and the second one includes uncertainties associated\nwith scale variation, mass effects, fit limits, non-perturbative schemes and\nnon-perturbative uncertainties."
      ]
    }
  },
  {
    "id":2412.02083,
    "research_type":"basic",
    "start_id":"b3",
    "start_title":"Learning internal representations by back-propagating errors",
    "start_abstract":"We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal \u2018hidden\u2019 units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b8"
      ],
      "title":[
        "Quantum algorithms for supervised and unsupervised machine learning"
      ],
      "abstract":[
        "Machine-learning tasks frequently involve problems of manipulating and classifying large numbers vectors in high-dimensional spaces. Classical algorithms for solving such typically take time polynomial the number dimension space. Quantum computers are good at tensor product This paper provides supervised unsupervised quantum machine learning cluster assignment finding. can logarithmic both their dimension, an exponential speed-up over classical algorithms."
      ],
      "categories":[
        "Quantum Physics"
      ]
    },
    "list":{
      "title":[
        "Financial Adviser Misconduct and Labor Market Penalties: Uncovering\n  Racial Disparities in the Absence of Gender Gaps",
        "Geometric origin of self-intersection points in non-Hermitian energy\n  spectra",
        "$L^{p}-L^{q}$ existence for the open compressible MHD system",
        "A New Approach for Fourier Extension Based on Weighted Generalized\n  Inverse",
        "Analog Quantum Teleportation",
        "An estimate for $\\beta$-Hermite ensembles via the zeros of Hermite\n  polynomials",
        "Asymptotic coefficients of Weil-Petersson volumes in the large genus",
        "A search for sterile neutrinos in interacting dark energy models using\n  DESI baryon acoustic oscillations and DES supernovae data",
        "Cluster Ages to Reconstruct the Milky Way Assembly (CARMA). III. NGC 288\n  as the first Splashed globular cluster",
        "Decoding Financial Health in Kenyas' Medical Insurance Sector: A\n  Data-Driven Cluster Analysis",
        "Path degeneracy and applications",
        "The latest monthly highs suggest that the 1.5{\\deg}C Paris Agreement\n  threshold will probably be exceeded before 2028",
        "Strain-Induced Optical and Molecular Transformations in PET Films for\n  Organic Electronic Applications",
        "Continuous Variable Quantum MacWilliams Identities",
        "Caught in the Act of Quenching: A Population of Post-Starburst\n  Ultra-Diffuse Galaxies",
        "Coboundaries of 3-IETs",
        "Relative Entropy Methods for Calculating Committors",
        "Pervasiveness of $\\mathcal{L}^r(E,F)$ in $\\mathcal{L}^r(E,F^{\\delta})$",
        "Electric Power Enhancement using Spin-Polarized Fuel in Fusion Power\n  Plants",
        "Fault tolerance for metric dimension and its variants",
        "Enhanced and Efficient Extraction of Uranyl Ions from Aqueous Waste\n  through Graphene\/CNT-PAMAM Nanocomposites",
        "On Pancyclicity in a Mixed Model for Domination Reconfiguration",
        "On counting numerical semigroups by maximum primitive and Wilf's\n  conjecture",
        "2024 'Key Reflections' on the 1824 Sadi Carnot's 'Reflexions' and 200\n  Year Legacy",
        "On conservative, stable boundary and coupling conditions for diffusion\n  equations I -- The conservation property for explicit schemes",
        "Subtree Distances, Tight Spans and Diversities",
        "Managing target of opportunity (ToO) observations at Observatorio\n  Astrof\\'isico de Javalambre (OAJ)",
        "Object Detection with Deep Learning for Rare Event Search in the GADGET\n  II TPC",
        "Orbits of photon in Bardeen-boson stars and their frozen states"
      ],
      "abstract":[
        "Using a comprehensive matched employer-employee dataset for U.S. financial\nadvisers from 2008 to 2018, we revisit established evidence on labor market\npenalties following financial misconduct. Prior studies report that female\nadvisers are 20% more likely to exit their firms following misconduct and that\nsimilar disparities exist for non-white advisers. However, by disaggregating\nmisconduct into distinct disclosure events - differentiating those that nearly\nalways trigger job terminations from those that do not - we show that the\napparent gender gap vanishes, while significant racial disparities persist.\nSpecifically, non-white advisers face approximately 24% higher job separation\nrates than their white counterparts. Robustness checks confirm these findings\nacross alternative specifications, suggesting that race-based differential\ntreatment in the labor market is a distinct phenomenon warranting further\ninvestigation.",
        "Unlike Hermitian systems, non-Hermitian energy spectra under periodic\nboundary conditions can form closed loops in the complex energy plane, a\nphenomenon known as point gap topology. In this paper, we investigate the\nself-intersection points of such non-Hermitian energy spectra and reveal their\ngeometric origins. We rigorously demonstrate that these self-intersection\npoints result from the intersection of the auxiliary generalized Brillouin zone\nand the Brillouin zone in one-band systems, as confirmed by an extended\nHatano-Nelson model. This finding is further generalized to multi-band systems,\nillustrated through a non-Hermitian Su-Schrieffer-Heeger model. Moreover, we\naddress multiple self-intersection points and derive the geometric conditions\nfor general n-fold self-intersection points. Our results enhance the\nfundamental understanding of generic non-Hermitian quantum systems and provide\ntheoretical support for further experimental investigations of energy\nself-intersection points.",
        "We study the local existence of solutions to the magnetohydrodynamics (MHD)\nsystem describing the motion of a compressible, viscous, electrically and heat\nconducting fluid in the $L^p-L^q$ class with inhomogeneous boundary conditions.\nThe open system is allowed to receive incoming matter from the outside through\n(part of) the boundary which we refer to as an inflow boundary. This setup\nbrings about a difficulty in estimating the regularity of the density $\\varrho$\nwhich we remedy by assuming appropriate hypotheses on the velocity field,\ndomain boundary and on the boundary and initial data of $\\varrho$. The main\nresult ensures the local well-posedness of the full MHD system which is shown\nthrough a linearization combined with a Banach fixed-point theorem.",
        "This paper examines the Fourier extension from a new perspective of solving\nthe compact operator equation with perturbed data. By converting the\napproximation target from the best approximate solution to the weighted best\napproximate solution, the oscillation in the extended region has been overcome.\nThe error estimation of the solution is theoretically established. Furthermore,\nwe point out the difficulties faced by the original weighted operator in\ncalculation due to the limitation of machine precision and propose an effective\ncorrection operator. The relevant parameters involved in the method are further\ntested, and finally the effectiveness of the method is verified through\nnumerical experiments.",
        "Digital teleportation protocols make use of entanglement, local measurements\nand a classical communication channel to transfer quantum states between remote\nparties. We consider analog teleportation protocols, where classical\ncommunication is replaced by transmission through a noisy quantum channel. We\nshow that analog teleportation protocols outperform digital protocols if and\nonly if Alice and Bob are linked by a channel that does not reduce entanglement\nwhen applied to a part of the resource state. We first derive general\nanalytical results in the broader context of Gaussian-channel simulation. Then,\nwe apply it to the quantum teleportation of a uniformly distributed codebook of\ncoherent states, showing that an analog protocol is optimal for a wide range of\ncommunication channel transmissivities. Our result contributes to mitigating\nnoise in the intermediate case when the communication channel is far from being\nideal but is not too lossy, as is the case of cryogenic links in microwave\nsuperconducting circuits.",
        "Let $X$ be an $N$-dimensional random vector which describes the ordered\neigenvalues of a $\\beta$-Hermite ensemble, and let $z$ the vector containing\nthe ordered zeros of the Hermite poynomial $H_N$. We present an explicit\nestimate for $P(\\|X-z\\|_2\\ge\\epsilon)$ for small $\\epsilon>0$ and large\nparameters $\\beta$. The proof is based on a central limit theorem for these\nensembles for $\\beta\\to\\infty$ with explicit eigenvalues of the covariance\nmatrices of the limit. The estimate is similar to previous estimates of Dette\nand Imhof (2009).",
        "Mirzakhani-Zograf proved the large genus asymptotic expansions of\nWeil-Petersson volumes and showed that the asymptotic coefficients are\npolynomials in $\\mathbb Q[\\pi^{-2},\\pi^2]$. They also conjectured that these\nare actually polynomials in $\\mathbb Q[\\pi^{-2}]$. In this paper, we prove\nMirzakhani-Zograf's conjecture.",
        "Sterile neutrinos can influence the evolution of the universe, and thus\ncosmological observations can be used to search for sterile neutrinos. In this\nstudy, we utilized the latest baryon acoustic oscillations data from DESI,\ncombined with the cosmic microwave background data from Planck and the\nfive-year supernova data from DES, to constrain the interacting dark energy\n(IDE) models involving both cases of massless and massive sterile neutrinos. We\nconsider four typical forms of the interaction term $Q=\\beta H \\rho_{\\rm de}$,\n$Q=\\beta H \\rho_{\\rm c}$, $Q=\\beta H_{0} \\rho_{\\rm de}$, and $Q=\\beta H_{0}\n\\rho_{\\rm c}$, respectively. Our analysis indicates that the current data\nprovide only a hint of the existence of massless sterile neutrinos (as dark\nradiation) at about the $1\\sigma$ level. In contrast, no evidence supports the\nexistence of massive sterile neutrinos. Furthermore, in IDE models, the\ninclusion of (massless\/massive) sterile neutrinos has a negligible impact on\nthe constraint of the coupling parameter $\\beta$. The IDE model of $Q=\\beta H\n\\rho_{\\rm c}$ with sterile neutrinos does not favor an interaction. However,\nthe other three IDE models with sterile neutrinos support an interaction in\nwhich dark energy decays into dark matter.",
        "The globular clusters (GCs) system of the Milky Way (MW) comprises a mixture\nof both in situ and accreted clusters. Tracing the origin of GCs provides\ninvaluable insights into the formation history of the MW. However, reconciling\ndiverse strands of evidence is often challenging: a notable example is NGC 288,\nwhere despite significant efforts in the literature, the available\nchrono-chemodynamical data have yet to provide a definitive conclusion\nregarding its origin. On one side, all post-Gaia dynamical studies indicate an\naccreted origin for NGC 288 from in the Gaia-Sausage-Enceladus (GSE) dwarf\ngalaxy. On the other, NGC 288 has been found to be 2.5 Gyr older than other GSE\nGCs at the same metallicity, this suggesting a different, possibly in situ\norigin. In this work, we address the unresolved question on the origin of NGC\n288 by analyzing its chrono-chemical properties in an unprecedentedly\nhomogeneous framework. First, we compare the location of NGC 288 in the\nage-metallicity plane with that of other two in situ GCs at similar\nmetallicity, namely NGC 6218 and NGC 6362. The age estimates obtained within\nthe homogeneous framework of the CARMA collaboration show that the three\nclusters are coeval, this reinforcing the contrast with the dynamical\ninterpretation. Then, we compare the abundances with the sample of in situ and\naccreted clusters at similar metallicity presented in Ceccarelli et al. 2024,\nfinding again consistency with the chemistry of in situ systems. To reconcile\nthese results with its orbital properties, we propose a scenario where NGC 288\nformed in the proto-disc of the MW, and then was dynamically heated by the\ninteraction with the GSE merger, a fate similar to that of proto-disc stars\nexperiencing the so-called Splash event. NGC 288 therefore demonstrates the\nimportance of a homogeneous chrono-chemodynamical information in the\ninterpretation of the origin of MW GCs.",
        "This study examines insurance companies' financial performance and reporting\ntrends within the medical sector using advanced clustering techniques to\nidentify distinct patterns. Four clusters were identified by analyzing\nfinancial ratios and time series data, each representing unique financial\nperformance and reporting consistency combinations. Dynamic Time Warping (DTW)\nand KMeans clustering were employed to capture temporal variations and uncover\nkey insights into company behaviors. The findings reveal that resilient\nperformers consistently report and have financial stability, making them\nreliable options for policyholders. In contrast, clusters of underperforming\ncompanies and those with reporting gaps highlight operational challenges and\nissues related to data consistency. These insights emphasize the importance of\ntransparency and timely reporting to ensure the sector's resilience. This study\ncontributes to the literature by integrating time series analysis into\nfinancial clustering, offering practical recommendations for improving data\ngovernance and financial stability in the insurance sector. Future research\ncould further investigate non-financial indicators and explore alternative\nclustering methods to provide a deeper understanding of performance dynamics.",
        "In this work, we relate girth and path-degeneracy in classes with\nsub-exponential expansion, with explicit bounds for classes with polynomial\nexpansion and proper minor-closed classes that are tight up to a constant\nfactor (and tight up to second order terms if a classical conjecture on\nexistence of $g$-cages is verified). As an application, we derive bounds on the\ngeneralized acyclic indices, on the generalized arboricities, and on the weak\ncoloring numbers of high-girth graphs in such classes. Along the way, we prove\na conjecture proposed in [T.~Bartnicki et al., Generalized arboricity of graphs\nwith large girth, Discrete Mathematics 342 (2019), no.~5, 1343--1350.], which\nasserts that, for every integer $k$, there is an integer $g(p,k)$ such that\nevery $K_k$ minor-free graph with girth at least $g(p,k)$ has $p$-arboricity at\nmost $p+1$.",
        "An attempt is made to estimate and forecast the trend of the global annual\nand monthly mean temperatures. The results of a conventional statistical\nanalysis suggest that in the absence of unforeseeable events such as a sudden\nacceleration in the rate of warming, the 1.5{\\deg}C Paris Agreement threshold\ncould be exceeded between 2027 and 2031. However, carrying out a proper\nseasonal adjustment and examining the autocorrelation structure carefully, we\nfind in a subsequent purely statistical simulation study that even the\npessimistic scenario of a breach in late 2027 is inconsistent with the recent\nmonthly highs, which means that it will probably happen much sooner.",
        "Poly(ethylene terephthalate) (PET) films are widely used in flexible\nelectronics and optoelectronics, where their mechanical durability and optical\nperformance under strain are essential for device reliability. This study\ninvestigates the impact of applied mechanical strain on the optical and\nmolecular properties of PET at room temperature,using UV-Vis absorption and\nRaman spectroscopy. The work explores how varying strain levels, from 0%\n(unstretched) to 30%, affect the transparency, vibrational modes, and molecular\nreorganization within PET films. UV-Vis absorbance measurements reveal that\nstrain induces significant changes in the light transmission properties of PET,\nparticularly in the visible range, and increases absorption in the UVA and\nvisible region by up to 100%. Raman spectra indicate that strain levels higher\nthan 5% lead to irreversible shifts of vibrational lines, accompanied by an\nincrease of their full width at half maximum (FWHM), suggesting molecular\nreorientation and crystallinity changes. The phonon mode coupled with C-O\nstretching [O-CH2] shows the strongest response to applied mechanical stress.\nThis study provides a comprehensive understanding of strain-induced optical and\nstructural alterations in PET, with implications for improving the mechanical\nand optical performance of PET-based devices in strainsensitive applications,\nsuch as organic solar cells (OSCs), organic light-emitting diodes (OLEDs), and\nflexible sensors.",
        "We derive bounds on general quantum error correcting codes against the\ndisplacement noise channel. The bounds limit the distances attainable by codes\nand also apply in an approximate setting. Our main result is a quantum analogue\nof the classical Cohn-Elkies bound on sphere packing densities attainable in\nEuclidean space. We further derive a quantum version of Levenshtein's sphere\npacking bound and argue that Gottesman--Kitaev--Preskill (GKP) codes based on\nthe $E_8$ and Leech lattices achieve optimal distances. The main technical tool\nis a continuous variable version of the quantum MacWilliams identities, which\nwe introduce. The identities relate a pair of weight distributions which can be\nobtained for any two trace-class operators. General properties of these weight\ndistributions are discussed, along with several examples.",
        "We report the discovery of post-starburst ultra-diffuse galaxies (UDGs),\nidentified through spectroscopic analysis with KCWI at the Keck II Telescope.\nOur analysis is based on a sample of 44 candidate UDGs selected from the\nSystematically Measuring Ultra-Diffuse Galaxies (SMUDGes) program. Our measured\nspectroscopic redshifts reveal $\\sim 80\\%$ of the entire KCWI sample exhibit\nlarge physical sizes ($R_{e} \\gtrsim 1~{\\rm kpc}$) and low surface brightnesses\n($24 \\lesssim \\mu_{0,g} \\lesssim 25$ mag arcsec$^{-2}$) which categorize them\nas UDGs. We find $20\\%$ of the confirmed UDG population contain post-starburst\n(or K+A) features, characterized by minimal to no emission in H$\\beta$\nindicative of quenched star formation and a predominant presence of spectral\nA-type stars. Studying the local environments of the post-starburst UDGs, we\nfind that half are isolated systems, including two systems that reside\n$2-3~R_{\\rm vir}$ away from potential nearby massive hosts ($M_{\\star}\n>10^{10}~\\mathrm{M}_{\\odot}$). Without the influence of external environmental\nmechanisms, these post-starburst UDGs may represent systems experiencing star\nformation feedback such that a recent burst may lead to (at least temporary)\nquenching. Overall, our results highlight the potentially diverse quenching\npathways of UDGs in the local Universe.",
        "In this note, we investigate the coboundaries of interval exchange\ntransformations of 3 intervals (3-IETs). More precisely, we show that a\ndifferentiable function with absolutely continuous derivative with bounded\nvariation, whose integral and integral of its derivative is 0, is a coboundary\nfor typical 3-IET if and only if the values at the endpoints of the domain are\nzero. We also show the existence of rare counterexamples for both cases of\npossible values at the endpoints of the interval. We obtain our result by\nstudying the properties of associated skew products.",
        "Motivated by challenges arising in molecular simulation, we analyze and\ndevelop methods of computing reactive trajectories and committor functions for\nsystems described by the overdamped Langevin dynamics. Our main technical\nadvance is a new loss function that measures the accuracy of approximations to\nthe committor function related to a given chemical reaction or other rare\ntransition event. Our loss admits a simple interpretation in terms of the\ndistribution of reactive trajectories, and it can be computed in practice to\ncompare the accuracies of different approximations of the committor. We also\nderive a method of calculating committors by direct minimization of the loss\nvia stochastic gradient descent.",
        "Let $E, F$ be Archimedean Riesz spaces, and let $F^{\\delta}$ denote an order\ncompletion of $F$. In this note, we provide necessary conditions under which\nthe space of regular operators $\\mathcal{L}^r(E, F)$ is pervasive in\n$\\mathcal{L}^r(E, F^{\\delta})$. Pervasiveness of $\\mathcal{L}^r(E, F)$ in\n$\\mathcal{L}^r(E, F^{\\delta})$ implies that the Riesz completion of $\n\\mathcal{L}^r(E, F)$ can be realized as a Riesz subspace of $ \\mathcal{L}^r(E,\nF^{\\delta}$. It also ensures that the regular part of the space of order\ncontinuous operators $\\mathcal{L}^{oc}(E, F)$ forms a band of $\\mathcal{L}^r(E,\nF)$. Furthermore, the positive part $T^+$ of any operator $T \\in\n\\mathcal{L}^r(E, F)$, provided it exists, is given by the Riesz-Kantorovich\nformula. The results apply in particular to cases where $E = \\ell_0^{\\infty}$,\n$E = c$, or $F$ is atomic, and they provide solutions to some problems posed in\n[3] and [16].",
        "Using a range of fusion power plant (FPP) concepts, we demonstrate that\nspin-polarized fuel (SPF) can significantly enhance net electric power output,\noften by many multiples. Notably, the electric power gain from SPF generally\nexceeds the corresponding increase in thermal fusion power. Plants close to\nengineering breakeven stand to benefit most, where even modest boosts in fusion\npower produce disproportionately larger gains in net electricity. As a\nrepresentative example, a 25% increase in fusion power via SPF could allow an\nITER-like device (with an added turbine to recover thermal fusion power) to\nachieve engineering breakeven. These findings strongly motivate the development\nof spin-polarized fuel for FPPs.",
        "Hernando et al. (2008) introduced the fault-tolerant metric dimension\n$\\text{ftdim}(G)$, which is the size of the smallest resolving set $S$ of a\ngraph $G$ such that $S-\\left\\{s\\right\\}$ is also a resolving set of $G$ for\nevery $s \\in S$. They found an upper bound $\\text{ftdim}(G) \\le \\dim(G) (1+2\n\\cdot 5^{\\dim(G)-1})$, where $\\dim(G)$ denotes the standard metric dimension of\n$G$. It was unknown whether there exists a family of graphs where\n$\\text{ftdim}(G)$ grows exponentially in terms of $\\dim(G)$, until recently\nwhen Knor et al. (2024) found a family with $\\text{ftdim}(G) =\n\\dim(G)+2^{\\dim(G)-1}$ for any possible value of $\\dim(G)$. We improve the\nupper bound on fault-tolerant metric dimension by showing that $\\text{ftdim}(G)\n\\le \\dim(G)(1+3^{\\dim(G)-1})$ for every connected graph $G$. Moreover, we find\nan infinite family of connected graphs $J_k$ such that $\\dim(J_k) = k$ and\n$\\text{ftdim}(J_k) \\ge 3^{k-1}-k-1$ for each positive integer $k$. Together,\nour results show that \\[\\lim_{k \\rightarrow \\infty} \\left( \\max_{G: \\text{ }\n\\dim(G) = k} \\frac{\\log_3(\\text{ftdim}(G))}{k} \\right) = 1.\\] In addition, we\nconsider the fault-tolerant edge metric dimension $\\text{ftedim}(G)$ and bound\nit with respect to the edge metric dimension $\\text{edim}(G)$, showing that\n\\[\\lim_{k \\rightarrow \\infty} \\left( \\max_{G: \\text{ } \\text{edim}(G) = k}\n\\frac{\\log_2(\\text{ftedim}(G))}{k} \\right) = 1.\\] We also obtain sharp extremal\nbounds on fault-tolerance for adjacency dimension and $k$-truncated metric\ndimension. Furthermore, we obtain sharp bounds for some other extremal problems\nabout metric dimension and its variants. In particular, we prove an equivalence\nbetween an extremal problem about edge metric dimension and an open problem of\nErd\\H{o}s and Kleitman (1974) in extremal set theory.",
        "The increasing threat of uranium contamination to environmental and human\nhealth due to its radiotoxicity demands the development of novel and efficient\nadsorbents for remediation. In this study, we investigated the potential of\npoly(amidoamine) (PAMAM) dendrimers of generations 1 to 4 (G1 - G4)\nfunctionalized with graphene and carbon nanotubes (CNTs) as adsorbents for\nuranyl ion removal from aqueous solutions. By combining atomistic molecular\ndynamics (MD) simulations with experimental validation, we examined the\ninfluence of pH, uranyl ion concentration, and dendrimer generation on\nadsorption behavior. Our study revealed that uranyl ion adsorption is greater\nwhen PAMAM is grafted onto graphene\/CNT than pristine PAMAM. However,\nPAMAM-grafted CNTs exhibit superior adsorption capacity at specific uranyl\nconcentrations due to their curvature and abundant accessible binding sites.\nHigher-generation PAMAM dendrimers grafted onto graphene\/CNTs exhibit greater\nadsorption capacity due to the increased availability of binding sites, which\nis consistent with experimental observations. The adsorption capability of\nuranyl ions in all four generations of the PAMAM dendrimer increased as the\nconcentration of uranyl ions increased. Adsorption capacity increases with\nincreasing uranyl ion concentration, and adsorption occurs on both PAMAM and\ngraphene\/CNT surfaces, with saturation observed at higher concentrations. This\nstudy provides insights into the adsorption mechanisms and highlights the\npotential of PAMAM-based nanocomposites for efficient uranyl ion extraction and\nenvironmental remediation.",
        "A new model for domination reconfiguration is introduced which combines the\nproperties of the preexisting token addition\/removal (TAR) and token sliding\n(TS) models. The vertices of the TARS-graph correspond to the dominating sets\nof $G$, where two vertices are adjacent if and only if they are adjacent via\neither the TAR reconfiguration rule or the TS reconfiguration rule. While the\ndomination reconfiguration graph obtained by using only the TAR rule (sometimes\ncalled the dominating graph) will never have a Hamilton cycle, we show that for\nsome classes of graphs $G$, by adding a relatively small number of token\nsliding edges, the resulting graph is not only hamiltonian, but is in fact\npancyclic. In particular, if the underlying graphs are trees, complete graphs,\nor complete multipartite graphs, their TARS-graphs will be pancyclic. We also\nprovide pancyclicity results for TARS-graphs of graph unions and joins, and\nconclude by posing the question: Are all TARS-graphs pancyclic?",
        "We introduce a new way of counting numerical semigroups, namely by their\nmaximum primitive, and show its relation with the counting of numerical\nsemigroups by their Frobenius number. For any positive integer $n$, let $A_{n}$\ndenote the number of numerical semigroups whose maximum primitive is $n$, and\nlet $N_{n}$ denote the number of numerical semigroups whose Frobenius number is\n$n$. We show that the sequences $(A_{n})$ and $(N_{n})$ are M\\\"obius transforms\nof one another. We also establish that almost all numerical semigroups with\nlarge enough maximum primitive satisfy Wilf's conjecture. A crucial step in the\nproof is a result of independent interest: a numerical semigroup $S$ with\nmultiplicity $\\mathrm{m}$ such that $|S\\cap (\\mathrm{m},2 \\mathrm{m})|\\geq\n\\sqrt{3\\mathrm{m}}$ satisfies Wilf's conjecture.",
        "This author is not a philosopher nor historian of science, but an engineering\nthermodynamicist. In that regard and in addition to various philosophical \"why\n& how\" treatises and existing historical analyses, the physical and logical\n\"what it is\" reflections, as sequential Key Points, where a key Sadi Carnot's\nreasoning infers the next one, along with novel contributions and original\ngeneralizations, are presented. We need to keep in mind that in Sadi Carnot's\ntime (early 1800s) the steam engines were inefficient (below 5%, so the heat in\nand out were comparable within experimental uncertainty, as if caloric were\nconserved), the conservation of caloric flourished (might be a fortunate\nmisconception leading to the critical analogy with the waterwheel), and many\ncritical thermal-concepts, including the conservation of energy (The First Law)\nwere not even established. Since Clausius and Kelvin earned to be \"Fathers of\nthermodynamics,\" then Sadi Carnot was 'the ingenious' \"Forefather of\nthermodynamics-to-become\".",
        "This paper introduces improved numerical techniques for addressing numerical\nboundary and interface coupling conditions in the context of diffusion\nequations in cellular biophysics or heat conduction problems in fluid-structure\ninteractions. Our primary focus is on two critical numerical aspects related to\ncoupling conditions: the preservation of the conservation property and ensuring\nstability. Notably, a key oversight in some existing literature on coupling\nmethods is the neglect of upholding the conservation property within the\noverall scheme. This oversight forms the central theme of the initial part of\nour research. As a first step, we limited ourselves to explicit schemes on\nuniform grids. Implicit schemes and the consideration of varying mesh sizes at\nthe interface will be reserved for a subsequent paper \\cite{CMW3}. Another\npaper \\cite{CMW2} will address the issue of stability.\n  We examine these schemes from the perspective of finite differences,\nincluding finite elements, following the application of a nodal quadrature\nrule. Additionally, we explore a finite volume-based scheme involving cells and\nflux considerations. Our analysis reveals that discrete boundary and flux\ncoupling conditions uphold the conservation property in distinct ways in\nnodal-based and cell-based schemes. The coupling conditions under investigation\nencompass well-known approaches such as Dirichlet-Neumann coupling, heat flux\ncoupling, and specific channel and pumping flux conditions drawn from the field\nof biophysics. The theoretical findings pertaining to the conservation property\nare corroborated through computations across a range of test cases.",
        "Metric embeddings are central to metric theory and its applications. Here we\nconsider embeddings of a different sort: maps from a set to subsets of a metric\nspace so that distances between points are approximated by minimal distances\nbetween subsets. Our main result is a characterization of when a set of\ndistances $d(x,y)$ between elements in a set $X$ have a subtree representation,\na real tree $T$ and a collection $\\{S_x\\}_{x \\in X}$ of subtrees of~$T$ such\nthat $d(x,y)$ equals the length of the shortest path in~$T$ from a point in\n$S_x$ to a point in $S_y$ for all $x,y \\in X$. The characterization was first\nestablished for {\\em finite} $X$ by Hirai (2006) using a tight span\nconstruction defined for distance spaces, metric spaces without the triangle\ninequality. To extend Hirai's result beyond finite $X$ we establish fundamental\nresults of tight span theory for general distance spaces, including the\nsurprising observation that the tight span of a distance space is hyperconvex.\nWe apply the results to obtain the first characterization of when a diversity\n-- a generalization of a metric space which assigns values to all finite\nsubsets of $X$, not just to pairs -- has a tight span which is tree-like.",
        "The Observatorio Astrof\\'isico de Javalambre (OAJ) is a Spanish astronomical\nICTS (Unique Scientific and Technical Infrastructures) located in the Sierra de\nJavalambre in Teruel (Spain). It has been particularly conceived for carrying\nout large-sky multi-filter surveys. As an ICTS, the OAJ offers Open Time to the\nastronomical community, offering more than 25% through Legacy Surveys, Regular\nPrograms (RP) and Director discretionary time (DDT). Regarding the RP, a new\ncall for proposals is made public each semester accepting only proposals under\nthe modality of Target of Opportunity (ToO).\n  This contribution summarizes how ToOs are managed at OAJ presenting the\ndifferent applications designed and implemented at the observatory to deal with\nthem: the Proposal Preparation portal (to request observing time), the Phase2\nObserving tool and the submitphase2 web service (to trigger the ToOs), the TAC\nTracking portal (for telescope operators to support the observations) and the\nTACData portal (to publish and offer the images and their data products).",
        "In the pursuit of identifying rare two-particle events within the GADGET II\nTime Projection Chamber (TPC), this paper presents a comprehensive approach for\nleveraging Convolutional Neural Networks (CNNs) and various data processing\nmethods. To address the inherent complexities of 3D TPC track reconstructions,\nthe data is expressed in 2D projections and 1D quantities. This approach\ncapitalizes on the diverse data modalities of the TPC, allowing for the\nefficient representation of the distinct features of the 3D events, with no\nloss in topology uniqueness. Additionally, it leverages the computational\nefficiency of 2D CNNs and benefits from the extensive availability of\npre-trained models. Given the scarcity of real training data for the rare\nevents of interest, simulated events are used to train the models to detect\nreal events. To account for potential distribution shifts when predominantly\ndepending on simulations, significant perturbations are embedded within the\nsimulations. This produces a broad parameter space that works to account for\npotential physics parameter and detector response variations and uncertainties.\nThese parameter-varied simulations are used to train sensitive 2D CNN object\ndetectors. When combined with 1D histogram peak detection algorithms, this\nmulti-modal detection framework is highly adept at identifying rare,\ntwo-particle events in data taken during experiment 21072 at the Facility for\nRare Isotope Beams (FRIB), demonstrating a 100% recall for events of interest.\nWe present the methods and outcomes of our investigation and discuss the\npotential future applications of these techniques.",
        "In a recent study [1], the Bardeen-boson star (BBS) model involving a scalar\nfield minimally coupled to Einstein gravity and a Bardeen's nonlinear\nelectromagnetic field was investigated. It was found that when the magnetic\ncharge $q$ of the electromagnetic field exceeds a certain critical value $q_c$,\na frozen Bardeen-boson star (FBBS) can be obtained with the frequency\napproaching zero. In this paper, we study the null orbits in the background of\nthe general BBS and FBBS. We find that similar to the boson star (BS), all BBSs\ndo not have the event horizon and possess complete null geodesics, allowing\nphotons to move throughout the entire spacetime of BBS. Among these BBSs, the\nFBBSs whose spacetime is very similar to that of black holes are particularly\nspecial. The null orbits around the FBBSs exhibit sharp deflections near the\ncritical horizon while becoming nearly straight inside the critical horizon.\nFurthermore, the photon in the background of FBBSs moves for a very long time\ninside the critical horizon from the perspective of an infinity viewer."
      ]
    }
  },
  {
    "id":2412.02083,
    "research_type":"basic",
    "start_id":"b8",
    "start_title":"Quantum algorithms for supervised and unsupervised machine learning",
    "start_abstract":"Machine-learning tasks frequently involve problems of manipulating and classifying large numbers vectors in high-dimensional spaces. Classical algorithms for solving such typically take time polynomial the number dimension space. Quantum computers are good at tensor product This paper provides supervised unsupervised quantum machine learning cluster assignment finding. can logarithmic both their dimension, an exponential speed-up over classical algorithms.",
    "start_categories":[
      "Quantum Physics"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "Learning internal representations by back-propagating errors"
      ],
      "abstract":[
        "We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal \u2018hidden\u2019 units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "CBVLM: Training-free Explainable Concept-based Large Vision Language\n  Models for Medical Image Classification",
        "Regular black holes in Lovelock with degenerated ground state",
        "MITracker: Multi-View Integration for Visual Object Tracking",
        "The fibration sequences of the lattice path operad",
        "Optimal Transport-based Conformal Prediction",
        "Photometric Calibration & Spectral Validation of the Solar Ultraviolet\n  Imaging Telescope onboard Aditya-L1",
        "Solitonic vortices and black holes with vortex hair in AdS$_3$",
        "Quantum neural compressive sensing for ghost imaging",
        "Dimension-free estimates for discrete maximal functions and lattice\n  points in high-dimensional spheres and balls with small radii",
        "Fineness and smoothness of a KSBA moduli of marked cubic surfaces",
        "Wearable Haptics for a Marionette-inspired Teleoperation of Highly\n  Redundant Robotic Systems",
        "Improved Training Technique for Latent Consistency Models",
        "EXACT-CT: EXplainable Analysis for Crohn's and Tuberculosis using CT",
        "Dynamics of Quantum Correlations within the double Caldeira-Leggett\n  formalism",
        "PIMutation: Exploring the Potential of PIM Architecture for Quantum\n  Circuit Simulation",
        "Computational Assessment of Hemodynamics in Asymmetric-type Lesion of\n  Idealized Coronary Stenoses",
        "Digit quantum simulation of a fermion field in an expanding universe",
        "A new reducibility results for minihypers in finite projective\n  geometries",
        "AdaST: Dynamically Adapting Encoder States in the Decoder for End-to-End\n  Speech-to-Text Translation",
        "Unveiling Symmetry Instability induced by Topological Phase Transitions",
        "Latent-space adversarial training with post-aware calibration for\n  defending large language models against jailbreak attacks",
        "MIH-TCCT: Mitigating Inconsistent Hallucinations in LLMs via\n  Event-Driven Text-Code Cyclic Training",
        "An Improved Lower Bound on the Image of the 2-adic Character Map for the\n  Heisenberg Algebra via Modular Linear Differential Equations",
        "Pure Shape Dynamics: Relational General Relativity",
        "SPLD polynomial optimization and bounded degree SOS hierarchies",
        "Benchmarking global optimization techniques for unmanned aerial vehicle\n  path planning",
        "STGCN-LSTM for Olympic Medal Prediction: Dynamic Power Modeling and\n  Causal Policy Optimization",
        "scGSDR: Harnessing Gene Semantics for Single-Cell Pharmacological\n  Profiling",
        "To Patch or Not to Patch: Motivations, Challenges, and Implications for\n  Cybersecurity"
      ],
      "abstract":[
        "The main challenges limiting the adoption of deep learning-based solutions in\nmedical workflows are the availability of annotated data and the lack of\ninterpretability of such systems. Concept Bottleneck Models (CBMs) tackle the\nlatter by constraining the final disease prediction on a set of predefined and\nhuman-interpretable concepts. However, the increased interpretability achieved\nthrough these concept-based explanations implies a higher annotation burden.\nMoreover, if a new concept needs to be added, the whole system needs to be\nretrained. Inspired by the remarkable performance shown by Large\nVision-Language Models (LVLMs) in few-shot settings, we propose a simple, yet\neffective, methodology, CBVLM, which tackles both of the aforementioned\nchallenges. First, for each concept, we prompt the LVLM to answer if the\nconcept is present in the input image. Then, we ask the LVLM to classify the\nimage based on the previous concept predictions. Moreover, in both stages, we\nincorporate a retrieval module responsible for selecting the best examples for\nin-context learning. By grounding the final diagnosis on the predicted\nconcepts, we ensure explainability, and by leveraging the few-shot capabilities\nof LVLMs, we drastically lower the annotation cost. We validate our approach\nwith extensive experiments across four medical datasets and twelve LVLMs (both\ngeneric and medical) and show that CBVLM consistently outperforms CBMs and\ntask-specific supervised methods without requiring any training and using just\na few annotated examples. More information on our project page:\nhttps:\/\/cristianopatricio.github.io\/CBVLM\/.",
        "A new regular black hole solution for Lovelock gravity with an\n\\textit{n}-fold degenerate ground state AdS is provided. An alternative\ndefinition of the Kretschmann scalar for this theory is proposed, which is\nassociated with the gravitational tension of the Schwarzschild vacuum AdS of\nthis theory, which diverges at the radial origin. The proposed energy density\nencodes the latter's information in such a way that it takes a finite value at\nthe origin, thereby suppressing the existence of a central singularity. There\nis a value of the extremal radius, $r_{ext}$, which can be of the order of the\nPlanck length, such that for a value $r_* > r_{ext}$, just slightly greater\nthan it (also of the order of Planck), the solutions of the vacuum AdS black\nhole (BH) and our regular solution become indistinguishable. Thus, for\nrealistic values of the horizon radius, both cases mentioned are numerically\nindistinguishable. However, at short length scales such that $r < r_*$, both\nbehaviors differ. Therefore, the proposed matter sources lead to the formation\nof a de Sitter core rather than a central singularity. The thermodynamics of\nboth cases become indistinguishable for $r \\geq r_*$, \\text{i.e.}, for\nrealistic scales. However, at shorter scales $r < r_*$, quantum effects would\narise, meaning that instead of the temperature evolving to infinity as in the\nvacuum AdS BH, the matter sources proposed in this work cause the cooling of\nthe BH, through a phase transition, such that a BH remnant is reached at $T =\n0$, which represents what remains of the BH once the evaporation process halts.",
        "Multi-view object tracking (MVOT) offers promising solutions to challenges\nsuch as occlusion and target loss, which are common in traditional single-view\ntracking. However, progress has been limited by the lack of comprehensive\nmulti-view datasets and effective cross-view integration methods. To overcome\nthese limitations, we compiled a Multi-View object Tracking (MVTrack) dataset\nof 234K high-quality annotated frames featuring 27 distinct objects across\nvarious scenes. In conjunction with this dataset, we introduce a novel MVOT\nmethod, Multi-View Integration Tracker (MITracker), to efficiently integrate\nmulti-view object features and provide stable tracking outcomes. MITracker can\ntrack any object in video frames of arbitrary length from arbitrary viewpoints.\nThe key advancements of our method over traditional single-view approaches come\nfrom two aspects: (1) MITracker transforms 2D image features into a 3D feature\nvolume and compresses it into a bird's eye view (BEV) plane, facilitating\ninter-view information fusion; (2) we propose an attention mechanism that\nleverages geometric information from fused 3D feature volume to refine the\ntracking results at each view. MITracker outperforms existing methods on the\nMVTrack and GMTD datasets, achieving state-of-the-art performance. The code and\nthe new dataset will be available at\nhttps:\/\/mii-laboratory.github.io\/MITracker\/.",
        "We introduce a notion of an operad of complexity $m$, for $m \\geq 1$. Operads\nof complexity $1$ are monoids in the category of $\\mathbb{N}$-indexed\ncollections, with monoidal product given by the Day convolution, and operads of\ncomplexity $2$ are non-symmetric operads. In general, we prove that the operad\nfor operads of complexity $m$ is a suboperad of the $m$-th stage filtration of\nthe lattice path operad introduced by Batanin and Berger. Finally, we exhibit\nfibration sequences involving this new notion, extending the results of Turchin\nand Dwyer-Hess.",
        "Conformal Prediction (CP) is a principled framework for quantifying\nuncertainty in blackbox learning models, by constructing prediction sets with\nfinite-sample coverage guarantees. Traditional approaches rely on scalar\nnonconformity scores, which fail to fully exploit the geometric structure of\nmultivariate outputs, such as in multi-output regression or multiclass\nclassification. Recent methods addressing this limitation impose predefined\nconvex shapes for the prediction sets, potentially misaligning with the\nintrinsic data geometry. We introduce a novel CP procedure handling\nmultivariate score functions through the lens of optimal transport.\nSpecifically, we leverage Monge-Kantorovich vector ranks and quantiles to\nconstruct prediction region with flexible, potentially non-convex shapes,\nbetter suited to the complex uncertainty patterns encountered in multivariate\nlearning tasks. We prove that our approach ensures finite-sample,\ndistribution-free coverage properties, similar to typical CP methods. We then\nadapt our method for multi-output regression and multiclass classification, and\nalso propose simple adjustments to generate adaptive prediction regions with\nasymptotic conditional coverage guarantees. Finally, we evaluate our method on\npractical regression and classification problems, illustrating its advantages\nin terms of (conditional) coverage and efficiency.",
        "The Solar Ultraviolet Imaging Telescope (SUIT) is one of the seven payloads\non board Aditya-L1 mission of the Indian Space Research Organization (ISRO).\nSUIT provides full and partial disk images of the Sun in the 200-400 nm\nwavelength range. This would help us probe the solar atmosphere at different\nheights and understand the mass and energy transfer process between its layers.\nFor the first time, SUIT will also help us measure spatially resolved solar\nspectral irradiance at this wavelength band, which is significant for studying\nthe sun-climate relationships. To perform these studies, it is necessary to\nphotometrically calibrate the payload and validate the spectral coverage of the\nvarious bandpasses. We perform the photometric calibration and spectral\nvalidation of 8 bandpasses using light of known intensity and spectral\ncoverage. For photometric calibration, the telescope throughput is modeled\nusing sun-as-a-star spectrum from SOLSTICE and SOLSPEC. The modeled throughput\nis compared with in-lab measurements taken with light of known intensity. The\nratio of measured photoelectrons gathered with the modeled prediction agree\nwithin 20%. For spectral validation, readings are taken across the transmission\nspectrum of each filter, keeping adjacent readings independent of each other.\nThe relative intensity measured at each wavelength is seen to trace the modeled\ntelescope bandpass for that filter. These tests could not be performed for\nfilters with bandpasses operating below 250 nm (NB01, BB01 and BB02), primarily\ndue to heavy atmospheric attenuation in these wavelengths leading to decreased\nSNR of the data. The experimentally measured results agree closely with the\nmodeled values, validating SUIT's optical performance and presenting the\nreliability of the developed throughput model.",
        "We study soliton and black hole solutions with scalar hair in AdS$_3$ in a\ntheory with a Maxwell field and a charged scalar field with double trace\nboundary conditions, which can trigger the dual boundary theory to become a\nsuperconductor. We investigate the phase diagram as a function of the\ntemperature $T$ and of the double trace coupling $\\kappa$ and we find a rich\npattern of phase transitions, which can be of the Hawking-Page kind or can be\ndue to the condensation of the order parameter. We also find a transition\nbetween vortex solutions and the zero temperature limit of the black hole for a\ncritical value of the double trace coupling $\\kappa$. The Little-Park\nperiodicity is realized for the dual of the black hole solution with hair as a\nshift in the winding number and in the gauge field.",
        "Demonstrating the utility of quantum algorithms is a long-standing challenge,\nwhere quantum machine learning becomes one of the most promising candidate that\ncan be resorted to. In this study, we investigate a quantum neural compressive\nsensing algorithm for ghost imaging to showcase its utility. The algorithm\nutilizes the variational quantum circuits to reparameterize the inverse problem\nof ghost imaging and uses the inductive bias of the physical forward model to\nperform optimization. To validate the algorithm's effectiveness, we conduct\noptical ghost imaging experiments, capturing signals from objects at different\nphysical sampling rates and detection signal-to-noise ratios. The experimental\nresults show that our proposed algorithm surpasses conventional methods in both\nvisual appearance and quantitative metrics, achieving state-of-the-art\nperformance. Importantly, we observe that the quantum neural network, guided by\nprior knowledge of physics, effectively overcomes the challenge of barren\nplateau in the optimization process. The proposed algorithm demonstrates\nrobustness against various quantum noise levels, making it suitable for\nnear-term quantum devices. Our study leverages physical inductive bias guided\nvariational quantum algorithm, underscoring the potential of quantum\ncomputation in tackling a broad range of optimization and inverse problems.",
        "We prove that the discrete Hardy-Littlewood maximal function associated with\nEuclidean spheres with small radii has dimension-free estimates on\n$\\ell^p(\\mathbb{Z}^d)$ for $p\\in[2,\\infty).$ This implies an analogous result\nfor the Euclidean balls, thus making progress on a question of E.M. Stein from\nthe mid 1990s. Our work provides the first dimension-free estimates for full\ndiscrete maximal functions related to spheres and balls without relying on\ncomparisons with their continuous counterparts. An important part of our\nargument is a uniform (dimension-free) count of lattice points in\nhigh-dimensional spheres and balls with small radii. We also established a\ndimension-free estimate for a multi-parameter maximal function of a\ncombinatorial nature, which is a new phenomenon and may be useful for studying\nsimilar problems in the future.",
        "By work of Gallardo-Kerr-Schaffler, it is known that Naruki's\ncompactification of the moduli space of marked cubic surfaces is isomorphic to\nthe normalization of the Koll\\'ar, Shepherd-Barron, and Alexeev\ncompactification parametrizing pairs\n$\\left(S,\\left(\\frac{1}{9}+\\epsilon\\right)D\\right)$, with $D$ the sum of the\n$27$ marked lines on $S$, and their stable degenerations. In the current paper,\nwe show that the normalization assumption is not necessary as we prove that\nthis KSBA compactification is smooth. Additionally, we show it is a fine moduli\nspace. This is done by studying the automorphisms and the\n$\\mathbb{Q}$-Gorenstein obstructions of the stable pairs parametrized by it.",
        "The teleoperation of complex, kinematically redundant robots with\nloco-manipulation capabilities represents a challenge for human operators, who\nhave to learn how to operate the many degrees of freedom of the robot to\naccomplish a desired task. In this context, developing an easy-to-learn and\neasy-to-use human-robot interface is paramount. Recent works introduced a novel\nteleoperation concept, which relies on a virtual physical interaction interface\nbetween the human operator and the remote robot equivalent to a \"Marionette\"\ncontrol, but whose feedback was limited to only visual feedback on the human\nside. In this paper, we propose extending the \"Marionette\" interface by adding\na wearable haptic interface to cope with the limitations given by the previous\nworks. Leveraging the additional haptic feedback modality, the human operator\ngains full sensorimotor control over the robot, and the awareness about the\nrobot's response and interactions with the environment is greatly improved. We\nevaluated the proposed interface and the related teleoperation framework with\nnaive users, assessing the teleoperation performance and the user experience\nwith and without haptic feedback. The conducted experiments consisted in a\nloco-manipulation mission with the CENTAURO robot, a hybrid leg-wheel quadruped\nwith a humanoid dual-arm upper body.",
        "Consistency models are a new family of generative models capable of producing\nhigh-quality samples in either a single step or multiple steps. Recently,\nconsistency models have demonstrated impressive performance, achieving results\non par with diffusion models in the pixel space. However, the success of\nscaling consistency training to large-scale datasets, particularly for\ntext-to-image and video generation tasks, is determined by performance in the\nlatent space. In this work, we analyze the statistical differences between\npixel and latent spaces, discovering that latent data often contains highly\nimpulsive outliers, which significantly degrade the performance of iCT in the\nlatent space. To address this, we replace Pseudo-Huber losses with Cauchy\nlosses, effectively mitigating the impact of outliers. Additionally, we\nintroduce a diffusion loss at early timesteps and employ optimal transport (OT)\ncoupling to further enhance performance. Lastly, we introduce the adaptive\nscaling-$c$ scheduler to manage the robust training process and adopt\nNon-scaling LayerNorm in the architecture to better capture the statistics of\nthe features and reduce outlier impact. With these strategies, we successfully\ntrain latent consistency models capable of high-quality sampling with one or\ntwo steps, significantly narrowing the performance gap between latent\nconsistency and diffusion models. The implementation is released here:\nhttps:\/\/github.com\/quandao10\/sLCT\/",
        "Crohn's disease and intestinal tuberculosis share many overlapping features\nsuch as clinical, radiological, endoscopic, and histological features -\nparticularly granulomas, making it challenging to clinically differentiate\nthem. Our research leverages 3D CTE scans, computer vision, and machine\nlearning to improve this differentiation to avoid harmful treatment\nmismanagement such as unnecessary anti-tuberculosis therapy for Crohn's disease\nor exacerbation of tuberculosis with immunosuppressants. Our study proposes a\nnovel method to identify radiologist - identified biomarkers such as VF to SF\nratio, necrosis, calcifications, comb sign and pulmonary TB to enhance\naccuracy. We demonstrate the effectiveness by using different ML techniques on\nthe features extracted from these biomarkers, computing SHAP on XGBoost for\nunderstanding feature importance towards predictions, and comparing against\nSOTA methods such as pretrained ResNet and CTFoundation.",
        "This study investigates the effects of decoherence and squeezing on the\ndynamics of various kinds of quantum features--local quantum coherence, local\nentropy, EPR correlations, and entanglement--in the high-temperature limit of\nthe double Caldeira-Leggett model, focusing on initially squeezed states. We\ncompare two scenarios: (1) particles interacting with distinct environments and\n(2) particles coupled to a common environment. Our analysis reveals that common\nenvironments better preserve local coherence over time, whereas distinct\nenvironments accelerate decoherence. Temperature enhances decoherence and\nsuppresses coherence revivals, while squeezing affects transient dynamics but\nnot long-term coherence saturation. Local entropy increases with temperature\nand squeezing, though their underlying physical mechanisms differ. EPR\ncorrelations degrade due to environmental interactions, with squeezing\ninitially enhancing them but failing to prevent their eventual loss.\nEntanglement exhibits distinct behaviors: in separate environments, it\nundergoes sudden death, whereas in common environments, it experiences a dark\nperiod whose duration shortens with stronger squeezing. These findings provide\na comprehensive understanding of how decoherence and squeezing influence\nquantum correlations in open quantum systems.",
        "Quantum circuit simulations are essential for the verification of quantum\nalgorithms on behalf of real quantum devices. However, the memory requirements\nfor such simulations grow exponentially with the number of qubits involved in\nquantum programs. Moreover, a substantial number of computations in quantum\ncircuit simulations cause low locality data accesses, as they require extensive\ncomputations across the entire table of the full state vector. These\ncharacteristics lead to significant latency and energy overheads during data\ntransfers between the CPU and main memory. Processing-in-Memory (PIM), which\nintegrates computational logic near DRAM banks, could present a promising\nsolution to address these challenges. In this paper, we introduce PIMutation\n(PIM framework for qUanTum circuit simulATION) for achieving fast and\nenergy-efficient quantum circuit simulation. PIMutation is the first attempt to\nleverage UPMEM, a publicly available PIM-integrated DIMM, to implement quantum\ncircuit simulations. PIMutation incorporates three optimization strategies to\novercome the overhead of quantum circuit simulation using the real PIM system:\n(i) gate merging, (ii) row swapping, and (iii) vector partitioning. Our\nevaluations show that PIMutation achieves an average speedup of 2.99x and\n16.51x with a reduction of energy of 25.23% and 75.29% over the QuEST simulator\non CPU in 16- and 32-qubit benchmarks, respectively.",
        "Coronary artery stenosis, characterized by the narrowing of the lumen,\nsignificantly affects blood flow and contributes to the progression of\ncardiovascular diseases. This study investigates the hemodynamics of coronary\nartery models with varying stenosis configurations, all maintaining an 80%\nlumen reduction, to determine how differences in morphology influence flow\nbehavior and mechanical stresses. We employed computational fluid dynamics to\nanalyze five idealized geometries with (10% & 70%), (20% & 60%), (30% & 50%),\n(40% & 40%), and (0% & 80%) stenosis configurations. Through physiological\npulsatile flow conditions, we evaluated key hemodynamic pattern including\nvelocity profiles, wall shear stress, and pressure distribution. Our results\nreveal that despite the same degree of lumen reduction, each stenosis\nconfiguration produced distinct flow patterns and hemodynamic profiles.\nAsymmetric configurations, such as 10% & 70% and 20% & 60%, exhibited\npronounced flow disruptions and higher wall shear stress at the stenosis\nthroats, while symmetric configurations, such as 40% & 40%, demonstrated more\nuniform flow and reduced vortex. Our findings challenge the practice of\ngeneralizing results across stenosis configurations without accounting for\nmorphological variations, which is prevalent in many CFD studies using\nidealized models. This study emphasizes the importance of considering\nstenosis-specific morphology in CFD analyses and clinical interpretations to\nenhance the accuracy of diagnostic tools, improve personalized treatment\nplanning, and guide the design of medical devices such as stents.",
        "Quantum simulation is a rapidly evolving tool with great potential for\nresearch at the frontiers of physics, and is particularly suited to be used in\ncomputationally intensive lattice simulations, such as problems with\nnon-equilibrium. In this work, a basic scenario, namely free fermions in an\nexpanding universe, is considered and quantum simulations are used to perform\nthe evolution and study the phenomena involved. Using digital quantum\nsimulations with the Jordan-Wigner transformation and Trotter expansion, the\nevolutions of fermion number density, correlation functions, polarization, and\nchiral condensation are analyzed. A spread out phenomenon can be observed in\nthe simulation, which is a consequence of momentum redshift. This work also\ndemonstrates the simplicity and convenience of using quantum simulations when\nstudying time-evolution problems.",
        "In this paper we prove a new reducibility result for mini-hypers in\nprojective geometries over finite fields. It is further used to characterize\nthe minihypers with parameters (70, 22) in PG(4, 3). The latter can be used to\nattack the existence problem for some hypothetical ternary Griesmer codes of\ndimension 6.",
        "In end-to-end speech translation, acoustic representations learned by the\nencoder are usually fixed and static, from the perspective of the decoder,\nwhich is not desirable for dealing with the cross-modal and cross-lingual\nchallenge in speech translation. In this paper, we show the benefits of varying\nacoustic states according to decoder hidden states and propose an adaptive\nspeech-to-text translation model that is able to dynamically adapt acoustic\nstates in the decoder. We concatenate the acoustic state and target word\nembedding sequence and feed the concatenated sequence into subsequent blocks in\nthe decoder. In order to model the deep interaction between acoustic states and\ntarget hidden states, a speech-text mixed attention sublayer is introduced to\nreplace the conventional cross-attention network. Experiment results on two\nwidely-used datasets show that the proposed method significantly outperforms\nstate-of-the-art neural speech translation models.",
        "The symmetry-topology interplay dictates how to define order parameters and\nclassify material ordered phases. However, current understanding of this\ninterplay has been predominately approached from a one-sided perspective, with\ntopological states being classified within the constraints imposed by specific\nfixed symmetries. Here we complete this full circle by demonstrating\nspontaneous symmetry breaking that results from a periodic alteration of\ntopological phases induced by light in a centrosymmetric Dirac material\nZrTe$_5$. The distinguishing feature is the observation of robust correlation\nand striking anomalies in the fluence and temperature dependence of key\ntransport parameters.First, both shift current $J_{\\text{s}}$ and displacement\ncurrent $J_{\\text{d}}$, arising from interband transition and infrared phonon\ndriving, respectively, along with charge carrier pumping, exhibit similar\nbehaviors. Second, they all peak at similar low pump fluence, followed by a\nsubsequent reduction as the fluence further increases. This behavior cannot be\nexplained by conventional energetically allowed, direct excitations. Third, all\nthe three observables exhibit anomalies when they approach the topological\nphase transition temperature. These results highlight the unique low-energy\npumping behaviors in ZrTe$_5$, characterized by reversible fluence dependence\nand a 'hinge-like' interaction that connects various electronic and lattice\nobservables, including phonons, charge carriers, and currents. Our findings,\nsupported by model analysis, provide key insights into the fragility of\ncrystalline (inversion) and time-reversal symmetries during the dynamics of\ntopological phase transitions. This fragility drives spontaneous symmetry\nbreaking, evidenced by the synchronized emergence of off-resonant infrared\nphonons and broken-symmetry photocurrents.",
        "Ensuring safety alignment has become a critical requirement for large\nlanguage models (LLMs), particularly given their widespread deployment in\nreal-world applications. However, LLMs remain susceptible to jailbreak attacks,\nwhich exploit system vulnerabilities to bypass safety measures and generate\nharmful outputs. Although numerous defense mechanisms based on adversarial\ntraining have been proposed, a persistent challenge lies in the exacerbation of\nover-refusal behaviors, which compromise the overall utility of the model. To\naddress these challenges, we propose a Latent-space Adversarial Training with\nPost-aware Calibration (LATPC) framework. During the adversarial training\nphase, LATPC compares harmful and harmless instructions in the latent space and\nextracts safety-critical dimensions to construct refusal features attack,\nprecisely simulating agnostic jailbreak attack types requiring adversarial\nmitigation. At the inference stage, an embedding-level calibration mechanism is\nemployed to alleviate over-refusal behaviors with minimal computational\noverhead. Experimental results demonstrate that, compared to various defense\nmethods across five types of jailbreak attacks, LATPC framework achieves a\nsuperior balance between safety and utility. Moreover, our analysis underscores\nthe effectiveness of extracting safety-critical dimensions from the latent\nspace for constructing robust refusal feature attacks.",
        "Recent methodologies utilizing synthetic datasets have aimed to address\ninconsistent hallucinations in large language models (LLMs); however,these\napproaches are primarily tailored to specific tasks, limiting their\ngeneralizability. Inspired by the strong performance of code-trained models in\nlogic-intensive domains, we propose a novel framework that leverages\nevent-based text to generate corresponding code and employs cyclic training to\ntransfer the logical consistency of code to natural language effectively. Our\nmethod significantly reduces inconsistent hallucinations across three leading\nLLMs and two categories of natural language tasks while maintaining overall\nperformance. This framework effectively alleviates hallucinations without\nnecessitating adaptation to downstream tasks, demonstrating generality and\nproviding new perspectives to tackle the challenge of inconsistent\nhallucinations.",
        "We describe families of MLDEs whose solutions are modular forms of level one\nthat converge, $2$-adically, to a Hauptmodul on $\\Gamma_0(2)$ by using a\ntheorem of Serre. Then, we apply this to show that the image of the character\nmap on the $2$-adic Heisenberg VOA $S_{1}$ contains the space of $2$-adic\noverconvergent modular forms $M_{2}^{\\dagger}(1\/2)$ of weight zero.",
        "We present a Pure Shape Dynamics (PSD) formulation of General Relativity\n(GR), which implements full relationalism by eliminating absolute scale and\nexternal time references from the fundamental description of gravity. Starting\nfrom the Arnowitt-Deser-Misner (ADM) formulation, we derive a decoupled\ndynamical system that governs the evolution of the spatial conformal geometry\nand relational matter degrees of freedom, while eliminating the total volume\nand York time as independent dynamical variables. This results in an autonomous\nsubsystem describing an unparametrized trajectory in the conformal superspace\nof metric and matter configurations, with its evolution encoded in an equation\nof state that characterises the intrinsic geometric properties of the curve in\nshape space. We show that this equation of state is structurally analogous to\nthe corresponding PSD description of the Newtonian $N$-body problem,\nreinforcing the fundamental similarity between gravity and relational particle\ndynamics. Our framework is applied to the homogeneous Bianchi IX cosmological\nmodel, demonstrating that the Janus point evolution through the Big Bang, as\npreviously found in a symmetry-reduced setting, is a generic feature of the\nfull inhomogeneous PSD description. This work establishes PSD as a fully scale-\nand reparametrization-invariant formulation of classical gravity and lays the\nfoundation for addressing key open questions that are discussed at the end of\nthe paper.",
        "In this paper, a new class of structured polynomials, which we dub the {\\it\nseparable plus lower degree {\\rm (SPLD in short)} polynomials}, is introduced.\nThe formal definition of an SPLD polynomial, which extends the concept of the\nSPQ polynomial (Ahmadi et al. in Math Oper Res 48:1316--1343, 2023), is\ndefined. A type of bounded degree SOS hierarchy (BSOS-SPLD) is proposed to\nefficiently solve the optimization problems with SPLD polynomials, and several\nnumerical examples are performed much better than the bounded degree SOS\nhierarchy (Lasserre et al. in EURO J Comput Optim 5:87--117, 2017). An exact\nSOS relaxation for a class of convex SPLD polynomial optimization problems is\nproposed. Finally, an application of SPLD polynomials to polynomial regression\nproblems in statistics is presented.",
        "The Unmanned Aerial Vehicle (UAV) path planning problem is a complex\noptimization problem in the field of robotics. In this paper, we investigate\nthe possible utilization of this problem in benchmarking global optimization\nmethods. We devise a problem instance generator and pick 56 representative\ninstances, which we compare to established benchmarking suits through\nExploratory Landscape Analysis to show their uniqueness. For the computational\ncomparison, we select twelve well-performing global optimization techniques\nfrom both subfields of stochastic algorithms (evolutionary computation methods)\nand deterministic algorithms (Dividing RECTangles, or DIRECT-type methods). The\nexperiments were conducted in settings with varying dimensionality and\ncomputational budgets. The results were analyzed through several criteria\n(number of best-found solutions, mean relative error, Friedman ranks) and\nutilized established statistical tests. The best-ranking methods for the UAV\nproblems were almost universally the top-performing evolutionary techniques\nfrom recent competitions on numerical optimization at the Institute of\nElectrical and Electronics Engineers Congress on Evolutionary Computation.\nLastly, we discussed the variable dimension characteristics of the studied UAV\nproblems that remain still largely under-investigated.",
        "This paper proposes a novel hybrid model, STGCN-LSTM, to forecast Olympic\nmedal distributions by integrating the spatio-temporal relationships among\ncountries and the long-term dependencies of national performance. The\nSpatial-Temporal Graph Convolution Network (STGCN) captures geographic and\ninteractive factors-such as coaching exchange and socio-economic links-while\nthe Long Short-Term Memory (LSTM) module models historical trends in medal\ncounts, economic data, and demographics. To address zero-inflated outputs\n(i.e., the disparity between countries that consistently yield wins and those\nnever having won medals), a Zero-Inflated Compound Poisson (ZICP) framework is\nincorporated to separate random zeros from structural zeros, providing a\nclearer view of potential breakthrough performances. Validation includes\nhistorical backtracking, policy shock simulations, and causal inference checks,\nconfirming the robustness of the proposed method. Results shed light on the\ninfluence of coaching mobility, event specialization, and strategic investment\non medal forecasts, offering a data-driven foundation for optimizing sports\npolicies and resource allocation in diverse Olympic contexts.",
        "The rise of single-cell sequencing technologies has revolutionized the\nexploration of drug resistance, revealing the crucial role of cellular\nheterogeneity in advancing precision medicine. By building computational models\nfrom existing single-cell drug response data, we can rapidly annotate cellular\nresponses to drugs in subsequent trials. To this end, we developed scGSDR, a\nmodel that integrates two computational pipelines grounded in the knowledge of\ncellular states and gene signaling pathways, both essential for understanding\nbiological gene semantics. scGSDR enhances predictive performance by\nincorporating gene semantics and employs an interpretability module to identify\nkey pathways contributing to drug resistance phenotypes. Our extensive\nvalidation, which included 16 experiments covering 11 drugs, demonstrates\nscGSDR's superior predictive accuracy, when trained with either bulk-seq or\nscRNA-seq data, achieving high AUROC, AUPR, and F1 Scores. The model's\napplication has extended from single-drug predictions to scenarios involving\ndrug combinations. Leveraging pathways of known drug target genes, we found\nthat scGSDR's cell-pathway attention scores are biologically interpretable,\nwhich helped us identify other potential drug-related genes. Literature review\nof top-ranking genes in our predictions such as BCL2, CCND1, the AKT family,\nand PIK3CA for PLX4720; and ICAM1, VCAM1, NFKB1, NFKBIA, and RAC1 for\nPaclitaxel confirmed their relevance. In conclusion, scGSDR, by incorporating\ngene semantics, enhances predictive modeling of cellular responses to diverse\ndrugs, proving invaluable for scenarios involving both single drug and\ncombination therapies and effectively identifying key resistance-related\npathways, thus advancing precision medicine and targeted therapy development.",
        "As technology has become more embedded into our society, the security of\nmodern-day systems is paramount. One topic which is constantly under discussion\nis that of patching, or more specifically, the installation of updates that\nremediate security vulnerabilities in software or hardware systems. This\ncontinued deliberation is motivated by complexities involved with patching; in\nparticular, the various incentives and disincentives for organizations and\ntheir cybersecurity teams when deciding whether to patch. In this paper, we\ntake a fresh look at the question of patching and critically explore why\norganizations and IT\/security teams choose to patch or decide against it\n(either explicitly or due to inaction). We tackle this question by aggregating\nand synthesizing prominent research and industry literature on the incentives\nand disincentives for patching, specifically considering the human aspects in\nthe context of these motives. Through this research, this study identifies key\nmotivators such as organizational needs, the IT\/security team's relationship\nwith vendors, and legal and regulatory requirements placed on the business and\nits staff. There are also numerous significant reasons discovered for why the\ndecision is taken not to patch, including limited resources (e.g.,\nperson-power), challenges with manual patch management tasks, human error, bad\npatches, unreliable patch management tools, and the perception that related\nvulnerabilities would not be exploited. These disincentives, in combination\nwith the motivators above, highlight the difficult balance that organizations\nand their security teams need to maintain on a daily basis. Finally, we\nconclude by discussing implications of these findings and important future\nconsiderations."
      ]
    }
  },
  {
    "id":2411.0675,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"3-D ultrasound imaging: a review",
    "start_abstract":"The development of 3-D ultrasound imaging is a way to address the disadvantages conventional imaging. In this article authors review approaches that have been attempted in such as B-mode, color Doppler, and power Doppler systems. Acquisition, reconstruction, rendering techniques for are discussed, well applications limitations.",
    "start_categories":[
      "cs.CV",
      "eess.IV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b13"
      ],
      "title":[
        "Symmetric diffeomorphic image registration with crosscorrelation: evaluating automated labeling of elderly and neurodegenerative brain"
      ],
      "abstract":[
        "One of the most challenging problems in modern neuroimaging is detailed characterization of neurodegeneration. Quantifying spatial and longitudinal atrophy patterns is an important component of this process. These spatiotemporal signals will aid in discriminating between related diseases, such as frontotemporal dementia (FTD) and Alzheimer's disease (AD), which manifest themselves in the same at-risk population. Here, we develop a novel symmetric image normalization method (SyN) for maximizing the cross-correlation within the space of diffeomorphic maps and provide the Euler-Lagrange equations necessary for this optimization. We then turn to a careful evaluation of our method. Our evaluation uses gold standard, human cortical segmentation to contrast SyN's performance with a related elastic method and with the standard ITK implementation of Thirion's Demons algorithm. The new method compares favorably with both approaches, in particular when the distance between the template brain and the target brain is large. We then report the correlation of volumes gained by algorithmic cortical labelings of FTD and control subjects with those gained by the manual rater. This comparison shows that, of the three methods tested, SyN's volume measurements are the most strongly correlated with volume measurements gained by expert labeling. This study indicates that SyN, with cross-correlation, is a reliable method for normalizing and making anatomical measurements in volumetric MRI of patients and at-risk elderly individuals."
      ],
      "categories":[
        "q-bio.NC"
      ]
    },
    "list":{
      "title":[
        "Super-resolution Live-cell Fluorescence Lifetime Imaging",
        "A New Circle Theorem for Two Dimensional Ising Spin Glasses",
        "Learning Conditional Average Treatment Effects in Regression\n  Discontinuity Designs using Bayesian Additive Regression Trees",
        "Reducing Frequency Bias of Fourier Neural Operators in 3D Seismic\n  Wavefield Simulations Through Multi-Stage Training",
        "Left Jacobson Rings",
        "Safe exploration in reproducing kernel Hilbert spaces",
        "Orthogonal Delay-Doppler Division Multiplexing Modulation with\n  Hierarchical Mode-Based Index Modulation",
        "Periodic Response Solutions to Multi-Dimensional Nonlinear Schr\\\"odinger\n  equation with unbounded perturbation",
        "DESI DR2 Results II: Measurements of Baryon Acoustic Oscillations and\n  Cosmological Constraints",
        "Spectral modelling of Cygnus A between 110 and 250 MHz. Impact on the\n  LOFAR 21-cm signal power spectrum",
        "Calculating the Hawking Temperature of Black Holes in $f(Q)$ Gravity\n  Using the RVB Method: A Residue-Based Approach",
        "Slow Light Waveguides based on Bound States in the Continuum",
        "Multivariable $p$-adic Hodge theory for products of Galois groups",
        "Convex Split Lemma without Inequalities",
        "FGM Modeling of Thermo-Diffusive Unstable Lean Premixed Hydrogen-Air\n  Flames",
        "An edge crack and a crack close to the vertex of a wedge",
        "An Arbitrary Time Interval Generator Base on Vernier Clocks with 0.67 ps\n  Adjustable Steps Implemented in FPGA",
        "Beyond the Median Voter Theorem: A New Framework for Ideological\n  Positioning",
        "Analyzing the Impact of AC False Data Injection Attacks on Power System\n  Operation",
        "Measurement of energy reduction by inertial Alfv\\'en waves propagating\n  through parallel gradients in the Alfv\\'en speed",
        "Minimax rates of convergence for the nonparametric estimation of the\n  diffusion coefficient from time-homogeneous SDE paths",
        "Gas excitation in galaxies and active galactic nuclei with He\n  II{\\lambda}4686 and X-ray emission",
        "A Practical Introduction to Kernel Discrepancies: MMD, HSIC & KSD",
        "New developments in 3D-trench electrode sensors",
        "Self-propulsion and self-rotation of an inertial chiral active\n  Ornstein-Uhlenbeck particle",
        "Deriving pulsar pair-production multiplicities from pulsar wind nebulae\n  using H.E.S.S. and LHAASO observations",
        "Double metasurfaces and Optimal transport",
        "Edge spectrum for truncated $\\mathbb{Z}_2$-insulators",
        "On the emergence and properties of weird quasiperiodic attractors"
      ],
      "abstract":[
        "Super-resolution Structured Illumination Microscopy (SR-SIM) enables\nfluorescence microscopy beyond the diffraction limit at high frame rates.\nCompared to other super-resolution microscopy techniques, the low photon\nfluence used in SR-SIM makes it readily compatible with live-cell imaging.\nHere, we combine SR-SIM with electro-optic fluorescence lifetime imaging\n(EOFLIM), adding the capability of monitoring physicochemical parameters with\n156 nm spatial resolution at high frame rate for live-cell imaging. We\ndemonstrate that our new SIMFLIM technique enables super-resolved multiplexed\nimaging of spectrally overlapping fluorophores, environmental sensing, and\nlive-cell imaging.",
        "The Lee-Yang circle theorem revolutionized our understanding of phase\ntransitions in ferromagnetic systems by showing that the complex zeros of\npartition functions lie on the unit circle, with criticality arising as these\nzeros approach the real axis in the thermodynamic limit. However, in frustrated\nsystems such as antiferromagnets and spin glasses, the zeros deviate from this\nstructure, making it challenging to extend the Lee-Yang theory to disordered\nsystems. In this work, we establish a new circle theorem for two-dimensional\nIsing spin glasses, proving that the square of the partition function exhibits\nzeros densely packed along the unit circle. Numerical simulations on the square\nlattice confirm our theoretical predictions, demonstrating the validity of the\ncircle law for quenched disorder. Furthermore, our results uncover a\nfinite-temperature crossover in $\\pm J$ spin glasses, characterized by the\nemergence of a spectral gap in the angular distribution of zeros. This result\nextends the Lee-Yang framework to disordered systems, offering new insights\ninto spin-glass criticality.",
        "BART (Bayesian additive regression trees) has been established as a leading\nsupervised learning method, particularly in the field of causal inference. This\npaper explores the use of BART models for learning conditional average\ntreatment effects (CATE) from regression discontinuity designs, where treatment\nassignment is based on whether an observed covariate (called the running\nvariable) exceeds a pre-specified threshold. A purpose-built version of BART\nthat uses linear regression leaf models (of the running variable and treatment\nassignment dummy) is shown to out-perform off-the-shelf BART implementations as\nwell as a local polynomial regression approach and a CART-based approach. The\nnew method is evaluated in thorough simulation studies as well as an empirical\napplication looking at the effect of academic probation on student performance.",
        "The recent development of Neural Operator (NeurOp) learning for solutions to\nthe elastic wave equation shows promising results and provides the basis for\nfast large-scale simulations for different seismological applications. In this\npaper, we use the Fourier Neural Operator (FNO) model to directly solve the 3D\nHelmholtz wave equation for fast seismic ground motion simulations on different\nfrequencies, and show the frequency bias of the FNO model, i.e. it learns the\nlower frequencies better comparing to the higher frequencies. To reduce the\nfrequency bias, we adopt the multi-stage FNO training, i.e., after training a\n1st stage FNO model for estimating the ground motion, we use a second FNO model\nas the 2nd stage to learn from the residual, which greatly reduced the errors\non the higher frequencies. By adopting this multi-stage training, the FNO\nmodels show reduced biases on higher frequencies, which enhanced the overall\nresults of the ground motion simulations. Thus the multi-stage training FNO\nimproves the accuracy and realism of the ground motion simulations.",
        "We say that a ring is strongly (resp. weakly) left Jacobson if every\nsemiprime (resp. prime) left ideal is an intersection of maximal left ideals.\nThere exist Jacobson rings that are not weakly left Jacobson, e.g. the Weyl\nalgebra. Our main result is the following one-sided noncommutative\nNullstellensatz: For any finite-dimensional F-algebra A the ring\nA[$x_1$,...,$x_n$] of polynomials with coefficients in A is strongly left\nJacobson and every maximal left ideal of A[$x_1$,...,$x_n$] has a special form.\nWe also prove that an algebra that is a finitely generated module over its\ncenter is weakly left Jacobson iff it is Jacobson, and that an Azumaya algebra\nis strongly left Jacobson iff its center is Jacobson.",
        "Popular safe Bayesian optimization (BO) algorithms learn control policies for\nsafety-critical systems in unknown environments. However, most algorithms make\na smoothness assumption, which is encoded by a known bounded norm in a\nreproducing kernel Hilbert space (RKHS). The RKHS is a potentially\ninfinite-dimensional space, and it remains unclear how to reliably obtain the\nRKHS norm of an unknown function. In this work, we propose a safe BO algorithm\ncapable of estimating the RKHS norm from data. We provide statistical\nguarantees on the RKHS norm estimation, integrate the estimated RKHS norm into\nexisting confidence intervals and show that we retain theoretical guarantees,\nand prove safety of the resulting safe BO algorithm. We apply our algorithm to\nsafely optimize reinforcement learning policies on physics simulators and on a\nreal inverted pendulum, demonstrating improved performance, safety, and\nscalability compared to the state-of-the-art.",
        "The orthogonal time frequency space with index modulation (OTFS-IM) offers\nflexible tradeoffs between spectral efficiency (SE) and bit error rate (BER) in\ndoubly selective fading channels. While OTFS-IM schemes demonstrated such\npotential, a persistent challenge lies in the detection complexity. To address\nthis problem, we propose the hierarchical mode-based index modulation (HMIM).\nHMIM introduces a novel approach to modulate information bits by IM patterns,\nsignificantly simplifying the complexity of maximum a posteriori (MAP)\nestimation with Gaussian noise. Further, we incorporate HMIM with the recently\nproposed orthogonal delay-Doppler division multiplexing (ODDM) modulation,\nnamely ODDM-HMIM, to exploit the full diversity of the delay-Doppler (DD)\nchannel. The BER performance of ODDM-HMIM is analyzed considering a maximum\nlikelihood (ML) detector. Our numerical results reveal that, with the same SE,\nHMIM can outperform conventional IM in terms of both BER and computational\ncomplexity. In addition, we propose a successive interference\ncancellation-based minimum mean square error (SIC-MMSE) detector for ODDM-HMIM,\nwhich enables low-complexity detection with large frame sizes.",
        "By applying the Craig-Wayne-Bourgain (CWB) method, we establish the existence\nof periodic response solutions to multi-dimensional nonlinear Schr\\\"{o}dinger\nequations (NLS) with unbounded perturbation.",
        "We present baryon acoustic oscillation (BAO) measurements from more than 14\nmillion galaxies and quasars drawn from the Dark Energy Spectroscopic\nInstrument (DESI) Data Release 2 (DR2), based on three years of operation. For\ncosmology inference, these galaxy measurements are combined with DESI\nLyman-$\\alpha$ forest BAO results presented in a companion paper. The DR2 BAO\nresults are consistent with DESI DR1 and SDSS, and their distance-redshift\nrelationship matches those from recent compilations of supernovae (SNe) over\nthe same redshift range. The results are well described by a flat $\\Lambda$CDM\nmodel, but the parameters preferred by BAO are in mild, $2.3\\sigma$ tension\nwith those determined from the cosmic microwave background (CMB), although the\nDESI results are consistent with the acoustic angular scale $\\theta_*$ that is\nwell-measured by Planck. This tension is alleviated by dark energy with a\ntime-evolving equation of state parametrized by $w_0$ and $w_a$, which provides\na better fit to the data, with a favored solution in the quadrant with $w_0>-1$\nand $w_a<0$. This solution is preferred over $\\Lambda$CDM at $3.1\\sigma$ for\nthe combination of DESI BAO and CMB data. When also including SNe, the\npreference for a dynamical dark energy model over $\\Lambda$CDM ranges from\n$2.8-4.2\\sigma$ depending on which SNe sample is used. We present evidence from\nother data combinations which also favor the same behavior at high\nsignificance. From the combination of DESI and CMB we derive 95% upper limits\non the sum of neutrino masses, finding $\\sum m_\\nu<0.064$ eV assuming\n$\\Lambda$CDM and $\\sum m_\\nu<0.16$ eV in the $w_0w_a$ model. Unless there is an\nunknown systematic error associated with one or more datasets, it is clear that\n$\\Lambda$CDM is being challenged by the combination of DESI BAO with other\nmeasurements and that dynamical dark energy offers a possible solution.",
        "Studying the redshifted 21-cm signal from the the neutral hydrogen during the\nEpoch of Reionization and Cosmic Dawn is fundamental for understanding the\nphysics of the early universe. One of the challenges that 21-cm experiments\nface is the contamination by bright foreground sources, such as Cygnus A, for\nwhich accurate spatial and spectral models are needed to minimise the residual\ncontamination after their removal. In this work, we develop a new,\nhigh-resolution model of Cygnus A using Low Frequency Array (LOFAR)\nobservations in the $110{-}250$ MHz range, improving upon previous models by\nincorporating physical spectral information through the forced-spectrum method\nduring multi-frequency deconvolution. This approach addresses the limitations\nof earlier models by providing a more accurate representation of the complex\nstructure and spectral behaviour of Cygnus A, including the spectral turnover\nin its brightest hotspots. The impact of this new model on the LOFAR 21-cm\nsignal power spectrum is assessed by comparing it with both simulated and\nobserved North Celestial Pole data sets. Significant improvements are observed\nin the cylindrical power spectrum along the Cygnus A direction, highlighting\nthe importance of having spectrally accurate models of the brightest foreground\nsources. However, this improvement is washed out in the spherical power\nspectrum, where we measure differences of a few hundred mK at\n$k<0.63\\,h\\,\\text{cMpc}^{-1}$, but not statistically significant. The results\nsuggest that other systematic effects must be mitigated before a substantial\nimpact on 21-cm power spectrum can be achieved.",
        "This paper investigates the computation of Hawking temperatures for black\nholes within various $f(Q)$ gravity models using the RVB method. This\ntopological approach uncovers an additional term in the temperature\ncalculation, which we propose originates from the residue of a specific contour\nintegral related to the metric or curvature. By examining several specific\n$f(Q)$ models including quadratic, logarithmic, square root, and power law\nmodifications as well as well known black hole solutions such as RN, Kerr, and\nKN black holes, we demonstrate that the correction term can consistently be\ninterpreted as this residue. Our findings provide new insights into black hole\nthermodynamics within modified gravity frameworks.",
        "The concept of bound states in the continuum (BIC) has been advancing light\nconfinement technology in leaky environments. In this letter, we propose and\nnumerically demonstrate a slow light waveguide based on a BIC mode. We\nconsidered a waveguide with a polymer core loaded on a plane slab, which\nsupports a leaky guided mode coupled to the radiation continuum in the slab. We\nfound that periodic modulation of the polymer core along the propagation\ndirection can result in a high group index mode with a low propagation loss due\nto BIC confinement. The introduction of one-dimensional photonic crystals into\nthe BIC waveguides will largely expand its functionality and applications in\nintegrated photonics.",
        "In this paper we explain how to attach to a family of $p$-adic\nrepresentations of a product of Galois groups an overconvergent family of\nmultivariable $(\\varphi,\\Gamma)$-modules, generalizing results from Pal-Zabradi\nand Carter-Kedlaya-Zabradi, using Colmez-Sen-Tate descent. We also define rings\nof multivariable crystalline and semistable periods, and explain how to recover\nthis multivariable $p$-adic theory attached to a family of representations from\nits multivariable $(\\varphi,\\Gamma)$-module. We also explain how our framework\nallows us to recover the main results of Brinon-Chiarellotto-Mazzari on\nmultivariable $p$-adic Galois representations.",
        "We introduce a refinement to the convex split lemma by replacing the max\nmutual information with the collision mutual information, transforming the\ninequality into an equality. This refinement yields tighter achievability\nbounds for quantum source coding tasks, including state merging and state\nsplitting. Furthermore, we derive a universal upper bound on the smoothed max\nmutual information, where \"universal\" signifies that the bound depends\nexclusively on R\\'enyi entropies and is independent of the system's dimensions.\nThis result has significant implications for quantum information processing,\nparticularly in applications such as the reverse quantum Shannon theorem.",
        "Ultra-lean premixed hydrogen combustion is a possible solution to decarbonize\nindustry, while limiting flame temperatures and thus nitrous oxide emissions.\nThese lean hydrogen\/air flames experience strong preferential diffusion\neffects, which result in thermo-diffusive (TD) instabilities. To efficiently\nand accurately model lean premixed hydrogen flames, it is crucial to\nincorporate these preferential diffusion effects into flamelet tabulated\nchemistry frameworks, such as the Flamelet-Generated Manifold (FGM) method.\nThis is challenging because the preferential diffusion terms in the control\nvariable transport equations contain diffusion fluxes of all species in the\nmechanism. In this work, a new implementation is presented; the full term is\nreduced by only considering the most contributing species. When carefully\nselecting this set of major species, preferential diffusion fluxes along the\nflame front, i.e., cross-diffusion, can be captured. This is particularly\nimportant for manifolds that include heat loss effects, where enthalpy is one\nof the control variables. The diffusion of the H-radical has a significant\ncontribution to the enthalpy transport equation, and cross-diffusion of the\nH-radical is non-negligible. Two manifolds, without and with heat loss effects,\nand the set of major species are analyzed in an a-priori and a-posteriori\nmanner. Simulations of TD unstable hydrogen-air flames with detailed chemistry\nand several FGM models show that accurately capturing cross-diffusion of\nenthalpy is important for correctly predicting the flame shape and dynamics.",
        "Two model problems of an elastic wedge with an internal and edge crack are\nanalyzed. The problem of an internal crack reduces to an order-4 vector\nRiemann-Hilbert problem whose matrix kernel entries are meromorphic functions\nand have exponential factors. When the internal crack is located along one of\nthe wedge sides, an efficient method of solution is proposed. It requires a\nfactorization of the order-2 matrix coefficient associated with the\ncorresponding problem of an edge crack and the solution of an infinite system\nof linear algebraic system with an exponential rate of convergence of an\napproximate solution to the exact one. The order-2 Khrapkov's factorization is\nmodified by splitting the matrix kernel into a scalar dominant function and a\n``regular\" matrix whose factorization is more convenient for numerical\npurposes. Expressions for the stress intensity coefficients and the potential\nenergy released when the crack advances are derived. Asymptotic relations for\nthe stress intensity coefficients and the potential energy when one of the\ncrack tips is close the wedge vertex are obtained.",
        "In TDC testing or timing system implementation tasks, it is often desirable\nto generate signal pulses with fine adjustable time intervals. In delay\ncell-based schemes, the time adjustment steps are limited by the propagation\ndelays of the cells, which are typically 15 to 20 picoseconds per step and are\nsensitive to temperature and operating voltage. In this document, a purely\ndigital scheme based on two vernier clocks with small frequency difference\ngenerated using cascaded PLL is reported. The scheme is tested in two families\nof low-cost FPGA and 0.67 and 0.97 picoseconds adjustable steps of the time\nintervals are achieved.",
        "This paper revisits the limitations of the Median Voter Theorem and\nintroduces a novel framework to analyze the optimal economic ideological\npositions of political parties. By incorporating Nash equilibrium, we examine\nthe mechanisms and elasticity of ideal deviation costs, voter distribution, and\npolicy feasibility. Our findings show that an increase in a party's ideal\ndeviation cost shifts its optimal ideological position closer to its ideal\npoint. Additionally, if a voter distribution can be expressed as a positive\nlinear combination of two other distributions, its equilibrium point must lie\nwithin the interval defined by the equilibrium points of the latter two. We\nalso find that decreasing feasibility costs incentivize governments, regardless\nof political orientation, to increase fiscal expenditures (e.g., welfare) and\nreduce fiscal revenues (e.g., taxes). This dynamic highlights the fiscal\npressures commonly faced by democratic nations under globalization. Moreover,\nwe demonstrate that even with uncertain voter distributions, parties can\nidentify optimal ideological positions to maximize their utility. Lastly, we\nexplain why the proposed framework cannot be applied to community ideologies\ndue to their fundamentally different nature. This study provides new\ntheoretical insights into political strategies and establishes a foundation for\nfuture empirical research.",
        "False Data Injection (FDI) attacks are a significant threat to modern power\nsystems. Although numerous research studies have focused on FDI attacks on\npower systems, these studies have primarily concentrated on designing or\ndetecting DC FDI attacks, with less attention given to the impact analysis of\nAC FDI attacks. AC FDI attacks are potentially more harmful as they can easily\nbypass bad data detection (BDD) algorithms. In this paper, we present a unified\napproach to investigate the impact of AC FDI attacks on power transmission\nlines using the PowerWorld simulator. We also investigate the impact of\ndifferent FDI attack designs, including those optimally designed to evade BDD\nalgorithms and compare them accordingly. Our findings demonstrate that in\ndesigning optimal AC FDI attacks, a trade-off between the residuals of state\nvariables and the corresponding impacts of the proposed attack should be\nconsidered. This is because optimal attacks result in fewer changes in the\nattacked variable states and their estimated residuals compared to arbitrary AC\nFDI attacks. Moreover, the impacts of optimal AC FDI attacks can be less severe\nthan those of arbitrary attacks. We implement and analyze the proposed approach\non the IEEE 39-bus test system using PowerWorld simulator.",
        "We have studied the propagation of inertial Alfv\\'en waves through parallel\ngradients in the Alfv\\'en speed using the Large Plasma Device at the University\nof California, Los Angeles. The reflection and transmission of Alfv\\'en waves\nthrough inhomogeneities in the background plasma is important for understanding\nwave propagation, turbulence, and heating in space, laboratory, and\nastrophysical plasmas. Here we \\rev{present inertial Alfv\\'en waves, under\nconditions relevant to solar flares and the solar corona. We find} that the\ntransmission of the inertial Alfv\\'en waves is reduced as the sharpness of the\ngradient is increased. Any reflected waves were below the detection limit of\nour experiment and reflection cannot account for all of the energy not\ntransmitted through the gradient. Our findings indicate that, for both kinetic\nand inertial Alfv\\'en waves, the controlling parameter for the transmission of\nthe waves through an Alfv\\'en speed gradient is the ratio of the Alfv\\'en\nwavelength along the gradient divided by the scale length of the gradient.\nFurthermore, our results suggest that an as-yet-unidentified damping process\noccurs in the gradient.",
        "Consider a diffusion process X, solution of a time-homogeneous stochastic\ndifferential equation. We assume that the diffusion process X is observed at\ndiscrete times, at high frequency, which means that the time step tends toward\nzero. In addition, the drift and diffusion coefficients of the process X are\nassumed to be unknown. In this paper, we study the minimax rates of convergence\nof the nonparametric estimators of the square of the diffusion coefficient. Two\nobservation schemes are considered depending on the estimation interval. The\nsquare of the diffusion coefficient is estimated on the real line from repeated\nobservations of the process X, where the number of diffusion paths tends to\ninfinity. For the case of a compact estimation interval, we study the\nnonparametric estimation of the square of the diffusion coefficient constructed\nfrom a single diffusion path on one side and from repeated observations on the\nother side, where the number of trajectories tends to infinity. In each of\nthese cases, we establish minimax convergence rates of the risk of estimation\nof the diffusion coefficient over a space of Holder functions.",
        "The origin of He II emission in galaxies remains a debated topic, requiring\nionizing photons with energies exceeding 54 eV. While massive stars, such as\nWolf-Rayet stars, have been considered potential sources, their UV flux often\nfails to fully explain the observed He II emission. Recent studies suggest that\nX-ray binaries (XRBs) might contribute significantly to this ionization. We\nexplore the relationship between X-ray and $\\rm He~II \\lambda4686$ emission in\na statistically significant sample of galaxies, investigating whether X-ray\nsources, including active galactic nuclei (AGNs) and XRBs, serve as the primary\nmechanism for He II ionization across different galactic environments. We\ncross-matched a sample of known well-detected He II galaxies with the Chandra\nSource Catalog, yielding 165 galaxies with X-ray and $\\rm He~II \\lambda4686$\ndetections. The sources were classified into star-forming galaxies (SFGs) and\nAGNs based on the BPT diagram and a classification scheme defined for He II\ngalaxies. We find a strong, linear correlation between X-ray and He II\nluminosity across AGNs and SFGs spanning over seven orders of magnitude. AGNs\ngenerally exhibit higher He II\/H$\\beta$ flux ratios, stronger extinction, and\nharder X-ray spectra. The O32 ratio of SFGs is tightly correlated with the\nH$\\beta$ equivalent width ($\\rm EW_{H\\beta}$) but not with the He II\/H$\\beta$\nratio, suggesting a different excitation mechanism. We derive an O32--$\\rm\nEW_{H\\beta}$ line above which only AGNs of our sample reside. The tight\ncorrelation between X-ray and He II luminosity supports X-rays as the primary\ndriver of He II excitation. While AGNs have one common ionization source, the\ncentral black hole, in SFGs low-energy species are mainly excited by UV\nemission related to star-forming activity, however, high-energy species like He\nII require the presence of XRBs.",
        "This article provides a practical introduction to kernel discrepancies,\nfocusing on the Maximum Mean Discrepancy (MMD), the Hilbert-Schmidt\nIndependence Criterion (HSIC), and the Kernel Stein Discrepancy (KSD). Various\nestimators for these discrepancies are presented, including the commonly-used\nV-statistics and U-statistics, as well as several forms of the more\ncomputationally-efficient incomplete U-statistics. The importance of the choice\nof kernel bandwidth is stressed, showing how it affects the behaviour of the\ndiscrepancy estimation. Adaptive estimators are introduced, which combine\nmultiple estimators with various kernels, addressing the problem of kernel\nselection.",
        "Future high-luminosity hadron collider experiments feature unprecedented\nlevels of event pile-up and extreme radiation environments, calling for sensors\ncapable of 4D tracking, even after significant radiation damage. To this\npurpose, 3D sensors represent a viable solution, since they provide excellent\nradiation tolerance and very good temporal resolution. In particular, owing to\nthe uniform electric field and weighting field distributions, 3D-trench\nelectrode sensors from the INFN TIMESPOT project have shown a temporal\nresolution of $\\sim$10 ps after irradiation fluences up to 1$\\times$10$^{17}$\n1-Mev n$_{eq}$\/cm$^2$. In spite of the excellent performance of these sensors,\n3D-trench pixel technology is not yet fully established and the fabrication\nyield is not yet adequate for the production of large size pixel sensors. To\nimprove the potential of the 3D-trench concept for large-area sensors, a new\nbatch of sensors was designed at the University of Trento and fabricated at\nFBK, as part of the AIDA Innova project. Besides introducing some process\nimprovements, this batch includes two different sensor variants: the standard\none with continuous ohmic trenches, and a modified one with dashed ohmic\ntrenches. On-wafer electrical test results show that most of the sensors have\nlow leakage current and high breakdown voltage. Moreover, the fabrication yield\nfor the new design variant is higher than that of the standard design.",
        "We investigate the transport feature of an inertial chiral active\nOrnstein-Uhlenbeck particle moving on a two-dimensional surface. Using both\nanalytical approach and numerical simulations, we have exactly explored the\ntransient and steady-state behavior of the particle by analyzing the simulated\nparticle trajectories, probability distribution functions for position and\nvelocity, mean square displacement, mean square velocity, and effective kinetic\ntemperature of the medium. From the mean square displacement calculations, we\nobserve that, unlike an inertial active Brownian particle, a chiral active\nparticle manifests an initial ballistic, intermediate sub-diffusive to\nnon-diffusive, and the conventional long-time diffusive behavior. The\nintermediate sub-diffusive to non-diffusive behavior is prominent for the\nself-propulsion of an overdamped particle. It can be understood by\nchirality-induced transient confinement, which persists for short time\nintervals and diffuses away in the time asymptotic limit or at the steady\nstate. This behavior is further complemented by the exact calculation of mean\nsquare velocity or effective kinetic temperature of the medium, which is a\ndecreasing function of the magnitude of chirality. Moreover, the steady-state\nMSD and MSV are found to have a dependence both on chirality and activity time\nscale and hence can be controlled by tuning the persistent duration of activity\nor strength of the chirality of the particle.",
        "Pulsar wind nebulae (PWNe) dominate the galactic gamma-ray sky at very high\nenergies and they are major contributors to the leptonic cosmic ray flux.\nHowever, the question of whether or not pulsars also accelerate ions to\ncomparable energies has not yet been experimentally confirmed. We aim to\nconstrain the birth period and pair-production multiplicity for a set of\npulsars. In doing so, we aim to constrain the proportion of ions in the pulsar\nmagnetosphere and, hence, the proportion of ions that could enter the pulsar\nwind. We estimated possible ranges of the value of the average pair production\nmultiplicity for a sample of 26 pulsars in the Australia Telescope National\nFacility (ATNF) catalogue, which have also been observed by the High Energy\nStereoscopic System (H.E.S.S.) telescopes. We then derived lower limits for the\npulsar birth periods and average pair production multiplicities for a subset of\nthese sources where the extent of the pulsar wind nebula and surrounding\nsupernova shell have been measured in the radio. We also derived curves for the\naverage pair production multiplicities as a function of birth period for\nsources recently observed by the Large High Altitude Air Shower Observatory\n(LHAASO). We show that there is a potential for hadrons entering the pulsar\nwind for most of the H.E.S.S. and LHAASO sources we consider here, which is\ndependent upon the efficiency of luminosity conversion into particles. We also\npresent estimates of the pulsar birth period for six of these sources, all\nfalling into the range of $\\sim$10-50 ms.",
        "This paper constructs metalenses that separate homogeneous media with\ndifferent refractive indices, refracting one domain into another while\nconserving a prescribed energy distribution. Using optimal transport theory, we\ndesign singlet and doublet metalenses for energy-conserving by refraction and\nemploy multi-marginal optimal transport to create a refracting-reflecting\nmetalens that preserves given energy distributions.",
        "Fermionic time-reversal-invariant insulators in two dimensions -- class AII\nin the Kitaev table -- come in two different topological phases. These are\ncharacterized by a $\\mathbb{Z}_2$-index: the Fu-Kane-Mele index. We prove that\nif two such insulators with different indices occupy regions containing\narbitrarily large balls, then the spectrum of the resulting operator fills the\nbulk spectral gap. Our argument follows a proof by contradiction developed in\nan earlier work by two of the authors for quantum Hall systems. It boils down\nto showing that the $\\mathbb{Z}_2$-index can be computed only from bulk\ninformation in sufficiently large balls. This is achieved via a result of\nindependent interest: a local trace formula for the $\\mathbb{Z}_2$-index.",
        "We recently described a specific type of attractors of two-dimensional\ndiscontinuous piecewise linear maps, characterized by two discontinuity lines\ndividing the phase plane into three partitions, related to economic\napplications. To our knowledge, this type of attractor, which we call a weird\nquasiperiodic attractor, has not yet been studied in detail. They have a rather\ncomplex geometric structure and other interesting properties that are worth\nunderstanding better. To this end, we consider a simpler map that can also\npossess weird quasiperiodic attractors, namely, a 2D discontinuous piecewise\nlinear map $F$ with a single discontinuity line dividing the phase plane into\ntwo partitions, where two different homogeneous linear maps are defined. Map\n$F$ depends on four parameters -- the traces and determinants of the two\nJacobian matrices. In the parameter space of map $F$, we obtain specific\nregions associated with the existence of weird quasiperiodic attractors;\ndescribe some characteristic properties of these attractors; and explain one of\nthe possible mechanisms of their appearance."
      ]
    }
  },
  {
    "id":2411.0675,
    "research_type":"applied",
    "start_id":"b21",
    "start_title":"Generative AI for Medical Imaging: extending the MONAI Framework",
    "start_abstract":"Recent advances in generative AI have brought incredible breakthroughs several areas, including medical imaging. These models tremendous potential not only to help safely share data via synthetic datasets but also perform an array of diverse applications, such as anomaly detection, image-to-image translation, denoising, and MRI reconstruction. However, due the complexity these models, their implementation reproducibility can be difficult. This hinder progress, act a use barrier, dissuade comparison new methods with existing works. In this study, we present MONAI Generative Models, freely available open-source platform that allows researchers developers easily train, evaluate, deploy related applications. Our reproduces state-of-art studies standardised way involving different architectures (such diffusion autoregressive transformers, GANs), provides pre-trained for community. We implemented generalisable fashion, illustrating results extended 2D or 3D scenarios, images modalities (like CT, MRI, X-Ray data) from anatomical areas. Finally, adopt modular extensible approach, ensuring long-term maintainability extension current applications future features.",
    "start_categories":[
      "cs.CV",
      "eess.IV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b13"
      ],
      "title":[
        "Symmetric diffeomorphic image registration with crosscorrelation: evaluating automated labeling of elderly and neurodegenerative brain"
      ],
      "abstract":[
        "One of the most challenging problems in modern neuroimaging is detailed characterization of neurodegeneration. Quantifying spatial and longitudinal atrophy patterns is an important component of this process. These spatiotemporal signals will aid in discriminating between related diseases, such as frontotemporal dementia (FTD) and Alzheimer's disease (AD), which manifest themselves in the same at-risk population. Here, we develop a novel symmetric image normalization method (SyN) for maximizing the cross-correlation within the space of diffeomorphic maps and provide the Euler-Lagrange equations necessary for this optimization. We then turn to a careful evaluation of our method. Our evaluation uses gold standard, human cortical segmentation to contrast SyN's performance with a related elastic method and with the standard ITK implementation of Thirion's Demons algorithm. The new method compares favorably with both approaches, in particular when the distance between the template brain and the target brain is large. We then report the correlation of volumes gained by algorithmic cortical labelings of FTD and control subjects with those gained by the manual rater. This comparison shows that, of the three methods tested, SyN's volume measurements are the most strongly correlated with volume measurements gained by expert labeling. This study indicates that SyN, with cross-correlation, is a reliable method for normalizing and making anatomical measurements in volumetric MRI of patients and at-risk elderly individuals."
      ],
      "categories":[
        "q-bio.NC"
      ]
    },
    "list":{
      "title":[
        "Super-resolution Live-cell Fluorescence Lifetime Imaging",
        "A New Circle Theorem for Two Dimensional Ising Spin Glasses",
        "Learning Conditional Average Treatment Effects in Regression\n  Discontinuity Designs using Bayesian Additive Regression Trees",
        "Reducing Frequency Bias of Fourier Neural Operators in 3D Seismic\n  Wavefield Simulations Through Multi-Stage Training",
        "Left Jacobson Rings",
        "Safe exploration in reproducing kernel Hilbert spaces",
        "Orthogonal Delay-Doppler Division Multiplexing Modulation with\n  Hierarchical Mode-Based Index Modulation",
        "Periodic Response Solutions to Multi-Dimensional Nonlinear Schr\\\"odinger\n  equation with unbounded perturbation",
        "DESI DR2 Results II: Measurements of Baryon Acoustic Oscillations and\n  Cosmological Constraints",
        "Spectral modelling of Cygnus A between 110 and 250 MHz. Impact on the\n  LOFAR 21-cm signal power spectrum",
        "Calculating the Hawking Temperature of Black Holes in $f(Q)$ Gravity\n  Using the RVB Method: A Residue-Based Approach",
        "Slow Light Waveguides based on Bound States in the Continuum",
        "Multivariable $p$-adic Hodge theory for products of Galois groups",
        "Convex Split Lemma without Inequalities",
        "FGM Modeling of Thermo-Diffusive Unstable Lean Premixed Hydrogen-Air\n  Flames",
        "An edge crack and a crack close to the vertex of a wedge",
        "An Arbitrary Time Interval Generator Base on Vernier Clocks with 0.67 ps\n  Adjustable Steps Implemented in FPGA",
        "Beyond the Median Voter Theorem: A New Framework for Ideological\n  Positioning",
        "Analyzing the Impact of AC False Data Injection Attacks on Power System\n  Operation",
        "Measurement of energy reduction by inertial Alfv\\'en waves propagating\n  through parallel gradients in the Alfv\\'en speed",
        "Minimax rates of convergence for the nonparametric estimation of the\n  diffusion coefficient from time-homogeneous SDE paths",
        "Gas excitation in galaxies and active galactic nuclei with He\n  II{\\lambda}4686 and X-ray emission",
        "A Practical Introduction to Kernel Discrepancies: MMD, HSIC & KSD",
        "New developments in 3D-trench electrode sensors",
        "Self-propulsion and self-rotation of an inertial chiral active\n  Ornstein-Uhlenbeck particle",
        "Deriving pulsar pair-production multiplicities from pulsar wind nebulae\n  using H.E.S.S. and LHAASO observations",
        "Double metasurfaces and Optimal transport",
        "Edge spectrum for truncated $\\mathbb{Z}_2$-insulators",
        "On the emergence and properties of weird quasiperiodic attractors"
      ],
      "abstract":[
        "Super-resolution Structured Illumination Microscopy (SR-SIM) enables\nfluorescence microscopy beyond the diffraction limit at high frame rates.\nCompared to other super-resolution microscopy techniques, the low photon\nfluence used in SR-SIM makes it readily compatible with live-cell imaging.\nHere, we combine SR-SIM with electro-optic fluorescence lifetime imaging\n(EOFLIM), adding the capability of monitoring physicochemical parameters with\n156 nm spatial resolution at high frame rate for live-cell imaging. We\ndemonstrate that our new SIMFLIM technique enables super-resolved multiplexed\nimaging of spectrally overlapping fluorophores, environmental sensing, and\nlive-cell imaging.",
        "The Lee-Yang circle theorem revolutionized our understanding of phase\ntransitions in ferromagnetic systems by showing that the complex zeros of\npartition functions lie on the unit circle, with criticality arising as these\nzeros approach the real axis in the thermodynamic limit. However, in frustrated\nsystems such as antiferromagnets and spin glasses, the zeros deviate from this\nstructure, making it challenging to extend the Lee-Yang theory to disordered\nsystems. In this work, we establish a new circle theorem for two-dimensional\nIsing spin glasses, proving that the square of the partition function exhibits\nzeros densely packed along the unit circle. Numerical simulations on the square\nlattice confirm our theoretical predictions, demonstrating the validity of the\ncircle law for quenched disorder. Furthermore, our results uncover a\nfinite-temperature crossover in $\\pm J$ spin glasses, characterized by the\nemergence of a spectral gap in the angular distribution of zeros. This result\nextends the Lee-Yang framework to disordered systems, offering new insights\ninto spin-glass criticality.",
        "BART (Bayesian additive regression trees) has been established as a leading\nsupervised learning method, particularly in the field of causal inference. This\npaper explores the use of BART models for learning conditional average\ntreatment effects (CATE) from regression discontinuity designs, where treatment\nassignment is based on whether an observed covariate (called the running\nvariable) exceeds a pre-specified threshold. A purpose-built version of BART\nthat uses linear regression leaf models (of the running variable and treatment\nassignment dummy) is shown to out-perform off-the-shelf BART implementations as\nwell as a local polynomial regression approach and a CART-based approach. The\nnew method is evaluated in thorough simulation studies as well as an empirical\napplication looking at the effect of academic probation on student performance.",
        "The recent development of Neural Operator (NeurOp) learning for solutions to\nthe elastic wave equation shows promising results and provides the basis for\nfast large-scale simulations for different seismological applications. In this\npaper, we use the Fourier Neural Operator (FNO) model to directly solve the 3D\nHelmholtz wave equation for fast seismic ground motion simulations on different\nfrequencies, and show the frequency bias of the FNO model, i.e. it learns the\nlower frequencies better comparing to the higher frequencies. To reduce the\nfrequency bias, we adopt the multi-stage FNO training, i.e., after training a\n1st stage FNO model for estimating the ground motion, we use a second FNO model\nas the 2nd stage to learn from the residual, which greatly reduced the errors\non the higher frequencies. By adopting this multi-stage training, the FNO\nmodels show reduced biases on higher frequencies, which enhanced the overall\nresults of the ground motion simulations. Thus the multi-stage training FNO\nimproves the accuracy and realism of the ground motion simulations.",
        "We say that a ring is strongly (resp. weakly) left Jacobson if every\nsemiprime (resp. prime) left ideal is an intersection of maximal left ideals.\nThere exist Jacobson rings that are not weakly left Jacobson, e.g. the Weyl\nalgebra. Our main result is the following one-sided noncommutative\nNullstellensatz: For any finite-dimensional F-algebra A the ring\nA[$x_1$,...,$x_n$] of polynomials with coefficients in A is strongly left\nJacobson and every maximal left ideal of A[$x_1$,...,$x_n$] has a special form.\nWe also prove that an algebra that is a finitely generated module over its\ncenter is weakly left Jacobson iff it is Jacobson, and that an Azumaya algebra\nis strongly left Jacobson iff its center is Jacobson.",
        "Popular safe Bayesian optimization (BO) algorithms learn control policies for\nsafety-critical systems in unknown environments. However, most algorithms make\na smoothness assumption, which is encoded by a known bounded norm in a\nreproducing kernel Hilbert space (RKHS). The RKHS is a potentially\ninfinite-dimensional space, and it remains unclear how to reliably obtain the\nRKHS norm of an unknown function. In this work, we propose a safe BO algorithm\ncapable of estimating the RKHS norm from data. We provide statistical\nguarantees on the RKHS norm estimation, integrate the estimated RKHS norm into\nexisting confidence intervals and show that we retain theoretical guarantees,\nand prove safety of the resulting safe BO algorithm. We apply our algorithm to\nsafely optimize reinforcement learning policies on physics simulators and on a\nreal inverted pendulum, demonstrating improved performance, safety, and\nscalability compared to the state-of-the-art.",
        "The orthogonal time frequency space with index modulation (OTFS-IM) offers\nflexible tradeoffs between spectral efficiency (SE) and bit error rate (BER) in\ndoubly selective fading channels. While OTFS-IM schemes demonstrated such\npotential, a persistent challenge lies in the detection complexity. To address\nthis problem, we propose the hierarchical mode-based index modulation (HMIM).\nHMIM introduces a novel approach to modulate information bits by IM patterns,\nsignificantly simplifying the complexity of maximum a posteriori (MAP)\nestimation with Gaussian noise. Further, we incorporate HMIM with the recently\nproposed orthogonal delay-Doppler division multiplexing (ODDM) modulation,\nnamely ODDM-HMIM, to exploit the full diversity of the delay-Doppler (DD)\nchannel. The BER performance of ODDM-HMIM is analyzed considering a maximum\nlikelihood (ML) detector. Our numerical results reveal that, with the same SE,\nHMIM can outperform conventional IM in terms of both BER and computational\ncomplexity. In addition, we propose a successive interference\ncancellation-based minimum mean square error (SIC-MMSE) detector for ODDM-HMIM,\nwhich enables low-complexity detection with large frame sizes.",
        "By applying the Craig-Wayne-Bourgain (CWB) method, we establish the existence\nof periodic response solutions to multi-dimensional nonlinear Schr\\\"{o}dinger\nequations (NLS) with unbounded perturbation.",
        "We present baryon acoustic oscillation (BAO) measurements from more than 14\nmillion galaxies and quasars drawn from the Dark Energy Spectroscopic\nInstrument (DESI) Data Release 2 (DR2), based on three years of operation. For\ncosmology inference, these galaxy measurements are combined with DESI\nLyman-$\\alpha$ forest BAO results presented in a companion paper. The DR2 BAO\nresults are consistent with DESI DR1 and SDSS, and their distance-redshift\nrelationship matches those from recent compilations of supernovae (SNe) over\nthe same redshift range. The results are well described by a flat $\\Lambda$CDM\nmodel, but the parameters preferred by BAO are in mild, $2.3\\sigma$ tension\nwith those determined from the cosmic microwave background (CMB), although the\nDESI results are consistent with the acoustic angular scale $\\theta_*$ that is\nwell-measured by Planck. This tension is alleviated by dark energy with a\ntime-evolving equation of state parametrized by $w_0$ and $w_a$, which provides\na better fit to the data, with a favored solution in the quadrant with $w_0>-1$\nand $w_a<0$. This solution is preferred over $\\Lambda$CDM at $3.1\\sigma$ for\nthe combination of DESI BAO and CMB data. When also including SNe, the\npreference for a dynamical dark energy model over $\\Lambda$CDM ranges from\n$2.8-4.2\\sigma$ depending on which SNe sample is used. We present evidence from\nother data combinations which also favor the same behavior at high\nsignificance. From the combination of DESI and CMB we derive 95% upper limits\non the sum of neutrino masses, finding $\\sum m_\\nu<0.064$ eV assuming\n$\\Lambda$CDM and $\\sum m_\\nu<0.16$ eV in the $w_0w_a$ model. Unless there is an\nunknown systematic error associated with one or more datasets, it is clear that\n$\\Lambda$CDM is being challenged by the combination of DESI BAO with other\nmeasurements and that dynamical dark energy offers a possible solution.",
        "Studying the redshifted 21-cm signal from the the neutral hydrogen during the\nEpoch of Reionization and Cosmic Dawn is fundamental for understanding the\nphysics of the early universe. One of the challenges that 21-cm experiments\nface is the contamination by bright foreground sources, such as Cygnus A, for\nwhich accurate spatial and spectral models are needed to minimise the residual\ncontamination after their removal. In this work, we develop a new,\nhigh-resolution model of Cygnus A using Low Frequency Array (LOFAR)\nobservations in the $110{-}250$ MHz range, improving upon previous models by\nincorporating physical spectral information through the forced-spectrum method\nduring multi-frequency deconvolution. This approach addresses the limitations\nof earlier models by providing a more accurate representation of the complex\nstructure and spectral behaviour of Cygnus A, including the spectral turnover\nin its brightest hotspots. The impact of this new model on the LOFAR 21-cm\nsignal power spectrum is assessed by comparing it with both simulated and\nobserved North Celestial Pole data sets. Significant improvements are observed\nin the cylindrical power spectrum along the Cygnus A direction, highlighting\nthe importance of having spectrally accurate models of the brightest foreground\nsources. However, this improvement is washed out in the spherical power\nspectrum, where we measure differences of a few hundred mK at\n$k<0.63\\,h\\,\\text{cMpc}^{-1}$, but not statistically significant. The results\nsuggest that other systematic effects must be mitigated before a substantial\nimpact on 21-cm power spectrum can be achieved.",
        "This paper investigates the computation of Hawking temperatures for black\nholes within various $f(Q)$ gravity models using the RVB method. This\ntopological approach uncovers an additional term in the temperature\ncalculation, which we propose originates from the residue of a specific contour\nintegral related to the metric or curvature. By examining several specific\n$f(Q)$ models including quadratic, logarithmic, square root, and power law\nmodifications as well as well known black hole solutions such as RN, Kerr, and\nKN black holes, we demonstrate that the correction term can consistently be\ninterpreted as this residue. Our findings provide new insights into black hole\nthermodynamics within modified gravity frameworks.",
        "The concept of bound states in the continuum (BIC) has been advancing light\nconfinement technology in leaky environments. In this letter, we propose and\nnumerically demonstrate a slow light waveguide based on a BIC mode. We\nconsidered a waveguide with a polymer core loaded on a plane slab, which\nsupports a leaky guided mode coupled to the radiation continuum in the slab. We\nfound that periodic modulation of the polymer core along the propagation\ndirection can result in a high group index mode with a low propagation loss due\nto BIC confinement. The introduction of one-dimensional photonic crystals into\nthe BIC waveguides will largely expand its functionality and applications in\nintegrated photonics.",
        "In this paper we explain how to attach to a family of $p$-adic\nrepresentations of a product of Galois groups an overconvergent family of\nmultivariable $(\\varphi,\\Gamma)$-modules, generalizing results from Pal-Zabradi\nand Carter-Kedlaya-Zabradi, using Colmez-Sen-Tate descent. We also define rings\nof multivariable crystalline and semistable periods, and explain how to recover\nthis multivariable $p$-adic theory attached to a family of representations from\nits multivariable $(\\varphi,\\Gamma)$-module. We also explain how our framework\nallows us to recover the main results of Brinon-Chiarellotto-Mazzari on\nmultivariable $p$-adic Galois representations.",
        "We introduce a refinement to the convex split lemma by replacing the max\nmutual information with the collision mutual information, transforming the\ninequality into an equality. This refinement yields tighter achievability\nbounds for quantum source coding tasks, including state merging and state\nsplitting. Furthermore, we derive a universal upper bound on the smoothed max\nmutual information, where \"universal\" signifies that the bound depends\nexclusively on R\\'enyi entropies and is independent of the system's dimensions.\nThis result has significant implications for quantum information processing,\nparticularly in applications such as the reverse quantum Shannon theorem.",
        "Ultra-lean premixed hydrogen combustion is a possible solution to decarbonize\nindustry, while limiting flame temperatures and thus nitrous oxide emissions.\nThese lean hydrogen\/air flames experience strong preferential diffusion\neffects, which result in thermo-diffusive (TD) instabilities. To efficiently\nand accurately model lean premixed hydrogen flames, it is crucial to\nincorporate these preferential diffusion effects into flamelet tabulated\nchemistry frameworks, such as the Flamelet-Generated Manifold (FGM) method.\nThis is challenging because the preferential diffusion terms in the control\nvariable transport equations contain diffusion fluxes of all species in the\nmechanism. In this work, a new implementation is presented; the full term is\nreduced by only considering the most contributing species. When carefully\nselecting this set of major species, preferential diffusion fluxes along the\nflame front, i.e., cross-diffusion, can be captured. This is particularly\nimportant for manifolds that include heat loss effects, where enthalpy is one\nof the control variables. The diffusion of the H-radical has a significant\ncontribution to the enthalpy transport equation, and cross-diffusion of the\nH-radical is non-negligible. Two manifolds, without and with heat loss effects,\nand the set of major species are analyzed in an a-priori and a-posteriori\nmanner. Simulations of TD unstable hydrogen-air flames with detailed chemistry\nand several FGM models show that accurately capturing cross-diffusion of\nenthalpy is important for correctly predicting the flame shape and dynamics.",
        "Two model problems of an elastic wedge with an internal and edge crack are\nanalyzed. The problem of an internal crack reduces to an order-4 vector\nRiemann-Hilbert problem whose matrix kernel entries are meromorphic functions\nand have exponential factors. When the internal crack is located along one of\nthe wedge sides, an efficient method of solution is proposed. It requires a\nfactorization of the order-2 matrix coefficient associated with the\ncorresponding problem of an edge crack and the solution of an infinite system\nof linear algebraic system with an exponential rate of convergence of an\napproximate solution to the exact one. The order-2 Khrapkov's factorization is\nmodified by splitting the matrix kernel into a scalar dominant function and a\n``regular\" matrix whose factorization is more convenient for numerical\npurposes. Expressions for the stress intensity coefficients and the potential\nenergy released when the crack advances are derived. Asymptotic relations for\nthe stress intensity coefficients and the potential energy when one of the\ncrack tips is close the wedge vertex are obtained.",
        "In TDC testing or timing system implementation tasks, it is often desirable\nto generate signal pulses with fine adjustable time intervals. In delay\ncell-based schemes, the time adjustment steps are limited by the propagation\ndelays of the cells, which are typically 15 to 20 picoseconds per step and are\nsensitive to temperature and operating voltage. In this document, a purely\ndigital scheme based on two vernier clocks with small frequency difference\ngenerated using cascaded PLL is reported. The scheme is tested in two families\nof low-cost FPGA and 0.67 and 0.97 picoseconds adjustable steps of the time\nintervals are achieved.",
        "This paper revisits the limitations of the Median Voter Theorem and\nintroduces a novel framework to analyze the optimal economic ideological\npositions of political parties. By incorporating Nash equilibrium, we examine\nthe mechanisms and elasticity of ideal deviation costs, voter distribution, and\npolicy feasibility. Our findings show that an increase in a party's ideal\ndeviation cost shifts its optimal ideological position closer to its ideal\npoint. Additionally, if a voter distribution can be expressed as a positive\nlinear combination of two other distributions, its equilibrium point must lie\nwithin the interval defined by the equilibrium points of the latter two. We\nalso find that decreasing feasibility costs incentivize governments, regardless\nof political orientation, to increase fiscal expenditures (e.g., welfare) and\nreduce fiscal revenues (e.g., taxes). This dynamic highlights the fiscal\npressures commonly faced by democratic nations under globalization. Moreover,\nwe demonstrate that even with uncertain voter distributions, parties can\nidentify optimal ideological positions to maximize their utility. Lastly, we\nexplain why the proposed framework cannot be applied to community ideologies\ndue to their fundamentally different nature. This study provides new\ntheoretical insights into political strategies and establishes a foundation for\nfuture empirical research.",
        "False Data Injection (FDI) attacks are a significant threat to modern power\nsystems. Although numerous research studies have focused on FDI attacks on\npower systems, these studies have primarily concentrated on designing or\ndetecting DC FDI attacks, with less attention given to the impact analysis of\nAC FDI attacks. AC FDI attacks are potentially more harmful as they can easily\nbypass bad data detection (BDD) algorithms. In this paper, we present a unified\napproach to investigate the impact of AC FDI attacks on power transmission\nlines using the PowerWorld simulator. We also investigate the impact of\ndifferent FDI attack designs, including those optimally designed to evade BDD\nalgorithms and compare them accordingly. Our findings demonstrate that in\ndesigning optimal AC FDI attacks, a trade-off between the residuals of state\nvariables and the corresponding impacts of the proposed attack should be\nconsidered. This is because optimal attacks result in fewer changes in the\nattacked variable states and their estimated residuals compared to arbitrary AC\nFDI attacks. Moreover, the impacts of optimal AC FDI attacks can be less severe\nthan those of arbitrary attacks. We implement and analyze the proposed approach\non the IEEE 39-bus test system using PowerWorld simulator.",
        "We have studied the propagation of inertial Alfv\\'en waves through parallel\ngradients in the Alfv\\'en speed using the Large Plasma Device at the University\nof California, Los Angeles. The reflection and transmission of Alfv\\'en waves\nthrough inhomogeneities in the background plasma is important for understanding\nwave propagation, turbulence, and heating in space, laboratory, and\nastrophysical plasmas. Here we \\rev{present inertial Alfv\\'en waves, under\nconditions relevant to solar flares and the solar corona. We find} that the\ntransmission of the inertial Alfv\\'en waves is reduced as the sharpness of the\ngradient is increased. Any reflected waves were below the detection limit of\nour experiment and reflection cannot account for all of the energy not\ntransmitted through the gradient. Our findings indicate that, for both kinetic\nand inertial Alfv\\'en waves, the controlling parameter for the transmission of\nthe waves through an Alfv\\'en speed gradient is the ratio of the Alfv\\'en\nwavelength along the gradient divided by the scale length of the gradient.\nFurthermore, our results suggest that an as-yet-unidentified damping process\noccurs in the gradient.",
        "Consider a diffusion process X, solution of a time-homogeneous stochastic\ndifferential equation. We assume that the diffusion process X is observed at\ndiscrete times, at high frequency, which means that the time step tends toward\nzero. In addition, the drift and diffusion coefficients of the process X are\nassumed to be unknown. In this paper, we study the minimax rates of convergence\nof the nonparametric estimators of the square of the diffusion coefficient. Two\nobservation schemes are considered depending on the estimation interval. The\nsquare of the diffusion coefficient is estimated on the real line from repeated\nobservations of the process X, where the number of diffusion paths tends to\ninfinity. For the case of a compact estimation interval, we study the\nnonparametric estimation of the square of the diffusion coefficient constructed\nfrom a single diffusion path on one side and from repeated observations on the\nother side, where the number of trajectories tends to infinity. In each of\nthese cases, we establish minimax convergence rates of the risk of estimation\nof the diffusion coefficient over a space of Holder functions.",
        "The origin of He II emission in galaxies remains a debated topic, requiring\nionizing photons with energies exceeding 54 eV. While massive stars, such as\nWolf-Rayet stars, have been considered potential sources, their UV flux often\nfails to fully explain the observed He II emission. Recent studies suggest that\nX-ray binaries (XRBs) might contribute significantly to this ionization. We\nexplore the relationship between X-ray and $\\rm He~II \\lambda4686$ emission in\na statistically significant sample of galaxies, investigating whether X-ray\nsources, including active galactic nuclei (AGNs) and XRBs, serve as the primary\nmechanism for He II ionization across different galactic environments. We\ncross-matched a sample of known well-detected He II galaxies with the Chandra\nSource Catalog, yielding 165 galaxies with X-ray and $\\rm He~II \\lambda4686$\ndetections. The sources were classified into star-forming galaxies (SFGs) and\nAGNs based on the BPT diagram and a classification scheme defined for He II\ngalaxies. We find a strong, linear correlation between X-ray and He II\nluminosity across AGNs and SFGs spanning over seven orders of magnitude. AGNs\ngenerally exhibit higher He II\/H$\\beta$ flux ratios, stronger extinction, and\nharder X-ray spectra. The O32 ratio of SFGs is tightly correlated with the\nH$\\beta$ equivalent width ($\\rm EW_{H\\beta}$) but not with the He II\/H$\\beta$\nratio, suggesting a different excitation mechanism. We derive an O32--$\\rm\nEW_{H\\beta}$ line above which only AGNs of our sample reside. The tight\ncorrelation between X-ray and He II luminosity supports X-rays as the primary\ndriver of He II excitation. While AGNs have one common ionization source, the\ncentral black hole, in SFGs low-energy species are mainly excited by UV\nemission related to star-forming activity, however, high-energy species like He\nII require the presence of XRBs.",
        "This article provides a practical introduction to kernel discrepancies,\nfocusing on the Maximum Mean Discrepancy (MMD), the Hilbert-Schmidt\nIndependence Criterion (HSIC), and the Kernel Stein Discrepancy (KSD). Various\nestimators for these discrepancies are presented, including the commonly-used\nV-statistics and U-statistics, as well as several forms of the more\ncomputationally-efficient incomplete U-statistics. The importance of the choice\nof kernel bandwidth is stressed, showing how it affects the behaviour of the\ndiscrepancy estimation. Adaptive estimators are introduced, which combine\nmultiple estimators with various kernels, addressing the problem of kernel\nselection.",
        "Future high-luminosity hadron collider experiments feature unprecedented\nlevels of event pile-up and extreme radiation environments, calling for sensors\ncapable of 4D tracking, even after significant radiation damage. To this\npurpose, 3D sensors represent a viable solution, since they provide excellent\nradiation tolerance and very good temporal resolution. In particular, owing to\nthe uniform electric field and weighting field distributions, 3D-trench\nelectrode sensors from the INFN TIMESPOT project have shown a temporal\nresolution of $\\sim$10 ps after irradiation fluences up to 1$\\times$10$^{17}$\n1-Mev n$_{eq}$\/cm$^2$. In spite of the excellent performance of these sensors,\n3D-trench pixel technology is not yet fully established and the fabrication\nyield is not yet adequate for the production of large size pixel sensors. To\nimprove the potential of the 3D-trench concept for large-area sensors, a new\nbatch of sensors was designed at the University of Trento and fabricated at\nFBK, as part of the AIDA Innova project. Besides introducing some process\nimprovements, this batch includes two different sensor variants: the standard\none with continuous ohmic trenches, and a modified one with dashed ohmic\ntrenches. On-wafer electrical test results show that most of the sensors have\nlow leakage current and high breakdown voltage. Moreover, the fabrication yield\nfor the new design variant is higher than that of the standard design.",
        "We investigate the transport feature of an inertial chiral active\nOrnstein-Uhlenbeck particle moving on a two-dimensional surface. Using both\nanalytical approach and numerical simulations, we have exactly explored the\ntransient and steady-state behavior of the particle by analyzing the simulated\nparticle trajectories, probability distribution functions for position and\nvelocity, mean square displacement, mean square velocity, and effective kinetic\ntemperature of the medium. From the mean square displacement calculations, we\nobserve that, unlike an inertial active Brownian particle, a chiral active\nparticle manifests an initial ballistic, intermediate sub-diffusive to\nnon-diffusive, and the conventional long-time diffusive behavior. The\nintermediate sub-diffusive to non-diffusive behavior is prominent for the\nself-propulsion of an overdamped particle. It can be understood by\nchirality-induced transient confinement, which persists for short time\nintervals and diffuses away in the time asymptotic limit or at the steady\nstate. This behavior is further complemented by the exact calculation of mean\nsquare velocity or effective kinetic temperature of the medium, which is a\ndecreasing function of the magnitude of chirality. Moreover, the steady-state\nMSD and MSV are found to have a dependence both on chirality and activity time\nscale and hence can be controlled by tuning the persistent duration of activity\nor strength of the chirality of the particle.",
        "Pulsar wind nebulae (PWNe) dominate the galactic gamma-ray sky at very high\nenergies and they are major contributors to the leptonic cosmic ray flux.\nHowever, the question of whether or not pulsars also accelerate ions to\ncomparable energies has not yet been experimentally confirmed. We aim to\nconstrain the birth period and pair-production multiplicity for a set of\npulsars. In doing so, we aim to constrain the proportion of ions in the pulsar\nmagnetosphere and, hence, the proportion of ions that could enter the pulsar\nwind. We estimated possible ranges of the value of the average pair production\nmultiplicity for a sample of 26 pulsars in the Australia Telescope National\nFacility (ATNF) catalogue, which have also been observed by the High Energy\nStereoscopic System (H.E.S.S.) telescopes. We then derived lower limits for the\npulsar birth periods and average pair production multiplicities for a subset of\nthese sources where the extent of the pulsar wind nebula and surrounding\nsupernova shell have been measured in the radio. We also derived curves for the\naverage pair production multiplicities as a function of birth period for\nsources recently observed by the Large High Altitude Air Shower Observatory\n(LHAASO). We show that there is a potential for hadrons entering the pulsar\nwind for most of the H.E.S.S. and LHAASO sources we consider here, which is\ndependent upon the efficiency of luminosity conversion into particles. We also\npresent estimates of the pulsar birth period for six of these sources, all\nfalling into the range of $\\sim$10-50 ms.",
        "This paper constructs metalenses that separate homogeneous media with\ndifferent refractive indices, refracting one domain into another while\nconserving a prescribed energy distribution. Using optimal transport theory, we\ndesign singlet and doublet metalenses for energy-conserving by refraction and\nemploy multi-marginal optimal transport to create a refracting-reflecting\nmetalens that preserves given energy distributions.",
        "Fermionic time-reversal-invariant insulators in two dimensions -- class AII\nin the Kitaev table -- come in two different topological phases. These are\ncharacterized by a $\\mathbb{Z}_2$-index: the Fu-Kane-Mele index. We prove that\nif two such insulators with different indices occupy regions containing\narbitrarily large balls, then the spectrum of the resulting operator fills the\nbulk spectral gap. Our argument follows a proof by contradiction developed in\nan earlier work by two of the authors for quantum Hall systems. It boils down\nto showing that the $\\mathbb{Z}_2$-index can be computed only from bulk\ninformation in sufficiently large balls. This is achieved via a result of\nindependent interest: a local trace formula for the $\\mathbb{Z}_2$-index.",
        "We recently described a specific type of attractors of two-dimensional\ndiscontinuous piecewise linear maps, characterized by two discontinuity lines\ndividing the phase plane into three partitions, related to economic\napplications. To our knowledge, this type of attractor, which we call a weird\nquasiperiodic attractor, has not yet been studied in detail. They have a rather\ncomplex geometric structure and other interesting properties that are worth\nunderstanding better. To this end, we consider a simpler map that can also\npossess weird quasiperiodic attractors, namely, a 2D discontinuous piecewise\nlinear map $F$ with a single discontinuity line dividing the phase plane into\ntwo partitions, where two different homogeneous linear maps are defined. Map\n$F$ depends on four parameters -- the traces and determinants of the two\nJacobian matrices. In the parameter space of map $F$, we obtain specific\nregions associated with the existence of weird quasiperiodic attractors;\ndescribe some characteristic properties of these attractors; and explain one of\nthe possible mechanisms of their appearance."
      ]
    }
  },
  {
    "id":2411.0675,
    "research_type":"applied",
    "start_id":"b13",
    "start_title":"Symmetric diffeomorphic image registration with crosscorrelation: evaluating automated labeling of elderly and neurodegenerative brain",
    "start_abstract":"One of the most challenging problems in modern neuroimaging is detailed characterization of neurodegeneration. Quantifying spatial and longitudinal atrophy patterns is an important component of this process. These spatiotemporal signals will aid in discriminating between related diseases, such as frontotemporal dementia (FTD) and Alzheimer's disease (AD), which manifest themselves in the same at-risk population. Here, we develop a novel symmetric image normalization method (SyN) for maximizing the cross-correlation within the space of diffeomorphic maps and provide the Euler-Lagrange equations necessary for this optimization. We then turn to a careful evaluation of our method. Our evaluation uses gold standard, human cortical segmentation to contrast SyN's performance with a related elastic method and with the standard ITK implementation of Thirion's Demons algorithm. The new method compares favorably with both approaches, in particular when the distance between the template brain and the target brain is large. We then report the correlation of volumes gained by algorithmic cortical labelings of FTD and control subjects with those gained by the manual rater. This comparison shows that, of the three methods tested, SyN's volume measurements are the most strongly correlated with volume measurements gained by expert labeling. This study indicates that SyN, with cross-correlation, is a reliable method for normalizing and making anatomical measurements in volumetric MRI of patients and at-risk elderly individuals.",
    "start_categories":[
      "q-bio.NC"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b3",
        "b21"
      ],
      "title":[
        "3-D ultrasound imaging: a review",
        "Generative AI for Medical Imaging: extending the MONAI Framework"
      ],
      "abstract":[
        "The development of 3-D ultrasound imaging is a way to address the disadvantages conventional imaging. In this article authors review approaches that have been attempted in such as B-mode, color Doppler, and power Doppler systems. Acquisition, reconstruction, rendering techniques for are discussed, well applications limitations.",
        "Recent advances in generative AI have brought incredible breakthroughs several areas, including medical imaging. These models tremendous potential not only to help safely share data via synthetic datasets but also perform an array of diverse applications, such as anomaly detection, image-to-image translation, denoising, and MRI reconstruction. However, due the complexity these models, their implementation reproducibility can be difficult. This hinder progress, act a use barrier, dissuade comparison new methods with existing works. In this study, we present MONAI Generative Models, freely available open-source platform that allows researchers developers easily train, evaluate, deploy related applications. Our reproduces state-of-art studies standardised way involving different architectures (such diffusion autoregressive transformers, GANs), provides pre-trained for community. We implemented generalisable fashion, illustrating results extended 2D or 3D scenarios, images modalities (like CT, MRI, X-Ray data) from anatomical areas. Finally, adopt modular extensible approach, ensuring long-term maintainability extension current applications future features."
      ],
      "categories":[
        "cs.CV",
        "eess.IV"
      ]
    },
    "list":{
      "title":[
        "General mean-field stochastic linear quadratic control problem driven by\n  L\\'evy processes with random coefficients",
        "Slowly decaying strain solitons in nonlinear viscoelastic waveguides",
        "AI-Invented Tonal Languages: Preventing a Machine Lingua Franca Beyond\n  Human Understanding",
        "Bounded Synthesis of Synchronized Distributed Models from Lightweight\n  Specifications",
        "EEG-CLIP : Learning EEG representations from natural language\n  descriptions",
        "Please, do tell",
        "Hot-carrier thermal breakdown and S-type current-voltage characteristics\n  in perforated graphene structures",
        "Goal-oriented Transmission Scheduling: Structure-guided DRL with a\n  Unified Dual On-policy and Off-policy Approach",
        "Optically Detected Magnetic Resonance Imaging and Sensing Within\n  Functionalized Additively Manufactured Microporous Structures",
        "Craw4LLM: Efficient Web Crawling for LLM Pretraining",
        "High-Accuracy ECG Image Interpretation using Parameter-Efficient LoRA\n  Fine-Tuning with Multimodal LLaMA 3.2",
        "Modeling of Rumor Propagation in Large Populations with Network via\n  Graphon Games",
        "On the Chermak-Delgado lattice of a finite group",
        "OfficeMate: Pilot Evaluation of an Office Assistant Robot",
        "MetaOpenFOAM 2.0: Large Language Model Driven Chain of Thought for\n  Automating CFD Simulation and Post-Processing",
        "New Plasma Sheath Potential Solutions in Cylindrical and Spherical\n  Coordinates",
        "Large language models in finance : what is financial sentiment?",
        "Conditional Diffusion Model with OOD Mitigation as High-Dimensional\n  Offline Resource Allocation Planner in Clustered Ad Hoc Networks",
        "Effect of Pt bottom electrode texture selection on the tetragonality and\n  physical properties of Ba0.8Sr0.2TiO3 thin films produced by pulsed laser\n  deposition",
        "Measuring Diversity in Synthetic Datasets",
        "Noise Reversal by Entropy Quantum Computing",
        "The Geostrategy of Youth Player Recruitment in Portuguese Clubs",
        "Unlocking Transparent Alignment Through Enhanced Inverse Constitutional\n  AI for Principle Extraction",
        "Data-augmented Learning of Geodesic Distances in Irregular Domains\n  through Soner Boundary Conditions",
        "To investigate event-by-event fluctuations of mean transverse momentum\n  in proton-proton collisions at $\\sqrt{s}$ = 13 TeV with PYTHIA8 and HERWIG7\n  models",
        "Monte Carlo model of distilled remote entanglement between\n  superconducting qubits across optical channels",
        "An Ensemble Information Filter: Retrieving Markov-information from the\n  SPDE discretisation",
        "Warnings based on risk matrices: a coherent framework with consistent\n  evaluation"
      ],
      "abstract":[
        "This paper studies a stochastic mean-field linear-quadratic optimal control\nproblem with random coefficients. The state equation is a general linear\nstochastic differential equation with mean-field terms $\\EE X(t)$ and $\\EE\nu(t)$ of the state and the control processes and is driven by a Brownian motion\nand a Poisson random measure. By the coupled system of Riccati equations, an\nexplicit expressions for the optimal state feedback control is obtained. As a\nby-product, the non-homogeneous stochastic linear-quadratic control problem\nwith random coefficients and L\\'evy driving noises is also studied.",
        "This paper is devoted to the modeling of longitudinal strain waves in a rod\ncomposed of a nonlinear viscoelastic material characterized by\nfrequency-dependent second- and third-order elastic constants. We demonstrate\nthat long waves in such a material can be effectively described by a damped\nBoussinesq-type equation for the longitudinal strain, incorporating dissipation\nthrough retarded operators. Using the existing theory of solitary wave\nsolutions in nearly integrable systems, we derive a slowly-decaying strain\nsoliton solution to this equation. The derived soliton characteristics are\nshown to be in a good agreement with results from full 3D simulations. We\ndemonstrate the importance of taking into account the frequency dependence of\nthird-order elastic constants for the description of strain solitons.",
        "This paper investigates the potential for large language models (LLMs) to\ndevelop private tonal languages for machine-to-machine (M2M) communication.\nInspired by cryptophasia in human twins (affecting up to 50% of twin births)\nand natural tonal languages like Mandarin and Vietnamese, we implement a\nprecise character-to-frequency mapping system that encodes the full ASCII\ncharacter set (32-126) using musical semitones. Each character is assigned a\nunique frequency, creating a logarithmic progression beginning with space (220\nHz) and ending with tilde (50,175.42 Hz). This spans approximately 7.9 octaves,\nwith higher characters deliberately mapped to ultrasonic frequencies beyond\nhuman perception (>20 kHz). Our implemented software prototype demonstrates\nthis encoding through visualization, auditory playback, and ABC musical\nnotation, allowing for analysis of information density and transmission speed.\nTesting reveals that tonal encoding can achieve information rates exceeding\nhuman speech while operating partially outside human perceptual boundaries.\nThis work responds directly to concerns about AI systems catastrophically\ndeveloping private languages within the next five years, providing a concrete\nprototype software example of how such communication might function and the\ntechnical foundation required for its emergence, detection, and governance.",
        "We present an approach to automatically synthesize synchronized models from\nlightweight formal specifications. Our approach takes as input a specification\nof a distributed system along with a global linear time constraint, which must\nbe fulfilled by the interaction of the system's components. It produces\nexecutable models for the component specifications (in the style of Promela\nlanguage) whose concurrent execution satisfies the global constraint. The\ncomponent specifications consist of a collection of actions described by means\nof pre and post conditions together with first-order relational formulas\nprescribing their behavior. We use the Alloy Analyzer to encode the component\nspecifications and enumerate their potential implementations up to some bound,\nwhose concurrent composition is model checked against the global property. Even\nthough this approach is sound and complete up to the selected bound, it is\nimpractical as the number of candidate implementations grows exponentially. To\naddress this, we propose an algorithm that uses batches of counterexamples to\nprune the solution space, it has two main phases: exploration, the algorithm\ncollects a batch of counterexamples, and exploitation, where this knowledge is\nused to speed up the search. The approach is sound, while its completeness\ndepends on the batches used. We present a prototype tool, describe some\nexperiments, and compare it with related approaches.",
        "Deep networks for electroencephalogram (EEG) decoding are currently often\ntrained to only solve a specific task like pathology or gender decoding. A more\ngeneral approach leveraging the medical reports of clinical EEG recordings is\nto learn mappings between medical reports and EEG recordings. This approach was\npioneered in the computer vision domain matching images and their text captions\nand subsequently allowed to do successful zero-shot decoding using textual\nclass prompts. In this work, we follow this approach and develop a contrastive\nlearning framework EEG-CLIP that aligns EEG time series and their corresponding\nclinical text descriptions in a shared embedding space. We investigate its\npotential for versatile EEG decoding, assessing performance on a range of\nfew-shot and zero-shot settings. Overall, results show that EEG-CLIP manages to\nnontrivially align text and EEG representations. Our work presents a promising\napproach to learn general EEG representations, which could enable easier\nanalyses of diverse decoding questions through zero shot decoding or training\ntask-specific models from fewer training examples. The code for reproducing our\nresults is available at https:\/\/github.com\/tidiane-camaret\/EEGClip.",
        "\"Math is not a spectator sport.\" \"Lecturing is educational malpractice.\"\nSlogans like these rally some mathematicians to teach classes that feature\n\"active learning\", where lecturing is eschewed for student participation. Yet\nas much as I believe that students must do math to learn math, I also find\nblanket statements to be more about bandwagons than considered reflection on\nteaching. In this column, published in the Fall 2021 AWM Newsletter, I urge us\nto think through the math we offer students and how we set up students to\nlearn. Although I draw primarily from my experiences teaching proofs in\nabstract algebra and real analysis, the scenarios extend to other topics in\nfirst year undergraduate education and beyond.",
        "We investigate the carrier transport characteristics of perforated graphene\nlayer (PGL) composed of arrays of interdigital coplanar graphene microribbons\n(GMRs) connected by graphene nanoribbon (GNR) bridges. We analyze their\noperation at room-temperature. Under an applied bias voltage, two-dimensional\nelectron and hole systems (2DES and 2DHS) form in adjacent GMRs. The terminal\ncurrent in these PGL structures is primarily governed by thermionic transport\nacross the GNR bridges. As electrons and holes traverse the GNRs, they induce\nheating in the 2DES and 2DHS, creating a positive feedback loop between carrier\nheating and thermionic emission. This phenomenon, characterized as hot-carrier\nthermal breakdown, can give rise to S-shaped inter-GMR current-voltage\ncharacteristics. These unique transport properties make PGLs promising\ncandidates for fast, voltage-controlled room-temperature switches and\nelectromagnetic radiation detectors.",
        "Goal-oriented communications prioritize application-driven objectives over\ndata accuracy, enabling intelligent next-generation wireless systems. Efficient\nscheduling in multi-device, multi-channel systems poses significant challenges\ndue to high-dimensional state and action spaces. We address these challenges by\nderiving key structural properties of the optimal solution to the goal-oriented\nscheduling problem, incorporating Age of Information (AoI) and channel states.\nSpecifically, we establish the monotonicity of the optimal state value function\n(a measure of long-term system performance) w.r.t. channel states and prove its\nasymptotic convexity w.r.t. AoI states. Additionally, we derive the\nmonotonicity of the optimal policy w.r.t. channel states, advancing the\ntheoretical framework for optimal scheduling. Leveraging these insights, we\npropose the structure-guided unified dual on-off policy DRL (SUDO-DRL), a\nhybrid algorithm that combines the stability of on-policy training with the\nsample efficiency of off-policy methods. Through a novel structural property\nevaluation framework, SUDO-DRL enables effective and scalable training,\naddressing the complexities of large-scale systems. Numerical results show\nSUDO-DRL improves system performance by up to 45% and reduces convergence time\nby 40% compared to state-of-the-art methods. It also effectively handles\nscheduling in much larger systems, where off-policy DRL fails and on-policy\nbenchmarks exhibit significant performance loss, demonstrating its scalability\nand efficacy in goal-oriented communications.",
        "Quantum sensing with nitrogen-vacancy centers in diamond has emerged as a\npowerful tool for measuring diverse physical parameters, yet the versatility of\nthese measurement approaches is often limited by the achievable layout and\ndimensionality of bulk-crystal platforms. Here, we demonstrate a versatile\napproach to creating designer quantum sensors by surface-functionalizing\nmultiphoton lithography microstructures with NV-containing nanodiamonds. We\nshowcase this capability by fabricating a 150 $\\mu$m x 150 $\\mu$m x 150 $\\mu$m\ntriply periodic minimal surface gyroid structure with millions of attached\nnanodiamonds. We demonstrate a means to volumetrically image these structures\nusing a refractive index matching confocal imaging technique, and extract ODMR\nspectra from 1.86 $\\mu$m x 1.86 $\\mu$m areas of highly concentrated\nnanodiamonds across a cross section of the gyroid. Furthermore, the high\ndensity of sensing elements enables ensemble temperature measurements with\nsensitivity of 0.548 {\\deg}K\/$\\sqrt{Hz}$ at 5 mW excitation power. This\napproach to creating quantum-enabled microarchitectures opens new possibilities\nfor multimodal sensing in complex three-dimensional environments.",
        "Web crawl is a main source of large language models' (LLMs) pretraining data,\nbut the majority of crawled web pages are discarded in pretraining due to low\ndata quality. This paper presents Craw4LLM, an efficient web crawling method\nthat explores the web graph based on the preference of LLM pretraining.\nSpecifically, it leverages the influence of a webpage in LLM pretraining as the\npriority score of the web crawler's scheduler, replacing the standard graph\nconnectivity based priority. Our experiments on a web graph containing 900\nmillion webpages from a commercial search engine's index demonstrate the\nefficiency of Craw4LLM in obtaining high-quality pretraining data. With just\n21% URLs crawled, LLMs pretrained on Craw4LLM data reach the same downstream\nperformances of previous crawls, significantly reducing the crawling waste and\nalleviating the burdens on websites. Our code is publicly available at\nhttps:\/\/github.com\/cxcscmu\/Craw4LLM.",
        "Electrocardiogram (ECG) interpretation is a cornerstone of cardiac\ndiagnostics. This paper explores a practical approach to enhance ECG image\ninterpretation using the multimodal LLaMA 3.2 model. We used a\nparameter-efficient fine-tuning strategy, Low-Rank Adaptation (LoRA),\nspecifically designed to boost the model's ability to understand ECG images and\nachieve better outcomes across a wide range of cardiac conditions. Our method\nis tailored for ECG analysis and leverages ECGInstruct, a large-scale\ninstruction dataset with 1 Million samples. This dataset is a rich collection\nof synthesized ECG images, generated from raw ECG data from trusted open-source\nrepositories like MIMIC-IV ECG and PTB-XL. Each ECG image in ECGInstruct comes\nwith expert-written questions and detailed answers, covering diverse ECG\ninterpretation scenarios, including complex cardiac conditions like Myocardial\nInfarction and Conduction Disturbances. Our fine-tuning approach efficiently\nadapts the LLaMA 3.2 model (built upon LLaMA 3) by integrating low-rank\nadaptation techniques, focusing on efficiency by updating only a small set of\nparameters, specifically ignoring the `lm_head` and `embed_tokens` layers. This\npaper details the model setup, our efficient fine-tuning method, and\nimplementation specifics. We provide a thorough evaluation through extensive\nexperiments, demonstrating the effectiveness of our method across various ECG\ninterpretation tasks. The results convincingly show that our\nparameter-efficient LoRA fine-tuning achieves excellent performance in ECG\nimage interpretation, significantly outperforming baseline models and reaching\naccuracy comparable to or exceeding traditional CNN-based methods in\nidentifying a wide range of cardiac abnormalities, including over 70 conditions\nfrom the PTB-XL dataset.",
        "In this paper, we propose a graphon game model to understand how rumor (such\nas fake news) propagates in large populations that are interacting on a network\nand how different policies affect the spread. We extend the SKIR model that is\nused to model rumor propagation and implement individual controls and weighted\ninteractions with other agents to have controlled dynamics. The agents aim to\nminimize their own expected costs non-cooperatively. We give the finite player\ngame model and the limiting graphon game model to approximate the Nash\nequilibrium in the population. We give the graphon game Nash equilibrium as a\nsolution to a continuum of ordinary differential equations (ODEs) and give\nexistence results. Finally, we give a numerical approach and analyze examples\nwhere we use piecewise constant graphon.",
        "By imposing conditions upon the index of a self-centralizing subgroup of a\ngroup, and upon the index of the center of the group, we are able to classify\nthe Chermak-Delgado lattice of the group. This is our main result. We use this\nresult to classify the Chermak-Delgado lattices of dicyclic groups and of\nmetabelian $p$-groups of maximal class.",
        "Office Assistant Robots (OARs) offer a promising solution to proactively\nprovide in-situ support to enhance employee well-being and productivity in\noffice spaces. We introduce OfficeMate, a social OAR designed to assist with\npractical tasks, foster social interaction, and promote health and well-being.\nThrough a pilot evaluation with seven participants in an office environment, we\nfound that users see potential in OARs for reducing stress and promoting\nhealthy habits and value the robot's ability to provide companionship and\nphysical activity reminders in the office space. However, concerns regarding\nprivacy, communication, and the robot's interaction timing were also raised.\nThe feedback highlights the need to carefully consider the robot's appearance\nand behaviour to ensure it enhances user experience and aligns with office\nsocial norms. We believe these insights will better inform the development of\nadaptive, intelligent OAR systems for future office space integration.",
        "Computational Fluid Dynamics (CFD) is widely used in aerospace, energy, and\nbiology to model fluid flow, heat transfer, and chemical reactions. While Large\nLanguage Models (LLMs) have transformed various domains, their application in\nCFD remains limited, particularly for complex tasks like post-processing. To\nbridge this gap, we introduce MetaOpenFOAM 2.0, which leverages Chain of\nThought (COT) decomposition and iterative verification to enhance accessibility\nfor non-expert users through natural language inputs. Tested on a new benchmark\ncovering simulation (fluid flow, heat transfer, combustion) and post-processing\n(extraction, visualization), MetaOpenFOAM 2.0 achieved an Executability score\nof 6.3\/7 and a pass rate of 86.9%, significantly outperforming MetaOpenFOAM 1.0\n(2.1\/7, 0%). Additionally, it proved cost-efficient, averaging $0.15 per case.\nAn ablation study confirmed that COT-driven decomposition and iterative\nrefinement substantially improved task performance. Furthermore, scaling laws\nshowed that increasing COT steps enhanced accuracy while raising token usage,\naligning with LLM post-training scaling trends. These results highlight the\ntransformative potential of LLMs in automating CFD workflows for industrial and\nresearch applications. Code is available at\nhttps:\/\/github.com\/Terry-cyx\/MetaOpenFOAM",
        "Leading edges of hypersonic vehicles can reach temperatures greater than 2000\n{\\deg}C, and radii of curvature smaller than 1 cm, at which thermionic emission\n(also known as electron transpiration) can play a significant role in cooling\nthe leading edge alongside other heat transfer modes such as convection and\nradiation. Existing theoretical analyses of thermionic cooling with\nspace-charge effects at a leading edge are limited to one-dimensional (1D),\nanalytical and numerical models that do not capture the influences of geometric\ncurvature of the leading edge or temperature gradients along the leading edge.\nThe key to understanding space-charge effects is development of the plasma\nsheath potential, and to that end we demonstrate a generalized methodology to\ncalculate the sheath potential space in 1D Cartesian, cylindrical, and\nspherical coordinate systems. We accomplish this by extending Takamura's\napproach beyond the Cartesian system, and motivate sheath formation conditions\nfor potential sheathes with and without a virtual cathode similar in nature to\nhow Bohm originally presented his criterion of minimum Mach number for a valid\n1D Cartesian sheath. By observing for what parameter inputs we satisfy the\nsheath formation conditions, we illustrate parameter spaces of minimum Mach\nnumber, potential derivative at the wall, and net current, for each coordinate\nsystem and for two different input work functions; we also show example\npotential spaces for each coordinate system. With our numerical approach,\ngeneralized to multiple coordinate systems, we enable computationally efficient\nand higher fidelity analysis of thermionic emission with space-charge effects\nfor more realistic system geometries.",
        "Financial sentiment has become a crucial yet complex concept in finance,\nincreasingly used in market forecasting and investment strategies. Despite its\ngrowing importance, there remains a need to define and understand what\nfinancial sentiment truly represents and how it can be effectively measured. We\nexplore the nature of financial sentiment and investigate how large language\nmodels (LLMs) contribute to its estimation. We trace the evolution of sentiment\nmeasurement in finance, from market-based and lexicon-based methods to advanced\nnatural language processing techniques. The emergence of LLMs has significantly\nenhanced sentiment analysis, providing deeper contextual understanding and\ngreater accuracy in extracting sentiment from financial text. We examine how\nBERT-based models, such as RoBERTa and FinBERT, are optimized for structured\nsentiment classification, while GPT-based models, including GPT-4, OPT, and\nLLaMA, excel in financial text generation and real-time sentiment\ninterpretation. A comparative analysis of bidirectional and autoregressive\ntransformer architectures highlights their respective roles in investor\nsentiment analysis, algorithmic trading, and financial decision-making. By\nexploring what financial sentiment is and how it is estimated within LLMs, we\nprovide insights into the growing role of AI-driven sentiment analysis in\nfinance.",
        "Due to network delays and scalability limitations, clustered ad hoc networks\nwidely adopt Reinforcement Learning (RL) for on-demand resource allocation.\nAlbeit its demonstrated agility, traditional Model-Free RL (MFRL) solutions\nstruggle to tackle the huge action space, which generally explodes\nexponentially along with the number of resource allocation units, enduring low\nsampling efficiency and high interaction cost. In contrast to MFRL, Model-Based\nRL (MBRL) offers an alternative solution to boost sample efficiency and\nstabilize the training by explicitly leveraging a learned environment model.\nHowever, establishing an accurate dynamic model for complex and noisy\nenvironments necessitates a careful balance between model accuracy and\ncomputational complexity $\\&$ stability. To address these issues, we propose a\nConditional Diffusion Model Planner (CDMP) for high-dimensional offline\nresource allocation in clustered ad hoc networks. By leveraging the astonishing\ngenerative capability of Diffusion Models (DMs), our approach enables the\naccurate modeling of high-quality environmental dynamics while leveraging an\ninverse dynamics model to plan a superior policy. Beyond simply adopting DMs in\noffline RL, we further incorporate the CDMP algorithm with a theoretically\nguaranteed, uncertainty-aware penalty metric, which theoretically and\nempirically manifests itself in mitigating the Out-of-Distribution\n(OOD)-induced distribution shift issue underlying scarce training data.\nExtensive experiments also show that our model outperforms MFRL in average\nreward and Quality of Service (QoS) while demonstrating comparable performance\nto other MBRL algorithms.",
        "The effect of platinum (Pt) bottom electrode texture on the tetragonality,\ndielectric, ferroelectric, and polarization switching response of pulsed laser\ndeposited Ba0.8Sr0.2TiO3 (BST) thin films has been studied. The x-ray\ndiffraction and Raman analysis revealed the higher tetragonality of BST films\nwhen they were grown on higher (111) textured Pt layer. The properties like\ndielectric permittivity, polarization, switching time, and leakage currents\nwere found to be correlated to tetragonality and orientation of the BST films.\nThe polarization current was observed to be higher in BST films on Pt epitaxial\nlayer and it exhibits exponential dependence on the electric field. The\nvoltage-current measurements displayed Ohmic behavior of leakage current\nirrespective of Pt texture for low voltages (up to 1 V), whereas at higher\nvoltages the conduction mechanism was found to be dependent on texture\nselection of bottom Pt electrode.",
        "Large language models (LLMs) are widely adopted to generate synthetic\ndatasets for various natural language processing (NLP) tasks, such as text\nclassification and summarization. However, accurately measuring the diversity\nof these synthetic datasets-an aspect crucial for robust model\nperformance-remains a significant challenge. In this paper, we introduce\nDCScore, a novel method for measuring synthetic dataset diversity from a\nclassification perspective. Specifically, DCScore formulates diversity\nevaluation as a sample classification task, leveraging mutual relationships\namong samples. We further provide theoretical verification of the\ndiversity-related axioms satisfied by DCScore, highlighting its role as a\nprincipled diversity evaluation method. Experimental results on synthetic\ndatasets reveal that DCScore enjoys a stronger correlation with multiple\ndiversity pseudo-truths of evaluated datasets, underscoring its effectiveness.\nMoreover, both empirical and theoretical evidence demonstrate that DCScore\nsubstantially reduces computational costs compared to existing approaches. Code\nis available at: https:\/\/github.com\/BlueWhaleLab\/DCScore.",
        "Signal to noise ratio is key to any measurement. Recent progress in\nsemi\/super-conductor technology have pushed the signal detection sensitivity to\nthe ultimate quantum level, but the noise issue remains largely untouched and,\nin many cases, becomes even more severe because of the high sensitivity. In\nthis paper, we explore a hardware-based approach to noise removal using entropy\nquantum computing. Distinct to any existing de-noising approach, it observes\nand reproduces the quantum statistical properties of noise in an optical system\nto emulate and thereby reverse the noise from data. We show how it can recover\n1D and 2D image data mixed with much stronger noise.",
        "Portugal's prominent role as a global exporter of football talent is\nprimarily driven by youth academies. Notably, Portugal leads the global ranking\nin terms of net transfer balance. This study aims to uncover and understand the\nrecruitment strategies of Portuguese clubs for sourcing young talent and\nevaluate the relative success of different strategies. A comprehensive dataset\nspanning recent decades of Portuguese youth and professional football provides\ngranular insights, including information such as players' birthplaces and the\ninitial grassroots clubs where they developed. The initial findings suggest a\ncorrelation between a club's prominence and the geographic reach of its youth\nscouting operations, with larger clubs able to cast their net wider. Analysis\nof the correlation between players' birthplace and high-tier football club\nlocation suggests that the performance of senior teams acts as a catalyst for\ninvestment in youth teams. Regions without professional clubs are often left\nunderserved. That said, certain clubs have made significant gains by focusing\non player recruitment outside their district, such as the Algarve region,\ndemonstrating how geographically targeted strategies can deliver substantial\nreturns on investment. This study underscores data's role in sharpening youth\nplayer recruitment operations at football clubs. Clubs have access to in-depth\nand comprehensive datasets that can be used for resource allocation,\nterritorial coverage planning, and identifying strategic partnerships with\nother clubs, potentially influencing their future success both on the field and\nfinancially. This offers opportunities for growth for individual clubs and\nholds implications for the continued strength of Portuguese football.",
        "Traditional methods for aligning Large Language Models (LLMs), such as\nReinforcement Learning from Human Feedback (RLHF) and Direct Preference\nOptimization (DPO), rely on implicit principles, limiting interpretability.\nConstitutional AI (CAI) offers an explicit, rule-based framework for guiding\nmodel outputs. Building on this, we refine the Inverse Constitutional AI (ICAI)\nalgorithm, which extracts constitutions from preference datasets. By improving\nprinciple generation, clustering, and embedding processes, our approach\nenhances the accuracy and generalizability of extracted principles across\nsynthetic and real-world datasets. While in-context alignment yields modest\nimprovements, our results highlight the potential of these principles to foster\nmore transparent and adaptable alignment methods, offering a promising\ndirection for future advancements beyond traditional fine-tuning.",
        "Geodesic distances play a fundamental role in robotics, as they efficiently\nencode global geometric information of the domain. Recent methods use neural\nnetworks to approximate geodesic distances by solving the Eikonal equation\nthrough physics-informed approaches. While effective, these approaches often\nsuffer from unstable convergence during training in complex environments. We\npropose a framework to learn geodesic distances in irregular domains by using\nthe Soner boundary condition, and systematically evaluate the impact of data\nlosses on training stability and solution accuracy. Our experiments demonstrate\nthat incorporating data losses significantly improves convergence robustness,\nreducing training instabilities and sensitivity to initialization. These\nfindings suggest that hybrid data-physics approaches can effectively enhance\nthe reliability of learning-based geodesic distance solvers with sparse data.",
        "Estimations of event-by-event mean transverse momentum ($\\langle p_{\\rm T}\n\\rangle$) fluctuations are reported in terms of the integral correlator,\n$\\langle \\Delta p_{\\rm T} \\Delta p_{\\rm T}\\rangle$, and the skewness of\nevent-wise $\\langle p_{\\rm T} \\rangle$ distribution in proton$-$proton (pp)\ncollisions at $\\sqrt{s}=13$ TeV with the Monte Carlo event generators PYTHIA8\nand HERWIG7. The final-state charged particles with transverse momentum\n($p_{\\rm T}$) and pseudorapidity ($\\eta$) ranges $0.15 \\leq p_{\\rm T}\\leq 2.0$\nGeV\/$c$ and $|\\eta| \\leq 0.8$ were considered for the investigation. The\ncorrelator, $\\langle \\Delta p_{\\rm T} \\Delta p_{\\rm T}\\rangle$, is observed to\nfollow distinct decreasing trends with average charged particle multiplicity\n($\\langle N_{\\rm ch} \\rangle$) for the models. Furthermore, both models yield\npositive finite skewness in low-multiplicity events. The fluctuations are\nadditionally studied using the transverse spherocity estimator ($S_{\\rm 0}$) to\ncomprehend the relative contributions from hard scattering (jets) and other\nsoft processes to the observed fluctuations. Comparing the model predictions\nwould enhance our understanding of fluctuation dynamics in pp collisions,\nestablishing a crucial baseline for studying non-trivial fluctuations in\nheavy-ion collisions.",
        "A promising quantum computing architecture comprises modules of\nsuperconducting quantum processors linked by optical channels via quantum\ntransducers. To map transducer device performance to system-level channel\nperformance, our model uses Monte Carlo simulations that incorporate 2-to-1 and\n3-to-1 entanglement distillation protocols. We show that the Extreme Photon\nLoss distillation protocol is particularly high performing and that, even\nwithout distillation, present-day transducers are at the threshold of enabling\nBell pair distribution with fidelities of 50%. If the next generation of\ntransducers can improve by 3 orders of magnitude in both added noise and\nefficiency, and increase repetition rates by 50x, then they would allow for\nremote two-qubit gates achieving 99.7% fidelities at 100 kHz rates. These\nresults set targets for transducers to be ready for deployment into scaled\nsuperconducting quantum computers.",
        "Ensemble-based Data Assimilation faces significant challenges in\nhigh-dimensional systems due to spurious correlations and ensemble collapse.\nThese issues arise from estimating dense dependencies with limited ensemble\nsizes. This paper introduces the Ensemble Information Filter, which encodes\nMarkov properties directly into the statistical model's precision matrix,\nleveraging structure from SPDE dynamics to constrain information to propagate\nlocally. EnIF eliminates the need for ad-hoc localisation, improving\nstatistical consistency and scalability. Numerical experiments demonstrate its\nadvantages in filtering, smoothing, and parameter estimation, making EnIF a\nrobust and efficient solution for large-scale data assimilation problems.",
        "Risk matrices are widely used across a range of fields and have found\nincreasing utility in warning decision practices globally. However, their\napplication in this context presents challenges, which range from potentially\nperverse warning outcomes to a lack of objective verification (i.e.,\nevaluation) methods. This paper introduces a coherent framework for generating\nmulti-level warnings from risk matrices to address these challenges. The\nproposed framework is general, is based on probabilistic forecasts of hazard\nseverity or impact and is compatible with the Common Alerting Protocol (CAP).\nMoreover, it includes a family of consistent scoring functions for objectively\nevaluating the predictive performance of risk matrix assessments and the\nwarnings they produce. These scoring functions enable the ranking of\nforecasters or warning systems and the tracking of system improvements by\nrewarding accurate probabilistic forecasts and compliance with warning service\ndirectives. A synthetic experiment demonstrates the efficacy of these scoring\nfunctions, while the framework is illustrated through warnings for heavy\nrainfall based on operational ensemble prediction system forecasts for Tropical\nCyclone Jasper (Queensland, Australia, 2023). This work establishes a robust\nfoundation for enhancing the reliability and verifiability of risk-based\nwarning systems."
      ]
    }
  },
  {
    "id":2411.1742,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"Deep learning-based classification of healthy aging controls, mild cognitive impairment and alzheimer's disease using fusion of mri-pet imaging",
    "start_abstract":"Automated detection of dementia stage using multimodal imaging modalities will be helpful for improving the clinical diagnosis. In this study, we develop the Inception-ResNet wrapper model in differentiating the healthy controls (HC), mild cognitive impairment (MCI), and Alzheimer\u2019s disease (AD) using conjoint magnetic resonance imaging (MRI) and positron emission tomography (PET) scans. We use T1-weighted MR and PET images of individuals aged between 42 and 95 years, including HC, MCI and AD patients. We first perform 3D tissue segmentation of MR images after skull striping. The atlas-based segmented MR image tissue is fused with PET image. Then we transform PET images from RGB to HSI color space and apply fusion of MRI with PET images using two-dimensional Fourier and discrete wavelet transform (DWT) and then reconstruct the MR-PET fused image using inverse Fourier and DWT methods. After the fusion of MRI and PET imaging modalities, we used 60 % training, 20 % for validation and the remaining 20 % as a test set using various convolutional neural networks. We found the proposed model as the best classifier with an accuracy of 95.5 %, 94.1 % and 95.9 % in classifying HC vs MCI, MCI vs AD and AD vs HC respectively when compared to the existing methods. We conclude that the proposed deep learning model has potential in automated classification of healthy and dementia stages using combined MRI and PET modalities with good performance.",
    "start_categories":[
      "cs.CV",
      "cs.NE"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b19"
      ],
      "title":[
        "Bidirectional Mapping of Brain MRI and PET With 3D Reversible GAN for the Diagnosis of Alzheimer\u2019s Disease"
      ],
      "abstract":[
        "<jats:p>Combining multi-modality data for brain disease diagnosis such as Alzheimer\u2019s disease (AD) commonly leads to improved performance than those using a single modality. However, it is still challenging to train a multi-modality model since it is difficult in clinical practice to obtain complete data that includes all modality data. Generally speaking, it is difficult to obtain both magnetic resonance images (MRI) and positron emission tomography (PET) images of a single patient. PET is expensive and requires the injection of radioactive substances into the patient\u2019s body, while MR images are cheaper, safer, and more widely used in practice. Discarding samples without PET data is a common method in previous studies, but the reduction in the number of samples will result in a decrease in model performance. To take advantage of multi-modal complementary information, we first adopt the Reversible Generative Adversarial Network (RevGAN) model to reconstruct the missing data. After that, a 3D convolutional neural network (CNN) classification model with multi-modality input was proposed to perform AD diagnosis. We have evaluated our method on the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) database, and compared the performance of the proposed method with those using state-of-the-art methods. The experimental results show that the structural and functional information of brain tissue can be mapped well and that the image synthesized by our method is close to the real image. In addition, the use of synthetic data is beneficial for the diagnosis and prediction of Alzheimer\u2019s disease, demonstrating the effectiveness of the proposed framework.<\/jats:p>"
      ],
      "categories":[
        "q-bio.NC"
      ]
    },
    "list":{
      "title":[
        "2D Surface Brightness Modelling of Large 2MASS Galaxies II: The Role of\n  Classical Bulges and Pseudobulges on Galaxy Scaling Relations and its\n  implication for Supermassive Black Hole Formation",
        "Feasibility of short blocklength Reed-Muller codes for physical layer\n  security in real environment",
        "Compact superconducting vacuum-gap capacitors with low microwave loss\n  and high mechanical coherence for scalable quantum circuits",
        "Probabilistic Shielding for Safe Reinforcement Learning",
        "Bayesian mixture modeling using a mixture of finite mixtures with\n  normalized inverse Gaussian weights",
        "Fitting multivariate Hawkes processes to interval count data with an\n  application to terrorist activity modelling -- a particle Markov chain Monte\n  Carlo approach",
        "Tuning topologically nontrivial states in the BHT-Ni metal organic\n  framework",
        "The R Package WMAP: Tools for Causal Meta-Analysis by Integrating\n  Multiple Observational Studies",
        "Operator-isomorphism pairs and Zak transform methods for the study of\n  Gabor systems",
        "Change Point Detection for Random Objects with Possibly Periodic\n  Behavior",
        "Enhancing energy transport utilising permanent molecular dipoles",
        "Continuum-wise hyperbolicity and periodic points",
        "Limits of nonlinear and dispersive fiber propagation for photonic\n  extreme learning",
        "Enhancement of Superconductivity in WP via Oxide-Assisted Chemical Vapor\n  Transport",
        "Structure factors and quantum geometry in multiband BCS superconductors",
        "Where are the earliest stars relics in the simulated Milky Way\n  analogues?",
        "Mean value of cubic $L$-funcitons with fixed genus",
        "Peak splitting and bias fields in ferroelectric hafnia mediated by\n  interface charge effects",
        "A new and flexible class of sharp asymptotic time-uniform confidence\n  sequences",
        "SPYGLASS. VI. Feedback-Driven Star Formation in the Circinus Complex",
        "Volumetric modulated arc therapy or step-shoot IMRT? A 4D dosimetry\n  study of motion effect in lung SBRT using a dynamic virtual patient model",
        "The effect of a band gap gradient on the radiative losses in the open\n  circuit voltage of solar cells",
        "The effect of spacetime torsion on neutrino mixing",
        "Stability of Khintchine inequalities with optimal constants between the\n  second and the $p$-th moment for $p \\ge 3$",
        "Counting lifts of irreducible Brauer characters",
        "Mitigation of Artifacts in Multistatic & Passive Radar Imaging Using\n  Microlocal Analysis",
        "High-Sensitivity Imaging and Modeling of Ultra-Weak Photon Emission in\n  Plants Under Stress",
        "Weak Lefschetz property of equigenerated complete intersections.\n  Applications",
        "DNA Sensing with Whispering Gallery Mode Microlasers"
      ],
      "abstract":[
        "We have generated 2D-multicomponent surface brightness (SB) modelling for 100\ngalaxies in the Large Galaxy Atlas (LGA) together with 19 nearby cD galaxies\nusing the near-infrared (NIR) images from 2MASS (J, H and Ks ). Our final\nsample of 119 galaxies includes cD galaxies, Virgo cluster galaxies, group\ngalaxies, and field galaxies. We revisited known scaling relations (SRs)\ninvolving structural parameters, as well as those involving supermassive black\nholes (SMBHs) and ultramassive black holes (UMBHs). Refining the SRs, we also\nrevisited the bulge classification and considered the Fundamental Plane (FP)\nand its projections, as well as other SRs, such as the colour-magnitude\nrelation (CMR), Tully-Fisher relation (TFR) and luminosity concentration\nrelation (LCR). Classical bulges follow the same relations as elliptical\ngalaxies, while pseudobulges are usually outliers. The NIR colours of classical\nbulges and pseudobulges indicate that their ages are not radically different\ndespite their spread in luminosity, but we noticed that classical bulges are\nmore luminous than pseudobulges, therefore, this property provides a\ncomplementary bulge classification criterion. We included pseudobulges from\nother studies to strengthen the tendencies seen for pseudobulges in our sample.\nFrom the SRs for BHs, we found that pseudobulges do not follow SRs for\nearly-type galaxies and classical bulges. Additionally, the lack of correlation\nbetween BHs and discs may indicate these structures have not coevolved. From\nthe revision of SRs, we present a sample of galaxies likely to host SMBHs or\nUMBHs, which are suitable for dynamical BH mass determination from the ground.",
        "In this paper, we investigate the application of Reed-Muller (RM) codes for\nPhysical-layer security in a real world wiretap channel scenario. Utilizing\nsoftware-defined radios (SDRs) in a real indoor environment, we implement a\ncoset coding scheme that leverages the hierarchical structure of RM codes to\nsecure data transmission. The generator matrix of the RM code is used to\npartition codewords into cosets in the usual way, where each message\ncorresponds to a unique coset, and auxiliary bits select specific codewords\nwithin each coset. This approach enables the legitimate receiver (Bob) can\ndecode the transmitted message with minimal information leakage to eavesdropper\n(Eve) thus protecting the confidentiality of the communication with the help of\ncoset structure. Mutual information neural estimation (MINE) is used to\nquantify information leakage and validate the effectiveness of the scheme.\nExperimental results indicate that RM codes can achieve robust security even in\npractical environments affected by real-world channel impairments. These\nfindings demonstrate the potential of RM codes as an efficient solution for\nphysical-layer security, particularly for applications that require low latency\nand short blocklengths.",
        "Vacuum gap capacitors have recently gained considerable attention in\nsuperconducting circuit platforms due to their compact design and low\ndielectric losses in the microwave regime. Their ability to support mechanical\nvibrational modes makes them ideal candidates for circuit optomechanics.\nHowever, precise control of gap size and achieving high coherence in mechanical\nmodes remain long-standing challenges. Here, we present a detailed fabrication\nprocess for scalable vacuum gap capacitors that support ultra-high-coherence\nmechanical motion, exhibit low microwave loss, and maintain a small footprint\ncompared to planar geometries. We fabricate arrays of up to 24 LC resonators,\nwith capacitors featuring nanometer-scale gap size variations. We demonstrate\nthat the mechanical quality factors can reach up to $40 \\times 10^6$, a\n100-fold improvement over other platforms, with microwave quality factors\n$\\mathcal{O}(10^5)$ at low photon number levels. This platform also achieves a\nsizable single-photon optomechanical coupling rate of approximately 20 Hz.\nUsing this, we cooled the mechanical oscillator to its ground state (0.07\nquanta) and squeezed its motion below the vacuum level by 2.7 dB. We further\ndemonstrate the scalability of this platform by implementing large-scale\noptomechanical arrays, a strained graphene model, and observing quantum\ncollective phenomena in a mechanical hexamer. These vacuum gap capacitors are\npromising candidates for coupling superconducting qubits with mechanical\nsystems, serving as storage elements in quantum computing, and exploring\ngravitational effects on quantum mechanics.",
        "In real-life scenarios, a Reinforcement Learning (RL) agent aiming to\nmaximise their reward, must often also behave in a safe manner, including at\ntraining time. Thus, much attention in recent years has been given to Safe RL,\nwhere an agent aims to learn an optimal policy among all policies that satisfy\na given safety constraint. However, strict safety guarantees are often provided\nthrough approaches based on linear programming, and thus have limited scaling.\nIn this paper we present a new, scalable method, which enjoys strict formal\nguarantees for Safe RL, in the case where the safety dynamics of the Markov\nDecision Process (MDP) are known, and safety is defined as an undiscounted\nprobabilistic avoidance property. Our approach is based on state-augmentation\nof the MDP, and on the design of a shield that restricts the actions available\nto the agent. We show that our approach provides a strict formal safety\nguarantee that the agent stays safe at training and test time. Furthermore, we\ndemonstrate that our approach is viable in practice through experimental\nevaluation.",
        "In Bayesian inference for mixture models with an unknown number of\ncomponents, a finite mixture model is usually employed that assumes prior\ndistributions for mixing weights and the number of components. This model is\ncalled a mixture of finite mixtures (MFM). As a prior distribution for the\nweights, a (symmetric) Dirichlet distribution is widely used for conjugacy and\ncomputational simplicity, while the selection of the concentration parameter\ninfluences the estimate of the number of components. In this paper, we focus on\nestimating the number of components. As a robust alternative Dirichlet weights,\nwe present a method based on a mixture of finite mixtures with normalized\ninverse Gaussian weights. The motivation is similar to the use of normalized\ninverse Gaussian processes instead of Dirichlet processes for infinite mixture\nmodeling. Introducing latent variables, the posterior computation is carried\nout using block Gibbs sampling without using the reversible jump algorithm. The\nperformance of the proposed method is illustrated through some numerical\nexperiments and real data examples, including clustering, density estimation,\nand community detection.",
        "Terrorist activities often exhibit temporal and spatial clustering, making\nthe multivariate Hawkes process (MHP) a useful statistical model for analysing\nterrorism across different geographic regions. However, terror attack data from\nthe Global Terrorism Database is reported as total event counts in disjoint\nobservation periods, with precise event times unknown. When the MHP is only\nobserved discretely, the likelihood function becomes intractable, hindering\nlikelihood-based inference. To address this, we design an unbiased estimate of\nthe intractable likelihood function using sequential Monte Carlo (SMC) based on\na representation of the unobserved event times as latent variables in a\nstate-space model. The unbiasedness of the SMC estimate allows for its use in\nplace of the true likelihood in a Metropolis-Hastings algorithm, from which we\nconstruct a Markov Chain Monte Carlo sample of the distribution over the\nparameters of the MHP. Using simulated data, we assess the performance of our\nmethod and demonstrate that it outperforms an alternative method in the\nliterature based on mean squared error. Terrorist activity in Afghanistan and\nPakistan from 2018 to 2021 is analysed based on daily count data to examine the\nself- and cross-excitation effects of terrorism events.",
        "Using first principles calculations, we have demonstrated the creation of\nmultiple quantum states, in the experimentally accessible metal organic\nframework BHT-Ni. Specifically, quantum spin Hall and quantum anomalous Hall\nstates are induced by two and four electron doping, respectively. The\ngeometrical symmetry breaking, is also investigated. For a low electron doping\nconcentration of two electrons per unit cell, the Fermi energy shifts to a\nnontrivial band gap, between Dirac bands and a quantized spin Hall conductivity\nis predicted. Subsequently in a high electron doping concentration, Anomalous\nHall conductivity with a quantized value was observed. In addition, for\ncentrosymmetric (trans-like) and non-centrosymmetric (cis-like) structures, we\nfound that the trans-like structure preserves quantum spin Hall and quantized\nspin Hall conductivity. In contrast, in the cis-like structure, space inversion\nsymmetry breaking leads to the appearance of valley Hall effect and the\ndisappearance of spin Hall conductivity.",
        "Integrating multiple observational studies for meta-analysis has sparked much\ninterest. The presented R package WMAP (Weighted Meta-Analysis with\nPseudo-Population) addresses a critical gap in the implementation of\nintegrative weighting approaches for multiple observational studies and causal\ninferences about various groups of subjects, such as disease subtypes. The\npackage features three weighting approaches, each representing a special case\nof the unified weighting framework introduced by Guha and Li (2024), which\nincludes an extension of inverse probability weights for data integration\nsettings. It performs meta-analysis on user-inputted datasets as follows: (i)\nit first estimates the propensity scores for study-group combinations,\ncalculates subject balancing weights, and determines the effective sample size\n(ESS) for a user-specified weighting method; and (ii) it then estimates various\nfeatures of multiple counterfactual group outcomes, such as group medians and\ndifferences in group means for the mRNA expression of eight genes.\nAdditionally, bootstrap variability estimates are provided. Among the\nimplemented weighting methods, we highlight the FLEXible, Optimized, and\nRealistic (FLEXOR) method, which is specifically designed to maximize the ESS\nwithin the unified framework. The use of the software is illustrated by\nsimulations as well as a multi-site breast cancer study conducted in seven\nmedical centers.",
        "We collect and summarize results on the unitary equivalence of Gabor systems\nby pairs of unitary operators and global isometries. The methods are then used\nto study Gabor systems with Hermite functions. We provide new proofs of some\nknown results and an outlook on double over-sampling.",
        "Time-varying random objects have been increasingly encountered in modern data\nanalysis. Moreover, in a substantial number of these applications, periodic\nbehavior of the random objects has been observed. We introduce a new, powerful\nscan statistic and corresponding test for the precise identification and\nlocalization of abrupt changes in the distribution of non-Euclidean random\nobjects with possibly periodic behavior. Our approach is nonparametric and\neffectively captures the entire distribution of these random objects.\nRemarkably, it operates with minimal tuning parameters, requiring only the\nspecification of cut-off intervals near endpoints, where change points are\nassumed not to occur. Our theoretical contributions include deriving the\nasymptotic distribution of the test statistic under the null hypothesis of no\nchange points, establishing the consistency of the test in the presence of\nchange points under contiguous alternatives and providing rigorous guarantees\non the near-optimal consistency in estimating the number and locations of\nchange points, whether dealing with a single change point or multiple ones. We\ndemonstrate that the most competitive method currently in the literature for\nchange point detection in random objects is degraded by periodic behavior, as\nperiodicity leads to blurring of the changes that this procedure aims to\ndiscover. Through comprehensive simulation studies, we demonstrate the superior\npower and accuracy of our approach in both detecting change points and\npinpointing their locations, across scenarios involving both periodic and\nnonperiodic random objects. Our main application is to weighted networks,\nrepresented through graph Laplacians. The proposed method delivers highly\ninterpretable results, as evidenced by the identification of meaningful change\npoints in the New York City Citi Bike sharing system that align with\nsignificant historical events.",
        "We study exciton quantum transfer along a molecular chain whilst accounting\nfor the effects of permanent dipoles that are induced by charge displacements\nin the molecular orbitals. These effects are typically neglected as they do not\narise in atomic quantum optics; however, they can play an important role in\nmolecular systems. We also consider novel collective photon-assisted transport\nand compare it against the scaling of phonon-assisted transport in chains\nfeaturing permanent dipoles, and determine a linear scaling with the number of\ndipoles, akin to single-excitation superradiance. We further demonstrate how\npermanent dipoles, dipoles can preferentially arrange energy eigenstates to\nsupport excitation transport. Finally, we show how permanent dipoles can\nenhance the ability of the molecular chain to support excitation transport\ncompared to that of systems that do not possess permanent dipoles across a\nrange of environmental and system configurations.",
        "We prove that cw-hyperbolic homeomorphisms with jointly continuous\nstable\/unstable holonomies satisfy the periodic shadowing property and, if they\nare topologically mixing, the periodic specification property. We discuss\ndifficulties to adapt Bowen's techniques to obtain a measure of maximal entropy\nfor cw-hyperbolic homeomorphisms, exhibit the unique measure of maximal entropy\nfor Walter's pseudo-Anosov diffeomorphism of $\\mathbb{S}^2$, and prove it can\nbe obtained, as in the expansive case, as the weak* limit of an average of\nDirac measures on periodic orbits. As an application, we exhibit the unique\nmeasure of maximal entropy for the homeomorphism on the Sierpi\\'nski Carpet\ndefined in [12], which does not satisfy the specification property.",
        "We report a generalized nonlinear Schr\\\"odinger equation simulation model of\nan extreme learning machine (ELM) based on optical fiber propagation. Using\nhandwritten digit classification as a benchmark, we study how accuracy depends\non propagation dynamics, as well as parameters governing spectral encoding,\nreadout, and noise. Test accuracies of over 91% and 93% are found for\npropagation in the anomalous and normal dispersion regimes respectively. Our\nsimulation results also suggest that quantum noise on the input pulses\nintroduces an intrinsic penalty to ELM performance.",
        "Tungsten monophosphide (WP) has been reported to superconduct below 0.8 K,\nand theoretical work has predicted an unconventional Cooper pairing mechanism.\nHere we present data for WP single crystals grown by means of chemical vapor\ntransport (CVT) of WO3, P, and I2. In comparison to synthesis using WP powder\nas a starting material, this technique results in samples with substantially\ndecreased low-temperature scattering and favors a more three dimensional\nmorphology. We also find that the resistive superconducting transitions in\nthese samples begin above 1 K. Variation in Tc is often found in strongly\ncorrelated superconductors, and its presence in WP could be the result of\ninfluence from a competing order and\/or a non s-wave gap.",
        "We consider multiband BCS superconductors that exhibit time-reversal symmetry\nand uniform pairing, and analyze their dynamic density and spin structure\nfactors using linear-response theory within the mean-field BCS-BEC crossover\nframework at zero temperature. Our results for the multi-orbital Hubbard model\nsatisfy the associated f-sum rules in several limits. In particular, in the\nstrong-coupling limit, they coincide with those of a weakly-interacting Bose\ngas of Cooper pairs, where the low-energy collective Goldstone modes serve as\nBogoliubov phonons. We further reveal that the quantum-geometric origin of the\nlow-energy structure factors, along with related observables such as the\nsuperfluid-weight tensor and the effective-mass tensor of Cooper pairs, can be\ntraced all the way back to the effective-mass theorem for Bloch bands in this\nlimit. As an illustration, we investigate the pyrochlore-Hubbard model\nnumerically and demonstrate that the Goldstone modes are the only relevant\ncollective degrees of freedom in the flat-band regime.",
        "Using 6 Milky Way analogues with two different numerical resolutions from the\nAuriga simulation, we investigate the total mass, spatial distribution and\nkinematics of the earliest stars relics in the Milky Way at $z=0$. These relics\n(second generation stars) formed over a wide redshift range, from about $z=22$\nto $z=4$, with an average formation redshift of $z \\sim 10.0$, and comprise\nabout $2\\times10^{-5}$ of the entire galactic stellar population. The disk and\nbulge components host only a small fraction of these relics, contributing less\nthan $12$ percent in total. The stellar halo, in particular the outer stellar\nhalo of which galactic radius $r>30$ kpc, hosts the largest fraction (about 46\npercent on average), with an average of one relic star for per $4,000$ to\n$10,000$ stars, making it a promising region for observational searches.\nAdditionally, around $18$ percent of the earliest stars relics are found in\nsatellite galaxies, with smaller and older satellite galaxies tending to\ncontain a higher proportion of these stars. Thus, low-mass and early-formed\nsatellite galaxies are also ideal targets for finding such relics, although\nsome satellite galaxies may lack them entirely. The spatial distribution and\nkinematics of these stars show good numerical convergence across different\nsimulation resolutions. Our results provide valuable guidance for searches of\nthe earliest stars relics and offer insights for interpreting findings from\nongoing and future stellar archaeology surveys.",
        "We investigate the mean value of the first moment of primitive cubic\n$L$-functions over $\\mathbb{F}_q(T)$ in the non-Kummer setting. Specifically,\nwe study the sum\n  \\begin{equation*}\n  \\sum_{\\substack{\\chi\\ primitive\\ cubic\\\\ genus(\\chi)=g}}L_q(\\frac{1}{2},\n\\chi),\n  \\end{equation*} where $L_q(s,\\chi)$ denotes the $L$-function associated with\nprimitive cubic character $\\chi$. Using double Dirichlet series, we derive an\nerror term of size $q^{(\\frac{7}{8}+\\varepsilon)g}$.",
        "The pristine state of hafnium based ferroelectric devices exhibits various\nunwanted properties, such as imprint and peak splitting, which diminish with\nbipolar cycling. The incorporation of a niobium oxide layer at different\npositions in metal-ferroelectric-metal and metal-ferroelectric-insulator-metal\nstacks is used to modify the pristine state of the device. X-ray photoelectron\nspectroscopy and transmission electron microscopy measurements are used to\ninvestigate the influence of niobium oxide on the zirconium hafnium oxide\nlayer. It is hypothesized that the charged vacancies generated by the\nintroduced niobium oxide in the adjacent zirconium hafnium oxide layer result\nin an electric bias field that influences the pristine polarization state of\nthe domains. A comparison of different stacks shows that peak splitting in the\npristine state is most likely related to the formation of opposing electric\nbias fields in upwards and downwards polarized domains. Furthermore, the\nincorporation of niobium oxide in the zirconium hafnium oxide\/aluminum oxide\ncapacitor stack in between the ferroelectric and insulating layer leads to a\npeak splitting free device without imprint, which could be explained by the\nincreased influence of charge trapping near the zirconium hafnium\noxide-\/niobium oxide and niobium oxide-\/aluminum oxide interfaces.",
        "Confidence sequences are anytime-valid analogues of classical confidence\nintervals that do not suffer from multiplicity issues under optional\ncontinuation of the data collection. As in classical statistics, asymptotic\nconfidence sequences are a nonparametric tool showing under which high-level\nassumptions asymptotic coverage is achieved so that they also give a certain\nrobustness guarantee against distributional deviations. In this paper, we\npropose a new flexible class of confidence sequences yielding sharp asymptotic\ntime-uniform confidence sequences under mild assumptions. Furthermore, we\nhighlight the connection to corresponding sequential testing problems and\ndetail the underlying limit theorem.",
        "Young associations provide a record that traces the star formation process,\nand the youngest populations connect progenitor gas dynamics to the resulting\nstellar populations. We therefore conduct the first comprehensive overview of\nthe Circinus Complex, an under-studied and massive ($\\sim$1500 M$_{\\odot}$)\nregion consisting of approximately 3100 recently formed stars alongside the\nCircinus Molecular Cloud (CMC). We find a clear age pattern in the contiguous\ncentral region (CirCe), where younger stars are found further from the massive\ncentral cluster, and where the velocities are consistent with uniform\nexpansion. By comparing this structure to an analogous STARFORGE simulation, we\nfind that the age structure and dynamics of the association are consistent with\nstar formation in two stages: the global collapse of the parent cloud that\nbuilds the $500 M_{\\odot}$ central cluster ASCC 79, followed by triggered star\nformation in a shell swept up after the first massive stars form. We also find\nthat filaments with a range of distances from the central cluster can naturally\nproduce multi-generational age sequences due to differences in feedback\nstrength and exposure. Outlying populations show velocities consistent with\nformation independent from the CirCe region, but with similar enough velocities\nthat they may be difficult to distinguish from one another later in their\nexpansion. We therefore provide a new alternative view of sequential star\nformation that relies on feedback from a single central cluster rather than the\nmultiple sequential generations that are traditionally invoked, while also\nproviding insight into the star formation history of older populations.",
        "Purpose: To investigate the impact of delivery techniques and planning\nparameters on interplay effect in lung SBRT.\n  Methods: A dynamic virtual patient model containing normal structures and a\ntumor with adjustable sizes, locations, and 3D breathing motion was utilized.\nSBRT plans were developed using both step-and-shoot IMRT and VMAT with\ndifferent planning parameters (energy, isocenter location, PTV margin, and PTV\ndose heterogeneity). 4D doses were calculated by simulating synchronized\ndelivery of SBRT to the virtual patient model with random initial positions of\ntumor motion. The expected dose (average) and the standard deviation of the 4D\ndoses were obtained. The relative difference between the expected GTV\nminimal\/mean (GTVMin\/GTVMean) dose and the planned ITVMin\/ITVMean dose (denoted\nby %E\/P), and between the GTVMin and the prescription dose (DRx) were computed.\n  Results: The %E\/P for GTVMean was significantly lower for IMRT than VMAT\n(0.5% +\/- 7.7% v.s. 3.5% +\/- 5.0%, p=0.04). The expected GTVMin was lower than\nDRx in 9.4% of all IMRT plans versus 3.1% in VMAT. The worst-case scenario, 4D\nGTVMin was 14.1% lower than the ITVMin. Choices of PTV margin or dose\nheterogeneity to be achieved in PTV can result in significant difference\n(p<0.05) in motion interplay depending on delivery techniques.\n  Conclusion: Motion interplay may cause the expected GTVMin to be less than\nthe planned ITV minimal dose and DRx for both IMRT and VMAT plans. The\ndifferences between the expected GTV dose and the ITV dose depended on the\ndelivery technique and planning parameters. Overall, VMAT is less prone to\nmotion interplay than IMRT.",
        "The radiative open circuit voltage loss in a solar cell occurs because the\nabsorptance spectrum near the band gap shows gradual increase rather than sharp\nstep function like transition. This broadening effect has been attributed to\nband gap fluctuations and or to Urbach tails. In this report, we use modelling\nbased on Planck s generalized law to distinguish between these two effects. Our\nresults demonstrate that Urbach tails have only a minimal effect on the\nabsorptance edge broadening and clarify that even an ideal direct semiconductor\nwith no band gap fluctuations shows broadening at the absorptance onset.\nFurthermore, state of the art inorganic thin film solar cells often incorporate\na band gap gradient across their thickness, which can further contribute to\nabsorptance broadening. Using Cu(In,Ga)Se2 (CIGSe) absorbers as a case study,\nwe perform a comprehensive analysis of voltage losses through absolute\nphotoluminescence and electroluminescence spectroscopy, combined with\nphotospectrometry and high-spatial-resolution cathodoluminescence measurements.\nWe find that the loss analysis based on the combination of radiative,\ngeneration and non-radiative losses is complete. Samples with a graded band gap\nprofile show more pronounced broadening of the absorptance onset and up to 16\nmV higher radiative losses compared to the samples with uniform band gap. There\nis indication, that band gap-graded samples also have larger lateral band gap\ninhomogeneity.",
        "In the framework of quantum field theory, we analyze the neutrino\noscillations in the presence of a torsion background. We consider the Einstein\nCartan theory and we study the cases of constant torsion and of linearly\ntime-dependent torsion. We derive new neutrino oscillation formulae which\ndepend on the spin orientation and the CP asymmetry formula. Experiment such as\nPTOLEMY which analyzes the cosmological background of neutrino, can provide\ninsights into the effect shown here.",
        "We give a strengthening of the classical Khintchine inequality between the\nsecond and the $p$-th moment for $p \\ge 3$ with optimal constant by adding a\ndeficit depending on the vector of coefficients of the Rademacher sum.",
        "Let $p$ be an odd prime, and suppose that $G$ is a $p$-solvable group and\n$\\varphi\\in {\\rm IBr}(G)$ has vertex $Q$. In 2011, Cossey, Lewis and Navarro\nproved that the number of lifts of $\\varphi$ is at most $|Q:Q'|$ whenever $Q$\nis normal in $G$. In this paper, we present an explicit description of the set\nof lifts of $\\varphi$ with a given vertex pair $(Q,\\delta)$ under a weaker\ncondition on $Q$, and thus generalize their result.",
        "In the analysis of many synthetic aperture radar (SAR) experiments the\npossibility of passive background signals being recorded simultaneously and\ncorrupting the image is often overlooked. Our work addresses this by\nconsidering the multistatic experiment where two stationary emitters are\n\"always on\" so there is \"crosstalk\" between their signals. The model for the\nradar data is given by a Fourier integral operator, and we assume that the data\ncannot be separated into contributions from individual emitters. Using\ntechniques of microlocal analysis, we show that \"crosstalk\" between emitters\nleads to artifacts in the image and we determine their locations relative to\nthe scatterers that produced the data.\n  To combat the harmful effects of crosstalk, we develop methods that allow us\nto create an image of a region of interest (ROI) that is free from such\nartifacts. The first method makes use of a carefully designed data acquisition\ngeometry to localise artifacts away from a ROI, and the second is an image\nprocessing technique that displaces artifacts away from a ROI. These methods\nare verified via numerical implementation in MATLAB. The analysis carried out\nhere is valuable in bistatic and multistatic radar experiments, where an\nunwanted, passive source is also being detected, as well as in passive imaging,\nwhere one wishes to produce a high-quality image purely from uncontrolled\nsources of illumination.",
        "Ultra-weak photon emission (UPE) is a noninvasive diagnostic tool that\neffectively reflects the function and health status of plant cells. However,\ncurrent UPE measurement techniques are limited by resolution and sensitivity,\nparticularly when monitoring different plant species and stress types. This\nstudy analyzes the delayed luminescence (DL) properties of Hydrocotyle\nvulgaris, Arabidopsis leaves, and Ginkgo leaves under both stress and control\nconditions using an independently developed UPE imaging system. The results\nshowed a significant increase in initial DL intensity and an accelerated\noxidative metabolic rate under mechanical injury and oxidative stress. DL decay\ncharacteristics were significantly correlated with the plant's physiological\nstate, with stress conditions exhibiting decay curves that closely matched\ntheoretical models. These findings confirm the established correlation between\nDL and plant stress responses. The high-resolution, low-noise imaging system\nsignificantly improves the accuracy of plant physiological state monitoring and\nprovides new insights into the potential of optical signals for non-chemical\ncommunication research and agricultural applications. This technology has great\npotential for monitoring plant growth, assessing environmental stress, and\nsupporting precision agriculture.",
        "In this paper, we prove that any Artinian complete intersection homogeneous\nideal $I$ in $K[x_0,\\cdots,x_n]$ generated by $n+1$ forms of degree $d\\ge 2$\nsatisfies the weak Lefschetz property (WLP) in degree $t< d+\\lceil \\frac{d}{n}\n\\rceil$. As a consequence, we get that the Jacobian ideal of a smooth 3-fold of\ndegree $d\\ge 7$ in ${\\mathbb P}^4$ satisfies the weak Lefschetz property in\ndegree $d$, answering a recent question of Beauville.",
        "Nucleic acid sensing is crucial for advancing diagnostics, therapeutic\nmonitoring and molecu-lar biology research, by enabling the precise\nidentification of DNA and RNA interactions. Here, we present an innovative\nsensing platform based on DNA-functionalized whispering gallery mode (WGM)\nmicrolasers. By correlating spectral shifts in laser emission to changes in\nrefractive index, we demonstrate real-time detection of DNA hybridization and\nstructural changes. The addition of gold nanoparticles to the DNA strands\nsignificantly enhances sensi-tivity, and labeling exclusively the sensing\nstrand or a hairpin strand eliminates the need for secondary labeling of the\ntarget strand. We further show that ionic strength influences DNA compactness,\nand we introduce a hairpin-based system as a dual-purpose sensor and\ncon-trolled release mechanism for potential drug delivery. This versatile\nWGM-based platform of-fers promise for sequence-specific nucleic acid sensing,\nmultiplexed detection, and in vivo ap-plications in diagnostics and cellular\nresearch."
      ]
    }
  },
  {
    "id":2411.1742,
    "research_type":"applied",
    "start_id":"b17",
    "start_title":"Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks",
    "start_abstract":"Image-to-image translation is a class of vision and graphics problems where the goal to learn mapping between an input image output using training set aligned pairs. However, for many tasks, paired data will not be available. We present approach learning translate from source domain X target Y in absence examples. Our G : \u2192 such that distribution images G(X) indistinguishable adversarial loss. Because this highly under-constrained, we couple it with inverse F introduce cycle consistency loss push F(G(X)) \u2248 (and vice versa). Qualitative results are presented on several tasks does exist, including collection style transfer, object transfiguration, season photo enhancement, etc. Quantitative comparisons against prior methods demonstrate superiority our approach.",
    "start_categories":[
      "cs.CV",
      "cs.NE"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b19"
      ],
      "title":[
        "Bidirectional Mapping of Brain MRI and PET With 3D Reversible GAN for the Diagnosis of Alzheimer\u2019s Disease"
      ],
      "abstract":[
        "<jats:p>Combining multi-modality data for brain disease diagnosis such as Alzheimer\u2019s disease (AD) commonly leads to improved performance than those using a single modality. However, it is still challenging to train a multi-modality model since it is difficult in clinical practice to obtain complete data that includes all modality data. Generally speaking, it is difficult to obtain both magnetic resonance images (MRI) and positron emission tomography (PET) images of a single patient. PET is expensive and requires the injection of radioactive substances into the patient\u2019s body, while MR images are cheaper, safer, and more widely used in practice. Discarding samples without PET data is a common method in previous studies, but the reduction in the number of samples will result in a decrease in model performance. To take advantage of multi-modal complementary information, we first adopt the Reversible Generative Adversarial Network (RevGAN) model to reconstruct the missing data. After that, a 3D convolutional neural network (CNN) classification model with multi-modality input was proposed to perform AD diagnosis. We have evaluated our method on the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) database, and compared the performance of the proposed method with those using state-of-the-art methods. The experimental results show that the structural and functional information of brain tissue can be mapped well and that the image synthesized by our method is close to the real image. In addition, the use of synthetic data is beneficial for the diagnosis and prediction of Alzheimer\u2019s disease, demonstrating the effectiveness of the proposed framework.<\/jats:p>"
      ],
      "categories":[
        "q-bio.NC"
      ]
    },
    "list":{
      "title":[
        "2D Surface Brightness Modelling of Large 2MASS Galaxies II: The Role of\n  Classical Bulges and Pseudobulges on Galaxy Scaling Relations and its\n  implication for Supermassive Black Hole Formation",
        "Feasibility of short blocklength Reed-Muller codes for physical layer\n  security in real environment",
        "Compact superconducting vacuum-gap capacitors with low microwave loss\n  and high mechanical coherence for scalable quantum circuits",
        "Probabilistic Shielding for Safe Reinforcement Learning",
        "Bayesian mixture modeling using a mixture of finite mixtures with\n  normalized inverse Gaussian weights",
        "Fitting multivariate Hawkes processes to interval count data with an\n  application to terrorist activity modelling -- a particle Markov chain Monte\n  Carlo approach",
        "Tuning topologically nontrivial states in the BHT-Ni metal organic\n  framework",
        "The R Package WMAP: Tools for Causal Meta-Analysis by Integrating\n  Multiple Observational Studies",
        "Operator-isomorphism pairs and Zak transform methods for the study of\n  Gabor systems",
        "Change Point Detection for Random Objects with Possibly Periodic\n  Behavior",
        "Enhancing energy transport utilising permanent molecular dipoles",
        "Continuum-wise hyperbolicity and periodic points",
        "Limits of nonlinear and dispersive fiber propagation for photonic\n  extreme learning",
        "Enhancement of Superconductivity in WP via Oxide-Assisted Chemical Vapor\n  Transport",
        "Structure factors and quantum geometry in multiband BCS superconductors",
        "Where are the earliest stars relics in the simulated Milky Way\n  analogues?",
        "Mean value of cubic $L$-funcitons with fixed genus",
        "Peak splitting and bias fields in ferroelectric hafnia mediated by\n  interface charge effects",
        "A new and flexible class of sharp asymptotic time-uniform confidence\n  sequences",
        "SPYGLASS. VI. Feedback-Driven Star Formation in the Circinus Complex",
        "Volumetric modulated arc therapy or step-shoot IMRT? A 4D dosimetry\n  study of motion effect in lung SBRT using a dynamic virtual patient model",
        "The effect of a band gap gradient on the radiative losses in the open\n  circuit voltage of solar cells",
        "The effect of spacetime torsion on neutrino mixing",
        "Stability of Khintchine inequalities with optimal constants between the\n  second and the $p$-th moment for $p \\ge 3$",
        "Counting lifts of irreducible Brauer characters",
        "Mitigation of Artifacts in Multistatic & Passive Radar Imaging Using\n  Microlocal Analysis",
        "High-Sensitivity Imaging and Modeling of Ultra-Weak Photon Emission in\n  Plants Under Stress",
        "Weak Lefschetz property of equigenerated complete intersections.\n  Applications",
        "DNA Sensing with Whispering Gallery Mode Microlasers"
      ],
      "abstract":[
        "We have generated 2D-multicomponent surface brightness (SB) modelling for 100\ngalaxies in the Large Galaxy Atlas (LGA) together with 19 nearby cD galaxies\nusing the near-infrared (NIR) images from 2MASS (J, H and Ks ). Our final\nsample of 119 galaxies includes cD galaxies, Virgo cluster galaxies, group\ngalaxies, and field galaxies. We revisited known scaling relations (SRs)\ninvolving structural parameters, as well as those involving supermassive black\nholes (SMBHs) and ultramassive black holes (UMBHs). Refining the SRs, we also\nrevisited the bulge classification and considered the Fundamental Plane (FP)\nand its projections, as well as other SRs, such as the colour-magnitude\nrelation (CMR), Tully-Fisher relation (TFR) and luminosity concentration\nrelation (LCR). Classical bulges follow the same relations as elliptical\ngalaxies, while pseudobulges are usually outliers. The NIR colours of classical\nbulges and pseudobulges indicate that their ages are not radically different\ndespite their spread in luminosity, but we noticed that classical bulges are\nmore luminous than pseudobulges, therefore, this property provides a\ncomplementary bulge classification criterion. We included pseudobulges from\nother studies to strengthen the tendencies seen for pseudobulges in our sample.\nFrom the SRs for BHs, we found that pseudobulges do not follow SRs for\nearly-type galaxies and classical bulges. Additionally, the lack of correlation\nbetween BHs and discs may indicate these structures have not coevolved. From\nthe revision of SRs, we present a sample of galaxies likely to host SMBHs or\nUMBHs, which are suitable for dynamical BH mass determination from the ground.",
        "In this paper, we investigate the application of Reed-Muller (RM) codes for\nPhysical-layer security in a real world wiretap channel scenario. Utilizing\nsoftware-defined radios (SDRs) in a real indoor environment, we implement a\ncoset coding scheme that leverages the hierarchical structure of RM codes to\nsecure data transmission. The generator matrix of the RM code is used to\npartition codewords into cosets in the usual way, where each message\ncorresponds to a unique coset, and auxiliary bits select specific codewords\nwithin each coset. This approach enables the legitimate receiver (Bob) can\ndecode the transmitted message with minimal information leakage to eavesdropper\n(Eve) thus protecting the confidentiality of the communication with the help of\ncoset structure. Mutual information neural estimation (MINE) is used to\nquantify information leakage and validate the effectiveness of the scheme.\nExperimental results indicate that RM codes can achieve robust security even in\npractical environments affected by real-world channel impairments. These\nfindings demonstrate the potential of RM codes as an efficient solution for\nphysical-layer security, particularly for applications that require low latency\nand short blocklengths.",
        "Vacuum gap capacitors have recently gained considerable attention in\nsuperconducting circuit platforms due to their compact design and low\ndielectric losses in the microwave regime. Their ability to support mechanical\nvibrational modes makes them ideal candidates for circuit optomechanics.\nHowever, precise control of gap size and achieving high coherence in mechanical\nmodes remain long-standing challenges. Here, we present a detailed fabrication\nprocess for scalable vacuum gap capacitors that support ultra-high-coherence\nmechanical motion, exhibit low microwave loss, and maintain a small footprint\ncompared to planar geometries. We fabricate arrays of up to 24 LC resonators,\nwith capacitors featuring nanometer-scale gap size variations. We demonstrate\nthat the mechanical quality factors can reach up to $40 \\times 10^6$, a\n100-fold improvement over other platforms, with microwave quality factors\n$\\mathcal{O}(10^5)$ at low photon number levels. This platform also achieves a\nsizable single-photon optomechanical coupling rate of approximately 20 Hz.\nUsing this, we cooled the mechanical oscillator to its ground state (0.07\nquanta) and squeezed its motion below the vacuum level by 2.7 dB. We further\ndemonstrate the scalability of this platform by implementing large-scale\noptomechanical arrays, a strained graphene model, and observing quantum\ncollective phenomena in a mechanical hexamer. These vacuum gap capacitors are\npromising candidates for coupling superconducting qubits with mechanical\nsystems, serving as storage elements in quantum computing, and exploring\ngravitational effects on quantum mechanics.",
        "In real-life scenarios, a Reinforcement Learning (RL) agent aiming to\nmaximise their reward, must often also behave in a safe manner, including at\ntraining time. Thus, much attention in recent years has been given to Safe RL,\nwhere an agent aims to learn an optimal policy among all policies that satisfy\na given safety constraint. However, strict safety guarantees are often provided\nthrough approaches based on linear programming, and thus have limited scaling.\nIn this paper we present a new, scalable method, which enjoys strict formal\nguarantees for Safe RL, in the case where the safety dynamics of the Markov\nDecision Process (MDP) are known, and safety is defined as an undiscounted\nprobabilistic avoidance property. Our approach is based on state-augmentation\nof the MDP, and on the design of a shield that restricts the actions available\nto the agent. We show that our approach provides a strict formal safety\nguarantee that the agent stays safe at training and test time. Furthermore, we\ndemonstrate that our approach is viable in practice through experimental\nevaluation.",
        "In Bayesian inference for mixture models with an unknown number of\ncomponents, a finite mixture model is usually employed that assumes prior\ndistributions for mixing weights and the number of components. This model is\ncalled a mixture of finite mixtures (MFM). As a prior distribution for the\nweights, a (symmetric) Dirichlet distribution is widely used for conjugacy and\ncomputational simplicity, while the selection of the concentration parameter\ninfluences the estimate of the number of components. In this paper, we focus on\nestimating the number of components. As a robust alternative Dirichlet weights,\nwe present a method based on a mixture of finite mixtures with normalized\ninverse Gaussian weights. The motivation is similar to the use of normalized\ninverse Gaussian processes instead of Dirichlet processes for infinite mixture\nmodeling. Introducing latent variables, the posterior computation is carried\nout using block Gibbs sampling without using the reversible jump algorithm. The\nperformance of the proposed method is illustrated through some numerical\nexperiments and real data examples, including clustering, density estimation,\nand community detection.",
        "Terrorist activities often exhibit temporal and spatial clustering, making\nthe multivariate Hawkes process (MHP) a useful statistical model for analysing\nterrorism across different geographic regions. However, terror attack data from\nthe Global Terrorism Database is reported as total event counts in disjoint\nobservation periods, with precise event times unknown. When the MHP is only\nobserved discretely, the likelihood function becomes intractable, hindering\nlikelihood-based inference. To address this, we design an unbiased estimate of\nthe intractable likelihood function using sequential Monte Carlo (SMC) based on\na representation of the unobserved event times as latent variables in a\nstate-space model. The unbiasedness of the SMC estimate allows for its use in\nplace of the true likelihood in a Metropolis-Hastings algorithm, from which we\nconstruct a Markov Chain Monte Carlo sample of the distribution over the\nparameters of the MHP. Using simulated data, we assess the performance of our\nmethod and demonstrate that it outperforms an alternative method in the\nliterature based on mean squared error. Terrorist activity in Afghanistan and\nPakistan from 2018 to 2021 is analysed based on daily count data to examine the\nself- and cross-excitation effects of terrorism events.",
        "Using first principles calculations, we have demonstrated the creation of\nmultiple quantum states, in the experimentally accessible metal organic\nframework BHT-Ni. Specifically, quantum spin Hall and quantum anomalous Hall\nstates are induced by two and four electron doping, respectively. The\ngeometrical symmetry breaking, is also investigated. For a low electron doping\nconcentration of two electrons per unit cell, the Fermi energy shifts to a\nnontrivial band gap, between Dirac bands and a quantized spin Hall conductivity\nis predicted. Subsequently in a high electron doping concentration, Anomalous\nHall conductivity with a quantized value was observed. In addition, for\ncentrosymmetric (trans-like) and non-centrosymmetric (cis-like) structures, we\nfound that the trans-like structure preserves quantum spin Hall and quantized\nspin Hall conductivity. In contrast, in the cis-like structure, space inversion\nsymmetry breaking leads to the appearance of valley Hall effect and the\ndisappearance of spin Hall conductivity.",
        "Integrating multiple observational studies for meta-analysis has sparked much\ninterest. The presented R package WMAP (Weighted Meta-Analysis with\nPseudo-Population) addresses a critical gap in the implementation of\nintegrative weighting approaches for multiple observational studies and causal\ninferences about various groups of subjects, such as disease subtypes. The\npackage features three weighting approaches, each representing a special case\nof the unified weighting framework introduced by Guha and Li (2024), which\nincludes an extension of inverse probability weights for data integration\nsettings. It performs meta-analysis on user-inputted datasets as follows: (i)\nit first estimates the propensity scores for study-group combinations,\ncalculates subject balancing weights, and determines the effective sample size\n(ESS) for a user-specified weighting method; and (ii) it then estimates various\nfeatures of multiple counterfactual group outcomes, such as group medians and\ndifferences in group means for the mRNA expression of eight genes.\nAdditionally, bootstrap variability estimates are provided. Among the\nimplemented weighting methods, we highlight the FLEXible, Optimized, and\nRealistic (FLEXOR) method, which is specifically designed to maximize the ESS\nwithin the unified framework. The use of the software is illustrated by\nsimulations as well as a multi-site breast cancer study conducted in seven\nmedical centers.",
        "We collect and summarize results on the unitary equivalence of Gabor systems\nby pairs of unitary operators and global isometries. The methods are then used\nto study Gabor systems with Hermite functions. We provide new proofs of some\nknown results and an outlook on double over-sampling.",
        "Time-varying random objects have been increasingly encountered in modern data\nanalysis. Moreover, in a substantial number of these applications, periodic\nbehavior of the random objects has been observed. We introduce a new, powerful\nscan statistic and corresponding test for the precise identification and\nlocalization of abrupt changes in the distribution of non-Euclidean random\nobjects with possibly periodic behavior. Our approach is nonparametric and\neffectively captures the entire distribution of these random objects.\nRemarkably, it operates with minimal tuning parameters, requiring only the\nspecification of cut-off intervals near endpoints, where change points are\nassumed not to occur. Our theoretical contributions include deriving the\nasymptotic distribution of the test statistic under the null hypothesis of no\nchange points, establishing the consistency of the test in the presence of\nchange points under contiguous alternatives and providing rigorous guarantees\non the near-optimal consistency in estimating the number and locations of\nchange points, whether dealing with a single change point or multiple ones. We\ndemonstrate that the most competitive method currently in the literature for\nchange point detection in random objects is degraded by periodic behavior, as\nperiodicity leads to blurring of the changes that this procedure aims to\ndiscover. Through comprehensive simulation studies, we demonstrate the superior\npower and accuracy of our approach in both detecting change points and\npinpointing their locations, across scenarios involving both periodic and\nnonperiodic random objects. Our main application is to weighted networks,\nrepresented through graph Laplacians. The proposed method delivers highly\ninterpretable results, as evidenced by the identification of meaningful change\npoints in the New York City Citi Bike sharing system that align with\nsignificant historical events.",
        "We study exciton quantum transfer along a molecular chain whilst accounting\nfor the effects of permanent dipoles that are induced by charge displacements\nin the molecular orbitals. These effects are typically neglected as they do not\narise in atomic quantum optics; however, they can play an important role in\nmolecular systems. We also consider novel collective photon-assisted transport\nand compare it against the scaling of phonon-assisted transport in chains\nfeaturing permanent dipoles, and determine a linear scaling with the number of\ndipoles, akin to single-excitation superradiance. We further demonstrate how\npermanent dipoles, dipoles can preferentially arrange energy eigenstates to\nsupport excitation transport. Finally, we show how permanent dipoles can\nenhance the ability of the molecular chain to support excitation transport\ncompared to that of systems that do not possess permanent dipoles across a\nrange of environmental and system configurations.",
        "We prove that cw-hyperbolic homeomorphisms with jointly continuous\nstable\/unstable holonomies satisfy the periodic shadowing property and, if they\nare topologically mixing, the periodic specification property. We discuss\ndifficulties to adapt Bowen's techniques to obtain a measure of maximal entropy\nfor cw-hyperbolic homeomorphisms, exhibit the unique measure of maximal entropy\nfor Walter's pseudo-Anosov diffeomorphism of $\\mathbb{S}^2$, and prove it can\nbe obtained, as in the expansive case, as the weak* limit of an average of\nDirac measures on periodic orbits. As an application, we exhibit the unique\nmeasure of maximal entropy for the homeomorphism on the Sierpi\\'nski Carpet\ndefined in [12], which does not satisfy the specification property.",
        "We report a generalized nonlinear Schr\\\"odinger equation simulation model of\nan extreme learning machine (ELM) based on optical fiber propagation. Using\nhandwritten digit classification as a benchmark, we study how accuracy depends\non propagation dynamics, as well as parameters governing spectral encoding,\nreadout, and noise. Test accuracies of over 91% and 93% are found for\npropagation in the anomalous and normal dispersion regimes respectively. Our\nsimulation results also suggest that quantum noise on the input pulses\nintroduces an intrinsic penalty to ELM performance.",
        "Tungsten monophosphide (WP) has been reported to superconduct below 0.8 K,\nand theoretical work has predicted an unconventional Cooper pairing mechanism.\nHere we present data for WP single crystals grown by means of chemical vapor\ntransport (CVT) of WO3, P, and I2. In comparison to synthesis using WP powder\nas a starting material, this technique results in samples with substantially\ndecreased low-temperature scattering and favors a more three dimensional\nmorphology. We also find that the resistive superconducting transitions in\nthese samples begin above 1 K. Variation in Tc is often found in strongly\ncorrelated superconductors, and its presence in WP could be the result of\ninfluence from a competing order and\/or a non s-wave gap.",
        "We consider multiband BCS superconductors that exhibit time-reversal symmetry\nand uniform pairing, and analyze their dynamic density and spin structure\nfactors using linear-response theory within the mean-field BCS-BEC crossover\nframework at zero temperature. Our results for the multi-orbital Hubbard model\nsatisfy the associated f-sum rules in several limits. In particular, in the\nstrong-coupling limit, they coincide with those of a weakly-interacting Bose\ngas of Cooper pairs, where the low-energy collective Goldstone modes serve as\nBogoliubov phonons. We further reveal that the quantum-geometric origin of the\nlow-energy structure factors, along with related observables such as the\nsuperfluid-weight tensor and the effective-mass tensor of Cooper pairs, can be\ntraced all the way back to the effective-mass theorem for Bloch bands in this\nlimit. As an illustration, we investigate the pyrochlore-Hubbard model\nnumerically and demonstrate that the Goldstone modes are the only relevant\ncollective degrees of freedom in the flat-band regime.",
        "Using 6 Milky Way analogues with two different numerical resolutions from the\nAuriga simulation, we investigate the total mass, spatial distribution and\nkinematics of the earliest stars relics in the Milky Way at $z=0$. These relics\n(second generation stars) formed over a wide redshift range, from about $z=22$\nto $z=4$, with an average formation redshift of $z \\sim 10.0$, and comprise\nabout $2\\times10^{-5}$ of the entire galactic stellar population. The disk and\nbulge components host only a small fraction of these relics, contributing less\nthan $12$ percent in total. The stellar halo, in particular the outer stellar\nhalo of which galactic radius $r>30$ kpc, hosts the largest fraction (about 46\npercent on average), with an average of one relic star for per $4,000$ to\n$10,000$ stars, making it a promising region for observational searches.\nAdditionally, around $18$ percent of the earliest stars relics are found in\nsatellite galaxies, with smaller and older satellite galaxies tending to\ncontain a higher proportion of these stars. Thus, low-mass and early-formed\nsatellite galaxies are also ideal targets for finding such relics, although\nsome satellite galaxies may lack them entirely. The spatial distribution and\nkinematics of these stars show good numerical convergence across different\nsimulation resolutions. Our results provide valuable guidance for searches of\nthe earliest stars relics and offer insights for interpreting findings from\nongoing and future stellar archaeology surveys.",
        "We investigate the mean value of the first moment of primitive cubic\n$L$-functions over $\\mathbb{F}_q(T)$ in the non-Kummer setting. Specifically,\nwe study the sum\n  \\begin{equation*}\n  \\sum_{\\substack{\\chi\\ primitive\\ cubic\\\\ genus(\\chi)=g}}L_q(\\frac{1}{2},\n\\chi),\n  \\end{equation*} where $L_q(s,\\chi)$ denotes the $L$-function associated with\nprimitive cubic character $\\chi$. Using double Dirichlet series, we derive an\nerror term of size $q^{(\\frac{7}{8}+\\varepsilon)g}$.",
        "The pristine state of hafnium based ferroelectric devices exhibits various\nunwanted properties, such as imprint and peak splitting, which diminish with\nbipolar cycling. The incorporation of a niobium oxide layer at different\npositions in metal-ferroelectric-metal and metal-ferroelectric-insulator-metal\nstacks is used to modify the pristine state of the device. X-ray photoelectron\nspectroscopy and transmission electron microscopy measurements are used to\ninvestigate the influence of niobium oxide on the zirconium hafnium oxide\nlayer. It is hypothesized that the charged vacancies generated by the\nintroduced niobium oxide in the adjacent zirconium hafnium oxide layer result\nin an electric bias field that influences the pristine polarization state of\nthe domains. A comparison of different stacks shows that peak splitting in the\npristine state is most likely related to the formation of opposing electric\nbias fields in upwards and downwards polarized domains. Furthermore, the\nincorporation of niobium oxide in the zirconium hafnium oxide\/aluminum oxide\ncapacitor stack in between the ferroelectric and insulating layer leads to a\npeak splitting free device without imprint, which could be explained by the\nincreased influence of charge trapping near the zirconium hafnium\noxide-\/niobium oxide and niobium oxide-\/aluminum oxide interfaces.",
        "Confidence sequences are anytime-valid analogues of classical confidence\nintervals that do not suffer from multiplicity issues under optional\ncontinuation of the data collection. As in classical statistics, asymptotic\nconfidence sequences are a nonparametric tool showing under which high-level\nassumptions asymptotic coverage is achieved so that they also give a certain\nrobustness guarantee against distributional deviations. In this paper, we\npropose a new flexible class of confidence sequences yielding sharp asymptotic\ntime-uniform confidence sequences under mild assumptions. Furthermore, we\nhighlight the connection to corresponding sequential testing problems and\ndetail the underlying limit theorem.",
        "Young associations provide a record that traces the star formation process,\nand the youngest populations connect progenitor gas dynamics to the resulting\nstellar populations. We therefore conduct the first comprehensive overview of\nthe Circinus Complex, an under-studied and massive ($\\sim$1500 M$_{\\odot}$)\nregion consisting of approximately 3100 recently formed stars alongside the\nCircinus Molecular Cloud (CMC). We find a clear age pattern in the contiguous\ncentral region (CirCe), where younger stars are found further from the massive\ncentral cluster, and where the velocities are consistent with uniform\nexpansion. By comparing this structure to an analogous STARFORGE simulation, we\nfind that the age structure and dynamics of the association are consistent with\nstar formation in two stages: the global collapse of the parent cloud that\nbuilds the $500 M_{\\odot}$ central cluster ASCC 79, followed by triggered star\nformation in a shell swept up after the first massive stars form. We also find\nthat filaments with a range of distances from the central cluster can naturally\nproduce multi-generational age sequences due to differences in feedback\nstrength and exposure. Outlying populations show velocities consistent with\nformation independent from the CirCe region, but with similar enough velocities\nthat they may be difficult to distinguish from one another later in their\nexpansion. We therefore provide a new alternative view of sequential star\nformation that relies on feedback from a single central cluster rather than the\nmultiple sequential generations that are traditionally invoked, while also\nproviding insight into the star formation history of older populations.",
        "Purpose: To investigate the impact of delivery techniques and planning\nparameters on interplay effect in lung SBRT.\n  Methods: A dynamic virtual patient model containing normal structures and a\ntumor with adjustable sizes, locations, and 3D breathing motion was utilized.\nSBRT plans were developed using both step-and-shoot IMRT and VMAT with\ndifferent planning parameters (energy, isocenter location, PTV margin, and PTV\ndose heterogeneity). 4D doses were calculated by simulating synchronized\ndelivery of SBRT to the virtual patient model with random initial positions of\ntumor motion. The expected dose (average) and the standard deviation of the 4D\ndoses were obtained. The relative difference between the expected GTV\nminimal\/mean (GTVMin\/GTVMean) dose and the planned ITVMin\/ITVMean dose (denoted\nby %E\/P), and between the GTVMin and the prescription dose (DRx) were computed.\n  Results: The %E\/P for GTVMean was significantly lower for IMRT than VMAT\n(0.5% +\/- 7.7% v.s. 3.5% +\/- 5.0%, p=0.04). The expected GTVMin was lower than\nDRx in 9.4% of all IMRT plans versus 3.1% in VMAT. The worst-case scenario, 4D\nGTVMin was 14.1% lower than the ITVMin. Choices of PTV margin or dose\nheterogeneity to be achieved in PTV can result in significant difference\n(p<0.05) in motion interplay depending on delivery techniques.\n  Conclusion: Motion interplay may cause the expected GTVMin to be less than\nthe planned ITV minimal dose and DRx for both IMRT and VMAT plans. The\ndifferences between the expected GTV dose and the ITV dose depended on the\ndelivery technique and planning parameters. Overall, VMAT is less prone to\nmotion interplay than IMRT.",
        "The radiative open circuit voltage loss in a solar cell occurs because the\nabsorptance spectrum near the band gap shows gradual increase rather than sharp\nstep function like transition. This broadening effect has been attributed to\nband gap fluctuations and or to Urbach tails. In this report, we use modelling\nbased on Planck s generalized law to distinguish between these two effects. Our\nresults demonstrate that Urbach tails have only a minimal effect on the\nabsorptance edge broadening and clarify that even an ideal direct semiconductor\nwith no band gap fluctuations shows broadening at the absorptance onset.\nFurthermore, state of the art inorganic thin film solar cells often incorporate\na band gap gradient across their thickness, which can further contribute to\nabsorptance broadening. Using Cu(In,Ga)Se2 (CIGSe) absorbers as a case study,\nwe perform a comprehensive analysis of voltage losses through absolute\nphotoluminescence and electroluminescence spectroscopy, combined with\nphotospectrometry and high-spatial-resolution cathodoluminescence measurements.\nWe find that the loss analysis based on the combination of radiative,\ngeneration and non-radiative losses is complete. Samples with a graded band gap\nprofile show more pronounced broadening of the absorptance onset and up to 16\nmV higher radiative losses compared to the samples with uniform band gap. There\nis indication, that band gap-graded samples also have larger lateral band gap\ninhomogeneity.",
        "In the framework of quantum field theory, we analyze the neutrino\noscillations in the presence of a torsion background. We consider the Einstein\nCartan theory and we study the cases of constant torsion and of linearly\ntime-dependent torsion. We derive new neutrino oscillation formulae which\ndepend on the spin orientation and the CP asymmetry formula. Experiment such as\nPTOLEMY which analyzes the cosmological background of neutrino, can provide\ninsights into the effect shown here.",
        "We give a strengthening of the classical Khintchine inequality between the\nsecond and the $p$-th moment for $p \\ge 3$ with optimal constant by adding a\ndeficit depending on the vector of coefficients of the Rademacher sum.",
        "Let $p$ be an odd prime, and suppose that $G$ is a $p$-solvable group and\n$\\varphi\\in {\\rm IBr}(G)$ has vertex $Q$. In 2011, Cossey, Lewis and Navarro\nproved that the number of lifts of $\\varphi$ is at most $|Q:Q'|$ whenever $Q$\nis normal in $G$. In this paper, we present an explicit description of the set\nof lifts of $\\varphi$ with a given vertex pair $(Q,\\delta)$ under a weaker\ncondition on $Q$, and thus generalize their result.",
        "In the analysis of many synthetic aperture radar (SAR) experiments the\npossibility of passive background signals being recorded simultaneously and\ncorrupting the image is often overlooked. Our work addresses this by\nconsidering the multistatic experiment where two stationary emitters are\n\"always on\" so there is \"crosstalk\" between their signals. The model for the\nradar data is given by a Fourier integral operator, and we assume that the data\ncannot be separated into contributions from individual emitters. Using\ntechniques of microlocal analysis, we show that \"crosstalk\" between emitters\nleads to artifacts in the image and we determine their locations relative to\nthe scatterers that produced the data.\n  To combat the harmful effects of crosstalk, we develop methods that allow us\nto create an image of a region of interest (ROI) that is free from such\nartifacts. The first method makes use of a carefully designed data acquisition\ngeometry to localise artifacts away from a ROI, and the second is an image\nprocessing technique that displaces artifacts away from a ROI. These methods\nare verified via numerical implementation in MATLAB. The analysis carried out\nhere is valuable in bistatic and multistatic radar experiments, where an\nunwanted, passive source is also being detected, as well as in passive imaging,\nwhere one wishes to produce a high-quality image purely from uncontrolled\nsources of illumination.",
        "Ultra-weak photon emission (UPE) is a noninvasive diagnostic tool that\neffectively reflects the function and health status of plant cells. However,\ncurrent UPE measurement techniques are limited by resolution and sensitivity,\nparticularly when monitoring different plant species and stress types. This\nstudy analyzes the delayed luminescence (DL) properties of Hydrocotyle\nvulgaris, Arabidopsis leaves, and Ginkgo leaves under both stress and control\nconditions using an independently developed UPE imaging system. The results\nshowed a significant increase in initial DL intensity and an accelerated\noxidative metabolic rate under mechanical injury and oxidative stress. DL decay\ncharacteristics were significantly correlated with the plant's physiological\nstate, with stress conditions exhibiting decay curves that closely matched\ntheoretical models. These findings confirm the established correlation between\nDL and plant stress responses. The high-resolution, low-noise imaging system\nsignificantly improves the accuracy of plant physiological state monitoring and\nprovides new insights into the potential of optical signals for non-chemical\ncommunication research and agricultural applications. This technology has great\npotential for monitoring plant growth, assessing environmental stress, and\nsupporting precision agriculture.",
        "In this paper, we prove that any Artinian complete intersection homogeneous\nideal $I$ in $K[x_0,\\cdots,x_n]$ generated by $n+1$ forms of degree $d\\ge 2$\nsatisfies the weak Lefschetz property (WLP) in degree $t< d+\\lceil \\frac{d}{n}\n\\rceil$. As a consequence, we get that the Jacobian ideal of a smooth 3-fold of\ndegree $d\\ge 7$ in ${\\mathbb P}^4$ satisfies the weak Lefschetz property in\ndegree $d$, answering a recent question of Beauville.",
        "Nucleic acid sensing is crucial for advancing diagnostics, therapeutic\nmonitoring and molecu-lar biology research, by enabling the precise\nidentification of DNA and RNA interactions. Here, we present an innovative\nsensing platform based on DNA-functionalized whispering gallery mode (WGM)\nmicrolasers. By correlating spectral shifts in laser emission to changes in\nrefractive index, we demonstrate real-time detection of DNA hybridization and\nstructural changes. The addition of gold nanoparticles to the DNA strands\nsignificantly enhances sensi-tivity, and labeling exclusively the sensing\nstrand or a hairpin strand eliminates the need for secondary labeling of the\ntarget strand. We further show that ionic strength influences DNA compactness,\nand we introduce a hairpin-based system as a dual-purpose sensor and\ncon-trolled release mechanism for potential drug delivery. This versatile\nWGM-based platform of-fers promise for sequence-specific nucleic acid sensing,\nmultiplexed detection, and in vivo ap-plications in diagnostics and cellular\nresearch."
      ]
    }
  },
  {
    "id":2411.1742,
    "research_type":"applied",
    "start_id":"b19",
    "start_title":"Bidirectional Mapping of Brain MRI and PET With 3D Reversible GAN for the Diagnosis of Alzheimer\u2019s Disease",
    "start_abstract":"<jats:p>Combining multi-modality data for brain disease diagnosis such as Alzheimer\u2019s disease (AD) commonly leads to improved performance than those using a single modality. However, it is still challenging to train a multi-modality model since it is difficult in clinical practice to obtain complete data that includes all modality data. Generally speaking, it is difficult to obtain both magnetic resonance images (MRI) and positron emission tomography (PET) images of a single patient. PET is expensive and requires the injection of radioactive substances into the patient\u2019s body, while MR images are cheaper, safer, and more widely used in practice. Discarding samples without PET data is a common method in previous studies, but the reduction in the number of samples will result in a decrease in model performance. To take advantage of multi-modal complementary information, we first adopt the Reversible Generative Adversarial Network (RevGAN) model to reconstruct the missing data. After that, a 3D convolutional neural network (CNN) classification model with multi-modality input was proposed to perform AD diagnosis. We have evaluated our method on the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) database, and compared the performance of the proposed method with those using state-of-the-art methods. The experimental results show that the structural and functional information of brain tissue can be mapped well and that the image synthesized by our method is close to the real image. In addition, the use of synthetic data is beneficial for the diagnosis and prediction of Alzheimer\u2019s disease, demonstrating the effectiveness of the proposed framework.<\/jats:p>",
    "start_categories":[
      "q-bio.NC"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b3",
        "b17"
      ],
      "title":[
        "Deep learning-based classification of healthy aging controls, mild cognitive impairment and alzheimer's disease using fusion of mri-pet imaging",
        "Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks"
      ],
      "abstract":[
        "Automated detection of dementia stage using multimodal imaging modalities will be helpful for improving the clinical diagnosis. In this study, we develop the Inception-ResNet wrapper model in differentiating the healthy controls (HC), mild cognitive impairment (MCI), and Alzheimer\u2019s disease (AD) using conjoint magnetic resonance imaging (MRI) and positron emission tomography (PET) scans. We use T1-weighted MR and PET images of individuals aged between 42 and 95 years, including HC, MCI and AD patients. We first perform 3D tissue segmentation of MR images after skull striping. The atlas-based segmented MR image tissue is fused with PET image. Then we transform PET images from RGB to HSI color space and apply fusion of MRI with PET images using two-dimensional Fourier and discrete wavelet transform (DWT) and then reconstruct the MR-PET fused image using inverse Fourier and DWT methods. After the fusion of MRI and PET imaging modalities, we used 60 % training, 20 % for validation and the remaining 20 % as a test set using various convolutional neural networks. We found the proposed model as the best classifier with an accuracy of 95.5 %, 94.1 % and 95.9 % in classifying HC vs MCI, MCI vs AD and AD vs HC respectively when compared to the existing methods. We conclude that the proposed deep learning model has potential in automated classification of healthy and dementia stages using combined MRI and PET modalities with good performance.",
        "Image-to-image translation is a class of vision and graphics problems where the goal to learn mapping between an input image output using training set aligned pairs. However, for many tasks, paired data will not be available. We present approach learning translate from source domain X target Y in absence examples. Our G : \u2192 such that distribution images G(X) indistinguishable adversarial loss. Because this highly under-constrained, we couple it with inverse F introduce cycle consistency loss push F(G(X)) \u2248 (and vice versa). Qualitative results are presented on several tasks does exist, including collection style transfer, object transfiguration, season photo enhancement, etc. Quantitative comparisons against prior methods demonstrate superiority our approach."
      ],
      "categories":[
        "cs.CV",
        "cs.NE"
      ]
    },
    "list":{
      "title":[
        "A mathematical perspective on the paradox that chemotherapy sometimes\n  works backwards",
        "An Efficient Quantum Approximate Optimization Algorithm with Fixed\n  Linear Ramp Schedule for Truss Structure Optimization",
        "The COSMOS-Web ring: Spectroscopic confirmation of the background source\n  at z = 5.1",
        "Financial Fraud Detection with Entropy Computing",
        "Survey on Question Answering over Visually Rich Documents: Methods,\n  Challenges, and Trends",
        "Cracking Vector Search Indexes",
        "Fineness and smoothness of a KSBA moduli of marked cubic surfaces",
        "Aspects of Complexity in Quantum Evolutions on the Bloch Sphere",
        "Fluctuation-driven topological Hall effect in room-temperature itinerant\n  helimagnet Fe3Ga4",
        "Invariant and non-invariant almost complex structures on compact\n  quotients of Lie groups",
        "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search",
        "Self-supervised conformal prediction for uncertainty quantification in\n  Poisson imaging problems",
        "Dynamic Manipulation of Multiphase Fluid in Microgravity Using\n  Photoresponsive Surfactant",
        "InfiGUIAgent: A Multimodal Generalist GUI Agent with Native Reasoning\n  and Reflection",
        "EXALT: EXplainable ALgorithmic Tools for Optimization Problems",
        "Disentangled Latent Spaces for Reduced Order Models using Deterministic\n  Autoencoders",
        "Simulation and Harmonic Analysis of k-Space Readout (SHAKER)",
        "Skillful High-Resolution Ensemble Precipitation Forecasting with an\n  Integrated Deep Learning Framework",
        "Efficient Deep Learning-based Forward Solvers for Brain Tumor Growth\n  Models",
        "DICE: Distilling Classifier-Free Guidance into Text Embeddings",
        "Optimal Rebate Design: Incentives, Competition and Efficiency in Auction\n  Markets",
        "220 GHz RIS-Aided Multi-user Terahertz Communication System: Prototype\n  Design and Over-the-Air Experimental Trials",
        "Tomographic Signatures of Interacting Majorana and Andreev States in\n  Superconductor-Semiconductor Transmon Qubits",
        "VerifBFL: Leveraging zk-SNARKs for A Verifiable Blockchained Federated\n  Learning",
        "Symmetry violation-driven hysteresis loops as measurands for\n  noise-resilient sensors",
        "Energy-Conscious LLM Decoding: Impact of Text Generation Strategies on\n  GPU Energy Consumption",
        "Topological-to-Topological Transition Induced by On-Site Nonlinearity in\n  a One-Dimensional Topological Insulator",
        "Using gradient of Lagrangian function to compute efficient channels for\n  the ideal observer"
      ],
      "abstract":[
        "Doctors are well aware that sometimes cancer treatments not only fail, but\neven work backwards, i.e. they make the treated tumor grow. In this work we\npresent a mathematical perspective on this paradox in the case of chemotherapy,\nby studying a minimally parameterized mathematical model for the system\ncomposed of the tumor and the surrounding vasculature. To this end, we will use\na system of two well-established nonlinear ordinary differential equations,\nwhich incorporates the cytotoxic (via the Norton-Simon hypothesis) and\nantiangiogenic effects of chemotherapy. Finally, we provide two theoretical\nways to avoid these anomalies.",
        "This study proposes a novel structural optimization framework based on\nquantum variational circuits, in which the multiplier acting on the\ncross-sectional area of each rod in a truss structure as an updater is used as\na design variable. Specifically, we employ a classical processor for structural\nanalysis with the finite element method, and the Quantum Approximate\nOptimization Algorithm (QAOA) is subsequently performed to update the\ncross-sectional area so that the compliance is minimized. The advantages of\nthis framework can be seen in three key aspects. First, by defining design\nvariables as multipliers, rather than simply reducing the design variable to a\nbinary candidate of inclusion or exclusion (corresponding to qubit states, ``0\"\nand ``1\"), it provides greater flexibility in adjusting the cross-sectional\narea of the rod at each iteration of the optimization process. Second, the\nmultipliers acting on rods are encoded with on-off encoding, eliminating\nadditional constraints in the convergence judgement. As a result, the objective\nfunction is in a simple format, enabling efficient optimization using\nQAOA.Third, a fixed linear ramp schedule (FLRS) for variational parameter\nsetting bypasses the classical optimization process, thereby improving the\noperational efficiency of the framework. In the two structural cases\ninvestigated in this study, the proposed approach highlights the feasibility\nand applicability potential of quantum computing in advancing engineering\ndesign and optimization. Numerical experiments have demonstrated the\neffectiveness of this framework, providing a firm foundation for future\nresearch on quantum-assisted optimization methods in engineering fields.",
        "We report the spectroscopic confirmation of the background source of the most\ndistant Einstein ring known to date, the COSMOS-Web ring. This system consists\nof a complete Einstein ring at $z=5.1$, lensed by a massive early-type galaxy\nat $z\\sim2$. The redshift $z=5.1043\\pm0.0004$ is unambiguously identified with\nour NOEMA and Keck\/MOSFIRE spectroscopy, where the NOEMA observations reveal\nthe CO(4-3) and CO(5-4) lines at $>8\\,\\sigma$, and the MOSFIRE data detect\n[O\\textsc{ii}] at $\\sim 6\\,\\sigma$. Using multi-wavelength photometry spanning\nnear-infrared to radio bands, we find that the lensed galaxy is a dust-obscured\nstarburst ($M_{\\star} \\sim 1.8\\times10^{10}\\,{\\rm M_{\\odot}}$, ${\\rm\nSFR_{IR}\\sim 60\\,{\\rm M_{\\odot}} ~yr^{-1}}$) with high star-formation\nefficiency (gas depletion time $\\tau_{\\rm dep}<100~$Myr) as indicated by the\n[C\\textsc{i}](1-0) non-detection. The redshift confirmation revalidates that\nthe total lens mass budget within the Einstein radius is fully accounted for by\nthe stellar and dark matter components, without the need of modifying the\ninitial mass function or dark matter distribution profile. This work paves the\nway for detailed studies and future follow-ups of this unique lensing system,\nproviding an ideal laboratory for studying mass distribution at $z\\sim2$ and\nphysical conditions of star formation at $z\\sim5$.",
        "We introduce CVQBoost, a novel classification algorithm that leverages early\nhardware implementing Quantum Computing Inc's Entropy Quantum Computing (EQC)\nparadigm, Dirac-3 [Nguyen et. al. arXiv:2407.04512]. We apply CVQBoost to a\nfraud detection test case and benchmark its performance against XGBoost, a\nwidely utilized ML method. Running on Dirac-3, CVQBoost demonstrates a\nsignificant runtime advantage over XGBoost, which we evaluate on\nhigh-performance hardware comprising up to 48 CPUs and four NVIDIA L4 GPUs\nusing the RAPIDS AI framework. Our results show that CVQBoost maintains\ncompetitive accuracy (measured by AUC) while significantly reducing training\ntime, particularly as dataset size and feature complexity increase. To assess\nscalability, we extend our study to large synthetic datasets ranging from 1M to\n70M samples, demonstrating that CVQBoost on Dirac-3 is well-suited for\nlarge-scale classification tasks. These findings position CVQBoost as a\npromising alternative to gradient boosting methods, offering superior\nscalability and efficiency for high-dimensional ML applications such as fraud\ndetection.",
        "The field of visually-rich document understanding, which involves interacting\nwith visually-rich documents (whether scanned or born-digital), is rapidly\nevolving and still lacks consensus on several key aspects of the processing\npipeline. In this work, we provide a comprehensive overview of state-of-the-art\napproaches, emphasizing their strengths and limitations, pointing out the main\nchallenges in the field, and proposing promising research directions.",
        "Retrieval Augmented Generation (RAG) uses vector databases to expand the\nexpertise of an LLM model without having to retrain it. This idea can be\napplied over data lakes, leading to the notion of embeddings data lakes, i.e.,\na pool of vector databases ready to be used by RAGs. The key component in these\nsystems is the indexes enabling Approximated Nearest Neighbor Search (ANNS).\nHowever, in data lakes, one cannot realistically expect to build indexes for\nevery possible dataset. In this paper, we propose an adaptive, partition-based\nindex, CrackIVF, that performs much better than up-front index building.\nCrackIVF starts answering queries by near brute force search and only expands\nas it sees enough queries. It does so by progressively adapting the index to\nthe query workload. That way, queries can be answered right away without having\nto build a full index first. After seeing enough queries, CrackIVF will produce\nan index comparable to the best of those built using conventional techniques.\nAs the experimental evaluation shows, CrackIVF can often answer more than 1\nmillion queries before other approaches have even built the index and can start\nanswering queries immediately, achieving 10-1000x faster initialization times.\nThis makes it ideal when working with cold data or infrequently used data or as\na way to bootstrap access to unseen datasets.",
        "By work of Gallardo-Kerr-Schaffler, it is known that Naruki's\ncompactification of the moduli space of marked cubic surfaces is isomorphic to\nthe normalization of the Koll\\'ar, Shepherd-Barron, and Alexeev\ncompactification parametrizing pairs\n$\\left(S,\\left(\\frac{1}{9}+\\epsilon\\right)D\\right)$, with $D$ the sum of the\n$27$ marked lines on $S$, and their stable degenerations. In the current paper,\nwe show that the normalization assumption is not necessary as we prove that\nthis KSBA compactification is smooth. Additionally, we show it is a fine moduli\nspace. This is done by studying the automorphisms and the\n$\\mathbb{Q}$-Gorenstein obstructions of the stable pairs parametrized by it.",
        "We enhance our quantitative comprehension of the complexity associated with\nboth time-optimal and time sub-optimal quantum Hamiltonian evolutions that\nconnect arbitrary source and target states on the Bloch sphere, as recently\npresented in Nucl. Phys. B1010, 116755 (2025). Initially, we examine each\nunitary Schrodinger quantum evolution selected through various metrics, such as\npath length, geodesic efficiency, speed efficiency, and the curvature\ncoefficient of the corresponding quantum-mechanical trajectory that connects\nthe source state to the target state on the Bloch sphere. Subsequently, we\nevaluate the selected evolutions using our proposed measure of complexity, as\nwell as in relation to the concept of complexity length scale. The choice of\nboth time-optimal and time sub-optimal evolutions, along with the selection of\nsource and target states, enables us to conduct pertinent sanity checks that\nseek to validate the physical relevance of the framework supporting our\nproposed complexity measure. Our research suggests that, in general, efficient\nquantum evolutions possess a lower complexity than their inefficient\ncounterparts. However, it is important to recognize that complexity is not\nsolely determined by length; in fact, longer trajectories that are adequately\ncurved may exhibit a complexity that is less than or equal to that of shorter\ntrajectories with a lower curvature coefficient.",
        "The topological Hall effect (THE) is a hallmark of a non-trivial geometric\nspin arrangement in a magnetic metal, originating from a finite scalar spin\nchirality (SSC). The associated Berry phase is often a consequence of\nnon-coplanar magnetic structures identified by multiple k-vectors. For single-k\nmagnetic structures however with zero SSC, the emergence of a finite\ntopological Hall signal presents a conceptual challenge. Here, we report that a\nfluctuation-driven mechanism involving chiral magnons is responsible for the\nobserved THE in a low-symmetry compound, monoclinic Fe3Ga4. Through neutron\nscattering experiments, we discovered several nontrivial magnetic phases in\nthis system. In our focus is the helical spiral phase at room temperature,\nwhich transforms into a transverse conical state in applied magnetic field,\nsupporting a significant THE signal up to and above room temperature. Our work\noffers a fresh perspective in the search for novel materials with intertwined\ntopological magnetic and transport properties.",
        "In this paper we briefly survey the classical problem of understanding which\nLie algebras admit a complex structure, put in the broader perspective of\nalmost complex structures with special properties. We focus on the different\nbehavior of invariant and non-invariant structures, with a special attention to\ntheir canonical bundle and Kodaira dimension. We provide new examples of\ncomputations of Kodaira dimension of invariant and non-invariant structures.",
        "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index.\n  This paper introduces VSAG, an open-source framework that aims to enhance the\nin production performance of graph-based ANNS algorithms. VSAG has been\ndeployed at scale in the services of Ant Group, and it incorporates three key\noptimizations: (i) efficient memory access: it reduces L3 cache misses with\npre-fetching and cache-friendly vector organization; (ii) automated parameter\ntuning: it automatically selects performance-optimal parameters without\nrequiring index rebuilding; (iii) efficient distance computation: it leverages\nmodern hardware, scalar quantization, and smartly switches to low-precision\nrepresentation to dramatically reduce the distance computation costs. We\nevaluate VSAG on real-world datasets. The experimental results show that VSAG\nachieves the state-of-the-art performance and provides up to 4x speedup over\nHNSWlib (an industry-standard library) while ensuring the same accuracy.",
        "Image restoration problems are often ill-posed, leading to significant\nuncertainty in reconstructed images. Accurately quantifying this uncertainty is\nessential for the reliable interpretation of reconstructed images. However,\nimage restoration methods often lack uncertainty quantification capabilities.\nConformal prediction offers a rigorous framework to augment image restoration\nmethods with accurate uncertainty quantification estimates, but it typically\nrequires abundant ground truth data for calibration. This paper presents a\nself-supervised conformal prediction method for Poisson imaging problems which\nleverages Poisson Unbiased Risk Estimator to eliminate the need for ground\ntruth data. The resulting self-calibrating conformal prediction approach is\napplicable to any Poisson linear imaging problem that is ill-conditioned, and\nis particularly effective when combined with modern self-supervised image\nrestoration techniques trained directly on measurement data. The proposed\nmethod is demonstrated through numerical experiments on image denoising and\ndeblurring; its performance are comparable to supervised conformal prediction\nmethods relying on ground truth data.",
        "Control of bubble motion is essential for improving efficiency and creating\nnew functionalities in electrochemistry, heat transfer, and biomedical systems.\nPhotoresponsive surfactants enable bubble manipulation by creating surface\ntension gradients, inducing a photo-Marangoni flow under illumination, without\nneeding any engineered substrates, by leveraging a reversible switch in\nmolecular conformation. Although previous studies have demonstrated bubble\nmanipulation using photo-responsive surfactants, a comprehensive understanding\nof how fluid behavior is affected by critical parameters, such as bubble size,\nillumination, photo-switching kinetics, concentration, and adsorption\ndesorption kinetics, remains elusive. Advances have been limited by the complex\nmultiphysics processed involved, and by the fact that earth-bound experiments\ncannot study bubble photo-Marangoni dynamics without interference from bubble\nbuoyancy and photo-thermal convection. We elucidate the factors enabling fast\nphoto-Marangoni-driven bubble motion, by performing microgravity experiments,\nenabled by a bespoke photo-surfactant, complemented by a detailed modeling\nframework. We identify an optimal bubble size for migration, since smaller and\nlarger bubbles incur weaker photo-Marangoni stresses and larger drag,\nrespectively. Surfactants that switch rapidly under illumination drive fast\nmigration, provided their reverse switch (in darkness) is much slower, yet not\nnegligible. These foundational results enable the synthesis of next-generation\nphoto-surfactants and photo-Marangoni manipulation across multiphase fluid\nsystems.",
        "Graphical User Interface (GUI) Agents, powered by multimodal large language\nmodels (MLLMs), have shown great potential for task automation on computing\ndevices such as computers and mobile phones. However, existing agents face\nchallenges in multi-step reasoning and reliance on textual annotations,\nlimiting their effectiveness. We introduce \\textit{InfiGUIAgent}, an MLLM-based\nGUI Agent trained with a two-stage supervised fine-tuning pipeline. Stage 1\nenhances fundamental skills such as GUI understanding and grounding, while\nStage 2 integrates hierarchical reasoning and expectation-reflection reasoning\nskills using synthesized data to enable native reasoning abilities of the\nagents. \\textit{InfiGUIAgent} achieves competitive performance on several GUI\nbenchmarks, highlighting the impact of native reasoning skills in enhancing GUI\ninteraction for automation tasks. Resources are available at\n\\url{https:\/\/github.com\/Reallm-Labs\/InfiGUIAgent}.",
        "Algorithmic solutions have significant potential to improve decision-making\nacross various domains, from healthcare to e-commerce. However, the widespread\nadoption of these solutions is hindered by a critical challenge: the lack of\nhuman-interpretable explanations. Current approaches to Explainable AI (XAI)\npredominantly focus on complex machine learning models, often producing brittle\nand non-intuitive explanations. This project proposes a novel approach to\ndeveloping explainable algorithms by starting with optimization problems,\nspecifically the assignment problem. The developed software library enriches\nbasic algorithms with human-understandable explanations through four key\nmethodologies: generating meaningful alternative solutions, creating robust\nsolutions through input perturbation, generating concise decision trees and\nproviding reports with comprehensive explanation of the results. Currently\ndeveloped tools are often designed with specific clustering algorithms in mind,\nwhich limits their adaptability and flexibility to incorporate alternative\ntechniques. Additionally, many of these tools fail to integrate expert\nknowledge, which could enhance the clustering process by providing valuable\ninsights and context. This lack of adaptability and integration can hinder the\neffectiveness and robustness of the clustering outcomes in various\napplications. The represents a step towards making algorithmic solutions more\ntransparent, trustworthy, and accessible. By collaborating with industry\npartners in sectors such as sales, we demonstrate the practical relevance and\ntransformative potential of our approach.",
        "Data-driven reduced-order models based on autoencoders generally lack\ninterpretability compared to classical methods such as the proper orthogonal\ndecomposition. More interpretability can be gained by disentangling the latent\nvariables and analyzing the resulting modes. For this purpose, probabilistic\n$\\beta$-variational autoencoders ($\\beta$-VAEs) are frequently used in\ncomputational fluid dynamics and other simulation sciences. Using a benchmark\nperiodic flow dataset, we show that competitive results can be achieved using\nnon-probabilistic autoencoder approaches that either promote orthogonality or\npenalize correlation between latent variables. Compared to probabilistic\nautoencoders, these approaches offer more robustness with respect to the choice\nof hyperparameters entering the loss function. We further demonstrate the\nability of a non-probabilistic approach to identify a reduced number of active\nlatent variables by introducing a correlation penalty, a function also known\nfrom the use of $\\beta$-VAE. The investigated probabilistic and\nnon-probabilistic autoencoder models are finally used for the dimensionality\nreduction of aircraft ditching loads, which serves as an industrial application\nin this work.",
        "In the realm of neuroimaging research, the demand for efficient and accurate\nsimulation tools for functional magnetic resonance imaging (fMRI) data is ever\nincreasing. We present SHAKER, a comprehensive MATLAB package for simulating\ncomplex-valued fMRI time series data that will advance understanding and\nimplementation of the MR signal equation and related physics principles to fMRI\nsimulation. The core objective of the package is to provide researchers with a\nuser-friendly MATLAB graphical user interface (GUI) tool capable of generating\ncomplex-valued fMRI time series data. This tool will allow researchers to input\nvarious parameters related to the MRI scan and receive simulated k-space data\nwith ease, facilitating a deeper understanding of the intricacies of the\ngeneration and interpretation of fMRI data.",
        "High-resolution precipitation forecasts are crucial for providing accurate\nweather prediction and supporting effective responses to extreme weather\nevents. Traditional numerical models struggle with stochastic subgrid-scale\nprocesses, while recent deep learning models often produce blurry results. To\naddress these challenges, we propose a physics-inspired deep learning framework\nfor high-resolution (0.05\\textdegree{} $\\times$ 0.05\\textdegree{}) ensemble\nprecipitation forecasting. Trained on ERA5 and CMPA high-resolution\nprecipitation datasets, the framework integrates deterministic and\nprobabilistic components. The deterministic model, based on a 3D\nSwinTransformer, captures average precipitation at mesoscale resolution and\nincorporates strategies to enhance performance, particularly for moderate to\nheavy rainfall. The probabilistic model employs conditional diffusion in latent\nspace to account for uncertainties in residual precipitation at convective\nscales. During inference, ensemble members are generated by repeatedly sampling\nlatent variables, enabling the model to represent precipitation uncertainty.\nOur model significantly enhances spatial resolution and forecast accuracy. Rank\nhistogram shows that the ensemble system is reliable and unbiased. In a case\nstudy of heavy precipitation in southern China, the model outputs align more\nclosely with observed precipitation distributions than ERA5, demonstrating\nsuperior capability in capturing extreme precipitation events. Additionally,\n5-day real-time forecasts show good performance in terms of CSI scores.",
        "Glioblastoma, a highly aggressive brain tumor, poses major challenges due to\nits poor prognosis and high morbidity rates. Partial differential\nequation-based models offer promising potential to enhance therapeutic outcomes\nby simulating patient-specific tumor behavior for improved radiotherapy\nplanning. However, model calibration remains a bottleneck due to the high\ncomputational demands of optimization methods like Monte Carlo sampling and\nevolutionary algorithms. To address this, we recently introduced an approach\nleveraging a neural forward solver with gradient-based optimization to\nsignificantly reduce calibration time. This approach requires a highly accurate\nand fully differentiable forward model. We investigate multiple architectures,\nincluding (i) an enhanced TumorSurrogate, (ii) a modified nnU-Net, and (iii) a\n3D Vision Transformer (ViT). The optimized TumorSurrogate achieved the best\noverall results, excelling in both tumor outline matching and voxel-level\nprediction of tumor cell concentration. It halved the MSE relative to the\nbaseline model and achieved the highest Dice score across all tumor cell\nconcentration thresholds. Our study demonstrates significant enhancement in\nforward solver performance and outlines important future research directions.",
        "Text-to-image diffusion models are capable of generating high-quality images,\nbut these images often fail to align closely with the given text prompts.\nClassifier-free guidance (CFG) is a popular and effective technique for\nimproving text-image alignment in the generative process. However, using CFG\nintroduces significant computational overhead and deviates from the established\ntheoretical foundations of diffusion models. In this paper, we present\nDIstilling CFG by enhancing text Embeddings (DICE), a novel approach that\nremoves the reliance on CFG in the generative process while maintaining the\nbenefits it provides. DICE distills a CFG-based text-to-image diffusion model\ninto a CFG-free version by refining text embeddings to replicate CFG-based\ndirections. In this way, we avoid the computational and theoretical drawbacks\nof CFG, enabling high-quality, well-aligned image generation at a fast sampling\nspeed. Extensive experiments on multiple Stable Diffusion v1.5 variants, SDXL\nand PixArt-$\\alpha$ demonstrate the effectiveness of our method. Furthermore,\nDICE supports negative prompts for image editing to improve image quality\nfurther. Code will be available soon.",
        "This study explores the design of an efficient rebate policy in auction\nmarkets, focusing on a continuous-time setting with competition among market\nparticipants. In this model, a stock exchange collects transaction fees from\nauction investors executing block trades to buy or sell a risky asset, then\nredistributes these fees as rebates to competing market makers submitting limit\norders. Market makers influence both the price at which the asset trades and\ntheir arrival intensity in the auction. We frame this problem as a\nprincipal-multi-agent problem and provide necessary and sufficient conditions\nto characterize the Nash equilibrium among market makers. The exchange's\noptimization problem is formulated as a high-dimensional\nHamilton-Jacobi-Bellman equation with Poisson jump processes, which is solved\nusing a verification result. To numerically compute the optimal rebate and\ntransaction fee policies, we apply the Deep BSDE method. Our results show that\noptimal transaction fees and rebate structures improve market efficiency by\nnarrowing the spread between the auction clearing price and the asset's\nfundamental value, while ensuring a minimal gain for both market makers indexed\non the price of the asset on a coexisting limit order book.",
        "Terahertz (THz) communication technology is regarded as a promising enabler\nfor achieving ultra-high data rate transmission in next-generation\ncommunication systems. To mitigate the high path loss in THz systems, the\ntransmitting beams are typically narrow and highly directional, which makes it\ndifficult for a single beam to serve multiple users simultaneously. To address\nthis challenge, reconfigurable intelligent surfaces (RIS), which can\ndynamically manipulate the wireless propagation environment, have been\nintegrated into THz communication systems to extend coverage. Existing works\nmostly remain theoretical analysis and simulation, while prototype validation\nof RIS-assisted THz communication systems is scarce. In this paper, we designed\na liquid crystal-based RIS operating at 220 GHz supporting both single-user and\nmulti-user communication scenarios, followed by a RIS-aided THz communication\nsystem prototype. To enhance the system performance, we developed a beamforming\nmethod including a real-time power feedback control, which is compatible with\nboth single-beam and multibeam modes. To support simultaneous multi-user\ntransmission, we designed an OFDM-based resource allocation scheme. In our\nexperiments, the received power gain with RIS is no less than 10 dB in the\nsingle-beam mode, and no less than 5 dB in the multi-beam mode. With the\nassistance of RIS, the achievable rate of the system could reach 2.341 Gbps\nwith 3 users sharing 400 MHz bandwidth and the bit error rate (BER) of the\nsystem decreased sharply. Finally, an image transmission experiment was\nconducted to vividly show that the receiver could recover the transmitted\ninformation correctly with the help of RIS. The experimental results also\ndemonstrated that the received signal quality was enhanced through power\nfeedback adjustments.",
        "Semiconductor-based Josephson junctions embedded within a Cooper-pair-box can\nhost complex many-body states, such as interacting Andreev states and\npotentially other quasi-particles of topological origin. Here, we study the\ninsights that could be revealed from a tomographic reconstruction of the\nCooper-pair charge distribution of the junction prepared in its ground state.\nWe posit that interacting and topological states can be identified from\ndistinct signatures within the probability distribution of the charge states.\nFurthermore, the comprehensive dataset provides direct access to information\ntheory metrics elucidating the entanglement between the charge sector of the\nsuperconductor and the microscopic degrees of freedom in the junction. We\ndemonstrate how these metrics serve to further classify differences between the\ntypes of excitations in the junction.",
        "Blockchain-based Federated Learning (FL) is an emerging decentralized machine\nlearning paradigm that enables model training without relying on a central\nserver. Although some BFL frameworks are considered privacy-preserving, they\nare still vulnerable to various attacks, including inference and model\npoisoning. Additionally, most of these solutions employ strong trust\nassumptions among all participating entities or introduce incentive mechanisms\nto encourage collaboration, making them susceptible to multiple security flaws.\nThis work presents VerifBFL, a trustless, privacy-preserving, and verifiable\nfederated learning framework that integrates blockchain technology and\ncryptographic protocols. By employing zero-knowledge Succinct Non-Interactive\nArgument of Knowledge (zk-SNARKs) and incrementally verifiable computation\n(IVC), VerifBFL ensures the verifiability of both local training and\naggregation processes. The proofs of training and aggregation are verified\non-chain, guaranteeing the integrity and auditability of each participant's\ncontributions. To protect training data from inference attacks, VerifBFL\nleverages differential privacy. Finally, to demonstrate the efficiency of the\nproposed protocols, we built a proof of concept using emerging tools. The\nresults show that generating proofs for local training and aggregation in\nVerifBFL takes less than 81s and 2s, respectively, while verifying them\non-chain takes less than 0.6s.",
        "Sublinear resonant deviations from an exceptional point degeneracy (EPD) has\nbeen recently promoted as a sensing scheme. However, there is still an ongoing\ndebate whether the sensitivity advantage is negated by an increase in\nfundamental noise - especially when active elements induce self-oscillations.\nIn this case, nonlinearities are crucial in stabilizing amplifying modes and\nmitigating noise effects. A drawback is the formation of hysteresis loops that\nsignal a transition to unstable modes. This can only be alleviated by precise\ncavity symmetry management. Here, utilizing two coupled nonlinear RLC tanks\nwith balanced amplification and attenuation, we demonstrate that an explicit\nsymmetry violation, induced by sweeping the resonant detuning of the RLC tanks,\nreveals a hysteresis loop near the EPD whose width scales sublinearly with the\ninter-tank coupling. Our proposal re-envisions this disadvantageous feature as\na sensing protocol with diverging sensitivity, enhanced signal-to-noise ratio,\nand self-calibration without requiring delicate symmetry control. As such, it\nopens new avenues in metrology as well as for optical or RF switching and\ntriggering.",
        "Decoding strategies significantly influence the quality and diversity of the\ngenerated texts in large language models (LLMs), yet their impact on\ncomputational resource consumption, particularly GPU energy usage, is\ninsufficiently studied. This paper investigates the relationship between text\ngeneration decoding methods and energy efficiency, focusing on the trade-off\nbetween generation quality and GPU energy consumption across diverse tasks and\ndecoding configurations. By benchmarking multiple strategies across different\ntext generation tasks, such as Translation, Code Summarization, and Math\nProblem Solving, we reveal how selecting appropriate decoding techniques with\ntheir tuned hyperparameters affects text quality and has measurable\nimplications for resource utilization, emphasizing the need for balanced\noptimization. To the best of our knowledge, this study is among the first to\nexplore decoding strategies in LLMs through the lens of energy consumption,\noffering actionable insights for designing resource-aware applications that\nmaintain high-quality text generation.",
        "Recent studies have extended the notion of band topology to nonlinear systems\nby defining nonlinear counterparts of eigenvalue problems. They have found the\nnonlinearity-induced topological transition, while it has required complicated\nnonlinearity such as off-diagonal one. Thus, the existence of\nnonlinearity-induced transitions has been unclear under homogeneous on-site\nnonlinearity, which is ubiquitously found in nature. We here reveal that such\non-site nonlinearity can induce transitions of topological modes, where\ntopological modes converging to zero begin to converge to nonzero values. Since\nsuch nonlinearity-induced transition remains the bulk band topology unchanged,\nwe can regard it as a transition from a conventional topological mode to one\nunique to nonlinear systems. We analyze a nonlinear eigenvalue problem by\nrewriting it to a dynamical system in the spatial direction and clarify that\nthe nonlinearity-induced transition is a result of the bifurcation in the\nspatial dynamics. We also propose a possible setup to observe the\nnonlinearity-induced transition that uses a gradual amplification of nonlinear\nwaves. These results provide a general designing principle of topological\ninsulators controlled by nonlinearity.",
        "It is widely accepted that the Bayesian ideal observer (IO) should be used to\nguide the objective assessment and optimization of medical imaging systems. The\nIO employs complete task-specific information to compute test statistics for\nmaking inference decisions and performs optimally in signal detection tasks.\nHowever, the IO test statistic typically depends non-linearly on the image data\nand cannot be analytically determined. The ideal linear observer, known as the\nHotelling observer (HO), can sometimes be used as a surrogate for the IO.\nHowever, when image data are high dimensional, HO computation can be difficult.\nEfficient channels that can extract task-relevant features have been\ninvestigated to reduce the dimensionality of image data to approximate IO and\nHO performance. This work proposes a novel method for generating efficient\nchannels by use of the gradient of a Lagrangian-based loss function that was\ndesigned to learn the HO. The generated channels are referred to as the\nLagrangian-gradient (L-grad) channels. Numerical studies are conducted that\nconsider binary signal detection tasks involving various backgrounds and\nsignals. It is demonstrated that channelized HO (CHO) using L-grad channels can\nproduce significantly better signal detection performance compared to the CHO\nusing PLS channels. Moreover, it is shown that the proposed L-grad method can\nachieve significantly lower computation time compared to the PLS method."
      ]
    }
  },
  {
    "id":2412.16425,
    "research_type":"applied",
    "start_id":"b40",
    "start_title":"Whole Slide Imaging Versus Microscopy for Primary Diagnosis in Surgical Pathology: A Multicenter Blinded Randomized Noninferiority Study of 1992 Cases (Pivotal Study)",
    "start_abstract":"Most prior studies of primary diagnosis in surgical pathology using whole slide imaging (WSI) versus microscopy have focused on specific organ systems or included relatively few cases. The objective this study was to demonstrate that WSI is noninferior for pathology. A blinded randomized noninferiority conducted across the entire range cases (biopsies and resections, including hematoxylin eosin, immunohistochemistry, special stains) from 4 institutions original sign-out (baseline diagnosis) as reference standard. Cases were scanned, converted randomized. Sixteen pathologists interpreted by WSI, followed a wash-out period \u22654 weeks, after which read same observers other modality. Major discordances identified an adjudication panel, differences between major discordance rates both (against standard) calculated. total 1992 included, resulting 15,925 reads. rate with standard 4.9% 4.6% microscopy. difference 0.4% (95% confidence interval, -0.30% 1.01%). highest endocrine (1.8%), neoplastic kidney (1.5%), urinary bladder (1.3%), gynecologic (1.2%). Detailed analysis these revealed no instances where interpretation consistently inaccurate compared multiple observers. We conclude pathology, biopsies resections stained immunohistochemistry stains. This conclusion valid wide variety specimen types.",
    "start_categories":[
      "q-bio.CB"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b15"
      ],
      "title":[
        "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      ],
      "abstract":[
        "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "CFT-RAG: An Entity Tree Based Retrieval Augmented Generation Algorithm\n  With Cuckoo Filter",
        "Polyregular Model Checking",
        "Quasinormal modes of nonthermal fixed points",
        "Constrained multi-fidelity Bayesian optimization with automatic stop\n  condition",
        "A Probabilistic WxChallenge Proposal",
        "LIFT-GS: Cross-Scene Render-Supervised Distillation for 3D Language\n  Grounding",
        "AdaptiveLog: An Adaptive Log Analysis Framework with the Collaboration\n  of Large and Small Language Model",
        "MBTSAD: Mitigating Backdoors in Language Models Based on Token Splitting\n  and Attention Distillation",
        "Movable Antenna Enhanced DF and AF Relaying Systems: Performance\n  Analysis and Optimization",
        "Connection between planetary He I $\\lambda$10830 \\AA\\ absorption and\n  extreme-ultraviolet emission of planet-host stars",
        "Low-Complexity Event Detection and Identification in Coherent\n  Correlation OTDR Measurements",
        "Assortment optimization given basket shopping behavior using the Ising\n  model",
        "NavG: Risk-Aware Navigation in Crowded Environments Based on\n  Reinforcement Learning with Guidance Points",
        "Leveraging Randomness in Model and Data Partitioning for Privacy\n  Amplification",
        "Adaptive Retrieval Without Self-Knowledge? Bringing Uncertainty Back\n  Home",
        "Improving discrepancy by moving a few points",
        "Streaming DiLoCo with overlapping communication: Towards a Distributed\n  Free Lunch",
        "Effect of spin on the dynamics of multi-component trans-relativistic\n  accretion flows around Kerr black holes",
        "Supercooled phase transitions in conformal dark sectors explain NANOGrav\n  data",
        "Advancing Tumor Budding Detection with Fourier Ptychography Microscopy",
        "Enhancing 5G O-RAN Communication Efficiency Through AI-Based Latency\n  Forecasting",
        "Thiolation and PEGylation of silicon carbide nanoparticle",
        "Geodesic Connectedness on Statistical Manifolds with Divisible Cubic\n  Forms",
        "Noetherianity of polynomial rings up to group actions",
        "From obstacle to opportunity: uncovering the silver lining of pileup",
        "METAGENE-1: Metagenomic Foundation Model for Pandemic Monitoring",
        "New Co-Simulation Variants for Emissions and Cost Reduction of\n  Sustainable District Heating Planning",
        "Bursting Filter Bubble: Enhancing Serendipity Recommendations with\n  Aligned Large Language Models",
        "3d Mirrors and Phase Diagrams of Abelian Gauge Theories"
      ],
      "abstract":[
        "Although retrieval-augmented generation(RAG) significantly improves\ngeneration quality by retrieving external knowledge bases and integrating\ngenerated content, it faces computational efficiency bottlenecks, particularly\nin knowledge retrieval tasks involving hierarchical structures for Tree-RAG.\nThis paper proposes a Tree-RAG acceleration method based on the improved Cuckoo\nFilter, which optimizes entity localization during the retrieval process to\nachieve significant performance improvements. Tree-RAG effectively organizes\nentities through the introduction of a hierarchical tree structure, while the\nCuckoo Filter serves as an efficient data structure that supports rapid\nmembership queries and dynamic updates. The experiment results demonstrate that\nour method is much faster than naive Tree-RAG while maintaining high levels of\ngenerative quality. When the number of trees is large, our method is hundreds\nof times faster than naive Tree-RAG. Our work is available at\nhttps:\/\/github.com\/TUPYP7180\/CFT-RAG-2025.",
        "We reduce the model checking problem for a subset of Python to the\nsatisfiability of a first-order formula over finite words, which is known to be\ndecidable. The reduction is based on the theory of polyregular functions, a\nrecently developed generalization of regular languages to polynomial output\nstring-to-string functions. We implemented this reduction in a verification\ntool called PolyCheck, that can use both automata-based solvers and classical\nSMT solvers as backends.",
        "Quasinormal modes play a prominent role in relaxation of diverse physical\nsystems to equilibria, ranging from astrophysical black holes to tiny droplets\nof quark-gluon plasma at RHIC and LHC accelerators. We propose that a novel\nkind of quasinormal modes govern the direct approach to self-similar time\nevolution of nonthermal fixed points, whose relevance ranges from high energy\nphysics to cold atom gases. We utilize black hole perturbation theory\ntechniques to compute the spectrum of these far from equilibrium quasinormal\nmodes for a kinetic theory with a Focker-Planck collision kernel in isotropic\nand homogeneous states. Our conclusion is that quasinormal modes of nonthermal\nfixed points give rise to a tower of progressively more decaying power-law\ncontributions. A byproduct of our analysis is a precise determination and\nimproved understanding of the distribution function characterizing nonthermal\nfixed points.",
        "Bayesian optimization (BO) is increasingly employed in critical applications\nto find the optimal design with minimal cost. While BO is known for its sample\nefficiency, relying solely on costly high-fidelity data can still result in\nhigh costs. This is especially the case in constrained search spaces where BO\nmust not only optimize but also ensure feasibility. A related issue in the BO\nliterature is the lack of a systematic stopping criterion. To solve these\nchallenges, we develop a constrained cost-aware multi-fidelity BO (CMFBO)\nframework whose goal is to minimize overall sampling costs by utilizing\ninexpensive low-fidelity sources while ensuring feasibility. In our case, the\nconstraints can change across the data sources and may be even black-box\nfunctions. We also introduce a systematic stopping criterion that addresses the\nlong-lasting issue associated with BO's convergence assessment. Our framework\nis publicly available on GitHub through the GP+ Python package and herein we\nvalidate it's efficacy on multiple benchmark problems.",
        "The national forecasting competition WxChallenge, brainchild of Brad Illston\nat the University of Oklahoma in 2005, has become a cherished institution\nplayed across the United States each year. Participants include students,\nfaculty, alumni, and industry professionals. However, forecasts are given as\nscalar values without expression of uncertainty, probabilities being a keystone\nof meteorological forecasting today, and previous attempts to add probabilistic\nelements to WxChallenge have failed partly due to challenges in making\nprobability forecasting accessible to all, and inability to combine scores with\ndifferent units while also appropriately rewarding forecasts using proper\nscoring rules. Much of the competition's maintenance relies on dedicated\nvolunteers, highlighting need for more automation. Hence I propose three new\nfeatures: (1) automated forecast problems based on morning ensemble guidance,\nforming prediction baselines, thresholds over which the players demonstrate\nskill in their later forecast; (2) a spread betting game, where the players\nallocate 100 confidence credits to the over-under for exceeding a percentile\n(e.g., 50pc) threshold of a variable (e.g., maximum temperature) derived from\nthe ensemble baseline; and (3) a game where players distribute 100 confidence\ncredits across bins of a continuous variable (e.g., accumulated precipitation)\napproximating a probability mass function. Forecasts are evaluated using\nShannon information gained over the baseline forecast, yielding additive units\nof bits that allow score combinations of different variables and units.\nInformation gain parallels the Brier Score and is likewise a sound measure of\nskill due its punishment of hedging. This proposal objective is to augment\nWxChallenge with two new probabilistic games that are accessible,\nscientifically sound, enjoyable, and optional.",
        "Our approach to training 3D vision-language understanding models is to train\na feedforward model that makes predictions in 3D, but never requires 3D labels\nand is supervised only in 2D, using 2D losses and differentiable rendering. The\napproach is new for vision-language understanding. By treating the\nreconstruction as a ``latent variable'', we can render the outputs without\nplacing unnecessary constraints on the network architecture (e.g. can be used\nwith decoder-only models). For training, only need images and camera pose, and\n2D labels. We show that we can even remove the need for 2D labels by using\npseudo-labels from pretrained 2D models. We demonstrate this to pretrain a\nnetwork, and we finetune it for 3D vision-language understanding tasks. We show\nthis approach outperforms baselines\/sota for 3D vision-language grounding, and\nalso outperforms other 3D pretraining techniques. Project page:\nhttps:\/\/liftgs.github.io.",
        "Automated log analysis is crucial to ensure high availability and reliability\nof complex systems. The advent of LLMs in NLP has ushered in a new era of\nlanguage model-driven automated log analysis, garnering significant interest.\nWithin this field, two primary paradigms based on language models for log\nanalysis have become prominent. Small Language Models (SLMs) follow the\npre-train and fine-tune paradigm, focusing on the specific log analysis task\nthrough fine-tuning on supervised datasets. On the other hand, LLMs following\nthe in-context learning paradigm, analyze logs by providing a few examples in\nprompt contexts without updating parameters. Despite their respective\nstrengths, we notice that SLMs are more cost-effective but less powerful,\nwhereas LLMs with large parameters are highly powerful but expensive and\ninefficient. To trade-off between the performance and inference costs of both\nmodels in automated log analysis, this paper introduces an adaptive log\nanalysis framework known as AdaptiveLog, which effectively reduces the costs\nassociated with LLM while ensuring superior results. This framework\ncollaborates an LLM and a small language model, strategically allocating the\nLLM to tackle complex logs while delegating simpler logs to the SLM.\nSpecifically, to efficiently query the LLM, we propose an adaptive selection\nstrategy based on the uncertainty estimation of the SLM, where the LLM is\ninvoked only when the SLM is uncertain. In addition, to enhance the reasoning\nability of the LLM in log analysis tasks, we propose a novel prompt strategy by\nretrieving similar error-prone cases as the reference, enabling the model to\nleverage past error experiences and learn solutions from these cases. Extensive\nexperiments demonstrate that AdaptiveLog achieves state-of-the-art results\nacross different tasks, elevating the overall accuracy of log analysis while\nmaintaining cost efficiency.",
        "In recent years, attention-based models have excelled across various domains\nbut remain vulnerable to backdoor attacks, often from downloading or\nfine-tuning on poisoned datasets. Many current methods to mitigate backdoors in\nNLP models rely on the pre-trained (unfine-tuned) weights, but these methods\nfail in scenarios where the pre-trained weights are not available. In this\nwork, we propose MBTSAD, which can mitigate backdoors in the language model by\nutilizing only a small subset of clean data and does not require pre-trained\nweights. Specifically, MBTSAD retrains the backdoored model on a dataset\ngenerated by token splitting. Then MBTSAD leverages attention distillation, the\nretrained model is the teacher model, and the original backdoored model is the\nstudent model. Experimental results demonstrate that MBTSAD achieves comparable\nbackdoor mitigation performance as the methods based on pre-trained weights\nwhile maintaining the performance on clean data. MBTSAD does not rely on\npre-trained weights, enhancing its utility in scenarios where pre-trained\nweights are inaccessible. In addition, we simplify the min-max problem of\nadversarial training and visualize text representations to discover that the\ntoken splitting method in MBTSAD's first step generates Out-of-Distribution\n(OOD) data, leading the model to learn more generalized features and eliminate\nbackdoor patterns.",
        "Movable antenna (MA) has been deemed as a promising technology to flexibly\nreconfigure wireless channels by adjusting the antenna positions in a given\nlocal region. In this paper, we investigate the application of the MA\ntechnology in both decode-and-forward (DF) and amplify-and-forward (AF)\nrelaying systems, where a relay is equipped with multiple MAs to assist in the\ndata transmission between two single-antenna nodes. For the DF relaying system,\nour objective is to maximize the achievable rate at the destination by jointly\noptimizing the positions of the MAs in two stages for receiving signals from\nthe source and transmitting signals to the destination, respectively. To drive\nessential insights, we first derive a closed-form upper bound on the maximum\nachievable rate of the DF relaying system. Then, a low-complexity algorithm\nbased on projected gradient ascent (PGA) and alternating optimization (AO) is\nproposed to solve the antenna position optimization problem. For the AF\nrelaying system, our objective is to maximize the achievable rate by jointly\noptimizing the two-stage MA positions as well as the AF beamforming matrix at\nthe relay, which results in a more challenging optimization problem due to the\nintricate coupling variables. To tackle this challenge, we first reveal the\nhidden separability among the antenna position optimization in the two stages\nand the beamforming optimization. Based on such separability, we derive a\nclosed-form upper bound on the maximum achievable rate of the AF relaying\nsystem and propose a low-complexity algorithm to obtain a high-quality\nsuboptimal solution to the considered problem. Simulation results validate the\nefficacy of our theoretical analysis and demonstrate the superiority of the\nMA-enhanced relaying systems to the conventional relaying systems with\nfixed-position antennas (FPAs) and other benchmark schemes.",
        "Context. The detection of the He I 10830 A triplet in exoplanet atmospheres\nhas opened a new window for probing planetary properties, including atmospheric\nescape. Unlike Lyman alpha, the triplet is less affected by ISM absorption.\nSufficient XUV stellar irradiation may trigger the formation of the He I\ntriplet via photoionization and posterior recombination processes in the planet\natmospheres. Only a weak trend between stellar XUV and the planetary He I\nstrength has been observed so far. Aims. We aim to confirm this mechanism for\nproducing the He I absorption in exoplanetary atmospheres by examining a sample\nof planetary systems. Methods. We obtained homogeneous measurements of the\nplanetary He I line EW and consistently computed the stellar XUV ionizing\nirradiation. We first derived new coronal models for the planet-host stars. We\nused updated data from the X-exoplanets database, archival X-ray spectra of\nM-type stars (including AU Mic and Proxima Cen), and new XMM-Newton X-ray data\nobtained for the CARMENES project. These data were complemented at longer\nwavelengths with publicly available HST, FUSE, and EUVE spectra. A total of 75\nstars are carefully analyzed to obtain a new calibration between X-ray and EUV\nemission. Results. Two distinct relationships between stellar X-ray emission\n(5-100 A) and EUV_H (100-920 A) or EUV_He (100-504 A) radiation are obtained to\nscale the emission from late-type stellar coronae. A total of 48 systems with\nreported planetary He I 10830 A studies, exhibit a robust relationship between\nthe planetary He I feature and the ionizing XUV_He received by the planet,\ncorrected by stellar and planetary radii, and the planet's gravitational\npotential. Some outliers could be explained by a different atmospheric\ncomposition or the lack of planetary gaseous atmospheres. This relation may be\nused to predict the He I 10830 A absorption in exoplanet atmospheres.",
        "Pairing coherent correlation OTDR with low-complexity analysis methods, we\ninvestigate the detection of fast temperature changes and vibrations in optical\nfibers. A localization accuracy of ~2 m and extraction of vibration amplitudes\nand frequencies is demonstrated.",
        "In markets where customers tend to purchase baskets of products rather than\nsingle products, assortment optimization is a major challenge for retailers.\nRemoving a product from a retailer's assortment can result in a severe drop in\naggregate demand if this product is a complement to other products. Therefore,\naccounting for the complementarity effect is essential when making assortment\ndecisions. In this paper, we develop a modeling framework designed to address\nthis problem. We model customers' choices using a Markov random field -- in\nparticular, the Ising model -- which captures pairwise demand dependencies as\nwell as the individual attractiveness of each product. Using the Ising model\nallows us to leverage existing methodologies for various purposes including\nparameter estimation and efficient simulation of customer choices. We formulate\nthe assortment optimization problem under this model and show that its decision\nversion is NP-hard. We also provide multiple theoretical insights into the\nstructure of the optimal assortments based on the graphical representation of\nthe Ising model, and propose several heuristic algorithms that can be used to\nobtain high-quality solutions to the assortment optimization problem. Our\nnumerical analysis demonstrates that the developed simulated annealing\nprocedure leads to an expected profit gain of 15% compared to offering an\nunoptimized assortment (where all products are included) and around 5% compared\nto using a revenue-ordered heuristic algorithm.",
        "Motion planning in navigation systems is highly susceptible to upstream\nperceptual errors, particularly in human detection and tracking. To mitigate\nthis issue, the concept of guidance points--a novel directional cue within a\nreinforcement learning-based framework--is introduced. A structured method for\nidentifying guidance points is developed, consisting of obstacle boundary\nextraction, potential guidance point detection, and redundancy elimination. To\nintegrate guidance points into the navigation pipeline, a\nperception-to-planning mapping strategy is proposed, unifying guidance points\nwith other perceptual inputs and enabling the RL agent to effectively leverage\nthe complementary relationships among raw laser data, human detection and\ntracking, and guidance points. Qualitative and quantitative simulations\ndemonstrate that the proposed approach achieves the highest success rate and\nnear-optimal travel times, greatly improving both safety and efficiency.\nFurthermore, real-world experiments in dynamic corridors and lobbies validate\nthe robot's ability to confidently navigate around obstacles and robustly avoid\npedestrians.",
        "We study how inherent randomness in the training process -- where each sample\n(or client in federated learning) contributes only to a randomly selected\nportion of training -- can be leveraged for privacy amplification. This\nincludes (1) data partitioning, where a sample participates in only a subset of\ntraining iterations, and (2) model partitioning, where a sample updates only a\nsubset of the model parameters. We apply our framework to model parallelism in\nfederated learning, where each client updates a randomly selected subnetwork to\nreduce memory and computational overhead, and show that existing methods, e.g.\nmodel splitting or dropout, provide a significant privacy amplification gain\nnot captured by previous privacy analysis techniques. Additionally, we\nintroduce Balanced Iteration Subsampling, a new data partitioning method where\neach sample (or client) participates in a fixed number of training iterations.\nWe show that this method yields stronger privacy amplification than Poisson\n(i.i.d.) sampling of data (or clients). Our results demonstrate that randomness\nin the training process, which is structured rather than i.i.d. and interacts\nwith data in complex ways, can be systematically leveraged for significant\nprivacy amplification.",
        "Retrieval Augmented Generation (RAG) improves correctness of Question\nAnswering (QA) and addresses hallucinations in Large Language Models (LLMs),\nyet greatly increase computational costs. Besides, RAG is not always needed as\nmay introduce irrelevant information. Recent adaptive retrieval methods\nintegrate LLMs' intrinsic knowledge with external information appealing to LLM\nself-knowledge, but they often neglect efficiency evaluations and comparisons\nwith uncertainty estimation techniques. We bridge this gap by conducting a\ncomprehensive analysis of 35 adaptive retrieval methods, including 8 recent\napproaches and 27 uncertainty estimation techniques, across 6 datasets using 10\nmetrics for QA performance, self-knowledge, and efficiency. Our findings show\nthat uncertainty estimation techniques often outperform complex pipelines in\nterms of efficiency and self-knowledge, while maintaining comparable QA\nperformance.",
        "We show how to improve the discrepancy of an iid sample by moving only a few\npoints. Specifically, modifying \\( O(m) \\) sample points on average reduces the\nKolmogorov-Smirnov distance to the population distribution to \\(1\/m\\).",
        "Training of large language models (LLMs) is typically distributed across a\nlarge number of accelerators to reduce training time. Since internal states and\nparameter gradients need to be exchanged at each and every single gradient\nstep, all devices need to be co-located using low-latency high-bandwidth\ncommunication links to support the required high volume of exchanged bits.\nRecently, distributed algorithms like DiLoCo have relaxed such co-location\nconstraint: accelerators can be grouped into ``workers'', where\nsynchronizations between workers only occur infrequently. This in turn means\nthat workers can afford being connected by lower bandwidth communication links\nwithout affecting learning quality. However, in these methods, communication\nacross workers still requires the same peak bandwidth as before, as the\nsynchronizations require all parameters to be exchanged across all workers. In\nthis paper, we improve DiLoCo in three ways. First, we synchronize only subsets\nof parameters in sequence, rather than all at once, which greatly reduces peak\nbandwidth. Second, we allow workers to continue training while synchronizing,\nwhich decreases wall clock time. Third, we quantize the data exchanged by\nworkers, which further reduces bandwidth across workers. By properly combining\nthese modifications, we show experimentally that we can distribute training of\nbillion-scale parameters and reach similar quality as before, but reducing\nrequired bandwidth by two orders of magnitude.",
        "We investigate the axially symmetric accretion of low angular momentum\nhydrodynamic matter onto a rotating black hole. The gravitational field under\nconsideration is assumed to be described by a pseudo-Newtonian Kerr potential.\nThe accreting matter consists of different species defined by a relativistic\nequation of state with a variable adiabatic index.We construct and solve the\nhydrodynamical conservation equations governing such a flow, and find out the\ncorresponding stationary integral solutions. We find that depending on the\nvalues of initial boundary conditions, accretion flow may exhibit\nmulti-transonic behaviour, and a standing shock may form. We investigate, in\nminute detail, how the spin angular momentum of the black hole, as well as the\ncomposition of the accreting matter influence the dynamics of accretion flow\nand the astrophysics of shock formation in the aforementioned accreting black\nhole systems.",
        "According to recent lore, it is difficult to explain the evidence for a\nstochastic gravitational wave background obtained by pulsar timing arrays with\nsupercooled first-order phase transitions (FOPTs). We demonstrate that\nsupercooled FOPTs in dark U(1)' models with a conformal dark sector easily\nexplain the nHz signal at NANOGrav.",
        "Background: Tumour budding is an independent predictor of metastasis and\nprognosis in colorectal cancer and is a vital part of the pathology\nspecification report. In a conventional pathological section observation\nprocess, pathologists have to repeatedly switch from 10x objective to 20x\nobjective several times to localize and image the target region. Besides the\nswitching operations, repeated manual or electro-mechanical focusing is also\nvery time-consuming, affecting the total time for pathological diagnosis. In\naddition, It is usually necessary to remove the manually marked symbols on the\nstained pathology slides used for classification and management before\nobservation. Methods: In this paper, we utilize Fourier ptychographic\nmicroscopy (FPM) in the pathological diagnosis process to realize large\nspace-bandwidth product imaging, quantitative phase imaging, and digital\nrefocusing in the observation process without any mechanical operations, which\ncan therefore simplify the above-mentioned cumbersome diagnostic processes. We\nfirst verify the effectiveness and efficiency of the proposed method with\nseveral typical pathological sections. Then, instead of manually erasing, we\nalso prove that FP framework can digitally remove the artificial markers with\nits digital refocusing ability. Results: At last, we demonstrated pathologists\ncan achieve 100% diagnostic accuracy with FPM imaging results. Conclusions: The\nproposed method can greatly simplify the process of pathological diagnosis, and\nthe related addon hardware system does not require expensive components, which\nmakes it have great potential for promotion in the field of pathological\ndiagnosis.",
        "The increasing complexity and dynamic nature of 5G open radio access networks\n(O-RAN) pose significant challenges to maintaining low latency, high\nthroughput, and resource efficiency. While existing methods leverage machine\nlearning for latency prediction and resource management, they often lack\nreal-world scalability and hardware validation. This paper addresses these\nlimitations by presenting an artificial intelligence-driven latency forecasting\nsystem integrated into a functional O-RAN prototype. The system uses a\nbidirectional long short-term memory model to predict latency in real time\nwithin a scalable, open-source framework built with FlexRIC. Experimental\nresults demonstrate the model's efficacy, achieving a loss metric below 0.04,\nthus validating its applicability in dynamic 5G environments.",
        "In this study, we implement thiol termination on the surface of\nfew-nanometer-sized silicon carbide (SiC) nanoparticles (NPs) to enable further\napplications, such as fluorescent biomarkers. Various spectroscopic techniques\nare employed to monitor the effectiveness of the surface treatment.\nAdditionally, a thiol-Michael addition reaction is performed by conjugating\n4-arm PEG-maleimide molecules to the thiol groups of SiC NPs, further\ndemonstrating the reactivity of thiol-terminated SiC NPs. These thiolated SiC\nNPs, both with and without conjugated molecules, open new avenues in\nbiotechnology.",
        "The class of statistical manifolds with divisible cubic forms arises from\naffine differential geometry. We examine the geodesic connectedness of affine\nconnections on this class of statistical manifolds. In information geometry,\nthe geodesic connectedness of the affine connections are often assumed, as in\nthe generalized Pythagorean theorem. In Riemannian geometry, the geodesic\nconnectedness of the Levi-Civita connection follows from its geodesic\ncompleteness by the well-known Hopf-Rinow theorem. However, the geodesic\nconnectedness of general affine connections is more challenging to achieve,\neven for the Levi-Civita connection in pseudo-Riemannian geometry or for affine\nconnections on compact manifolds. By analogy with the Hopf-Rinow theorem in\nRiemannian geometry, we establish the geodesic connectedness of the affine\nconnections on statistical manifolds with divisible cubic forms from their\ngeodesic completeness. As an application, we establish a Cartan-Hadamard type\ntheorem for statistical manifolds.",
        "Let $k$ be a commutative Noetherian ring, and $k[S]$ the polynomial ring with\nindeterminates parameterized by elements in a set $S$. We show that $k[S]$ is\nNoetherian up to actions of permutation groups on $S$ satisfying certain\ncombinatorial conditions. Moreover, there is a special linear order on every\ninfinite $S$ such that $k[S]$ is Noetherian up to the action of the\norder-preserving permutation group, and the existence of such a linear order is\nequivalent to the Axiom of Choice. These Noetherian results are proved via a\nsheaf theoretic approach and the work of Nagel-R\\\"{o}mer.",
        "The lack of evidence for Beyond Standard Model (BSM) particles might be due\nto their light mass and very weak interactions, as exemplified by BSM\nlong-lived particles (LLPs). Such particles can be produced from $B$ or $D$\nhadron decays. Typically, the high values of pileup (PU) in hadron colliders\nare expected to pose a major challenge in light new physics searches. We\npropose a fresh perspective that counters this conventional wisdom: instead of\nviewing PU solely as an impediment, we highlight its potential benefits in\nsearches for light LLPs from $B$ or $D$ hadron decays at HL-LHC and FCC-hh. In\nparticular, certain forward detectors in LHC experiments, such as the Zero\nDegree Calorimeters (ZDC), which are currently not utilized for LLP searches,\ncan be repurposed with strategic modifications to play a crucial role in this\nendeavor. Leveraging a combination of forward and central detectors, along with\nsmart strategies for triggering and offline analysis, we demonstrate the\npotential for exploring light LLPs in high PU scenarios.",
        "We pretrain METAGENE-1, a 7-billion-parameter autoregressive transformer\nmodel, which we refer to as a metagenomic foundation model, on a novel corpus\nof diverse metagenomic DNA and RNA sequences comprising over 1.5 trillion base\npairs. This dataset is sourced from a large collection of human wastewater\nsamples, processed and sequenced using deep metagenomic (next-generation)\nsequencing methods. Unlike genomic models that focus on individual genomes or\ncurated sets of specific species, the aim of METAGENE-1 is to capture the full\ndistribution of genomic information present within this wastewater, to aid in\ntasks relevant to pandemic monitoring and pathogen detection. We carry out\nbyte-pair encoding (BPE) tokenization on our dataset, tailored for metagenomic\nsequences, and then pretrain our model. In this paper, we first detail the\npretraining dataset, tokenization strategy, and model architecture,\nhighlighting the considerations and design choices that enable the effective\nmodeling of metagenomic data. We then show results of pretraining this model on\nour metagenomic dataset, providing details about our losses, system metrics,\nand training stability over the course of pretraining. Finally, we demonstrate\nthe performance of METAGENE-1, which achieves state-of-the-art results on a set\nof genomic benchmarks and new evaluations focused on human-pathogen detection\nand genomic sequence embedding, showcasing its potential for public health\napplications in pandemic monitoring, biosurveillance, and early detection of\nemerging health threats.",
        "Classical heating of residential areas is very energy-intensive, so\nalternatives are needed, including renewable energies and advanced heating\ntechnologies. Thus, the present paper introduces a new methodology for\ncomprehensive variant analysis for future district heating planning, aiming at\noptimizing emissions and costs. For this, an extensive Modelica-based modeling\nstudy comprising models of heating center, heat grid pipelines and heating\ninterface units to buildings are coupled in co-simulations. These enable a\ncomparative analysis of the economic feasibility and sustainability for various\ntechnologies and energy carriers to be carried out. The new modular and highly\nparameterizable building model serves for validation of the introduced heat\ngrid model. The results show that bio-methane as an energy source reduces\ncarbon equivalent emissions by nearly 70% compared to conventional natural gas\nheating, and the use of hydrogen as an energy source reduces carbon equivalent\nemissions by 77% when equipped with a heat pump. In addition, the use of ground\nsource heat pumps has a high economic viability when economic benefits are\ntaken into account. The study findings highlight the importance of strategic\nplanning and flexible design in the early stages of district development in\norder to achieve improved energy efficiency and a reduced carbon footprint.",
        "Recommender systems (RSs) often suffer from the feedback loop phenomenon,\ne.g., RSs are trained on data biased by their recommendations. This leads to\nthe filter bubble effect that reinforces homogeneous content and reduces user\nsatisfaction. To this end, serendipity recommendations, which offer unexpected\nyet relevant items, are proposed. Recently, large language models (LLMs) have\nshown potential in serendipity prediction due to their extensive world\nknowledge and reasoning capabilities. However, they still face challenges in\naligning serendipity judgments with human assessments, handling long user\nbehavior sequences, and meeting the latency requirements of industrial RSs. To\naddress these issues, we propose SERAL (Serendipity Recommendations with\nAligned Large Language Models), a framework comprising three stages: (1)\nCognition Profile Generation to compress user behavior into multi-level\nprofiles; (2) SerenGPT Alignment to align serendipity judgments with human\npreferences using enriched training data; and (3) Nearline Adaptation to\nintegrate SerenGPT into industrial RSs pipelines efficiently. Online\nexperiments demonstrate that SERAL improves exposure ratio (PVR), clicks, and\ntransactions of serendipitous items by 5.7%, 29.56%, and 27.6%, enhancing user\nexperience without much impact on overall revenue. Now, it has been fully\ndeployed in the \"Guess What You Like\" of the Taobao App homepage.",
        "This paper presents new developments in the study of 3d mirror symmetry and\nthe phase structure of Abelian gauge theories. Previous works identified 3d\nmirrors for a specific class of theories, termed ``simple\" Abelian theories.\nThis work extends this framework by proposing 3d mirrors for ``non-simple\"\nAbelian theories with both discrete and continuous gauge group factors. The\nproposal is supported by evidence from an exact operator map between the\nHiggs\/Coulomb branch of one theory and the Coulomb\/Higgs branch of its 3d\nmirror. Further support is provided by explicit Hilbert series computations. An\nalgorithm for computing the Hasse (phase) diagram of the Higgs branch of both\nsimple and non-simple Abelian theories is introduced, uncovering a recently\ndiscovered family of isolated singularities among the elementary slices. A\nbottom-up algorithm for computing the Coulomb branch Hasse diagram of these\ntheories is also introduced, and the two algorithms are tested against each\nother via 3d mirror symmetry."
      ]
    }
  },
  {
    "id":2412.16425,
    "research_type":"applied",
    "start_id":"b15",
    "start_title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
    "start_abstract":"While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b40"
      ],
      "title":[
        "Whole Slide Imaging Versus Microscopy for Primary Diagnosis in Surgical Pathology: A Multicenter Blinded Randomized Noninferiority Study of 1992 Cases (Pivotal Study)"
      ],
      "abstract":[
        "Most prior studies of primary diagnosis in surgical pathology using whole slide imaging (WSI) versus microscopy have focused on specific organ systems or included relatively few cases. The objective this study was to demonstrate that WSI is noninferior for pathology. A blinded randomized noninferiority conducted across the entire range cases (biopsies and resections, including hematoxylin eosin, immunohistochemistry, special stains) from 4 institutions original sign-out (baseline diagnosis) as reference standard. Cases were scanned, converted randomized. Sixteen pathologists interpreted by WSI, followed a wash-out period \u22654 weeks, after which read same observers other modality. Major discordances identified an adjudication panel, differences between major discordance rates both (against standard) calculated. total 1992 included, resulting 15,925 reads. rate with standard 4.9% 4.6% microscopy. difference 0.4% (95% confidence interval, -0.30% 1.01%). highest endocrine (1.8%), neoplastic kidney (1.5%), urinary bladder (1.3%), gynecologic (1.2%). Detailed analysis these revealed no instances where interpretation consistently inaccurate compared multiple observers. We conclude pathology, biopsies resections stained immunohistochemistry stains. This conclusion valid wide variety specimen types."
      ],
      "categories":[
        "q-bio.CB"
      ]
    },
    "list":{
      "title":[
        "Modular Forms and Certain ${}_2F_1(1)$ Hypergeometric Series",
        "Signal amplification in a solid-state quantum sensor via asymmetric\n  time-reversal of many-body dynamics",
        "Flavor dependence of Energy-energy correlators",
        "Integral Ricci Curvature for Graphs",
        "Optimal Low degree hardness for Broadcasting on Trees",
        "Geodesics for Discrete manifolds",
        "Controlling spin currents with magnon interference in a canted\n  antiferromagnet",
        "Unique continuation for locally uniformly distributed measures",
        "Isospin sum rules for bottom-baryon weak decays",
        "Sampling Binary Data by Denoising through Score Functions",
        "Consonance in music -- the Pythagorean approach revisited",
        "Optimal lower Lipschitz bounds for ReLU layers, saturation, and phase\n  retrieval",
        "The Andreadakis Problem for the McCool groups",
        "Intrinsic superconducting diode effect and nonreciprocal\n  superconductivity in rhombohedral graphene multilayers",
        "On cyclotomic nearly-doubly-regular tournaments",
        "Anatomy of Spin Wave Polarization in Ferromagnets",
        "Periodic elements in finite type Artin-Tits groups and stability\n  conditions",
        "Impact of phonon lifetimes on the single-photon indistinguishability in\n  quantum emitters based on 2D materials",
        "Optimization of the Woodcock Particle Tracking Method Using Neural\n  Network",
        "Machine learning algorithms to predict stroke in China based on causal\n  inference of time series analysis",
        "Biases in Edge Language Models: Detection, Analysis, and Mitigation",
        "Pressure suppresses the density wave order in kagome metal\n  LuNb$_6$Sn$_6$",
        "Comparative Analysis of Perturbed $f(R)$ Gravity and Perturbed Rastall\n  Gravity Models in Describing Cosmic Evolution from Early to Late Universe\n  Relative to the $\\Lambda$CDM Model",
        "Slow magnetic quantum oscillations in the c-axis magnetoresistance of\n  UTe$_2$",
        "Fractons from covariant higher-rank 3D BF theory",
        "A Perspective on Symbolic Machine Learning in Physical Sciences",
        "Simultaneously decoding the unknown stationary state and function\n  parameters for mean field games",
        "Central series' and ($n$)-isoclinism of skew left braces",
        "Analog of the Carnot engine for fluctuating diffusivity in living cells"
      ],
      "abstract":[
        "Using the framework relating hypergeometric motives to modular forms, we\ndefine an explicit family of weight 2 Hecke eigenforms with complex\nmultiplication. We use the theory of ${}_2F_1(1)$ hypergeometric series and\nRamanujan's theory of alternative bases to compute the exact central $L$-value\nof these Hecke eigenforms in terms of special beta values. We also show the\nintegral Fourier coefficients can be written in terms of Jacobi sums,\nreflecting a motivic relation between the hypergeometric series and the modular\nforms.",
        "Electronic spins of nitrogen vacancy (NV) centers in diamond constitute a\npromising system for micro- and nano-scale magnetic sensing, due to their\noperation under ambient conditions, ease of placement in close proximity to\nsensing targets, and biological compatibility. At high densities, the\nelectronic spins interact through dipolar coupling, which typically limits but\ncan also potentially enhance sensing performance. Here we report the\nexperimental demonstration of many-body signal amplification in a solid-state,\nroom temperature quantum sensor. Our approach utilizes time-reversed\ntwo-axis-twisting interactions, engineered through dynamical control of the\nquantization axis and Floquet engineering in a two-dimensional ensemble of NV\ncenters. Strikingly, we observe that the optimal amplification occurs when the\nbackward evolution time equals twice the forward evolution time, in sharp\ncontrast to the conventional Loschmidt echo. These observations can be\nunderstood as resulting from an underlying time-reversed mirror symmetry of the\nmicroscopic dynamics, providing key insights into signal amplification and\nopening the door towards entanglement-enhanced practical quantum sensing.",
        "Energy-energy correlators (EECs) within high energy jets serve as a key\nexperimentally accessible quantity to probe the scale and structure of the\nquark-gluon plasma (QGP) in relativistic heavy-ion collisions. The CMS\nCollaboration's first measurement of the modification to the EEC within single\ninclusive jets in Pb+Pb collisions relative to p+p collisions reveals a\nsignificant enhancement at small angles, which may arise from jet transverse\nmomentum $p_T$ selection biases due to jet energy loss. We investigate the\ndependence of jet EECs on the flavor of the initiating parton. The EEC\ndistribution of a gluon jet is broader and the peak of transition from\nperturbative to non-perturbative regime occurs at a larger angle than a quark\njet. Such flavor dependence leads to the different EECs for $\\gamma$-jets and\nsingle inclusive jets due to their different flavor composition. It is also\nresponsible for a colliding energy dependence of EECs of single inclusive jets\nat fixed jet energy. We also investigate the impact of flavor composition\nvariation on the $p_T$ dependence of the jet EEC. We further propose that a\nchange in the gluon jet fraction in A+A collisions compared to p+p can also\ncontribute to a non-negligible enhancement of the medium modified EEC at small\nangles. Using the \\textsc{Jewel} model, we predict the reduction of the gluon\njet fraction in A+A collisions and estimate its impact on the EEC.",
        "We introduce the notion of integral Ricci curvature $I_{\\kappa_0}$ for\ngraphs, which measures the amount of Ricci curvature below a given threshold\n$\\kappa_0$. We focus our attention on the Lin-Lu-Yau Ricci curvature. As\napplications, we prove a Bonnet-Myers-type diameter estimate, a Moore-type\nestimate on the number of vertices of a graph in terms of the maximum degree\n$d_M$ and diameter $D$, and a Lichnerowicz-type estimate for the first\neigenvalue $\\lambda_1$ of the Graph Laplacian, generalizing the results\nobtained by Lin, Lu, and Yau. All estimates are uniform, depending only on\ngeometric parameters like $\\kappa_0$, $I_{\\kappa_0}$, $d_M$, or $D$, and do not\nrequire the graphs to be positively curved.",
        "Broadcasting on trees is a fundamental model from statistical physics that\nplays an important role in information theory, noisy computation and\nphylogenetic reconstruction within computational biology and linguistics. While\nthis model permits efficient linear-time algorithms for the inference of the\nroot from the leaves, recent work suggests that non-trivial computational\ncomplexity may be required for inference.\n  The inference of the root state can be performed using the celebrated Belief\nPropagation (BP) algorithm, which achieves Bayes-optimal performance. Although\nBP runs in linear time using real arithmetic operations, recent research\nindicates that it requires non-trivial computational complexity using more\nrefined complexity measures.\n  Moitra, Mossel, and Sandon demonstrated such complexity by constructing a\nMarkov chain for which estimating the root better than random guessing (for\ntypical inputs) is $NC^1$-complete. Kohler and Mossel constructed chains where,\nfor trees with $N$ leaves, achieving better-than-random root recovery requires\npolynomials of degree $N^{\\Omega(1)}$. The papers above raised the question of\nwhether such complexity bounds hold generally below the celebrated\nKesten-Stigum bound.\n  In a recent work, Huang and Mossel established a general degree lower bound\nof $\\Omega(\\log N)$ below the Kesten-Stigum bound. Specifically, they proved\nthat any function expressed as a linear combination of functions of at most\n$O(log N)$ leaves has vanishing correlation with the root. In this work, we get\nan exponential improvement of this lower bound by establishing an\n$N^{\\Omega(1)}$ degree lower bound, for any broadcast process in the whole\nregime below the Kesten-Stigum bound.",
        "The geodesic flow on a finite discrete q-manifold with or without boundary is\ndefined as as a permutation of its ordered q-simplices. This allows to define\ngeodesic sheets and a notion of sectional curvature.",
        "Controlling spin current lies at the heart of spintronics and its\napplications. The sign of spin currents is monotonous in ferromagnets once the\ncurrent direction is determined. Spin currents in antiferromagnets can possess\nopposite polarization, but requires enormous magnetic fields to lift the\ndegeneracy. Controlling spin currents with different polarization is urgently\ndemanded but remains hitherto elusive. Here, we demonstrate the control of spin\ncurrents at room temperature by magnon interference in a canted\nantiferromagnet, hematite recently also classified as an altermagnet.\nMagneto-optical characterization by Brillouin light scattering revealed that\nthe spatial periodicity of the beating patterns was tunable via the microwave\nfrequency. The inverse spin-Hall voltage changed sign as the frequency was\nscanned, i.e., a frequency-controlled switching of polarization in pure spin\ncurrents was obtained. Our work marks the use of antiferromagnetic magnon\ninterference to control spin currents, which substantially extends the horizon\nfor the emerging field of coherent antiferromagnetic spintronics.",
        "In this note we show that the support of a locally $k$-uniform measure in\n$\\mathbb R^{n+1}$ satisfies a kind of unique continuation property. As a\nconsequence, we show that locally uniformly distributed measures satisfy a\nweaker unique continuation property. This continues work of Kirchheim and\nPreiss (Math. Scand. 2002) and David, Kenig and Toro (Comm. Pure Appl. Math.\n2001) and lends additional evidence to the conjecture proposed by Kowalski and\nPreiss (J. Reine Angew. Math. 1987) that each connected component of the\nsupport of a locally $n$-uniform measure in $\\mathbb R^{n+1}$ is contained in\nthe zero set of a quadratic polynomial.",
        "Isospin symmetry, as the most precise flavor symmetry, can be used to extract\ninformation about hadronic dynamics. The effective Hamiltonian operators of\nbottom quark weak decays are zero under a series of isospin lowering operators\n$I_-^n$, which permits us to generate isospin sum rules without the\nWigner-Eckhart invariants. In this work, we derive hundreds of isospin sum\nrules for the two- and three-body non-leptonic decays of bottom baryons. They\nprovide hints for new decay modes and the isospin partners of pentaquark\nstates.",
        "Gaussian smoothing combined with a probabilistic framework for denoising via\nthe empirical Bayes formalism, i.e., the Tweedie-Miyasawa formula (TMF), are\nthe two key ingredients in the success of score-based generative models in\nEuclidean spaces. Smoothing holds the key for easing the problem of learning\nand sampling in high dimensions, denoising is needed for recovering the\noriginal signal, and TMF ties these together via the score function of noisy\ndata. In this work, we extend this paradigm to the problem of learning and\nsampling the distribution of binary data on the Boolean hypercube by adopting\nBernoulli noise, instead of Gaussian noise, as a smoothing device. We first\nderive a TMF-like expression for the optimal denoiser for the Hamming loss,\nwhere a score function naturally appears. Sampling noisy binary data is then\nachieved using a Langevin-like sampler which we theoretically analyze for\ndifferent noise levels. At high Bernoulli noise levels sampling becomes easy,\nakin to log-concave sampling in Euclidean spaces. In addition, we extend the\nsequential multi-measurement sampling of Saremi et al. (2024) to the binary\nsetting where we can bring the \"effective noise\" down by sampling multiple\nnoisy measurements at a fixed noise level, without the need for continuous-time\nstochastic processes. We validate our formalism and theoretical findings by\nexperiments on synthetic data and binarized images.",
        "The Pythagorean school attributed consonance in music to simplicity of\nfrequency ratios between musical tones. In the last two centuries, the\nconsonance curves developed by Helmholtz, Plompt and Levelt shifted focus to\npsycho-acoustic considerations in perceiving consonances. The appearance of\npeaks of these curves at the ratios considered by the Pythagorean school, and\nwhich were a consequence of an attempt to understand the world by nice\nmathematical proportions, remained a curiosity. This paper addresses this\ncuriosity, by describing a mathematical model of musical sound, along with a\nmathematical definition of consonance. First, we define pure, complex and mixed\ntones as mathematical models of musical sound. By a sequence of numerical\nexperiments and analytic calculations, we show that continuous cosine\nsimilarity, abbreviated as cosim, applied to these models quantifies the\nelusive concept of consonance as a frequency ratio which gives a local maximum\nof the cosim function. We prove that these maxima occur at the ratios\nconsidered as consonant in classical music theory. Moreover, we provide a\nsimple explanation why the number of musical intervals considered as consonant\nby musicians is finite, but has been increasing over the centuries.\nSpecifically, our formulas show that the number of consonant intervals changes\nwith the depth of the tone (the number of harmonics present).",
        "The injectivity of ReLU layers in neural networks, the recovery of vectors\nfrom clipped or saturated measurements, and (real) phase retrieval in\n$\\mathbb{R}^n$ allow for a similar problem formulation and characterization\nusing frame theory. In this paper, we revisit all three problems with a unified\nperspective and derive lower Lipschitz bounds for ReLU layers and clipping\nwhich are analogous to the previously known result for phase retrieval and are\noptimal up to a constant factor.",
        "In this short paper, we show that the McCool group does not satisfy the\nAndreadakis equality from degree $7$, and we give a lower bound for the size of\nthe difference between the two relevant filtrations. As a consequence, we see\nthat the Andreadakis problem for the McCool group does not stabilize.",
        "Rhombohedral tetralayer graphene has recently emerged as an exciting platform\nfor a possible chiral superconducting state. Here, we theoretically demonstrate\nand study the emergence of nonreciprocal superconductivity and an intrinsic\nsuperconducting diode effect in this system. Our results are based on a fully\nself-consistent framework for determining the superconducting order parameter\nfrom a Kohn-Luttinger mechanism to superconductivity and show that large diode\nefficiencies, $\\sim$ 60%, are achievable and highly tunable by an external\ndisplacement field. Moreover, we also find that the diodicity shows a\ncharacteristic angular dependence with multiple enhanced lobes, which depend on\nthe Fermi surface structure of the underlying normal state. Hence, our results\nsuggest that the intrinsic superconducting diode effect could provide insights\ninto the type of Fermi surface topology from which superconductivity arises.",
        "Nearly-doubly-regular tournaments have played significant roles in extremal\ngraph theory. In this note, we construct new cyclotomic nearly-doubly-regular\ntournaments and determine their spectrum by establishing a new connection\nbetween cyclotomic nearly-doubly-regular tournaments and almost difference sets\nfrom combinatorial design theory. Furthermore, under the celebrated\nHardy-Littlewood conjecture F in analytic number theory, our results confirm\nthe conjecture due to Sergey Savchenko (J. Graph Theory {\\bf 83} (2016),\n44--77) on the existence of infinitely many nearly-doubly-regular tournaments\nwith the canonical spectrum.",
        "Spin waves in ferromagnetic materials are predominantly characterized by\nright-handed circular polarization due to symmetry breaking induced by net\nmagnetization. However, magnetic interactions, including the external magnetic\nfield, Heisenberg exchange, Dzyaloshinskii-Moriya interaction, and\ndipole-dipole interaction, can modify this behavior, leading to elliptical\npolarization. This study provides a systematic analysis of these interactions\nand their influence on spin wave polarization, establishing principles to\npredict traits such as polarization degree and orientation based on equilibrium\nmagnetization textures. The framework is applied to diverse magnetic\nconfigurations, including spin spirals, domain walls, and Skyrmions, offering a\ncomprehensive yet simple approach to understanding polarization dynamics in\nferromagnetic systems.",
        "Periodic elements in finite type Artin--Tits groups are elements some\npositive power of which is central. We give a dynamical characterisation of\nperiodic elements via their action on the corresponding 2-Calabi--Yau category\nand on its space of (fusion equivariant) Bridgeland stability conditions. The\nmain theorem is that an element $\\beta$ is periodic if and only if $\\beta$ has\na fixed point in the stability manifold.",
        "Localized excitons in two-dimensional (2D) materials are considered as\npromising sources of single photons on demand. The photon indistinguishability\nas key figure of merit for quantum information processing is strongly\ninfluenced by the coupling of charge excitations to lattice vibrations of the\nsurrounding semiconductor material. Here, we quantify the impact of\nexciton-acoustic-phonon-interaction and cavity QED effects on photon\nindistinguishability in a Hong-Ou-Mandel setup by solving fully quantum\nmechanical equations for the coupled QD-cavity-phonon system including\nnon-Markovian effects. We find a strong reduction of indistinguishability\ncompared to 3D systems due to increased exciton-phonon coupling efficiency.\nMoreover, we show that the coherence properties of photons are significantly\ninfluenced by the finite phonon lifetime in the surrounding material giving\nrise to pure dephasing. Only if these limitations are overcome, localized\nexcitons in 2D semiconductors can become a new avenue for quantum light\nsources.",
        "The acceptance rate in Woodcock tracking algorithm is generalized to an\narbitrary position-dependent variable $q(x)$. A neural network is used to\noptimize $q(x)$, and the FOM value is used as the loss function. This idea\ncomes from physics informed neural network(PINN), where a neural network is\nused to represent the solution of differential equations. Here the neural\nnetwork $q(x)$ should solve the functional equations that optimize FOM. For a\n1d transmission problem with Gaussian absorption cross section, we observe a\nsignificant improvement of the FOM value compared to the constant $q$ case and\nthe original Woodcock method. Generalizations of the neural network\nWoodcock(NNW) method to 3d voxel models are waiting to be explored.",
        "Participants: This study employed a combination of Vector Autoregression\n(VAR) model and Graph Neural Networks (GNN) to systematically construct dynamic\ncausal inference. Multiple classic classification algorithms were compared,\nincluding Random Forest, Logistic Regression, XGBoost, Support Vector Machine\n(SVM), K-Nearest Neighbor (KNN), Gradient Boosting, and Multi Layer Perceptron\n(MLP). The SMOTE algorithm was used to undersample a small number of samples\nand employed Stratified K-fold Cross Validation. Results: This study included a\ntotal of 11,789 participants, including 6,334 females (53.73%) and 5,455 males\n(46.27%), with an average age of 65 years. Introduction of dynamic causal\ninference features has significantly improved the performance of almost all\nmodels. The area under the ROC curve of each model ranged from 0.78 to 0.83,\nindicating significant difference (P < 0.01). Among all the models, the\nGradient Boosting model demonstrated the highest performance and stability.\nModel explanation and feature importance analysis generated model\ninterpretation that illustrated significant contributors associated with risks\nof stroke. Conclusions and Relevance: This study proposes a stroke risk\nprediction method that combines dynamic causal inference with machine learning\nmodels, significantly improving prediction accuracy and revealing key health\nfactors that affect stroke. The research results indicate that dynamic causal\ninference features have important value in predicting stroke risk, especially\nin capturing the impact of changes in health status over time on stroke risk.\nBy further optimizing the model and introducing more variables, this study\nprovides theoretical basis and practical guidance for future stroke prevention\nand intervention strategies.",
        "The integration of large language models (LLMs) on low-power edge devices\nsuch as Raspberry Pi, known as edge language models (ELMs), has introduced\nopportunities for more personalized, secure, and low-latency language\nintelligence that is accessible to all. However, the resource constraints\ninherent in edge devices and the lack of robust ethical safeguards in language\nmodels raise significant concerns about fairness, accountability, and\ntransparency in model output generation. This paper conducts a comparative\nanalysis of text-based bias across language model deployments on edge, cloud,\nand desktop environments, aiming to evaluate how deployment settings influence\nmodel fairness. Specifically, we examined an optimized Llama-2 model running on\na Raspberry Pi 4; GPT 4o-mini, Gemini-1.5-flash, and Grok-beta models running\non cloud servers; and Gemma2 and Mistral models running on a MacOS desktop\nmachine. Our results demonstrate that Llama-2 running on Raspberry Pi 4 is\n43.23% and 21.89% more prone to showing bias over time compared to models\nrunning on the desktop and cloud-based environments. We also propose the\nimplementation of a feedback loop, a mechanism that iteratively adjusts model\nbehavior based on previous outputs, where predefined constraint weights are\napplied layer-by-layer during inference, allowing the model to correct bias\npatterns, resulting in 79.28% reduction in model bias.",
        "Dancing tins pair up,\n  But compressing the framework\n  Thwarts the displacements.\n  The density waves that develop in kagome metals ScV$_6$Sn$_6$ and\nLuNb$_6$Sn$_6$ at low temperature appear to arise from under-filled atomic\ncolumns within a V-Sn or Nb-Sn scaffolding. Compressing this network with\napplied pressure in ScV$_6$Sn$_6$ suppressed the structural transition\ntemperature by constraining atomic rattling and inhibiting the shifts that\ndefine the structural modulation. We predicted that the density wave transition\nin LuNb$_6$Sn$_6$ at 68 K would be suppressed by pressure as well. In this\nbrief study we examine the pressure dependence of the density wave transition\nby remeasuring resistance vs temperature up to 2.26 GPa. We found the\ntransition temperature is smoothly depressed and disappears around 1.9 GPa.\nThis result not only addresses our prediction, but strengthens the rattling\nchains origin of structural instabilities in the HfFe$_6$Ge$_6$-type kagome\nmetals.",
        "This study conducts a meticulous examination of the cosmological implications\ninherent in Rastall gravity and $f(R)$ gravity models, assessing their efficacy\nacross distinct cosmic epochs, from early universe structure formation to\nlate-time acceleration. In the initial stages, both models exhibit commendable\ncompatibility with observed features of structure formation, aligning with the\nestablished $\\Lambda$CDM model. The derived Jeans' wavenumbers for each model\nsupport their viability. However, as the cosmic timeline progresses into the\nlate universe, a discernible disparity surfaces. Utilizing the Markov Chain\nMonte Carlo method, we reconstruct the deceleration parameter $(q)$ and\nidentify Deceleration - Acceleration redshift transition values. For $f(R)$\ngravity, our results align closely with previous studies, emphasizing its\nsuperior ability to elucidate the recent cosmic acceleration. In contrast,\nRastall gravity exhibits distinct redshift transition values. Our rigorous\nanalysis underscores the prowess of $f(R)$ gravity in capturing the observed\ncosmic acceleration, positioning it as a compelling alternative to the\nconventional $\\Lambda$CDM model. The discernible shifts observed in the peaks\nof the CMB power spectrum and evolution of deceleration parameter (q) for both\n$f(R)$ gravity and Rastall gravity models in the Early and Late universe, in\nrelation to the $\\Lambda $CDM model, provide compelling evidence supporting the\nproposition that these alternative gravity models can account for the\nanisotropy of the universe without invoking the need for dark energy.",
        "Details of the electronic band structure in unconventional superconductors\nare key to the understanding of their fundamental ground state. The potential\nspin-triplet superconductor UTe$_2$, with $T_\\mathrm{c}\\approx 2.1\\,$K, has\nattracted attention recently. Its main Fermi surface consists of weakly\ncorrugated, two-dimensional Fermi-surface cylinders that run along the\ncrystallographic $c$ axis. In addition, there is evidence for the presence of\nan additional small three-dimensional band. This has been discussed\ncontroversially as it may be essential for the realization of superconductivity\nin UTe$_2$. Here, we investigate the angle-resolved magnetoresistance and Hall\neffect in bulk crystalline samples with current along the $c$ axis in fields up\nto $60\\,$T. We observe low-frequency magnetic quantum oscillations with light\neffective masses that are most pronounced for magnetic field applied along the\n$a$ axis. Two distinct frequencies indicate two separate changes in the\nFermi-surface topology, likely connected with Lifshitz transitions. We discuss\nthe origin of these oscillations in terms of magnetic breakdown, quantum\ninterference, and other potential mechanisms.",
        "In this paper we study the 3D gauge theory of two tensor gauge fields:\n$a_{\\mu\\nu}(x)$, which we take symmetric, and $B_{\\mu\\nu}(x)$, with no symmetry\non its indices. The corresponding invariant action is a higher-rank BF-like\nmodel, which is first considered from a purely field theoretical point of view,\nand the propagators with their poles and the degrees of freedom are studied.\nOnce matter is introduced, a fracton behaviour naturally emerges. We show that\nour theory can be mapped to the low-energy effective field theory describing\nthe Rank-2 Toric Code (R2TC). This relation between our covariant BF-like\ntheory and the R2TC is a higher-rank generalization of the equivalence between\nthe ordinary 3D BF theory and the Kitaev's Toric Code. In the last part of the\npaper we analyze the case in which the field $B_{\\mu\\nu}(x)$ is a symmetric\ntensor. It turns out that the obtained BF-like action can be cast into the sum\nof two rank-2 Chern-Simons actions, thus generalizing the ordinary abelian\ncase. Therefore, this represents a higher-rank generalization of the ordinary\n3D BF theory, which well describes the low-energy physics of quantum spin Hall\ninsulators in two spatial dimensions.",
        "Machine learning is rapidly making its pathway across all of the natural\nsciences, including physical sciences. The rate at which ML is impacting\nnon-scientific disciplines is incomparable to that in the physical sciences.\nThis is partly due to the uninterpretable nature of deep neural networks.\nSymbolic machine learning stands as an equal and complementary partner to\nnumerical machine learning in speeding up scientific discovery in physics. This\nperspective discusses the main differences between the ML and scientific\napproaches. It stresses the need to develop and apply symbolic machine learning\nto physics problems equally, in parallel to numerical machine learning, because\nof the dual nature of physics research.",
        "Mean field games (MFGs) offer a versatile framework for modeling large-scale\ninteractive systems across multiple domains. This paper builds upon a previous\nwork, by developing a state-of-the-art unified approach to decode or design the\nunknown stationary state of MFGs, in addition to the underlying parameter\nfunctions governing their behavior. This result is novel, even in the general\nrealm of inverse problems for nonlinear PDEs. By enabling agents to distill\ncrucial insights from observed data and unveil intricate hidden structures and\nunknown states within MFG systems, our approach surmounts a significant\nobstacle, enhancing the applicability of MFGs in real-world scenarios. This\nadvancement not only enriches our understanding of MFG dynamics but also\nbroadens the scope for their practical deployment in various contexts.",
        "The aim of this article is to advance the knowledge on the theory of skew\nleft braces. We introduce a sub class of skew left braces, which we denote by\n$\\mathcal{I}_n$, $n \\ge 1$, such that elements of the annihilator and lower\ncentral series' interact 'nicely' with respect to commutation. That allows us\nto define a concept of $n$-isoclinism of skew left braces in $\\mathcal{I}_n$,\nby using a concept of brace commutator words, which we have introduced. We\nprove various results on $1$-isoclinism (isoclinism) of skew left braces\nanalogous to results in group theory. For any two symmetric $n$-isoclinic skew\nleft braces $A$ and $B$, we prove that, there exist skew left braces $C$ and\n$R$ such that both $A$ and $B$ are $n$-isoclinic to both $C$ and $R$ and (i)\n$A$ and $B$ are quotient skew left braces of $C$; (ii) $A$ and $B$ are sub skew\nleft braces of $R$. Connections between a skew left brace and the group which\noccurs as a natural semi-direct product of additive and multiplicative groups\nof the skew left brace are investigated, and it is proved that $n$-isoclinism\nis preserved from braces to groups. We also show that various nilpotency\nconcepts on skew left braces are invariant under $n$-isoclinism.",
        "Recently, a formal analogy between the fluctuating diffusivity and\nthermodynamics has been proposed based on phenomena of heterogeneous diffusion\nobserved in living cells. This not only offers the analogs of the quantity of\nheat and work as well as the internal energy but also achieves that of the\nClausius inequality for the entropy concerning diffusivity fluctuations. Here,\na discussion is developed about constructing a heat-like engine in terms of the\nfluctuating diffusivity. The engine constitutes two kinds of processes with the\naverage diffusivity or the average local temperature being kept fixed, along\nwhich the fluctuation distribution obeys an exponential law. The efficiency of\nthe engine in a cycle, which quantifies how much the diffusivity change as the\nanalog of work can be extracted, is found to formally coincide with that of\nCarnot's. During the cycle, the total change of the entropy is also shown to\nvanish."
      ]
    }
  }
]