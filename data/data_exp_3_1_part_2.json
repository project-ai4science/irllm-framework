[
  {
    "id":2411.03551,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"Interobserver Variability in the CT Assessment of Honeycombing in the Lungs",
    "start_abstract":"To quantify observer agreement and analyze causes of disagreement in identifying honeycombing at chest computed tomography (CT).The institutional review board approved this multiinstitutional HIPAA-compliant retrospective study, informed patient consent was not required. Five core study members scored 80 CT images with a five-point scale (5 = definitely yes to 1 no) establish reference standard for the identification honeycombing. Forty-three observers from various subspecialties geographic regions by using same scoring system. Weighted \u03ba values scores compared were analyzed investigate intergroup differences. Images divided into four groups allow analysis imaging features cases which there disagreement: on presence honeycombing, absence other (none preceding three applied).Agreement 43 moderate (Cohen weighted values: 0.40-0.58). There no significant differences among defined either subspecialty or region (Tukey-Kramer test, P .38 >.99). In 29% cases, These included mixed traction bronchiectasis, large cysts, superimposed pulmonary emphysema.Identification is subjective, largely caused conditions that mimic",
    "start_categories":[
      "Radiology"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b9"
      ],
      "title":[
        "DuPL: Dual Student with Trustworthy Progressive Learning for Robust Weakly Supervised Semantic Segmentation"
      ],
      "abstract":[
        "Recently, One-stage Weakly Supervised Semantic Segmentation (WSSS) with image-level labels has gained increasing interest due to simplification over its cumbersome multi-stage counterpart. Limited by the inherent ambiguity of Class Activation Map (CAM), we observe that one-stage pipelines often encounter confirmation bias caused by incorrect CAM pseudo-labels, impairing their final segmentation performance. Although recent works discard many unreliable pseudo-labels to implicitly alleviate this issue, they fail to exploit sufficient supervision for their models. To this end, we propose a dual student framework with trustworthy progressive learning (DuPL). Specifically, we propose a dual student network with a discrepancy loss to yield diverse CAMs for each sub-net. The two sub-nets generate supervision for each other, mitigating the confirmation bias caused by learning their own incorrect pseudo-labels. In this process, we progressively introduce more trustworthy pseudo-labels to be involved in the supervision through dynamic threshold adjustment with an adaptive noise filtering strategy. Moreover, we believe that every pixel, even discarded from supervision due to its unreliability, is important for WSSS. Thus, we develop consistency regularization on these discarded regions, providing supervision of every pixel. Experiment results demonstrate the superiority of the proposed DuPL over the recent state-of-the-art alternatives on PASCAL VOC 2012 and MS COCO datasets."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "Discrete curve theory in space forms: planar elastic and\n  area-constrained elastic curves",
        "ScaleMAI: Accelerating the Development of Trusted Datasets and AI Models",
        "Ilargi: a GPU Compatible Factorized ML Model Training Framework",
        "Binary $k$-Center with Missing Entries: Structure Leads to Tractability",
        "3-symmetric spaces, Ricci solitons, and homogeneous structures",
        "Mixed Reality Outperforms Virtual Reality for Remote Error Resolution in\n  Pick-and-Place Tasks",
        "A numerical toy model of Langevin dynamics provides real-time\n  visualization of colloidal microdroplet evaporation",
        "Space-Time-Coupled Qubits for Enhanced Superconducting Quantum Computing",
        "A simple and flexible algorithm to generate real-world networks",
        "Development of a Test System for Data Links of the ATLAS Inner Tracker\n  (ITk) Upgrade Silicon Pixel Detector",
        "SFC-GAN: A Generative Adversarial Network for Brain Functional and\n  Structural Connectome Translation",
        "MoBA: Mixture of Block Attention for Long-Context LLMs",
        "CIBPU: A Conflict-Invisible Secure Branch Prediction Unit",
        "Cayley trees and increasing 1,2-trees: let's twist!",
        "Formation of Magnonic Waveguides via Surface Anisotropy-Induced Bragg\n  Mirrors",
        "Triple Difference Designs with Heterogeneous Treatment Effects",
        "On a new proof of the key step in the proof of Brouwer's fixed point\n  theorem",
        "From time crystals to time quasicrystals: Exploring novel phases in\n  transverse field Ising chains",
        "Efficient measure of information backflow with quasi-stochastic process",
        "The Paradox of Success in Evolutionary and Bioinspired Optimization:\n  Revisiting Critical Issues, Key Studies, and Methodological Pathways",
        "On a Problem of Kac concerning Anisotropic Lacunary Sums",
        "On Computational Complexity of 3D Ising Spin Glass: Lessons from D-Wave\n  Annealer",
        "Human Genome Book: Words, Sentences and Paragraphs",
        "Optimizing Return Distributions with Distributional Dynamic Programming",
        "Demystifying the Accuracy-Interpretability Trade-Off: A Case Study of\n  Inferring Ratings from Reviews",
        "Fast Variational Boosting for Latent Variable Models",
        "Matroid intersection and packing\/covering conjectures are true in the\n  class of finitary matroids",
        "Rigorous analytic solution to the gravitational-wave overlapping event\n  rates",
        "Orientation tracking method for anisotropic particles"
      ],
      "abstract":[
        "We propose a notion of discrete elastic and area-constrained elastic curves\nin 2-dimensional space forms. Our definition extends the well-known discrete\nEuclidean curvature equation to space forms and reflects various geometric\nproperties known from their smooth counterparts. Special emphasis is paid to\ndiscrete flows built from B\\\"acklund transformations in the respective space\nforms. The invariants of the flows form a hierarchy of curves and we show that\ndiscrete elastic and constrained elastic curves can be characterized as\nelements of this hierarchy. This work also includes an introductory chapter on\ndiscrete curve theory in space forms, where we find discrete Frenet-type\nformulas and describe an associated family related to a fundamental theorem.",
        "Building trusted datasets is critical for transparent and responsible Medical\nAI (MAI) research, but creating even small, high-quality datasets can take\nyears of effort from multidisciplinary teams. This process often delays AI\nbenefits, as human-centric data creation and AI-centric model development are\ntreated as separate, sequential steps. To overcome this, we propose ScaleMAI,\nan agent of AI-integrated data curation and annotation, allowing data quality\nand AI performance to improve in a self-reinforcing cycle and reducing\ndevelopment time from years to months. We adopt pancreatic tumor detection as\nan example. First, ScaleMAI progressively creates a dataset of 25,362 CT scans,\nincluding per-voxel annotations for benign\/malignant tumors and 24 anatomical\nstructures. Second, through progressive human-in-the-loop iterations, ScaleMAI\nprovides Flagship AI Model that can approach the proficiency of expert\nannotators (30-year experience) in detecting pancreatic tumors. Flagship Model\nsignificantly outperforms models developed from smaller, fixed-quality\ndatasets, with substantial gains in tumor detection (+14%), segmentation (+5%),\nand classification (72%) on three prestigious benchmarks. In summary, ScaleMAI\ntransforms the speed, scale, and reliability of medical dataset creation,\npaving the way for a variety of impactful, data-driven applications.",
        "The machine learning (ML) training over disparate data sources traditionally\ninvolves materialization, which can impose substantial time and space overhead\ndue to data movement and replication. Factorized learning, which leverages\ndirect computation on disparate sources through linear algebra (LA) rewriting,\nhas emerged as a viable alternative to improve computational efficiency.\nHowever, the adaptation of factorized learning to leverage the full\ncapabilities of modern LA-friendly hardware like GPUs has been limited, often\nrequiring manual intervention for algorithm compatibility. This paper\nintroduces Ilargi, a novel factorized learning framework that utilizes\nmatrix-represented data integration (DI) metadata to facilitate automatic\nfactorization across CPU and GPU environments without the need for costly\nrelational joins. Ilargi incorporates an ML-based cost estimator to\nintelligently selects between factorization and materialization based on data\nproperties, algorithm complexity, hardware environments, and their\ninteractions. This strategy ensures up to 8.9x speedups on GPUs and achieves\nover 20% acceleration in batch ML training workloads, thereby enhancing the\npracticability of ML training across diverse data integration scenarios and\nhardware platforms. To our knowledge, this work is the very first effort in\nGPU-compatible factorized learning.",
        "$\\kC$ clustering is a fundamental classification problem, where the task is\nto categorize the given collection of entities into $k$ clusters and come up\nwith a representative for each cluster, so that the maximum distance between an\nentity and its representative is minimized. In this work, we focus on the\nsetting where the entities are represented by binary vectors with missing\nentries, which model incomplete categorical data. This version of the problem\nhas wide applications, from predictive analytics to bioinformatics.\n  Our main finding is that the problem, which is notoriously hard from the\nclassical complexity viewpoint, becomes tractable as soon as the known entries\nare sparse and exhibit a certain structure. Formally, we show fixed-parameter\ntractable algorithms for the parameters vertex cover, fracture number, and\ntreewidth of the row-column graph, which encodes the positions of the known\nentries of the matrix. Additionally, we tie the complexity of the 1-cluster\nvariant of the problem, which is famous under the name Closest String, to the\ncomplexity of solving integer linear programs with few constraints. This\nimplies, in particular, that improving upon the running times of our algorithms\nwould lead to more efficient algorithms for integer linear programming in\ngeneral.",
        "The full classification of Riemannian $3$-symmetric spaces is presented. Up\nto Riemannian products the main building blocks consist in (possibly symmetric)\nspaces with semisimple isometry group, nilpotent Lie groups of step at most $2$\nand spaces of type III and IV.\n  For the most interesting family of examples, the Type III spaces, we produce\nan explicit description including results concerning the moduli space of all\n$3$-symmetric metrics living on a given Type III space. Each moduli space\ncontains a unique distinguished point corresponding to an (almost-K\\\"ahler)\nexpanding Ricci soliton metric. For certain classes of 3-symmetric metrics\nthere are many different groups acting transitively and isometrically on a\nfixed Riemannian 3-symmetric space. The construction of expanding Ricci\nsolitons on spaces of Type III is also shown to generalize to \\emph{any}\neffective representation of a simple Lie group of non-compact type, yielding a\nvery general construction of homogeneous Ricci solitons. We also give a\nprocedure to compute the isometry group of any Ambrose--Singer space.",
        "This study evaluates the performance and usability of Mixed Reality (MR),\nVirtual Reality (VR), and camera stream interfaces for remote error resolution\ntasks, such as correcting warehouse packaging errors. Specifically, we consider\na scenario where a robotic arm halts after detecting an error, requiring a\nremote operator to intervene and resolve it via pick-and-place actions.\nTwenty-one participants performed simulated pick-and-place tasks using each\ninterface. A linear mixed model (LMM) analysis of task resolution time,\nusability scores (SUS), and mental workload scores (NASA-TLX) showed that the\nMR interface outperformed both VR and camera interfaces. MR enabled\nsignificantly faster task completion, was rated higher in usability, and was\nperceived to be less cognitively demanding. Notably, the MR interface, which\nprojected a virtual robot onto a physical table, provided superior spatial\nunderstanding and physical reference cues. Post-study surveys further confirmed\nparticipants' preference for MR over other interfaces.",
        "We have developed and tested a simplified but versatile numerical model of\nnanoparticles' aggregation using Langevin dynamics. The model is particularly\ncapable of simulating aggregation in an evaporating (or condensing)\nmicrodroplet. It runs on a graphics processing unit (GPU), which makes it\nsufficiently fast for real-time conceptualization tasks. We have verified the\nresults of modeling against the findings from two types of experiments we\nconducted in electrodynamic traps. Firstly, our model helped us to elucidate\nthe phenomenon of scattering `revival', often observed during the evaporation\nof composite microdroplets. Further on, we were able to mimic our experiments,\nin which the microdroplets were dried up to form nanoparticle (NP) aggregates,\nand then soft-landed. Thus we could compare model predictions with SEM imaging.\nThe model was tested for up to $2.5\\times 10^5$ nanoparticles of several\ncoexisting types. Several types of interactions can be accounted for:\ninter-particle: Lennard-Jones and Coulomb; external: dispersion medium\nviscosity, centrifugal force, gravity, surface tension, and interface movement.\nBrownian motion of nanoparticles can be freely controlled. The core program is\naccompanied by scripts extracting statistical NP aggregates properties in\npost-processing -- fractal dimension and radial distribution functions. The\ncodes are made available in public repositories. Several diverse evolution\nscenarios are presented.",
        "The pursuit of scalable and robust quantum computing necessitates innovative\napproaches to overcome the inherent challenges of qubit connectivity,\ndecoherence, and susceptibility to noise and crosstalk. Conventional\nmonochromatic qubit coupling architectures, constrained by nearest-neighbor\ninteractions and limited algorithmic flexibility, exacerbate these issues,\nhindering the realization of practical large-scale quantum processors. In this\nwork, we introduce a paradigm leveraging a space-time-modulated\ncryogenic-compatible Josephson metasurface to enable polychromatic qubit\ncoupling. This metasurface facilitates frequency-selective interactions,\ntransforming nearest-neighbor connectivity into all-to-all qubit interactions,\nwhile significantly enhancing coherence, noise robustness, and entanglement\nfidelity. Our proposed approach capitalizes on the unique capabilities of\nspace-time-modulated Josephson metasurfaces, including dynamic four-dimensional\nwave manipulation, nonreciprocal state transmission, and state-frequency\nconversion, to mediate multi-frequency qubit interactions. By isolating qubit\ncouplings into distinct spectral channels, the cryogenic-compatible metasurface\nmitigates crosstalk and environmental decoherence, extending coherence times\nand preserving quantum state fidelity. Full-wave simulations and quantum\nperformance analyses demonstrate a significant enhancement in the operational\nefficiency of a superconducting qubit array, showcasing improved connectivity,\nrobustness, and entanglement stability. This study establishes the potential of\nspace-time-modulated cryogenic-compatible Josephson metasurfaces as a\ntransformative platform for next-generation quantum computing, addressing\ncritical bottlenecks and paving the way for scalable, high-performance quantum\nprocessors.",
        "This study introduces an algorithm that generates undirected graphs with\nthree main characteristics of real-world networks: scale-freeness, short\ndistances between nodes (small-world phenomenon), and large clustering\ncoefficients. The main idea is to perform random walks across the network and,\nat each iteration, add special edges with a decreasing probability to link more\ndistant nodes, following a specific probability distribution. A key advantage\nof our algorithm is its simplicity and flexibility in creating networks with\ndifferent characteristics without using global information about network\ntopology. We show how the parameters can be adjusted to generate networks with\nspecific average distances and clustering coefficients, maintaining a\nlong-tailed degree distribution. The implementation of our algorithm is\npublicly available on a GitHub repository.",
        "This contribution introduces a novel test system developed to evaluate the\nsignal transmission quality in high-speed data links for the 2026 Inner Tracker\n(ITk) upgrade of the ATLAS experiment. Using an FPGA-based data acquisition\n(DAQ) framework, the setup can run simultaneous Bit Error Rate (BER) tests for\nup to 64 channels and generate virtual eye diagrams, for qualifying the\n$\\sim$26k electrical links at the ATLAS ITk data rate of 1.28Gb\/s. The paper\nincludes results from system calibration, yielding its contribution to the\nmeasured losses, and preliminary results from tests of prototype and\npre-production assemblies of on-detector links of the three ATLAS ITk Pixel\nsubsystems.",
        "Modern brain imaging technologies have enabled the detailed reconstruction of\nhuman brain connectomes, capturing structural connectivity (SC) from diffusion\nMRI and functional connectivity (FC) from functional MRI. Understanding the\nintricate relationships between SC and FC is vital for gaining deeper insights\ninto the brain's functional and organizational mechanisms. However, obtaining\nboth SC and FC modalities simultaneously remains challenging, hindering\ncomprehensive analyses. Existing deep generative models typically focus on\nsynthesizing a single modality or unidirectional translation between FC and SC,\nthereby missing the potential benefits of bi-directional translation,\nespecially in scenarios where only one connectome is available. Therefore, we\npropose Structural-Functional Connectivity GAN (SFC-GAN), a novel framework for\nbidirectional translation between SC and FC. This approach leverages the\nCycleGAN architecture, incorporating convolutional layers to effectively\ncapture the spatial structures of brain connectomes. To preserve the\ntopological integrity of these connectomes, we employ a structure-preserving\nloss that guides the model in capturing both global and local connectome\npatterns while maintaining symmetry. Our framework demonstrates superior\nperformance in translating between SC and FC, outperforming baseline models in\nsimilarity and graph property evaluations compared to ground truth data, each\ntranslated modality can be effectively utilized for downstream classification.",
        "Scaling the effective context length is essential for advancing large\nlanguage models (LLMs) toward artificial general intelligence (AGI). However,\nthe quadratic increase in computational complexity inherent in traditional\nattention mechanisms presents a prohibitive overhead. Existing approaches\neither impose strongly biased structures, such as sink or window attention\nwhich are task-specific, or radically modify the attention mechanism into\nlinear approximations, whose performance in complex reasoning tasks remains\ninadequately explored.\n  In this work, we propose a solution that adheres to the ``less structure''\nprinciple, allowing the model to determine where to attend autonomously, rather\nthan introducing predefined biases. We introduce Mixture of Block Attention\n(MoBA), an innovative approach that applies the principles of Mixture of\nExperts (MoE) to the attention mechanism. This novel architecture demonstrates\nsuperior performance on long-context tasks while offering a key advantage: the\nability to seamlessly transition between full and sparse attention, enhancing\nefficiency without the risk of compromising performance. MoBA has already been\ndeployed to support Kimi's long-context requests and demonstrates significant\nadvancements in efficient attention computation for LLMs. Our code is available\nat https:\/\/github.com\/MoonshotAI\/MoBA.",
        "Previous schemes for designing secure branch prediction unit (SBPU) based on\nphysical isolation can only offer limited security and significantly affect\nBPU's prediction capability, leading to prominent performance degradation.\nMoreover, encryption-based SBPU schemes based on periodic key re-randomization\nhave the risk of being compromised by advanced attack algorithms, and the\nperformance overhead is also considerable. To this end, this paper proposes a\nconflict-invisible SBPU (CIBPU). CIBPU employs redundant storage design,\nload-aware indexing, and replacement design, as well as an encryption mechanism\nwithout requiring periodic key updates, to prevent attackers' perception of\nbranch conflicts. We provide a thorough security analysis, which shows that\nCIBPU achieves strong security throughout the BPU's lifecycle. We implement\nCIBPU in a RISC-V core model in gem5. The experimental results show that CIBPU\ncauses an average performance overhead of only 1.12%-2.20% with acceptable\nhardware storage overhead, which is the lowest among the state-of-the-art SBPU\nschemes. CIBPU has also been implemented in the open-source RISC-V core,\nSonicBOOM, which is then burned onto an FPGA board. The evaluation based on the\nboard shows an average performance degradation of 2.01%, which is approximately\nconsistent with the result obtained in gem5.",
        "An increasing 1,2-tree is a labeled graph formed by starting with a vertex\nand then repeatedly attaching a leaf to a vertex or a triangle to an edge, the\nlabeling of the vertices corresponding to the order in which the vertices are\nadded. Equivalently, increasing 1,2-trees are connected chordal graphs of\ntreewidth at most 2 labeled with a reversed perfect elimination ordering. We\nprove that this family is equinumerous with Cayley trees, which are\nunconstrained labeled trees. In particular, the number of triangles in an\nincreasing 1,2-tree corresponds to the number of twists. A twist (also called\nimproper edge) is an edge whose endpoint closer to vertex 1 has a greater label\nthan some vertex in the subtree rooted at the other endpoint of the edge. We\nprovide three proofs of this result, the rst being based on similar recursive\ndecompositions, the second on the resolution of generating functions, and the\nthird describing a bijection. Finally, we propose ecient random generators for\nthese two combinatorial families.",
        "Waveguides are fundamental components for signal transmission in integrated\nwave-based processing systems. In this paper, we address the challenges\nassociated with designing magnonic waveguides and propose a novel type with\npromising properties. Specifically, we study a magnonic waveguide formed within\na uniform ferromagnetic layer (Co$_{20}$Fe$_{60}$B$_{20}$) through surface\nanisotropy applied in stripe regions, creating Bragg mirror structures. The\nproposed waveguide enables the propagation of high-frequency spin waves with\nhigh velocities in the ferromagnetic layer while avoiding static demagnetizing\neffects. Using finite element simulations, we calculate the dispersion relation\nof the waveguide modes and analyze their spatial profiles. Additionally, we\nevaluate the group velocity and localization characteristics, providing a\ncomprehensive understanding of the waveguide's performance.",
        "Triple difference designs have become increasingly popular in empirical\neconomics. The advantage of a triple difference design is that, within\ntreatment group, it allows for another subgroup of the population --\npotentially less impacted by the treatment -- to serve as a control for the\nsubgroup of interest. While literature on difference-in-differences has\ndiscussed heterogeneity in treatment effects between treated and control groups\nor over time, little attention has been given to the implications of\nheterogeneity in treatment effects between subgroups. In this paper, I show\nthat interpretation of the usual triple difference parameter of interest, the\ndifference in average treatment effects on the treated between subgroups, may\nbe affected by this kind of heterogeneity. I propose a new parameter of\ninterest, the causal difference in average treatment effects on the treated,\nwhich makes causal comparisons between subgroups. I discuss assumptions for\nidentification and derive the semiparametric efficiency bounds for this\nparameter. I then propose doubly-robust, efficient estimators for this\nparameter. I use a simulation study to highlight the desirable finite-sample\nproperties of these estimators, as well as to show the difference between this\nparameter and the usual triple difference parameter of interest. An empirical\napplication shows the importance of considering treatment effect heterogeneity\nin practical applications.",
        "We present a solution of Exercise 1.2.1 of [2] which yields a short new proof\nof a key step in one of proofs of Brouwer's fixed point theorem, 1910. A few\npeople asked the author about the details of the solution and they might be\ninteresting to a broader audience. Our approach is absolutely different from\nthe ones using algebraic or differential topology or differential calculus and\nis based on a simple observation which somehow escaped many authors treating\nthis theorem in the past.",
        "Time crystals (TCs) and time quasicrystals (TQCs) represent novel phases of\nmatter that arise from the breaking of time translational symmetry in\nperiodically and quasiperiodically driven quantum systems. In this study, we\nexplore the formation of a TQC phase within the disordered quantum Ising chain\nmodel under a transverse field (ITF). Notably, TQCs demonstrate robust\nsubharmonic responses at multiple incommensurate frequencies, unlike\ntraditional TCs which respond at a single frequency. Our analysis reveals that\nthe TQC phase exhibits stable magnetization responses even in the presence of\ninteraction perturbations and imperfections in the quasiperiodic driving\nfields. Employing exact diagonalization techniques, we find that increasing the\nchain length further stabilizes both TC and TQC phases. These results suggest\npromising pathways for experimental realization of TQCs in cold atomic systems\nand quantum simulators, opening avenues for deeper investigation into these\nintriguing dynamical phenomena.",
        "Characterization and quantification of non-Markovian dynamics in open quantum\nsystems is a topical issue in the rapidly developing field of quantum\ncomputation and quantum communication. A standard approach based on the notion\nof information backflow detects the flow of information from the environment\nback to the system. Numerous measures of information backflow have been\nproposed using different definitions of distinguishability between pairs of\nquantum states. These measures, however, necessitate optimization over the\nstate space which can be analytically challenging or numerically demanding.\nHere, we propose an alternative witness and measure of information backflow\nthat is explicitly state-independent by utilizing the concept of\nquasiprobability representation and recent advances in the theory of\nmajorization for quasiprobabilities. We illustrate its use over several\nparadigmatic examples, demonstrating consistent Markovian conditions with known\nresults and also reported necessary and sufficient condition for qutrit system\nin random unitary channel. The paper concludes with discussions on the\nfoundational implications of quantum dynamical evolution.",
        "Evolutionary and bioinspired computation are crucial for efficiently\naddressing complex optimization problems across diverse application domains. By\nmimicking processes observed in nature, like evolution itself, these algorithms\noffer innovative solutions beyond the reach of traditional optimization\nmethods. They excel at finding near-optimal solutions in large, complex search\nspaces, making them invaluable in numerous fields. However, both areas are\nplagued by challenges at their core, including inadequate benchmarking,\nproblem-specific overfitting, insufficient theoretical grounding, and\nsuperfluous proposals justified only by their biological metaphor. This\noverview recapitulates and analyzes in depth the criticisms concerning the lack\nof innovation and rigor in experimental studies within the field. To this end,\nwe examine the judgmental positions of the existing literature in an informed\nattempt to guide the research community toward directions of solid contribution\nand advancement in these areas. We summarize guidelines for the design of\nevolutionary and bioinspired optimizers, the development of experimental\ncomparisons, and the derivation of novel proposals that take a step further in\nthe field. We provide a brief note on automating the process of creating these\nalgorithms, which may help align metaheuristic optimization research with its\nprimary objective (solving real-world problems), provided that our identified\npathways are followed. Our conclusions underscore the need for a sustained push\ntowards innovation and the enforcement of methodological rigor in prospective\nstudies to fully realize the potential of these advanced computational\ntechniques.",
        "Given a lacunary sequence $(n_k)_{k \\in \\mathbb{N}}$, arbitrary positive\nweights $(c_k)_{k \\in \\mathbb{N}}$ that satisfy a Lindeberg-Feller condition,\nand a function $f: \\mathbb{T} \\to \\mathbb{R}$ whose Fourier coefficients\n$\\hat{f_k}$ decay at rate $\\frac{1}{k^{1\/2 + \\varepsilon}}$, we prove central\nlimit theorems for $\\sum_{k \\leq N}c_kf(n_kx)$, provided $(n_k)_{k \\in\n\\mathbb{N}}$ satisfies a Diophantine condition that is necessary in general.\nThis addresses a question raised by M. Kac [Ann. of Math., 1946].",
        "Finding an exact ground state of a 3D Ising spin glass is proven to be an\nNP-hard problem. Given validity of the exponential time hypothesis, its\ncomputational complexity was proven to be no less than $2^{N^{2\/3}}$, where $N$\nis the total number of spins. Here we report results of extensive\nexperimentation with D-Wave 3D annealer with $N\\leq 5627$. We found exact\nground states (in a probabilistic sense) for typical realizations of 3D spin\nglasses with the efficiency, which scales as $2^{N\/\\beta}$ with $\\beta\\approx\n10^{3}$. Based on statistical analysis of low energy states, we argue that with\nan improvement of annealing protocols and device noise reduction, $\\beta$ can\nbe increased even further. This suggests that, for $N<\\beta^3$, annealing\ndevices provide a most efficient way to find the ground state.",
        "Since the completion of the human genome sequencing project in 2001,\nsignificant progress has been made in areas such as gene regulation editing and\nprotein structure prediction. However, given the vast amount of genomic data,\nthe segments that can be fully annotated and understood remain relatively\nlimited. If we consider the genome as a book, constructing its equivalents of\nwords, sentences, and paragraphs has been a long-standing and popular research\ndirection. Recently, studies on transfer learning in large language models have\nprovided a novel approach to this challenge.Multilingual transfer ability,\nwhich assesses how well models fine-tuned on a source language can be applied\nto other languages, has been extensively studied in multilingual pre-trained\nmodels. Similarly, the transfer of natural language capabilities to \"DNA\nlanguage\" has also been validated. Building upon these findings, we first\ntrained a foundational model capable of transferring linguistic capabilities\nfrom English to DNA sequences. Using this model, we constructed a vocabulary of\nDNA words and mapped DNA words to their English equivalents.Subsequently, we\nfine-tuned this model using English datasets for paragraphing and sentence\nsegmentation to develop models capable of segmenting DNA sequences into\nsentences and paragraphs. Leveraging these models, we processed the GRCh38.p14\nhuman genome by segmenting, tokenizing, and organizing it into a \"book\"\ncomprised of genomic \"words,\" \"sentences,\" and \"paragraphs.\" Additionally,\nbased on the DNA-to-English vocabulary mapping, we created an \"English version\"\nof the genomic book. This study offers a novel perspective for understanding\nthe genome and provides exciting possibilities for developing innovative tools\nfor DNA search, generation, and analysis.",
        "We introduce distributional dynamic programming (DP) methods for optimizing\nstatistical functionals of the return distribution, with standard reinforcement\nlearning as a special case. Previous distributional DP methods could optimize\nthe same class of expected utilities as classic DP. To go beyond expected\nutilities, we combine distributional DP with stock augmentation, a technique\npreviously introduced for classic DP in the context of risk-sensitive RL, where\nthe MDP state is augmented with a statistic of the rewards obtained so far\n(since the first time step). We find that a number of recently studied problems\ncan be formulated as stock-augmented return distribution optimization, and we\nshow that we can use distributional DP to solve them. We analyze distributional\nvalue and policy iteration, with bounds and a study of what objectives these\ndistributional DP methods can or cannot optimize. We describe a number of\napplications outlining how to use distributional DP to solve different\nstock-augmented return distribution optimization problems, for example\nmaximizing conditional value-at-risk, and homeostatic regulation. To highlight\nthe practical potential of stock-augmented return distribution optimization and\ndistributional DP, we combine the core ideas of distributional value iteration\nwith the deep RL agent DQN, and empirically evaluate it for solving instances\nof the applications discussed.",
        "Interpretable machine learning models offer understandable reasoning behind\ntheir decision-making process, though they may not always match the performance\nof their black-box counterparts. This trade-off between interpretability and\nmodel performance has sparked discussions around the deployment of AI,\nparticularly in critical applications where knowing the rationale of\ndecision-making is essential for trust and accountability. In this study, we\nconduct a comparative analysis of several black-box and interpretable models,\nfocusing on a specific NLP use case that has received limited attention:\ninferring ratings from reviews. Through this use case, we explore the intricate\nrelationship between the performance and interpretability of different models.\nWe introduce a quantitative score called Composite Interpretability (CI) to\nhelp visualize the trade-off between interpretability and performance,\nparticularly in the case of composite models. Our results indicate that, in\ngeneral, the learning performance improves as interpretability decreases, but\nthis relationship is not strictly monotonic, and there are instances where\ninterpretable models are more advantageous.",
        "We consider the problem of estimating complex statistical latent variable\nmodels using variational Bayes methods. These methods are used when exact\nposterior inference is either infeasible or computationally expensive, and they\napproximate the posterior density with a family of tractable distributions. The\nparameters of the approximating distribution are estimated using optimisation\nmethods. This article develops a flexible Gaussian mixture variational\napproximation, where we impose sparsity in the precision matrix of each\nGaussian component to reflect the appropriate conditional independence\nstructure in the model. By introducing sparsity in the precision matrix and\nparameterising it using the Cholesky factor, each Gaussian mixture component\nbecomes parsimonious (with a reduced number of non-zero parameters), while\nstill capturing the dependence in the posterior distribution. Fast estimation\nmethods based on global and local variational boosting moves combined with\nnatural gradients and variance reduction methods are developed. The local\nboosting moves adjust an existing mixture component, and optimisation is only\ncarried out on a subset of the variational parameters of a new component. The\nsubset is chosen to target improvement of the current approximation in aspects\nwhere it is poor. The local boosting moves are fast because only a small number\nof variational parameters need to be optimised. The efficacy of the approach is\nillustrated by using simulated and real datasets to estimate generalised linear\nmixed models and state space models.",
        "Given two finite matroids on the same ground set, a celebrated result of\nEdmonds says that the ground set can be partitioned into two disjoint subsets\nin a manner that there is a common independent set in both matroids whose\nintersection with the first subset spans that subset in the first matroid, and\nwhose intersection with the second subset spans that subset in the second\nmatroid.\n  There is a longstanding conjecture regarding the situation of two matroids\ndefined on the same infinite ground set. Infinite matroids were only recently\naxiomatized in the early 2010s in the work of Bruhn et al., while the\nconjecture had been proposed during the 1990s for the class of structures that\nare now called finitary matroids, which are matroids all of whose circuits are\nfinite sets.\n  The packing\/covering conjecture, due to Bowler and Carmesin, is a related\nconjecture in the sense that it is true in the class of all matroids if and\nonly if the matroid intersection conjecture is true in the class of all\nmatroids. Given any family of matroids on the same ground set, the conjecture\nasks if it is possible to partition the ground set into two disjoint subsets in\na way such that the corresponding family of matroids restricted to the first\nsubset admits a packing while the family of matroids contracted to the second\nsubset admits a covering.\n  We prove both of these conjectures in the class of finitary matroids. Our\nmain tool is nonstandard analysis, specifically the technique of iterated\nnonstandard extensions. Roughly, we first embed any infinite matroid inside a\nhyperfinite matroid defined on a subset of the nonstandard extension of the\noriginal ground set, and we iteratively nonstandardly extend the hyperfinite\nstructure again in order to prove results in the internal universe that can be\ndirectly transferred to obtain results about the matroid(s) we started with.",
        "In the era of the next-generation gravitational-wave detectors, signal\noverlaps will become prevalent due to high detection rate and long signal\nduration, posing significant challenges to data analysis. While effective\nalgorithms are being developed, there still lacks an integrated understanding\non the statistical properties for the population of overlapping events. For the\nfirst time we rigorously derive and establish analytical expressions for the\nexpectation and variance for the number of overlapping events to aid rapid and\nrobust estimation. We also mathematically prove that the time difference\nbetween events in a single observation run is described by the beta\ndistribution, offering an analytical prior reference for Bayesian analysis.",
        "A method for particle orientation tracking is developed and demonstrated\nspecifically for anisotropic particles. Using (high-speed) multi-camera\nrecordings of anisotropic particles from different viewpoints, we reconstruct\nthe 3D location and orientation of these particles using their known shape.\nThis paper describes an algorithm which tracks the location and orientation of\nmultiple anisotropic particles over time, enabling detailed investigations of\nlocation, orientation, and rotation statistics. The robustness and error of\nthis method is quantified, and we explore the effects of noise, image size, the\nnumber of used cameras, and the camera arrangement by applying the algorithm to\nsynthetic images. We showcase several use-cases of this method in several\nexperiments (in both quiescent and turbulent fluids), demonstrating the\neffectiveness and broad applicability of the described tracking method. The\nproposed method is shown to work for widely different particle shapes,\nsuccessfully tracks multiple particles simultaneously, and the method can\ndistinguish between different types of particles."
      ]
    }
  },
  {
    "id":2411.03551,
    "research_type":"applied",
    "start_id":"b9",
    "start_title":"DuPL: Dual Student with Trustworthy Progressive Learning for Robust Weakly Supervised Semantic Segmentation",
    "start_abstract":"Recently, One-stage Weakly Supervised Semantic Segmentation (WSSS) with image-level labels has gained increasing interest due to simplification over its cumbersome multi-stage counterpart. Limited by the inherent ambiguity of Class Activation Map (CAM), we observe that one-stage pipelines often encounter confirmation bias caused by incorrect CAM pseudo-labels, impairing their final segmentation performance. Although recent works discard many unreliable pseudo-labels to implicitly alleviate this issue, they fail to exploit sufficient supervision for their models. To this end, we propose a dual student framework with trustworthy progressive learning (DuPL). Specifically, we propose a dual student network with a discrepancy loss to yield diverse CAMs for each sub-net. The two sub-nets generate supervision for each other, mitigating the confirmation bias caused by learning their own incorrect pseudo-labels. In this process, we progressively introduce more trustworthy pseudo-labels to be involved in the supervision through dynamic threshold adjustment with an adaptive noise filtering strategy. Moreover, we believe that every pixel, even discarded from supervision due to its unreliability, is important for WSSS. Thus, we develop consistency regularization on these discarded regions, providing supervision of every pixel. Experiment results demonstrate the superiority of the proposed DuPL over the recent state-of-the-art alternatives on PASCAL VOC 2012 and MS COCO datasets.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Interobserver Variability in the CT Assessment of Honeycombing in the Lungs"
      ],
      "abstract":[
        "To quantify observer agreement and analyze causes of disagreement in identifying honeycombing at chest computed tomography (CT).The institutional review board approved this multiinstitutional HIPAA-compliant retrospective study, informed patient consent was not required. Five core study members scored 80 CT images with a five-point scale (5 = definitely yes to 1 no) establish reference standard for the identification honeycombing. Forty-three observers from various subspecialties geographic regions by using same scoring system. Weighted \u03ba values scores compared were analyzed investigate intergroup differences. Images divided into four groups allow analysis imaging features cases which there disagreement: on presence honeycombing, absence other (none preceding three applied).Agreement 43 moderate (Cohen weighted values: 0.40-0.58). There no significant differences among defined either subspecialty or region (Tukey-Kramer test, P .38 >.99). In 29% cases, These included mixed traction bronchiectasis, large cysts, superimposed pulmonary emphysema.Identification is subjective, largely caused conditions that mimic"
      ],
      "categories":[
        "Radiology"
      ]
    },
    "list":{
      "title":[
        "Quantum Spin Correlation Amplification Enables Macroscopic Detection of\n  Atomic-Level Fatigue in Ferromagnetic Metals",
        "Electron-phonon coupling and phonon dynamics in single-layer NbSe$_2$ on\n  graphene: the role of moir\\'e phonons",
        "Stability of N-front and N-back solutions in the Barkley model",
        "Optimizing the Critical Temperature and Superfluid Density of a\n  Metal-Superconductor Bilayer",
        "Medium-band Astrophysics with the Grism of NIRCam In Frontier fields\n  (MAGNIF): Spectroscopic Census of H$\\alpha$ Luminosity Functions and Cosmic\n  Star Formation at $z\\sim 4.5$ and 6.3",
        "Three-dimensional chiral active Ornstein-Uhlenbeck model for helical\n  motion of microorganisms",
        "Self-consistent solution to the semiclassical Einstein equations of a\n  star",
        "Star-crossed Clusters: Asteroseismic Ages for Individual Stars are in\n  Tension with the Ages of their Host Clusters",
        "Constraints on 1-0 texture through neutrino phenomenology and dark\n  matter in minimal inverse seesaw",
        "HyperNOs: Automated and Parallel Library for Neural Operators Research",
        "The MACIV multiscale seismic experiments in the French Massif Central\n  (2023-2027): deployment, data quality and availability",
        "On the convergence rate of noisy Bayesian Optimization with Expected\n  Improvement",
        "Microscopic description of quadrupole-hexadecapole coupling in radium,\n  thorium, uranium and plutonium isotopes with the Gogny energy density\n  functional",
        "Simulating Hawking radiation in quantum many-body systems: deviations\n  from the thermal spectrum",
        "Dynamics of DNA-Templated Ultrafine Silver Nanowires Formation",
        "Proceedings of the 14th International Computational Accelerator Physics\n  Conference (ICAP24)",
        "Quantization of the Momentum Map via $\\frak{g}$-adapted Formalities",
        "Annealed mean-field epidemiological model on scale-free networks with a\n  mitigating factor",
        "Anisotropic flows of identified hadrons in the equal-velocity quark\n  combination model at RHIC energy",
        "On refined enumerations of plane partitions of a given shape with\n  bounded entries",
        "Stable antiferroelectric phase in calcium-doped lead scandium tantalate",
        "Symmetric channel verification for purifying noisy quantum channels",
        "Measuring the degree of clustering and diffusion of trans-Neptunian\n  objects",
        "Do Short GRBs Exhibit an Anticorrelation between Their Intrinsic\n  Duration and Redshift?",
        "Asymmetry analysis of Autler-Townes doublet in the trap-loss\n  fluorescence spectroscopy of cesium MOT with single step Rydberg excitation",
        "Cylindrically confined $H$ atom in magnetic field: variational cut-off\n  factor",
        "The Normal Play of the Domination Game",
        "Perturbing finite temperature multicomponent DFT 1D Kohn-Sham systems:\n  Peierls Gap & Kohn Anomaly",
        "Populations of Neutron Star Ultraluminous X-ray Sources: Mind your b's\n  and B's"
      ],
      "abstract":[
        "Structural fatigue failures account for most of catastrophic metal component\nfailures, annually causing thousands of accidents, tens of thousands of\ncasualties, and $100 billion in global economic losses. Current detection\nmethods struggle to identify early-stage fatigue damage characterized by\nsub-nanometer atomic displacements and localized bond rupture. Here we present\na quantum-enhanced monitoring framework leveraging the fundamental symbiosis\nbetween metallic bonding forces and magnetic interactions. Through magnetic\nexcitation of quantum spin correlation in metallic structures, we establish a\nmacroscopic quantum spin correlation amplification technology that visualizes\nfatigue-induced magnetic flux variations corresponding to bond strength\ndegradation. Our multi-scale analysis integrates fatigue life prediction with\nquantum mechanical parameters (bonding force constants, crystal orbital overlap\npopulation) and ferromagnetic element dynamics, achieving unprecedented\nprediction accuracy (R^2>0.9, p<0.0001). In comprehensive fatigue trials\nencompassing 193 ferromagnetic metal specimens across 3,700 testing hours, this\nquantum magnetic signature consistently provided macroscopic fracture warnings\nprior to failure - a critical advance enabling 100% early detection success.\nThis transformative framework establishes the first operational platform for\npreemptive fatigue mitigation in critical infrastructure, offering a paradigm\nshift from post-failure analysis to quantum-enabled predictive maintenance.",
        "The interplay between substrate interactions and electron-phonon coupling in\ntwo-dimensional (2D) materials presents a significant challenge in\nunderstanding and controlling their electronic properties. Here, we present a\ncomparative study of the structural characteristics, phonon dynamics, and\nelectron-phonon interactions in bulk and monolayer NbSe$_2$ on epitaxial\nbilayer graphene (BLG) using helium atom scattering (HAS). High-resolution\nhelium diffraction reveals a (9x9)0$^{\\circ}$ superstructure within the\nNbSe$_2$ monolayer, commensurate with the BLG lattice, while out-of-plane HAS\ndiffraction spectra indicate a low-corrugated\n(3$\\sqrt{3}$x3$\\sqrt{3}$)30$^{\\circ}$ substructure. By monitoring the thermal\nattenuation of the specular peak across a temperature range of 100 K to 300 K,\nwe determined the electron-phonon coupling constant $\\lambda_{HAS}$ as 0.76 for\nbulk 2H-NbSe$_2$. In contrast, the NbSe$_2$ monolayer on graphene exhibits a\nreduced $\\lambda_{HAS}$ of 0.55, corresponding to a superconducting critical\ntemperature (T$_C$) of 1.56 K according to the MacMillan formula, consistent\nwith transport measurement findings. Inelastic HAS data provide, besides a set\nof dispersion curves of acoustic and lower optical phonons, a soft,\ndispersionless branch of phonons at 1.7 meV, attributed to the interface\nlocalized defects distributed with the superstructure period, and thus termed\nmoir\\'e phonons. Our data show that moir\\'e phonons contribute significantly to\nthe electron-phonon coupling in monolayer NbSe$_2$. These results highlight the\ncrucial role of the BLG on the electron-phonon coupling in monolayer NbSe$_2$,\nattributed to enhanced charge transfer effects, providing valuable insights\ninto substrate-dependent electronic interactions in 2D superconductors.",
        "In this paper we establish for an intermediate Reynolds number domain the\nstability of N-front and N-back solutions for each N > 1 corresponding to\ntraveling waves, in an experimentally validated model for the transition to\nturbulence in pipe flow proposed in [Barkley et al., Nature 526(7574):550-553,\n2015]. We base our work on the existence analysis of a heteroclinic loop\nbetween a turbulent and a laminar equilibrium proved by Engel, Kuehn and de\nRijk in [Engel, Kuehn, de Rijk, Nonlinearity 35:5903, 2022], as well as some\nresults from this work. The stability proof follows the verification of a set\nof abstract stability hypotheses stated by Sandstede in [SIAM Journal on\nMathematical Analysis 29.1 (1998), pp. 183-207] for traveling waves motivated\nby the FitzHugh-Nagumo equations. In particular, this completes the first\ndetailed analysis of Engel, Kuehn and de Rijk in [Engel, Kuehn, de Rijk,\nNonlinearity 35:5903, 2022] leading to a complete existence and stability\nstatement that nicely fits within the abstract framework of waves generated by\ntwisted heteroclinic loops.",
        "A promising path to realizing higher superconducting transition temperatures\n$T_c$ is the strategic engineering of artificial heterostructures. For example,\nquantum materials exhibiting some but not all of the characteristics necessary\nfor a robust superconducting state could, in principle, be coupled with other\nmaterials in a way that alleviates their intrinsic shortcomings. In this work,\nwe add numerical support to the hypothesis that a strongly interacting\nsuperconductor weakened by phase fluctuations can boost its $T_c$ by\nhybridizing the system with a metal. Using determinant quantum Monte Carlo\n(DQMC), we simulate a two-dimensional bilayer composed of an attractive Hubbard\nmodel and a metallic layer in two regimes of the interaction strength $-|U|$.\nIn the strongly interacting regime, we find that increasing the interlayer\nhybridization $t_\\perp$ results in a nonmonotonic enhancement of $T_c$, with an\noptimal value comparable to the maximum $T_c$ observed in the single-layer\nattractive Hubbard model, confirming trends inferred from other approaches. In\nthe intermediate coupling regime, when $-|U|$ is close to the value associated\nwith the maximum $T_c$ of the single-layer model, increasing $t_\\perp$ tends to\ndecrease $T_c$, implying that the correlated layer was already optimally tuned.\nImportantly, we demonstrate that the mechanism behind these trends is related\nto enhancement in the superfluid stiffness, as was initially proposed by\nKivelson [Physica B: Condensed Matter 318, 61 (2002)].",
        "We measure H$\\alpha$ luminosity functions (LFs) at redshifts $z \\sim 4.5$ and\n6.3 using the JWST MAGNIF (Medium-band Astrophysics with the Grism of NIRCam In\nFrontier fields) survey. MAGNIF obtained NIRCam grism spectra with the F360M\nand F480M filters in four Frontier Fields. We identify 248 H$\\alpha$ emitters\nbased on the grism spectra and photometric redshifts from combined HST and JWST\nimaging data. The numbers of the H$\\alpha$ emitters show a large field-to-field\nvariation, highlighting the necessity of multiple fields to mitigate cosmic\nvariance. We calculate both observed and dust-corrected H$\\alpha$ LFs in the\ntwo redshift bins. Thanks to the gravitational lensing, the measured H$\\alpha$\nLFs span three orders of magnitude in luminosity, and the faint-end luminosity\nreaches $L_{\\mathrm{H}\\alpha} \\sim 10^{40.3} \\mathrm{erg} \\mathrm{s}^{-1}$ at\n$z \\sim 4.5$ and $10^{41.5} \\mathrm{erg} \\mathrm{s}^{-1}$ at $z \\sim 6.3$,\ncorresponding to star-formation rates (SFRs) of $\\sim$ 0.1 and 1.7\n$\\mathrm{M}_\\odot \\mathrm{yr}^{-1}$. We conclude no or weak redshift evolution\nof the faint-end slope of H$\\alpha$ LF across $z\\simeq0.4-6.3$, and the\ncomparison with the faint-end slopes of UV LF indicates stochastic star\nformation history among low-mass H$\\alpha$ emitters. The derived cosmic SFR\ndensities are $0.058^{+0.008}_{-0.006}\\ \\ M_\\odot\\ \\mathrm{yr}^{-1}\\\n\\mathrm{Mpc}^{-3}$ at $z \\sim 4.5$ and $0.025^{+0.009}_{-0.007}\\ \\ M_\\odot\\\n\\mathrm{yr}^{-1}\\ \\mathrm{Mpc}^{-3}$ at $z \\sim 6.3$. These are approximately\n2.2 times higher than previous estimates based on dust-corrected UV LFs, but\nconsistent with recent measurements from infrared surveys. We discuss\nuncertainties in the H$\\alpha$ LF measurements, including those propagate from\nthe lens models, cosmic variance, and AGN contribution.",
        "Active movement is essential for the survival of microorganisms like\nbacteria, algae and unicellular parasites, for example the ones causing the\ndisease malaria. In three dimensions, both swimming and gliding microorganisms\noften exhibit helical trajectories. Here we introduce a stochastic dynamics\nmodel for chiral self-propelled particles, for which both propulsion force and\ntorque are internally generated, the latter stochastically by an Ornstein\nUhlenbeck process. We demonstrate that a truncated version of the full model\ncan be solved analytically, in very good agreement with computer simulations\nfor the full model. Our main result is that this model allows for larger\nlong-time mean squared displacements for helical compared to straight 3D\nmovement at the same speed, suggesting an evolutionary benefit of the often\nobserved helical movements of microorganisms, and opposite to the reduction of\ndiffusion caused by chirality in 2D. We then provide an experimental example by\nanalyzing imaging data for malaria parasites that glide through hydrogels on\nhelical trajectories.",
        "We present the interior solution for a static, spherically symmetric perfect\nfluid star backreacted by QFT in four dimensions invoking no arbitrary\nparameters. It corresponds to a constant energy density star and is fully\nnon-perturbative. The space of solutions includes ultra-compact configurations\nthat have neither singularities nor light rings inside the star and can exist\narbitrarily close to the Schwarzschild limit, showing that the classical\nparadigm of astrophysics does not hold once QFT in curved space is taken into\naccount.",
        "A meta-analysis of seismic ages determined for individual stars in the\nwell-studied open and globular clusters NGC 6819, NGC 6791, M67, M4, M19, M80,\nand M9 reveals both high variance across measurements and significant\ndiscrepancy with independent, isochrone-based age determinations for the\nclusters in which these stars reside. The scatter among asteroseismic ages for\nindividual stars in any one of these clusters far surpasses both the absolute\nage uncertainty computed for reference cluster M92 (5.4\\%) and the\nmodel-to-model systematic uncertainties in isochrones (roughly 10\\%). This\nsuggests that either binary processes are significantly altering the masses of\nstars in these clusters, or some additional corrections, perhaps as a function\nof mass, metallicity, or surface gravity, are required to bring the\nasteroseismic age scale into concordance with ages inferred from isochrone or\nsimilar model fitting.",
        "In this work we have realized texture zero structures of neutrino mass matrix\nthrough our study of neutrino phenomenology and dark matter. For analysing\nthese processes, we have constructed a model in minimal inverse seesaw,\nISS(2,3) by using $A_4$ discrete symmetry. The particle content of ISS(2.3) has\nbeen augmented by a scalar triplet $\\eta=(\\eta_1,\\eta_2,\\eta_3)$. The probable\ndark matter candidates for this model are the neutral components of $\\eta$. The\nthree mass matices of ISS(2,3), $M_D$, $M_{NS}$ and $M_S$ contribute to the\nstructure of light neutrino mass matrix $m_\\nu$. Here we try to examine the\nimpact on texture structures of $m_\\nu$ due to different possible 2-0\nstructures of $M_D$. To examine further possible contraints, we have evaluated\nthe neutrino parameters and calculated relic density of dark matter for the\nfavourable cases. From our analysis we find that out of the fifteen possible\n2-0 structures, only two of them ($M_{D3}$ and $M_{D6}$) successfully generates\nall the mixing angles in the allowed ranges.",
        "This paper introduces HyperNOs, a PyTorch library designed to streamline and\nautomate the process of exploring neural operators, with a special focus on\nhyperparameter optimization for comprehensive and exhaustive exploration.\nIndeed, HyperNOs takes advantage of state-of-the-art optimization algorithms\nand parallel computing implemented in the Ray-tune library to efficiently\nexplore the hyperparameter space of neural operators. We also implement many\nuseful functionalities for studying neural operators with a user-friendly\ninterface, such as the possibility to train the model with a fixed number of\nparameters or to train the model with multiple datasets and different\nresolutions. We integrate Fourier neural operators and convolutional neural\noperators in our library, achieving state of the art results on many\nrepresentative benchmarks, demonstrating the capabilities of HyperNOs to handle\nreal datasets and modern architectures. The library is designed to be easy to\nuse with the provided model and datasets, but also to be easily extended to use\nnew datasets and custom neural operator architectures.",
        "In the framework of the MACIV project, a consortium of French laboratories\nhas deployed a temporary seismic network of 100 broadband stations in the\nFrench Massif Central (FMC) for 3-4 years (2023-2027). The project aims at\nimaging the crust and upper mantle of the FMC to better assess the sources of\nvolcanism, and the impacts of the Variscan inheritance or the Cenozoic rift\nsystem on volcanic systems. A large-scale array of 35 broadband stations covers\nthe entire FMC and complements the permanent networks to reach a homogeneous\ncoverage with ~35 km spacing. This network, with XP code, is the French\ncontribution to AdriaArray. The XP array is complemented with 3 quasi-linear\nnorth-south, east-west and northwest-southeast profiles with inter-station\nspacing of 5-20 km, making up the XF network of 65 stations. The profiles cross\nvolcanic areas and the main Variscan structures. We describe the experimental\nsetup designed to optimize the performance\/cost ratio and minimize the number\nof field visits, the deployment, the state-of-health monitoring, the data\nmanagement and the data quality control strategies, outcomes of our 15-years'\nexperience with major temporary seismic experiments in France and neighboring\ncountries, including AlpArray. We also show some preliminary results including\nhypocenter locations and receiver function analysis. The 2 broadband arrays\nwill be supplemented in 2025 by a month-long deployment of 3 large-N dense\narrays of 625 3-C short-period nodes. These dense arrays will complete our\nmulti-scale seismic experiment and illuminate active faults and possible\nplumbing systems of the youngest volcanoes.",
        "Expected improvement (EI) is one of the most widely used acquisition\nfunctions in Bayesian optimization (BO). Despite its proven success in\napplications for decades, important open questions remain on the theoretical\nconvergence behaviors and rates for EI. In this paper, we contribute to the\nconvergence theory of EI in three novel and critical areas. First, we consider\nobjective functions that fit under the Gaussian process (GP) prior assumption,\nwhereas existing works mostly focus on functions in the reproducing kernel\nHilbert space (RKHS). Second, we establish for the first time the asymptotic\nerror bound and its corresponding rate for GP-EI with noisy observations under\nthe GP prior assumption. Third, by investigating the exploration and\nexploitation properties of the non-convex EI function, we establish improved\nerror bounds of GP-EI for both the noise-free and noisy cases.",
        "The emergence and stability of static hexadecapole deformations as well as\nthe impact in the development of dynamic deformation due to collective motion\nconsidering quadrupole-hexadecapole coupling are studied for a selected set of\nradium, thorium, uranium and plutonium isotopes, using the Gogny\nHartree-Fock-Bogoliubov and Generator Coordinate Method frameworks. Sizable\nhexadecapole deformations are found to play a significant role in the ground\nand excited states of nuclei in the neighborhood of $^{238}$U. For each of the\nstudied isotopic chains, it is shown that a region with small negative\nhexadecapole deformation, just below the neutron magic number $N =184$, remains\nstable once zero-point quadrupole-hexadecapole fluctuations are taken into\naccount. A transition is predicted, with increasing mass number, from a regime\nin which the quadrupole and hexadecapole degrees of freedom are interwoven to a\nregime in which they are decoupled, accompanied by an enhanced shape\ncoexistence in the more neutron-rich sectors of the isotopic chains. It is also\nshown, that quadrupole-hexadecapole configuration mixing brings a nontrivial\nadditional correlation energy gain comparable to the quadrupole correlation\nenergy itself.",
        "We investigate a recently proposed one-to-one correspondence between quantum\nfield theories in two-dimensional curved spacetime and quantum many-body\nsystems, which enables the simulation of Hawking radiation in static background\nspacetimes. In particular, we demonstrate that deviations from the thermal\nspectrum, as predicted by the well-known tunneling method, can be observed in\nmany-body simulations.",
        "Recent research on silver nanowires prepared on DNA templates has focused on\ntwo fundamental applications: nano-scale circuits and sensors. Despite its\nbroad potential, the formation kinetics of DNA-templated silver nanowires\nremains unclear. Here, we present an experimental demonstration of the\nformation of silver nanowires with a diameter of 2.2+0.4 nm at the\nsingle-molecule level through chemical reduction. We conducted equilibrium and\nperturbation kinetic experiments to measure force spectroscopy during the\nformation of Ag+ -DNA complexes and Ag-DNA complexes, using optical tweezers\ncombined with microfluidics. The addition of AgNO3 resulted in an increase in\nforce of 5.5-7.5 pN within 2 minutes, indicating that Ag+ compacts the DNA\nstructure. In contrast, the addition of hydroquinone caused the force to\ndecrease by 4-5 pN. Morphological characterization confirmed the presence of a\ndense structure formed by silver atoms bridging the DNA strands, and revealed\nconformational differences before and after metallization. We compare our\nexperimental data with Brownian dynamics simulations using a coarse-grained\ndouble-stranded DNA (dsDNA) model that provides insights on the dependency of\nthe force on the persistence length.",
        "This is the proceedings of the 14th International Computational Accelerator\nPhysics Conference, ICAP'24, which was held at the Lufthansa Seeheim Conference\nHotel in Germany from October 2-5, 2024, hosted by TU Darmstadt and GSI\nHelmholtzzentrum f\\\"ur Schwerionenforschung. ICAP'24 has focused on advances in\nComputational Accelerator Physics and their application to existing machines\nand future facilities. It has provided a forum for researchers in modeling and\nsimulation to exchange information and discuss new ideas that benefit a wide\narea of accelerator science and technology. Topics of the conference have\nincluded computational needs and challenges, beam dynamics and electromagnetic\nfield calculations, code development and validation, data processing and\nvisualization, high performance computing, machine learning and advanced\noptimization as well as emerging technologies that will impact computing for\naccelerator design.",
        "In this note, we provide a proof of the existence and complete classification\nof $G$-invariant star products with quantum momentum maps on Poisson manifolds\nby means of an equivariant version of the formality theorem.",
        "An annealed version of the quenched mean-field model for epidemic spread is\nintroduced and investigated analytically and assisted by numerical\ncalculations. The interaction between individuals follows a prescription that\nis used to generate a scale-free network, and we have adjusted the number of\nconnections to produce a sparse network. Specifically, the model's behavior\nnear the infection threshold is examined, as well as the behavior of the\nstationary prevalence and the probability that a connection between individuals\nencounters an infected one. We found that these functions display a\nmonotonically increasing dependence on the infection rate. Subsequently, a\nmodification that mimics the mitigation in the probability of encountering an\ninfected individual is introduced, following an old idea rooted in the\nMalthus-Verhulst model. We found that this modification drastically changes the\nprobability that a connection meets an infected individual. However, despite\nthis change, it does not alter the monotonically increasing behavior of the\nstationary prevalence.",
        "We employ an equal-velocity quark combination model to study anisotropic\nflows $v_{2}$, $v_{3}$ and $v_{4}$ of identified hadrons at mid-rapidity in\nheavy-ion collisions at RHIC energies. Under the equal-velocity combination\nmechanism of constituent quarks at hadronization, we build analytical formulas\nof anisotropic flows of hadrons in terms of those of quarks just before\nhadronization. We systematically analyze the contribution of higher order flows\nof quarks, and show how simple formulas of $v_{2}$, $v_{3}$ and $v_{4}$ of\nidentified hadrons with the desired precision can be obtained by neglecting the\nsmall contribution of higher order flows of quarks. We systematically test\nthese simple formulas of hadronic flows by the experimental data of $v_{2}$,\n$v_{3}$ and $v_{4}$ of identified hadrons $\\phi$, $\\Lambda$, $\\Xi^{-}$,\n$\\Omega^{-}$, $\\bar{\\Lambda}$, $\\bar{\\Xi}^{+}$, $\\bar{\\Omega}^{+}$, $p$ and\n$\\bar{p}$ in Au+Au collisions at $\\sqrt{s_{NN}}=$ 19.6, 54.4 and 200 GeV, and\nwe find that the equal-velocity quark combination model can well describe the\nmeasured $v_{2}$, $v_{3}$ and $v_{4}$ of identified hadrons in Au+Au collisions\nat those collision energies. We further study the obtained anisotropic flows of\nquarks and find two scaling properties\\textcolor{red}{{} }which can be\nqualitatively understood by the hydrodynamic evolution of thermal quark medium\nproduced in relativistic heavy-ion collisions.",
        "In this paper, we consider plane partitions $\\text{PP}(\\lambda; m)$ of a\ngiven shape $\\lambda$, with entries at most $m$. We prove that the\ndistributions of two statistics on $\\text{PP}(\\lambda; m)$ coincide: one is the\nnumber of rows containing $0$ and the other is the number of rows containing\n$m$. We also provide a bijective proof.",
        "Antiferroelectrics are valuable dielectric materials, offering promise for\nboth high energy storage and solid-state caloric cooling applications. However,\nfew antiferroelectrics are known or available. Therefore, it is crucial to\ndiscover or design materials showing antiferroelectric behaviour. In this\nstudy, we fabricated highly ordered lead scandium tantalate ceramics doped with\ncalcium. From calorimetry and polarization-electric field loops, we demonstrate\nthe effect of calcium on the thermal properties and phase transition sequences\nof lead scandium tantalate. We identify an antiferroelectric phase appearing at\ntemperatures intermediate between the ferroelectric and paraelectric phases,\nwhich becomes increasingly stable as the calcium concentration increases. These\nfindings are supported by density functional theory calculations and Raman\nspectroscopy. Finally, we propose a phase diagram for calcium-doped lead\nscandium tantalate. Our results highlight the potential of stabilizing\nantiferroelectricity in ferroelectric perovskite materials through A-site\ndoping.",
        "Symmetry inherent in quantum states has been widely used to reduce the effect\nof noise in quantum error correction and a quantum error mitigation technique\nknown as symmetry verification. However, these symmetry-based techniques\nexploit symmetry in quantum states rather than quantum channels, limiting their\napplication to cases where the entire circuit shares the same symmetry. In this\nwork, we propose symmetric channel verification (SCV), a channel purification\nprotocol that leverages the symmetry inherent in quantum channels. By\nintroducing different phases to each symmetric subspace and employing a quantum\nphase estimation-like circuit, SCV can detect and correct symmetry-breaking\nnoise in quantum channels. We further propose a hardware-efficient\nimplementation of SCV at the virtual level, which requires only a single-qubit\nancilla and is robust against the noise in the ancilla qubit. Our protocol is\napplied to various Hamiltonian simulation circuits and phase estimation\ncircuits, resulting in a significant reduction of errors. Furthermore, in\nsetups where only Clifford unitaries can be used for noise purification, which\nis relevant in the early fault-tolerant regime, we show that SCV under Pauli\nsymmetry represents the optimal purification method.",
        "The outer solar system is populated by a broad aggregate of minor bodies,\nwhich occupy orbits whose dynamical character ranges from long-term stable to\nrapidly diffusive. We investigate the chaotic properties of known distant\ntrans-Neptunian objects (TNOs) by numerically integrating TNO clones and\nstatistically analyzing their orbital diffusion. Comparing the measured\ndiffusion with an analytical criterion yields a dynamically motivated\nseparation into classes of stable, metastable and unstable objects. We then\nmeasure the level of clustering of the longitudes of perihelia and of the\norbital poles, as functions of orbital distance and of their stability\nproperties. Distant (meta)stable objects appear increasingly clustered in\nperihelion around $\\varpi \\sim 50^\\circ$ for increasing semi-major axis, while\nthe orbits of unstable objects are well described by two, roughly\nequally-populated groups of \"clustered\" and \"anti-clustered\" objects, with\nmeans around $\\sim 25^\\circ$ and $\\sim 205^\\circ$ respectively. We further find\nthat, compared to the solar system's total angular momentum vector, the mean\norbital poles of distant TNOs are significantly more misaligned for\n(meta)stable objects, while they remain roughly aligned for unstable objects.\nTNOs with intermediate orbital periods also appear to be misaligned with\nrespect to the forced plane predicted by secular theory with the known planets.\nThis gradation based on stability, if validated further by the upcoming VRO\nsurvey, necessitates a dynamical explanation.",
        "Gamma-ray bursts (GRBs) are violent stellar explosions that are traditionally\ndivided into two groups: short bursts (SGRBs) with an observed duration T90 < 2\ns, and long bursts (LGRBs) with an observed duration T90 > 2 s, where T90\nrefers to the time needed for 90% of the fluence to be detected. Studies of\nprogenitor models suggest that LGRBs emanate from the core collapse of massive\nstars, while SGRBs result from the merging of two compact objects, like two\nneutron stars or a neutron star and a black hole. Recent studies have found\nevidence that there is an anticorrelation between the intrinsic duration and\nthe redshift of long GRBs. In this study, we first check whether LGRBs exhibit\nan anticorrelation between their intrinsic duration and redshift using an\nexpanded dataset of long bursts that we have compiled. Next, we investigate\nwhether this anticorrelation applies to SGRBs as well using a sample of short\nGRBs that we have compiled. Our analysis confirms the results obtained by\nprevious studies regarding the anticorrelation for LGRBs. On the other hand,\nour results indicate that short GRBs do not exhibit such an anticorrelation. We\ndiscuss the implications of our results in the context of how metallicity\nevolves with redshift and the role that it might play in the aforementioned\nanticorrelation.",
        "Autler-Townes (AT) doublet, a fundamental manifestation of quantum\ninterference effects, serves as a critical tool for studying the dynamic\nbehavior of Rydberg atoms. Here, we investigate the asymmetry of the\nAutler-Townes (AT) doublet in trap-loss fluorescence spectroscopy (TLFS) of\ncesium (Cs) atoms confined in a magneto-optical trap (MOT) with single-step\nRydberg excitation using a 319-nm ultraviolet (UV) laser. A V-type three-level\nsystem involving the ground state $6\\text{S}_{1\/2}$ ($\\text{F}$=4), excited\nstate $6\\text{P}_{3\/2}$ ($\\text{F}^{'}$=5) , and Rydberg state\n($n\\text{P}_{3\/2}$ ($\\text{m}_\\text{J}$=+3\/2)) is theoretically modeled to\nanalyze the nonlinear dependence of the AT doublet's asymmetry and interval on\nthe cooling laser detuning. Experiments reveal that as the cooling laser\ndetuning $\\Delta_1$ decreases from $-$15 MHz to $-$10 MHz, the AT doublet\nexhibits increasing symmetry, while its interval shows a nonlinear decrease.\nTheoretical simulations based on the density matrix equation and Lindblad\nmaster equation align closely with experimental data, confirming the model's\nvalidity. This study provides insights into quantum interference dynamics in\nmulti-level systems and offers a systematic approach for optimizing precision\nmeasurements in cold atom spectroscopy.",
        "In the present study, we consider the hydrogen atom confined within an\nimpenetrable infinite cylindrical cavity of radius $\\rho_{0}$ in the presence\nof a constant magnetic field ${\\bf B} = B\\,\\hat{\\bf z}$ oriented along the main\ncylinder's axis. In the Born-Oppenheimer approximation, anchoring the nucleus\nto the geometric center of the cylinder, a physically meaningful 3-parametric\ntrial function is used to determine the ground state energy $E$ of the system.\nThis trial function incorporates the exact symmetries and key limiting\nbehaviors of the problem explicitly. In particular, it does not treat the\nCoulomb potential nor the magnetic interaction as a \\textit{perturbation}. The\nnovel inclusion of a variational cut-off factor $\\big(1 -\n\\big(\\frac{\\rho}{\\rho_0}\\big)^\\nu\\big)$, $\\nu \\geq 1$, appears to represent a\nsignificant improvement compared to the non-variational cut off factors\ncommonly employed in the literature. The dependence of the total energy\n$E=E(\\rho_0,\\,B)$ and the binding energy $E_b=E_b(\\rho_0,\\,B)$ on the cavity\nradius $\\rho_0 \\in [0.8,\\,5] \\,$a.u. and the magnetic field strength $B\\in\n[0.0,\\,1.0]\\,$a.u. is presented in detail. The expectation values $\\langle \\rho\n\\rangle$ and $\\langle|z| \\rangle$, and the Shannon entropy in position space\nare computed to provide additional insights into the system's localization. A\nbrief discussion is provided comparing the 2D and 3D cases as well.",
        "In 2010, Bre\\v{s}ar, Klav\\v{z}ar and Rall introduced the optimization variant\nof the graph domination game and the game domination number. In 2024, Leo\nVersteegen obtained the celebrated proof of the Conjecture $\\frac{3}{5}$ on\nthis variant of the domination game, proposed by Kinnersley, West and Zamani in\n2013. In this paper, we investigate for the first time the normal play of the\ndomination game, which we call \\textsc{Normal Domination Game}, that is an\nimpartial game where the last to play wins. We use the Sprague-Grundy theory to\nprove that Alice (the first player) wins in the path $P_n$ if and only if $n$\nis not a multiple of $4$, and wins in the cycle $C_n$ if and only if $n=4k+3$\nfor some integer $k$. Finally, we obtain a polynomial time algorithm to decide\nthe winner for any disjoint union of paths and cycles in the \\textsc{Normal\nDomination Game} and its natural partizan variant.",
        "One of the greatest challenges when designing new technologies that make use\nof non-trivial quantum materials is the difficulty associated with predicting\nmaterial-specific properties, such as critical temperature, gap parameter, etc.\nThere is naturally a great amount of interest in these types of condensed\nmatter systems because of their application to quantum sensing, quantum\nelectronics, and quantum computation; however, they are exceedingly difficult\nto address from first principles because of the famous many-body problem. For\nthis reason, a full electron-nuclear quantum calculation will likely remain\ncompletely out of reach for the foreseeable future. A practical alternative is\nprovided by finite temperature, multi component density functional theory\n(MCDFT), which is a formally exact method of computing the equilibrium state\nenergy of a many-body quantum system. In this work, we use this construction\nalongside a perturbative scheme to demonstrate that the phenomena Peierls\neffect and Kohn Anomaly are both natural features of the KS equations without\nadditional structure needed. We find the temperature dependent ionic density\nfor a simple 1D lattice which is then used to derive the ionic densities\ntemperature dependent affect on the electronic band structure. This is\naccomplished by Fourier transforming the ionic density term found within this\nKS electronic equation. Using the Peierls effect phonon distortion gap openings\nin relation to the Fermi level, we then perturb the KS ionic equation with a\nconduction electron density, deriving the Kohn Anomaly. This provides a\nworkable predictive strategy for interesting electro-phonon related material\nproperties which could be extended to 2D and 3D real materials while retaining\nthe otherwise complicated temperature dependence.",
        "Ultraluminous X-ray sources (ULXs) with neutron star (NS) accretors challenge\ntraditional accretion models, and have sparked a debate regarding the role of\ngeometrical beaming and strong magnetic fields (B). The reduction of the\nThomson cross-section in the presence of strong B, leads to a modification of\nthe Eddington limit, and therefore is expected to affect significantly the\nobservational appearance of NS-ULXs. We investigate the role of this\nmodification using population synthesis models, and explore its effects on the\nX-ray luminosity functions, spin-up rates, and outflow energetics of the\nobserved NS-ULXs. Our results show that the new prescription allows NS-ULXs to\nachieve super-Eddington luminosities with milder beaming compared to before,\nimproving the agreement with observations. In addition, it broadens the range\nof spin-up rates allowing for more diverse conditions in NS-ULXs in terms of\naccretion rates and magnetic fields. More importantly, the reduced beaming\nincreases the likelihood of observing the NS-ULXs within wind-powered nebulae\nsuch as NGC 5907 ULX-1. Our findings highlight the necessity of taking into\naccount B effects independently of the approach: geometrical beaming or strong\nB, and call for magnetospheric accretion prescriptions that can be integrated\nin population synthesis codes."
      ]
    }
  },
  {
    "id":2411.09469,
    "research_type":"applied",
    "start_id":"b36",
    "start_title":"CBAM: Convolutional block attention module",
    "start_abstract":"We propose Convolutional Block Attention Module (CBAM), a simple yet effective attention module for feed-forward convolutional neural networks. Given an intermediate feature map, our module sequentially infers attention maps along two separate dimensions, channel and spatial, then the attention maps are multiplied to the input feature map for adaptive feature refinement. Because CBAM is a lightweight and general module, it can be integrated into any CNN architectures seamlessly with negligible overheads and is end-to-end trainable along with base CNNs. We validate our CBAM through extensive experiments on ImageNet-1K, MS~COCO detection, and VOC~2007 detection datasets. Our experiments show consistent improvements in classification and detection performances with various models, demonstrating the wide applicability of CBAM. The code and models will be publicly available.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b11"
      ],
      "title":[
        "The accuracy of colposcopic biopsy: Analyses from the placebo arm of the Gardasil clinical trials"
      ],
      "abstract":[
        "We evaluated the overall agreement between colposcopically directed biopsies and definitive excisional specimens within context of three clinical trials. A total 737 women aged 16-45 who had a cervical biopsy taken 6 months before their therapy were included. Per-protocol, colposcopists to also obtain representative immediately therapy. Using adjudicated histological diagnoses, initial same day correlated with surgically excised specimens. The therapy, diagnoses was 42% (weighted kappa = 0.34) (95% CI: 0.29-0.39). underestimation intraepithelial neoplasia grade 2\/3 or adenocarcinoma in situ (CIN2-3\/AIS) CIN3\/AIS 26 42%, respectively. When allowing for one degree variance correlation, 92% CIN2-3\/AIS. specimen 56% 0.41) 0.36-0.47), CIN2-3\/AIS 57%. There significant associations when patients stratified by age, number biopsies, lesion size, presence human papillomavirus (HPV)16\/18 region. Of 178 diagnostic endocervical curettages performed, 14 (7.9%) found any HPV disease. Colposcopic accuracy improved CIN2 grouped as single predictive measure high-grade Colposcopy functioned well allowed one-degree difference surgical histologic interpretations, done practice. Taking more than colposcopic could improve patient management."
      ],
      "categories":[
        "Clinical Trial"
      ]
    },
    "list":{
      "title":[
        "Beamfocusing and Power Allocation for AN-Based PLS in Multiuser XL-MIMO\n  with Multiple Eavesdroppers",
        "Understanding the core limitations of second-order correlation-based\n  functionals through: functional, orbital, and eigenvalue-driven analysis",
        "Two-quanta processes in coupled double-quantum-dot cavity systems",
        "Efficient cavity-mediated energy transfer between photosynthetic light\n  harvesting complexes from strong to weak coupling regime",
        "More on unconstrained descriptions of Higher Spin Massless Particles",
        "Freeze-and-release direct optimization method for variational\n  calculations of excited electronic states",
        "Unique existence of solution and Hyers-Ulam stability for a new\n  fractional differential quasi-variational inequality with Mittag-Leffler\n  kernel and its applications",
        "Compare Similarities Between DNA Sequences Using Permutation-Invariant\n  Quantum Kernel",
        "Advanced muon-spin spectroscopy with high lateral resolution using\n  Si-pixel detectors",
        "Cosmic Rays Masquerading as Hot CGM Gas: An Inverse-Compton Origin for\n  Diffuse X-ray Emission in the Circumgalactic Medium",
        "String islands, discrete theta angles and the 6D $\\mathcal{N} = (1,1)$\n  string landscape",
        "Highly reflective white clouds on the western dayside of an exo-Neptune",
        "Instability of Baryonic Black Branes",
        "Directional polynomial wavelets on spheres",
        "Halbach 2.0 -- Creating homogeneous fields with finite size magnets",
        "Preserving Simultaneity and Chronology for Sensing in Wireless\n  Perceptive Networks",
        "Large Anomalous Hall Effect in a Noncoplanar Magnetic Heterostructure",
        "Suppression of coherent errors during entangling operations in NV\n  centers in diamond",
        "Cosmological constraints from the Minkowski functionals of the BOSS\n  CMASS galaxy sample",
        "Finite symmetric algebras in tensor categories and Verlinde categories\n  of algebraic groups",
        "High-frequency readout free from transmon multi-excitation resonances",
        "A Bivariate Poisson-Gamma Distribution: Statistical Properties and\n  Practical Applications",
        "Mechanical resonant sensing of spin texture dynamics in a\n  two-dimensional antiferromagnet",
        "Kinematic power corrections to DVCS to twist-six accuracy",
        "The two extremal rays of some Hyper-K\\\"ahler fourfolds",
        "Construction of exact refinements for the two-dimensional HB\/THB-spline\n  de Rham complex",
        "Radiation of a particle performing helical motion in a multilayer\n  cylindrical waveguide",
        "Verification of the PICLS electromagnetic upgrade in mixed variables",
        "Si-compatible topological and infrared materials: the promise of Low-Sn\n  GeSn digital alloys"
      ],
      "abstract":[
        "This paper investigates the downlink (DL) physical layer security (PLS) in a\nnear-field (NF) extra-large multiple-input multiple-output MIMO (XL-MIMO)\nsystem. To enhance the secrecy rate (SR), null-space artificial noise (AN) is\ntransmitted alongside the confidential message, ensuring orthogonality with\nlegitimate user equipment (LUE) channels. The objective is to maximize the\nminimum SR by optimizing the NF beamfocusing matrix and power allocation\nbetween the signal and AN, considering various channel state information (CSI)\nconditions and transmit power constraints. The proposed approach uses\nsuccessive convex approximation (SCA) for beamfocusing optimization and golden\nsection search (GSS) for power allocation. The following open questions are\naddressed: (i) Can AN transmission further enhance SR for multiple LUEs in the\npresence of multiple eavesdropping user equipment (EUEs)? (ii) Can null-space\nAN transmission achieve attractive SR performance even without CSI availability\nfor EUEs? Both questions are affirmatively answered and explored in detail,\nwith an algorithm presented for joint beamfocusing design and AN-aided power\nallocation. The proposed method outperforms state-of-the-art approaches that\neither omit AN transmission or rely on maximal-ratio transmission (MRT) for\nbeamfocusing.",
        "Density Functional Theory has long struggled to obtain the exact\nexchange-correlational (XC) functional. Numerous approximations have been\ndesigned with the hope of achieving chemical accuracy. However, designing a\nfunctional involves numerous methodologies, which has a greater possibility for\nerror accumulation if the functionals are poorly formulated. This study aims to\ninvestigate the performance and limitations of second-order correlation\nfunctionals within the framework of density functional theory. Specifically, we\nfocus on three major classes of density functional approximations that\nincorporate second-order energy expressions: \\textit{ab initio} (primarily\nG\\\"{o}rling-Levy) functionals, adiabatic connection models, and double-hybrid\nfunctionals. The principal objectives of this research are to evaluate the\naccuracy of second-order correlation functionals, to understand how the choice\nof reference orbitals and eigenvalues affects the performance of these\nfunctionals, to identify the intrinsic limitations of second-order energy\nexpressions, especially when using arbitrary orbitals or non-canonical\nconfigurations, and propose strategies for improving their accuracy. By\naddressing these questions, we aim to provide deeper insights into the factors\ngoverning the accuracy of second-order correlation functionals, thereby guiding\nfuture functional development.",
        "The quantum dynamics of a compound sample consisting from a semiconductor\ndouble quantum dot (DQD) system non-linearly coupled with a leaking single-mode\nmicro-resonator is theoretically investigated. The focus is on the resonance\ncondition when the transition frequency of the double quantum dot equals to the\ndoubled resonator frequency, respectively, and the resulting interplay among\nthe involved phonon or photon channels. As a result, the steady-state quantum\ndynamics of this complex non-linear system exhibits a variety of possible\neffects that have been demonstrated here. Particularly, we have found the\nrelationship among the electrical current through the double quantum dot and\nthe microwave field inside the resonator that is nonlinearly coupled to it,\nwith a corresponding emphasizing on their critical behaviors. Additionally, the\nquantum correlations of the photon flux generated into the resonator mode vary\nfrom super-Poissonian to Poissonian photon statistics, leading to single-qubit\nlasing phenomena at microwave frequencies.",
        "Excitation energy transfer between photosynthetic light-harvesting complexes\nis vital for highly efficient primary photosynthesis. Controlling this process\nis the key for advancing the emerging artificial photosynthetic systems. Here,\nwe experimentally demonstrate the enhanced excitation energy transfer between\nphotosynthetic light-harvesting 2 complexes (LH2) mediated through the\nFabry-Perot optical microcavity. Using intensity-dependent pump-probe\nspectroscopy, we analyse the exciton-exciton annihilation (EEA) due to\ninter-LH2 energy transfer. Comparing EEA in LH2 within cavity samples and the\nbare LH2 films, we observe enhanced EEA in cavities indicating improved\nexcitation energy transfer via coupling to a common cavity mode. Surprisingly,\nthe effect remains even in the weak coupling regime. The enhancement is\nattributed to the additional connectivity between LH2s introduced by the\nresonant optical microcavity. Our results suggest that optical microcavities\ncan be a strategic tool for modifying excitation energy transfer between\nmolecular complexes, offering a promising approach towards efficient artificial\nlight harvesting.",
        "Here we suggest a new local action describing arbitrary integer spin-$s$\nmassless particles in terms of only two symmetric fields $\\varphi $ and\n$\\alpha$ of rank-$s$ and $(s-3)$ respectively. It is an unconstrained version\nof the Fronsdal theory where the double traceless constraint on the physical\nfield is evaded via a rank-$(s-4)$ Weyl like symmetry. The constrained higher\nspin diffeomorphism is enlarged to full diffeomorphism via the Stueckelberg\nfield $\\alpha$ through an appropriate field redefinition. After a partial gauge\nfixing where the Weyl symmetry is broken while preserving diffeomorphisms, the\nfield equations reproduce, for arbitrary integer spin-$s$, diffeomorphism\ninvariant equations of motion previously obtained via a truncation of the\nspectrum of the open bosonic string field theory in the tensionless limit. In\nthe $s=4$ case we show that the functional integration over $\\alpha$ leads to a\nunique non local Weyl and diffeomorphism invariant action given only in terms\nof the physical field $\\varphi$ whose spectrum is confirmed via an analysis of\nthe analytic structure of the spin-4 propagator for which we introduce a\ncomplete basis of projection and transition non local differential operators.\nWe also show that the elimination of $\\alpha$ after the Weyl gauge fixing leads\nto a non local diffeomorphism invariant action previously obtained in the\nliterature.",
        "Time-independent, orbital-optimized density functional approaches outperform\ntime-dependent density functional theory (TDDFT) in calculations of excited\nelectronic states involving a large rearrangement of the electron density, such\nas charge transfer excitations. However, optimizing orbitals for excited states\nremains challenging, as the latter typically correspond to saddle points on the\nelectronic energy surface. A simple and robust strategy for variational orbital\noptimization of excited states is presented. The approach involves two steps:\n(1) a constrained energy minimization, where a subset of orbitals changed by\nthe excitation are frozen, followed by (2) a fully unconstrained saddle point\noptimization. The constrained minimization step makes it possible to identify\nthe electronic degrees of freedom along which the energy needs to be maximized,\npreventing variational collapse. Both steps of this freeze-and-release strategy\nare carried out using direct optimization algorithms with a computational\nscaling comparable to ground state calculations. Numerical tests using a\nsemilocal functional are performed on intramolecular charge transfer states of\norganic molecules and intermolecular charge transfer states of molecular\ndimers. It is shown that the freeze-and-release direct optimization (FR-DO)\napproach can successfully converge challenging charge transfer states,\novercoming limitations of conventional algorithms based on the maximum overlap\nmethod, which either collapse to lower energy, charge-delocalized solutions or\nfail to converge. While FR-DO requires more iterations on average, the overall\nincrease in computational cost is small. For the NH3-F2 dimer, it is found that\nunlike TDDFT, orbital-optimized calculations reproduce the correct long-range\ndependency of the energy with respect to the donor-acceptor separation without\nthe need to include exact exchange in the long range.",
        "This paper considers a new fractional differential quasi-variational\ninequality with Mittag-Leffler kernel comprising a fractional differential\nequation with Mittag-Leffler kernel and a quasi-variational inequality in\nHilbert spaces. Some properties of the solution for the parameterized\nquasi-variational inequality are investigated, which improve the known results.\nMoreover, the unique existence of the solution and Hyers-Ulam stability are\nobtained for such a novel system under mild conditions. Finally, the obtained\nabstract results are applied to analyze the unique solvability and stability\nfor a multi-agent optimization problem and a price control problem.",
        "Computing the similarity between two DNA sequences is of vital importance in\nbioscience. However, traditional computational methods can be\nresource-intensive due to the enormous sequence length encountered in practice.\nRecently, applied quantum algorithms have been anticipated to provide potential\nadvantages over classical approaches. In this paper, we propose a\npermutation-invariant variational quantum kernel method specifically designed\nfor DNA comparison. To represent the four nucleotide bases in DNA sequences\nwith quantum states, we introduce a novel, theoretically motivated encoding\nscheme: the four distinct bases are encoded using the states of symmetric,\ninformationally complete, positive operator-valued measures (SIC-POVMs). This\nencoding ensures mutual equality: each pair of symbols is equidistant on the\nBloch sphere. Also, since permutation invariance is inherent to common DNA\nsimilarity measures such as Levenshtein distance, we realize it by using a\nspecially designed parameterized quantum layer. We show that our novel encoding\nmethod and parameterized layers used in the quantum kernel model can\neffectively capture the symmetric characteristics of the pairwise DNA sequence\ncomparison task. We validate our model through numerical experiments, which\nyield promising results on length-$8$ DNA sequences.",
        "Muon-spin spectroscopy at continuous sources has stagnated at a stopped muons\nrate of ~40 kHz for the last few decades. The major limiting factor is the\nrequirement of a single muon in the sample during the typical 10 {\\mu}s data\ngate window. To overcome this limit and to be able to perform muon-spin\nrelaxation ({\\mu}SR) measurements on millimeter-sized samples, one can use\nvertex reconstruction methods to construct {\\mu}SR spectra. This is now\npossible thanks to the availability of very thin monolithic Si-pixel chips,\nwhich offer minimal particle scattering and high count rate. Here we present\nresults from a Si-pixel based spectrometer that utilizes vertex reconstruction\nschemes for the incoming muons and emitted positrons. With this spectrometer we\nwere able to obtain a first vertex reconstructed {\\mu}SR (VR-{\\mu}SR) spectrum.\nThe unique capabilities and benefits of such a spectrometer are discussed.",
        "Observations have argued that Milky Way (MW), Andromeda, and lower-mass\ngalaxies exhibit extended soft X-ray diffuse halos to radii $R\\gtrsim100\\,$kpc\nin the circumgalactic medium (CGM). If interpreted as thermal emission, the\nshallow surface brightness profiles $S_{X}\\propto R^{-1}$ are difficult to\nexplain and contradict other observations. We show that such halos instead\narise from inverse Compton (IC) scattering of CMB photons with GeV cosmic ray\n(CR) electrons. GeV electrons have ~Gyr lifetimes and escape the galaxy,\nforming a shallow extended profile out to $\\gtrsim100\\,$kpc, where IC off the\nCMB should produce soft, thermal-like X-ray spectra peaked at ~keV. The\nobserved keV halo luminosities and brightness profiles agree well with those\nexpected for CRs observed in the local interstellar medium (LISM) escaping the\ngalaxy, with energetics consistent with known CRs from SNe and\/or AGN, around\ngalaxies with stellar masses $M_{\\ast}\\lesssim2\\times 10^{11}\\,M_{\\odot}$. At\nhigher masses observed X-ray luminosities are larger than predicted from IC and\nshould be dominated by hot gas. In the MW+M31, the same models of escaping CRs\nreproduce gamma-ray observations if we assume an LISM-like proton-to-electron\nratio and CR-pressure-dominated halo. In all other halos, the radio and\n$\\gamma$-ray brightness is below detectable limits. If true, the observations\nprovide qualitatively new constraints on CGM and CR physics: X-ray brightness\ndirectly traces the CR lepton energy density in the CGM. This agrees with LISM\nvalues within 10 kpc, which following the profile expected for escaping CRs in\nthe CGM. The inferred CR pressure is a major part of the MW CGM pressure\nbudget. X-ray surface brightness and luminosity allows one to further determine\nthe CGM diffusivity at radii $\\sim10-1000\\,$kpc. These also agree with LISM\nvalues at small radii but increase in the CGM.",
        "The complete classification of the landscape of 6D $\\mathcal{N} = (1,1)$\nstring vacua remains an open problem. In this work we prove a classification\ntheorem for 6D $\\mathcal{N} = (1,1)$ asymmetric orbifolds utilizing a\ncorrespondence with orbifolds of chiral 2D SCFTs with $c= 24$ (or $c= 12$).\nInterestingly, this class of theories can give rise to 6D vacua in which the\nonly massless degrees of freedom reside in the gravity multiplet, with no\nmoduli other than the dilaton, thus corresponding to truly isolated vacua,\ncalled string islands. It is expected that there exist five new type II islands\nwith as-yet-unknown constructions. In this work we construct them all using\nasymmetric $\\mathbb{Z}_n$-orbifolds of Type II on $T^4$ with $n = 5,8,10,12$.\nWe show that the cases $n = 5,8$ admit non-trivial discrete theta angles which\nhave important consequences for both the string and particle charge lattices.\nIn fact they provide examples of BPS-incompleteness and the strongest failure\nof the lattice weak gravity conjecture. Our work is expected to finalize our\nunderstanding of all perturbative 6D $\\mathcal{N} = (1,1)$ theories.",
        "Highly-irradiated gas giant exoplanets are predicted to show circulation\npatterns dominated by day-to-night heat transport and a spatial distribution of\nclouds that is driven by advection and local heating. Hot-Jupiters have been\nextensively studied from broadband phase-curve observations at infrared and\noptical wavelengths, but spectroscopic observations in the reflected light are\nrare and the regime of smaller and higher-metallicity ultra-hot planets, such\nas hot-Neptunes, remains largely unexplored to date. Here we present the\nphase-resolved reflected-light and thermal-emission spectroscopy of the\nultra-hot Neptune LTT 9779b, obtained through observing its full phase-curve\nfrom 0.6 to 2.8 $\\mu$m with JWST NIRISS\/SOSS. We detect an asymmetric dayside\nin reflected light (3.1$\\sigma$ significance) with highly-reflective white\nclouds on the western dayside (A = 0.79$\\pm$0.15) and a much lower-albedo\neastern dayside (A = 0.41$\\pm$0.10), resulting in an overall dayside albedo of\nA = 0.50$\\pm$0.07. The thermal phase curve is symmetric about the substellar\npoint, with a dayside effective temperature of T$_\\mathrm{eff,day}$ =\n2,260$^{+40}_{-50}$ K and a cold nightside (T$_\\mathrm{eff,night}$ <1,330 K at\n3-$\\sigma$ confidence), indicative of short radiative timescales. We propose an\natmospheric circulation and cloud distribution regime in which heat is\ntransported eastward from the dayside towards the cold nightside by an\nequatorial jet, leading to a colder western dayside where temperatures are\nsufficiently low for the condensation of silicate clouds.",
        "Baryonic black branes describe the quantum critical phase of the conformal\nconifold gauge theory at strong coupling. This phase extends to zero\ntemperature at a finite baryonic chemical potential, represented by extremal\nblack branes with $AdS_2\\times R^3\\times T^{1,1}$ throat in asymptotic\n$AdS_5\\times T^{1,1}$ geometry. We demonstrate here that this phase is\ndynamically unstable below some critical value of $T_c\/\\mu$: the instability is\nrepresented by a diffusive mode in the hydrodynamic sound channel with a\nnegative diffusion coefficient. We also identify a new (exotic) ordered phase\nof the conifold gauge theory: this phase originates at the same critical value\nof $T_c\/\\mu$, but extends to arbitrary high temperatures, and is characterized\nby an expectation value of a dimension-2 operator, ${\\cal O}_2\\propto T^2$, in\nthe limit $\\frac \\mu T\\to 0$.",
        "In this article, we construct discrete tight frames for\n$L^2(\\mathbb{S}^{d-1})$, $d\\geq3$, which consist of localized polynomial\nwavelets with adjustable degrees of directionality. In contrast to the well\nstudied isotropic case, these systems are well suited for the direction\nsensitive analysis of anisotropic features such as edges. The price paid for\nthis is the fact that at each scale the wavelet transform lives on the rotation\ngroup $SO(d)$, and not on $\\mathbb{S}^{d-1}$ as in the zonal setting. Thus, the\nstandard approach of building discrete frames by sampling the continuous\nwavelet transform requires a significantly larger amount of sample points.\nHowever, by keeping the directionality limited, this number can be greatly\nreduced to the point where it is comparable to the number of samples needed in\nthe isotropic case. Moreover, the limited directionality is reflected in the\nwavelets being steerable and their great localization in space leads to a fast\nconvergence of the wavelet expansion in the spaces $L^p(\\mathbb{S}^{d-1})$,\n$1\\leq p \\leq \\infty$.",
        "Homogeneous magnetic fields can be generated through the strategic\narrangement of permanent magnets. The Halbach array serves as a prominent\nexample of an effective design following this principle. However, it is a\ntwo-dimensional approach because it is optimal when placing infinitely long\nmagnets -- line dipoles -- on a circle. If shorter, more realistic magnets are\nto be used, the optimal arrangement of magnetic moments diverges from the\nclassical Halbach geometry. This paper presents optimal solutions for\nthree-dimensional arrangements calculated for point dipoles, including\noptimized orientations for single rings and stacks of two rings. They are\nsuperior to the original Halbach arrangement and a modification described in\nthe literature, both in terms of the strength and the homogeneity of the\nmagnetic field. Analytic formulae are provided for both cases and tested by\nexperimental realizations.",
        "We address the challenge of preserving the simultaneity and chronology of\nsensing events in multi-sensory systems with wireless links. The network uses\ntemporal windows of integration (TWIs), borrowed multi-sensory perception, to\npreserve the temporal structure of the sensing data at the application side. We\nintroduce a composite latency model for propagation, sensing, and communication\nthat leads to the derivation of the probability of simultaneity violation. This\nis used to select the TWI duration aiming to achieve the desired degrees of\nchronological preservation, while maintaining the throughput of events. The\nletter provides important insights and analytical tools about the TWI impact on\nthe event registration.",
        "The anomalous Hall effect (AHE) occurs in magnetic systems and also\nunexpectedly in non-magnetic materials adjacent to magnetic insulators via the\nheterointerface interactions. However, the AHE in heterostructures induced by\nmagnetic proximity effect remains quite weak, restricting their practical\ndevice applications. Here, we report a large intrinsic AHE with a resistivity\nof 114 n{\\Omega} cm at 5 K in noncoplanar magnetic heterostructures of\nCr5Te6\/Pt. This is the record-high AHE value among all the magnetic\ninsulators\/heavy metal heterostructures. A reversal of the AHE signal occurs\ndue to the reconstruction of Berry curvature at the Fermi level, which is\nverified by the first-principles calculations. Topological spin textures at the\ninterface are directly visualized via high-magnetic-field magnetic force\nmicroscopy, which accounts for the large AHE, as confirmed by the atomic\nsimulations. These findings open a new avenue for exploring the large AHE in\nheterointerfaces and facilitate the potential applications in topological\nspintronic devices.",
        "We consider entangling operations in a single nitrogen-vacancy (NV) center in\ndiamond where the hyperfine-coupled nuclear spin qubits are addressed with\nradio-frequency (rf) pulses conditioned on the state of the central electron\nspin. Limiting factors for the gate fidelity are coherent errors due to\noff-resonant driving of neighboring transitions in the dense, hyperfine-split\nenergy spectrum of the defect and non-negligible perpendicular hyperfine tensor\ncomponents that narrow the choice of $^{13}\\rm C$ nuclear spin qubits. We\naddress these issues by presenting protocols based on synchronization effects\nthat allow for a complete suppression of both error sources in state-of-the-art\nCNOT gate schemes. This is possible by a suitable choice of parameter sets that\nincorporate the error into the scheme instead of avoiding it. These results\ncontribute to the recent progress toward scalable quantum computation with\ndefects in solids.",
        "For the first time, we develop a simulation-based model for the Minkowski\nfunctionals (MFs) of large-scale structure, which allows us to extract the full\ninformation available from the MFs (including both the Gaussian and\nnon-Gaussian part), and apply it to the BOSS DR12 CMASS galaxy sample. Our\nmodel is based on high-fidelity mock galaxy catalogs constructed from the\n\\textsc{Abacus}\\textsc{Summit} simulations using the halo occupation\ndistribution (HOD) framework, which include the redshift-space distortions and\nAlcock-Paczynski distortions, incorporate survey realism, including survey\ngeometry and veto masks, and account for angular plus radial selection effects.\nThe cosmological and HOD parameter dependence of the MFs is captured with a\nneural network emulator trained from the galaxy mocks with various cosmological\nand HOD parameters. To benchmark the constraining power of the MFs, we also\ntrain an emulator for the galaxy 2-point correlation function (2PCF) using the\nsame pipeline. Having validated our approach through successful parameter\nrecovery tests on both internal and external mocks, including non-HOD forward\nmodels of the halo-galaxy connection, we apply our forward model to analyze the\nCMASS data in the redshift range $0.45<z<0.58$. We find the MFs provide\nstronger constraints on the cosmological parameters than the 2PCF. The\ncombination of the two gives $\\omega_{\\rm cdm}=0.1172^{+0.0020}_{-0.0023}$,\n$\\sigma_8=0.783\\pm 0.026$, and $n_s=0.966^{+0.019}_{-0.015}$, which are tighter\nby a factor of 2.0, 1.9, and 1.6 than the 2PCF alone. The derived constraint\n$f\\sigma_8=0.453 \\pm 0.016$ is also improved by a factor of 1.9, compared to\nthe 2PCF, and agrees well with Planck 2018 predictions and other results from a\nseries of studies in the literature.",
        "We investigate objects in symmetric tensor categories that have\nsimultaneously finite symmetric and finite exterior algebra. This forces the\ncharacteristic of the base field to be $p>0$, and the maximal degree of\nnon-vanishing symmetric and exterior powers to add up to a multiple of $p$. We\ngive a complete classification of objects in tensor categories for which this\nsum equals $p$. All resulting tensor categories are Verlinde categories of\nreductive groups and we fill in some gaps in the literature on these\ncategories.",
        "Quantum computation will rely on quantum error correction to counteract\ndecoherence. Successfully implementing an error correction protocol requires\nthe fidelity of qubit operations to be well-above error correction thresholds.\nIn superconducting quantum computers, measurement of the qubit state remains\nthe lowest-fidelity operation. For the transmon, a prototypical superconducting\nqubit, measurement is carried out by scattering a microwave tone off the qubit.\nConventionally, the frequency of this tone is of the same order as the transmon\nfrequency. The measurement fidelity in this approach is limited by\nmulti-excitation resonances in the transmon spectrum which are activated at\nhigh readout power. These resonances excite the qubit outside of the\ncomputational basis, violating the desired quantum non-demolition character of\nthe measurement. Here, we find that strongly detuning the readout frequency\nfrom that of the transmon exponentially suppresses the strength of spurious\nmulti-excitation resonances. By increasing the readout frequency up to twelve\ntimes the transmon frequency, we achieve a quantum non-demolition measurement\nfidelity of 99.93% with a residual probability of leakage to non-computational\nstates of only 0.02%.",
        "Although the specification of bivariate probability models using a collection\nof assumed conditional distributions is not a novel concept, it has received\nconsiderable attention in the last decade. In this study, a bivariate\ndistribution-the bivariate Poisson-Gamma conditional distribution-is\nintroduced, combining both univariate continuous and discrete distributions.\nThis work explores aspects of this model's structure and statistical inference\nthat have not been studied before. This paper contributes to the field of\nstatistical modeling and distribution theory through the use of maximum\nlikelihood estimation, along with simulations and analyses of real data.",
        "The coupling between the spin degrees of freedom and macroscopic mechanical\nmotions, including striction, shearing, and rotation, has attracted wide\ninterest with applications in actuation, transduction, and information\nprocessing. Experiments so far have established the mechanical responses to the\nlong-range ordered or isolated single spin states. However, it remains elusive\nwhether mechanical motions can couple to a different type of magnetic\nstructure, the non-collinear spin textures, which exhibit nanoscale spatial\nvariations of spin (domain walls, skyrmions, etc.) and are promising candidates\nto realize high-speed computing devices. Here, we report the detection of\ncollective spin texture dynamics with nanoelectromechanical resonators made of\ntwo-dimensional antiferromagnetic (AFM) MnPS3 with $10^{-9}$ strain\nsensitivity. By examining radio frequency mechanical oscillations under\nmagnetic fields, new magnetic transitions were identified with sharp dips in\nresonant frequency. They are attributed to the collective AFM domain wall\nmotions as supported by the analytical modeling of magnetostriction and\nlarge-scale spin-dynamics simulations. Additionally, an abnormally large\nmodulation in the mechanical nonlinearity at the transition field infers a\nfluid-like response due to the ultrafast domain motion. Our work establishes a\nstrong coupling between spin texture and mechanical dynamics, laying the\nfoundation for electromechanical manipulation of spin texture and developing\nquantum hybrid devices.",
        "We calculate $(\\sqrt{-t}\/Q)^k $ and $(m\/Q)^k$ power corrections with $k\\le\n4$, where $m$ is the target mass and $t$ is the momentum transfer, to several\nkey observables in Deeply Virtual Compton Scattering (DVCS). We find that the\npower expansion is well convergent up to $|t|\/Q^2\\lesssim 1\/4$ for most of the\nobservables, but is naturally organized in terms of $1\/(Q^2+t)$ rather than the\nnominal hard scale $1\/Q^2$. We also argue that target mass corrections remain\nunder control and do not endanger QCD factorization for coherent DVCS on\nnuclei. These results remove an important source of uncertainties due to the\nframe dependence and violation of electromagnetic Ward identities in the QCD\npredictions for the DVCS amplitudes in the leading-twist approximation.",
        "We consider projective Hyper-K\\\"ahler manifolds of dimension four that are\ndeformation equivalent to Hilbert squares of K3 surfaces. In case such a\nmanifold admits a divisorial contraction, the exceptional divisor is a conic\nbundle over a K3 surface. There are five types of such conic bundles. In case\nthe manifold has Picard rank two and has two (birational) divisorial\ncontractions we determine the types of these conic bundles. There are exactly\nseven cases. For the Fano varieties of cubic fourfolds there are only four\ncases and we provide examples of these.",
        "Studying the de Rham complex is a natural choice when working with problems\nin electromagnetics and fluid mechanics. By discretizing the complex correctly,\nit is possible to attain stable numerical methods to tackle these problems. An\nimportant consideration when constructing the discrete complex is that it must\npreserve the cohomology structure of the original one. This property is not\nguaranteed when the discrete function spaces chosen are hierarchical B-splines.\nResearch shows that a poor choice of refinement domains may give rise to\nspurious harmonic forms that ruin the accuracy of solutions, even for the\nsimplest partial differential equations. Another crucial aspect to consider in\nthe hierarchical setting is the notion of admissibility, as it is possible to\nobtain optimal convergence rates of numerical solutions by limiting the\nmulti-level interaction of basis functions. We will focus on the\ntwo-dimensional de Rham complex over the unit square $\\Omega \\subseteq\n\\mathbb{R}^2$. In this scenario, the discrete de Rham complex should be exact,\nand we provide both the theoretical and the algorithm-implementation framework\nto ensure this is the case. Moreover, we show that, under a common restriction,\nthe admissibility class of the first space of the discrete complex persists\nthroughout the remaining spaces. Finally, we include numerical results that\nmotivate the importance of the previous concerns for the vector Laplace and\nMaxwell eigenvalue problems.",
        "- An algorithm for calculating the radiation field of a charged point\nparticle performing a spiral motion in an infinite cylindrical waveguide with a\nmultilayer side wall is found. The number of layers and their filling is\narbitrary. The axis of the spiral is aligned with the axis of the waveguide, so\nthat the geometry of the problem has cylindrical symmetry. Explicit expressions\nfor modal frequency distributions and equations for resonant frequencies for\nsingle-layer and double-layer waveguides are given. Examples of graphical\nconstructions of modal frequency distributions of modes for single-layer\n(resistive), double-layer (metal-dielectric) and triple-layer (metal-dielectric\nwith internal NEG coating) waveguides are presented.",
        "The gyrokinetic particle-in-cell code PICLS is a full-f finite element tool\nto simulate turbulence in the tokamak scrape-off layer. During the previous\nyear, the capability of PICLS was extended to encompass electromagnetic\neffects. Successful tests using the method of manufactured solutions were\nconducted on the freshly added Amp\\`ere's-law-solver, and shear Alfv\\'en waves\nwere simulated to verify the new electromagnetic time step. However, as a code\nbased on the $p_{||}$-formulation of the gyrokinetic equations, PICLS is\naffected by the Amp\\`ere-cancellation problem. In order to bring higher-beta\nsimulations within reach of our computational capacity, we implemented the\nmixed-variable formulation with pullback-scheme in a similar fashion to, e.g.,\nEUTERPE, ORB5, or XGC. Here, we present the successful verification of the\ndifferent electromagnetic formulations of PICLS by simulating shear-Alfv\\'en\nwaves in a test setup designed to minimize kinetic effects.",
        "Recently, GeSn alloys have attracted much interest for direct-gap infrared\nphotonics and as potential topological materials which are compatible with the\nsemiconductor industry. However, for photonics, the high-Sn content required\nleads to low detectivity, associated with poor material quality, and the (>35%)\nSn required for topological properties have been out of reach experimentally.\nHere, we demonstrate that by patterning the Sn distribution within Ge, the\nelectronic properties have a far greater tunability than is possible with the\nrandom alloy. For the GeSn \\delta-digital alloy (DA) formed by confining Sn\natoms in atomic layer(s) along the [111] direction of Ge, we show that ~10% Sn\ncan lead to a triple-point semimetal. These findings are understood in terms of\nSn ordering causing spatial separation of Sn and Ge band edges, leading to band\ninversion. This mechanism can also lead to a weak topological insulator, Weyl\nsemimetal, and enables tunable direct bandgaps down to 2 meV, covering the\nentire infrared range. Our findings are generally applicable to other\nsemiconductors DAs and point to a new class of currently unexplored topological\nsystems accessible by epitaxy and establish the promise of low-Sn GeSn DAs for\napplication as infrared laser diodes and photodetectors in Si photonic\nintegrated circuits and infrared image sensors."
      ]
    }
  },
  {
    "id":2411.09469,
    "research_type":"applied",
    "start_id":"b11",
    "start_title":"The accuracy of colposcopic biopsy: Analyses from the placebo arm of the Gardasil clinical trials",
    "start_abstract":"We evaluated the overall agreement between colposcopically directed biopsies and definitive excisional specimens within context of three clinical trials. A total 737 women aged 16-45 who had a cervical biopsy taken 6 months before their therapy were included. Per-protocol, colposcopists to also obtain representative immediately therapy. Using adjudicated histological diagnoses, initial same day correlated with surgically excised specimens. The therapy, diagnoses was 42% (weighted kappa = 0.34) (95% CI: 0.29-0.39). underestimation intraepithelial neoplasia grade 2\/3 or adenocarcinoma in situ (CIN2-3\/AIS) CIN3\/AIS 26 42%, respectively. When allowing for one degree variance correlation, 92% CIN2-3\/AIS. specimen 56% 0.41) 0.36-0.47), CIN2-3\/AIS 57%. There significant associations when patients stratified by age, number biopsies, lesion size, presence human papillomavirus (HPV)16\/18 region. Of 178 diagnostic endocervical curettages performed, 14 (7.9%) found any HPV disease. Colposcopic accuracy improved CIN2 grouped as single predictive measure high-grade Colposcopy functioned well allowed one-degree difference surgical histologic interpretations, done practice. Taking more than colposcopic could improve patient management.",
    "start_categories":[
      "Clinical Trial"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b36"
      ],
      "title":[
        "CBAM: Convolutional block attention module"
      ],
      "abstract":[
        "We propose Convolutional Block Attention Module (CBAM), a simple yet effective attention module for feed-forward convolutional neural networks. Given an intermediate feature map, our module sequentially infers attention maps along two separate dimensions, channel and spatial, then the attention maps are multiplied to the input feature map for adaptive feature refinement. Because CBAM is a lightweight and general module, it can be integrated into any CNN architectures seamlessly with negligible overheads and is end-to-end trainable along with base CNNs. We validate our CBAM through extensive experiments on ImageNet-1K, MS~COCO detection, and VOC~2007 detection datasets. Our experiments show consistent improvements in classification and detection performances with various models, demonstrating the wide applicability of CBAM. The code and models will be publicly available."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "Balancing the Budget: Understanding Trade-offs Between Supervised and\n  Preference-Based Finetuning",
        "Random Number Generation from Pulsars",
        "ChordFormer: A Conformer-Based Architecture for Large-Vocabulary Audio\n  Chord Recognition",
        "Safe-VAR: Safe Visual Autoregressive Model for Text-to-Image Generative\n  Watermarking",
        "Optimizing Knowledge Distillation in Transformers: Enabling Multi-Head\n  Attention without Alignment Barriers",
        "ChartCoder: Advancing Multimodal Large Language Model for Chart-to-Code\n  Generation",
        "Evaluating the Impacts of Swapping on the US Decennial Census",
        "The Distributionally Robust Optimization Model of Sparse Principal\n  Component Analysis",
        "A class of anisotropic diffusion-transport equations in non-divergence\n  form",
        "Dark Distillation: Backdooring Distilled Datasets without Accessing Raw\n  Data",
        "Testing quantum gravity with dilute dipolar Bose gases",
        "The uncollapsed LaFe2As2 phase: compensated, highly doped,\n  electron-phonon coupled, iron-based superconductor",
        "Dimensions and metric dyadic cubes",
        "Histoires Morales: A French Dataset for Assessing Moral Alignment",
        "Generative Predictive Control: Flow Matching Policies for Dynamic and\n  Difficult-to-Demonstrate Tasks",
        "BEYONDWORDS is All You Need: Agentic Generative AI based Social Media\n  Themes Extractor",
        "Existence and uniqueness of control sets with a nonempty interior for\n  linear control systems on solvable groups",
        "Trustworthiness in Stochastic Systems: Towards Opening the Black Box",
        "Semantic Neural Radiance Fields for Multi-Date Satellite Data",
        "Non-Commutative fluid: an alternative source of cosmic acceleration",
        "On the Robustness of Cluster Clustering Covariance Calibration",
        "BRIDLE: Generalized Self-supervised Learning with Quantization",
        "Wavelet-Assisted Multi-Frequency Attention Network for Pansharpening",
        "Quasi-perfect spatiotemporal optical vortex with suppressed mode\n  degradation",
        "Generalizable Image Repair for Robust Visual Autonomous Racing",
        "CutPaste&Find: Efficient Multimodal Hallucination Detector with\n  Visual-aid Knowledge Base",
        "Examining Two Hop Reasoning Through Information Content Scaling",
        "Helix-mRNA: A Hybrid Foundation Model For Full Sequence mRNA\n  Therapeutics",
        "GPDFlow: Generative Multivariate Threshold Exceedance Modeling via\n  Normalizing Flows"
      ],
      "abstract":[
        "Post-training of Large Language Models often involves a pipeline of\nSupervised Finetuning (SFT) followed by Preference Finetuning (PFT) using\nmethods like Direct Preference Optimization. Both stages require annotated data\nthat are very different in structure and costs. We study how to optimally\nallocate a fixed training data budget between the two stages, through extensive\nexperiments spanning four diverse tasks, multiple model sizes and various data\nannotation costs. Our findings reveal that just SFT on the base model dominates\nperformance in low-data regimes ($<1,000$ annotated examples). With larger\ndata-budgets, we observe that a combination of SFT and PFT, often with\nincreasing portions allocated towards preference data yields optimal\nperformance. However, completely eliminating SFT and running PFT directly on\nthe base model yields suboptimal performance, described as the cold start\nproblem on tasks like mathematics. We observe that this is due to the\ndistribution shift arising from using DPO directly on the base model to elicit\nstep-by-step reasoning. This limitation can be effectively addressed by\nallocating even a small portion ($<10$%) of the budget to SFT first, resulting\nin performance improvements of $15-20$% on analytical benchmarks like GSM8k.\nThese results provide actionable insights for researchers and practitioners\noptimizing model development under budget constraints, where high-quality data\ncuration often represents a significant portion of the total costs of model\ndevelopment.",
        "Pulsars exhibit signals with precise inter-arrival times that are on the\norder of milliseconds to seconds depending on the individual pulsar. There is\nsubtle variation in the timing of pulsar signals, primarily due to the presence\nof gravitational waves, intrinsic variance in the period of the pulsar, and\nerrors in the realization of Terrestrial Time (TT). Traditionally, these\nvariations are dismissed as noise in high-precision timing experiments. In this\npaper, we show that these variations serve as a natural entropy source for the\ncreation of Random Number Generators (RNG). We also explore the effects of\nusing randomness extractors to increase the entropy of random bits extracted\nfrom Pulsar timing data. To evaluate the quality of the Pulsar RNG, we model\nits entropy as a $k$-source and use well-known cryptographic results to show\nits closeness to a theoretically ideal uniformly random source. To remain\nconsistent with prior work, we also show that the Pulsar RNG passes well-known\nstatistical tests such as the NIST test suite.",
        "Chord recognition serves as a critical task in music information retrieval\ndue to the abstract and descriptive nature of chords in music analysis. While\naudio chord recognition systems have achieved significant accuracy for small\nvocabularies (e.g., major\/minor chords), large-vocabulary chord recognition\nremains a challenging problem. This complexity also arises from the inherent\nlong-tail distribution of chords, where rare chord types are underrepresented\nin most datasets, leading to insufficient training samples. Effective chord\nrecognition requires leveraging contextual information from audio sequences,\nyet existing models, such as combinations of convolutional neural networks,\nbidirectional long short-term memory networks, and bidirectional transformers,\nface limitations in capturing long-term dependencies and exhibit suboptimal\nperformance on large-vocabulary chord recognition tasks. This work proposes\nChordFormer, a novel conformer-based architecture designed to tackle structural\nchord recognition (e.g., triads, bass, sevenths) for large vocabularies.\nChordFormer leverages conformer blocks that integrate convolutional neural\nnetworks with transformers, thus enabling the model to capture both local\npatterns and global dependencies effectively. By addressing challenges such as\nclass imbalance through a reweighted loss function and structured chord\nrepresentations, ChordFormer outperforms state-of-the-art models, achieving a\n2% improvement in frame-wise accuracy and a 6% increase in class-wise accuracy\non large-vocabulary chord datasets. Furthermore, ChordFormer excels in handling\nclass imbalance, providing robust and balanced recognition across chord types.\nThis approach bridges the gap between theoretical music knowledge and practical\napplications, advancing the field of large-vocabulary chord recognition.",
        "With the success of autoregressive learning in large language models, it has\nbecome a dominant approach for text-to-image generation, offering high\nefficiency and visual quality. However, invisible watermarking for visual\nautoregressive (VAR) models remains underexplored, despite its importance in\nmisuse prevention. Existing watermarking methods, designed for diffusion\nmodels, often struggle to adapt to the sequential nature of VAR models. To\nbridge this gap, we propose Safe-VAR, the first watermarking framework\nspecifically designed for autoregressive text-to-image generation. Our study\nreveals that the timing of watermark injection significantly impacts generation\nquality, and watermarks of different complexities exhibit varying optimal\ninjection times. Motivated by this observation, we propose an Adaptive Scale\nInteraction Module, which dynamically determines the optimal watermark\nembedding strategy based on the watermark information and the visual\ncharacteristics of the generated image. This ensures watermark robustness while\nminimizing its impact on image quality. Furthermore, we introduce a Cross-Scale\nFusion mechanism, which integrates mixture of both heads and experts to\neffectively fuse multi-resolution features and handle complex interactions\nbetween image content and watermark patterns. Experimental results demonstrate\nthat Safe-VAR achieves state-of-the-art performance, significantly surpassing\nexisting counterparts regarding image quality, watermarking fidelity, and\nrobustness against perturbations. Moreover, our method exhibits strong\ngeneralization to an out-of-domain watermark dataset QR Codes.",
        "Knowledge distillation (KD) in transformers often faces challenges due to\nmisalignment in the number of attention heads between teacher and student\nmodels. Existing methods either require identical head counts or introduce\nprojectors to bridge dimensional gaps, limiting flexibility and efficiency. We\npropose Squeezing-Heads Distillation (SHD), a novel approach that enables\nseamless knowledge transfer between models with varying head counts by\ncompressing multi-head attention maps via efficient linear approximation.\nUnlike prior work, SHD eliminates alignment barriers without additional\nparameters or architectural modifications. Our method dynamically approximates\nthe combined effect of multiple teacher heads into fewer student heads,\npreserving fine-grained attention patterns while reducing redundancy.\nExperiments across language (LLaMA, GPT) and vision (DiT, MDT) generative and\nvision (DeiT) discriminative tasks demonstrate SHD's effectiveness: it\noutperforms logit-based and feature-alignment KD baselines, achieving\nstate-of-the-art results in image classification, image generation language\nfine-tuning, and language pre-training. The key innovations of flexible head\ncompression, projector-free design, and linear-time complexity make SHD a\nversatile and scalable solution for distilling modern transformers. This work\nbridges a critical gap in KD, enabling efficient deployment of compact models\nwithout compromising performance.",
        "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in chart understanding tasks. However, interpreting charts with\ntextual descriptions often leads to information loss, as it fails to fully\ncapture the dense information embedded in charts. In contrast, parsing charts\ninto code provides lossless representations that can effectively contain all\ncritical details. Although existing open-source MLLMs have achieved success in\nchart understanding tasks, they still face two major challenges when applied to\nchart-to-code tasks.: (1) Low executability and poor restoration of chart\ndetails in the generated code and (2) Lack of large-scale and diverse training\ndata. To address these challenges, we propose \\textbf{ChartCoder}, the first\ndedicated chart-to-code MLLM, which leverages Code LLMs as the language\nbackbone to enhance the executability of the generated code. Furthermore, we\nintroduce \\textbf{Chart2Code-160k}, the first large-scale and diverse dataset\nfor chart-to-code generation, and propose the \\textbf{Snippet-of-Thought (SoT)}\nmethod, which transforms direct chart-to-code generation data into step-by-step\ngeneration. Experiments demonstrate that ChartCoder, with only 7B parameters,\nsurpasses existing open-source MLLMs on chart-to-code benchmarks, achieving\nsuperior chart restoration and code excitability. Our code will be available at\nhttps:\/\/github.com\/thunlp\/ChartCoder.",
        "To meet its dual burdens of providing useful statistics and ensuring privacy\nof individual respondents, the US Census Bureau has for decades introduced some\nform of \"noise\" into published statistics. Initially, they used a method known\nas \"swapping\" (1990-2010). In 2020, they switched to an algorithm called\nTopDown that ensures a form of Differential Privacy. While the TopDown\nalgorithm has been made public, no implementation of swapping has been released\nand many details of the deployed swapping methodology deployed have been kept\nsecret. Further, the Bureau has not published (even a synthetic) \"original\"\ndataset and its swapped version. It is therefore difficult to evaluate the\neffects of swapping, and to compare these effects to those of other privacy\ntechnologies. To address these difficulties we describe and implement a\nparameterized swapping algorithm based on Census publications, court documents,\nand informal interviews with Census employees. With this implementation, we\ncharacterize the impacts of swapping on a range of statistical quantities of\ninterest. We provide intuition for the types of shifts induced by swapping and\ncompare against those introduced by TopDown. We find that even when swapping\nand TopDown introduce errors of similar magnitude, the direction in which\nstatistics are biased need not be the same across the two techniques. More\nbroadly, our implementation provides researchers with the tools to analyze and\npotentially correct for the impacts of disclosure avoidance systems on the\nquantities they study.",
        "We consider sparse principal component analysis (PCA) under a stochastic\nsetting where the underlying probability distribution of the random parameter\nis uncertain. This problem is formulated as a distributionally robust\noptimization (DRO) model based on a constructive approach to capturing\nuncertainty in the covariance matrix, which constitutes a nonsmooth constrained\nmin-max optimization problem. We further prove that the inner maximization\nproblem admits a closed-form solution, reformulating the original DRO model\ninto an equivalent minimization problem on the Stiefel manifold. This\ntransformation leads to a Riemannian optimization problem with intricate\nnonsmooth terms, a challenging formulation beyond the reach of existing\nalgorithms. To address this issue, we devise an efficient smoothing manifold\nproximal gradient algorithm. We prove the Riemannian gradient consistency and\nglobal convergence of our algorithm to a stationary point of the nonsmooth\nminimization problem. Moreover, we establish the iteration complexity of our\nalgorithm. Finally, numerical experiments are conducted to validate the\neffectiveness and scalability of our algorithm, as well as to highlight the\nnecessity and rationality of adopting the DRO model for sparse PCA.",
        "We generalize Einstein's probabilistic method for the Brownian motion to\nstudy compressible fluids in porous media. The multi-dimensional case is\nconsidered with general probability distribution functions. By relating the\nexpected displacement per unit time with the velocity of the fluid, we derive\nan anisotropic diffusion equation in non-divergence form that contains a\ntransport term. Under the Darcy law assumption, a corresponding nonlinear\npartial differential equations for the density function is obtained. The\nclassical solutions of this equation are studied, and the maximum and strong\nmaximum principles are established. We also obtain exponential decay estimates\nfor the solutions for all time, and particularly, their exponential convergence\nas time tends to infinity. Our analysis uses some transformations of the\nBernstein-Cole--Hopf type which are explicitly constructed even for very\ngeneral equation of state. Moreover, the Lemma of Growth in time is proved and\nutilized in order to achieve the above decaying estimates.",
        "Dataset distillation (DD) enhances training efficiency and reduces bandwidth\nby condensing large datasets into smaller synthetic ones. It enables models to\nachieve performance comparable to those trained on the raw full dataset and has\nbecome a widely adopted method for data sharing. However, security concerns in\nDD remain underexplored. Existing studies typically assume that malicious\nbehavior originates from dataset owners during the initial distillation\nprocess, where backdoors are injected into raw datasets. In contrast, this work\nis the first to address a more realistic and concerning threat: attackers may\nintercept the dataset distribution process, inject backdoors into the distilled\ndatasets, and redistribute them to users. While distilled datasets were\npreviously considered resistant to backdoor attacks, we demonstrate that they\nremain vulnerable to such attacks. Furthermore, we show that attackers do not\neven require access to any raw data to inject the backdoors successfully.\nSpecifically, our approach reconstructs conceptual archetypes for each class\nfrom the model trained on the distilled dataset. Backdoors are then injected\ninto these archetypes to update the distilled dataset. Moreover, we ensure the\nupdated dataset not only retains the backdoor but also preserves the original\noptimization trajectory, thus maintaining the knowledge of the raw dataset. To\nachieve this, a hybrid loss is designed to integrate backdoor information along\nthe benign optimization trajectory, ensuring that previously learned\ninformation is not forgotten. Extensive experiments demonstrate that distilled\ndatasets are highly vulnerable to backdoor attacks, with risks pervasive across\nvarious raw datasets, distillation methods, and downstream training strategies.\nMoreover, our attack method is efficient, capable of synthesizing a malicious\ndistilled dataset in under one minute in certain cases.",
        "We systematically investigate the effects of quantum gravity on the\nground-state properties of dilute homogeneous dipolar Bose gases using the\nHartree-Fock-Bogoliubov theory based on the generalized uncertainty principle.\nWe calculate quantum gravity corrections to the condensed fraction, the\nequation of state, the critical temperature and the superfluid fraction.\nImproved upper bounds on the generalized uncertainty principle parameters are\nfound. We compare our predictions with previous experimental and theoretical\nresults.",
        "The recently discovered LaFe2As2 superconducting compound, member of the 122\nfamily of iron pnictide superconductors, becomes superconducting below Tc=13K,\nyet its nominal doping apparently places it in the extreme overdoped limit,\nwhere superconductivity should be suppressed. In this work, we investigate the\nnormal state of magneto- and thermo-electric transport and specific heat of\nthis compound. The experimental data are consistent with the presence of highly\ncompensated electron and hole bands, with around 0.42 electrons per unit cell\njust above Tc, and high effective masses around 3m0. The temperature dependence\nof transport properties strongly resembles that of conventional\nsuperconductors, pointing to a key role of electron-phonon coupling. From these\nevidences, LaFe2As2 can be regarded as the connecting compound between\nunconventional and conventional superconductors.",
        "In this note, we provide equivalent definitions for fractal geometric\ndimensions through dyadic cube constructions. Given a metric space $X$ with\nfinite Assouad dimension, i.e., satisfying the doubling property, we show that\nthe construction of systems of dyadic cubes by Hyt\\\"onen-Kairema is compatible\nwith many dimensions. In particular, the Hausdorff, Minkowski, and Assouad\ndimensions can be equivalently expressed solely using dyadic cubes in the\naforementioned system. The same is true for the Assouad spectrum, a collection\nof dimensions introduced by Fraser-Yu.",
        "Aligning language models with human values is crucial, especially as they\nbecome more integrated into everyday life. While models are often adapted to\nuser preferences, it is equally important to ensure they align with moral norms\nand behaviours in real-world social situations. Despite significant progress in\nlanguages like English and Chinese, French has seen little attention in this\narea, leaving a gap in understanding how LLMs handle moral reasoning in this\nlanguage. To address this gap, we introduce Histoires Morales, a French dataset\nderived from Moral Stories, created through translation and subsequently\nrefined with the assistance of native speakers to guarantee grammatical\naccuracy and adaptation to the French cultural context. We also rely on\nannotations of the moral values within the dataset to ensure their alignment\nwith French norms. Histoires Morales covers a wide range of social situations,\nincluding differences in tipping practices, expressions of honesty in\nrelationships, and responsibilities toward animals. To foster future research,\nwe also conduct preliminary experiments on the alignment of multilingual models\non French and English data and the robustness of the alignment. We find that\nwhile LLMs are generally aligned with human moral norms by default, they can be\neasily influenced with user-preference optimization for both moral and immoral\ndata.",
        "Generative control policies have recently unlocked major progress in\nrobotics. These methods produce action sequences via diffusion or flow\nmatching, with training data provided by demonstrations. But despite enjoying\nconsiderable success on difficult manipulation problems, generative policies\ncome with two key limitations. First, behavior cloning requires expert\ndemonstrations, which can be time-consuming and expensive to obtain. Second,\nexisting methods are limited to relatively slow, quasi-static tasks. In this\npaper, we leverage a tight connection between sampling-based predictive control\nand generative modeling to address each of these issues. In particular, we\nintroduce generative predictive control, a supervised learning framework for\ntasks with fast dynamics that are easy to simulate but difficult to\ndemonstrate. We then show how trained flow-matching policies can be\nwarm-started at run-time, maintaining temporal consistency and enabling fast\nfeedback rates. We believe that generative predictive control offers a\ncomplementary approach to existing behavior cloning methods, and hope that it\npaves the way toward generalist policies that extend beyond quasi-static\ndemonstration-oriented tasks.",
        "Thematic analysis of social media posts provides a major understanding of\npublic discourse, yet traditional methods often struggle to capture the\ncomplexity and nuance of unstructured, large-scale text data. This study\nintroduces a novel methodology for thematic analysis that integrates tweet\nembeddings from pre-trained language models, dimensionality reduction using and\nmatrix factorization, and generative AI to identify and refine latent themes.\nOur approach clusters compressed tweet representations and employs generative\nAI to extract and articulate themes through an agentic Chain of Thought (CoT)\nprompting, with a secondary LLM for quality assurance. This methodology is\napplied to tweets from the autistic community, a group that increasingly uses\nsocial media to discuss their experiences and challenges. By automating the\nthematic extraction process, the aim is to uncover key insights while\nmaintaining the richness of the original discourse. This autism case study\ndemonstrates the utility of the proposed approach in improving thematic\nanalysis of social media data, offering a scalable and adaptable framework that\ncan be applied to diverse contexts. The results highlight the potential of\ncombining machine learning and Generative AI to enhance the depth and accuracy\nof theme identification in online communities.",
        "In this paper, we obtain weak conditions for the existence of a control set\nwith a nonempty interior for a linear control system on a solvable Lie group.\nWe show that the Lie algebra rank condition together with the compactness of\nthe nilpotent part of the generalized kernel of the drift are enough to assure\nthe existence of such a control set. Moreover, this control set is unique and\ncontains the whole generalized kernel in its closure.",
        "AI systems are increasingly tasked to complete responsibilities with\ndecreasing oversight. This delegation requires users to accept certain risks,\ntypically mitigated by perceived or actual alignment of values between humans\nand AI, leading to confidence that the system will act as intended. However,\nstochastic behavior by an AI system threatens to undermine alignment and\npotential trust. In this work, we take a philosophical perspective to the\ntension and potential conflict between stochasticity and trustworthiness. We\ndemonstrate how stochasticity complicates traditional methods of establishing\ntrust and evaluate two extant approaches to managing it: (1) eliminating\nuser-facing stochasticity to create deterministic experiences, and (2) allowing\nusers to independently control tolerances for stochasticity. We argue that both\napproaches are insufficient, as not all forms of stochasticity affect\ntrustworthiness in the same way or to the same degree. Instead, we introduce a\nnovel definition of stochasticity and propose latent value modeling for both AI\nsystems and users to better assess alignment. This work lays a foundational\nstep toward understanding how and when stochasticity impacts trustworthiness,\nenabling more precise trust calibration in complex AI systems, and underscoring\nthe importance of sociotechnical analyses to effectively address these\nchallenges.",
        "In this work we propose a satellite specific Neural Radiance Fields (NeRF)\nmodel capable to obtain a three-dimensional semantic representation (neural\nsemantic field) of the scene. The model derives the output from a set of\nmulti-date satellite images with corresponding pixel-wise semantic labels. We\ndemonstrate the robustness of our approach and its capability to improve noisy\ninput labels. We enhance the color prediction by utilizing the semantic\ninformation to address temporal image inconsistencies caused by non-stationary\ncategories such as vehicles. To facilitate further research in this domain, we\npresent a dataset comprising manually generated labels for popular multi-view\nsatellite images. Our code and dataset are available at\nhttps:\/\/github.com\/wagnva\/semantic-nerf-for-satellite-data.",
        "We have developed a Hubble function based on Newtonian Cosmology using\nnon-commutative fluid equations. Our Hubble function contains cosmic fluids\nwith the signature of a new cosmological parameter $\\sigma$, motivated by a\nnon-commutative Poisson bracket structure. Interestingly, this Hubble function\ndoes not include any external fluid content related to dark energy or the\nCosmological constant; the parameter $\\sigma$ acts as the source of accelerated\nexpansion. In this work, we aim to explain the phenomenon of the accelerating\nexpansion of the universe without \"dark energy\". Additionally, we have verified\nthe observational bounds for $\\sigma$ to assess its potential in explaining the\naccelerated expansion.",
        "Ongoing and upcoming wide-field surveys at different wavelengths will measure\nthe distribution of galaxy clusters with unprecedented precision, demanding\naccurate models for the two-point correlation function (2PCF) covariance. In\nthis work, we assess a semi-analytical framework for the cluster 2PCF\ncovariance that employs three nuisance parameters to account for non-Poissonian\nshot noise, residual uncertainties in the halo bias model, and subleading noise\nterms. We calibrate these parameters on a suite of fast approximate simulations\ngenerated by PINOCCHIO as well as full $N$-body simulations from OpenGADGET3.\nWe demonstrate that PINOCCHIO can reproduce the 2PCF covariance measured in\nOpenGADGET3 at the few percent level, provided the mass functions are carefully\nrescaled. Resolution tests confirm that high particle counts are necessary to\ncapture shot-noise corrections, especially at high redshifts. We perform the\nparameter calibration across multiple cosmological models, showing that one of\nthe nuisance parameters, the non-Poissonian shot-noise correction $\\alpha$,\ndepends mildly on the amplitude of matter fluctuations $\\sigma_8$. In contrast,\nthe remaining two parameters, $\\beta$ controlling the bias correction and\n$\\gamma$ controlling the secondary shot-noise correction, exhibit more\nsignificant variation with redshift and halo mass. Overall, our results\nunderscore the importance of calibrating covariance models on realistic mock\ncatalogs that replicate the selection function of forthcoming surveys and\nhighlight that approximate methods, when properly tuned, can effectively\ncomplement full $N$-body simulations for precision cluster cosmology.",
        "Self-supervised learning has been a powerful approach for learning meaningful\nrepresentations from unlabeled data across various domains, reducing the\nreliance on large labeled datasets. Inspired by BERT's success in capturing\ndeep bidirectional contexts in natural language processing, similar frameworks\nhave been adapted to other modalities such as audio, with models like BEATs\nextending the bidirectional training paradigm to audio signals using vector\nquantization (VQ). However, these frameworks face challenges, notably their\ndependence on a single codebook for quantization, which may not capture the\ncomplex, multifaceted nature of signals. In addition, inefficiencies in\ncodebook utilization lead to underutilized code vectors. To address these\nlimitations, we introduce BRIDLE (Bidirectional Residual Quantization\nInterleaved Discrete Learning Encoder), a self-supervised encoder pretraining\nframework that incorporates residual quantization (RQ) into the bidirectional\ntraining process, and is generalized for pretraining with audio, image, and\nvideo. Using multiple hierarchical codebooks, RQ enables fine-grained\ndiscretization in the latent space, enhancing representation quality. BRIDLE\ninvolves an interleaved training procedure between the encoder and tokenizer.\nWe evaluate BRIDLE on audio understanding tasks using classification\nbenchmarks, achieving state-of-the-art results, and demonstrate competitive\nperformance on image classification and video classification tasks, showing\nconsistent improvements over traditional VQ methods in downstream performance.",
        "Pansharpening aims to combine a high-resolution panchromatic (PAN) image with\na low-resolution multispectral (LRMS) image to produce a high-resolution\nmultispectral (HRMS) image. Although pansharpening in the frequency domain\noffers clear advantages, most existing methods either continue to operate\nsolely in the spatial domain or fail to fully exploit the benefits of the\nfrequency domain. To address this issue, we innovatively propose\nMulti-Frequency Fusion Attention (MFFA), which leverages wavelet transforms to\ncleanly separate frequencies and enable lossless reconstruction across\ndifferent frequency domains. Then, we generate Frequency-Query, Spatial-Key,\nand Fusion-Value based on the physical meanings represented by different\nfeatures, which enables a more effective capture of specific information in the\nfrequency domain. Additionally, we focus on the preservation of frequency\nfeatures across different operations. On a broader level, our network employs a\nwavelet pyramid to progressively fuse information across multiple scales.\nCompared to previous frequency domain approaches, our network better prevents\nconfusion and loss of different frequency features during the fusion process.\nQuantitative and qualitative experiments on multiple datasets demonstrate that\nour method outperforms existing approaches and shows significant generalization\ncapabilities for real-world scenarios.",
        "Spatiotemporal optical vortex (STOV) carrying transverse orbital angular\nmomentum (OAM) enriches the family of vortex beams and exhibit unique\nproperties. Typically, a high-order STOV with an intensity null degrades into\nmultiple first-order STOVs embedded within a single wave packet during\npropagation, a phenomenon known as time diffraction or mode degradation.\nHowever, this degradation limits the applicability of STOVs in specialized\nfields. Therefore, the generation of mode degradation-suppressed STOVs\n(MDS-STOVs) is of significant for both practical applications and theoretical\nstudies. Herein, we theoretically analyze the generation of MDS-STOVs by\nutilizing a conical phase to localize the energy of the STOV into a ring-shaped\nstructure. For MDS-STOVs with large topological charges (TCs), the ring-shaped\nprofile can be well-maintained, and the rapid expansion of the beam size with\nincreasing TC is significantly suppressed compared to conventional STOVs. As a\nresult, these MDS-STOVs can be regarded as quasi-perfect STOVs (QPSTOVs).\nFurthermore, QPSTOVs exhibit strong resistance to group delay dispersion (GDD),\neliminating the need for precise dispersion control and facilitating their\ngeneration and application. This work advances our understanding of the\nphysical properties of light carrying transverse OAM and opens up exciting\navenues for the application of STOVs in diverse fields, such as optical\ncommunication and quantum information processing.",
        "Vision-based autonomous racing relies on accurate perception for robust\ncontrol. However, image distribution changes caused by sensor noise, adverse\nweather, and dynamic lighting can degrade perception, leading to suboptimal\ncontrol decisions. Existing approaches, including domain adaptation and\nadversarial training, improve robustness but struggle to generalize to unseen\ncorruptions while introducing computational overhead. To address this\nchallenge, we propose a real-time image repair module that restores corrupted\nimages before they are used by the controller. Our method leverages generative\nadversarial models, specifically CycleGAN and pix2pix, for image repair.\nCycleGAN enables unpaired image-to-image translation to adapt to novel\ncorruptions, while pix2pix exploits paired image data when available to improve\nthe quality. To ensure alignment with control performance, we introduce a\ncontrol-focused loss function that prioritizes perceptual consistency in\nrepaired images. We evaluated our method in a simulated autonomous racing\nenvironment with various visual corruptions. The results show that our approach\nsignificantly improves performance compared to baselines, mitigating\ndistribution shift and enhancing controller reliability.",
        "Large Vision-Language Models (LVLMs) have demonstrated impressive multimodal\nreasoning capabilities, but they remain susceptible to hallucination,\nparticularly object hallucination where non-existent objects or incorrect\nattributes are fabricated in generated descriptions. Existing detection methods\nachieve strong performance but rely heavily on expensive API calls and\niterative LVLM-based validation, making them impractical for large-scale or\noffline use. To address these limitations, we propose CutPaste\\&Find, a\nlightweight and training-free framework for detecting hallucinations in\nLVLM-generated outputs. Our approach leverages off-the-shelf visual and\nlinguistic modules to perform multi-step verification efficiently without\nrequiring LVLM inference. At the core of our framework is a Visual-aid\nKnowledge Base that encodes rich entity-attribute relationships and associated\nimage representations. We introduce a scaling factor to refine similarity\nscores, mitigating the issue of suboptimal alignment values even for\nground-truth image-text pairs. Comprehensive evaluations on benchmark datasets,\nincluding POPE and R-Bench, demonstrate that CutPaste\\&Find achieves\ncompetitive hallucination detection performance while being significantly more\nefficient and cost-effective than previous methods.",
        "Prior work has found that transformers have an inconsistent ability to learn\nto answer latent two-hop questions -- questions of the form \"Who is Bob's\nmother's boss?\" We study why this is the case by examining how transformers'\ncapacity to learn datasets of two-hop questions and answers (two-hop QA) scales\nwith their size, motivated by prior work on transformer knowledge capacity for\nsimple factual memorization. We find that capacity scaling and generalization\nboth support the hypothesis that latent two-hop QA requires transformers to\nlearn each fact twice, while two-hop QA with chain of thought does not. We also\nshow that with appropriate dataset parameters, it is possible to \"trap\" very\nsmall models in a regime where they memorize answers to two-hop questions\nindependently, even though they would perform better if they could learn to\nanswer them with function composition. Our findings show that measurement of\ncapacity scaling can complement existing interpretability methods, though there\nare challenges in using it for this purpose.",
        "mRNA-based vaccines have become a major focus in the pharmaceutical industry.\nThe coding sequence as well as the Untranslated Regions (UTRs) of an mRNA can\nstrongly influence translation efficiency, stability, degradation, and other\nfactors that collectively determine a vaccine's effectiveness. However,\noptimizing mRNA sequences for those properties remains a complex challenge.\nExisting deep learning models often focus solely on coding region optimization,\noverlooking the UTRs. We present Helix-mRNA, a structured state-space-based and\nattention hybrid model to address these challenges. In addition to a first\npre-training, a second pre-training stage allows us to specialise the model\nwith high-quality data. We employ single nucleotide tokenization of mRNA\nsequences with codon separation, ensuring prior biological and structural\ninformation from the original mRNA sequence is not lost. Our model, Helix-mRNA,\noutperforms existing methods in analysing both UTRs and coding region\nproperties. It can process sequences 6x longer than current approaches while\nusing only 10% of the parameters of existing foundation models. Its predictive\ncapabilities extend to all mRNA regions. We open-source the model\n(https:\/\/github.com\/helicalAI\/helical) and model weights\n(https:\/\/huggingface.co\/helical-ai\/helix-mRNA).",
        "The multivariate generalized Pareto distribution (mGPD) is a common method\nfor modeling extreme threshold exceedance probabilities in environmental and\nfinancial risk management. Despite its broad applicability, mGPD faces\nchallenges due to the infinite possible parametrizations of its dependence\nfunction, with only a few parametric models available in practice. To address\nthis limitation, we introduce GPDFlow, an innovative mGPD model that leverages\nnormalizing flows to flexibly represent the dependence structure. Unlike\ntraditional parametric mGPD approaches, GPDFlow does not impose explicit\nparametric assumptions on dependence, resulting in greater flexibility and\nenhanced performance. Additionally, GPDFlow allows direct inference of marginal\nparameters, providing insights into marginal tail behavior. We derive tail\ndependence coefficients for GPDFlow, including a bivariate formulation, a\n$d$-dimensional extension, and an alternative measure for partial exceedance\ndependence. A general relationship between the bivariate tail dependence\ncoefficient and the generative samples from normalizing flows is discussed.\nThrough simulations and a practical application analyzing the risk among five\nmajor US banks, we demonstrate that GPDFlow significantly improves modeling\naccuracy and flexibility compared to traditional parametric methods."
      ]
    }
  },
  {
    "id":2411.14752,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"\u201cApr\u00e8s Mois, Le D\u00e9luge\u201d: Preparing for the Coming Data Flood in the MRI-Guided Radiotherapy Era",
    "start_abstract":"Magnetic resonance imaging provides a sea of quantitative and semi-quantitative data. While radiation oncologists already navigate pool clinical (semantic) data, the tide will swell with advent hybrid MRI\/linear accelerator devices increasing interest in MRI-guided radiotherapy (MRIgRT), including adaptive MRIgRT. The variety MR sequences (of greater complexity than single parameter Hounsfield unit CT scanning routinely used radiotherapy), workflow fractionation, sheer quantity daily images acquired are challenges for scaling this technology. Biomedical informatics, which is science information biomedicine, can provide helpful insights looming transition. Funneling MRIgRT data into clinically meaningful streams requires committing to flow inter-institutional accessibility interoperability initiatives, standardizing dosimetry methods, streamlining linear workflow, MRI acquisition post-processing, current topic review attempt conceptually ford using informatics approaches as theoretical bridge.",
    "start_categories":[
      "Oncology"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b5"
      ],
      "title":[
        "MedNeXt: Transformer-driven Scaling of ConvNets for Medical Image Segmentation",
        "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation"
      ],
      "abstract":[
        "There has been exploding interest in embracing Transformer-based architectures for medical image segmentation. However, the lack of large-scale annotated medical datasets make achieving performances equivalent to those in natural images challenging. Convolutional networks, in contrast, have higher inductive biases and consequently, are easily trainable to high performance. Recently, the ConvNeXt architecture attempted to modernize the standard ConvNet by mirroring Transformer blocks. In this work, we improve upon this to design a modernized and scalable convolutional architecture customized to challenges of data-scarce medical settings. We introduce MedNeXt, a Transformer-inspired large kernel segmentation network which introduces - 1) A fully ConvNeXt 3D Encoder-Decoder Network for medical image segmentation, 2) Residual ConvNeXt up and downsampling blocks to preserve semantic richness across scales, 3) A novel technique to iteratively increase kernel sizes by upsampling small kernel networks, to prevent performance saturation on limited medical data, 4) Compound scaling at multiple levels (depth, width, kernel size) of MedNeXt. This leads to state-of-the-art performance on 4 tasks on CT and MRI modalities and varying dataset sizes, representing a modernized deep architecture for medical image segmentation. Our code is made publicly available at: https:\/\/github.com\/MIC-DKFZ\/MedNeXt.",
        "Biomedical imaging is a driver of scientific discovery and a core component of medical care and is being stimulated by the field of deep learning. While semantic segmentation algorithms enable image analysis and quantification in many applications, the design of respective specialized solutions is non-trivial and highly dependent on dataset properties and hardware conditions. We developed nnU-Net, a deep learning-based segmentation method that automatically configures itself, including preprocessing, network architecture, training and post-processing for any new task. The key design choices in this process are modeled as a set of fixed parameters, interdependent rules and empirical decisions. Without manual intervention, nnU-Net surpasses most existing approaches, including highly specialized solutions on 23 public datasets used in international biomedical segmentation competitions. We make nnU-Net publicly available as an out-of-the-box tool, rendering state-of-the-art segmentation accessible to a broad audience by requiring neither expert knowledge nor computing resources beyond standard network training."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "A Survey on Algorithmic Developments in Optimal Transport Problem with\n  Applications",
        "Multi-Lepton Jets from Quadruple $Z'$ via the Higgs Decay at LHC",
        "Human-Aided Trajectory Planning for Automated Vehicles through\n  Teleoperation and Arbitration Graphs",
        "Prototype Contrastive Consistency Learning for Semi-Supervised Medical\n  Image Segmentation",
        "A Conditional Point Cloud Diffusion Model for Deformable Liver Motion\n  Tracking Via a Single Arbitrarily-Angled X-ray Projection",
        "No Evidence for LLMs Being Useful in Problem Reframing",
        "Retracing the Cold Plasma Dispersion Law in Pulsar B0329+54: New\n  Insights into Frequency-Dependent Dispersion Measures",
        "KM-UNet KAN Mamba UNet for medical image segmentation",
        "MAPN: Enhancing Heterogeneous Sparse Graph Representation by Mamba-based\n  Asynchronous Aggregation",
        "Asymptotic Optimism of Random-Design Linear and Kernel Regression Models",
        "Some remarks on singular capillary cones with free boundary",
        "Characterization of Fractal Basins Using Deep Convolutional Neural\n  Networks",
        "Investigating Vulnerabilities of GPS Trip Data to Trajectory-User\n  Linking Attacks",
        "Simulating Raman Scattering Impairments with Depolarization Noise in\n  Quantum-Classical Links",
        "Finite Gr\\\"obner bases for quantum symmetric groups",
        "Fully-heavy tetraquarks in the vacuum and in a hot environment",
        "The GALAH survey: Improving chemical abundances using star clusters",
        "MedLoRD: A Medical Low-Resource Diffusion Model for High-Resolution 3D\n  CT Image Synthesis",
        "PixelWorld: Towards Perceiving Everything as Pixels",
        "Numerical Solution and Errors Analysis of Iterative Method for a\n  Nonlinear Plate Bending Problem",
        "C2GM: Cascading Conditional Generation of Multi-scale Maps from Remote\n  Sensing Images Constrained by Geographic Features",
        "SLCGC: A lightweight Self-supervised Low-pass Contrastive Graph\n  Clustering Network for Hyperspectral Images",
        "Brief analysis of DeepSeek R1 and its implications for Generative AI",
        "$L^2$ decay estimates of weak solutions to 3D fractional MHD equations\n  in exterior domains",
        "Multi-agent Multi-armed Bandits with Minimum Reward Guarantee Fairness",
        "Energy Dispersion, Superconductivity and Magnetic Fluctuations in\n  Stacked Altermagnetism Materials",
        "Scalable Trajectory-User Linking with Dual-Stream Representation\n  Networks",
        "Counting two-step nilpotent wildly ramified extensions of function\n  fields",
        "Towards a Multimodal MRI-Based Foundation Model for Multi-Level Feature\n  Exploration in Segmentation, Molecular Subtyping, and Grading of Glioma"
      ],
      "abstract":[
        "Optimal Transport (OT) has established itself as a robust framework for\nquantifying differences between distributions, with applications that span\nfields such as machine learning, data science, and computer vision. This paper\noffers a detailed examination of the OT problem, beginning with its theoretical\nfoundations, including the classical formulations of Monge and Kantorovich and\ntheir extensions to modern computational techniques. It explores cutting-edge\nalgorithms, including Sinkhorn iterations, primal-dual strategies, and\nreduction-based approaches, emphasizing their efficiency and scalability in\naddressing high-dimensional problems. The paper also highlights emerging\ntrends, such as integrating OT into machine learning frameworks, the\ndevelopment of novel problem variants, and ongoing theoretical advancements.\nApplications of OT are presented across a range of domains, with particular\nattention to its innovative application in time series data analysis via\nOptimal Transport Warping (OTW), a robust alternative to methods like Dynamic\nTime Warping. Despite the significant progress made, challenges related to\nscalability, robustness, and ethical considerations remain, necessitating\nfurther research. The paper underscores OT's potential to bridge theoretical\ndepth and practical utility, fostering impactful advancements across diverse\ndisciplines.",
        "We investigate multi-lepton jet events from the decay of the 125 GeV Higgs\nboson ($h$) into quadruple new gauge bosons $(Z')$ at the LHC. Such an exotic\ndecay is realized via the process of $h \\to \\phi \\phi \\to Z'Z'Z'Z'$ with new\nscalar boson $\\phi$ in models with an additional $U(1)$ gauge symmetry. Charged\nleptons coming from the $Z'$ decay tend to be observed as lepton-jets rather\nthan isolated leptons when the masses of $Z'$ and $\\phi$ are smaller than\n${\\cal O}$(10) GeV, because of the highly-boosted effects. Performing the\nsignal and background analyses, we find that the branching ratio of $h \\to 4Z'$\nis maximally constrained to be smaller than of order $10^{-6}$ ($10^{-7}$) by\nusing the muonic-lepton jets assuming the integrated luminosity of 140\nfb$^{-1}$ (3000 fb$^{-1}$) at LHC. For lighter $Z'$ ($< 2m_\\mu$), we can use\nthe electronic-lepton jets instead of the muon-jets, by which the upper limit\non the branching ratio is obtained to be of order $10^{-6}$-$10^{-5}$. These\nbounds can be converted into the constraint on model parameters such as a\nmixing angle between $h$ and $\\phi$. It is shown that stronger bounds on the\nmixing angle are obtained in the dark photon case as compared with the previous\nconstraints given by flavor experiments and the Higgs decay $h \\to Z'Z'$ in the\nmass range of $m_{Z'}\\lesssim 10$ GeV.",
        "Teleoperation enables remote human support of automated vehicles in scenarios\nwhere the automation is not able to find an appropriate solution. Remote\nassistance concepts, where operators provide discrete inputs to aid specific\nautomation modules like planning, is gaining interest due to its reduced\nworkload on the human remote operator and improved safety. However, these\nconcepts are challenging to implement and maintain due to their deep\nintegration and interaction with the automated driving system. In this paper,\nwe propose a solution to facilitate the implementation of remote assistance\nconcepts that intervene on planning level and extend the operational design\ndomain of the vehicle at runtime. Using arbitration graphs, a modular\ndecision-making framework, we integrate remote assistance into an existing\nautomated driving system without modifying the original software components.\nOur simulative implementation demonstrates this approach in two use cases,\nallowing operators to adjust planner constraints and enable trajectory\ngeneration beyond nominal operational design domains.",
        "Medical image segmentation is a crucial task in medical image analysis, but\nit can be very challenging especially when there are less labeled data but with\nlarge unlabeled data. Contrastive learning has proven to be effective for\nmedical image segmentation in semi-supervised learning by constructing\ncontrastive samples from partial pixels. However, although previous contrastive\nlearning methods can mine semantic information from partial pixels within\nimages, they ignore the whole context information of unlabeled images, which is\nvery important to precise segmentation. In order to solve this problem, we\npropose a novel prototype contrastive learning method called Prototype\nContrastive Consistency Segmentation (PCCS) for semi-supervised medical image\nsegmentation. The core idea is to enforce the prototypes of the same semantic\nclass to be closer and push the prototypes in different semantic classes far\naway from each other. Specifically, we construct a signed distance map and an\nuncertainty map from unlabeled images. The signed distance map is used to\nconstruct prototypes for contrastive learning, and then we estimate the\nprototype uncertainty from the uncertainty map as trade-off among prototypes.\nIn order to obtain better prototypes, based on the student-teacher\narchitecture, a new mechanism named prototype updating prototype is designed to\nassist in updating the prototypes for contrastive learning. In addition, we\npropose an uncertainty-consistency loss to mine more reliable information from\nunlabeled data. Extensive experiments on medical image segmentation demonstrate\nthat PCCS achieves better segmentation performance than the state-of-the-art\nmethods. The code is available at https:\/\/github.com\/comphsh\/PCCS.",
        "Deformable liver motion tracking using a single X-ray projection enables\nreal-time motion monitoring and treatment intervention. We introduce a\nconditional point cloud diffusion model-based framework for accurate and robust\nliver motion tracking from arbitrarily angled single X-ray projections\n(PCD-Liver), which estimates volumetric liver motion by solving deformable\nvector fields (DVFs) of a prior liver surface point cloud based on a single\nX-ray image. The model is patient-specific and consists of two main components:\na rigid alignment model to estimate the liver's overall shifts and a\nconditional point cloud diffusion model that further corrects for liver surface\ndeformations. Conditioned on motion-encoded features extracted from a single\nX-ray projection via a geometry-informed feature pooling layer, the diffusion\nmodel iteratively solves detailed liver surface DVFs in a projection\nangle-agnostic manner. The liver surface motion estimated by PCD-Liver serves\nas a boundary condition for a U-Net-based biomechanical model to infer internal\nliver motion and localize liver tumors. A dataset of ten liver cancer patients\nwas used for evaluation. The accuracy of liver point cloud motion estimation\nwas assessed using root mean square error (RMSE) and 95th-percentile Hausdorff\ndistance (HD95), while liver tumor localization error was quantified using\ncenter-of-mass error (COME). The mean (standard deviation) RMSE, HD95, and COME\nof the prior liver or tumor before motion estimation were 8.86(1.51) mm,\n10.88(2.56) mm, and 9.41(3.08) mm, respectively. After PCD-Liver motion\nestimation, the corresponding values improved to 3.59(0.28) mm, 4.29(0.62) mm,\nand 3.45(0.96) mm. Under highly noisy conditions, PCD-Liver maintained stable\nperformance. This study presents an accurate and robust framework for\ndeformable liver motion estimation and tumor localization in image-guided\nradiotherapy.",
        "Problem reframing is a designerly activity wherein alternative perspectives\nare created to recast what a stated design problem is about. Generating\nalternative problem frames is challenging because it requires devising novel\nand useful perspectives that fit the given problem context. Large language\nmodels (LLMs) could assist this activity via their generative capability.\nHowever, it is not clear whether they can help designers produce high-quality\nframes. Therefore, we asked if there are benefits to working with LLMs. To this\nend, we compared three ways of using LLMs (N=280): 1) free-form, 2) direct\ngeneration, and 3) a structured approach informed by a theory of reframing. We\nfound that using LLMs does not help improve the quality of problem frames. In\nfact, it increases the competence gap between experienced and inexperienced\ndesigners. Also, inexperienced ones perceived lower agency when working with\nLLMs. We conclude that there is no benefit to using LLMs in problem reframing\nand discuss possible factors for this lack of effect.",
        "Multiple studies have investigated potential frequency-dependent dispersion\nmeasures (DM) in PSR B0329+54, with sensitivities at levels of $10^{-3} \\,\n\\text{pc} \\, \\text{cm}^{-3}$ or higher, using frequencies below 1 GHz.\nUtilizing the extensive bandwidth of the upgraded Giant Meterwave Radio\nTelescope, we conducted simultaneous observations of this pulsar across a\nfrequency range of 300 to 1460 MHz. Our observations reveal a distinct point in\nthe pulse profile of PSR B0329+54 that appears to align remarkably well with\nthe cold-plasma dispersion law, resulting in a unique measured DM across the\nentire frequency range. In contrast, using times of arrival (ToAs) from widely\nadopted pulsar timing techniques (e.g., FFTFIT)-leads to frequency-dependent\nDMs. We investigated the potential causes of these frequency-dependent DMs in\nthis pulsar and their relationship with the underlying magnetic field geometry\ncorresponding to the radio emission. Our study reveals that all frequencies in\nthe range 300-1460 MHz originate from a region no larger than 204 km, and the\ndipolar magnetic-field geometry model indicates that the emission region is\ncentered at $\\sim$800 km from the star. This is the tightest constraint on the\nsize of the emission region reported so far for PSR B0329+54 at the given\nfrequencies, and it is at least five times more stringent than the existing\nemission height constraints based on the dipolar geometry model.",
        "Medical image segmentation is a critical task in medical imaging analysis.\nTraditional CNN-based methods struggle with modeling long-range dependencies,\nwhile Transformer-based models, despite their success, suffer from quadratic\ncomputational complexity. To address these limitations, we propose KM-UNet, a\nnovel U-shaped network architecture that combines the strengths of\nKolmogorov-Arnold Networks (KANs) and state-space models (SSMs). KM-UNet\nleverages the Kolmogorov-Arnold representation theorem for efficient feature\nrepresentation and SSMs for scalable long-range modeling, achieving a balance\nbetween accuracy and computational efficiency. We evaluate KM-UNet on five\nbenchmark datasets: ISIC17, ISIC18, CVC, BUSI, and GLAS. Experimental results\ndemonstrate that KM-UNet achieves competitive performance compared to\nstate-of-the-art methods in medical image segmentation tasks. To the best of\nour knowledge, KM-UNet is the first medical image segmentation framework\nintegrating KANs and SSMs. This work provides a valuable baseline and new\ninsights for the development of more efficient and interpretable medical image\nsegmentation systems. The code is open source at\nhttps:\/\/github.com\/2760613195\/KM_UNet\n  Keywords:KAN,Manba, state-space models,UNet, Medical image segmentation, Deep\nlearning",
        "Graph neural networks (GNNs) have become the state of the art for various\ngraph-related tasks and are particularly prominent in heterogeneous graphs\n(HetGs). However, several issues plague this paradigm: first, the difficulty in\nfully utilizing long-range information, known as over-squashing; second, the\ntendency for excessive message-passing layers to produce indistinguishable\nrepresentations, referred to as over-smoothing; and finally, the inadequacy of\nconventional MPNNs to train effectively on large sparse graphs. To address\nthese challenges in deep neural networks for large-scale heterogeneous graphs,\nthis paper introduces the Mamba-based Asynchronous Propagation Network (MAPN),\nwhich enhances the representation of heterogeneous sparse graphs. MAPN consists\nof two primary components: node sequence generation and semantic information\naggregation. Node sequences are initially generated based on meta-paths through\nrandom walks, which serve as the foundation for a spatial state model that\nextracts essential information from nodes at various distances. It then\nasynchronously aggregates semantic information across multiple hops and layers,\neffectively preserving unique node characteristics and mitigating issues\nrelated to deep network degradation. Extensive experiments across diverse\ndatasets demonstrate the effectiveness of MAPN in graph embeddings for various\ndownstream tasks underscoring its substantial benefits for graph representation\nin large sparse heterogeneous graphs.",
        "We derived the closed-form asymptotic optimism of linear regression models\nunder random designs, and generalizes it to kernel ridge regression. Using\nscaled asymptotic optimism as a generic predictive model complexity measure, we\nstudied the fundamental different behaviors of linear regression model, tangent\nkernel (NTK) regression model and three-layer fully connected neural networks\n(NN). Our contribution is two-fold: we provided theoretical ground for using\nscaled optimism as a model predictive complexity measure; and we show\nempirically that NN with ReLUs behaves differently from kernel models under\nthis measure. With resampling techniques, we can also compute the optimism for\nregression models with real data.",
        "We study minimizing singular cones with free boundary associated with the\ncapillarity problem. Precisely, we provide a stability criterion $\\`a$ la\nJerison-Savin for capillary hypersurfaces and show that, in dimensions up to\n$4$, minimizing cones with non-sign-changing mean curvature are flat. We apply\nthis criterion to minimizing capillary drops and, additionally, establish the\ninstability of non-trivial axially symmetric cones in dimensions up to $6$.\n  The main results are based on a Simons-type inequality for a class of convex,\nhomogeneous, symmetric functions of the principal curvatures, combined with a\nboundary condition specific to the capillary setting.",
        "Neural network models have recently demonstrated impressive prediction\nperformance in complex systems where chaos and unpredictability appear. In\nspite of the research efforts carried out on predicting future trajectories or\nimproving their accuracy compared to numerical methods, not sufficient work has\nbeen done by using deep learning techniques in which they characterize the\nunpredictability of chaotic systems or give a general view of the global\nunpredictability of a system. In this work we propose a novel approach based on\ndeep learning techniques to measure the fractal dimension of the basins of\nattraction of the Duffing oscillator for a variety of parameters. As a\nconsequence, we provide an algorithm capable of predicting fractal dimension\nmeasures as accurately as the conventional algorithm, but with a computation\nspeed about ten times faster.",
        "Open human mobility data is considered an essential basis for the profound\nresearch and analysis required for the transition to sustainable mobility and\nsustainable urban planning. Cycling data has especially been the focus of data\ncollection endeavors in recent years. Although privacy risks regarding location\ndata are widely known, practitioners often refrain from advanced privacy\nmechanisms to prevent utility losses. Removing user identifiers from trips is\nthereby deemed a major privacy gain, as it supposedly prevents linking single\ntrips to obtain entire movement patterns. In this paper, we propose a novel\nattack to reconstruct user identifiers in GPS trip datasets consisting of\nsingle trips, unlike previous ones that are dedicated to evaluating\ntrajectory-user linking in the context of check-in data. We evaluate the\nremaining privacy risk for users in such datasets and our empirical findings\nfrom two real-world datasets show that the risk of re-identification is\nsignificant even when personal identifiers have been removed, and that\ntruncation as a simple additional privacy mechanism may not be effective in\nprotecting user privacy. Further investigations indicate that users who\nfrequently visit locations that are only visited by a small number of others,\ntend to be more vulnerable to re-identification.",
        "We model spontaneous Raman scattering noise in polarization-encoded quantum\ncommunication channels co-propagating with classical signals using the\ndepolarization channel. Utilizing NetSquid simulations, we validate the model\nagainst demonstrations of qubit transmission, entanglement distribution, and\nteleportation.",
        "Non-commutative Gr\\\"obner bases of two-sided ideals are not necessarily\nfinite. Motivated by this, we provide a closed-form description of a finite and\nreduced Gr\\\"obner bases for the two-sided ideal used in the construction of\nWangs quantum symmetric group. In particular, this proves that the word problem\nfor quantum symmetric groups is decidable.",
        "We study the thermal behavior of quarkonia and fully-heavy tetraquark states\nassociated to the charmonium, bottomonium and bottom-charmonium mass spectra.\nThe starting point is the Schr\\\"odinger formalism with a vacuum Cornell-like\npotential. The spin-spin, spin-orbit and tensor contributions are also\nconsidered to describe the structure of the vacuum quarkonia $Q\\bar Q$ spectra\n($Q$ denoting $c,b$ quarks). The parameters of the model are fixed using the\nexperimental data of the $Q\\bar Q$ states. After that, this formalism is\nextended to the fully-heavy tetraquark states within the $1^3 S_1$ axial\ndiquark--$1^3 S_1$ axial antidiquark configuration $[QQ] [\\bar Q \\bar Q]$, and\ntheir vacuum mass spectra are obtained and compared to the experimental data\nrecently obtained. Our predictions support the interpretation of the $X(6600)$\n(or $X(6552)$), $X(6900)$ and $X(7200)$ states as the radially-excited\n$T_{4c}(n^1S_0)$ configurations with $n=2,3,4$. In the sequence, we evaluate\nthe mass spectra behavior in a thermal medium, by introducing a modified\ntemperature-dependent Cornell potential. As a consequence, this formalism\nenables us to get some insight into the dissociation mechanism of $[QQ] [\\bar Q\n\\bar Q]$ states caused by a thermal medium, and into the temperature range at\nwhich the tetraquark states might be formed. We find that these structures\ncannot be formed in the thermal medium when the system has a temperature higher\nthan about twice the critical temperature. These findings may be useful to\nbetter understand the features of the exotics in heavy-ion collisions.",
        "Large spectroscopic surveys aim to consistently compute stellar parameters of\nvery diverse stars while minimizing systematic errors. We explore the use of\nstellar clusters as benchmarks to verify the precision of spectroscopic\nparameters in the 4. data release (DR4) of the GALAH survey. We examine 58 open\nand globular clusters and associations to validate measurements of temperature,\ngravity, chemical abundances, and stellar ages. We focus on identifying\nsystematic errors and understanding trends between stellar parameters,\nparticularly temperature and chemical abundances. We identify trends by\nstacking measurements of chemical abundances against effective temperature and\nmodelling them with splines. We also refit spectra in three clusters with the\nSpectroscopy Made Easy and Korg packages to reproduce the trends in DR4 and to\nsearch for their origin by varying temperature and gravity priors, linelists,\nand spectral continuum. Trends are consistent between clusters of different\nages and metallicities, can reach amplitudes of ~0.5 dex and differ for dwarfs\nand giants. We use the derived trends to correct the DR4 abundances of 24 and\n31 chemical elements for dwarfs and giants, and publish a detrended catalogue.\nWhile the origin of the trends could not be pinpointed, we found that: i)\nphotometric priors affect derived abundances, ii) temperature, metallicity, and\ncontinuum levels are degenerate in spectral fitting, and it is hard to break\nthe degeneracy even by using independent measurements, iii) the completeness of\nthe linelist used in spectral synthesis is essential for cool stars, and iv)\ndifferent spectral fitting codes produce significantly different iron\nabundances for stars of all temperatures. We conclude that clusters can be used\nto characterise the systematic errors of parameters produced in large surveys,\nbut further research is needed to explain the origin of the trends.",
        "Advancements in AI for medical imaging offer significant potential. However,\ntheir applications are constrained by the limited availability of data and the\nreluctance of medical centers to share it due to patient privacy concerns.\nGenerative models present a promising solution by creating synthetic data as a\nsubstitute for real patient data. However, medical images are typically\nhigh-dimensional, and current state-of-the-art methods are often impractical\nfor computational resource-constrained healthcare environments. These models\nrely on data sub-sampling, raising doubts about their feasibility and\nreal-world applicability. Furthermore, many of these models are evaluated on\nquantitative metrics that alone can be misleading in assessing the image\nquality and clinical meaningfulness of the generated images. To address this,\nwe introduce MedLoRD, a generative diffusion model designed for computational\nresource-constrained environments. MedLoRD is capable of generating\nhigh-dimensional medical volumes with resolutions up to\n512$\\times$512$\\times$256, utilizing GPUs with only 24GB VRAM, which are\ncommonly found in standard desktop workstations. MedLoRD is evaluated across\nmultiple modalities, including Coronary Computed Tomography Angiography and\nLung Computed Tomography datasets. Extensive evaluations through radiological\nevaluation, relative regional volume analysis, adherence to conditional masks,\nand downstream tasks show that MedLoRD generates high-fidelity images closely\nadhering to segmentation mask conditions, surpassing the capabilities of\ncurrent state-of-the-art generative models for medical image synthesis in\ncomputational resource-constrained environments.",
        "Existing foundation models typically process visual input as pixels and\ntextual input as tokens, a paradigm that contrasts with human perception, where\nboth modalities are processed in a unified manner. With the rise of embodied\nand agentic AI, where inputs primarily come from camera pixels, the need for a\nunified perception framework becomes increasingly evident. In this paper, we\npropose to unify all modalities (text, tables, code, diagrams, images, etc) as\npixel inputs, i.e. \"Perceive Everything as Pixels\" (PEAP). We introduce\nPixelWorld, a novel evaluation suite that unifies all the mentioned modalities\ninto pixel space to gauge the existing models' performance. Our findings show\nthat (1) PEAP outperforms baseline with token-based input in multimodal\ndatasets, benefiting from unified input for better disambiguation, (2)\nsignificant declines in reasoning and coding capabilities across all models\nwhen processing pixel-based input, underscoring the need to enhance foundation\nmodels' perceptual abilities, (3) larger models can maintain strong performance\non non-reasoning tasks under PEAP, while smaller models like Phi-3.5-V suffer\nsignificant performance degradation, (4) the attention pattern of PEAP is\nhighly aligned with text token input, (5) PEAP can be accelerated significantly\nby exploiting the spatial sparsity. We conclude that the existing frontier\nmodels are competent in pixel perception, however, there is still headroom for\nimprovement. Our code, dataset will be released upon acceptance.",
        "This paper uses the HCT finite element method and mesh adaptation technology\nto solve the nonlinear plate bending problem and conducts error analysis on the\niterative method, including a priori and a posteriori error estimates. Our\ninvestigation exploits Hermite finite elements such as BELL and\nHSIEH-CLOUGH-TOCHER (HCT) triangles for conforming finite element\ndiscretization. Then, the existence and uniqueness of the approximation\nsolution are proven by using a variant of the Brezzi-Rappaz-Raviart theorem. We\nsolve the approximation problem through a fixed-point strategy and an iterative\nalgorithm, and study the convergence of the iterative algorithm, and provide\nthe convergence conditions. An optimal a priori error estimation has been\nestablished. We construct a posteriori error indicators by distinguishing\nbetween discretization and linearization errors and prove their reliability and\noptimality. A numerical test is carried out and the results obtained confirm\nthose established theoreticall.",
        "Multi-scale maps are essential representations of surveying and cartographic\nresults, serving as fundamental components of geographic services. Current\nimage generation networks can quickly produce map tiles from remote-sensing\nimages. However, generative models designed for natural images often focus on\ntexture features, neglecting the unique characteristics of remote-sensing\nfeatures and the scale attributes of tile maps. This limitation in generative\nmodels impairs the accurate representation of geographic information, and the\nquality of tile map generation still needs improvement. Diffusion models have\ndemonstrated remarkable success in various image generation tasks, highlighting\ntheir potential to address this challenge. This paper presents C2GM, a novel\nframework for generating multi-scale tile maps through conditional guided\ndiffusion and multi-scale cascade generation. Specifically, we implement a\nconditional feature fusion encoder to extract object priors from remote sensing\nimages and cascade reference double branch input, ensuring an accurate\nrepresentation of complex features. Low-level generated tiles act as\nconstraints for high-level map generation, enhancing visual continuity.\nMoreover, we incorporate map scale modality information using CLIP to simulate\nthe relationship between map scale and cartographic generalization in tile\nmaps. Extensive experimental evaluations demonstrate that C2GM consistently\nachieves the state-of-the-art (SOTA) performance on all metrics, facilitating\nthe rapid and effective generation of multi-scale large-format maps for\nemergency response and remote mapping applications.",
        "Self-supervised hyperspectral image (HSI) clustering remains a fundamental\nyet challenging task due to the absence of labeled data and the inherent\ncomplexity of spatial-spectral interactions. While recent advancements have\nexplored innovative approaches, existing methods face critical limitations in\nclustering accuracy, feature discriminability, computational efficiency, and\nrobustness to noise, hindering their practical deployment. In this paper, a\nself-supervised efficient low-pass contrastive graph clustering (SLCGC) is\nintroduced for HSIs. Our approach begins with homogeneous region generation,\nwhich aggregates pixels into spectrally consistent regions to preserve local\nspatial-spectral coherence while drastically reducing graph complexity. We then\nconstruct a structural graph using an adjacency matrix A and introduce a\nlow-pass graph denoising mechanism to suppress high-frequency noise in the\ngraph topology, ensuring stable feature propagation. A dual-branch graph\ncontrastive learning module is developed, where Gaussian noise perturbations\ngenerate augmented views through two multilayer perceptrons (MLPs), and a\ncross-view contrastive loss enforces structural consistency between views to\nlearn noise-invariant representations. Finally, latent embeddings optimized by\nthis process are clustered via K-means. Extensive experiments and repeated\ncomparative analysis have verified that our SLCGC contains high clustering\naccuracy, low computational complexity, and strong robustness. The code source\nwill be available at https:\/\/github.com\/DY-HYX.",
        "In late January 2025, DeepSeek released their new reasoning model (DeepSeek\nR1); which was developed at a fraction of the cost yet remains competitive with\nOpenAI's models, despite the US's GPU export ban. This report discusses the\nmodel, and what its release means for the field of Generative AI more widely.\nWe briefly discuss other models released from China in recent weeks, their\nsimilarities; innovative use of Mixture of Experts (MoE), Reinforcement\nLearning (RL) and clever engineering appear to be key factors in the\ncapabilities of these models. This think piece has been written to a tight\ntimescale, providing broad coverage of the topic, and serves as introductory\nmaterial for those looking to understand the model's technical advancements, as\nwell as its place in the ecosystem. Several further areas of research are\nidentified.",
        "Consider three-dimensional fractional MHD equations in an exterior domain\nwith the Dirichlet boundary condition assumed. Asymptotic behaviours of weak\nsolutions to the three-dimensional exterior fractional MHD equations are\nstudied. $L^2$ decay estimates of the weak solutions are obtained.",
        "We investigate the problem of maximizing social welfare while ensuring\nfairness in a multi-agent multi-armed bandit (MA-MAB) setting. In this problem,\na centralized decision-maker takes actions over time, generating random rewards\nfor various agents. Our goal is to maximize the sum of expected cumulative\nrewards, a.k.a. social welfare, while ensuring that each agent receives an\nexpected reward that is at least a constant fraction of the maximum possible\nexpected reward.\n  Our proposed algorithm, RewardFairUCB, leverages the Upper Confidence Bound\n(UCB) technique to achieve sublinear regret bounds for both fairness and social\nwelfare. The fairness regret measures the positive difference between the\nminimum reward guarantee and the expected reward of a given policy, whereas the\nsocial welfare regret measures the difference between the social welfare of the\noptimal fair policy and that of the given policy.\n  We show that RewardFairUCB algorithm achieves instance-independent social\nwelfare regret guarantees of $\\tilde{O}(T^{1\/2})$ and a fairness regret upper\nbound of $\\tilde{O}(T^{3\/4})$. We also give the lower bound of\n$\\Omega(\\sqrt{T})$ for both social welfare and fairness regret. We evaluate\nRewardFairUCB's performance against various baseline and heuristic algorithms\nusing simulated data and real world data, highlighting trade-offs between\nfairness and social welfare regrets.",
        "Recently, altermagnetism (AM) has emerged as a new category of magnetism,\nalongside conventional antiferromagnetism (AFM) and ferromagnetism (FM). In an\nAM, superconductivity (SC) is faced with a dilemma that the spin-polarized\nbands, induced by the broken time reversal (T ) symmetry, dominantly supports\nspin-triplet pairing. In contrast, AM spin fluctuations routinely facilitate\nspin-singlet pairing as in AFM. Consequently, unconventional SC is either\nabsent or weak in AM materials. Here, we propose that stacking 2D AM materials\ncould resolve this dilemma. Stacked 2D materials have yielded a variety of new\nelectronic properties by altering the symmetries inherent in the monolayer. In\na 2D anisotropic Hubbard model, we investigate the general energy dispersions\nof both single-layer and stacked AM materials. We demonstrate that AM sheet\nstacking can alter the original symmetries, consequently affecting the energy\ndispersion. The interlayer magnetic coupling enhances the low q magnetic\nfluctuations. T symmetry is restored in the AA stacking with an\nantiferromagnetic interlayer coupling, and then both the energy dispersion and\npairing interaction are in favor of spin-singlet SC. The ferromagnetic\ninterlayer coupling in the AB stacking not only recovers T symmetry but also\nsupports spin-triplet pairing. It is further anticipated that twisted bilayer\nAM sheets could exhibit additional novel electronic properties, including\ntopology, flat bands, and collective excitations. Our work illustrates that\nstacking sheets of AM materials could open up a unique research domain in\nexploring novel quantum phenomena and offer a fertile ground for potential\nelectronic applications.",
        "Trajectory-user linking (TUL) aims to match anonymous trajectories to the\nmost likely users who generated them, offering benefits for a wide range of\nreal-world spatio-temporal applications. However, existing TUL methods are\nlimited by high model complexity and poor learning of the effective\nrepresentations of trajectories, rendering them ineffective in handling\nlarge-scale user trajectory data. In this work, we propose a novel\n$\\underline{Scal}$abl$\\underline{e}$ Trajectory-User Linking with dual-stream\nrepresentation networks for large-scale $\\underline{TUL}$ problem, named\nScaleTUL. Specifically, ScaleTUL generates two views using temporal and spatial\naugmentations to exploit supervised contrastive learning framework to\neffectively capture the irregularities of trajectories. In each view, a\ndual-stream trajectory encoder, consisting of a long-term encoder and a\nshort-term encoder, is designed to learn unified trajectory representations\nthat fuse different temporal-spatial dependencies. Then, a TUL layer is used to\nassociate the trajectories with the corresponding users in the representation\nspace using a two-stage training model. Experimental results on check-in\nmobility datasets from three real-world cities and the nationwide U.S.\ndemonstrate the superiority of ScaleTUL over state-of-the-art baselines for\nlarge-scale TUL tasks.",
        "We study the asymptotic distribution of wildly ramified extensions of\nfunction fields in characteristic $p > 2$, focusing on (certain) $p$-groups of\nnilpotency class at most $2$. Rather than the discriminant, we count extensions\naccording to an invariant describing the last jump in the ramification\nfiltration at each place. We prove a local-global principle relating the\ndistribution of extensions over global function fields to their distribution\nover local fields, leading to an asymptotic formula for the number of\nextensions with a given global last-jump invariant. A key ingredient is\nAbrashkin's nilpotent Artin-Schreier theory, which lets us parametrize\nextensions and obtain bounds on the ramification of local extensions by\nestimating the number of solutions to certain polynomial equations over finite\nfields.",
        "Accurate, noninvasive glioma characterization is crucial for effective\nclinical management. Traditional methods, dependent on invasive tissue\nsampling, often fail to capture the spatial heterogeneity of the tumor. While\ndeep learning has improved segmentation and molecular profiling, few approaches\nsimultaneously integrate tumor morphology and molecular features. Foundation\ndeep learning models, which learn robust, task-agnostic representations from\nlarge-scale datasets, hold great promise but remain underutilized in glioma\nimaging biomarkers. We propose the Multi-Task SWIN-UNETR (MTS-UNET) model, a\nnovel foundation-based framework built on the BrainSegFounder model, pretrained\non large-scale neuroimaging data. MTS-UNET simultaneously performs glioma\nsegmentation, histological grading, and molecular subtyping (IDH mutation and\n1p\/19q co-deletion). It incorporates two key modules: Tumor-Aware Feature\nEncoding (TAFE) for multi-scale, tumor-focused feature extraction and\nCross-Modality Differential (CMD) for highlighting subtle T2-FLAIR mismatch\nsignals associated with IDH mutation. The model was trained and validated on a\ndiverse, multi-center cohort of 2,249 glioma patients from seven public\ndatasets. MTS-UNET achieved a mean Dice score of 84% for segmentation, along\nwith AUCs of 90.58% for IDH mutation, 69.22% for 1p\/19q co-deletion prediction,\nand 87.54% for grading, significantly outperforming baseline models (p<=0.05).\nAblation studies validated the essential contributions of the TAFE and CMD\nmodules and demonstrated the robustness of the framework. The foundation-based\nMTS-UNET model effectively integrates tumor segmentation with multi-level\nclassification, exhibiting strong generalizability across diverse MRI datasets.\nThis framework shows significant potential for advancing noninvasive,\npersonalized glioma management by improving predictive accuracy and\ninterpretability."
      ]
    }
  },
  {
    "id":2411.14752,
    "research_type":"applied",
    "start_id":"b5",
    "start_title":"MedNeXt: Transformer-driven Scaling of ConvNets for Medical Image Segmentation",
    "start_abstract":"There has been exploding interest in embracing Transformer-based architectures for medical image segmentation. However, the lack of large-scale annotated medical datasets make achieving performances equivalent to those in natural images challenging. Convolutional networks, in contrast, have higher inductive biases and consequently, are easily trainable to high performance. Recently, the ConvNeXt architecture attempted to modernize the standard ConvNet by mirroring Transformer blocks. In this work, we improve upon this to design a modernized and scalable convolutional architecture customized to challenges of data-scarce medical settings. We introduce MedNeXt, a Transformer-inspired large kernel segmentation network which introduces - 1) A fully ConvNeXt 3D Encoder-Decoder Network for medical image segmentation, 2) Residual ConvNeXt up and downsampling blocks to preserve semantic richness across scales, 3) A novel technique to iteratively increase kernel sizes by upsampling small kernel networks, to prevent performance saturation on limited medical data, 4) Compound scaling at multiple levels (depth, width, kernel size) of MedNeXt. This leads to state-of-the-art performance on 4 tasks on CT and MRI modalities and varying dataset sizes, representing a modernized deep architecture for medical image segmentation. Our code is made publicly available at: https:\/\/github.com\/MIC-DKFZ\/MedNeXt.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "\u201cApr\u00e8s Mois, Le D\u00e9luge\u201d: Preparing for the Coming Data Flood in the MRI-Guided Radiotherapy Era"
      ],
      "abstract":[
        "Magnetic resonance imaging provides a sea of quantitative and semi-quantitative data. While radiation oncologists already navigate pool clinical (semantic) data, the tide will swell with advent hybrid MRI\/linear accelerator devices increasing interest in MRI-guided radiotherapy (MRIgRT), including adaptive MRIgRT. The variety MR sequences (of greater complexity than single parameter Hounsfield unit CT scanning routinely used radiotherapy), workflow fractionation, sheer quantity daily images acquired are challenges for scaling this technology. Biomedical informatics, which is science information biomedicine, can provide helpful insights looming transition. Funneling MRIgRT data into clinically meaningful streams requires committing to flow inter-institutional accessibility interoperability initiatives, standardizing dosimetry methods, streamlining linear workflow, MRI acquisition post-processing, current topic review attempt conceptually ford using informatics approaches as theoretical bridge."
      ],
      "categories":[
        "Oncology"
      ]
    },
    "list":{
      "title":[
        "Soliquidy: a descriptor for atomic geometrical confusion",
        "Development of the Timing System for the X-Ray Imaging and Spectroscopy\n  Mission",
        "Statistical Challenges in Analyzing Migrant Backgrounds Among University\n  Students: a Case Study from Italy",
        "On the equivalence between galaxy angular correlation function and power\n  spectrum in constraining primordial non-Gaussianity",
        "Two-loop light-quark Electroweak corrections to Higgs boson pair\n  production in gluon fusion",
        "The Change of Variable Formula Integrals, do they have equal value?",
        "Molecular laser cooling using serrodynes: Implementation,\n  characterization and prospects",
        "Optimal control in combination therapy for heterogeneous cell\n  populations with drug synergies",
        "The surface binding and energy issues in rational design of the\n  separation membrane of Li||S batteries",
        "Rigidity of anti-de Sitter (2+1)-spacetimes with convex boundary near\n  the Fuchsian locus",
        "Transferable Foundation Models for Geometric Tasks on Point Cloud\n  Representations: Geometric Neural Operators",
        "First generation 4H-SiC LGAD production and its performance evaluation",
        "Quadratic BSDEs with Singular Generators and Unbounded Terminal\n  Conditions: Theory and Applications",
        "Game of grounds",
        "Sharp stability for critical points of the Sobolev inequality in the\n  absence of bubbling",
        "Ideal MHD. Part II: Rigidity from infinity for ideal Alfv\\'en waves in\n  3D thin domains",
        "Thin Spectra for Periodic and Ergodic Word Models",
        "Peaky Finders: Characterizing Double-Peaked Type IIb Supernovae in\n  Large-Scale Live-Stream Photometric Surveys",
        "Benefits of Early Stopping in Gradient Descent for Overparameterized\n  Logistic Regression",
        "Combining Physics and Mathematics Learning: A Taylor Series Analysis of\n  an Oscillating Magnetic Field",
        "Exceptional field theories",
        "A single-component regularity criterion and Inviscid limit of axially\n  symmetric MHD-Boussinesq system",
        "Sophomore's dream function: asymptotics, complex plane behavior and\n  relation to the error function",
        "Quantum critical electro-optic and piezo-electric nonlinearities",
        "Ion-kinetic-energy sampling in a 22-pole trap using ring-electrode\n  evaporation",
        "The category of anyon sectors for non-abelian quantum double models",
        "Averaging method for quasi-periodic response solutions",
        "First-principles Investigation of Exceptional Coarsening-resistant\n  V-Sc(Al2Cu)4 Nanoprecipitates in Al-Cu-Mg-Ag-Sc Alloys",
        "Counterdiabatic-influenced Floquet-engineering: State preparation,\n  annealing and learning the adiabatic gauge potential"
      ],
      "abstract":[
        "Tailoring material properties often requires understanding the solidification\nprocess. Herein, we introduce the geometric descriptor Soliquidy, which\nnumerically captures the Euclidean transport cost between the translationally\ndisordered versus ordered states of a materials. As a testbed, we apply\nSoliquidy to the classification of glass-forming metal alloys. By extending and\ncombining an experimental library of metallic thin-films (glass\/no-glass) with\nthe aflow.org computational database (geometrical and energetic information of\nmixtures) we found that the combination of Soliquity and formation enthalpies\ngenerates an effective classifier for glass formation. Such classifier is then\nused to tackle a public dataset of metallic glasses showing that the\nglass-agnostic assumptions of Soliquity can be useful for understanding\nkinetically-controlled phase transitions.",
        "This paper describes the development, design, ground verification, and\nin-orbit verification, performance measurement, and calibration of the timing\nsystem for the X-Ray Imaging and Spectroscopy Mission (XRISM). The scientific\ngoals of the mission require an absolute timing accuracy of 1.0~ms. All\ncomponents of the timing system were designed and verified to be within the\ntiming error budgets, which were assigned by component to meet the\nrequirements. After the launch of XRISM, the timing capability of the\nground-tuned timing system was verified using the millisecond pulsar\nPSR~B1937+21 during the commissioning period, and the timing jitter of the bus\nand the ground component were found to be below $15~\\mu$s compared to the NICER\n(Neutron star Interior Composition ExploreR) profile. During the performance\nverification and calibration period, simultaneous observations of the Crab\npulsar by XRISM, NuSTAR (Nuclear Spectroscopic Telescope Array), and NICER were\nmade to measure the absolute timing offset of the system, showing that the\narrival time of the main pulse with XRISM was aligned with that of NICER and\nNuSTAR to within $200~\\mu$s. In conclusion, the absolute timing accuracy of the\nbus and the ground component of the XRISM timing system meets the timing error\nbudget of $500~\\mu$s.",
        "The methodological issues and statistical complexities of analyzing\nuniversity students with migrant backgrounds is explored, focusing on Italian\ndata from the University of Milano-Bicocca. With the increasing size of migrant\npopulations and the growth of the second and middle generations, the need has\nrisen for deeper knowledge of the various strata of this population, including\nuniversity students with migrant backgrounds. This presents challenges due to\ninconsistent recording in university datasets. By leveraging both\nadministrative records and an original targeted survey we propose a methodology\nto fully identify the study population of students with migrant histories, and\nto distinguish relevant subpopulations within it such as second-generation born\nin Italy. Traditional logistic regression and machine learning random forest\nmodels are used and compared to predict migrant status. The primary\ncontribution lies in creating an expanded administrative dataset enriched with\nindicators of students' migrant backgrounds and status. The expanded dataset\nprovides a critical foundation for analyzing the characteristics of students\nwith migration histories across all variables routinely registered in the\nadministrative data set. Additionally, findings highlight the presence of\nselection bias in the targeted survey data, underscoring the need of further\nresearch.",
        "We investigate the angular power spectrum ($C_\\ell)$ and angular correlation\nfunction ($w(\\theta)$) of galaxy number density field in the presence of the\nlocal-type primordial non-Gaussianity (PNG), explicitly accounting for the\nintegral constraint in an all-sky survey. We show that the PNG signature in\n$C_{\\ell}$ is confined to low multipoles in the linear regime, whereas its\nsignature in $w(\\theta)$ extends across a wide range of angular scales,\nincluding those below the nonlinear scale. Therefore, the equivalence between\n$C_\\ell$ and $w(\\theta)$ can be violated when scale cuts of multipoles or\nangular scales -- for example, to mitigate systematic effects -- are applied in\nthe analysis. Assuming samples of photometric galaxies divided into multiple\nredshift bins in the range $0<z<7$, we forecast the precision of constraining\nthe PNG parameter ($f_{\\rm NL}$) from the hypothetical measurements of $C_\\ell$\nor $w(\\theta)$ assuming different scale cuts in the multipoles or angular\nscales, respectively. Our results imply that the PNG information can be\nextracted from $w(\\theta)$ on relatively small angular scales such as $\\lesssim\n10$ degree for a high-redshift galaxy sample or from $w(\\theta)$ measured in a\nsurvey with partial area coverage.",
        "We compute two-loop electroweak corrections to double Higgs boson production\nin gluon fusion mediated by light quarks in a fully analytical way. We\ndetermine a basis of master integrals satisfying canonical differential\nequations in $\\mathrm{d}\\log$ form, enhanced by subsequent rotations to remove\nunnecessary functions that do not appear in the analytic expressions of the\namplitudes. We determine the integration constants by matching our expressions\nto the large mass expansion limit of the canonical integrals. We express the\nsolution of differential equations in terms of Chen iterated integrals up to\ntranscendental weight six over logarithmic kernels with algebraic arguments,\nand further decompose them by employing a basis of uniform weight functions. By\nderiving differential equations for such basis, we provide numerical results as\nwell as routines for optimised numerical evaluations.",
        "Assuming that the two integrals in the Change of Variable Formula for the\nunidimensional Riemann integral are finite, one can ask if they have equal\nvalue. We give a positive answer to this question. The proof is very easy to\nfollow and to keep in mind. An example is given.",
        "An important effort is currently underway to extend optical cycling and laser\ncooling to more molecular species. Significant challenges arise in particular\nwhen multiple nuclear spins give rise to complex, resolved hyperfine spectra,\nas is the case for several molecular species relevant to precision tests of\nfundamental symmetries. We provide a detailed introduction to the use of\noptical spectra generated via serrodyne waveforms to address this complexity.\nWe discuss our experimental implementation of these serrodynes, characterize\ntheir properties, and outline procedures to find optimized sideband\nconfigurations that generate strong laser cooling forces. We demonstrate the\napplication of these techniques to barium monofluoride molecules and explore\ntheir prospects for the cooling of other species relevant to the study of\nfundamental physics.",
        "Cell heterogeneity plays an important role in patient responses to drug\ntreatments. In many cancers, it is associated with poor treatment outcomes.\nMany modern drug combination therapies aim to exploit cell heterogeneity, but\ndetermining how to optimise responses from heterogeneous cell populations while\naccounting for multi-drug synergies remains a challenge. In this work, we\nintroduce and analyse a general optimal control framework that can be used to\nmodel the treatment response of multiple cell populations that are treated with\nmultiple drugs that mutually interact. In this framework, we model the effect\nof multiple drugs on the cell populations using a system of coupled semi-linear\nordinary differential equations and derive general results for the optimal\nsolutions. We then apply this framework to three canonical examples and discuss\nthe wider question of how to relate mathematical optimality to clinically\nobservable outcomes, introducing a systematic approach to propose qualitatively\ndifferent classes of drug dosing inspired by optimal control.",
        "Lithium-sulfur batteries (LSBs) represent one of the most promising\nnext-generation energy storage technologies, offering exceptionally high energy\ndensities. However, their widespread adoption remains hindered by challenges\nsuch as sluggish conversion reactions and the dissolution of lithium\npolysulfides, which lead to poor cycling stability and reduced performance.\nWhile significant efforts have been made to address these limitations, the\nenergy storage capabilities of LSBs in practical devices remain far from\nachieving their full potential. This report delves into recent advancements in\nthe rational design of separation membranes for LSBs, focusing on addressing\nfundamental issues related to surface binding and surface energy interactions\nwithin materials science. By examining the functionalization and optimization\nof separation membranes, we aim to highlight strategies that can guide the\ndevelopment of more robust and efficient LSBs, bringing them closer to\npractical implementation.",
        "We prove that globally hyperbolic compact anti-de Sitter (2+1)-spacetimes\nwith strictly convex spacelike boundary that is either smooth or polyhedral and\nwhose holonomy is close to Fuchsian are determined by the induced metric on the\nboundary.",
        "We introduce methods for obtaining pretrained Geometric Neural Operators\n(GNPs) that can serve as basal foundation models for use in obtaining geometric\nfeatures. These can be used within data processing pipelines for machine\nlearning tasks and numerical methods. We show how our GNPs can be trained to\nlearn robust latent representations for the differential geometry of\npoint-clouds to provide estimates of metric, curvature, and other shape-related\nfeatures. We demonstrate how our pre-trained GNPs can be used (i) to estimate\nthe geometric properties of surfaces of arbitrary shape and topologies with\nrobustness in the presence of noise, (ii) to approximate solutions of geometric\npartial differential equations (PDEs) on manifolds, and (iii) to solve\nequations for shape deformations such as curvature driven flows. We also\nrelease a package of the codes and weights for using our pre-trained GNPs for\nprocessing point cloud representations. This allows for incorporating our\npre-trained GNPs as components for reuse within existing and new data\nprocessing pipelines. The GNPs also can be used as part of numerical solvers\ninvolving geometry or as part of methods for performing inference and other\ngeometric tasks.",
        "This contribution will delve into the design and performance of the newly\nproduced Silicon Carbide Low Gain Avalanche Detectors (4H-SiC LGADs) and\nprovide a comprehensive summary of their measured characteristics. This\nincludes an analysis of the detector's performance, temperature stability, and\nthe effectiveness of the internal gain layer in improving signal generation.\n  The 4H-SiC is re-emerging as a strong candidate for the next generation of\nsemiconductor detectors. This material offers several advantages, including\nhigh radiation tolerance and the ability to operate over a wide range of\ntemperatures without significant annealing effects. However, the signals\ngenerated by minimum ionizing particles in the 4H-SiC detector are lower\ncompared to the signal produced by standard silicon detectors due to their\nhigher bandgap energy. This is addressed by implementing a charge\nmultiplication layer, which results in the intrinsic gain of the device.\n  The presented 4H-SiC LGADs produced by onsemi are specifically designed and\noptimized for fabrication on the n-type substrate\/epi wafer with the gain layer\nimplanted approximately $1~\\mathrm{\\mu m}$ below the surface. The first\niteration of these LGAD structures was manufactured in early 2024 and since\nthen has been subjected to laboratory evaluation. The measured properties of\nthese detectors align well with the predictions arising from the extensive TCAD\nsimulation studies.",
        "We investigate a class of quadratic backward stochastic differential\nequations (BSDEs) with generators singular in $ y $. First, we establish the\nexistence of solutions and a comparison theorem, thereby extending results in\nthe literature. Additionally, we analyze the stability property and the\nFeynman-Kac formula, and prove the uniqueness of viscosity solutions for the\ncorresponding singular semilinear partial differential equations (PDEs).\nFinally, we demonstrate applications in the context of robust control linked to\nstochastic differential utility and certainty equivalent based on\n$g$-expectation. In these applications, the coefficient of the quadratic term\nin the generator captures the level of ambiguity aversion and the coefficient\nof absolute risk aversion, respectively.",
        "In this paper, we propose to connect Prawitz's theory of grounds with\nGirard's Ludics. This connection is carried out on two levels. On a more\nphilosophical one, we highlight some differences between Prawitz's and Girard's\napproaches, but we also argue that they share some basic ideas about proofs and\ndeduction. On a more formal one, we sketch an indicative translation of\nPrawitz's theory grounds into Girard's Ludics relative to the implicational\nfragment of propositional intuitionistic logic. This may allow for a dialogical\nreading of Prawitz's ground-theoretic approach. Moreover, it becomes possible\nto provide a formal definition of a notion of ground-candidate introduced by\nCozzo.",
        "When $u$ is close to a single Talenti bubble $v$ of the $p$-Sobolev\ninequality, we show that\n  \\begin{equation*}\n  \\|Du-Dv\\|_{L^p(\\mathbb{R}^n)}^{\\max\\{1,p-1\\}}\\le C \\|-{\\rm\ndiv}(|Du|^{p-2}Du)-|u|^{p^*-2}u\\|_{W^{-1,q}(\\mathbb{R}^n)}, \\end{equation*}\nwhere $C=C(n,p)>0$. This estimate provides a sharp stability estimate for the\nStruwe-type decomposition in the single bubble case, generalizing the result of\nCiraolo, Figalli, and Maggi \\cite{CFM2018} (focusing on the case $p=2$) to the\narbitrary $p$. Also, in the Sobolev setting, this answers an open problem\nraised by Zhou and Zou in \\cite[Remark 1.17]{ZZ2023}.",
        "This paper concerns the rigidity from infinity for Alfv\\'en waves governed by\nideal incompressible magnetohydrodynamic equations subjected to strong\nbackground magnetic fields along the $x_1$-axis in 3D thin domains\n$\\Omega_\\delta=\\mathbb{R}^2\\times(-\\delta,\\delta)$ with $\\delta\\in(0,1]$ and\nslip boundary conditions. We show that in any thin domain $\\Omega_\\delta$,\nAlfv\\'en waves must vanish identically if their scattering fields vanish at\ninfinities. As an application, the rigidity of Alfv\\'en waves in\n$\\Omega_{\\delta}$, propagating along the horizontal direction, can be\napproximated by the rigidity of Alfv\\'en waves in $\\mathbb{R}^2$ when $\\delta$\nis sufficiently small. Our proof relies on the uniform (with respect to\n$\\delta$) weighted energy estimates with a position parameter in weights to\ntrack the center of Alfv\\'en waves. The key issues in the analysis include\ndealing with the nonlinear nature of Alfv\\'en waves and the geometry of thin\ndomains.",
        "We establish a new and simple criterion that suffices to generate many\nspectral gaps for periodic word models. This leads to new examples of ergodic\nSchr\\\"odinger operators with Cantor spectra having zero Hausdorff dimension\nthat simultaneously may have arbitrarily small supremum norm together with\narbitrarily long runs on which the potential vanishes.",
        "We present the first photometric population study of double-peaked Type IIb\nsupernovae (SNe IIb). SNe IIb are produced from the core-collapse of massive\nstars whose outermost Hydrogen layer has been partially stripped prior to\nexplosion. These double-peaked light curves, consisting of a shock-cooling\nemission peak (SCE) followed by the main nickel-powered peak, contain more\ncrucial information about the progenitor system than the typical single-peaked\nlight curves. We compiled and analyzed a sample of 14 spectroscopically\nconfirmed SNe IIb -- including previously unpublished and re-classified -- with\npublicly available photometric observations, discovered between 2018--2022,\nfrom the ZTF and ATLAS surveys. We developed and fit a piecewise linear model,\nreferred to as the ``lightning bolt model,'' to describe the early-time\nbehavior of these objects to measure population statistics. Notably, we find\nthe SCE peak lasts, on average, fewer than five days above half-maximum light\nwith a mean rise time of $2.07\\pm1.0$ and $1.1\\pm0.8$ mags\/day in the g- and\nr-band respectively. These SCE rise rates are over 10x faster than -- and last\nonly a third the duration of -- the rise to the nickel-powered peak. These rise\ntimes are comparable to those of fast blue optical transient (FBOT) events and\nwe discuss the implications in the text. Finally, we present a proof-of-concept\nalert filter, using the ANTARES broker, to demonstrate how to translate these\npopulation statistics into simple and effective filters to find potential\ndouble-peaked SNe IIb in large-scale survey alert streams, like the imminent\nVera C. Rubin Observatory Legacy Survey of Space and Time (Rubin LSST).",
        "In overparameterized logistic regression, gradient descent (GD) iterates\ndiverge in norm while converging in direction to the maximum $\\ell_2$-margin\nsolution -- a phenomenon known as the implicit bias of GD. This work\ninvestigates additional regularization effects induced by early stopping in\nwell-specified high-dimensional logistic regression. We first demonstrate that\nthe excess logistic risk vanishes for early-stopped GD but diverges to infinity\nfor GD iterates at convergence. This suggests that early-stopped GD is\nwell-calibrated, whereas asymptotic GD is statistically inconsistent. Second,\nwe show that to attain a small excess zero-one risk, polynomially many samples\nare sufficient for early-stopped GD, while exponentially many samples are\nnecessary for any interpolating estimator, including asymptotic GD. This\nseparation underscores the statistical benefits of early stopping in the\noverparameterized regime. Finally, we establish nonasymptotic bounds on the\nnorm and angular differences between early-stopped GD and $\\ell_2$-regularized\nempirical risk minimizer, thereby connecting the implicit regularization of GD\nwith explicit $\\ell_2$-regularization.",
        "In this work, we present a simple and low-cost experiment designed to study\nthe oscillations of the magnetic field created by a cylindrical magnet under\ntwo different conditions: far and short distances from the magnetic sensor. A\nTaylor series expansion of the magnetic field function has been done to study\nthe convergence of the polynomial series to the real field in both situations.\nTo carry out the experiment, a small cylindrical magnet has been attached to an\noscillating and well-known spring-mass system. The resulting oscillating\nmagnetic field has been registered with the smartphone by using the\nmagnetometer sensor. A very good agreement has been obtained between the\ntheoretical model for the magnetic field and the experimental data collected\nwith the sensor located near and far from a cylindrical magnet and along its\nlongitudinal axis.",
        "We review exceptional field theories as the duality-covariant reformulation\nof maximal supergravity theories in ten and eleven dimensions, that make the\nunderlying exceptional symmetries explicit. Beyond their structural role in\nunifying the various maximal supergravities, we illustrate how they also\nprovide access to very efficient techniques for tackling concrete computational\nproblems in supergravity.",
        "In this paper, we first give a critical BKM-type blow-up criterion that only\ninvolves the horizontal swirl component of the velocity for the inviscid\naxially symmetric MHD-Boussinesq system. Moreover, we consider the inviscid\nlimit of the viscous MHD-Boussinesq system, and the convergence rate for the\nviscosity coefficient tending to zero is obtained.",
        "Sophomore's dream sum $S=\\sum_{n=1}^\\infty n^{-n}$ is extended to the\nfunction $f(t,a)=t\\int_{0}^{1}(ax)^{-tx}dx$ with $f(1,1)=S$. Asymptotic\nbehavior for a large $|t|$ is obtained, which is exponential for $t>0$ and\n$t<0,a>1$, and inverse-logarithmic for $t<0,a<1$. An advanced approximation\nincludes a half-derivative of the exponent and is expressed in terms of the\nerror function. This approach provides excellent interpolation description in\nthe complex plane. The function $f(t,a)$ demonstrates for $a>1$ oscillating\nbehavior along the imaginary axis with slowly increasing amplitude and the\nperiod of $2\\pi iea$, modulation by high-frequency oscillations being present.\nAlso, $f(t,a)$ has non-trivial zeros in the left complex half-plane with Im$t_n\n\\simeq 2(n-1\/8)\\pi e\/a$ for $a>1$. The results obtained describe analytical\nintegration of the function $x^{tx}$.",
        "Electro-optics, the tuning of optical properties of materials with electric\nfields, is key to a multitude of quantum and classical photonics applications.\nHowever, a major obstacle preventing many emerging use cases is inefficient\nmodulation in cryogenic environments, as traditional tuning mechanisms degrade\nat low temperatures. Guided by the connection between phase transitions and\nnonlinearity, we identify the quantum paraelectric perovskite SrTiO$_3$ (STO)\nas the strongest cryogenic electro-optic photonic material. As a result of the\nunique quantum paraelectric phase of STO, we demonstrate a dynamically tunable\nlinear Pockels coefficient ($r_{33}$) exceeding 500 pm\/V at $T=5$ K, and study\nits full temperature and bias dependence. We also measure an enhanced\npiezo-electric coefficient ($d_{33}$) above 90 pC\/N. Both of these coefficients\nexceed all previously reported values for cryogenic materials, including\nlithium niobate ($r_{33}\\approx24$ pm\/V) and barium titanate\n($r_{42}\\approx170$ pm\/V). Furthermore, by tuning STO towards \\textit{quantum\ncriticality} with oxygen isotope substitution we more than double the optical\nand piezo-electric nonlinearities, demonstrating a linear Pockels coefficient\nabove 1100 pm\/V. Our results probe the link between quantum phase transitions,\ndielectric susceptibility, and optical nonlinearities, unlocking opportunities\nin cryogenic optical and mechanical systems, and provide a framework for\ndiscovering new nonlinear materials.",
        "We present an experimental method for the characterization of the kinetic\nenergies of ions confined in a 22-pole radio frequency trap by inducing a small\npotential barrier using the surrounding ring electrodes, allowing the selective\nextraction of ions. Energy sampling experiments have been performed on buffer\ngas thermalized He$^+$ ions at trap temperatures between 10-180 K, resulting in\ndistinct extraction curves as a function of the potential barrier, and a\ndifferentiated behavior depending on the escape time from the trap. The\nexperiments are complemented by Monte Carlo simulations of the ion trajectories\ninside the calculated trap potential and allow us to investigate the properties\nof the sampling method, the role of ion motion coupling, and the impact of\nresidual buffer gas collisions on the observed results. The technique has also\nbeen successfully applied to identify energetic H$_3^+$ ions produced in an\nexothermic reaction inside the trap. Upon calibration, this method can provide\nrelative kinetic energy distributions or be used to filter the maximum desired\nkinetic energy of the ions inside the trap.",
        "We study Kitaev's quantum double model for arbitrary finite gauge group in\ninfinite volume, using an operator-algebraic approach. The quantum double model\nhosts anyonic excitations which can be identified with equivalence classes of\n`localized and transportable endomorphisms', which produce anyonic excitations\nfrom the ground state. Following the Doplicher--Haag--Roberts (DHR) sector\ntheory from AQFT, we organize these endomorphisms into a braided monoidal\ncategory capturing the fusion and braiding properties of the anyons. We show\nthat this category is equivalent to $\\mathrm{Rep}_f \\mathcal{D}(G)$, the\nrepresentation category of the quantum double of $G$. This establishes for the\nfirst time the full DHR structure for a class of 2d quantum lattice models with\nnon-abelian anyons.",
        "In this paper, we present an averaging method for obtaining quasi-periodic\nresponse solutions in perturbed real analytic quasi-periodic systems with\nDiophantine frequency vectors. Assuming that the averaged system possesses a\nnon-degenerate equilibrium and the eigenvalues of the linearized matrix are\npairwise distinct, we show that the original system admits a quasi-periodic\nresponse solution for the parameter belonging to a Cantorian set. The proof is\nbased on the KAM techniques, and this averaging method can be extended to the\nsecond-order systems. It is worth mentioning that our results do not require\nthe equilibrium point to be hyperbolic, which means that the eigenvalues of the\nlinearized matrix of the averaging system may be purely imaginary.",
        "Aluminum-copper-magnesium-sliver (Al-Cu-Mg-Ag) alloys are extensively\nutilized in aerospace industries due to the formation of Omega\nnano-plates.However, the rapid coarsening of these nano-plates above 475 K\nrestricts their application at elevated temperatures.When introducing scandium\n(Sc) to these alloys, the service temperature of the resultant alloys can reach\nan unprecedented 675 K, attributed to the in situ formation of a\ncoarsening-resistant V-Sc(Al2Cu)4 phase within the Omega nano-plates. However,\nthe fundamental thermodynamic properties and mechanisms behind the remarkable\ncoarsening resistance of V nano-plates remain unexplored.Here, we employ\nfirst-principles calculations to investigate the phase stability of\nV-Sc(Al2Cu)4 phase, the basic kinetic features of V phase formation within\nOmega nano-plates, and the origins of the extremely high thermal stability of V\nnano-plates. Our results indicate that V-Sc(Al2Cu)4 is meta-stable and\nthermodynamically tends to evolve into a stable ScAl7Cu5 phase. We also\ndemonstrate that kinetic factors are mainly responsible for the temperature\ndependence of V phase formation. Notably, the formation of V-Sc(Al2Cu)4 within\nOmega nano-plates modifies the Kagome lattice in the shell layer of the Omega\nnano-plates, inhibiting further thickening of V nano-plates through the\nthickening pathway of Omega nano-plates. This interface transition leads to the\nexceptional coarsening resistance of the V nano-plates. Moreover, we also\nscreened 14 promising element substitutions for Sc. These findings are\nanticipated to accelerate the development of high-performance Al alloys with\nsuperior heat resistance.",
        "Counterdiabatic driving, which enforces adiabatic evolution in arbitrary\ntimescales, can be realised by engineering a Floquet Hamiltonian which\noscillates between the Hamiltonian and its derivative requiring no additional\ncontrol terms. However, the coefficients of the Floquet Hamiltoinan require\nknowledge of the counterdiabatic terms, which can be difficult to derive\noutside of a limited set of examples. We introduce a new hybrid technique for\nthe control of quantum systems, Counterdiabatic-influenced Floquet-engineering\nor CAFFEINE for short. CAFFEINE parameterises the Floquet Hamiltonian for\ncounterdiabatic driving and utilises numerical quantum optimal control in order\nto obtain the desired target state. This removes the need to both obtain and\nimplement counterdiabatic terms, however, it does require the ability to\nquickly oscillate each term in the Hamiltonian. If this oscillation is\npossible, then CAFFEINE provides a framework to implement quantum annealing\nprotocols and general quantum state preparation. We illustrate this through\noptimisation of two numerical examples of preparing a Bell state with two\nqubits and performing annealing protocols for the one-dimensional Ising model.\nBeyond this, we also illustrate CAFFEINE's capabilities to learn the\ncounterdiabatic terms, which can potentially be used as a probe of quantum\nchaos and the geometry of quantum dynamics."
      ]
    }
  },
  {
    "id":2411.06785,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"RNA-Seq: a revolutionary tool for transcriptomics",
    "start_abstract":"RNA-Seq is a recently developed approach to transcriptome profiling that uses deep-sequencing technologies. Studies using this method have already altered our view of the extent and complexity of eukaryotic transcriptomes. RNA-Seq also provides a far more precise measurement of levels of transcripts and their isoforms than other methods. This article describes the RNA-Seq approach, the challenges associated with its application, and the advances made so far in characterizing several eukaryote transcriptomes.",
    "start_categories":[
      "q-bio.GN"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b25"
      ],
      "title":[
        "White-Box Transformers via Sparse Rate Reduction"
      ],
      "abstract":[
        "In this paper, we contend that the objective of representation learning is to compress and transform distribution data, say sets tokens, towards a mixture low-dimensional Gaussian distributions supported on incoherent subspaces. The quality final can be measured by unified function called sparse rate reduction. From perspective, popular deep networks such as transformers naturally viewed realizing iterative schemes optimize incrementally. Particularly, show standard transformer block derived from alternating optimization complementary parts objective: multi-head self-attention operator gradient descent step token minimizing their lossy coding rate, subsequent multi-layer perceptron attempting sparsify tokens. This leads family white-box transformer-like network architectures which are mathematically fully interpretable. Despite simplicity, experiments these indeed learn designed they representations large-scale real-world vision datasets ImageNet, achieve performance very close thoroughly engineered ViT."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "On the Fly Adaptation of Behavior Tree-Based Policies through\n  Reinforcement Learning",
        "A novel inversion algorithm for weak gravitational lensing using\n  quasi-conformal geometry",
        "Synthetic $\\pi$-flux system in 2D superconducting qubit array with\n  tunable coupling",
        "A Note on Exact State Visit Probabilities in Two-State Markov Chains",
        "Erd\\H{o}s's integer dilation approximation problem and GCD graphs",
        "Cracking Vector Search Indexes",
        "Computing Game Symmetries and Equilibria That Respect Them",
        "Revealing quantum operator scrambling via measuring Holevo information\n  on digital quantum simulators",
        "It Helps to Take a Second Opinion: Teaching Smaller LLMs to Deliberate\n  Mutually via Selective Rationale Optimisation",
        "A Geometric Perspective for High-Dimensional Multiplex Graphs",
        "Employee Turnover Prediction: A Cross-component Attention Transformer\n  with Consideration of Competitor Influence and Contagious Effect",
        "Towards High-performance Spiking Transformers from ANN to SNN Conversion",
        "Cohort-attention Evaluation Metric against Tied Data: Studying\n  Performance of Classification Models in Cancer Detection",
        "Stability, periodic orbits and KAM tori in the dynamics of the three\n  fixed centers problem",
        "The Unbearable Lightness of Prompting: A Critical Reflection on the\n  Environmental Impact of genAI use in Design Education",
        "Simultaneous extension of generalized BT-inverses and core-EP inverses",
        "Maximal Magic for Two-qubit States",
        "Electronic and optical excitations of K-Sb and Na-Sb crystals",
        "Bridging Simulation and Reality: A 3D Clustering-Based Deep Learning\n  Model for UAV-Based RF Source Localization",
        "Challenges and Trends in Egocentric Vision: A Survey",
        "Efficient Over-parameterized Matrix Sensing from Noisy Measurements via\n  Alternating Preconditioned Gradient Descent",
        "Towards Interactive Deepfake Analysis",
        "Low-temperature magnetic behaviour on the triangular lattice in\n  hexagonal Ba$_3$Tb(BO$_3$)$_3$",
        "Crystal tensor properties of magnetic materials with and without\n  spin-orbit coupling. Application of spin point groups as approximate\n  symmetries",
        "FeatureGS: Eigenvalue-Feature Optimization in 3D Gaussian Splatting for\n  Geometrically Accurate and Artifact-Reduced Reconstruction",
        "Intervals in Dyck paths and the wreath conjecture",
        "Bandit Optimal Transport",
        "LongSafety: Evaluating Long-Context Safety of Large Language Models",
        "Monitoring Reasoning Models for Misbehavior and the Risks of Promoting\n  Obfuscation"
      ],
      "abstract":[
        "With the rising demand for flexible manufacturing, robots are increasingly\nexpected to operate in dynamic environments where local -- such as slight\noffsets or size differences in workpieces -- are common. We propose to address\nthe problem of adapting robot behaviors to these task variations with a\nsample-efficient hierarchical reinforcement learning approach adapting Behavior\nTree (BT)-based policies. We maintain the core BT properties as an\ninterpretable, modular framework for structuring reactive behaviors, but extend\ntheir use beyond static tasks by inherently accommodating local task\nvariations. To show the efficiency and effectiveness of our approach, we\nconduct experiments both in simulation and on a Franka Emika Panda 7-DoF, with\nthe manipulator adapting to different obstacle avoidance and pivoting tasks.",
        "One of the challenges in weak gravitational lensing by galaxies and clusters\nis to infer the projected mass density distribution from gravitational lensing\nmeasurements, which is known as inversion problem. We introduce a novel\ntheoretical approach to solve the inversion problem. The cornerstone of the\nproposed method lies in a complex formalism that describes the lens mapping as\nquasi-conformal mapping with the Beltrami coefficient given by the negative of\nthe reduced shear, which is, in principle, observable from the image\nellipticities. We propose an algorithm called QCLens that is based on this\ncomplex formalism. QCLens computes the underlying quasi-conformal mapping with\na finite element approach by reducing the problem to two elliptic partial\ndifferential equations solely depending on the reduced shear field.\nExperimental results for both the Schwarzschild and singular isothermal lens\ndemonstrate the agreement of our proposed method with the analytically\ncomputable solutions.",
        "Flat-band systems provide an ideal platform for exploring exotic quantum\nphenomena, where the strongly suppressed kinetic energy in these flat energy\nbands suggests the potential for exotic phases driven by geometric structure,\ndisorder, and interactions. While intriguing phenomena and physical mechanisms\nhave been unveiled in theoretical models, synthesizing such systems within\nscalable quantum platforms remains challenging. Here, we present the\nexperimental realization of a $\\pi$-flux rhombic system using a two-dimensional\nsuperconducting qubit array with tunable coupling. We experimentally observe\ncharacteristic dynamics, e.g., $\\pi$-flux driven destructive interference, and\ndemonstrate the protocol for eigenstate preparation in this rhombic array with\ncoupler-assisted flux. Our results provide future possibilities for exploring\nthe interplay of geometry, interactions, and quantum information encoding in\nsuch degenerate systems.",
        "In this note we derive the exact probability that a specific state in a\ntwo-state Markov chain is visited exactly $k$ times after $N$ transitions. We\nprovide a closed-form solution for $\\mathbb{P}(N_l = k \\mid N)$, considering\ninitial state probabilities and transition dynamics. The solution corrects and\nextends prior incomplete results, offering a rigorous framework for enumerating\nstate transitions. Numerical simulations validate the derived expressions,\ndemonstrating their applicability in stochastic modeling.",
        "Let $\\mathcal{A}\\subset\\mathbb{R}_{\\geqslant1}$ be a countable set such that\n$\\limsup_{x\\to\\infty}\\frac{1}{\\log\nx}\\sum_{\\alpha\\in\\mathcal{A}\\cap[1,x]}\\frac{1}{\\alpha}>0$. We prove that, for\nevery $\\varepsilon>0$, there exist infinitely many pairs $(\\alpha, \\beta)\\in\n\\mathcal{A}^2$ such that $\\alpha\\neq \\beta$ and $|n\\alpha-\\beta| <\\varepsilon$\nfor some positive integer $n$. This resolves a problem of Erd\\H{o}s from 1948.\nA critical role in the proof is played by the machinery of GCD graphs, which\nwere introduced by the first author and by James Maynard in their work on the\nDuffin--Schaeffer conjecture in Diophantine approximation.",
        "Retrieval Augmented Generation (RAG) uses vector databases to expand the\nexpertise of an LLM model without having to retrain it. This idea can be\napplied over data lakes, leading to the notion of embeddings data lakes, i.e.,\na pool of vector databases ready to be used by RAGs. The key component in these\nsystems is the indexes enabling Approximated Nearest Neighbor Search (ANNS).\nHowever, in data lakes, one cannot realistically expect to build indexes for\nevery possible dataset. In this paper, we propose an adaptive, partition-based\nindex, CrackIVF, that performs much better than up-front index building.\nCrackIVF starts answering queries by near brute force search and only expands\nas it sees enough queries. It does so by progressively adapting the index to\nthe query workload. That way, queries can be answered right away without having\nto build a full index first. After seeing enough queries, CrackIVF will produce\nan index comparable to the best of those built using conventional techniques.\nAs the experimental evaluation shows, CrackIVF can often answer more than 1\nmillion queries before other approaches have even built the index and can start\nanswering queries immediately, achieving 10-1000x faster initialization times.\nThis makes it ideal when working with cold data or infrequently used data or as\na way to bootstrap access to unseen datasets.",
        "Strategic interactions can be represented more concisely, and analyzed and\nsolved more efficiently, if we are aware of the symmetries within the\nmultiagent system. Symmetries also have conceptual implications, for example\nfor equilibrium selection. We study the computational complexity of identifying\nand using symmetries. Using the classical framework of normal-form games, we\nconsider game symmetries that can be across some or all players and\/or actions.\nWe find a strong connection between game symmetries and graph automorphisms,\nyielding graph automorphism and graph isomorphism completeness results for\ncharacterizing the symmetries present in a game. On the other hand, we also\nshow that the problem becomes polynomial-time solvable when we restrict the\nconsideration of actions in one of two ways.\n  Next, we investigate when exactly game symmetries can be successfully\nleveraged for Nash equilibrium computation. We show that finding a Nash\nequilibrium that respects a given set of symmetries is PPAD- and CLS-complete\nin general-sum and team games respectively -- that is, exactly as hard as\nBrouwer fixed point and gradient descent problems. Finally, we present\npolynomial-time methods for the special cases where we are aware of a vast\nnumber of symmetries, or where the game is two-player zero-sum and we do not\neven know the symmetries.",
        "Quantum operator scrambling describes the spreading of local operators into\nthe whole system in the picture of Heisenberg evolution, which is often\nquantified by the operator size growth. Here we propose a measure of quantum\noperator scrambling via Holevo information of operators, by taking its capacity\nto distinguish operator information locally. We show that the operator size is\nclosely related to a special kind of Holevo information of operators. Moreover,\nwe propose a feasible protocol for measuring Holevo information of operators on\ndigital quantum simulators based on random states. Our numerical simulations\nshow that the integrable system can be told apart from the chaotic system by\nmeasuring the spatial-temporal patterns of Holevo information. Furthermore, we\nfind that error mitigation is required to restore the time-oscillation behavior\nof Holevo information for the integrable system, a crucial feature distinct\nfrom the chaotic one. Our work provides a new perspective to understand the\ninformation scrambling and quantum chaos from aspects of Holevo information of\noperators.",
        "Very large language models (LLMs) such as GPT-4 have shown the ability to\nhandle complex tasks by generating and self-refining step-by-step rationales.\nSmaller language models (SLMs), typically with < 13B parameters, have been\nimproved by using the data generated from very-large LMs through knowledge\ndistillation. However, various practical constraints such as API costs,\ncopyright, legal and ethical policies restrict using large (often opaque)\nmodels to train smaller models for commercial use. Limited success has been\nachieved at improving the ability of an SLM to explore the space of possible\nrationales and evaluate them by itself through self-deliberation. To address\nthis, we propose COALITION, a trainable framework that facilitates interaction\nbetween two variants of the same SLM and trains them to generate and refine\nrationales optimized for the end-task. The variants exhibit different behaviors\nto produce a set of diverse candidate rationales during the generation and\nrefinement steps. The model is then trained via Selective Rationale\nOptimization (SRO) to prefer generating rationale candidates that maximize the\nlikelihood of producing the ground-truth answer. During inference, COALITION\nemploys a controller to select the suitable variant for generating and refining\nthe rationales. On five different datasets covering mathematical problems,\ncommonsense reasoning, and natural language inference, COALITION outperforms\nseveral baselines by up to 5%. Our ablation studies reveal that\ncross-communication between the two variants performs better than using the\nsingle model to self-refine the rationales. We also demonstrate the\napplicability of COALITION for LMs of varying scales (4B to 14B parameters) and\nmodel families (Mistral, Llama, Qwen, Phi). We release the code for this work\nat https:\/\/github.com\/Sohanpatnaik106\/coalition.",
        "High-dimensional multiplex graphs are characterized by their high number of\ncomplementary and divergent dimensions. The existence of multiple hierarchical\nlatent relations between the graph dimensions poses significant challenges to\nembedding methods. In particular, the geometric distortions that might occur in\nthe representational space have been overlooked in the literature. This work\nstudies the problem of high-dimensional multiplex graph embedding from a\ngeometric perspective. We find that the node representations reside on highly\ncurved manifolds, thus rendering their exploitation more challenging for\ndownstream tasks. Moreover, our study reveals that increasing the number of\ngraph dimensions can cause further distortions to the highly curved manifolds.\nTo address this problem, we propose a novel multiplex graph embedding method\nthat harnesses hierarchical dimension embedding and Hyperbolic Graph Neural\nNetworks. The proposed approach hierarchically extracts hyperbolic node\nrepresentations that reside on Riemannian manifolds while gradually learning\nfewer and more expressive latent dimensions of the multiplex graph.\nExperimental results on real-world high-dimensional multiplex graphs show that\nthe synergy between hierarchical and hyperbolic embeddings incurs much fewer\ngeometric distortions and brings notable improvements over state-of-the-art\napproaches on downstream tasks.",
        "Employee turnover refers to an individual's termination of employment from\nthe current organization. It is one of the most persistent challenges for\nfirms, especially those ones in Information Technology (IT) industry that\nconfront high turnover rates. Effective prediction of potential employee\nturnovers benefits multiple stakeholders such as firms and online recruiters.\nPrior studies have focused on either the turnover prediction within a single\nfirm or the aggregated employee movement among firms. How to predict the\nindividual employees' turnovers among multiple firms has gained little\nattention in literature, and thus remains a great research challenge. In this\nstudy, we propose a novel deep learning approach based on job embeddedness\ntheory to predict the turnovers of individual employees across different firms.\nThrough extensive experimental evaluations using a real-world dataset, our\ndeveloped method demonstrates superior performance over several\nstate-of-the-art benchmark methods. Additionally, we estimate the cost saving\nfor recruiters by using our turnover prediction solution and interpret the\nattributions of various driving factors to employee's turnover to showcase its\npractical business value.",
        "Spiking neural networks (SNNs) show great potential due to their energy\nefficiency, fast processing capabilities, and robustness. There are two main\napproaches to constructing SNNs. Direct training methods require much memory,\nwhile conversion methods offer a simpler and more efficient option. However,\ncurrent conversion methods mainly focus on converting convolutional neural\nnetworks (CNNs) to SNNs. Converting Transformers to SNN is challenging because\nof the presence of non-linear modules. In this paper, we propose an Expectation\nCompensation Module to preserve the accuracy of the conversion. The core idea\nis to use information from the previous T time-steps to calculate the expected\noutput at time-step T. We also propose a Multi-Threshold Neuron and the\ncorresponding Parallel Parameter normalization to address the challenge of\nlarge time steps needed for high accuracy, aiming to reduce network latency and\npower consumption. Our experimental results demonstrate that our approach\nachieves state-of-the-art performance. For example, we achieve a top-1 accuracy\nof 88.60\\% with only a 1\\% loss in accuracy using 4 time steps while consuming\nonly 35\\% of the original power of the Transformer. To our knowledge, this is\nthe first successful Artificial Neural Network (ANN) to SNN conversion for\nSpiking Transformers that achieves high accuracy, low latency, and low power\nconsumption on complex datasets. The source codes of the proposed method are\navailable at https:\/\/github.com\/h-z-h-cell\/Transformer-to-SNN-ECMT.",
        "Artificial intelligence (AI) has significantly improved medical screening\naccuracy, particularly in cancer detection and risk assessment. However,\ntraditional classification metrics often fail to account for imbalanced data,\nvarying performance across cohorts, and patient-level inconsistencies, leading\nto biased evaluations. We propose the Cohort-Attention Evaluation Metrics (CAT)\nframework to address these challenges. CAT introduces patient-level assessment,\nentropy-based distribution weighting, and cohort-weighted sensitivity and\nspecificity. Key metrics like CATSensitivity (CATSen), CATSpecificity (CATSpe),\nand CATMean ensure balanced and fair evaluation across diverse populations.\nThis approach enhances predictive reliability, fairness, and interpretability,\nproviding a robust evaluation method for AI-driven medical screening models.",
        "We investigate the motion in space of an infinitesimal particle in the\ngravitational field generated by three primary bodies positioned at the\nvertices of a fixed equilateral triangle. We assume that the distances between\nthe primaries are small compared to their separation from the particle. By\napplying a Lie-Deprit normalization, we simplify the Hamiltonian, relegating\nboth the mean anomaly and the argument of periapisis to third-order terms or\nhigher. After reducing out the symmetries associated with the Kepler flow and\nthe central action of the angular momentum, we examine the relative equilibria\nin the first and second reduced spaces. We are able to identify the conditions\nfor the existence of circular periodic orbits and KAM tori, thus providing\ninsight into the system's long-term stability and dynamic structure.",
        "Design educators are finding ways to support students in skillfully using\nGenAI tools in their practices while encouraging the critical scrutiny of the\nethical and social issues around these technologies. However, the issue of\nenvironmental sustainability remains unaddressed. There is a lack of both\nresources to grasp the environmental costs of genAI in education and a lack of\nshared practices for engaging with the issue. This paper critically reflects on\nthe energy costs of using genAI in design education, using a workshop held in\n2023 with 49 students as a motivating example. Through this reflection, we\ndevelop a set of five alternative stances, with related actions, that support\nthe conscious use of genAI in design education. The work contributes to the\nfield of design and HCI by bringing together ways for educators to reflect on\ntheir practices, informing the future development of educational programs\naround genAI.",
        "In this paper we introduce the generalized inverse of complex square matrix\nwith respect to other matrix having same size. Some of its representations,\nproperties and characterizations are obtained. Also some new representation\nmatrices of W-weighted BT-inverse and W-weighted core-EP inverse are determined\nas well as characterizations of generalized inverses A A^\\odagger,\nA^{odagger,W}, A^\\diamond, A^{\\diamond,W}.",
        "Magic is a quantum resource essential for universal quantum computation and\nrepresents the deviation of quantum states from those that can be simulated\nefficiently using classical algorithms. Using the Stabilizer R\\'enyi Entropy\n(SRE), we investigate two-qubit states with maximal magic, which are most\ndistinct from classical simulability, and provide strong numerical evidence\nthat the maximal second order SRE is $\\log (16\/7)\\approx 0.827$, establishing a\ntighter bound than the prior $\\log(5\/2)\\approx 0.916$. We identity 480 states\nsaturating the new bound, which turn out to be the fiducial states for the\nmutually unbiased bases (MUBs) generated by the orbits of the Weyl-Heisenberg\n(WH) group, and conjecture that WH-MUBs are the maximal magic states for\n$n$-qubit, when $n\\neq 1$ and 3. We also reveal a striking interplay between\nmagic and entanglement: the entanglement of maximal magic states is restricted\nto two possible values, $1\/2$ and $1\/\\sqrt{2}$, as quantified by the\nconcurrence; none is maximally entangled.",
        "Recent advances in experimental techniques and computational methods have\nsignificantly expanded the family of alkali antimonides, a class of\nsemiconducting materials used as photocathodes in particle accelerators,\nunveiling new crystal structures and stoichiometries with improved stability\nand quantum efficiency. This work investigates the electronic and optical\nproperties of eight Na- and K-based alkali antimonide binary crystals with 3:1\nand 1:1 alkali-to-antimony ratios, which were predicted to be stable in a\nrecent high-throughput screening study. Employing density functional theory and\nmany-body perturbation theory, we find that all systems exhibit direct band\ngaps, except for monoclinic Na$_8$Sb$_8$, which has a nearly degenerate\nindirect gap. Optical spectra are characterized by near-infrared absorption\nonsets and intense visible excitations. Our analysis highlights the significant\nrole of electron-hole correlations, particularly in K-based compounds, leading\nto exciton binding energies above 100~meV and sharper absorption peaks. An\nin-depth analysis of the electronic contributions to the excited states\nprovides additional insight into the role of excitonic effects. By shedding\nlight on the fundamental properties of alkali antimonide binary crystals, our\nresults are relevant for the design and optimization of next-generation\nelectron sources for particle accelerators.",
        "Localization of radio frequency (RF) sources has critical applications,\nincluding search and rescue, jammer detection, and monitoring of hostile\nactivities. Unmanned aerial vehicles (UAVs) offer significant advantages for RF\nsource localization (RFSL) over terrestrial methods, leveraging autonomous 3D\nnavigation and improved signal capture at higher altitudes. Recent advancements\nin deep learning (DL) have further enhanced localization accuracy, particularly\nfor outdoor scenarios. DL models often face challenges in real-world\nperformance, as they are typically trained on simulated datasets that fail to\nreplicate real-world conditions fully. To address this, we first propose the\nEnhanced Two-Ray propagation model, reducing the simulation-to-reality gap by\nimproving the accuracy of propagation environment modeling. For RFSL, we\npropose the 3D Cluster-Based RealAdaptRNet, a DL-based method leveraging 3D\nclustering-based feature extraction for robust localization. Experimental\nresults demonstrate that the proposed Enhanced Two-Ray model provides superior\naccuracy in simulating real-world propagation scenarios compared to\nconventional free-space and two-ray models. Notably, the 3D Cluster-Based\nRealAdaptRNet, trained entirely on simulated datasets, achieves exceptional\nperformance when validated in real-world environments using the AERPAW physical\ntestbed, with an average localization error of 18.2 m. The proposed approach is\ncomputationally efficient, utilizing 33.5 times fewer parameters, and\ndemonstrates strong generalization capabilities across diverse trajectories,\nmaking it highly suitable for real-world applications.",
        "With the rapid development of artificial intelligence technologies and\nwearable devices, egocentric vision understanding has emerged as a new and\nchallenging research direction, gradually attracting widespread attention from\nboth academia and industry. Egocentric vision captures visual and multimodal\ndata through cameras or sensors worn on the human body, offering a unique\nperspective that simulates human visual experiences. This paper provides a\ncomprehensive survey of the research on egocentric vision understanding,\nsystematically analyzing the components of egocentric scenes and categorizing\nthe tasks into four main areas: subject understanding, object understanding,\nenvironment understanding, and hybrid understanding. We explore in detail the\nsub-tasks within each category. We also summarize the main challenges and\ntrends currently existing in the field. Furthermore, this paper presents an\noverview of high-quality egocentric vision datasets, offering valuable\nresources for future research. By summarizing the latest advancements, we\nanticipate the broad applications of egocentric vision technologies in fields\nsuch as augmented reality, virtual reality, and embodied intelligence, and\npropose future research directions based on the latest developments in the\nfield.",
        "We consider the noisy matrix sensing problem in the over-parameterization\nsetting, where the estimated rank $r$ is larger than the true rank $r_\\star$.\nSpecifically, our main objective is to recover a matrix $ X_\\star \\in\n\\mathbb{R}^{n_1 \\times n_2} $ with rank $ r_\\star $ from noisy measurements\nusing an over-parameterized factorized form $ LR^\\top $, where $ L \\in\n\\mathbb{R}^{n_1 \\times r}, \\, R \\in \\mathbb{R}^{n_2 \\times r} $ and $\n\\min\\{n_1, n_2\\} \\ge r > r_\\star $, with the true rank $ r_\\star $ being\nunknown. Recently, preconditioning methods have been proposed to accelerate the\nconvergence of matrix sensing problem compared to vanilla gradient descent,\nincorporating preconditioning terms $ (L^\\top L + \\lambda I)^{-1} $ and $\n(R^\\top R + \\lambda I)^{-1} $ into the original gradient. However, these\nmethods require careful tuning of the damping parameter $\\lambda$ and are\nsensitive to initial points and step size. To address these limitations, we\npropose the alternating preconditioned gradient descent (APGD) algorithm, which\nalternately updates the two factor matrices, eliminating the need for the\ndamping parameter and enabling faster convergence with larger step sizes. We\ntheoretically prove that APGD achieves near-optimal error convergence at a\nlinear rate, starting from arbitrary random initializations. Through extensive\nexperiments, we validate our theoretical results and demonstrate that APGD\noutperforms other methods, achieving the fastest convergence rate. Notably,\nboth our theoretical analysis and experimental results illustrate that APGD\ndoes not rely on the initialization procedure, making it more practical and\nversatile.",
        "Existing deepfake analysis methods are primarily based on discriminative\nmodels, which significantly limit their application scenarios. This paper aims\nto explore interactive deepfake analysis by performing instruction tuning on\nmulti-modal large language models (MLLMs). This will face challenges such as\nthe lack of datasets and benchmarks, and low training efficiency. To address\nthese issues, we introduce (1) a GPT-assisted data construction process\nresulting in an instruction-following dataset called DFA-Instruct, (2) a\nbenchmark named DFA-Bench, designed to comprehensively evaluate the\ncapabilities of MLLMs in deepfake detection, deepfake classification, and\nartifact description, and (3) construct an interactive deepfake analysis system\ncalled DFA-GPT, as a strong baseline for the community, with the Low-Rank\nAdaptation (LoRA) module. The dataset and code will be made available at\nhttps:\/\/github.com\/lxq1000\/DFA-Instruct to facilitate further research.",
        "The hexagonal polymorph of Ba$_3$Tb(BO$_3$)$_3$ contains Tb$^{3+}$ ions on a\nquasi-2D triangular lattice, resulting in geometric magnetic frustration.\nPowder samples of Ba$_3$Tb(BO$_3$)$_3$ have been investigated using specific\nheat, powder neutron diffraction (PND), inelastic neutron scattering (INS) and\nmuon-spin relaxation spectroscopy ($\\mu$SR). No long-range magnetic ordering is\nobserved down to the lowest measured temperatures of 75 mK in PND and specific\nheat data and 1.5 K in the $\\mu$SR data. Modelling the INS spectrum using a\npoint charge model suggests that the ground state is a singlet with a low-lying\ndoublet on each of the two crystallographically independent Tb$^{3+}$ sites and\nthat both the Tb ions display weak XY single-ion anisotropy.",
        "Spin space groups, formed by operations where the rotation of the spins is\nindependent of the accompanying operation acting on the crystal structure, are\nappropriate groups to describe the symmetry of magnetic structures with null\nspin-orbit coupling. Their corresponding spin point groups are the symmetry\ngroups to be considered for deriving the symmetry constraints on the form of\nthe crystal tensor properties of such idealized structures. These groups can\nalso be taken as approximate symmetries (with some restrictions) of real\nmagnetic structures, where spin-orbit and magnetic anisotropy are however\npresent. Here we formalize the invariance transformation properties that must\nsatisfy the most important crystal tensors under a spin point group. This is\ndone using modified Jahn symbols, which generalize those applicable to ordinary\nmagnetic point groups [Gallego et al., Acta Cryst. (2019) A{\\bf 75}, 438-447].\nThe analysis includes not only equilibrium tensors, but also transport, optical\nand non-linear optical susceptibility tensors. The constraints imposed by spin\ncollinearity and coplanarity within the spin group formalism on a series of\nrepresentative tensors are discussed and compiled. As illustrative examples,\nthe defined tensor invariance equations have been applied to some known\nmagnetic structures, showing the differences of the symmetry-adapted form of\nsome relevant tensors, when considered under the constraints of its spin point\ngroup or its magnetic point group. This comparison, with the spin point group\nimplying additional constraints in the tensor form, may allow to distinguish\nthose magnetic-related properties that can be solely attributed to spin-orbit\ncoupling from those that are expected to be present even under negligible\nspin-orbit effects.",
        "3D Gaussian Splatting (3DGS) has emerged as a powerful approach for 3D scene\nreconstruction using 3D Gaussians. However, neither the centers nor surfaces of\nthe Gaussians are accurately aligned to the object surface, complicating their\ndirect use in point cloud and mesh reconstruction. Additionally, 3DGS typically\nproduces floater artifacts, increasing the number of Gaussians and storage\nrequirements. To address these issues, we present FeatureGS, which incorporates\nan additional geometric loss term based on an eigenvalue-derived 3D shape\nfeature into the optimization process of 3DGS. The goal is to improve geometric\naccuracy and enhance properties of planar surfaces with reduced structural\nentropy in local 3D neighborhoods.We present four alternative formulations for\nthe geometric loss term based on 'planarity' of Gaussians, as well as\n'planarity', 'omnivariance', and 'eigenentropy' of Gaussian neighborhoods. We\nprovide quantitative and qualitative evaluations on 15 scenes of the DTU\nbenchmark dataset focusing on following key aspects: Geometric accuracy and\nartifact-reduction, measured by the Chamfer distance, and memory efficiency,\nevaluated by the total number of Gaussians. Additionally, rendering quality is\nmonitored by Peak Signal-to-Noise Ratio. FeatureGS achieves a 30 % improvement\nin geometric accuracy, reduces the number of Gaussians by 90 %, and suppresses\nfloater artifacts, while maintaining comparable photometric rendering quality.\nThe geometric loss with 'planarity' from Gaussians provides the highest\ngeometric accuracy, while 'omnivariance' in Gaussian neighborhoods reduces\nfloater artifacts and number of Gaussians the most. This makes FeatureGS a\nstrong method for geometrically accurate, artifact-reduced and memory-efficient\n3D scene reconstruction, enabling the direct use of Gaussian centers for\ngeometric representation.",
        "Let $\\iota_{k}(m,l)$ denote the total number of intervals of length $m$\nacross all Dyck paths of semilength $k$ such that each interval contains\nprecisely $l$ falls. We give the formula for $\\iota_{k}(m,l)$ and show that\n$\\iota_{k}(k,l)=\\binom{k}{l}^2$. Motivated by this, we propose two stronger\nvariants of the wreath conjecture due to Baranyai for $n=2k+1$.",
        "Despite the impressive progress in statistical Optimal Transport (OT) in\nrecent years, there has been little interest in the study of the\n\\emph{sequential learning} of OT. Surprisingly so, as this problem is both\npractically motivated and a challenging extension of existing settings such as\nlinear bandits. This article considers (for the first time) the stochastic\nbandit problem of learning to solve generic Kantorovich and entropic OT\nproblems from repeated interactions when the marginals are known but the cost\nis unknown. We provide $\\tilde{\\mathcal O}(\\sqrt{T})$ regret algorithms for\nboth problems by extending linear bandits on Hilbert spaces. These results\nprovide a reduction to infinite-dimensional linear bandits. To deal with the\ndimension, we provide a method to exploit the intrinsic regularity of the cost\nto learn, yielding corresponding regret bounds which interpolate between\n$\\tilde{\\mathcal O}(\\sqrt{T})$ and $\\tilde{\\mathcal O}(T)$.",
        "As Large Language Models (LLMs) continue to advance in understanding and\ngenerating long sequences, new safety concerns have been introduced through the\nlong context. However, the safety of LLMs in long-context tasks remains\nunder-explored, leaving a significant gap in both evaluation and improvement of\ntheir safety. To address this, we introduce LongSafety, the first comprehensive\nbenchmark specifically designed to evaluate LLM safety in open-ended\nlong-context tasks. LongSafety encompasses 7 categories of safety issues and 6\nuser-oriented long-context tasks, with a total of 1,543 test cases, averaging\n5,424 words per context. Our evaluation towards 16 representative LLMs reveals\nsignificant safety vulnerabilities, with most models achieving safety rates\nbelow 55%. Our findings also indicate that strong safety performance in\nshort-context scenarios does not necessarily correlate with safety in\nlong-context tasks, emphasizing the unique challenges and urgency of improving\nlong-context safety. Moreover, through extensive analysis, we identify\nchallenging safety issues and task types for long-context models. Furthermore,\nwe find that relevant context and extended input sequences can exacerbate\nsafety risks in long-context scenarios, highlighting the critical need for\nongoing attention to long-context safety challenges. Our code and data are\navailable at https:\/\/github.com\/thu-coai\/LongSafety.",
        "Mitigating reward hacking--where AI systems misbehave due to flaws or\nmisspecifications in their learning objectives--remains a key challenge in\nconstructing capable and aligned models. We show that we can monitor a frontier\nreasoning model, such as OpenAI o3-mini, for reward hacking in agentic coding\nenvironments by using another LLM that observes the model's chain-of-thought\n(CoT) reasoning. CoT monitoring can be far more effective than monitoring agent\nactions and outputs alone, and we further found that a LLM weaker than o3-mini,\nnamely GPT-4o, can effectively monitor a stronger model. Because CoT monitors\ncan be effective at detecting exploits, it is natural to ask whether those\nexploits can be suppressed by incorporating a CoT monitor directly into the\nagent's training objective. While we show that integrating CoT monitors into\nthe reinforcement learning reward can indeed produce more capable and more\naligned agents in the low optimization regime, we find that with too much\noptimization, agents learn obfuscated reward hacking, hiding their intent\nwithin the CoT while still exhibiting a significant rate of reward hacking.\nBecause it is difficult to tell when CoTs have become obfuscated, it may be\nnecessary to pay a monitorability tax by not applying strong optimization\npressures directly to the chain-of-thought, ensuring that CoTs remain\nmonitorable and useful for detecting misaligned behavior."
      ]
    }
  },
  {
    "id":2411.06785,
    "research_type":"applied",
    "start_id":"b25",
    "start_title":"White-Box Transformers via Sparse Rate Reduction",
    "start_abstract":"In this paper, we contend that the objective of representation learning is to compress and transform distribution data, say sets tokens, towards a mixture low-dimensional Gaussian distributions supported on incoherent subspaces. The quality final can be measured by unified function called sparse rate reduction. From perspective, popular deep networks such as transformers naturally viewed realizing iterative schemes optimize incrementally. Particularly, show standard transformer block derived from alternating optimization complementary parts objective: multi-head self-attention operator gradient descent step token minimizing their lossy coding rate, subsequent multi-layer perceptron attempting sparsify tokens. This leads family white-box transformer-like network architectures which are mathematically fully interpretable. Despite simplicity, experiments these indeed learn designed they representations large-scale real-world vision datasets ImageNet, achieve performance very close thoroughly engineered ViT.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "RNA-Seq: a revolutionary tool for transcriptomics"
      ],
      "abstract":[
        "RNA-Seq is a recently developed approach to transcriptome profiling that uses deep-sequencing technologies. Studies using this method have already altered our view of the extent and complexity of eukaryotic transcriptomes. RNA-Seq also provides a far more precise measurement of levels of transcripts and their isoforms than other methods. This article describes the RNA-Seq approach, the challenges associated with its application, and the advances made so far in characterizing several eukaryote transcriptomes."
      ],
      "categories":[
        "q-bio.GN"
      ]
    },
    "list":{
      "title":[
        "Impact of transverse strain on linear, transitional and self-similar\n  turbulent mixing layers",
        "Dynamics of the general $Q$-tensor model interacting with a rigid body",
        "Magnetic mirror stars in five dimensions",
        "Quantum Electrodynamics from Quantum Cellular Automata, and the Tension\n  Between Symmetry, Locality and Positive Energy",
        "Oddities in the Entanglement Scaling of the Quantum Six-Vertex Model",
        "The Impact of Stellar Flares on the Atmospheric Escape of Exoplanets\n  orbiting M stars I: Insights from the AU Mic System",
        "Gradient-Based Optimization of Core-Shell Particles with Discrete\n  Materials for Directional Scattering",
        "Effects of GaAs Buffer Layer on Structural, Magnetic, and Transport\n  Properties of Magnetic Topological Insulators\n  Cr$_y$(Bi$_x$Sb$_{1-x}$)$_{2-y}$Te$_3$ and\n  V$_y$(Bi$_x$Sb$_{1-x}$)$_{2-y}$Te$_3$ Films",
        "Unlocking ultra-deep wide-field imaging with sidereal visibility\n  averaging",
        "Enhanced Electromechanical Properties of Solution-Processed\n  K$_{0.5}$Na$_{0.5}$NbO$_{3}$ Thin Films",
        "Enhancing Efficiency of Local Projections Estimation with Volatility\n  Clustering in High-Frequency Data",
        "Nice q-analogs of orthogonal polynomials with nice moments: Some simple\n  examples",
        "Euclid Quick Data Release (Q1): First visual morphology catalogue",
        "Table-top three-dimensional photoemission orbital tomography with a\n  femtosecond extreme ultraviolet light source",
        "Predicting the spectrum and decay constants of positive-parity\n  heavy-strange mesons using domain-wall fermions",
        "Topological Operations Around Exceptional Points via Shortcuts to\n  Adiabaticity",
        "Time-periodic driving of a bath-coupled open quantum gas of light",
        "Effective multipliers for weights whose log are H\\\"older continuous.\n  Application to the cost of fast boundary controls for the 1D Schr{\\\"o}dinger\n  equation",
        "OneForecast: A Universal Framework for Global and Regional Weather\n  Forecasting",
        "Green's function estimates for time measurable parabolic operators on\n  polyhedrons and polyhedral cones",
        "Left invertible quasi-isometric liftings",
        "Finite-Sample Analysis of Policy Evaluation for Robust Average Reward\n  Reinforcement Learning",
        "Integrability and charge transport in asymmetric quantum-circuit\n  geometries",
        "Class Imbalance in Anomaly Detection: Learning from an Exactly Solvable\n  Model",
        "New Proofs of the Explicit Formulas of Arakawa--Kaneko Zeta Values and\n  Kaneko--Tsumura $\\eta$- and $\\psi$- Values",
        "Selective band interaction and long-range hopping in a structured\n  environment with giant atoms",
        "Introducing APERTURE: A GPU-based General Relativistic Particle-in-Cell\n  Simulation Framework",
        "Bridging Voting and Deliberation with Algorithms: Field Insights from\n  vTaiwan and Kultur Komitee",
        "Quantum computation via Floquet-tailored Rydberg interactions"
      ],
      "abstract":[
        "The growth of interfacial instabilities such as the Rayleigh-Taylor (RTI) and\nRichtmyer-Meshkov instability (RMI) are modified when developing in convergent\ngeometries. Whilst these modifications are usually quantified by the\ncompression rate and convergence rate of the mixing layer, an alternative\nframework is proposed, describing the evolution of the mixing layer through the\neffects of the mean strain rates experienced by the mixing layer. An\ninvestigation into the effect of the transverse strain rate on the mixing layer\ndevelopment is conducted through application of transverse strain rates in\nplanar geometry. A model for the linear regime in planar geometry with\ntransverse strain rate is derived, with equivalent solutions to convergent\ngeometry, and validated with two-dimensional simulations demonstrating the\namplification of the instability growth under transverse compression. The\neffect of the transverse strain rate on the transitional-to-turbulent mixing\nlayer is investigated with implicit large eddy simulation based on the\nmulti-mode quarter-scale $\\theta$-group case by Thornber et al. (Phys. Fluids,\nvol. 29, 2017, 105107). The mixing layer's growth exhibits the opposite trend\nto the linear regime model, with reduced growth under transverse compression.\nThe effect of shear-production under transverse compression causes the mixing\nlayer to become more mixed and the turbulent kinetic energy is increasingly\ndominated by the transverse directions, deviating from the unstrained\nself-similar state. The mixing layer width is able to be predicted by adjusting\nthe buoyancy-drag model by Youngs & Thornber (Physica D, vol. 410, 2020,\n132517) to utilise a drag length scale that scales with the transverse\nexpansion.",
        "In this article, the fluid-rigid body interaction problem of nematic liquid\ncrystals described by the general Beris-Edwards $Q$-tensor model is studied. It\nis proved first that the total energy of this problem decreases in time. The\nassociated mathematical problem is a quasilinear mixed-order system with moving\nboundary. After the transformation to a fixed domain, a monolithic approach\nbased on the added mass operator and lifting arguments is employed to establish\nthe maximal $L^p$-regularity of the linearized problem in an anisotropic ground\nspace. This paves the way for the local strong well-posedness for large data\nand global strong well-posedness for small data of the interaction problem.",
        "We discuss a class of solutions of multidimensional gravity which are\nformally related to black-hole solutions but can observationally look like\ncompact stars whose surface reflects back all particles or signals getting\nthere. Some particular examples of such solutions are presented and studied,\nincluding those with a magnetic field in Maxwell or nonlinear electrodynamics\n(NED) in five dimensions. For NED as a possible source for magnetic mirror\nstars, we formulate a methodology of solving the 5D Einstein-NED equations and\npoint out the conditions under which there always exist mirror star solutions.\nWe also note that some of the Einstein-Maxwell solutions under consideration\nare discussed in the literature and called ``topological stars'' due to the\ncircular topology of the fifth dimension.",
        "We show that free QED is equivalent to the continuous-space-and-time limit of\nFermi and Bose lattice quantum cellular automata theories derived from quantum\nrandom walks satisfying simple symmetry and unitarity conditions. In doing so\nwe define the Fermi and Bose theories in a unified manner using the usual\nfermion internal space but a boson internal space that is six-dimensional. We\nshow that the reduction to a two-dimensional boson internal space (two helicity\nstates arising from spin-1 plus the photon transversality condition) comes from\nrestricting the quantum cellular automaton theory to positive energies. We\nbriefly examine common symmetries of quantum cellular automata, and how\ntime-reversal symmetry demands the existence of negative-energy solutions.\nThese solutions produce a tension in coupling the Fermi and Bose theories, in\nwhich the strong locality of quantum cellular automata seems to require a\nnonzero amplitude to produce negative-energy states, leading to an unphysical\ncascade of negative-energy particles. However, we show in a 1D model that by\nextending interactions over a larger (but finite) range it is possible to\nexponentially suppress the production of negative-energy particles to the point\nwhere they can be neglected.",
        "We investigate the entanglement properties of the Quantum Six-Vertex Model on\na cylinder, focusing on the Shannon-Renyi entropy in the limit of Renyi order\n$n = \\infty$. This entropy, calculated from the ground state amplitudes of the\nequivalent XXZ spin-1\/2 chain, allows us to determine the Renyi entanglement\nentropy of the corresponding Rokhsar-Kivelson wavefunctions, which describe the\nground states of certain conformal quantum critical points. Our analysis\nreveals a novel logarithmic correction to the expected entanglement scaling\nwhen the system size is odd. This anomaly arises from the geometric frustration\nof spin configurations imposed by periodic boundary conditions on odd-sized\nchains. We demonstrate that the scaling prefactor of this logarithmic term is\ndirectly related to the compactification radius of the low-energy bosonic field\ntheory description, or equivalently, the Luttinger parameter. Thus, this\ncorrection provides a direct probe of the underlying Conformal Field Theory\n(CFT) describing the critical point. Our findings highlight the crucial role of\nsystem size parity in determining the entanglement properties of this model and\noffer insights into the interplay between geometry, frustration, and\ncriticality.",
        "The X-rays and Extreme Ultraviolet (XUV) emission from M stars can drive the\natmospheric escape on planets orbiting them. M stars are also known for their\nfrequent emission of stellar flares, which will increase the high-energy flux\nreceived by their orbiting planets. To understand how stellar flares impact the\nprimordial atmospheres of planets orbiting young M stars, we use UV\nspectroscopic data of flares from the Habitable Zones and M dwarf Activity\nacross Time (HAZMAT) and Measurements of the Ultraviolet Spectral\nCharacteristics of Low-mass Exoplanetary Systems (MUSCLES) programs as a proxy\nto the XUV flare emission. Using the software package VPLanet, we simulate the\nyoung AU Mic planetary system composed of two Neptune-sized and one Earth-sized\nplanet orbiting a 23-Myr-old M1 star. Our findings show that the Earth-sized\nplanet AU Mic d should be in the process of losing completely its atmosphere in\nthe next couple million years, solely due to the quiescent emission, with\nflares not significantly contributing to its atmospheric escape due to the\nsmall size of AU mic d and its close-in distance from the star. However, our\nresults indicate that flares would play a crucial role for such planets further\naway, in the habitable zone (i.e. 0.2935 AU) of AU Mic-like stars during the\npost-saturation phase, accelerating the total atmospheric loss process by a few\nbillion years. For planets between 0.365 AU and the HZ outer edge, the\nadditional XUV from flares is necessary to deplete primordial atmospheres fully\nsince the quiescent emission alone is insufficient.",
        "Designing nanophotonic structures traditionally grapples with the\ncomplexities of discrete parameters, such as real materials, often resorting to\ncostly global optimization methods. This paper introduces an approach that\nleverages generative deep learning to map discrete parameter sets into a\ncontinuous latent space, enabling direct gradient-based optimization. For\nscenarios with non-differentiable physics evaluation functions, a neural\nnetwork is employed as a differentiable surrogate model. The efficacy of this\nmethodology is demonstrated by optimizing the directional scattering properties\nof core-shell nanoparticles composed of a selection of realistic materials. We\nderive suggestions for core-shell geometries with strong forward scattering and\nminimized backscattering. Our findings reveal significant improvements in\ncomputational efficiency and performance when compared to global optimization\ntechniques. Beyond nanophotonics design problems, this framework holds promise\nfor broad applications across all types of inverse problems constrained by\ndiscrete variables.",
        "Here, we study the effects of a GaAs buffer layer on the structural,\nmagnetic, and transport properties of Cr$_y$(Bi$_x$Sb$_{1-x}$)$_{2-y}$Te$_3$\nmagnetic topological insulator thin films and compare them with those of\nV$_y$(Bi$_x$Sb$_{1-x}$)$_{2-y}$Te$_3$, which we recently reported. Similar to\nthe case of V$_y$(Bi$_x$Sb$_{1-x}$)$_{2-y}$Te$_3$, growth on a GaAs buffer\nlayer leads to some distinctly different properties than direct growth on InP\nsubstrates. These include improved interface quality confirmed by transmission\nelectron microscopy, enhanced magnetic coercive fields, and smaller resistivity\npeaks at the magnetization reversals. Furthermore, the Bi-ratio dependence of\nthe carrier density reveals that the interface property also affects the Fermi\nlevel. These results demonstrate the importance of the buffer layer in\ncontrolling the electronic properties of the magnetic topological insulator\nfilms.",
        "Producing ultra-deep high-angular-resolution images with current and\nnext-generation radio interferometers introduces significant computational\nchallenges. In particular, the imaging is so demanding that processing large\ndatasets, accumulated over hundreds of hours on the same pointing, is likely\ninfeasible in the current data reduction schemes. In this paper, we revisit a\nsolution to this problem that was considered in the past but is not being used\nin modern software: sidereal visibility averaging (SVA). This technique\ncombines individual observations taken at different sidereal days into one much\nsmaller dataset by averaging visibilities at similar baseline coordinates. We\npresent our method and validated it using four separate 8-hour observations of\nthe ELAIS-N1 deep field, taken with the International LOw Frequency ARray\n(LOFAR) Telescope (ILT) at 140~MHz. Additionally, we assessed the accuracy\nconstraints imposed by Earth's orbital motion relative to the observed pointing\nwhen combining multiple datasets. We find, with four observations, data volume\nreductions of a factor of 1.8 and computational time improvements of a factor\nof 1.6 compared to standard imaging. These factors will increase when more\nobservations are combined with SVA. For instance, with 3000~hours of LOFAR data\naimed at achieving sensitivities of the order of {\\mu}Jy\/beam at sub-arcsecond\nresolutions, we estimate data volume reductions of up to a factor of 169 and a\n14-fold decrease in computing time using our current algorithm. This\nadvancement for imaging large deep interferometric datasets will benefit\ncurrent generation instruments, such as LOFAR, and upcoming instruments such as\nthe Square Kilometre Array (SKA), provided the calibrated visibility data of\nthe individual observations are retained.",
        "K$_{0.5}$Na$_{0.5}$NbO$_{3}$ is among the most promising lead-free\npiezoelectrics. While its sputtered films match the performance of the champion\npiezoelectric Pb(Zr,Ti)O$_{3}$, processing of high-quality, reproducible, and\ntime-stable solution-processed K$_{0.5}$Na$_{0.5}$NbO$_{3}$ films remains\nchallenging. Here, we report 1 $\\mu$m-thick Mn-doped\nK$_{0.5}$Na$_{0.5}$NbO$_{3}$ films prepared through a chemical solution\ndeposition process, which have perfectly dense microstructure and uniform\ncomposition across their thickness. The films exhibit a high transverse\npiezoelectric coefficient (e$_{31,f}$ = -14.8 C\/m$^{2}$), high dielectric\npermittivity (${\\epsilon}_{r}$ = 920), low dielectric losses (tan${\\delta}$ =\n0.05) and can withstand electric fields up to at least 1 MV\/cm. The functional\nproperties show excellent stability over time, and the synthesis process is\nreproducible. The results demonstrate the high potential of Mn-doped\nK$_{0.5}$Na$_{0.5}$NbO$_{3}$ films to become a replacement for lead-based\nPb(Zr,Ti)O$_{3}$ films in piezoelectric applications.",
        "This paper advances the local projections (LP) method by addressing its\ninefficiency in high-frequency economic and financial data with volatility\nclustering. We incorporate a generalized autoregressive conditional\nheteroskedasticity (GARCH) process to resolve serial correlation issues and\nextend the model with GARCH-X and GARCH-HAR structures. Monte Carlo simulations\nshow that exploiting serial dependence in LP error structures improves\nefficiency across forecast horizons, remains robust to persistent volatility,\nand yields greater gains as sample size increases. Our findings contribute to\nrefining LP estimation, enhancing its applicability in analyzing economic\ninterventions and financial market dynamics.",
        "In this note I collect some typical examples of orthogonal polynomials with\nsimple moments where both moments and orthogonal polynomials have nice\nq-analogs.",
        "We present a detailed visual morphology catalogue for Euclid's Quick Release\n1 (Q1). Our catalogue includes galaxy features such as bars, spiral arms, and\nongoing mergers, for the 378000 bright ($I_E < 20.5$) or extended (area $\\geq\n700\\,$pixels) galaxies in Q1. The catalogue was created by finetuning the\nZoobot galaxy foundation models on annotations from an intensive one month\ncampaign by Galaxy Zoo volunteers. Our measurements are fully automated and\nhence fully scaleable. This catalogue is the first 0.4% of the approximately\n100 million galaxies where Euclid will ultimately resolve detailed morphology.",
        "Following electronic processes in molecules and materials at the level of the\nquantum mechanical electron wavefunction with angstrom-level spatial resolution\nand with full access to its femtosecond temporal dynamics is at the heart of\nultrafast condensed matter physics. A breakthrough invention allowing\nexperimental access to electron wavefunctions was the reconstruction of\nmolecular orbitals from angle-resolved photoelectron spectroscopy data in 2009,\ntermed photoemission orbital tomography (POT). This invention puts ultrafast\nthree-dimensional (3D) POT in reach, with many new prospects for the study of\nultrafast light-matter interaction, femtochemistry and photo-induced phase\ntransitions. Here, we develop a synergistic experimental-algorithmic approach\nto realize the first 3D-POT experiment using a short-pulse extreme ultraviolet\nlight source. We combine a new variant of photoelectron spectroscopy, namely\nultrafast momentum microscopy, with a table-top spectrally-tunable\nhigh-harmonic generation light source and a tailored algorithm for efficient 3D\nreconstruction from sparse, undersampled data. This combination dramatically\nspeeds up the experimental data acquisition, while at the same time reducing\nthe sampling requirements to achieve complete 3D information. We demonstrate\nthe power of this approach by full 3D imaging of the frontier orbitals of a\nprototypical organic semiconductor absorbed on pristine Ag(110).",
        "We present a lattice-QCD calculation of the masses and decay constants of the\npositive-parity heavy-strange mesons $D^*_{s0}$, $D_{s1}$, $B^*_{s0}$, and\n$B_{s1}$. The calculations are performed with domain-wall fermions for the\nlight and strange quarks and an anisotropic clover action for the charm and\nbottom quarks. We use seven different RBC\/UKQCD ensembles with pion masses\nranging from a near-physical 139 MeV up to 431 MeV. We consider two different\nanalysis types, with or without two-meson operators at the source. We observe\nthe expected below-threshold ground states. The fits without the two-meson\noperators appear to be more stable, but may overestimate the ground-state\nenergies, while preliminary fits with two-meson operators at the source only\nappear to underestimate the ground-state energies.",
        "The existence of singularities in the spectrum of non-Hermitian Hamiltonians\nleads to a non-trivial spectral topology which can be exploited to generate\ntopological operations. However, their implementation has remained elusive due\nto the difficulty of generating a true adiabatic evolution. Here, we develop\nfast, robust control protocols that generate a desired topological operation.\nOur strategy relies on shortcuts to adiabaticity, but is not a trivial\nextension. The presence of spectral singularities renders the strategy\ndeveloped for Hermitian Hamiltonians impractical as it will lead to faulty\ncontrol protocols. Moreover, due to the dynamics sensitivity to parameter\nuncertainties, not all shortcuts to adiabaticity can be used in a realistic\nsetting. We illustrate our method in the context of a two-mode non-Hermitian\nHamiltonian and discuss why in general celebrated shortcuts to adiabaticiy like\ntransitionless driving and superadiabatic transitionless driving are not\nappropriate control protocols for non-Hermitian systems.",
        "We study the frequency-resolved density response of a photon Bose-Einstein\ncondensate coupled to a bath of dye molecules by time-periodic driving. By\nmonitoring the photon number dynamics for different drive frequencies, we\nobtain the spectral response of the condensate in a phase-sensitive way. We\nfind that as the photon number increases, the response of the coupled\ncondensate-bath system transitions from overdamped to resonant behavior,\nindicating a transition from closed to open system dynamics. Our spectroscopy\nmethod paves the way for studies of collective excitations in complex\ndriven-dissipative systems.",
        "We give a simple proof of the Beurling-Malliavin multiplier theorem (BM1) in\nthe particular case of weights that verify the usual finite logarithmic\nintegral condition and such that their log are H{\\\"o}lder continuous with\nexponent less than 1. Our proof has the advantage to give an explicit version\nof BM1, in the sense that one can give precise estimates from below and above\nfor the multiplier, in terms of the exponential type we want to reach, and the\nconstants appearing in the H{\\\"o}lder condition of our weights. The same ideas\ncan be applied to a particular weight, that will lead to an improvement on the\nestimation of the cost of fast boundary controls for the 1D Schr{\\\"o}dinger\nequation on a segment. Our proof is mainly based on the use of a modified\nHilbert transform together with its link with the harmonic extension in the\ncomplex upper half plane and some modified conjugate harmonic extension in the\nupper half plane.",
        "Accurate weather forecasts are important for disaster prevention,\nagricultural planning, and water resource management. Traditional numerical\nweather prediction (NWP) methods offer physically interpretable high-accuracy\npredictions but are computationally expensive and fail to fully leverage\nrapidly growing historical data. In recent years, deep learning methods have\nmade significant progress in weather forecasting, but challenges remain, such\nas balancing global and regional high-resolution forecasts, excessive smoothing\nin extreme event predictions, and insufficient dynamic system modeling. To\naddress these issues, this paper proposes a global-regional nested weather\nforecasting framework based on graph neural networks (GNNs). By combining a\ndynamic system perspective with multi-grid theory, we construct a multi-scale\ngraph structure and densify the target region to capture local high-frequency\nfeatures. We introduce an adaptive information propagation mechanism, using\ndynamic gating units to deeply integrate node and edge features for more\naccurate extreme event forecasting. For high-resolution regional forecasts, we\npropose a neural nested grid method to mitigate boundary information loss.\nExperimental results show that the proposed method performs excellently across\nglobal to regional scales and short-term to long-term forecasts, especially in\nextreme event predictions (e.g., typhoons), significantly improving forecast\naccuracy. Our codes are available at https:\/\/github.com\/YuanGao-YG\/OneForecast.",
        "We provide Green's function estimates for parabolic operators on polyhedrons\nand polyhedral cones in $\\mathbb{R}^3$. These estimates incorporate mixed\nweights, which include appropriate powers of the distances to the vertices, the\nedges, and the boundary of the domains. The allowable ranges for the weight\nparameters are explicitly determined by the geometry of the domains.",
        "Quasi-isometric liftings similar to isometries, for the operators similar to\ncontractions in Hilbert spaces, are investigated. The existence of such\nliftings is established, and their applications are explored for specific\noperator classes, including quasicontractions. A particular focus is placed on\noperators that admit left invertible minimal quasi-isometric liftings. These\noperators are characterized within the framework of $A$-contractions, and the\nmatrix structures of their liftings are analyzed, highlighting parallels with\nthe isometric liftings of contractions.",
        "We present the first finite-sample analysis for policy evaluation in robust\naverage-reward Markov Decision Processes (MDPs). Prior works in this setting\nhave established only asymptotic convergence guarantees, leaving open the\nquestion of sample complexity. In this work, we address this gap by\nestablishing that the robust Bellman operator is a contraction under the span\nsemi-norm, and developing a stochastic approximation framework with controlled\nbias. Our approach builds upon Multi-Level Monte Carlo (MLMC) techniques to\nestimate the robust Bellman operator efficiently. To overcome the infinite\nexpected sample complexity inherent in standard MLMC, we introduce a truncation\nmechanism based on a geometric distribution, ensuring a finite constant sample\ncomplexity while maintaining a small bias that decays exponentially with the\ntruncation level. Our method achieves the order-optimal sample complexity of\n$\\tilde{\\mathcal{O}}(\\epsilon^{-2})$ for robust policy evaluation and robust\naverage reward estimation, marking a significant advancement in robust\nreinforcement learning theory.",
        "We revisit the integrability of quantum circuits constructed from two-qubit\nunitary gates $U$ that satisfy the Yang-Baxter equation. A brickwork\narrangement of $U$ typically corresponds to an integrable Trotterization of\nsome Hamiltonian dynamics. Here, we consider more general circuit geometries\nwhich include circuits without any nontrivial space periodicity. We show that\nany time-periodic quantum circuit in which $U$ is applied to each pair of\nneighbouring qubits exactly once per period remains integrable. We further\ngeneralize this framework to circuits with time-varying two-qubit gates. The\nspatial arrangement of gates in the integrable circuits considered herein can\nbreak the space-reflection symmetry even when $U$ itself is symmetric. By\nanalyzing the dynamical spin susceptibility on ballistic hydrodynamic scale, we\ninvestigate how an asymmetric arrangement of gates affects the spin transport.\nWhile it induces nonzero higher odd moments in the dynamical spin\nsusceptibility, the first moment, which corresponds to a drift in the spreading\nof correlations, remains zero. We explain this within a quasiparticle picture\nwhich suggests that a nonzero drift necessitates gates acting on distinct\ndegrees of freedom.",
        "Class imbalance (CI) is a longstanding problem in machine learning, slowing\ndown training and reducing performances. Although empirical remedies exist, it\nis often unclear which ones work best and when, due to the lack of an\noverarching theory. We address a common case of imbalance, that of anomaly (or\noutlier) detection. We provide a theoretical framework to analyze, interpret\nand address CI. It is based on an exact solution of the teacher-student\nperceptron model, through replica theory. Within this framework, one can\ndistinguish several sources of CI: either intrinsic, train or test imbalance.\nOur analysis reveals that the optimal train imbalance is generally different\nfrom 50%, with a non trivial dependence on the intrinsic imbalance, the\nabundance of data and on the noise in the learning. Moreover, there is a\ncrossover between a small noise training regime where results are independent\nof the noise level to a high noise regime where performances quickly degrade\nwith noise. Our results challenge some of the conventional wisdom on CI and\noffer practical guidelines to address it.",
        "In this paper, we establish some new identities of integrals involving\nmultiple polylogarithm functions and their level two analogues in terms of\nHurwitz-type multiple zeta (star) values. Using these identities, we provide\nnew proofs of the explicit formulas of Arakawa--Kaneko zeta values,\nKaneko--Tsumura $\\eta$- and $\\psi$-values, and also give a formula for double\n$T$-values.",
        "Giant atoms, which couple to the environment at multiple discrete points,\nexhibit various nontrivial phenomena in quantum optics due to their nonlocal\ncouplings. In this study, we propose a one-dimensional cross-stitch ladder\nlattice featuring both a dispersive band and a flat band. By modulating the\nrelative phase between the coupling points, the giant atom selectively\ninteracts with either band. First, we analyze the scenario where the dispersive\nand flat bands intersect at two points, and the atomic frequency lies within\nthe band. Unlike the small atom, which simultaneously interacts with both\nbands, a single giant atom with a controllable phase interacts exclusively with\nthe dispersive or flat band. Second, in the bandgap regime, where two atoms\ninteract through bound-state overlaps manifesting as dipole-dipole\ninteractions, we demonstrate that giant atoms enable deterministic long-range\nhopping and energy exchange with higher fidelity compared to small atoms. These\nfindings provide promising applications in quantum information processing,\noffering enhanced controllability and selectivity for quantum systems and\ndevices.",
        "Low-luminosity Active Galactic Nuclei (AGN) are believed to be surrounded by\na collisionless, highly magnetized accretion flow. As a result,\nParticle-in-Cell simulations are the best tools to study the immediate vicinity\nof the event horizons of these supermassive black holes. We present a GPU-based\ngeneral relativistic particle-in-cell (GRPIC) code framework called Aperture.\nAperture is developed in C++, with compute kernels written in CUDA and HIP to\ntake advantage of the massive acceleration modern GPUs enable. The code is\norganized in a fully modular way, allowing easy extensions to new physics\nproblems. In this paper, we describe in detail the particle pusher, field\nsolver, and charge-conserving current deposition algorithms employed in\nAperture, and present test cases to validate their correctness. Then, we apply\nthe code to study spark gaps and plasma injection in black hole magnetospheres.\nWe find that the apparent location and time-evolution of the gap depend on the\nobserver. Our results reconcile the previous conflicting findings from 1D and\n2D simulations in the literature.",
        "Democratic processes increasingly aim to integrate large-scale voting with\nface-to-face deliberation, addressing the challenge of reconciling individual\npreferences with collective decision-making. This work introduces new methods\nthat use algorithms and computational tools to bridge online voting with\nface-to-face deliberation, tested in two real-world scenarios: Kultur Komitee\n2024 (KK24) and vTaiwan. These case studies highlight the practical\napplications and impacts of the proposed methods.\n  We present three key contributions: (1) Radial Clustering for Preference\nBased Subgroups, which enables both in-depth and broad discussions in\ndeliberative settings by computing homogeneous and heterogeneous group\ncompositions with balanced and adjustable group sizes; (2) Human-in-the-loop\nMES, a practical method that enhances the Method of Equal Shares (MES)\nalgorithm with real-time digital feedback. This builds algorithmic trust by\ngiving participants full control over how much decision-making is delegated to\nthe voting aggregation algorithm as compared to deliberation; and (3) the\nReadTheRoom deliberation method, which uses opinion space mapping to identify\nagreement and divergence, along with spectrum-based preference visualisation to\ntrack opinion shifts during deliberation. This approach enhances transparency\nby clarifying collective sentiment and fosters collaboration by encouraging\nparticipants to engage constructively with differing perspectives.\n  By introducing these actionable frameworks, this research extends in-person\ndeliberation with scalable digital methods that address the complexities of\nmodern decision-making in participatory processes.",
        "Rydberg atoms stand out as a highly promising platform for realizing quantum\ncomputation with significant advantages in constructing high-fidelity quantum\ngates. Floquet frequency modulation (FFM), in Rydberg-atom systems, provides a\nunique platform for achieving precise quantum control and uncovering exotic\nphysical phenomena, paving the way for innovative methodologies in quantum\ndynamics research. This work introduces a method to realize controlled\narbitrary phase gates in Rydberg atoms by manipulating system dynamics using\nFFM. Notably, this method eliminates the need for laser addressing of\nindividual atoms, significantly enhancing convenience for future practical\napplications. Furthermore, this approach can be integrated with soft quantum\ncontrol strategies to enhance the fidelity and robustness of the resultant\ncontrolled-phase gates. Finally, as an example, this methodology is applied in\nGrover-Long algorithm to search target items with zero failure rate,\ndemonstrating its substantial significance for future quantum information\nprocessing applications. This work leveraging Rydberg atoms and Floquet\nfrequency modulation may herald a new era of scalable and reliable quantum\ncomputing."
      ]
    }
  },
  {
    "id":2412.00868,
    "research_type":"applied",
    "start_id":"b22",
    "start_title":"Context-Aware Testing: A New Paradigm for Model Testing with Large\n  Language Models",
    "start_abstract":"The predominant de facto paradigm of testing ML models relies on either using\nonly held-out data to compute aggregate evaluation metrics or by assessing the\nperformance on different subgroups. However, such data-only testing methods\noperate under the restrictive assumption that the available empirical data is\nthe sole input for testing ML models, disregarding valuable contextual\ninformation that could guide model testing. In this paper, we challenge the\ngo-to approach of data-only testing and introduce context-aware testing (CAT)\nwhich uses context as an inductive bias to guide the search for meaningful\nmodel failures. We instantiate the first CAT system, SMART Testing, which\nemploys large language models to hypothesize relevant and likely failures,\nwhich are evaluated on data using a self-falsification mechanism. Through\nempirical evaluations in diverse settings, we show that SMART automatically\nidentifies more relevant and impactful failures than alternatives,\ndemonstrating the potential of CAT as a testing paradigm.",
    "start_categories":[
      "stat.ME"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "How resilient are language models to text perturbations"
      ],
      "abstract":[
        "Large language models typically rely on highly curated datasets that lack common irregularities such as typos and contractions, resulting in a mismatch between their training environments and real-world applications. This study evaluates the resilience of four prominent models in five different NLP tasks when confronted with perturbed inputs. We investigate three categories of perturbations: character-level, word-level and miscellaneous perturbations. By comparing performance on original and altered datasets, our results reveal a significant sensitivity to input perturbations across all models, with varying degrees of vulnerability depending on both the specific task and the type of perturbation. In particular, the XLNet model consistently shows superior robustness, while tasks involving grammatical coherence are most adversely affected."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Apparent violations of the second law in the quantum-classical dynamics\n  of interacting levitated nanoparticles",
        "Does dark matter fall in the same way as standard model particles? A\n  direct constraint of Euler's equation with cosmological data",
        "A countable Boolean algebra that is Reichenbach's common cause complete",
        "Exploring Sentiment Manipulation by LLM-Enabled Intelligent Trading\n  Agents",
        "Effects of galactic environment on size and dark matter content in\n  low-mass galaxies",
        "Lattice-Based Pruning in Recurrent Neural Networks via Poset Modeling",
        "Lipschitz Decompositions of Finite $\\ell_{p}$ Metrics",
        "DIN-CTS: Low-Complexity Depthwise-Inception Neural Network with\n  Contrastive Training Strategy for Deepfake Speech Detection",
        "Recent open heavy flavor studies for the Electron-Ion Collider",
        "An Investigation of FP8 Across Accelerators for LLM Inference",
        "ASTRAL: Automated Safety Testing of Large Language Models",
        "Scalable and Site-Specific Frequency Tuning of Two-Level System Defects\n  in Superconducting Qubit Arrays",
        "Fabrication of Soft and Comfortable Pressure-Sensing Shoe Sole for\n  Intuitive Monitoring of Human Quality Gaits",
        "Cognitive Performance Measurements and the Impact of Sleep Quality Using\n  Wearable and Mobile Sensors",
        "High-throughput Discovery of Anti-gap Semiconductors",
        "AI-Facilitated Collective Judgements",
        "Predictions for Bottomonium from a Relativistic Screened Potential Model",
        "Analysis of harmonic average method for interface problems with\n  discontinuous solutions and fluxes",
        "Ground-Optimized 4D Radar-Inertial Odometry via Continuous Velocity\n  Integration using Gaussian Process",
        "GraphTEN: Graph Enhanced Texture Encoding Network",
        "Anomalous nuclear effects on ion charge state distribution in helium gas",
        "Prediction-Powered E-Values",
        "Minimal Shortfall Strategies for Liquidation of a Basket of Stocks using\n  Reinforcement Learning",
        "Responsible Artificial Intelligence (RAI) in U.S. Federal Government :\n  Principles, Policies, and Practices",
        "A Transfer Learning Framework for Anomaly Detection in Multivariate IoT\n  Traffic Data",
        "Code Readability in the Age of Large Language Models: An Industrial Case\n  Study from Atlassian",
        "Propulsion of flapping foils undergoing in-plane clap-and-fling and\n  deviation motions",
        "Update on the isospin breaking corrections to the HVP with C-periodic\n  boundary conditions",
        "EmbodiedEval: Evaluate Multimodal LLMs as Embodied Agents"
      ],
      "abstract":[
        "Random exchanges of energy arise naturally in stochastic systems. As a\nconsequence, apparent violations of the second law of thermodynamics can occur,\nas it holds true on average. Here we investigate the occurrence of these\napparent violations -- termed free lunches -- in a quantum-classical system\ncomprised of levitated nanoparticles exchanging energy via the Coulomb\ninteraction. We consider different initial states for the quantum system, and\nhow these exert work and fluctuations upon the classical particle affecting the\nprobability of free lunches. With that, we initiate the study of hybrid\nquantum-classical systems through the lens of stochastic thermodynamics.",
        "Since dark matter particles have never been directly detected, we do not know\nhow they move, and in particular we do not know how they fall inside\ngravitational potential wells. Usually it is assumed that dark matter only\ninteracts gravitationally with itself and with particles of the standard model,\nand therefore that its motion is governed by Euler's equation. In this paper,\nwe test this assumption for the first time at cosmological scales, by combining\nmeasurements of galaxy velocities with measurements of gravitational potential\nwells, encoded in the Weyl potential. We find that current data are consistent\nwith Euler's equation at redshifts $z\\in [0.3,0.8]$, and we place constraints\non the strength of a potential fifth force, which would alter the way dark\nmatter particles fall. We find that a positive fifth force cannot exceed 7% of\nthe gravitational interaction strength, while a negative fifth force is limited\nto 21%. The coming generation of surveys, including the Legacy Survey of Space\nand Time (LSST) of the Vera C. Rubin Observatory and the Dark Energy\nSpectroscopic Instrument (DESI) will drastically improve the constraints,\nallowing to constrain a departure from pure gravitational interaction at the\nlevel of 2%.",
        "The common cause completeness (CCC) is a philosophical principle that asserts\nthat if we consider two positively correlated events then it evokes a common\ncause. The principle is due to H. Reichenbach and has been largely studied in\nBoolean algebras and elsewhere.The results published so far bring about a\nquestion whether there is a small (countable) Boolean algebra with CCC. In this\nnote we construct such an example.",
        "Companies across all economic sectors continue to deploy large language\nmodels at a rapid pace. Reinforcement learning is experiencing a resurgence of\ninterest due to its association with the fine-tuning of language models from\nhuman feedback. Tool-chain language models control task-specific agents; if the\nconverse has not already appeared, it soon will. In this paper, we present what\nwe believe is the first investigation of an intelligent trading agent based on\ncontinuous deep reinforcement learning that also controls a large language\nmodel with which it can post to a social media feed observed by other traders.\nWe empirically investigate the performance and impact of such an agent in a\nsimulated financial market, finding that it learns to optimize its total\nreward, and thereby augment its profit, by manipulating the sentiment of the\nposts it produces. The paper concludes with discussion, limitations, and\nsuggestions for future work.",
        "We utilize the cosmological volume simulation, FIREbox, to investigate how a\ngalaxy's environment influences its size and dark matter content. Our study\nfocuses on approximately 1,200 galaxies (886 central and 332 satellite halos)\nin the low-mass regime, with stellar masses between $10^6$ to $10^9$\n$M_{\\odot}$. We analyze the size-mass relation ($r_{50} - M_{\\star}$), inner\ndark matter mass-stellar mass ($M^{50}_{\\rm DM} - M_{\\star}$) relation, and the\nhalo mass-stellar mass ($M_{\\rm halo} - M_{\\star}$) relation. At fixed stellar\nmass, we find the galaxies experiencing stronger tidal influences, indicated by\nhigher Perturbation Indices (PI $>$ 1) are generally larger and have lower\nmasses relative to their counterparts with lower Perturbation Indices (PI $<$\n1). Applying a Random Forest regression model, we show that both the\nenvironment (PI) and halo mass ($M_{rm halo}$) are significant predictors of a\ngalaxy's relative size and dark matter content. Notably, because $M_{\\rm halo}$\nis also strongly affected by the environment, our findings indicate that\nenvironmental conditions not only influence galactic sizes and relative inner\ndark matter content directly, but also indirectly through their impact on halo\nmass. Our results highlight a critical interplay between environmental factors\nand halo mass in shaping galaxy properties, affirming the environment as a\nfundamental driver in galaxy formation and evolution.",
        "Recurrent neural networks (RNNs) are central to sequence modeling tasks, yet\ntheir high computational complexity poses challenges for scalability and\nreal-time deployment. Traditional pruning techniques, predominantly based on\nweight magnitudes, often overlook the intrinsic structural properties of these\nnetworks. We introduce a novel framework that models RNNs as partially ordered\nsets (posets) and constructs corresponding dependency lattices. By identifying\nmeet irreducible neurons, our lattice-based pruning algorithm selectively\nretains critical connections while eliminating redundant ones. The method is\nimplemented using both binary and continuous-valued adjacency matrices to\ncapture different aspects of network connectivity. Evaluated on the MNIST\ndataset, our approach exhibits a clear trade-off between sparsity and\nclassification accuracy. Moderate pruning maintains accuracy above 98%, while\naggressive pruning achieves higher sparsity with only a modest performance\ndecline. Unlike conventional magnitude-based pruning, our method leverages the\nstructural organization of RNNs, resulting in more effective preservation of\nfunctional connectivity and improved efficiency in multilayer networks with\ntop-down feedback. The proposed lattice-based pruning framework offers a\nrigorous and scalable approach for reducing RNN complexity while sustaining\nrobust performance, paving the way for more efficient hierarchical models in\nboth machine learning and computational neuroscience.",
        "Lipschitz decomposition is a useful tool in the design of efficient\nalgorithms involving metric spaces. While many bounds are known for different\nfamilies of finite metrics, the optimal parameters for $n$-point subsets of\n$\\ell_p$, for $p > 2$, remained open, see e.g. [Naor, SODA 2017]. We make\nsignificant progress on this question and establish the bound\n$\\beta=O(\\log^{1-1\/p} n)$. Building on prior work, we demonstrate applications\nof this result to two problems, high-dimensional geometric spanners and\ndistance labeling schemes. In addition, we sharpen a related decomposition\nbound for $1<p<2$, due to Filtser and Neiman [Algorithmica 2022].",
        "In this paper, we propose a deep neural network approach for deepfake speech\ndetection (DSD) based on a lowcomplexity Depthwise-Inception Network (DIN)\ntrained with a contrastive training strategy (CTS). In this framework, input\naudio recordings are first transformed into spectrograms using Short-Time\nFourier Transform (STFT) and Linear Filter (LF), which are then used to train\nthe DIN. Once trained, the DIN processes bonafide utterances to extract audio\nembeddings, which are used to construct a Gaussian distribution representing\ngenuine speech. Deepfake detection is then performed by computing the distance\nbetween a test utterance and this distribution to determine whether the\nutterance is fake or bonafide. To evaluate our proposed systems, we conducted\nextensive experiments on the benchmark dataset of ASVspoof 2019 LA. The\nexperimental results demonstrate the effectiveness of combining the\nDepthwise-Inception Network with the contrastive learning strategy in\ndistinguishing between fake and bonafide utterances. We achieved Equal Error\nRate (EER), Accuracy (Acc.), F1, AUC scores of 4.6%, 95.4%, 97.3%, and 98.9%\nrespectively using a single, low-complexity DIN with just 1.77 M parameters and\n985 M FLOPS on short audio segments (4 seconds). Furthermore, our proposed\nsystem outperforms the single-system submissions in the ASVspoof 2019 LA\nchallenge, showcasing its potential for real-time applications.",
        "The future Electron-Ion Collider (EIC) will operate a series of\nhigh-luminosity high-energy electron+proton ($e+p$) and electron+nucleus\n($\\textit{e + A}$) collisions to study several fundamental questions in the\nhigh energy and nuclear physics field. Heavy flavor hadron and jet production\nat the EIC plays an important role in exploring both potential modification on\nthe initial-state nuclear parton distribution functions (nPDFs) and final-state\nparton propagation and hadronization processes under different nuclear medium\nconditions. The current design of the EIC ePIC detector has good performance of\nvertex and track reconstruction, particle identification and energy\ndetermination in the pseudorapidity region of $-3.5<\\eta<3.5$, which will\nenable a series of high precision heavy flavor hadron and jet measurements.\nLatest simulation studies of the projected nuclear modification factor $R_{eA}$\nof heavy flavor jets and heavy flavor hadron inside jets in $e+p$ and\n$\\textit{e + Au}$ collisions at $\\sqrt{s} =$ 28.6 GeV and 63.2 GeV as well as\nthe projected statistical accuracy of inclusive and differential charm baryon\nover meson ratio measurements in $e+p$ collisions will be presented. The\nimpacts of these proposed EIC measurements on constraining the heavy quark\npropagation properties in cold nuclear medium and exploring the heavy quark\nhadronization process will be discussed.",
        "The introduction of 8-bit floating-point (FP8) computation units in modern AI\naccelerators has generated significant interest in FP8-based large language\nmodel (LLM) inference. Unlike 16-bit floating-point formats, FP8 in deep\nlearning requires a shared scaling factor. Additionally, while E4M3 and E5M2\nare well-defined at the individual value level, their scaling and accumulation\nmethods remain unspecified and vary across hardware and software\nimplementations. As a result, FP8 behaves more like a quantization format than\na standard numeric representation. In this work, we provide the first\ncomprehensive analysis of FP8 computation and acceleration on two AI\naccelerators: the NVIDIA H100 and Intel Gaudi 2. Our findings highlight that\nthe Gaudi 2, by leveraging FP8, achieves higher throughput-to-power efficiency\nduring LLM inference, offering valuable insights into the practical\nimplications of FP8 adoption for datacenter-scale LLM serving.",
        "Large Language Models (LLMs) have recently gained attention due to their\nability to understand and generate sophisticated human-like content. However,\nensuring their safety is paramount as they might provide harmful and unsafe\nresponses. Existing LLM testing frameworks address various safety-related\nconcerns (e.g., drugs, terrorism, animal abuse) but often face challenges due\nto unbalanced and obsolete datasets. In this paper, we present ASTRAL, a tool\nthat automates the generation and execution of test cases (i.e., prompts) for\ntesting the safety of LLMs. First, we introduce a novel black-box coverage\ncriterion to generate balanced and diverse unsafe test inputs across a diverse\nset of safety categories as well as linguistic writing characteristics (i.e.,\ndifferent style and persuasive writing techniques). Second, we propose an\nLLM-based approach that leverages Retrieval Augmented Generation (RAG),\nfew-shot prompting strategies and web browsing to generate up-to-date test\ninputs. Lastly, similar to current LLM test automation techniques, we leverage\nLLMs as test oracles to distinguish between safe and unsafe test outputs,\nallowing a fully automated testing approach. We conduct an extensive evaluation\non well-known LLMs, revealing the following key findings: i) GPT3.5 outperforms\nother LLMs when acting as the test oracle, accurately detecting unsafe\nresponses, and even surpassing more recent LLMs (e.g., GPT-4), as well as LLMs\nthat are specifically tailored to detect unsafe LLM outputs (e.g., LlamaGuard);\nii) the results confirm that our approach can uncover nearly twice as many\nunsafe LLM behaviors with the same number of test inputs compared to currently\nused static datasets; and iii) our black-box coverage criterion combined with\nweb browsing can effectively guide the LLM on generating up-to-date unsafe test\ninputs, significantly increasing the number of unsafe LLM behaviors.",
        "State-of-the-art superconducting quantum processors containing tens to\nhundreds of qubits have demonstrated the building blocks for realizing\nfault-tolerant quantum computation. Nonetheless, a fundamental barrier to\nscaling further is the prevalence of fluctuating quantum two-level system (TLS)\ndefects that can couple resonantly to qubits, causing excess decoherence and\nenhanced gate errors. Here we introduce a scalable architecture for\nsite-specific and in-situ manipulation of TLS frequencies out of the spectral\nvicinity of our qubits. Our method is resource efficient, combining TLS\nfrequency tuning and universal single qubit control into a single on-chip\ncontrol line per qubit. We independently control each qubit's dissipative\nenvironment to dynamically improve both qubit coherence times and single qubit\ngate fidelities -- with a constant time overhead that does not scale with the\ndevice size. Over a period of 40 hours across 6 qubits, we demonstrate a $36\\%$\nimprovement in average single qubit error rates and a $17\\%$ improvement in\naverage energy relaxation times. Critically, we realize a 4-fold suppression in\nthe occurrence of TLS-induced performance outliers, and a complete reduction of\nsimultaneous outlier events. These results mark a significant step toward\novercoming the challenges that TLS defects pose to scaling superconducting\nquantum processors.",
        "The study discusses the design and fabrication of flexible pressure sensors\nusing Ecoflex\/Graphene composites. The fabricated sensor is used for the\napplication of intuitive monitoring of human quality gaits and implementation\nof the soft and comfortable shoe sole for rehabilitation of the patients with\nfoot disorder is also taken into consideration. The sensor is fabricated using\nmolding and casting technique by sandwiching the thin film Ecoflex\/Graphene\ncomposites between the copper (Cu) electrodes with the dimension of 15 x 15 mm2\nwith high sensitivity. There are five pressure sensors integrated in the shoe\nsole, a sensor at the forefoot, three sensors at the midfoot and one sensor at\nthe lower foot (heel). The behavior of the sensor is negative piezoresistive in\nwhich the resistance decreases as the pressure increases. The sensors are\nembedded in a soft and comfortable shoe sole and then integrated with a laptop\nor mobile application to monitor and analyze human gait in real-time.\nFurthermore, a dedicated Graphical User Interface (GUI) is designed to read the\ndata. The pressure sensors are integrated with ESP32 microcontroller which\nwirelessly transmit data to the GUI and smart phones which could be further\nused in the intuitive monitoring, rehabilitation of the patients with foot\ndisorder or neuromotor diseases.",
        "Human cognitive performance is an underlying factor in most of our daily\nlives, and numerous factors influence cognitive performance. In this work, we\ninvestigate how changes in sleep quality influence cognitive performance,\nmeasured from a dataset collected during a 2-month field study. We collected\ncognitive performance data (alertness) with the Psychomotor Vigilance Task\n(PVT), mobile keyboard typing metrics from participants' smartphones, and sleep\nquality metrics through a wearable sleep tracking ring. Our findings highlight\nthat specific sleep metrics like night-time heart rate, sleep latency, sleep\ntiming, sleep restfulness, and overall sleep quantity significantly influence\ncognitive performance. To strengthen the current research on cognitive\nmeasurements, we introduce smartphone typing metrics as a proxy or a\ncomplementary method for continuous passive measurement of cognitive\nperformance. Together, our findings contribute to ubiquitous computing via a\nlongitudinal case study with a novel wearable device, the resulting findings on\nthe association between sleep and cognitive function, and the introduction of\nsmartphone keyboard typing as a proxy of cognitive function.",
        "Conventional semiconductors typically have bonding states near the valence\nband maximum (VBM) and antibonding states near the conduction band minimum\n(CBM). Semiconductors with the opposite electronic configuration, namely an\nantibonding VBM and a bonding CBM, are here termed ``anti-gap semiconductors\".\nThey have been theoretically proposed to exhibit excellent optoelectronic\nproperties because of their strong tolerance to defects. However, no anti-gap\nsemiconductors have been identified so far, despite a known list of\nsemiconductors with an antibonding VBM. Here, we use high-throughput\ncomputation to identify over 100 anti-gap semiconductors. From this group, we\nanalyze the transition metal dichalcogenide MX$_2$ (M=Hf, Zr; X=S, Se) family\nin detail. In addition to verifying their defect tolerance for both electrons\nand holes using first-principles simulations, we also discovered that\nphotoexcitation of charge carriers can lead to significant lattice stiffening\nand increased thermal conductivity in anti-gap semiconductors, which can be\npotentially used as photo-driven thermal switches. Our work analyzes the\nformation of the anti-gap electronic structure and showcases their unusual\nphotoinduced lattice dynamics that can have a potential impact on their\nphotophysical applications.",
        "This article unpacks the design choices behind longstanding and newly\nproposed computational frameworks aimed at finding common grounds across\ncollective preferences and examines their potential future impacts, both\ntechnically and normatively. It begins by situating AI-assisted preference\nelicitation within the historical role of opinion polls, emphasizing that\npreferences are shaped by the decision-making context and are seldom\nobjectively captured. With that caveat in mind, we explore AI-facilitated\ncollective judgment as a discovery tool for fostering reasonable\nrepresentations of a collective will, sense-making, and agreement-seeking. At\nthe same time, we caution against dangerously misguided uses, such as enabling\nbinding decisions, fostering gradual disempowerment or post-rationalizing\npolitical outcomes.",
        "In this work, a comprehensive analysis of the mass spectra and decay\nproperties of bottomonium states using a relativistic screened potential model\nis carried out. The mass spectrum, decay constants, $E1$ transitions, $M1$\ntransitions, and annihilation decay widths are evaluated. The interpretation of\n$\\Upsilon(10355)$, $\\Upsilon(10580)$,$\\Upsilon(10860)$, and $\\Upsilon(11020)$\nas $S-D$ mixed bottomonium states are analysed. The $\\Upsilon(10355)$ state is\nconsidered to be $3S-2D$, $\\Upsilon(10580)$ state is considered to be $4S-3D$\nmixed state, the $\\Upsilon(10753)$ is obtained as purely $\\Upsilon_{1}(3D)$\nbottomonium state, and the $\\Upsilon(10860)$ and $\\Upsilon(11020)$ are deemed\nto be $5S-4D$ mixed states.",
        "Harmonic average method has been widely utilized to deal with heterogeneous\ncoefficients in solving differential equations. One remarkable advantage of the\nharmonic averaging method is that no derivative of the coefficient is needed.\nFurthermore, the coefficient matrix of the finite difference equations is an\nM-matrix which guarantees the stability of the algorithm. It has been\nnumerically observed but not theoretically proved that the method produces\nsecond order pointwise accuracy when the solution and flux are continuous even\nif the coefficient has finite discontinuities for which the method is\ninconsistent ($O(1)$ in the local truncation errors). It has been believed that\nthere are some fortunate error cancellations. The harmonic average method does\nnot converge when the solution or the flux has finite discontinuities. In this\npaper, not only we rigorously prove the second order convergence of the\nharmonic averaging method for one-dimensional interface problem when the\ncoefficient has a finite discontinuities and the solution and the flux are\ncontinuous, but also proposed an {\\em improved harmonic average method} that is\nalso second order accurate (in the $L^{\\infty}$ norm), which allows\ndiscontinuous solutions and fluxes along with the discontinuous coefficients.\nThe key in the convergence proof is the construction of the Green's function.\nThe proof shows how the error cancellations occur in a subtle way. Numerical\nexperiments in both 1D and 2D confirmed the theoretical proof of the improved\nharmonic average method.",
        "Radar ensures robust sensing capabilities in adverse weather conditions, yet\nchallenges remain due to its high inherent noise level. Existing radar odometry\nhas overcome these challenges with strategies such as filtering spurious\npoints, exploiting Doppler velocity, or integrating with inertial measurements.\nThis paper presents two novel improvements beyond the existing radar-inertial\nodometry: ground-optimized noise filtering and continuous velocity\npreintegration. Despite the widespread use of ground planes in LiDAR odometry,\nimprecise ground point distributions of radar measurements cause naive plane\nfitting to fail. Unlike plane fitting in LiDAR, we introduce a zone-based\nuncertainty-aware ground modeling specifically designed for radar. Secondly, we\nnote that radar velocity measurements can be better combined with IMU for a\nmore accurate preintegration in radar-inertial odometry. Existing methods often\nignore temporal discrepancies between radar and IMU by simplifying the\ncomplexities of asynchronous data streams with discretized propagation models.\nTackling this issue, we leverage GP and formulate a continuous preintegration\nmethod for tightly integrating 3-DOF linear velocity with IMU, facilitating\nfull 6-DOF motion directly from the raw measurements. Our approach demonstrates\nremarkable performance (less than 1% vertical drift) in public datasets with\nmeticulous conditions, illustrating substantial improvement in elevation\naccuracy. The code will be released as open source for the community:\nhttps:\/\/github.com\/wooseongY\/Go-RIO.",
        "Texture recognition is a fundamental problem in computer vision and pattern\nrecognition. Recent progress leverages feature aggregation into discriminative\ndescriptions based on convolutional neural networks (CNNs). However, modeling\nnon-local context relations through visual primitives remains challenging due\nto the variability and randomness of texture primitives in spatial\ndistributions. In this paper, we propose a graph-enhanced texture encoding\nnetwork (GraphTEN) designed to capture both local and global features of\ntexture primitives. GraphTEN models global associations through fully connected\ngraphs and captures cross-scale dependencies of texture primitives via\nbipartite graphs. Additionally, we introduce a patch encoding module that\nutilizes a codebook to achieve an orderless representation of texture by\nencoding multi-scale patch features into a unified feature space. The proposed\nGraphTEN achieves superior performance compared to state-of-the-art methods\nacross five publicly available datasets.",
        "The influence of isotope differences on ion charge state yield ratios has\nnever been studied in detail, having been considered negligible. However, we\nhave observed anomalous ion charge state distributions in the thermalization of\nenergetic atomic ions in helium gas; the charge state distributions varied\nbetween not only isotopes but also between nuclear states within the same\nnuclide. The magnitude of the observed results suggests that this anomaly is a\nuniversal phenomenon that cannot be explained by the framework of the known\nisotope effects. Nuclear spin and deformation could be key to unraveling this,\nbut the mechanisms remain an open question.",
        "Quality statistical inference requires a sufficient amount of data, which can\nbe missing or hard to obtain. To this end, prediction-powered inference has\nrisen as a promising methodology, but existing approaches are largely limited\nto Z-estimation problems such as inference of means and quantiles. In this\npaper, we apply ideas of prediction-powered inference to e-values. By doing so,\nwe inherit all the usual benefits of e-values -- such as anytime-validity,\npost-hoc validity and versatile sequential inference -- as well as greatly\nexpand the set of inferences achievable in a prediction-powered manner. In\nparticular, we show that every inference procedure that can be framed in terms\nof e-values has a prediction-powered counterpart, given by our method. We\nshowcase the effectiveness of our framework across a wide range of inference\ntasks, from simple hypothesis testing and confidence intervals to more involved\nprocedures for change-point detection and causal discovery, which were out of\nreach of previous techniques. Our approach is modular and easily integrable\ninto existing algorithms, making it a compelling choice for practical\napplications.",
        "This paper studies the ubiquitous problem of liquidating large quantities of\nhighly correlated stocks, a task frequently encountered by institutional\ninvestors and proprietary trading firms. Traditional methods in this setting\nsuffer from the curse of dimensionality, making them impractical for\nhigh-dimensional problems. In this work, we propose a novel method based on\nstochastic optimal control to optimally tackle this complex multidimensional\nproblem. The proposed method minimizes the overall execution shortfall of\nhighly correlated stocks using a reinforcement learning approach. We rigorously\nestablish the convergence of our optimal trading strategy and present an\nimplementation of our algorithm using intra-day market data.",
        "Artificial intelligence (AI) and machine learning (ML) have made tremendous\nadvancements in the past decades. From simple recommendation systems to more\ncomplex tumor identification systems, AI\/ML systems have been utilized in a\nplethora of applications. This rapid growth of AI\/ML and its proliferation in\nnumerous private and public sector applications, while successful, has also\nopened new challenges and obstacles for regulators. With almost little to no\nhuman involvement required for some of the new decision-making AI\/ML systems,\nthere is now a pressing need to ensure the responsible use of these systems.\nParticularly in federal government use-cases, the use of AI technologies must\nbe carefully governed by appropriate transparency and accountability\nmechanisms. This has given rise to new interdisciplinary fields of AI research\nsuch as \\textit{Responsible AI (RAI)}. In this position paper we provide a\nbrief overview of development in RAI and discuss some of the motivating\nprinciples commonly explored in the field. An overview of the current\nregulatory landscape relating to AI is also discussed with analysis of\ndifferent Executive Orders, policies and frameworks. We then present examples\nof how federal agencies are aiming for the responsible use of AI, specifically\nwe present use-case examples of different projects and research from the Census\nBureau on implementing the responsible use of AI. We also provide a brief\noverview for a Responsible AI Assessment Toolkit currently under-development\naimed at helping federal agencies operationalize RAI principles. Finally, a\nrobust discussion on how different policies\/regulations map to RAI principles,\nalong with challenges and opportunities for regulation\/governance of\nresponsible AI within the federal government is presented.",
        "In recent years, rapid technological advancements and expanded Internet\naccess have led to a significant rise in anomalies within network traffic and\ntime-series data. Prompt detection of these irregularities is crucial for\nensuring service quality, preventing financial losses, and maintaining robust\nsecurity standards. While machine learning algorithms have shown promise in\nachieving high accuracy for anomaly detection, their performance is often\nconstrained by the specific conditions of their training data. A persistent\nchallenge in this domain is the scarcity of labeled data for anomaly detection\nin time-series datasets. This limitation hampers the training efficacy of both\ntraditional machine learning and advanced deep learning models. To address\nthis, unsupervised transfer learning emerges as a viable solution, leveraging\nunlabeled data from a source domain to identify anomalies in an unlabeled\ntarget domain. However, many existing approaches still depend on a small amount\nof labeled data from the target domain. To overcome these constraints, we\npropose a transfer learning-based model for anomaly detection in multivariate\ntime-series datasets. Unlike conventional methods, our approach does not\nrequire labeled data in either the source or target domains. Empirical\nevaluations on novel intrusion detection datasets demonstrate that our model\noutperforms existing techniques in accurately identifying anomalies within an\nentirely unlabeled target domain.",
        "Programmers spend a significant amount of time reading code during the\nsoftware development process. This trend is amplified by the emergence of large\nlanguage models (LLMs) that automatically generate code. However, little is\nknown about the readability of the LLM-generated code and whether it is still\nimportant from practitioners' perspectives in this new era. In this paper, we\nconduct a survey to explore the practitioners' perspectives on code readability\nin the age of LLMs and investigate the readability of our LLM-based software\ndevelopment agents framework, HULA, by comparing its generated code with\nhuman-written code in real-world scenarios. Overall, the findings underscore\nthat (1) readability remains a critical aspect of software development; (2) the\nreadability of our LLM-generated code is comparable to human-written code,\nfostering the establishment of appropriate trust and driving the broad adoption\nof our LLM-powered software development platform.",
        "This study examines the performance of two flapping flat-plate foils\ninteracting with each other while generating thrust at a Reynolds number of 800\nthrough two-dimensional numerical simulations. These fluid dynamics simulations\nwere conducted with a commercial computational fluid dynamics solver\nimplementing a finite-volume method and an overset mesh capability. The foils\nperformed a combined motion involving pitching, heaving, and deviation. Both\nfoils exhibit similar movements, with one foil mirroring the other. The heaving\nand pitching motions occur at the same frequency but with a phase shift between\nthem. The effects of varying the phase shift and the minimum spacing between\nthe foils during motion were first explored. The study revealed that a maximum\nefficiency of 0.542 can be achieved by using two foils, representing an\nincrease of 13.5% relative to the optimal single-foil case. Then, the impacts\nof the deviation motion were investigated. The deviation motion was introduced\nwith a frequency twice as fast as the other motions, and a phase shift relative\nto the heaving motion. The other parameters such as the minimum spacing between\nthe foils, the heaving and pitching amplitudes, and the frequency were those of\nthe best configuration without deviation. The numerical simulations\ndemonstrated that deviation can increase efficiency further to a value of\n0.560, a relative increase of 3.95%.",
        "In the RC$^\\star$ collaboration, we simulate lattice QCD+QED using\n$C-$periodic spatial boundary conditions to ensure that locality, gauge\ninvariance, and translational invariance are preserved throughout the\ncalculation. We present our progress in computing isospin-breaking (IB)\ncorrections to the leading hadronic contribution to $(g-2)_\\mu$. We compare two\nways of including the IB corrections: the RM123 method and dynamical QCD+QED\nsimulations, both with $C-$periodic boundary conditions. The two calculations\nare performed at $\\beta=3.24$ with four flavours of $\\mathcal{O}(a)-$improved\nWilson fermions; the QCD ensemble features $SU(3)-$symmetric sea quarks plus\ncharm, while down and strange quarks are degenerate in QCD+QED gauge ensembles.",
        "Multimodal Large Language Models (MLLMs) have shown significant advancements,\nproviding a promising future for embodied agents. Existing benchmarks for\nevaluating MLLMs primarily utilize static images or videos, limiting\nassessments to non-interactive scenarios. Meanwhile, existing embodied AI\nbenchmarks are task-specific and not diverse enough, which do not adequately\nevaluate the embodied capabilities of MLLMs. To address this, we propose\nEmbodiedEval, a comprehensive and interactive evaluation benchmark for MLLMs\nwith embodied tasks. EmbodiedEval features 328 distinct tasks within 125 varied\n3D scenes, each of which is rigorously selected and annotated. It covers a\nbroad spectrum of existing embodied AI tasks with significantly enhanced\ndiversity, all within a unified simulation and evaluation framework tailored\nfor MLLMs. The tasks are organized into five categories: navigation, object\ninteraction, social interaction, attribute question answering, and spatial\nquestion answering to assess different capabilities of the agents. We evaluated\nthe state-of-the-art MLLMs on EmbodiedEval and found that they have a\nsignificant shortfall compared to human level on embodied tasks. Our analysis\ndemonstrates the limitations of existing MLLMs in embodied capabilities,\nproviding insights for their future development. We open-source all evaluation\ndata and simulation framework at https:\/\/github.com\/thunlp\/EmbodiedEval."
      ]
    }
  },
  {
    "id":2412.00868,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"How resilient are language models to text perturbations",
    "start_abstract":"Large language models typically rely on highly curated datasets that lack common irregularities such as typos and contractions, resulting in a mismatch between their training environments and real-world applications. This study evaluates the resilience of four prominent models in five different NLP tasks when confronted with perturbed inputs. We investigate three categories of perturbations: character-level, word-level and miscellaneous perturbations. By comparing performance on original and altered datasets, our results reveal a significant sensitivity to input perturbations across all models, with varying degrees of vulnerability depending on both the specific task and the type of perturbation. In particular, the XLNet model consistently shows superior robustness, while tasks involving grammatical coherence are most adversely affected.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b22"
      ],
      "title":[
        "Context-Aware Testing: A New Paradigm for Model Testing with Large\n  Language Models"
      ],
      "abstract":[
        "The predominant de facto paradigm of testing ML models relies on either using\nonly held-out data to compute aggregate evaluation metrics or by assessing the\nperformance on different subgroups. However, such data-only testing methods\noperate under the restrictive assumption that the available empirical data is\nthe sole input for testing ML models, disregarding valuable contextual\ninformation that could guide model testing. In this paper, we challenge the\ngo-to approach of data-only testing and introduce context-aware testing (CAT)\nwhich uses context as an inductive bias to guide the search for meaningful\nmodel failures. We instantiate the first CAT system, SMART Testing, which\nemploys large language models to hypothesize relevant and likely failures,\nwhich are evaluated on data using a self-falsification mechanism. Through\nempirical evaluations in diverse settings, we show that SMART automatically\nidentifies more relevant and impactful failures than alternatives,\ndemonstrating the potential of CAT as a testing paradigm."
      ],
      "categories":[
        "stat.ME"
      ]
    },
    "list":{
      "title":[
        "Qmod: Expressive High-Level Quantum Modeling",
        "On almost Gallai colourings in complete graphs",
        "Assessment of spectral phases of non-Hermitian quantum systems through\n  complex and singular values",
        "Bifurcations and stability of synchronized solutions in the Kuramoto\n  model with uniformly spaced natural frequencies",
        "Construction of exact refinements for the two-dimensional HB\/THB-spline\n  de Rham complex",
        "AI-powered virtual tissues from spatial proteomics for clinical\n  diagnostics and biomedical discovery",
        "Exact calculation of spectral properties of a particle interacting with\n  a one-dimensional Fermi gas in optical lattices",
        "Independent transversal blow-up of graphs",
        "WISDOM Project -- XXII. A 5% precision CO-dynamical supermassive black\n  hole mass measurement in the galaxy NGC 383",
        "Effects of Ru-doping on the magnetism of Ag3LiIr2O6, a candidate Kitaev\n  quantum spin liquid",
        "A Bayesian Record Linkage Approach to Applications in Tree Demography\n  Using Overlapping LiDAR Scans",
        "Multipoint stress mixed finite element methods for elasticity on cuboid\n  grids",
        "$q$-deformation of random partitions, determinantal structure, and\n  Riemann-Hilbert problem",
        "Efficient parameter inference in networked dynamical systems via steady\n  states: A surrogate objective function approach integrating mean-field and\n  nonlinear least squares",
        "A note on Arveson's hyperrigidity and non-degenerate C*-correspondences",
        "Explicit polynomial bounds on Dehn functions of subgroups of hyperbolic\n  groups",
        "Transfer Learning for Individualized Treatment Rules: Application to\n  Sepsis Patients Data from eICU-CRD and MIMIC-III Databases",
        "Two-Loop Master Integrals for Mixed QCD-EW Corrections to $gg \\to H$\n  Through $\\mathcal{O}(\\epsilon^2)$",
        "Towards Efficient PCSEL Design: A Data-Driven Approach for Design\n  Insights",
        "FOGGIE X: Characterizing the Small-Scale Structure of the CGM and its\n  Imprint on Observables",
        "A new Lagrangian approach to optimal control of second-order systems",
        "On Bass' conjecture of the small Davenport constant",
        "Individual causal effect estimation accounting for latent disease state\n  modification among bipolar participants in mobile health studies",
        "Recent Developments in Stochastic Inflation",
        "Engineering nonlinear Hall effect in bilayer graphene\/black phosphorus\n  heterostructures",
        "Matrix Time Series Modeling: A Hybrid Framework Combining Autoregression\n  and Common Factors",
        "Model-based time super-sampling of turbulent flow field sequences",
        "Transverse Nucleon Single-Spin Asymmetry for Single-Inclusive Hadron and\n  Jet Production at NLO Accuracy",
        "Revisiting the Extraction of Coupling Strength for Polaron Hopping from\n  $ab~initio$ Approach"
      ],
      "abstract":[
        "Quantum computing hardware is advancing at a rapid pace, yet the lack of\nhigh-level programming abstractions remains a serious bottleneck in the\ndevelopment of new applications. Widely used frameworks still rely on\ngate-level circuit descriptions, causing the algorithm's functional intent to\nbecome lost in low-level implementation details, and hindering flexibility and\nreuse. While various high-level quantum programming languages have emerged in\nrecent years - offering a significant step toward higher abstraction - many\nstill lack support for classical-like expression syntax, and native constructs\nfor useful quantum algorithmic idioms. This paper presents Qmod, a high-level\nquantum programming language designed to capture algorithmic intent in natural\nterms while delegating implementation decisions to automation. Qmod introduces\nquantum numeric variables and expressions, including digital fixed-point\narithmetic tuned for compact representations and optimal resource usage. Beyond\ndigital encoding, Qmod also supports non-digital expression modes - phase and\namplitude encoding - frequently exploited by quantum algorithms to achieve\ncomputational advantages. We describe the language's constructs, demonstrate\npractical usage examples, and outline future work on evaluating Qmod across a\nbroader set of use cases.",
        "For $t \\in \\mathbb{N}$, we say that a colouring of $E(K_n)$ is\n$\\textit{almost}$ $t$-$\\textit{Gallai}$ if no two rainbow $t$-cliques share an\nedge. Motivated by a lemma of Berkowitz on bounding the modulus of the\ncharacteristic function of clique counts in random graphs, we study the maximum\nnumber $\\tau_t(n)$ of rainbow $t$-cliques in an almost $t$-Gallai colouring of\n$E(K_n)$. For every $t \\ge 4$, we show that $n^{2-o(1)} \\leq \\tau_t(n) =\no(n^2)$. For $t=3$, surprisingly, the behaviour is substantially different. Our\nmain result establishes that $$\\left ( \\frac{1}{2}-o(1) \\right ) n\\log n \\le\n\\tau_3(n) = O\\big (n^{\\sqrt{2}}\\log n \\big ),$$ which gives the first\nnon-trivial improvements over the simple lower and upper bounds. Our proof\ncombines various applications of the probabilistic method and a generalisation\nof the edge-isoperimetric inequality for the hypercube.",
        "Chaotic behavior or lack thereof in non-Hermitian systems is often diagnosed\nvia spectral analysis of associated complex eigenvalues. Very recently,\nsingular values of the associated non-Hermitian systems have been proposed as\nan effective measure to study dissipative quantum chaos. Motivated by the rich\nproperties of non-Hermitian power-law banded random matrices and its promise as\na platform to study localized and delocalized phases in non-Hermitian systems,\nwe make an in-depth study to assess different spectral phases of these matrices\nthrough the lens of both complex eigenvalues and singular values. Remarkably,\nthe results from complex spectra and singular value analysis are seemingly\ndifferent, thereby necessitating caution while identifying different phases. We\nalso exemplify our findings by studying a non-Hermitian Hamiltonian with a\ncomplex on-site disorder. Our work indicates that systems, where disorder is\npresent both in the Hermitian and non-Hermitian segments of a Hamiltonian, are\nsensitive to the specific diagnostic tool that needs to be employed to study\nquantum chaos.",
        "We consider the classical Kuramoto model (KM) with natural frequencies and\nits continuum limit (CL), and discuss the existence of synchronized solutions\nand their bifurcations and stability. We specifically assume that the frequency\nfunction is symmetric and linear in the CL, so that the natural frequencies are\nevenly spaced in the KM. We show that in the KM, $O(2^n)$ one-parameter\nfamilies of synchronized solutions are born and $O(2^n)$ {saddle-node and}\npitchfork bifurcations occur at least, when the node number $n$ is odd and\ntends to infinity. Moreover, we prove that the family of synchronized solutions\nobtained in the previous work suffers a saddle-node bifurcation at which its\nstability changes from asymptotically stable to unstable and the other families\nof synchronized solutions are unstable in the KM. For the CL, we show that the\none-parameter family of synchronized solutions obtained in the previous work is\nthe only continuous one and there exist uncountably many one-parameter families\nof noncontinuous synchronized solutions and that the former is asymptotically\nstable and the latter are unstable.",
        "Studying the de Rham complex is a natural choice when working with problems\nin electromagnetics and fluid mechanics. By discretizing the complex correctly,\nit is possible to attain stable numerical methods to tackle these problems. An\nimportant consideration when constructing the discrete complex is that it must\npreserve the cohomology structure of the original one. This property is not\nguaranteed when the discrete function spaces chosen are hierarchical B-splines.\nResearch shows that a poor choice of refinement domains may give rise to\nspurious harmonic forms that ruin the accuracy of solutions, even for the\nsimplest partial differential equations. Another crucial aspect to consider in\nthe hierarchical setting is the notion of admissibility, as it is possible to\nobtain optimal convergence rates of numerical solutions by limiting the\nmulti-level interaction of basis functions. We will focus on the\ntwo-dimensional de Rham complex over the unit square $\\Omega \\subseteq\n\\mathbb{R}^2$. In this scenario, the discrete de Rham complex should be exact,\nand we provide both the theoretical and the algorithm-implementation framework\nto ensure this is the case. Moreover, we show that, under a common restriction,\nthe admissibility class of the first space of the discrete complex persists\nthroughout the remaining spaces. Finally, we include numerical results that\nmotivate the importance of the previous concerns for the vector Laplace and\nMaxwell eigenvalue problems.",
        "Spatial proteomics technologies have transformed our understanding of complex\ntissue architectures by enabling simultaneous analysis of multiple molecular\nmarkers and their spatial organization. The high dimensionality of these data,\nvarying marker combinations across experiments and heterogeneous study designs\npose unique challenges for computational analysis. Here, we present Virtual\nTissues (VirTues), a foundation model framework for biological tissues that\noperates across the molecular, cellular and tissue scale. VirTues introduces\ninnovations in transformer architecture design, including a novel tokenization\nscheme that captures both spatial and marker dimensions, and attention\nmechanisms that scale to high-dimensional multiplex data while maintaining\ninterpretability. Trained on diverse cancer and non-cancer tissue datasets,\nVirTues demonstrates strong generalization capabilities without task-specific\nfine-tuning, enabling cross-study analysis and novel marker integration. As a\ngeneralist model, VirTues outperforms existing approaches across clinical\ndiagnostics, biological discovery and patient case retrieval tasks, while\nproviding insights into tissue function and disease mechanisms.",
        "By using the exact Bethe wavefunctions of the one-dimensional Hubbard model\nwith $N$ spin-up fermions and one spin-down impurity, we derive an analytic\nexpression of the impurity form factor, in the form of a determinant of a\n$(N+1)$ by $(N+1)$ matrix. This analytic expression enables us to exactly\ncalculate spectral properties of one-dimensional Fermi polarons in lattices,\nwhen the masses of the impurity particle and the Fermi bath are equal. We\npresent the impurity spectral function as functions of the on-site interaction\nstrength and the filling factor of the Fermi bath, and discuss the origin of\nFermi singularities in the spectral function at small momentum and the\nemergence of polaron quasiparticles at large momentum near the boundary of\nBrillouin zone. Our analytic expression of the impurity form factors pave the\nway to exploring the intriguing dynamics of a particle interacting with a Fermi\nbath. Our exact predictions on the impurity spectral function could be directly\nexamined in cold-atom laboratories by using the radio-frequency spectroscopy\nand Ramsey spectroscopy.",
        "In an $r$-partite graph, an independent transversal of size $s$ (ITS)\nconsists of $s$ vertices from each part forming an independent set. Motivated\nby a question from Bollob\\'as, Erd\\H{o}s, and Szemer\\'edi (1975), Di Braccio\nand Illingworth (2024) inquired about the minimum degree needed to ensure an $n\n\\times \\cdots \\times n$ $r$-partite graph contains $K_r(s)$, a complete\n$r$-partite graph with $s$ vertices in each part. We reformulate this as\nfinding the smallest $n$ such that any $n \\times \\cdots \\times n$ $r$-partite\ngraph with maximum degree $\\Delta$ has an ITS. For any $\\varepsilon>0$, we\nprove the existence of a $\\gamma>0$ ensuring that if $G$ is a multipartite\ngraph partitioned as $(V_1, V_2, \\ldots, V_r)$, where the average degree of\neach part $V_i$ is at most $D$, the maximum degree of any vertex to any part\n$V_i$ is at most $\\gamma D$, and the size of each part $V_i$ is at least $(s +\n\\varepsilon)D$, then $G$ possesses an ITS. The constraint $(s + \\varepsilon)D$\non the part size is tight. This extends results of Loh and Sudakov (2007),\nGlock and Sudakov (2022), and Kang and Kelly (2022). We also show that any $n\n\\times \\cdots \\times n$ $r$-partite graph with minimum degree at least\n$\\left(r-1-\\frac{1}{2s^2}\\right)n$ contains $K_r(s)$ and provide a relative\nTur\\'an-type result. Additionally, this paper explores counting ITSs in\nmultipartite graphs.",
        "We present a measurement of the supermassive black hole (SMBH) mass of the\nnearby lenticular galaxy NGC 383, based on Atacama Large\nMillimeter\/sub-millimeter Array (ALMA) observations of the $^{12}$CO(2-1)\nemission line with an angular resolution of $0.''050\\times0.''024$\n($\\approx16\\times8$ pc$^2$). These observations spatially resolve the nuclear\nmolecular gas disc down to $\\approx41,300$ Schwarzschild radii and the SMBH\nsphere of influence by a factor of $\\approx24$ radially, better than any other\nSMBH mass measurement using molecular gas to date. The high resolution enables\nus to probe material with a maximum circular velocity of $\\approx1040$ km\/s,\neven higher than those of the highest-resolution SMBH mass measurements using\nmegamasers. We detect a clear Keplerian increase (from the outside in) of the\nline-of-sight rotation velocities, a slight offset between the gas disc\nkinematic (i.e. the position of the SMBH) and morphological (i.e. the centre of\nthe molecular gas emission) centres, an asymmetry of the innermost rotation\nvelocity peaks and evidence for a mild position angle warp and\/or non-circular\nmotions within the central $\\approx0.''3$. By forward modelling the mass\ndistribution and ALMA data cube, we infer a SMBH mass of\n$(3.58\\pm0.19)\\times10^9$ M$_\\odot$ ($1\\sigma$ confidence interval), more\nprecise ($5\\%$) but consistent within $\\approx1.4\\sigma$ with the previous\nmeasurement using lower-resolution molecular gas data. Our measurement\nemphasises the importance of high spatial resolution observations for precise\nSMBH mass determinations.",
        "We report our investigations on Ag3LiIr1.4Ru0.6O6, which results from the Ru\nsubstitution in the Kitaev quantum spin liquid candidate Ag3LiIr2O6. It\ncrystallizes in the monoclinic C2\/m space group like its parent compound,\nAg3LiIr2O6. Our susceptibility measurements reveal an effective moment = 2.6\nmuB, which is higher than the moments of the parent compound and less than that\nof the Ru-analog (Ag3LiRu2O6), suggesting the presence of magnetic Ir4+ (Jeff=\n1\/2) and Ru4+ (S=1). Bulk magnetic susceptibility suggests long-range order\n(LRO)at T~20 K, whereas no clear signature is present in the heat capacity.\nLikewise, there is a loss of the 7Li NMR spectral intensity around T~20 K as\nexpected at the onset of LRO, but a complete wipe-out is not seen in contrast\nto the result in Ag3LiIr2O6. There is also a T~20 K anomaly in the 7Li NMR\nrelaxation rate and also a fall in the 7Li NMR shift with decreasing\ntemperature. These results suggest LRO at T~20 K in Ag3LiIr1.4Ru0.6O6. However,\nat low-T below 10 K, we observe a power law variation in magnetic heat capacity\nand spin lattice relaxation rate, temperature-independent-7K, and no further\nloss of the 7Li NMR spectral intensity. These results might suggest the\npersistence or stabilisation of a quantum spin liquid-like phase, perhaps from\na fraction of the sample in Ag3LiIr1.4Ru0.6O6 below 10 K. Our muon spin\nrelaxation measurements suggest ordering around 20 K, consistent with our other\nprobes. It appears that the main effect of Ru-substitution is to shift the LRO\nto a higher temperature in comparison with Ag3LiIr2O6, though there are\nsignatures of a novel phase below about 10 K.",
        "In the information age, it has become increasingly common for data containing\nrecords about overlapping individuals to be distributed across multiple\nsources, making it necessary to identify which records refer to the same\nindividual. The goal of record linkage is to estimate this unknown structure in\nthe absence of a unique identifiable attribute. We introduce a Bayesian\nhierarchical record linkage model for spatial location data motivated by the\nestimation of individual specific growth-size curves for conifer species using\ndata derived from overlapping LiDAR scans. Annual tree growth may be estimated\ndependent upon correctly identifying unique individuals across scans in the\npresence of noise. We formalize a two-stage modeling framework, connecting the\nrecord linkage model and a flexible downstream individual tree growth model,\nthat provides robust uncertainty quantification and propagation through both\nstages of the modeling pipeline via an extension of the linkage-averaging\napproach of Sadinle (2018). In this paper, we discuss the two-stage model\nformulation, outline the computational strategies required to achieve\nscalability, assess the model performance on simulated data, and fit the model\nto a bi-temporal dataset derived from LiDAR scans of the Upper Gunnison\nWatershed provided by the Rocky Mountain Biological Laboratory to assess the\nimpact of key topographic covariates on the growth behavior of conifer species\nin the Southern Rocky Mountains (USA).",
        "We develop multipoint stress mixed finite element methods for linear\nelasticity with weak stress symmetry on cuboid grids, which can be reduced to a\nsymmetric and positive definite cell-centered system. The methods employ the\nlowest-order enhanced Raviart-Thomas finite element space for the stress and\npiecewise constant displacement. The vertex quadrature rule is employed to\nlocalize the interaction of stress degrees of freedom, enabling local stress\nelimination around each vertex. We introduce two methods. The first method uses\na piecewise constant rotation, resulting in a cell-centered system for the\ndisplacement and rotation. The second method employs a continuous piecewise\ntrilinear rotation and the vertex quadrature rule for the asymmetry bilinear\nforms, allowing for further elimination of the rotation and resulting in a\ncell-centered system for the displacement only. Stability and error analysis is\nperformed for both methods. For the stability analysis of the second method, a\nnew auxiliary H-curl conforming matrix-valued space is constructed, which forms\nan exact sequence with the stress space. A matrix-matrix inf-sup condition is\nshown for the curl of this auxiliary space and the trilinear rotation space.\nFirst-order convergence is established for all variables in their natural\nnorms, as well as second-order superconvergence of the displacement at the cell\ncenters. Numerical results are presented to verify the theory.",
        "We study $q$-deformation of the probability measure on partitions, i.e.,\n$q$-deformed random partitions. We in particular consider the $q$-Plancherel\nmeasure and show a determinantal formula for the correlation function using a\n$q$-deformation of the discrete Bessel kernel. We also investigate\nRiemann-Hilbert problems associated with the corresponding orthogonal\npolynomials and obtain $q$-Painlev{\\'e} equations from the $q$-difference Lax\nformalism.",
        "In networked dynamical systems, inferring governing parameters is crucial for\npredicting nodal dynamics, such as gene expression levels, species abundance,\nor population density. While many parameter estimation techniques rely on\ntime-series data, particularly systems that converge over extreme time ranges,\nonly noisy steady-state data is available, requiring a new approach to infer\ndynamical parameters from noisy observations of steady states. However, the\ntraditional optimization process is computationally demanding, requiring\nrepeated simulation of coupled ordinary differential equations (ODEs). To\novercome these limitations, we introduce a surrogate objective function that\nleverages decoupled equations to compute steady states, significantly reducing\ncomputational complexity. Furthermore, by optimizing the surrogate objective\nfunction, we obtain steady states that more accurately approximate the ground\ntruth than noisy observations and predict future equilibria when topology\nchanges. We empirically demonstrate the effectiveness of the proposed method\nacross ecological, gene regulatory, and epidemic networks. Our approach\nprovides an efficient and effective way to estimate parameters from\nsteady-state data and has the potential to improve predictions in networked\ndynamical systems.",
        "We revisit the results of Kim, and of Katsoulis and Ramsey concerning\nhyperrigidity for non-degenerate C*-correspondences. We show that the tensor\nalgebra is hyperrigid, if and only if Katsura's ideal acts non-degenerately, if\nand only if Katsura's ideal acts non-degenerately under any representation.\nThis gives a positive answer to the question of Katsoulis and Ramsey, showing\nthat their necessary condition and their sufficient condition for hyperrigidity\nof the tensor algebra are equivalent. Non-degeneracy of the left action of\nKatsura's ideal was also shown by Kim to be equivalent to hyperrigidity for the\nselfadjoint operator space associated with the C*-correspondence, and our\napproach provides a simplified proof of this result as well. In the process we\nrevisit Arveson's criterion connecting maximality with the unique extension\nproperty and hyperrigidity, in conjunction with the work of Salomon on\ngenerating sets.",
        "In 1999 Brady constructed the first example of a non-hyperbolic finitely\npresented subgroup of a hyperbolic group by fibring a non-positively curved\ncube complex over the circle. We show that his example has Dehn function\nbounded above by $n^{96}$. This provides the first explicit polynomial upper\nbound on the Dehn function of a finitely presented non-hyperbolic subgroup of a\nhyperbolic group. We also determine the precise hyperbolicity constant for the\n$1$-skeleton of the universal cover of the cube complex in Brady's construction\nwith respect to the $4$-point condition for hyperbolicity.",
        "Modern precision medicine aims to utilize real-world data to provide the best\ntreatment for an individual patient. An individualized treatment rule (ITR)\nmaps each patient's characteristics to a recommended treatment scheme that\nmaximizes the expected outcome of the patient. A challenge precision medicine\nfaces is population heterogeneity, as studies on treatment effects are often\nconducted on source populations that differ from the populations of interest in\nterms of the distribution of patient characteristics. Our research goal is to\nexplore a transfer learning algorithm that aims to address the population\nheterogeneity problem and obtain targeted, optimal, and interpretable ITRs. The\nalgorithm incorporates a calibrated augmented inverse probability weighting\n(CAIPW) estimator for the average treatment effect (ATE) and employs value\nfunction maximization for the target population using Genetic Algorithm (GA) to\nproduce our desired ITR. To demonstrate its practical utility, we apply this\ntransfer learning algorithm to two large medical databases, Electronic\nIntensive Care Unit Collaborative Research Database (eICU-CRD) and Medical\nInformation Mart for Intensive Care III (MIMIC-III). We first identify the\nimportant covariates, treatment options, and outcomes of interest based on the\ntwo databases, and then estimate the optimal linear ITRs for patients with\nsepsis. Our research introduces and applies new techniques for data fusion to\nobtain data-driven ITRs that cater to patients' individual medical needs in a\npopulation of interest. By emphasizing generalizability and personalized\ndecision-making, this methodology extends its potential application beyond\nmedicine to fields such as marketing, technology, social sciences, and\neducation.",
        "We consider mixed strong-electroweak corrections to Higgs production via\ngluon fusion, in which the Higgs boson couples to the top quark. Using the\nmethod of differential equations, we compute all of the master integrals that\ncontribute to this process at two loops through $\\mathcal{O}(\\epsilon^2)$ in\nthe dimensional regularization parameter $\\epsilon = (d-4)\/2$, keeping full\nanalytic dependence on the top quark, Higgs, W, and Z boson masses. We present\nthe results for these master integrals in terms of iterated integrals whose\nkernels depend on elliptic curves.",
        "We present a data-driven design approach for photonic crystals to achieve\nhigh efficiency in photonic crystal surface-emitting lasers (PCSELs). By\ndiscretizing the photonic crystal structure into a grid, we enable the\ngeneration of arbitrary lattice designs. Multiple fully connected layers\ncombined with a position embedding module extract essential features from the\nphotonic crystal designs, while coupled-wave theory (CWT) is used to evaluate\nthe efficiency (based on the ratio of surface-emitting to edge-emitting\nresonant) and quality factor Q. We introduce the Neural Networks (NNs) model to\nevaluate the structures, and to find a better performance design according to\nthe evaluation result. The model achieves high prediction accuracy, with\nPearson correlation coefficients of 0.780 for SEE and 0.887 for the\nlog-transformed Q. Additionally, we perform Shapley value analysis to identify\nthe most important Fourier coefficients, providing insights into the factors\nthat impact the performance of PCSEL designs. Our work speeds up the design\nprocess and offers valuable guidance for optimizing high-performance PCSELs,\nsupporting the development of fully photonic design automation (PDA).",
        "One of the main unknowns in galaxy evolution is how gas flows into and out of\ngalaxies in the circumgalactic medium (CGM). Studies observing the CGM in\nabsorption using multiple or extended background objects suggest a high degree\nof variation on relatively small ($\\lesssim 1$ kpc) spatial scales. Similarly,\nhigh-resolution simulations generally exhibit small-scale substructure in the\ngas around galaxies. We examine the small-scale structure of the $z = 1$ CGM\nusing simulations from the FOGGIE (Figuring Out Gas & Galaxies in Enzo)\nproject. We select gaseous substructures (\"clumps\") by their local overdensity\nand investigate their physical properties, including temperature, metallicity,\nand kinematics with respect to the galaxy and the nearby surroundings. FOGGIE\nresolves clumps down to sphericalized radii $R \\sim 0.25$ kpc at $z = 1$. The\ndistribution of clumps peaks at $\\sim 10^5$ $\\rm M_{\\odot}$ and $10^{4}$ K,\nconsistent with relatively condensed, cool gas with a slight preference for\ninflow-like velocities. Many clumps show internal temperature and density\nvariations, and thus internally varying ionization levels for key diagnostic\nions such as HI, MgII, and OVI. The average metallicity in clumps is about a\nfactor 1.5--2$\\times$ lower in metallicity than nearby gas, suggesting that the\nmetals are not well-mixed between structured and diffuse CGM, which may have\nimplications for observational metallicity estimations of dense CGM clouds. We\nestimate the survivability of CGM clumps and find that structures larger than\n0.5 kpc are generally long-lived. Finally, we qualitatively compare the\nsimulated cloud properties to Milky Way high-velocity clouds.",
        "In this work, we propose and study a new approach to formulate the optimal\ncontrol problem of second-order differential equations, with a particular\ninterest in those derived from force-controlled Lagrangian systems. The\nformulation results in a new hyperregular control Langrangian and, thus, a new\ncontrol Hamiltonian whose equations of motion provide necessary optimality\nconditions. We compare this approach to Pontryagin's maximum principle (PMP) in\nthis setting, providing geometric insight into their relation. This leads us to\ndefine an extended Tulczyjew's triple with controls. Moreover, we study the\nrelationship between Noether symmetries of this new formulation and those of\nthe PMP.",
        "Let $G$ be a finite group. The small Davenport constant $\\mathsf d(G)$ of $G$\nis the maximal integer $\\ell$ such that there is a sequence of length $\\ell$\nover $G$ which has no nonempty product-one subsequence. In 2007, Bass\nconjectured that $\\mathsf d(G_{m,n})=m+n-2$, where $G_{m,n}=\\langle x, y|\nx^m=y^n=1, x^{-1}yx=y^s\\rangle$, and $s$ has order $m$ modulo $n$. In this\npaper, we confirm the conjecture for any group $G_{m,n}$ with additional\nconditions that $s$ has order $m$ modulo $q$, for every prime divisor $q$ of\n$n$. Moreover, we solve the associated inverse problem characterizing the\nstructure of any product-one free sequence with extremal length $\\mathsf\nd(G_{m,n})$. Our results generalize some obtained theorems on this problem.",
        "Individuals with bipolar disorder tend to cycle through disease states such\nas depression and mania. The heterogeneous nature of disease across states\ncomplicates the evaluation of interventions for bipolar disorder patients, as\nvaried interventional success is observed within and across individuals. In\nfact, we hypothesize that disease state acts as an effect modifier for the\ncausal effect of a given intervention on health outcomes. To address this\ndilemma, we propose an N-of-1 approach using an adapted autoregressive hidden\nMarkov model, applied to longitudinal mobile health data collected from\nindividuals with bipolar disorder. This method allows us to identify a latent\nvariable from mobile health data to be treated as an effect modifier between\nthe exposure and outcome of interest while allowing for missing data in the\noutcome. A counterfactual approach is employed for causal inference and to\nobtain a g-formula estimator to recover said effect. The performance of the\nproposed method is compared with a naive approach across extensive simulations\nand application to a multi-year smartphone study of bipolar patients,\nevaluating the individual effect of digital social activity on sleep duration\nacross different latent disease states.",
        "This article is dedicated to the memory of Alexei Starobinsky. I begin with\nsome recollections of him and then review the generalization of his wonderful\nstochastic formalism from scalar potential models to theories which interact\nwith fermions and photons, and finally to theories with derivative interactions\nsuch as nonlinear sigma models and gravity. This entails effective potentials\ngenerated by the usual field-dependent masses, as well as by field-dependent\nfield strengths, and by field-dependent Hubble parameters. I also discuss\nsecular loop corrections which cannot be captured by stochastic techniques.",
        "Two-dimensional van der Waals materials offer a highly tunable platform for\ngenerating emergent quantum phenomena through symmetry breaking.\nStacking-induced symmetry breaking at interfaces provides an effective method\nto modulate their electronic properties for functional devices. Here, we\nstrategically stack bilayer graphene with black phosphorus, a low-symmetry\nsemiconductor, to break the symmetries and induce the nonlinear Hall effect\n(NLHE) that can persist up to room temperature. Intriguingly, it is found the\nNLHE undergoes sign reversals by varying the electrical displacement field\nunder fixed carrier density. The scaling analysis reveals that the sign\nreversal of the NLHE is contributed from both the Berry curvature dipole (BCD)\nand extrinsic scatterings. The displacement field-induced sign reversal of the\nBCD indicates asymmetric distributions of Berry curvature hot spots across\ndifferent Fermi pockets in bilayer graphene. Our findings suggest that symmetry\nengineering of van der Waals heterostructures is promising for room-temperature\napplications based on nonlinear quantum devices, such as high-frequency\nrectifiers and wireless charging.",
        "Matrix-valued time series analysis has gained prominence in econometrics and\nfinance due to the increasing availability of high-dimensional data with\ninherent matrix structures. Traditional approaches, such as Matrix\nAutoregressive (MAR) models and Dynamic Matrix Factor (DMF) models, often\nimpose restrictive assumptions that may not align with real-world data\ncomplexities. To address this gap, we propose a novel Matrix Autoregressive\nwith Common Factors (MARCF) model, which bridges the gap between MAR and DMF\nframeworks by introducing common bases between predictor and response\nsubspaces. The MARCF model achieves significant dimension reduction and enables\na more flexible and interpretable factor representation of dynamic\nrelationships. We develop a computationally efficient estimator and a gradient\ndescent algorithm. Theoretical guarantees for computational and statistical\nconvergence are provided, and extensive simulations demonstrate the robustness\nand accuracy of the model. Applied to a multinational macroeconomic dataset,\nthe MARCF model outperforms existing methods in forecasting and provides\nmeaningful insights into the interplay between countries and economic factors.",
        "We propose a novel method for model-based time super-sampling of turbulent\nflow fields. The key enabler is the identification of an empirical Galerkin\nmodel from the projection of the Navier-Stokes equations on a data-tailored\nbasis. The basis is obtained from a Proper Orthogonal Decomposition (POD) of\nthe measured fields. Time super-sampling is thus achieved by a time-marching\nintegration of the identified dynamical system, taking the original snapshots\nas initial conditions. Temporal continuity of the reconstructed velocity fields\nis achieved through a forward-backwards integration between consecutive\nmeasured Particle Image Velocimetry measurements of a turbulent jet flow. The\nresults are compared with the interpolation of the POD temporal coefficients\nand the low-order reconstruction of data measured at a higher sampling rate. In\nboth cases, the results obtained show the ability of the method to reconstruct\nthe dynamics of the flow with small errors during several flow characteristic\ntimes.",
        "We investigate the single-spin asymmetry for the single-inclusive production\nof hadrons and jets in collisions of transversely polarized nucleons and\nunpolarized leptons, $\\ell N^\\uparrow \\to (h\\,\\mathrm{or\\,jet})X$. We compute\nthe spin-dependent cross section within collinear twist-3 factorization in\nperturbative QCD at next-to-leading order (NLO) accuracy. In this approach,\nmultiparton correlations generate a non-vanishing effect. For the present\npaper, we focus on correlations in the nucleon initial-state rather than in the\nfragmentation process. We explicitly verify that collinear twist-3\nfactorization is valid at the one-loop level. Our analytical results show that\nat NLO the relevant multiparton correlation functions in the nucleon are probed\non their full support in momentum fractions. Our numerical analysis for\ncollisions at the Electron-Ion Collider indicates that the NLO corrections can\nbe large and are sensitive to the functional form of the twist-3 correlation\nfunctions.",
        "Accurately determining the coupling strength between polaron states is\nessential to describe charge-hopping transport in materials. In this work, we\nrevisit methodologies to extract coupling strengths using ab initio approaches.\nOur findings underscore the critical role of incorporating anharmonic effects\nin the model Hamiltonian when analyzing total energy variations along reaction\ncoordinates. Furthermore, we demonstrate that coupling strength extraction\nbased on total energy calculated from ab initio approaches is fundamentally\nlimited due to the stabilization of diabatic states, which do not involve\ncoupling strength. We demonstrate that such limitation exists in both DFT+U and\nHSE hybrid functionals, which are widely used in the study of polaron\ntransport. Instead, we suggest extracting coupling strength directly from the\nelectronic structure of a system in neutral conditions. The neutral condition\navoids overestimating coupling strength due to additional energy splitting\nbetween bonding and anti-bonding states from the charging energy. This study\nhighlights the limitations of existing methods and introduces a robust\nframework for accurately extracting coupling parameters, paving the way for\nimproved modeling of charge transport in complex materials."
      ]
    }
  },
  {
    "id":2411.01668,
    "research_type":"basic",
    "start_id":"b4",
    "start_title":"Mean\u2010field games with differing beliefs for algorithmic trading",
    "start_abstract":"Abstract Even when confronted with the same data, agents often disagree on a model of real world. Here, we address question how interacting heterogeneous agents, who what world follows, optimize their trading actions. The market has latent factors that drive prices, and account for permanent impact they have prices. This leads to large stochastic game, where each performance criteria are computed under different probability measure. We analyze mean\u2010field game (MFG) limit show Nash equilibrium is given by solution nonstandard vector\u2010valued forward\u2013backward differential equation. Under some mild assumptions, construct in terms expectations filtered states. Furthermore, prove MFG strategy forms an \u03b5\u2010Nash finite player game. Finally, present least square Monte Carlo based algorithm computing equilibria through simulations increasing disagreement may increase price volatility activity.",
    "start_categories":[
      "q-fin.MF"
    ],
    "start_fields":[
      "Economics and Quantitative Finance"
    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "Linear-quadratic mean field games"
      ],
      "abstract":[
        "In this article, we provide a comprehensive study of the linear-quadratic mean field games via the adjoint equation approach; although the problem has been considered in the literature by Huang, Caines and Malhame (HCM, 2007a), their method is based on Dynamic Programming. It turns out that two methods are not equivalent, as far as giving sufficient condition for the existence of a solution is concerned. Due to the linearity of the adjoint equations, the optimal mean field term satisfies a linear forward-backward ordinary differential equation. For the one dimensional case, we show that the equilibrium strategy always exists uniquely. For dimension greater than one, by choosing a suitable norm and then applying the Banach Fixed Point Theorem, a sufficient condition, which is independent of the solution of the standard Riccati differential equation, for the unique existence of the equilibrium strategy is provided. As a by-product, we also establish a neat and instructive sufficient condition for the unique existence of the solution for a class of non-trivial nonsymmetric Riccati equations. Numerical examples of non-existence of the equilibrium strategy and the comparison of HCM's approach will also be provided."
      ],
      "categories":[
        "math.OC"
      ]
    },
    "list":{
      "title":[
        "The Eigenfunctions of the Transfer Operator for the Dyson model in a\n  field",
        "Minding Fuzzy Regions: A Data-driven Alternating Learning Paradigm for\n  Stable Lesion Segmentation",
        "Learning Privacy from Visual Entities",
        "A proposal for removing $\\pi N$-state contamination from the nucleon\n  induced pseudoscalar form factor in lattice QCD",
        "Modular Units on $X_{1}( p)$ and Quotients of the Cuspidal Group",
        "Entente: Cross-silo Intrusion Detection on Network Log Graphs with\n  Federated Learning",
        "Twin-Space Representation of Classical Mapping Model in the Constraint\n  Phase Space Representation: Numerically Exact Approach to Open Quantum\n  Systems",
        "Structure and Dynamics of Deep Eutectic Systems from Cluster-Optimized\n  Energy Functions",
        "SPRIG: Stackelberg Perception-Reinforcement Learning with Internal Game\n  Dynamics",
        "Towards Heisenberg limit without critical slowing down via quantum\n  reinforcement learning",
        "Gumbel-Softmax Flow Matching with Straight-Through Guidance for\n  Controllable Biological Sequence Generation",
        "Unreflected Use of Tabular Data Repositories Can Undermine Research\n  Quality",
        "Quantum stochastic communication via high-dimensional entanglement",
        "The closure of linear foliations",
        "Dynamics near a class of nonhyperbolic fixed points",
        "Deformation theory and Koszul duality for Rota-Baxter systems",
        "Characterising planetary material accreted by cool helium atmosphere\n  white dwarfs using an exponentially decaying disc model",
        "Multisymplectic structure of nonintegrable Henon-Heiles system",
        "Sphere Precoding for Robust Near-Field Communications",
        "A Framework for Supporting the Reproducibility of Computational\n  Experiments in Multiple Scientific Domains",
        "Choroidal image analysis for OCT image sequences with applications in\n  systemic health",
        "MetaDE: Evolving Differential Evolution by Differential Evolution",
        "Semantic Neural Radiance Fields for Multi-Date Satellite Data",
        "Rethinking LLM Unlearning Objectives: A Gradient Perspective and Go\n  Beyond",
        "Variance Reduction via Resampling and Experience Replay",
        "Explainable AI for Clinical Outcome Prediction: A Survey of Clinician\n  Perceptions and Preferences",
        "SGNetPose+: Stepwise Goal-Driven Networks with Pose Information for\n  Trajectory Prediction in Autonomous Driving",
        "ACCEPT: Diagnostic Forecasting of Battery Degradation Through\n  Contrastive Learning",
        "On Target Pattern Formation in the CHNS system"
      ],
      "abstract":[
        "The recent works \\cite{EFMV2024} and \\cite{JOP2023} have studied the spectral\nproperties of the Dyson model in the absence of an external field. This paper\nis a continuation of \\cite{EFMV2024} and aims to bridge the gap in the\nliterature by investigating the Dyson model in a field.\\\\ In this paper, we\nprove that, for high temperatures or strong magnetic fields, there exists a\nnon-negative, integrable (with respect to the unique half-line Gibbs measure)\neigenfunction of the transfer operator for the Dyson model if $\\alpha\\in(\\frac\n3 2,2]$. However, unlike in the zero-magnetic-field case, this eigenfunction is\nnot continuous.",
        "Deep learning has achieved significant advancements in medical image\nsegmentation, but existing models still face challenges in accurately\nsegmenting lesion regions. The main reason is that some lesion regions in\nmedical images have unclear boundaries, irregular shapes, and small tissue\ndensity differences, leading to label ambiguity. However, the existing model\ntreats all data equally without taking quality differences into account in the\ntraining process, resulting in noisy labels negatively impacting model training\nand unstable feature representations. In this paper, a data-driven alternating\nlearning (DALE) paradigm is proposed to optimize the model's training process,\nachieving stable and high-precision segmentation. The paradigm focuses on two\nkey points: (1) reducing the impact of noisy labels, and (2) calibrating\nunstable representations. To mitigate the negative impact of noisy labels, a\nloss consistency-based collaborative optimization method is proposed, and its\neffectiveness is theoretically demonstrated. Specifically, the label confidence\nparameters are introduced to dynamically adjust the influence of labels of\ndifferent confidence levels during model training, thus reducing the influence\nof noise labels. To calibrate the learning bias of unstable representations, a\ndistribution alignment method is proposed. This method restores the underlying\ndistribution of unstable representations, thereby enhancing the discriminative\ncapability of fuzzy region representations. Extensive experiments on various\nbenchmarks and model backbones demonstrate the superiority of the DALE\nparadigm, achieving an average performance improvement of up to 7.16%.",
        "Subjective interpretation and content diversity make predicting whether an\nimage is private or public a challenging task. Graph neural networks combined\nwith convolutional neural networks (CNNs), which consist of 14,000 to 500\nmillions parameters, generate features for visual entities (e.g., scene and\nobject types) and identify the entities that contribute to the decision. In\nthis paper, we show that using a simpler combination of transfer learning and a\nCNN to relate privacy with scene types optimises only 732 parameters while\nachieving comparable performance to that of graph-based methods. On the\ncontrary, end-to-end training of graph-based methods can mask the contribution\nof individual components to the classification performance. Furthermore, we\nshow that a high-dimensional feature vector, extracted with CNNs for each\nvisual entity, is unnecessary and complexifies the model. The graph component\nhas also negligible impact on performance, which is driven by fine-tuning the\nCNN to optimise image features for privacy nodes.",
        "In the PACS10 project, the PACS collaboration has generated three sets of the\nPACS10 gauge configurations at the physical point with lattice volume larger\nthan $(10\\;{\\rm fm})^4$ and three different lattice spacings. The isovector\nnucleon form factors had been already calculated by using two sets of the\nPACS10 gauge configurations. In our strategy, the smearing parameters of the\nnucleon interpolation operator were highly optimized to eliminate as much as\npossible the contribution of excited states in the nucleon two-point function.\nThis strategy was quite successful in calculations of the electric ($G_E$),\nmagnetic ($G_M$) and axial-vector ($F_A$) form factors, while the induced\npseudoscalar ($F_P$) and pseudoscalar ($G_P$) form factors remained strongly\naffected by residual contamination of $\\pi N$-state contribution. In this work,\nwe propose a simple method to remove the $\\pi N$-state contamination from the\n$F_P$ form factor, and then evaluate the induced pseudoscalar charge $g_P^\\ast$\nand the pion-nucleon coupling $g_{\\pi NN}$ from existing data in a new\nanalysis. Applying this method to the $G_P$ form factor is also considered with\na help of the axial Ward-Takahashi identity.",
        "Modular units are functions on modular curves whose divisors are supported on\nthe cusps. They form a free abelian group of rank at most one less than the\nnumber of cusps. In this paper we study the group of modular units on $X_{1}( p\n)$, with prime level $p \\ge 5$. We give an explicit basis for this group and\nstudy certain rational subgroups of it. We use the basis to numerically\ninvestigate the structure of the cuspidal group of $X_{1}( p)$ and its rational\nsubgroup. In the later stages of this paper we use our basis to determine a\nspecific large quotient of the cuspidal group.",
        "Graph-based Network Intrusion Detection System (GNIDS) has gained significant\nmomentum in detecting sophisticated cyber-attacks, like Advanced Persistent\nThreat (APT), in an organization or across organizations. Though achieving\nsatisfying detection accuracy and adapting to ever-changing attacks and normal\npatterns, all prior GNIDSs assume the centralized data settings directly, but\nnon-trivial data collection is not always practical under privacy regulations\nnowadays. We argue that training a GNIDS model has to consider privacy\nregulations, and propose to leverage federated learning (FL) to address this\nprominent challenge.\n  Yet, directly applying FL to GNIDS is unlikely to succeed, due to issues like\nnon-IID (independent and identically distributed) graph data over clients and\nthe diverse design choices taken by different GNIDS. We address these issues\nwith a set of novel techniques tailored to the graph datasets, including\nreference graph synthesis, graph sketching and adaptive contribution scaling,\nand develop a new system Entente. We evaluate Entente on the large-scale LANL,\nOpTC and Pivoting datasets. The result shows Entente outperforms the other\nbaseline FL algorithms and sometimes even the non-FL GNIDS. We also evaluate\nEntente under FL poisoning attacks tailored to the GNIDS setting, and show\nEntente is able to bound the attack success rate to low values. Overall, our\nresult suggests building cross-silo GNIDS is feasible and we hope to encourage\nmore efforts in this direction.",
        "The constraint coordinate-momentum \\textit{phase space} (CPS) has recently\nbeen developed to study nonadiabatic dynamics in gas-phase and condensed-phase\nmolecular systems. Although the CPS formulation is exact for describing the\ndiscrete (electronic\/ vibrational\/spin) state degrees of freedom (DOFs), when\nsystem-bath models in condense phase are studied, previous works often employ\nthe discretization of environmental bath DOFs, which breaks the time\nirreversibility and may make it difficult to obtain numerically converged\nresults in the long-time limit. In this paper, we develop an exact\ntrajectory-based phase space approach by adopting the twin-space (TS)\nformulation of quantum statistical mechanics, in which the density operator of\nthe reduced system is transformed to the wavefunction of an expanded system\nwith twice the DOFs. The classical mapping model (CMM) is then used to map the\nHamiltonian of the expanded system to its equivalent classical counterpart on\nCPS. To demonstrate the applicability of the TS-CMM approach, we compare\nsimulated population dynamics and nonlinear spectra for a few benchmark\ncondensed phase system-bath models with those obtained from the hierarchical\nequations of motion method, which shows that our approach yields accurate\ndynamics of open quantum systems.",
        "Generating energy functions for heterogeneous systems suitable for\nquantitative and predictive atomistic simulations is a challenging undertaking.\nThe present work combines a cluster-based approach with electronic structure\ncalculations at the density functional theory level and machine learning-based\nenergy functions for a spectroscopic reporter for eutectic mixtures consisting\nof water, acetamide and KSCN. Two water models are considered: TIP3P which is\nconsistent with the CGenFF energy function and TIP4P which - as a water model -\nis superior to TIP4P. Both fitted models, {\\bf M2$^{\\rm TIP3P}$} and {\\bf\n  M2$^{\\rm TIP4P}$}, yield favourable thermodynamic, structural, spectroscopic\nand transport properties from extensive molecular dynamics simulations. In\nparticular, the slow and fast decay times from 2-dimensional infrared\nspectroscopy and the viscosity for water-rich mixtures are described\nrealistically and consistent with experiments. On the other hand, including the\nco-solvent (acetamide) in the present case is expected to further improve the\ncomputed viscosity for low-water content. It is concluded that such a\ncluster-based approach is a promising and generalizable route for routine\nparametrization of heterogeneous, electrostatically dominated systems.",
        "Deep reinforcement learning agents often face challenges to effectively\ncoordinate perception and decision-making components, particularly in\nenvironments with high-dimensional sensory inputs where feature relevance\nvaries. This work introduces SPRIG (Stackelberg Perception-Reinforcement\nlearning with Internal Game dynamics), a framework that models the internal\nperception-policy interaction within a single agent as a cooperative\nStackelberg game. In SPRIG, the perception module acts as a leader,\nstrategically processing raw sensory states, while the policy module follows,\nmaking decisions based on extracted features. SPRIG provides theoretical\nguarantees through a modified Bellman operator while preserving the benefits of\nmodern policy optimization. Experimental results on the Atari BeamRider\nenvironment demonstrate SPRIG's effectiveness, achieving around 30% higher\nreturns than standard PPO through its game-theoretical balance of feature\nextraction and decision-making.",
        "Critical ground states of quantum many-body systems have emerged as vital\nresources for quantum-enhanced sensing. Traditional methods to prepare these\nstates often rely on adiabatic evolution, which may diminish the quantum\nsensing advantage. In this work, we propose a quantum reinforcement learning\n(QRL)-enhanced critical sensing protocol for quantum many-body systems with\nexotic phase diagrams. Starting from product states and utilizing\nQRL-discovered gate sequences, we explore sensing accuracy in the presence of\nunknown external magnetic fields, covering both local and global regimes. Our\nresults demonstrate that QRL-learned sequences reach the finite quantum speed\nlimit and generalize effectively across systems of arbitrary size, ensuring\naccuracy regardless of preparation time. This method can robustly achieve\nHeisenberg and super-Heisenberg limits, even in noisy environments with\npractical Pauli measurements. Our study highlights the efficacy of QRL in\nenabling precise quantum state preparation, thereby advancing scalable,\nhigh-accuracy quantum critical sensing.",
        "Flow matching in the continuous simplex has emerged as a promising strategy\nfor DNA sequence design, but struggles to scale to higher simplex dimensions\nrequired for peptide and protein generation. We introduce Gumbel-Softmax Flow\nand Score Matching, a generative framework on the simplex based on a novel\nGumbel-Softmax interpolant with a time-dependent temperature. Using this\ninterpolant, we introduce Gumbel-Softmax Flow Matching by deriving a\nparameterized velocity field that transports from smooth categorical\ndistributions to distributions concentrated at a single vertex of the simplex.\nWe alternatively present Gumbel-Softmax Score Matching which learns to regress\nthe gradient of the probability density. Our framework enables high-quality,\ndiverse generation and scales efficiently to higher-dimensional simplices. To\nenable training-free guidance, we propose Straight-Through Guided Flows\n(STGFlow), a classifier-based guidance method that leverages straight-through\nestimators to steer the unconditional velocity field toward optimal vertices of\nthe simplex. STGFlow enables efficient inference-time guidance using\nclassifiers pre-trained on clean sequences, and can be used with any discrete\nflow method. Together, these components form a robust framework for\ncontrollable de novo sequence generation. We demonstrate state-of-the-art\nperformance in conditional DNA promoter design, sequence-only protein\ngeneration, and target-binding peptide design for rare disease treatment.",
        "Data repositories have accumulated a large number of tabular datasets from\nvarious domains. Machine Learning researchers are actively using these datasets\nto evaluate novel approaches. Consequently, data repositories have an important\nstanding in tabular data research. They not only host datasets but also provide\ninformation on how to use them in supervised learning tasks. In this paper, we\nargue that, despite great achievements in usability, the unreflected usage of\ndatasets from data repositories may have led to reduced research quality and\nscientific rigor. We present examples from prominent recent studies that\nillustrate the problematic use of datasets from OpenML, a large data repository\nfor tabular data. Our illustrations help users of data repositories avoid\nfalling into the traps of (1) using suboptimal model selection strategies, (2)\noverlooking strong baselines, and (3) inappropriate preprocessing. In response,\nwe discuss possible solutions for how data repositories can prevent the\ninappropriate use of datasets and become the cornerstones for improved overall\nquality of empirical research studies.",
        "Entanglement has the ability to enhance the transmission of classical\ninformation over a quantum channel. However, fully harvesting this advantage\ntypically requires complex entangling measurements, which are challenging to\nimplement and scale with the system's size. In this work, we consider a natural\nquantum information primitive in which the message to be communicated is\nselected stochastically. We introduce a protocol that leverages\nhigh-dimensional entanglement to perform this task perfectly, without requiring\nquantum interference between particles at the measurement station. We\nexperimentally demonstrate the protocol's scalability in an optical setup using\n8-dimensional entanglement and multi-outcome detection, providing a practical\nsolution for stochastic communication and a robust method for certifying the\ndimensionality of entanglement in communication experiments.",
        "This paper presents a simplified geometric proof of the\nMolino-Alexandrino-Radeschi (MAR) Theorem, which states that the closure of a\nsingular Riemannian foliation on a complete Riemannian manifold is itself a\nsmooth singular Riemannian foliation. Our approach circumvents several\ntechnical and analytical tools employed in the previous proof of the Theorem,\nresulting in a more direct geometric demonstration. We first establish\nconditions for a projectable foliation to be Riemannian, focusing on compatible\nconnections. We then apply these results to linear foliations on vector bundles\nand their lifts to frame bundles. Finally, we use these findings to the\nlinearization of singular Riemannian foliations around leaf closures. This\nmethod allows us to prove the smoothness of the closure directly for the linear\nsemi-local model, bypassing the need for intermediate results on orbit-like\nfoliations.",
        "In this paper, we investigate some dynamical properties near a nonhyperbolic\nfixed point. Under some conditions on the higher nonlinear terms, we establish\na stable manifold theorem and a degenerate Hartman theorem. Furthermore, the\nfinite shadowing property also be discussed.",
        "This paper investigates Rota-Baxter systems in the sense of Brzezi\\'nski from\nthe perspective of operad theory. The minimal model of the Rota-Baxter system\noperad is constructed, equivalently a concrete construction of its Koszul dual\nhomotopy cooperad is given. The concept of homotopy Rota-Baxter systems and the\n$L_\\infty$-algebra that governs deformations of a Rota-Baxter system are\nderived from the Koszul dual homotopy cooperad. The notion of\ninfinity-Yang-Baxter pairs is introduced, which is a higher-order\ngeneralization of the traditional Yang-Baxter pairs. It is shown that a\nhomotopy Rota-Baxter system structure on the endomorphism algebra of a graded\nspace is equivalent to an associative infinity-Yang-Baxter pair on this graded\nalgebra, thereby generalizing the classical correspondence between Yang-Baxter\npairs and Rota-Baxter systems.",
        "We present Keck High Resolution Echelle Spectrometer (HIRES) observations and\nmodel atmosphere analysis for two nearby, cool, helium-dominated atmosphere\nwhite dwarfs that have been polluted by accretion: WD J1927-0355 and WD\nJ2141-3300. Detected elements common to both white dwarfs are Mg, Ca, Ti, Cr,\nFe, and Ni, with additional detections of Na, Al, Si and Sr in WD J2141-3300.\nWe present an approach for inferring the composition of the accreted material,\nby adopting a physically motivated model in which the mass accretion rate\ndecays exponentially with time, which provides constraints on the time since\nthe start of the accretion event. The accretion events were most likely to have\nbegan at least 1 Myr ago, however the characteristic disc lifetime could not be\nconstrained due to degeneracies. Both white dwarfs were found to have accreted\nbulk planetary material with compositions similar to that of both bulk Earth\nand chondritic meteorites. The parent bodies causing pollution in both cases\nwere inferred to be the mass of a small moon or dwarf planet.",
        "Multi-symplectic integrators are typically regarded as a discretization of\nthe Hamiltonian partial differential equations. This is due to the fact that,\nfor generic finite-dimensional Hamiltonian systems, there exists only one\nindependent symplectic structure. In this note, the second invariant symplectic\nform is presented for the nonintegrable Henon-Heiles system, Kepler problem,\nintegrable and non-integrable Toda type systems. This approach facilitates the\nconstruction of a multi-symplectic integrator, which effectively preserves both\nsymplectic forms for these benchmark problems.",
        "Near-field communication with large antenna arrays promises significant\nbeamforming and multiplexing gains. These communication links, however, are\nvery sensitive to user mobility as any small change in the user position may\nsuddenly drop the signal power. This leads to critical challenges for the\nrobustness of these near-field communication systems. In this paper, we propose\n\\textit{sphere precoding}, which is a robust precoding design to address user\nmobility in near-field communications. To gain insights into the spatial\ncorrelation of near-field channels, we extend the one-ring channel model to\nwhat we call one-sphere channel model and derive the channel covariance\nconsidering user mobility. Based on the one-sphere channel model, a robust\nprecoding design problem is defined to optimize the minimum\nsignal-to-interference-plus-noise ratio (SINR) satisfaction probability among\nmobile users. By utilizing the eigen structure of channel covariance, we\nfurther design a relaxed convex problem to approximate the solution of the\noriginal non-convex problem. The low-complexity solution effectively shapes a\nsphere that maintains the signal power for the target user and also nulls its\ninterference within spheres around the other users. Simulation results\nhighlight the efficacy of the proposed solution in achieving robust precoding\nyet high achievable rates in near-field communication systems.",
        "In recent years, the research community, but also the general public, has\nraised serious questions about the reproducibility and replicability of\nscientific work. Since many studies include some kind of computational work,\nthese issues are also a technological challenge, not only in computer science,\nbut also in most research domains. Computational replicability and\nreproducibility are not easy to achieve due to the variety of computational\nenvironments that can be used. Indeed, it is challenging to recreate the same\nenvironment via the same frameworks, code, programming languages, dependencies,\nand so on. We propose a framework, known as SciRep, that supports the\nconfiguration, execution, and packaging of computational experiments by\ndefining their code, data, programming languages, dependencies, databases, and\ncommands to be executed. After the initial configuration, the experiments can\nbe executed any number of times, always producing exactly the same results. Our\napproach allows the creation of a reproducibility package for experiments from\nmultiple scientific fields, from medicine to computer science, which can be\nre-executed on any computer. The produced package acts as a capsule, holding\nabsolutely everything necessary to re-execute the experiment. To evaluate our\nframework, we compare it with three state-of-the-art tools and use it to\nreproduce 18 experiments extracted from published scientific articles. With our\napproach, we were able to execute 16 (89%) of those experiments, while the\nothers reached only 61%, thus showing that our approach is effective. Moreover,\nall the experiments that were executed produced the results presented in the\noriginal publication. Thus, SciRep was able to reproduce 100% of the\nexperiments it could run.",
        "The choroid, a highly vascular layer behind the retina, is an extension of\nthe central nervous system and has parallels with the renal cortex, with blood\nflow far exceeding that of the brain and kidney. Thus, there has been growing\ninterest of choroidal blood flow reflecting physiological status of systemic\ndisease. Optical coherence tomography (OCT) enables high-resolution imaging of\nthe choroid, but conventional analysis methods remain manual or semi-automatic,\nlimiting reproducibility, standardisation and clinical utility. In this thesis,\nI develop several new methods to analyse the choroid in OCT image sequences,\nwith each successive method improving on its predecessors. I first develop two\nsemi-automatic approaches for choroid region (Gaussian Process Edge Tracing,\nGPET) and vessel (Multi-scale Median Cut Quantisation, MMCQ) analysis, which\nimprove on manual approaches but remain user-dependent. To address this, I\nintroduce DeepGPET, a deep learning-based region segmentation method which\nimproves on execution time, reproducibility, and end-user accessibility, but\nlacks choroid vessel analysis and automatic feature measurement. Improving on\nthis, I developed Choroidalyzer, a deep learning-based pipeline to segment the\nchoroidal space and vessels and generate fully automatic, clinically meaningful\nand reproducible choroidal features. I provide rigorous evaluation of these\nfour approaches and consider their potential clinical value in three\napplications into systemic health: OCTANE, assessing choroidal changes in renal\ntransplant recipients and donors; PREVENT, exploring choroidal associations\nwith Alzheimer's risk factors at mid-life; D-RISCii, assessing choroidal\nvariation and feasibility of OCT in critical care. In short, this thesis\ncontributes many open-source tools for standardised choroidal measurement and\nhighlights the choroid's potential as a biomarker in systemic health.",
        "As a cornerstone in the Evolutionary Computation (EC) domain, Differential\nEvolution (DE) is known for its simplicity and effectiveness in handling\nchallenging black-box optimization problems. While the advantages of DE are\nwell-recognized, achieving peak performance heavily depends on its\nhyperparameters such as the mutation factor, crossover probability, and the\nselection of specific DE strategies. Traditional approaches to this\nhyperparameter dilemma have leaned towards parameter tuning or adaptive\nmechanisms. However, identifying the optimal settings tailored for specific\nproblems remains a persistent challenge. In response, we introduce MetaDE, an\napproach that evolves DE's intrinsic hyperparameters and strategies using DE\nitself at a meta-level. A pivotal aspect of MetaDE is a specialized\nparameterization technique, which endows it with the capability to dynamically\nmodify DE's parameters and strategies throughout the evolutionary process. To\naugment computational efficiency, MetaDE incorporates a design that leverages\nparallel processing through a GPU-accelerated computing framework. Within such\na framework, DE is not just a solver but also an optimizer for its own\nconfigurations, thus streamlining the process of hyperparameter optimization\nand problem-solving into a cohesive and automated workflow. Extensive\nevaluations on the CEC2022 benchmark suite demonstrate MetaDE's promising\nperformance. Moreover, when applied to robot control via evolutionary\nreinforcement learning, MetaDE also demonstrates promising performance. The\nsource code of MetaDE is publicly accessible at:\nhttps:\/\/github.com\/EMI-Group\/metade.",
        "In this work we propose a satellite specific Neural Radiance Fields (NeRF)\nmodel capable to obtain a three-dimensional semantic representation (neural\nsemantic field) of the scene. The model derives the output from a set of\nmulti-date satellite images with corresponding pixel-wise semantic labels. We\ndemonstrate the robustness of our approach and its capability to improve noisy\ninput labels. We enhance the color prediction by utilizing the semantic\ninformation to address temporal image inconsistencies caused by non-stationary\ncategories such as vehicles. To facilitate further research in this domain, we\npresent a dataset comprising manually generated labels for popular multi-view\nsatellite images. Our code and dataset are available at\nhttps:\/\/github.com\/wagnva\/semantic-nerf-for-satellite-data.",
        "Large language models (LLMs) should undergo rigorous audits to identify\npotential risks, such as copyright and privacy infringements. Once these risks\nemerge, timely updates are crucial to remove undesirable responses, ensuring\nlegal and safe model usage. It has spurred recent research into LLM unlearning,\nfocusing on erasing targeted undesirable knowledge without compromising the\nintegrity of other, non-targeted responses. Existing studies have introduced\nvarious unlearning objectives to pursue LLM unlearning without necessitating\ncomplete retraining. However, each of these objectives has unique properties,\nand no unified framework is currently available to comprehend them thoroughly.\nTo fill the gap, we propose a toolkit of the gradient effect (G-effect),\nquantifying the impacts of unlearning objectives on model performance from a\ngradient perspective. A notable advantage is its broad ability to detail the\nunlearning impacts from various aspects across instances, updating steps, and\nLLM layers. Accordingly, the G-effect offers new insights into identifying\ndrawbacks of existing unlearning objectives, further motivating us to explore a\nseries of new solutions for their mitigation and improvements. Finally, we\noutline promising directions that merit further studies, aiming at contributing\nto the community to advance this important field.",
        "Experience replay is a foundational technique in reinforcement learning that\nenhances learning stability by storing past experiences in a replay buffer and\nreusing them during training. Despite its practical success, its theoretical\nproperties remain underexplored. In this paper, we present a theoretical\nframework that models experience replay using resampled $U$- and\n$V$-statistics, providing rigorous variance reduction guarantees. We apply this\nframework to policy evaluation tasks using the Least-Squares Temporal\nDifference (LSTD) algorithm and a Partial Differential Equation (PDE)-based\nmodel-free algorithm, demonstrating significant improvements in stability and\nefficiency, particularly in data-scarce scenarios. Beyond policy evaluation, we\nextend the framework to kernel ridge regression, showing that the experience\nreplay-based method reduces the computational cost from the traditional\n$O(n^3)$ in time to as low as $O(n^2)$ in time while simultaneously reducing\nvariance. Extensive numerical experiments validate our theoretical findings,\ndemonstrating the broad applicability and effectiveness of experience replay in\ndiverse machine learning tasks.",
        "Explainable AI (XAI) techniques are necessary to help clinicians make sense\nof AI predictions and integrate predictions into their decision-making\nworkflow. In this work, we conduct a survey study to understand clinician\npreference among different XAI techniques when they are used to interpret model\npredictions over text-based EHR data. We implement four XAI techniques (LIME,\nAttention-based span highlights, exemplar patient retrieval, and free-text\nrationales generated by LLMs) on an outcome prediction model that uses ICU\nadmission notes to predict a patient's likelihood of experiencing in-hospital\nmortality. Using these XAI implementations, we design and conduct a survey\nstudy of 32 practicing clinicians, collecting their feedback and preferences on\nthe four techniques. We synthesize our findings into a set of recommendations\ndescribing when each of the XAI techniques may be more appropriate, their\npotential limitations, as well as recommendations for improvement.",
        "Predicting pedestrian trajectories is essential for autonomous driving\nsystems, as it significantly enhances safety and supports informed\ndecision-making. Accurate predictions enable the prevention of collisions,\nanticipation of crossing intent, and improved overall system efficiency. In\nthis study, we present SGNetPose+, an enhancement of the SGNet architecture\ndesigned to integrate skeleton information or body segment angles with bounding\nboxes to predict pedestrian trajectories from video data to avoid hazards in\nautonomous driving. Skeleton information was extracted using a pose estimation\nmodel, and joint angles were computed based on the extracted joint data. We\nalso apply temporal data augmentation by horizontally flipping video frames to\nincrease the dataset size and improve performance. Our approach achieves\nstate-of-the-art results on the JAAD and PIE datasets using pose data with the\nbounding boxes, outperforming the SGNet model. Code is available on Github:\nSGNetPose+.",
        "Modeling lithium-ion battery (LIB) degradation offers significant cost\nsavings and enhances the safety and reliability of electric vehicles (EVs) and\nbattery energy storage systems (BESS). Whilst data-driven methods have received\ngreat attention for forecasting degradation, they often demonstrate limited\ngeneralization ability and tend to underperform particularly in critical\nscenarios involving accelerated degradation, which are crucial to predict\naccurately. These methods also fail to elucidate the underlying causes of\ndegradation. Alternatively, physical models provide a deeper understanding, but\ntheir complex parameters and inherent uncertainties limit their applicability\nin real-world settings. To this end, we propose a new model - ACCEPT. Our novel\nframework uses contrastive learning to map the relationship between the\nunderlying physical degradation parameters and observable operational\nquantities, combining the benefits of both approaches. Furthermore, due to the\nsimilarity of degradation paths between LIBs with the same chemistry, this\nmodel transfers non-trivially to most downstream tasks, allowing for zero-shot\ninference. Additionally, since categorical features can be included in the\nmodel, it can generalize to other LIB chemistries. This work establishes a\nfoundational battery degradation model, providing reliable forecasts across a\nrange of battery types and operating conditions.",
        "We study the concentration field in a prescribed 2D Cahn-Hilliard\nNavier-Stokes (CHNS) system. We formulate a description for the target pattern\nformation and pattern merging processes, and compare this description with\nsimulation results. Shear-augmented diffusion along streamlines causes a\nseparation of time scales, thus 2D CHNS system can be simplified to a 1D\nsystem. In this 1D system, target pattern formation is induced by linear\ninstability. The waveform of patterns are described by Jacobi Elliptic\nFunctions. The interface (of pattern) migration or coarsening velocity is\ndetermined by the derivative of interface curvature. The anomalous migration of\ninner pattern can be explained by the singularity at the origin and therefore\nthe boundary motion in the quasi-one-dimension system. Finally we derive a\nsimple criterion for when CHNS system becomes dynamic by following similar\ncases in MHD."
      ]
    }
  },
  {
    "id":2411.01668,
    "research_type":"basic",
    "start_id":"b3",
    "start_title":"Linear-quadratic mean field games",
    "start_abstract":"In this article, we provide a comprehensive study of the linear-quadratic mean field games via the adjoint equation approach; although the problem has been considered in the literature by Huang, Caines and Malhame (HCM, 2007a), their method is based on Dynamic Programming. It turns out that two methods are not equivalent, as far as giving sufficient condition for the existence of a solution is concerned. Due to the linearity of the adjoint equations, the optimal mean field term satisfies a linear forward-backward ordinary differential equation. For the one dimensional case, we show that the equilibrium strategy always exists uniquely. For dimension greater than one, by choosing a suitable norm and then applying the Banach Fixed Point Theorem, a sufficient condition, which is independent of the solution of the standard Riccati differential equation, for the unique existence of the equilibrium strategy is provided. As a by-product, we also establish a neat and instructive sufficient condition for the unique existence of the solution for a class of non-trivial nonsymmetric Riccati equations. Numerical examples of non-existence of the equilibrium strategy and the comparison of HCM's approach will also be provided.",
    "start_categories":[
      "math.OC"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Mean\u2010field games with differing beliefs for algorithmic trading"
      ],
      "abstract":[
        "Abstract Even when confronted with the same data, agents often disagree on a model of real world. Here, we address question how interacting heterogeneous agents, who what world follows, optimize their trading actions. The market has latent factors that drive prices, and account for permanent impact they have prices. This leads to large stochastic game, where each performance criteria are computed under different probability measure. We analyze mean\u2010field game (MFG) limit show Nash equilibrium is given by solution nonstandard vector\u2010valued forward\u2013backward differential equation. Under some mild assumptions, construct in terms expectations filtered states. Furthermore, prove MFG strategy forms an \u03b5\u2010Nash finite player game. Finally, present least square Monte Carlo based algorithm computing equilibria through simulations increasing disagreement may increase price volatility activity."
      ],
      "categories":[
        "q-fin.MF"
      ]
    },
    "list":{
      "title":[
        "GM-MoE: Low-Light Enhancement with Gated-Mechanism Mixture-of-Experts",
        "Large Language Models with Human-In-The-Loop Validation for Systematic\n  Review Data Extraction",
        "MODRIC: A Cost Effective MODular Data Center Network Architecture with\n  Rich InterConnections",
        "Investigating Human-Aligned Large Language Model Uncertainty",
        "Synthesizing Consistent Novel Views via 3D Epipolar Attention without\n  Re-Training",
        "An X-ray view of the Cataclysmic Variable V902 Mon: Discovery of an\n  X-ray eclipse",
        "Superconducting LaPtH$_{ 6 }$ with triatomic hydrogen units",
        "Bridging Structural Dynamics and Biomechanics: Human Motion Estimation\n  through Footstep-Induced Floor Vibrations",
        "Connecting the dots: Tracing the evolutionary pathway of Polar Ring\n  Galaxies in the cases of NGC 3718, NGC 2685, and NGC 4262",
        "TRADES: Generating Realistic Market Simulations with Diffusion Models",
        "Rethinking LLM Unlearning Objectives: A Gradient Perspective and Go\n  Beyond",
        "Optimizing CNN Architectures for Advanced Thoracic Disease\n  Classification",
        "Evolution of Spots and Stripes in Cellular Automata",
        "Formation of super-Earths and mini-Neptunes from rings of planetesimals",
        "Swift4D:Adaptive divide-and-conquer Gaussian Splatting for compact and\n  efficient reconstruction of dynamic scene",
        "Practical programming research of Linear DML model based on the simplest\n  Python code: From the standpoint of novice researchers",
        "The NANOGrav 15-year Data Set: Search for Gravitational Wave Memory",
        "Online Nonstochastic Control with Convex Safety Constraints",
        "Curiosity-Diffuser: Curiosity Guide Diffusion Models for Reliability",
        "Toward Responsible Federated Large Language Models: Leveraging a Safety\n  Filter and Constitutional AI",
        "Formation of ultracold triatomic molecules by electric microwave\n  association",
        "Provable Benefits of Unsupervised Pre-training and Transfer Learning via\n  Single-Index Models",
        "Sliding Window Attention Training for Efficient Large Language Models",
        "Federated Learning Strategies for Coordinated Beamforming in Multicell\n  ISAC",
        "Spatial locking of chimera states to frequency heterogeneity in\n  nonlocally coupled oscillators",
        "Finding the ultra-narrow $^3\\!P_2 \\rightarrow \\, ^3\\!P_0$ electric\n  quadrupole transition in Ni$^{12+}$ ion for an optical clock",
        "Endogenous Persistence at the Effective Lower Bound",
        "Perception-Guided EEG Analysis: A Deep Learning Approach Inspired by\n  Level of Detail (LOD) Theory",
        "R.I.P.: Better Models by Survival of the Fittest Prompts"
      ],
      "abstract":[
        "Low-light enhancement has wide applications in autonomous driving, 3D\nreconstruction, remote sensing, surveillance, and so on, which can\nsignificantly improve information utilization. However, most existing methods\nlack generalization and are limited to specific tasks such as image recovery.\nTo address these issues, we propose \\textbf{Gated-Mechanism Mixture-of-Experts\n(GM-MoE)}, the first framework to introduce a mixture-of-experts network for\nlow-light image enhancement. GM-MoE comprises a dynamic gated weight\nconditioning network and three sub-expert networks, each specializing in a\ndistinct enhancement task. Combining a self-designed gated mechanism that\ndynamically adjusts the weights of the sub-expert networks for different data\ndomains. Additionally, we integrate local and global feature fusion within\nsub-expert networks to enhance image quality by capturing multi-scale features.\nExperimental results demonstrate that the GM-MoE achieves superior\ngeneralization with respect to 25 compared approaches, reaching\nstate-of-the-art performance on PSNR on 5 benchmarks and SSIM on 4 benchmarks,\nrespectively.",
        "Systematic reviews are time-consuming endeavors. Historically speaking,\nknowledgeable humans have had to screen and extract data from studies before it\ncan be analyzed. However, large language models (LLMs) hold promise to greatly\naccelerate this process. After a pilot study which showed great promise, we\ninvestigated the use of freely available LLMs for extracting data for\nsystematic reviews. Using three different LLMs, we extracted 24 types of data,\n9 explicitly stated variables and 15 derived categorical variables, from 112\nstudies that were included in a published scoping review. Overall we found that\nGemini 1.5 Flash, Gemini 1.5 Pro, and Mistral Large 2 performed reasonably\nwell, with 71.17%, 72.14%, and 62.43% of data extracted being consistent with\nhuman coding, respectively. While promising, these results highlight the dire\nneed for a human-in-the-loop (HIL) process for AI-assisted data extraction. As\na result, we present a free, open-source program we developed (AIDE) to\nfacilitate user-friendly, HIL data extraction with LLMs.",
        "Shipping container based modular architectures provide design flexibility in\ndata centers with building blocks to expand the network as and when needed. In\nthis paper, high capacity Modular Data Center (MDC) network architecture with\nRich Inter Connections named MODRIC is proposed. MODRIC is a cost-effective\nswitch-centric network design which allows building a flexible MDC network with\ncommodity switches. It uses an inter-container connectivity similar to the\nstructure of generalized hypercube in order to provide high inter-container\nbandwidth. Further, a hybrid Clos topology is used to build the container\nnetwork. MODRIC is highly suitable for cost effectively building mega data\ncenters requiring high throughput capacity and resilience against failures.\nThis paper presents the proposed architecture, discusses its relevant\nproperties, and proposes suitable addressing, routing and network construction\nschemes. The paper also presents comparative studies on its cost and\nperformance with existing network topologies.",
        "Recent work has sought to quantify large language model uncertainty to\nfacilitate model control and modulate user trust. Previous works focus on\nmeasures of uncertainty that are theoretically grounded or reflect the average\novert behavior of the model. In this work, we investigate a variety of\nuncertainty measures, in order to identify measures that correlate with human\ngroup-level uncertainty. We find that Bayesian measures and a variation on\nentropy measures, top-k entropy, tend to agree with human behavior as a\nfunction of model size. We find that some strong measures decrease in\nhuman-similarity with model size, but, by multiple linear regression, we find\nthat combining multiple uncertainty measures provide comparable human-alignment\nwith reduced size-dependency.",
        "Large diffusion models demonstrate remarkable zero-shot capabilities in novel\nview synthesis from a single image. However, these models often face challenges\nin maintaining consistency across novel and reference views. A crucial factor\nleading to this issue is the limited utilization of contextual information from\nreference views. Specifically, when there is an overlap in the viewing frustum\nbetween two views, it is essential to ensure that the corresponding regions\nmaintain consistency in both geometry and appearance. This observation leads to\na simple yet effective approach, where we propose to use epipolar geometry to\nlocate and retrieve overlapping information from the input view. This\ninformation is then incorporated into the generation of target views,\neliminating the need for training or fine-tuning, as the process requires no\nlearnable parameters. Furthermore, to enhance the overall consistency of\ngenerated views, we extend the utilization of epipolar attention to a\nmulti-view setting, allowing retrieval of overlapping information from the\ninput view and other target views. Qualitative and quantitative experimental\nresults demonstrate the effectiveness of our method in significantly improving\nthe consistency of synthesized views without the need for any fine-tuning.\nMoreover, This enhancement also boosts the performance of downstream\napplications such as 3D reconstruction. The code is available at\nhttps:\/\/github.com\/botaoye\/ConsisSyn.",
        "V902 Mon is one of a few eclipsing Intermediate Polars (IPs), and show deep\neclipses in the optical lightcurves. The presence of a strong Fe K$\\alpha$\nfluorescence line in its X-ray spectrum and its low X-ray flux compared to\nother IPs suggests significant absorption, most likely from an accretion disk.\nIn an observation carried out using the Nuclear Spectroscopic Telescope Array\n(NuSTAR), we confirm the presence of an X-ray eclipse in the energy resolved\nlightcurves, coincident with the optical AAVSO\/CV-band lightcurves. Broadband\nX-ray spectral analysis using NuSTAR and XMM-Newton observations confirm a\nstrong absorption N$_{H}$ $\\sim 10^{23}$ cm$^{-2}$ local to the source, along\nwith a high equivalent width of about 0.7 keV for a Fe K$\\alpha$ fluorescence\nline. We interpret this using a model similar to an Accretion Disk Corona\nsource, which have a very high inclination and the compact object is heavily\nobscured by the body of the accretion disk. We propose that the primary X-rays\nfrom the accretion column in V902 Mon is hidden from our direct view at all\ntimes by the accretion disk. In this scenario, the observed scattered X-rays\nindicate substantial absorption of direct X-rays by the accretion disk.\nAdditionally, a strong Fe fluorescence line suggests reprocessing of the\nradiation by a more extended region, such as the pre-shock region, which could\nbe located a few white dwarf radii above the orbital plane.",
        "To veryfy \"hot supreconductivity\" recently proposed in lanthanum\nhydride-based compounds, we explored thermodynamically stable and\nsuperconducting phases in the lanthanum (La)-platinum (Pt)-hydrogen (H) ternary\nsystem at 20 GPa using an evolutionary construction scheme of a\nformation-enthalpy convex hull, universal neural network potential\ncalculations, and density functional theory calculations. Although we found no\nevidence of the hot superconductivity in this ternary system, we predicted a\nunique compound, LaPtH$_{ 6 }$, which has equilateral triangular H$_{ 3 }$\nunits nearly forming a two-dimensional kagome lattice between La and Pt layers\nand shows the superconductivity at 18.67 K. This structure is dynamically\nstable from ambient pressure to at least 200 GPa and the superconducting\ncritical temperature increases from 13.51 to 40.63 K.",
        "Quantitative estimation of human joint motion in daily living spaces is\nessential for early detection and rehabilitation tracking of\nneuromusculoskeletal disorders (e.g., Parkinson's) and mitigating trip and fall\nrisks for older adults. Existing approaches involve monitoring devices such as\ncameras, wearables, and pressure mats, but have operational constraints such as\ndirect line-of-sight, carrying devices, and dense deployment. To overcome these\nlimitations, we leverage gait-induced floor vibration to estimate lower-limb\njoint motion (e.g., ankle, knee, and hip flexion angles), allowing\nnon-intrusive and contactless gait health monitoring in people's living spaces.\nTo overcome the high uncertainty in lower-limb movement given the limited\ninformation provided by the gait-induced floor vibrations, we formulate a\nphysics-informed graph to integrate domain knowledge of gait biomechanics and\nstructural dynamics into the model. Specifically, different types of nodes\nrepresent heterogeneous information from joint motions and floor vibrations;\nTheir connecting edges represent the physiological relationships between joints\nand forces governed by gait biomechanics, as well as the relationships between\nforces and floor responses governed by the structural dynamics. As a result,\nour model poses physical constraints to reduce uncertainty while allowing\ninformation sharing between the body and the floor to make more accurate\npredictions. We evaluate our approach with 20 participants through a real-world\nwalking experiment. We achieved an average of 3.7 degrees of mean absolute\nerror in estimating 12 joint flexion angles (38% error reduction from\nbaseline), which is comparable to the performance of cameras and wearables in\ncurrent medical practices.",
        "Polar Ring Galaxies (PRGs) are a unique class of galaxies characterised by a\nring of gas and stars orbiting nearly orthogonal to the main body. This study\ndelves into the evolutionary trajectory of PRGs using the exemplary trio of NGC\n3718, NGC 2685, and NGC 4262. We investigate the distinct features of PRGs by\nanalysing their ring and host components to reveal their unique characteristics\nthrough Spectral Energy Distribution (SED) fitting. Using CIGALE, we performed\nSED fitting to independently analyse the ring and host spatially resolved\nregions, marking the first decomposed SED analysis for PRGs, which examines\nstellar populations using high-resolution observations from AstroSat UVIT at a\nresolved scale. The UV-optical surface profiles provide an initial idea that\ndistinct patterns in the galaxies, with differences in FUV and NUV, suggest\nthree distinct stages of ring evolution in the selected galaxies. The study of\nresolved-scale stellar regions reveals that the ring regions are generally\nyounger than their host galaxies, with the age disparity progressively\ndecreasing along the evolutionary sequence from NGC 3718 to NGC 4262. Star\nformation rates (SFR) also exhibit a consistent pattern, with higher SFR in the\nring of NGC 3718 compared to the others, and a progressive decrease through NGC\n2685 and NGC 4262. Finally, the representation of the galaxies in the HI gas\nfraction versus the NUV- r plane supports the idea that they are in three\ndifferent evolutionary stages of PRG evolution, with NGC 3718 in the initial\nstage, NGC 2685 in the intermediate stage, and NGC 4262 representing the final\nstage. NGC 3718, NGC 2685, and NGC 4262 represent different stages of this\nevolution, highlighting the dynamic nature of PRGs and emphasising the\nimportance of studying their evolutionary processes to gain insights into\ngalactic formation and evolution.",
        "Financial markets are complex systems characterized by high statistical\nnoise, nonlinearity, and constant evolution. Thus, modeling them is extremely\nhard. We address the task of generating realistic and responsive Limit Order\nBook (LOB) market simulations, which are fundamental for calibrating and\ntesting trading strategies, performing market impact experiments, and\ngenerating synthetic market data. Previous works lack realism, usefulness, and\nresponsiveness of the generated simulations. To bridge this gap, we propose a\nnovel TRAnsformer-based Denoising Diffusion Probabilistic Engine for LOB\nSimulations (TRADES). TRADES generates realistic order flows conditioned on the\nstate of the market, leveraging a transformer-based architecture that captures\nthe temporal and spatial characteristics of high-frequency market data. There\nis a notable absence of quantitative metrics for evaluating generative market\nsimulation models in the literature. To tackle this problem, we adapt the\npredictive score, a metric measured as an MAE, by training a stock price\npredictive model on synthetic data and testing it on real data. We compare\nTRADES with previous works on two stocks, reporting an x3.27 and x3.47\nimprovement over SoTA according to the predictive score, demonstrating that we\ngenerate useful synthetic market data for financial downstream tasks. We assess\nTRADES's market simulation realism and responsiveness, showing that it\neffectively learns the conditional data distribution and successfully reacts to\nan experimental agent, giving sprout to possible calibrations and evaluations\nof trading strategies and market impact experiments. We developed DeepMarket,\nthe first open-source Python framework for market simulation with deep\nlearning. Our repository includes a synthetic LOB dataset composed of TRADES's\ngenerates simulations. We release the code at\ngithub.com\/LeonardoBerti00\/DeepMarket.",
        "Large language models (LLMs) should undergo rigorous audits to identify\npotential risks, such as copyright and privacy infringements. Once these risks\nemerge, timely updates are crucial to remove undesirable responses, ensuring\nlegal and safe model usage. It has spurred recent research into LLM unlearning,\nfocusing on erasing targeted undesirable knowledge without compromising the\nintegrity of other, non-targeted responses. Existing studies have introduced\nvarious unlearning objectives to pursue LLM unlearning without necessitating\ncomplete retraining. However, each of these objectives has unique properties,\nand no unified framework is currently available to comprehend them thoroughly.\nTo fill the gap, we propose a toolkit of the gradient effect (G-effect),\nquantifying the impacts of unlearning objectives on model performance from a\ngradient perspective. A notable advantage is its broad ability to detail the\nunlearning impacts from various aspects across instances, updating steps, and\nLLM layers. Accordingly, the G-effect offers new insights into identifying\ndrawbacks of existing unlearning objectives, further motivating us to explore a\nseries of new solutions for their mitigation and improvements. Finally, we\noutline promising directions that merit further studies, aiming at contributing\nto the community to advance this important field.",
        "Machine learning, particularly convolutional neural networks (CNNs), has\nshown promise in medical image analysis, especially for thoracic disease\ndetection using chest X-ray images. In this study, we evaluate various CNN\narchitectures, including binary classification, multi-label classification, and\nResNet50 models, to address challenges like dataset imbalance, variations in\nimage quality, and hidden biases. We introduce advanced preprocessing\ntechniques such as principal component analysis (PCA) for image compression and\npropose a novel class-weighted loss function to mitigate imbalance issues. Our\nresults highlight the potential of CNNs in medical imaging but emphasize that\nissues like unbalanced datasets and variations in image acquisition methods\nmust be addressed for optimal model performance.",
        "Cellular automata are computers, similar to Turing machines. The main\ndifference is that Turing machines use a one-dimensional tape, whereas cellular\nautomata use a two-dimensional grid. The best-known cellular automaton is the\nGame of Life, which is a universal computer. It belongs to a family of cellular\nautomata with 262,144 members. Playing the Game of Life generally involves\nengineering; that is, assembling a device composed of various parts that are\ncombined to achieve a specific intended result. Instead of engineering cellular\nautomata, we propose evolving cellular automata. Evolution applies mutation and\nselection to a population of organisms. If a mutation increases the fitness of\nan organism, it may have many descendants, displacing the less fit organisms.\nUnlike engineering, evolution does not work towards an imagined goal. Evolution\nworks towards increasing fitness, with no expectations about the specific form\nof the final result. Mutation, selection, and fitness yield structures that\nappear to be more organic and life-like than engineered structures. In our\nexperiments, the patterns resulting from evolving cellular automata look much\nlike the spots on leopards and the stripes on tigers.",
        "The solar system planetary architecture has been proposed to be consistent\nwith the terrestrial and giant planets forming from material rings at ~1 au and\n~5 au, respectively. Here, we show that super-Earths and mini-Neptunes may\nshare a similar formation pathway. In our simulations conducted with a disk\nalpha-viscosity of 4e-3, super-Earths accrete from rings of rocky material in\nthe inner disk, growing predominantly via planetesimal accretion. Mini-Neptunes\nprimarily originate from rings located beyond the water snowline, forming via\npebble accretion. Our simulations broadly match the period-ratio distribution,\nthe intra-system size uniformity, and the planet multiplicity distribution of\nexoplanets. The radius valley constrains the typical total mass available for\nrocky planet formation to be less than 3-6 Earth masses. Our results predict\nthat planets at ~1 au in systems with close-in super-Earths and mini-Neptunes\nare predominantly water-rich. Though relatively uncommon, at ~1% level, such\nsystems might also host rocky Earth-sized planets in the habitable zone that\nunderwent late giant impacts, akin to the Moon-forming event.",
        "Novel view synthesis has long been a practical but challenging task, although\nthe introduction of numerous methods to solve this problem, even combining\nadvanced representations like 3D Gaussian Splatting, they still struggle to\nrecover high-quality results and often consume too much storage memory and\ntraining time. In this paper we propose Swift4D, a divide-and-conquer 3D\nGaussian Splatting method that can handle static and dynamic primitives\nseparately, achieving a good trade-off between rendering quality and\nefficiency, motivated by the fact that most of the scene is the static\nprimitive and does not require additional dynamic properties. Concretely, we\nfocus on modeling dynamic transformations only for the dynamic primitives which\nbenefits both efficiency and quality. We first employ a learnable decomposition\nstrategy to separate the primitives, which relies on an additional parameter to\nclassify primitives as static or dynamic. For the dynamic primitives, we employ\na compact multi-resolution 4D Hash mapper to transform these primitives from\ncanonical space into deformation space at each timestamp, and then mix the\nstatic and dynamic primitives to produce the final output. This\ndivide-and-conquer method facilitates efficient training and reduces storage\nredundancy. Our method not only achieves state-of-the-art rendering quality\nwhile being 20X faster in training than previous SOTA methods with a minimum\nstorage requirement of only 30MB on real-world datasets. Code is available at\nhttps:\/\/github.com\/WuJH2001\/swift4d.",
        "This paper presents linear DML models for causal inference using the simplest\nPython code on a Jupyter notebook based on an Anaconda platform and compares\nthe performance of different DML models. The results show that current Library\nAPI technology is not yet sufficient to enable novice Python users to build\nqualified and high-quality DML models with the simplest coding approach. Novice\nusers attempting to perform DML causal inference using Python still have to\nimprove their mathematical and computer knowledge to adapt to more flexible DML\nprogramming. Additionally, the issue of mismatched outcome variable dimensions\nis also widespread when building linear DML models in Jupyter notebook.",
        "We present the results of a search for nonlinear gravitational wave memory in\nthe NANOGrav 15-year data set. We find no significant evidence for memory\nsignals in the dataset, with a maximum Bayes factor of 3.1 in favor of a model\nincluding memory. We therefore place upper limits on the strain of potential\ngravitational wave memory events as a function of sky location and observing\nepoch. We find upper limits that are not always more constraining than previous\nNANOGrav results. We show that it is likely due to the increase in common red\nnoise between the 12.5-year and 15-year NANOGrav datasets.",
        "This paper considers the online nonstochastic control problem of a linear\ntime-invariant system under convex state and input constraints that need to be\nsatisfied at all times. We propose an algorithm called Online Gradient Descent\nwith Buffer Zone for Convex Constraints (OGD-BZC), designed to handle scenarios\nwhere the system operates within general convex safety constraints. We\ndemonstrate that OGD-BZC, with appropriate parameter selection, satisfies all\nthe safety constraints under bounded adversarial disturbances. Additionally, to\nevaluate the performance of OGD-BZC, we define the regret with respect to the\nbest safe linear policy in hindsight. We prove that OGD-BZC achieves $\\tilde{O}\n(\\sqrt{T})$ regret given proper parameter choices. Our numerical results\nhighlight the efficacy and robustness of the proposed algorithm.",
        "One of the bottlenecks in robotic intelligence is the instability of neural\nnetwork models, which, unlike control models, lack a well-defined convergence\ndomain and stability. This leads to risks when applying intelligence in the\nphysical world. Specifically, imitation policy based on neural network may\ngenerate hallucinations, leading to inaccurate behaviors that impact the safety\nof real-world applications. To address this issue, this paper proposes the\nCuriosity-Diffuser, aimed at guiding the conditional diffusion model to\ngenerate trajectories with lower curiosity, thereby improving the reliability\nof policy. The core idea is to use a Random Network Distillation (RND)\ncuriosity module to assess whether the model's behavior aligns with the\ntraining data, and then minimize curiosity by classifier guidance diffusion to\nreduce overgeneralization during inference. Additionally, we propose a\ncomputationally efficient metric for evaluating the reliability of the policy,\nmeasuring the similarity between the generated behaviors and the training\ndataset, to facilitate research about reliability learning. Finally, simulation\nverify the effectiveness and applicability of the proposed method to a variety\nof scenarios, showing that Curiosity-Diffuser significantly improves task\nperformance and produces behaviors that are more similar to the training data.\nThe code for this work is available at: github.com\/CarlDegio\/Curiosity-Diffuser",
        "Recent research has increasingly focused on training large language models\n(LLMs) using federated learning, known as FedLLM. However, responsible AI\n(RAI), which aims to ensure safe responses, remains underexplored in the\ncontext of FedLLM. In FedLLM, client data used for training may contain harmful\ncontent, leading to unsafe LLMs that generate harmful responses. Aggregating\nsuch unsafe LLMs into the global model and distributing them to clients may\nresult in the widespread deployment of unsafe LLMs. To address this issue, we\nincorporate two well-known RAI methods into FedLLM: the safety filter and\nconstitutional AI. Our experiments demonstrate that these methods significantly\nenhance the safety of the LLM, achieving over a 20% improvement on AdvBench, a\nbenchmark for evaluating safety performance.",
        "A theoretical model is proposed for the formation of ultracold ground-state\ntriatomic molecules in weakly bound energy levels. The process is driven by the\nelectric component of a microwave field, which induces the association of an\nultracold atom colliding with an ultracold diatomic molecule. This model is\nexemplified using $^{39}$K atoms and $^{23}$Na$^{39}$K molecules, both in their\nground states, a scenario of experimental relevance. The model assumes that the\ndynamics of the association are dominated by the long-range van der Waals\ninteraction between $^{39}$K and $^{23}$Na$^{39}$K. The electric microwave\nassociation mechanism relies on the intrinsic electric dipole moment of\n$^{23}$Na$^{39}$K, which drives transitions between its lowest rotational\nlevels ( $j$=0 and $j$=1). The energies of the uppermost triatomic energy\nlevels are computed by numerically solving coupled Schr\\\"odinger equations\nusing the Mapped Fourier Grid Hamiltonian method. Measurable association rates\nare derived within the framework of a perturbative approach. This method of\nelectric microwave association provides an alternative to atom-molecule\nassociation via magnetic Feshbach resonances for forming ultracold, deeply\nbound triatomic molecules, and is applicable to a wide range of polar diatomic\nmolecules.",
        "Unsupervised pre-training and transfer learning are commonly used techniques\nto initialize training algorithms for neural networks, particularly in settings\nwith limited labeled data. In this paper, we study the effects of unsupervised\npre-training and transfer learning on the sample complexity of high-dimensional\nsupervised learning. Specifically, we consider the problem of training a\nsingle-layer neural network via online stochastic gradient descent. We\nestablish that pre-training and transfer learning (under concept shift) reduce\nsample complexity by polynomial factors (in the dimension) under very general\nassumptions. We also uncover some surprising settings where pre-training grants\nexponential improvement over random initialization in terms of sample\ncomplexity.",
        "Recent advances in transformer-based Large Language Models (LLMs) have\ndemonstrated remarkable capabilities across various tasks. However, their\nquadratic computational complexity concerning sequence length remains a\nsignificant bottleneck for processing long documents. As a result, many efforts\nlike sparse attention and state space models have been proposed to improve the\nefficiency of LLMs over long sequences. Though effective, these approaches\ncompromise the performance or introduce structural complexity. This calls for a\nsimple yet efficient model that preserves the fundamental Transformer\narchitecture. To this end, we introduce SWAT, which enables efficient\nlong-context handling via Sliding Window Attention Training. This paper first\nattributes the inefficiency of Transformers to the attention sink phenomenon\nresulting from the high variance of softmax operation. Then, we replace softmax\nwith the sigmoid function and utilize a balanced ALiBi and Rotary Position\nEmbedding for efficient information compression and retention. Experiments\ndemonstrate that SWAT achieves SOTA performance compared with state-of-the-art\nlinear recurrent architectures on eight benchmarks. Code is available at\nhttps:\/\/anonymous.4open.science\/r\/SWAT-attention.",
        "We propose two cooperative beamforming frameworks based on federated learning\n(FL) for multi-cell integrated sensing and communications (ISAC) systems. Our\nobjective is to address the following dilemma in multicell ISAC: 1) Beamforming\nstrategies that rely solely on local channel information risk generating\nsignificant inter-cell interference (ICI), which degrades network performance\nfor both communication users and sensing receivers in neighboring cells; 2)\nconversely centralized beamforming strategies can mitigate ICI by leveraging\nglobal channel information, but they come with substantial transmission\noverhead and latency that can be prohibitive for latency-sensitive and\nsource-constrained applications. To tackle these challenges, we first propose a\npartially decentralized training framework motivated by the vertical federated\nlearning (VFL) paradigm. In this framework, the participating base stations\n(BSs) collaboratively design beamforming matrices under the guidance of a\ncentral server. The central server aggregates local information from the BSs\nand provides feedback, allowing BSs to implicitly manage ICI without accessing\nthe global channel information. To make the solution scalable for densely\ndeployed wireless networks, we take further steps to reduce communication\noverhead by presenting a fully decentralized design based on the horizontal\nfederated learning (HFL). Specifically, we develop a novel loss function to\ncontrol the interference leakage power, enabling a more efficient training\nprocess by entirely eliminating local channel information exchange. Numerical\nresults show that the proposed solutions can achieve significant performance\nimprovements comparable to the benchmarks in terms of both communication and\nradar information rates.",
        "Chimera states in systems of nonlocally coupled oscillators, i.e.,\nself-organized coexistence of coherent and incoherent oscillator populations,\nhave attracted much attention. In this study, we consider the effect of\nfrequency heterogeneities on the chimera state and reveal that it induces\nspatial locking of the chimera state, i.e., the coherent and incoherent domains\nalign with lower and higher frequency regions, respectively, in a self-adaptive\nmanner. Using an extended self-consistency approach, we show that such\nspatially locked chimera states can be reproduced as steady solutions of the\nsystem in the continuum limit. Furthermore, we develop a variational argument\nto explain the mechanism leading to spatial locking. Our analysis reveals how\nheterogeneity can affect the collective dynamics of the chimera states and\noffers insights into their control and applications.",
        "The Ni$^{12+}$ ion features an electronic transition with a natural width of\nonly 8 mHz, allowing for a highly stable optical clock. We predict that the\nenergy of this strongly forbidden $3s^2 3p^4\\, ^3\\!P_2 \\rightarrow 3s^2 3p^4 \\,\n^3\\!P_0$ electric quadrupole transition is 20081(10) cm$^{-1}$. For this, we\nuse both a hybrid approach combining configuration interaction (CI) with\ncoupled-cluster (CC) method and a pure CI calculation for the complete\n16-electron system, ensuring convergence. The resulting very small theoretical\nuncertainty of only 0.05\\% allowed us to find the transition experimentally in\na few hours, yielding an energy of 20078.984(10) cm$^{-1}$. This level of\nagreement for a 16-electron system is unprecedented and qualifies our method\nfor future calculations of many other complex atomic systems. While paving the\nway for a high-precision optical clock based on Ni$^{12+}$, our theory and code\ndevelopment will also enable better predictions for other highly charged ions\nand other complex atomic systems.",
        "We develop a perfect foresight method to solve models with an interest rate\nlower bound constraint that nests OccBin\/DynareOBC and \\cite{Eggertsson2010}'s\nas well as \\cite{Mertens2014}'s pen and paper solutions as special cases. Our\nmethod generalizes the pen-and-paper solutions by allowing for endogenous\npersistence while maintaining tractability and interpretability. We prove that\nour method necessarily gives stable multipliers. We use it to solve a New\nKeynesian model with habit formation and government spending, which we match to\nexpectations data from the Great Recession. We find an output multiplier of\ngovernment spending close to 1 for the US and Japan.",
        "Objective: This study explores a novel deep learning approach for EEG\nanalysis and perceptual state guidance, inspired by Level of Detail (LOD)\ntheory. The goal is to improve perceptual state identification accuracy and\nadvance personalized psychological therapy. Methods: Portable EEG devices and\nmusic rhythm signals were used for data collection. LOD theory was applied to\ndynamically adjust EEG signal processing, extracting core perceptual features.\nA Unity-based software system integrated EEG data with audio materials. The\ndeep learning model combined a CNN for feature extraction and classification,\nand a DQN for reinforcement learning to optimize rhythm adjustments. Results:\nThe CNN achieved 94.05% accuracy in perceptual state classification. The DQN\nguided subjects to target states with a 92.45% success rate, averaging 13.2\nrhythm cycles. However, only 50% of users reported psychological alignment with\nthe target state, indicating room for improvement. Discussion: The results\nvalidate the potential of LOD-based EEG biofeedback. Limitations include\ndataset source, label subjectivity, and reward function optimization. Future\nwork will expand to diverse subjects, incorporate varied musical elements, and\nrefine reward functions for better generalization and personalization.",
        "Training data quality is one of the most important drivers of final model\nquality. In this work, we introduce a method for evaluating data integrity\nbased on the assumption that low-quality input prompts result in high variance\nand low quality responses. This is achieved by measuring the rejected response\nquality and the reward gap between the chosen and rejected preference pair. Our\nmethod, Rejecting Instruction Preferences (RIP) can be used to filter prompts\nfrom existing training sets, or to make high quality synthetic datasets,\nyielding large performance gains across various benchmarks compared to\nunfiltered data. Using Llama 3.1-8B-Instruct, RIP improves AlpacaEval2 LC Win\nRate by 9.4%, Arena-Hard by 8.7%, and WildBench by 9.9%. Using Llama\n3.3-70B-Instruct, RIP improves Arena-Hard from 67.5 to 82.9, which is from 18th\nplace to 6th overall in the leaderboard."
      ]
    }
  },
  {
    "id":2411.00575,
    "research_type":"basic",
    "start_id":"b0",
    "start_title":"Weak Existence for the Semigeostrophic Equations Formulated as a Coupled Monge--Amp\u00e8re\/Transport Problem",
    "start_abstract":"Hoskins's semigeostrophic equations are reformulated as a coupled Monge--Amp\u00e8re\/ transport problem [B. J. Hoskins, Quart. Royal Met. Soc., 97 (1971), pp. 139--153]. Existence of global weak solutions is obtained for this formulation.",
    "start_categories":[
      "math.OC"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b22"
      ],
      "title":[
        "Vertical slice modelling of nonlinear Eady waves using a compatible finite element method"
      ],
      "abstract":[
        "A vertical slice model is developed for the Euler-Boussinesq equations with a\nconstant temperature gradient in the direction normal to the slice (the\nEady-Boussinesq model). The model is a solution of the full three-dimensional\nequations with no variation normal to the slice, which is an idealized problem\nused to study the formation and subsequent evolution of weather fronts. A\ncompatible finite element method is used to discretise the governing equations.\nTo extend the Charney-Phillips grid staggering in the compatible finite element\nframework, we use the same node locations for buoyancy as the vertical part of\nvelocity and apply a transport scheme for a partially continuous finite element\nspace. For the time discretisation, we solve the semi-implicit equations\ntogether with an explicit strong-stability-preserving Runge-Kutta scheme to all\nof the advection terms. The model reproduces several quasi-periodic lifecycles\nof fronts despite the presence of strong discontinuities. An asymptotic limit\nanalysis based on the semi-geostrophic theory shows that the model solutions\nare converging to a solution in cross-front geostrophic balance. The results\nare consistent with the previous results using finite difference methods,\nindicating that the compatible finite element method is performing as well as\nfinite difference methods for this test problem. We observe dissipation of\nkinetic energy of the cross-front velocity in the model due to the lack of\nresolution at the fronts, even though the energy loss is not likely to account\nfor the large gap on the strength of the fronts between the model result and\nthe semi-geostrophic limit solution."
      ],
      "categories":[
        "math.MP"
      ]
    },
    "list":{
      "title":[
        "Decentralized RISE-based Control for Exponential Heterogeneous\n  Multi-Agent Target Tracking of Second-Order Nonlinear Systems",
        "On the Origin and Fate of Our Universe",
        "Trend-Aware Supervision: On Learning Invariance for Semi-Supervised\n  Facial Action Unit Intensity Estimation",
        "Data-Driven Decision Making for Enhancing Small-Signal Stability in\n  Hybrid AC\/DC Grids Through Converter Control Role Assignment",
        "Multi-Year-to-Decadal Temperature Prediction using a Machine Learning\n  Model-Analog Framework",
        "Decentralized Inference for Spatial Data Using Low-Rank Models",
        "Quadratic quasinormal modes at null infinity on a Schwarzschild\n  spacetime",
        "On the Adversarial Vulnerabilities of Transfer Learning in Remote\n  Sensing",
        "Stress field in the vicinity of a bubble\/sphere moving in a dilute\n  surfactant solution",
        "A Hybrid Swarm Intelligence Approach for Optimizing Multimodal Large\n  Language Models Deployment in Edge-Cloud-based Federated Learning\n  Environments",
        "Making Puzzle Pieces Fit or Reshaping MiMiC for Multiscale Simulations\n  with CP2K and More",
        "To Hedge or Not to Hedge: Optimal Strategies for Stochastic Trade Flow\n  Management",
        "Generalized Venn and Venn-Abers Calibration with Applications in\n  Conformal Prediction",
        "Keyword Search in the Deep Web",
        "Holistic Semantic Representation for Navigational Trajectory Generation",
        "Fast Two-photon Microscopy by Neuroimaging with Oblong Random\n  Acquisition (NORA)",
        "Diatomic and Polyatomic Heteronuclear Ultralong-Range Rydberg Molecules",
        "Probing Green's Function Zeros by Co-tunneling through Mott Insulators",
        "Recursive Inference Scaling: A Winning Path to Scalable Inference in\n  Language and Multimodal Systems",
        "L1: Controlling How Long A Reasoning Model Thinks With Reinforcement\n  Learning",
        "Low RCS High-Gain Broadband Substrate Integrated Waveguide Antenna Based\n  on Elliptical Polarization Conversion Metasurface",
        "OverThink: Slowdown Attacks on Reasoning LLMs",
        "Scalable solution chemical synthesis and comprehensive analysis of\n  Bi2Te3 and Sb2Te3",
        "Model Selection for Off-policy Evaluation: New Algorithms and\n  Experimental Protocol",
        "Projecting Assumptions: The Duality Between Sparse Autoencoders and\n  Concept Geometry",
        "Homomorphic Encryption in Healthcare Industry Applications for\n  Protecting Data Privacy",
        "A space-resolved visible spectrometer system using compact endoscopic\n  optics for full vertical profile measurement of impurity line emissions in\n  superconducting EAST tokamak",
        "Small Binary Stabilizer Subsystem Codes",
        "A Corrugated All-Metal Vivaldi Antenna for 5G Phased Array Applications"
      ],
      "abstract":[
        "This work presents a decentralized implementation of a Robust Integral of the\nSign of the Error (RISE) controller for multi-agent target tracking problems\nwith exponential convergence guarantees. Previous RISE-based approaches for\nmulti-agent systems required 2-hop communication, limiting practical\napplicability. New insights from a Lyapunov-based design-analysis approach are\nused to eliminate the need for multi-hop communication required in previous\nliterature, while yielding exponential target tracking. The new insights\ninclude the development of a new P-function which is developed which works in\ntandem with the inclusion of the interaction matrix in the Lyapunov function.\nNonsmooth Lyapunov-based stability analysis methods are used to yield\nsemi-global exponential convergence to the target agent state despite the\npresence of bounded disturbances with bounded derivatives. The resulting\noutcome is a controller that achieves exponential target tracking with only\nlocal information exchange between neighboring agents.",
        "This brief review, intended for high energy and astrophysics researchers,\nexplores the implications of recent theoretical advances in string theory and\nthe Swampland program for understanding bounds on the structure of positive\npotentials allowed in quantum gravity. This has a bearing on both inflationary\nmodels for the early universe as well as the fate of our universe. The paper\nincludes a review of the dS conjecture as well as the TransPlanckian Censorship\nConjecture (TCC) and its relation to the species scale. We provide evidence for\nthese principles as well as what they may lead to in terms of phenomenological\npredictions. (Talk presented at Lemaitre Conference 2024)",
        "With the increasing need for facial behavior analysis, semi-supervised AU\nintensity estimation using only keyframe annotations has emerged as a practical\nand effective solution to relieve the burden of annotation. However, the lack\nof annotations makes the spurious correlation problem caused by AU\nco-occurrences and subject variation much more prominent, leading to non-robust\nintensity estimation that is entangled among AUs and biased among subjects. We\nobserve that trend information inherent in keyframe annotations could act as\nextra supervision and raising the awareness of AU-specific facial appearance\nchanging trends during training is the key to learning invariant AU-specific\nfeatures. To this end, we propose \\textbf{T}rend-\\textbf{A}ware\n\\textbf{S}upervision (TAS), which pursues three kinds of trend awareness,\nincluding intra-trend ranking awareness, intra-trend speed awareness, and\ninter-trend subject awareness. TAS alleviates the spurious correlation problem\nby raising trend awareness during training to learn AU-specific features that\nrepresent the corresponding facial appearance changes, to achieve intensity\nestimation invariance. Experiments conducted on two commonly used AU benchmark\ndatasets, BP4D and DISFA, show the effectiveness of each kind of awareness. And\nunder trend-aware supervision, the performance can be improved without extra\ncomputational or storage costs during inference.",
        "Hybrid AC\/DC transmission grids incorporate Modular Multilevel Converters\nfunctioning as Interconnecting Power Converters (IPCs). The control role\nassigned to each converter significantly influences grid dynamics.\nTraditionally, these converters operate with static control roles, but recent\nstudies have proposed scheduling their roles based on day-ahead forecasts to\nenhance stability performance. However, in systems with high renewable energy\npenetration, forecast deviations can render scheduled control assignments\nsuboptimal or even lead to instability. To address this challenge, this work\nproposes an online scheduling recalculation algorithm that dynamically adapts\nIPC control roles during system operation. The approach leverages a data-driven\nmulti-criteria decision-making framework, integrating surrogate models of\nconventional small-signal stability analysis tools to enable a fast computation\nof system stability and stability performance indicators.",
        "Multi-year-to-decadal climate prediction is a key tool in understanding the\nrange of potential regional and global climate futures. Here, we present a\nframework that combines machine learning and analog forecasting for predictions\non these timescales. A neural network is used to learn a mask, specific to a\nregion and lead time, with global weights based on relative importance as\nprecursors to the evolution of that prediction target. A library of\nmask-weighted model states, or potential analogs, are then compared to a single\nmask-weighted observational state. The known future of the best matching\npotential analogs serve as the prediction for the future of the observational\nstate. We match and predict 2-meter temperature using the Berkeley Earth\nSurface Temperature dataset for observations, and a set of CMIP6 models as the\nanalog library. We find improved performance over traditional analog methods\nand initialized decadal predictions.",
        "Advancements in information technology have enabled the creation of massive\nspatial datasets, driving the need for scalable and efficient computational\nmethodologies. While offering viable solutions, centralized frameworks are\nlimited by vulnerabilities such as single-point failures and communication\nbottlenecks. This paper presents a decentralized framework tailored for\nparameter inference in spatial low-rank models to address these challenges. A\nkey obstacle arises from the spatial dependence among observations, which\nprevents the log-likelihood from being expressed as a summation-a critical\nrequirement for decentralized optimization approaches. To overcome this\nchallenge, we propose a novel objective function leveraging the evidence lower\nbound, which facilitates the use of decentralized optimization techniques. Our\napproach employs a block descent method integrated with multi-consensus and\ndynamic consensus averaging for effective parameter optimization. We prove the\nconvexity of the new objective function in the vicinity of the true parameters,\nensuring the convergence of the proposed method. Additionally, we present the\nfirst theoretical results establishing the consistency and asymptotic normality\nof the estimator within the context of spatial low-rank models. Extensive\nsimulations and real-world data experiments corroborate these theoretical\nfindings, showcasing the robustness and scalability of the framework.",
        "The ringdown of perturbed black holes has been studied since the 1970s, but\nuntil recently, studies have focused on linear perturbations. There is now\nburgeoning interest in nonlinear perturbative effects during ringdown. Here,\nusing a hyperboloidal framework, we provide a complete treatment of linear and\nquadratic quasinormal modes (QNMs and QQNMs) in second-order perturbation\ntheory, in Schwarzschild spacetime. We include novel methods for extracting\nQNMs and QQNMs amplitudes using a Laplace transform treatment, allowing for the\ninclusion of arbitrary initial data. We produce both time- and frequency-domain\ncodes. From these codes, we present new results further exploring the\nunforeseen dependence of QQNMs amplitudes on the parity of the progenitor\nsystem, as demonstrated in our letter [Phys. Rev. Lett. 134, 061401 (2025)].\nOur numerical results are restricted to perturbations of a Schwarzschild black\nhole, but our methods extend straightforwardly to the astrophysically realistic\ncase of a Kerr black hole.",
        "The use of pretrained models from general computer vision tasks is widespread\nin remote sensing, significantly reducing training costs and improving\nperformance. However, this practice also introduces vulnerabilities to\ndownstream tasks, where publicly available pretrained models can be used as a\nproxy to compromise downstream models. This paper presents a novel Adversarial\nNeuron Manipulation method, which generates transferable perturbations by\nselectively manipulating single or multiple neurons in pretrained models.\nUnlike existing attacks, this method eliminates the need for domain-specific\ninformation, making it more broadly applicable and efficient. By targeting\nmultiple fragile neurons, the perturbations achieve superior attack\nperformance, revealing critical vulnerabilities in deep learning models.\nExperiments on diverse models and remote sensing datasets validate the\neffectiveness of the proposed method. This low-access adversarial neuron\nmanipulation technique highlights a significant security risk in transfer\nlearning models, emphasizing the urgent need for more robust defenses in their\ndesign when addressing the safety-critical remote sensing tasks.",
        "In this study, we experimentally investigate the stress field around a bubble\nrising in a dilute surfactant solution (20 < Re < 220, high Peclet numbers)\nwhose surface gradually becomes contaminated, and compare it with that around a\nsphere free from surface contamination. We employ a newly developed\npolarization measurement technique, highly sensitive to stress fields near\ninterfaces. First, we validate this method by measuring the flow around a solid\nsphere settling at Re = 120 and comparing results with numerical predictions,\nconfirming its accuracy. We then measure the stress field around a bubble whose\ndrag force transitions from that of a clean interface to that of a rigid\ninterface within the observation region. The stress near the bubble's front\nresembles that of a clean bubble, while the rear behaves like a solid sphere.\nBetween these regions, a discontinuous phase retardation near the cap angle\nindicates a transition from slip to no-slip boundary conditions. Axisymmetric\nstress reconstruction reveals localized stress spike at the cap angle, which\nshifts as surfactant accumulates and increases the drag. Remarkably, the\nmeasured cap angle versus normalized drag coefficient agrees well with\nnumerical simulations at Re = 100 (Cuenot et al. 1997) and shows only a slight\ndeviation from the creeping-flow stagnant cap model (Sadhal and Johnson 1983).\nThis work demonstrates that polarization-based stress field measurements\neffectively capture the interplay between surface contamination and\nhydrodynamics at intermediate Reynolds numbers.",
        "The combination of Federated Learning (FL), Multimodal Large Language Models\n(MLLMs), and edge-cloud computing enables distributed and real-time data\nprocessing while preserving privacy across edge devices and cloud\ninfrastructure. However, the deployment of MLLMs in FL environments with\nresource-constrained edge devices presents significant challenges, including\nresource management, communication overhead, and non-IID data. To address these\nchallenges, we propose a novel hybrid framework wherein MLLMs are deployed on\nedge devices equipped with sufficient resources and battery life, while the\nmajority of training occurs in the cloud. To identify suitable edge devices for\ndeployment, we employ Particle Swarm Optimization (PSO), and Ant Colony\nOptimization (ACO) is utilized to optimize the transmission of model updates\nbetween edge and cloud nodes. This proposed swarm intelligence-based framework\naims to enhance the efficiency of MLLM training by conducting extensive\ntraining in the cloud and fine-tuning at the edge, thereby reducing energy\nconsumption and communication costs. Our experimental results show that the\nproposed method significantly improves system performance, achieving an\naccuracy of 92%, reducing communication cost by 30%, and enhancing client\nparticipation compared to traditional FL methods. These results make the\nproposed approach highly suitable for large-scale edge-cloud computing systems.",
        "MiMiC is a framework for modeling large-scale chemical processes that require\ntreatment at multiple resolutions. It does not aim to implement single-handedly\nall methods required to treat individual subsystems, but instead, it relegates\nthis task to specialized computational chemistry software while it serves as an\nintermediary between these external programs, and computes the interactions\nbetween the subsystems. MiMiC minimizes issues typically associated with\nmolecular dynamics performed with multiple programs, by adopting a\nmultiple-program multiple-data paradigm combined with a loose-coupling model.\nIn this article, we present the addition of a new client program, CP2K, to the\nMiMiC ecosystem, which required a major refactoring of the entire framework and\nin the end allowed us to unlock its full flexibility. By thorough timing\nanalysis, we verify that the introduced changes do not affect the performance\nof MiMiC or CP2K, and neither are they a source of significant computational\noverheads that would be detrimental to simulation efficiency. Moreover, we\ndemonstrate the benefits of the framework's modular design, by performing a\nQM\/MM MD simulation combining CP2K with previously interfaced OpenMM, with no\nadditional implementation effort required.",
        "This paper addresses the trade-off between internalisation and\nexternalisation in the management of stochastic trade flows. We consider agents\nwho must absorb flows and manage risk by deciding whether to warehouse it or\nhedge in the market, thereby incurring transaction costs and market impact.\nUnlike market makers, these agents cannot skew their quotes to attract\noffsetting flows and deter risk-increasing ones, leading to a fundamentally\ndifferent problem. Within the Almgren-Chriss framework, we derive\nalmost-closed-form solutions in the case of quadratic execution costs, while\nmore general cases require numerical methods. In particular, we discuss the\nchallenges posed by artificial boundary conditions when using classical\ngrid-based numerical PDE techniques and propose reinforcement learning methods\nas an alternative.",
        "Ensuring model calibration is critical for reliable predictions, yet popular\ndistribution-free methods, such as histogram binning and isotonic regression,\nprovide only asymptotic guarantees. We introduce a unified framework for Venn\nand Venn-Abers calibration, generalizing Vovk's binary classification approach\nto arbitrary prediction tasks and loss functions. Venn calibration leverages\nbinning calibrators to construct prediction sets that contain at least one\nmarginally perfectly calibrated point prediction in finite samples, capturing\nepistemic uncertainty in the calibration process. The width of these sets\nshrinks asymptotically to zero, converging to a conditionally calibrated point\nprediction. Furthermore, we propose Venn multicalibration, a novel methodology\nfor finite-sample calibration across subpopulations. For quantile loss,\ngroup-conditional and multicalibrated conformal prediction arise as special\ncases of Venn multicalibration, and Venn calibration produces novel conformal\nprediction intervals that achieve quantile-conditional coverage. As a separate\ncontribution, we extend distribution-free conditional calibration guarantees of\nhistogram binning and isotonic calibration to general losses.",
        "The Deep Web is constituted by data that are accessible through Web pages,\nbut not readily indexable by search engines as they are returned in dynamic\npages. In this paper we propose a conceptual framework for answering keyword\nqueries on Deep Web sources represented as relational tables with so-called\naccess limitations. We formalize the notion of optimal answer, characterize\nqueries for which an answer can be found, and present a method for query\nprocessing based on the construction of a query plan that minimizes the\naccesses to the data sources.",
        "Trajectory generation has garnered significant attention from researchers in\nthe field of spatio-temporal analysis, as it can generate substantial\nsynthesized human mobility trajectories that enhance user privacy and alleviate\ndata scarcity. However, existing trajectory generation methods often focus on\nimproving trajectory generation quality from a singular perspective, lacking a\ncomprehensive semantic understanding across various scales. Consequently, we\nare inspired to develop a HOlistic SEmantic Representation (HOSER) framework\nfor navigational trajectory generation. Given an origin-and-destination (OD)\npair and the starting time point of a latent trajectory, we first propose a\nRoad Network Encoder to expand the receptive field of road- and zone-level\nsemantics. Second, we design a Multi-Granularity Trajectory Encoder to\nintegrate the spatio-temporal semantics of the generated trajectory at both the\npoint and trajectory levels. Finally, we employ a Destination-Oriented\nNavigator to seamlessly integrate destination-oriented guidance. Extensive\nexperiments on three real-world datasets demonstrate that HOSER outperforms\nstate-of-the-art baselines by a significant margin. Moreover, the model's\nperformance in few-shot learning and zero-shot learning scenarios further\nverifies the effectiveness of our holistic semantic representation.",
        "Advances in neural imaging have enabled neuroscience to study how the joint\nactivity of large neural populations conspire to produce perception, behavior\nand cognition. Despite many advances in optical methods, there exists a\nfundamental tradeoff between imaging speed, field of view, and resolution that\nlimits the scope of neural imaging, especially for the raster-scanning\nmulti-photon imaging needed for imaging deeper into the brain. One approach to\novercoming this trade-off is in computational imaging: the co-development of\noptics and algorithms where the optics are designed to encode the target images\ninto fewer measurements that are faster to acquire, and the algorithms\ncompensate by inverting the optical image coding process to recover a larger or\nhigher resolution image. We present here one such approach for raster-scanning\ntwo-photon imaging: Neuroimaging with Oblong Random Acquisition (NORA). NORA\nquickly acquires each frame in a microscopic video by subsampling only a\nfraction of the fast scanning lines, ignoring large portions of each frame.\nNORA mitigates the information loss by extending the point-spread function in\nthe slow-scan direction to integrate the fluorescence of neighboring lines into\na single set of measurements. By imaging different, randomly selected, lines at\neach frame, NORA diversifies the information collected across frames and\nenables video-level reconstruction. Rather than reconstruct the video\nframe-by-frame using image-level recovery, NORA recovers full video sequences\nthrough a nuclear-norm minimization (i.e., matrix completion) on the\npixels-by-time matrix. We simulated NORA imaging using the Neural Anatomy and\nOptical Microscopy (NAOMi) biophysical simulation suite. Using these\nsimulations we demonstrate that NORA imaging can accurately recover 400 um X\n400 um fields of view at subsampling rates up to 20X, despite realistic noise\nand motion conditions.",
        "Ultra-long-range Rydberg molecules (ULRMs) have attracted significant\ninterest due to their unique electronic properties and potential applications\nin quantum technologies. We theoretically investigate the formation and\ncharacteristics of heteronuclear ULRMs, focusing on Rb-Cs systems. We explore\nthe vibrational energy levels of heteronuclear nD ULRMs and compare them with\nhomonuclear counterparts. We also predict the formation of polyatomic\nheteronuclear ULRMs, discussing how the binding energy and spectral features\nevolve as the number of ground-state atoms increases. Our theoretical\npredictions are presented in terms of molecular spectra and provide insight\ninto the formation dynamics of these systems. The study further explores the\npotential applications of heteronuclear ULRMs in quantum information\nprocessing, quantum simulation, and precision measurements, offering new\navenues for future research in many-body physics and quantum technologies.",
        "Quantum tunneling experiments have provided deep insights into basic\nexcitations occurring as Green's function poles in the realm of complex quantum\nmatter. However, strongly correlated quantum materials also allow for Green's\nfunctions zeros (GFZ) that may be seen as an antidote to the familiar poles,\nand have so far largely eluded direct experimental study. Here, we propose and\ninvestigate theoretically how co-tunneling through Mott insulators enables\ndirect access to the shadow band structure of GFZ. In particular, we derive an\neffective Hamiltonian for the GFZ that is shown to govern the co-tunneling\namplitude and reveal fingerprints of many-body correlations clearly\ndistinguishing the GFZ structure from the underlying free Bloch band structure\nof the system. Our perturbative analytical results are corroborated by\nnumerical data both in the framework of exact diagonalization and matrix\nproduct state simulations for a one-dimensional model system consisting of a\nSu-Schrieffer-Heeger-Hubbard model coupled to two single level quantum dots.",
        "Recent research in language modeling reveals two scaling effects: the\nwell-known improvement from increased training compute, and a lesser-known\nboost from applying more sophisticated or computationally intensive inference\nmethods. Inspired by recent findings on the fractal geometry of language, we\nintroduce Recursive INference Scaling (RINS) as a complementary, plug-in recipe\nfor scaling inference time. For a given fixed model architecture and training\ncompute budget, RINS substantially improves language modeling performance. It\nalso generalizes beyond pure language tasks, delivering gains in multimodal\nsystems, including a +2% improvement in 0-shot ImageNet accuracy for\nSigLIP-B\/16. Additionally, by deriving data scaling laws, we show that RINS\nimproves both the asymptotic performance limits and the scaling exponents.\nThese advantages are maintained even when compared to state-of-the-art\nrecursive techniques like the \"repeat-all-over\" (RAO) strategy in Mobile LLM.\nFinally, stochastic RINS not only can enhance performance further but also\nprovides the flexibility to optionally forgo increased inference computation at\ntest time with minimal performance degradation.",
        "Reasoning language models have shown an uncanny ability to improve\nperformance at test-time by ``thinking longer''-that is, by generating longer\nchain-of-thought sequences and hence using more compute. However, the length of\ntheir chain-of-thought reasoning is not controllable, making it impossible to\nallocate test-time compute to achieve a desired level of performance. We\nintroduce Length Controlled Policy Optimization (LCPO), a simple reinforcement\nlearning method that optimizes for accuracy and adherence to user-specified\nlength constraints. We use LCPO to train L1, a reasoning language model that\nproduces outputs satisfying a length constraint given in its prompt. L1's\nlength control allows for smoothly trading off computational cost and accuracy\non a wide range of tasks, and outperforms the state-of-the-art S1 method for\nlength control. Furthermore, we uncover an unexpected short chain-of-thought\ncapability in models trained with LCPO. For instance, our 1.5B L1 model\nsurpasses GPT-4o at equal reasoning lengths. Overall, LCPO enables precise\ncontrol over reasoning length, allowing for fine-grained allocation of\ntest-time compute and accuracy. We release code and models at\nhttps:\/\/www.cmu-l3.github.io\/l1",
        "Designed an elliptical polarization conversion metasurface (PCM) for Ka-band\napplications, alongside a high-gain substrate integrated waveguide (SIW)\nantenna. The PCM elements are integrated into the antenna design in a\nchessboard array configuration, with the goal of achieving effective reduction\nin the antenna's radar cross section (RCS). Both the PCM elements and antenna\nstructure exhibit a simple design. The top layer of the metasurface (MS)\nelements employs an elliptical pattern symmetric along the diagonal, enabling\nefficient conversion of linearly polarized waves. The antenna component, on the\nother hand, consists of a broadband dipole antenna fed by SIW slot coupling.\nVerified through simulations, the polarization conversion bandwidth of this PCM\nunit reaches 80.38% where polarization conversion ratio (PCR) exceeds 90%\n(25.3-59.3GHz), demonstrating exceptional conversion performance. When the\ndipole antenna is combined with the PCM, its -10dB impedance bandwidth reaches\nto 15.09% (33.7-39.2GHz), with a maximum realized gain of 9.1dBi. Notably, the\nantenna loaded with the chessboard PCM structure effectively disperses the\nenergy of scattered echoes around, significantly reducing the concentration of\nscattered energy in the direction of the incident wave, thereby achieving an\neffective reduction in RCS.",
        "We increase overhead for applications that rely on reasoning LLMs-we force\nmodels to spend an amplified number of reasoning tokens, i.e., \"overthink\", to\nrespond to the user query while providing contextually correct answers. The\nadversary performs an OVERTHINK attack by injecting decoy reasoning problems\ninto the public content that is used by the reasoning LLM (e.g., for RAG\napplications) during inference time. Due to the nature of our decoy problems\n(e.g., a Markov Decision Process), modified texts do not violate safety\nguardrails. We evaluated our attack across closed-(OpenAI o1, o1-mini, o3-mini)\nand open-(DeepSeek R1) weights reasoning models on the FreshQA and SQuAD\ndatasets. Our results show up to 18x slowdown on FreshQA dataset and 46x\nslowdown on SQuAD dataset. The attack also shows high transferability across\nmodels. To protect applications, we discuss and implement defenses leveraging\nLLM-based and system design approaches. Finally, we discuss societal,\nfinancial, and energy impacts of OVERTHINK attack which could amplify the costs\nfor third-party applications operating reasoning models.",
        "Thermoelectric (TE) materials can directly convert heat into electrical\nenergy. However, they sustain costly production procedures and batch-to-batch\nperformance variations. Therefore, developing scalable synthetic techniques for\nlarge-scale and reproducible quality TE materials is critical for advancing TE\ntechnology. This study developed a facile, high throughput, solution-chemical\nsynthetic technique. Microwave-assisted thermolysis process, providing\nenergy-efficient volumetric heating, was used for the synthesis of bismuth and\nantimony telluride (Bi2Te3, Sb2Te3). As-made materials were characterized using\nvarious techniques, including XRPD, SEM, TEM, XAS, and XPS. Detailed\ninvestigation of the local atomic structure of the synthesized Bi2Te3 and\nSb2Te3 powder samples was conducted through synchrotron radiation XAS\nexperiments. The sintered TE materials exhibited low thermal conductivity,\nachieving the highest TE figure-of-merit values of 0.7 (573 K) and 0.9 (523 K)\nfor n-type Bi2Te3 and p-type Sb2Te3, respectively, shifted significantly to the\nhigh-temperature region when compared to earlier reports, highlighting their\npotential for power generation applications. The scalable, energyand\ntime-efficient synthetic method developed, along with the demonstration of its\npotential for TE materials, opens the door for a wider application of these\nmaterials with minimal environmental impact.",
        "Holdout validation and hyperparameter tuning from data is a long-standing\nproblem in offline reinforcement learning (RL). A standard framework is to use\noff-policy evaluation (OPE) methods to evaluate and select the policies, but\nOPE either incurs exponential variance (e.g., importance sampling) or has\nhyperparameters on their own (e.g., FQE and model-based). In this work we focus\non hyperparameter tuning for OPE itself, which is even more under-investigated.\nConcretely, we select among candidate value functions (\"model-free\") or\ndynamics (\"model-based\") to best assess the performance of a target policy. Our\ncontributions are two fold. We develop: (1) new model-free and model-based\nselectors with theoretical guarantees, and (2) a new experimental protocol for\nempirically evaluating them. Compared to the model-free protocol in prior\nworks, our new protocol allows for more stable generation of candidate value\nfunctions, better control of misspecification, and evaluation of model-free and\nmodel-based methods alike. We exemplify the protocol on a Gym environment, and\nfind that our new model-free selector, LSTD-Tournament, demonstrates promising\nempirical performance.",
        "Sparse Autoencoders (SAEs) are widely used to interpret neural networks by\nidentifying meaningful concepts from their representations. However, do SAEs\ntruly uncover all concepts a model relies on, or are they inherently biased\ntoward certain kinds of concepts? We introduce a unified framework that recasts\nSAEs as solutions to a bilevel optimization problem, revealing a fundamental\nchallenge: each SAE imposes structural assumptions about how concepts are\nencoded in model representations, which in turn shapes what it can and cannot\ndetect. This means different SAEs are not interchangeable -- switching\narchitectures can expose entirely new concepts or obscure existing ones. To\nsystematically probe this effect, we evaluate SAEs across a spectrum of\nsettings: from controlled toy models that isolate key variables, to\nsemi-synthetic experiments on real model activations and finally to\nlarge-scale, naturalistic datasets. Across this progression, we examine two\nfundamental properties that real-world concepts often exhibit: heterogeneity in\nintrinsic dimensionality (some concepts are inherently low-dimensional, others\nare not) and nonlinear separability. We show that SAEs fail to recover concepts\nwhen these properties are ignored, and we design a new SAE that explicitly\nincorporates both, enabling the discovery of previously hidden concepts and\nreinforcing our theoretical insights. Our findings challenge the idea of a\nuniversal SAE and underscores the need for architecture-specific choices in\nmodel interpretability. Overall, we argue an SAE does not just reveal concepts\n-- it determines what can be seen at all.",
        "Focussing on two different use cases-Quality Control methods in industrial\ncontexts and Neural Network algorithms for healthcare diagnostics-this research\ninvestigates the inclusion of Fully Homomorphic Encryption into real-world\napplications in the healthcare sector. We evaluate the performance, resource\nrequirements, and viability of deploying FHE in these settings through\nextensive testing and analysis, highlighting the progress made in FHE tooling\nand the obstacles still facing addressing the gap between conceptual research\nand practical applications. We start our research by describing the specific\ncase study and trust model were working with. Choosing the two FHE frameworks\nmost appropriate for industry development, we assess the resources and\nperformance requirements for implementing each of the two FHE frameworks in the\nfirst scenario, Quality Control algorithms. In conclusion, our findings\ndemonstrate the effectiveness and resource consumption of the two use\ncases-complex NN models and simple QC algorithms-when implemented in an FHE\nsetting.",
        "In Experimental Advanced Superconducting Tokamak (EAST tokamak) with tungsten\ndivertors and molybdenum first wall, lithiumization and boronization have been\nfrequently carried out to improve the plasma performance, in particular, in\nlong pulse discharges. A study on impurity behaviors of lithium, boron and\ntungsten atoms\/ions in the edge plasma is then crucially important. For the\npurpose, a space-resolved visible spectrometer system has been newly developed\nto observe full vertical profiles over a length of 1.7m of impurity line\nemissions in wavelength range of 320-800nm. For the full vertical profile\nmeasurement compact endoscopic optics is employed with an optical fiber bundle\nfor the system, which can be inserted into a 1.5m long extension tube called\n'long nose', because the distance between the diagnostic port and plasma center\nis considerably long. Therefore, a quartz glass window mounted from the vacuum\nvessel side is designed to withstand the reverse pressure. A mechanical shutter\nis also designed to open at a large angle of 235 degree so that the viewing\nangle of nearby ports is not blocked. Two sets of the fiber bundle, 60-channel\nlinear array and 11*10 channel planar array , with a length of 30m are attached\nto two sets of Czerny-Turner visible spectrometers for one-dimensional (1D)\nvertical profile measurement of core plasma and two-dimensional (2D)\nspectroscopy of divertor plasma, respectively. A complementary metal oxide\nsemiconductor (CMOS) detector with 2048*2048 pixels is used for the visible\nspectrometers. A preliminary result on the full vertical profile is obtained\nfor BII line emission at 703.19nm in the 1D system",
        "We establish a database consisting of a representative of every binary\nquantum stabilizer code under local Clifford permutation equivalence for $n\\leq\n9$.",
        "In this paper, a corrugated Vivaldi phased array antenna in the 28 GHz\nfrequency band is proposed for 5G communication applications. The presented\nconfiguration features an all-metal antipodal antenna structure with a broad\nbandwidth ranging from 26 to 30 GHz and beam steering capabilities from -30 to\n+30 degrees. The proposed antenna consists of a 4x4 array configuration, where\neach element has dimensions of 6.46x6.46x14.25 mm, resulting in an overall\nantenna structure with dimensions of 25.84x25.84x14.25 mm. The corrugation\nmethod is applied to minimize surface currents, resulting in a reduction in\ninterelement mutual couplings. Therefore, the return loss in the array\nstructure for central elements is decreased, and the antenna gain and radiation\nefficiency are improved. Moreover, the improved radiation efficiency allows for\nhigher power transmission and reception from an antenna, resulting in\npotentially higher data rates and better performance."
      ]
    }
  },
  {
    "id":2411.00575,
    "research_type":"basic",
    "start_id":"b22",
    "start_title":"Vertical slice modelling of nonlinear Eady waves using a compatible finite element method",
    "start_abstract":"A vertical slice model is developed for the Euler-Boussinesq equations with a\nconstant temperature gradient in the direction normal to the slice (the\nEady-Boussinesq model). The model is a solution of the full three-dimensional\nequations with no variation normal to the slice, which is an idealized problem\nused to study the formation and subsequent evolution of weather fronts. A\ncompatible finite element method is used to discretise the governing equations.\nTo extend the Charney-Phillips grid staggering in the compatible finite element\nframework, we use the same node locations for buoyancy as the vertical part of\nvelocity and apply a transport scheme for a partially continuous finite element\nspace. For the time discretisation, we solve the semi-implicit equations\ntogether with an explicit strong-stability-preserving Runge-Kutta scheme to all\nof the advection terms. The model reproduces several quasi-periodic lifecycles\nof fronts despite the presence of strong discontinuities. An asymptotic limit\nanalysis based on the semi-geostrophic theory shows that the model solutions\nare converging to a solution in cross-front geostrophic balance. The results\nare consistent with the previous results using finite difference methods,\nindicating that the compatible finite element method is performing as well as\nfinite difference methods for this test problem. We observe dissipation of\nkinetic energy of the cross-front velocity in the model due to the lack of\nresolution at the fronts, even though the energy loss is not likely to account\nfor the large gap on the strength of the fronts between the model result and\nthe semi-geostrophic limit solution.",
    "start_categories":[
      "math.MP"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Weak Existence for the Semigeostrophic Equations Formulated as a Coupled Monge--Amp\u00e8re\/Transport Problem"
      ],
      "abstract":[
        "Hoskins's semigeostrophic equations are reformulated as a coupled Monge--Amp\u00e8re\/ transport problem [B. J. Hoskins, Quart. Royal Met. Soc., 97 (1971), pp. 139--153]. Existence of global weak solutions is obtained for this formulation."
      ],
      "categories":[
        "math.OC"
      ]
    },
    "list":{
      "title":[
        "Decentralized RISE-based Control for Exponential Heterogeneous\n  Multi-Agent Target Tracking of Second-Order Nonlinear Systems",
        "On the Origin and Fate of Our Universe",
        "Trend-Aware Supervision: On Learning Invariance for Semi-Supervised\n  Facial Action Unit Intensity Estimation",
        "Data-Driven Decision Making for Enhancing Small-Signal Stability in\n  Hybrid AC\/DC Grids Through Converter Control Role Assignment",
        "Multi-Year-to-Decadal Temperature Prediction using a Machine Learning\n  Model-Analog Framework",
        "Decentralized Inference for Spatial Data Using Low-Rank Models",
        "Quadratic quasinormal modes at null infinity on a Schwarzschild\n  spacetime",
        "On the Adversarial Vulnerabilities of Transfer Learning in Remote\n  Sensing",
        "Stress field in the vicinity of a bubble\/sphere moving in a dilute\n  surfactant solution",
        "A Hybrid Swarm Intelligence Approach for Optimizing Multimodal Large\n  Language Models Deployment in Edge-Cloud-based Federated Learning\n  Environments",
        "Making Puzzle Pieces Fit or Reshaping MiMiC for Multiscale Simulations\n  with CP2K and More",
        "To Hedge or Not to Hedge: Optimal Strategies for Stochastic Trade Flow\n  Management",
        "Generalized Venn and Venn-Abers Calibration with Applications in\n  Conformal Prediction",
        "Keyword Search in the Deep Web",
        "Holistic Semantic Representation for Navigational Trajectory Generation",
        "Fast Two-photon Microscopy by Neuroimaging with Oblong Random\n  Acquisition (NORA)",
        "Diatomic and Polyatomic Heteronuclear Ultralong-Range Rydberg Molecules",
        "Probing Green's Function Zeros by Co-tunneling through Mott Insulators",
        "Recursive Inference Scaling: A Winning Path to Scalable Inference in\n  Language and Multimodal Systems",
        "L1: Controlling How Long A Reasoning Model Thinks With Reinforcement\n  Learning",
        "Low RCS High-Gain Broadband Substrate Integrated Waveguide Antenna Based\n  on Elliptical Polarization Conversion Metasurface",
        "OverThink: Slowdown Attacks on Reasoning LLMs",
        "Scalable solution chemical synthesis and comprehensive analysis of\n  Bi2Te3 and Sb2Te3",
        "Model Selection for Off-policy Evaluation: New Algorithms and\n  Experimental Protocol",
        "Projecting Assumptions: The Duality Between Sparse Autoencoders and\n  Concept Geometry",
        "Homomorphic Encryption in Healthcare Industry Applications for\n  Protecting Data Privacy",
        "A space-resolved visible spectrometer system using compact endoscopic\n  optics for full vertical profile measurement of impurity line emissions in\n  superconducting EAST tokamak",
        "Small Binary Stabilizer Subsystem Codes",
        "A Corrugated All-Metal Vivaldi Antenna for 5G Phased Array Applications"
      ],
      "abstract":[
        "This work presents a decentralized implementation of a Robust Integral of the\nSign of the Error (RISE) controller for multi-agent target tracking problems\nwith exponential convergence guarantees. Previous RISE-based approaches for\nmulti-agent systems required 2-hop communication, limiting practical\napplicability. New insights from a Lyapunov-based design-analysis approach are\nused to eliminate the need for multi-hop communication required in previous\nliterature, while yielding exponential target tracking. The new insights\ninclude the development of a new P-function which is developed which works in\ntandem with the inclusion of the interaction matrix in the Lyapunov function.\nNonsmooth Lyapunov-based stability analysis methods are used to yield\nsemi-global exponential convergence to the target agent state despite the\npresence of bounded disturbances with bounded derivatives. The resulting\noutcome is a controller that achieves exponential target tracking with only\nlocal information exchange between neighboring agents.",
        "This brief review, intended for high energy and astrophysics researchers,\nexplores the implications of recent theoretical advances in string theory and\nthe Swampland program for understanding bounds on the structure of positive\npotentials allowed in quantum gravity. This has a bearing on both inflationary\nmodels for the early universe as well as the fate of our universe. The paper\nincludes a review of the dS conjecture as well as the TransPlanckian Censorship\nConjecture (TCC) and its relation to the species scale. We provide evidence for\nthese principles as well as what they may lead to in terms of phenomenological\npredictions. (Talk presented at Lemaitre Conference 2024)",
        "With the increasing need for facial behavior analysis, semi-supervised AU\nintensity estimation using only keyframe annotations has emerged as a practical\nand effective solution to relieve the burden of annotation. However, the lack\nof annotations makes the spurious correlation problem caused by AU\nco-occurrences and subject variation much more prominent, leading to non-robust\nintensity estimation that is entangled among AUs and biased among subjects. We\nobserve that trend information inherent in keyframe annotations could act as\nextra supervision and raising the awareness of AU-specific facial appearance\nchanging trends during training is the key to learning invariant AU-specific\nfeatures. To this end, we propose \\textbf{T}rend-\\textbf{A}ware\n\\textbf{S}upervision (TAS), which pursues three kinds of trend awareness,\nincluding intra-trend ranking awareness, intra-trend speed awareness, and\ninter-trend subject awareness. TAS alleviates the spurious correlation problem\nby raising trend awareness during training to learn AU-specific features that\nrepresent the corresponding facial appearance changes, to achieve intensity\nestimation invariance. Experiments conducted on two commonly used AU benchmark\ndatasets, BP4D and DISFA, show the effectiveness of each kind of awareness. And\nunder trend-aware supervision, the performance can be improved without extra\ncomputational or storage costs during inference.",
        "Hybrid AC\/DC transmission grids incorporate Modular Multilevel Converters\nfunctioning as Interconnecting Power Converters (IPCs). The control role\nassigned to each converter significantly influences grid dynamics.\nTraditionally, these converters operate with static control roles, but recent\nstudies have proposed scheduling their roles based on day-ahead forecasts to\nenhance stability performance. However, in systems with high renewable energy\npenetration, forecast deviations can render scheduled control assignments\nsuboptimal or even lead to instability. To address this challenge, this work\nproposes an online scheduling recalculation algorithm that dynamically adapts\nIPC control roles during system operation. The approach leverages a data-driven\nmulti-criteria decision-making framework, integrating surrogate models of\nconventional small-signal stability analysis tools to enable a fast computation\nof system stability and stability performance indicators.",
        "Multi-year-to-decadal climate prediction is a key tool in understanding the\nrange of potential regional and global climate futures. Here, we present a\nframework that combines machine learning and analog forecasting for predictions\non these timescales. A neural network is used to learn a mask, specific to a\nregion and lead time, with global weights based on relative importance as\nprecursors to the evolution of that prediction target. A library of\nmask-weighted model states, or potential analogs, are then compared to a single\nmask-weighted observational state. The known future of the best matching\npotential analogs serve as the prediction for the future of the observational\nstate. We match and predict 2-meter temperature using the Berkeley Earth\nSurface Temperature dataset for observations, and a set of CMIP6 models as the\nanalog library. We find improved performance over traditional analog methods\nand initialized decadal predictions.",
        "Advancements in information technology have enabled the creation of massive\nspatial datasets, driving the need for scalable and efficient computational\nmethodologies. While offering viable solutions, centralized frameworks are\nlimited by vulnerabilities such as single-point failures and communication\nbottlenecks. This paper presents a decentralized framework tailored for\nparameter inference in spatial low-rank models to address these challenges. A\nkey obstacle arises from the spatial dependence among observations, which\nprevents the log-likelihood from being expressed as a summation-a critical\nrequirement for decentralized optimization approaches. To overcome this\nchallenge, we propose a novel objective function leveraging the evidence lower\nbound, which facilitates the use of decentralized optimization techniques. Our\napproach employs a block descent method integrated with multi-consensus and\ndynamic consensus averaging for effective parameter optimization. We prove the\nconvexity of the new objective function in the vicinity of the true parameters,\nensuring the convergence of the proposed method. Additionally, we present the\nfirst theoretical results establishing the consistency and asymptotic normality\nof the estimator within the context of spatial low-rank models. Extensive\nsimulations and real-world data experiments corroborate these theoretical\nfindings, showcasing the robustness and scalability of the framework.",
        "The ringdown of perturbed black holes has been studied since the 1970s, but\nuntil recently, studies have focused on linear perturbations. There is now\nburgeoning interest in nonlinear perturbative effects during ringdown. Here,\nusing a hyperboloidal framework, we provide a complete treatment of linear and\nquadratic quasinormal modes (QNMs and QQNMs) in second-order perturbation\ntheory, in Schwarzschild spacetime. We include novel methods for extracting\nQNMs and QQNMs amplitudes using a Laplace transform treatment, allowing for the\ninclusion of arbitrary initial data. We produce both time- and frequency-domain\ncodes. From these codes, we present new results further exploring the\nunforeseen dependence of QQNMs amplitudes on the parity of the progenitor\nsystem, as demonstrated in our letter [Phys. Rev. Lett. 134, 061401 (2025)].\nOur numerical results are restricted to perturbations of a Schwarzschild black\nhole, but our methods extend straightforwardly to the astrophysically realistic\ncase of a Kerr black hole.",
        "The use of pretrained models from general computer vision tasks is widespread\nin remote sensing, significantly reducing training costs and improving\nperformance. However, this practice also introduces vulnerabilities to\ndownstream tasks, where publicly available pretrained models can be used as a\nproxy to compromise downstream models. This paper presents a novel Adversarial\nNeuron Manipulation method, which generates transferable perturbations by\nselectively manipulating single or multiple neurons in pretrained models.\nUnlike existing attacks, this method eliminates the need for domain-specific\ninformation, making it more broadly applicable and efficient. By targeting\nmultiple fragile neurons, the perturbations achieve superior attack\nperformance, revealing critical vulnerabilities in deep learning models.\nExperiments on diverse models and remote sensing datasets validate the\neffectiveness of the proposed method. This low-access adversarial neuron\nmanipulation technique highlights a significant security risk in transfer\nlearning models, emphasizing the urgent need for more robust defenses in their\ndesign when addressing the safety-critical remote sensing tasks.",
        "In this study, we experimentally investigate the stress field around a bubble\nrising in a dilute surfactant solution (20 < Re < 220, high Peclet numbers)\nwhose surface gradually becomes contaminated, and compare it with that around a\nsphere free from surface contamination. We employ a newly developed\npolarization measurement technique, highly sensitive to stress fields near\ninterfaces. First, we validate this method by measuring the flow around a solid\nsphere settling at Re = 120 and comparing results with numerical predictions,\nconfirming its accuracy. We then measure the stress field around a bubble whose\ndrag force transitions from that of a clean interface to that of a rigid\ninterface within the observation region. The stress near the bubble's front\nresembles that of a clean bubble, while the rear behaves like a solid sphere.\nBetween these regions, a discontinuous phase retardation near the cap angle\nindicates a transition from slip to no-slip boundary conditions. Axisymmetric\nstress reconstruction reveals localized stress spike at the cap angle, which\nshifts as surfactant accumulates and increases the drag. Remarkably, the\nmeasured cap angle versus normalized drag coefficient agrees well with\nnumerical simulations at Re = 100 (Cuenot et al. 1997) and shows only a slight\ndeviation from the creeping-flow stagnant cap model (Sadhal and Johnson 1983).\nThis work demonstrates that polarization-based stress field measurements\neffectively capture the interplay between surface contamination and\nhydrodynamics at intermediate Reynolds numbers.",
        "The combination of Federated Learning (FL), Multimodal Large Language Models\n(MLLMs), and edge-cloud computing enables distributed and real-time data\nprocessing while preserving privacy across edge devices and cloud\ninfrastructure. However, the deployment of MLLMs in FL environments with\nresource-constrained edge devices presents significant challenges, including\nresource management, communication overhead, and non-IID data. To address these\nchallenges, we propose a novel hybrid framework wherein MLLMs are deployed on\nedge devices equipped with sufficient resources and battery life, while the\nmajority of training occurs in the cloud. To identify suitable edge devices for\ndeployment, we employ Particle Swarm Optimization (PSO), and Ant Colony\nOptimization (ACO) is utilized to optimize the transmission of model updates\nbetween edge and cloud nodes. This proposed swarm intelligence-based framework\naims to enhance the efficiency of MLLM training by conducting extensive\ntraining in the cloud and fine-tuning at the edge, thereby reducing energy\nconsumption and communication costs. Our experimental results show that the\nproposed method significantly improves system performance, achieving an\naccuracy of 92%, reducing communication cost by 30%, and enhancing client\nparticipation compared to traditional FL methods. These results make the\nproposed approach highly suitable for large-scale edge-cloud computing systems.",
        "MiMiC is a framework for modeling large-scale chemical processes that require\ntreatment at multiple resolutions. It does not aim to implement single-handedly\nall methods required to treat individual subsystems, but instead, it relegates\nthis task to specialized computational chemistry software while it serves as an\nintermediary between these external programs, and computes the interactions\nbetween the subsystems. MiMiC minimizes issues typically associated with\nmolecular dynamics performed with multiple programs, by adopting a\nmultiple-program multiple-data paradigm combined with a loose-coupling model.\nIn this article, we present the addition of a new client program, CP2K, to the\nMiMiC ecosystem, which required a major refactoring of the entire framework and\nin the end allowed us to unlock its full flexibility. By thorough timing\nanalysis, we verify that the introduced changes do not affect the performance\nof MiMiC or CP2K, and neither are they a source of significant computational\noverheads that would be detrimental to simulation efficiency. Moreover, we\ndemonstrate the benefits of the framework's modular design, by performing a\nQM\/MM MD simulation combining CP2K with previously interfaced OpenMM, with no\nadditional implementation effort required.",
        "This paper addresses the trade-off between internalisation and\nexternalisation in the management of stochastic trade flows. We consider agents\nwho must absorb flows and manage risk by deciding whether to warehouse it or\nhedge in the market, thereby incurring transaction costs and market impact.\nUnlike market makers, these agents cannot skew their quotes to attract\noffsetting flows and deter risk-increasing ones, leading to a fundamentally\ndifferent problem. Within the Almgren-Chriss framework, we derive\nalmost-closed-form solutions in the case of quadratic execution costs, while\nmore general cases require numerical methods. In particular, we discuss the\nchallenges posed by artificial boundary conditions when using classical\ngrid-based numerical PDE techniques and propose reinforcement learning methods\nas an alternative.",
        "Ensuring model calibration is critical for reliable predictions, yet popular\ndistribution-free methods, such as histogram binning and isotonic regression,\nprovide only asymptotic guarantees. We introduce a unified framework for Venn\nand Venn-Abers calibration, generalizing Vovk's binary classification approach\nto arbitrary prediction tasks and loss functions. Venn calibration leverages\nbinning calibrators to construct prediction sets that contain at least one\nmarginally perfectly calibrated point prediction in finite samples, capturing\nepistemic uncertainty in the calibration process. The width of these sets\nshrinks asymptotically to zero, converging to a conditionally calibrated point\nprediction. Furthermore, we propose Venn multicalibration, a novel methodology\nfor finite-sample calibration across subpopulations. For quantile loss,\ngroup-conditional and multicalibrated conformal prediction arise as special\ncases of Venn multicalibration, and Venn calibration produces novel conformal\nprediction intervals that achieve quantile-conditional coverage. As a separate\ncontribution, we extend distribution-free conditional calibration guarantees of\nhistogram binning and isotonic calibration to general losses.",
        "The Deep Web is constituted by data that are accessible through Web pages,\nbut not readily indexable by search engines as they are returned in dynamic\npages. In this paper we propose a conceptual framework for answering keyword\nqueries on Deep Web sources represented as relational tables with so-called\naccess limitations. We formalize the notion of optimal answer, characterize\nqueries for which an answer can be found, and present a method for query\nprocessing based on the construction of a query plan that minimizes the\naccesses to the data sources.",
        "Trajectory generation has garnered significant attention from researchers in\nthe field of spatio-temporal analysis, as it can generate substantial\nsynthesized human mobility trajectories that enhance user privacy and alleviate\ndata scarcity. However, existing trajectory generation methods often focus on\nimproving trajectory generation quality from a singular perspective, lacking a\ncomprehensive semantic understanding across various scales. Consequently, we\nare inspired to develop a HOlistic SEmantic Representation (HOSER) framework\nfor navigational trajectory generation. Given an origin-and-destination (OD)\npair and the starting time point of a latent trajectory, we first propose a\nRoad Network Encoder to expand the receptive field of road- and zone-level\nsemantics. Second, we design a Multi-Granularity Trajectory Encoder to\nintegrate the spatio-temporal semantics of the generated trajectory at both the\npoint and trajectory levels. Finally, we employ a Destination-Oriented\nNavigator to seamlessly integrate destination-oriented guidance. Extensive\nexperiments on three real-world datasets demonstrate that HOSER outperforms\nstate-of-the-art baselines by a significant margin. Moreover, the model's\nperformance in few-shot learning and zero-shot learning scenarios further\nverifies the effectiveness of our holistic semantic representation.",
        "Advances in neural imaging have enabled neuroscience to study how the joint\nactivity of large neural populations conspire to produce perception, behavior\nand cognition. Despite many advances in optical methods, there exists a\nfundamental tradeoff between imaging speed, field of view, and resolution that\nlimits the scope of neural imaging, especially for the raster-scanning\nmulti-photon imaging needed for imaging deeper into the brain. One approach to\novercoming this trade-off is in computational imaging: the co-development of\noptics and algorithms where the optics are designed to encode the target images\ninto fewer measurements that are faster to acquire, and the algorithms\ncompensate by inverting the optical image coding process to recover a larger or\nhigher resolution image. We present here one such approach for raster-scanning\ntwo-photon imaging: Neuroimaging with Oblong Random Acquisition (NORA). NORA\nquickly acquires each frame in a microscopic video by subsampling only a\nfraction of the fast scanning lines, ignoring large portions of each frame.\nNORA mitigates the information loss by extending the point-spread function in\nthe slow-scan direction to integrate the fluorescence of neighboring lines into\na single set of measurements. By imaging different, randomly selected, lines at\neach frame, NORA diversifies the information collected across frames and\nenables video-level reconstruction. Rather than reconstruct the video\nframe-by-frame using image-level recovery, NORA recovers full video sequences\nthrough a nuclear-norm minimization (i.e., matrix completion) on the\npixels-by-time matrix. We simulated NORA imaging using the Neural Anatomy and\nOptical Microscopy (NAOMi) biophysical simulation suite. Using these\nsimulations we demonstrate that NORA imaging can accurately recover 400 um X\n400 um fields of view at subsampling rates up to 20X, despite realistic noise\nand motion conditions.",
        "Ultra-long-range Rydberg molecules (ULRMs) have attracted significant\ninterest due to their unique electronic properties and potential applications\nin quantum technologies. We theoretically investigate the formation and\ncharacteristics of heteronuclear ULRMs, focusing on Rb-Cs systems. We explore\nthe vibrational energy levels of heteronuclear nD ULRMs and compare them with\nhomonuclear counterparts. We also predict the formation of polyatomic\nheteronuclear ULRMs, discussing how the binding energy and spectral features\nevolve as the number of ground-state atoms increases. Our theoretical\npredictions are presented in terms of molecular spectra and provide insight\ninto the formation dynamics of these systems. The study further explores the\npotential applications of heteronuclear ULRMs in quantum information\nprocessing, quantum simulation, and precision measurements, offering new\navenues for future research in many-body physics and quantum technologies.",
        "Quantum tunneling experiments have provided deep insights into basic\nexcitations occurring as Green's function poles in the realm of complex quantum\nmatter. However, strongly correlated quantum materials also allow for Green's\nfunctions zeros (GFZ) that may be seen as an antidote to the familiar poles,\nand have so far largely eluded direct experimental study. Here, we propose and\ninvestigate theoretically how co-tunneling through Mott insulators enables\ndirect access to the shadow band structure of GFZ. In particular, we derive an\neffective Hamiltonian for the GFZ that is shown to govern the co-tunneling\namplitude and reveal fingerprints of many-body correlations clearly\ndistinguishing the GFZ structure from the underlying free Bloch band structure\nof the system. Our perturbative analytical results are corroborated by\nnumerical data both in the framework of exact diagonalization and matrix\nproduct state simulations for a one-dimensional model system consisting of a\nSu-Schrieffer-Heeger-Hubbard model coupled to two single level quantum dots.",
        "Recent research in language modeling reveals two scaling effects: the\nwell-known improvement from increased training compute, and a lesser-known\nboost from applying more sophisticated or computationally intensive inference\nmethods. Inspired by recent findings on the fractal geometry of language, we\nintroduce Recursive INference Scaling (RINS) as a complementary, plug-in recipe\nfor scaling inference time. For a given fixed model architecture and training\ncompute budget, RINS substantially improves language modeling performance. It\nalso generalizes beyond pure language tasks, delivering gains in multimodal\nsystems, including a +2% improvement in 0-shot ImageNet accuracy for\nSigLIP-B\/16. Additionally, by deriving data scaling laws, we show that RINS\nimproves both the asymptotic performance limits and the scaling exponents.\nThese advantages are maintained even when compared to state-of-the-art\nrecursive techniques like the \"repeat-all-over\" (RAO) strategy in Mobile LLM.\nFinally, stochastic RINS not only can enhance performance further but also\nprovides the flexibility to optionally forgo increased inference computation at\ntest time with minimal performance degradation.",
        "Reasoning language models have shown an uncanny ability to improve\nperformance at test-time by ``thinking longer''-that is, by generating longer\nchain-of-thought sequences and hence using more compute. However, the length of\ntheir chain-of-thought reasoning is not controllable, making it impossible to\nallocate test-time compute to achieve a desired level of performance. We\nintroduce Length Controlled Policy Optimization (LCPO), a simple reinforcement\nlearning method that optimizes for accuracy and adherence to user-specified\nlength constraints. We use LCPO to train L1, a reasoning language model that\nproduces outputs satisfying a length constraint given in its prompt. L1's\nlength control allows for smoothly trading off computational cost and accuracy\non a wide range of tasks, and outperforms the state-of-the-art S1 method for\nlength control. Furthermore, we uncover an unexpected short chain-of-thought\ncapability in models trained with LCPO. For instance, our 1.5B L1 model\nsurpasses GPT-4o at equal reasoning lengths. Overall, LCPO enables precise\ncontrol over reasoning length, allowing for fine-grained allocation of\ntest-time compute and accuracy. We release code and models at\nhttps:\/\/www.cmu-l3.github.io\/l1",
        "Designed an elliptical polarization conversion metasurface (PCM) for Ka-band\napplications, alongside a high-gain substrate integrated waveguide (SIW)\nantenna. The PCM elements are integrated into the antenna design in a\nchessboard array configuration, with the goal of achieving effective reduction\nin the antenna's radar cross section (RCS). Both the PCM elements and antenna\nstructure exhibit a simple design. The top layer of the metasurface (MS)\nelements employs an elliptical pattern symmetric along the diagonal, enabling\nefficient conversion of linearly polarized waves. The antenna component, on the\nother hand, consists of a broadband dipole antenna fed by SIW slot coupling.\nVerified through simulations, the polarization conversion bandwidth of this PCM\nunit reaches 80.38% where polarization conversion ratio (PCR) exceeds 90%\n(25.3-59.3GHz), demonstrating exceptional conversion performance. When the\ndipole antenna is combined with the PCM, its -10dB impedance bandwidth reaches\nto 15.09% (33.7-39.2GHz), with a maximum realized gain of 9.1dBi. Notably, the\nantenna loaded with the chessboard PCM structure effectively disperses the\nenergy of scattered echoes around, significantly reducing the concentration of\nscattered energy in the direction of the incident wave, thereby achieving an\neffective reduction in RCS.",
        "We increase overhead for applications that rely on reasoning LLMs-we force\nmodels to spend an amplified number of reasoning tokens, i.e., \"overthink\", to\nrespond to the user query while providing contextually correct answers. The\nadversary performs an OVERTHINK attack by injecting decoy reasoning problems\ninto the public content that is used by the reasoning LLM (e.g., for RAG\napplications) during inference time. Due to the nature of our decoy problems\n(e.g., a Markov Decision Process), modified texts do not violate safety\nguardrails. We evaluated our attack across closed-(OpenAI o1, o1-mini, o3-mini)\nand open-(DeepSeek R1) weights reasoning models on the FreshQA and SQuAD\ndatasets. Our results show up to 18x slowdown on FreshQA dataset and 46x\nslowdown on SQuAD dataset. The attack also shows high transferability across\nmodels. To protect applications, we discuss and implement defenses leveraging\nLLM-based and system design approaches. Finally, we discuss societal,\nfinancial, and energy impacts of OVERTHINK attack which could amplify the costs\nfor third-party applications operating reasoning models.",
        "Thermoelectric (TE) materials can directly convert heat into electrical\nenergy. However, they sustain costly production procedures and batch-to-batch\nperformance variations. Therefore, developing scalable synthetic techniques for\nlarge-scale and reproducible quality TE materials is critical for advancing TE\ntechnology. This study developed a facile, high throughput, solution-chemical\nsynthetic technique. Microwave-assisted thermolysis process, providing\nenergy-efficient volumetric heating, was used for the synthesis of bismuth and\nantimony telluride (Bi2Te3, Sb2Te3). As-made materials were characterized using\nvarious techniques, including XRPD, SEM, TEM, XAS, and XPS. Detailed\ninvestigation of the local atomic structure of the synthesized Bi2Te3 and\nSb2Te3 powder samples was conducted through synchrotron radiation XAS\nexperiments. The sintered TE materials exhibited low thermal conductivity,\nachieving the highest TE figure-of-merit values of 0.7 (573 K) and 0.9 (523 K)\nfor n-type Bi2Te3 and p-type Sb2Te3, respectively, shifted significantly to the\nhigh-temperature region when compared to earlier reports, highlighting their\npotential for power generation applications. The scalable, energyand\ntime-efficient synthetic method developed, along with the demonstration of its\npotential for TE materials, opens the door for a wider application of these\nmaterials with minimal environmental impact.",
        "Holdout validation and hyperparameter tuning from data is a long-standing\nproblem in offline reinforcement learning (RL). A standard framework is to use\noff-policy evaluation (OPE) methods to evaluate and select the policies, but\nOPE either incurs exponential variance (e.g., importance sampling) or has\nhyperparameters on their own (e.g., FQE and model-based). In this work we focus\non hyperparameter tuning for OPE itself, which is even more under-investigated.\nConcretely, we select among candidate value functions (\"model-free\") or\ndynamics (\"model-based\") to best assess the performance of a target policy. Our\ncontributions are two fold. We develop: (1) new model-free and model-based\nselectors with theoretical guarantees, and (2) a new experimental protocol for\nempirically evaluating them. Compared to the model-free protocol in prior\nworks, our new protocol allows for more stable generation of candidate value\nfunctions, better control of misspecification, and evaluation of model-free and\nmodel-based methods alike. We exemplify the protocol on a Gym environment, and\nfind that our new model-free selector, LSTD-Tournament, demonstrates promising\nempirical performance.",
        "Sparse Autoencoders (SAEs) are widely used to interpret neural networks by\nidentifying meaningful concepts from their representations. However, do SAEs\ntruly uncover all concepts a model relies on, or are they inherently biased\ntoward certain kinds of concepts? We introduce a unified framework that recasts\nSAEs as solutions to a bilevel optimization problem, revealing a fundamental\nchallenge: each SAE imposes structural assumptions about how concepts are\nencoded in model representations, which in turn shapes what it can and cannot\ndetect. This means different SAEs are not interchangeable -- switching\narchitectures can expose entirely new concepts or obscure existing ones. To\nsystematically probe this effect, we evaluate SAEs across a spectrum of\nsettings: from controlled toy models that isolate key variables, to\nsemi-synthetic experiments on real model activations and finally to\nlarge-scale, naturalistic datasets. Across this progression, we examine two\nfundamental properties that real-world concepts often exhibit: heterogeneity in\nintrinsic dimensionality (some concepts are inherently low-dimensional, others\nare not) and nonlinear separability. We show that SAEs fail to recover concepts\nwhen these properties are ignored, and we design a new SAE that explicitly\nincorporates both, enabling the discovery of previously hidden concepts and\nreinforcing our theoretical insights. Our findings challenge the idea of a\nuniversal SAE and underscores the need for architecture-specific choices in\nmodel interpretability. Overall, we argue an SAE does not just reveal concepts\n-- it determines what can be seen at all.",
        "Focussing on two different use cases-Quality Control methods in industrial\ncontexts and Neural Network algorithms for healthcare diagnostics-this research\ninvestigates the inclusion of Fully Homomorphic Encryption into real-world\napplications in the healthcare sector. We evaluate the performance, resource\nrequirements, and viability of deploying FHE in these settings through\nextensive testing and analysis, highlighting the progress made in FHE tooling\nand the obstacles still facing addressing the gap between conceptual research\nand practical applications. We start our research by describing the specific\ncase study and trust model were working with. Choosing the two FHE frameworks\nmost appropriate for industry development, we assess the resources and\nperformance requirements for implementing each of the two FHE frameworks in the\nfirst scenario, Quality Control algorithms. In conclusion, our findings\ndemonstrate the effectiveness and resource consumption of the two use\ncases-complex NN models and simple QC algorithms-when implemented in an FHE\nsetting.",
        "In Experimental Advanced Superconducting Tokamak (EAST tokamak) with tungsten\ndivertors and molybdenum first wall, lithiumization and boronization have been\nfrequently carried out to improve the plasma performance, in particular, in\nlong pulse discharges. A study on impurity behaviors of lithium, boron and\ntungsten atoms\/ions in the edge plasma is then crucially important. For the\npurpose, a space-resolved visible spectrometer system has been newly developed\nto observe full vertical profiles over a length of 1.7m of impurity line\nemissions in wavelength range of 320-800nm. For the full vertical profile\nmeasurement compact endoscopic optics is employed with an optical fiber bundle\nfor the system, which can be inserted into a 1.5m long extension tube called\n'long nose', because the distance between the diagnostic port and plasma center\nis considerably long. Therefore, a quartz glass window mounted from the vacuum\nvessel side is designed to withstand the reverse pressure. A mechanical shutter\nis also designed to open at a large angle of 235 degree so that the viewing\nangle of nearby ports is not blocked. Two sets of the fiber bundle, 60-channel\nlinear array and 11*10 channel planar array , with a length of 30m are attached\nto two sets of Czerny-Turner visible spectrometers for one-dimensional (1D)\nvertical profile measurement of core plasma and two-dimensional (2D)\nspectroscopy of divertor plasma, respectively. A complementary metal oxide\nsemiconductor (CMOS) detector with 2048*2048 pixels is used for the visible\nspectrometers. A preliminary result on the full vertical profile is obtained\nfor BII line emission at 703.19nm in the 1D system",
        "We establish a database consisting of a representative of every binary\nquantum stabilizer code under local Clifford permutation equivalence for $n\\leq\n9$.",
        "In this paper, a corrugated Vivaldi phased array antenna in the 28 GHz\nfrequency band is proposed for 5G communication applications. The presented\nconfiguration features an all-metal antipodal antenna structure with a broad\nbandwidth ranging from 26 to 30 GHz and beam steering capabilities from -30 to\n+30 degrees. The proposed antenna consists of a 4x4 array configuration, where\neach element has dimensions of 6.46x6.46x14.25 mm, resulting in an overall\nantenna structure with dimensions of 25.84x25.84x14.25 mm. The corrugation\nmethod is applied to minimize surface currents, resulting in a reduction in\ninterelement mutual couplings. Therefore, the return loss in the array\nstructure for central elements is decreased, and the antenna gain and radiation\nefficiency are improved. Moreover, the improved radiation efficiency allows for\nhigher power transmission and reception from an antenna, resulting in\npotentially higher data rates and better performance."
      ]
    }
  },
  {
    "id":2411.00578,
    "research_type":"applied",
    "start_id":"b8",
    "start_title":"American Heart Association\/American Stroke Association. 2022 guideline for the management of patients with spontaneous intracerebral hemorrhage: A guideline from the american heart association\/american stroke association",
    "start_abstract":"Approximately 10% of the 795\u2009000 strokes per year in the United States are intracerebral hemorrhages (ICHs),1 defined by brain injury attributable to acute blood extravasation into the brain parenchyma from a ruptured cerebral blood vessel. The clinical impact of ICH appears disproportionately high among lower-resource populations both in the United States and internationally. In US-based studies, ICH incidence has been reported to be \u22481.6-fold greater among Black than White people2 and 1.6-fold greater among Mexican American than non-Hispanic White people.3 Internationally, ICH incidence is substantially higher in low- and middle-income versus high-income countries, both as a proportion of all strokes and in absolute incidence rates.4,5 Several additional features of ICH make it a greater public health threat than conveyed by incidence numbers alone. ICH is arguably the deadliest form of acute stroke, with early-term mortality about 30% to 40% and no or minimal trend toward improvement over more recent time epochs.6\u20139 Incidence of ICH increases sharply with age and is therefore expected to remain substantial as the population ages, even with counterbalancing public health improvements in blood pressure (BP) control.8 Another growing source of ICH is more widespread use of anticoagulants,10 a trend likely to counterbalance the reduced ICH risk associated with increasing prescription of direct oral anticoagulants (DOACs) relative to vitamin K antagonists (VKAs).11 ICH thus remains in need of novel treatments and improved application of established approaches for every aspect of the disease: primary and secondary prevention, acute inpatient care, and poststroke rehabilitation and recovery. This guideline seeks to synthesize data in the ICH field into practical recommendations for clinical practice.",
    "start_categories":[
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b25"
      ],
      "title":[
        "Voxel Scene Graph for Intracranial Hemorrhage"
      ],
      "abstract":[
        "Patients with Intracranial Hemorrhage (ICH) face a potentially life-threatening condition, and patient-centered individualized treatment remains challenging due to possible clinical complications. Deep-Learning-based methods can efficiently analyze the routinely acquired head CTs support decision-making. The majority of early work focuses on detection segmentation ICH, but do not model complex relations between ICH adjacent brain structures. In this work, we design tailored object method for which unite segmentation-grounded Scene Graph Generation (SGG) learn holistic representation cerebral scene. To best our knowledge, is first application SGG 3D voxel images. We evaluate two head-CT datasets demonstrate that recall up 74% clinically relevant relations. This lays foundation towards data. generated Graphs already provide insights clinician, are also valuable all downstream tasks as compact interpretable representation."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "CMoE: Fast Carving of Mixture-of-Experts for Efficient LLM Inference",
        "Mobile Application Threats and Security",
        "Water Flow Detection Device Based on Sound Data Analysis and Machine\n  Learning to Detect Water Leakage",
        "Recent open heavy flavor studies for the Electron-Ion Collider",
        "A Training-Free Length Extrapolation Approach for LLMs: Greedy Attention\n  Logit Interpolation (GALI)",
        "ReasonGraph: Visualisation of Reasoning Paths",
        "Foundations of block-parallel automata networks",
        "Enhancing Quantum-ready QUBO-based Suppression for Object Detection with\n  Appearance and Confidence Features",
        "Uncertainty-Aware Decoding with Minimum Bayes Risk",
        "StructVPR++: Distill Structural and Semantic Knowledge with Weighting\n  Samples for Visual Place Recognition",
        "Infinitely many solutions for elliptic system with Hamiltonian type",
        "Stochastic resonance in Schmitt trigger and its application towards weak\n  signal detection",
        "mmDEAR: mmWave Point Cloud Density Enhancement for Accurate Human Body\n  Reconstruction",
        "Dubrovin duality and mirror symmetry for ADE resolutions",
        "Geometric deformations of cuspidal $S_1$ singularities",
        "The thermal index of neutron-star matter in the virial approximation",
        "T CrA has a companion: First direct detection of T CrA B with\n  VLTI\/MATISSE",
        "MARS: Mesh AutoRegressive Model for 3D Shape Detailization",
        "Strengthening the Internal Adversarial Robustness in Lifted Neural\n  Networks",
        "Exchange Rate Sensitivity in Free Zone Trade: An Empirical Study of the\n  Istanbul Ataturk Airport Free Zone",
        "Mean and Variance Estimation Complexity in Arbitrary Distributions via\n  Wasserstein Minimization",
        "A dataset-free approach for self-supervised learning of 3D reflectional\n  symmetries",
        "Label-Efficient LiDAR Panoptic Segmentation",
        "ALLVB: All-in-One Long Video Understanding Benchmark",
        "Optimal power procurement for green cellular wireless networks under\n  uncertainty and chance constraints",
        "Efficient dynamic modal load reconstruction using physics-informed\n  Gaussian processes based on frequency-sparse Fourier basis functions",
        "Flexible Bayesian Tensor Decomposition for Verbal Autopsy Data",
        "Perceptual Visual Quality Assessment: Principles, Methods, and Future\n  Directions",
        "Analyzing public sentiment to gauge key stock events and determine\n  volatility in conjunction with time and options premiums"
      ],
      "abstract":[
        "Large language models (LLMs) achieve impressive performance by scaling model\nparameters, but this comes with significant inference overhead. Feed-forward\nnetworks (FFNs), which dominate LLM parameters, exhibit high activation\nsparsity in hidden neurons. To exploit this, researchers have proposed using a\nmixture-of-experts (MoE) architecture, where only a subset of parameters is\nactivated. However, existing approaches often require extensive training data\nand resources, limiting their practicality. We propose CMoE (Carved MoE), a\nnovel framework to efficiently carve MoE models from dense models. CMoE\nachieves remarkable performance through efficient expert grouping and\nlightweight adaptation. First, neurons are grouped into shared and routed\nexperts based on activation rates. Next, we construct a routing mechanism\nwithout training from scratch, incorporating a differentiable routing process\nand load balancing. Using modest data, CMoE produces a well-designed, usable\nMoE from a 7B dense model within five minutes. With lightweight fine-tuning, it\nachieves high-performance recovery in under an hour. We make our code publicly\navailable at https:\/\/github.com\/JarvisPei\/CMoE.",
        "The movement to mobile computing solutions provides flexibility to different\nusers whether it is a business user, a student, or even providing entertainment\nto children and adults of all ages. Due to these emerging technologies mobile\nusers are unable to safeguard private information in a very effective way and\ncybercrimes are increasing day by day. This manuscript will focus on security\nvulnerabilities in the mobile computing industry, especially focusing on\ntablets and smart phones. This study will dive into current security threats\nfor the Android & Apple iOS market, exposing security risks and threats that\nthe novice or average user may not be aware of. The purpose of this study is to\nanalyze current security risks and threats, and provide solutions that may be\ndeployed to protect against such threats.",
        "In this paper, we introduce a novel mechanism that uses machine learning\ntechniques to detect water leaks in pipes. The proposed simple and low-cost\nmechanism is designed that can be easily installed on building pipes with\nvarious sizes. The system works based on gathering and amplifying water flow\nsignals using a mechanical sound amplifier. Then sounds are recorded and\nconverted to digital signals in order to be analyzed. After feature extraction\nand selection, deep neural networks are used to discriminate between with and\nwithout leak pipes. The experimental results show that this device can detect\nat least 100 milliliters per minute (mL\/min) of water flow in a pipe so that it\ncan be used as a core of a water leakage detection system.",
        "The future Electron-Ion Collider (EIC) will operate a series of\nhigh-luminosity high-energy electron+proton ($e+p$) and electron+nucleus\n($\\textit{e + A}$) collisions to study several fundamental questions in the\nhigh energy and nuclear physics field. Heavy flavor hadron and jet production\nat the EIC plays an important role in exploring both potential modification on\nthe initial-state nuclear parton distribution functions (nPDFs) and final-state\nparton propagation and hadronization processes under different nuclear medium\nconditions. The current design of the EIC ePIC detector has good performance of\nvertex and track reconstruction, particle identification and energy\ndetermination in the pseudorapidity region of $-3.5<\\eta<3.5$, which will\nenable a series of high precision heavy flavor hadron and jet measurements.\nLatest simulation studies of the projected nuclear modification factor $R_{eA}$\nof heavy flavor jets and heavy flavor hadron inside jets in $e+p$ and\n$\\textit{e + Au}$ collisions at $\\sqrt{s} =$ 28.6 GeV and 63.2 GeV as well as\nthe projected statistical accuracy of inclusive and differential charm baryon\nover meson ratio measurements in $e+p$ collisions will be presented. The\nimpacts of these proposed EIC measurements on constraining the heavy quark\npropagation properties in cold nuclear medium and exploring the heavy quark\nhadronization process will be discussed.",
        "Transformer-based Large Language Models (LLMs) struggle to process inputs\nexceeding their training context window, with performance degrading due to\npositional out-of-distribution (O.O.D.) that disrupt attention computations.\nExisting solutions, fine-tuning and training-free methods, are limited by\ncomputational inefficiency, attention logit outliers or loss of local\npositional information. To address this, we propose Greedy Attention Logit\nInterpolation (GALI), a training-free length extrapolation method that\nmaximizes the utilization of pretrained positional intervals while avoiding\nattention logit outliers through attention logit interpolation. The result\ndemonstrates that GALI consistently outperforms state-of-the-art training-free\nmethods. Our findings reveal that LLMs interpret positional intervals unevenly\nwithin their training context window, suggesting that extrapolating within a\nsmaller positional interval range yields superior results-even for\nshort-context tasks. GALI represents a significant step toward resolving the\npositional O.O.D. challenge, enabling more reliable long-text understanding in\nLLMs. Our implementation of GALI, along with the experiments from our paper, is\nopen-sourced at https:\/\/github.com\/AcademyCityL\/GALI.",
        "Large Language Models (LLMs) reasoning processes are challenging to analyze\ndue to their complexity and the lack of organized visualization tools. We\npresent ReasonGraph, a web-based platform for visualizing and analyzing LLM\nreasoning processes. It supports both sequential and tree-based reasoning\nmethods while integrating with major LLM providers and over fifty\nstate-of-the-art models. ReasonGraph incorporates an intuitive UI with meta\nreasoning method selection, configurable visualization parameters, and a\nmodular framework that facilitates efficient extension. Our evaluation shows\nhigh parsing reliability, efficient processing, and strong usability across\nvarious downstream applications. By providing a unified visualization\nframework, ReasonGraph reduces cognitive load in analyzing complex reasoning\npaths, improves error detection in logical processes, and enables more\neffective development of LLM-based applications. The platform is open-source,\npromoting accessibility and reproducibility in LLM reasoning analysis.",
        "We settle the theoretical ground for the study of automata networks under\nblock-parallel update schedules, which are somehow dual to the block-sequential\nones, but allow for repetitions of automaton updates. This gain in expressivity\nbrings new challenges, and we analyse natural equivalence classes of update\nschedules: those leading to the same dynamics, and to the same limit dynamics,\nfor any automata network. Countings and enumeration algorithms are provided,\nfor their numerical study. We also prove computational complexity bounds for\nmany classical problems, involving fixed points, limit cycles, the recognition\nof subdynamics, reachability, etc. The PSPACE-completeness of computing the\nimage of a single configuration lifts the complexity of most problems, but the\nlandscape keeps some relief, in particular for reversible computations.",
        "Quadratic Unconstrained Binary Optimization (QUBO)-based suppression in\nobject detection is known to have superiority to conventional Non-Maximum\nSuppression (NMS), especially for crowded scenes where NMS possibly suppresses\nthe (partially-) occluded true positives with low confidence scores. Whereas\nexisting QUBO formulations are less likely to miss occluded objects than NMS,\nthere is room for improvement because existing QUBO formulations naively\nconsider confidence scores and pairwise scores based on spatial overlap between\npredictions. This study proposes new QUBO formulations that aim to distinguish\nwhether the overlap between predictions is due to the occlusion of objects or\ndue to redundancy in prediction, i.e., multiple predictions for a single\nobject. The proposed QUBO formulation integrates two features into the pairwise\nscore of the existing QUBO formulation: i) the appearance feature calculated by\nthe image similarity metric and ii) the product of confidence scores. These\nfeatures are derived from the hypothesis that redundant predictions share a\nsimilar appearance feature and (partially-) occluded objects have low\nconfidence scores, respectively. The proposed methods demonstrate significant\nadvancement over state-of-the-art QUBO-based suppression without a notable\nincrease in runtime, achieving up to 4.54 points improvement in mAP and 9.89\npoints gain in mAR.",
        "Despite their outstanding performance in the majority of scenarios,\ncontemporary language models still occasionally generate undesirable outputs,\nfor example, hallucinated text. While such behaviors have previously been\nlinked to uncertainty, there is a notable lack of methods that actively\nconsider uncertainty during text generation. In this work, we show how Minimum\nBayes Risk (MBR) decoding, which selects model generations according to an\nexpected risk, can be generalized into a principled uncertainty-aware decoding\nmethod. In short, we account for model uncertainty during decoding by\nincorporating a posterior over model parameters into MBR's computation of\nexpected risk. We show that this modified expected risk is useful for both\nchoosing outputs and deciding when to abstain from generation and can provide\nimprovements without incurring overhead. We benchmark different methods for\nlearning posteriors and show that performance improves with prediction\ndiversity. We release our code publicly.",
        "Visual place recognition is a challenging task for autonomous driving and\nrobotics, which is usually considered as an image retrieval problem. A commonly\nused two-stage strategy involves global retrieval followed by re-ranking using\npatch-level descriptors. Most deep learning-based methods in an end-to-end\nmanner cannot extract global features with sufficient semantic information from\nRGB images. In contrast, re-ranking can utilize more explicit structural and\nsemantic information in one-to-one matching process, but it is time-consuming.\nTo bridge the gap between global retrieval and re-ranking and achieve a good\ntrade-off between accuracy and efficiency, we propose StructVPR++, a framework\nthat embeds structural and semantic knowledge into RGB global representations\nvia segmentation-guided distillation. Our key innovation lies in decoupling\nlabel-specific features from global descriptors, enabling explicit semantic\nalignment between image pairs without requiring segmentation during deployment.\nFurthermore, we introduce a sample-wise weighted distillation strategy that\nprioritizes reliable training pairs while suppressing noisy ones. Experiments\non four benchmarks demonstrate that StructVPR++ surpasses state-of-the-art\nglobal methods by 5-23% in Recall@1 and even outperforms many two-stage\napproaches, achieving real-time efficiency with a single RGB input.",
        "In this paper, we use Legendre-Fenchel transform and a space decomposition to\ncarry out Fountain theorem and dual Fountain theorem for the following elliptic\nsystem of Hamiltonian type: \\[ \\begin{cases} \\begin{aligned} -\\Delta u&=H_v(u,\nv) \\,\\quad&&\\text{in}~\\Omega,\\\\ -\\Delta v&=H_u(u, v)\n\\,\\quad&&\\text{in}~\\Omega,\\\\ u,\\,v&=0~~&&\\text{on} ~ \\partial\\Omega,\\\\\n\\end{aligned} \\end{cases} \\] where $N\\ge 1$, $\\Omega \\subset \\mathbb{R}^N$ is a\nbounded domain and $H\\in C^1( \\mathbb{R}^2)$ is strictly convex, even and\nsubcritical. We mainly present two results: (i) When $H$ is superlinear, the\nsystem has infinitely many solutions, whose energies tend to infinity. (ii)\nWhen $H$ is sublinear, the system has infinitely many solutions, whose energies\nare negative and tend to 0. As a byproduct, the Lane-Emden system under\nsubcritical growth has infinitely many solutions.",
        "This study explores stochastic resonance (SR) in a Schmitt trigger circuit\nand its application to weak signal detection. SR, a phenomenon where noise\nsynchronizes with weak signals to enhance detectability, was demonstrated using\na custom-designed bi-stable Schmitt trigger system. The circuit's bi-stability\nwas validated through hysteresis curve analysis, confirming its suitability for\nSR studies. Experimental results revealed SR behavior by analyzing\nsignal-to-noise ratio (SNR) responses to noise amplitude variations. Detection\nexperiments were conducted to determine frequency and amplitude of damping\nsinusoidal pulses. Frequency detection proved effective, albeit with\nlimitations at low frequencies, while amplitude detection faced challenges due\nto mathematical complexities. Nonetheless, the study highlights SR's potential\nfor weak signal detection, with proposed enhancements to improve detection\naccuracy. This work underscores the adaptability of classical SR principles to\npractical detection systems and suggests future applications in advanced\ndetection technologies, including quantum systems.",
        "Millimeter-wave (mmWave) radar offers robust sensing capabilities in diverse\nenvironments, making it a highly promising solution for human body\nreconstruction due to its privacy-friendly and non-intrusive nature. However,\nthe significant sparsity of mmWave point clouds limits the estimation accuracy.\nTo overcome this challenge, we propose a two-stage deep learning framework that\nenhances mmWave point clouds and improves human body reconstruction accuracy.\nOur method includes a mmWave point cloud enhancement module that densifies the\nraw data by leveraging temporal features and a multi-stage completion network,\nfollowed by a 2D-3D fusion module that extracts both 2D and 3D motion features\nto refine SMPL parameters. The mmWave point cloud enhancement module learns the\ndetailed shape and posture information from 2D human masks in single-view\nimages. However, image-based supervision is involved only during the training\nphase, and the inference relies solely on sparse point clouds to maintain\nprivacy. Experiments on multiple datasets demonstrate that our approach\noutperforms state-of-the-art methods, with the enhanced point clouds further\nimproving performance when integrated into existing models.",
        "We show that, under Dubrovin's notion of ''almost'' duality, the Frobenius\nmanifold structure on the orbit spaces of the extended affine Weyl groups of\ntype $\\mathrm{ADE}$ is dual, for suitable choices of weight markings, to the\nequivariant quantum cohomology of the minimal resolution of the du Val\nsingularity of the same Dynkin type. We also provide a uniform Lie-theoretic\nconstruction of Landau-Ginzburg mirrors for the quantum cohomology of\n$\\mathrm{ADE}$ resolutions. The mirror B-model is described by a\none-dimensional LG superpotential associated to the spectral curve of the\n$\\widehat{\\mathrm{ADE}}$ affine relativistic Toda chain.",
        "To study a deformation of a singularity taking into consideration their\ndifferential geometric properties, a form representing the deformation using\nonly diffeomorphisms on the source space and isometries of the target space\nplays a crucial role. Such a form for an $S_1$ singularity is obtained by the\nauthor's previous work. On this form, we give a necessary and sufficient\ncondition for such a map is being a frontal. The form for an $S_1$ singularity\nwith the frontal condition can be considered such a form for a cuspidal $S_1$\nsingularity. Using this form, we investigate geometric properties of cuspidal\n$S_1$ singularities and the cuspidal cross caps appearing in the deformation.",
        "Motivated by gravitational wave observations of binary neutron-star mergers,\nwe study the thermal index of low-density, high-temperature dense matter. We\nuse the virial expansion to account for nuclear interaction effects. We focus\non the region of validity of the expansion, which reaches $10^{-3}$ fm$^{-3}$\nat $T=5$ MeV up to almost saturation density at $T=50$ MeV. In pure neutron\nmatter, we find an analytical expression for the thermal index, and show that\nit is nearly density- and temperature-independent, within a fraction of a\npercent of the non-interacting, non-relativistic value of $\\Gamma_\\text{th}\n\\approx 5\/3$. When we incorporate protons, electrons and photons, we find that\nthe density and temperature dependence of the thermal index changes\nsignificantly. We predict a smooth transition between an electron-dominated\nregime with $\\Gamma_\\text{th} \\approx 4\/3$ at low densities to a\nneutron-dominated region with $\\Gamma_\\text{th} \\approx 5\/3$ at high densities.\nThis behavior is by and large independent of proton fraction and is not\naffected by nuclear interactions in the region where the virial expansion\nconverges. We model this smooth transition analytically and provide a simple\nbut accurate parametrization of the inflection point between these regimes.\nWhen compared to tabulated realistic models of the thermal index, we find an\noverall agreement at high temperatures that weakens for colder matter. The\ndiscrepancies can be attributed to the missing contributions of nuclear\nclusters. The virial approximation provides a clear and physically intuitive\nframework for understanding the thermal properties of dense matter, offering a\ncomputationally efficient solution that makes it particularly well-suited for\nthe regimes relevant to neutron star binary remnants.",
        "T CrA is a Herbig Ae-type young star in a complex circumstellar environment;\nit includes a circumstellar disk, accretion streamers, jets, and outflows. It\nhas long been suspected to be a binary. However, until now, there has been no\ndirect detection of a companion. Here we present new VLTI\/MATISSE L- and N-band\nobservations of T CrA taken between 2023 May and 2024 August with the aim of\ntesting the binary nature of the system. We modeled the data with a geometric\nmodel using the Python tool oimodeler. We detected a companion (T CrA B) with a\nprojected separation of $\\Delta r = 153.2 \\pm 1.2$ mas ($\\approx 23$ au) toward\nthe west direction at a position angle of $275.4 \\pm 0.1^\\circ$, in 2024\nMay-August. Our results support that the companion has a nearly edge-on orbit\nthat is highly misaligned with respect to the circumprimary disk. Such a\nconfiguration could cause warping and tearing of the disk around the primary,\nwhich has been proposed by recent studies. In the L band the companion is\nextended, with a full width at half maximum (FWHM) size of $\\sim 1$ au,\nsuggesting that the emission comes from a disk around the secondary star. The\ncompanion flux is 0.2-0.3 Jy in the L band, and 0.2-0.7 Jy in the N band,\naccounting for 4-20% of the total emission at those wavelengths. The SED of the\ncompanion is compatible with thermal radiation of warm dust (600-800 K).",
        "State-of-the-art methods for mesh detailization predominantly utilize\nGenerative Adversarial Networks (GANs) to generate detailed meshes from coarse\nones. These methods typically learn a specific style code for each category or\nsimilar categories without enforcing geometry supervision across different\nLevels of Detail (LODs). Consequently, such methods often fail to generalize\nacross a broader range of categories and cannot ensure shape consistency\nthroughout the detailization process. In this paper, we introduce MARS, a novel\napproach for 3D shape detailization. Our method capitalizes on a novel\nmulti-LOD, multi-category mesh representation to learn shape-consistent mesh\nrepresentations in latent space across different LODs. We further propose a\nmesh autoregressive model capable of generating such latent representations\nthrough next-LOD token prediction. This approach significantly enhances the\nrealism of the generated shapes. Extensive experiments conducted on the\nchallenging 3D Shape Detailization benchmark demonstrate that our proposed MARS\nmodel achieves state-of-the-art performance, surpassing existing methods in\nboth qualitative and quantitative assessments. Notably, the model's capability\nto generate fine-grained details while preserving the overall shape integrity\nis particularly commendable.",
        "Lifted neural networks (i.e. neural architectures explicitly optimizing over\nrespective network potentials to determine the neural activities) can be\ncombined with a type of adversarial training to gain robustness for internal as\nwell as input layers, in addition to improved generalization performance. In\nthis work we first investigate how adversarial robustness in this framework can\nbe further strengthened by solely modifying the training loss. In a second step\nwe fix some remaining limitations and arrive at a novel training loss for\nlifted neural networks, that combines targeted and untargeted adversarial\nperturbations.",
        "This study as part of an ongoing research effort, empirically examines the\nrelationship between foreign trade in the Istanbul Ataturk Airport Free Zone\nand exchange rate movements. Monthly data from 2003 to 2016 were analyzed\nthrough stationarity tests (Unit Root), followed by the Vector Autoregressive\n(VAR) model, Cointegration Analysis, and the Toda-Yamamoto Causality Test. The\nfindings indicate that the exchange rate does not significantly affect imports\nand exports in the free zone. This result suggests that free zones, due to\ntheir structural characteristics and operational framework, may be relatively\ninsulated from exchange rate fluctuations. The study contributes to the\nliterature by providing a focused analysis of a specific free zone in Turkiye,\nhighlighting the potential independence of free zone trade from exchange rate\nvolatility.",
        "Parameter estimation is a fundamental challenge in machine learning, crucial\nfor tasks such as neural network weight fitting and Bayesian inference. This\npaper focuses on the complexity of estimating translation $\\boldsymbol{\\mu} \\in\n\\mathbb{R}^l$ and shrinkage $\\sigma \\in \\mathbb{R}_{++}$ parameters for a\ndistribution of the form $\\frac{1}{\\sigma^l} f_0 \\left( \\frac{\\boldsymbol{x} -\n\\boldsymbol{\\mu}}{\\sigma} \\right)$, where $f_0$ is a known density in\n$\\mathbb{R}^l$ given $n$ samples. We highlight that while the problem is\nNP-hard for Maximum Likelihood Estimation (MLE), it is possible to obtain\n$\\varepsilon$-approximations for arbitrary $\\varepsilon > 0$ within\n$\\text{poly} \\left( \\frac{1}{\\varepsilon} \\right)$ time using the Wasserstein\ndistance.",
        "In this paper, we explore a self-supervised model that learns to detect the\nsymmetry of a single object without requiring a dataset-relying solely on the\ninput object itself. We hypothesize that the symmetry of an object can be\ndetermined by its intrinsic features, eliminating the need for large datasets\nduring training. Additionally, we design a self-supervised learning strategy\nthat removes the necessity of ground truth labels. These two key elements make\nour approach both effective and efficient, addressing the prohibitive costs\nassociated with constructing large, labeled datasets for this task. The novelty\nof our method lies in computing features for each point on the object based on\nthe idea that symmetric points should exhibit similar visual appearances. To\nachieve this, we leverage features extracted from a foundational image model to\ncompute a visual descriptor for the points. This approach equips the point\ncloud with visual features that facilitate the optimization of our\nself-supervised model. Experimental results demonstrate that our method\nsurpasses the state-of-the-art models trained on large datasets. Furthermore,\nour model is more efficient, effective, and operates with minimal computational\nand data resources.",
        "A main bottleneck of learning-based robotic scene understanding methods is\nthe heavy reliance on extensive annotated training data, which often limits\ntheir generalization ability. In LiDAR panoptic segmentation, this challenge\nbecomes even more pronounced due to the need to simultaneously address both\nsemantic and instance segmentation from complex, high-dimensional point cloud\ndata. In this work, we address the challenge of LiDAR panoptic segmentation\nwith very few labeled samples by leveraging recent advances in label-efficient\nvision panoptic segmentation. To this end, we propose a novel method,\nLimited-Label LiDAR Panoptic Segmentation (L3PS), which requires only a minimal\namount of labeled data. Our approach first utilizes a label-efficient 2D\nnetwork to generate panoptic pseudo-labels from a small set of annotated\nimages, which are subsequently projected onto point clouds. We then introduce a\nnovel 3D refinement module that capitalizes on the geometric properties of\npoint clouds. By incorporating clustering techniques, sequential scan\naccumulation, and ground point separation, this module significantly enhances\nthe accuracy of the pseudo-labels, improving segmentation quality by up to\n+10.6 PQ and +7.9 mIoU. We demonstrate that these refined pseudo-labels can be\nused to effectively train off-the-shelf LiDAR segmentation networks. Through\nextensive experiments, we show that L3PS not only outperforms existing methods\nbut also substantially reduces the annotation burden. We release the code of\nour work at https:\/\/l3ps.cs.uni-freiburg.de.",
        "From image to video understanding, the capabilities of Multi-modal LLMs\n(MLLMs) are increasingly powerful. However, most existing video understanding\nbenchmarks are relatively short, which makes them inadequate for effectively\nevaluating the long-sequence modeling capabilities of MLLMs. This highlights\nthe urgent need for a comprehensive and integrated long video understanding\nbenchmark to assess the ability of MLLMs thoroughly. To this end, we propose\nALLVB (ALL-in-One Long Video Understanding Benchmark). ALLVB's main\ncontributions include: 1) It integrates 9 major video understanding tasks.\nThese tasks are converted into video QA formats, allowing a single benchmark to\nevaluate 9 different video understanding capabilities of MLLMs, highlighting\nthe versatility, comprehensiveness, and challenging nature of ALLVB. 2) A fully\nautomated annotation pipeline using GPT-4o is designed, requiring only human\nquality control, which facilitates the maintenance and expansion of the\nbenchmark. 3) It contains 1,376 videos across 16 categories, averaging nearly 2\nhours each, with a total of 252k QAs. To the best of our knowledge, it is the\nlargest long video understanding benchmark in terms of the number of videos,\naverage duration, and number of QAs. We have tested various mainstream MLLMs on\nALLVB, and the results indicate that even the most advanced commercial models\nhave significant room for improvement. This reflects the benchmark's\nchallenging nature and demonstrates the substantial potential for development\nin long video understanding.",
        "Given the increasing global emphasis on sustainable energy usage and the\nrising energy demands of cellular wireless networks, this work seeks an optimal\nshort-term, continuous-time power procurement schedule to minimize operating\nexpenditure and the carbon footprint of cellular wireless networks equipped\nwith energy storage capacity, and hybrid energy systems comprising uncertain\nrenewable energy sources. Despite the stochastic nature of wireless fading\nchannels, the network operator must ensure a certain quality-of-service (QoS)\nconstraint with high probability. This probabilistic constraint prevents using\nthe dynamic programming principle to solve the stochastic optimal control\nproblem. This work introduces a novel time-continuous Lagrangian relaxation\napproach tailored for real-time, near-optimal energy procurement in cellular\nnetworks, overcoming tractability problems associated with the probabilistic\nQoS constraint. The numerical solution procedure includes an efficient upwind\nfinite-difference solver for the Hamilton--Jacobi--Bellman equation\ncorresponding to the relaxed problem, and an effective combination of the\nlimited memory bundle method (LMBM) for handling nonsmooth optimization and the\nstochastic subgradient method (SSM) to navigate the stochasticity of the dual\nproblem. Numerical results, based on the German power system and daily cellular\ntraffic data, demonstrate the computational efficiency of the proposed\nnumerical approach, providing a near-optimal policy in a practical timeframe.",
        "Knowledge of the force time history of a structure is essential to assess its\nbehaviour, ensure safety and maintain reliability. However, direct measurement\nof external forces is often challenging due to sensor limitations, unknown\nforce characteristics, or inaccessible load points. This paper presents an\nefficient dynamic load reconstruction method using physics-informed Gaussian\nprocesses (GP) based on frequency-sparse Fourier basis functions. The GP's\ncovariance matrices are built using the description of the system dynamics, and\nthe model is trained using structural response measurements. This provides\nsupport and interpretability to the machine learning model, in contrast to\npurely data-driven methods. In addition, the model filters out irrelevant\ncomponents in the Fourier basis function by leveraging the sparsity of\nstructural responses in the frequency domain, thereby reducing computational\ncomplexity during optimization. The trained model for structural responses is\nthen integrated with the differential equation for a harmonic oscillator,\ncreating a probabilistic dynamic load model that predicts load patterns without\nrequiring force data during training. The model's effectiveness is validated\nthrough two case studies: a numerical model of a wind-excited 76-story building\nand an experiment using a physical scale model of the Lilleb{\\ae}lt Bridge in\nDenmark, excited by a servo motor. For both cases, validation of the\nreconstructed forces is provided using comparison metrics for several signal\nproperties. The developed model holds potential for applications in structural\nhealth monitoring, damage prognosis, and load model validation.",
        "Cause-of-death data is fundamental for understanding population health trends\nand inequalities as well as designing and evaluating public health\ninterventions. A significant proportion of global deaths, particularly in low-\nand middle-income countries (LMICs), do not have medically certified causes\nassigned. In such settings, verbal autopsy (VA) is a widely adopted approach to\nestimate disease burdens by interviewing caregivers of the deceased. Recently,\nlatent class models have been developed to model the joint distribution of\nsymptoms and perform probabilistic cause-of-death assignment. A large number of\nlatent classes are usually needed in order to characterize the complex\ndependence among symptoms, making the estimated symptom profiles challenging to\nsummarize and interpret. In this paper, we propose a flexible Bayesian tensor\ndecomposition framework that balances the predictive accuracy of the\ncause-of-death assignment task and the interpretability of the latent\nstructures. The key to our approach is to partition symptoms into groups and\nmodel the joint distributions of group-level symptom sub-profiles. The proposed\nmethods achieve better predictive accuracy than existing VA methods and provide\na more parsimonious representation of the symptom distributions. We show our\nmethods provide new insights into the clustering patterns of both symptoms and\ncauses using the PHMRC gold-standard VA dataset.",
        "As multimedia services such as video streaming, video conferencing, virtual\nreality (VR), and online gaming continue to expand, ensuring high perceptual\nvisual quality becomes a priority to maintain user satisfaction and\ncompetitiveness. However, multimedia content undergoes various distortions\nduring acquisition, compression, transmission, and storage, resulting in the\ndegradation of experienced quality. Thus, perceptual visual quality assessment\n(PVQA), which focuses on evaluating the quality of multimedia content based on\nhuman perception, is essential for optimizing user experiences in advanced\ncommunication systems. Several challenges are involved in the PVQA process,\nincluding diverse characteristics of multimedia content such as image, video,\nVR, point cloud, mesh, multimodality, etc., and complex distortion scenarios as\nwell as viewing conditions. In this paper, we first present an overview of PVQA\nprinciples and methods. This includes both subjective methods, where users\ndirectly rate their experiences, and objective methods, where algorithms\npredict human perception based on measurable factors such as bitrate, frame\nrate, and compression levels. Based on the basics of PVQA, quality predictors\nfor different multimedia data are then introduced. In addition to traditional\nimages and videos, immersive multimedia and generative artificial intelligence\n(GenAI) content are also discussed. Finally, the paper concludes with a\ndiscussion on the future directions of PVQA research.",
        "Analyzing stocks and making higher accurate predictions on where the price is\nheading continues to become more and more challenging therefore, we designed a\nnew financial algorithm that leverages social media sentiment analysis to\nenhance the prediction of key stock earnings and associated volatility. Our\nmodel integrates sentiment analysis and data retrieval techniques to extract\ncritical information from social media, analyze company financials, and compare\nsentiments between Wall Street and the general public. This approach aims to\nprovide investors with timely data to execute trades based on key events,\nrather than relying on long-term stock holding strategies. The stock market is\ncharacterized by rapid data flow and fluctuating community sentiments, which\ncan significantly impact trading outcomes. Stock forecasting is complex given\nits stochastic dynamic. Standard traditional prediction methods often overlook\nkey events and media engagement, focusing its practice into long-term\ninvestment options. Our research seeks to change the stochastic dynamic to a\nmore predictable environment by examining the impact of media on stock\nvolatility, understanding and identifying sentiment differences between Wall\nStreet and retail investors, and evaluating the impact of various media\nnetworks in predicting earning reports."
      ]
    }
  },
  {
    "id":2411.00578,
    "research_type":"applied",
    "start_id":"b25",
    "start_title":"Voxel Scene Graph for Intracranial Hemorrhage",
    "start_abstract":"Patients with Intracranial Hemorrhage (ICH) face a potentially life-threatening condition, and patient-centered individualized treatment remains challenging due to possible clinical complications. Deep-Learning-based methods can efficiently analyze the routinely acquired head CTs support decision-making. The majority of early work focuses on detection segmentation ICH, but do not model complex relations between ICH adjacent brain structures. In this work, we design tailored object method for which unite segmentation-grounded Scene Graph Generation (SGG) learn holistic representation cerebral scene. To best our knowledge, is first application SGG 3D voxel images. We evaluate two head-CT datasets demonstrate that recall up 74% clinically relevant relations. This lays foundation towards data. generated Graphs already provide insights clinician, are also valuable all downstream tasks as compact interpretable representation.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b8"
      ],
      "title":[
        "American Heart Association\/American Stroke Association. 2022 guideline for the management of patients with spontaneous intracerebral hemorrhage: A guideline from the american heart association\/american stroke association"
      ],
      "abstract":[
        "Approximately 10% of the 795\u2009000 strokes per year in the United States are intracerebral hemorrhages (ICHs),1 defined by brain injury attributable to acute blood extravasation into the brain parenchyma from a ruptured cerebral blood vessel. The clinical impact of ICH appears disproportionately high among lower-resource populations both in the United States and internationally. In US-based studies, ICH incidence has been reported to be \u22481.6-fold greater among Black than White people2 and 1.6-fold greater among Mexican American than non-Hispanic White people.3 Internationally, ICH incidence is substantially higher in low- and middle-income versus high-income countries, both as a proportion of all strokes and in absolute incidence rates.4,5 Several additional features of ICH make it a greater public health threat than conveyed by incidence numbers alone. ICH is arguably the deadliest form of acute stroke, with early-term mortality about 30% to 40% and no or minimal trend toward improvement over more recent time epochs.6\u20139 Incidence of ICH increases sharply with age and is therefore expected to remain substantial as the population ages, even with counterbalancing public health improvements in blood pressure (BP) control.8 Another growing source of ICH is more widespread use of anticoagulants,10 a trend likely to counterbalance the reduced ICH risk associated with increasing prescription of direct oral anticoagulants (DOACs) relative to vitamin K antagonists (VKAs).11 ICH thus remains in need of novel treatments and improved application of established approaches for every aspect of the disease: primary and secondary prevention, acute inpatient care, and poststroke rehabilitation and recovery. This guideline seeks to synthesize data in the ICH field into practical recommendations for clinical practice."
      ],
      "categories":[
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "An intriguing coincidence between the majority of vast polar structure\n  dwarfs and a recent major merger at the M31 position",
        "Fault-Resilience of Dissipative Processes for Quantum Computing",
        "Does a Large Language Model Really Speak in Human-Like Language?",
        "Fully-Automated Code Generation for Efficient Computation of Sparse\n  Matrix Permanents on GPUs",
        "Time Evolution of the Symmetry Resolved Entanglement Entropy after a\n  Mass Quench",
        "Quantum metric induced magneto-optical effects in\n  $\\mathcal{PT}$-symmetric antiferromagnets",
        "Systematic Search for Long-Term Trends in Fermi-LAT Jetted Active\n  Galactic Nuclei",
        "Approximate Evaluation Method for the Probability of the Union of\n  Independent Events",
        "Unveiling stellar spin: Determining inclination angles in Be stars",
        "FEASTS Combined with Interferometry (IV): Mapping HI Emission to a limit\n  of $N_{\\text{HI}}=10^{17.7} \\text{cm}^{-2}$ in Seven Edge-on Galaxies",
        "Randomized Spectral Clustering for Large-Scale Multi-Layer Networks",
        "Liquidity provision of utility indifference type in decentralized\n  exchanges",
        "How good is PAC-Bayes at explaining generalisation?",
        "Sliding ferroelectric control of unconventional magnetism in stacked\n  bilayers",
        "The centimeter emission from planet-forming disks in Taurus",
        "The{N\/D}-Conjecture for Nonresonant Hyperplane Arrangements",
        "Fine structure of phase diagram for social impact theory",
        "Spherical accretion in the Schwarzschild spacetime in the Newtonian\n  analogous construct",
        "Finite groups in which some particular non-nilpotent maximal invariant\n  subgroups have indices a prime-power",
        "Functional Linear Projection and Impulse Response Analysis",
        "Are there minimal exceptional aGUTs from stable 5D orbifolds?",
        "An $\\epsilon$-regularity theorem for Perelman's reduced volume",
        "Interpreting the HI 21-cm cosmology maps through Largest Cluster\n  Statistics. Part II. Impact of the realistic foreground and instrumental\n  noise on synthetic SKA1-Low observations",
        "Distribution Transformers: Fast Approximate Bayesian Inference With\n  On-The-Fly Prior Adaptation",
        "Thin Spectra for Periodic and Ergodic Word Models",
        "The generalized Lelong numbers and intersection theory",
        "Bell nonlocality in quantum networks with unreliable sources:\n  Loophole-free postelection via self-testing",
        "Origin of the Zeroth Law of Thermodynamics and its Role in Statistical\n  Mechanics",
        "Divisibility rules for integers presented as permutations"
      ],
      "abstract":[
        "A significant part of the Milky Way (MW) dwarf galaxies orbit within a Vast\nPOlar Structure (VPOS), which is perpendicular to the Galactic disc and whose\norigin has not yet been identified. It includes the Large Magellanic Cloud\n(LMC) and its six dynamically associated dwarf galaxies. Andromeda Galaxy (M31)\nexperienced a major merger two to three billion years ago, and its accurate\nmodelling predicts that an associated tidal tail is pointing towards the\nGalaxy. Here, we tested a possible association between M31 tidal tail particles\nand MW dwarf galaxies, focusing first on the LMC and its associated dwarfs\nsince they are less affected by ram pressure. We traced back these dwarf galaxy\norbits by one billion years and calculated their association with the tidal\ntail particles in the 6D phase space, based on their proper motion from\n\\textit{Gaia} DR3. We find that for low-mass MW models (total mass less than 5\n$\\times 10^{11} M_{\\odot}$), the separation in the 6D space can be less than\n1$\\sigma$ for most of the M31 modelling, albeit with a significant degree of\nfreedom due to the still unknown proper motion of M31. We further discover that\nmany other dwarfs could also be associated with the M31 tidal tails if their\nmotions had been radially slowed, as expected from the ram pressure exerted by\nthe MW corona. This intriguing coincidence could explain the origin of the\nVPOS, which resulted from a matter exchange between M31 and MW.",
        "Dissipative processes have long been proposed as a means of performing\ncomputational tasks on quantum computers that may be intrinsically more robust\nto noise. In this work, we prove two main results concerning the\nerror-resilience capabilities of two types of dissipative algorithms:\ndissipative ground state preparation in the form of the dissipative quantum\neigensolver (DQE), and dissipative quantum computation (DQC). The first result\nis that under circuit-level depolarizing noise, a version of the DQE algorithm\napplied to the geometrically local, stabilizer-encoded Hamiltonians that arise\nnaturally when fermionic Hamiltonians are represented in qubits, can suppress\nthe additive error in the ground space overlap of the final output state\nexponentially in the code distance. This enables us to get closer to\nfault-tolerance for this task without the associated overhead. In contrast, for\ncomputation as opposed to ground state preparation, the second result proves\nthat DQC is no more robust to noise than the standard quantum circuit model.",
        "Large Language Models (LLMs) have recently emerged, attracting considerable\nattention due to their ability to generate highly natural, human-like text.\nThis study compares the latent community structures of LLM-generated text and\nhuman-written text within a hypothesis testing procedure. Specifically, we\nanalyze three text sets: original human-written texts ($\\mathcal{O}$), their\nLLM-paraphrased versions ($\\mathcal{G}$), and a twice-paraphrased set\n($\\mathcal{S}$) derived from $\\mathcal{G}$. Our analysis addresses two key\nquestions: (1) Is the difference in latent community structures between\n$\\mathcal{O}$ and $\\mathcal{G}$ the same as that between $\\mathcal{G}$ and\n$\\mathcal{S}$? (2) Does $\\mathcal{G}$ become more similar to $\\mathcal{O}$ as\nthe LLM parameter controlling text variability is adjusted? The first question\nis based on the assumption that if LLM-generated text truly resembles human\nlanguage, then the gap between the pair ($\\mathcal{O}$, $\\mathcal{G}$) should\nbe similar to that between the pair ($\\mathcal{G}$, $\\mathcal{S}$), as both\npairs consist of an original text and its paraphrase. The second question\nexamines whether the degree of similarity between LLM-generated and human text\nvaries with changes in the breadth of text generation. To address these\nquestions, we propose a statistical hypothesis testing framework that leverages\nthe fact that each text has corresponding parts across all datasets due to\ntheir paraphrasing relationship. This relationship enables the mapping of one\ndataset's relative position to another, allowing two datasets to be mapped to a\nthird dataset. As a result, both mapped datasets can be quantified with respect\nto the space characterized by the third dataset, facilitating a direct\ncomparison between them. Our results indicate that GPT-generated text remains\ndistinct from human-authored text.",
        "Registers are the fastest memory components within the GPU's complex memory\nhierarchy, accessed by names rather than addresses. They are managed entirely\nby the compiler through a process called register allocation, during which the\ncompiler attempts to cache predictable data from thread-local memory into\nthread-private registers. Computing the permanent of a sparse matrix poses a\nchallenge for compilers, as optimizing this process is hindered by the\nunpredictable distribution of nonzero elements, which only become known at\nruntime. In this work, we employ fully-automated code generation to address\nthis, producing highly optimized kernels tailored to the matrix's sparsity\npattern. State-of-the-art permanent computation algorithms require each thread\nto store a private array, denoted x, of size n. We first propose a technique\nthat fully stores these arrays in registers, with inclusion and exclusion\nkernels generated for each column. To minimize control divergence and reduce\nthe number of unique kernels within a warp, we exploit the internal structure\nof Gray codes, which are also used in the state-of-the-art algorithm. Our\nsecond technique reduces register pressure by utilizing both registers and\nglobal memory and introduces a matrix ordering and partitioning strategy for\ngreater efficiency. On synthetic matrices, this approach achieves a 31x speedup\nover state-of-the-art CPU implementations on 112 cores, and an 8x speedup\ncompared to our traditional GPU implementation. For real-world matrices, these\nspeedups are 24.9x and 4.9x.",
        "In this paper we investigate the properties of the symmetry resolved\nentanglement entropy after a mass quench in the Ising field theory. Since the\ntheory is free and the post-quench state known explicitly, the one-point\nfunction of the relevant (composite) branch point twist field can be computed\nusing form factor techniques, similar to previous work on the branch point\ntwist field and the magnetisation, respectively. We find that the symmetry\nresolved entropy grows linearly in time at the same rate as the total entropy,\nand that there are sub-leading oscillatory corrections. This result provides\nthe first explicit computation of the out-of-equilibrium dynamics of the\nsymmetry resolved entropy employing twist fields in quantum field theory and is\nconsistent with existing results based on the quasiparticle picture.",
        "The magneto-optical effects (MOEs), as a fundamental physical phenomenon, can\nreveal the electronic structures of materials. The related probing methods are\nwidely used in the study of magnetic materials. However, space-time inversion\n($\\mathcal{PT}$) symmetric antiferromagnets were previously believed to be\nmagneto-optically inactive. Here, we point out that this traditional\nunderstanding is incorrect. Based on our generic formulas and symmetry\nanalysis, we find that in $\\mathcal{PT}$-symmetric antiferromagnets, it is the\nquantum metric, i.e., the real part of the quantum geometry, that induces MOEs.\nCombining a tight-binding model and first-principles calculations, we confirm\nthis observation by showing MOEs in the $\\mathcal{PT}$-symmetric\nantiferromagnet. Our work demonstrates that $\\mathcal{PT}$-symmetric\nantiferromagnets previously thought to lack MOEs can indeed exhibit MOEs and\ngreatly broaden the research on MOEs.",
        "Jetted Active Galactic Nuclei (AGN) exhibit variability across a wide range\nof time scales. Traditionally, this variability can often be modeled well as a\nstochastic process. However, in certain cases, jetted AGN variability displays\nregular patterns, enabling us to conduct investigations aimed at understanding\nits origins. Additionally, a novel type of variability has emerged in jetted\nAGN lightcurves, specifically, the observation of a long-term trend\ncharacterized by a linear increase of the flux with time in blazars such as PG\n1553+113, which is among the objects most likely to display periodic behavior.\nIn this paper, we present the results of a systematic search for long-term\ntrends, spanning $\\approx$10\\, years, utilizing 12 years of Fermi-LAT\nobservations. The study is focused on detecting the presence of linear or\nquadratic long-term trends in a sample of 3308 jetted AGN. Our analysis has\nidentified 40 jetted AGN that exhibit long-term trends, each with distinct\nproperties, which we also characterize in this study. These long-term trends\nmay originate from the dynamics of a supermassive black hole binary system, or\nthey could be the result of intrinsic phenomena within the jet itself. Our\nfindings can help in addressing questions pertaining to the astrophysical\norigins of variability and periodicity within jetted AGN.",
        "The evaluation of the probability of union of a large number of independent\nevents requires several combinations involving the factorial and the use of\nhigh performance computers with several hours of processing. Bounds and\nsimplifications on the probability of the union are useful in the analysis of\nstochastic problems across various areas including (but not limited to) systems\nreliability, biological systems, real-time fault-tolerant systems, probability\ntheory, information theory and communications. We propose an approximation to\nevaluate the probability of the union of several independent events that uses\nthe arithmetic mean of the probability of all of them. The approximate results\nare very close to, but larger than the exact values. The method allows a much\nsmaller number of operations with a similar result and more simplicity.",
        "The physical properties of stellar atmospheres in rapidly rotating massive\nstars, such as Be stars, are critical to understanding their evolution and\ntheir role as progenitors of supernovae. These stars, which often have\nnear-critical rotation, exhibit equatorial stretching and gravity darkening,\nwhich significantly complicates the determination of parameters such as the\ninclination angle. Be stars, characterized by their extreme rotational\nvelocities, serve as excellent candidates for exploring these phenomena.\nHowever, fundamental quantities such as polar and equatorial radii and\ninclination angles are typically derived from interferometry, which applies\nonly to a limited number of stars. This study aims to enhance the determination\nof inclination angles for Be stars using the ZPEKTR spectral synthesis code. By\nincorporating advanced models of gravity darkening and stellar deformation, we\nevaluated the effectiveness of this method with a sample of ten Be stars from\nthe BeSOS database, comparing results with established interferometric data.\nMethods. We used the ZPEKTR code to model the effects of stellar oblateness and\ngravity darkening on spectral lines, focusing on the HeI 4471 line. We applied\na chi-squared test minimization approach to identify the best-fitting models,\nand we evaluated the inclination angles derived against interferometric\nmeasurements. Our analysis reveals a robust linear correlation between the\ninclination angles derived from ZPEKTR and using interferometric techniques,\nwhich demonstrates an excellent agreement. The ZPEKTR code effectively models\nhigh rotational velocity effects, providing precise stellar parameter\ndeterminations. The results underscore the potential of advanced spectroscopic\ntechniques to yield inclination measurements comparable to interferometry,\nwhich offers a pathway to studying distant massive stars.",
        "We present a statistical study of the neutral atomic hydrogen (HI) gas\nextending into the circumgalactic medium perpendicular to the disk for 7\nedge-on galaxies with inclinations above $85^{\\circ}$ from the FEASTS program\nwith a $3\\sigma$ ($20\\,\\text{km}\\,\\text{s}^{-1}$) column density\n($N_{\\text{HI}}$) depth of $5\\times10^{17} \\text{cm}^{-2}$. We develop two\nphotometric methods to separate the extraplanar HI from the disk component,\nbased on existing interferometric data and parametric modeling of the disk flux\ndistribution respectively. With both methods, the FEASTS data exhibit clear\nextended wings beyond the disk along the minor axis. The extraplanar HI\naccounts for 5% to 20% of the total HI mass and extends to $20\\text{-}50$ kpc\nat $N_{\\text{HI}}=10^{18} \\text{cm}^{-2}$. We find a tight positive correlation\nbetween vertical extensions of the extraplanar HI and total HI mass\n$M_\\text{HI}$. The iso-density shape of HI at $N_{\\text{HI}}=10^{18}\n\\text{cm}^{-2}$ has an average axis ratio of $0.56\\pm0.11$. The off-disk\n$N_{\\text{HI}}$ profiles of these edge-on galaxies well represent the lower\nenvelop of previous Lyman-$\\alpha$ absorption measurements at low-redshift. Our\nresults suggest that at $N_{\\text{HI}}=5\\times10^{17} \\text{cm}^{-2}$, the HI\nextends considerably further than the known thin and thick disks in the\nvertical direction, but still remains much flattener than a spherical\ndistribution, consistent with theoretical expectations that outflow,\ncirculation, and accretion should have different impacts in these two\ndirections. We show the tension of our results with Illustris and TNG\npredictions, highlighting the constraining power of our results for future\nsimulations.",
        "Large-scale multi-layer networks with large numbers of nodes, edges, and\nlayers arise across various domains, which poses a great computational\nchallenge for the downstream analysis. In this paper, we develop an efficient\nrandomized spectral clustering algorithm for community detection of multi-layer\nnetworks. We first utilize the random sampling strategy to sparsify the\nadjacency matrix of each layer. Then we use the random projection strategy to\naccelerate the eigen-decomposition of the sum-of-squared sparsified adjacency\nmatrices of all layers. The communities are finally obtained via the k-means of\nthe eigenvectors. The algorithm not only has low time complexity but also saves\nthe storage space. Theoretically, we study the misclassification error rate of\nthe proposed algorithm under the multi-layer stochastic block models, which\nshows that the randomization does not deteriorate the error bound under certain\nconditions. Numerical studies on multi-layer networks with millions of nodes\nshow the superior efficiency of the proposed algorithm, which achieves\nclustering results rapidly. A new R package called MLRclust is developed and\nmade available to the public.",
        "We present a mathematical formulation of liquidity provision in decentralized\nexchanges. We focus on constant function market makers of utility indifference\ntype, which include constant product market makers with concentrated liquidity\nas a special case. First, we examine no-arbitrage conditions for a liquidity\npool and compute an optimal arbitrage strategy when there is an external liquid\nmarket. Second, we show that liquidity provision suffers from impermanent loss\nunless a transaction fee is levied under the general framework with\nconcentrated liquidity. Third, we establish the well-definedness of\narbitrage-free reserve processes of a liquidity pool in continuous-time and\nshow that there is no loss-versus-rebalancing under a nonzero fee if the\nexternal market price is continuous. We then argue that liquidity provision by\nmultiple liquidity providers can be understood as liquidity provision by a\nrepresentative liquidity provider, meaning that the analysis boils down to that\nfor a single liquidity provider. Last, but not least, we give an answer to the\nfundamental question in which sense the very construction of constant function\nmarket makers with concentrated liquidity in the popular platform Uniswap v3 is\noptimal.",
        "We discuss necessary conditions for a PAC-Bayes bound to provide a meaningful\ngeneralisation guarantee. Our analysis reveals that the optimal generalisation\nguarantee depends solely on the distribution of the risk induced by the prior\ndistribution. In particular, achieving a target generalisation level is only\nachievable if the prior places sufficient mass on high-performing predictors.\nWe relate these requirements to the prevalent practice of using data-dependent\npriors in deep learning PAC-Bayes applications, and discuss the implications\nfor the claim that PAC-Bayes ``explains'' generalisation.",
        "The control of unconventional magnetism, which displays an antiferromagnetic\nconfiguration with ferromagnetism-like properties, has drawn intense attention\nfor advancing antiferromagnetic spintronics. Here, through symmetry analysis,\nwe propose a general stacking rule, characterized by a connection operator\nlinking two stacked bilayers, for controlling unconventional magnetism via\nsliding ferroelectricity. Such rule enables the simultaneous switching of both\nelectric polarization and nonrelativistic spin splitting or anomalous Hall\neffect in altermagnets, a class of collinear unconventional magnets. By\ncomprehensively surveying the 80 layer groups, we identify all the stacking\norders that allow for such two types of simultaneous switching. Combined with\nfirst-principles calculations, we demonstrate the sliding ferroelectric control\nof spin polarization and anomalous Hall effect in the altermagnetic AgF2\nbilayer. Our work provides a symmetry strategy for achieving ferroelectric\ncontrol of unconventional magnetism in bilayer systems and opens avenues for\nexploring new types of magnetoelectric coupling.",
        "The last decade has witnessed remarkable advances in the characterization of\nthe (sub-)millimeter emission from planet-forming disks. Instead, the study of\nthe (sub-)centimeter emission has made more limited progress, to the point that\nonly a few exceptional disk-bearing objects have been characterized in the\ncentimeter regime. This work takes a broad view of the centimeter emission from\na large sample with VLA observations that is selected from previous ALMA\nsurveys of more representative disks in brightness and extent. We report on the\ndetection and characterization of flux at centimeter wavelengths from 21\nsources in the Taurus star-forming region. Complemented by literature and\narchival data, the entire photometry from 0.85 mm to 6 cm is fitted by a\ntwo-component model that determines the ubiquitous presence of free-free\nemission entangled with the dust emission. The flux density of the free-free\nemission is found to scale with the accretion rate but is independent of the\nouter disk morphology depicted by ALMA. The dust emission at 2 cm is still\nappreciable, and offers the possibility to extract an unprecedented large set\nof dust spectral indices in the centimeter regime. A pronounced change between\nthe median millimeter indices (2.3) and centimeter indices (2.8) suggests that\na large portion of the disk emission is optically thick up to 3 mm. The\ncomparison of both indices and fluxes with the ALMA disk extent indicates that\nthis portion can be as large as 40 au, and suggests that the grain population\nwithin this disk region that emits the observed centimeter emission is similar\nin disks with different size and morphology. All these results await\nconfirmation and dedicated dust modeling once facilities like ngVLA or SKA-mid\nare able to resolve the centimeter emission from planet-forming disks and\ndisentangle the various components.",
        "This paper studies Bernstein--Sato polynomials $b_{f,0}$ for homogeneous\npolynomials $f$ of degree $d$ with $n$ variables. It is open to know when\n$-{n\\over d}$ is a root of $b_{f,0}$. For essential indecomposable hyperplane\narrangements, this is a conjecture by Budur, Musta\\c{t}\\u{a} and Teitler and\nimplies the strong topological monodromy conjecture for arrangements. U.\nWalther gave a sufficient condition that a certain differential form does not\nvanish in the top cohomology group of Milnor fiber. We use Walther's result to\nverify the $n\\over d$-conjecture for weighted hyperplane arrangements\nsatisfying the nonresonant condition. We also give some essential\nindecomposable homogeneous polynomials $f$ such that $-{n\\over d}$ is not a\nroot of $b_{f,0}$. This leads to a conjectural sufficient condition for\n$b_{f,0}(-{n\\over d})=0$.",
        "In this paper, the social impact theory introduced by Latan\\'e is\nreconsidered. A fully differentiated society is considered; that is, initially\nevery actor has their own opinion. The equivalent of Muller's ratchet guards\nthat -- even for the non-deterministic case (with a positive social\ntemperature) -- any opinion once removed from the opinion space does not appear\nagain. With computer simulation, we construct the phase diagram for Latan\\'e\nmodel based on the number of surviving opinions after various evolution times.\nThe phase diagram is constructed on the two-dimensional plane of model control\nparameters responsible for the effective range of interaction among actors and\nthe social temperature. Introducing the Muller's ratchet-like mechanism gives a\nnon-zero chance for any opinion to be removed from the system. We believe that\nin such a case, for any positive temperature, ultimately a consensus is\nreached. However, even for a moderate system size, the time to reach consensus\nis very long. In contrast, for the deterministic case (without social\ntemperature), the system may be frozen with clusters of actors having several\ndifferent opinions, or even reach the cycle limit (with blinking structures).",
        "The velocity-dependent Newtonian analogous potentials (NAPs) corresponding to\ngeneral relativistic (GR) spacetimes accurately capture most of the\nrelativistic features, including all classical tests of GR, effectively\nrepresenting spacetime geometries in Newtonian terms. The NAP formulated by\nTejeda \\& Rosswog (TR13) for Schwarzschild spacetime has been applied to the\nstandard thin accretion disk around a black hole (BH) as well as in the context\nof streamlines of noninteracting particles accreting onto a Schwarzschild BH,\nshowing good agreement with the exact relativistic solutions. As a further\napplication, here we explore the extent to which TR13 NAP could describe a\ntransonic hydrodynamical spherical accretion flow in Schwarzschild spacetime\nwithin the framework of standard Newtonian hydrodynamics. Instead of obtaining\na typical single \"saddle-type\" sonic transition, a \"saddle-spiral pair\" is\nproduced, with the inner sonic point being an (unphysical) \"spiral type\" and\nthe outer being a usual \"saddle type.\" The Bondi accretion rate at outer sonic\nradii, however, remains consistent with that of the GR case. The primary reason\nfor the deviation of our findings from the classical Bondi solution is likely\ndue to the inconsistency between the Euler-type equation in the presence of\nvelocity-dependent TR13 NAP within the standard Newtonian hydrodynamics\nframework, and the corresponding GR Euler equation, regardless of the fluid's\nenergy. Our study suggests that a (modified) hydrodynamical formalism is needed\nto effectively implement such potentials in transonic accretion studies that\nalign with the spirit of TR13-like NAP, while remaining consistent with the GR\nhydrodynamics. This could then essentially circumvent GR hydrodynamics or GR\nmagnetohydrodynamics equations.",
        "Let $A$ and $G$ be finite groups such that $A$ acts coprimely on $G$ by\nautomorphisms, assume that $G$ has a maximal $A$-invariant subgroup $M$ that is\na direct product of some isomorphic simple groups, we prove that if $G$ has a\nnon-trivial $A$-invariant normal subgroup $N$ such that $N\\leq M$ and every\nnon-nilpotent maximal $A$-invariant subgroup $K$ of $G$ not containing $N$ has\nindex a prime-power and the projective special linear group $PSL_2(7)$ is not a\ncomposition factor of $G$, then $G$ is solvable.",
        "This paper proposes econometric methods for studying how economic variables\nrespond to function-valued shocks. Our methods are developed based on linear\nprojection estimation of predictive regression models with a function-valued\npredictor and other control variables. We show that the linear projection\ncoefficient associated with the functional variable allows for the impulse\nresponse interpretation in a functional structural vector autoregressive model\nunder a certain identification scheme, similar to well-known Sims' (1972)\ncausal chain, but with nontrivial complications in our functional setup. A\nnovel estimator based on an operator Schur complement is proposed and its\nasymptotic properties are studied. We illustrate its empirical applicability\nwith two examples involving functional variables: economy sentiment\ndistributions and functional monetary policy shocks.",
        "In analysing five dimensional orbifolds with exceptional gauge groups, we\nseek to find stable vacua configurations which satisfy the minimal requirements\nfor asymptotic grand unified models. In this respect we show that no minimal\nasymptotic grand unified theory can be built. Our results point towards\nnon-minimal models based on $E_6$: one featuring supersymmetry, and the other\nneeding a modification of the Coleman-Weinberg potential to stabilise the\nbreaking of $E_6$ to the standard model gauge group.",
        "In this article, we prove an $\\epsilon$-regularity theorem for Perelman's\nreduced volume. We show that on a Ricci flow, if Perelman's reduced volume is\nclose to $1$, then the curvature radius at the base point cannot be too small.",
        "The Largest Cluster Statistics\\,(LCS) analysis of the redshifted 21\\,cm maps\nhas been demonstrated to be an efficient and robust method for following the\ntime evolution of the largest ionized regions\\,(LIRs) during the Epoch of\nReionization\\,(EoR). The LCS can, in principle, constrain the reionization\nmodel and history by quantifying the morphology of neutral hydrogen\\,(\\HI)\ndistribution during the different stages of the EoR. Specifically, the\npercolation transition of ionized regions, quantified and constrained via LCS,\nprovides a crucial insight about the underlying reionization model. The\nprevious LCS analysis of EoR 21\\,cm maps demonstrates that the convolution of\nthe synthesized beam of the radio interferometric arrays, e.g. SKA1-Low with\nthe target signal, shifts the apparent percolation transition of ionized\nregions towards the lower redshifts. In this study, we present an optimal\nthresholding strategy to reduce this bias in the recovered percolation\ntransition. We assess the robustness of LCS analysis of the 21\\,cm maps in the\npresence of antenna-based gain calibration errors and instrumental noise for\nSKA1-Low. This analysis is performed using synthetic observations simulated by\nthe \\textsc{21cmE2E} pipeline, considering SKA1-Low AA4 configuration within a\nradius of 2\\,km from the array centre. Our findings suggest that a minimum of\n$1500$\\,hours of observation (SNR $\\gtrapprox 3$) are required for the LCS\nanalysis to credibly suppress the confusion introduced by thermal noise.\nFurther, we also demonstrate that for a maximum antenna-based calibration error\ntolerance of $\\sim 0.05\\%$ (post calibration), the reionization history can be\nrecovered in a robust and relatively unbiased manner using the LCS.",
        "While Bayesian inference provides a principled framework for reasoning under\nuncertainty, its widespread adoption is limited by the intractability of exact\nposterior computation, necessitating the use of approximate inference. However,\nexisting methods are often computationally expensive, or demand costly\nretraining when priors change, limiting their utility, particularly in\nsequential inference problems such as real-time sensor fusion. To address these\nchallenges, we introduce the Distribution Transformer -- a novel architecture\nthat can learn arbitrary distribution-to-distribution mappings. Our method can\nbe trained to map a prior to the corresponding posterior, conditioned on some\ndataset -- thus performing approximate Bayesian inference. Our novel\narchitecture represents a prior distribution as a (universally-approximating)\nGaussian Mixture Model (GMM), and transforms it into a GMM representation of\nthe posterior. The components of the GMM attend to each other via\nself-attention, and to the datapoints via cross-attention. We demonstrate that\nDistribution Transformers both maintain flexibility to vary the prior, and\nsignificantly reduces computation times-from minutes to milliseconds-while\nachieving log-likelihood performance on par with or superior to existing\napproximate inference methods across tasks such as sequential inference,\nquantum system parameter inference, and Gaussian Process predictive posterior\ninference with hyperpriors.",
        "We establish a new and simple criterion that suffices to generate many\nspectral gaps for periodic word models. This leads to new examples of ergodic\nSchr\\\"odinger operators with Cantor spectra having zero Hausdorff dimension\nthat simultaneously may have arbitrarily small supremum norm together with\narbitrarily long runs on which the potential vanishes.",
        "Let $X$ be a complex manifold of dimension $k,$ and $(V,\\omega)$ be a\nK\\\"ahler submanifold of dimension $l$ in $X,$ and $B\\Subset V$ be a domain with\n$\\mathcal{C}^2$-smooth boundary. Let $T$ be a positive plurisubharmonic current\non $X$ such that $T$ satisfies a reasonable approximation condition on $X$ and\nnear $\\partial B.$ In our previous work we introduce the concept of the\ngeneralized Lelong numbers $\\nu_j(T,B)\\in\\mathbb{R}$ of $T$ along $B$ for\n$0\\leq j\\leq l.$ When $l=0,$ $V=B$ is a single point $x,$ $\\nu_0(T,B)$ is none\nother than the classical Lelong number of $T$ at $x.$\n  This article has five purposes: Firstly, we formulate the notion of the\ngeneralized Lelong number of $T$ associated to every closed smooth $(j,j)$-form\non $V.$ This concept extends the previous notion of the generalized Lelong\nnumbers. We also establish their basic properties. Secondly, we define the\nhorizontal dimension $\\hbar$ of such a current $T$ along $B.$ Next, we\ncharacterize $\\hbar$ in terms of the generalized Lelong numbers. We also\nestablish a Siu's upper-semicontinuity type theorem for the generalized Lelong\nnumbers. In their above-mentioned context, Dinh and Sibony introduced some\ncohomology classes which may be regarded as their analogues of the classical\nLelong numbers. Our third objective is to generalize their notion to the\nbroader context where $T$ is (merely) positive pluriharmonic. Moreover, we also\nestablish a formula relating Dinh-Sibony classes and the generalized Lelong\nnumbers. Fourthly, we obtain an effective sufficient condition for defining the\nintersection of $m$ positive closed currents in the sense of Dinh-Sibony's\ntheory of tangent currents on a compact K\\\"ahler manifold. Finally, we\nestablish an effective sufficient condition for the continuity of the above\nintersection.",
        "We discuss Bell nonlocality in quantum networks with unreliable sources. Our\nmain result is a condition on the observed data which ensures that inconclusive\nevents can be safely discarded, without introducing any loophole. More\nformally, we characterize the fair-sampling property for measurements in a\nnetwork. When all measurements are fair-sampling, we show that the\npost-selection of conclusive outcomes does not compromise the assumption of\nsource independence, hence avoiding the detection loophole. Furthermore, we\nshow that in some cases, the fair-sampling property can in fact be guaranteed\nbased only on observed data. To show this, we prove that saturation of the\nFinner inequality provides a self-test of the underlying quantum model. We\nillustrate the relevance of our results by demonstrating an improvement in\ndevice-independent randomness generation for a photonic Bell test with a\nprobabilistic source and for the triangle network.",
        "In statistical mechanics the zeroth law of thermodynamics is taken as a\npostulate which, as its name indicates, logically precedes the first and second\nlaws. Treating it as a postulate has consequences for how temperature is\nintroduced into statistical mechanics and for the molecular interpretation of\ntemperature. One can, however, derive the zeroth law from first principles\nstarting from a classical Hamiltonian using basic mechanics and a geometric\nrepresentation of the phase space of kinetic energy configurations - the\nvelocity hypersphere. In this approach there is no difficulty in providing a\nmolecular interpretation of temperature, nor in deriving equality of\ntemperature as the condition of thermal equilibrium. The approach to the\nmacroscopic limit as a function of the number of atoms is easily determined.\nOne also obtains with little difficulty the Boltzmann probability distribution,\nthe statistical mechanical definition of entropy and the configuration\npartition function. These relations, along with the zeroth law, emerge as\nstraightforward consequences of atoms in random motion.",
        "In this note, we represent integers in a type of factoradic notation. Rather\nthan use the corresponding Lehmer code, we will view integers as permutations.\nGiven a pair of integers n and k, we give a formula for n mod k in terms of the\nfactoradic digits, and use this to deduce various divisibility rules."
      ]
    }
  },
  {
    "id":2411.00614,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Multimodal pooled Perturb-CITE-seq screens in patient models define mechanisms of cancer immune evasion",
    "start_abstract":"Resistance to immune checkpoint inhibitors (ICIs) is a key challenge in cancer therapy. To elucidate underlying mechanisms, we developed Perturb-CITE-sequencing (Perturb-CITE-seq), enabling pooled clustered regularly interspaced short palindromic repeat (CRISPR)\u2013Cas9 perturbations with single-cell transcriptome and protein readouts. In patient-derived melanoma cells and autologous tumor-infiltrating lymphocyte (TIL) co-cultures, we profiled transcriptomes and 20\u2009proteins in ~218,000\u2009cells under ~750\u2009perturbations associated with cancer cell-intrinsic ICI resistance (ICR). We recover known mechanisms of resistance, including defects in the interferon-\u03b3 (IFN-\u03b3)\u2013JAK\/STAT and antigen-presentation pathways in RNA, protein and perturbation space, and new ones, including loss\/downregulation of CD58. Loss of CD58 conferred immune evasion in multiple co-culture models and was downregulated in tumors of melanoma patients with ICR. CD58 protein expression was not induced by IFN-\u03b3 signaling, and CD58 loss conferred immune evasion without compromising major histocompatibility complex (MHC) expression, suggesting that it acts orthogonally to known mechanisms of ICR. This work provides a framework for the deciphering of complex mechanisms by large-scale perturbation screens with multimodal, single-cell readouts, and discovers potentially clinically relevant mechanisms of immune evasion.",
    "start_categories":[
      "q-bio.BM"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b11"
      ],
      "title":[
        "Causal identification of single-cell experimental perturbation effects with CINEMA-OT"
      ],
      "abstract":[
        "Abstract Recent advancements in single-cell technologies allow characterization of experimental perturbations at resolution. While methods have been developed to analyze such experiments, the application a strict causal framework has not yet explored for inference treatment effects level. Here we present causal-inference-based approach perturbation analysis, termed CINEMA-OT (causal independent effect module attribution + optimal transport). separates confounding sources variation from obtain an transport matching that reflects counterfactual cell pairs. These pairs represent responses permitting number novel analyses, as individual treatment-effect response clustering, and synergy analysis. We benchmark on array estimation tasks several simulated real datasets show it outperforms other analysis methods. Finally, perform two newly generated datasets: (1) rhinovirus cigarette-smoke-exposed airway organoids, (2) combinatorial cytokine stimulation immune cells. In these reveals potential mechanisms by which cigarette-smoke exposure dulls antiviral response, well logic governs chemokine secretion peripheral recruitment."
      ],
      "categories":[
        "math.OC"
      ]
    },
    "list":{
      "title":[
        "A Survey on Foundation-Model-Based Industrial Defect Detection",
        "Generalized $\\eta$-pairing theory and anomalous localization in\n  non-Hermitian systems",
        "Aerial Vision-and-Language Navigation with Grid-based View Selection and\n  Map Construction",
        "Real-Time Fast Marching Tree for Mobile Robot Motion Planning in Dynamic\n  Environments",
        "The Muddy Waters of Modeling Empathy in Language: The Practical Impacts\n  of Theoretical Constructs",
        "Theoretical study of the $\\Sigma N$ cusp in the\n  $K^-d\\rightarrow\\pi\\Lambda N$ reaction",
        "A modeling framework to support the electrification of private transport\n  in African cities: a case study of Addis Ababa",
        "Stacked Intelligent Metasurface Enabled Near-Field Multiuser\n  Beamfocusing in the Wave Domain",
        "Systematic Abductive Reasoning via Diverse Relation Representations in\n  Vector-symbolic Architecture",
        "A note On the existence of solutions to Hitchin's self-duality equations",
        "General relativistic quasi-spherical accretion in a dark matter halo",
        "General relativistic particle trajectories via quantum mechanical weak\n  values and the Schwarzschild-Alcubierre spacetime",
        "ProPINN: Demystifying Propagation Failures in Physics-Informed Neural\n  Networks",
        "RePanda: Pandas-powered Tabular Verification and Reasoning",
        "Comparing Native and Non-native English Speakers' Behaviors in\n  Collaborative Writing through Visual Analytics",
        "Multifractal Terrain Generation for Evaluating Autonomous Off-Road\n  Ground Vehicles",
        "Real-world actor-based image steganalysis via classifier inconsistency\n  detection",
        "OpenGERT: Open Source Automated Geometry Extraction with Geometric and\n  Electromagnetic Sensitivity Analyses for Ray-Tracing Propagation Models",
        "Improving TCM Question Answering through Tree-Organized Self-Reflective\n  Retrieval with LLMs",
        "Unsupervised Particle Tracking with Neuromorphic Computing",
        "Imagine to Hear: Auditory Knowledge Generation can be an Effective\n  Assistant for Language Models",
        "Efficient Point Clouds Upsampling via Flow Matching",
        "A classical proof of quantum knowledge for multi-prover interactive\n  proof systems",
        "Transient synchronization stability analysis and assessment of DFIG\n  system under severe faults",
        "Improving action segmentation via explicit similarity measurement",
        "VarDrop: Enhancing Training Efficiency by Reducing Variate Redundancy in\n  Periodic Time Series Forecasting",
        "Deep Understanding of Sign Language for Sign to Subtitle Alignment",
        "Efficient Hierarchical Contrastive Self-supervising Learning for Time\n  Series Classification via Importance-aware Resolution Selection",
        "Exploring Energy Landscapes for Minimal Counterfactual Explanations:\n  Applications in Cybersecurity and Beyond"
      ],
      "abstract":[
        "As industrial products become abundant and sophisticated, visual industrial\ndefect detection receives much attention, including two-dimensional and\nthree-dimensional visual feature modeling. Traditional methods use statistical\nanalysis, abnormal data synthesis modeling, and generation-based models to\nseparate product defect features and complete defect detection. Recently, the\nemergence of foundation models has brought visual and textual semantic prior\nknowledge. Many methods are based on foundation models (FM) to improve the\naccuracy of detection, but at the same time, increase model complexity and slow\ndown inference speed. Some FM-based methods have begun to explore lightweight\nmodeling ways, which have gradually attracted attention and deserve to be\nsystematically analyzed. In this paper, we conduct a systematic survey with\ncomparisons and discussions of foundation model methods from different aspects\nand briefly review non-foundation model (NFM) methods recently published.\nFurthermore, we discuss the differences between FM and NFM methods from\ntraining objectives, model structure and scale, model performance, and\npotential directions for future exploration. Through comparison, we find FM\nmethods are more suitable for few-shot and zero-shot learning, which are more\nin line with actual industrial application scenarios and worthy of in-depth\nresearch.",
        "By generalizing the eta-pairing theory to non-Hermitian Hubbard models on\narbitrary lattices, we obtain the sufficient and necessary condition for the\neta-pairing operator to be an eigenoperator of the Hamiltonian $H$, and find\nunique eta-pairing phenomena without Hermitian analogs. For instance, the\nHermitian conjugate of an eta-pairing eigenoperator may not be an\neigenoperator, eta-pairing eigenoperators can be spatially modulated, and the\n$SU(2)$ pseudospin symmetry may not be respected even if $H$ commutes with the\neta-pairing operators. Remarkably, these novel non-Hermitian phenomena are\nclosely related to each other by several theorems we establish and can lead to,\ne.g., the notion of non-Hermitian angular-momentum operators and the anomalous\nlocalization of eta-pairing eigenstates. Some issues on the $SO(4)$ and\nparticle-hole symmetries are clarified. Our general eta-pairing theory also\nreveals a previously unnoticed unification of these symmetries of the Hubbard\nmodel. To exemplify these findings, we propose the Hatano-Nelson-Hubbard model.\nIn this interacting non-Hermitian system without even the bulk translation\ninvariance, the right and left two-particle eta-pairing eigenstates are\nexponentially localized at opposite boundaries of the chain. We then generalize\nthis model to two dimensions and find that the eta-pairing eigenstate can\nexhibit the first- or second-order skin effect. Thus, eta-pairing may represent\na new mechanism for skin effects in interacting non-Hermitian systems, even in\nhigher dimensions and without the bulk translation symmetry. To realize all of\nthe non-Hermitian eta-pairing phenomena, we construct a general two-sublattice\nmodel defined on an arbitrary lattice, which can exhibit anomalous localization\nof eta-pairing eigenstates; besides, this model can reveal the eta-pairing\nstructure [e.g., the $SO(4)$ symmetry] in systems with Hermitian hoppings.",
        "Aerial Vision-and-Language Navigation (Aerial VLN) aims to obtain an unmanned\naerial vehicle agent to navigate aerial 3D environments following human\ninstruction. Compared to ground-based VLN, aerial VLN requires the agent to\ndecide the next action in both horizontal and vertical directions based on the\nfirst-person view observations. Previous methods struggle to perform well due\nto the longer navigation path, more complicated 3D scenes, and the neglect of\nthe interplay between vertical and horizontal actions. In this paper, we\npropose a novel grid-based view selection framework that formulates aerial VLN\naction prediction as a grid-based view selection task, incorporating vertical\naction prediction in a manner that accounts for the coupling with horizontal\nactions, thereby enabling effective altitude adjustments. We further introduce\na grid-based bird's eye view map for aerial space to fuse the visual\ninformation in the navigation history, provide contextual scene information,\nand mitigate the impact of obstacles. Finally, a cross-modal transformer is\nadopted to explicitly align the long navigation history with the instruction.\nWe demonstrate the superiority of our method in extensive experiments.",
        "This paper proposes the Real-Time Fast Marching Tree (RT-FMT), a real-time\nplanning algorithm that features local and global path generation,\nmultiple-query planning, and dynamic obstacle avoidance. During the search,\nRT-FMT quickly looks for the global solution and, in the meantime, generates\nlocal paths that can be used by the robot to start execution faster. In\naddition, our algorithm constantly rewires the tree to keep branches from\nforming inside the dynamic obstacles and to maintain the tree root near the\nrobot, which allows the tree to be reused multiple times for different goals.\nOur algorithm is based on the planners Fast Marching Tree (FMT*) and Real-time\nRapidly-Exploring Random Tree (RT-RRT*). We show via simulations that RT-FMT\noutperforms RT- RRT* in both execution cost and arrival time, in most cases.\nMoreover, we also demonstrate via simulation that it is worthwhile taking the\nlocal path before the global path is available in order to reduce arrival time,\neven though there is a small possibility of taking an inferior path.",
        "Conceptual operationalizations of empathy in NLP are varied, with some having\nspecific behaviors and properties, while others are more abstract. How these\nvariations relate to one another and capture properties of empathy observable\nin text remains unclear. To provide insight into this, we analyze the transfer\nperformance of empathy models adapted to empathy tasks with different\ntheoretical groundings. We study (1) the dimensionality of empathy definitions,\n(2) the correspondence between the defined dimensions and measured\/observed\nproperties, and (3) the conduciveness of the data to represent them, finding\nthey have a significant impact to performance compared to other transfer\nsetting features. Characterizing the theoretical grounding of empathy tasks as\ndirect, abstract, or adjacent further indicates that tasks that directly\npredict specified empathy components have higher transferability. Our work\nprovides empirical evidence for the need for precise and multidimensional\nempathy operationalizations.",
        "The $K^-d\\rightarrow\\pi\\Lambda N$ reaction is useful for exploring the\nhyperon-nucleon interaction through final state interactions. In particular,\nthe cusp structure of the $\\Lambda N$ invariant mass spectrum at the $\\Sigma N$\nthreshold contains information about the s-wave interaction of 1\/2-isospin\nhyperon-nucleon systems. The calculation of the spectrum is performed with the\naim of extracting the scattering length of the $\\Sigma N(I=1\/2)$ channel that\ncouples to the $\\Lambda N$ channel from this reaction, and the results are\ndiscussed in comparison with experimental data to highlight the factors that\nshould be considered.",
        "The electrification of road transport, as the predominant mode of\ntransportation in Africa, represents a great opportunity to reduce greenhouse\ngas emissions and dependence on costly fuel imports. However, it introduces\nmajor challenges for local energy infrastructures, including the deployment of\ncharging stations and the impact on often fragile electricity grids. Despite\nits importance, research on electric mobility planning in Africa remains\nlimited, while existing planning tools rely on detailed local mobility data\nthat is often unavailable, especially for privately owned passenger vehicles.\nIn this study, we introduce a novel framework designed to support private\nvehicle electrification in data-scarce regions and apply it to Addis Ababa,\nsimulating the mobility patterns and charging needs of 100,000 electric\nvehicles. Our analysis indicate that these vehicles generate a daily charging\ndemand of approximately 350 MWh and emphasize the significant influence of the\ncharging location on the spatial and temporal distribution of this demand.\nNotably, charging at public places can help smooth the charging demand\nthroughout the day, mitigating peak charging loads on the electricity grid. We\nalso estimate charging station requirements, finding that workplace charging\nrequires approximately one charging point per three electric vehicles, while\npublic charging requires only one per thirty. Finally, we demonstrate that\nphotovoltaic energy can cover a substantial share of the charging needs,\nemphasizing the potential for renewable energy integration. This study lays the\ngroundwork for electric mobility planning in Addis Ababa while offering a\ntransferable framework for other African cities.",
        "Intelligent surfaces represent a breakthrough technology capable of\ncustomizing the wireless channel cost-effectively. However, the existing works\ngenerally focus on planar wavefront, neglecting near-field spherical wavefront\ncharacteristics caused by large array aperture and high operation frequencies\nin the terahertz (THz). Additionally, the single-layer reconfigurable\nintelligent surface (RIS) lacks the signal processing ability to mitigate the\ncomputational complexity at the base station (BS). To address this issue, we\nintroduce a novel stacked intelligent metasurfaces (SIM) comprised of an array\nof programmable metasurface layers. The SIM aims to substitute conventional\ndigital baseband architecture to execute computing tasks with ultra-low\nprocessing delay, albeit with a reduced number of radio-frequency (RF) chains\nand low-resolution digital-to-analog converters. In this paper, we present a\nSIM-aided multiuser multiple-input single-output (MU-MISO) near-field system,\nwhere the SIM is integrated into the BS to perform beamfocusing in the wave\ndomain and customize an end-to-end channel with minimized inter-user\ninterference. Finally, the numerical results demonstrate that near-field\ncommunication achieves superior spatial gain over the far-field, and the SIM\neffectively suppresses inter-user interference as the wireless signals\npropagate through it.",
        "In abstract visual reasoning, monolithic deep learning models suffer from\nlimited interpretability and generalization, while existing neuro-symbolic\napproaches fall short in capturing the diversity and systematicity of\nattributes and relation representations. To address these challenges, we\npropose a Systematic Abductive Reasoning model with diverse relation\nrepresentations (Rel-SAR) in Vector-symbolic Architecture (VSA) to solve\nRaven's Progressive Matrices (RPM). To derive attribute representations with\nsymbolic reasoning potential, we introduce not only various types of atomic\nvectors that represent numeric, periodic and logical semantics, but also the\nstructured high-dimentional representation (SHDR) for the overall Grid\ncomponent. For systematic reasoning, we propose novel numerical and logical\nrelation functions and perform rule abduction and execution in a unified\nframework that integrates these relation representations. Experimental results\ndemonstrate that Rel-SAR achieves significant improvement on RPM tasks and\nexhibits robust out-of-distribution generalization. Rel-SAR leverages the\nsynergy between HD attribute representations and symbolic reasoning to achieve\nsystematic abductive reasoning with both interpretable and computable\nsemantics.",
        "In 1987, Hitchin introduced the self-duality equations on rank-2 complex\nvector bundles over compact Riemann surfaces with genus greater than one as a\nreduction of the Yang-Mills equation and established the existence of solutions\nto these equations starting from a Higgs stable bundle. In this paper, we fill\nin some technical details in Hitchin's original proof by the following three\nsteps. First, we reduce the existence of a solution of class $L_1^2$ to\nminimizing the energy functional within a Higgs stable orbit of the $L_2^2$\ncomplex gauge group action. Second, using this transformation, we obtain a\nsolution of class $L_1^2$ in this orbit. These two steps primarily follow\nHitchin's original approach. Finally, using the Coulomb gauge, we construct a\nsmooth solution by applying an $L_2^2$ unitary gauge transformation to the\n$L_1^2$ solution constructed previously. This last step provides additional\ntechnical details to Hitchin's original proof.",
        "Context. The Bondi spherical accretion solution has been used to model\naccretion onto compact objects in a variety of situations, from interpretation\nof observations to subgrid models in cosmological simulations. Aims. We aim to\ninvestigate how the presence of dark matter (DM) alters the dynamics and\nphysical properties of accretion onto supermassive black holes on scales\nranging from ~ 10 pc to the event horizon. Methods. In particular, we\ninvestigate Bondi-like accretion flows with zero and low specific angular\nmomentum around supermassive black holes surrounded by dark-matter halos by\nperforming 1D and 2.5D general relativistic hydrodynamics (GRHD) simulations\nusing the black hole accretion code (BHAC). Results. We find notable\ndifferences in the dynamics and structure of spherical accretion flows in the\npresence of DM. The most significant effects include increases in density,\ntemperature, and pressure, as well as variations in radial velocity both inside\nand outside the regions containing DM or even the production of outflow.\nConclusions. This investigation provides valuable insights into the role of\ncosmological effects, particularly DM, in shaping the behavior of accretion\nflows and black holes (BHs). Our simulations may be directly applicable to\nmodel systems with a large black hole-to-halo mass ratio, which are expected to\nbe found at very high redshifts.",
        "We show that the average trajectories of relativistic quantum particles in\nSchwarzschild spacetime, obtained via quantum mechanical weak measurements of\nmomentum and energy, are equivalent to the predicted flow lines of probability\ncurrent in curved spacetime quantum theory. We subsequently demonstrate that\nthese trajectories correspond exactly to classical null geodesics in a hybrid\nSchwarzschild-Alcubierre spacetime. This threefold equivalence demonstrates how\nquantum theory in curved spacetime can be formulated via operationally-defined\nmeasurements, and that such a theory may be interpreted deterministically, in\nthe spirit of hidden-variable models such as Bohmian mechanics, through the\nnovel connection to an underlying \"guiding metric.\"",
        "Physics-informed neural networks (PINNs) have earned high expectations in\nsolving partial differential equations (PDEs), but their optimization usually\nfaces thorny challenges due to the unique derivative-dependent loss function.\nBy analyzing the loss distribution, previous research observed the propagation\nfailure phenomenon of PINNs, intuitively described as the correct supervision\nfor model outputs cannot ``propagate'' from initial states or boundaries to the\ninterior domain. Going beyond intuitive understanding, this paper provides the\nfirst formal and in-depth study of propagation failure and its root cause.\nBased on a detailed comparison with classical finite element methods, we\nascribe the failure to the conventional single-point-processing architecture of\nPINNs and further prove that propagation failure is essentially caused by the\nlower gradient correlation of PINN models on nearby collocation points.\nCompared to superficial loss maps, this new perspective provides a more precise\nquantitative criterion to identify where and why PINN fails. The theoretical\nfinding also inspires us to present a new PINN architecture, named ProPINN,\nwhich can effectively unite the gradient of region points for better\npropagation. ProPINN can reliably resolve PINN failure modes and significantly\nsurpass advanced Transformer-based models with 46% relative promotion.",
        "Fact-checking tabular data is essential for ensuring the accuracy of\nstructured information. However, existing methods often rely on black-box\nmodels with opaque reasoning. We introduce RePanda, a structured fact\nverification approach that translates claims into executable pandas queries,\nenabling interpretable and verifiable reasoning.\n  To train RePanda, we construct PanTabFact, a structured dataset derived from\nthe TabFact train set, where claims are paired with executable queries\ngenerated using DeepSeek-Chat and refined through automated error correction.\nFine-tuning DeepSeek-coder-7B-instruct-v1.5 on PanTabFact, RePanda achieves\n84.09% accuracy on the TabFact test set.\n  To evaluate Out-of-Distribution (OOD) generalization, we interpret\nquestion-answer pairs from WikiTableQuestions as factual claims and refer to\nthis dataset as WikiFact. Without additional fine-tuning, RePanda achieves\n84.72% accuracy on WikiFact, significantly outperforming all other baselines\nand demonstrating strong OOD robustness. Notably, these results closely match\nthe zero-shot performance of DeepSeek-Chat (671B), indicating that our\nfine-tuning approach effectively distills structured reasoning from a much\nlarger model into a compact, locally executable 7B model.\n  Beyond fact verification, RePanda extends to tabular question answering by\ngenerating executable queries that retrieve precise answers. To support this,\nwe introduce PanWiki, a dataset mapping WikiTableQuestions to pandas queries.\nFine-tuning on PanWiki, RePanda achieves 75.1% accuracy in direct answer\nretrieval. These results highlight the effectiveness of structured\nexecution-based reasoning for tabular verification and question answering.\n  We have publicly released the dataset on Hugging Face at\ndatasets\/AtoosaChegini\/PanTabFact.",
        "Understanding collaborative writing dynamics between native speakers (NS) and\nnon-native speakers (NNS) is critical for enhancing collaboration quality and\nteam inclusivity. In this paper, we partnered with communication researchers to\ndevelop visual analytics solutions for comparing NS and NNS behaviors in 162\nwriting sessions across 27 teams. The primary challenges in analyzing writing\nbehaviors are data complexity and the uncertainties introduced by automated\nmethods. In response, we present \\textsc{COALA}, a novel visual analytics tool\nthat improves model interpretability by displaying uncertainties in author\nclusters, generating behavior summaries using large language models, and\nvisualizing writing-related actions at multiple granularities. We validated the\neffectiveness of \\textsc{COALA} through user studies with domain experts\n(N=2+2) and researchers with relevant experience (N=8). We present the insights\ndiscovered by participants using \\textsc{COALA}, suggest features for future\nAI-assisted collaborative writing tools, and discuss the broader implications\nfor analyzing collaborative processes beyond writing.",
        "We present a multifractal artificial terrain generation method that uses the\n3D Weierstrass-Mandelbrot function to control roughness. By varying the fractal\ndimension used in terrain generation across three different values, we generate\n60 unique off-road terrains. We use gradient maps to categorize the roughness\nof each terrain, consisting of low-, semi-, and high-roughness areas. To test\nhow the fractal dimension affects the difficulty of vehicle traversals, we\nmeasure the success rates, vertical accelerations, pitch and roll rates, and\ntraversal times of an autonomous ground vehicle traversing 20 randomized\nstraight-line paths in each terrain. As we increase the fractal dimension from\n2.3 to 2.45 and from 2.45 to 2.6, we find that the median area of low-roughness\nterrain decreases 13.8% and 7.16%, the median area of semi-rough terrain\nincreases 11.7% and 5.63%, and the median area of high-roughness terrain\nincreases 1.54% and 3.33%, all respectively. We find that the median success\nrate of the vehicle decreases 22.5% and 25% as the fractal dimension increases\nfrom 2.3 to 2.45 and from 2.45 to 2.6, respectively. Successful traversal\nresults show that the median root-mean-squared vertical accelerations, median\nroot-mean-squared pitch and roll rates, and median traversal times all increase\nwith the fractal dimension.",
        "In this paper, we propose a robust method for detecting guilty actors in\nimage steganography while effectively addressing the Cover Source Mismatch\n(CSM) problem, which arises when classifying images from one source using a\nclassifier trained on images from another source. Designed for an actor-based\nscenario, our method combines the use of Detection of Classifier\nInconsistencies (DCI) prediction with EfficientNet neural networks for feature\nextraction, and a Gradient Boosting Machine for the final classification. The\nproposed approach successfully determines whether an actor is innocent or\nguilty, or if they should be discarded due to excessive CSM. We show that the\nmethod remains reliable even in scenarios with high CSM, consistently achieving\naccuracy above 80% and outperforming the baseline method. This novel approach\ncontributes to the field of steganalysis by offering a practical and efficient\nsolution for handling CSM and detecting guilty actors in real-world\napplications.",
        "Accurate RF propagation modeling in urban environments is critical for\ndeveloping digital spectrum twins and optimizing wireless communication\nsystems. We introduce OpenGERT, an open-source automated Geometry Extraction\ntool for Ray Tracing, which collects and processes terrain and building data\nfrom OpenStreetMap, Microsoft Global ML Building Footprints, and USGS elevation\ndata. Using the Blender Python API, it creates detailed urban models for\nhigh-fidelity simulations with NVIDIA Sionna RT. We perform sensitivity\nanalyses to examine how variations in building height, position, and\nelectromagnetic material properties affect ray-tracing accuracy. Specifically,\nwe present pairwise dispersion plots of channel statistics (path gain, mean\nexcess delay, delay spread, link outage, and Rician K-factor) and investigate\nhow their sensitivities change with distance from transmitters. We also\nvisualize the variance of these statistics for selected transmitter locations\nto gain deeper insights. Our study covers Munich and Etoile scenes, each with\n10 transmitter locations. For each location, we apply five types of\nperturbations: material, position, height, height-position, and all combined,\nwith 50 perturbations each. Results show that small changes in permittivity and\nconductivity minimally affect channel statistics, whereas variations in\nbuilding height and position significantly alter all statistics, even with\nnoise standard deviations of 1 meter in height and 0.4 meters in position.\nThese findings highlight the importance of precise environmental modeling for\naccurate propagation predictions, essential for digital spectrum twins and\nadvanced communication networks. The code for geometry extraction and\nsensitivity analyses is available at github.com\/serhatadik\/OpenGERT\/.",
        "Objectives: Large language models (LLMs) can harness medical knowledge for\nintelligent question answering (Q&A), promising support for auxiliary diagnosis\nand medical talent cultivation. However, there is a deficiency of highly\nefficient retrieval-augmented generation (RAG) frameworks within the domain of\nTraditional Chinese Medicine (TCM). Our purpose is to observe the effect of the\nTree-Organized Self-Reflective Retrieval (TOSRR) framework on LLMs in TCM Q&A\ntasks.\n  Materials and Methods: We introduce the novel approach of knowledge\norganization, constructing a tree structure knowledge base with hierarchy. At\ninference time, our self-reflection framework retrieves from this knowledge\nbase, integrating information across chapters. Questions from the TCM Medical\nLicensing Examination (MLE) and the college Classics Course Exam (CCE) were\nrandomly selected as benchmark datasets.\n  Results: By coupling with GPT-4, the framework can improve the best\nperformance on the TCM MLE benchmark by 19.85% in absolute accuracy, and\nimprove recall accuracy from 27% to 38% on CCE datasets. In manual evaluation,\nthe framework improves a total of 18.52 points across dimensions of safety,\nconsistency, explainability, compliance, and coherence.\n  Conclusion: The TOSRR framework can effectively improve LLM's capability in\nQ&A tasks of TCM.",
        "We study the application of a neural network architecture for identifying\ncharged particle trajectories via unsupervised learning of delays and synaptic\nweights using a spike-time-dependent plasticity rule. In the considered model,\nthe neurons receive time-encoded information on the position of particle hits\nin a tracking detector for a particle collider, modeled according to the\ngeometry of the Compact Muon Solenoid Phase II detector. We show how a spiking\nneural network is capable of successfully identifying in a completely\nunsupervised way the signal left by charged particles in the presence of\nconspicuous noise from accidental or combinatorial hits. These results open the\nway to applications of neuromorphic computing to particle tracking, motivating\nfurther studies into its potential for real-time, low-power particle tracking\nin future high-energy physics experiments.",
        "Language models pretrained on text-only corpora often struggle with tasks\nthat require auditory commonsense knowledge. Previous work addresses this\nproblem by augmenting the language model to retrieve knowledge from external\naudio databases. This approach has several limitations, such as the potential\nlack of relevant audio in databases and the high costs associated with\nconstructing and querying the databases. To address these issues, we propose\nImagine to Hear, a novel approach that dynamically generates auditory knowledge\nusing generative models. Our framework detects multiple audio-related textual\nspans from the given prompt and generates corresponding auditory knowledge. We\ndevelop several mechanisms to efficiently process multiple auditory knowledge,\nincluding a CLAP-based rejection sampler and a language-audio fusion module.\nOur experiments show that our method achieves state-of-the-art performance on\nAuditoryBench without relying on external databases, highlighting the\neffectiveness of our generation-based approach.",
        "Diffusion models are a powerful framework for tackling ill-posed problems,\nwith recent advancements extending their use to point cloud upsampling. Despite\ntheir potential, existing diffusion models struggle with inefficiencies as they\nmap Gaussian noise to real point clouds, overlooking the geometric information\ninherent in sparse point clouds. To address these inefficiencies, we propose\nPUFM, a flow matching approach to directly map sparse point clouds to their\nhigh-fidelity dense counterparts. Our method first employs midpoint\ninterpolation to sparse point clouds, resolving the density mismatch between\nsparse and dense point clouds. Since point clouds are unordered\nrepresentations, we introduce a pre-alignment method based on Earth Mover's\nDistance (EMD) optimization to ensure coherent interpolation between sparse and\ndense point clouds, which enables a more stable learning path in flow matching.\nExperiments on synthetic datasets demonstrate that our method delivers superior\nupsampling quality but with fewer sampling steps. Further experiments on\nScanNet and KITTI also show that our approach generalizes well on RGB-D point\nclouds and LiDAR point clouds, making it more practical for real-world\napplications.",
        "In a proof of knowledge (PoK), a verifier becomes convinced that a prover\npossesses privileged information. In combination with zero-knowledge proof\nsystems, PoKs are an important part of secure protocols such as digital\nsignature schemes and authentication schemes as they enable a prover to\ndemonstrate possession of a certain piece of information (such as a private key\nor a credential), without revealing it. Formally, A PoK is defined via the\nexistence of an extractor, which is capable of reconstructing the key\ninformation that makes a verifier accept, given oracle access to the prover. We\nextend the concept of a PoK in the setting of a single classical verifier and\ntwo quantum provers, and exhibit the PoK property for a non-local game for the\nlocal Hamiltonian problem. More specifically, we construct an extractor which,\ngiven oracle access to a provers' strategy that leads to high acceptance\nprobability, is able to reconstruct the ground state of a local Hamiltonian.\nOur result can be seen as a new form of self-testing, where, in addition to\ncertifying a pre-shared entangled state and the prover's strategy, the verifier\nalso certifies a local quantum state. This technique thus provides a method to\nascertain that a prover has access to a quantum system, in particular, a ground\nstate, thus indicating a new level of verification for a proof of quantumness.",
        "In the transient stability analysis of renewable energy grid-tied systems,\nalthough a large amount of works have devoted to the detailed electromagnetic\ntransient simulation and the stability analyses of during-fault stage, the\nwhole low-voltage ride through (LVRT) process and relevant transient stability\nmechanism remain to be uncovered. Taking the doubly fed induction generator\nsystem as the objective, this paper divides the transient processes into four\ndifferent stages, including the pre-fault, during-fault, early post-fault, and\nlate post-fault ones, establishes the full mechanism models for each stage, and\nstudies the switching dynamics in detail. It is found that the during-fault\ndynamics can be determined by the phase-lock loop second-order equation within\nthe framework of the generalized swing equation (GSE). For the early post-fault\nstage, it can be treated as a series of quasi-steady states and its dominant\ndriving system dynamics can still be described by the GSE. Based on the local\ndynamics of unstable equilibrium point, the system transient stability can be\ncompletely determined by whether the initial state of the early post-fault\nstage is within or out of its basin of attraction (BOA). Based on these\nobservations, the BOA-based and equal area criterion (EAC)-based transient\nstability assessment methods are developed, which are supported by broad\nnumerical simulations and hardware-in-the-loop experiments. This work provides\na clear physical picture and perfectly solves the difficult stability analysis\nproblem when severe faults and LVRT have to be considered in most of DFIG\nengineering situations.",
        "Existing supervised action segmentation methods depend on the quality of\nframe-wise classification using attention mechanisms or temporal convolutions\nto capture temporal dependencies. Even boundary detection-based methods\nprimarily depend on the accuracy of an initial frame-wise classification, which\ncan overlook precise identification of segments and boundaries in case of\nlow-quality prediction. To address this problem, this paper proposes ASESM\n(Action Segmentation via Explicit Similarity Measurement) to enhance the\nsegmentation accuracy by incorporating explicit similarity evaluation across\nframes and predictions. Our supervised learning architecture uses frame-level\nmulti-resolution features as input to multiple Transformer encoders. The\nresulting multiple frame-wise predictions are used for similarity voting to\nobtain high quality initial prediction. We apply a newly proposed boundary\ncorrection algorithm that operates based on feature similarity between\nconsecutive frames to adjust the boundary locations iteratively through the\nlearning process. The corrected prediction is then further refined through\nmultiple stages of temporal convolutions. As post-processing, we optionally\napply boundary correction again followed by a segment smoothing method that\nremoves outlier classes within segments using similarity measurement between\nconsecutive predictions. Additionally, we propose a fully unsupervised boundary\ndetection-correction algorithm that identifies segment boundaries based solely\non feature similarity without any training. Experiments on 50Salads, GTEA, and\nBreakfast datasets show the effectiveness of both the supervised and\nunsupervised algorithms. Code and models are made available on Github.",
        "Variate tokenization, which independently embeds each variate as separate\ntokens, has achieved remarkable improvements in multivariate time series\nforecasting. However, employing self-attention with variate tokens incurs a\nquadratic computational cost with respect to the number of variates, thus\nlimiting its training efficiency for large-scale applications. To address this\nissue, we propose VarDrop, a simple yet efficient strategy that reduces the\ntoken usage by omitting redundant variate tokens during training. VarDrop\nadaptively excludes redundant tokens within a given batch, thereby reducing the\nnumber of tokens used for dot-product attention while preserving essential\ninformation. Specifically, we introduce k-dominant frequency hashing (k-DFH),\nwhich utilizes the ranked dominant frequencies in the frequency domain as a\nhash value to efficiently group variate tokens exhibiting similar periodic\nbehaviors. Then, only representative tokens in each group are sampled through\nstratified sampling. By performing sparse attention with these selected tokens,\nthe computational cost of scaled dot-product attention is significantly\nalleviated. Experiments conducted on public benchmark datasets demonstrate that\nVarDrop outperforms existing efficient baselines.",
        "The objective of this work is to align asynchronous subtitles in sign\nlanguage videos with limited labelled data. To achieve this goal, we propose a\nnovel framework with the following contributions: (1) we leverage fundamental\ngrammatical rules of British Sign Language (BSL) to pre-process the input\nsubtitles, (2) we design a selective alignment loss to optimise the model for\npredicting the temporal location of signs only when the queried sign actually\noccurs in a scene, and (3) we conduct self-training with refined pseudo-labels\nwhich are more accurate than the heuristic audio-aligned labels. From this, our\nmodel not only better understands the correlation between the text and the\nsigns, but also holds potential for application in the translation of sign\nlanguages, particularly in scenarios where manual labelling of large-scale sign\ndata is impractical or challenging. Extensive experimental results demonstrate\nthat our approach achieves state-of-the-art results, surpassing previous\nbaselines by substantial margins in terms of both frame-level accuracy and\nF1-score. This highlights the effectiveness and practicality of our framework\nin advancing the field of sign language video alignment and translation.",
        "Recently, there has been a significant advancement in designing\nSelf-Supervised Learning (SSL) frameworks for time series data to reduce the\ndependency on data labels. Among these works, hierarchical contrastive\nlearning-based SSL frameworks, which learn representations by contrasting data\nembeddings at multiple resolutions, have gained considerable attention. Due to\ntheir ability to gather more information, they exhibit better generalization in\nvarious downstream tasks. However, when the time series data length is\nsignificant long, the computational cost is often significantly higher than\nthat of other SSL frameworks. In this paper, to address this challenge, we\npropose an efficient way to train hierarchical contrastive learning models.\nInspired by the fact that each resolution's data embedding is highly dependent,\nwe introduce importance-aware resolution selection based training framework to\nreduce the computational cost. In the experiment, we demonstrate that the\nproposed method significantly improves training time while preserving the\noriginal model's integrity in extensive time series classification performance\nevaluations. Our code could be found here, https:\/\/github.com\/KEEBVIN\/IARS",
        "Counterfactual explanations have emerged as a prominent method in Explainable\nArtificial Intelligence (XAI), providing intuitive and actionable insights into\nMachine Learning model decisions. In contrast to other traditional feature\nattribution methods that assess the importance of input variables,\ncounterfactual explanations focus on identifying the minimal changes required\nto alter a model's prediction, offering a ``what-if'' analysis that is close to\nhuman reasoning. In the context of XAI, counterfactuals enhance transparency,\ntrustworthiness and fairness, offering explanations that are not just\ninterpretable but directly applicable in the decision-making processes.\n  In this paper, we present a novel framework that integrates perturbation\ntheory and statistical mechanics to generate minimal counterfactual\nexplanations in explainable AI. We employ a local Taylor expansion of a Machine\nLearning model's predictive function and reformulate the counterfactual search\nas an energy minimization problem over a complex landscape. In sequence, we\nmodel the probability of candidate perturbations leveraging the Boltzmann\ndistribution and use simulated annealing for iterative refinement. Our approach\nsystematically identifies the smallest modifications required to change a\nmodel's prediction while maintaining plausibility. Experimental results on\nbenchmark datasets for cybersecurity in Internet of Things environments,\ndemonstrate that our method provides actionable, interpretable counterfactuals\nand offers deeper insights into model sensitivity and decision boundaries in\nhigh-dimensional spaces."
      ]
    }
  },
  {
    "id":2411.00614,
    "research_type":"applied",
    "start_id":"b11",
    "start_title":"Causal identification of single-cell experimental perturbation effects with CINEMA-OT",
    "start_abstract":"Abstract Recent advancements in single-cell technologies allow characterization of experimental perturbations at resolution. While methods have been developed to analyze such experiments, the application a strict causal framework has not yet explored for inference treatment effects level. Here we present causal-inference-based approach perturbation analysis, termed CINEMA-OT (causal independent effect module attribution + optimal transport). separates confounding sources variation from obtain an transport matching that reflects counterfactual cell pairs. These pairs represent responses permitting number novel analyses, as individual treatment-effect response clustering, and synergy analysis. We benchmark on array estimation tasks several simulated real datasets show it outperforms other analysis methods. Finally, perform two newly generated datasets: (1) rhinovirus cigarette-smoke-exposed airway organoids, (2) combinatorial cytokine stimulation immune cells. In these reveals potential mechanisms by which cigarette-smoke exposure dulls antiviral response, well logic governs chemokine secretion peripheral recruitment.",
    "start_categories":[
      "math.OC"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Multimodal pooled Perturb-CITE-seq screens in patient models define mechanisms of cancer immune evasion"
      ],
      "abstract":[
        "Resistance to immune checkpoint inhibitors (ICIs) is a key challenge in cancer therapy. To elucidate underlying mechanisms, we developed Perturb-CITE-sequencing (Perturb-CITE-seq), enabling pooled clustered regularly interspaced short palindromic repeat (CRISPR)\u2013Cas9 perturbations with single-cell transcriptome and protein readouts. In patient-derived melanoma cells and autologous tumor-infiltrating lymphocyte (TIL) co-cultures, we profiled transcriptomes and 20\u2009proteins in ~218,000\u2009cells under ~750\u2009perturbations associated with cancer cell-intrinsic ICI resistance (ICR). We recover known mechanisms of resistance, including defects in the interferon-\u03b3 (IFN-\u03b3)\u2013JAK\/STAT and antigen-presentation pathways in RNA, protein and perturbation space, and new ones, including loss\/downregulation of CD58. Loss of CD58 conferred immune evasion in multiple co-culture models and was downregulated in tumors of melanoma patients with ICR. CD58 protein expression was not induced by IFN-\u03b3 signaling, and CD58 loss conferred immune evasion without compromising major histocompatibility complex (MHC) expression, suggesting that it acts orthogonally to known mechanisms of ICR. This work provides a framework for the deciphering of complex mechanisms by large-scale perturbation screens with multimodal, single-cell readouts, and discovers potentially clinically relevant mechanisms of immune evasion."
      ],
      "categories":[
        "q-bio.BM"
      ]
    },
    "list":{
      "title":[
        "Nuclear matter in relativistic Brueckner-Hartree-Fock theory with local\n  and nonlocal covariant chiral interactions at leading order",
        "Nucleolus Credit Assignment for Effective Coalitions in Multi-agent\n  Reinforcement Learning",
        "Mechanics and Design of Metastructured Auxetic Patches with Bio-inspired\n  Materials",
        "Rethinking High-speed Image Reconstruction Framework with Spike Camera",
        "iFlame: Interleaving Full and Linear Attention for Efficient Mesh\n  Generation",
        "Optimal disk packing of chloroplasts in plant cells",
        "Are the Majority of Public Computational Notebooks Pathologically\n  Non-Executable?",
        "Native Three-Body Interactions in a Superconducting Lattice Gauge\n  Quantum Simulator",
        "Test Schedule Generation for Acceptance Testing of Mission-Critical\n  Satellite Systems",
        "A novel approach to data generation in generative model",
        "Towards Location-Specific Precipitation Projections Using Deep Neural\n  Networks",
        "Rotatable Antenna Enabled Wireless Communication: Modeling and\n  Optimization",
        "Adapting Automatic Speech Recognition for Accented Air Traffic Control\n  Communications",
        "A spatially varying differential equation for multi-patch pandemic\n  propagation",
        "Quantification of Uncertainties in Probabilistic Deep Neural Network by\n  Implementing Boosting of Variational Inference",
        "Loop Quantum Gravitational Signatures via Love Numbers",
        "Genetic algorithm enhanced Solovay-Kitaev algorithm for quantum\n  compiling",
        "Boosting MCSat Modulo Nonlinear Integer Arithmetic via Local Search",
        "You Can't Eat Your Cake and Have It Too: The Performance Degradation of\n  LLMs with Jailbreak Defense",
        "A Differentiable Rank-Based Objective For Better Feature Learning",
        "A framework for IoT-Enabled Smart Agriculture",
        "ClassInvGen: Class Invariant Synthesis using Large Language Models",
        "Language Models Can See Better: Visual Contrastive Decoding For LLM\n  Multimodal Reasoning",
        "Quasi-one-dimensional Supersolids in Luther-Emery Liquids",
        "Filament Mass Losses Forced by Magnetic Reconnection in the Solar Corona",
        "$\\phi$-Decoding: Adaptive Foresight Sampling for Balanced Inference-Time\n  Exploration and Exploitation",
        "Bayesian optimization of beam injection and storage in the PSI muEDM\n  Experiment",
        "Financial Fraud Detection with Entropy Computing",
        "Euclid Quick Data Release (Q1). First Euclid statistical study of the\n  active galactic nuclei contribution fraction"
      ],
      "abstract":[
        "The simultaneous description for nuclear matter and finite nuclei has been a\nlong-standing challenge in nuclear ab initio theory. With the success for\nnuclear matter, the relativistic Brueckner-Hartree-Fock (RBHF) theory with\ncovariant chiral interactions is a promising ab initio approach to describe\nboth nuclear matter and finite nuclei. In the description of the finite nuclei\nwith the current RBHF theory, the covariant chiral interactions have to be\nlocalized to make calculations feasible. In order to examine the reliability\nand validity, in this letter, the RBHF theory with local and nonlocal covariant\nchiral interactions at leading order are applied for nuclear matter. The\nlow-energy constants in the covariant chiral interactions determined with the\nlocal regularization are close to those with the nonlocal regularization.\nMoreover, the RBHF theory with local and nonlocal covariant chiral interactions\nprovide equally well description of the saturation properties of nuclear\nmatter. The present work paves the way for the implementation of covariant\nchiral interactions in RBHF theory for finite nuclei.",
        "In cooperative multi-agent reinforcement learning (MARL), agents typically\nform a single grand coalition based on credit assignment to tackle a composite\ntask, often resulting in suboptimal performance. This paper proposed a\nnucleolus-based credit assignment grounded in cooperative game theory, enabling\nthe autonomous partitioning of agents into multiple small coalitions that can\neffectively identify and complete subtasks within a larger composite task.\nSpecifically, our designed nucleolus Q-learning could assign fair credits to\neach agent, and the nucleolus Q-operator provides theoretical guarantees with\ninterpretability for both learning convergence and the stability of the formed\nsmall coalitions. Through experiments on Predator-Prey and StarCraft scenarios\nacross varying difficulty levels, our approach demonstrated the emergence of\nmultiple effective coalitions during MARL training, leading to faster learning\nand superior performance in terms of win rate and cumulative rewards especially\nin hard and super-hard environments, compared to four baseline methods. Our\nnucleolus-based credit assignment showed the promise for complex composite\ntasks requiring effective subteams of agents.",
        "Metastructured auxetic patches, characterized by negative Poisson's ratios,\noffer unique mechanical properties that closely resemble the behavior of human\ntissues and organs. As a result, these patches have gained significant\nattention for their potential applications in organ repair and tissue\nregeneration. This study focuses on neural networks-based computational\nmodeling of auxetic patches with a sinusoidal metastructure fabricated from\nsilk fibroin, a bio-inspired material known for its biocompatibility and\nstrength. The primary objective of this research is to introduce a novel,\ndata-driven framework for patch design. To achieve this, we conducted\nexperimental fabrication and mechanical testing to determine material\nproperties and validate the corresponding finite element models. Finite element\nsimulations were then employed to generate the necessary data, while greedy\nsampling, an active learning technique, was utilized to reduce the\ncomputational cost associated with data labeling. Two neural networks were\ntrained to accurately predict Poisson's ratios and stresses for strains up to\n15\\%, respectively. Both models achieved $R^2$ scores exceeding 0.995, which\nindicates highly reliable predictions. Building on this, we developed a neural\nnetwork-based design model capable of tailoring patch designs to achieve\nspecific mechanical properties. This model demonstrated superior performance\nwhen compared to traditional optimization methods, such as genetic algorithms,\nby providing more efficient and precise design solutions. The proposed\nframework represents a significant advancement in the design of bio-inspired\nmetastructures for medical applications, paving the way for future innovations\nin tissue engineering and regenerative medicine.",
        "Spike cameras, as innovative neuromorphic devices, generate continuous spike\nstreams to capture high-speed scenes with lower bandwidth and higher dynamic\nrange than traditional RGB cameras. However, reconstructing high-quality images\nfrom the spike input under low-light conditions remains challenging.\nConventional learning-based methods often rely on the synthetic dataset as the\nsupervision for training. Still, these approaches falter when dealing with\nnoisy spikes fired under the low-light environment, leading to further\nperformance degradation in the real-world dataset. This phenomenon is primarily\ndue to inadequate noise modelling and the domain gap between synthetic and real\ndatasets, resulting in recovered images with unclear textures, excessive noise,\nand diminished brightness. To address these challenges, we introduce a novel\nspike-to-image reconstruction framework SpikeCLIP that goes beyond traditional\ntraining paradigms. Leveraging the CLIP model's powerful capability to align\ntext and images, we incorporate the textual description of the captured scene\nand unpaired high-quality datasets as the supervision. Our experiments on\nreal-world low-light datasets U-CALTECH and U-CIFAR demonstrate that SpikeCLIP\nsignificantly enhances texture details and the luminance balance of recovered\nimages. Furthermore, the reconstructed images are well-aligned with the broader\nvisual features needed for downstream tasks, ensuring more robust and versatile\nperformance in challenging environments.",
        "This paper propose iFlame, a novel transformer-based network architecture for\nmesh generation. While attention-based models have demonstrated remarkable\nperformance in mesh generation, their quadratic computational complexity limits\nscalability, particularly for high-resolution 3D data. Conversely, linear\nattention mechanisms offer lower computational costs but often struggle to\ncapture long-range dependencies, resulting in suboptimal outcomes. To address\nthis trade-off, we propose an interleaving autoregressive mesh generation\nframework that combines the efficiency of linear attention with the expressive\npower of full attention mechanisms. To further enhance efficiency and leverage\nthe inherent structure of mesh representations, we integrate this interleaving\napproach into an hourglass architecture, which significantly boosts efficiency.\nOur approach reduces training time while achieving performance comparable to\npure attention-based models. To improve inference efficiency, we implemented a\ncaching algorithm that almost doubles the speed and reduces the KV cache size\nby seven-eighths compared to the original Transformer. We evaluate our\nframework on ShapeNet and Objaverse, demonstrating its ability to generate\nhigh-quality 3D meshes efficiently. Our results indicate that the proposed\ninterleaving framework effectively balances computational efficiency and\ngenerative performance, making it a practical solution for mesh generation. The\ntraining takes only 2 days with 4 GPUs on 39k data with a maximum of 4k faces\non Objaverse.",
        "Photosynthesis is vital for the survival of entire ecosystems on Earth. While\nlight is fundamental to this process, excessive exposure can be detrimental to\nplant cells. Chloroplasts, the photosynthetic organelles, actively move in\nresponse to light and self-organize within the cell to tune light absorption.\nThese disk-shaped motile organelles must balance dense packing for enhanced\nlight absorption under dim conditions with spatial rearrangements to avoid\ndamage from excessive light exposure. Here, we reveal that the packing\ncharacteristics of chloroplasts within plant cells show signatures of\noptimality. Combining measurements of chloroplast densities and\nthree-dimensional cell shape in the water plant Elodea densa, we construct an\nargument for optimal cell shape versus chloroplast size to achieve two targets:\ndense packing into a two-dimensional monolayer for optimal absorption under dim\nlight conditions and packing at the sidewalls for optimal light avoidance. We\nformalize these constraints using a model for random close packing matched with\npacking simulations of polydisperse hard disks confined within rectangular\nboxes. The optimal cell shape resulting from these models corresponds closely\nto that measured in the box-like plant cells, highlighting the importance of\nparticle packing in the light adaptation of plants. Understanding the interplay\nbetween structure and function sheds light on how plants achieve efficient\nphoto adaptation. It also highlights a broader principle: how cell shape\nrelates to the optimization of packing finite and relatively small numbers of\norganelles under confinement. This universal challenge in biological systems\nshares fundamental features with the mechanics of confined granular media and\nthe jamming transitions in dense active and passive systems across various\nscales and contexts.",
        "Computational notebooks are the de facto platforms for exploratory data\nscience, offering an interactive programming environment where users can\ncreate, modify, and execute code cells in any sequence. However, this\nflexibility often introduces code quality issues, with prior studies showing\nthat approximately 76% of public notebooks are non-executable, raising\nsignificant concerns about reusability. We argue that the traditional notion of\nexecutability - requiring a notebook to run fully and without error - is overly\nrigid, misclassifying many notebooks and overestimating their\nnon-executability. This paper investigates pathological executability issues in\npublic notebooks under varying notions and degrees of executability. Even\npartially improving executability can improve code comprehension and offer a\npathway for dynamic analyses. With this insight, we first categorize notebooks\ninto potentially restorable and pathological non-executable notebooks and then\nmeasure how removing misconfiguration and superficial execution issues in\nnotebooks can improve their executability (i.e., additional cells executed\nwithout error). In a dataset of 42,546 popular public notebooks containing\n34,659 non-executable notebooks, only 21.3% are truly pathologically\nnon-executable. For restorable notebooks, LLM-based methods fully restore 5.4%\nof previously non-executable notebooks. Among the partially restored, the\nexecutability of notebooks improves by 42.7% and 28% by installing the correct\nmodules and generating synthetic data. These findings challenge prior\nassumptions, suggesting that notebooks have higher executability than\npreviously reported, many of which offer valuable partial execution, and that\ntheir executability should be evaluated within the interactive notebook\nparadigm rather than through traditional software executability standards.",
        "While universal quantum computers remain under development, analog quantum\nsimulators offer a powerful alternative for understanding complex systems in\ncondensed matter, chemistry, and high-energy physics. One compelling\napplication is the characterization of real-time lattice gauge theories (LGTs).\nLGTs are nonperturbative tools, utilizing discretized spacetime to describe\ngauge-invariant models. They hold immense potential for understanding\nfundamental physics but require enforcing local constraints analogous to\nelectromagnetism's Gauss's Law. These constraints, which arise from gauge\nsymmetries and dictate the form of the interaction between matter and gauge\nfields, are a significant challenge for simulators to enforce. Implementing\nthese constraints at the hardware level in analog simulations is crucial. This\nrequires realizing multibody interactions between matter and gauge-field\nelements, enabling them to evolve together while suppressing unwanted two-body\ninteractions that violate the gauge symmetry. In this paper, we propose and\nimplement a novel parametrically activated three-qubit interaction within a\ncircuit quantum electrodynamics architecture. We experimentally demonstrate a\nminimal $U(1)$ spin-1\/2 model with a time evolution that intrinsically\nsatisfies Gauss's law in the system. This design serves as the foundational\nblock for simulating LGTs on a superconducting photonic lattice.",
        "Mission-critical system, such as satellite systems, healthcare systems, and\nnuclear power plant control systems, undergo rigorous testing to ensure they\nmeet specific operational requirements throughout their operation. This\nincludes Operational Acceptance Testing (OAT), which aims to ensure that the\nsystem functions correctly under real-world operational conditions. In\nsatellite development, In-Orbit Testing (IOT) is a crucial OAT activity\nperformed regularly and as needed after deployment in orbit to check the\nsatellite's performance and ensure that operational requirements are met. The\nscheduling of an IOT campaign, which executes multiple IOT procedures, is an\nimportant yet challenging problem, as it accounts for various factors,\nincluding satellite visibility, antenna usage costs, testing time periods, and\noperational constraints. To address the IOT scheduling problem, we propose a\nmulti-objective approach to generate near-optimal IOT schedules, accounting for\noperational costs, fragmentation (i.e., the splitting of tests), and resource\nefficiency, which align with practitioners' objectives for IOT scheduling. Our\nindustrial case study with SES Techcom shows significant improvements, as\nfollows: an average improvement of 49.4% in the cost objective, 60.4% in the\nfragmentation objective, and 30% in the resource usage objective, compared to\nour baselines. Additionally, our approach improves cost efficiency by 538% and\nresource usage efficiency by 39.42% compared to manually constructed schedules\nprovided by practitioners, while requiring only 12.5% of the time needed for\nmanual IOT scheduling.",
        "Variational Autoencoders (VAEs) and other generative models are widely\nemployed in artificial intelligence to synthesize new data. However, current\napproaches rely on Euclidean geometric assumptions and statistical\napproximations that fail to capture the structured and emergent nature of data\ngeneration. This paper introduces the Convergent Fusion Paradigm (CFP) theory,\na novel geometric framework that redefines data generation by integrating\ndimensional expansion accompanied by qualitative transformation. By modifying\nthe latent space geometry to interact with emergent high-dimensional\nstructures, CFP theory addresses key challenges such as identifiability issues\nand unintended artifacts like hallucinations in Large Language Models (LLMs).\nCFP theory is based on two key conceptual hypotheses that redefine how\ngenerative models structure relationships between data and algorithms. Through\nthe lens of CFP theory, we critically examine existing metric-learning\napproaches. CFP theory advances this perspective by introducing time-reversed\nmetric embeddings and structural convergence mechanisms, leading to a novel\ngeometric approach that better accounts for data generation as a structured\nepistemic process. Beyond its computational implications, CFP theory provides\nphilosophical insights into the ontological underpinnings of data generation.\nBy offering a systematic framework for high-dimensional learning dynamics, CFP\ntheory contributes to establishing a theoretical foundation for understanding\nthe data-relationship structures in AI. Finally, future research in CFP theory\nwill be led to its implications for fully realizing qualitative\ntransformations, introducing the potential of Hilbert space in generative\nmodeling.",
        "Accurate precipitation estimates at individual locations are crucial for\nweather forecasting and spatial analysis. This study presents a paradigm shift\nby leveraging Deep Neural Networks (DNNs) to surpass traditional methods like\nKriging for station-specific precipitation approximation. We propose two\ninnovative NN architectures: one utilizing precipitation, elevation, and\nlocation, and another incorporating additional meteorological parameters like\nhumidity, temperature, and wind speed. Trained on a vast dataset (1980-2019),\nthese models outperform Kriging across various evaluation metrics (correlation\ncoefficient, root mean square error, bias, and skill score) on a five-year\nvalidation set. This compelling evidence demonstrates the transformative power\nof deep learning for spatial prediction, offering a robust and precise\nalternative for station-specific precipitation estimation.",
        "Fluid antenna system (FAS) and movable antenna (MA) have recently emerged as\npromising technologies to exploit new spatial degrees of freedom (DoFs), which\nhave attracted growing attention in wireless communication. In this paper, we\npropose a new rotatable antenna (RA) model to improve the performance of\nwireless communication systems. Different from conventional fixed antennas, the\nproposed RA system can flexibly alter the three-dimensional (3D) boresight\ndirection of each antenna independently by adjusting its deflection angles to\nachieve a desired array directional gain pattern. Specifically, we investigate\nan RA-enabled uplink communication system, where the receive beamforming and\nthe deflection angles of all RAs at the base station (BS) are jointly optimized\nto maximize the minimum signal-to-interference-plus-noise ratio (SINR) among\nall the users. In the special single-user and free-space propagation setup, the\noptimal deflection angles of RAs are derived in closed form with the\nmaximum-ratio combining (MRC) beamformer applied at the BS. Moreover, we\nanalyze the asymptotic performance with an infinite number of antennas based on\nthis solution, which theoretically proves that the RA system can achieve a\nhigher array gain as compared to the fixed-antenna system. In the general\nmulti-user and multi-path channel setup, we first propose an alternating\noptimization (AO) algorithm to alternately optimize the receive beamforming and\nthe deflection angles of RAs in an iterative manner. Then, a two-stage\nalgorithm that solves the formulated problem without the need for iteration is\nfurther proposed to reduce computational complexity. Simulation results are\nprovided to validate our analytical results and demonstrate that the proposed\nRA system can significantly outperform other benchmark schemes.",
        "Effective communication in Air Traffic Control (ATC) is critical to\nmaintaining aviation safety, yet the challenges posed by accented English\nremain largely unaddressed in Automatic Speech Recognition (ASR) systems.\nExisting models struggle with transcription accuracy for Southeast\nAsian-accented (SEA-accented) speech, particularly in noisy ATC environments.\nThis study presents the development of ASR models fine-tuned specifically for\nSoutheast Asian accents using a newly created dataset. Our research achieves\nsignificant improvements, achieving a Word Error Rate (WER) of 0.0982 or 9.82%\non SEA-accented ATC speech. Additionally, the paper highlights the importance\nof region-specific datasets and accent-focused training, offering a pathway for\ndeploying ASR systems in resource-constrained military operations. The findings\nemphasize the need for noise-robust training techniques and region-specific\ndatasets to improve transcription accuracy for non-Western accents in ATC\ncommunications.",
        "We develop an extension of the Susceptible-Infected-Recovery (SIR) model to\naccount for spatial variations in population as well as infection and recovery\nparameters. The equations are derived by taking the continuum limit of discrete\ninteracting patches, and results in a diffusion equation with some nonlinear\nterms. The resulting population dynamics can be reinterpreted as a nonlinear\nheat flow equation where the temperature vector captures both infected and\nrecovered populations across multiple patches.",
        "Modern neural network architectures have achieved remarkable accuracies but\nremain highly dependent on their training data, often lacking interpretability\nin their learned mappings. While effective on large datasets, they tend to\noverfit on smaller ones. Probabilistic neural networks, such as those utilizing\nvariational inference, address this limitation by incorporating uncertainty\nestimation through weight distributions rather than point estimates. However,\nstandard variational inference often relies on a single-density approximation,\nwhich can lead to poor posterior estimates and hinder model performance. We\npropose Boosted Bayesian Neural Networks (BBNN), a novel approach that enhances\nneural network weight distribution approximations using Boosting Variational\nInference (BVI). By iteratively constructing a mixture of densities, BVI\nexpands the approximating family, enabling a more expressive posterior that\nleads to improved generalization and uncertainty estimation. While this\napproach increases computational complexity, it significantly enhances accuracy\nan essential tradeoff, particularly in high-stakes applications such as medical\ndiagnostics, where false negatives can have severe consequences. Our\nexperimental results demonstrate that BBNN achieves ~5% higher accuracy\ncompared to conventional neural networks while providing superior uncertainty\nquantification. This improvement highlights the effectiveness of leveraging a\nmixture-based variational family to better approximate the posterior\ndistribution, ultimately advancing probabilistic deep learning.",
        "Loop quantum gravitational effects can resolve the central singularity of\nblack holes while potentially leaving tiny traces of quantization in the\nexterior spacetime. We show the way these residues can, in principle, be\nexplored using tidal Love numbers (TLNs). We consider loop quantized\nSchwarzschild black hole, in particular the Ashtekar-Olmedo-Singh (AOS) model,\nand study the static response to external tidal fields of spin zero (scalar\nfield), spin one (vector field), and spin two (axial gravitational field)\ntypes. We find that, in contrast to the classical theory, where TLNs vanish,\nthey are non-vanishing and negative for all three responses and for all\nmultipoles. Besides, the magnitude of TLNs decreases as the black hole mass\nincreases, and TLNs, in response to the axial gravitational field, have the\nlargest magnitude among these three responses. Our results show that for black\nholes of mass $M \\gtrsim 4.3 \\times 10^{4} M_{\\textrm{Pl}}$, the AOS model is\nconsistent with current and next-generation detection limits for TLNs. Our\nfindings suggest that the quantum deformability of loop quantum black holes,\narising from the inherent fuzziness of spacetime geometry, reveals a\nfundamentally distinct internal structure compared to their classical\ncounterparts. This unique feature manifests as quantum hair, which, in\nprinciple, can be detected by future observations.",
        "Quantum compiling trying to approximate the target qubit gate by finding an\noptimal sequence (braid word) of basic braid operations is a fundamental\nproblem in quantum computing. We develop a genetic algorithm (GA) enhanced\nSolovay-Kitaev algorithm (SKA) to approximate single qubit gates with four\nbasic braid matrices of Fibonacci anyons. The GA-enhanced SKA demonstrates that\nthe algorithm performs strongly and can easily find the ideal braid word from\nan exponentially large space. The resulting precision of the approximate\nsingle-qubit quantum gate is superior to that of the Monte Carlo (MC) enhanced\nSKA, as well as comparable to that of the deep reinforcement learning (RL) for\nthe length of braid word greater than 25. The 2(3)-order approximation of\nGA-enhanced SKA for basic braiding length l0=50(30) leads to an optimal braid\nword at a distance of 5.9*10-7, which is sufficient for most cases of quantum\ncomputing. Our work provides an alternative approach to solving and optimizing\nquantum compilation of non-Abelian anyon quantum gates and is useful for\nrealizing topological quantum computation in the future.",
        "The Model Constructing Satisfiability (MCSat) approach to the SMT problem\nextends the ideas of CDCL from the SAT level to the theory level. Like SAT, its\nsearch is driven by incrementally constructing a model by assigning concrete\nvalues to theory variables and performing theory-level reasoning to learn\nlemmas when conflicts arise. Therefore, the selection of values can\nsignificantly impact the search process and the solver's performance. In this\nwork, we propose guiding the MCSat search by utilizing assignment values\ndiscovered through local search. First, we present a theory-agnostic framework\nto seamlessly integrate local search techniques within the MCSat framework.\nThen, we highlight how to use the framework to design a search procedure for\n(quantifier-free) Nonlinear Integer Arithmetic (NIA), utilizing accelerated\nhill-climbing and a new operation called feasible-sets jumping. We implement\nthe proposed approach in the MCSat engine of the Yices2 solver, and empirically\nevaluate its performance over the N IA benchmarks of SMT-LIB.",
        "With the rise of generative large language models (LLMs) like LLaMA and\nChatGPT, these models have significantly transformed daily life and work by\nproviding advanced insights. However, as jailbreak attacks continue to\ncircumvent built-in safety mechanisms, exploiting carefully crafted scenarios\nor tokens, the safety risks of LLMs have come into focus. While numerous\ndefense strategies--such as prompt detection, modification, and model\nfine-tuning--have been proposed to counter these attacks, a critical question\narises: do these defenses compromise the utility and usability of LLMs for\nlegitimate users? Existing research predominantly focuses on the effectiveness\nof defense strategies without thoroughly examining their impact on performance,\nleaving a gap in understanding the trade-offs between LLM safety and\nperformance. Our research addresses this gap by conducting a comprehensive\nstudy on the utility degradation, safety elevation, and exaggerated-safety\nescalation of LLMs with jailbreak defense strategies. We propose USEBench, a\nnovel benchmark designed to evaluate these aspects, along with USEIndex, a\ncomprehensive metric for assessing overall model performance. Through\nexperiments on seven state-of-the-art LLMs, we found that mainstream jailbreak\ndefenses fail to ensure both safety and performance simultaneously. Although\nmodel-finetuning performs the best overall, their effectiveness varies across\nLLMs. Furthermore, vertical comparisons reveal that developers commonly\nprioritize performance over safety when iterating or fine-tuning their LLMs.",
        "In this paper, we leverage existing statistical methods to better understand\nfeature learning from data. We tackle this by modifying the model-free variable\nselection method, Feature Ordering by Conditional Independence (FOCI), which is\nintroduced in \\cite{azadkia2021simple}. While FOCI is based on a non-parametric\ncoefficient of conditional dependence, we introduce its parametric,\ndifferentiable approximation. With this approximate coefficient of correlation,\nwe present a new algorithm called difFOCI, which is applicable to a wider range\nof machine learning problems thanks to its differentiable nature and learnable\nparameters. We present difFOCI in three contexts: (1) as a variable selection\nmethod with baseline comparisons to FOCI, (2) as a trainable model parametrized\nwith a neural network, and (3) as a generic, widely applicable neural network\nregularizer, one that improves feature learning with better management of\nspurious correlations. We evaluate difFOCI on increasingly complex problems\nranging from basic variable selection in toy examples to saliency map\ncomparisons in convolutional networks. We then show how difFOCI can be\nincorporated in the context of fairness to facilitate classifications without\nrelying on sensitive data.",
        "Unpredictable weather patterns and a lack of timely, accurate information\nsignificantly challenge farmers in Uganda, leading to poor crop management,\nreduced yields, and heightened vulnerability to environmental stress. This\nresearch presents a framework for IoT-enabled smart agriculture, leveraging\nRaspberry Pi-based technology to provide real-time monitoring of weather and\nenvironmental conditions. The framework integrates sensors for temperature,\nrainfall, soil moisture, and pressure, connected via an MCP3208\nanalog-to-digital converter. Data is displayed on an LCD for immediate feedback\nand transmitted to the ThingSpeak platform for centralized storage, analysis,\nand remote access through a mobile app or web interface. Farmers can leverage\nthis framework to optimize irrigation schedules and improve crop productivity\nthrough actionable insights derived from real-time and forecasted data on\nrainfall, temperature, pressure and soil moisture. Additionally, the system\nincorporates predictive weather forecasting to dynamically control sensor\nactivity, reducing energy consumption and extending sensor lifespan. Simulated\nusing Proteus, the proposed framework demonstrates significant potential to\nmitigate the impacts of unpredictable weather by reducing water consumption,\nimproving forecasting accuracy, and boosting productivity.",
        "Formal program specifications in the form of preconditions, postconditions,\nand class invariants have several benefits for the construction and maintenance\nof programs. They not only aid in program understanding due to their\nunambiguous semantics but can also be enforced dynamically (or even statically\nwhen the language supports a formal verifier). However, synthesizing\nhigh-quality specifications in an underlying programming language is limited by\nthe expressivity of the specifications or the need to express them in a\ndeclarative manner. Prior work has demonstrated the potential of large language\nmodels (LLMs) for synthesizing high-quality method pre\/postconditions for\nPython and Java, but does not consider class invariants.\n  In this work, we describe ClassInvGen, a method for co-generating executable\nclass invariants and test inputs to produce high-quality class invariants for a\nmainstream language such as C++, leveraging LLMs' ability to synthesize pure\nfunctions. We show that ClassInvGen outperforms a pure LLM-based technique to\ngenerate specifications (from code) as well as prior data-driven invariant\ninference techniques such as Daikon. We contribute a benchmark of standard C++\ndata structures along with a harness that can help measure both the correctness\nand completeness of generated specifications using tests and mutants. We also\ndemonstrate its applicability to real-world code by performing a case study on\nseveral classes within a widely used and high-integrity C++ codebase.",
        "Although Large Language Models (LLMs) excel in reasoning and generation for\nlanguage tasks, they are not specifically designed for multimodal challenges.\nTraining Multimodal Large Language Models (MLLMs), however, is\nresource-intensive and constrained by various training limitations. In this\npaper, we propose the Modular-based Visual Contrastive Decoding (MVCD)\nframework to move this obstacle. Our framework leverages LLMs' In-Context\nLearning (ICL) capability and the proposed visual contrastive-example decoding\n(CED), specifically tailored for this framework, without requiring any\nadditional training. By converting visual signals into text and focusing on\ncontrastive output distributions during decoding, we can highlight the new\ninformation introduced by contextual examples, explore their connections, and\navoid over-reliance on prior encoded knowledge. MVCD enhances LLMs' visual\nperception to make it see and reason over the input visuals. To demonstrate\nMVCD's effectiveness, we conduct experiments with four LLMs across five\nquestion answering datasets. Our results not only show consistent improvement\nin model accuracy but well explain the effective components inside our decoding\nstrategy. Our code will be available at https:\/\/github.com\/Pbhgit\/MVCD.",
        "The supersolid is a long-sought phase in condensed matter physics,\ncharacterized by the coexistence of density wave and superfluid orders. This\nphase is counterintuitive, as different symmetry-breaking orders typically\ncompete with one another. A deeper understanding of how such a state forms in\ncondensed matter systems remains an open question, especially in\nquasi-one-dimensional correlated systems. In this work, we investigate the\nemergence of supersolids in Luttinger-Emery liquids using a variational method.\nAs the system consists of coupled Luttinger-Emery liquid chains, we refer to\nthis phase as a quasi-one-dimensional supersolid. Notably, we demonstrate that\nthe quasi-one-dimensional supersolid phase is energetically favorable in chains\nwith finite size or short-range order. Furthermore, we investigate the\ncollective dynamics of these coexisting charge density waves and\nsuperconducting states, identifying a quasi-Goldstone mode. Our theory provides\nvaluable insights into both the ground state and the dynamic properties of\nsupersolids in strongly correlated systems.",
        "Recent observations of the solar atmosphere in cool extreme ultraviolet (EUV)\nlines have reported the prevalence of coronal rain falling from coronal cloud\nfilaments that are associated with the magnetic dips of coronal X-point\nstructures. These filaments mysteriously appear as clouds of mass in the corona\nthat subsequently shrink and disappear due to mass losses that drain as coronal\nrain along arced field lines. Using a two and a half dimensional,\nmagnetohydrodynamic model, we investigated evaporation-condensation as the\nformation mechanism of the subset of coronal cloud filaments that form above\ncoronal X-points. Our simulation included the effects of field-aligned thermal\nconduction and optically thin radiation and used the state-of-the-art\nTransition Region Adaptive Conduction (TRAC) method to model the formation,\nmaintenance, and mass loss of a filament above a coronal X-point. This paper\npresents a physical model that demonstrates magnetic reconnection as a filament\nloss mechanism, producing hybrid filament\/coronal rain via mass losses through\nthe X-point. A detailed analysis of how the mass of the filament forces the\nfield to reconnect is also presented, revealing three phases that characterize\nthe evolution of the reconnecting current sheet and associated mass losses. We\nconclude that the formation of certain coronal cloud filaments and subsequent\nmass losses via coronal rain can be explained by the evaporation-condensation\nmodel combined with filament mass losses forced by magnetic reconnection. We\nalso report that rebound shocks generated by the impact of coronal rain\ncondensations on the chromosphere together with retractive upflows can cause\nupward propagating condensations to form through a dynamic thermal runaway\nprocess.",
        "Inference-time optimization scales computation to derive deliberate reasoning\nsteps for effective performance. While previous search-based strategies address\nthe short-sightedness of auto-regressive generation, the vast search space\nleads to excessive exploration and insufficient exploitation. To strike an\nefficient balance to derive the optimal step, we frame the decoding strategy as\nforesight sampling, leveraging simulated future steps to obtain globally\noptimal step estimation. Built on it, we propose a novel decoding strategy,\nnamed $\\phi$-Decoding. To provide a precise and expressive estimation of step\nvalue, $\\phi$-Decoding approximates two distributions via foresight and\nclustering. Sampling from the joint distribution, the optimal steps can be\nselected for exploitation. To support adaptive computation allocation, we\npropose in-width and in-depth pruning strategies, featuring a light-weight\nsolution to achieve inference efficiency. Extensive experiments across seven\nbenchmarks show $\\phi$-Decoding outperforms strong baselines in both\nperformance and efficiency. Additional analysis demonstrates its generalization\nacross various LLMs and scalability across a wide range of computing budgets.\nThe code will be released at https:\/\/github.com\/xufangzhi\/phi-Decoding, and the\nopen-source PyPI package is coming soon.",
        "The muEDM experiment at the Paul Scherrer Institute aims to measure the\nelectric dipole moment with an unprecedented sensitivity of $6 \\times\n10^{-23}\\,\\mathrm{e}\\cdot\\mathrm{cm}$. A key aspect of this experiment is the\ninjection and storage of the muon beam, which traverses a long, narrow\nsuperconducting channel before entering a solenoid magnet. The muon is then\nkicked by a pulsed magnetic field into a stable orbit within the solenoid's\ncentral region, where the electric dipole moment is measured. To study the beam\ninjection and storage process, we developed a G4beamline simulation to model\nthe dynamics of beam injection and storage, incorporating all relevant electric\nand magnetic fields. We subsequently employed a Bayesian optimization technique\nto improve the muon storage efficiency for Phase I of the muEDM experiment. The\noptimization is demonstrated using data simulated by G4beamline. We have\nobserved an enhancement in the beam injection and storage efficiency, which\nincreased to 0.556\\% through the utilization of Bayesian optimization with\nGaussian processes, compared to 0.324\\% when employing the polynomial chaos\nexpansion. This approach can be applied to adjust actual experimental\nparameters, aiding in achieving the desired performance for beam injection and\nstorage in the muEDM experiment.",
        "We introduce CVQBoost, a novel classification algorithm that leverages early\nhardware implementing Quantum Computing Inc's Entropy Quantum Computing (EQC)\nparadigm, Dirac-3 [Nguyen et. al. arXiv:2407.04512]. We apply CVQBoost to a\nfraud detection test case and benchmark its performance against XGBoost, a\nwidely utilized ML method. Running on Dirac-3, CVQBoost demonstrates a\nsignificant runtime advantage over XGBoost, which we evaluate on\nhigh-performance hardware comprising up to 48 CPUs and four NVIDIA L4 GPUs\nusing the RAPIDS AI framework. Our results show that CVQBoost maintains\ncompetitive accuracy (measured by AUC) while significantly reducing training\ntime, particularly as dataset size and feature complexity increase. To assess\nscalability, we extend our study to large synthetic datasets ranging from 1M to\n70M samples, demonstrating that CVQBoost on Dirac-3 is well-suited for\nlarge-scale classification tasks. These findings position CVQBoost as a\npromising alternative to gradient boosting methods, offering superior\nscalability and efficiency for high-dimensional ML applications such as fraud\ndetection.",
        "Active galactic nuclei (AGN) play a key role in galaxy evolution but are\nchallenging to identify due to their varied observational signatures.\nFurthermore, understanding their impact requires quantifying their strength\nrelative to their host galaxies. We developed a deep learning (DL) model for\nidentifying AGN in imaging data by deriving the contribution of the central\npoint source. Trained on Euclidised mock galaxy images with injected AGN\nlevels, in the form of varying contributions of the point-spread function\n(PSF), our model can precisely and accurately recover the injected AGN\ncontribution fraction $f_{\\rm PSF}$, with a mean difference between the\npredicted and true $f_{\\rm PSF}$ of $-0.0078$ and an overall root mean square\nerror (RMSE) of 0.051. This method moves beyond binary AGN classification,\nenabling precise AGN contribution measurements. Applying our model to a\nstellar-mass-limited sample ($M_{\\ast} \\ge 10^{9.8} M_{\\odot}$, $0.5 \\le z \\le\n2.0$) from the first \\Euclid quick data release (Q1), we identify $48,840 \\pm\n78$ AGN over 63.1 deg$^2$ ($7.8\\pm0.1$%) using a threshold of $f_{\\rm PSF} >\n0.2$. We compare our DL-selected AGN with those identified in X-ray,\nmid-infrared (MIR), and optical spectroscopy and investigate their overlapping\nfractions depending on different thresholds on the PSF contribution. We find\nthat the overlap increases with increasing X-ray or bolometric AGN luminosity.\nThe AGN luminosity in the $I_{\\rm E}$ filter correlates with host galaxy\nstellar mass, suggesting faster supermassive black hole (SMBH) growth in more\nmassive galaxies. Moreover, the mean relative contribution of the AGN is higher\nin quiescent galaxies than in star-forming ones. Starburst galaxies and the\nmost massive galaxies (across the star-formation main sequence) tend to host\nthe most luminous AGN, indicating concomitant assembly of the SMBH and the host\ngalaxy."
      ]
    }
  },
  {
    "id":2411.00714,
    "research_type":"basic",
    "start_id":"b0",
    "start_title":"Universality, criticality and complexity of information propagation in social media",
    "start_abstract":"Abstract Statistical laws of information avalanches in social media appear, at least according to existing empirical studies, not robust across systems. As a consequence, radically different processes may represent plausible driving mechanisms for propagation. Here, we analyze almost one billion time-stamped events collected from several online platforms \u2013 including Telegram, Twitter and Weibo over observation windows longer than ten years, show that the propagation is universal critical process. Universality arises identical macroscopic patterns platforms, irrespective details specific system hand. Critical behavior deduced power-law distributions, corresponding hyperscaling relations, characterizing size duration information. testing on our data indicates mixture simple complex contagion characterizes media. Data suggest complexity process correlated with semantic content propagated.",
    "start_categories":[
      "Human Behaviors"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b14"
      ],
      "title":[
        "Random graphs with arbitrary degree distributions and their applications"
      ],
      "abstract":[
        "Recent work on the structure of social networks and internet has focused attention graphs with distributions vertex degree that are significantly different from Poisson have been widely studied in past. In this paper we develop detail theory random arbitrary distributions. addition to simple undirected, unipartite graphs, examine properties directed bipartite graphs. Among other results, derive exact expressions for position phase transition at which a giant component first forms, mean size, size if there is one, number vertices certain distance away randomly chosen vertex, average vertex-vertex within graph. We apply our some real-world including world-wide web collaboration scientists Fortune 1000 company directors. demonstrate cases appropriate predict surprising accuracy behavior real world, while others measurable discrepancy between reality, perhaps indicating presence additional network not captured by"
      ],
      "categories":[
        "Modeling"
      ]
    },
    "list":{
      "title":[
        "GiantHunter: Accurate detection of giant virus in metagenomic data using\n  reinforcement-learning and Monte Carlo tree search",
        "Monetary-Fiscal Interaction and the Liquidity of Government Debt",
        "Efficient Long Speech Sequence Modelling for Time-Domain Depression\n  Level Estimation",
        "Revisiting Near-Far Field Boundary in Dual-Polarized XL-MIMO Systems",
        "Model Fusion via Neuron Transplantation",
        "Semantic Neural Radiance Fields for Multi-Date Satellite Data",
        "Exploring non-supersymmetric black holes with multiple bubbles in\n  five-dimensional minimal supergravity",
        "Computation of Magnetohydrodynamic Equilibria with Voigt Regularization",
        "MFSR-GAN: Multi-Frame Super-Resolution with Handheld Motion Modeling",
        "Enhanced Hybrid Deep Learning Approach for Botnet Attacks Detection in\n  IoT Environment",
        "Adopting Whisper for Confidence Estimation",
        "Rigidity of Higson coronas",
        "Photo-induced Dynamics and Momentum Distribution of Chiral Charge\n  Density Waves in 1T-TiSe$_{2}$",
        "Lyapunov exponents as probes for phase transitions of Kerr-AdS black\n  holes",
        "The Geometry of Optimal Gait Families for Steering Kinematic Locomoting\n  Systems",
        "Computation of whispering gallery modes for spherical symmetric\n  heterogeneous Helmholtz problems with piecewise smooth refractive index",
        "ResMoE: Space-efficient Compression of Mixture of Experts LLMs via\n  Residual Restoration",
        "A novel multi-agent dynamic portfolio optimization learning system based\n  on hierarchical deep reinforcement learning",
        "The number of smooth varieties in an MMP on a 3-fold of Fano type",
        "Using Read Promotion and Mixed Isolation Levels for Performant Yet\n  Serializable Execution of Transaction Programs",
        "Learning to Identify Conflicts in RPKI",
        "Long-Term Planning Around Humans in Domestic Environments with 3D Scene\n  Graphs",
        "Towards a definition of a meteor cluster: Detection of meteor clusters\n  from meteor orbit databases",
        "Optimizing Decomposition for Optimal Claim Verification",
        "Structural Embedding Projection for Contextual Large Language Model\n  Inference",
        "Toward Integrated Solutions: A Systematic Interdisciplinary Review of\n  Cybergrooming Research",
        "BERT-based model for Vietnamese Fact Verification Dataset",
        "xJailbreak: Representation Space Guided Reinforcement Learning for\n  Interpretable LLM Jailbreaking",
        "Are Cognitive Biases as Important as they Seem for Data Visualization?"
      ],
      "abstract":[
        "Motivation: Nucleocytoplasmic large DNA viruses (NCLDVs) are notable for\ntheir large genomes and extensive gene repertoires, which contribute to their\nwidespread environmental presence and critical roles in processes such as host\nmetabolic reprogramming and nutrient cycling. Metagenomic sequencing has\nemerged as a powerful tool for uncovering novel NCLDVs in environmental\nsamples. However, identifying NCLDV sequences in metagenomic data remains\nchallenging due to their high genomic diversity, limited reference genomes, and\nshared regions with other microbes. Existing alignment-based and machine\nlearning methods struggle with achieving optimal trade-offs between sensitivity\nand precision. Results: In this work, we present GiantHunter, a reinforcement\nlearning-based tool for identifying NCLDVs from metagenomic data. By employing\na Monte Carlo tree search strategy, GiantHunter dynamically selects\nrepresentative non-NCLDV sequences as the negative training data, enabling the\nmodel to establish a robust decision boundary. Benchmarking on rigorously\ndesigned experiments shows that GiantHunter achieves high precision while\nmaintaining competitive sensitivity, improving the F1-score by 10% and reducing\ncomputational cost by 90% compared to the second-best method. To demonstrate\nits real-world utility, we applied GiantHunter to 60 metagenomic datasets\ncollected from six cities along the Yangtze River, located both upstream and\ndownstream of the Three Gorges Dam. The results reveal significant differences\nin NCLDV diversity correlated with proximity to the dam, likely influenced by\nreduced flow velocity caused by the dam. These findings highlight the potential\nof GiantSeeker to advance our understanding of NCLDVs and their ecological\nroles in diverse environments.",
        "How does the monetary and fiscal policy mix alter households' saving\nincentives? To answer these questions, we build a heterogenous agents New\nKeynesian model where three different types of agents can save in assets with\ndifferent liquidity profiles to insure against idiosyncratic risk. Policy mixes\naffect saving incentives differently according to their effect on the liquidity\npremium -- the return difference between less liquid assets and public debt. We\nderive an intuitive analytical expression linking the liquidity premium with\nconsumption differentials amongst different types of agents. This underscores\nthe presence of a transmission mechanism through which the interaction of\nmonetary and fiscal policy shapes economic stability via its effect on the\nportfolio choice of private agents. We call it the 'self-insurance demand\nchannel', which moves the liquidity premium in the opposite direction to the\nstandard 'policy-driven supply channel'. Our analysis thus reveals the presence\nof two competing forces driving the liquidity premium. We show that the\nrelative strength of the two is tightly linked to the policy mix in place and\nthe type of business cycle shock hitting the economy. This implies that to\nstabilize the economy, monetary policy should consider the impact of the\n'self-insurance' on the liquidity premium.",
        "Depression significantly affects emotions, thoughts, and daily activities.\nRecent research indicates that speech signals contain vital cues about\ndepression, sparking interest in audio-based deep-learning methods for\nestimating its severity. However, most methods rely on time-frequency\nrepresentations of speech which have recently been criticized for their\nlimitations due to the loss of information when performing time-frequency\nprojections, e.g. Fourier transform, and Mel-scale transformation. Furthermore,\nsegmenting real-world speech into brief intervals risks losing critical\ninterconnections between recordings. Additionally, such an approach may not\nadequately reflect real-world scenarios, as individuals with depression often\npause and slow down in their conversations and interactions. Building on these\nobservations, we present an efficient method for depression level estimation\nusing long speech signals in the time domain. The proposed method leverages a\nstate space model coupled with the dual-path structure-based long sequence\nmodelling module and temporal external attention module to reconstruct and\nenhance the detection of depression-related cues hidden in the raw audio\nwaveforms. Experimental results on the AVEC2013 and AVEC2014 datasets show\npromising results in capturing consequential long-sequence depression cues and\ndemonstrate outstanding performance over the state-of-the-art.",
        "Extremely large-scale multiple-input multiple-output (XL-MIMO) is expected to\nbe an important technology in future sixth generation (6G) networks. Compared\nwith conventional single-polarized XL-MIMO, where signals are transmitted and\nreceived in only one polarization direction, dual-polarized XL-MIMO systems\nachieve higher data rate by improving multiplexing performances, and thus are\nthe focus of this paper. Due to enlarged aperture, near-field regions become\nnon-negligible in XL-MIMO communications, necessitating accurate near-far field\nboundary characterizations. However, existing boundaries developed for\nsingle-polarized systems only consider phase or power differences across array\nelements while irrespective of cross-polarization discrimination (XPD)\nvariances in dual-polarized XL-MIMO systems, deteriorating transmit covariance\noptimization performances. In this paper, we revisit near-far field boundaries\nfor dual-polarized XL-MIMO systems by taking XPD differences into account,\nwhich faces the following challenge. Unlike existing near-far field boundaries,\nwhich only need to consider co-polarized channel components, deriving\nboundaries for dual-polarized XL-MIMO systems requires modeling joint effects\nof co-polarized and cross-polarized components. To address this issue, we model\nXPD variations across antennas and introduce a non-uniform XPD distance to\ncomplement existing near-far field boundaries. Based on the new distance\ncriterion, we propose an efficient scheme to optimize transmit covariance.\nNumerical results validate our analysis and demonstrate the proposed\nalgorithm's effectiveness.",
        "Ensemble learning is a widespread technique to improve the prediction\nperformance of neural networks. However, it comes at the price of increased\nmemory and inference time. In this work we propose a novel model fusion\ntechnique called \\emph{Neuron Transplantation (NT)} in which we fuse an\nensemble of models by transplanting important neurons from all ensemble members\ninto the vacant space obtained by pruning insignificant neurons. An initial\nloss in performance post-transplantation can be quickly recovered via\nfine-tuning, consistently outperforming individual ensemble members of the same\nmodel capacity and architecture. Furthermore, NT enables all the ensemble\nmembers to be jointly pruned and jointly trained in a combined model. Comparing\nit to alignment-based averaging (like Optimal-Transport-fusion), it requires\nless fine-tuning than the corresponding OT-fused model, the fusion itself is\nfaster and requires less memory, while the resulting model performance is\ncomparable or better. The code is available under the following link:\nhttps:\/\/github.com\/masterbaer\/neuron-transplantation.",
        "In this work we propose a satellite specific Neural Radiance Fields (NeRF)\nmodel capable to obtain a three-dimensional semantic representation (neural\nsemantic field) of the scene. The model derives the output from a set of\nmulti-date satellite images with corresponding pixel-wise semantic labels. We\ndemonstrate the robustness of our approach and its capability to improve noisy\ninput labels. We enhance the color prediction by utilizing the semantic\ninformation to address temporal image inconsistencies caused by non-stationary\ncategories such as vehicles. To facilitate further research in this domain, we\npresent a dataset comprising manually generated labels for popular multi-view\nsatellite images. Our code and dataset are available at\nhttps:\/\/github.com\/wagnva\/semantic-nerf-for-satellite-data.",
        "The topological censorship theorem suggests that higher dimensional black\nholes can possess the domain of outer communication (DOC) of nontrivial\ntopology. In this paper, we seek for a black hole coexisting with two bubbles\nadjacent to the horizon in five-dimensional minimal supergravity, under the\nassumptions of stationarity and bi-axisymmetry. For simplicity, we also assume\nthat the spacetime is symmetric under the exchange of the two axisymmetric\nKilling vectors. To find the solution, we combine the inverse scattering method\nand the Harrison transformation, and we present the conditions for the absence\nof conical, orbifold and Dirac-Misner string singularities, respectively. As\nthe result, we find that the black hole with topology of $S^3$ or $S^2\\times\nS^1$ can be supported by two bubbles if we admit the conical singularities\n(deficits).",
        "This work presents the first numerical investigation of using Voigt\nregularization as a method for obtaining magnetohydrodynamic (MHD) equilibria\nwithout the assumption of nested magnetic flux surfaces. Voigt regularization\nmodifies the MHD dynamics by introducing additional terms that vanish in the\ninfinite-time limit, allowing for magnetic reconnection and the formation of\nmagnetic islands, which can overlap and produce field-line chaos. The utility\nof this approach is demonstrated through numerical solutions of two-dimensional\nideal and resistive test problems. Our results show that Voigt regularization\ncan significantly accelerate the convergence to solutions in resistive MHD\nproblems, while also highlighting challenges in applying the method to ideal\nMHD systems. This research opens up new possibilities for developing more\nefficient and robust MHD equilibrium solvers, which could contribute to the\ndesign and optimization of future fusion devices.",
        "Smartphone cameras have become ubiquitous imaging tools, yet their small\nsensors and compact optics often limit spatial resolution and introduce\ndistortions. Combining information from multiple low-resolution (LR) frames to\nproduce a high-resolution (HR) image has been explored to overcome the inherent\nlimitations of smartphone cameras. Despite the promise of multi-frame\nsuper-resolution (MFSR), current approaches are hindered by datasets that fail\nto capture the characteristic noise and motion patterns found in real-world\nhandheld burst images. In this work, we address this gap by introducing a novel\nsynthetic data engine that uses multi-exposure static images to synthesize\nLR-HR training pairs while preserving sensor-specific noise characteristics and\nimage motion found during handheld burst photography. We also propose MFSR-GAN:\na multi-scale RAW-to-RGB network for MFSR. Compared to prior approaches,\nMFSR-GAN emphasizes a \"base frame\" throughout its architecture to mitigate\nartifacts. Experimental results on both synthetic and real data demonstrates\nthat MFSR-GAN trained with our synthetic engine yields sharper, more realistic\nreconstructions than existing methods for real-world MFSR.",
        "Cyberattacks in an Internet of Things (IoT) environment can have significant\nimpacts because of the interconnected nature of devices and systems. An\nattacker uses a network of compromised IoT devices in a botnet attack to carry\nout various harmful activities. Detecting botnet attacks poses several\nchallenges because of the intricate and evolving nature of these threats.\nBotnet attacks erode trust in IoT devices and systems, undermining confidence\nin their security, reliability, and integrity. Deep learning techniques have\nsignificantly enhanced the detection of botnet attacks due to their ability to\nanalyze and learn from complex patterns in data. This research proposed the\nstacking of Deep convolutional neural networks, Bi-Directional Long Short-Term\nMemory (Bi-LSTM), Bi-Directional Gated Recurrent Unit (Bi-GRU), and Recurrent\nNeural Networks (RNN) for botnet attacks detection. The UNSW-NB15 dataset is\nutilized for botnet attacks detection. According to experimental results, the\nproposed model accurately provides for the intricate patterns and features of\nbotnet attacks, with a testing accuracy of 99.76%. The proposed model also\nidentifies botnets with a high ROC-AUC curve value of 99.18%. A performance\ncomparison of the proposed method with existing state-of-the-art models\nconfirms its higher performance. The outcomes of this research could strengthen\ncyber security procedures and safeguard against new attacks.",
        "Recent research on word-level confidence estimation for speech recognition\nsystems has primarily focused on lightweight models known as Confidence\nEstimation Modules (CEMs), which rely on hand-engineered features derived from\nAutomatic Speech Recognition (ASR) outputs. In contrast, we propose a novel\nend-to-end approach that leverages the ASR model itself (Whisper) to generate\nword-level confidence scores. Specifically, we introduce a method in which the\nWhisper model is fine-tuned to produce scalar confidence scores given an audio\ninput and its corresponding hypothesis transcript. Our experiments demonstrate\nthat the fine-tuned Whisper-tiny model, comparable in size to a strong CEM\nbaseline, achieves similar performance on the in-domain dataset and surpasses\nthe CEM baseline on eight out-of-domain datasets, whereas the fine-tuned\nWhisper-large model consistently outperforms the CEM baseline by a substantial\nmargin across all datasets.",
        "We show that under mild set theoretic hypotheses we have rigidity for\nalgebras of continuous functions over Higson coronas, topological spaces\narising in coarse geometry. In particular, we show that under $\\mathsf{OCA}$\nand $\\mathsf {MA}_{\\aleph_1}$, if two uniformly locally finite metric spaces\n$X$ and $Y$ have homeomorphic Higson coronas $\\nu X$ and $\\nu Y$, then $X$ and\n$Y$ are coarsely equivalent, a statement which provably does not follow from\n$\\mathsf{ZFC}$ alone.",
        "Exploring the photoinduced dynamics of chiral states offers promising avenues\nfor advanced control of condensed matter systems. Photoinduced or photoenhanced\nchirality in 1T-TiSe$_{2}$ has been suggested as a fascinating platform for\noptical manipulation of chiral states. However, the mechanisms underlying\nchirality training and its interplay with the charge density wave (CDW) phase\nremain elusive. Here, we use time-resolved X-ray diffraction (tr-XRD) with\ncircularly polarized pump lasers to probe the photoinduced dynamics of\nchirality in 1T-TiSe$_{2}$. We observe a notable ($\\sim$20%) difference in CDW\nintensity suppression between left- and right-circularly polarized pumps.\nAdditionally, we reveal momentum-resolved circular dichroism arising from\ndomains of different chirality, providing a direct link between CDW and\nchirality. An immediate increase in CDW correlation length upon laser pumping\nis detected, suggesting the photoinduced expansion of chiral domains. These\nresults both advance the potential of light-driven chirality by elucidating the\nmechanism driving chirality manipulation in TiSe$_2$, and they demonstrate that\ntr-XRD with circularly polarized pumps is an effective tool for chirality\ndetection in condensed matter systems.",
        "In this paper, we study proper time Lyapunov exponents and coordinate time\nLyapunov exponents of chaos for both massless and massive particles orbiting\nfour-dimensional and five-dimensional Kerr-AdS black holes, and explore their\nrelationships with phase transitions of these black holes. The results reveal\nthat these exponents can reflect the occurrence of phase transitions.\nSpecifically, when compared to the Lyapunov exponents of massive particles in\nchaotic states, the exponents corresponding to massless particles demonstrate a\nmore robust capability in describing the phase transitions. Furthermore, we\nconduct a study on critical exponents associated with the Lyapunov exponents in\nthese black holes, identifying a critical exponent value of 1\/2.",
        "Motion planning for locomotion systems typically requires translating\nhigh-level rigid-body tasks into low-level joint trajectories-a process that is\nstraightforward for car-like robots with fixed, unbounded actuation inputs but\nmore challenging for systems like snake robots, where the mapping depends on\nthe current configuration and is constrained by joint limits. In this paper, we\nfocus on generating continuous families of optimal gaits-collections of gaits\nparameterized by step size or steering rate-to enhance controllability and\nmaneuverability. We uncover the underlying geometric structure of these optimal\ngait families and propose methods for constructing them using both global and\nlocal search strategies, where the local method and the global method\ncompensate each other. The global search approach is robust to nonsmooth\nbehavior, albeit yielding reduced-order solutions, while the local search\nprovides higher accuracy but can be unstable near nonsmooth regions. To\ndemonstrate our framework, we generate optimal gait families for viscous and\nperfect-fluid three-link swimmers. This work lays a foundation for integrating\nlow-level joint controllers with higher-level motion planners in complex\nlocomotion systems.",
        "In this paper, we develop a numerical method for the computation of\n(quasi-)resonances in spherical symmetric heterogeneous Helmholtz problems with\npiecewise smooth refractive index. Our focus lies in resonances very close to\nthe real axis, which characterize the so-called whispering gallery modes. Our\nmethod involves a modal equation incorporating fundamental solutions to\ndecoupled problems, extending the known modal equation to the case of piecewise\nsmooth coefficients. We first establish the well-posedeness of the fundamental\nsystem, then we formulate the problem of resonances as a nonlinear eigenvalue\nproblem, whose determinant will be the modal equation in the piecewise smooth\ncase. In combination with the numerical approximation of the fundamental\nsolutions using a spectral method, we derive a Newton method to solve the\nnonlinear modal equation with a proper scaling. We show the local convergence\nof the algorithm in the piecewise constant case by proving the simplicity of\nthe roots. We confirm our approach through a series of numerical experiments in\nthe piecewise constant and variable case.",
        "Mixture-of-Experts (MoE) Transformer, the backbone architecture of multiple\nphenomenal language models, leverages sparsity by activating only a fraction of\nmodel parameters for each input token. The sparse structure, while allowing\nconstant time costs, results in space inefficiency: we still need to load all\nthe model parameters during inference. We introduce ResMoE, an innovative MoE\napproximation framework that utilizes Wasserstein barycenter to extract a\ncommon expert (barycenter expert) and approximate the residuals between this\nbarycenter expert and the original ones. ResMoE enhances the space efficiency\nfor inference of large-scale MoE Transformers in a one-shot and data-agnostic\nmanner without retraining while maintaining minimal accuracy loss, thereby\npaving the way for broader accessibility to large language models. We\ndemonstrate the effectiveness of ResMoE through extensive experiments on Switch\nTransformer, Mixtral, and DeepSeekMoE models. The results show that ResMoE can\nreduce the number of parameters in an expert by up to 75% while maintaining\ncomparable performance. The code is available at\nhttps:\/\/github.com\/iDEA-iSAIL-Lab-UIUC\/ResMoE.",
        "Deep Reinforcement Learning (DRL) has been extensively used to address\nportfolio optimization problems. The DRL agents acquire knowledge and make\ndecisions through unsupervised interactions with their environment without\nrequiring explicit knowledge of the joint dynamics of portfolio assets. Among\nthese DRL algorithms, the combination of actor-critic algorithms and deep\nfunction approximators is the most widely used DRL algorithm. Here, we find\nthat training the DRL agent using the actor-critic algorithm and deep function\napproximators may lead to scenarios where the improvement in the DRL agent's\nrisk-adjusted profitability is not significant. We propose that such situations\nprimarily arise from the following two problems: sparsity in positive reward\nand the curse of dimensionality. These limitations prevent DRL agents from\ncomprehensively learning asset price change patterns in the training\nenvironment. As a result, the DRL agents cannot explore the dynamic portfolio\noptimization policy to improve the risk-adjusted profitability in the training\nprocess. To address these problems, we propose a novel multi-agent Hierarchical\nDeep Reinforcement Learning (HDRL) algorithmic framework in this research.\nUnder this framework, the agents work together as a learning system for\nportfolio optimization. Specifically, by designing an auxiliary agent that\nworks together with the executive agent for optimal policy exploration, the\nlearning system can focus on exploring the policy with higher risk-adjusted\nreturn in the action space with positive return and low variance. In this way,\nwe can overcome the issue of the curse of dimensionality and improve the\ntraining efficiency in the positive reward sparse environment.",
        "In this paper, we prove that for a threefold of Fano type $X$ and a movable\n$\\mathbb{Q}$-Cartier Weil divisor $D$ on $X$, the number of smooth varieties\nthat arise during the running of a $D$-MMP is bounded by $1 + h^1(X, 2D)$.\nAdditionally, we prove a partial converse to the Kodaira vanishing theorem for\na movable divisor on a threefold of Fano type.",
        "We propose a theory that can determine the lowest isolation level that can be\nallocated to each transaction program in an application in a\nmixed-isolation-level setting, to guarantee that all executions will be\nserializable and thus preserve all integrity constraints, even those that are\nnot explicitly declared. This extends prior work applied to completely known\ntransactions, to deal with the realistic situation where transactions are\ngenerated by running programs with parameters that are not known in advance.\nUsing our theory, we propose an optimization method that allows for high\nthroughput while ensuring that all executions are serializable. Our method is\nbased on searching for application code modifications that are\nsemantics-preserving while improving the isolation level allocation. We\nillustrate our approach to the SmallBank benchmark.",
        "The long history of misconfigurations and errors in RPKI indicates that they\ncannot be easily avoided and will most probably persist also in the future.\nThese errors create conflicts between BGP announcements and their covering\nROAs, causing the RPKI validation to result in status invalid. Networks that\nenforce RPKI filtering with Route Origin Validation (ROV) would block such\nconflicting BGP announcements and as a result lose traffic from the\ncorresponding origins. Since the business incentives of networks are tightly\ncoupled with the traffic they relay, filtering legitimate traffic leads to a\nloss of revenue, reducing the motivation to filter invalid announcements with\nROV.\n  In this work, we introduce a new mechanism, LOV, designed for whitelisting\nbenign conflicts on an Internet scale. The resulting whitelist is made\navailable to RPKI supporting ASes to avoid filtering RPKI-invalid but benign\nroutes. Saving legitimate traffic resolves one main obstacle towards RPKI\ndeployment. We measure live BGP updates using LOV during a period of half a\nyear and whitelist 52,846 routes with benign origin errors.",
        "Long-term planning for robots operating in domestic environments poses unique\nchallenges due to the interactions between humans, objects, and spaces. Recent\nadvancements in trajectory planning have leveraged vision-language models\n(VLMs) to extract contextual information for robots operating in real-world\nenvironments. While these methods achieve satisfying performance, they do not\nexplicitly model human activities. Such activities influence surrounding\nobjects and reshape spatial constraints. This paper presents a novel approach\nto trajectory planning that integrates human preferences, activities, and\nspatial context through an enriched 3D scene graph (3DSG) representation. By\nincorporating activity-based relationships, our method captures the spatial\nimpact of human actions, leading to more context-sensitive trajectory\nadaptation. Preliminary results demonstrate that our approach effectively\nassigns costs to spaces influenced by human activities, ensuring that the robot\ntrajectory remains contextually appropriate and sensitive to the ongoing\nenvironment. This balance between task efficiency and social appropriateness\nenhances context-aware human-robot interactions in domestic settings. Future\nwork includes implementing a full planning pipeline and conducting user studies\nto evaluate trajectory acceptability.",
        "As of today, there is no official definition of a meteor cluster. It is\nusually identified as a large number of meteors sharing a similar radiant and\nvelocity, all occurring within a few seconds. Only eight clusters have been\nreported so far, from single-camera or camera network observations. We aim to\nprovide an overview of meteor clusters to help define what constitutes a\ncluster by potentially adding more to the already identified ones and\ndetermining their common parameters. A search for new clusters is performed in\npublicly available International Astronomical Union meteor databases with the\nDBSCAN algorithm. Then, a statistical significance method is applied to derive\nthe most promising cluster candidates. However, the method still lacks a way to\ndebias the atmospheric area surveyed by the cameras due to a lack of publicly\navailable data. A set of 16 statistically significant potential clusters is\nidentified, involving 4 to 7 fragments. The 90th percentile includes a duration\nof 8 seconds, a velocity difference of 2.2 km\/s, and a radiant spread of nearly\n4 degrees. The velocity difference may arise from the method used for orbit\ncomputation. Meteor clusters might be more frequent than currently reported.\nHowever, we recommend that future meteor orbit databases also include a way to\nestimate the surveyed area by the cameras involved in the detection. This would\nstrengthen the veracity of the 16 identified cluster candidates and ultimately\nallow scientists to fully debias the number of clusters, and hence derive the\nphysical lifetime expectancy of meteoroids, which is often overlooked due to\nthe focus on collisional lifetime estimates only. We also recommend that any\nfuture cluster observation report includes the expected number of random\noccurrences and consider the event to be real if this value is below 0.1.",
        "Current research on the \\textit{Decompose-Then-Verify} paradigm for\nevaluating the factuality of long-form text typically treats decomposition and\nverification in isolation, overlooking their interactions and potential\nmisalignment. We find that existing decomposition policies, typically\nhand-crafted demonstrations, do not align well with downstream verifiers in\nterms of atomicity -- a novel metric quantifying information density -- leading\nto suboptimal verification results. We formulate finding the optimal\ndecomposition policy for optimal verification as a bilevel optimization\nproblem. To approximate a solution for this strongly NP-hard problem, we\npropose dynamic decomposition, a reinforcement learning framework that\nleverages verifier feedback to learn a policy for dynamically decomposing\nclaims to verifier-preferred atomicity. Experimental results show that dynamic\ndecomposition outperforms existing decomposition policies, improving\nverification confidence by 0.07 and accuracy by 0.12 (on a 0-1 scale) on\naverage across varying verifiers, datasets, and atomcities of input claims.",
        "Structured embedding transformations offer a promising approach for enhancing\nthe efficiency and coherence of language model inference. The introduction of\nStructural Embedding Projection (SEP) provides a mechanism for refining token\nrepresentations through projection matrices that integrate hierarchical and\nrelational dependencies. The mathematical formulation of SEP enables embedding\nspaces to capture structured contextual relationships, thereby improving\nsemantic fidelity without significantly increasing computational overhead.\nExperimental evaluations conducted on a range of linguistic datasets revealed\nthat SEP contributed to reductions in perplexity and enhanced contextual\ncoherence, demonstrating its potential to refine language model outputs.\nComputational efficiency assessments highlighted variations across different\ndatasets, suggesting that the integration of structured embeddings introduced\ndataset-dependent trade-offs between inference speed and representational\nrichness. The qualitative analysis of generated responses indicated that SEP\nenhanced narrative consistency and topic alignment, leading to improved fluency\nin multi-sentence text generation. The modifications to embedding layers\nrequired precise optimization to ensure stable training dynamics, as the\nintroduction of structured transformations altered the traditional\nrepresentation-learning process. The architectural adjustments necessary for\nSEP implementation influenced inference latency and memory consumption,\nrequiring a balance between efficiency gains and additional processing demands.\nThe impact of SEP on lexical diversity suggested that embedding modifications\ninfluenced the model's vocabulary usage, reflecting a more context-aware\nselection of generated tokens.",
        "Cybergrooming exploits minors through online trust-building, yet research\nremains fragmented, limiting holistic prevention. Social sciences focus on\nbehavioral insights, while computational methods emphasize detection, but their\nintegration remains insufficient. This review systematically synthesizes both\nfields using the PRISMA framework to enhance clarity, reproducibility, and\ncross-disciplinary collaboration. Findings show that qualitative methods offer\ndeep insights but are resource-intensive, machine learning models depend on\ndata quality, and standard metrics struggle with imbalance and cultural\nnuances. By bridging these gaps, this review advances interdisciplinary\ncybergrooming research, guiding future efforts toward more effective prevention\nand detection strategies.",
        "The rapid advancement of information and communication technology has\nfacilitated easier access to information. However, this progress has also\nnecessitated more stringent verification measures to ensure the accuracy of\ninformation, particularly within the context of Vietnam. This paper introduces\nan approach to address the challenges of Fact Verification using the Vietnamese\ndataset by integrating both sentence selection and classification modules into\na unified network architecture. The proposed approach leverages the power of\nlarge language models by utilizing pre-trained PhoBERT and XLM-RoBERTa as the\nbackbone of the network. The proposed model was trained on a Vietnamese\ndataset, named ISE-DSC01, and demonstrated superior performance compared to the\nbaseline model across all three metrics. Notably, we achieved a Strict Accuracy\nlevel of 75.11\\%, indicating a remarkable 28.83\\% improvement over the baseline\nmodel.",
        "Safety alignment mechanism are essential for preventing large language models\n(LLMs) from generating harmful information or unethical content. However,\ncleverly crafted prompts can bypass these safety measures without accessing the\nmodel's internal parameters, a phenomenon known as black-box jailbreak.\nExisting heuristic black-box attack methods, such as genetic algorithms, suffer\nfrom limited effectiveness due to their inherent randomness, while recent\nreinforcement learning (RL) based methods often lack robust and informative\nreward signals. To address these challenges, we propose a novel black-box\njailbreak method leveraging RL, which optimizes prompt generation by analyzing\nthe embedding proximity between benign and malicious prompts. This approach\nensures that the rewritten prompts closely align with the intent of the\noriginal prompts while enhancing the attack's effectiveness. Furthermore, we\nintroduce a comprehensive jailbreak evaluation framework incorporating\nkeywords, intent matching, and answer validation to provide a more rigorous and\nholistic assessment of jailbreak success. Experimental results show the\nsuperiority of our approach, achieving state-of-the-art (SOTA) performance on\nseveral prominent open and closed-source LLMs, including Qwen2.5-7B-Instruct,\nLlama3.1-8B-Instruct, and GPT-4o-0806. Our method sets a new benchmark in\njailbreak attack effectiveness, highlighting potential vulnerabilities in LLMs.\nThe codebase for this work is available at\nhttps:\/\/github.com\/Aegis1863\/xJailbreak.",
        "Research on cognitive biases and heuristics has become increasingly popular\nin the visualization literature in recent years. Researchers have studied the\neffects of biases on visualization interpretation and subsequent\ndecision-making. While this work is important, we contend that the view on\nbiases has presented human cognitive abilities in an unbalanced manner, placing\ntoo much emphasis on the flaws and limitations of human decision-making, and\npotentially suggesting that it should not be trusted. Several decision\nresearchers have argued that the flip side of biases -- i.e., mental shortcuts\nor heuristics -- demonstrate human ingenuity and serve as core markers of\nadaptive expertise. In this paper, we review the perspectives and sentiments of\nthe visualization community on biases and describe literature arguing for more\nbalanced views of biases and heuristics. We hope this paper will encourage\nvisualization researchers to consider a fuller picture of human cognitive\nlimitations and strategies for making decisions in complex environments."
      ]
    }
  },
  {
    "id":2411.00714,
    "research_type":"basic",
    "start_id":"b14",
    "start_title":"Random graphs with arbitrary degree distributions and their applications",
    "start_abstract":"Recent work on the structure of social networks and internet has focused attention graphs with distributions vertex degree that are significantly different from Poisson have been widely studied in past. In this paper we develop detail theory random arbitrary distributions. addition to simple undirected, unipartite graphs, examine properties directed bipartite graphs. Among other results, derive exact expressions for position phase transition at which a giant component first forms, mean size, size if there is one, number vertices certain distance away randomly chosen vertex, average vertex-vertex within graph. We apply our some real-world including world-wide web collaboration scientists Fortune 1000 company directors. demonstrate cases appropriate predict surprising accuracy behavior real world, while others measurable discrepancy between reality, perhaps indicating presence additional network not captured by",
    "start_categories":[
      "Modeling"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Universality, criticality and complexity of information propagation in social media"
      ],
      "abstract":[
        "Abstract Statistical laws of information avalanches in social media appear, at least according to existing empirical studies, not robust across systems. As a consequence, radically different processes may represent plausible driving mechanisms for propagation. Here, we analyze almost one billion time-stamped events collected from several online platforms \u2013 including Telegram, Twitter and Weibo over observation windows longer than ten years, show that the propagation is universal critical process. Universality arises identical macroscopic patterns platforms, irrespective details specific system hand. Critical behavior deduced power-law distributions, corresponding hyperscaling relations, characterizing size duration information. testing on our data indicates mixture simple complex contagion characterizes media. Data suggest complexity process correlated with semantic content propagated."
      ],
      "categories":[
        "Human Behaviors"
      ]
    },
    "list":{
      "title":[
        "GiantHunter: Accurate detection of giant virus in metagenomic data using\n  reinforcement-learning and Monte Carlo tree search",
        "Monetary-Fiscal Interaction and the Liquidity of Government Debt",
        "Efficient Long Speech Sequence Modelling for Time-Domain Depression\n  Level Estimation",
        "Revisiting Near-Far Field Boundary in Dual-Polarized XL-MIMO Systems",
        "Model Fusion via Neuron Transplantation",
        "Semantic Neural Radiance Fields for Multi-Date Satellite Data",
        "Exploring non-supersymmetric black holes with multiple bubbles in\n  five-dimensional minimal supergravity",
        "Computation of Magnetohydrodynamic Equilibria with Voigt Regularization",
        "MFSR-GAN: Multi-Frame Super-Resolution with Handheld Motion Modeling",
        "Enhanced Hybrid Deep Learning Approach for Botnet Attacks Detection in\n  IoT Environment",
        "Adopting Whisper for Confidence Estimation",
        "Rigidity of Higson coronas",
        "Photo-induced Dynamics and Momentum Distribution of Chiral Charge\n  Density Waves in 1T-TiSe$_{2}$",
        "Lyapunov exponents as probes for phase transitions of Kerr-AdS black\n  holes",
        "The Geometry of Optimal Gait Families for Steering Kinematic Locomoting\n  Systems",
        "Computation of whispering gallery modes for spherical symmetric\n  heterogeneous Helmholtz problems with piecewise smooth refractive index",
        "ResMoE: Space-efficient Compression of Mixture of Experts LLMs via\n  Residual Restoration",
        "A novel multi-agent dynamic portfolio optimization learning system based\n  on hierarchical deep reinforcement learning",
        "The number of smooth varieties in an MMP on a 3-fold of Fano type",
        "Using Read Promotion and Mixed Isolation Levels for Performant Yet\n  Serializable Execution of Transaction Programs",
        "Learning to Identify Conflicts in RPKI",
        "Long-Term Planning Around Humans in Domestic Environments with 3D Scene\n  Graphs",
        "Towards a definition of a meteor cluster: Detection of meteor clusters\n  from meteor orbit databases",
        "Optimizing Decomposition for Optimal Claim Verification",
        "Structural Embedding Projection for Contextual Large Language Model\n  Inference",
        "Toward Integrated Solutions: A Systematic Interdisciplinary Review of\n  Cybergrooming Research",
        "BERT-based model for Vietnamese Fact Verification Dataset",
        "xJailbreak: Representation Space Guided Reinforcement Learning for\n  Interpretable LLM Jailbreaking",
        "Are Cognitive Biases as Important as they Seem for Data Visualization?"
      ],
      "abstract":[
        "Motivation: Nucleocytoplasmic large DNA viruses (NCLDVs) are notable for\ntheir large genomes and extensive gene repertoires, which contribute to their\nwidespread environmental presence and critical roles in processes such as host\nmetabolic reprogramming and nutrient cycling. Metagenomic sequencing has\nemerged as a powerful tool for uncovering novel NCLDVs in environmental\nsamples. However, identifying NCLDV sequences in metagenomic data remains\nchallenging due to their high genomic diversity, limited reference genomes, and\nshared regions with other microbes. Existing alignment-based and machine\nlearning methods struggle with achieving optimal trade-offs between sensitivity\nand precision. Results: In this work, we present GiantHunter, a reinforcement\nlearning-based tool for identifying NCLDVs from metagenomic data. By employing\na Monte Carlo tree search strategy, GiantHunter dynamically selects\nrepresentative non-NCLDV sequences as the negative training data, enabling the\nmodel to establish a robust decision boundary. Benchmarking on rigorously\ndesigned experiments shows that GiantHunter achieves high precision while\nmaintaining competitive sensitivity, improving the F1-score by 10% and reducing\ncomputational cost by 90% compared to the second-best method. To demonstrate\nits real-world utility, we applied GiantHunter to 60 metagenomic datasets\ncollected from six cities along the Yangtze River, located both upstream and\ndownstream of the Three Gorges Dam. The results reveal significant differences\nin NCLDV diversity correlated with proximity to the dam, likely influenced by\nreduced flow velocity caused by the dam. These findings highlight the potential\nof GiantSeeker to advance our understanding of NCLDVs and their ecological\nroles in diverse environments.",
        "How does the monetary and fiscal policy mix alter households' saving\nincentives? To answer these questions, we build a heterogenous agents New\nKeynesian model where three different types of agents can save in assets with\ndifferent liquidity profiles to insure against idiosyncratic risk. Policy mixes\naffect saving incentives differently according to their effect on the liquidity\npremium -- the return difference between less liquid assets and public debt. We\nderive an intuitive analytical expression linking the liquidity premium with\nconsumption differentials amongst different types of agents. This underscores\nthe presence of a transmission mechanism through which the interaction of\nmonetary and fiscal policy shapes economic stability via its effect on the\nportfolio choice of private agents. We call it the 'self-insurance demand\nchannel', which moves the liquidity premium in the opposite direction to the\nstandard 'policy-driven supply channel'. Our analysis thus reveals the presence\nof two competing forces driving the liquidity premium. We show that the\nrelative strength of the two is tightly linked to the policy mix in place and\nthe type of business cycle shock hitting the economy. This implies that to\nstabilize the economy, monetary policy should consider the impact of the\n'self-insurance' on the liquidity premium.",
        "Depression significantly affects emotions, thoughts, and daily activities.\nRecent research indicates that speech signals contain vital cues about\ndepression, sparking interest in audio-based deep-learning methods for\nestimating its severity. However, most methods rely on time-frequency\nrepresentations of speech which have recently been criticized for their\nlimitations due to the loss of information when performing time-frequency\nprojections, e.g. Fourier transform, and Mel-scale transformation. Furthermore,\nsegmenting real-world speech into brief intervals risks losing critical\ninterconnections between recordings. Additionally, such an approach may not\nadequately reflect real-world scenarios, as individuals with depression often\npause and slow down in their conversations and interactions. Building on these\nobservations, we present an efficient method for depression level estimation\nusing long speech signals in the time domain. The proposed method leverages a\nstate space model coupled with the dual-path structure-based long sequence\nmodelling module and temporal external attention module to reconstruct and\nenhance the detection of depression-related cues hidden in the raw audio\nwaveforms. Experimental results on the AVEC2013 and AVEC2014 datasets show\npromising results in capturing consequential long-sequence depression cues and\ndemonstrate outstanding performance over the state-of-the-art.",
        "Extremely large-scale multiple-input multiple-output (XL-MIMO) is expected to\nbe an important technology in future sixth generation (6G) networks. Compared\nwith conventional single-polarized XL-MIMO, where signals are transmitted and\nreceived in only one polarization direction, dual-polarized XL-MIMO systems\nachieve higher data rate by improving multiplexing performances, and thus are\nthe focus of this paper. Due to enlarged aperture, near-field regions become\nnon-negligible in XL-MIMO communications, necessitating accurate near-far field\nboundary characterizations. However, existing boundaries developed for\nsingle-polarized systems only consider phase or power differences across array\nelements while irrespective of cross-polarization discrimination (XPD)\nvariances in dual-polarized XL-MIMO systems, deteriorating transmit covariance\noptimization performances. In this paper, we revisit near-far field boundaries\nfor dual-polarized XL-MIMO systems by taking XPD differences into account,\nwhich faces the following challenge. Unlike existing near-far field boundaries,\nwhich only need to consider co-polarized channel components, deriving\nboundaries for dual-polarized XL-MIMO systems requires modeling joint effects\nof co-polarized and cross-polarized components. To address this issue, we model\nXPD variations across antennas and introduce a non-uniform XPD distance to\ncomplement existing near-far field boundaries. Based on the new distance\ncriterion, we propose an efficient scheme to optimize transmit covariance.\nNumerical results validate our analysis and demonstrate the proposed\nalgorithm's effectiveness.",
        "Ensemble learning is a widespread technique to improve the prediction\nperformance of neural networks. However, it comes at the price of increased\nmemory and inference time. In this work we propose a novel model fusion\ntechnique called \\emph{Neuron Transplantation (NT)} in which we fuse an\nensemble of models by transplanting important neurons from all ensemble members\ninto the vacant space obtained by pruning insignificant neurons. An initial\nloss in performance post-transplantation can be quickly recovered via\nfine-tuning, consistently outperforming individual ensemble members of the same\nmodel capacity and architecture. Furthermore, NT enables all the ensemble\nmembers to be jointly pruned and jointly trained in a combined model. Comparing\nit to alignment-based averaging (like Optimal-Transport-fusion), it requires\nless fine-tuning than the corresponding OT-fused model, the fusion itself is\nfaster and requires less memory, while the resulting model performance is\ncomparable or better. The code is available under the following link:\nhttps:\/\/github.com\/masterbaer\/neuron-transplantation.",
        "In this work we propose a satellite specific Neural Radiance Fields (NeRF)\nmodel capable to obtain a three-dimensional semantic representation (neural\nsemantic field) of the scene. The model derives the output from a set of\nmulti-date satellite images with corresponding pixel-wise semantic labels. We\ndemonstrate the robustness of our approach and its capability to improve noisy\ninput labels. We enhance the color prediction by utilizing the semantic\ninformation to address temporal image inconsistencies caused by non-stationary\ncategories such as vehicles. To facilitate further research in this domain, we\npresent a dataset comprising manually generated labels for popular multi-view\nsatellite images. Our code and dataset are available at\nhttps:\/\/github.com\/wagnva\/semantic-nerf-for-satellite-data.",
        "The topological censorship theorem suggests that higher dimensional black\nholes can possess the domain of outer communication (DOC) of nontrivial\ntopology. In this paper, we seek for a black hole coexisting with two bubbles\nadjacent to the horizon in five-dimensional minimal supergravity, under the\nassumptions of stationarity and bi-axisymmetry. For simplicity, we also assume\nthat the spacetime is symmetric under the exchange of the two axisymmetric\nKilling vectors. To find the solution, we combine the inverse scattering method\nand the Harrison transformation, and we present the conditions for the absence\nof conical, orbifold and Dirac-Misner string singularities, respectively. As\nthe result, we find that the black hole with topology of $S^3$ or $S^2\\times\nS^1$ can be supported by two bubbles if we admit the conical singularities\n(deficits).",
        "This work presents the first numerical investigation of using Voigt\nregularization as a method for obtaining magnetohydrodynamic (MHD) equilibria\nwithout the assumption of nested magnetic flux surfaces. Voigt regularization\nmodifies the MHD dynamics by introducing additional terms that vanish in the\ninfinite-time limit, allowing for magnetic reconnection and the formation of\nmagnetic islands, which can overlap and produce field-line chaos. The utility\nof this approach is demonstrated through numerical solutions of two-dimensional\nideal and resistive test problems. Our results show that Voigt regularization\ncan significantly accelerate the convergence to solutions in resistive MHD\nproblems, while also highlighting challenges in applying the method to ideal\nMHD systems. This research opens up new possibilities for developing more\nefficient and robust MHD equilibrium solvers, which could contribute to the\ndesign and optimization of future fusion devices.",
        "Smartphone cameras have become ubiquitous imaging tools, yet their small\nsensors and compact optics often limit spatial resolution and introduce\ndistortions. Combining information from multiple low-resolution (LR) frames to\nproduce a high-resolution (HR) image has been explored to overcome the inherent\nlimitations of smartphone cameras. Despite the promise of multi-frame\nsuper-resolution (MFSR), current approaches are hindered by datasets that fail\nto capture the characteristic noise and motion patterns found in real-world\nhandheld burst images. In this work, we address this gap by introducing a novel\nsynthetic data engine that uses multi-exposure static images to synthesize\nLR-HR training pairs while preserving sensor-specific noise characteristics and\nimage motion found during handheld burst photography. We also propose MFSR-GAN:\na multi-scale RAW-to-RGB network for MFSR. Compared to prior approaches,\nMFSR-GAN emphasizes a \"base frame\" throughout its architecture to mitigate\nartifacts. Experimental results on both synthetic and real data demonstrates\nthat MFSR-GAN trained with our synthetic engine yields sharper, more realistic\nreconstructions than existing methods for real-world MFSR.",
        "Cyberattacks in an Internet of Things (IoT) environment can have significant\nimpacts because of the interconnected nature of devices and systems. An\nattacker uses a network of compromised IoT devices in a botnet attack to carry\nout various harmful activities. Detecting botnet attacks poses several\nchallenges because of the intricate and evolving nature of these threats.\nBotnet attacks erode trust in IoT devices and systems, undermining confidence\nin their security, reliability, and integrity. Deep learning techniques have\nsignificantly enhanced the detection of botnet attacks due to their ability to\nanalyze and learn from complex patterns in data. This research proposed the\nstacking of Deep convolutional neural networks, Bi-Directional Long Short-Term\nMemory (Bi-LSTM), Bi-Directional Gated Recurrent Unit (Bi-GRU), and Recurrent\nNeural Networks (RNN) for botnet attacks detection. The UNSW-NB15 dataset is\nutilized for botnet attacks detection. According to experimental results, the\nproposed model accurately provides for the intricate patterns and features of\nbotnet attacks, with a testing accuracy of 99.76%. The proposed model also\nidentifies botnets with a high ROC-AUC curve value of 99.18%. A performance\ncomparison of the proposed method with existing state-of-the-art models\nconfirms its higher performance. The outcomes of this research could strengthen\ncyber security procedures and safeguard against new attacks.",
        "Recent research on word-level confidence estimation for speech recognition\nsystems has primarily focused on lightweight models known as Confidence\nEstimation Modules (CEMs), which rely on hand-engineered features derived from\nAutomatic Speech Recognition (ASR) outputs. In contrast, we propose a novel\nend-to-end approach that leverages the ASR model itself (Whisper) to generate\nword-level confidence scores. Specifically, we introduce a method in which the\nWhisper model is fine-tuned to produce scalar confidence scores given an audio\ninput and its corresponding hypothesis transcript. Our experiments demonstrate\nthat the fine-tuned Whisper-tiny model, comparable in size to a strong CEM\nbaseline, achieves similar performance on the in-domain dataset and surpasses\nthe CEM baseline on eight out-of-domain datasets, whereas the fine-tuned\nWhisper-large model consistently outperforms the CEM baseline by a substantial\nmargin across all datasets.",
        "We show that under mild set theoretic hypotheses we have rigidity for\nalgebras of continuous functions over Higson coronas, topological spaces\narising in coarse geometry. In particular, we show that under $\\mathsf{OCA}$\nand $\\mathsf {MA}_{\\aleph_1}$, if two uniformly locally finite metric spaces\n$X$ and $Y$ have homeomorphic Higson coronas $\\nu X$ and $\\nu Y$, then $X$ and\n$Y$ are coarsely equivalent, a statement which provably does not follow from\n$\\mathsf{ZFC}$ alone.",
        "Exploring the photoinduced dynamics of chiral states offers promising avenues\nfor advanced control of condensed matter systems. Photoinduced or photoenhanced\nchirality in 1T-TiSe$_{2}$ has been suggested as a fascinating platform for\noptical manipulation of chiral states. However, the mechanisms underlying\nchirality training and its interplay with the charge density wave (CDW) phase\nremain elusive. Here, we use time-resolved X-ray diffraction (tr-XRD) with\ncircularly polarized pump lasers to probe the photoinduced dynamics of\nchirality in 1T-TiSe$_{2}$. We observe a notable ($\\sim$20%) difference in CDW\nintensity suppression between left- and right-circularly polarized pumps.\nAdditionally, we reveal momentum-resolved circular dichroism arising from\ndomains of different chirality, providing a direct link between CDW and\nchirality. An immediate increase in CDW correlation length upon laser pumping\nis detected, suggesting the photoinduced expansion of chiral domains. These\nresults both advance the potential of light-driven chirality by elucidating the\nmechanism driving chirality manipulation in TiSe$_2$, and they demonstrate that\ntr-XRD with circularly polarized pumps is an effective tool for chirality\ndetection in condensed matter systems.",
        "In this paper, we study proper time Lyapunov exponents and coordinate time\nLyapunov exponents of chaos for both massless and massive particles orbiting\nfour-dimensional and five-dimensional Kerr-AdS black holes, and explore their\nrelationships with phase transitions of these black holes. The results reveal\nthat these exponents can reflect the occurrence of phase transitions.\nSpecifically, when compared to the Lyapunov exponents of massive particles in\nchaotic states, the exponents corresponding to massless particles demonstrate a\nmore robust capability in describing the phase transitions. Furthermore, we\nconduct a study on critical exponents associated with the Lyapunov exponents in\nthese black holes, identifying a critical exponent value of 1\/2.",
        "Motion planning for locomotion systems typically requires translating\nhigh-level rigid-body tasks into low-level joint trajectories-a process that is\nstraightforward for car-like robots with fixed, unbounded actuation inputs but\nmore challenging for systems like snake robots, where the mapping depends on\nthe current configuration and is constrained by joint limits. In this paper, we\nfocus on generating continuous families of optimal gaits-collections of gaits\nparameterized by step size or steering rate-to enhance controllability and\nmaneuverability. We uncover the underlying geometric structure of these optimal\ngait families and propose methods for constructing them using both global and\nlocal search strategies, where the local method and the global method\ncompensate each other. The global search approach is robust to nonsmooth\nbehavior, albeit yielding reduced-order solutions, while the local search\nprovides higher accuracy but can be unstable near nonsmooth regions. To\ndemonstrate our framework, we generate optimal gait families for viscous and\nperfect-fluid three-link swimmers. This work lays a foundation for integrating\nlow-level joint controllers with higher-level motion planners in complex\nlocomotion systems.",
        "In this paper, we develop a numerical method for the computation of\n(quasi-)resonances in spherical symmetric heterogeneous Helmholtz problems with\npiecewise smooth refractive index. Our focus lies in resonances very close to\nthe real axis, which characterize the so-called whispering gallery modes. Our\nmethod involves a modal equation incorporating fundamental solutions to\ndecoupled problems, extending the known modal equation to the case of piecewise\nsmooth coefficients. We first establish the well-posedeness of the fundamental\nsystem, then we formulate the problem of resonances as a nonlinear eigenvalue\nproblem, whose determinant will be the modal equation in the piecewise smooth\ncase. In combination with the numerical approximation of the fundamental\nsolutions using a spectral method, we derive a Newton method to solve the\nnonlinear modal equation with a proper scaling. We show the local convergence\nof the algorithm in the piecewise constant case by proving the simplicity of\nthe roots. We confirm our approach through a series of numerical experiments in\nthe piecewise constant and variable case.",
        "Mixture-of-Experts (MoE) Transformer, the backbone architecture of multiple\nphenomenal language models, leverages sparsity by activating only a fraction of\nmodel parameters for each input token. The sparse structure, while allowing\nconstant time costs, results in space inefficiency: we still need to load all\nthe model parameters during inference. We introduce ResMoE, an innovative MoE\napproximation framework that utilizes Wasserstein barycenter to extract a\ncommon expert (barycenter expert) and approximate the residuals between this\nbarycenter expert and the original ones. ResMoE enhances the space efficiency\nfor inference of large-scale MoE Transformers in a one-shot and data-agnostic\nmanner without retraining while maintaining minimal accuracy loss, thereby\npaving the way for broader accessibility to large language models. We\ndemonstrate the effectiveness of ResMoE through extensive experiments on Switch\nTransformer, Mixtral, and DeepSeekMoE models. The results show that ResMoE can\nreduce the number of parameters in an expert by up to 75% while maintaining\ncomparable performance. The code is available at\nhttps:\/\/github.com\/iDEA-iSAIL-Lab-UIUC\/ResMoE.",
        "Deep Reinforcement Learning (DRL) has been extensively used to address\nportfolio optimization problems. The DRL agents acquire knowledge and make\ndecisions through unsupervised interactions with their environment without\nrequiring explicit knowledge of the joint dynamics of portfolio assets. Among\nthese DRL algorithms, the combination of actor-critic algorithms and deep\nfunction approximators is the most widely used DRL algorithm. Here, we find\nthat training the DRL agent using the actor-critic algorithm and deep function\napproximators may lead to scenarios where the improvement in the DRL agent's\nrisk-adjusted profitability is not significant. We propose that such situations\nprimarily arise from the following two problems: sparsity in positive reward\nand the curse of dimensionality. These limitations prevent DRL agents from\ncomprehensively learning asset price change patterns in the training\nenvironment. As a result, the DRL agents cannot explore the dynamic portfolio\noptimization policy to improve the risk-adjusted profitability in the training\nprocess. To address these problems, we propose a novel multi-agent Hierarchical\nDeep Reinforcement Learning (HDRL) algorithmic framework in this research.\nUnder this framework, the agents work together as a learning system for\nportfolio optimization. Specifically, by designing an auxiliary agent that\nworks together with the executive agent for optimal policy exploration, the\nlearning system can focus on exploring the policy with higher risk-adjusted\nreturn in the action space with positive return and low variance. In this way,\nwe can overcome the issue of the curse of dimensionality and improve the\ntraining efficiency in the positive reward sparse environment.",
        "In this paper, we prove that for a threefold of Fano type $X$ and a movable\n$\\mathbb{Q}$-Cartier Weil divisor $D$ on $X$, the number of smooth varieties\nthat arise during the running of a $D$-MMP is bounded by $1 + h^1(X, 2D)$.\nAdditionally, we prove a partial converse to the Kodaira vanishing theorem for\na movable divisor on a threefold of Fano type.",
        "We propose a theory that can determine the lowest isolation level that can be\nallocated to each transaction program in an application in a\nmixed-isolation-level setting, to guarantee that all executions will be\nserializable and thus preserve all integrity constraints, even those that are\nnot explicitly declared. This extends prior work applied to completely known\ntransactions, to deal with the realistic situation where transactions are\ngenerated by running programs with parameters that are not known in advance.\nUsing our theory, we propose an optimization method that allows for high\nthroughput while ensuring that all executions are serializable. Our method is\nbased on searching for application code modifications that are\nsemantics-preserving while improving the isolation level allocation. We\nillustrate our approach to the SmallBank benchmark.",
        "The long history of misconfigurations and errors in RPKI indicates that they\ncannot be easily avoided and will most probably persist also in the future.\nThese errors create conflicts between BGP announcements and their covering\nROAs, causing the RPKI validation to result in status invalid. Networks that\nenforce RPKI filtering with Route Origin Validation (ROV) would block such\nconflicting BGP announcements and as a result lose traffic from the\ncorresponding origins. Since the business incentives of networks are tightly\ncoupled with the traffic they relay, filtering legitimate traffic leads to a\nloss of revenue, reducing the motivation to filter invalid announcements with\nROV.\n  In this work, we introduce a new mechanism, LOV, designed for whitelisting\nbenign conflicts on an Internet scale. The resulting whitelist is made\navailable to RPKI supporting ASes to avoid filtering RPKI-invalid but benign\nroutes. Saving legitimate traffic resolves one main obstacle towards RPKI\ndeployment. We measure live BGP updates using LOV during a period of half a\nyear and whitelist 52,846 routes with benign origin errors.",
        "Long-term planning for robots operating in domestic environments poses unique\nchallenges due to the interactions between humans, objects, and spaces. Recent\nadvancements in trajectory planning have leveraged vision-language models\n(VLMs) to extract contextual information for robots operating in real-world\nenvironments. While these methods achieve satisfying performance, they do not\nexplicitly model human activities. Such activities influence surrounding\nobjects and reshape spatial constraints. This paper presents a novel approach\nto trajectory planning that integrates human preferences, activities, and\nspatial context through an enriched 3D scene graph (3DSG) representation. By\nincorporating activity-based relationships, our method captures the spatial\nimpact of human actions, leading to more context-sensitive trajectory\nadaptation. Preliminary results demonstrate that our approach effectively\nassigns costs to spaces influenced by human activities, ensuring that the robot\ntrajectory remains contextually appropriate and sensitive to the ongoing\nenvironment. This balance between task efficiency and social appropriateness\nenhances context-aware human-robot interactions in domestic settings. Future\nwork includes implementing a full planning pipeline and conducting user studies\nto evaluate trajectory acceptability.",
        "As of today, there is no official definition of a meteor cluster. It is\nusually identified as a large number of meteors sharing a similar radiant and\nvelocity, all occurring within a few seconds. Only eight clusters have been\nreported so far, from single-camera or camera network observations. We aim to\nprovide an overview of meteor clusters to help define what constitutes a\ncluster by potentially adding more to the already identified ones and\ndetermining their common parameters. A search for new clusters is performed in\npublicly available International Astronomical Union meteor databases with the\nDBSCAN algorithm. Then, a statistical significance method is applied to derive\nthe most promising cluster candidates. However, the method still lacks a way to\ndebias the atmospheric area surveyed by the cameras due to a lack of publicly\navailable data. A set of 16 statistically significant potential clusters is\nidentified, involving 4 to 7 fragments. The 90th percentile includes a duration\nof 8 seconds, a velocity difference of 2.2 km\/s, and a radiant spread of nearly\n4 degrees. The velocity difference may arise from the method used for orbit\ncomputation. Meteor clusters might be more frequent than currently reported.\nHowever, we recommend that future meteor orbit databases also include a way to\nestimate the surveyed area by the cameras involved in the detection. This would\nstrengthen the veracity of the 16 identified cluster candidates and ultimately\nallow scientists to fully debias the number of clusters, and hence derive the\nphysical lifetime expectancy of meteoroids, which is often overlooked due to\nthe focus on collisional lifetime estimates only. We also recommend that any\nfuture cluster observation report includes the expected number of random\noccurrences and consider the event to be real if this value is below 0.1.",
        "Current research on the \\textit{Decompose-Then-Verify} paradigm for\nevaluating the factuality of long-form text typically treats decomposition and\nverification in isolation, overlooking their interactions and potential\nmisalignment. We find that existing decomposition policies, typically\nhand-crafted demonstrations, do not align well with downstream verifiers in\nterms of atomicity -- a novel metric quantifying information density -- leading\nto suboptimal verification results. We formulate finding the optimal\ndecomposition policy for optimal verification as a bilevel optimization\nproblem. To approximate a solution for this strongly NP-hard problem, we\npropose dynamic decomposition, a reinforcement learning framework that\nleverages verifier feedback to learn a policy for dynamically decomposing\nclaims to verifier-preferred atomicity. Experimental results show that dynamic\ndecomposition outperforms existing decomposition policies, improving\nverification confidence by 0.07 and accuracy by 0.12 (on a 0-1 scale) on\naverage across varying verifiers, datasets, and atomcities of input claims.",
        "Structured embedding transformations offer a promising approach for enhancing\nthe efficiency and coherence of language model inference. The introduction of\nStructural Embedding Projection (SEP) provides a mechanism for refining token\nrepresentations through projection matrices that integrate hierarchical and\nrelational dependencies. The mathematical formulation of SEP enables embedding\nspaces to capture structured contextual relationships, thereby improving\nsemantic fidelity without significantly increasing computational overhead.\nExperimental evaluations conducted on a range of linguistic datasets revealed\nthat SEP contributed to reductions in perplexity and enhanced contextual\ncoherence, demonstrating its potential to refine language model outputs.\nComputational efficiency assessments highlighted variations across different\ndatasets, suggesting that the integration of structured embeddings introduced\ndataset-dependent trade-offs between inference speed and representational\nrichness. The qualitative analysis of generated responses indicated that SEP\nenhanced narrative consistency and topic alignment, leading to improved fluency\nin multi-sentence text generation. The modifications to embedding layers\nrequired precise optimization to ensure stable training dynamics, as the\nintroduction of structured transformations altered the traditional\nrepresentation-learning process. The architectural adjustments necessary for\nSEP implementation influenced inference latency and memory consumption,\nrequiring a balance between efficiency gains and additional processing demands.\nThe impact of SEP on lexical diversity suggested that embedding modifications\ninfluenced the model's vocabulary usage, reflecting a more context-aware\nselection of generated tokens.",
        "Cybergrooming exploits minors through online trust-building, yet research\nremains fragmented, limiting holistic prevention. Social sciences focus on\nbehavioral insights, while computational methods emphasize detection, but their\nintegration remains insufficient. This review systematically synthesizes both\nfields using the PRISMA framework to enhance clarity, reproducibility, and\ncross-disciplinary collaboration. Findings show that qualitative methods offer\ndeep insights but are resource-intensive, machine learning models depend on\ndata quality, and standard metrics struggle with imbalance and cultural\nnuances. By bridging these gaps, this review advances interdisciplinary\ncybergrooming research, guiding future efforts toward more effective prevention\nand detection strategies.",
        "The rapid advancement of information and communication technology has\nfacilitated easier access to information. However, this progress has also\nnecessitated more stringent verification measures to ensure the accuracy of\ninformation, particularly within the context of Vietnam. This paper introduces\nan approach to address the challenges of Fact Verification using the Vietnamese\ndataset by integrating both sentence selection and classification modules into\na unified network architecture. The proposed approach leverages the power of\nlarge language models by utilizing pre-trained PhoBERT and XLM-RoBERTa as the\nbackbone of the network. The proposed model was trained on a Vietnamese\ndataset, named ISE-DSC01, and demonstrated superior performance compared to the\nbaseline model across all three metrics. Notably, we achieved a Strict Accuracy\nlevel of 75.11\\%, indicating a remarkable 28.83\\% improvement over the baseline\nmodel.",
        "Safety alignment mechanism are essential for preventing large language models\n(LLMs) from generating harmful information or unethical content. However,\ncleverly crafted prompts can bypass these safety measures without accessing the\nmodel's internal parameters, a phenomenon known as black-box jailbreak.\nExisting heuristic black-box attack methods, such as genetic algorithms, suffer\nfrom limited effectiveness due to their inherent randomness, while recent\nreinforcement learning (RL) based methods often lack robust and informative\nreward signals. To address these challenges, we propose a novel black-box\njailbreak method leveraging RL, which optimizes prompt generation by analyzing\nthe embedding proximity between benign and malicious prompts. This approach\nensures that the rewritten prompts closely align with the intent of the\noriginal prompts while enhancing the attack's effectiveness. Furthermore, we\nintroduce a comprehensive jailbreak evaluation framework incorporating\nkeywords, intent matching, and answer validation to provide a more rigorous and\nholistic assessment of jailbreak success. Experimental results show the\nsuperiority of our approach, achieving state-of-the-art (SOTA) performance on\nseveral prominent open and closed-source LLMs, including Qwen2.5-7B-Instruct,\nLlama3.1-8B-Instruct, and GPT-4o-0806. Our method sets a new benchmark in\njailbreak attack effectiveness, highlighting potential vulnerabilities in LLMs.\nThe codebase for this work is available at\nhttps:\/\/github.com\/Aegis1863\/xJailbreak.",
        "Research on cognitive biases and heuristics has become increasingly popular\nin the visualization literature in recent years. Researchers have studied the\neffects of biases on visualization interpretation and subsequent\ndecision-making. While this work is important, we contend that the view on\nbiases has presented human cognitive abilities in an unbalanced manner, placing\ntoo much emphasis on the flaws and limitations of human decision-making, and\npotentially suggesting that it should not be trusted. Several decision\nresearchers have argued that the flip side of biases -- i.e., mental shortcuts\nor heuristics -- demonstrate human ingenuity and serve as core markers of\nadaptive expertise. In this paper, we review the perspectives and sentiments of\nthe visualization community on biases and describe literature arguing for more\nbalanced views of biases and heuristics. We hope this paper will encourage\nvisualization researchers to consider a fuller picture of human cognitive\nlimitations and strategies for making decisions in complex environments."
      ]
    }
  },
  {
    "id":2411.00749,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"A 2021 update on cancer image analytics with deep learning",
    "start_abstract":"Deep learning (DL)-based interpretation of medical images has reached a critical juncture of expanding outside research projects into translational ones, and is ready to make its way to the clinics. Advances over the last decade in data availability, DL techniques, as well as computing capabilities have accelerated this journey. Through this journey, today we have a better understanding of the challenges to and pitfalls of wider adoption of DL into clinical care, which, according to us, should and will drive the advances in this field in the next few years. The most important among these challenges are the lack of an appropriately digitized environment within healthcare institutions, the lack of adequate open and representative datasets on which DL algorithms can be trained and tested, and the lack of robustness of widely used DL training algorithms to certain pervasive pathological characteristics of medical images and repositories. In this review, we provide an overview of the role of imaging in oncology, the different techniques that are shaping the way DL algorithms are being made ready for clinical use, and also the problems that DL techniques still need to address before DL can find a home in clinics. Finally, we also provide a summary of how DL can potentially drive the adoption of digital pathology, vendor neutral archives, and picture archival and communication systems. We caution that the respective researchers may find the coverage of their own fields to be at a high-level. This is so by design as this format is meant to only introduce those looking in from outside of deep learning and medical research, respectively, to gain an appreciation for the main concerns and limitations of these two fields instead of telling them something new about their own.",
    "start_categories":[
      "q-bio.CB"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "TransMIL: Transformer based Correlated Multiple Instance Learning for Whole Slide Image Classification"
      ],
      "abstract":[
        "Multiple instance learning (MIL) is a powerful tool to solve the weakly supervised classification in whole slide image (WSI) based pathology diagnosis. However, current MIL methods are usually on independent and identical distribution hypothesis, thus neglect correlation among different instances. To address this problem, we proposed new framework, called correlated MIL, provided proof for convergence. Based devised Transformer (TransMIL), which explored both morphological spatial information. The TransMIL can effectively deal with unbalanced\/balanced binary\/multiple great visualization interpretability. We conducted various experiments three computational problems achieved better performance faster convergence compared state-of-the-art methods. test AUC binary tumor be up 93.09% over CAMELYON16 dataset. And cancer subtypes 96.03% 98.82% TCGA-NSCLC dataset TCGA-RCC dataset, respectively. Implementation available at: https:\/\/github.com\/szc19990412\/TransMIL."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "SparAMX: Accelerating Compressed LLMs Token Generation on AMX-powered\n  CPUs",
        "VarGes: Improving Variation in Co-Speech 3D Gesture Generation via\n  StyleCLIPS",
        "Thermodynamics of driven systems with explicitly broken detailed balance",
        "How to Upscale Neural Networks with Scaling Law? A Survey and Practical\n  Guidelines",
        "Charge symmetry breaking in hypernuclei within RMF model",
        "The nonlinear limit of Babinet's Principle",
        "Structural and optical properties of in situ Eu-doped ZnCdO\/ZnMgO\n  superlattices grown by plasma-assisted molecular beam epitaxy",
        "Accenture-NVS1: A Novel View Synthesis Dataset",
        "CANUCS\/Technicolor: JWST Medium Band Photometry Finds Half of the Star\n  Formation at $z>7.5$ is Obscured",
        "Sparse wavefield reconstruction and denoising with boostlets",
        "Theoretical study of the Spectroscopic measurements of Kerr non-linear\n  resonators with four-body interaction",
        "Energy dynamics in a class of local random matrix Hamiltonians",
        "Attention Reallocation: Towards Zero-cost and Controllable Hallucination\n  Mitigation of MLLMs",
        "Pointwise estimates for the fundamental solutions of higher order\n  schr\\\"{o}dinger equations with finite rank perturbations",
        "3D ReX: Causal Explanations in 3D Neuroimaging Classification",
        "Spectroscopy of Supernova Remnants and Candidates in M31",
        "Learning to Unlearn while Retaining: Combating Gradient Conflicts in\n  Machine Unlearning",
        "Masking Countermeasures Against Side-Channel Attacks on Quantum\n  Computers",
        "Nonstatic Reissner-Nordstr$\\phi$m metric in the perturbative $f(R)$\n  theory: Embedding in the background of the FLRW cosmology, uniqueness of\n  solutions, the TOV equation",
        "Data Overvaluation Attack and Truthful Data Valuation",
        "Spatial Transcriptomics Analysis of Spatially Dense Gene Expression\n  Prediction",
        "Shadowing for Infinite Dimensional Dynamical Systems",
        "Quantum-Centric Algorithm for Sample-Based Krylov Diagonalization",
        "OmniParser V2: Structured-Points-of-Thought for Unified Visual Text\n  Parsing and Its Generality to Multimodal Large Language Models",
        "Characterising the Atacama segment of the Chile subduction margin\n  (24{\\deg}S-31{\\deg}S) with >165,000 earthquakes",
        "Milliwatt-level UV generation using sidewall poled lithium niobate",
        "An Attempt to Catch Up with JIT Compilers: The False Lead of Optimizing\n  Inline Caches",
        "Strategies for political-statement segmentation and labelling in\n  unstructured text",
        "Evidence of Galactic Interaction in the Small Magellanic Cloud Probed by\n  Gaia Selected Massive Star Candidates"
      ],
      "abstract":[
        "Large language models have high compute, latency, and memory requirements.\nWhile specialized accelerators such as GPUs and TPUs typically run these\nworkloads, CPUs are more widely available and consume less energy. Accelerating\nLLMs with CPUs enables broader AI access at a lower cost and power consumption.\nThis acceleration potential for CPUs is especially relevant during the\nmemory-bound decoding stage of LLM inference, which processes one token at a\ntime and is becoming increasingly utilized with reasoning models. We utilize\nAdvanced Matrix Extensions (AMX) support on the latest Intel CPUs together with\nunstructured sparsity to achieve a $1.42 \\times$ reduction in end-to-end\nlatency compared to the current PyTorch implementation by applying our\ntechnique in linear layers. We provide a set of open-source customized sparse\nkernels that can speed up any PyTorch model by automatically replacing all\nlinear layers with our custom sparse implementation. Furthermore, we\ndemonstrate for the first time the use of unstructured sparsity in the\nattention computation achieving a $1.14 \\times$ speedup over the current\nsystems without compromising accuracy. Code:\nhttps:\/\/github.com\/IntelLabs\/Hardware-Aware-Automated-Machine-Learning\/tree\/main\/SparAMX",
        "Generating expressive and diverse human gestures from audio is crucial in\nfields like human-computer interaction, virtual reality, and animation. Though\nexisting methods have achieved remarkable performance, they often exhibit\nlimitations due to constrained dataset diversity and the restricted amount of\ninformation derived from audio inputs. To address these challenges, we present\nVarGes, a novel variation-driven framework designed to enhance co-speech\ngesture generation by integrating visual stylistic cues while maintaining\nnaturalness. Our approach begins with the Variation-Enhanced Feature Extraction\n(VEFE) module, which seamlessly incorporates \\textcolor{blue}{style-reference}\nvideo data into a 3D human pose estimation network to extract StyleCLIPS,\nthereby enriching the input with stylistic information. Subsequently, we employ\nthe Variation-Compensation Style Encoder (VCSE), a transformer-style encoder\nequipped with an additive attention mechanism pooling layer, to robustly encode\ndiverse StyleCLIPS representations and effectively manage stylistic variations.\nFinally, the Variation-Driven Gesture Predictor (VDGP) module fuses MFCC audio\nfeatures with StyleCLIPS encodings via cross-attention, injecting this fused\ndata into a cross-conditional autoregressive model to modulate 3D human gesture\ngeneration based on audio input and stylistic clues. The efficacy of our\napproach is validated on benchmark datasets, where it outperforms existing\nmethods in terms of gesture diversity and naturalness. The code and video\nresults will be made publicly available upon\nacceptance:https:\/\/github.com\/mookerr\/VarGES\/ .",
        "In systems with detailed balance, the stationary distribution and the\nequilibrium distribution are identical, creating a clear connection between\nenergetic and entropic quantities. Many driven systems violate detailed balance\nand still pose a challenge for a consistent thermodynamic interpretation. Even\nsteady-state potentials like entropy or free energy are no longer state\nvariables. Here, we use a framework for systems with broken detailed balance,\nwhere Boltzmann entropy can be computed while properly taking constraints on\nstate transitions into account. As an illustration, we establish the\nthermodynamic relations for arbitrarily driven sample space-reducing processes\nthat are non-equilibrium but show steady states. We demonstrate that, despite\nexplicitly broken detailed balance, it remains feasible to define and\nunambiguously interpret the effective thermodynamic potentials.",
        "Neural scaling laws have revolutionized the design and optimization of\nlarge-scale AI models by revealing predictable relationships between model\nsize, dataset volume, and computational resources. Early research established\npower-law relationships in model performance, leading to compute-optimal\nscaling strategies. However, recent studies highlighted their limitations\nacross architectures, modalities, and deployment contexts. Sparse models,\nmixture-of-experts, retrieval-augmented learning, and multimodal models often\ndeviate from traditional scaling patterns. Moreover, scaling behaviors vary\nacross domains such as vision, reinforcement learning, and fine-tuning,\nunderscoring the need for more nuanced approaches. In this survey, we\nsynthesize insights from over 50 studies, examining the theoretical\nfoundations, empirical findings, and practical implications of scaling laws. We\nalso explore key challenges, including data efficiency, inference scaling, and\narchitecture-specific constraints, advocating for adaptive scaling strategies\ntailored to real-world applications. We suggest that while scaling laws provide\na useful guide, they do not always generalize across all architectures and\ntraining strategies.",
        "We study the charge symmetry breaking (CSB) effect in the binding energy of\nmirror hypernuclei in the mass region $A=7\\sim 48$ in relativistic mean field\n(RMF) models introducing $NN$ and $\\Lambda N$ interactions. The\nphenomenological $\\Lambda N$ CSB interaction is introduced and the strength\nparameter is fitted to reproduce the experimental binding energy difference\nbetween the mirror hypernuclei $^{12}_\\Lambda$B and $^{12}_\\Lambda$C. This\nmodel is applied to calculate the CSB energy anomaly in mirror hypernuclei with\nthe mass $A=7\\sim48$. The model is further applied to predict the binding\nenergy difference of mirror hypernuclei of $A$=40 with the isospin $T=1\/2$,\n$3\/2$ and $5\/2$ nuclei together with various hyper Ca isotopes and their mirror\nhypernuclei. Finally the binding energy systematics of $A=$48 hypernuclei are\npredicted with\/without the CSB effect by the PK1 and TM2 energy density\nfunctionals (EDFs).",
        "Babinet's principle is a powerful tool for predicting the scattering behavior\nof planar structures where the solution for the complementary structure is\nalready known. This makes it ubiquitous in the design of aperture antennas or\nmetamaterials. Even for plasmonic nanostructures, a qualitative match of the\nbehavior for complementary structures has been reported. Here, we discuss\nwhether Babinet's principle can be extended to nonlinear scattering. We compare\nthe third harmonic emission of plasmonic nanorods and complementary nanoslits\nby far field imaging and simulation. We find significantly different far field\nimages, in agreement between experiment and simulation. We explain these\ndifferences by the higher spatial resolution at the third harmonic wavelength\nand by additional eddy currents in slits that are not present in rods. Within\nthese limits, Babinet's principle can guide the design of inverted nonlinear\nplasmonic resonators, which promise to be more stable at high excitation power\ndue to better thermal conductivity.",
        "In situ Eu-doped ZnCdO-ZnMgO superlattices with varying ZnCdO:Eu and ZnMgO\nsublayers thicknesses were deposited by plasma assisted molecular beam epitaxy.",
        "This paper introduces ACC-NVS1, a specialized dataset designed for research\non Novel View Synthesis specifically for airborne and ground imagery. Data for\nACC-NVS1 was collected in Austin, TX and Pittsburgh, PA in 2023 and 2024. The\ncollection encompasses six diverse real-world scenes captured from both\nairborne and ground cameras, resulting in a total of 148,000 images. ACC-NVS1\naddresses challenges such as varying altitudes and transient objects. This\ndataset is intended to supplement existing datasets, providing additional\nresources for comprehensive research, rather than serving as a benchmark.",
        "We present a sample of 146 high-redshift ($z>7.5$) galaxies from the\nCANUCS\/Technicolor surveys, showcasing photometry in every wide- and\nmedium-band NIRCam filter in addition to ancillary HST data sampling $0.4-5 \\mu\nm$ (22 JWST bands out of 29 bands total). Additionally, 48 ($33\\%$) galaxies in\nour sample meet criteria to be classified as extreme emission line galaxies, 15\n($10\\%$) of which are completely missed by typical dropout selections due to\nfaint UV emission. By fitting the SEDs covering the rest-frame UV to optical at\n$z > 7.5$, we investigate the dust obscuration properties, giving an unbiased\nview of dust buildup in high-redshift galaxies free from spectroscopic\nfollow-up selection effects. Dust attenuation correlates with stellar mass, but\nmore strongly with star formation rate. We find typical galaxies at $z>7.5$\nhave $\\sim 25 \\%$ of their star formation obscured. However, since galaxies\nwith higher star formation rates suffer more attenuation, $\\sim 50 \\%$ of the\ntotal star formation rate density at $7.5<z<9$ is obscured. The obscured\nfraction drops to $\\sim 35 \\%$ in our $9<z<12$ bin, possibly due to substantial\ndust grain growth in the interstellar medium not having time to occur.\nExtrapolating the decline in dust obscuration of galaxies to higher redshifts,\nwe infer that dust obscuration should approach zero at $z > 15$, implying that\nepoch as when dust first forms in bright galaxies.",
        "Boostlets are spatiotemporal functions that decompose nondispersive\nwavefields into a collection of localized waveforms parametrized by dilations,\nhyperbolic rotations, and translations. We study the sparsity properties of\nboostlets and find that the resulting decompositions are significantly sparser\nthan those of other state-of-the-art representation systems, such as wavelets\nand shearlets. This translates into improved denoising performance when\nhard-thresholding the boostlet coefficients. The results suggest that boostlets\noffer a natural framework for sparsely decomposing wavefields in unified\nspace-time.",
        "Quantum annealing provides a promising way to solve combinational\noptimization problems where the solutions correspond to the ground state of the\nIsing Hamiltonian. We can implement quantum annealing using the Kerr non-linear\nresonators, with bifurcation phenomena emerging when subjected to a parametric\ndrive. These bifurcated states can function as bases of qubits. Moreover,\nintegrating four-body interactions between physical qubits enables the\nestablishment of effective all-to-all long-range interactions between logical\nqubits, which is essential for practical quantum annealing. While theoretical\nproposals exist for creating four-body interactions within Kerr non-linear\nresonators, there has not been experimental verification through their\nspectroscopic signatures. In this paper, we theoretically investigate the\nspectroscopic measurements of Kerr non-linear resonators featuring four-body\ninteraction. We identify six distinct frequencies exhibiting population changes\nby employing resonant driving on one resonator and weak driving on another.\nAnalytical and numerical calculations validate these findings. Our study\ndemonstrates the potential of spectroscopy in characterizing systems with\nfour-body interactions, offering insights for realizing quantum annealing with\nKerr parametric oscillators.",
        "Random matrix theory yields valuable insights into the universal features of\nquantum many-body chaotic systems. Although all-to-all interactions are\ntraditionally studied, many interesting dynamical questions, such as transport\nof a conserved density, require a notion of spatially local interactions. We\nstudy the transport of the energy, the most basic conserved density, in\nfew-body and 1D chains of nearest-neighbor random matrix terms that square to\none. In the few-body but large local Hilbert space dimension case, we develop a\nmapping for the energy dynamics to a single-particle hopping picture. This\nallows for the computation of the energy density autocorrelators and an\nout-of-time-ordered correlator of the energy density. In the 1D chain, we\nnumerically study the energy transport for a small local Hilbert space\ndimension. We also discuss the density of states throughout and touch upon the\nrelation to free probability theory.",
        "Multi-Modal Large Language Models (MLLMs) stand out in various tasks but\nstill struggle with hallucinations. While recent training-free mitigation\nmethods mostly introduce additional inference overhead via retrospection\nstrategy and contrastive decoding, we propose attention reallocation (AttnReal)\nto mitigate hallucinations with nearly zero extra cost. Our approach is\nmotivated by the key observations that, MLLM's unreasonable attention\ndistribution causes features to be dominated by historical output tokens, which\nfurther contributes to hallucinated responses because of the distribution gap\nbetween different token types. Based on the observations, AttnReal recycles\nexcessive attention from output tokens and reallocates it to visual tokens,\nwhich reduces MLLM's reliance on language priors and ensures the decoding\nprocess depends more on the visual inputs. More interestingly, we find that, by\ncontrolling the intensity of AttnReal, we can achieve a wide-range trade-off\nbetween the response faithfulness and overall performance. Comprehensive\nresults from different benchmarks validate the effectiveness of AttnReal across\nsix open-source MLLMs and three decoding strategies.",
        "This paper is dedicated to studying pointwise estimates of the fundamental\nsolution for the higher order Schr\\\"{o}dinger equation: % we investigate the\nfundamental solution of the higher order Schr\\\"{o}dinger equation\n$$i{\\partial}_{t}u(x,t)=Hu(x,t),\\ \\ \\ t\\in \\mathbb{R},\\ x\\in\n{\\mathbb{R}}^{n},$$ where the Hamiltonian $H$ is defined as\n$$H={(-\\Delta)}^{m}+\\displaystyle\\sum_{j=1}^{N} \\langle\\cdotp ,{\\varphi }_{j}\n\\rangle{\\varphi }_{j},$$ with each $\\varphi_j$ ($1\\le j\\le N$) satisfying\ncertain smoothness and decay conditions. %Let ${P}_{ac}(H)$ denote the\nprojection onto the absolutely continuous space of $H$. We show that for any\npositive integer $m>1$ and spatial dimension $n\\ge 1$, %under a spectral\nassumption, the operator is sharp in the sense that it\n  ${e}^{-i tH}P_{ac}(H)$ has an integral kernel $K(t,x,y)$ satisfying the\nfollowing pointwise estimate: $$\\left |K(t,x,y)\\right |\\lesssim\n|t|^{-\\frac{n}{2m}}(1+|t|^{-\\frac{1}{2m}}\\left | x-y\\right\n|)^{-\\frac{n(m-1)}{2m-1}} ,\\ \\ t\\ne 0,\\ x,y\\in {\\mathbb{R}}^{n}.$$ This\nestimate is consistent with the upper bounds in the free case. As an\napplication, we derive $L^p-L^q$ decay estimates for the propagator ${e}^{-\\i\ntH}P_{ac}(H)$, where the pairs $(1\/p, 1\/q)$ lie within a quadrilateral region\nin the plane.",
        "Explainability remains a significant problem for AI models in medical\nimaging, making it challenging for clinicians to trust AI-driven predictions.\nWe introduce 3D ReX, the first causality-based post-hoc explainability tool for\n3D models. 3D ReX uses the theory of actual causality to generate\nresponsibility maps which highlight the regions most crucial to the model's\ndecision. We test 3D ReX on a stroke detection model, providing insight into\nthe spatial distribution of features relevant to stroke.",
        "With a star formation rate of order 0.4 M$_\\odot $ yr$^{-1}$, M31 should have\nsignificant population of supernova remnants (SNRs), and, in fact, 156 SNR and\nSNR candidates have been suggested by Lee et al. (2014) by searching for\nnebulae with elevated [SII]\/H${\\alpha}$ ratios in narrow band images. Here we\nuse a combination of low and high resolution optical spectroscopy obtained with\nHectospec on the MMT to characterize 152 of these nebulae. Of these candidates,\nwe find 93 nebulae that have [SII]\/H${\\alpha}$ ratios that exceed 0.4, the\ntraditional ratio used to separate SNRs from HII regions, strongly suggesting\nthat at least these objects are SNRs. Our high resolution spectroscopy reveals\n108 nebulae that have velocity widths in H${\\alpha} $ (full-width at 20% peak\nflux) that exceed 50 km s$^{-1}$, significantly larger than found in HII\nregions. There are 72 objects that satisfy both tests. Here we discuss the\nspectroscopic characteristics of all of the objects in our sample, and the\nlikelihood that other objects in the sample of Lee et al. are also SNRs, and we\nbriefly consider confirmation by X-ray, radio and UV observations. We also\ndiscuss several new candidates that have been identified serendipitously in the\ncourse of examining a large amount of archival Hectospec data.",
        "Machine Unlearning has recently garnered significant attention, aiming to\nselectively remove knowledge associated with specific data while preserving the\nmodel's performance on the remaining data. A fundamental challenge in this\nprocess is balancing effective unlearning with knowledge retention, as naive\noptimization of these competing objectives can lead to conflicting gradients,\nhindering convergence and degrading overall performance. To address this issue,\nwe propose Learning to Unlearn while Retaining, aimed to mitigate gradient\nconflicts between unlearning and retention objectives. Our approach\nstrategically avoids conflicts through an implicit gradient regularization\nmechanism that emerges naturally within the proposed framework. This prevents\nconflicting gradients between unlearning and retention, leading to effective\nunlearning while preserving the model's utility. We validate our approach\nacross both discriminative and generative tasks, demonstrating its\neffectiveness in achieving unlearning without compromising performance on\nremaining data. Our results highlight the advantages of avoiding such gradient\nconflicts, outperforming existing methods that fail to account for these\ninteractions.",
        "We propose a modification to the transpiler of a quantum computer to\nsafeguard against side-channel attacks aimed at learning information about a\nquantum circuit. We demonstrate that if it is feasible to shield a specific\nsubset of gates from side-channel attacks, then it is possible to conceal all\ninformation in a quantum circuit by transpiling it into a new circuit whose\ndepth grows linearly, depending on the quantum computer's architecture. We\nprovide concrete examples of implementing this protection on IBM's quantum\ncomputers, utilizing their virtual gates and editing their transpiler.",
        "This article introduces a nonstatic Reissner-Nordstr$\\phi$m metric, a metric\nthat does not emit electromagnetic waves but can emit gravitational waves. We\nfirst use the GR theory to study a charged spherically symmetric gravitational\nsource (CSSGS), the obtained results are further improved in comparison with\nthe previous studies. In particular, this article considers that the field is\nnot necessarily static. The metric tensors $ g_{\\mu\\nu} $ are considered both\noutside and inside the gravitational source (the results show that in the first\ncase $ g_{\\mu\\nu} $ are time independent, in the latter case they are time\ndependent). The gravitational acceleration and the event horizon of a charged\nblack hole are investigated. The results prove that the gravitational field is\nalways attractive. We then use the perturbative $ f(R) $ theory to consider\nCSSGS. The obtained results not only correct the solution of Einstein's\nequation in magnitude (this will describe astronomical and cosmological\nquantities more accurately than Einstein's equation), but also reveal new\neffects. Outside the gravitational source, the metric tensors can depend on\ntime, this makes it possible for a spherically symmetric gravitational source\nto emit gravitational waves (Einstein's equation cannot give this effect).\nHowever, a spherically symmetric field still does not emit electromagnetic\nwaves. Next we present a new method for embedding the spherically symmetric\nmetrics of a star (or a black hole) in the background of the FLRW cosmological.\nFinally, we discuss the uniqueness of the solutions of the f(R) theory. The\nperturbative TOV equation is also found.",
        "In collaborative machine learning, data valuation, i.e., evaluating the\ncontribution of each client' data to the machine learning model, has become a\ncritical task for incentivizing and selecting positive data contributions.\nHowever, existing studies often assume that clients engage in data valuation\ntruthfully, overlooking the practical motivation for clients to exaggerate\ntheir contributions. To unlock this threat, this paper introduces the first\ndata overvaluation attack, enabling strategic clients to have their data\nsignificantly overvalued. Furthermore, we propose a truthful data valuation\nmetric, named Truth-Shapley. Truth-Shapley is the unique metric that guarantees\nsome promising axioms for data valuation while ensuring that clients' optimal\nstrategy is to perform truthful data valuation. Our experiments demonstrate the\nvulnerability of existing data valuation metrics to the data overvaluation\nattack and validate the robustness and effectiveness of Truth-Shapley.",
        "Spatial transcriptomics (ST) measures gene expression at fine-grained spatial\nresolution, offering insights into tissue molecular landscapes. Previous\nmethods for spatial gene expression prediction usually crop spots of interest\nfrom pathology tissue slide images, and learn a model that maps each spot to a\nsingle gene expression profile. However, it fundamentally loses spatial\nresolution of gene expression: 1) each spot often contains multiple cells with\ndistinct gene expression; 2) spots are cropped at fixed resolutions, limiting\nthe ability to predict gene expression at varying spatial scales. To address\nthese limitations, this paper presents PixNet, a dense prediction network\ncapable of predicting spatially resolved gene expression across spots of\nvarying sizes and scales directly from pathology images. Different from\nprevious methods that map individual spots to gene expression values, we\ngenerate a dense continuous gene expression map from the pathology image, and\naggregate values within spots of interest to predict the gene expression. Our\nPixNet outperforms state-of-the-art methods on 3 common ST datasets, while\nshowing superior performance in predicting gene expression across multiple\nspatial scales. The source code will be publicly available.",
        "In this paper we extend some results about Shadowing Lemma there are known on\nfinite dimensional compact manifolds without border and $\\mathbb{R}^n$, to an\ninfinite dimensional space. In fact, we prove that if $\\{\\mathcal{T}(t):t\\ge\n0\\}$ is a Morse-Smale semigroup defined in a Hilbert space with global\nattractor $\\mathcal{A}$, then $\\mathcal{T}(1)|_{\\mathcal{A}}:\\mathcal{A}\\to\n\\mathcal{A} $ admits the Lipschitz Shadowing property. Moreover, for any\npositively invariant bounded neighborhood $\\mathcal{U}\\supset\\mathcal{A}$ of\nthe global attractor, the map $\\mathcal{T}(1)|_{\\mathcal{U}}:\\mathcal{U}\\to\n\\mathcal{U}$ has the H\\\"{o}lder-Shadowing property. As applications, we obtain\nnew results related to the structural stability of Morse-Smale semigroups\ndefined in Hilbert spaces and continuity of global attractors.",
        "Approximating the ground state of many-body systems is a key computational\nbottleneck underlying important applications in physics and chemistry. It has\nlong been viewed as a promising application for quantum computers. The most\nwidely known quantum algorithm for ground state approximation, quantum phase\nestimation, is out of reach of current quantum processors due to its high\ncircuit-depths. Quantum diagonalization algorithms based on subspaces represent\nalternatives to phase estimation, which are feasible for pre-fault-tolerant and\nearly-fault-tolerant quantum computers. Here, we introduce a quantum\ndiagonalization algorithm which combines two key ideas on quantum subspaces: a\nclassical diagonalization based on quantum samples, and subspaces constructed\nwith quantum Krylov states. We prove that our algorithm converges in polynomial\ntime under the working assumptions of Krylov quantum diagonalization and\nsparseness of the ground state. We then show numerical investigations of\nlattice Hamiltonians, which indicate that our method can outperform existing\nKrylov quantum diagonalization in the presence of shot noise, making our\napproach well-suited for near-term quantum devices. Finally, we carry out the\nlargest ground-state quantum simulation of the single-impurity Anderson model\non a system with $41$ bath sites, using $85$ qubits and up to $6 \\cdot 10^3$\ntwo-qubit gates on a Heron quantum processor, showing excellent agreement with\ndensity matrix renormalization group calculations.",
        "Visually-situated text parsing (VsTP) has recently seen notable advancements,\ndriven by the growing demand for automated document understanding and the\nemergence of large language models capable of processing document-based\nquestions. While various methods have been proposed to tackle the complexities\nof VsTP, existing solutions often rely on task-specific architectures and\nobjectives for individual tasks. This leads to modal isolation and complex\nworkflows due to the diversified targets and heterogeneous schemas. In this\npaper, we introduce OmniParser V2, a universal model that unifies VsTP typical\ntasks, including text spotting, key information extraction, table recognition,\nand layout analysis, into a unified framework. Central to our approach is the\nproposed Structured-Points-of-Thought (SPOT) prompting schemas, which improves\nmodel performance across diverse scenarios by leveraging a unified\nencoder-decoder architecture, objective, and input\\&output representation. SPOT\neliminates the need for task-specific architectures and loss functions,\nsignificantly simplifying the processing pipeline. Our extensive evaluations\nacross four tasks on eight different datasets show that OmniParser V2 achieves\nstate-of-the-art or competitive results in VsTP. Additionally, we explore the\nintegration of SPOT within a multimodal large language model structure, further\nenhancing text localization and recognition capabilities, thereby confirming\nthe generality of SPOT prompting technique. The code is available at\n\\href{https:\/\/github.com\/AlibabaResearch\/AdvancedLiterateMachinery}{AdvancedLiterateMachinery}.",
        "The Atacama segment in Northern Chile (24{\\deg}S to 31{\\deg}S) is a mature\nseismic gap with no major event (Mw>8) since 1922. In addition to regular\nseismicity, around the subducting Copiap\\'o ridge, the region hosts seismic\nswarms, and shallow and deep slow slip events. To characterize the fine\nstructure of this seismic gap and its seismic-aseismic interplay, we\ninstrumented the region with almost 200 seismic and geodetic stations. Using\nmachine learning, we derived a dense, high-resolution seismicity catalog,\nencompassing over 165,000 events with double-difference relocated hypocenters.\nOur catalog details the outer rise, interface, intraslab, crustal and mantle\nwedge seismicity. We infer a detailed slab geometry, showing that the flat slab\nis dipping towards the south with a narrower extent along dip. The slab\ngeometry controls the intraslab seismicity, with cross-cutting activity in the\nregion of highest bending and a downdip limit around 105 km slab depth. Our\ncatalogue exhibits significant seismicity in the mantle wedge upper corner\nbetween 28{\\deg}S and 31{\\deg}S, highlighting the brittle behavior of the cold\nnose. On the subduction interface, interplate locking controls the updip end of\nthe seismicity, with seismicity extending closer to the trench in low-locking\nareas. On fine scales, resolved by relative uncertainties below 50 m, the\nsubduction interface has a complex 3D structure, showing a fractal distribution\nof seismic patches down to a scale of tens of meters. Our results provide a\nholistic view of this complex subduction zone, while at the same time giving\ninsights into fine-scale structures and processes.",
        "Integrated coherent sources of ultra-violet (UV) light are essential for a\nwide range of applications, from ion-based quantum computing and optical clocks\nto gas sensing and microscopy. Conventional approaches that rely on UV gain\nmaterials face limitations in terms of wavelength versatility; in response\nfrequency upconversion approaches that leverage various optical nonlinearities\nhave received considerable attention. Among these, the integrated thin-film\nlithium niobate (TFLN) photonic platform shows particular promise owing to\nlithium niobate's transparency into the UV range, its strong second order\nnonlinearity, and high optical confinement. However, to date, the high\npropagation losses and lack of reliable techniques for consistent poling of\ncm-long waveguides with small poling periods have severely limited the utility\nof this platform. Here we present a sidewall poled lithium niobate (SPLN)\nwaveguide approach that overcomes these obstacles and results in a more than\ntwo orders of magnitude increase in generated UV power compared to the\nstate-of-the-art. Our UV SPLN waveguides feature record-low propagation losses\nof 2.3 dB\/cm, complete domain inversion of the waveguide cross-section, and an\noptimum 50% duty cycle, resulting in a record-high normalized conversion\nefficiency of 5050 %W$^{-1}$cm$^{-2}$, and 4.2 mW of generated on-chip power at\n390 nm wavelength. This advancement makes the TFLN photonic platform a viable\noption for high-quality on-chip UV generation, benefiting emerging\napplications.",
        "Context: Just-in-Time (JIT) compilers are able to specialize the code they\ngenerate according to a continuous profiling of the running programs. This\ngives them an advantage when compared to Ahead-of-Time (AoT) compilers that\nmust choose the code to generate once for all.\n  Inquiry: Is it possible to improve the performance of AoT compilers by adding\nDynamic Binary Modification (DBM) to the executions?\n  Approach: We added to the Hopc AoT JavaScript compiler a new optimization\nbased on DBM to the inline cache (IC), a classical optimization dynamic\nlanguages use to implement object property accesses efficiently.\n  Knowledge: Reducing the number of memory accesses as the new optimization\ndoes, does not shorten execution times on contemporary architectures.\n  Grounding: The DBM optimization we have implemented is fully operational on\nx86_64 architectures. We have conducted several experiments to evaluate its\nimpact on performance and to study the reasons of the lack of acceleration.\n  Importance: The (negative) result we present in this paper sheds new light on\nthe best strategy to be used to implement dynamic languages. It tells that the\nold days were removing instructions or removing memory reads always yielded to\nspeed up is over. Nowadays, implementing sophisticated compiler optimizations\nis only worth the effort if the processor is not able by itself to accelerate\nthe code. This result applies to AoT compilers as well as JIT compilers.",
        "Analysis of parliamentary speeches and political-party manifestos has become\nan integral area of computational study of political texts. While speeches have\nbeen overwhelmingly analysed using unsupervised methods, a large corpus of\nmanifestos with by-statement political-stance labels has been created by the\nparticipants of the MARPOR project. It has been recently shown that these\nlabels can be predicted by a neural model; however, the current approach relies\non provided statement boundaries, limiting out-of-domain applicability. In this\nwork, we propose and test a range of unified split-and-label frameworks --\nbased on linear-chain CRFs, fine-tuned text-to-text models, and the combination\nof in-context learning with constrained decoding -- that can be used to jointly\nsegment and classify statements from raw textual data. We show that our\napproaches achieve competitive accuracy when applied to raw text of political\nmanifestos, and then demonstrate the research potential of our method by\napplying it to the records of the UK House of Commons and tracing the political\ntrajectories of four major parties in the last three decades.",
        "We present identifications and kinematic analysis of 7,426 massive\n($\\mathrm{\\geq}8M_{\\odot}$) stars in the Small Magellanic Cloud (SMC), using\nGaia DR3 data. We used Gaia ($G_\\mathrm{BP}-G_\\mathrm{RP}$, $G$)\ncolor-magnitude diagram to select the population of massive stars, and parallax\nto omit foreground objects. The spatial distribution of the 7,426 massive star\ncandidates is generally consistent with the spatial distribution of the\ninterstellar medium, such as H$\\alpha$ and H i emission. The identified massive\nstars show inhomogeneous distributions over the galaxy, showing several\nsuperstructures formed by massive stars with several hundred parsecs scale. The\nstellar superstructures defined by the surface density have opposite mean\nproper motions in the east and west, moving away from each other. Similarly,\nthe mean line-of-sight velocities of the superstructures are larger to the\nsoutheast and smaller to the northwest. The different east-west properties of\nthe superstructures' proper motion, line-of-sight velocity indicate that the\nSMC is being stretched by tidal forces and\/or ram pressure from the Large\nMagellanic Cloud to the southeast, thereby rejecting the presence of galaxy\nrotation in the SMC."
      ]
    }
  },
  {
    "id":2411.00749,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"TransMIL: Transformer based Correlated Multiple Instance Learning for Whole Slide Image Classification",
    "start_abstract":"Multiple instance learning (MIL) is a powerful tool to solve the weakly supervised classification in whole slide image (WSI) based pathology diagnosis. However, current MIL methods are usually on independent and identical distribution hypothesis, thus neglect correlation among different instances. To address this problem, we proposed new framework, called correlated MIL, provided proof for convergence. Based devised Transformer (TransMIL), which explored both morphological spatial information. The TransMIL can effectively deal with unbalanced\/balanced binary\/multiple great visualization interpretability. We conducted various experiments three computational problems achieved better performance faster convergence compared state-of-the-art methods. test AUC binary tumor be up 93.09% over CAMELYON16 dataset. And cancer subtypes 96.03% 98.82% TCGA-NSCLC dataset TCGA-RCC dataset, respectively. Implementation available at: https:\/\/github.com\/szc19990412\/TransMIL.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "A 2021 update on cancer image analytics with deep learning"
      ],
      "abstract":[
        "Deep learning (DL)-based interpretation of medical images has reached a critical juncture of expanding outside research projects into translational ones, and is ready to make its way to the clinics. Advances over the last decade in data availability, DL techniques, as well as computing capabilities have accelerated this journey. Through this journey, today we have a better understanding of the challenges to and pitfalls of wider adoption of DL into clinical care, which, according to us, should and will drive the advances in this field in the next few years. The most important among these challenges are the lack of an appropriately digitized environment within healthcare institutions, the lack of adequate open and representative datasets on which DL algorithms can be trained and tested, and the lack of robustness of widely used DL training algorithms to certain pervasive pathological characteristics of medical images and repositories. In this review, we provide an overview of the role of imaging in oncology, the different techniques that are shaping the way DL algorithms are being made ready for clinical use, and also the problems that DL techniques still need to address before DL can find a home in clinics. Finally, we also provide a summary of how DL can potentially drive the adoption of digital pathology, vendor neutral archives, and picture archival and communication systems. We caution that the respective researchers may find the coverage of their own fields to be at a high-level. This is so by design as this format is meant to only introduce those looking in from outside of deep learning and medical research, respectively, to gain an appreciation for the main concerns and limitations of these two fields instead of telling them something new about their own."
      ],
      "categories":[
        "q-bio.CB"
      ]
    },
    "list":{
      "title":[
        "Weight Distribution of the Weighted Coordinates Poset Block Space and\n  Singleton Bound",
        "Feedback-augmented Non-homogeneous Hidden Markov Models for Longitudinal\n  Causal Inference",
        "Nonlocal characteristics of two-qubit gates and their argand diagrams",
        "Stability of the Euclidean 3-ball under L2-curvature pinching",
        "Observational and Theoretical Constraints on First-Order Phase\n  Transitions in Neutron Stars",
        "Breather interactions in the integrable discrete Manakov system and\n  trigonometric Yang-Baxter maps",
        "Structure evolution with cosmic backgrounds from radio to far infrared",
        "A Family of Semi-norms in $C^*$-algebras",
        "An Iterative Block Matrix Inversion (IBMI) Algorithm for Symmetric\n  Positive Definite Matrices with Applications to Covariance Matrices",
        "Mean-Field Analysis of Latent Variable Process Models on Dynamically\n  Evolving Graphs with Feedback Effects",
        "Dirichlet problem for diffusions with jumps",
        "Remote preparation of motional Schr\\\"{o}dinger cat states via\n  dissipatively-driven non-Gaussian mechanical entanglement",
        "Frequency-resolved time lags due to X-ray disk reprocessing in AGN",
        "The rise of the galactic empire: luminosity functions at $z\\sim17$ and\n  $z\\sim25$ estimated with the MIDIS$+$NGDEEP ultra-deep JWST\/NIRCam dataset",
        "Subdomains of Post-COVID-Syndrome (PCS) -- A Population-Based Study",
        "Function-Coherent Gambles",
        "Generators of the algebraic symplectic bordism ring",
        "Improvements to monoscopic analysis for imaging atmospheric Cherenkov\n  telescopes: Application to H.E.S.S",
        "Theory of Irreversibility in Quantum Many-Body Systems",
        "Strictly Metrizable Graphs are Minor-Closed",
        "Supersymmetry in nonlinear and linear Quantum Optics: the Kerr-like and\n  multiphoton Jaynes-Cummings models",
        "Electron scale magnetic holes generation driven by Whistler-to-Bernstein\n  mode conversion in fully kinetic plasma turbulence",
        "Gradient-free Importance Sampling Scheme for Efficient Reliability\n  Estimation",
        "Ensemble-averaged mean-field many-body level density: an indicator of\n  integrable versus chaotic single-particle dynamics",
        "Deep Learning-Based Diffusion MRI Tractography: Integrating Spatial and\n  Anatomical Information",
        "Geometric Properties of Periodic Lattices in Function Fields",
        "Consonance in music -- the Pythagorean approach revisited",
        "Perturbing finite temperature multicomponent DFT 1D Kohn-Sham systems:\n  Peierls Gap & Kohn Anomaly",
        "Rough estimates of solar system gravitomagnetic effects in\n  post-Newtonian gravity"
      ],
      "abstract":[
        "In this paper, we determine the complete weight distribution of the space $\n\\mathbb{F}_q^N $ endowed by the weighted coordinates poset block metric\n($(P,w,\\pi)$-metric), also known as the $(P,w,\\pi)$-space, thereby obtaining it\nfor $(P,w)$-space, $(P,\\pi)$-space, $\\pi$-space, and $P$-space as special\ncases. Further, when $P$ is a chain, the resulting space is called as\nNiederreiter-Rosenbloom-Tsfasman (NRT) weighted block space and when $P$ is\nhierarchical, the resulting space is called as weighted coordinates\nhierarchical poset block space. The complete weight distribution of both the\nspaces are deduced from the main result. Moreover, we define an $I$-ball for an\nideal $I$ in $P$ and study the characteristics of it in $(P,w,\\pi)$-space.\n  We investigate the relationship between the $I$-perfect codes and $t$-perfect\ncodes in $(P,w,\\pi)$-space. Given an ideal $I$, we investigate how the maximum\ndistance separability (MDS) is related with $I$-perfect codes and $t$-perfect\ncodes in $(P,w,\\pi)$-space. Duality theorem is derived for an MDS\n$(P,w,\\pi)$-code when all the blocks are of same length. Finally, the\ndistribution of codewords among $r$-balls is analyzed in the case of chain\nposet, when all the blocks are of same length.",
        "Hidden Markov models are widely used for modeling sequential data but\ntypically have limited applicability in observational causal inference due to\ntheir strong conditional independence assumptions. I introduce\nfeedback-augmented non-homogeneous hidden Markov model (FAN-HMM), which\nincorporate time-varying covariates and feedback mechanisms from past\nobservations to latent states and future responses. Integrating these models\nwith the structural causal model framework allows flexible causal inference in\nlongitudinal data with time-varying unobserved heterogeneity and multiple\ncausal pathways. I show how, in a common case of categorical response\nvariables, long-term causal effects can be estimated efficiently without the\nneed for simulating counterfactual trajectories. Using simulation experiments,\nI study the performance of FAN-HMM under the common misspecification of the\nnumber of latent states, and finally apply the proposed approach to estimate\nthe effect of the 2013 parental leave reform on fathers' paternal leave uptake\nin Finnish workplaces.",
        "In this paper, we show the usefulness of the chords present in the argand\ndiagram of squared eigenvalues of nonlocal part of two-qubit gates to study\ntheir nonlocal characteristics. We discuss the criteria for perfect entanglers\nto transform a pair of orthonormal product states into a pair of orthonormal\nmaximally entangled states. Perfect entanglers with a chord passing through\norigin can do such a transformation. In the Weyl chamber, we identify the\nregions of perfect entanglers with at least one chord passing through origin.\nWe also provide the conditions for a perfect entangler without any chord\npassing through origin to transform a pair of orthonormal product states into\northonormal maximally entangled states. Finally, we show that similar to\nentangling power, gate typicality can also be described using the chords\npresent in the argand diagram. For each chord describing the entangling power,\nthere exists a chord describing the gate typicality. We show the geometrical\nrelation between the two sets of chords.",
        "In this article, we consider compact Riemannian 3-manifolds with boundary. We\nprove that if the $L^2$-norm of the curvature is small and if the\n$H^{1\/2}$-norm of the difference of the fundamental forms of the boundary is\nsmall, then the manifold is diffeomorphic to the Euclidean ball. Moreover, we\nobtain that the manifold and the ball are metrically close (uniformly and in\n$H^2$-norm), with a quantitative, optimal bound. The required smallness\nassumption only depends on the volumes of the manifold and its boundary and on\na trace and Sobolev constant of the manifold. The proof only relies on\nelementary computations based on the Bochner formula for harmonic functions and\ntensors, and on the 2-spheres effective uniformisation result of\nKlainerman-Szeftel.",
        "Understanding the equation of state (EOS) of neutron stars (NSs) is a\nfundamental challenge in astrophysics and nuclear physics. A first-order phase\ntransition (FOPT) at high densities could lead to the formation of a quark\ncore, significantly affecting NS properties. This review explores observational\nand theoretical constraints on such transitions using multi-messenger\nastrophysics. X-ray observations, including mass-radius measurements from NICER\nand spectral features like quasi-periodic oscillations (QPOs) and cyclotron\nresonance scattering features (CRSFs), provide indirect evidence of EOS\nmodifications. Gravitational wave detections, particularly from binary NS\nmergers such as GW170817, constrain tidal deformability and post-merger\noscillations, which may carry signatures of phase transitions. Pulsar timing\noffers additional constraints through measurements of mass, spin evolution, and\nglitches, with millisecond pulsars exceeding twice the solar mass posing\nchallenges to purely hadronic EOSs. Theoretical models and numerical\nsimulations predict that an FOPT could impact gravitational wave signals,\ntwin-star configurations, and NS cooling. Future advancements, including\nnext-generation gravitational wave detectors, high-precision X-ray telescopes,\nand improved theoretical modeling, will enhance our ability to probe phase\ntransitions in NSs. A combination of these approaches will provide crucial\ninsights into the existence and properties of deconfined quark matter in NS\ninteriors.",
        "The goal of this work is to obtain a complete characterization of soliton and\nbreather interactions in the integrable discrete Manakov (IDM) system, a vector\ngeneralization of the Ablowitz-Ladik model. The IDM system, which in the\ncontinuous limit reduces to the Manakov system (i.e., a 2-component vector\nnonlinear Schrodinger equation), was shown to admit a variety of discrete\nvector soliton solutions: fundamental solitons, fundamental breathers, and\ncomposite breathers. While the interaction of fundamental solitons was studied\nearly on, no results are presently available for other types of\nsoliton-breather and breather-breather interactions. Our study reveals that\nupon interacting with a fundamental breather, a fundamental soliton becomes a\nfundamental breather. Conversely, the interaction of two fundamental breathers\ngenerically yields two fundamental breathers with polarization shifts, but may\nalso result in a fundamental soliton and a fundamental breather. Composite\nbreathers interact trivially both with each other and with a fundamental\nsoliton or breather. Explicit formulas for the scattering coefficients that\ncharacterize fundamental and composite breathers are given. This allows us to\ninterpret the interactions in terms of a refactorization problem and derive the\nassociated Yang-Baxter maps describing the effect of interactions on the\npolarizations. These give the first examples of parametric Yang-Baxter maps of\ntrigonometric type.",
        "Cosmic background radiation, both diffuse and discrete in nature, produced at\ndifferent cosmic epochs before and after recombination, provides key\ninformation on the evolution of cosmic structures. We discuss the main classes\nof sources that contribute to the extragalactic background light from radio to\nsub-millimetre wavelenghs and the currently open question on the level of the\ncosmic radio background spectrum. The redshifted 21cm line signal from\ncosmological neutral Hydrogen during the primeval phases of cosmic structures\nas a probe of the cosmological reionisation process is presented, along with\nthe route for confident detection of this signal. We then describe the basic\nformalism and the feasibility to study via a differential approach, based\nmainly on dipole analysis, the tiny imprints in the CB spectrum expected from a\nvariety of cosmological and astrophysical processes at work during the early\nphases of cosmic perturbation and structure evolution. Finally, we discuss the\nidentification of high-redshift sub-millimetre lensed galaxies with extreme\nmagnifications in the Planck maps and their use for the comprehension of\nfundamental processes in early galaxy formation and evolution.",
        "We introduce a new family of non-negative real-valued functions on a\n$C^*$-algebra $\\mathcal{A}$, i.e., for $0\\leq \\mu \\leq 1,$\n$$\\|a\\|_{\\sigma_{\\mu}}= \\text{sup}\\left\\lbrace \\sqrt{|f(a)|^2 \\sigma_{\\mu}\nf(a^*a)}: f\\in \\mathcal{A}', \\, f(1)=\\|f\\|=1 \\right\\rbrace, \\quad $$ where\n$a\\in \\mathcal{A}$ and $\\sigma_{\\mu}$ is an interpolation path of the symmetric\nmean $\\sigma$. These functions are semi-norms as they satisfy the norm axioms,\nexcept for the triangle inequality. Special cases satisfying triangle\ninequality, and a complete equality characterization is also discussed. Various\nbounds and relationships will be established for this new family, with a\nconnection to the existing literature in the algebra of all bounded linear\noperators on a Hilbert space.",
        "Obtaining the inverse of a large symmetric positive definite matrix\n$\\mathcal{A}\\in\\mathbb{R}^{p\\times p}$ is a continual challenge across many\nmathematical disciplines. The computational complexity associated with direct\nmethods can be prohibitively expensive, making it infeasible to compute the\ninverse. In this paper, we present a novel iterative algorithm (IBMI), which is\ndesigned to approximate the inverse of a large, dense, symmetric positive\ndefinite matrix. The matrix is first partitioned into blocks, and an iterative\nprocess using block matrix inversion is repeated until the matrix approximation\nreaches a satisfactory level of accuracy. We demonstrate that the two-block,\nnon-overlapping approach converges for any positive definite matrix, while\nnumerical results provide strong evidence that the multi-block, overlapping\napproach also converges for such matrices.",
        "In this paper, we study the asymptotic behavior of a class of dynamic\nco-evolving latent space networks. The model we study is subject to\nbi-directional feedback effects, meaning that at any given time, the latent\nprocess depends on its own value and the graph structure at the previous time\nstep, and the graph structure at the current time depends on the value of the\nlatent processes at the current time but also on the graph structure at the\nprevious time instance (sometimes called a persistence effect). We construct\nthe mean-field limit of this model, which we use to characterize the limiting\nbehavior of a random sample taken from the latent space network in the limit as\nthe number of nodes in the network diverges. From this limiting model, we can\nderive the limiting behavior of the empirical measure of the latent process and\nestablish the related graphon limit of the latent particle network process. We\nalso provide a description of the rich conditional probabilistic structure of\nthe limiting model. The inherent dependence structure complicates the\nmathematical analysis significantly. In the process of proving our main\nresults, we derive a general conditional propagation of chaos result, which is\nof independent interest. In addition, our novel approach of studying the\nlimiting behavior of random samples proves to be a very useful methodology for\nfully grasping the asymptotic behavior of co-evolving particle systems.\nNumerical results are included to illustrate the theoretical findings.",
        "In this paper, we study Dirichlet problem for non-local operator on bounded\ndomains in ${\\mathbb R}^d$\n  $$\n  {\\cal L}u = {\\rm div}(A(x) \\nabla (x)) + b(x) \\cdot \\nabla u(x)\n  + \\int_{{\\mathbb R}^d} (u(y)-u(x) ) J(x, dy) , $$ where\n$A(x)=(a_{ij}(x))_{1\\leq i,j\\leq d}$ is a measurable $d\\times d$ matrix-valued\nfunction on ${\\mathbb R}^d$ that is uniformly elliptic and bounded, $b$ is an\n${\\mathbb R}^d$-valued function so that $|b|^2$ is in some Kato class ${\\mathbb\nK}_d$, for each $x\\in {\\mathbb R}^d$, $J(x, dy)$ is a finite measure on\n${\\mathbb R}^d$ so that $x\\mapsto J(x, {\\mathbb R}^d)$ is in the Kato class\n${\\mathbb K}_d$. We show there is a unique Feller process $X$ having strong\nFeller property associated with ${\\cal L}$, which can be obtained from the\ndiffusion process having generator $ {\\rm div}(A(x) \\nabla (x)) + b(x) \\cdot\n\\nabla u(x) $ through redistribution. We further show that for any bounded\nconnected open subset $D\\subset{\\mathbb R}^d$ that is regular with respect to\nthe Laplace operator $\\Delta$ and for any bounded continuous function $\\varphi\n$ on $D^c$, the Dirichlet problem ${\\cal L} u=0$ in $D$ with $u=\\varphi$ on\n$D^c$ has a unique bounded continuous weak solution on ${\\mathbb R}^d$. This\nunique weak solution can be represented in terms of the Feller process\nassociated with ${\\cal L}$.",
        "In this paper, we propose a driven-dissipative scheme for generating\nnon-Gaussian mechanical entangled states and remotely preparing mechanical\nSchr\\\"{o}dinger cat states via the entanglement. The system under study\nconsists of a cavity optomechanical setup with two frequency-mismatched\nmechanical oscillators coupled to a cavity field driven by a bichromatic pump.\nWe show that under proper conditions, an effective Hamiltonian for\nnondegenerate parametric downconversion involving the two mechanical\noscillators and the cavity field can be engineered. We demonstrate analytically\nand numerically that the cavity dissipation drives the mechanical oscillators\ninto a steady-state pair-coherent state. The no-Gaussianity and nonclassical\nproperties, including Winger negativity, entanglement and quantum steering, of\nthe achieved non-Gaussian mechanical state are investigated in detail. We\nfurther show that homodyne detection on one mechanical oscillator enables the\nremote generation of Schr\\\"{o}dinger cat states in the other oscillator through\nthe non-Gaussian mechanical entanglement. As we show, this detection can be\nimplemented by transferring the mechanical state to the output field of an\nauxiliary probe cavity coupled to the target oscillator, followed by homodyne\ndetection on the output field. We also discuss the robustness of the mechanical\nentangled states and cat states against thermal fluctuations. Our findings\nestablish a feasible approach for the dissipative and remote preparation of\nmechanical nonclassical states.",
        "Over the last years, a number of broadband reverberation mapping campaigns\nhave been conducted to explore the short-term UV and optical variability of\nnearby AGN. Despite the extensive data collected, the origin of the observed\nvariability is still debated in the literature. Frequency-resolved time lags\noffer a promising approach to distinguish between different scenarios, as they\nprobe variability on different time scales. In this study, we present the\nexpected frequency-resolved lags resulting from X-ray reprocessing in the\naccretion disk. The predicted lags are found to feature a general shape that\nresembles that of observational measurements, while exhibiting strong\ndependence on various physical parameters. Additionally, we compare our model\npredictions to observational data for the case of NGC 5548, concluding that the\nX-ray illumination of the disk can effectively account for the observed\nfrequency-resolved lags and power spectra in a self-consistent way. To date,\nX-ray disk reprocessing is the only physical model that has successfully\nreproduced the observed multi-wavelength variability, in both amplitude and\ntime delays, across a range of temporal frequencies.",
        "We present a sample of six F200W and three F277W dropout sources identified\nas $16<z<25$ galaxy candidates based on the deepest JWST\/NIRCam data to date,\nprovided by the MIRI Deep Imaging Survey (MIDIS) and the Next Generation Deep\nExtragalactic Exploratory Public survey (NGDEEP), reaching 5$\\sigma$ depths of\n$\\sim31.5$ mag (AB) at $\\geq2$ $\\mu$m. We estimate ultraviolet (UV) luminosity\nfunctions and densities at $z\\sim17$ and $z\\sim25$. We find that the number\ndensity of galaxies with absolute magnitudes $-19<M_\\mathrm{UV}<-18$ (AB) at\n$z\\sim17$ ($z\\sim25$) is a factor of 4 (25) smaller than at $z\\sim12$; a\nsimilar evolution is observed for the luminosity density. Compared to\nstate-of-the-art galaxy simulations, we find the need for an enhanced UV-photon\nproduction at $z=17-25$ in $\\mathrm{M}_\\mathrm{DM}=10^{8.5-9.5}$ M$_\\odot$ dark\nmatter halos, maybe provided by an increase in the star formation efficiency at\nearly times and\/or by intense bursts fed by very low metallicity or primordial\ngas. There are few robust theoretical predictions for the evolution of galaxies\nabove $z\\sim20$ in the literature, however, the continuing rapid drop in the\nhalo mass function suggests more rapid evolution than we observe if photon\nproduction efficiencies remained constant. Our $z>16$ galaxy candidates present\nmass-weighted ages around 30 Myr, and attenuations $\\mathrm{A(V)}<0.1$ mag.\nTheir average stellar mass is\n$\\mathrm{M}_\\bigstar\\sim10^{7}\\,\\mathrm{M}_\\odot$, implying a star formation\nefficiency (stellar-to-baryon mass fraction) around 10%. We find three galaxies\nwith very blue UV spectral slopes ($\\beta\\sim-3$) compatible with low\nmetallicity or Pop III and young ($\\lesssim10$ Myr) stars and\/or high escape\nfractions of ionizing photons, the rest presenting slopes $\\beta\\sim-2.5$\nsimilar to $z=10-12$ samples.",
        "Post-COVID Syndrome (PCS), encompassing the multifaceted sequelae of\nCOVID-19, can be severity-graded using a score comprising 12 different\nlong-term symptom complexes. Acute COVID-19 severity and individual resilience\nwere previously identified as key predictors of this score. This study\nvalidated these predictors and examined their relationship to PCS symptom\ncomplexes, using an expanded dataset (n=3,372) from the COVIDOM cohort study.\nClassification and Regression Tree (CART) analysis resolved the detailed\nrelationship between the predictors and the constituting symptom complexes of\nthe PCS score. Among newly recruited COVIDOM participants (n=1,930), the PCS\nscore was again found to be associated with both its putative predictors. Of\nthe score-constituting symptom complexes, neurological symptoms, sleep\ndisturbance, and fatigue were predicted by individual resilience, whereas acute\ndisease severity predicted exercise intolerance, chemosensory deficits, joint\nor muscle pain, signs of infection, and fatigue. These associations inspired\nthe definition of two novel PCS scores that included the above-mentioned\nsubsets of symptom complexes only. Both novel scores were inversely correlated\nwith quality of life, measured by the EQ-5D-5L index. The newly defined scores\nmay enhance the assessment of PCS severity, both in a research context and to\ndelineate distinct PCS subdomains with different therapeutic and interventional\nneeds in clinical practise.",
        "The desirable gambles framework provides a foundational approach to imprecise\nprobability theory but relies heavily on linear utility assumptions. This paper\nintroduces {\\em function-coherent gambles}, a generalization that accommodates\nnon-linear utility while preserving essential rationality properties. We\nestablish core axioms for function-coherence and prove a representation theorem\nthat characterizes acceptable gambles through continuous linear functionals.\nThe framework is then applied to analyze various forms of discounting in\nintertemporal choice, including hyperbolic, quasi-hyperbolic, scale-dependent,\nand state-dependent discounting. We demonstrate how these alternatives to\nconstant-rate exponential discounting can be integrated within the\nfunction-coherent framework. This unified treatment provides theoretical\nfoundations for modeling sophisticated patterns of time preference within the\ndesirability paradigm, bridging a gap between normative theory and observed\nbehavior in intertemporal decision-making under genuine uncertainty.",
        "Algebraic symplectic cobordism is the universal symplectically oriented\ncohomology theory for schemes, represented by the motivic commutative ring\nspectrum $\\text{MSp}$ constructed by Panin and Walter. The graded algebraic\ndiagonal $\\text{MSp}^*$ of the coefficient ring of $\\text{MSp}$ is unknown.\nThrough a symplectic version of the Pontryagin-Thom construction, one can\nassociate any symplectic variety $X$ with a symplectic class $[X]_\\text{MSp}$\nin $\\text{MSp}^{-\\text{dim} X}$. Still, the problem in using these classes to\nstudy the ring $\\text{MSp}^*$ is the paucity of non-trivial examples of\nsymplectic varieties. We modify this construction to obtain elements in\n$\\text{MSp}^*$ from a large family of varieties that are not symplectic but\ncarry a certain \"symplectic twist\". Then, using a strategy relying on the Adams\nspectral sequence for $\\text{MSp}$, we find a criterion to select generators\namong these classes, after taking a completion along the motivic Hopf map\n$\\eta$.",
        "Imaging atmospheric Cherenkov telescopes (IACTs) detect gamma rays by\nmeasuring the Cherenkov light emitted by secondary particles in the air shower\nwhen the gamma rays hit the atmosphere. At low energies, the limited amount of\nCherenkov light produced typically implies that the event is registered by one\nIACT only. Such events are called monoscopic events, and their analysis is\nparticularly difficult. Challenges include the reconstruction of the event's\narrival direction, energy, and the rejection of background events. Here, we\npresent a set of improvements, including a machine-learning algorithm to\ndetermine the correct orientation of the image, an intensity-dependent\nselection cut that ensures optimal performance, and a collection of new image\nparameters. To quantify these improvements, we use the central telescope of the\nH.E.S.S. IACT array. Knowing the correct image orientation, which corresponds\nto the arrival direction of the photon in the camera frame, is especially\nimportant for the angular reconstruction, which could be improved in resolution\nby 57% at 100 GeV. The event selection cut, which now depends on the total\nmeasured intensity of the events, leads to a reduction of the low-energy\nthreshold for source analyses by ~50%. The new image parameters characterize\nthe intensity and time distribution within the recorded images and complement\nthe traditionally used Hillas parameters in the machine learning algorithms. We\nevaluate their importance to the algorithms in a systematic approach and\ncarefully evaluate associated systematic uncertainties. We find that including\nsubsets of the new variables in machine-learning algorithms improves the\nreconstruction and background rejection, resulting in a sensitivity improved by\n41% at the low-energy threshold.",
        "We address the longstanding challenge in quantum many-body theory of\nreconciling unitary dynamics with irreversible relaxation. In classical chaos,\nthe unitary evolution operator develops Ruelle-Pollicott (RP) resonances inside\nthe unit circle in the continuum limit, leading to mixing. In the semiclassical\nlimit, chaotic single-particle quantum systems relax with the same RP\nresonances. In contrast, the theory of quantum many-body RP resonances and\ntheir link to irreversibility remain underdeveloped. Here, we relate the\nspectral form factor to the sum of autocorrelation functions and, in generic\nmany-body lattice systems without conservation laws, argue that all quantum\nmany-body RP resonances converge inside the unit disk, highlighting the role of\nnonunitary and the thermodynamic limit. While we conjecture this picture to be\ngeneral, we analytically prove the emergence of irreversibility in the random\nphase model (RPM), a paradigmatic Floquet quantum circuit model, in the limit\nof large local Hilbert space dimension. To this end, we couple it to local\nenvironments and compute the exact time evolution of autocorrelation functions,\nthe dissipative form factor, and out-of-time-order correlation functions\n(OTOCs). Although valid for any dissipation strength, we then focus on weak\ndissipation to clarify the origin of irreversibility in unitary systems. When\nthe dissipationless limit is taken after the thermodynamic limit, the unitary\nquantum map develops an infinite tower of decaying RP resonances -- chaotic\nsystems display so-called anomalous relaxation. We also show that the OTOC in\nthe RPM can undergo a two-stage relaxation and that during the second stage,\nthe approach to the stationary value is again controlled by the leading RP\nresonance.\n  [See the paper for the full abstract.]",
        "A consistent path system in a graph $G$ is an collection of paths, with\nexactly one path between any two vertices in $G$. A path system is said to be\nconsistent if it is intersection-closed. We say that $G$ is strictly metrizable\nif every consistent path system in $G$ can be realized as the system of unique\ngeodesics with respect to some assignment of positive edge weight. In this\npaper, we show that the family of strictly metrizable graphs is minor-closed.",
        "A novel approach is proposed to analyze a rather vast counter-rotating\nHamiltonian interaction in the context of cavity quantum electrodynamics. The\nmethod relies upon the supersymmetric mapping of the corresponding rotating\ninteraction and allows the analysis of the dynamics in the counter-rotating\nsystem in a fully general and exact analytical manner. Intriguing features of\nthe counter-rotating system are revealed through the simple supersymmetric\ntransformation. In turn, such interesting attributes have an important range of\npotential technological applications. In this way, supersymmetry emerges as a\nuseful tool to both connect and construct exactly solvable photonic systems in\ncavity quantum electrodynamics, and more generally in quantum optics, as well\nas to analyze the corresponding physical consequences and their possible\ntechnology implementations.",
        "Magnetic holes (MHs) are coherent structures characterized by a strong and\nlocalized magnetic field amplitude dip, commonly observed in the solar wind and\nplanetary magnetosheaths. These structures come in different sizes, from\nmagnetohydrodynamic to kinetic scales. Magnetospheric Multiscale (MMS)\nobservations have revealed electron scale MHs to be ubiquitous in the turbulent\nEarth's magnetosheath, potentially playing an important role in the energy\ncascade and dissipation. Despite abundant observations, the origin of electron\nscale MHs is still unclear and debated. In this work, we use fully kinetic\nsimulations to investigate the role of plasma turbulence in generating electron\nscale MHs. We perform a fully kinetic simulation of freely decaying plasma\nturbulence, initialized with typical Earth's magnetosheath parameters. We find\nthat electron scale MHs can be generated by turbulence via the following\nmechanism: first, large-scale turbulent velocity shears produce regions with\nhigh electron temperature anisotropy; these localized regions become unstable,\ngenerating oblique electron scale whistler waves; as they propagate over the\ninhomogeneous turbulent background, whistler fluctuations develop an\nelectrostatic component, turning into Bernstein-like modes; the strong\nelectrostatic fluctuations produce current filaments that merge into an\nelectron scale current vortex; the resulting electron vortex locally reduces\nthe magnetic field amplitude, finally evolving into an electron scale MH. We\nshow that MHs generated by this mechanism have properties consistent with MMS\nobservations and nontrivial kinetic features. We provide numerical evidence of\na new electron scale MH generation mechanism, driven by turbulence. Our results\nhave potential implications for understanding the formation and occurrence of\nelectron scale MHs in turbulent environments, such as the Earth's\nmagnetosheath.",
        "This work presents a novel gradient-free importance sampling-based framework\nfor precisely and efficiently estimating rare event probabilities, often\nencountered in reliability analyses of engineering systems. The approach is\nformulated around our foundational Approximate Sampling Target with\nPost-processing Adjustment (ASTPA) methodology. ASTPA uniquely constructs and\ndirectly samples an unnormalized target distribution, relaxing the optimal\nimportance sampling distribution (ISD). The target's normalizing constant is\nthen estimated using our inverse importance sampling (IIS) scheme, employing an\nISD fitted based on the obtained samples. In this work, a gradient-free\nsampling method within ASTPA is developed through a guided dimension-robust\npreconditioned Crank-Nicolson (pCN) algorithm, particularly suitable for\nblack-box computational models where analytical gradient information is not\navailable. To boost the sampling efficiency of pCN in our context, a\ncomputationally effective, general discovery stage for the rare event domain is\ndevised, providing (multi-modal) rare event samples used in initializing the\npCN chains. A series of diverse test functions and engineering problems\ninvolving high dimensionality and strong nonlinearity is presented,\ndemonstrating the advantages of the proposed framework compared to several\nstate-of-the-art sampling methods.",
        "According to the quantum chaos paradigm, the nature of a system's classical\ndynamics, whether integrable or chaotic, is universally reflected in the\nfluctuations of its quantum spectrum. However, since many-body spectra in the\nmean field limit are composed of independent single-particle energy levels,\ntheir spectral fluctuations always display Poissonian behavior and hence cannot\nbe used to distinguish underlying chaotic from integrable single-particle\ndynamics. We demonstrate that this distinction can, instead, be revealed from\nthe mean many-body level density (at fixed energy) and its variance after\naveraging over ensembles representing different types of single-particle\ndynamics. This is in strong contrast to the energy-averaged mean level density\n(of a given system) that is assumed not to carry such information and is\nroutinely removed to focus on universal signatures. To support our claim we\nsystematically analyze the role of single-particle level correlations, that\nenter through Poisson and random matrix statistics (of various symmetry\nclasses) into the ensemble-averaged density of states and its variance,\ncontrasting bosonic and fermionic many-body systems. Our analytical study,\ntogether with extensive numerical simulations for systems with $N \\ge 5$\nparticles consistently reveal significant differences (up to an order of\nmagnitude for fermions and even larger for bosons) in the mean many-body level\ndensities, depending on the nature of the underlying dynamics. Notably, in the\nfermionic case Poisson-type single-particle level fluctuations precisely cancel\ncontributions from indistinguishability, such that the average many-body\nspectral density equals the (Thomas-Fermi) volume term. We further highlight\nthe difference between the mean level density and its variance as functions of\nthe total energy $E$ and the excitation energy $Q$.",
        "Diffusion MRI tractography technique enables non-invasive visualization of\nthe white matter pathways in the brain. It plays a crucial role in neuroscience\nand clinical fields by facilitating the study of brain connectivity and\nneurological disorders. However, the accuracy of reconstructed tractograms has\nbeen a longstanding challenge. Recently, deep learning methods have been\napplied to improve tractograms for better white matter coverage, but often\ncomes at the expense of generating excessive false-positive connections. This\nis largely due to their reliance on local information to predict long range\nstreamlines. To improve the accuracy of streamline propagation predictions, we\nintroduce a novel deep learning framework that integrates image-domain spatial\ninformation and anatomical information along tracts, with the former extracted\nthrough convolutional layers and the later modeled via a Transformer-decoder.\nAdditionally, we employ a weighted loss function to address fiber class\nimbalance encountered during training. We evaluate the proposed method on the\nsimulated ISMRM 2015 Tractography Challenge dataset, achieving a valid\nstreamline rate of 66.2%, white matter coverage of 63.8%, and successfully\nreconstructing 24 out of 25 bundles. Furthermore, on the multi-site\nTractoinferno dataset, the proposed method demonstrates its ability to handle\nvarious diffusion MRI acquisition schemes, achieving a 5.7% increase in white\nmatter coverage and a 4.1% decrease in overreach compared to RNN-based methods.",
        "Periodic lattices are natural generalizations of lattices, which arise\nnaturally in diophantine approximations with rationals of bounded denominators.\nIn this paper, we prove analogues of classical theorems in geometry of numbers\nfor periodic lattices in function fields. Moreover, we use special matrices to\ncompute the covering and packing radii of special periodic lattices.",
        "The Pythagorean school attributed consonance in music to simplicity of\nfrequency ratios between musical tones. In the last two centuries, the\nconsonance curves developed by Helmholtz, Plompt and Levelt shifted focus to\npsycho-acoustic considerations in perceiving consonances. The appearance of\npeaks of these curves at the ratios considered by the Pythagorean school, and\nwhich were a consequence of an attempt to understand the world by nice\nmathematical proportions, remained a curiosity. This paper addresses this\ncuriosity, by describing a mathematical model of musical sound, along with a\nmathematical definition of consonance. First, we define pure, complex and mixed\ntones as mathematical models of musical sound. By a sequence of numerical\nexperiments and analytic calculations, we show that continuous cosine\nsimilarity, abbreviated as cosim, applied to these models quantifies the\nelusive concept of consonance as a frequency ratio which gives a local maximum\nof the cosim function. We prove that these maxima occur at the ratios\nconsidered as consonant in classical music theory. Moreover, we provide a\nsimple explanation why the number of musical intervals considered as consonant\nby musicians is finite, but has been increasing over the centuries.\nSpecifically, our formulas show that the number of consonant intervals changes\nwith the depth of the tone (the number of harmonics present).",
        "One of the greatest challenges when designing new technologies that make use\nof non-trivial quantum materials is the difficulty associated with predicting\nmaterial-specific properties, such as critical temperature, gap parameter, etc.\nThere is naturally a great amount of interest in these types of condensed\nmatter systems because of their application to quantum sensing, quantum\nelectronics, and quantum computation; however, they are exceedingly difficult\nto address from first principles because of the famous many-body problem. For\nthis reason, a full electron-nuclear quantum calculation will likely remain\ncompletely out of reach for the foreseeable future. A practical alternative is\nprovided by finite temperature, multi component density functional theory\n(MCDFT), which is a formally exact method of computing the equilibrium state\nenergy of a many-body quantum system. In this work, we use this construction\nalongside a perturbative scheme to demonstrate that the phenomena Peierls\neffect and Kohn Anomaly are both natural features of the KS equations without\nadditional structure needed. We find the temperature dependent ionic density\nfor a simple 1D lattice which is then used to derive the ionic densities\ntemperature dependent affect on the electronic band structure. This is\naccomplished by Fourier transforming the ionic density term found within this\nKS electronic equation. Using the Peierls effect phonon distortion gap openings\nin relation to the Fermi level, we then perturb the KS ionic equation with a\nconduction electron density, deriving the Kohn Anomaly. This provides a\nworkable predictive strategy for interesting electro-phonon related material\nproperties which could be extended to 2D and 3D real materials while retaining\nthe otherwise complicated temperature dependence.",
        "In order to describe properly the gravity interactions including the mass\ncurrents, in the gravitomagnetism we construct four Maxwell type gravitational\nequations which are shown to be analogs of the Maxwell equations in the\nelectromagnetism. Next, exploiting the Maxwell type gravitational equations, we\nexplicitly predict the mass magnetic fields for both the isolated system of the\nspinning Moon orbiting the spinning Earth and that of the Sun and solar system\nplanets orbiting the spinning Sun, whose phenomenological values have not been\nevaluated in the precedented Newtonian gravity formalisms. In the\ngravitomagnetism we also phenomenologically investigate the mass magnetic\ngeneral relativity (GR) forces associated with the mass magnetic fields, to\nfind that they are extremely small but non-vanishing compared to the\ncorresponding mass electric Newtonian forces. Moreover, the directions of the\nmass magnetic GR forces for the solar system planets except Venus and Uranus\nare shown to be anti-parallel to those of their mass electric Newtonian forces.\nNext we investigate the mass magnetic dipole moment related with the B-ring of\nSaturn, to evaluate $\\vec{m}_{M}(Ring)=-1.141\\times 10^{4}~{\\rm\nm^{3}~sec^{-1}}~\\hat{\\omega}$ with $\\hat{\\omega}$ being the unit vector along\nthe axis direction of the spinning B-ring. The predicted value of\n$\\vec{m}_{M}(Ring)$ is shown to be directly related with the Cassini data on\nthe total mass of the rings of Saturn."
      ]
    }
  },
  {
    "id":2411.00758,
    "research_type":"basic",
    "start_id":"b6",
    "start_title":"Introduction to Nonimaging Optics, second edition",
    "start_abstract":"Introduction to Nonimaging Optics covers the theoretical foundations and design methods of nonimaging optics, as well as key concepts from related fields. This fully updated, revised, and expanded Second Edition: \u2022 Features a new and intuitive introduction with a basic description of the advantages of nonimaging optics \u2022 Adds new chapters on wavefronts for a prescribed output (irradiance or intensity), infinitesimal \u00e9tendue optics (generalization of the aplanatic optics), and K\u00f6hler optics and color mixing \u2022 Incorporates new material on the simultaneous multiple surface (SMS) design method in 3-D, integral invariants, and \u00e9tendue 2-D \u2022 Contains 21 chapters, 24 fully worked and several other examples, and 1,000+ illustrations, including photos of real devices \u2022 Addresses applications ranging from solar energy concentration to illumination engineering Introduction to Nonimaging Optics, Second Edition invites newcomers to explore the growing field of nonimaging optics, while providing seasoned veterans with an extensive reference book.",
    "start_categories":[
      "physics.optics"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b18"
      ],
      "title":[
        "Inverse methods for illumination optics"
      ],
      "abstract":[
        "\u2022 A submitted manuscript is the version of the article upon submission and before peer-review. There can be important differences between the submitted version and the official published version of record. People interested in the research are advised to contact the author for the final version of the publication, or visit the DOI to the publisher's website. \u2022 The final author version and the galley proof are versions of the publication after peer review. \u2022 The final published version features the final layout of the paper including the volume, issue and page numbers."
      ],
      "categories":[
        "math.MP"
      ]
    },
    "list":{
      "title":[
        "Increasing the Energy-Efficiency of Wearables Using Low-Precision Posit\n  Arithmetic with PHEE",
        "Coherent Local Explanations for Mathematical Optimization",
        "Transformers with Joint Tokens and Local-Global Attention for Efficient\n  Human Pose Estimation",
        "Imperfect Knowledge Management (IKM) in GEFRED (GENeralized model for\n  Fuzzy RElational Databases)",
        "An Interpretable Neural Control Network with Adaptable Online Learning\n  for Sample Efficient Robot Locomotion Learning",
        "We Need to Effectively Integrate Computing Skills Across Discipline\n  Curricula",
        "Sweeping Orders for Simplicial Complex Reconstruction",
        "Rapid and Inexpensive Inertia Tensor Estimation from a Single Object\n  Throw",
        "Fractional Correspondence Framework in Detection Transformer",
        "Modelling Capillary Rise with a Slip Boundary Condition: Well-posedness\n  and Long-time Dynamics of Solutions to Washburn's Equation",
        "A comprehensive study of bound-states for the nonlinear Schr\\\"odinger\n  equation on single-knot metric graphs",
        "A precise asymptotic analysis of learning diffusion models: theory and\n  insights",
        "BoKDiff: Best-of-K Diffusion Alignment for Target-Specific 3D Molecule\n  Generation",
        "TMI-CLNet: Triple-Modal Interaction Network for Chronic Liver Disease\n  Prognosis From Imaging, Clinical, and Radiomic Data Fusion",
        "Constructing self-similar subsets within the fractal support of Lacunary\n  Wavelet Series for their multifractal analysis",
        "Fenchel-Young Variational Learning",
        "HyperNOs: Automated and Parallel Library for Neural Operators Research",
        "Understanding Generalization in Transformers: Error Bounds and Training\n  Dynamics Under Benign and Harmful Overfitting",
        "Linear extrapolation for the graph of function of single variable based\n  on walks",
        "RayFlow: Instance-Aware Diffusion Acceleration via Adaptive Flow\n  Trajectories",
        "The Society of HiveMind: Multi-Agent Optimization of Foundation Model\n  Swarms to Unlock the Potential of Collective Intelligence",
        "Food Recommendation With Balancing Comfort and Curiosity",
        "Movable Antenna Aided Multiuser Communications: Antenna Position\n  Optimization Based on Statistical Channel Information",
        "ACCESS : A Benchmark for Abstract Causal Event Discovery and Reasoning",
        "A characterization of binomial Macaulay dual generators for complete\n  intersections",
        "VINGS-Mono: Visual-Inertial Gaussian Splatting Monocular SLAM in Large\n  Scenes",
        "Sheaf-Theoretic Causal Emergence for Resilience Analysis in Distributed\n  Systems",
        "Gradient Deconfliction via Orthogonal Projections onto Subspaces For\n  Multi-task Learning",
        "Coordinated Inauthentic Behavior and Information Spreading on Twitter"
      ],
      "abstract":[
        "Wearable biomedical devices are increasingly being used for continuous\npatient health monitoring, enabling real-time insights and extended data\ncollection without the need for prolonged hospital stays. These devices must be\nenergy efficient to minimize battery size, improve comfort, and reduce\nrecharging intervals. This paper investigates the use of specialized\nlow-precision arithmetic formats to enhance the energy efficiency of biomedical\nwearables. Specifically, we explore posit arithmetic, a floating-point-like\nrepresentation, in two key applications: cough detection for chronic cough\nmonitoring and R peak detection in ECG analysis. Simulations reveal that 16-bit\nposits can replace 32-bit IEEE 754 floating point numbers with minimal accuracy\nloss in cough detection. For R peak detection, posit arithmetic achieves\nsatisfactory accuracy with as few as 10 or 8 bits, compared to the 16-bit\nrequirement for floating-point formats. To further this exploration, we\nintroduce PHEE, a modular and extensible architecture that integrates the\nCoprosit posit coprocessor within a RISC-V-based system. Using the X-HEEP\nframework, PHEE seamlessly incorporates posit arithmetic, demonstrating reduced\nhardware area and power consumption compared to a floating-point counterpart\nsystem. Post-synthesis results targeting 16nm TSMC technology show that the\nposit hardware targeting these biomedical applications can be 38% smaller and\nconsume up to 54% less energy at the functional unit level, with no performance\ncompromise. These findings establish the potential of low-precision posit\narithmetic to significantly improve the energy efficiency of wearable\nbiomedical devices.",
        "The surge of explainable artificial intelligence methods seeks to enhance\ntransparency and explainability in machine learning models. At the same time,\nthere is a growing demand for explaining decisions taken through complex\nalgorithms used in mathematical optimization. However, current explanation\nmethods do not take into account the structure of the underlying optimization\nproblem, leading to unreliable outcomes. In response to this need, we introduce\nCoherent Local Explanations for Mathematical Optimization (CLEMO). CLEMO\nprovides explanations for multiple components of optimization models, the\nobjective value and decision variables, which are coherent with the underlying\nmodel structure. Our sampling-based procedure can provide explanations for the\nbehavior of exact and heuristic solution algorithms. The effectiveness of CLEMO\nis illustrated by experiments for the shortest path problem, the knapsack\nproblem, and the vehicle routing problem.",
        "Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) have led\nto significant progress in 2D body pose estimation. However, achieving a good\nbalance between accuracy, efficiency, and robustness remains a challenge. For\ninstance, CNNs are computationally efficient but struggle with long-range\ndependencies, while ViTs excel in capturing such dependencies but suffer from\nquadratic computational complexity. This paper proposes two ViT-based models\nfor accurate, efficient, and robust 2D pose estimation. The first one,\nEViTPose, operates in a computationally efficient manner without sacrificing\naccuracy by utilizing learnable joint tokens to select and process a subset of\nthe most important body patches, enabling us to control the trade-off between\naccuracy and efficiency by changing the number of patches to be processed. The\nsecond one, UniTransPose, while not allowing for the same level of direct\ncontrol over the trade-off, efficiently handles multiple scales by combining\n(1) an efficient multi-scale transformer encoder that uses both local and\nglobal attention with (2) an efficient sub-pixel CNN decoder for better speed\nand accuracy. Moreover, by incorporating all joints from different benchmarks\ninto a unified skeletal representation, we train robust methods that learn from\nmultiple datasets simultaneously and perform well across a range of scenarios\n-- including pose variations, lighting conditions, and occlusions. Experiments\non six benchmarks demonstrate that the proposed methods significantly\noutperform state-of-the-art methods while improving computational efficiency.\nEViTPose exhibits a significant decrease in computational complexity (30% to\n44% less in GFLOPs) with a minimal drop of accuracy (0% to 3.5% less), and\nUniTransPose achieves accuracy improvements ranging from 0.9% to 43.8% across\nthese benchmarks.",
        "Imperfect Knowledge Management (IKM) aids in managing imprecise, uncertain,\nor incomplete aspects of meaning. IKM acknowledges that an enterprise's\nknowledge is often imperfect, characterized by varying degrees of imprecision,\nuncertainty, or incompleteness. In this context, knowledge is viewed as an\nobject described by attributes and values. Our focus is on the domain of\ncompetencies (know how) in the production of coated cardboard, particularly the\nprocess of converting finished products from the manufactured cardboard. This\nprocess involves both classic and fuzzy attributes that are used to assess the\nquality of the cardboard. This article introduces a set of protocols designed\nto model a fuzzy metaknowledge base using GEFRED (GENeralized model for Fuzzy\nRElational Databases) in an Oracle 8i relational database system.",
        "Robot locomotion learning using reinforcement learning suffers from training\nsample inefficiency and exhibits the non-understandable\/black-box nature. Thus,\nthis work presents a novel SME-AGOL to address such problems. Firstly,\nSequential Motion Executor (SME) is a three-layer interpretable neural network,\nwhere the first produces the sequentially propagating hidden states, the second\nconstructs the corresponding triangular bases with minor non-neighbor\ninterference, and the third maps the bases to the motor commands. Secondly, the\nAdaptable Gradient-weighting Online Learning (AGOL) algorithm prioritizes the\nupdate of the parameters with high relevance score, allowing the learning to\nfocus more on the highly relevant ones. Thus, these two components lead to an\nanalyzable framework, where each sequential hidden state\/basis represents the\nlearned key poses\/robot configuration. Compared to state-of-the-art methods,\nthe SME-AGOL requires 40% fewer samples and receives 150% higher final\nreward\/locomotion performance on a simulated hexapod robot, while taking merely\n10 minutes of learning time from scratch on a physical hexapod robot. Taken\ntogether, this work not only proposes the SME-AGOL for sample efficient and\nunderstandable locomotion learning but also emphasizes the potential\nexploitation of interpretability for improving sample efficiency and learning\nperformance.",
        "Computing is increasingly central to innovation across a wide range of\ndisciplinary and interdisciplinary problem domains. Students across\nnoncomputing disciplines need to apply sophisticated computational skills and\nmethods to fields as diverse as biology, linguistics, and art. Furthermore,\ncomputing plays a critical role in \"momentous geopolitical events\", such as\nelections in several countries including the US, and is changing how people\n\"work, collaborate, communicate, shop, eat, travel, get news and entertainment,\nand quite simply live\". Traditional computing courses, however, fail to equip\nnon-computing discipline students with the necessary computing skills - if they\ncan even get into classes packed with CS majors. A pressing question facing\nacademics today is: How do we effectively integrate computing skills that are\nuseful for the discipline into discipline curricula?\n  We advocate an approach where courses in discipline X include the computing\nrelevant to the learning outcomes of that course, as used by practitioners in\nX. We refer to the computing skills relevant to a course in discipline X as an\n\"ounce of computing skills\", to highlight our belief regarding the amount of\ncomputing to be integrated in that course. In this article, we outline our\ninsights regarding the development of an ounce of computing skills for a\ndiscipline course, and the evaluation of the developed ounce. The key takeaways\nare that the goal has to be to advance students in their disciplines, and only\nthe disciplinary experts can tell us how computing is used in that discipline.\nComputer scientists know how to teach computing, but the classes can't be about\nCS values. The disciplinary values are paramount.",
        "Simplicial complexes arising from real-world settings may not be directly\nobservable. Hence, for an unknown simplicial complex in Euclidean space, we\nwant to efficiently reconstruct it by querying local structure. In particular,\nwe are interested in queries for the indegree of a simplex $\\sigma$ in some\ndirection: the number of cofacets of $\\sigma$ contained in some halfspace\n\"below\" $\\sigma$. Fasy et al. proposed a method that, given the vertex set of a\nsimplicial complex, uses indegree queries to reconstruct the set of edges. In\nparticular, they use a sweep algorithm through the vertex set, identifying\nedges adjacent to and above each vertex in the sweeping order. The algorithm\nrelies on a natural but crucial property of the sweeping order: at a given\nvertex $v$, all edges adjacent to $v$ contained in the halfspace below $v$ have\nanother endpoint that appeared earlier in the order.\n  The edge reconstruction algorithm does not immediately extend to\nhigher-dimensional simplex reconstruction. In particular, it is not possible to\nsweep through a set of $i$-simplices in a fixed direction and maintain that all\n$(i+1)$-cofacets of a given simplex $\\sigma$ that come below $\\sigma$ are\nknown. We circumvent this by defining a sweeping order on a set of\n$i$-simplices, that additionally pairs each $i$-simplex $\\sigma$ with a\ndirection perpendicular to $\\sigma$. Analogous to Fasy et al., our order has\nthe crucial property that, at any $i$-simplex $\\sigma$ paired with direction\n$s$, each $(i+1)$-dimensional coface of $\\sigma$ that lies in the halfspace\nbelow $\\sigma$ with respect to the direction $s$ has an $i$-dimensional face\nthat appeared earlier in the order. We show how to compute such an order and\nuse it to extend the edge reconstruction algorithm of Fasy et al. to simplicial\ncomplex reconstruction. Our algorithm can reconstruct arbitrary embedded\nsimplicial complexes.",
        "The inertia tensor is an important parameter in many engineering fields, but\nmeasuring it can be cumbersome and involve multiple experiments or accurate and\nexpensive equipment. We propose a method to measure the moment of inertia\ntensor of a rigid body from a single spinning throw, by attaching a small and\ninexpensive stand-alone measurement device consisting of a gyroscope,\naccelerometer and a reaction wheel. The method includes a compensation for the\nincrease of moment of inertia due to adding the measurement device to the body,\nand additionally obtains the location of the centre of gravity of the body as\nan intermediate result. Experiments performed with known rigid bodies show that\nthe mean accuracy is around 2\\%.",
        "The Detection Transformer (DETR), by incorporating the Hungarian algorithm,\nhas significantly simplified the matching process in object detection tasks.\nThis algorithm facilitates optimal one-to-one matching of predicted bounding\nboxes to ground-truth annotations during training. While effective, this strict\nmatching process does not inherently account for the varying densities and\ndistributions of objects, leading to suboptimal correspondences such as failing\nto handle multiple detections of the same object or missing small objects. To\naddress this, we propose the Regularized Transport Plan (RTP). RTP introduces a\nflexible matching strategy that captures the cost of aligning predictions with\nground truths to find the most accurate correspondences between these sets. By\nutilizing the differentiable Sinkhorn algorithm, RTP allows for soft,\nfractional matching rather than strict one-to-one assignments. This approach\nenhances the model's capability to manage varying object densities and\ndistributions effectively. Our extensive evaluations on the MS-COCO and VOC\nbenchmarks demonstrate the effectiveness of our approach. RTP-DETR, surpassing\nthe performance of the Deform-DETR and the recently introduced DINO-DETR,\nachieving absolute gains in mAP of +3.8% and +1.7%, respectively.",
        "The aim of this paper is to extend Washburn's capillary rise equation by\nincorporating a slip condition at the pipe wall. The governing equation is\nderived using fundamental principles from continuum mechanics. A new scaling is\nintroduced, allowing for a systematic analysis of different flow regimes. We\nprove the global-in-time existence and uniqueness of a bounded positive\nsolution to Washburn's equation that includes the slip parameter, as well as\nthe continuous dependence of the solution in the maximum norm on the initial\ndata. Thus, the initial-value problem for Washburn's equation is shown to be\nwell-posed in the sense of Hadamard. Additionally, we show that the unique\nequilibrium solution may be reached either monotonically or in an oscillatory\nfashion, similarly to the no-slip case. Finally, we determine the basin of\nattraction for the system, ensuring that the equilibrium state will be reached\nfrom the initial data we impose. These results hold for any positive value of\nthe nondimensional slip parameter in the model, and for all values of the ratio\n$h_0\/h_e$ in the range $[0,3\/2]$, where $h_0$ is the initial height of the\nfluid column and $h_e$ is its equilibrium height.",
        "We study the existence and qualitative properties of action ground-states\n(that is, bound-states with minimal action) {of the nonlinear Schr\\\"odinger\nequation} over single-knot metric graphs -- which are made of half-lines, loops\nand pendants, all connected at a single vertex. First, we prove existence of\naction ground-state for generic single-knot graphs, even in the absence of an\nassociated variational problem. Second, for regular single-knot graphs of\nlength $\\ell$, we perform a complete analysis of positive monotone\nbound-states. Furthermore, we characterize all positive bound-states when\n$\\ell$ is small and prove some symmetry-breaking results for large $\\ell$.\nFinally, we apply the results to some particular graphs to illustrate the\ncomplex relation between action ground-states and the topological {and metric}\nfeatures of the underlying metric graph.\n  The proofs are nonvariational, using a careful phase-plane analysis, the\nstudy of sections of period functions, asymptotic estimates and blowup\narguments. We show, in particular, how nonvariational techniques are\ncomplementary to variational ones in order to deeply understand bound-states of\nthe nonlinear Schr\\\"odinger equation on metric graphs.",
        "In this manuscript, we consider the problem of learning a flow or\ndiffusion-based generative model parametrized by a two-layer auto-encoder,\ntrained with online stochastic gradient descent, on a high-dimensional target\ndensity with an underlying low-dimensional manifold structure. We derive a\ntight asymptotic characterization of low-dimensional projections of the\ndistribution of samples generated by the learned model, ascertaining in\nparticular its dependence on the number of training samples. Building on this\nanalysis, we discuss how mode collapse can arise, and lead to model collapse\nwhen the generative model is re-trained on generated synthetic data.",
        "Structure-based drug design (SBDD) leverages the 3D structure of biomolecular\ntargets to guide the creation of new therapeutic agents. Recent advances in\ngenerative models, including diffusion models and geometric deep learning, have\ndemonstrated promise in optimizing ligand generation. However, the scarcity of\nhigh-quality protein-ligand complex data and the inherent challenges in\naligning generated ligands with target proteins limit the effectiveness of\nthese methods. We propose BoKDiff, a novel framework that enhances ligand\ngeneration by combining multi-objective optimization and Best-of-K alignment\nmethodologies. Built upon the DecompDiff model, BoKDiff generates diverse\ncandidates and ranks them using a weighted evaluation of molecular properties\nsuch as QED, SA, and docking scores. To address alignment challenges, we\nintroduce a method that relocates the center of mass of generated ligands to\ntheir docking poses, enabling accurate sub-component extraction. Additionally,\nwe integrate a Best-of-N (BoN) sampling approach, which selects the optimal\nligand from multiple generated candidates without requiring fine-tuning. BoN\nachieves exceptional results, with QED values exceeding 0.6, SA scores above\n0.75, and a success rate surpassing 35%, demonstrating its efficiency and\npracticality. BoKDiff achieves state-of-the-art results on the CrossDocked2020\ndataset, including a -8.58 average Vina docking score and a 26% success rate in\nmolecule generation. This study is the first to apply Best-of-K alignment and\nBest-of-N sampling to SBDD, highlighting their potential to bridge generative\nmodeling with practical drug discovery requirements. The code is provided at\nhttps:\/\/github.com\/khodabandeh-ali\/BoKDiff.git.",
        "Chronic liver disease represents a significant health challenge worldwide and\naccurate prognostic evaluations are essential for personalized treatment plans.\nRecent evidence suggests that integrating multimodal data, such as computed\ntomography imaging, radiomic features, and clinical information, can provide\nmore comprehensive prognostic information. However, modalities have an inherent\nheterogeneity, and incorporating additional modalities may exacerbate the\nchallenges of heterogeneous data fusion. Moreover, existing multimodal fusion\nmethods often struggle to adapt to richer medical modalities, making it\ndifficult to capture inter-modal relationships. To overcome these limitations,\nWe present the Triple-Modal Interaction Chronic Liver Network (TMI-CLNet).\nSpecifically, we develop an Intra-Modality Aggregation module and a\nTriple-Modal Cross-Attention Fusion module, which are designed to eliminate\nintra-modality redundancy and extract cross-modal information, respectively.\nFurthermore, we design a Triple-Modal Feature Fusion loss function to align\nfeature representations across modalities. Extensive experiments on the liver\nprognosis dataset demonstrate that our approach significantly outperforms\nexisting state-of-the-art unimodal models and other multi-modal techniques. Our\ncode is available at https:\/\/github.com\/Mysterwll\/liver.git.",
        "Given a fractal $\\mathcal{I}$ whose Hausdorff dimension matches with the\nupper-box dimension, we propose a new method which consists in selecting inside\n$\\mathcal{I}$ some subsets (called quasi-Cantor sets) of almost same dimension\nand with controled properties of self-similarties at prescribed scales. It\nallows us to estimate below the Hausdorff dimension $\\mathcal{I}$ intersected\nto limsup sets of contracted balls selected according a Bernoulli law, in\ncontexts where classical Mass Transference Principles cannot be applied. We\napply this result to the computation of the increasing multifractal spectrum of\nlacunary wavelet series supported on $\\mathcal{I}$.",
        "From a variational perspective, many statistical learning criteria involve\nseeking a distribution that balances empirical risk and regularization. In this\npaper, we broaden this perspective by introducing a new general class of\nvariational methods based on Fenchel-Young (FY) losses, treated as divergences\nthat generalize (and encompass) the familiar Kullback-Leibler divergence at the\ncore of classical variational learning. Our proposed formulation -- FY\nvariational learning -- includes as key ingredients new notions of FY free\nenergy, FY evidence, FY evidence lower bound, and FY posterior. We derive\nalternating minimization and gradient backpropagation algorithms to compute (or\nlower bound) the FY evidence, which enables learning a wider class of models\nthan previous variational formulations. This leads to generalized FY variants\nof classical algorithms, such as an FY expectation-maximization (FYEM)\nalgorithm, and latent-variable models, such as an FY variational autoencoder\n(FYVAE). Our new methods are shown to be empirically competitive, often\noutperforming their classical counterparts, and most importantly, to have\nqualitatively novel features. For example, FYEM has an adaptively sparse\nE-step, while the FYVAE can support models with sparse observations and sparse\nposteriors.",
        "This paper introduces HyperNOs, a PyTorch library designed to streamline and\nautomate the process of exploring neural operators, with a special focus on\nhyperparameter optimization for comprehensive and exhaustive exploration.\nIndeed, HyperNOs takes advantage of state-of-the-art optimization algorithms\nand parallel computing implemented in the Ray-tune library to efficiently\nexplore the hyperparameter space of neural operators. We also implement many\nuseful functionalities for studying neural operators with a user-friendly\ninterface, such as the possibility to train the model with a fixed number of\nparameters or to train the model with multiple datasets and different\nresolutions. We integrate Fourier neural operators and convolutional neural\noperators in our library, achieving state of the art results on many\nrepresentative benchmarks, demonstrating the capabilities of HyperNOs to handle\nreal datasets and modern architectures. The library is designed to be easy to\nuse with the provided model and datasets, but also to be easily extended to use\nnew datasets and custom neural operator architectures.",
        "Transformers serve as the foundational architecture for many successful\nlarge-scale models, demonstrating the ability to overfit the training data\nwhile maintaining strong generalization on unseen data, a phenomenon known as\nbenign overfitting. However, research on how the training dynamics influence\nerror bounds within the context of benign overfitting has been limited. This\npaper addresses this gap by developing a generalization theory for a two-layer\ntransformer with labeled flip noise. Specifically, we present generalization\nerror bounds for both benign and harmful overfitting under varying\nsignal-to-noise ratios (SNR), where the training dynamics are categorized into\nthree distinct stages, each with its corresponding error bounds. Additionally,\nwe conduct extensive experiments to identify key factors that influence test\nerrors in transformers. Our experimental results align closely with the\ntheoretical predictions, validating our findings.",
        "The quantum walk was introduced as a quantum counterpart of the random walk\nand has been intensively studied since around 2000. Its applications include\ntopological insulators, radioactive waste reduction, and quantum search. The\nfirst author in 2019 defined a time-series model based on the measure of the\n``discrete-time\" and ``discrete-space\" quantum walk in one dimension. Inspired\nby his model, this paper proposes a new model for the graph of a function of a\nsingle variable determined by the measure which comes from the weak limit\nmeasure of a ``continuous-time or discrete-time\" and ``discrete-space\" walk.\nThe measure corresponds to a ``continuous-time\" and ``continuous-space\" walk in\none dimension. Moreover, we also presents a method of a linear extrapolation\nfor the graph by our model.",
        "Diffusion models have achieved remarkable success across various domains.\nHowever, their slow generation speed remains a critical challenge. Existing\nacceleration methods, while aiming to reduce steps, often compromise sample\nquality, controllability, or introduce training complexities. Therefore, we\npropose RayFlow, a novel diffusion framework that addresses these limitations.\nUnlike previous methods, RayFlow guides each sample along a unique path towards\nan instance-specific target distribution. This method minimizes sampling steps\nwhile preserving generation diversity and stability. Furthermore, we introduce\nTime Sampler, an importance sampling technique to enhance training efficiency\nby focusing on crucial timesteps. Extensive experiments demonstrate RayFlow's\nsuperiority in generating high-quality images with improved speed, control, and\ntraining efficiency compared to existing acceleration techniques.",
        "Multi-agent systems address issues of accessibility and scalability of\nartificial intelligence (AI) foundation models, which are often represented by\nlarge language models. We develop a framework - the \"Society of HiveMind\"\n(SOHM) - that orchestrates the interaction between multiple AI foundation\nmodels, imitating the observed behavior of animal swarms in nature by following\nmodern evolutionary theories. On the one hand, we find that the SOHM provides a\nnegligible benefit on tasks that mainly require real-world knowledge. On the\nother hand, we remark a significant improvement on tasks that require intensive\nlogical reasoning, indicating that multi-agent systems are capable of\nincreasing the reasoning capabilities of the collective compared to the\nindividual agents. Our findings demonstrate the potential of combining a\nmultitude of diverse AI foundation models to form an artificial swarm\nintelligence capable of self-improvement through interactions with a given\nenvironment.",
        "Food is a key pleasure of traveling, but travelers face a trade-off between\nexploring curious new local food and choosing comfortable, familiar options.\nThis creates demand for personalized recommendation systems that balance these\ncompeting factors. To the best of our knowledge, conventional recommendation\nmethods cannot provide recommendations that offer both curiosity and comfort\nfor food unknown to the user at a travel destination. In this study, we propose\nnew quantitative methods for estimating comfort and curiosity: Kernel Density\nScoring (KDS) and Mahalanobis Distance Scoring (MDS). KDS probabilistically\nestimates food history distribution using kernel density estimation, while MDS\nuses Mahalanobis distances between foods. These methods score food based on how\ntheir representation vectors fit the estimated distributions. We also propose a\nranking method measuring the balance between comfort and curiosity based on\ntaste and ingredients. This balance is defined as curiosity (return) gained per\nunit of comfort (risk) in choosing a food. For evaluation the proposed method,\nwe newly collected a dataset containing user surveys on Japanese food and\nassessments of foreign food regarding comfort and curiosity. Comparing our\nmethods against the existing method, the Wilcoxon signed-rank test showed that\nwhen estimating comfort from taste and curiosity from ingredients, the\nMDS-based method outperformed the Baseline, while the KDS-based method showed\nno significant differences. When estimating curiosity from taste and comfort\nfrom ingredients, both methods outperformed the Baseline. The MDS-based method\nconsistently outperformed KDS in ROC-AUC values.",
        "The movable antenna (MA) technology has attracted great attention recently\ndue to its promising capability in improving wireless channel conditions by\nflexibly adjusting antenna positions. To reap maximal performance gains of MA\nsystems, existing works mainly focus on MA position optimization to cater to\nthe instantaneous channel state information (CSI). However, the resulting\nreal-time antenna movement may face challenges in practical implementation due\nto the additional time overhead and energy consumption required, especially in\nfast time-varying channel scenarios. To address this issue, we propose in this\npaper a new approach to optimize the MA positions based on the users'\nstatistical CSI over a large timescale. In particular, we propose a general\nfield response based statistical channel model to characterize the random\nchannel variations caused by the local movement of users. Based on this model,\na two-timescale optimization problem is formulated to maximize the ergodic sum\nrate of multiple users, where the precoding matrix and the positions of MAs at\nthe base station (BS) are optimized based on the instantaneous and statistical\nCSI, respectively. To solve this non-convex optimization problem, a log-barrier\npenalized gradient ascent algorithm is developed to optimize the MA positions,\nwhere two methods are proposed to approximate the ergodic sum rate and its\ngradients with different complexities. Finally, we present simulation results\nto evaluate the performance of the proposed design and algorithms based on\npractical channels generated by ray-tracing. The results verify the performance\nadvantages of MA systems compared to their fixed-position antenna (FPA)\ncounterparts in terms of long-term rate improvement, especially for scenarios\nwith more diverse channel power distributions in the angular domain.",
        "Identifying cause-and-effect relationships is critical to understanding\nreal-world dynamics and ultimately causal reasoning. Existing methods for\nidentifying event causality in NLP, including those based on Large Language\nModels (LLMs), exhibit difficulties in out-of-distribution settings due to the\nlimited scale and heavy reliance on lexical cues within available benchmarks.\nModern benchmarks, inspired by probabilistic causal inference, have attempted\nto construct causal graphs of events as a robust representation of causal\nknowledge, where \\texttt{CRAB} \\citep{romanou2023crab} is one such recent\nbenchmark along this line. In this paper, we introduce \\texttt{ACCESS}, a\nbenchmark designed for discovery and reasoning over abstract causal events.\nUnlike existing resources, \\texttt{ACCESS} focuses on causality of everyday\nlife events on the abstraction level. We propose a pipeline for identifying\nabstractions for event generalizations from \\texttt{GLUCOSE}\n\\citep{mostafazadeh-etal-2020-glucose}, a large-scale dataset of implicit\ncommonsense causal knowledge, from which we subsequently extract $1,4$K causal\npairs. Our experiments highlight the ongoing challenges of using statistical\nmethods and\/or LLMs for automatic abstraction identification and causal\ndiscovery in NLP. Nonetheless, we demonstrate that the abstract causal\nknowledge provided in \\texttt{ACCESS} can be leveraged for enhancing QA\nreasoning performance in LLMs.",
        "We characterize a binomial such that the Artinian algebra whose Macaulay dual\ngenerator is the binomial is a complete intersection. As an application, we\nprove that the Artinian algebra with a binomial Macaulay dual generator has the\nstrong Lefschetz property in characteristic 0 if the Artinian algebra is a\ncomplete intersection.",
        "VINGS-Mono is a monocular (inertial) Gaussian Splatting (GS) SLAM framework\ndesigned for large scenes. The framework comprises four main components: VIO\nFront End, 2D Gaussian Map, NVS Loop Closure, and Dynamic Eraser. In the VIO\nFront End, RGB frames are processed through dense bundle adjustment and\nuncertainty estimation to extract scene geometry and poses. Based on this\noutput, the mapping module incrementally constructs and maintains a 2D Gaussian\nmap. Key components of the 2D Gaussian Map include a Sample-based Rasterizer,\nScore Manager, and Pose Refinement, which collectively improve mapping speed\nand localization accuracy. This enables the SLAM system to handle large-scale\nurban environments with up to 50 million Gaussian ellipsoids. To ensure global\nconsistency in large-scale scenes, we design a Loop Closure module, which\ninnovatively leverages the Novel View Synthesis (NVS) capabilities of Gaussian\nSplatting for loop closure detection and correction of the Gaussian map.\nAdditionally, we propose a Dynamic Eraser to address the inevitable presence of\ndynamic objects in real-world outdoor scenes. Extensive evaluations in indoor\nand outdoor environments demonstrate that our approach achieves localization\nperformance on par with Visual-Inertial Odometry while surpassing recent\nGS\/NeRF SLAM methods. It also significantly outperforms all existing methods in\nterms of mapping and rendering quality. Furthermore, we developed a mobile app\nand verified that our framework can generate high-quality Gaussian maps in real\ntime using only a smartphone camera and a low-frequency IMU sensor. To the best\nof our knowledge, VINGS-Mono is the first monocular Gaussian SLAM method\ncapable of operating in outdoor environments and supporting kilometer-scale\nlarge scenes.",
        "Distributed systems often exhibit emergent behaviors that impact their\nresilience (Franz-Kaiser et al., 2020; Adilson E. Motter, 2002; Jianxi Gao,\n2016). This paper presents a theoretical framework combining attributed graph\nmodels, flow-on-graph simulation, and sheaf-theoretic causal emergence analysis\nto evaluate system resilience. We model a distributed system as a graph with\nattributes (capturing component state and connections) and use sheaf theory to\nformalize how local interactions compose into global states. A flow simulation\non this graph propagates functional loads and failures. To assess resilience,\nwe apply the concept of causal emergence, quantifying whether macro-level\ndynamics (coarse-grained groupings) exhibit stronger causal efficacy (via\neffective information) than micro-level dynamics. The novelty lies in uniting\nsheaf-based formalization with causal metrics to identify emergent resilient\nstructures. We discuss limitless potential applications (illustrated by\nmicroservices, neural networks, and power grids) and outline future steps\ntoward implementing this framework (Lake et al., 2015).",
        "Although multi-task learning (MTL) has been a preferred approach and\nsuccessfully applied in many real-world scenarios, MTL models are not\nguaranteed to outperform single-task models on all tasks mainly due to the\nnegative effects of conflicting gradients among the tasks. In this paper, we\nfully examine the influence of conflicting gradients and further emphasize the\nimportance and advantages of achieving non-conflicting gradients which allows\nsimple but effective trade-off strategies among the tasks with stable\nperformance. Based on our findings, we propose the Gradient Deconfliction via\nOrthogonal Projections onto Subspaces (GradOPS) spanned by other task-specific\ngradients. Our method not only solves all conflicts among the tasks, but can\nalso effectively search for diverse solutions towards different trade-off\npreferences among the tasks. Theoretical analysis on convergence is provided,\nand performance of our algorithm is fully testified on multiple benchmarks in\nvarious domains. Results demonstrate that our method can effectively find\nmultiple state-of-the-art solutions with different trade-off strategies among\nthe tasks on multiple datasets.",
        "We explore the effects of coordinated users (i.e., users characterized by an\nunexpected, suspicious, or exceptional similarity) in information spreading on\nTwitter by quantifying the efficacy of their tactics in deceiving feed\nalgorithms to maximize information outreach. In particular, we investigate the\nbehavior of coordinated accounts within a large set of retweet-based\ninformation cascades identifying key differences between coordinated and\nnon-coordinated accounts in terms of position within the cascade, action delay\nand outreach. On average, coordinated accounts occupy higher positions of the\ninformation cascade (i.e., closer to the root), spread messages faster and\ninvolve a slightly higher number of users. When considering cascade metrics\nsuch as size, number of edges and height, we observe clear differences among\ninformation cascades that are associated to a systematically larger proportion\nof coordinated accounts, as confirmed by comparisons with statistical null\nmodels. To further characterize the activity of coordinated accounts we\nintroduce two new measures capturing their infectivity within the information\ncascade (i.e., their ability to involve other users) and their interaction with\nnon-coordinated accounts. Finally, we find that the interaction pattern between\nthe two classes of users follows a saturation-like process. A larger-scale\ntargeting of non-coordinated users does not require a larger amount of\ncoordinated accounts after a threshold value approximately 50%, after which\ninvolving more coordinated accounts within a cascade yields a null marginal\neffect. Our results contribute to shed light on the role of coordinated\naccounts and their effect on information diffusion."
      ]
    }
  },
  {
    "id":2411.00758,
    "research_type":"basic",
    "start_id":"b18",
    "start_title":"Inverse methods for illumination optics",
    "start_abstract":"\u2022 A submitted manuscript is the version of the article upon submission and before peer-review. There can be important differences between the submitted version and the official published version of record. People interested in the research are advised to contact the author for the final version of the publication, or visit the DOI to the publisher's website. \u2022 The final author version and the galley proof are versions of the publication after peer review. \u2022 The final published version features the final layout of the paper including the volume, issue and page numbers.",
    "start_categories":[
      "math.MP"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b6"
      ],
      "title":[
        "Introduction to Nonimaging Optics, second edition"
      ],
      "abstract":[
        "Introduction to Nonimaging Optics covers the theoretical foundations and design methods of nonimaging optics, as well as key concepts from related fields. This fully updated, revised, and expanded Second Edition: \u2022 Features a new and intuitive introduction with a basic description of the advantages of nonimaging optics \u2022 Adds new chapters on wavefronts for a prescribed output (irradiance or intensity), infinitesimal \u00e9tendue optics (generalization of the aplanatic optics), and K\u00f6hler optics and color mixing \u2022 Incorporates new material on the simultaneous multiple surface (SMS) design method in 3-D, integral invariants, and \u00e9tendue 2-D \u2022 Contains 21 chapters, 24 fully worked and several other examples, and 1,000+ illustrations, including photos of real devices \u2022 Addresses applications ranging from solar energy concentration to illumination engineering Introduction to Nonimaging Optics, Second Edition invites newcomers to explore the growing field of nonimaging optics, while providing seasoned veterans with an extensive reference book."
      ],
      "categories":[
        "physics.optics"
      ]
    },
    "list":{
      "title":[
        "Opinion Dynamics with Multiple Adversaries",
        "Text to Band Gap: Pre-trained Language Models as Encoders for\n  Semiconductor Band Gap Prediction",
        "Dynamical analysis of an HIV infection model including quiescent cells\n  and immune response",
        "GAIA: A Global, Multi-modal, Multi-scale Vision-Language Dataset for\n  Remote Sensing Image Analysis",
        "Controlling spin currents with magnon interference in a canted\n  antiferromagnet",
        "Pre-trained Models Succeed in Medical Imaging with Representation\n  Similarity Degradation",
        "Mass and Metal Flows in Isolated IllustrisTNG Halos",
        "Resonance nuclear excitation of the $^{229}$Th nucleus via electronic\n  bridge process in Th~II",
        "Adaptive Prompt: Unlocking the Power of Visual Prompt Tuning",
        "SN1987A bounds on neutrino quantum decoherence",
        "Adaptive Contextual Caching for Mobile Edge Large Language Model Service",
        "Cryogenic operation of silicon photomultiplier arrays",
        "LexGenie: Automated Generation of Structured Reports for European Court\n  of Human Rights Case Law",
        "Low-Rank Adapters Meet Neural Architecture Search for LLM Compression",
        "Strong Solutions and Quantization-Based Numerical Schemes for a Class of\n  Non-Markovian Volatility Models",
        "Shining Yourself: High-Fidelity Ornaments Virtual Try-on with Diffusion\n  Model",
        "Bulk-edge correspondence at the spin-to-integer quantum Hall effect\n  crossover in topological superconductors",
        "Privacy Ripple Effects from Adding or Removing Personal Information in\n  Language Model Training",
        "Generalized reciprocal diffractive imaging for stand-alone,\n  reference-free, fast-measurable quantitative phase microscopy",
        "Reinforcement Learning for Long-Horizon Interactive LLM Agents",
        "A Fluctuation Theory of Transport Properties in Liquid-Phase Solutions",
        "Evaluating unsupervised contrastive learning framework for MRI sequences\n  classification",
        "Paper Copilot: The Artificial Intelligence and Machine Learning\n  Community Should Adopt a More Transparent and Regulated Peer Review Process",
        "A pilot survey on globular clusters with the Wide Field Survey Telescope\n  (WFST)",
        "Flux-Driven Circular Current in a Non-Hermitian Dimerized Aharonov-Bohm\n  Ring: Impact of Physical Gain and Loss",
        "Parameter-robust Preconditioners for the Stokes-Darcy Coupled Problem\n  without Fractional Operators",
        "Sub-kHz single-frequency pulsed semiconductor laser based on NPRO\n  injection locking",
        "Direct Evidence for the $\\bar{D}D^*\/D\\bar{D}^*$ Molecular Nature of\n  $G(3900)$ Through Triangular Singularity Mechanisms",
        "Sidebands to mHz QPOs in 4U 1626$-$67 in the second spin-down state"
      ],
      "abstract":[
        "Opinion dynamics model how the publicly expressed opinions of users in a\nsocial network coevolve according to their neighbors as well as their own\nintrinsic opinion. Motivated by the real-world manipulation of social networks\nduring the 2016 US elections and the 2019 Hong Kong protests, a growing body of\nwork models the effects of a strategic actor who interferes with the network to\ninduce disagreement or polarization. We lift the assumption of a single\nstrategic actor by introducing a model in which any subset of network users can\nmanipulate network outcomes. They do so by acting according to a fictitious\nintrinsic opinion. Strategic actors can have conflicting goals, and push\ncompeting narratives. We characterize the Nash Equilibrium of the resulting\nmeta-game played by the strategic actors. Experiments on real-world social\nnetwork datasets from Twitter, Reddit, and Political Blogs show that strategic\nagents can significantly increase polarization and disagreement, as well as\nincrease the \"cost\" of the equilibrium. To this end, we give worst-case upper\nbounds on the Price of Misreporting (analogous to the Price of Anarchy).\nFinally, we give efficient learning algorithms for the platform to (i) detect\nwhether strategic manipulation has occurred, and (ii) learn who the strategic\nactors are. Our algorithms are accurate on the same real-world datasets,\nsuggesting how platforms can take steps to mitigate the effects of strategic\nbehavior.",
        "In this study, we explore the use of a transformer-based language model as an\nencoder to predict the band gaps of semiconductor materials directly from their\ntext descriptions. Quantum chemistry simulations, including Density Functional\nTheory (DFT), are computationally intensive and time-consuming, which limits\ntheir practicality for high-throughput material screening, particularly for\ncomplex systems. Shallow machine learning (ML) models, while effective, often\nrequire extensive data preprocessing to convert non-numerical material\nproperties into numerical inputs. In contrast, our approach leverages textual\ndata directly, bypassing the need for complex feature engineering. We generate\nmaterial descriptions in two formats: formatted strings combining features and\nnatural language text generated using the ChatGPT API. We demonstrate that the\nRoBERTa model, pre-trained on natural language processing tasks, performs\neffectively as an encoder for prediction tasks. With minimal fine-tuning, it\nachieves a mean absolute error (MAE) of approximately 0.33 eV, performing\nbetter than shallow machine learning models such as Support Vector Regression,\nRandom Forest, and XGBoost. Even when only the linear regression head is\ntrained while keeping the RoBERTa encoder layers frozen, the accuracy remains\nnearly identical to that of the fully trained model. This demonstrates that the\npre-trained RoBERTa encoder is highly adaptable for processing domain-specific\ntext related to material properties, such as the band gap, significantly\nreducing the need for extensive retraining. This study highlights the potential\nof transformer-based language models to serve as efficient and versatile\nencoders for semiconductor materials property prediction tasks.",
        "This research gives a thorough examination of an HIV infection model that\nincludes quiescent cells and immune response dynamics in the host. The model,\nrepresented by a system of ordinary differential equations, captures the\ncomplex interaction between the host's immune response and viral infection. The\nstudy focuses on the model's fundamental aspects, such as equilibrium analysis,\ncomputing the basic reproduction number $\\mathcal{R}_0$, stability analysis,\nbifurcation phenomena, numerical simulations, and sensitivity analysis.\n  The analysis reveals both an infection equilibrium, which indicates the\npersistence of the illness, and an infection-free equilibrium, which represents\ndisease control possibilities. Applying matrix-theoretical approaches,\nstability analysis proved that the infection-free equilibrium is both locally\nand globally stable for $\\mathcal{R}_0 < 1$. For the situation of\n$\\mathcal{R}_0 > 1$, the infection equilibrium is locally asymptotically stable\nvia the Routh--Hurwitz criterion. We also studied the uniform persistence of\nthe infection, demonstrating that the infection remains present above a\npositive threshold under certain conditions. The study also found a\ntranscritical forward-type bifurcation at $\\mathcal{R}_0 = 1$, indicating a\ncritical threshold that affects the system's behavior. The model's temporal\ndynamics are studied using numerical simulations, and sensitivity analysis\nidentifies the most significant variables by assessing the effects of parameter\nchanges on system behavior.",
        "The continuous operation of Earth-orbiting satellites generates vast and\never-growing archives of Remote Sensing (RS) images. Natural language presents\nan intuitive interface for accessing, querying, and interpreting the data from\nsuch archives. However, existing Vision-Language Models (VLMs) are\npredominantly trained on web-scraped, noisy image-text data, exhibiting limited\nexposure to the specialized domain of RS. This deficiency results in poor\nperformance on RS-specific tasks, as commonly used datasets often lack\ndetailed, scientifically accurate textual descriptions and instead emphasize\nsolely on attributes like date and location. To bridge this critical gap, we\nintroduce GAIA, a novel dataset designed for multi-scale, multi-sensor, and\nmulti-modal RS image analysis. GAIA comprises of 205,150 meticulously curated\nRS image-text pairs, representing a diverse range of RS modalities associated\nto different spatial resolutions. Unlike existing vision-language datasets in\nRS, GAIA specifically focuses on capturing a diverse range of RS applications,\nproviding unique information about environmental changes, natural disasters,\nand various other dynamic phenomena. The dataset provides a spatially and\ntemporally balanced distribution, spanning across the globe, covering the last\n25 years with a balanced temporal distribution of observations. GAIA's\nconstruction involved a two-stage process: (1) targeted web-scraping of images\nand accompanying text from reputable RS-related sources, and (2) generation of\nfive high-quality, scientifically grounded synthetic captions for each image\nusing carefully crafted prompts that leverage the advanced vision-language\ncapabilities of GPT-4o. Our extensive experiments, including fine-tuning of\nCLIP and BLIP2 models, demonstrate that GAIA significantly improves performance\non RS image classification, cross-modal retrieval and image captioning tasks.",
        "Controlling spin current lies at the heart of spintronics and its\napplications. The sign of spin currents is monotonous in ferromagnets once the\ncurrent direction is determined. Spin currents in antiferromagnets can possess\nopposite polarization, but requires enormous magnetic fields to lift the\ndegeneracy. Controlling spin currents with different polarization is urgently\ndemanded but remains hitherto elusive. Here, we demonstrate the control of spin\ncurrents at room temperature by magnon interference in a canted\nantiferromagnet, hematite recently also classified as an altermagnet.\nMagneto-optical characterization by Brillouin light scattering revealed that\nthe spatial periodicity of the beating patterns was tunable via the microwave\nfrequency. The inverse spin-Hall voltage changed sign as the frequency was\nscanned, i.e., a frequency-controlled switching of polarization in pure spin\ncurrents was obtained. Our work marks the use of antiferromagnetic magnon\ninterference to control spin currents, which substantially extends the horizon\nfor the emerging field of coherent antiferromagnetic spintronics.",
        "This paper investigates the critical problem of representation similarity\nevolution during cross-domain transfer learning, with particular focus on\nunderstanding why pre-trained models maintain effectiveness when adapted to\nmedical imaging tasks despite significant domain gaps. The study establishes a\nrigorous problem definition centered on quantifying and analyzing\nrepresentation similarity trajectories throughout the fine-tuning process,\nwhile carefully delineating the scope to encompass both medical image analysis\nand broader cross-domain adaptation scenarios. Our empirical findings reveal\nthree critical discoveries: the potential existence of high-performance models\nthat preserve both task accuracy and representation similarity to their\npre-trained origins; a robust linear correlation between layer-wise similarity\nmetrics and representation quality indicators; and distinct adaptation patterns\nthat differentiate supervised versus self-supervised pre-training paradigms.\nThe proposed similarity space framework not only provides mechanistic insights\ninto knowledge transfer dynamics but also raises fundamental questions about\noptimal utilization of pre-trained models. These results advance our\nunderstanding of neural network adaptation processes while offering practical\nimplications for transfer learning strategies that extend beyond medical\nimaging applications. The code will be available once accepted.",
        "The cicumgalactic medium (CGM) is a reservoir of metals and star-forming\nfuel. Most baryons in the universe are in the circumgalactic medium (CGM) or\nintergalactic medium (IGM). The baryon cycle -- how mass and metals reach the\nCGM from the inner regions of the galaxy and how gas from the CGM replenishes\nstar-forming activity in the inner regions -- is an essential question in\ngalaxy evolution. In this paper, we study the flow of mass and metals in a\nstacked sample of 2770 isolated halos from the IllustrisTNG cosmological\nhydrodynamic simulation. The mean gas flow as a function of radius and angle is\nsimilar across a large galactic mass range when accounting for different\nfeedback modes. Although both star formation and black holes cause powerful\noutflows, the flows from star formation are more angularly restricted. Black\nhole feedback dominates massflow throughout the halo, while star-formation\nfeedback mainly affects the inner region. When scaling by virial radius\n($R_v$), large dynamical changes occur at $0.2R_v$ for most halos, suggesting a\ncharacteristic size for the inner galaxy. Despite radio mode feedback from\nblack holes being the primary quenching mechanism in IllustrisTNG, a small\npopulation of high mass radio mode disks are able to form stars.",
        "The 8.4 eV transition in the $^{229}$Th nucleus is the basis for a\nhigh-precision nuclear clock with exceptional sensitivity to new physics\neffects. We have identified several cases in the Th$^+$ ion where electronic\nexcitations closely resonate with the nuclear excitation, with the smallest\nenergy difference being $\\Delta = -0.09$ cm$^{-1}$. We investigate the\nelectronic bridge process, in which nuclear excitation is induced via\nelectronic transitions, and demonstrate that a proper selection of laser\nfrequencies can lead to a dramatic enhancement of this effect. Additionally, we\nshow that the interaction with electrons significantly shortens the lifetime of\nthe nuclear excited state.",
        "Visual Prompt Tuning (VPT) has recently emerged as a powerful method for\nadapting pre-trained vision models to downstream tasks. By introducing\nlearnable prompt tokens as task-specific instructions, VPT effectively guides\npre-trained transformer models with minimal overhead. Despite its empirical\nsuccess, a comprehensive theoretical understanding of VPT remains an active\narea of research. Building on recent insights into the connection between\nmixture of experts and prompt-based approaches, we identify a key limitation in\nVPT: the restricted functional expressiveness in prompt formulation. To address\nthis limitation, we propose Visual Adaptive Prompt Tuning (VAPT), a new\ngeneration of prompts that redefines prompts as adaptive functions of the\ninput. Our theoretical analysis shows that this simple yet intuitive approach\nachieves optimal sample efficiency. Empirical results on VTAB-1K and FGVC\nfurther demonstrate VAPT's effectiveness, with performance gains of 7.34% and\n1.04% over fully fine-tuning baselines, respectively. Notably, VAPT also\nsurpasses VPT by a substantial margin while using fewer parameters. These\nresults highlight both the effectiveness and efficiency of our method and pave\nthe way for future research to explore the potential of adaptive prompts.",
        "We obtain stringent bounds on neutrino quantum decoherence from the analysis\nof SN1987A data. We show that for the decoherence model considered here, which\nallows for neutrino-loss along the trajectory, the bounds are many orders of\nmagnitude stronger than the ones that can be obtained from the analysis of data\nfrom reactor neutrino oscillation experiments or neutrino telescopes.",
        "Mobile edge Large Language Model (LLM) deployments face inherent constraints,\nsuch as limited computational resources and network bandwidth. Although\nRetrieval-Augmented Generation (RAG) mitigates some challenges by integrating\nexternal knowledge bases, inefficient cache management can still result in high\nretrieval latency and frequent cache updates. To address these issues, we\npropose an Adaptive Contextual Caching (ACC) framework that anticipates user\nneeds by proactively caching semantically relevant data for mobile-edge LLMs.\nACC utilizes a deep reinforcement learning (DRL) module to refine cache\nreplacement policies, balancing user context, document similarity, and the\noverhead associated with cache misses. Experimental results demonstrate that\nACC increases cache hit rates to over 80\\% after only 11 training episodes,\noutperforming FIFO, LRU, and semantic-only caching while reducing retrieval\nlatency by up to 40\\%. In particular, ACC also reduces local caching overhead\n(i.e., the cost of updating the cache when a miss occurs) by as much as 55\\%,\nenabling scalable, low-latency LLM services in resource-constrained edge\nenvironments.",
        "The LHCb experiment at CERN has been upgraded for the Run 3 operation of the\nLarge Hadron Collider (LHC). A new concept of tracking detector based on\nScintillating Fibres (SciFi) read out with multichannel silicon\nphotomultipliers (SiPMs) was installed during its upgrade. One of the main\nchallenges the SciFi tracker will face during the Run 4 operation of the LHC is\nthe higher radiation environment due to fast neutrons, where the SiPMs are\nlocated. To cope with the increase in radiation, cryogenic cooling with liquid\nnitrogen is being investigated as a possible solution to mitigate the\nperformance degradation of the SiPMs induced by radiation damage. Thus, a\ndetailed performance study of different layouts of SiPM arrays produced by\nFondazione Bruno Kessler (FBK) and Hamamatsu Photonics K.K. is being carried\nout. These SiPMs have been designed to operate at cryogenic temperatures.\nSeveral SiPMs have been tested in a dedicated cryogenic setup down to 100 K.\nKey performance parameters such as breakdown voltage, dark count rate, photon\ndetection efficiency, gain and direct cross-talk are characterized as a\nfunction of the temperature. The main results of this study are going to be\npresented here.",
        "Analyzing large volumes of case law to uncover evolving legal principles,\nacross multiple cases, on a given topic is a demanding task for legal\nprofessionals. Structured topical reports provide an effective solution by\nsummarizing key issues, principles, and judgments, enabling comprehensive legal\nanalysis on a particular topic. While prior works have advanced query-based\nindividual case summarization, none have extended to automatically generating\nmulti-case structured reports. To address this, we introduce LexGenie, an\nautomated LLM-based pipeline designed to create structured reports using the\nentire body of case law on user-specified topics within the European Court of\nHuman Rights jurisdiction. LexGenie retrieves, clusters, and organizes relevant\npassages by topic to generate a structured outline and cohesive content for\neach section. Expert evaluation confirms LexGenie's utility in producing\nstructured reports that enhance efficient, scalable legal analysis.",
        "The rapid expansion of Large Language Models (LLMs) has posed significant\nchallenges regarding the computational resources required for fine-tuning and\ndeployment. Recent advancements in low-rank adapters have demonstrated their\nefficacy in parameter-efficient fine-tuning (PEFT) of these models. This\nretrospective paper comprehensively discusses innovative approaches that\nsynergize low-rank representations with Neural Architecture Search (NAS)\ntechniques, particularly weight-sharing super-networks. Robust solutions for\ncompressing and fine-tuning large pre-trained models are developed by\nintegrating these methodologies. Our analysis highlights the potential of these\ncombined strategies to democratize the use of LLMs, making them more accessible\nfor deployment in resource-constrained environments. The resulting models\nexhibit reduced memory footprints and faster inference times, paving the way\nfor more practical and scalable applications of LLMs. Models and code are\navailable at\nhttps:\/\/github.com\/IntelLabs\/Hardware-Aware-Automated-Machine-Learning.",
        "We investigate a class of non-Markovian processes that hold particular\nrelevance in the realm of mathematical finance. This family encompasses\npath-dependent volatility models, including those pioneered by [Platen and\nRendek, 2018] and, more recently, by [Guyon and Lekeufack, 2023], as well as an\nextension of the framework proposed by [Blanc et al., 2017]. Our study unfolds\nin two principal phases. In the first phase, we introduce a functional\nquantization scheme based on an extended version of the Lamperti transformation\nthat we propose to handle the presence of a memory term incorporated into the\ndiffusion coefficient. For scenarios involving a Brownian integral in the\ndiffusion term, we propose alternative numerical schemes that leverage the\npower of marginal recursive quantization. In the second phase, we study the\nproblem of existence and uniqueness of a strong solution for the SDEs related\nto the examples that motivate our study, in order to provide a theoretical\nbasis to correctly apply the proposed numerical schemes.",
        "While virtual try-on for clothes and shoes with diffusion models has gained\nattraction, virtual try-on for ornaments, such as bracelets, rings, earrings,\nand necklaces, remains largely unexplored. Due to the intricate tiny patterns\nand repeated geometric sub-structures in most ornaments, it is much more\ndifficult to guarantee identity and appearance consistency under large pose and\nscale variances between ornaments and models. This paper proposes the task of\nvirtual try-on for ornaments and presents a method to improve the geometric and\nappearance preservation of ornament virtual try-ons. Specifically, we estimate\nan accurate wearing mask to improve the alignments between ornaments and models\nin an iterative scheme alongside the denoising process. To preserve structure\ndetails, we further regularize attention layers to map the reference ornament\nmask to the wearing mask in an implicit way. Experimental results demonstrate\nthat our method successfully wears ornaments from reference images onto target\nmodels, handling substantial differences in scale and pose while preserving\nidentity and achieving realistic visual effects.",
        "The spin and integer quantum Hall effects are two cousins of topological\nphase transitions in two-dimensional electronic systems. Their close\nrelationship makes it possible to transform spin to integer quantum Hall effect\nin two-dimensional topological superconductors by continuous increase in a\nsymmetry breaking Zeeman magnetic field. We study peculiarities of bulk-edge\ncorrespondence and a fate of massless edge and bulk topological (instantons)\nexcitations at such the crossover.",
        "Due to the sensitive nature of personally identifiable information (PII), its\nowners may have the authority to control its inclusion or request its removal\nfrom large-language model (LLM) training. Beyond this, PII may be added or\nremoved from training datasets due to evolving dataset curation techniques,\nbecause they were newly scraped for retraining, or because they were included\nin a new downstream fine-tuning stage. We find that the amount and ease of PII\nmemorization is a dynamic property of a model that evolves throughout training\npipelines and depends on commonly altered design choices. We characterize three\nsuch novel phenomena: (1) similar-appearing PII seen later in training can\nelicit memorization of earlier-seen sequences in what we call assisted\nmemorization, and this is a significant factor (in our settings, up to 1\/3);\n(2) adding PII can increase memorization of other PII significantly (in our\nsettings, as much as $\\approx\\!7.5\\times$); and (3) removing PII can lead to\nother PII being memorized. Model creators should consider these first- and\nsecond-order privacy risks when training models to avoid the risk of new PII\nregurgitation.",
        "Optical microscopy has been employed to derive salient characteristics of an\nobject in various fields, including cell biology, flow cytometry, biopsy, and\nneuroscience. In particular, measuring the phase of light scattered from an\nobject aroused great interest by allowing retrieving quantitative parameters\nsuch as refractive index, an intrinsic property of a material. Reciprocal\ndiffractive imaging (RDI) has succeeded in recovering the light field scattered\nfrom diffusive objects without special restrictions on illumination and sample\nsupport from a single-shot intensity in the reference-free regime. However, RDI\nis limited to imaging samples in the diffusive regime, making application to\nbiological samples difficult. Here, we extend RDI to biological applications by\nspatially filtering the transmitted fields in the pupil plane. The proposed\nmethod is demonstrated by imaging the objects with known structures and various\nbiological samples, showing its capability as a stand-alone optical microscope.\nWe believe that the presented advance could be at the forefront of quantitative\nphase imaging due to the unique advantages the technique possesses.",
        "Interactive digital agents (IDAs) leverage APIs of stateful digital\nenvironments to perform tasks in response to user requests. While IDAs powered\nby instruction-tuned large language models (LLMs) can react to feedback from\ninterface invocations in multi-step exchanges, they have not been trained in\ntheir respective digital environments. Prior methods accomplish less than half\nof tasks in sophisticated benchmarks such as AppWorld. We present a\nreinforcement learning (RL) approach that trains IDAs directly in their target\nenvironments. We formalize this training as a partially observable Markov\ndecision process and derive LOOP, a data- and memory-efficient variant of\nproximal policy optimization. LOOP uses no value network and maintains exactly\none copy of the underlying LLM in memory, making its implementation\nstraightforward and as memory-efficient as fine-tuning a single LLM. A\n32-billion-parameter agent trained with LOOP in the AppWorld environment\noutperforms the much larger OpenAI o1 agent by 9 percentage points (15%\nrelative). To our knowledge, this is the first reported application of RL to\nIDAs that interact with a stateful, multi-domain, multi-app environment via\ndirect API calls. Our analysis sheds light on the effectiveness of RL in this\narea, showing that the agent learns to consult the API documentation, avoid\nunwarranted assumptions, minimize confabulation, and recover from setbacks.",
        "The challenge of comprehensively describing liquids and their mixtures beyond\nequilibrium continues to be a main concern in modern chemical physics and\nphysical chemistry, particularly in the context of calculating the transport\nproperties of liquid-phase systems. This paper presents a step towards a\nphenomenological nonequilibrium theory tailored for multicomponent liquid-phase\nsolutions. This field-theoretical framework, grounded in the principles of\nnonequilibrium statistical mechanics, integrates quasi-stationary concentration\nfluctuations consistent with equilibrium liquid theory as described by\nclassical density functional theory. This approach, which is a phenomenological\nextension of the well-known Dean-Kawasaki stochastic density functional theory,\nallows for the computation of practically important transport properties such\nas mobility and shear viscosity. We apply our approach to the calculation of\nthe corresponding quantities for solutions with a single solute. We derive\ngeneral formulas for the mobility of solute molecules and the shear viscosity\nof a single-solute solution. Based on these findings, we present new results\nand reproduce previously established results for systems such as the\nGaussian-core model, one-component plasma, and near-critical solutions.",
        "The automatic identification of Magnetic Resonance Imaging (MRI) sequences\ncan streamline clinical workflows by reducing the time radiologists spend\nmanually sorting and identifying sequences, thereby enabling faster diagnosis\nand treatment planning for patients. However, the lack of standardization in\nthe parameters of MRI scans poses challenges for automated systems and\ncomplicates the generation and utilization of datasets for machine learning\nresearch. To address this issue, we propose a system for MRI sequence\nidentification using an unsupervised contrastive deep learning framework. By\ntraining a convolutional neural network based on the ResNet-18 architecture,\nour system classifies nine common MRI sequence types as a 9-class\nclassification problem. The network was trained using an in-house internal\ndataset and validated on several public datasets, including BraTS, ADNI, Fused\nRadiology-Pathology Prostate Dataset, the Breast Cancer Dataset (ACRIN), among\nothers, encompassing diverse acquisition protocols and requiring only 2D slices\nfor training. Our system achieves a classification accuracy of over 0.95 across\nthe nine most common MRI sequence types.",
        "The rapid growth of submissions to top-tier Artificial Intelligence (AI) and\nMachine Learning (ML) conferences has prompted many venues to transition from\nclosed to open review platforms. Some have fully embraced open peer reviews,\nallowing public visibility throughout the process, while others adopt hybrid\napproaches, such as releasing reviews only after final decisions or keeping\nreviews private despite using open peer review systems. In this work, we\nanalyze the strengths and limitations of these models, highlighting the growing\ncommunity interest in transparent peer review. To support this discussion, we\nexamine insights from Paper Copilot, a website launched two years ago to\naggregate and analyze AI \/ ML conference data while engaging a global audience.\nThe site has attracted over 200,000 early-career researchers, particularly\nthose aged 18-34 from 177 countries, many of whom are actively engaged in the\npeer review process. Drawing on our findings, this position paper advocates for\na more transparent, open, and well-regulated peer review aiming to foster\ngreater community involvement and propel advancements in the field.",
        "We carry out an imaging survey of six globular clusters (GCs) with a limit\nmagnitude to 22 mag at the 5 sigma level, down to the main sequence stars of\nthe respective cluster, as one of the pilot observing program of the Wide Field\nSurvey Telescope (WFST). This paper present the early results of this survey,\nwhere we investigate the tidal characters at the periphery of the clusters NGC\n4147, NGC 5024, NGC 5053, NGC 5272, NGC 5904 and NGC 6341. We present the\nestimated number density of cluster candidates and their spatial distribution.\nWe confirm the presence of tidal arms in NGC 4147 and NGC 5904 and identify\nseveral intriguing potential tidal structures in NGC 4147, NGC 5024, NGC 5272,\ncorroborated the elliptical morphology of the periphery of NGC 6341. Our\nfindings underscore the WFST's capability for probing faint structural features\nin GCs, paving the way for future in-depth studies, especially for the search\nof the large scale tidal streams associated with the clusters with the future\nwide field survey.",
        "In the present work, we explore magnetic response of a dimerized ring\nsubjected to Aharonov-Bohm (AB) flux and environmental interactions.\nSpecifically, we introduce an imaginary site potential on the odd lattice sites\nto represent physical gain and loss, while the even lattice sites remain\nunperturbed. We investigate the induced current resulting from the AB flux in\nboth real and imaginary eigenspaces, aiming to enhance this current\nsignificantly by adjusting the gain\/loss parameter ($d$). Our analysis focuses\non how exceptional points in the real and imaginary eigenenergy spaces\ncontribute to notable increases in current at specific $d$ values, and the\nemergence of purely real current when the imaginary current vanishes. We focus\non how the converging and diverging nature of the energy spectrum leads to\ngradual increases and decreases in the current. Additionally, we study the\ninterplay between the correlations of dimerized hopping integrals and the\ngain-loss parameter, which affects the current and highlights key features\nassociated with these physical parameters. Furthermore, we consider how system\nsize impacts our findings. These investigations may reveal unconventional\ncharacteristics in various loop configurations, potentially paving the way for\nnew research directions.",
        "We consider the Stokes-Darcy coupled problem, which models the interaction\nbetween free-flow and porous medium flow. By enforcing the normal flux\ncontinuity interface condition directly within the finite-element spaces, we\nestablish unified well-posedness results for the coupled system under various\nboundary condition scenarios. Using the operator preconditioning framework, we\ndevelop a parameter-robust preconditioner that avoids the use of fractional\noperators. Numerical experiments employing both\n$H(\\operatorname{div})$-conforming and nonconforming finite-element methods are\npresented to confirm the theoretical findings and demonstrate the robustness of\nthe proposed block preconditioners with respect to the physical parameters and\nmesh size.",
        "We report a single-frequency, narrow-linewidth semiconductor pulsed laser\nbased on pump current modulation and optical injection locking technique. A\nmonolithic non-planar ring oscillator laser is employed as the seed source to\nguarantee the single-frequency narrow-linewidth performance. Simultaneously,\npulse operation is achieved by directly modulating the pump current of the\nsemiconductor laser. The single-frequency pulsed laser (SFPL) has achieved a\npulse repetition rate of 50 kHz-1 MHz, a pulse duration ranging from 120 ns to\na quasi-continuous state, and a peak power of 160 mW. Moreover, the SFPL has\nreached a pulsed laser linewidth as narrow as 905 Hz, optical spectrum\nsignal-to-noise ratio of better than 65 dB at a center wavelength of 1064.45\nnm. Such extremely narrow-linewidth, repetition-rate and pulse-width tunable\nSFPL has great potential for applications in coherent LIDAR, metrology, remote\nsensing, and nonlinear frequency conversion.",
        "The exotic hadron $G(3900)$, initially observed in the process $e^+ e^- \\to\nD\\bar{D}$, has been further supported by analyses from the BESIII\nCollaboration, which classify it as a $P$-wave molecular state of\n$D^{+}D^{*-}\/D^{-}D^{*+}$. However, theoretical discussions raise concerns\nabout its status as a true particle, emphasizing the need for additional\nstudies. In this Letter, we employ the triangular singularity mechanism to\ninvestigate $G(3900)$ across various reaction channels, allowing us to produce\nsignificant peaks without relying on the existence of a real particle. We\nidentify $X(4020)$, $Y(4320)$, and the tentative $X(4014)$ as potential sources\nof these peaks via decay processes to $\\gamma G(3900)$ or $\\pi G(3900)$. We\nstress the importance of experimental explorations of $e^+ e^- \\to X\/Y \\to\n\\gamma(\\pi) G(3900)$, which are essential for confirming the molecular\ncomposition of $D^{+}D^{*-}\/D^{-}D^{*+}$ and refining the mass measurement of\n$G(3900)$.",
        "We report results from an $AstroSat$ Target-of-Opportunity (ToO) observation\nof 4U 1626$-$67, performed on 2023 May 18, soon after the discovery of torque\nreversal to spin-down in the source. The X-ray emission exhibited significant\ndependence on both energy and torque state. This work highlights the comparison\nof timing features of 4U 1626$-$67 with a previous $AstroSat$ observation from\n2018, when the neutron star was in the spin-up state. The power density\nspectrum (PDS) of the 2023 observation comprised a sharp peak corresponding to\n$\\nu_{\\rm NS}\\sim$130 mHz X-ray pulsations along with a prominent\nquasi-periodic oscillation (QPO) feature at $\\nu_{\\rm QPO}\\sim$46 mHz with\n$\\sim$20\\% rms amplitude, which was positively correlated with energy. We also\nreport the detection of sidebands to QPO occurring at a beat frequency\n($\\nu_{\\rm NS}-\\nu_{\\rm QPO}$) of $\\sim$83 mHz with $\\sim$8\\% rms amplitude,\nhaving $>3\\sigma$ detection significance. Additionally, we utilized $Nuclear\n~Spectroscopic ~Telescope ~ARray$ ($NuSTAR$) observations from the same torque\nstate (2023 May-July) to analogize the presence and energy dependence of\nsidebands. The source retains timing properties in this spin-down torque state,\nsimilar to those seen in the previous spin-down phase. In sharp contrast, PDS\nfrom the 2018 observation was dominated by red noise, an absence of QPOs and a\nbroadening in the wings of the pulse frequency peak, indicating a coupling\nbetween periodic and low-frequency aperiodic variability. Furthermore, we\ndetected the known cyclotron resonance scattering feature (CRSF) at 37 keV in\nthe Large Area X-ray Proportional Counter (LAXPC) spectrum. We explore various\nmechanisms that could possibly explain the presence of QPOs exclusively during\nthe spin-down state."
      ]
    }
  },
  {
    "id":2411.01291,
    "research_type":"applied",
    "start_id":"b15",
    "start_title":"CMRxRecon2024: A Multi-Modality, Multi-View K-Space Dataset Boosting\n  Universal Machine Learning for Accelerated Cardiac MRI",
    "start_abstract":"Cardiac magnetic resonance imaging (MRI) has emerged as a clinically gold-standard technique for diagnosing cardiac diseases, thanks to its ability provide diverse information with multiple modalities and anatomical views. Accelerated MRI is highly expected achieve time-efficient patient-friendly imaging, then advanced image reconstruction approaches are required recover high-quality, interpretable images from undersampled measurements. However, the lack of publicly available k-space dataset in terms both quantity diversity severely hindered substantial technological progress, particularly data-driven artificial intelligence. Here, we standardized, diverse, high-quality CMRxRecon2024 facilitate technical development, fair evaluation, clinical transfer approaches, towards promoting universal frameworks that enable fast robust reconstructions across different protocols practice. To best our knowledge, largest most dataset. It acquired 330 healthy volunteers, covering commonly used modalities, views, acquisition trajectories workflows. Besides, an open platform tutorials, benchmarks, data processing tools provided usage, method performance evaluation.",
    "start_categories":[
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b19"
      ],
      "title":[
        "vSHARP: variable Splitting Half-quadratic ADMM algorithm for Reconstruction of inverse-Problems"
      ],
      "abstract":[
        "Medical Imaging (MI) tasks, such as accelerated parallel Magnetic Resonance Imaging (MRI), often involve reconstructing an image from noisy or incomplete measurements. This amounts to solving ill-posed inverse problems, where a satisfactory closed-form analytical solution is not available. Traditional methods such as Compressed Sensing (CS) in MRI reconstruction can be time-consuming or prone to obtaining low-fidelity images. Recently, a plethora of Deep Learning (DL) approaches have demonstrated superior performance in inverse-problem solving, surpassing conventional methods. In this study, we propose vSHARP (variable Splitting Half-quadratic ADMM algorithm for Reconstruction of inverse Problems), a novel DL-based method for solving ill-posed inverse problems arising in MI. vSHARP utilizes the Half-Quadratic Variable Splitting method and employs the Alternating Direction Method of Multipliers (ADMM) to unroll the optimization process. For data consistency, vSHARP unrolls a differentiable gradient descent process in the image domain, while a DL-based denoiser, such as a U-Net architecture, is applied to enhance image quality. vSHARP also employs a dilated-convolution DL-based model to predict the Lagrange multipliers for the ADMM initialization. We evaluate vSHARP on tasks of accelerated parallel MRI Reconstruction using two distinct datasets and on accelerated parallel dynamic MRI Reconstruction using another dataset. Our comparative analysis with state-of-the-art methods demonstrates the superior performance of vSHARP in these applications."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Developing and Evaluating an AI-Assisted Prediction Model for Unplanned\n  Intensive Care Admissions following Elective Neurosurgery using Natural\n  Language Processing within an Electronic Healthcare Record System",
        "A Vehicle-Infrastructure Multi-layer Cooperative Decision-making\n  Framework",
        "Three-body structures of low-lying nuclear states of $^8$Li",
        "Nonequilibrium Continuous Transition in a Fast Rotating Turbulence",
        "Training Allostery-Inspired Mechanical Response in Disordered Elastic\n  Networks",
        "Spin glass behavior in amorphous CrSiTe3 alloy",
        "Representative dietary behavior patterns and associations with\n  cardiometabolic outcomes in Puerto Rico using a Bayesian latent class\n  analysis for non-probability samples",
        "On the coefficients of Tutte polynomials with one variable at 1",
        "Enhancing, Refining, and Fusing: Towards Robust Multi-Scale and Dense\n  Ship Detection",
        "Fluctuations in the email size modeled by a log-normal-like distribution",
        "Heterogeneity Matters even More in Distributed Learning: Study from\n  Generalization Perspective",
        "From Dense to Dynamic: Token-Difficulty Driven MoEfication of\n  Pre-Trained LLMs",
        "Differentiable Information Enhanced Model-Based Reinforcement Learning",
        "Underwater Soft Fin Flapping Motion with Deep Neural Network Based\n  Surrogate Model",
        "Automatic detection of single-electron regime of quantum dots and\n  definition of virtual gates using U-Net and clustering",
        "Agricultural Field Boundary Detection through Integration of \"Simple\n  Non-Iterative Clustering (SNIC) Super Pixels\" and \"Canny Edge Detection\n  Method\"",
        "A rigid origami elliptic-hyperbolic vertex duality",
        "Automation of Electroweak Corrections",
        "Momentum-Resolved Signatures of Carrier Screening Effects on\n  Electron-Phonon Coupling in MoS$_2$",
        "K-Anonymous A\/B Testing",
        "CLIMB-3D: Continual Learning for Imbalanced 3D Instance Segmentation",
        "DenseSplat: Densifying Gaussian Splatting SLAM with Neural Radiance\n  Prior",
        "Optimal Transport Barycenter via Nonconvex-Concave Minimax Optimization",
        "Digital Twin Buildings: 3D Modeling, GIS Integration, and Visual\n  Descriptions Using Gaussian Splatting, ChatGPT\/Deepseek, and Google Maps\n  Platform",
        "Cyclotron Maser Cooling towards Coherent Particle Beams",
        "Wetting simulation of the porous structure of a heat pipe using an\n  eXtended Discontinuous Galerkin Method and a Parameterized Level-Set",
        "From Traditional to Deep Learning Approaches in Whole Slide Image\n  Registration: A Methodological Review",
        "Towards modeling the short-range interactions of hidden\/open charm\n  pentaquark molecular states",
        "On the Nowicki Conjecture for the free Lie algebra of rank 2"
      ],
      "abstract":[
        "Introduction: Timely care in a specialised neuro-intensive therapy unit (ITU)\nreduces mortality and hospital stays, with planned admissions being safer than\nunplanned ones. However, post-operative care decisions remain subjective. This\nstudy used artificial intelligence (AI), specifically natural language\nprocessing (NLP) to analyse electronic health records (EHRs) and predict ITU\nadmissions for elective surgery patients. Methods: This study analysed the EHRs\nof elective neurosurgery patients from University College London Hospital\n(UCLH) using NLP. Patients were categorised into planned high dependency unit\n(HDU) or ITU admission; unplanned HDU or ITU admission; or ward \/ overnight\nrecovery (ONR). The Medical Concept Annotation Tool (MedCAT) was used to\nidentify SNOMED-CT concepts within the clinical notes. We then explored the\nutility of these identified concepts for a range of AI algorithms trained to\npredict ITU admission. Results: The CogStack-MedCAT NLP model, initially\ntrained on hospital-wide EHRs, underwent two refinements: first with data from\npatients with Normal Pressure Hydrocephalus (NPH) and then with data from\nVestibular Schwannoma (VS) patients, achieving a concept detection F1-score of\n0.93. This refined model was then used to extract concepts from EHR notes of\n2,268 eligible neurosurgical patients. We integrated the extracted concepts\ninto AI models, including a decision tree model and a neural time-series model.\nUsing the simpler decision tree model, we achieved a recall of 0.87 (CI 0.82 -\n0.91) for ITU admissions, reducing the proportion of unplanned ITU cases missed\nby human experts from 36% to 4%. Conclusion: The NLP model, refined for\naccuracy, has proven its efficiency in extracting relevant concepts, providing\na reliable basis for predictive AI models to use in clinically valid\napplications.",
        "Autonomous driving has entered the testing phase, but due to the limited\ndecision-making capabilities of individual vehicle algorithms, safety and\nefficiency issues have become more apparent in complex scenarios. With the\nadvancement of connected communication technologies, autonomous vehicles\nequipped with connectivity can leverage vehicle-to-vehicle (V2V) and\nvehicle-to-infrastructure (V2I) communications, offering a potential solution\nto the decision-making challenges from individual vehicle's perspective. We\npropose a multi-level vehicle-infrastructure cooperative decision-making\nframework for complex conflict scenarios at unsignalized intersections. First,\nbased on vehicle states, we define a method for quantifying vehicle impacts and\ntheir propagation relationships, using accumulated impact to group vehicles\nthrough motif-based graph clustering. Next, within and between vehicle groups,\na pass order negotiation process based on Large Language Models (LLM) is\nemployed to determine the vehicle passage order, resulting in planned vehicle\nactions. Simulation results from ablation experiments show that our approach\nreduces negotiation complexity and ensures safer, more efficient vehicle\npassage at intersections, aligning with natural decision-making logic.",
        "The four nucleons in $^8$Li outside the $\\alpha$-particle ($\\alpha=^4$He) can\nbe divided into pairs of one neutron ($n$) and 3 nucleons in the triton\n($t=^3$H), or 2 in the deuteron ($d=^2$H) and two neutrons in a dineutron\n($^2n$). The corresponding three-body structures, $\\alpha$+$t$+$n$ or\n$\\alpha$+$d$+$^2n$, are suggested to describe the bulk part of the low-energy\n($<10$~MeV) states of $^8$Li. Several breakup thresholds influence the\nstructures and possible decays. We calculate the three-body structures of the\nvarious $J^{\\pi}$ states, where different clustering appear, e.g. $^7$Li*+$n$,\n$^6$Li*$+^2n$, $^6$He*$+d$. The experimental $^8$Li spectrum can be reproduced\nwith fine tuning by a three-body potential parameter. Three unobserved $0^+$\nand an excited 2$^+$ states are found. All states appear as bound states or\nresonances. The lowest or highest energies have cluster structures,\n$\\alpha$+$t$+$n$ or $\\alpha$+$d$+$^2n$, respectively. We give calculated energy\nand width (if possible), geometry, and partial wave decomposition for all\nstates.",
        "We study the saturation of three-dimensional unstable perturbations on a fast\nrotating turbulent flow using direct numerical simulations (DNSs). Under the\neffect of Kolmogorov forcing, a transition between states dominated by coherent\ntwo-dimensional modes to states with three-dimensional variations\n(quasi-two-dimensional) is observed as we change the global rotation rate. We\nfind this akin to a critical phenomenon, wherein the order parameter scales\nwith the distance to the critical point raised to an exponent. The exponent\nitself deviates from the predicted mean field value. Also, the nature of the\nfluctuations of the order parameter near the critical point indicate the\npresence of on-off intermittency. The critical rotation rate at which the\ntransition occurs exhibits a linear scaling behaviour with the forcing wave\nnumber. A reduced model based on linear stability analysis is used to find the\nlinear threshold estimates; we find these to be in good agreement with the 3D\nnonlinear DNS results.",
        "Disordered elastic networks are a model material system in which it is\npossible to achieve tunable and trainable functions. This work investigates the\nmodification of local mechanical properties in disordered networks inspired by\nallosteric interactions in proteins: applying strain locally to a set of source\nnodes triggers a strain response at a distant set of target nodes. This is\ndemonstrated first by using directed aging to modify the existing mechanical\ncoupling between pairs of distant source and target nodes, and later as a means\nfor inducing coupling between formerly isolated source-target pairs. The\nexperimental results are compared with those predicted by simulations.",
        "Owing to the intrinsically high crystallization temperatures, layered\nphase-change materials, such as CrGeTe3 and InGeTe3, are attracting attention\nfor embedded memory applications, In addition to the electrical contrast, a\nmajor change in magnetic properties is observed in CrGeTe3 upon switching from\nthe crystalline to the amorphous state. In this work, we report a combined ab\ninitio modeling and magnetic characterization study on the isostructural\nsilicon parent compound of CrGeTe3, namely, CrSiTe3. Amorphous CrSiTe3 has\nsimilar structural properties to amorphous CrGeTe3; however, it shows a smaller\nenergy difference between the ferromagnetic configuration and the random\nmagnetic configuration, indicating a high probability of spin glass formation.\nIndeed, direct-current and alternating-current magnetic measurements show that\nthe coercive force of amorphous CrSiTe3 is higher than that of amorphous\nCrGeTe3. Therefore, the pinning effect of spins is enhanced in amorphous\nCrSiTe3, leading to a more robust spin glass state with a higher freezing\ntemperature. The large magnetic contrast between the amorphous and crystalline\nphase could make CrSiTe3 a potential candidate for phase-change magnetic\nswitching applications.",
        "There is limited understanding of how dietary behaviors cluster together and\ninfluence cardiometabolic health at a population level in Puerto Rico. Data\navailability is scarce, particularly outside of urban areas, and is often\nlimited to non-probability sample (NPS) data where sample inclusion mechanisms\nare unknown. In order to generalize results to the broader Puerto Rican\npopulation, adjustments are necessary to account for selection bias but are\ndifficult to implement for NPS data. Although Bayesian latent class models\nenable summaries of dietary behavior variables through underlying patterns,\nthey have not yet been adapted to the NPS setting. We propose a novel Weighted\nOverfitted Latent Class Analysis for Non-probability samples (WOLCAN). WOLCAN\nutilizes a quasi-randomization framework to (1) model pseudo-weights for an NPS\nusing Bayesian additive regression trees (BART) and a reference probability\nsample, and (2) integrate the pseudo-weights within a weighted\npseudo-likelihood approach for Bayesian latent class analysis, while\npropagating pseudo-weight uncertainty into parameter estimation. A stacked\nsample approach is used to allow shared individuals between the NPS and the\nreference sample. We evaluate model performance through simulations and apply\nWOLCAN to data from the Puerto Rico Observational Study of Psychosocial,\nEnvironmental, and Chronic Disease Trends (PROSPECT). We identify dietary\nbehavior patterns for adults in Puerto Rico aged 30 to 75 and examine their\nassociations with type 2 diabetes, hypertension, and hypercholesterolemia. Our\nfindings suggest that an out-of-home eating pattern is associated with a higher\nlikelihood of these cardiometabolic outcomes compared to a nutrition-sensitive\npattern. WOLCAN effectively reveals generalizable dietary behavior patterns and\ndemonstrates relevant applications in studying diet-disease relationships.",
        "Denote the Tutte polynomial of a graph $G$ and a matroid $M$ by $T_G(x,y)$\nand $T_M(x,y)$ respectively. $T_G(x,1)$ and $T_G(1,y)$ were generalized to\nhypergraphs and further extended to integer polymatroids by K\\'{a}lm\\'{a}n\n\\cite{Kalman} in 2013, called interior and exterior polynomials respectively.\nLet $G$ be a $(k+1)$-edge connected graph of order $n$ and size $m$, and let\n$g=m-n+1$. Guan et al. (2023) \\cite{Guan} obtained the coefficients of\n$T_G(1,y)$: \\[[y^j]T_G(1,y)=\\binom{m-j-1}{n-2} \\text{ for } g-k\\leq j\\leq g,\\]\nwhich was deduced from coefficients of the exterior polynomial of polymatroids.\nRecently, Chen and Guo (2025) \\cite{Chen} further obtained\n\\[[y^j]T_G(1,y)=\\binom{m-j-1}{n-2}-\\sum_{i=k+1}^{g-j}\\binom{m-j-i-1}{n-2}|\\mathcal{EC}_i(G)|\\]\nfor $g-3(k+1)\/2< j\\leq g$, where $\\mathcal{EC}_i(G)$ denotes the set of all\nminimal edge cuts with $i$ edges. In this paper, for any matroid $M=(X,rk)$ we\nfirst obtain\n\\[[y^j]T_M(1,y)=\\sum_{t=j}^{|X|-r}(-1)^{t-j}\\binom{t}{j}\\sigma_{r+t}(M),\\]\nwhere $\\sigma_{r+t}(M)$ denotes the number of spanning sets with $r+t$ elements\nin $M$ and $r=rk(M)$. Moveover, the expression of $[x^i]T_M(x,1)$ is obtained\nimmediately from the duality of the Tutte polynomial. As applications of our\nresults, we generalize the two aforementioned results on graphs to the setting\nof matroids. This not only resolves two open problems posed by Chen and Guo in\n\\cite{Chen} but also provides a purely combinatorial proof that is\nsignificantly simpler than their original proofs.",
        "Synthetic aperture radar (SAR) imaging, celebrated for its high resolution,\nall-weather capability, and day-night operability, is indispensable for\nmaritime applications. However, ship detection in SAR imagery faces significant\nchallenges, including complex backgrounds, densely arranged targets, and large\nscale variations. To address these issues, we propose a novel framework,\nCenter-Aware SAR Ship Detector (CASS-Det), designed for robust multi-scale and\ndensely packed ship detection. CASS-Det integrates three key innovations: (1) a\ncenter enhancement module (CEM) that employs rotational convolution to\nemphasize ship centers, improving localization while suppressing background\ninterference; (2) a neighbor attention module (NAM) that leverages cross-layer\ndependencies to refine ship boundaries in densely populated scenes; and (3) a\ncross-connected feature pyramid network (CC-FPN) that enhances multi-scale\nfeature fusion by integrating shallow and deep features. Extensive experiments\non the SSDD, HRSID, and LS-SSDD-v1.0 datasets demonstrate the state-of-the-art\nperformance of CASS-Det, excelling at detecting multi-scale and densely\narranged ships.",
        "A previously established frequency distribution model combining a log-normal\ndistribution with a logarithmic equation describes fluctuations in the email\nsize during send requests. Although the frequency distribution fit was\nconsidered satisfactory, the underlying mechanism driving this distribution\nremains inadequately explained. To address this gap, this study introduced a\nnovel email-send model that characterizes the sending process as an exponential\nfunction modulated by noise from a normal distribution. This model is\nconsistent with both the observed frequency distribution and the previously\nproposed frequency distribution model.",
        "In this paper, we investigate the effect of data heterogeneity across clients\non the performance of distributed learning systems, i.e., one-round Federated\nLearning, as measured by the associated generalization error. Specifically,\n\\(K\\) clients have each \\(n\\) training samples generated independently\naccording to a possibly different data distribution and their individually\nchosen models are aggregated by a central server. We study the effect of the\ndiscrepancy between the clients' data distributions on the generalization error\nof the aggregated model. First, we establish in-expectation and tail upper\nbounds on the generalization error in terms of the distributions. In part, the\nbounds extend the popular Conditional Mutual Information (CMI) bound which was\ndeveloped for the centralized learning setting, i.e., \\(K=1\\), to the\ndistributed learning setting with arbitrary number of clients $K \\geq 1$. Then,\nwe use a connection with information theoretic rate-distortion theory to derive\npossibly tighter \\textit{lossy} versions of these bounds. Next, we apply our\nlossy bounds to study the effect of data heterogeneity across clients on the\ngeneralization error for distributed classification problem in which each\nclient uses Support Vector Machines (D-SVM). In this case, we establish\nexplicit generalization error bounds which depend explicitly on the data\nheterogeneity degree. It is shown that the bound gets smaller as the degree of\ndata heterogeneity across clients gets higher, thereby suggesting that D-SVM\ngeneralizes better when the dissimilarity between the clients' training samples\nis bigger. This finding, which goes beyond D-SVM, is validated experimentally\nthrough a number of experiments.",
        "Training large language models (LLMs) for different inference constraints is\ncomputationally expensive, limiting control over efficiency-accuracy\ntrade-offs. Moreover, once trained, these models typically process tokens\nuniformly, regardless of their complexity, leading to static and inflexible\nbehavior. In this paper, we introduce a post-training optimization framework,\nDynaMoE, that adapts a pre-trained dense LLM to a token-difficulty-driven\nMixture-of-Experts model with minimal fine-tuning cost. This adaptation makes\nthe model dynamic, with sensitivity control to customize the balance between\nefficiency and accuracy. DynaMoE features a token-difficulty-aware router that\npredicts the difficulty of tokens and directs them to the appropriate\nsub-networks or experts, enabling larger experts to handle more complex tokens\nand smaller experts to process simpler ones. Our experiments demonstrate that\nDynaMoE can generate a range of adaptive model variants of the existing trained\nLLM with a single fine-tuning step, utilizing only $10B$ tokens, a minimal cost\ncompared to the base model's training. Each variant offers distinct trade-offs\nbetween accuracy and performance. Compared to the baseline post-training\noptimization framework, Flextron, our method achieves similar aggregated\naccuracy across downstream tasks, despite using only $\\frac{1}{9}\\text{th}$ of\ntheir fine-tuning cost.",
        "Differentiable environments have heralded new possibilities for learning\ncontrol policies by offering rich differentiable information that facilitates\ngradient-based methods. In comparison to prevailing model-free reinforcement\nlearning approaches, model-based reinforcement learning (MBRL) methods exhibit\nthe potential to effectively harness the power of differentiable information\nfor recovering the underlying physical dynamics. However, this presents two\nprimary challenges: effectively utilizing differentiable information to 1)\nconstruct models with more accurate dynamic prediction and 2) enhance the\nstability of policy training. In this paper, we propose a Differentiable\nInformation Enhanced MBRL method, MB-MIX, to address both challenges. Firstly,\nwe adopt a Sobolev model training approach that penalizes incorrect model\ngradient outputs, enhancing prediction accuracy and yielding more precise\nmodels that faithfully capture system dynamics. Secondly, we introduce mixing\nlengths of truncated learning windows to reduce the variance in policy gradient\nestimation, resulting in improved stability during policy learning. To validate\nthe effectiveness of our approach in differentiable environments, we provide\ntheoretical analysis and empirical results. Notably, our approach outperforms\nprevious model-based and model-free methods, in multiple challenging tasks\ninvolving controllable rigid robots such as humanoid robots' motion control and\ndeformable object manipulation.",
        "This study presents a novel framework for precise force control of\nfin-actuated underwater robots by integrating a deep neural network (DNN)-based\nsurrogate model with reinforcement learning (RL). To address the complex\ninteractions with the underwater environment and the high experimental costs, a\nDNN surrogate model acts as a simulator for enabling efficient training for the\nRL agent. Additionally, grid-switching control is applied to select optimized\nmodels for specific force reference ranges, improving control accuracy and\nstability. Experimental results show that the RL agent, trained in the\nsurrogate simulation, generates complex thrust motions and achieves precise\ncontrol of a real soft fin actuator. This approach provides an efficient\ncontrol solution for fin-actuated robots in challenging underwater\nenvironments.",
        "To realize practical quantum computers, a large number of quantum bits\n(qubits) will be required. Semiconductor spin qubits offer advantages such as\nhigh scalability and compatibility with existing semiconductor technologies.\nHowever, as the number of qubits increases, manual qubit tuning becomes\ninfeasible, motivating automated tuning approaches. In this study, we use\nU-Net, a neural network method for object detection, to identify charge\ntransition lines in experimental charge stability diagrams. The extracted\ncharge transition lines are analyzed using the Hough transform to determine\ntheir positions and angles. Based on this analysis, we obtain the\ntransformation matrix to virtual gates. Furthermore, we identify the\nsingle-electron regime by clustering the Hough transform outputs. We also show\nthe single-electron regime within the virtual gate space. These sequential\nprocesses are performed automatically. This approach will advance automated\ncontrol technologies for large-scale quantum devices.",
        "Efficient use of cultivated areas is a necessary factor for sustainable\ndevelopment of agriculture and ensuring food security. Along with the rapid\ndevelopment of satellite technologies in developed countries, new methods are\nbeing searched for accurate and operational identification of cultivated areas.\nIn this context, identification of cropland boundaries based on spectral\nanalysis of data obtained from satellite images is considered one of the most\noptimal and accurate methods in modern agriculture. This article proposes a new\napproach to determine the suitability and green index of cultivated areas using\nsatellite data obtained through the \"Google Earth Engine\" (GEE) platform. In\nthis approach, two powerful algorithms, \"SNIC (Simple Non-Iterative Clustering)\nSuper Pixels\" and \"Canny Edge Detection Method\", are combined. The SNIC\nalgorithm combines pixels in a satellite image into larger regions (super\npixels) with similar characteristics, thereby providing better image analysis.\nThe Canny Edge Detection Method detects sharp changes (edges) in the image to\ndetermine the precise boundaries of agricultural fields. This study, carried\nout using high-resolution multispectral data from the Sentinel-2 satellite and\nthe Google Earth Engine JavaScript API, has shown that the proposed method is\neffective in accurately and reliably classifying randomly selected agricultural\nfields. The combined use of these two tools allows for more accurate\ndetermination of the boundaries of agricultural fields by minimizing the\neffects of outliers in satellite images. As a result, more accurate and\nreliable maps can be created for agricultural monitoring and resource\nmanagement over large areas based on the obtained data. By expanding the\napplication capabilities of cloud-based platforms and artificial intelligence\nmethods in the agricultural field.",
        "The field of rigid origami concerns the folding of stiff, inelastic plates of\nmaterial along crease lines that act like hinges and form a straight-line\nplanar graph, called the crease pattern of the origami. Crease pattern vertices\nin the interior of the folded material and that are adjacent to four crease\nlines, i.e. degree-4 vertices, have a single degree of freedom and can be\nchained together to make flexible polyhedral surfaces. Degree-4 vertices that\ncan fold to a completely flat state have folding kinematics that are very\nwell-understood, and thus they have been used in many engineering and physics\napplications. However, degree-4 vertices that are not flat-foldable or not\nfolded from flat paper so that the vertex forms either an elliptic or\nhyperbolic cone, have folding angles at the creases that follow more\ncomplicated kinematic equations. In this work we present a new duality between\ngeneral degree-4 rigid origami vertices, where dual vertices come in\nelliptic-hyperbolic pairs that have essentially equivalent kinematics. This\nreveals a mathematical structure in the space of degree-4 rigid origami\nvertices that can be leveraged in applications, for example in the construction\nof flexible 3D structures that possess metamaterial properties.",
        "This dissertation addresses a topic that I have worked on over the past\ndecade: the automation of next-to-leading order electroweak corrections in the\nStandard Model of particle physics. After introducing the basic concepts and\ntechniques of next-to-leading order QCD calculations that underpin the\nMadGraph5_aMC@NLO framework, I present a few key features relevant to the\nautomated next-to-leading order electroweak contributions to short-distance\ncross sections, with an emphasis on the mixed QCD and electroweak coupling\nexpansions. These include the FKS subtraction, the renormalization and\nelectroweak input parameter schemes, and the complex mass scheme for dealing\nwith unstable particles. Issues related to the initial or final photons and\nleptons are also discussed. Two remaining challenges are highlighted if one\nwishes to go beyond next-to-leading order computations. Some phenomenological\napplications at the LHC are given to demonstrate the relevance of electroweak\ncorrections at colliders. Finally, an outlook on future studies concludes the\ndissertation.",
        "Electron-phonon coupling is central to many condensed matter phenomena.\nHarnessing these effects for novel material functionality in materials always\ninvolves non-equilibrium electronic states, which in turn alter\nquasi-free-carrier density and screening. Thus, gaining a fundamental\nunderstanding of the interplay of carrier screening and electron-phonon\ncoupling is essential for advancing ultrafast science. Prior works have mainly\nfocused on the impact of carrier screening on electronic structure properties.\nHere we investigate the non-equilibrium lattice dynamics of MoS2 after a\nphotoinduced Mott transition. The experimental data are closely reproduced by\nab-initio ultrafast dynamics simulations. We find that the non-thermal diffuse\nscattering signals in the vicinity of the Bragg peaks, originating from\nlong-wavelength phonon emission, can only be reproduced upon explicitly\naccounting for the screening of electron-phonon interaction introduced by the\nMott transition. These results indicate the screening influences\nelectron-phonon coupling, leading to a suppression of intravalley\nphonon-assisted carrier relaxation. Overall, the combined experimental and\ncomputational approach introduced here offers new prospects for exploring the\ninfluence of screening of the electron-phonon interactions and relaxation\npathways in driven solids.",
        "A core principle of Privacy by Design (PbD) is minimizing the data that is\nstored or shared about each individual respondent. PbD principles are mandated\nby the GDPR (see Article 5c and Article 25), as well as informing aspects of\nCalifornia Privacy Rights Act (CPRA). This paper describes a simple and\neffective approach that can be used in many a\/b testing and similar contexts to\nhelp meet these PbD goals. Specifically, the method presented describes an\napproach to run OLS regression on k-anonymized data. To help illustrate the\ngeneral utility of this approach, descriptions of two important use cases are\noffered: 1) calculating partial f-tests as a simple way to both check for a\/b\ntest interactions and to test for heterogeneity of treatment effects; and 2)\nregression adjustment using an approach similar to the popular CUPED method, as\na variance reduction method for a\/b tests. Using this method has advantages for\nprivacy and compliance, as well as often reducing data storage and processing\ncosts, by storing, sharing, or analyzing only aggregate level rather than\nindividual level data.",
        "While 3D instance segmentation has made significant progress, current methods\nstruggle to address realistic scenarios where new categories emerge over time\nwith natural class imbalance. This limitation stems from existing datasets,\nwhich typically feature few well-balanced classes. Although few datasets\ninclude unbalanced class annotations, they lack the diverse incremental\nscenarios necessary for evaluating methods under incremental settings.\nAddressing these challenges requires frameworks that handle both incremental\nlearning and class imbalance. However, existing methods for 3D incremental\nsegmentation rely heavily on large exemplar replay, focusing only on\nincremental learning while neglecting class imbalance. Moreover,\nfrequency-based tuning for balanced learning is impractical in these setups due\nto the lack of prior class statistics. To overcome these limitations, we\npropose a framework to tackle both \\textbf{C}ontinual \\textbf{L}earning and\nclass \\textbf{Imb}alance for \\textbf{3D} instance segmentation\n(\\textbf{CLIMB-3D}). Our proposed approach combines Exemplar Replay (ER),\nKnowledge Distillation (KD), and a novel Imbalance Correction (IC) module.\nUnlike prior methods, our framework minimizes ER usage, with KD preventing\nforgetting and supporting the IC module in compiling past class statistics to\nbalance learning of rare classes during incremental updates. To evaluate our\nframework, we design three incremental scenarios based on class frequency,\nsemantic similarity, and random grouping that aim to mirror real-world dynamics\nin 3D environments. Experimental results show that our proposed framework\nachieves state-of-the-art performance, with an increase of up to 16.76\\% in mAP\ncompared to the baseline. Code will be available at:\n\\href{https:\/\/github.com\/vgthengane\/CLIMB3D}{https:\/\/github.com\/vgthengane\/CLIMB3D}",
        "Gaussian SLAM systems excel in real-time rendering and fine-grained\nreconstruction compared to NeRF-based systems. However, their reliance on\nextensive keyframes is impractical for deployment in real-world robotic\nsystems, which typically operate under sparse-view conditions that can result\nin substantial holes in the map. To address these challenges, we introduce\nDenseSplat, the first SLAM system that effectively combines the advantages of\nNeRF and 3DGS. DenseSplat utilizes sparse keyframes and NeRF priors for\ninitializing primitives that densely populate maps and seamlessly fill gaps. It\nalso implements geometry-aware primitive sampling and pruning strategies to\nmanage granularity and enhance rendering efficiency. Moreover, DenseSplat\nintegrates loop closure and bundle adjustment, significantly enhancing\nframe-to-frame tracking accuracy. Extensive experiments on multiple large-scale\ndatasets demonstrate that DenseSplat achieves superior performance in tracking\nand mapping compared to current state-of-the-art methods.",
        "The optimal transport barycenter (a.k.a. Wasserstein barycenter) is a\nfundamental notion of averaging that extends from the Euclidean space to the\nWasserstein space of probability distributions. Computation of the\nunregularized barycenter for discretized probability distributions on point\nclouds is a challenging task when the domain dimension $d > 1$. Most practical\nalgorithms for approximating the barycenter problem are based on entropic\nregularization. In this paper, we introduce a nearly linear time $O(m \\log{m})$\nand linear space complexity $O(m)$ primal-dual algorithm, the\nWasserstein-Descent $\\dot{\\mathbb{H}}^1$-Ascent (WDHA) algorithm, for computing\nthe exact barycenter when the input probability density functions are\ndiscretized on an $m$-point grid. The key success of the WDHA algorithm hinges\non alternating between two different yet closely related Wasserstein and\nSobolev optimization geometries for the primal barycenter and dual Kantorovich\npotential subproblems. Under reasonable assumptions, we establish the\nconvergence rate and iteration complexity of WDHA to its stationary point when\nthe step size is appropriately chosen. Superior computational efficacy,\nscalability, and accuracy over the existing Sinkhorn-type algorithms are\ndemonstrated on high-resolution (e.g., $1024 \\times 1024$ images) 2D synthetic\nand real data.",
        "Urban digital twins are virtual replicas of cities that use multi-source data\nand data analytics to optimize urban planning, infrastructure management, and\ndecision-making. Towards this, we propose a framework focused on the\nsingle-building scale. By connecting to cloud mapping platforms such as Google\nMap Platforms APIs, by leveraging state-of-the-art multi-agent Large Language\nModels data analysis using ChatGPT(4o) and Deepseek-V3\/R1, and by using our\nGaussian Splatting-based mesh extraction pipeline, our Digital Twin Buildings\nframework can retrieve a building's 3D model, visual descriptions, and achieve\ncloud-based mapping integration with large language model-based data analytics\nusing a building's address, postal code, or geographic coordinates.",
        "This article presents a new particle beam cooling scheme, namely cyclotron\nmaser cooling (CMC). Relativistic gyrating particles, forced by a solenoidal\nmagnetic field over some length of their trajectory, move in a helical path and\nundergo emission or absorption of radiations stimulated by a resonance RF\nfield. Theoretical and experimental investigations on electron beams indicate\nthat when the action of the RF field exceeds a critical value the beam jumps\nabruptly to a coherent radiative system undergoing CMC in which most electrons\nare accumulated to a discrete energy with the same gyration phase. The\nmechanism of CMC was proved to be an elementary cooling process that is common\nto dissipative systems consisting of a driving field and oscillators with a\nstable energy. It leads to the generation of a coherent beam of particles that\nprovides means to control miscellaneous particle induced reactions. For\nexample, CMC electrons would generate coherent X-ray and gamma-ray photons\nthrough coherent inverse Compton scattering of laser radiation.",
        "We perform high-order simulations of two-phase flows in capillaries, with and\nwithout evaporation. Since a sharp-interface model is used, singularities can\narise at the three-phase contact line, where the fluid-fluid interface\ninteracts with the capillary wall. These singularities are especially\nchallenging when a highly accurate, high-order method with very little\nnumerical diffusion is used for the flow solver. In this work, we employ the\neXtended Discontinuous Galerkin (XDG) method, which has a very high accuracy\nbut a severe limit regarding e.g., the time-step restriction.\n  To address this challenge and enhance the stability of our numerical method\nwe introduce a novel approach for representing a moving interface in the case\nof two-phase flows. We propose a global analytical representation of the\ninterface-describing level-set field, defined by a small set of time-dependent\nparameters. Noteworthy for its simplicity and efficiency, this method\neffectively addresses the inherent complexity of two-phase flow problems.\nFurthermore, it significantly improves numerical stability and enables the use\nof larger time steps, ensuring both reliability and computational efficiency in\nour simulations.\n  We compare different analytic expressions for level-set representation,\nincluding the elliptic function and the fourth-order polynomial, and validate\nthe method against established literature data for capillary rise, both with\nand without evaporation. These results highlight the effectiveness of our\napproach in resolving complex interfacial dynamics.",
        "Whole slide image (WSI) registration is an essential task for analysing the\ntumour microenvironment (TME) in histopathology. It involves the alignment of\nspatial information between WSIs of the same section or serial sections of a\ntissue sample. The tissue sections are usually stained with single or multiple\nbiomarkers before imaging, and the goal is to identify neighbouring nuclei\nalong the Z-axis for creating a 3D image or identifying subclasses of cells in\nthe TME. This task is considerably more challenging compared to radiology image\nregistration, such as magnetic resonance imaging or computed tomography, due to\nvarious factors. These include gigapixel size of images, variations in\nappearance between differently stained tissues, changes in structure and\nmorphology between non-consecutive sections, and the presence of artefacts,\ntears, and deformations. Currently, there is a noticeable gap in the literature\nregarding a review of the current approaches and their limitations, as well as\nthe challenges and opportunities they present. We aim to provide a\ncomprehensive understanding of the available approaches and their application\nfor various purposes. Furthermore, we investigate current deep learning methods\nused for WSI registration, emphasising their diverse methodologies. We examine\nthe available datasets and explore tools and software employed in the field.\nFinally, we identify open challenges and potential future trends in this area\nof research.",
        "The hadronic $\\Sigma_c^{(*)}\\bar{D}^{(*)}$ and $\\Sigma_c^{(*)}{D}^{(*)}$\ninteractions are revisited, with a focus on their short-range parts, motivated\nby a tension between the interpretations of $P_{c\\bar{c}}(4312)$,\n$P_{c\\bar{c}}(4440)$, and $P_{c\\bar{c}}(4457)$ in effective field theory (EFT)\nframeworks and the one-boson-exchange (OBE) model. While the three states can\nbe interpreted as $\\Sigma_c\\bar{D}^{(*)}$ molecular states within EFT\nframeworks, this is not feasible in the single-channel OBE model with\nconsistent cutoff. In this work, the possibility to reconcile OBE model with\nEFTs by refitting the $\\rho$-, $\\omega$- and $\\sigma$-exchange interaction is\nexplored and ruled out. It is pointed out that the problem in OBE arises from\nthe strong short-range spin-dependent one-pion-exchange (OPE) interaction and\nthe fixed signs of other short-range interactions in OBE model also prevent the\ncancellation. To address this issue, the short-range subtraction strategies\nwithin the OBE model are revisited. Two subtraction schemes are explored:\nremoving the delta-function from all interactions and eliminating it only from\nthe pseudoscalar-meson-exchange component. These schemes favor different spin\nassignments for $P_{c\\bar{c}}(4440)$ and $P_{c\\bar{c}}(4457)$. Though solving\nthe problem, there is no clear dynamical picture to support the subtraction\nschemes. We propose a new quark-exchange mechanism motivated by the Pauli\nprinciple. Different from the two subtraction schemes in OBE, the\nquark-exchange mechanism offers an explanation grounded in microscopic\ndynamics. It is shown that the spin-dependent quark-exchange interaction\ncancels those from OPE. The differences in the predictions for the spin,\nisospin, and open-charm partner states of the experimental $P_{c\\bar{c}}$\nstates offer a way to distinguish between the subtracted OBE model and the OBE\nmodel with quark-exchange contributions.",
        "Let K[X_n]=K[x_1,\\ldots,x_n] be the polynomial algebra in n variables over a\nfield K of characteristic zero. A locally nilpotent linear derivation \\delta of\nK[X_n] is called Weitzenb\\\"ock due to his well known result from 1932 stating\nthat the algebra \\text{\\rm ker}(\\delta)=K[X_n]^{\\delta} of constants of\n$\\delta$ is finitely generated. The explicit form of a generating set of\n$K[X_n,Y_n]^{\\delta}$ was conjectured by Nowicki in 1994 in the case \\delta was\nsuch that \\delta(y_{i})=x_{i}$, $\\delta(x_i)=0, i=1,\\ldots,n. Nowicki's\nconjecture turned out to be true and, recently, has been applied to several\nrelatively free associative algebras. In this paper, we consider the free Lie\nalgebra \\mathcal{L}(x,y) of rank 2 generated by x and y over K and we assume\nthe Weitzenb\\\"ock derivation \\delta sending y to x, and x to zero. We introduce\nthe idea of pseudodeterminants and we present a characterization of Hall\nmonomials that are constants showing they are not so far from being\npseudodeterminants. We also give a complete list of generators of the constants\nof degree less than 7 which are, of course, pseudodeterminants."
      ]
    }
  },
  {
    "id":2411.01291,
    "research_type":"applied",
    "start_id":"b19",
    "start_title":"vSHARP: variable Splitting Half-quadratic ADMM algorithm for Reconstruction of inverse-Problems",
    "start_abstract":"Medical Imaging (MI) tasks, such as accelerated parallel Magnetic Resonance Imaging (MRI), often involve reconstructing an image from noisy or incomplete measurements. This amounts to solving ill-posed inverse problems, where a satisfactory closed-form analytical solution is not available. Traditional methods such as Compressed Sensing (CS) in MRI reconstruction can be time-consuming or prone to obtaining low-fidelity images. Recently, a plethora of Deep Learning (DL) approaches have demonstrated superior performance in inverse-problem solving, surpassing conventional methods. In this study, we propose vSHARP (variable Splitting Half-quadratic ADMM algorithm for Reconstruction of inverse Problems), a novel DL-based method for solving ill-posed inverse problems arising in MI. vSHARP utilizes the Half-Quadratic Variable Splitting method and employs the Alternating Direction Method of Multipliers (ADMM) to unroll the optimization process. For data consistency, vSHARP unrolls a differentiable gradient descent process in the image domain, while a DL-based denoiser, such as a U-Net architecture, is applied to enhance image quality. vSHARP also employs a dilated-convolution DL-based model to predict the Lagrange multipliers for the ADMM initialization. We evaluate vSHARP on tasks of accelerated parallel MRI Reconstruction using two distinct datasets and on accelerated parallel dynamic MRI Reconstruction using another dataset. Our comparative analysis with state-of-the-art methods demonstrates the superior performance of vSHARP in these applications.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b15"
      ],
      "title":[
        "CMRxRecon2024: A Multi-Modality, Multi-View K-Space Dataset Boosting\n  Universal Machine Learning for Accelerated Cardiac MRI"
      ],
      "abstract":[
        "Cardiac magnetic resonance imaging (MRI) has emerged as a clinically gold-standard technique for diagnosing cardiac diseases, thanks to its ability provide diverse information with multiple modalities and anatomical views. Accelerated MRI is highly expected achieve time-efficient patient-friendly imaging, then advanced image reconstruction approaches are required recover high-quality, interpretable images from undersampled measurements. However, the lack of publicly available k-space dataset in terms both quantity diversity severely hindered substantial technological progress, particularly data-driven artificial intelligence. Here, we standardized, diverse, high-quality CMRxRecon2024 facilitate technical development, fair evaluation, clinical transfer approaches, towards promoting universal frameworks that enable fast robust reconstructions across different protocols practice. To best our knowledge, largest most dataset. It acquired 330 healthy volunteers, covering commonly used modalities, views, acquisition trajectories workflows. Besides, an open platform tutorials, benchmarks, data processing tools provided usage, method performance evaluation."
      ],
      "categories":[
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "$\\tt GrayHawk$: A public code for calculating the Gray Body Factors of\n  massless fields around spherically symmetric Black Holes",
        "MinGRU-Based Encoder for Turbo Autoencoder Frameworks",
        "Type semigroups for twisted groupoids and a dichotomy for groupoid\n  C*-algebras",
        "Algebraic surfaces as Hadamard products of curves",
        "$C^{1}$-Stable-Manifolds for Periodic Heteroclinic Chains in Bianchi IX:\n  Symbolic Computations and Statistical Properties",
        "Slow Light Waveguides based on Bound States in the Continuum",
        "High-Accuracy Physical Property Prediction for Organics via Molecular\n  Representation Learning: Bridging Data to Discovery",
        "Identical Suppression of Spin and Charge Density Wave Transitions in\n  La$_4$Ni$_3$O$_{10}$ by Pressure",
        "A Relaxed Wasserstein Distance Formulation for Mixtures of Radially\n  Contoured Distributions",
        "Weight Distribution of the Weighted Coordinates Poset Block Space and\n  Singleton Bound",
        "Cold dark gas in Cygnus X: The first large-scale mapping of\n  low-frequency carbon recombination lines",
        "Properties of Turnpike Functions for Discounted Finite MDPs",
        "Raman Forbidden Layer-Breathing Modes in Layered Semiconductor Materials\n  Activated by Phonon and Optical Cavity Effects",
        "Anomalous temperature-dependent magnetization in the nearly collinear\n  antiferromagnet Y$_2$Co$_3$",
        "The nebular spectra of SN 2023ixf: A lower mass, partially stripped\n  progenitor may be the result of binary interaction",
        "Segregation in Nuclear Stellar Clusters: Rates and Mass Distributions of\n  TDEs, QPEs, Plunges, and EMRIs",
        "Normal-normal continuous symmetric stress approximation in\n  three-dimensional linear elasticity",
        "KM3NeT Constraint on Lorentz-Violating Superluminal Neutrino Velocity",
        "Cosmic voids and the kinetic analysis. IV. Hubble tension and the\n  cosmological constant",
        "Machine Learns Quantum Complexity?",
        "MCMC for multi-modal distributions",
        "Weighted Heights and GIT Heights",
        "Physics-Aware Decoding for Communication Channels Governed by Partial\n  Differential Equations",
        "Energy-Adaptive Riemannian Conjugate Gradient Method for Density\n  Functional Theory",
        "Unified Micromechanics Theory of Composites",
        "What Bohmian mechanic says about arrival times of 1D vacuum squeezed\n  states",
        "Central galaxy alignments. Dependence on the mass and the large-scale\n  environment",
        "Propagation Times and Energy Losses of Cosmic Protons and Antiprotons in\n  Interplanetary Space",
        "Using Statistical Precision Medicine to Identify Optimal Treatments in a\n  Heart Failure Setting"
      ],
      "abstract":[
        "We introduce and describe $\\tt GrayHawk$, a publicly available\nMathematica-based tool designed for the efficient computation of gray-body\nfactors for spherically symmetric and asymptotically flat black holes. This\nprogram provides users with a rapid and reliable means to compute gray-body\nfactors for massless fields with spin \\(s = 0, 1\/2, 1, 2\\) in modes specified\nby the angular quantum number \\(l\\), given a black hole metric and the\nassociated parameter values. $\\tt GrayHawk$ is preloaded with seven different\nblack hole metrics, offering immediate applicability to a variety of\ntheoretical models. Additionally, its modular structure allows users to extend\nits functionality easily by incorporating alternative metrics or\nconfigurations. This versatility makes $\\tt GrayHawk$ a powerful and adaptable\nresource for researchers studying black hole physics and Hawking radiation. The\ncodes described in this work are publicly available at\nhttps:\/\/github.com\/marcocalza89\/GrayHawk.",
        "Early neural channel coding approaches leveraged dense neural networks with\none-hot encodings to design adaptive encoder-decoder pairs, improving block\nerror rate (BLER) and automating the design process. However, these methods\nstruggled with scalability as the size of message sets and block lengths\nincreased. TurboAE addressed this challenge by focusing on bit-sequence inputs\nrather than symbol-level representations, transforming the scalability issue\nassociated with large message sets into a sequence modeling problem. While\nrecurrent neural networks (RNNs) were a natural fit for sequence processing,\ntheir reliance on sequential computations made them computationally expensive\nand inefficient for long sequences. As a result, TurboAE adopted convolutional\nnetwork blocks, which were faster to train and more scalable, but lacked the\nsequential modeling advantages of RNNs. Recent advances in efficient RNN\narchitectures, such as minGRU and minLSTM, and structured state space models\n(SSMs) like S4 and S6, overcome these limitations by significantly reducing\nmemory and computational overhead. These models enable scalable sequence\nprocessing, making RNNs competitive for long-sequence tasks. In this work, we\nrevisit RNNs for Turbo autoencoders by integrating the lightweight minGRU model\nwith a Mamba block from SSMs into a parallel Turbo autoencoder framework. Our\nresults demonstrate that this hybrid design matches the performance of\nconvolutional network-based Turbo autoencoder approaches for short sequences\nwhile significantly improving scalability and training efficiency for long\nblock lengths. This highlights the potential of efficient RNNs in advancing\nneural channel coding for long-sequence scenarios.",
        "We develop a theory of type semigroups for arbitrary twisted, not necessarily\nHausdorff \\'etale groupoids. The type semigroup is a dynamical version of the\nCuntz semigroup. We relate it to traces, ideals, pure infiniteness, and stable\nfiniteness of the reduced and essential C*-algebras. If the reduced C*-algebra\nof a twisted groupoid is simple and the type semigroup satisfies a weak version\nof almost unperforation, then the C*-algebra is either stably finite or purely\ninfinite. We apply our theory to Cartan inclusions. We calculate the type\nsemigroup for the possibly non-Hausdorff groupoids associated to self-similar\ngroup actions on graphs and deduce a dichotomy for the resulting Exel-Pardo\nalgebras.",
        "We study projective surfaces in $\\mathbb{P}^3$ which can be written as\nHadamard product of two curves. We show that quadratic surfaces which are\nHadamard product of two lines are smooth and tangent to all coordinate planes,\nand such tangency points uniquely identify the quadric. The variety of such\nquadratic surfaces corresponds to the Zariski closure of the space of symmetric\nmatrices whose inverse has null diagonal. For higher-degree surfaces which are\nHadamard product of a line and a curve we show that the intersection with the\ncoordinate planes is always non-transversal.",
        "In this paper we study oscillatory Bianchi models of class A and are able to\nshow that for admissible periodic heteroclinic chains in Bianchi IX there\nexisist $C^{1}$- stable - manifolds of orbits that follow these chains towards\nthe big bang. A detailed study of Takens Linearization Theorem and the\nNon-Resonance-Conditions leads us to this new result in Bianchi class A. More\nprecisely, we can show that there are no heteroclinic chains in Bianchi IX with\nconstant continued fraction development that allow Takens-Linearization at all\nof their base points. Geometrically speaking, this excludes \"symmetric\"\nheteroclinic chains with the same number of \"bounces\" near all of the 3 Taub\nPoints - the result shows that we have to require some \"asymmetry\" in the\nbounces in order to allow for Takens Linearization, e.g. by considering\nadmissible 2-periodic continued fraction developments. We conclude by\ndiscussing the statistical properties of those solutions, including their\ntopological and measure-theoretic genericity.",
        "The concept of bound states in the continuum (BIC) has been advancing light\nconfinement technology in leaky environments. In this letter, we propose and\nnumerically demonstrate a slow light waveguide based on a BIC mode. We\nconsidered a waveguide with a polymer core loaded on a plane slab, which\nsupports a leaky guided mode coupled to the radiation continuum in the slab. We\nfound that periodic modulation of the polymer core along the propagation\ndirection can result in a high group index mode with a low propagation loss due\nto BIC confinement. The introduction of one-dimensional photonic crystals into\nthe BIC waveguides will largely expand its functionality and applications in\nintegrated photonics.",
        "The ongoing energy crisis has underscored the urgent need for\nenergy-efficient materials with high energy utilization efficiency, prompting a\nsurge in research into organic compounds due to their environmental\ncompatibility, cost-effective processing, and versatile modifiability. To\naddress the high experimental costs and time-consuming nature of traditional\ntrial-and-error methods in the discovery of highly functional organic\ncompounds, we apply the 3D transformer-based molecular representation learning\nalgorithm to construct a pre-trained model using 60 million semi-empirically\noptimized structures of small organic molecules, namely, Org-Mol, which is then\nfine-tuned with public experimental data to obtain prediction models for\nvarious physical properties. Despite the pre-training process relying solely on\nsingle molecular coordinates, the fine-tuned models achieves high accuracy\n(with $R^2$ values for the test set exceeding 0.95). These fine-tuned models\nare applied in a high-throughput screening process to identify novel immersion\ncoolants among millions of automatically constructed ester molecules, resulting\nin the experimental validation of two promising candidates. This work not only\ndemonstrates the potential of Org-Mol in predicting bulk properties for organic\ncompounds but also paves the way for the rational and efficient development of\nideal candidates for energy-saving materials.",
        "Understanding the interplay between magnetism and superconductivity in\nnickelate systems is a key focus of condensed matter research. Microscopic\ninsights into magnetism, which emerges near superconductivity, require a\nsynergistic approach that combines complementary techniques with controlled\nparameter tuning. In this paper, we present a systematic investigation of the\nthree-layer Ruddlesden-Popper (RP) nickelate La$_4$Ni$_3$O$_{10}$ using\nmuon-spin rotation\/relaxation ($\\mu$SR), neutron powder diffraction (NPD),\nresistivity, and specific heat measurements. At ambient pressure, two\nincommensurate spin density wave (SDW) transitions were identified at $T_{\\rm\nSDW} \\simeq 132$ K and $T^\\ast \\simeq 90$ K. NPD experiments revealed that the\nmagnetic wave vector $(0, 0.574, 0)$ remains unchanged below 130 K, indicating\nthat the transition at $T^\\ast$ corresponds to a reorientation of the Ni\nmagnetic moments within a similar magnetic structure. Comparison of the\nobserved internal magnetic fields with dipole-field calculations reveals a\nmagnetic structure consistent with an antiferromagnetically coupled SDW on the\nouter two Ni layers, with smaller moments on the inner Ni layer. The internal\nfields at muon stopping sites appeared abruptly at $T_{\\rm SDW}$, suggesting a\nfirst-order-like nature of the SDW transition, which is closely linked to the\ncharge density wave (CDW) order occurring at the same temperature ($T_{\\rm SDW}\n= T_{\\rm CDW}$). Under applied pressure, all transition temperatures, including\n$T_{\\rm SDW}$, $T^\\ast$, and $T_{\\rm CDW}$, were suppressed at a nearly uniform\nrate of $\\simeq -13$ K\/GPa. This behavior contrasts with the double-layer RP\nnickelate La$_3$Ni$_2$O$_7$, where pressure enhances the separation of the\ndensity wave transitions.",
        "Recently, a Wasserstein-type distance for Gaussian mixture models has been\nproposed. However, that framework can only be generalized to identifiable\nmixtures of general elliptically contoured distributions whose components come\nfrom the same family and satisfy marginal consistency. In this paper, we\npropose a simple relaxed Wasserstein distance for identifiable mixtures of\nradially contoured distributions whose components can come from different\nfamilies. We show some properties of this distance and that its definition does\nnot require marginal consistency. We apply this distance in color transfer\ntasks and compare its performance with the Wasserstein-type distance for\nGaussian mixture models in an experiment. The error of our method is more\nstable and the color distribution of our output image is more desirable.",
        "In this paper, we determine the complete weight distribution of the space $\n\\mathbb{F}_q^N $ endowed by the weighted coordinates poset block metric\n($(P,w,\\pi)$-metric), also known as the $(P,w,\\pi)$-space, thereby obtaining it\nfor $(P,w)$-space, $(P,\\pi)$-space, $\\pi$-space, and $P$-space as special\ncases. Further, when $P$ is a chain, the resulting space is called as\nNiederreiter-Rosenbloom-Tsfasman (NRT) weighted block space and when $P$ is\nhierarchical, the resulting space is called as weighted coordinates\nhierarchical poset block space. The complete weight distribution of both the\nspaces are deduced from the main result. Moreover, we define an $I$-ball for an\nideal $I$ in $P$ and study the characteristics of it in $(P,w,\\pi)$-space.\n  We investigate the relationship between the $I$-perfect codes and $t$-perfect\ncodes in $(P,w,\\pi)$-space. Given an ideal $I$, we investigate how the maximum\ndistance separability (MDS) is related with $I$-perfect codes and $t$-perfect\ncodes in $(P,w,\\pi)$-space. Duality theorem is derived for an MDS\n$(P,w,\\pi)$-code when all the blocks are of same length. Finally, the\ndistribution of codewords among $r$-balls is analyzed in the case of chain\nposet, when all the blocks are of same length.",
        "Understanding the transition from atomic gas to molecular gas is critical to\nexplain the formation and evolution of molecular clouds. However, the gas\nphases involved, cold HI and CO-dark molecular gas, are challenging to directly\nobserve and physically characterize. We observed the Cygnus X star-forming\ncomplex in carbon radio recombination lines (CRRLs) at 274--399 MHz with the\nGreen Bank Telescope at 21 pc (48') resolution. Of the 30 deg^2 surveyed, we\ndetect line-synthesized C273alpha emission from 24 deg^2 and produce the first\nlarge-area maps of low-frequency CRRLs. The morphology of the C273alpha\nemission reveals arcs, ridges, and extended possibly sheet-like gas which are\noften on the outskirts of CO emission and likely transitioning from HI-to-H_2.\nThe typical angular separation of C273alpha and 13CO emission is 12 pc, and we\nestimate C273alpha gas densities of n_H ~ 40 - 400 cm^3. The C273alpha line\nprofiles are Gaussian and likely turbulent broadened, spanning a large range of\nFWHM from 2 to 20 km\/s with a median of 10.6 km\/s. Mach numbers fall within\n10--30. The turbulent timescale is relatively short, 2.6 Myr, and we deduce\nthat the turbulent pressure likely dominates the evolution of the C273alpha\ngas. Velocity offsets between C273alpha and 13CO components are apparent\nthroughout the region and have a typical value of 2.9 km\/s. Two regimes have\nemerged from the data: one regime in which C273alpha and 13CO are strongly\nrelated (at N_H ~ 4 x 10^21 cm^-2), and a second, in which C273alpha emits\nindependently of the 13CO intensity. In the former regime, C273alpha may arise\nfrom the the envelopes of massive clouds (filaments), and in the latter,\nC273alpha emits from cold clumps in a more-diffuse mix of HI and H_2 gas.",
        "This paper studies discounted Markov Decision Processes (MDPs) with finite\nsets of states and actions. Value iteration is one of the major methods for\nfinding optimal policies. For each discount factor, starting from a finite\nnumber of iterations, which is called the turnpike integer, value iteration\nalgorithms always generate decision rules, which are deterministic optimal\npolicies for the infinite-horizon problems. This fact justifies the rolling\nhorizon approach for computing infinite-horizon optimal policies by conducting\na finite number of value iterations. This paper describes properties of\nturnpike integers and provides their upper bounds.",
        "We report Raman forbidden layer-breathing modes (LBMs) in layered\nsemiconductor materials (LSMs). The intensity distribution of all observed LBMs\ndepends on layer number, incident light wavelength and refractive index\nmismatch between LSM and underlying substrate. These results are understood by\na Raman scattering theory via the proposed spatial interference model, where\nthe naturally occurring optical and phonon cavities in LSMs enable spatially\ncoherent photon-phonon coupling mediated by the corresponding one-dimensional\nperiodic electronic states. Our work reveals the spatial coherence of photon\nand phonon fields on the phonon excitation via photon\/phonon cavity\nengineering.",
        "Y$_2$Co$_3$ is a newly discovered antiferromagnetic (AFM) compound with\ndistorted kagome layers. Previous investigations via bulk magnetization\nmeasurements suggested a complex noncollinear magnetic behavior, with magnetic\nmoments primarily anti-aligned along the $b$ axis and some canting towards the\n$ac$ plane. In this study, we report the magnetic structure of Y$_2$Co$_3$ to\nbe an A-type AFM structure with ferromagnetic (FM) interactions within the\ndistorted kagome plane and an interplane antiferromagnetic interaction, as\ndetermined by single-crystal neutron diffraction. The magnetic moments align\nalong the $b$ axis, with minimal canting towards the $c$ axis, at odds with the\nprevious interpretation of bulk magnetization measurements. The magnetic\nmoments on the two distinct Co sites are [0, -0.68(3), 0] $\\mu_B$ and [0,\n1.25(4), 0.07(1)] $\\mu_B$. We attribute the previously reported \"noncollinear\"\nbehavior to the considerable temperature dependence of itinerant AFM exchange\ninteractions, induced by thermal contraction along the $b$ axis. Additionally,\nour examination of lattice constants through pressure studies reveals\ncompensating effects on FM and AFM interactions, resulting in negligible\npressure dependence of $T_\\textrm{N}$.",
        "SN 2023ixf is one of the brightest Core Collapse Supernovae of the 21st\ncentury and offers a rare opportunity to investigate the late stage of a\nSupernova through nebular phase spectroscopy. We present four nebular phase\nspectra from day +291 to +413 after explosion. This is supplemented with high\ncadence early phase spectroscopic observations and photometry covering the\nfirst 500 days to investigate explosion parameters. The narrow and blue-shifted\nnebular Oxygen emission lines are used to infer an ejected Oxygen mass of\n$<0.65M_\\odot$, consistent with models of a relatively low mass\n($M_{ZAMS}<15M_\\odot$) progenitor. An energy of 0.3 to $1.4 \\times10^{51}$ erg\nand a light curve powered by an initial $^{56}$Ni mass of $0.049 \\pm 0.005\nM_\\odot$ appear consistent with a relatively standard Type II explosion, while\nan incomplete $\\gamma$-ray trapping (with timescale of $240\\pm4$ days) suggests\na lower ejecta mass. Assuming a typical explosion, the broad Hydrogen and\nCalcium profiles suggest a common origin within a lower mass, partially\nstripped envelope. Hydrogen emission broadens with time, indicating\ncontribution from an additional power source at an extended distance; while the\nemergence of high velocity ($\\sim$6,000 km s$^{-1}$) Hydrogen emission features\n(beginning around day +200) may be explained by Shock Interaction with a dense\nHydrogen-rich region located at $\\sim1.5 \\times 10^{16}$cm. Such envelope mass\nloss for a low mass progenitor may be explained through theoretical models of\nBinary interaction.",
        "Supermassive black holes at the centers of galaxies occasionally disrupt\nstars or consume stellar-mass black holes that wander too close, producing\nobservable electromagnetic or gravitational wave signals. We examine how mass\nsegregation impacts the rates and distributions of such events. Assuming a\nrelaxed stellar cluster, composed of stars and stellar-mass black holes, we\nshow that the tidal disruption rate of massive stars ($m\\gtrsim M_\\odot$) is\nenhanced relative to their abundance in the stellar population. For stars up to\n$m\\approx3M_\\odot$, this enhancement is roughly $m\/M_\\odot$ and it is driven by\nsegregation within the sphere of influence. Stars with masses\n$m\\gtrsim3M_\\odot$, if relaxed, are predominantly scattered by more massive\nstellar-mass black holes, leading to a constant enhancement factor of $\\approx\n10$, independent of mass. This aligns with observational evidence suggesting an\nover-representation of massive stars in tidal disruption events. For\nstellar-mass black holes, we predict an enhancement factor scaling as\n$m_\\bullet^{1\/2}$ for plunges and $m_\\bullet^{3\/2}$ for extreme-mass-ratio\ninspirals (EMRIs). The power of one-half in both cases reflects the shorter\nrelaxation times of heavier black holes, allowing them to segregate into the\nsphere of influence from greater distances, thereby increasing their abundance.\nThe additional power in the EMRIs' rate arises from the tendency of heavier\nblack holes to circularize and sink inward more efficiently. Finally, we\nestimate the rate of main sequence star inspirals and find that it favors\nlow-mass stars ($m\\lesssim M_\\odot$). This seems compatible with the\nobservationally estimated rate of quasi-periodic eruptions.",
        "We present a conforming setting for a mixed formulation of linear elasticity\nwith symmetric stress that has normal-normal continuous components across faces\nof tetrahedral meshes. We provide a stress element for this formulation with 30\ndegrees of freedom that correspond to standard boundary conditions. The\nresulting scheme converges quasi-optimally and is locking free. Numerical\nexperiments illustrate the performance.",
        "Lorentz invariance is a fundamental symmetry of spacetime and foundational to\nmodern physics. One of its most important consequences is the constancy of the\nspeed of light. This invariance, together with the geometry of spacetime,\nimplies that no particle can move faster than the speed of light. In this\narticle, we present the most stringent neutrino-based test of this prediction,\nusing the highest energy neutrino ever detected to date, KM3-230213A. The\narrival of this event, with an energy of $220^{+570}_{-110}\\,\\text{PeV}$, sets\na constraint on $\\delta \\equiv c_\\nu^2-1 < 4\\times10^{-22}$.",
        "The formation of the cosmic structures in the late Universe is considered\nusing Vlasov kinetic approach. The crucial point is the use of the\ngravitational potential with repulsive term of the cosmological constant which\nprovides a solution to the Hubble tension, that is the Hubble parameter for the\nlate Universe has to differ from its global cosmological value. This also\nprovides a mechanism of formation of stationary semi-periodic gravitating\nstructures of voids and walls, so that the cosmological constant has a role of\nthe scaling and hence can be compared with the observational data for given\nregions. The considered mechanism of the structure formation in late\ncosmological epoch then is succeeding the epoch described by the evolution of\nprimordial density fluctuations.",
        "We study how a machine based on deep learning algorithms learns Krylov spread\ncomplexity in quantum systems with N x N random Hamiltonians drawn from the\nGaussian unitary ensemble. Using thermofield double states as initial\nconditions, we demonstrate that a convolutional neural network-based algorithm\nsuccessfully learns the Krylov spread complexity across all timescales,\nincluding the late-time plateaus where states appear nearly featureless and\nrandom. Performance strongly depends on the basis choice, performing well with\nthe energy eigenbasis or the Krylov basis but failing in the original basis of\nthe random Hamiltonian. The algorithm also effectively distinguishes\ntemperature-dependent features of thermofield double states. Furthermore, we\nshow that the system time variable of state predicted by deep learning is an\nirrelevant quantity, reinforcing that the Krylov spread complexity well\ncaptures the essential features of the quantum state, even at late times.",
        "We explain the fundamental challenges of sampling from multimodal\ndistributions, particularly for high-dimensional problems. We present the major\ntypes of MCMC algorithms that are designed for this purpose, including parallel\ntempering, mode jumping and Wang-Landau, as well as several state-of-the-art\napproaches that have recently been proposed. We demonstrate these methods using\nboth synthetic and real-world examples of multimodal distributions with\ndiscrete or continuous state spaces.",
        "This paper examines the relationship between GIT heights and weighted\nheights, exploring their definitions and applications to weighted projective\nspaces and binary forms. Drawing on prior weighted height frameworks, we relate\nthem to Zhang's GIT height via the Veronese map, showing that for a semistable\ncycle Z in a weighted projective space over the algebraic closure of Q, the GIT\nheight h(Z) equals L(Z) plus an Archimedean Chow metric term. For binary forms\nf in V_d, we define an invariant height H(f) with respect to the Chow metric\nand establish that the moduli weighted height L(xi(f)) of f's invariants equals\nH(f) plus the field degree times the Chow height h_Ch(f), linking arithmetic\nand moduli properties.",
        "Digital communication systems inherently operate through physical media\ngoverned by partial differential equations (PDEs). In this paper, we introduce\na physics-aware decoding framework that integrates gradient descent-based error\ncorrecting algorithms with PDE-based channel modeling using differentiable PDE\nsolvers. At the core of our approach is gradient flow decoding, which harnesses\ngradient information directly from the PDE solver to guide the decoding\nprocess. We validate our method through numerical experiments on both the heat\nequation and the nonlinear Schr\\\"odinger equation (NLSE), demonstrating\nsignificant improvements in decoding performance. The implications of this work\nextend beyond decoding applications, establishing a new paradigm for\nphysics-aware signal processing that shows promise for various signal detection\nand signal recovery tasks.",
        "This paper presents a novel Riemannian conjugate gradient method for the\nKohn-Sham energy minimization problem in density functional theory (DFT), with\na focus on non-metallic crystal systems. We introduce an energy-adaptive metric\nthat preconditions the Kohn-Sham model, significantly enhancing optimization\nefficiency. Additionally, a carefully designed shift strategy and several\nalgorithmic improvements make the implementation comparable in performance to\nhighly optimized self-consistent field iterations. The energy-adaptive\nRiemannian conjugate gradient method has a sound mathematical foundation,\nincluding stability and convergence, offering a reliable and efficient\nalternative for DFT-based electronic structure calculations in computational\nchemistry.",
        "We consider the matrix composite materials (CM) of either random\n(statistically homogeneous or inhomogeneous), periodic, or deterministic\n(neither random nor periodic) structures. CMs exhibit linear or nonlinear\nbehavior, coupled or uncoupled multi-physical phenomena, locally elastic,\nweakly nonlocal (strain gradient and stress gradient), or strongly nonlocal\n(strain-type and displacement-type, peridynamics) phase properties. A modified\nComputational Analytical Micromechanics (CAM) approach introduces an exact\nAdditive General Integral Equation (AGIE) for CMs of any structure and phase\nproperties mentioned above. The unified iteration solution of static AGIEs is\nadapted to the body force with compact support serving as a fundamentally new\nuniversal training parameter. The approach also establishes a critical\nthreshold for filtering out unsuitable sub-datasets of effective parameters\nthrough a novel Representative Volume Element (RVE) concept, which extends\nHill's classical framework. This RVE concept eliminates sample size, boundary\nlayer, and edge effects, making it applicable to CMs of any structure and phase\nproperties, regardless of local or nonlocal, linear or nonlinear. Incorporating\nthis new RVE concept into machine learning and neural network techniques\nenables the construction of any unpredefined surrogate nonlocal operators. The\nmethodology is structured as a modular, block-based framework, allowing\nindependent development and refinement of software components. This flexible,\nrobust AGIE-CAM framework integrates data-driven, multi-scale, and\nmulti-physics modeling, accelerating research in CM of any microtopology and\nphase properties considered. The AGIE-CAM framework represents a groundbreaking\nparadigm shift in the micromechanics of composites, redefining the very\nphilosophy that underpins our understanding of their behavior at the\nmicroscopic level.",
        "We calculate the time of arrival probability distribution of a quantum\nparticle using the Bohmian formalism. The pilot-wave is given by the wave\nfunction of the one dimensional vacuum squeezed state but written in the\nSchr\\\"odinger representation. We made use of the unitary representation of the\nsymplectic group in the Hilbert space $L^2(\\mathbb{R})$. The solution to the\nBohmian equations are analytical function thus allowing for a closed expression\nof the time of arrival distribution which differs from the counterparts in the\nstandard quantum mechanics formulation.",
        "Observations indicate that central galaxies show a significant alignment of\ntheir main shape axes with other galaxies in their group, as well as with the\nlarge-scale structure of the universe. Simulations have corroborated this\nfinding, providing further insights into how the shape of the stellar component\naligns with the surrounding dark matter halo. Recent studies have also\ninvestigated the evolution of this alignment in bright central galaxies,\nrevealing that the shapes of the dark matter halo and the stellar component can\ndiffer. In this work, we aim at gaining a deeper understanding of galaxy\nalignments by quantifying how this property is related to the mass of the\nhaloes hosting central galaxies and to the large-scale environment measured at\ndifferent scales. By studying different angles, we describe how the alignments\nof central galaxies depend on the mass of the haloes they inhabit. We explore\nhow the main axes of central galaxies align across different scales, both in\nthree-dimensional and two-dimensional projections. We examine how halo mass\ninfluences these alignments and how they vary in the surrounding large-scale\nenvironment. To conduct this study, we employ the TNG300 hydrodynamical\nsimulations and compare our results with the spectroscopic data from the SDSS\nDR18. Three types of alignment were analysed: between stellar and dark matter\ncomponents, between satellite galaxies and the central galaxy, and between the\ncentral galaxy and its host halo. The results show that the alignment increases\nwith halo mass and varies with the environment (clusters, filaments, cluster\nperiphery, and others). However, after controlling for local density, we found\nthat most of the observed trends disappear. The SDSS observations confirm a\nmass dependence similar to the simulations, although observational biases limit\nthe detection of differences between the different environments.",
        "In this paper, we investigate the heliospheric modulation of cosmic rays in\ninterplanetary space, focusing on their propagation times and energy losses\nover the solar cycle. To perform the calculations, we employed a data-driven\nmodel based on the stochastic method. Our model was calibrated using\ntime-resolved and energy-resolved data from several missions including AMS-02,\nPAMELA, EPHIN\/SOHO, BESS, and data from Voyager-1. This approach allows us to\ncalculate probability density functions for the propagation time and energy\nlosses of cosmic protons and antiprotons in the heliosphere. Furthermore, we\nexplore the temporal evolution of these probabilities spanning from 1993 to\n2018, covering a full 22-year cycle of magnetic polarity, which includes two\nsolar minima and two magnetic reversals. Our calculations were carried out for\ncosmic protons and antiprotons, enabling us to investigate the role of\ncharge-sign dependent effects in cosmic ray transport. These findings provide\nvaluable insights into the physical processes of cosmic-ray propagation in the\nheliosphere and contribute to a deeper understanding of the solar modulation\nphenomenon.",
        "Identifying optimal medical treatments to improve survival has long been a\ncritical goal of pharmacoepidemiology. Traditionally, we use an average\ntreatment effect measure to compare outcomes between treatment plans. However,\nnew methods leveraging advantages of machine learning combined with the\nfoundational tenets of causal inference are offering an alternative to the\naverage treatment effect. Here, we use three unique, precision medicine\nalgorithms (random forests, residual weighted learning, efficient augmentation\nrelaxed learning) to identify optimal treatment rules where patients receive\nthe optimal treatment as indicated by their clinical history. First, we present\na simple hypothetical example and a real-world application among heart failure\npatients using Medicare claims data. We next demonstrate how the optimal\ntreatment rule improves the absolute risk in a hypothetical, three-modifier\nsetting. Finally, we identify an optimal treatment rule that optimizes the time\nto outcome in a real-world heart failure setting. In both examples, we compare\nthe average time to death under the optimized, tailored treatment rule with the\naverage time to death under a universal treatment rule to show the benefit of\nprecision medicine methods. The improvement under the optimal treatment rule in\nthe real-world setting is greatest (additional ~9 days under the tailored rule)\nfor survival time free of heart failure readmission."
      ]
    }
  },
  {
    "id":2411.01758,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Multi-site quality and variability analysis of 3D FDG PET segmentations based on phantom and clinical image data",
    "start_abstract":"Purpose: Radiomics utilizes a large number of image-derived features for quantifying tumor characteristics that can in turn be correlated with response and prognosis. Unfortunately, extraction and analysis of such image-based features is subject to measurement variability and bias. The challenge for radiomics is particularly acute in Positron Emission Tomography (PET) where limited resolution, a high noise component related to the limited stochastic nature of the raw data, and the wide variety of reconstruction options confound quantitative feature metrics. Extracted feature quality is also affected by tumor segmentation methods used to define regions over which to calculate features, making it challenging to produce consistent radiomics analysis results across multiple institutions that use different segmentation algorithms in their PET image analysis. Understanding each element contributing to these inconsistencies in quantitative image feature and metric generation is paramount for ultimate utilization of these methods in multi-institutional trials and clinical oncology decision making. Methods: To assess segmentation quality and consistency at the multi-institutional level, we conducted a study of seven institutional members of the National Cancer Institute Quantitative Imaging Network. For the study, members were asked to segment a common set of phantom PET scans acquired over a range of imaging conditions as well as a second set of head and neck cancer (HNC) PET scans. Segmentations were generated at each institution using their preferred approach. In addition, participants were asked to repeat segmentations with a time interval between initial and repeat segmentation. This procedure resulted in overall 806 phantom insert and 641 lesion segmentations. Subsequently, the volume was computed from the segmentations and compared to the corresponding reference volume by means of statistical analysis. Results: On the two test sets (phantom and HNC PET scans), the performance of the seven segmentation approaches was as follows. On the phantom test set, the mean relative volume errors ranged from 29.9 to 87.8% of the ground truth reference volumes, and the repeat difference for each institution ranged between -36.4 to 39.9%. On the HNC test set, the mean relative volume error ranged between -50.5 to 701.5%, and the repeat difference for each institution ranged between -37.7 to 31.5%. In addition, performance measures per phantom insert\/lesion size categories are given in the paper. On phantom data, regression analysis resulted in coefficient of variation (CV) components of 42.5% for scanners, 26.8% for institutional approaches, 21.1% for repeated segmentations, 14.3% for relative contrasts, 5.3% for count statistics (acquisition times), and 0.0% for repeated scans. Analysis showed that the CV components for approaches and repeated segmentations were significantly larger on the HNC test set with increases by 112.7% and 102.4%, respectively. Conclusion: Analysis results underline the importance of PET scanner reconstruction harmonization and imaging protocol standardization for quantification of lesion volumes. In addition, to enable a distributed multi-site analysis of FDG PET images, harmonization of analysis approaches and operator training in combination with highly automated segmentation methods seems to be advisable. Future work will focus on quantifying the impact of segmentation variation on radiomics system performance.",
    "start_categories":[
      "q-bio.CB"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b2"
      ],
      "title":[
        "A review on segmentation of positron emission tomography images"
      ],
      "abstract":[
        "Positron Emission Tomography (PET), a non-invasive functional imaging method at the molecular level, images the distribution of biologically targeted radiotracers with high sensitivity. PET imaging provides detailed quantitative information about many diseases and is often used to evaluate inflammation, infection, and cancer by detecting emitted photons from a radiotracer localized to abnormal cells. In order to differentiate abnormal tissue from surrounding areas in PET images, image segmentation methods play a vital role; therefore, accurate image segmentation is often necessary for proper disease detection, diagnosis, treatment planning, and follow-ups. In this review paper, we present state-of-the-art PET image segmentation methods, as well as the recent advances in image segmentation techniques. In order to make this manuscript self-contained, we also briefly explain the fundamentals of PET imaging, the challenges of diagnostic PET image analysis, and the effects of these challenges on the segmentation results."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "CoddLLM: Empowering Large Language Models for Data Analytics",
        "Unlocking the Potential of Unlabeled Data in Semi-Supervised Domain\n  Generalization",
        "Towards a Study of Low Energy Antiproton Annihilations on Nuclei",
        "Knowledge Phenomenology Research of Future Industrial Iconic Product\n  Innovation",
        "The simplest solutions of cold plasma equations: change in properties\n  from a hydrodynamic to a kinetic model",
        "Position: Stop Acting Like Language Model Agents Are Normal Agents",
        "Optimizing Energy Efficiency in Subthreshold RISC-V Cores",
        "Non-archimedean integration on totally disconnected spaces",
        "Cohomology of classifying spaces of rank 3 Kac-Moody groups",
        "Optimization Methods for Joint Eigendecomposition",
        "Graph factors and powers of Hamilton cycles in the budget-constrained\n  random graph process",
        "Teaching Dense Retrieval Models to Specialize with Listwise Distillation\n  and LLM Data Augmentation",
        "Learning to be Smooth: An End-to-End Differentiable Particle Smoother",
        "Advanced Zero-Shot Text-to-Speech for Background Removal and\n  Preservation with Controllable Masked Speech Prediction",
        "A diagrammatic approach to the Rasmussen invariant via tangles and\n  cobordisms",
        "Autonomous Robotic Bone Micro-Milling System with Automatic Calibration\n  and 3D Surface Fitting",
        "Structured Preconditioners in Adaptive Optimization: A Unified Analysis",
        "Learning a Game by Paying the Agents",
        "Invariant Measures for Data-Driven Dynamical System Identification:\n  Analysis and Application",
        "Non-dispersive graded impedance acoustic lenses",
        "Uncertainty-Aware Label Refinement on Hypergraphs for Personalized\n  Federated Facial Expression Recognition",
        "Spin Squeezing of Macroscopic Nuclear Spin Ensembles",
        "Locally and Polar Harmonic Maass Forms for Orthogonal Groups of\n  Signature $(2, n)$",
        "The properties of supermassive stars in galaxy merger driven direct\n  collapse I: models without rotation",
        "HEP High Power Targetry Roadmap -- Workshop Report",
        "G-Refer: Graph Retrieval-Augmented Large Language Model for Explainable\n  Recommendation",
        "MATS: An Audio Language Model under Text-only Supervision",
        "Bubbles-induced transition to elasto-inertial turbulence",
        "SFLD: Reducing the content bias for AI-generated Image Detection"
      ],
      "abstract":[
        "Large Language Models (LLMs) have the potential to revolutionize data\nanalytics by simplifying tasks such as data discovery and SQL query synthesis\nthrough natural language interactions. This work serves as a pivotal first step\ntoward the development of foundation models explicitly designed for data\nanalytics applications. To propel this vision forward, we unveil a new data\nrecipe for post-training LLMs, enhancing their comprehension of data management\nand empowering them to tackle complex real-world analytics tasks. Specifically,\nour innovative approach includes a scalable synthetic data generation method\nthat enables the creation of a broad spectrum of topics centered on data\nrepresentation and manipulation. Furthermore, we introduce two new tasks that\nseamlessly bridge tables and text. We show that such tasks can enhance models'\nunderstanding of schema creation and the nuanced translation between natural\nlanguage and tabular data. Leveraging this data recipe, we post-train a new\nfoundation model, named CoddLLM, based on Mistral-NeMo-12B. To assess the\nlanguage understanding and reasoning capabilities of LLMs in the realm of data\nanalytics, we contribute AnalyticsMMLU, a benchmark containing thousands of\nmultiple-choice questions on databases, data analysis, and machine learning.\nOur focus on data discovery, has resulted in the contribution of three\ncomprehensive benchmarks that address both database and data lake scenarios.\nCoddLLM not only excels in performance but also sets a new standard, achieving\nthe highest average accuracy across eight datasets. It outperforms\nGPT-3.5-Turbo on AnalyticsMMLU, exceeding GPT-4o by 12.1% in table selection\nand showing an average improvement of 24.9% in Text-to-SQL compared to the base\nmodel.",
        "We address the problem of semi-supervised domain generalization (SSDG), where\nthe distributions of train and test data differ, and only a small amount of\nlabeled data along with a larger amount of unlabeled data are available during\ntraining. Existing SSDG methods that leverage only the unlabeled samples for\nwhich the model's predictions are highly confident (confident-unlabeled\nsamples), limit the full utilization of the available unlabeled data. To the\nbest of our knowledge, we are the first to explore a method for incorporating\nthe unconfident-unlabeled samples that were previously disregarded in SSDG\nsetting. To this end, we propose UPCSC to utilize these unconfident-unlabeled\nsamples in SSDG that consists of two modules: 1) Unlabeled Proxy-based\nContrastive learning (UPC) module, treating unconfident-unlabeled samples as\nadditional negative pairs and 2) Surrogate Class learning (SC) module,\ngenerating positive pairs for unconfident-unlabeled samples using their\nconfusing class set. These modules are plug-and-play and do not require any\ndomain labels, which can be easily integrated into existing approaches.\nExperiments on four widely used SSDG benchmarks demonstrate that our approach\nconsistently improves performance when attached to baselines and outperforms\ncompeting plug-and-play methods. We also analyze the role of our method in\nSSDG, showing that it enhances class-level discriminability and mitigates\ndomain gaps. The code is available at https:\/\/github.com\/dongkwani\/UPCSC.",
        "A study of antiproton annihilations at rest on thin solid targets is underway\nat the ASACUSA facility, which now features a dedicated beam line for slow\nextraction at 250 eV. The experiment will employ new technologies, such as the\nTimepix4 ASICs coupled to silicon sensors, to measure the total multiplicity,\nenergy, and angular distribution of various prongs produced in thin solid\ntargets. A detection system consisting of seven Timepix4, covering most of the\nsolid angle, is being constructed. A 3D annihilation vertex reconstruction\nalgorithm from particle tracks in the single-plane detectors has been developed\nusing Monte Carlo simulations. The measurements will enable a study of\npbar-nucleus interactions, their dependence on nucleus mass and branching\nratios. The results will be used to assess and potentially improve various\nsimulation models.",
        "Iconic products, as innovative carriers supporting the development of future\nindustries, are key breakthrough points for driving the transformation of new\nquality productive forces. This article is grounded in the philosophy of\ntechnology and examines the evolution of human civilization to accurately\nidentify the patterns of product innovation. By integrating theories from\nsystems science, it analyzes the intrinsic logical differences between\ntraditional products and iconic products. The study finds that iconic products\nare based on a comprehensive knowledge system that integrates explicit and\ntacit knowledge, enabling them to adapt to complex dynamic environments.\nTherefore, based on the method of phenomenological essence reduction and the\nprocess of specialized knowledge acquisition, this study establishes the first\nprinciple of knowledge phenomenology: \"knowledge generation-moving from the\ntacit to the explicit-moving from the explicit to the tacit-fusion of the\nexplicit and tacit.\" Grounded in knowledge phenomenology, it reconstructs the\nproduct design evolution process and establishes a forward innovative design\nframework for iconic products, consisting of \"design problem space-explicit\nknowledge space-tacit knowledge space-innovative solution space.\" Furthermore,\nbased on FBS design theory, it develops a disruptive technology innovation\nforecasting framework of \"technology problem space-knowledge base\nprediction-application scenario prediction-coupled technology prediction,\"\nwhich collectively advances the innovation systems engineering of iconic\nproducts. In light of the analysis of the global future industrial competitive\nlandscape, it proposes a strategy for enhancing embodied intelligence in iconic\nproducts.",
        "We consider the transition from the kinetic model of Landau cold plasma to\nthe hydrodynamic one by constructing a \"multi-speed\" moment chain in the case\nof one spatial variable. Closing this chain at the first step leads to the\nstandard hydrodynamic system of cold plasma. The change in the properties of\nthe solution when closing the chain at the second step is discussed using the\nexample of two classes of solutions - affine in space and traveling waves, and\nit is shown that their properties change significantly compared to the\nhydrodynamic model.",
        "Language Model Agents (LMAs) are increasingly treated as capable of\nautonomously navigating interactions with humans and tools. Their design and\ndeployment tends to presume they are normal agents capable of sustaining\ncoherent goals, adapting across contexts and acting with a measure of\nintentionality. These assumptions are critical to prospective use cases in\nindustrial, social and governmental settings. But LMAs are not normal agents.\nThey inherit the structural problems of the large language models (LLMs) around\nwhich they are built: hallucinations, jailbreaking, misalignment and\nunpredictability. In this Position paper we argue LMAs should not be treated as\nnormal agents, because doing so leads to problems that undermine their utility\nand trustworthiness. We enumerate pathologies of agency intrinsic to LMAs.\nDespite scaffolding such as external memory and tools, they remain\nontologically stateless, stochastic, semantically sensitive, and linguistically\nintermediated. These pathologies destabilise the ontological properties of LMAs\nincluding identifiability, continuity, persistence and and consistency,\nproblematising their claim to agency. In response, we argue LMA ontological\nproperties should be measured before, during and after deployment so that the\nnegative effects of pathologies can be mitigated.",
        "Our goal in this paper is to understand how to maximize energy efficiency\nwhen designing standard-ISA processor cores for subthreshold operation. We\nhence develop a custom subthreshold library and use it to synthesize the\nopen-source RISC-V cores SERV, QERV, PicoRV32, Ibex, Rocket, and two variants\nof Vex, targeting a supply voltage of 300 mV in a commercial 130 nm process.\nSERV, QERV, and PicoRV32 are multi-cycle architectures, while Ibex, Vex, and\nRocket are pipelined architectures.\n  We find that SERV, QERV, PicoRV32, and Vex are Pareto optimal in one or more\nof performance, power, and area. The 2-stage Vex (Vex-2) is the most energy\nefficient core overall, mainly because it uses fewer cycles per instruction\nthan multi-cycle SERV, QERV, and PicoRV32 while retaining similar power\nconsumption. Pipelining increases core area, and we observe that for\nsubthreshold operation, the longer wires of pipelined designs require adding\nbuffers to maintain a cycle time that is low enough to achieve high energy\nefficiency. These buffers limit the performance gains achievable by deeper\npipelining because they result in cycle time no longer scaling proportionally\nwith pipeline stages. The added buffers and the additional area required for\npipelining logic however increase power consumption, and Vex-2 therefore\nprovides similar performance and lower power consumption than the 5-stage cores\nVex-5 and Rocket. A key contribution of this paper is therefore to demonstrate\nthat limited-depth pipelined RISC-V designs hit the sweet spot in balancing\nperformance and power consumption when optimizing for energy efficiency in\nsubthreshold operation.",
        "We work in the category $\\mathcal{CLM}^u_k$ of [5] of separated complete\nbounded $k$-linearly topologized modules over a complete linearly topologized\nring $k$ and discuss duality on certain exact subcategories. We study\ntopological and uniform structures on locally compact paracompact\n$0$-dimensional topological spaces $X$, named $td$-spaces in [11] and [17], and\nthe corresponding algebras $\\mathscr{C}_?(X,k)$ of continuous $k$-valued\nfunctions, with a choice of support and uniformity conditions. We apply the\nprevious duality theory to define and study the dual coalgebras\n$\\mathscr{D}_?(X,k)$ of $k$-valued measures on $X$. We then complete the\npicture by providing a direct definition of the various types of measures. In\nthe case of $X$ a commutative $td$-group $G$ the integration pairing provides\nperfect dualities of Hopf $k$-algebras between $$\\mathscr{C}_{\\rm unif}(G,k)\n\\longrightarrow \\mathscr{C}(G,k) \\;\\;\\;\\mbox{and}\\;\\;\\; \\mathscr{D}_{\\rm\nacs}(G,k) \\longrightarrow \\mathscr{D}_{\\rm unif}(G,k) \\;.$$ We conclude the\npaper with the remarkable example of $G= \\mathbb{G}_a(\\mathbb{Q}_p)$ and $k =\n\\mathbb{Z}_p$, leading to the basic Fontaine ring $${\\bf A}_{\\rm inf} = {\\rm W}\n\\left(\\widehat{\\mathbb{F}_p[[t^{1\/p^\\infty}]]}\\right) = \\mathscr{D}_{\\rm\nunif}(\\mathbb{Q}_p,\\mathbb{Z}_p) \\;.$$ We discuss Fourier duality between ${\\bf\nA}_{\\rm inf}$ and $\\mathscr{C}_{\\rm unif}(\\mathbb{Q}_p,\\mathbb{Z}_p)$ and\nexhibit a remarkable Fr\\'echet basis of $\\mathscr{C}_{\\rm\nunif}(\\mathbb{Q}_p,\\mathbb{Z}_p)$ related to the classical binomial\ncoefficients.",
        "We represent the rational and mod $p$ cohomology groups of classifying spaces\nof rank 3 Kac-Moody groups by a direct sum of the invariants of Weyl groups and\ntheir quotients. As an application, the authors conclude that there is a\n$p$-torsion for each prime $p$ in the integral cohomology groups of classifying\nspaces of rank 3 Kac-Moody groups. We also determine the ring structure of the\nrational cohomology with one exception case.",
        "Joint diagonalization, the process of finding a shared set of approximate\neigenvectors for a collection of matrices, arises in diverse applications such\nas multidimensional harmonic analysis or quantum information theory. This task\nis typically framed as an optimization problem: minimizing a non-convex\nfunction that quantifies off-diagonal matrix elements across possible bases. In\nthis work, we introduce a suite of efficient algorithms designed to locate\nlocal minimizers of this functional. Our methods leverage the Hessian's\nstructure to bypass direct computation of second-order derivatives, evaluating\nit as either an operator or bilinear form - a strategy that remains\ncomputationally feasible even for large-scale applications. Additionally, we\ndemonstrate that this Hessian-based information enables precise estimation of\nparameters, such as step-size, in first-order optimization techniques like\nGradient Descent and Conjugate Gradient, and the design of second-order methods\nsuch as (Quasi-)Newton. The resulting algorithms for joint diagonalization\noutperform existing techniques, and we provide comprehensive numerical evidence\nof their superior performance.",
        "We consider the following budget-constrained random graph process introduced\nby Frieze, Krivelevich and Michaeli. A player, called Builder, is presented\nwith $t$ distinct edges of $K_n$ one by one, chosen uniformly at random.\nBuilder may purchase at most $b$ of these edges, and must (irrevocably) decide\nwhether to purchase each edge as soon as it is offered. Builder's goal is to\nconstruct a graph which satisfies a certain property; we investigate the\nproperties of containing different $F$-factors or powers of Hamilton cycles.\n  We obtain general lower bounds on the budget $b$, as a function of $t$,\nrequired for Builder to obtain partial $F$-factors, for arbitrary $F$. These\nimply lower bounds for many distinct spanning structures, such as powers of\nHamilton cycles. Notably, our results show that, if $t$ is close to the hitting\ntime for a partial $F$-factor, then the budget $b$ cannot be substantially\nlower than $t$. These results give negative answers to questions of Frieze,\nKrivelevich and Michaeli.\n  Conversely, we also exhibit a simple strategy for constructing (partial)\n$F$-factors, in particular showing that our general lower bound is tight up to\nconstant factors. The ideas from this strategy can be exploited for other\nproperties. As an example, we obtain an essentially optimal strategy for powers\nof Hamilton cycles. In order to formally prove that this strategy succeeds, we\ndevelop novel tools for analysing multi-stage strategies, which may be of\ngeneral interest for studying other properties.",
        "While the current state-of-the-art dense retrieval models exhibit strong\nout-of-domain generalization, they might fail to capture nuanced\ndomain-specific knowledge. In principle, fine-tuning these models for\nspecialized retrieval tasks should yield higher effectiveness than relying on a\none-size-fits-all model, but in practice, results can disappoint. We show that\nstandard fine-tuning methods using an InfoNCE loss can unexpectedly degrade\neffectiveness rather than improve it, even for domain-specific scenarios. This\nholds true even when applying widely adopted techniques such as hard-negative\nmining and negative de-noising. To address this, we explore a training strategy\nthat uses listwise distillation from a teacher cross-encoder, leveraging rich\nrelevance signals to fine-tune the retriever. We further explore synthetic\nquery generation using large language models. Through listwise distillation and\ntraining with a diverse set of queries ranging from natural user searches and\nfactual claims to keyword-based queries, we achieve consistent effectiveness\ngains across multiple datasets. Our results also reveal that synthetic queries\ncan rival human-written queries in training utility. However, we also identify\nlimitations, particularly in the effectiveness of cross-encoder teachers as a\nbottleneck. We release our code and scripts to encourage further research.",
        "For challenging state estimation problems arising in domains like vision and\nrobotics, particle-based representations attractively enable temporal reasoning\nabout multiple posterior modes. Particle smoothers offer the potential for more\naccurate offline data analysis by propagating information both forward and\nbackward in time, but have classically required human-engineered dynamics and\nobservation models. Extending recent advances in discriminative training of\nparticle filters, we develop a framework for low-variance propagation of\ngradients across long time sequences when training particle smoothers. Our\n\"two-filter'' smoother integrates particle streams that are propagated forward\nand backward in time, while incorporating stratification and importance weights\nin the resampling step to provide low-variance gradient estimates for neural\nnetwork dynamics and observation models. The resulting mixture density particle\nsmoother is substantially more accurate than state-of-the-art particle filters,\nas well as search-based baselines, for city-scale global vehicle localization\nfrom real-world videos and maps.",
        "The acoustic background plays a crucial role in natural conversation. It\nprovides context and helps listeners understand the environment, but a strong\nbackground makes it difficult for listeners to understand spoken words. The\nappropriate handling of these backgrounds is situation-dependent: Although it\nmay be necessary to remove background to ensure speech clarity, preserving the\nbackground is sometimes crucial to maintaining the contextual integrity of the\nspeech. Despite recent advancements in zero-shot Text-to-Speech technologies,\ncurrent systems often struggle with speech prompts containing backgrounds. To\naddress these challenges, we propose a Controllable Masked Speech Prediction\nstrategy coupled with a dual-speaker encoder, utilizing a task-related control\nsignal to guide the prediction of dual background removal and preservation\ntargets. Experimental results demonstrate that our approach enables precise\ncontrol over the removal or preservation of background across various acoustic\nconditions and exhibits strong generalization capabilities in unseen scenarios.",
        "We introduce a diagrammatic approach to Rasmussen's $s$-invariant via tangles\nand cobordisms, combining Bar-Natan's formulation of Khovanov homology for\ntangles and cobordisms with the characterization of $s$ via the divisibility of\nthe Lee class, as developed in the author's previous works. This framework\nenables a \"divide-and-conquer\" method for computing $s$ from a tangle\ndecomposition of a given knot diagram, making it suitable for both\npen-and-paper calculations and algorithmic implementations. As an application,\nwe determine the $s$-invariants of pretzel knots of the form $P(p_1, -p_2,\n\\ldots, -p_l)$, where $l \\geq 3$ is odd, all $p_i$ are positive and odd, and\n$p_1 < \\min\\{p_2, \\ldots, p_l\\}$.",
        "Automating bone micro-milling using a robotic system presents challenges due\nto the uncertainties in both the external and internal features of bone tissue.\nFor example, during a mouse cranial window creation, a circular path with a\nradius of 2 to 4 mm needs to be milled on the mouse skull using a microdrill.\nThe uneven surface and non-uniform thickness of the mouse skull make it\ndifficult to fully automate this process, requiring the system to possess\nadvanced perceptual and adaptive capabilities. In this study, we propose an\nautomatic calibration and 3D surface fitting method and integrate it into an\nautonomous robotic bone micro-milling system, enabling it to quickly, in\nreal-time, and accurately perceive and adapt to the uneven surface and\nnon-uniform thickness of the target without human assistance. Validation\nexperiments on euthanized mice demonstrate that the improved system achieves a\nsuccess rate of 85.7 % and an average milling time of 2.1 minutes, showing not\nonly significant performance improvements over the previous system but also\nexceptional accuracy, speed, and stability compared to human operators.",
        "We present a novel unified analysis for a broad class of adaptive\noptimization algorithms with structured (e.g., layerwise, diagonal, and\nkronecker-factored) preconditioners for both online regret minimization and\noffline convex optimization. Our analysis not only provides matching rate to\nseveral important structured preconditioned algorithms including diagonal\nAdaGrad, full-matrix AdaGrad, and AdaGrad-Norm, but also gives an improved\nconvergence rate for a one-sided variant of Shampoo over that of original\nShampoo. Interestingly, more structured preconditioners (e.g., diagonal\nAdagrad, AdaGrad-Norm which use less space and compute) are often presented as\ncomputationally efficient approximations to full-matrix Adagrad, aiming for\nimproved optimization performance through better approximations. Our unified\nanalysis challenges this prevailing view and reveals, perhaps surprisingly,\nthat more structured preconditioners, despite using less space and computation\nper step, can outperform their less structured counterparts. To demonstrate\nthis, we show that one-sided Shampoo, which is relatively much cheaper than\nfull-matrix AdaGrad could outperform it both theoretically and experimentally.",
        "We study the problem of learning the utility functions of agents in a\nnormal-form game by observing the agents play the game repeatedly. Differing\nfrom most prior literature, we introduce a principal with the power to observe\nthe agents playing the game, send the agents signals, and send the agents\npayments as a function of their actions. Under reasonable behavioral models for\nthe agents such as iterated dominated action removal or a no-regret assumption,\nwe show that the principal can, using a number of rounds polynomial in the size\nof the game, learn the utility functions of all agents to any desirable\nprecision $\\varepsilon > 0$. We also show lower bounds in both models, which\nnearly match the upper bounds in the former model and also strictly separate\nthe two models: the principal can learn strictly faster in the iterated\ndominance model. Finally, we discuss implications for the problem of steering\nagents to a desired equilibrium: in particular, we introduce, using our\nutility-learning algorithm as a subroutine, the first algorithm for steering\nlearning agents without prior knowledge of their utilities.",
        "We propose a novel approach for performing dynamical system identification,\nbased upon the comparison of simulated and observed physical invariant\nmeasures. While standard methods adopt a Lagrangian perspective by directly\ntreating time-trajectories as inference data, we take on an Eulerian\nperspective and instead seek models fitting the observed global time-invariant\nstatistics. With this change in perspective, we gain robustness against\npervasive challenges in system identification including noise, chaos, and slow\nsampling. In the first half of this paper, we pose the system identification\ntask as a partial differential equation (PDE) constrained optimization problem,\nin which synthetic stationary solutions of the Fokker-Planck equation, obtained\nas fixed points of a finite-volume discretization, are compared to physical\ninvariant measures extracted from observed trajectory data. In the latter half\nof the paper, we improve upon this approach in two crucial directions. First,\nwe develop a Galerkin-inspired modification to the finite-volume surrogate\nmodel, based on data-adaptive unstructured meshes and Monte-Carlo integration,\nenabling the approach to efficiently scale to high-dimensional problems.\nSecond, we leverage Takens' seminal time-delay embedding theory to introduce a\ncritical data-dependent coordinate transformation which can guarantee unique\nsystem identifiability from the invariant measure alone. This contribution\nresolves a major challenge of system identification through invariant measures,\nas systems exhibiting distinct transient behaviors may still share the same\ntime-invariant statistics in their state-coordinates. Throughout, we present\ncomprehensive numerical tests which highlight the effectiveness of our approach\non a variety of challenging system identification tasks.",
        "Lenses are typically based on refractive index profiles derived from the\ngeometric approximation of high-frequency waves, yet the critical issue of\nimpedance mismatch is often neglected. Mismatched devices suffer from unwanted\nreflections and dispersion, which can significantly degrade performance in\npractical applications. In this work, we propose impedance profiles for lenses\nto achieve efficient wave transmission while maintaining the desired refractive\nindex and minimizing dispersion effects. A family of impedance profiles is\nderived from the acoustic wave equation such that the phase velocity is\npreserved. First, the 1D setting is considered to explain how dispersion occurs\ninside a lens and at its interfaces. Then, the method is applied to 2D\naxisymmetric configurations where the impedance mismatch is radially\nredistributed. These profiles are demonstrated in the acoustic setting of a\nLuneburg lens, but can be easily extended to more general scenarios such as\nimaging or cloaking in air and water, where matching the impedance of the\nbackground poses significant challenges.",
        "Most facial expression recognition (FER) models are trained on large-scale\nexpression data with centralized learning. Unfortunately, collecting a large\namount of centralized expression data is difficult in practice due to privacy\nconcerns of facial images. In this paper, we investigate FER under the\nframework of personalized federated learning, which is a valuable and practical\ndecentralized setting for real-world applications. To this end, we develop a\nnovel uncertainty-Aware label refineMent on hYpergraphs (AMY) method. For local\ntraining, each local model consists of a backbone, an uncertainty estimation\n(UE) block, and an expression classification (EC) block. In the UE block, we\nleverage a hypergraph to model complex high-order relationships between\nexpression samples and incorporate these relationships into uncertainty\nfeatures. A personalized uncertainty estimator is then introduced to estimate\nreliable uncertainty weights of samples in the local client. In the EC block,\nwe perform label propagation on the hypergraph, obtaining high-quality refined\nlabels for retraining an expression classifier. Based on the above, we\neffectively alleviate heterogeneous sample uncertainty across clients and learn\na robust personalized FER model in each client. Experimental results on two\nchallenging real-world facial expression databases show that our proposed\nmethod consistently outperforms several state-of-the-art methods. This\nindicates the superiority of hypergraph modeling for uncertainty estimation and\nlabel refinement on the personalized federated FER task. The source code will\nbe released at https:\/\/github.com\/mobei1006\/AMY.",
        "Spin squeezing has been explored in atomic systems as a tool for quantum\nsensing, improving experimental sensitivity beyond the spin standard quantum\nlimit for certain measurements. To optimize absolute metrological sensitivity,\nit is beneficial to consider macroscopic spin ensembles, such as nuclear spins\nin solids and liquids. Coupling a macroscopic spin ensemble to a\nparametrically-modulated resonant circuit can create collective spin squeezing\nby generating spin correlations mediated by the circuit. We analyze the\nsqueezing dynamics in the presence of decoherence and finite spin polarization,\nshowing that achieving 7 dB spin squeezing is feasible in several nuclear spin\nsystems. The metrological benefit of squeezing a macroscopic spin ensemble lies\nin the suppression of technical noise sources in the spin detection system\nrelative to the spin projection noise. This expands the experimental\nsensitivity bandwidth when searching for signals of unknown frequency and can\nimprove the resonant signal-to-noise ratio. Squeezing macroscopic spin\nensembles may prove to be a useful technique for fundamental physics\nexperiments aimed at detecting spin interactions with oscillating background\nfields, such as ultralight dark matter.",
        "We generalize the notions of locally and polar harmonic Maass forms to\ngeneral orthogonal groups of signature $(2, n)$ with singularities along real\nanalytic and algebraic cycles. We prove a current equation for locally harmonic\nMaass forms and recover the Fourier expansion of the Oda lift involving cycle\nintegrals. Moreover, using the newly defined polar harmonic Maass forms, we\nprove that meromorphic modular forms with singularities along special divisors\nare orthogonal to cusp forms with respect to a regularized Petersson inner\nproduct. Using this machinery, we derive a duality theorem involving cycle\nintegrals of meromorphic modular forms along real analytic cycles and cycle\nintegrals of locally harmonic Maass forms along algebraic cycles.",
        "The formation of the most massive quasars observed at high redshifts requires\nextreme accretion rates ($>1$ M$_\\odot$ yr$^{-1}$). Inflows of $10-1000$\nM$_\\odot$ yr$^{-1}$ are found in hydrodynamical simulations of galaxy mergers,\nleading to the formation of supermassive discs (SMDs) with high metallicities\n($>$ Z$_\\odot$). Supermassive stars (SMSs) born in these SMDs could be the\nprogenitors of the most extreme quasars. Here, we study the properties of\nnon-rotating SMSs forming in high metallicity SMDs. Using the stellar evolution\ncode GENEC, we compute numerically the hydrostatic structures of non-rotating\nSMSs with metallicities $Z=1-10$ Z$_\\odot$ by following their evolution under\nconstant accretion at rates $10-1000$ M$_\\odot$ yr$^{-1}$. We determine the\nfinal mass of the SMSs, set by the general-relativistic (GR) instability, by\napplying the relativistic equation of adiabatic pulsations to the hydrostatic\nstructures. We find that non-rotating SMSs with metallicities $Z=1-10$\nZ$_\\odot$ accreting at rates $10-1000$ M$_\\odot$ yr$^{-1}$ evolve as red\nsupergiant protostars until their final collapse. All the models reach the GR\ninstability during H-burning. The final mass is $\\sim10^6$ M$_\\odot$, nearly\nindependently of the metallicity and the accretion rate.",
        "Designing a reliable target is already a challenge for MW-class facilities\ntoday and has led several major accelerator facilities to operate at lower than\ndesign power due to target concerns. With present plans to increase beam power\nfor next generation accelerator facilities in the next decade, timely R and D\nin support of robust high power targets is critical to secure the full physics\nbenefits of ambitious accelerator power upgrades. A comprehensive R and D\nprogram must be implemented to address the many complex challenges faced by\nmulti MW beam intercepting devices. This roadmap is envisioned to be helpful to\nthe DOE-OHEP office when planning and prioritizing future R and D activities as\nwell as leveraging synergies across the Office of Science. The roadmap will be\nextremely beneficial to the broader (external to DOE HEP) HPT community by\ncommunicating OHEP s high level strategy and objectives for HPT R and D and\nhighlighting possible opportunities for collaboration.",
        "Explainable recommendation has demonstrated significant advantages in\ninforming users about the logic behind recommendations, thereby increasing\nsystem transparency, effectiveness, and trustworthiness. To provide\npersonalized and interpretable explanations, existing works often combine the\ngeneration capabilities of large language models (LLMs) with collaborative\nfiltering (CF) information. CF information extracted from the user-item\ninteraction graph captures the user behaviors and preferences, which is crucial\nfor providing informative explanations. However, due to the complexity of graph\nstructure, effectively extracting the CF information from graphs still remains\na challenge. Moreover, existing methods often struggle with the integration of\nextracted CF information with LLMs due to its implicit representation and the\nmodality gap between graph structures and natural language explanations. To\naddress these challenges, we propose G-Refer, a framework using graph\nretrieval-augmented large language models (LLMs) for explainable\nrecommendation. Specifically, we first employ a hybrid graph retrieval\nmechanism to retrieve explicit CF signals from both structural and semantic\nperspectives. The retrieved CF information is explicitly formulated as\nhuman-understandable text by the proposed graph translation and accounts for\nthe explanations generated by LLMs. To bridge the modality gap, we introduce\nknowledge pruning and retrieval-augmented fine-tuning to enhance the ability of\nLLMs to process and utilize the retrieved CF information to generate\nexplanations. Extensive experiments show that G-Refer achieves superior\nperformance compared with existing methods in both explainability and\nstability. Codes and data are available at https:\/\/github.com\/Yuhan1i\/G-Refer.",
        "Large audio-language models (LALMs), built upon powerful Large Language\nModels (LLMs), have exhibited remarkable audio comprehension and reasoning\ncapabilities. However, the training of LALMs demands a large corpus of\naudio-language pairs, which requires substantial costs in both data collection\nand training resources. In this paper, we propose MATS, an audio-language\nmultimodal LLM designed to handle Multiple Audio task using solely Text-only\nSupervision. By leveraging pre-trained audio-language alignment models such as\nCLAP, we develop a text-only training strategy that projects the shared\naudio-language latent space into LLM latent space, endowing the LLM with audio\ncomprehension capabilities without relying on audio data during training. To\nfurther bridge the modality gap between audio and language embeddings within\nCLAP, we propose the Strongly-related noisy text with audio (Santa) mechanism.\nSanta maps audio embeddings into CLAP language embedding space while preserving\nessential information from the audio input. Extensive experiments demonstrate\nthat MATS, despite being trained exclusively on text data, achieves competitive\nperformance compared to recent LALMs trained on large-scale audio-language\npairs.",
        "Interface-resolved direct numerical simulations are performed to investigate\nbubble-induced transition from laminar to elasto-inertial turbulent (EIT) state\nin a pressure-driven viscoelastic square channel flow. The Giesekus model is\nused to account for the viscoelasticity of the continuous phase while the\ndispersed phase is Newtonian. Simulations are performed for both single and\ntwo-phase flows for a wide range of the Reynolds (Re) and the Weissenberg (Wi)\nnumbers. It is demonstrated that injection of bubbles into a laminar\nviscoelastic flow introduces streamline curvature that is sufficient to trigger\nan elastic instability leading to a transition to a fully EIT regime. The\ntemporal turbulent kinetic energy spectrum shows a scaling of -2 for this\nmultiphase EIT regime. It is shown that, once the flow is fully transitioned to\na turbulent state by the injection of bubbles, the drag increases for all the\ncases. It is also observed that bubbles move towards the channel centreline and\nform a string-shaped alignment pattern in the core region at the lower values\nof Re=10 and Wi=1. In this regime, the flow exhibits an intermittent behaviour,\ni.e., there are turbulent like fluctuations in the core region while it is\nessentially laminar near the wall. Unlike the solid particles, it is found that\nincreasing shear-thinning effect breaks up the alignment of bubbles.\nInterestingly, the drag remains slightly lower in this intermittent regime than\nthe corresponding laminar state.",
        "Identifying AI-generated content is critical for the safe and ethical use of\ngenerative AI. Recent research has focused on developing detectors that\ngeneralize to unknown generators, with popular methods relying either on\nhigh-level features or low-level fingerprints. However, these methods have\nclear limitations: biased towards unseen content, or vulnerable to common image\ndegradations, such as JPEG compression. To address these issues, we propose a\nnovel approach, SFLD, which incorporates PatchShuffle to integrate high-level\nsemantic and low-level textural information. SFLD applies PatchShuffle at\nmultiple levels, improving robustness and generalization across various\ngenerative models. Additionally, current benchmarks face challenges such as low\nimage quality, insufficient content preservation, and limited class diversity.\nIn response, we introduce TwinSynths, a new benchmark generation methodology\nthat constructs visually near-identical pairs of real and synthetic images to\nensure high quality and content preservation. Our extensive experiments and\nanalysis show that SFLD outperforms existing methods on detecting a wide\nvariety of fake images sourced from GANs, diffusion models, and TwinSynths,\ndemonstrating the state-of-the-art performance and generalization capabilities\nto novel generative models."
      ]
    }
  },
  {
    "id":2411.01758,
    "research_type":"applied",
    "start_id":"b2",
    "start_title":"A review on segmentation of positron emission tomography images",
    "start_abstract":"Positron Emission Tomography (PET), a non-invasive functional imaging method at the molecular level, images the distribution of biologically targeted radiotracers with high sensitivity. PET imaging provides detailed quantitative information about many diseases and is often used to evaluate inflammation, infection, and cancer by detecting emitted photons from a radiotracer localized to abnormal cells. In order to differentiate abnormal tissue from surrounding areas in PET images, image segmentation methods play a vital role; therefore, accurate image segmentation is often necessary for proper disease detection, diagnosis, treatment planning, and follow-ups. In this review paper, we present state-of-the-art PET image segmentation methods, as well as the recent advances in image segmentation techniques. In order to make this manuscript self-contained, we also briefly explain the fundamentals of PET imaging, the challenges of diagnostic PET image analysis, and the effects of these challenges on the segmentation results.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Multi-site quality and variability analysis of 3D FDG PET segmentations based on phantom and clinical image data"
      ],
      "abstract":[
        "Purpose: Radiomics utilizes a large number of image-derived features for quantifying tumor characteristics that can in turn be correlated with response and prognosis. Unfortunately, extraction and analysis of such image-based features is subject to measurement variability and bias. The challenge for radiomics is particularly acute in Positron Emission Tomography (PET) where limited resolution, a high noise component related to the limited stochastic nature of the raw data, and the wide variety of reconstruction options confound quantitative feature metrics. Extracted feature quality is also affected by tumor segmentation methods used to define regions over which to calculate features, making it challenging to produce consistent radiomics analysis results across multiple institutions that use different segmentation algorithms in their PET image analysis. Understanding each element contributing to these inconsistencies in quantitative image feature and metric generation is paramount for ultimate utilization of these methods in multi-institutional trials and clinical oncology decision making. Methods: To assess segmentation quality and consistency at the multi-institutional level, we conducted a study of seven institutional members of the National Cancer Institute Quantitative Imaging Network. For the study, members were asked to segment a common set of phantom PET scans acquired over a range of imaging conditions as well as a second set of head and neck cancer (HNC) PET scans. Segmentations were generated at each institution using their preferred approach. In addition, participants were asked to repeat segmentations with a time interval between initial and repeat segmentation. This procedure resulted in overall 806 phantom insert and 641 lesion segmentations. Subsequently, the volume was computed from the segmentations and compared to the corresponding reference volume by means of statistical analysis. Results: On the two test sets (phantom and HNC PET scans), the performance of the seven segmentation approaches was as follows. On the phantom test set, the mean relative volume errors ranged from 29.9 to 87.8% of the ground truth reference volumes, and the repeat difference for each institution ranged between -36.4 to 39.9%. On the HNC test set, the mean relative volume error ranged between -50.5 to 701.5%, and the repeat difference for each institution ranged between -37.7 to 31.5%. In addition, performance measures per phantom insert\/lesion size categories are given in the paper. On phantom data, regression analysis resulted in coefficient of variation (CV) components of 42.5% for scanners, 26.8% for institutional approaches, 21.1% for repeated segmentations, 14.3% for relative contrasts, 5.3% for count statistics (acquisition times), and 0.0% for repeated scans. Analysis showed that the CV components for approaches and repeated segmentations were significantly larger on the HNC test set with increases by 112.7% and 102.4%, respectively. Conclusion: Analysis results underline the importance of PET scanner reconstruction harmonization and imaging protocol standardization for quantification of lesion volumes. In addition, to enable a distributed multi-site analysis of FDG PET images, harmonization of analysis approaches and operator training in combination with highly automated segmentation methods seems to be advisable. Future work will focus on quantifying the impact of segmentation variation on radiomics system performance."
      ],
      "categories":[
        "q-bio.CB"
      ]
    },
    "list":{
      "title":[
        "Operational Feasibility Analysis of a Cryogenic Active Intake Device for\n  Atmosphere-Breathing Electric Propulsion",
        "Exponentially Better Bounds for Quantum Optimization via Dynamical\n  Simulation",
        "Scalar Field Kantowski--Sachs Solutions in Teleparallel $F(T)$ Gravity",
        "In the graphical Sierpinski gasket, the reverse Riesz transform is\n  unbounded on $L^p$, $p\\in (1,2)$",
        "On the origin of radio polarization in pulsar polar caps",
        "Heavy Axions Can Disrupt $\\gamma$-ray Bursts",
        "TOPCAT\/STILTS Integration",
        "Estimating Task-based Performance Bounds for Accelerated MRI Image\n  Reconstruction Methods by Use of Learned-Ideal Observers",
        "JT Gravity in de Sitter Space and Its Extensions",
        "Generalised Process Theories",
        "Observer-Based Output-Feedback Backstepping Stabilization of Continua of\n  Hyperbolic PDEs and Application to Large-Scale $n+m$ Coupled Hyperbolic PDEs",
        "Deep inference of simulated strong lenses in ground-based surveys",
        "A simple recursive representation of the Faulhaber series",
        "Fragmentation measurements with the FOOT experiment",
        "Galactic structure dependence of cloud-cloud collisions driven star\n  formation in the barred galaxy NGC 3627",
        "Fabrication and characterization of bimetallic silica-based and\n  3D-printed active colloidal cubes",
        "Current Opinions on Memristor-Accelerated Machine Learning Hardware",
        "Time scale competition in the Active Coagulation Model",
        "Trial by FIRE: Probing the dark matter density profile of dwarf galaxies\n  with GraphNPE",
        "Testing the Flux Rope Paradigm for Coronal Mass Ejections Using a Three\n  Spacecraft Encounter Event",
        "Convergence analysis of linearized $\\ell_q$ penalty methods for\n  nonconvex optimization with nonlinear equality constraints",
        "Siegel modular forms associated to Weil representations",
        "A Quantum Signature Validation Algorithm for Efficient Detection of\n  Tampered Transactions in Blockchain",
        "Study of environment friendly gas mixtures for the Resistive Plate\n  Chambers of the ATLAS phase-2 upgrade",
        "Machine Learning for Ground State Preparation via Measurement and\n  Feedback",
        "Image resizing by neural network operators and their convergence rate\n  with respect to the $L^p$-norm and the dissimilarity index defined through\n  the continuous SSIM",
        "DISCD: Distributed Lossy Semantic Communication for Logical Deduction of\n  Hypothesis",
        "Power of $(L_0,L_1)$-Smoothness in Stochastic Convex Optimization:\n  First- and Zero-Order Algorithms",
        "HedgeAgents: A Balanced-aware Multi-agent Financial Trading System"
      ],
      "abstract":[
        "Atmosphere-breathing electric propulsion (ABEP) systems are emerging for\norbit maintenance in very-low-Earth orbit (VLEO) by capturing atmospheric\npropellant \\textit{in situ} using an intake device. A previous study proposed\nthe cryocondensation-regeneration active intake device (CRAID) to significantly\nenhance intake performance. This study investigates the operational feasibility\nof CRAID. A conceptual prototype model (CPM) is presented to verify its\nfeasibility, and numerical analyses demonstrate the practical operational\nsequences, required cryocooler capacity, intake performance, and flight\nenvelope. The numerical analyses employ the direct simulation Monte Carlo\n(DSMC) method with a phase change model and a 0D analytical model for RF ion\nthrusters. A significant improvement in intake performance is estimated based\non the practical sequences, with compression performance at least 1000 times\nhigher than that of prevalent intake devices. The capability for consistent\npropellant supply is observed regardless of atmospheric conditions. A model\nsatellite incorporating CPM confirms that CRAID enables complete drag\ncompensation at altitudes above 190 km without limiting the upper boundary of\nthe flight envelope.",
        "We provide several quantum algorithms for continuous optimization that do not\nrequire any gradient estimation. Instead, we encode the optimization problem\ninto the dynamics of a physical system and coherently simulate the time\nevolution. This allows us, in certain cases, to obtain exponentially better\nquery upper bounds relative to the best known upper bounds for gradient-based\noptimization schemes which utilize quantum computers only for the evaluation of\ngradients. Our first two algorithms can find local optima of a differentiable\nfunction $f: \\mathbb{R}^N \\rightarrow \\mathbb{R}$ by simulating either\nclassical or quantum dynamics with friction via a time-dependent Hamiltonian.\nWe show that these methods require $O(N\\kappa^2\/h_x^2\\epsilon)$ queries to a\nphase oracle to find an $\\epsilon$-approximate local optimum of a locally\nquadratic objective function, where $\\kappa$ is the condition number of the\nHessian matrix and $h_x$ is the discretization spacing. In contrast, we show\nthat gradient-based methods require $O(N(1\/\\epsilon)^{\\kappa \\log(3)\/4})$\nqueries. Our third algorithm can find the global optimum of $f$ by preparing a\nclassical low-temperature thermal state via simulation of the classical\nLiouvillian operator associated with the Nos\\'e Hamiltonian. We use results\nfrom the quantum thermodynamics literature to bound the thermalization time for\nthe discrete system. Additionally, we analyze barren plateau effects that\ncommonly plague quantum optimization algorithms and observe that our approach\nis vastly less sensitive to this problem than standard gradient-based\noptimization. Our results suggests that these dynamical optimization approaches\nmay be far more scalable for future quantum machine learning, optimization and\nvariational experiments than was widely believed.",
        "In this paper, we investigate time-dependent Kantowski--Sachs spherically\nsymmetric teleparallel $F(T)$ gravity with a scalar field source. We begin by\nsetting the exact field equations to be solved and solve conservation laws for\npossible scalar field potential, $V\\left(\\phi\\right)$, solutions. Then, we find\nnew non-trivial teleparallel $F(T)$ solutions by using power-law and\nexponential ansatz for each potential case arising from conservation laws, such\nas linear, quadratic, or logarithmic, to name a few. We find a general formula\nallowing us to compute all possible new teleparallel $F(T)$ solutions\napplicable for any scalar field potential and ansatz. Then, we apply this\nformula and find a large number of exact and approximate new teleparallel\n$F(T)$ solutions for several types of cases. Some new $F(T)$ solution classes\nmay be relevant for future cosmological applications, especially concerning\ndark matter, dark energy quintessence, phantom energy leading to the Big Rip\nevent, and quintom models of physical processes.",
        "In this article, we proved that the reverse Riesz transform on the graphical\nSierpinski gasket is unbounded on $L^p$ for $p\\in (1,2)$. Together with\nprevious results, it shows that the Riesz transform on the graphical Sierpinski\ngasket is bounded on $L^p$ if and only if $p\\in (1,2]$ and the reverse Riesz\ntransform is bounded on $L^p$ if and only if $p\\in [2,\\infty)$.\n  Moreover, our method is quite flexible - but requires explicit computations -\nand hints to the fact that the reverse Riesz transforms is never bounded on\n$L^p$, $p\\in (1,2)$, on graphs with slow diffusions.",
        "A knowledge of polarization properties of coherent radio waves escaping\npulsar polar caps is crucial for calculating radiative transfer through the\nmagnetosphere and for obtaining specific predictions of observable radio\nproperties. We describe the pair cascades in the pulsar polar cap, and for the\nfirst time, determine the Stokes parameters of the escaping radio waves from\nfirst-principle kinetic simulations for a pulsar with an inclination angle of\nthe magnetic axis 60{\\deg}.\n  Our model provides a quantitative and qualitative explanation of the observed\npulsar radio powers and spectra, the pulse profiles and polarization curves,\ntheir temporal variability, the strong Stokes L and weak Stokes V polarization\ncomponents, as well as the fact that linear polarization decreases with\nfrequency and the non-existence of a radius to frequency relationship. We find\nthat the radio emission from the polar cap can produce a diverse range of\nobserved pulsar properties, including single or double peaked profiles. Most of\nthe Stokes V curves from our simulations appear to be antisymmetric, but\nsymmetric curves are also present at some viewing angles. Although the PA swing\nof the radiation from the polar cap can be fitted by the rotating vector model\n(RVM) for most viewing angles, the angles obtained from the RVM do not\ncorrespond to the angular distance of the observer from the magnetic axis.\nInstead, the PA is directly related to the plasma flows in the polar cap and\nnot to the dipole geometry of the magnetic field. The observed range of other\npolarization features, in addition to our results, can be explained by\npropagation effects which are not part of the simulation.\n  Our simulations demonstrate that pair discharges determine the majority of\nits typically observed properties. The usage of RVM for estimations of the\nmagnetic field geometry from observations needs to be reevaluated.",
        "Axion-like particles (ALPs) can be produced in the hot dense plasma of\nfireballs that develop in the initial stage of $\\gamma$-ray burst (GRB)\noutflows. They can transport an enormous amount of energy away from the jet by\npropagating out of the fireball. The photons produced by the eventual decay of\nsuch ALPs do not reach a sufficient density to re-thermalize through pair\nproduction, preventing fireball re-emergence. Thus, the production of heavy\nALPs disrupts the fireball and dims GRBs, allowing bright GRB observations to\nstrongly constrain the existence of heavy ALPs. By adding ALP interactions to\nexisting models of GRB fireballs, we set competitive bounds on the ALP-photon\ncoupling down to $g_{a \\gamma \\gamma} \\sim 4 \\times\n10^{-12}~{\\mathrm{GeV}^{-1}}$ for ALPs in the mass range of 200 MeV - 5 GeV.",
        "TOPCAT and STILTS are related packages for desktop analysis of tabular data,\npresenting GUI and command-line interfaces respectively to much of the same\nfunctionality. This paper presents features in TOPCAT that facilitate use of\nSTILTS.",
        "Medical imaging systems are commonly assessed and optimized by the use of\nobjective measures of image quality (IQ). The performance of the ideal observer\n(IO) acting on imaging measurements has long been advocated as a\nfigure-of-merit to guide the optimization of imaging systems. For computed\nimaging systems, the performance of the IO acting on imaging measurements also\nsets an upper bound on task-performance that no image reconstruction method can\ntranscend. As such, estimation of IO performance can provide valuable guidance\nwhen designing under-sampled data-acquisition techniques by enabling the\nidentification of designs that will not permit the reconstruction of\ndiagnostically inappropriate images for a specified task - no matter how\nadvanced the reconstruction method is or how plausible the reconstructed images\nappear. The need for such analysis is urgent because of the substantial\nincrease of medical device submissions on deep learning-based image\nreconstruction methods and the fact that they may produce clean images\ndisguising the potential loss of diagnostic information when data is\naggressively under-sampled. Recently, convolutional neural network (CNN)\napproximated IOs (CNN-IOs) was investigated for estimating the performance of\ndata space IOs to establish task-based performance bounds for image\nreconstruction, under an X-ray computed tomographic (CT) context. In this work,\nthe application of such data space CNN-IO analysis to multi-coil magnetic\nresonance imaging (MRI) systems has been explored. This study utilized stylized\nmulti-coil sensitivity encoding (SENSE) MRI systems and deep-generated\nstochastic brain models to demonstrate the approach. Signal-known-statistically\nand background-known-statistically (SKS\/BKS) binary signal detection tasks were\nselected to study the impact of different acceleration factors on the data\nspace IO performance.",
        "We discuss and extend some aspects pertaining to the canonical quantisation\nof JT gravity in de Sitter space, including the problem of time and the\nconstruction of a Hilbert space. We then extend this discussion to other two\ndimensional models obtained by changing the dilaton potential and show that the\ncanonical quantisation procedure can be carried out for a large class of such\nmodels. Some discussion leading towards a path integral understanding for\nstates, other than the Hartle Hawking state, is also included here, along with\ncomments pertaining to Holography and the entropy of de Sitter space.",
        "Process theories provide a powerful framework for describing compositional\nstructures across diverse fields, from quantum mechanics to computational\nlinguistics. Traditionally, they have been formalized using symmetric monoidal\ncategories (SMCs). However, various generalizations, including time-neutral,\nhigher-order, and enriched process theories, do not naturally conform to this\nstructure. In this work, we propose an alternative formalization using operad\nalgebras, motivated by recent results connecting SMCs to operadic structures,\nwhich captures a broader class of process theories. By leveraging the\nstring-diagrammatic language, we provide an accessible yet rigorous formulation\nthat unifies and extends traditional process-theoretic approaches. Our operadic\nframework not only recovers standard process theories as a special case but\nalso enables new insights into quantum foundations and compositional\nstructures. This work paves the way for further investigations into the\nalgebraic and operational properties of generalised process theories within an\noperadic setting.",
        "We develop a non-collocated, observer-based output-feedback law for a class\nof continua of linear hyperbolic PDE systems, which are viewed as the continuum\nversion of $n+m$, general heterodirectional hyperbolic systems as $n\\to\\infty$.\nThe design relies on the introduction of a novel, continuum PDE backstepping\ntransformation, which enables the construction of a Lyapunov functional for the\nestimation error system. Stability under the observer-based output-feedback law\nis established by using the Lyapunov functional construction for the estimation\nerror system and proving well-posedness of the complete closed-loop system,\nwhich allows utilization of the separation principle.\n  Motivated by the fact that the continuum-based designs may provide\ncomputationally tractable control laws for large-scale, $n+m$ systems, we then\nutilize the control\/observer kernels and the observer constructed for the\ncontinuum system to introduce an output-feedback control design for the\noriginal $n+m$ system. We establish exponential stability of the resulting\nclosed-loop system, which consists of a mixed $n+m$-continuum PDE system\n(comprising the plant-observer dynamics), introducing a virtual continuum\nsystem with resets, which enables utilization of the continuum approximation\nproperty of the solutions of the $n+m$ system by its continuum counterpart (for\nlarge $n$). We illustrate the potential computational complexity\/flexibility\nbenefits of our approach via a numerical example of stabilization of a\nlarge-scale $n+m$ system, for which we employ the continuum observer-based\ncontroller, while the continuum-based stabilizing control\/observer kernels can\nbe computed in closed form.",
        "The large number of strong lenses discoverable in future astronomical surveys\nwill likely enhance the value of strong gravitational lensing as a cosmic probe\nof dark energy and dark matter. However, leveraging the increased statistical\npower of such large samples will require further development of automated lens\nmodeling techniques. We show that deep learning and simulation-based inference\n(SBI) methods produce informative and reliable estimates of parameter\nposteriors for strong lensing systems in ground-based surveys. We present the\nexamination and comparison of two approaches to lens parameter estimation for\nstrong galaxy-galaxy lenses -- Neural Posterior Estimation (NPE) and Bayesian\nNeural Networks (BNNs). We perform inference on 1-, 5-, and 12-parameter lens\nmodels for ground-based imaging data that mimics the Dark Energy Survey (DES).\nWe find that NPE outperforms BNNs, producing posterior distributions that are\nmore accurate, precise, and well-calibrated for most parameters. For the\n12-parameter NPE model, the calibration is consistently within $<$10\\% of\noptimal calibration for all parameters, while the BNN is rarely within 20\\% of\noptimal calibration for any of the parameters. Similarly, residuals for most of\nthe parameters are smaller (by up to an order of magnitude) with the NPE model\nthan the BNN model. This work takes important steps in the systematic\ncomparison of methods for different levels of model complexity.",
        "We present a simple elementary recursive representation of the so called\nFaulhaber series $\\sum_{k=1}^n k^N$ for integer $n$ and $N$, without reference\nto Bernoulli numbers or polynomials.",
        "Particle Therapy (PT) has emerged as a powerful tool in cancer treatment,\nleveraging the unique dose distribution of charged particles to deliver high\nradiation levels to the tumor while minimizing damage to surrounding healthy\ntissue. Despite its advantages, further improvements in Treatment Planning\nSystems (TPS) are needed to address uncertainties related to fragmentation\nprocess, which can affect both dose deposition and effectiveness. These\nfragmentation effects also play a critical role in Radiation Protection in\nSpace, where astronauts are exposed to high level of radiation, necessitating\nprecise models for shielding optimization. The FOOT (FragmentatiOn Of Target)\nexperiment addresses these challenges by measuring fragmentation cross-section\nwith high precision, providing essential data for improving TPS for PT and\nspace radiation protection strategies. This thesis contributes to the FOOT\nexperiment in two key areas. First, it focuses on the performances of the\nvertex detector, which is responsible for reconstructing particle tracks and\nfragmentation vertexes with high spatial resolution. The study evaluates the\ndetector's reconstruction algorithm and its efficiency to detect particles.\nSecond the thesis present a preliminary calculation of fragmentation cross\nsection, incorporating the vertex detector for the first time in these\nmeasurements.",
        "While cloud-cloud collisions (CCCs) have been proposed as a mechanism for\ntriggering massive star formation, it is suggested that higher collision\nvelocities ($v_{\\rm col}$) and lower GMC mass ($M_{\\rm GMC}$) or\/and density\n($\\Sigma_{\\rm GMC}$) tend to suppress star formation. In this study, we choose\nthe nearby barred galaxy NGC 3627 to examine the SFR and SFE of a colliding GMC\n($m^\\star_{\\rm CCC}$ and $\\epsilon_{\\rm CCC}$) and explore the connections\nbetween $m^\\star_{\\rm CCC}$ and $\\epsilon_{\\rm CCC}$, $M_{\\rm\nGMC}$($\\Sigma_{\\rm GMC}$) and $v_{\\rm col}$, and galactic structures (disk,\nbar, and bar-end). Using ALMA CO(2--1) data (60~pc resolution), we estimated\n$v_{\\rm col}$ within 500~pc apertures, based on line-of-sight GMC velocities,\nassuming random motion in a two-dimensional plane. We extracted apertures where\nat least 0.1 collisions occur per 1 Myr, identifying them as regions dominated\nby CCC-driven star formation, and then calculated $m^\\star_{\\rm CCC}$ and\n$\\epsilon_{\\rm CCC}$ using attenuation-corrected H$\\alpha$ data from VLT MUSE.\nWe found that both $m^\\star_{\\rm CCC}$ and $\\epsilon_{\\rm CCC}$ are lower in\nthe bar (median values: $10^{3.84}~M_\\odot$ and $0.18~\\%$), and higher in the\nbar-end ($10^{4.89}~M_\\odot$ and $1.10~\\%$) compared to the disk\n($10^{4.28}~M_\\odot$ and $0.75~\\%$). Furthermore, we found that structural\ndifferences within the parameter space of $v_{\\rm col}$ and $M_{\\rm\nGMC}$($\\Sigma_{\\rm GMC}$), with higher $M_{\\rm GMC}$($\\Sigma_{\\rm GMC}$) in the\nbar-end and higher $v_{\\rm col}$ in the bar compared to the disk, lead to\nhigher star formation activity in the bar-end and lower activity in the bar.\nOur results support the scenario that variations in CCC properties across\ndifferent galactic structures can explain the observed differences in SFE on a\nkpc scale within a disk galaxy.",
        "Simulations on self-propelling active cubes reveal interesting behaviors at\nboth the individual and the collective level, emphasizing the importance of\ndeveloping experimental analogs that allow to test these theoretical\npredictions. The majority of experimental realizations of active colloidal\ncubes rely on light actuation and or magnetic fields to have a persistent\nactive mechanism, and lack material versatility. Here we propose a system of\nactive bimetallic cubes whose propulsion mechanism is based on a catalytic\nreaction and study their behavior. We realize such a system from synthetic\nsilica cuboids and 3D printed micro cubes, followed by the deposition of gold\nand platinum layers on their surface. We characterize the colloids dynamics for\ndifferent thicknesses of the gold layer at low and high hydrogen peroxide\nconcentrations. We show that the thickness of the base gold layer has only a\nminor effect on the self propulsion speed and in addition induces a\ngravitational torque which leads to particles with a velocity director pointing\nout of the plane thus effectively suppressing propulsion. We find that a higher\nactive force can remedy the effects of torque, resulting in particle\norientations that are favorable for in plane propulsion. Finally, we use 3D\nprinting to compare our results to cubes made from a different material, size\nand roundness, and demonstrate that the speed scaling with increasing particle\nsize originates from the size-dependent drag. Our experiments extend\nfabrication of active cubes to different materials and propulsion mechanisms\nand highlight that the design of active particles with anisotropic shapes\nrequires consideration of the interplay between the shape and activity to\nachieve favorable sedimentation and efficient in plane propulsion.",
        "The unprecedented advancement of artificial intelligence has placed immense\ndemands on computing hardware, but traditional silicon-based semiconductor\ntechnologies are approaching their physical and economic limit, prompting the\nexploration of novel computing paradigms. Memristor offers a promising\nsolution, enabling in-memory analog computation and massive parallelism, which\nleads to low latency and power consumption. This manuscript reviews the current\nstatus of memristor-based machine learning accelerators, highlighting the\nmilestones achieved in developing prototype chips, that not only accelerate\nneural networks inference but also tackle other machine learning tasks. More\nimportantly, it discusses our opinion on current key challenges that remain in\nthis field, such as device variation, the need for efficient peripheral\ncircuitry, and systematic co-design and optimization. We also share our\nperspective on potential future directions, some of which address existing\nchallenges while others explore untouched territories. By addressing these\nchallenges through interdisciplinary efforts spanning device engineering,\ncircuit design, and systems architecture, memristor-based accelerators could\nsignificantly advance the capabilities of AI hardware, particularly for edge\napplications where power efficiency is paramount.",
        "Spreading processes on top of active dynamics provide a novel theoretical\nframework for capturing emerging collective behavior in living systems. I\nconsider run-and-tumble dynamics coupled with coagulation\/decoagulation\nreactions that lead to an absorbing state phase transition. While the active\ndynamics does not change the location of the transition point, the relaxation\ntoward the stationary state depends on motility parameters. Because of the\ncompetition between spreading dynamics and active motion, the system can\nsupport long-living currents whose typical time scale is a nontrivial function\nof motility and reaction rates. Beyond the mean-field regime, instability at\nfinite length scales regulates a crossover from periodic to diffusive modes.\nFinally, it is possible to individuate different mechanisms of pattern\nformation on a large time scale, ranging from Fisher-Kolmogorov to\nKardar-Parisi-Zhang equation.",
        "The Dark Matter (DM) distribution in dwarf galaxies provides crucial insights\ninto both structure formation and the particle nature of DM. GraphNPE (Graph\nNeural Posterior Estimator), first introduced in Nguyen et al. (2023), is a\nnovel simulation-based inference framework that combines graph neural networks\nand normalizing flows to infer the DM density profile from line-of-sight\nstellar velocities. Here, we apply GraphNPE to satellite dwarf galaxies in the\nFIRE-2 Latte simulation suite of Milky Way-mass halos, testing it against both\nCold and Self-Interacting DM scenarios. Our method demonstrates superior\nprecision compared to conventional Jeans-based approaches, recovering DM\ndensity profiles to within the 95% confidence level even in systems with as few\nas 30 tracers. Moreover, we present the first evaluation of mass modeling\nmethods in constraining two key parameters from realistic simulations: the peak\ncircular velocity, $V_\\mathrm{max}$, and the peak virial mass,\n$M_\\mathrm{200m}^\\mathrm{peak}$. Using only line-of-sight velocities, GraphNPE\ncan reliably recover both $V_\\mathrm{max}$ and $M_\\mathrm{200m}^\\mathrm{peak}$\nwithin our quoted uncertainties, including those experiencing tidal effects\n($\\gtrsim$ 63% of systems are recovered with our 68% confidence intervals and\n$\\gtrsim$ 92% within our 95% confidence intervals). The method achieves 10-20%\naccuracy in $V_\\mathrm{max}$ recovery, while $M_\\mathrm{200m}^\\mathrm{peak}$ is\nrecovered to 0.1-0.4 dex accuracy. This work establishes GraphNPE as a robust\ntool for inferring DM density profiles in dwarf galaxies, offering promising\navenues for constraining DM models. The framework's potential extends beyond\nthis study, as it can be adapted to non-spherical and disequilibrium models,\nshowcasing the broader utility of simulation-based inference and graph-based\nlearning in astrophysics.",
        "We present a 3-D morphological and field reconstruction of a coronal mass\nejection (CME) from 2023 November 28, which hits three spacecraft near 1 au:\nWind at Earth's L1 Lagrange point; STEREO-A with a longitudinal separation of\n$6.5^{\\circ}$ west of Earth; and Solar Orbiter (SolO) at $10.7^{\\circ}$ east of\nEarth. The reconstruction assumes a magnetic flux rope (MFR) structure for the\nCME. With this event, we test whether field tracings observed by a spacecraft\npassing near the central axis of a CME MFR (STEREO-A) can be used to\nsuccessfullly predict the field behavior seen by a spacecraft $17^{\\circ}$ away\n(SolO), which has a more grazing encounter with the CME. We find that the MFR\nmodel does have significant success in simultaneously reproducing the field\nsigns and rotations seen at STEREO-A, Wind, and SolO. This provides support for\nthe MFR paradigm for CME structure. However, the SolO measurements, which are\nfarthest from the central axis of the MFR, show less defined MFR signatures,\npresumably due to a greater degree of erosion and degradation of the MFR\nstructure far from its central axis.",
        "In this paper, we consider nonconvex optimization problems with nonlinear\nequality constraints. We assume that the objective function and the functional\nconstraints are locally smooth. To solve this problem, we introduce a\nlinearized $\\ell_q$ penalty based method, where $q \\in (1,2]$ is the parameter\ndefining the norm used in the construction of the penalty function. Our method\ninvolves linearizing the objective function and functional constraints in a\nGauss-Newton fashion at the current iteration in the penalty formulation and\nintroduces a quadratic regularization. This approach yields an easily solvable\nsubproblem, whose solution becomes the next iterate. By using a novel dynamic\nrule for the choice of the regularization parameter, we establish that the\niterates of our method converge to an $\\epsilon$-first-order solution in\n$\\mathcal{O}(1\/{\\epsilon^{2+ (q-1)\/q}})$ outer iterations. Finally, we put\ntheory into practice and evaluate the performance of the proposed algorithm by\nmaking numerical comparisons with existing methods from literature.",
        "We study some explicit Siegel modular forms from Weil representations. For\nthe classical theta group $\\Gamma_m(1,2)$ with $m > 1$, there are some eighth\nroots of unity associated with these modular forms, as noted in the works of\nAndrianov, Friedberg, Maloletkin, Stark, Styer, Richter, and others. We apply\n$2$-cocycles introduced by Rao, Kudla, Perrin, Lion-Vergne, Satake-Takase to\ninvestigate these unities. We extend our study to the full Siegel group\n$\\operatorname{Sp}_{2m}(\\mathbb{Z})$ and obtain two matrix-valued Siegel\nmodular forms from Weil representations; these forms arise from a\nfinite-dimensional representation\n$\\operatorname{Ind}_{\\widetilde{\\Gamma}'_m(1,2)}^{\\widetilde{\\operatorname{Sp}}'_{2m}(\\mathbb{Z})}\n(1_{\\Gamma_m(1,2)} \\cdot \\operatorname{Id}_{\\mu_8})^{-1}$, which is related to\nIgusa's quotient group\n$\\tfrac{\\operatorname{Sp}_{2m}(\\mathbb{Z})}{\\Gamma_m(4,8)}$.",
        "The Quantum Signature Validation Algorithm (QSVA) is introduced as a novel\nquantum-based approach designed to enhance the detection of tampered\ntransactions in blockchain systems. Leveraging the powerful capabilities of\nquantum computing, especially within the framework of transaction-based\nblockchains, the QSVA aims to surpass classical methods in both speed and\nefficiency. By utilizing a quantum walk approach integrated with PageRank-based\nsearch algorithms, QSVA provides a robust mechanism for identifying fraudulent\ntransactions. Our adaptation of the transaction graph representation\nefficiently verifies transactions by maintaining a current set of unspent\ntransaction outputs (UTXOs) characteristic of models like Bitcoin. The QSVA not\nonly amplifies detection efficacy through a quadratic speedup but also\nincorporates two competing quantum search algorithms$-$Quantum SearchRank and\nRandomized SearchRank$-$to explore their effectiveness as foundational\ncomponents. Our results indicate that Randomized SearchRank, in particular,\noutperforms its counterpart in aligning with transaction rankings based on the\nClassical PageRank algorithm, ensuring more consistent detection probabilities.\nThese findings highlight the potential for quantum algorithms to revolutionize\nblockchain security by improving detection times to $O(\\sqrt{N})$. Progress in\nDistributed Ledger Technologies (DLTs) could facilitate future integration of\nquantum solutions into more general distributed systems. As quantum technology\ncontinues to evolve, the QSVA stands as a promising strategy offering\nsignificant advancements in blockchain efficiency and security.",
        "The standard gas mixture for the Resistive Plate Chambers (RPC), composed of\nC2H2F4\/i-C4H10\/SF6, allows the detector operation in avalanche mode, as\nrequired by the high-luminosity collider experiments. The gas density, the low\ncurrent and the comfortable avalanche-streamer separation guarantee high\ndetection efficiency, rate capability and slow detector ageing. The mixture has\na high Global Warming Potential (GWP ~1430), primarily due to the presence of\nC2H2F4. The C2H2F4 and SF6 are not recommended for industrial uses anymore,\nthus their availability will be increasingly difficult over time and the search\nfor an alternative gas mixture is then of absolute priority. CERN is also\ndriving efforts to reduce these gases, as they contribute significantly to the\nLHC greenhouse gas emissions. The thin 1 mm gas gap foreseen for the ATLAS\nupgrade of the latter requires a high-density in order to achieve high\nefficiency, due to the less active target available for the primary ionization.\nThe mixture should also guarantee good timing performance and ensure the\ndetector longevity. In this paper, the results obtained on a RPC operated with\nalternative gas mixtures are shown, following two different approaches. The\nfirst study consists of the replacement of the C2H2F4 with a mixture of\nC3H2F4\/CO2 (GWP~200). The second approach consists in adding a modest fraction\nof CO2 in the standard gas, with the aim to reduce the C2H2F4 emissions. The\npaper provides a detailed study of efficiency, time resolution, and current\nunder different irradiation backgrounds.",
        "We present a recurrent neural network-based approach for ground state\npreparation utilizing mid-circuit measurement and feedback. Unlike previous\nmethods that use machine learning solely as an optimizer, our approach\ndynamically adjusts quantum circuits based on real-time measurement outcomes\nand learns distinct preparation protocols for different Hamiltonians. Notably,\nour machine learning algorithm consistently identifies a state preparation\nstrategy wherein all initial states are first steered toward an intermediate\nstate before transitioning to the target ground state. We demonstrate that\nperformance systematically improves as a larger fraction of ancilla qubits are\nutilized for measurement and feedback, highlighting the efficacy of mid-circuit\nmeasurements in state preparation tasks.",
        "In literature, several algorithms for imaging based on interpolation or\napproximation methods are available. The implementation of theoretical\nprocesses highlighted the necessity of providing theoretical frameworks for the\nconvergence and error estimate analysis to support the experimental setups. In\nthis paper, we establish new techniques for deriving quantitative estimates for\nthe order of approximation for multivariate linear operators of the\npointwise-type, with respect to the $L^p$-norm and to the so-called\ndissimilarity index defined through the continuous SSIM. In particular, we\nconsider a family of approximation operators known as neural network (NN)\noperators, that have been widely studied in the last years in view of their\nconnection with the theory of artificial neural networks. For these operators,\nwe first establish sharp estimates in case of $C^1$ and piecewise (everywhere\ndefined) $C^1$-functions. Then, the case of functions modeling digital images\nis considered, and specific quantitative estimates are achieved, including\nthose with respect to the mentioned dissimilarity index. Moreover, the above\nanalysis has also been extended to $L^p$-spaces, using a new constructive\ntechnique, in which the multivariate averaged modulus of smoothness has been\nemployed. Finally, numerical experiments of image resizing have been given to\nsupport the theoretical results. The accuracy of the proposed algorithm has\nbeen evaluated through similarity indexes such as SSIM, likelihood index\n(S-index) and PSNR, and compared with other rescaling methods, including\nbilinear, bicubic, and upscaling-de la Vall\\'ee-Poussin interpolation (u-VPI).\nNumerical simulations show the effectiveness of the proposed method for image\nprocessing tasks, particularly in terms of the aforementioned SSIM, and are\nconsistent with the provided theoretical analysis.",
        "In this paper, we address hypothesis testing in a distributed network of\nnodes, where each node has only partial information about the State of the\nWorld (SotW) and is tasked with determining which hypothesis, among a given\nset, is most supported by the data available within the node. However, due to\neach node's limited perspective of the SotW, individual nodes cannot reliably\ndetermine the most supported hypothesis independently. To overcome this\nlimitation, nodes must exchange information via an intermediate server. Our\nobjective is to introduce a novel distributed lossy semantic communication\nframework designed to minimize each node's uncertainty about the SotW while\noperating under limited communication budget. In each communication round,\nnodes determine the most content-informative message to send to the server. The\nserver aggregates incoming messages from all nodes, updates its view of the\nSotW, and transmits back the most semantically informative message. We\ndemonstrate that transmitting semantically most informative messages enables\nconvergence toward the true distribution over the state space, improving\ndeductive reasoning performance under communication constraints. For\nexperimental evaluation, we construct a dataset designed for logical deduction\nof hypotheses and compare our approach against random message selection.\nResults validate the effectiveness of our semantic communication framework,\nshowing significant improvements in nodes' understanding of the SotW for\nhypothesis testing, with reduced communication overhead.",
        "This paper is devoted to the study of stochastic optimization problems under\nthe generalized smoothness assumption. By considering the unbiased gradient\noracle in Stochastic Gradient Descent, we provide strategies to achieve the\ndesired accuracy with linear rate. Moreover, in the case of the strong growth\ncondition for smoothness $\\left(L_0 = 0\\right)$, we obtain in the convex setup\nthe iteration complexity: $N = \\mathcal{O}\\left(L_1R \\log\\frac{1}{\\varepsilon}\n+ \\frac{L_1 c R^2}{\\varepsilon}\\right)$ for Clipped Stochastic Gradient Descent\nand $N = \\mathcal{O}\\left(L_1R \\log\\frac{1}{\\varepsilon}\\right)$ for Normalized\nStochastic Gradient Descent. Furthermore, we generalize the convergence results\nto the case with a biased gradient oracle, and show that the power of\n$(L_0,L_1)$-smoothness extends to zero-order algorithms. Finally, we validate\nour theoretical results with a numerical experiment, which has aroused some\ninterest in the machine learning community.",
        "As automated trading gains traction in the financial market, algorithmic\ninvestment strategies are increasingly prominent. While Large Language Models\n(LLMs) and Agent-based models exhibit promising potential in real-time market\nanalysis and trading decisions, they still experience a significant -20% loss\nwhen confronted with rapid declines or frequent fluctuations, impeding their\npractical application. Hence, there is an imperative to explore a more robust\nand resilient framework. This paper introduces an innovative multi-agent\nsystem, HedgeAgents, aimed at bolstering system robustness via ``hedging''\nstrategies. In this well-balanced system, an array of hedging agents has been\ntailored, where HedgeAgents consist of a central fund manager and multiple\nhedging experts specializing in various financial asset classes. These agents\nleverage LLMs' cognitive capabilities to make decisions and coordinate through\nthree types of conferences. Benefiting from the powerful understanding of LLMs,\nour HedgeAgents attained a 70% annualized return and a 400% total return over a\nperiod of 3 years. Moreover, we have observed with delight that HedgeAgents can\neven formulate investment experience comparable to those of human experts\n(https:\/\/hedgeagents.github.io\/)."
      ]
    }
  },
  {
    "id":2411.03389,
    "research_type":"applied",
    "start_id":"b9",
    "start_title":"Attention Is All You Need",
    "start_abstract":"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. best performing also connect the encoder and decoder through attention mechanism. We propose a new simple network architecture, Transformer, solely mechanisms, dispensing with recurrence convolutions entirely. Experiments two machine translation tasks show these to be superior quality while being more parallelizable requiring significantly less time train. Our model achieves 28.4 BLEU WMT 2014 English-to-German task, improving over existing results, including ensembles by 2 BLEU. On English-to-French our establishes single-model state-of-the-art score of 41.8 after training for 3.5 days eight GPUs, small fraction costs from literature. that Transformer generalizes well other applying it successfully English constituency parsing both large limited data.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "Continuous-energy Monte Carlo neutron transport on GPUs in the Shift code"
      ],
      "abstract":[
        "A continuous-energy Monte Carlo neutron transport solver executing on GPUs has been developed within the Shift code. Several algorithmic approaches are considered, including both history-based and event-based implementations. Unlike in previous work involving multigroup Monte Carlo transport, it is demonstrated that event-based algorithms significantly outperform a history-based approach for continuous-energy transport as a result of increased device occupancy and reduced thread divergence. Numerical results are presented for detailed full-core models of a small modular reactor (SMR), including a model containing depleted fuel materials. These results demonstrate the substantial gains in performance that are possible with the latest-generation of GPUs. On the depleted SMR core configuration, an NVIDIA P100 GPU with 56 streaming multiprocessors provides performance equivalent to 90 CPU cores, and the latest V100 GPU with 80 multiprocessors offers the performance of more than 150 CPU cores."
      ],
      "categories":[
        "astro-ph.HE"
      ]
    },
    "list":{
      "title":[
        "Investigating Evolving Wormholes in $f(R,T)$ Gravity",
        "Bounded-Confidence Models of Multi-Dimensional Opinions with\n  Topic-Weighted Discordance",
        "Separation of the initial conditions in the inverse problem for 1D\n  non-linear tsunami wave run-up theory",
        "Energy Dispersion, Superconductivity and Magnetic Fluctuations in\n  Stacked Altermagnetism Materials",
        "SpecPT (Spectroscopy Pre-trained Transformer) Model for Extragalactic\n  Spectroscopy: I. Architecture and Automated Redshift Measurement",
        "Time-resolved second-order autocorrelation function of parametric\n  downconversion",
        "FinRLlama: A Solution to LLM-Engineered Signals Challenge at FinRL\n  Contest 2024",
        "Different physical and numerical sources of scatter in the\n  $M_{\\star}$-$M_{\\mathrm{BH}}$ relation and their connection to galaxy\n  evolution",
        "Membrane Charge Effects on Solute Transport in Polyamide Membranes",
        "Cycle Patterns and Mean Payoff Games",
        "Machine Learning-Driven Analytical Models for Threshold Displacement\n  Energy Prediction in Materials",
        "Non-Variational Quantum Random Access Optimization with Alternating\n  Operator Ansatz",
        "Rhizaform algebras",
        "Conical Targets for Enhanced High-Current Positron Sources",
        "Electroweak diboson production in association with a high-mass dijet\n  system in semileptonic final states from $pp$ collisions at $\\sqrt{s} = 13$\n  TeV with the ATLAS detector",
        "Probing the Limit of Heat Transfer in Inorganic Crystals with Deep\n  Learning",
        "A universal preprocessing algorithm of average kernel method with\n  Gauss-Laguerre quadrature for double integrals",
        "P-Order: A Unified Convergence-Analysis Framework for Multivariate\n  Iterative Methods",
        "Lepton flavor violating decays $l_j\\rightarrow l_i\\gamma$, $l_j\n  \\rightarrow 3l_i$ and $\\mu\\rightarrow e+ q\\bar q$ in the N-B-LSSM",
        "The Mass-Angular Momentum Inequality for Multiple Black Holes",
        "FORTE: An Open-Source System for Cost-Effective and Scalable\n  Environmental Monitoring",
        "Visualisation of multi-indication randomised control trial evidence to\n  support decision-making in oncology: a case study on bevacizumab",
        "Weak Error of Dean-Kawasaki Equation with Smooth Mean-Field Interactions",
        "Quantum entanglement of final particle states in the resonant trident\n  pair production in a strong electromagnetic wave",
        "Self-attention-based Diffusion Model for Time-series Imputation in\n  Partial Blackout Scenarios",
        "Structure-preserving and thermodynamically consistent finite element\n  discretization for visco-resistive MHD with thermoelectric effect",
        "Quantitative Flow Approximation Properties of Narrow Neural ODEs",
        "Ceresa Cycles of $X_{0}(N)$",
        "Modeling Cost-Associated Cooperation: A Dilemma of Species Interaction\n  Unveiling New Aspects of Fear Effect"
      ],
      "abstract":[
        "The present work examines whether evolving wormhole solution is possible or\nnot in $f(R,T)$ modified gravity theory. In the background of inhomogeneous\nFLRW type wormhole configuration the field equations are investigated for\ndifferent choices of scale factors and shape functions. For the power law and\nexponential choice of the scale factor from cosmological context and decoupled\npower law of $f(R,T)$ in each variable, wormhole configuration has been\nexamined for two viable choices of shape function. Energy conditions are\nexamined graphically for a range of values of the parameters involved. Finally,\nthe possibility of emergent scenario at early cosmic evolution has been\nexamined.",
        "People's opinions on a wide range of topics often evolve over time through\ntheir interactions with others. Models of opinion dynamics primarily focus on\none-dimensional opinions which represent opinions on one topic. However,\nopinions on various topics are rarely isolated; instead, they can be\ninterdependent and exhibit correlations. In a bounded-confidence model (BCM) of\nopinion dynamics, agents influence each other's opinions only if their opinions\nare sufficiently similar. We extend classical agent-based BCMs -- namely, the\nHegeselmann--Krause BCM, which has synchronous interactions, and the\nDeffuant--Weisbuch BCM, which has asynchronous interactions -- to a\nmultidimensional setting, in which the opinions are multidimensional vectors\nrepresenting opinions of different topics and opinions on different topics are\ninterdependent. To measure opinion differences between agents, we introduce\ntopic-weighted discordance functions that account for opinion differences in\nall topics. We use the regions of receptiveness to characterize the\nsteady-state opinion clusters and provide an analytical approach to compute\nthese regions. In addition, we numerically simulate our models on various\nnetworks with initial opinions drawn from a variety of distributions. When\ninitial opinions are correlated across different topics, our topic-weighted\nBCMs yield significantly different results in both transient and steady states\ncompared to baseline models, where the dynamics of each opinion topic are\nindependent.",
        "We investigate the inverse tsunami wave problem within the framework of the\n1D nonlinear shallow water equations (SWE). Specifically, we focus on\ndetermining the initial displacement $\\eta_0(x)$ and velocity $u_0(x)$ of the\nwave, given the known motion of the shoreline $R(t)$ (the wet\/dry free\nboundary). We demonstrate that for power-shaped inclined bathymetries, this\nproblem admits a complete solution for any $\\eta_0$ and $u_0$, provided the\nwave does not break. In particular, we show that the knowledge of $R(t)$\nenables the unique recovery of both $\\eta_0(x$) and $u_0(x)$ in terms of the\nAbel transform.\n  It is important to note that, in contrast to the direct problem (also known\nas the tsunami wave run-up problem), where $R(t)$ can be computed exactly only\nfor $u_0(x)=0$, our algorithm can recover $\\eta_0$ and $u_0$ exactly for any\nnon-zero $u_0$. This highlights an interesting asymmetry between the direct and\ninverse problems. Our results extend the work presented in\n\\cite{Rybkin23,Rybkin24}, where the inverse problem was solved for $u_0(x)=0$.\nAs in previous work, our approach utilizes the Carrier-Greenspan\ntransformation, which linearizes the SWE for inclined bathymetries. Extensive\nnumerical experiments confirm the efficiency of our algorithms.",
        "Recently, altermagnetism (AM) has emerged as a new category of magnetism,\nalongside conventional antiferromagnetism (AFM) and ferromagnetism (FM). In an\nAM, superconductivity (SC) is faced with a dilemma that the spin-polarized\nbands, induced by the broken time reversal (T ) symmetry, dominantly supports\nspin-triplet pairing. In contrast, AM spin fluctuations routinely facilitate\nspin-singlet pairing as in AFM. Consequently, unconventional SC is either\nabsent or weak in AM materials. Here, we propose that stacking 2D AM materials\ncould resolve this dilemma. Stacked 2D materials have yielded a variety of new\nelectronic properties by altering the symmetries inherent in the monolayer. In\na 2D anisotropic Hubbard model, we investigate the general energy dispersions\nof both single-layer and stacked AM materials. We demonstrate that AM sheet\nstacking can alter the original symmetries, consequently affecting the energy\ndispersion. The interlayer magnetic coupling enhances the low q magnetic\nfluctuations. T symmetry is restored in the AA stacking with an\nantiferromagnetic interlayer coupling, and then both the energy dispersion and\npairing interaction are in favor of spin-singlet SC. The ferromagnetic\ninterlayer coupling in the AB stacking not only recovers T symmetry but also\nsupports spin-triplet pairing. It is further anticipated that twisted bilayer\nAM sheets could exhibit additional novel electronic properties, including\ntopology, flat bands, and collective excitations. Our work illustrates that\nstacking sheets of AM materials could open up a unique research domain in\nexploring novel quantum phenomena and offer a fertile ground for potential\nelectronic applications.",
        "We introduce the Spectroscopy Pre-trained Transformer (SpecPT), a\ntransformer-based model designed to analyze spectroscopic data, with\napplications in spectrum reconstruction and redshift measurement. Using the\nEarly Data Release (EDR) of the DESI survey, we evaluate SpecPT's performance\non two distinct datasets: the Bright Galaxy Survey (BGS) and Emission Line\nGalaxy (ELG) samples. SpecPT successfully reconstructs spectra, accurately\ncapturing emission lines, absorption features, and continuum shapes while\neffectively reducing noise. For redshift prediction, SpecPT achieves\ncompetitive accuracy, with Normalized Median Absolute Deviation (NMAD) values\nof 0.0006 and 0.0008, and catastrophic outlier fractions of 0.20% and 0.80% for\nBGS and ELG, respectively. Notably, SpecPT performs consistently well across\nthe full redshift range ($0 < z < 1.6$), demonstrating its versatility and\nrobustness. By leveraging its learned latent representations, SpecPT lays the\ngroundwork for a foundational spectroscopic model, with potential applications\nin outlier detection, interstellar medium (ISM) property estimation, and\ntransfer learning to other datasets. This work represents a first step in\nbuilding a generalized framework for spectroscopic analysis, capable of scaling\nto the full DESI dataset and beyond.",
        "We study a possibility of measuring the time-resolved second-order\nautocorrelation function of one of two beams generated in type-II parametric\ndownconversion by means of temporal magnification of this beam, bringing its\ncorrelation time from the picosecond to the nanosecond scale, which can be\nresolved by modern photodetectors. We show that such a measurement enables one\nto infer directly the degree of global coherence of that beam, which is linked\nby a simple relation to the number of modes characterizing the entanglement\nbetween the two generated beams. We illustrate the proposed method by an\nexample of photon pairs generated in a periodically poled KTP crystal with a\nsymmetric group velocity matching for various durations of the pump pulse,\nresulting in different numbers of modes. Our theoretical model also shows that\nthe magnified double-heralded autocorrelation function of one beam exhibits a\nlocal maximum around zero delay time, corresponding to photon bunching at a\nshort time scale.",
        "In response to Task II of the FinRL Challenge at ACM ICAIF 2024, this study\nproposes a novel prompt framework for fine-tuning large language models (LLM)\nwith Reinforcement Learning from Market Feedback (RLMF). Our framework\nincorporates market-specific features and short-term price dynamics to generate\nmore precise trading signals. Traditional LLMs, while competent in sentiment\nanalysis, lack contextual alignment for financial market applications. To\nbridge this gap, we fine-tune the LLaMA-3.2-3B-Instruct model using a custom\nRLMF prompt design that integrates historical market data and reward-based\nfeedback. Our evaluation shows that this RLMF-tuned framework outperforms\nbaseline methods in signal consistency and achieving tighter trading outcomes;\nawarded as winner of Task II. You can find the code for this project on GitHub.",
        "Observations have established that the masses of supermassive black holes\n(SMBHs) correlate tightly with the stellar masses of their host galaxies,\nalbeit with substantial scatter. The size of this scatter as a function of\ngalaxy mass and redshift contains valuable information about the origin of\nSMBHs and the physical nature of their co-evolution with galaxies. In this\nwork, we highlight this connection by studying the scatter in the\n$M_{\\mathrm{BH}} - M_{\\star}$ relation for massive galaxies in the Illustris,\nIllustrisTNG (TNG), and EAGLE cosmological simulations. We find that the\nscatter in TNG is significantly lower than in Illustris and EAGLE, reflecting\ntheir different BH feedback models. By performing various numerical\nexperiments, we quantify different contributions to the scatter in the\nsimulations, and also identify a suitably defined intrinsic scatter. The\nintrinsic scatter in Illustris and EAGLE is $\\sim0.3$ dex at $z=0$, and is\ndominated by variations from BH accretion, whereas the smaller scatter of TNG\nis rather dominated by hierarchical merging, suggesting that the massive\ngalaxies in TNG are more tightly quenched. Variations in the BH seed mass can\ncontribute to the scatter of the $M_{\\rm BH}-M_{\\star}$ relation as well, but\nwhether this still plays a role at $z=0$ depends on the feedback model.\nSimulations with disabled AGN feedback produce much higher scatter for low-mass\ngalaxies than seen in our cosmological simulations, demonstrating the crucial\ninfluence of feedback for determining the co-evolution of SMBHs and their host\ngalaxies in this regime. In contrast, an important factor in reducing the\nscatter for massive galaxies is hierarchical merging of mostly quenched\nsystems. Based on our results, we expect that the scatter in the\n$M_{\\mathrm{BH}} - M_{\\star}$ relation at high redshift could be particularly\npowerful in providing clues to the origin of SMBHs.",
        "Polyamide membranes, such as nanofiltration (NF) and reverse osmosis (RO)\nmembranes, are widely used for water desalination and purification. However,\nthe mechanisms of solute transport and solute rejection due to charge\ninteractions remain unclear at the molecular level. Here we use molecular\ndynamics (MD) simulations to examine the transport of single-solute feeds\nthrough charged nanofiltration membranes with different membrane charge\nconcentrations of COO$^{\\text{-}}$ and NH$_2\\!^+$ corresponding to different pH\nlevels. Results show that Na$^+$ and Cl$^{\\text{-}}$ solute ions are better\nrejected when the membrane has a higher concentration of negatively charged\ngroups, corresponding to a higher pH, whereas CaCl$_2$ is well-rejected at all\npH levels studied. These results are consistent with experimental findings\nwhich are performed at the same pH conditions as simulation setup. Moreover,\nsolute transport behavior depends on the membrane functional group\ndistribution. When COO$^{\\text{-}}$ functional groups are concentrated at\nmembrane feed surface, ion permeation into the membrane is reduced.\nCounter-ions tend to associate with charged functional groups while co-ions\nseem to pass by the charged groups more easily. In addition, steric effects\nplay a role when ions of opposite charge cluster in pores of the membrane. This\nstudy reveals solute transport and rejection mechanisms related to membrane\ncharge and provides insights into how membranes might be designed to achieve\nspecific desired solute rejection.",
        "We introduce the concept of a \\emph{cycle pattern} for directed graphs as\nfunctions from the set of cycles to the set $\\{-,0,+\\}$. The key example for\nsuch a pattern is derived from a weight function, giving rise to the sign of\nthe total weight of the edges for each cycle. Hence, cycle patterns describe a\nfundamental structure of a weighted digraph, and they arise naturally in games\non graphs, in particular parity games, mean payoff games, and energy games.\n  Our contribution is threefold: we analyze the structure and derive hardness\nresults for the realization of cycle patterns by weight functions. Then we use\nthem to show hardness of solving games given the limited information of a cycle\npattern. Finally, we identify a novel geometric hardness measure for solving\nmean payoff games (MPG) using the framework of linear decision trees, and use\ncycle patterns to derive lower bounds with respect to this measure, for large\nclasses of algorithms for MPGs.",
        "Understanding the behavior of materials under irradiation is crucial for the\ndesign and safety of nuclear reactors, spacecraft, and other radiation\nenvironments. The threshold displacement energy (Ed) is a critical parameter\nfor understanding radiation damage in materials, yet its determination often\nrelies on costly experiments or simulations. This work leverages the machine\nlearning-based Sure Independence Screening and Sparsifying Operator (SISSO)\nmethod to derive accurate, analytical models for predicting Ed using\nfundamental material properties. The models outperform traditional approaches\nfor monoatomic materials, capturing key trends with high accuracy. While\npredictions for polyatomic materials highlight challenges due to dataset\ncomplexity, they reveal opportunities for improvement with expanded data. This\nstudy identifies cohesive energy and melting temperature as key factors\ninfluencing Ed, offering a robust framework for efficient, data-driven\npredictions of radiation damage in diverse materials.",
        "Solving hard optimization problems is one of the most promising application\ndomains for quantum computers due to the ubiquity of such problems in industry\nand the availability of broadly applicable quantum speedups. However, the\nability of near-term quantum computers to tackle industrial-scale optimization\nproblems is limited by their size and the overheads of quantum error\ncorrection. Quantum Random Access Optimization (QRAO) has been proposed to\nreduce the space requirements of quantum optimization. However, to date QRAO\nhas only been implemented using variational algorithms, which suffer from the\nneed to train instance-specific variational parameters, making them difficult\nto scale. We propose and benchmark a non-variational approach to QRAO based on\nthe Quantum Alternating Operator Ansatz (QAOA) for the MaxCut problem. We show\nthat instance-independent ``fixed'' parameters achieve good performance,\nremoving the need for variational parameter optimization. Additionally, we\nevaluate different design choices, such as various mixers and initial states,\nas well as QAOA operator implementations when customizing for QRAO, and\nidentify a strategy that performs well in practice. Our results pave the way\nfor the practical execution of QRAO on early fault-tolerant quantum computers.",
        "Any anti-associative algebra gives rise to a Jacobi-Jordan algebra by [x, y]\n= xy + yx. This article aims to introduce the concept of \"rhizaform algebras\",\nwhich offer an approach to addressing anti-associativity. These algebras are\ndefined by two operations whose sum is anti-associative, with the left and\nright multiplication operators forming bimodules of the sum of anti-associative\nalgebras. This characterization parallels that of dendriform algebras, where\nthe sum of operations preserves associativity. Additionally, the notions of\nO-operators and Rota-Baxter operators on anti-associative algebras are\npresented as tools to interpret rhizaform algebras. Notably, anti-associative\nalgebras with nondegenerate Connes cocycles admit compatible rhizaform algebra\nstructures.",
        "Previous pair-production-driven positron source designs have assumed that the\ntransverse dimension of the target is significantly greater than the secondary\nbeam it generates. This paper explores the use of targets with different\ntransverse profiles with the aim of enhancing positron production. The starting\npoint of this research is the concept of wire targets, proposed by M. James et\nal. in 1991 for the former SLC positron source. Building on this foundation,\nthis study takes this concept a step further by introducing conical-shaped\ntargets, which can substantially improve the yield by reducing the reabsorption\nof positrons by the target--an issue that is worsened by the high-field\nsolenoid lenses commonly used for positron capture. Using Geant4 simulations,\nwe propose new conical targets adapted for the parameters of the future\ncollider FCC-ee and its positron source test facility P-cubed (PSI Positron\nProduction experiment) at the Paul Scherrer Institute. We find that conical\ntargets can nearly double the positron production at the target and enhance the\nbaseline positron yield of FCC-ee by around 60%. Additionally, we present the\nthermo-mechanical studies for the conical targets based on the FCC-ee primary\nbeam power requirements and outline the mechanical implementation for a future\nproof-of-principle demonstration at the P-cubed facility.",
        "This paper reports the observation of electroweak diboson ($WW\/WZ\/ZZ$)\nproduction in association with a high-mass dijet system, in which final states\nwith one boson decaying leptonically and the other boson decaying hadronically\nare studied. The hadronically decaying $W\/Z$ boson is reconstructed as either\ntwo small-radius jets or one large-radius jet with jet substructure\nrequirements. The data analyzed correspond to an integrated luminosity of 140\nfb$^{-1}$ of proton-proton collisions at a center-of-mass energy of\n$\\sqrt{s}=13$ TeV collected with the ATLAS detector during the 2015-2018 data\ntaking at the Large Hadron Collider. The electroweak production of $WW\/WZ\/ZZ$\nin association with two jets is observed in a phase space dominated by\nvector-boson scattering with a significance of $7.4\\sigma$ (expected\n$6.1\\sigma$) and the signal strength is determined to be\n$1.28^{+0.23}_{-0.21}$. The corresponding production cross section in a\nfiducial phase space is measured in addition. The signal strengths of both\nelectroweak and QCD associated diboson productions are furthermore measured in\na two-dimensional fit, the result of which agrees with the Standard Model\nprediction. The data are interpreted in the context of a dimension-8 effective\nfield theory to probe anomalous quartic gauge couplings resulting in the first\nset of exclusion limits on the Wilson coefficients in the semileptonic channel\nreported by the ATLAS Collaboration. The observed limits for the S02, T0 and M0\noperators are $(-3.96 < f_{S02} \/ \\Lambda^4 < 3.96)$ TeV$^{-4}$, $(-0.25 <\nf_{T0} \/ \\Lambda^4 < 0.22)$ TeV$^{-4}$, $(-1.26 < f_{M0} \/ \\Lambda^4 < 1.25)$\nTeV$^{-4}$.",
        "Heat transfer is a fundamental property of matter. Research spanning decades\nhas attempted to discover materials with exceptional thermal conductivity, yet\nthe upper limit remains unknown. Using deep learning accelerated crystal\nstructure prediction and first-principles calculation, we systematically\nexplore the thermal conductivity landscape of inorganic crystals. We\nbrute-force over half a million ordered crystalline structures, encompassing an\nextensive coverage of local energy minima in binary compounds with up to four\natoms per primitive cell. We confirm diamond sets the upper bound of thermal\nconductivity within our search space, very likely also among all stable\ncrystalline solids at ambient conditions. We identify over 20 novel crystals\nwith high thermal conductivity surpassing silicon at room temperature validated\nby density functional theory. These include a series of metallic compounds,\nespecially MnV, exhibiting high lattice and electronic thermal conductivity\nsimultaneously, a distinctive feature not observed before. The fast deep\nlearning-driven screening method, as well as the large comprehensive thermal\nconductivity database, pave the way for the discovery and design of\nnext-generation materials with tailored thermal properties.",
        "To address the computational challenges posed by nonlinear collision kernels\nin the Smoluchowski equation, this study proposes a universal preprocessing\nalgorithm for the average kernel method based on the Gauss-Laguerre quadrature\nfor double integrals. With this algorithm, the numerical code accurately and\nefficiently determines the pre-exponential factor of the average kernel.\nAdditionally, the exact pre-exponential factors of the four fundamental average\nkernels and their associated truncation error estimations were analyzed. The\nresults demonstrate the reasonability and reliability of the preprocessing\nalgorithm.",
        "We propose P-order (Power-order), a unified, norm-independent framework for\nquantifying the convergence rates of iterative methods. Standard analyses based\non Q-order are norm-dependent and require some uniformity of error reductions\nasymptotically. Although the Ortega--Rheinboldt R-order supports non-uniform\n(including non-monotonic sequences) and is norm-independent, it does not\ndifferentiate among various sublinear rates and has ambiguities for some\nsuperlinear rates. In contrast, our proposed framework parameterizes the\nconvergence rate in direct analogy with asymptotic notation in combinatorial\nalgorithm analysis (including little-$o$, big-$\\Theta$, and little-$\\omega$),\nthereby precisely distinguishing linear, sublinear (e.g., fractional-power),\nand superlinear (e.g., linearithmic) regimes. We also introduce two subclasses\nof P-order: Quasi-Uniform P-order (QUP) and Uniform P-order (UP), and show\ntheir relations with Q-order and R-order. We demonstrate the ease-of-use and\nflexibility of QUP-order by analyzing fixed-point iterations and show that\nP-order can provide tighter bounds on the convergence rate than R-order for\nboth sublinearly and superlinearly convergent cases while avoiding some\nambiguities in R-order. Furthermore, we present a refined analysis of Newton's\nmethod for nonlinear equations with a wide spectrum of sublinear and\nsuperlinear convergence rates (including a newly defined anit-liearithmic\nrate).",
        "The N-B-LSSM is an extension of the minimal supersymmetric standard model\n(MSSM) with the addition of three singlet new Higgs superfields and\nright-handed neutrinos, whose local gauge group is $SU(3)_C\\times SU(2)_L\\times\nU(1)_Y\\times U(1)_{B-L}$. In the N-B-LSSM, we study lepton flavor violating\ndecays $l_j\\rightarrow l_i\\gamma$, $l_j \\rightarrow 3l_i$ and $\\mu\\rightarrow\ne+ q\\bar q$ $(j=\\tau,\\mu,~i=\\mu,e$ and $i\\neq j)$. Based on the current\nexperimental limitations, we carry out detailed parameter scanning and\nnumerical calculations to analyse the effects of different sensitive parameters\non lepton flavor violation (LFV) in the N-B-LSSM. The numerical results show\nthat the non-diagonal elements involving the initial and final leptons are main\nsensitive parameters and LFV sources. This work can provide a strong basis for\nexploring new physics (NP) beyond the Standard Model (SM).",
        "This is the second in a series of two papers to establish the conjectured\nmass-angular momentum inequality for multiple black holes, modulo the extreme\nblack hole 'no hair theorem'. More precisely it is shown that either there is a\ncounterexample to black hole uniqueness, in the form of a regular axisymmetric\nstationary vacuum spacetime with an asymptotically flat end and multiple\ndegenerate horizons which is 'ADM minimizing', or the following statement\nholds. Complete, simply connected, maximal initial data sets for the Einstein\nequations with multiple ends that are either asymptotically flat or\nasymptotically cylindrical, admit an ADM mass lower bound given by the square\nroot of total angular momentum, under the assumption of nonnegative energy\ndensity and axisymmetry. Moreover, equality is achieved in the mass lower bound\nonly for a constant time slice of an extreme Kerr spacetime. The proof is based\non a novel flow of singular harmonic maps with hyperbolic plane target, under\nwhich the renormalized harmonic map energy is monotonically nonincreasing.\nRelevant properties of the flow are achieved through a refined asymptotic\nanalysis of solutions to the harmonic map equations and their linearization.",
        "Forests are an essential part of our biosphere, regulating climate, acting as\na sink for greenhouse gases, and providing numerous other ecosystem services.\nHowever, they are negatively impacted by climatic stressors such as drought or\nheat waves. In this paper, we introduce FORTE, an open-source system for\nenvironmental monitoring with the aim of understanding how forests react to\nsuch stressors. It consists of two key components: (1) a wireless sensor\nnetwork (WSN) deployed in the forest for data collection, and (2) a Data\nInfrastructure for data processing, storage, and visualization. The WSN\ncontains a Central Unit capable of transmitting data to the Data Infrastructure\nvia LTE-M and several spatially independent Satellites that collect data over\nlarge areas and transmit them wirelessly to the Central Unit. Our prototype\ndeployments show that our solution is cost-effective compared to commercial\nsolutions, energy-efficient with sensor nodes lasting for several months on a\nsingle charge, and reliable in terms of data quality. FORTE's flexible\narchitecture makes it suitable for a wide range of environmental monitoring\napplications beyond forest monitoring. The contributions of this paper are\nthree-fold. First, we describe the high-level requirements necessary for\ndeveloping an environmental monitoring system. Second, we present an\narchitecture and prototype implementation of the requirements by introducing\nour FORTE platform and demonstrating its effectiveness through multiple field\ntests. Lastly, we provide source code, documentation, and hardware design\nartifacts as part of our open-source repository.",
        "Background: Evidence maps have been used in healthcare to understand existing\nevidence and to support decision-making. In oncology they have been used to\nsummarise evidence within a disease area but have not been used to compare\nevidence across different diseases. As an increasing number of oncology drugs\nare licensed for multiple indications, visualising the accumulation of evidence\nacross all indications can help inform policy-makers, support evidence\nsynthesis approaches, or to guide expert elicitation on appropriate\ncross-indication assumptions. Methods: The multi-indication oncology therapy\nbevacizumab was selected as a case-study. We used visualisation methods\nincluding timeline, ridgeline and split-violin plots to display evidence across\nseven licensed cancer types, focusing on the evolution of evidence on overall\nand progression-free survival over time as well as the quality of the evidence\navailable. Results: Evidence maps for bevacizumab allow for visualisation of\npatterns in study-level evidence, which can be updated as evidence accumulates\nover time. The developed tools display the observed data and synthesised\nevidence across- and within-indications. Limitations: The effectiveness of the\nplots produced are limited by the lack of complete and consistent reporting of\nevidence in trial reports. Trade-offs were necessary when deciding the level of\ndetail that could be shown while keeping the plots coherent. Conclusions: Clear\ngraphical representations of the evolution and accumulation of evidence can\nprovide a better understanding of the entire evidence base which can inform\njudgements regarding the appropriate use of data within and across indications.\nImplications: Improved visualisations of evidence can help the development of\nmulti-indication evidence synthesis. The proposed evidence displays can lead to\nthe efficient use of information for health technology assessment.",
        "We consider the weak-error rate of the SPDE approximation by regularized\nDean-Kawasaki equation with It\\^o noise for particle systems with mean-field\ninteractions both on the drift and the noise. The global existence and\nuniqueness of the corresponding SPDEs are established using the variational\napproach to SPDEs, and the weak-error rate is estimated using the technique of\nKolmogorov equations on the space of probability measures. In particular, the\nrate derived in this paper coincides with that is the previous work\narXiv:2212.11714, which considered free Brownian particles using Laplace\nduality.",
        "The resonant trident pair production process in the collision of\nultrarelativistic electrons with a strong electromagnetic wave is theoretically\nstudied. Under resonant conditions, the intermediate virtual gamma-quantum\nbecomes real. As a result, the original resonant trident pair production\nprocess effectively splits into two first-order processes by the fine structure\nconstant: the electromagnetic field-stimulated Compton-effect and the\nelectromagnetic field-stimulated Breit-Wheeler process. The kinematics of the\nresonant trident pair production process are studied in detail. It is shown\nthat there are two different cases for the energies and outgoing angles of\nfinal particles (an electron and an electron-positron pair) in which their\nquantum entanglement is realized. In the first case, the energy and outgoing\nangles of final ultrarelativistic particles are uniquely determined by the\nparameters of the electromagnetic field-stimulated Compton-effect (the outgoing\nangle of the final electron and the quantum parameter of the Compton effect).\nIn the second case, the energy and outgoing angles of final particles are\nuniquely determined by the electromagnetic field-stimulated Breit-Wheeler\nprocess (the electron-positron pair outgoing angle and the Breit-Wheeler\nquantum parameter). It is shown that in a sufficiently wide range of\nfrequencies and intensities of a strong electromagnetic wave, and in the case\nof ultrarelativistic initial electrons, the differential probability of the\nresonant trident pair production process with simultaneous registration of the\noutgoing angles of final particles can significantly (by several orders of\nmagnitude) exceed the total probability of the electromagnetic field-stimulated\nCompton-effect.",
        "Missing values in multivariate time series data can harm machine learning\nperformance and introduce bias. These gaps arise from sensor malfunctions,\nblackouts, and human error and are typically addressed by data imputation.\nPrevious work has tackled the imputation of missing data in random, complete\nblackouts and forecasting scenarios. The current paper addresses a more general\nmissing pattern, which we call \"partial blackout,\" where a subset of features\nis missing for consecutive time steps. We introduce a two-stage imputation\nprocess using self-attention and diffusion processes to model feature and\ntemporal correlations. Notably, our model effectively handles missing data\nduring training, enhancing adaptability and ensuring reliable imputation and\nperformance, even with incomplete datasets. Our experiments on benchmark and\ntwo real-world time series datasets demonstrate that our model outperforms the\nstate-of-the-art in partial blackout scenarios and shows better scalability.",
        "We present a structure-preserving and thermodynamically consistent numerical\nscheme for classical magnetohydrodynamics, incorporating viscosity, magnetic\nresistivity, heat transfer, and thermoelectric effect. The governing equations\nare shown to be derived from a generalized Hamilton's principle, with the\nresulting weak formulation being mimicked at the discrete level. The resulting\nnumerical method conserves mass and energy, satisfies Gauss' magnetic law and\nmagnetic helicity balance, and adheres to the Second Law of Thermodynamics, all\nat the fully discrete level. It is shown to perform well on magnetic\nRayleigh-B\\'enard convection.",
        "In this note, we revisit the problem of flow approximation properties of\nneural ordinary differential equations (NODEs). The approximation properties\nhave been considered as a flow controllability problem in recent literature.\nThe neural ODE is considered {\\it narrow} when the parameters have dimension\nequal to the input of the neural network, and hence have limited width. We\nderive the relation of narrow NODEs in approximating flows of shallow but wide\nNODEs. Due to existing results on approximation properties of shallow neural\nnetworks, this facilitates understanding which kind of flows of dynamical\nsystems can be approximated using narrow neural ODEs. While approximation\nproperties of narrow NODEs have been established in literature, the proofs\noften involve extensive constructions or require invoking deep controllability\ntheorems from control theory. In this paper, we provide a simpler proof\ntechnique that involves only ideas from ODEs and Gr{\\\"o}nwall's lemma.\nMoreover, we provide an estimate on the number of switches needed for the time\ndependent weights of the narrow NODE to mimic the behavior of a NODE with a\nsingle layer wide neural network as the velocity field.",
        "The Ceresa cycle is an algebraic 1-cycle on the Jacobian of an algebraic\ncurve. Although it is homologically trivial, Ceresa famously proved that for a\nvery general complex curve of genus at least 3, it is non-trivial in the Chow\ngroup. In this paper we study the Ceresa cycle attached to the complete modular\ncurve $X_{0}(N)$ modulo rational equivalence. For prime level $p$ we give a\ncomplete description, namely we prove that if $X_{0}(p)$ is not hyperelliptic,\nthen its Ceresa cycle is non-torsion. For general level $N$, we prove that\nthere are finitely many $X_{0}(N)$ with torsion Ceresa cycle. Our method relies\non the relationship between the vanishing of the Ceresa cycle and Chow-Heegner\npoints on the Jacobian. We use the geometry and arithmetic of modular Jacobians\nto prove that such points are of infinite order and therefore deduce\nnon-vanishing of the Ceresa cycle.",
        "With limited resources, competition is widespread, yet cooperation persists\nacross taxa, from microorganisms to large mammals. Recent observations reveal\ncontingent factors often drive cooperative interactions, with the intensity\nheterogeneously distributed within species. While cooperation has beneficial\noutcomes, it may also incur significant costs, largely depending on species\ndensity. This creates a dilemma that is pivotal in shaping sustainable\ncooperation strategies. Understanding how cooperation intensity governs the\ncost-benefit balance, and whether an optimal strategy exists for species\nsurvival, is a fundamental question in ecological research, and the focus of\nthis study. We develop a novel mathematical model within the Lotka-Volterra\nframework to explore the dynamics of cost-associated partial cooperation, which\nremains relatively unexplored in ODE model-based studies. Our findings\ndemonstrate that partial cooperation benefits ecosystems up to a certain\nintensity, beyond which costs become dominant, leading to system collapse via\nheteroclinic bifurcation. This outcome captures the cost-cooperation dilemma,\nproviding insights for adopting sustainable strategies and resource management\nfor species survival. We propose a novel mathematical approach to detect and\ntrack heteroclinic orbits in predator-prey systems. Moreover, we show that\nintroducing fear of predation can protect the regime shift, even with a type-I\nfunctional response, challenging traditional ecological views. Although fear is\nknown to resolve the \"paradox of enrichment,\" our results suggest that certain\nlevels of partial cooperation can reestablish this dynamic even at higher fear\nintensity. Finally, we validate the system's dynamical robustness across\nfunctional responses through structural sensitivity analysis."
      ]
    }
  },
  {
    "id":2411.03389,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"Continuous-energy Monte Carlo neutron transport on GPUs in the Shift code",
    "start_abstract":"A continuous-energy Monte Carlo neutron transport solver executing on GPUs has been developed within the Shift code. Several algorithmic approaches are considered, including both history-based and event-based implementations. Unlike in previous work involving multigroup Monte Carlo transport, it is demonstrated that event-based algorithms significantly outperform a history-based approach for continuous-energy transport as a result of increased device occupancy and reduced thread divergence. Numerical results are presented for detailed full-core models of a small modular reactor (SMR), including a model containing depleted fuel materials. These results demonstrate the substantial gains in performance that are possible with the latest-generation of GPUs. On the depleted SMR core configuration, an NVIDIA P100 GPU with 56 streaming multiprocessors provides performance equivalent to 90 CPU cores, and the latest V100 GPU with 80 multiprocessors offers the performance of more than 150 CPU cores.",
    "start_categories":[
      "astro-ph.HE"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b9"
      ],
      "title":[
        "Attention Is All You Need"
      ],
      "abstract":[
        "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. best performing also connect the encoder and decoder through attention mechanism. We propose a new simple network architecture, Transformer, solely mechanisms, dispensing with recurrence convolutions entirely. Experiments two machine translation tasks show these to be superior quality while being more parallelizable requiring significantly less time train. Our model achieves 28.4 BLEU WMT 2014 English-to-German task, improving over existing results, including ensembles by 2 BLEU. On English-to-French our establishes single-model state-of-the-art score of 41.8 after training for 3.5 days eight GPUs, small fraction costs from literature. that Transformer generalizes well other applying it successfully English constituency parsing both large limited data."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Types of elements in non-commutative Poisson algebras and Dixmier\n  Conjecture",
        "Deep Learning and Foundation Models for Weather Prediction: A Survey",
        "CacheMamba: Popularity Prediction for Mobile Edge Caching Networks via\n  Selective State Spaces",
        "Solving the Catastrophic Forgetting Problem in Generalized Category\n  Discovery",
        "Motion planning for highly-dynamic unconditioned reflexes based on\n  chained Signed Distance Functions",
        "Diffusion-Based Imitation Learning for Social Pose Generation",
        "Hochschild cohomology and extensions of triangulated categories",
        "Provably-Stable Neural Network-Based Control of Nonlinear Systems",
        "Towards a Digital Twin Modeling Method for Container Terminal Port",
        "Multiport Support for Vortex OpenGPU Memory Hierarchy",
        "Anomize: Better Open Vocabulary Video Anomaly Detection",
        "Piano Transcription by Hierarchical Language Modeling with Pretrained\n  Roll-based Encoders",
        "On the limit of random hives with GUE boundary conditions",
        "Transformer Semantic Genetic Programming for Symbolic Regression",
        "Mutation Testing via Iterative Large Language Model-Driven Scientific\n  Debugging",
        "Improving Transducer-Based Spoken Language Understanding with\n  Self-Conditioned CTC and Knowledge Transfer",
        "Simulation of Random LR Fuzzy Intervals",
        "Puntajes m\\'aximos en el juego de domin\\'o",
        "Enhancing Expressive Voice Conversion with Discrete Pitch-Conditioned\n  Flow Matching Model",
        "Navigating Automated Hiring: Perceptions, Strategy Use, and Outcomes\n  Among Young Job Seekers",
        "Community Notes Moderate Engagement With and Diffusion of False\n  Information Online",
        "On the number of cofinalities of cuts in ultraproducts of linear orders",
        "Scalable Sobolev IPM for Probability Measures on a Graph",
        "HeRCULES: Heterogeneous Radar Dataset in Complex Urban Environment for\n  Multi-session Radar SLAM",
        "Route to Chaos and Unified Dynamical Framework of Multi-Species\n  Ecosystems",
        "\"Playing the robot's advocate\": Bystanders' descriptions of a robot's\n  conduct in public settings",
        "Synergizing Deep Learning and Full-Waveform Inversion: Bridging\n  Data-Driven and Theory-Guided Approaches for Enhanced Seismic Imaging",
        "Commonsense Reasoning-Aided Autonomous Vehicle Systems",
        "Class-Conditional Neural Polarizer: A Lightweight and Effective Backdoor\n  Defense by Purifying Poisoned Features"
      ],
      "abstract":[
        "Non-commutative Poisson algebras are the algebras having an associative\nalgebra structure and a Lie algebra structure together with the Leibniz law.\nLet $P$ be a non-commutative Poisson algebra over some algebraically closed\nfield of characteristic zero. For any $z\\in P$, there exist four subalgebras of\n$P$ associated with the inner derivation $ad_z$ on $P$. Based on the\nrelationships between these four subalgebras, elements of $P$ can be divided\ninto eight types. We will mainly focus on two types of non-commutative Poisson\nalgebras: the usual Poisson algebras and the associative algebras with the\ncommutator as the Poisson bracket. The following problems are studied for such\nnon-commutative Poisson algebras: how the type of an element changes under\nhomomorphisms between non-commutative Poisson algebras, how the type of an\nelement changes after localization, and what the type of the elements of the\nform $z_1 \\otimes z_2$ and $z_1 \\otimes 1 + 1 \\otimes z_2$ is in the tensor\nproduct of non-commutative Poisson algebras $P_1\\otimes P_2$. As an application\nof above results, one knows that Dixmier Conjecture for $A_1$ holds under\ncertain conditions. Some properties of the Weyl algebras are also obtained,\nsuch as the commutativity of certain subalgebras.",
        "Physics-based numerical models have been the bedrock of atmospheric sciences\nfor decades, offering robust solutions but often at the cost of significant\ncomputational resources. Deep learning (DL) models have emerged as powerful\ntools in meteorology, capable of analyzing complex weather and climate data by\nlearning intricate dependencies and providing rapid predictions once trained.\nWhile these models demonstrate promising performance in weather prediction,\noften surpassing traditional physics-based methods, they still face critical\nchallenges. This paper presents a comprehensive survey of recent deep learning\nand foundation models for weather prediction. We propose a taxonomy to classify\nexisting models based on their training paradigms: deterministic predictive\nlearning, probabilistic generative learning, and pre-training and fine-tuning.\nFor each paradigm, we delve into the underlying model architectures, address\nmajor challenges, offer key insights, and propose targeted directions for\nfuture research. Furthermore, we explore real-world applications of these\nmethods and provide a curated summary of open-source code repositories and\nwidely used datasets, aiming to bridge research advancements with practical\nimplementations while fostering open and trustworthy scientific practices in\nadopting cutting-edge artificial intelligence for weather prediction. The\nrelated sources are available at https:\/\/github.com\/JimengShi\/\nDL-Foundation-Models-Weather.",
        "Mobile Edge Caching (MEC) plays a pivotal role in mitigating latency in\ndata-intensive services by dynamically caching frequently requested content on\nedge servers. This capability is critical for applications such as Augmented\nReality (AR), Virtual Reality (VR), and Autonomous Vehicles (AV), where\nefficient content caching and accurate popularity prediction are essential for\noptimizing performance. In this paper, we explore the problem of popularity\nprediction in MEC by utilizing historical time-series request data of intended\nfiles, formulating this problem as a ranking task. To this aim, we propose\nCacheMamba model by employing Mamba, a state-space model (SSM)-based\narchitecture, to identify the top-K files with the highest likelihood of being\nrequested. We then benchmark the proposed model against a Transformer-based\napproach, demonstrating its superior performance in terms of cache-hit rate,\nMean Average Precision (MAP), Normalized Discounted Cumulative Gain (NDCG), and\nFloating-Point Operations Per Second (FLOPS), particularly when dealing with\nlonger sequences.",
        "Generalized Category Discovery (GCD) aims to identify a mix of known and\nnovel categories within unlabeled data sets, providing a more realistic setting\nfor image recognition. Essentially, GCD needs to remember existing patterns\nthoroughly to recognize novel categories. Recent state-of-the-art method SimGCD\ntransfers the knowledge from known-class data to the learning of novel classes\nthrough debiased learning. However, some patterns are catastrophically forgot\nduring adaptation and thus lead to poor performance in novel categories\nclassification. To address this issue, we propose a novel learning approach,\nLegoGCD, which is seamlessly integrated into previous methods to enhance the\ndiscrimination of novel classes while maintaining performance on previously\nencountered known classes. Specifically, we design two types of techniques\ntermed as Local Entropy Regularization (LER) and Dual-views Kullback Leibler\ndivergence constraint (DKL). The LER optimizes the distribution of potential\nknown class samples in unlabeled data, thus ensuring the preservation of\nknowledge related to known categories while learning novel classes. Meanwhile,\nDKL introduces Kullback Leibler divergence to encourage the model to produce a\nsimilar prediction distribution of two view samples from the same image. In\nthis way, it successfully avoids mismatched prediction and generates more\nreliable potential known class samples simultaneously. Extensive experiments\nvalidate that the proposed LegoGCD effectively addresses the known category\nforgetting issue across all datasets, eg, delivering a 7.74% and 2.51% accuracy\nboost on known and novel classes in CUB, respectively. Our code is available\nat: https:\/\/github.com\/Cliffia123\/LegoGCD.",
        "The unconditioned reflex (e.g., protective reflex), which is the innate\nreaction of the organism and usually performed through the spinal cord rather\nthan the brain, can enable organisms to escape harms from environments. In this\npaper, we propose an online, highly-dynamic motion planning algorithm to endow\nmanipulators the highly-dynamic unconditioned reflexes to humans and\/or\nenvironments. Our method is based on a chained version of Signed Distance\nFunctions (SDFs), which can be pre-computed and stored. Our proposed algorithm\nis divided into two stages. In the offline stage, we create 3 groups of local\nSDFs to store the geometric information of the manipulator and its working\nenvironment. In the online stage, the pre-computed local SDFs are chained\ntogether according the configuration of the manipulator, to provide global\ngeometric information about the environment. While the point clouds of the\ndynamic objects serve as query points to look up these local SDFs for quickly\ngenerating escape velocity. Then we propose a modified geometric Jacobian\nmatrix and use the Jacobian-pseudo-inverse method to generate real-time reflex\nbehaviors to avoid the static and dynamic obstacles in the environment. The\nbenefits of our method are validated in both static and dynamic scenarios. In\nthe static scenario, our method identifies the path solutions with lower time\nconsumption and shorter trajectory length compared to existing solutions. In\nthe dynamic scenario, our method can reliably pursue the dynamic target point,\navoid dynamic obstacles, and react to these obstacles within 1ms, which\nsurpasses the unconditioned reflex reaction time of humans.",
        "Intelligent agents, such as robots and virtual agents, must understand the\ndynamics of complex social interactions to interact with humans. Effectively\nrepresenting social dynamics is challenging because we require multi-modal,\nsynchronized observations to understand a scene. We explore how using a single\nmodality, the pose behavior, of multiple individuals in a social interaction\ncan be used to generate nonverbal social cues for the facilitator of that\ninteraction. The facilitator acts to make a social interaction proceed smoothly\nand is an essential role for intelligent agents to replicate in human-robot\ninteractions. In this paper, we adapt an existing diffusion behavior cloning\nmodel to learn and replicate facilitator behaviors. Furthermore, we evaluate\ntwo representations of pose observations from a scene, one representation has\npre-processing applied and one does not. The purpose of this paper is to\nintroduce a new use for diffusion behavior cloning for pose generation in\nsocial interactions. The second is to understand the relationship between\nperformance and computational load for generating social pose behavior using\ntwo different techniques for collecting scene observations. As such, we are\nessentially testing the effectiveness of two different types of conditioning\nfor a diffusion model. We then evaluate the resulting generated behavior from\neach technique using quantitative measures such as mean per-joint position\nerror (MPJPE), training time, and inference time. Additionally, we plot\ntraining and inference time against MPJPE to examine the trade-offs between\nefficiency and performance. Our results suggest that the further pre-processed\ndata can successfully condition diffusion models to generate realistic social\nbehavior, with reasonable trade-offs in accuracy and processing time.",
        "We define a notion of categorical first order deformations for (enhanced)\ntriangulated categories. For a category $\\mathcal{T}$, we show that there is a\nbijection between $\\operatorname{HH}^2(\\mathcal{T})$ and the set of categorical\ndeformations of $\\mathcal{T}$. We show that in the case of curved deformations\nof dg algebras considered in arXiv:2406.04945, the $1$-derived category of the\ndeformation (introduced in arXiv:24020.8660) is a categorical deformation of\nthe derived category of the base; the Hochschild class identified by this\ndeformation is shown to restrict to the class defining the deformation of the\nalgebra. As an application, we give a conceptual proof of the fact that (for a\nsmooth base) the filtered derived category of a dg deformation yields a\ncategorical resolution of the classical derived category.",
        "In recent years, Neural Networks (NNs) have been employed to control\nnonlinear systems due to their potential capability in dealing with situations\nthat might be difficult for conventional nonlinear control schemes. However, to\nthe best of our knowledge, the current literature on NN-based control lacks\ntheoretical guarantees for stability and tracking performance. This precludes\nthe application of NN-based control schemes to systems where stringent\nstability and performance guarantees are required. To address this gap, this\npaper proposes a systematic and comprehensive methodology to design\nprovably-stable NN-based control schemes for affine nonlinear systems. Rigorous\nanalysis is provided to show that the proposed approach guarantees stability of\nthe closed-loop system with the NN in the loop. Also, it is shown that the\nresulting NN-based control scheme ensures that system states asymptotically\nconverge to a neighborhood around the desired equilibrium point, with a tunable\nproximity threshold. The proposed methodology is validated and evaluated via\nsimulation studies on an inverted pendulum and experimental studies on a Parrot\nBebop 2 drone.",
        "This paper introduces a novel strategy aimed at enhancing productivity and\nminimizing non-productive movements within container terminals, specifically\nfocusing on container yards. It advocates for the implementation of a digital\ntwin-based methodology to streamline the operations of stacking cranes (SCs)\nresponsible for container handling. The proposed approach entails the creation\nof a virtual container yard that mirrors the physical yard within a digital\ntwin system, facilitating real-time observation and validation. In addition,\nthis article demonstrates the effectiveness of using a digital twin to reduce\nunproductive movements and improve productivity through simulation. It defines\nvarious operational strategies and takes into account different yard contexts,\nproviding a comprehensive understanding of optimisation possibilities. By\nexploiting the capabilities of the digital twin, managers and operators are\nprovided with crucial information on operational dynamics, enabling them to\nidentify areas for improvement. This visualisation helps decision-makers to\nmake informed choices about their stacking strategies, thereby improving the\nefficiency of overall container terminal operations. Overall, this paper\npresent a digital twin solution in container terminal operations, offering a\npowerful tool for optimising productivity and minimising inefficiencies.",
        "Modern day applications have grown in size and require more computational\npower. The rise of machine learning and AI increased the need for parallel\ncomputation, which has increased the need for GPGPUs. With the increasing\ndemand for computational power, GPGPUs' SIMT architecture has solved this with\nan increase in the number of threads and the number of cores in a GPU,\nincreasing the throughput of these processors to match the demand of the\napplications. However, this created a larger demand for the memory, making the\nmemory bandwidth a bottleneck. The introduction of High-Bandwidth Memory (HBM)\nwith its increased number of memory ports offers a potential solution for the\nGPU to exploit its memory parallelism to increase the memory bandwidth.\nHowever, effectively leveraging HBM's memory parallelism to maximize bandwidth\npresents a unique and complex challenge for GPU architectures on how to\ndistribute those ports among the streaming multiprocessors in the GPGPU. In\nthis work, we extend the Vortex OpenGPU microarchitecture to incorporate a\nmultiport memory hierarchy, spanning from the L1 cache to the last-level cache\n(LLC). In addition, we propose various arbitration strategies to optimize\nmemory transfers across the cache hierarchy. The results have shown that an\nincrease in memory ports increases IPC, achieving an average speedup of 2.34x\nwith 8 memory ports in the tested configuration while showing relatively small\narea overhead.",
        "Open Vocabulary Video Anomaly Detection (OVVAD) seeks to detect and classify\nboth base and novel anomalies. However, existing methods face two specific\nchallenges related to novel anomalies. The first challenge is detection\nambiguity, where the model struggles to assign accurate anomaly scores to\nunfamiliar anomalies. The second challenge is categorization confusion, where\nnovel anomalies are often misclassified as visually similar base instances. To\naddress these challenges, we explore supplementary information from multiple\nsources to mitigate detection ambiguity by leveraging multiple levels of visual\ndata alongside matching textual information. Furthermore, we propose\nincorporating label relations to guide the encoding of new labels, thereby\nimproving alignment between novel videos and their corresponding labels, which\nhelps reduce categorization confusion. The resulting Anomize framework\neffectively tackles these issues, achieving superior performance on UCF-Crime\nand XD-Violence datasets, demonstrating its effectiveness in OVVAD.",
        "Automatic Music Transcription (AMT), aiming to get musical notes from raw\naudio, typically uses frame-level systems with piano-roll outputs or language\nmodel (LM)-based systems with note-level predictions. However, frame-level\nsystems require manual thresholding, while the LM-based systems struggle with\nlong sequences. In this paper, we propose a hybrid method combining pre-trained\nroll-based encoders with an LM decoder to leverage the strengths of both\nmethods. Besides, our approach employs a hierarchical prediction strategy,\nfirst predicting onset and pitch, then velocity, and finally offset. The\nhierarchical prediction strategy reduces computational costs by breaking down\nlong sequences into different hierarchies. Evaluated on two benchmark\nroll-based encoders, our method outperforms traditional piano-roll outputs 0.01\nand 0.022 in onset-offset-velocity F1 score, demonstrating its potential as a\nperformance-enhancing plug-in for arbitrary roll-based music transcription\nencoder.",
        "We show that hives chosen at random with independent GUE boundary conditions\non two sides, weighted by a Vandermonde factor depending on the third side\n(which is necessary in the context of the randomized Horn problem), when\nnormalized so that the eigenvalues at the edge are asymptotically constant,\nconverge in probability to a continuum hive as $n \\rightarrow \\infty.$ It had\npreviously been shown in joint work with Sheffield and Tao \\cite{NST} that the\nvariance of these scaled random hives tends to $0$ and consequently, from\ncompactness, that they converge in probability subsequentially. In the present\npaper, building on \\cite{NST}, we prove convergence in probability to a single\ncontinuum hive, without having to pass to a subsequence. We moreover show that\nthe value at a given point $v$ of this continuum hive equals the supremum of a\ncertain functional acting on asymptotic height functions of lozenge tilings.",
        "In standard genetic programming (stdGP), solutions are varied by modifying\ntheir syntax, with uncertain effects on their semantics. Geometric-semantic\ngenetic programming (GSGP), a popular variant of GP, effectively searches the\nsemantic solution space using variation operations based on linear\ncombinations, although it results in significantly larger solutions. This paper\npresents Transformer Semantic Genetic Programming (TSGP), a novel and flexible\nsemantic approach that uses a generative transformer model as search operator.\nThe transformer is trained on synthetic test problems and learns semantic\nsimilarities between solutions. Once the model is trained, it can be used to\ncreate offspring solutions with high semantic similarity also for unseen and\nunknown problems. Experiments on several symbolic regression problems show that\nTSGP generates solutions with comparable or even significantly better\nprediction quality than stdGP, SLIM_GSGP, DSR, and DAE-GP. Like SLIM_GSGP, TSGP\nis able to create new solutions that are semantically similar without creating\nsolutions of large size. An analysis of the search dynamic reveals that the\nsolutions generated by TSGP are semantically more similar than the solutions\ngenerated by the benchmark approaches allowing a better exploration of the\nsemantic solution space.",
        "Large Language Models (LLMs) can generate plausible test code. Intuitively\nthey generate this by imitating tests seen in their training data, rather than\nreasoning about execution semantics. However, such reasoning is important when\napplying mutation testing, where individual tests need to demonstrate\ndifferences in program behavior between a program and specific artificial\ndefects (mutants). In this paper, we evaluate whether Scientific Debugging,\nwhich has been shown to help LLMs when debugging, can also help them to\ngenerate tests for mutants. In the resulting approach, LLMs form hypotheses\nabout how to kill specific mutants, and then iteratively generate and refine\ntests until they succeed, all with detailed explanations for each step. We\ncompare this method to three baselines: (1) directly asking the LLM to generate\ntests, (2) repeatedly querying the LLM when tests fail, and (3) search-based\ntest generation with Pynguin. Our experiments evaluate these methods based on\nseveral factors, including mutation score, code coverage, success rate, and the\nability to identify equivalent mutants. The results demonstrate that LLMs,\nalthough requiring higher computation cost, consistently outperform Pynguin in\ngenerating tests with better fault detection and coverage. Importantly, we\nobserve that the iterative refinement of test cases is important for achieving\nhigh-quality test suites.",
        "In this paper, we propose to improve end-to-end (E2E) spoken language\nunderstand (SLU) in an RNN transducer model (RNN-T) by incorporating a joint\nself-conditioned CTC automatic speech recognition (ASR) objective. Our proposed\nmodel is akin to an E2E differentiable cascaded model which performs ASR and\nSLU sequentially and we ensure that the SLU task is conditioned on the ASR task\nby having CTC self conditioning. This novel joint modeling of ASR and SLU\nimproves SLU performance significantly over just using SLU optimization. We\nfurther improve the performance by aligning the acoustic embeddings of this\nmodel with the semantically richer BERT model. Our proposed knowledge transfer\nstrategy makes use of a bag-of-entity prediction layer on the aligned\nembeddings and the output of this is used to condition the RNN-T based SLU\ndecoding. These techniques show significant improvement over several strong\nbaselines and can perform at par with large models like Whisper with\nsignificantly fewer parameters.",
        "Random fuzzy variables join the modeling of the impreciseness (due to their\n``fuzzy part'') and randomness. Statistical samples of such objects are widely\nused, and their direct, numerically effective generation is therefore\nnecessary. Usually, these samples consist of triangular or trapezoidal fuzzy\nnumbers. In this paper, we describe theoretical results and simulation\nalgorithms for another family of fuzzy numbers -- LR fuzzy numbers with\ninterval-valued cores. Starting from a simulation perspective on the piecewise\nlinear LR fuzzy numbers with the interval-valued cores, their limiting behavior\nis then considered. This leads us to the numerically efficient algorithm for\nsimulating a sample consisting of such fuzzy values.",
        "In this work, we study the maximum scores that can be achieved in a\nteam-based domino game. Specifically, we show that if the game ends because it\nis blocked, then the maximum score that can be obtained under this assumption\nis 107.\n  --\n  En este trabajo estudiamos los puntajes m\\'aximos que se pueden obtener en\nuna partida, por equipos, en el juego del domin\\'o. Concretamente; nosotros\nmostramos que, si la partida termina porque el juego esta trancado, entonces el\npuntaje m\\'aximo que se puede obtener bajo este supuesto es de 107.",
        "This paper introduces PFlow-VC, a conditional flow matching voice conversion\nmodel that leverages fine-grained discrete pitch tokens and target speaker\nprompt information for expressive voice conversion (VC). Previous VC works\nprimarily focus on speaker conversion, with further exploration needed in\nenhancing expressiveness (such as prosody and emotion) for timbre conversion.\nUnlike previous methods, we adopt a simple and efficient approach to enhance\nthe style expressiveness of voice conversion models. Specifically, we pretrain\na self-supervised pitch VQVAE model to discretize speaker-irrelevant pitch\ninformation and leverage a masked pitch-conditioned flow matching model for\nMel-spectrogram synthesis, which provides in-context pitch modeling\ncapabilities for the speaker conversion model, effectively improving the voice\nstyle transfer capacity. Additionally, we improve timbre similarity by\ncombining global timbre embeddings with time-varying timbre tokens. Experiments\non unseen LibriTTS test-clean and emotional speech dataset ESD show the\nsuperiority of the PFlow-VC model in both timbre conversion and style transfer.\nAudio samples are available on the demo page\nhttps:\/\/speechai-demo.github.io\/PFlow-VC\/.",
        "As the use of automated employment decision tools (AEDTs) has rapidly\nincreased in hiring contexts, especially for computing jobs, there is still\nlimited work on applicants' perceptions of these emerging tools and their\nexperiences navigating them. To investigate, we conducted a survey with 448\ncomputer science students (young, current technology job-seekers) about\nperceptions of the procedural fairness of AEDTs, their willingness to be\nevaluated by different AEDTs, the strategies they use relating to automation in\nthe hiring process, and their job seeking success. We find that young job\nseekers' procedural fairness perceptions of and willingness to be evaluated by\nAEDTs varied with the level of automation involved in the AEDT, the technical\nnature of the task being evaluated, and their own use of strategies, such as\njob referrals. Examining the relationship of their strategies with job\noutcomes, notably, we find that referrals and family household income have\nsignificant and positive impacts on hiring success, while more egalitarian\nstrategies (using free online coding assessment practice or adding keywords to\nresumes) did not. Overall, our work speaks to young job seekers' distrust of\nautomation in hiring contexts, as well as the continued role of social and\nsocioeconomic privilege in job seeking, despite the use of AEDTs that promise\nto make hiring \"unbiased.\"",
        "Social networks scaffold the diffusion of information on social media. Much\nattention has been given to the spread of true vs. false content on online\nsocial platforms, including the structural differences between their diffusion\npatterns. However, much less is known about how platform interventions on false\ncontent alter the engagement with and diffusion of such content. In this work,\nwe estimate the causal effects of Community Notes, a novel fact-checking\nfeature adopted by X (formerly Twitter) to solicit and vet crowd-sourced\nfact-checking notes for false content. We gather detailed time series data for\n40,074 posts for which notes have been proposed and use synthetic control\nmethods to estimate a range of counterfactual outcomes. We find that attaching\nfact-checking notes significantly reduces the engagement with and diffusion of\nfalse content. We estimate that, on average, the notes resulted in reductions\nof 45.7% in reposts, 43.5% in likes, 22.9% in replies, and 14.0% in views after\nbeing attached. Over the posts' entire lifespans, these reductions amount to\n11.4% fewer reposts, 13.0% fewer likes, 7.3% fewer replies, and 5.7% fewer\nviews on average. In reducing reposts, we observe that diffusion cascades for\nfact-checked content are less deep, but not less broad, than synthetic control\nestimates for non-fact-checked content with similar reach. This structural\ndifference contrasts notably with differences between false vs. true content\ndiffusion itself, where false information diffuses farther, but with structural\npatterns that are otherwise indistinguishable from those of true information,\nconditional on reach.",
        "Suppose $\\kappa$ is a regular cardinal and $\\bar a=\\langle \\mu_i: i<\\kappa\n\\rangle$ is a non-decreasing sequence of regular cardinals. We study the set of\npossible cofinalities of cuts Pcut$(\\bar a)=\\{(\\lambda_1, \\lambda_2):$ for some\nultrafilter $D$ on $\\kappa$, $(\\lambda_1, \\lambda_2)$ is the cofinality of a\ncut of $\\prod\\limits_{i<\\kappa} \\mu_i \/ D \\}$.",
        "We investigate the Sobolev IPM problem for probability measures supported on\na graph metric space. Sobolev IPM is an important instance of integral\nprobability metrics (IPM), and is obtained by constraining a critic function\nwithin a unit ball defined by the Sobolev norm. In particular, it has been used\nto compare probability measures and is crucial for several theoretical works in\nmachine learning. However, to our knowledge, there are no efficient algorithmic\napproaches to compute Sobolev IPM effectively, which hinders its practical\napplications. In this work, we establish a relation between Sobolev norm and\nweighted $L^p$-norm, and leverage it to propose a \\emph{novel regularization}\nfor Sobolev IPM. By exploiting the graph structure, we demonstrate that the\nregularized Sobolev IPM provides a \\emph{closed-form} expression for fast\ncomputation. This advancement addresses long-standing computational challenges,\nand paves the way to apply Sobolev IPM for practical applications, even in\nlarge-scale settings. Additionally, the regularized Sobolev IPM is negative\ndefinite. Utilizing this property, we design positive-definite kernels upon the\nregularized Sobolev IPM, and provide preliminary evidences of their advantages\non document classification and topological data analysis for measures on a\ngraph.",
        "Recently, radars have been widely featured in robotics for their robustness\nin challenging weather conditions. Two commonly used radar types are spinning\nradars and phased-array radars, each offering distinct sensor characteristics.\nExisting datasets typically feature only a single type of radar, leading to the\ndevelopment of algorithms limited to that specific kind. In this work, we\nhighlight that combining different radar types offers complementary advantages,\nwhich can be leveraged through a heterogeneous radar dataset. Moreover, this\nnew dataset fosters research in multi-session and multi-robot scenarios where\nrobots are equipped with different types of radars. In this context, we\nintroduce the HeRCULES dataset, a comprehensive, multi-modal dataset with\nheterogeneous radars, FMCW LiDAR, IMU, GPS, and cameras. This is the first\ndataset to integrate 4D radar and spinning radar alongside FMCW LiDAR, offering\nunparalleled localization, mapping, and place recognition capabilities. The\ndataset covers diverse weather and lighting conditions and a range of urban\ntraffic scenarios, enabling a comprehensive analysis across various\nenvironments. The sequence paths with multiple revisits and ground truth pose\nfor each sensor enhance its suitability for place recognition research. We\nexpect the HeRCULES dataset to facilitate odometry, mapping, place recognition,\nand sensor fusion research. The dataset and development tools are available at\nhttps:\/\/sites.google.com\/view\/herculesdataset.",
        "We investigate species-rich mathematical models of ecosystems. Much of the\nexisting literature focuses on the properties of equilibrium fixed points, in\nparticular their stability and feasibility. Here we emphasize the emergence of\nlimit cycles following Hopf bifurcations tuned by the variability of\ninterspecies interaction. As the variability increases, and owing to the large\ndimensionality of the system, limit cycles typically acquire a growing spectrum\nof frequencies. This often leads to the appearance of strange attractors, with\na chaotic dynamics of species abundances characterized by a positive Lyapunov\nexponent. We find that limit cycles and strange attractors preserve\nbiodiversity as they maintain dynamical stability without species extinction.\nWe give numerical evidences that this route to chaos dominates in ecosystems\nwith strong enough interactions and where predator-prey behavior dominates over\ncompetition and mutualism. Based on arguments from random matrix theory, we\nfurther conjecture that this scenario is generic in ecosystems with large\nnumber of species, and identify the key parameters driving it. Overall, our\nwork proposes a unifying framework, where a wide range of population dynamics\nemerge from a single model.",
        "Relying on a large corpus of natural interactions between visitors and a\nrobot in a museum setting, we study a recurrent practice through which humans\n\"worked\" to maintain the robot as a competent participant: the description by\nbystanders, in a way that was made accessible to the main speaker, of the\nsocial action that the robot was taken to be accomplishing. Doing so,\nbystanders maintained the robot's (sometimes incongruous) behaviour as relevant\nto the activity at hand and preserved the robot itself as a competent\nparticipant. Relying on these data, we argue that ex ante definitions of a\nrobot as \"social\" (i.e. before any interaction occurred) run the risk of\nnaturalizing as self-evident the observable result from micro-sociological\nprocesses: namely, the interactional work of co-present humans through which\nthe robot's conduct is reconfigured as contextually relevant.",
        "This review explores the integration of deep learning (DL) with full-waveform\ninversion (FWI) for enhanced seismic imaging and subsurface characterization.\nIt covers FWI and DL fundamentals, geophysical applications (velocity\nestimation, deconvolution, tomography), and challenges (model complexity, data\nquality). The review also outlines future research directions, including\nhybrid, generative, and physics-informed models for improved accuracy,\nefficiency, and reliability in subsurface property estimation. The synergy\nbetween DL and FWI has the potential to transform geophysics, providing new\ninsights into Earth's subsurface.",
        "Autonomous Vehicle (AV) systems have been developed with a strong reliance on\nmachine learning techniques. While machine learning approaches, such as deep\nlearning, are extremely effective at tasks that involve observation and\nclassification, they struggle when it comes to performing higher level\nreasoning about situations on the road. This research involves incorporating\ncommonsense reasoning models that use image data to improve AV systems. This\nwill allow AV systems to perform more accurate reasoning while also making them\nmore adjustable, explainable, and ethical. This paper will discuss the findings\nso far and motivate its direction going forward.",
        "Recent studies have highlighted the vulnerability of deep neural networks to\nbackdoor attacks, where models are manipulated to rely on embedded triggers\nwithin poisoned samples, despite the presence of both benign and trigger\ninformation. While several defense methods have been proposed, they often\nstruggle to balance backdoor mitigation with maintaining benign performance.In\nthis work, inspired by the concept of optical polarizer-which allows light\nwaves of specific polarizations to pass while filtering others-we propose a\nlightweight backdoor defense approach, NPD. This method integrates a neural\npolarizer (NP) as an intermediate layer within the compromised model,\nimplemented as a lightweight linear transformation optimized via bi-level\noptimization. The learnable NP filters trigger information from poisoned\nsamples while preserving benign content. Despite its effectiveness, we identify\nthrough empirical studies that NPD's performance degrades when the target\nlabels (required for purification) are inaccurately estimated. To address this\nlimitation while harnessing the potential of targeted adversarial mitigation,\nwe propose class-conditional neural polarizer-based defense (CNPD). The key\ninnovation is a fusion module that integrates the backdoored model's predicted\nlabel with the features to be purified. This architecture inherently mimics\ntargeted adversarial defense mechanisms without requiring label estimation used\nin NPD. We propose three implementations of CNPD: the first is r-CNPD, which\ntrains a replicated NP layer for each class and, during inference, selects the\nappropriate NP layer for defense based on the predicted class from the\nbackdoored model. To efficiently handle a large number of classes, two variants\nare designed: e-CNPD, which embeds class information as additional features,\nand a-CNPD, which directs network attention using class information."
      ]
    }
  },
  {
    "id":2411.04682,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Distributed Sky Imaging Radiometry and Tomography",
    "start_abstract":"The composition of the atmosphere is significant to our ecosystem. Accordingly, there a need sense distributions atmospheric scatterers such as aerosols and cloud droplets. There growing interest in recovering these scattering fields three-dimensions (3D). Even so, current observations usually use expensive unscalable equipment. Moreover, analysis retrieves partial information (e.g., cloud-base altitudes, water droplet size at tops) based on simplified 1D models. To advance retrievals, we develop new computational imaging approach for sensing analyzing atmosphere, volumetrically. Our comprises ground-based network cameras. We deployed it conjunction with additional remote equipment, including Raman lidar sunphotometer, which provide initialization algorithms ground truth. camera scalable, low cost, enables 3D high spatial temporal resolution. describe how system calibrated absolute radiometric readouts light field. Consequently, recover volumetric field scatterers, using tomography. tomography process adapted relative prior art, run large-scale domains being in-situ within scatterer fields. empirically demonstrate feasibility clouds, data.",
    "start_categories":[
      "astro-ph.EP"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b8"
      ],
      "title":[
        "Variable Imaging Projection Cloud Scattering Tomography"
      ],
      "abstract":[
        "Scattering-based computed tomography (CT) recovers a heterogeneous volumetric scattering medium using images taken from multiple directions. It is nonlinear problem. Prior art mainly approached it by explicit physics-based optimization of image-fitting, being slow and difficult to scale. Scale particularly important when the objects constitute large cloud fields, where recovery for climate studies. Besides speed, imaging need be flexible, efficiently handle variable viewing geometries resolutions. These can caused perturbation in camera poses or fusion data different types observational sensors. There fast projection clouds (VIP-CT). We develop learning-based solution, deep-neural network (DNN) which trains on labeled dataset. The DNN parameters are oblivious domain scale, hence work with arbitrarily domains. VIP-CT offers much better quality than state art. inference speed flexibility make effectively real-time context spaceborne observations. paper first demonstrate CT real empirical directly DNN. may offer model solution problems other scientific Our code available online."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "Codes with symmetric distances",
        "RobotIQ: Empowering Mobile Robots with Human-Level Planning for\n  Real-World Execution",
        "UniDemoir\\'e: Towards Universal Image Demoir\\'eing with Data Generation\n  and Synthesis",
        "How Vital is the Jurisprudential Relevance: Law Article Intervened Legal\n  Case Retrieval and Matching",
        "GASP: Unifying Geometric and Semantic Self-Supervised Pre-training for\n  Autonomous Driving",
        "Enhanced Derivative-Free Optimization Using Adaptive Correlation-Induced\n  Finite Difference Estimators",
        "Advanced 3D-Printed Multiphasic Scaffold with Optimal PRP Dosage for\n  Chondrogenesis of BM-MSCs in Osteochondral Tissue Engineering",
        "Recovering Partially Corrupted Major Objects through Tri-modality Based\n  Image Completion",
        "Designing VR Simulation System for Clinical Communication Training with\n  LLMs-Based Embodied Conversational Agents",
        "On the asymptotic validity of confidence sets for linear functionals of\n  solutions to integral equations",
        "On the approaching geodesics property",
        "Multilevel Generative Samplers for Investigating Critical Phenomena",
        "Empowering LLMs with Logical Reasoning: A Comprehensive Survey",
        "Multi-View Depth Consistent Image Generation Using Generative AI Models:\n  Application on Architectural Design of University Buildings",
        "Universal Chern classes on the moduli of bundles",
        "Deep ARTMAP: Generalized Hierarchical Learning with Adaptive Resonance\n  Theory",
        "WyckoffDiff -- A Generative Diffusion Model for Crystal Symmetry",
        "GPT-PPG: A GPT-based Foundation Model for Photoplethysmography Signals",
        "A Simple Aerial Detection Baseline of Multimodal Language Models",
        "PaperHelper: Knowledge-Based LLM QA Paper Reading Assistant",
        "FormalSpecCpp: A Dataset of C++ Formal Specifications created using LLMs",
        "Canonical forms of oriented matroids",
        "XTS mode revisited: high hopes for key scopes?",
        "Efficient Coordination and Synchronization of Multi-Robot Systems Under\n  Recurring Linear Temporal Logic",
        "An Efficient Row-Based Sparse Fine-Tuning",
        "Maximal rigid modules over a gentle algebra and applications to higher\n  Auslander-Reiten theory",
        "Mechanics on flag manifolds",
        "Sparse Auto-Encoder Interprets Linguistic Features in Large Language\n  Models",
        "Prosperity: Accelerating Spiking Neural Networks via Product Sparsity"
      ],
      "abstract":[
        "For a code $C$ in a space with maximal distance $n$, we say that $C$ has\nsymmetric distances if its distance set $S(C)$ is symmetric with respect to $n\n\/ 2$. In this paper, we prove that if $C$ is a binary code with length $2n$,\nconstant weight $n$ and symmetric distances, then \\[\n  |C| \\leq \\binom{2 n - 1}{|S(C)|}. \\] This result can be interpreted using the\nlanguage of Johnson association schemes. More generally, we give a framework to\nstudy codes with symmetric distances in Q-bipartite Q-polynomial association\nschemes, and provide upper bounds for such codes. Moreover, we use number\ntheoretic techniques to determine when the equality holds.",
        "This paper introduces RobotIQ, a framework that empowers mobile robots with\nhuman-level planning capabilities, enabling seamless communication via natural\nlanguage instructions through any Large Language Model. The proposed framework\nis designed in the ROS architecture and aims to bridge the gap between humans\nand robots, enabling robots to comprehend and execute user-expressed text or\nvoice commands. Our research encompasses a wide spectrum of robotic tasks,\nranging from fundamental logical, mathematical, and learning reasoning for\ntransferring knowledge in domains like navigation, manipulation, and object\nlocalization, enabling the application of learned behaviors from simulated\nenvironments to real-world operations. All encapsulated within a modular\ncrafted robot library suite of API-wise control functions, RobotIQ offers a\nfully functional AI-ROS-based toolset that allows researchers to design and\ndevelop their own robotic actions tailored to specific applications and robot\nconfigurations. The effectiveness of the proposed system was tested and\nvalidated both in simulated and real-world experiments focusing on a home\nservice scenario that included an assistive application designed for elderly\npeople. RobotIQ with an open-source, easy-to-use, and adaptable robotic library\nsuite for any robot can be found at https:\/\/github.com\/emmarapt\/RobotIQ.",
        "Image demoir\\'eing poses one of the most formidable challenges in image\nrestoration, primarily due to the unpredictable and anisotropic nature of\nmoir\\'e patterns. Limited by the quantity and diversity of training data,\ncurrent methods tend to overfit to a single moir\\'e domain, resulting in\nperformance degradation for new domains and restricting their robustness in\nreal-world applications. In this paper, we propose a universal image\ndemoir\\'eing solution, UniDemoir\\'e, which has superior generalization\ncapability. Notably, we propose innovative and effective data generation and\nsynthesis methods that can automatically provide vast high-quality moir\\'e\nimages to train a universal demoir\\'eing model. Our extensive experiments\ndemonstrate the cutting-edge performance and broad potential of our approach\nfor generalized image demoir\\'eing.",
        "Legal case retrieval (LCR) aims to automatically scour for comparable legal\ncases based on a given query, which is crucial for offering relevant precedents\nto support the judgment in intelligent legal systems. Due to similar goals, it\nis often associated with a similar case matching (LCM) task. To address them, a\ndaunting challenge is assessing the uniquely defined legal-rational similarity\nwithin the judicial domain, which distinctly deviates from the semantic\nsimilarities in general text retrieval. Past works either tagged\ndomain-specific factors or incorporated reference laws to capture\nlegal-rational information. However, their heavy reliance on expert or\nunrealistic assumptions restricts their practical applicability in real-world\nscenarios. In this paper, we propose an end-to-end model named LCM-LAI to solve\nthe above challenges. Through meticulous theoretical analysis, LCM-LAI employs\na dependent multi-task learning framework to capture legal-rational information\nwithin legal cases by a law article prediction (LAP) sub-task, without any\nadditional assumptions in inference. Besides, LCM-LAI proposes an article-aware\nattention mechanism to evaluate the legal-rational similarity between\nacross-case sentences based on law distribution, which is more effective than\nconventional semantic similarity. Weperform a series of exhaustive experiments\nincluding two different tasks involving four real-world datasets. Results\ndemonstrate that LCM-LAI achieves state-of-the-art performance.",
        "Self-supervised pre-training based on next-token prediction has enabled large\nlanguage models to capture the underlying structure of text, and has led to\nunprecedented performance on a large array of tasks when applied at scale.\nSimilarly, autonomous driving generates vast amounts of spatiotemporal data,\nalluding to the possibility of harnessing scale to learn the underlying\ngeometric and semantic structure of the environment and its evolution over\ntime. In this direction, we propose a geometric and semantic self-supervised\npre-training method, GASP, that learns a unified representation by predicting,\nat any queried future point in spacetime, (1) general occupancy, capturing the\nevolving structure of the 3D scene; (2) ego occupancy, modeling the ego vehicle\npath through the environment; and (3) distilled high-level features from a\nvision foundation model. By modeling geometric and semantic 4D occupancy fields\ninstead of raw sensor measurements, the model learns a structured,\ngeneralizable representation of the environment and its evolution through time.\nWe validate GASP on multiple autonomous driving benchmarks, demonstrating\nsignificant improvements in semantic occupancy forecasting, online mapping, and\nego trajectory prediction. Our results demonstrate that continuous 4D geometric\nand semantic occupancy prediction provides a scalable and effective\npre-training paradigm for autonomous driving. For code and additional\nvisualizations, see \\href{https:\/\/research.zenseact.com\/publications\/gasp\/.",
        "Gradient-based methods are well-suited for derivative-free optimization\n(DFO), where finite-difference (FD) estimates are commonly used as gradient\nsurrogates. Traditional stochastic approximation methods, such as\nKiefer-Wolfowitz (KW) and simultaneous perturbation stochastic approximation\n(SPSA), typically utilize only two samples per iteration, resulting in\nimprecise gradient estimates and necessitating diminishing step sizes for\nconvergence. In this paper, we first explore an efficient FD estimate, referred\nto as correlation-induced FD estimate, which is a batch-based estimate. Then,\nwe propose an adaptive sampling strategy that dynamically determines the batch\nsize at each iteration. By combining these two components, we develop an\nalgorithm designed to enhance DFO in terms of both gradient estimation\nefficiency and sample efficiency. Furthermore, we establish the consistency of\nour proposed algorithm and demonstrate that, despite using a batch of samples\nper iteration, it achieves the same convergence rate as the KW and SPSA\nmethods. Additionally, we propose a novel stochastic line search technique to\nadaptively tune the step size in practice. Finally, comprehensive numerical\nexperiments confirm the superior empirical performance of the proposed\nalgorithm.",
        "In osteochondral tissue engineering (OCTE), simultaneously regenerating\nsubchondral bone and cartilage tissue presents a significant challenge.\nMultiphasic scaffolds were created and manufactured using 3D printing to\naddress this issue. Excellent interfacial mechanical properties and\nbiocompatibility enhance the growth and chondrogenic differentiation of bone\nmarrow mesenchymal stem cells (BM-MSCs). The subchondral bone bottom layer is\nmimicked by incorporating varying concentrations of graphene oxide (GO) (0%,\n1%, and 2% w\/v) into a bioink composed of alginate (Alg) and gelatin (Gel).\nBased on evaluations of mechanical and biocompatibility properties, 1% GO is\nselected for further studies. Subsequently, the GO concentration is kept\nconstant while varying the platelet-rich plasma (PRP) dosage in the multiphasic\nscaffolds. Different PRP dosages (0%, 1%, 2%, and 3% w\/v) are integrated into\nthe Alg-Gel bioink to simulate cartilage tissues. Results indicate that\n3D-printed scaffolds containing 1% or 2% PRP exhibit favorable biomechanical\nproperties, with no significant differences observed. However, BM-MSCs exposed\nto 2% PRP demonstrate enhanced adhesion, growth, and viability. Additionally,\nreal-time PCR and Alcian blue staining confirm increased chondrogenic\nexpression and glycosaminoglycans (GAGs) synthesis. This work highlights the\npromising potential of 3D-printed multiphasic frameworks in the development of\nOCTE.",
        "Diffusion models have become widely adopted in image completion tasks, with\ntext prompts commonly employed to ensure semantic coherence by providing\nhigh-level guidance. However, a persistent challenge arises when an object is\npartially obscured in the damaged region, yet its remaining parts are still\nvisible in the background. While text prompts offer semantic direction, they\noften fail to precisely recover fine-grained structural details, such as the\nobject's overall posture, ensuring alignment with the visible object\ninformation in the background. This limitation stems from the inability of text\nprompts to provide pixel-level specificity. To address this, we propose\nsupplementing text-based guidance with a novel visual aid: a casual sketch,\nwhich can be roughly drawn by anyone based on visible object parts. This sketch\nsupplies critical structural cues, enabling the generative model to produce an\nobject structure that seamlessly integrates with the existing background. We\nintroduce the Visual Sketch Self-Aware (VSSA) model, which integrates the\ncasual sketch into each iterative step of the diffusion process, offering\ndistinct advantages for partially corrupted scenarios. By blending\nsketch-derived features with those of the corrupted image, and leveraging text\nprompt guidance, the VSSA assists the diffusion model in generating images that\npreserve both the intended object semantics and structural consistency across\nthe restored objects and original regions. To support this research, we created\ntwo datasets, CUB-sketch and MSCOCO-sketch, each combining images, sketches,\nand text. Extensive qualitative and quantitative experiments demonstrate that\nour approach outperforms several state-of-the-art methods.",
        "VR simulation in Health Professions (HP) education demonstrates huge\npotential, but fixed learning content with little customization limits its\napplication beyond lab environments. To address these limitations in the\ncontext of VR for patient communication training, we conducted a user-centered\nstudy involving semi-structured interviews with advanced HP students to\nunderstand their challenges in clinical communication training and perceptions\nof VR-based solutions. From this, we derived design insights emphasizing the\nimportance of realistic scenarios, simple interactions, and unpredictable\ndialogues. Building on these insights, we developed the Virtual AI Patient\nSimulator (VAPS), a novel VR system powered by Large Language Models (LLMs) and\nEmbodied Conversational Agents (ECAs), supporting dynamic and customizable\npatient interactions for immersive learning. We also provided an example of how\nclinical professors could use user-friendly design forms to create personalized\nscenarios that align with course objectives in VAPS and discuss future\nimplications of integrating AI-driven technologies into VR education.",
        "This paper examines the construction of confidence sets for parameters\ndefined as a linear functional of the solution to an integral equation\ninvolving conditional expectations. We show that any confidence set uniformly\nvalid over a broad class of probability laws, allowing the integral equation to\nbe arbitrarily ill-posed must have, with high probability under some laws, a\ndiameter at least as large as the diameter of the parameter's range over the\nmodel. Additionally, we establish that uniformly consistent estimators of the\nparameter do not exist. We show that, consistent with the weak instruments\nliterature, Wald confidence intervals are not uniformly valid. Furthermore, we\nargue that inverting the score test, a successful approach in that literature,\ndoes not extend to the broader class of parameters considered here. We present\na method for constructing uniformly valid confidence sets in the special case\nwhere all variables are binary and discuss its limitations. Finally, we\nemphasize that developing uniformly valid confidence sets for the general class\nof parameters considered in this paper remains an open problem.",
        "We survey some recent results and open questions on the approaching geodesics\nproperty and its application to the study of the Gromov and horofunction\ncompactifications of a proper geodesic Gromov metric space. We obtain results\non the dynamics of isometries and we exhibit an example of a Gromov hyperbolic\ndomain of $\\mathbb{C}$ which does not satisfy the approaching geodesic\nproperty.",
        "Investigating critical phenomena or phase transitions is of high interest in\nphysics and chemistry, for which Monte Carlo (MC) simulations, a crucial tool\nfor numerically analyzing macroscopic properties of given systems, are often\nhindered by an emerging divergence of correlation length -- known as scale\ninvariance at criticality (SIC) in the renormalization group theory. SIC causes\nthe system to behave the same at any length scale, from which many existing\nsampling methods suffer: long-range correlations cause critical slowing down in\nMarkov chain Monte Carlo (MCMC), and require intractably large receptive fields\nfor generative samplers. In this paper, we propose a Renormalization-informed\nGenerative Critical Sampler (RiGCS) -- a novel sampler specialized for\nnear-critical systems, where SIC is leveraged as an advantage rather than a\nnuisance. Specifically, RiGCS builds on MultiLevel Monte Carlo (MLMC) with Heat\nBath (HB) algorithms, which perform ancestral sampling from low-resolution to\nhigh-resolution lattice configurations with site-wise-independent conditional\nHB sampling. Although MLMC-HB is highly efficient under exact SIC, it suffers\nfrom a low acceptance rate under slight SIC violation. Notably, SIC violation\nalways occurs in finite-size systems, and may induce long-range and\nhigher-order interactions in the renormalized distributions, which are not\nconsidered by independent HB samplers. RiGCS enhances MLMC-HB by replacing a\npart of the conditional HB sampler with generative models that capture those\nresidual interactions and improve the sampling efficiency. Our experiments show\nthat the effective sample size of RiGCS is a few orders of magnitude higher\nthan state-of-the-art generative model baselines in sampling configurations for\n128x128 two-dimensional Ising systems.",
        "Large language models (LLMs) have achieved remarkable successes on various\nnatural language tasks. However, recent studies have found that there are still\nsignificant challenges to the logical reasoning abilities of LLMs. This paper\nsummarizes and categorizes the main challenges into two aspects: (1) Logical\nquestion answering, LLMs often fail to generate the correct answer within\ncomplex logical problem which requires sophisticated deductive, inductive or\nabductive reasoning given a collection of premises and constrains. (2) Logical\nconsistency, LLMs are prone to producing responses contradicting themselves\nacross different questions. For example, a state-of-the-art Macaw\nquestion-answering LLM answers Yes to both questions Is a magpie a bird? and\nDoes a bird have wings? but answers No to Does a magpie have wings?. To\nfacilitate this research direction, we comprehensively investigate the most\ncutting-edge methods and propose detailed taxonomies of these methods.\nSpecifically, to accurately answer complex logic questions, previous methods\ncan be categorized based on reliance on external solvers, prompts, pretraining,\nand fine-tuning. To avoid logical contradictions, we discuss concepts and\nsolutions of various logical consistencies, including implication, negation,\ntransitivity, factuality consistency, and their composites. In addition, we\nreview commonly used benchmark datasets and evaluation metrics, and discuss\npromising research directions, such as extensions to modal logic to account for\nuncertainty, and efficient algorithms satisfying multiple logical consistencies\nsimultaneously.",
        "In the early stages of architectural design, shoebox models are typically\nused as a simplified representation of building structures but require\nextensive operations to transform them into detailed designs. Generative\nartificial intelligence (AI) provides a promising solution to automate this\ntransformation, but ensuring multi-view consistency remains a significant\nchallenge. To solve this issue, we propose a novel three-stage consistent image\ngeneration framework using generative AI models to generate architectural\ndesigns from shoebox model representations. The proposed method enhances\nstate-of-the-art image generation diffusion models to generate multi-view\nconsistent architectural images. We employ ControlNet as the backbone and\noptimize it to accommodate multi-view inputs of architectural shoebox models\ncaptured from predefined perspectives. To ensure stylistic and structural\nconsistency across multi-view images, we propose an image space loss module\nthat incorporates style loss, structural loss and angle alignment loss. We then\nuse depth estimation method to extract depth maps from the generated multi-view\nimages. Finally, we use the paired data of the architectural images and depth\nmaps as inputs to improve the multi-view consistency via the depth-aware 3D\nattention module. Experimental results demonstrate that the proposed framework\ncan generate multi-view architectural images with consistent style and\nstructural coherence from shoebox model inputs.",
        "The goal of this paper is to construct universal cohomology classes on the\nmoduli space of stable bundles over a curve when it is not a fine moduli space,\ni.e. when the rank and degree are not coprime. More precisely, we show that\ncertain Chern classes of the universal bundle on the product of the curve with\nthe moduli stack of bundles lift to the product of the curve with the moduli\nspace of stable bundles.",
        "This paper presents Deep ARTMAP, a novel extension of the ARTMAP architecture\nthat generalizes the self-consistent modular ART (SMART) architecture to enable\nhierarchical learning (supervised and unsupervised) across arbitrary\ntransformations of data. The Deep ARTMAP framework operates as a divisive\nclustering mechanism, supporting an arbitrary number of modules with\ncustomizable granularity within each module. Inter-ART modules regulate the\nclustering at each layer, permitting unsupervised learning while enforcing a\none-to-many mapping from clusters in one layer to the next. While Deep ARTMAP\nreduces to both ARTMAP and SMART in particular configurations, it offers\nsignificantly enhanced flexibility, accommodating a broader range of data\ntransformations and learning modalities.",
        "Crystalline materials often exhibit a high level of symmetry. However, most\ngenerative models do not account for symmetry, but rather model each atom\nwithout any constraints on its position or element. We propose a generative\nmodel, Wyckoff Diffusion (WyckoffDiff), which generates symmetry-based\ndescriptions of crystals. This is enabled by considering a crystal structure\nrepresentation that encodes all symmetry, and we design a novel neural network\narchitecture which enables using this representation inside a discrete\ngenerative model framework. In addition to respecting symmetry by construction,\nthe discrete nature of our model enables fast generation. We additionally\npresent a new metric, Fr\\'echet Wrenformer Distance, which captures the\nsymmetry aspects of the materials generated, and we benchmark WyckoffDiff\nagainst recently proposed generative models for crystal generation.",
        "This study introduces a novel application of a Generative Pre-trained\nTransformer (GPT) model tailored for photoplethysmography (PPG) signals,\nserving as a foundation model for various downstream tasks. Adapting the\nstandard GPT architecture to suit the continuous characteristics of PPG\nsignals, our approach demonstrates promising results. Our models are\npre-trained on our extensive dataset that contains more than 200 million 30s\nPPG samples. We explored different supervised fine-tuning techniques to adapt\nour model to downstream tasks, resulting in performance comparable to or\nsurpassing current state-of-the-art (SOTA) methods in tasks like atrial\nfibrillation detection. A standout feature of our GPT model is its inherent\ncapability to perform generative tasks such as signal denoising effectively,\nwithout the need for further fine-tuning. This success is attributed to the\ngenerative nature of the GPT framework.",
        "The multimodal language models (MLMs) based on generative pre-trained\nTransformer are considered powerful candidates for unifying various domains and\ntasks. MLMs developed for remote sensing (RS) have demonstrated outstanding\nperformance in multiple tasks, such as visual question answering and visual\ngrounding. In addition to visual grounding that detects specific objects\ncorresponded to given instruction, aerial detection, which detects all objects\nof multiple categories, is also a valuable and challenging task for RS\nfoundation models. However, aerial detection has not been explored by existing\nRS MLMs because the autoregressive prediction mechanism of MLMs differs\nsignificantly from the detection outputs. In this paper, we present a simple\nbaseline for applying MLMs to aerial detection for the first time, named\nLMMRotate. Specifically, we first introduce a normalization method to transform\ndetection outputs into textual outputs to be compatible with the MLM framework.\nThen, we propose a evaluation method, which ensures a fair comparison between\nMLMs and conventional object detection models. We construct the baseline by\nfine-tuning open-source general-purpose MLMs and achieve impressive detection\nperformance comparable to conventional detector. We hope that this baseline\nwill serve as a reference for future MLM development, enabling more\ncomprehensive capabilities for understanding RS images. Code is available at\nhttps:\/\/github.com\/Li-Qingyun\/mllm-mmrotate.",
        "In the paper, we introduce a paper reading assistant, PaperHelper, a potent\ntool designed to enhance the capabilities of researchers in efficiently\nbrowsing and understanding scientific literature. Utilizing the\nRetrieval-Augmented Generation (RAG) framework, PaperHelper effectively\nminimizes hallucinations commonly encountered in large language models (LLMs),\noptimizing the extraction of accurate, high-quality knowledge. The\nimplementation of advanced technologies such as RAFT and RAG Fusion\nsignificantly boosts the performance, accuracy, and reliability of the\nLLMs-based literature review process. Additionally, PaperHelper features a\nuser-friendly interface that facilitates the batch downloading of documents and\nuses the Mermaid format to illustrate structural relationships between\ndocuments. Experimental results demonstrate that PaperHelper, based on a\nfine-tuned GPT-4 API, achieves an F1 Score of 60.04, with a latency of only 5.8\nseconds, outperforming the basic RAG model by 7\\% in F1 Score.",
        "FormalSpecCpp is a dataset designed to fill the gap in standardized\nbenchmarks for verifying formal specifications in C++ programs. To the best of\nour knowledge, this is the first comprehensive collection of C++ programs with\nwell-defined preconditions and postconditions. It provides a structured\nbenchmark for evaluating specification inference tools and testing theaccuracy\nof generated specifications. Researchers and developers can use this dataset to\nbenchmark specification inference tools,fine-tune Large Language Models (LLMs)\nfor automated specification generation, and analyze the role of formal\nspecifications in improving program verification and automated testing. By\nmaking this dataset publicly available, we aim to advance research in program\nverification, specification inference, and AI-assisted software development.\nThe dataset and the code are available at\nhttps:\/\/github.com\/MadhuNimmo\/FormalSpecCpp.",
        "Positive geometries are semialgebraic sets equipped with a canonical\ndifferential form whose residues mirror the boundary structure of the geometry.\nEvery full-dimensional projective polytope is a positive geometry. Motivated by\nthe canonical forms of polytopes, we construct a canonical form for any tope of\nan oriented matroid, inside the Orlik--Solomon algebra of the underlying\nmatroid. Using these canonical forms, we construct bases for the Orlik--Solomon\nalgebra of a matroid, and for the Aomoto cohomology. These bases of canonical\nforms are a foundational input in the theory of matroid amplitudes introduced\nby the second author.",
        "This paper concisely summarizes the XTS block encryption mode for storage\nsector-based encryption applications and clarifies its limitations. In\nparticular, we aim to provide a unified basis for much needed discussions about\nthe newly proposed key scope change to the IEEE 1619 standard.",
        "We consider multi-robot systems under recurring tasks formalized as linear\ntemporal logic (LTL) specifications. To solve the planning problem efficiently,\nwe propose a bottom-up approach combining offline plan synthesis with online\ncoordination, dynamically adjusting plans via real-time communication. To\naddress action delays, we introduce a synchronization mechanism ensuring\ncoordinated task execution, leading to a multi-agent coordination and\nsynchronization framework that is adaptable to a wide range of multi-robot\napplications. The software package is developed in Python and ROS2 for broad\ndeployment. We validate our findings through lab experiments involving nine\nrobots showing enhanced adaptability compared to previous methods.\nAdditionally, we conduct simulations with up to ninety agents to demonstrate\nthe reduced computational complexity and the scalability features of our work.",
        "Fine-tuning is an important step in adapting foundation models such as large\nlanguage models to downstream tasks. To make this step more accessible to users\nwith limited computational budgets, it is crucial to develop fine-tuning\nmethods that are memory and computationally efficient. Sparse Fine-tuning (SFT)\nand Low-rank adaptation (LoRA) are two frameworks that have emerged for\naddressing this problem and have been adopted widely in practice. In this work,\nwe develop a new SFT framework, based on ideas from neural network pruning. At\na high level, we first identify \"important\" neurons\/nodes using feature\nimportance metrics from network pruning (specifically, we use the structural\npruning method), and then perform fine-tuning by restricting to weights\ninvolving these neurons. Using experiments on common language tasks, we\ndemonstrate that our method significantly improves the memory efficiency of SFT\nwithout increasing training time complexity and implementation complexity,\nwhile achieving accuracy comparable to state-of-the-art methods such as LoRA\nand its variants.",
        "We construct a bijective correspondence between the set of rigid modules over\na gentle algebra and the set of admissible arc systems on the associated\ncoordinated-marked surface. In particular, a maximal rigid module aligns with\nan equivalence class of admissible $5$-partial triangulations, which is an\n(admissible) set of simple arcs dissecting the surface into $s$-gons with\n$3\\leqslant s\\leqslant 5$. Furthermore, the rank of the maximal rigid module is\nequal to the rank of the algebra plus the number of internal $4$-gons and\n$5$-gons in the associated $5$-partial triangulation.\n  Subsequently, these results facilitate an exploration of the higher\nAuslander-Reiten theory for gentle algebras with global dimension $n$. The\n$\\tau_m$-closures of injective modules are realized as admissible\n$(m+2)$-partial triangulations, where $\\tau_m$ are higher Auslander-Reiten\ntranslations with $2\\leqslant m \\leqslant n$. Finally, we provide a complete\nclassification of gentle algebras that are $\\tau_n$-finite or $n$-complete\nintroduced by Iyama [I11].",
        "We study the connection between $\\mathrm{SU}(n)$ spin chains and\none-dimensional sigma models on flag manifolds. Using this connection, we\ncalculate the spectrum of the Laplace-Beltrami operator and geodesics for a\nparticular class of metrics on $\\mathbb{CP}^1$ and $\\mathcal{F}_3$, which is a\nmanifold of complete flags in $\\mathbb{C}^3$.",
        "Large language models (LLMs) excel in tasks that require complex linguistic\nabilities, such as reference disambiguation and metaphor\nrecognition\/generation. Although LLMs possess impressive capabilities, their\ninternal mechanisms for processing and representing linguistic knowledge remain\nlargely opaque. Previous work on linguistic mechanisms has been limited by\ncoarse granularity, insufficient causal analysis, and a narrow focus. In this\nstudy, we present a systematic and comprehensive causal investigation using\nsparse auto-encoders (SAEs). We extract a wide range of linguistic features\nfrom six dimensions: phonetics, phonology, morphology, syntax, semantics, and\npragmatics. We extract, evaluate, and intervene on these features by\nconstructing minimal contrast datasets and counterfactual sentence datasets. We\nintroduce two indices-Feature Representation Confidence (FRC) and Feature\nIntervention Confidence (FIC)-to measure the ability of linguistic features to\ncapture and control linguistic phenomena. Our results reveal inherent\nrepresentations of linguistic knowledge in LLMs and demonstrate the potential\nfor controlling model outputs. This work provides strong evidence that LLMs\npossess genuine linguistic knowledge and lays the foundation for more\ninterpretable and controllable language modeling in future research.",
        "Spiking Neural Networks (SNNs) are highly efficient due to their spike-based\nactivation, which inherently produces bit-sparse computation patterns. Existing\nhardware implementations of SNNs leverage this sparsity pattern to avoid\nwasteful zero-value computations, yet this approach fails to fully capitalize\non the potential efficiency of SNNs. This study introduces a novel sparsity\nparadigm called Product Sparsity, which leverages combinatorial similarities\nwithin matrix multiplication operations to reuse the inner product result and\nreduce redundant computations. Product Sparsity significantly enhances sparsity\nin SNNs without compromising the original computation results compared to\ntraditional bit sparsity methods. For instance, in the SpikeBERT SNN model,\nProduct Sparsity achieves a density of only $1.23\\%$ and reduces computation by\n$11\\times$, compared to bit sparsity, which has a density of $13.19\\%$. To\nefficiently implement Product Sparsity, we propose Prosperity, an architecture\nthat addresses the challenges of identifying and eliminating redundant\ncomputations in real-time. Compared to prior SNN accelerator PTB and the A100\nGPU, Prosperity achieves an average speedup of $7.4\\times$ and $1.8\\times$,\nrespectively, along with energy efficiency improvements of $8.0\\times$ and\n$193\\times$, respectively. The code for Prosperity is available at\nhttps:\/\/github.com\/dubcyfor3\/Prosperity."
      ]
    }
  },
  {
    "id":2411.04682,
    "research_type":"applied",
    "start_id":"b8",
    "start_title":"Variable Imaging Projection Cloud Scattering Tomography",
    "start_abstract":"Scattering-based computed tomography (CT) recovers a heterogeneous volumetric scattering medium using images taken from multiple directions. It is nonlinear problem. Prior art mainly approached it by explicit physics-based optimization of image-fitting, being slow and difficult to scale. Scale particularly important when the objects constitute large cloud fields, where recovery for climate studies. Besides speed, imaging need be flexible, efficiently handle variable viewing geometries resolutions. These can caused perturbation in camera poses or fusion data different types observational sensors. There fast projection clouds (VIP-CT). We develop learning-based solution, deep-neural network (DNN) which trains on labeled dataset. The DNN parameters are oblivious domain scale, hence work with arbitrarily domains. VIP-CT offers much better quality than state art. inference speed flexibility make effectively real-time context spaceborne observations. paper first demonstrate CT real empirical directly DNN. may offer model solution problems other scientific Our code available online.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Distributed Sky Imaging Radiometry and Tomography"
      ],
      "abstract":[
        "The composition of the atmosphere is significant to our ecosystem. Accordingly, there a need sense distributions atmospheric scatterers such as aerosols and cloud droplets. There growing interest in recovering these scattering fields three-dimensions (3D). Even so, current observations usually use expensive unscalable equipment. Moreover, analysis retrieves partial information (e.g., cloud-base altitudes, water droplet size at tops) based on simplified 1D models. To advance retrievals, we develop new computational imaging approach for sensing analyzing atmosphere, volumetrically. Our comprises ground-based network cameras. We deployed it conjunction with additional remote equipment, including Raman lidar sunphotometer, which provide initialization algorithms ground truth. camera scalable, low cost, enables 3D high spatial temporal resolution. describe how system calibrated absolute radiometric readouts light field. Consequently, recover volumetric field scatterers, using tomography. tomography process adapted relative prior art, run large-scale domains being in-situ within scatterer fields. empirically demonstrate feasibility clouds, data."
      ],
      "categories":[
        "astro-ph.EP"
      ]
    },
    "list":{
      "title":[
        "Signless Laplacian State Transfer on Vertex Complemented Coronae",
        "Testing and Combining Transient Spectral Classification Tools on\n  4MOST-like Blended Spectra",
        "Highly Entangled Magnetodielectric and Magnetostriction effects, and\n  Spin-Phonon coupling in the Antiferromagnetic Ni$_2$ScSbO$_6$",
        "Sky localization of gravitational waves from eccentric binaries",
        "Mean-Field Analysis of Latent Variable Process Models on Dynamically\n  Evolving Graphs with Feedback Effects",
        "J-braid groups are torus necklace groups",
        "PoSSUM: A Protocol for Surveying Social-media Users with Multimodal LLMs",
        "Revealing higher-order neural representations with generative artificial\n  intelligence",
        "Are compact open-charm tetraquarks consistent with recent lattice\n  results?",
        "Partial Condition Numbers for Double Saddle Point Problems",
        "ZAPBench: A Benchmark for Whole-Brain Activity Prediction in Zebrafish",
        "Regularity for free boundary surfaces minimizing degenerate area\n  functionals",
        "A parameterization method for quasi-periodic systems with noise:\n  computation of random invariant tori",
        "Really perverse periodic solutions of the planar N-body problem",
        "Topological superconductivity in hourglass Dirac chain metals (Ti,\n  Hf)IrGe",
        "High-Dimensional Bayesian Optimization Using Both Random and Supervised\n  Embeddings",
        "Smooth Calabi-Yau varieties with large index and Betti numbers",
        "On 1-11-representability and multi-1-11-representability of graphs",
        "The Jordan decomposition and Kaplansky's second test problem for\n  Hermitian holomorphic vector bundles",
        "Detection of [C\\,{\\sc i}] Emission in Nebular Spectra of a Peculiar Type\n  Ia Supernova 2022pul",
        "Latent computing by biological neural networks: A dynamical systems\n  framework",
        "ALMAGAL III. Compact source catalog: Fragmentation statistics and\n  physical evolution of the core population",
        "On the harmonic generalized Cauchy-Kovalevskaya extension and its\n  connection with the Fueter-Sce theorem",
        "ADAGE: A generic two-layer framework for adaptive agent based modelling",
        "Overfitting Regimes of Nadaraya-Watson Interpolators",
        "The Paradox of Intervention: Resilience in Adaptive Multi-Role\n  Coordination Networks",
        "Motivic stable stems and Galois approximations of cellular motivic\n  categories",
        "Modeling Driver Behavior in Speed Advisory Systems: Koopman-based\n  Approach with Online Update",
        "Approximating the Total Variation Distance between Gaussians"
      ],
      "abstract":[
        "Given a graph $G$ with vertex set $V(G)=\\{v_1,v_2,\\ldots,v_{n_1}\\}$ and a\ngraph $H$ of order $n_2$, the vertex complemented corona, denoted by\n$G\\tilde{\\circ}{H}$, is the graph produced by copying $H$ $n_1$ times, with the\n$i$-th copy of $H$ corresponding to the vertex $v_i$, and then adding edges\nbetween any vertex in $V(G)\\setminus\\{v_{i}\\}$ and any vertex of the $i$-th\ncopy of $H$. The present article deals with quantum state transfer of vertex\ncomplemented coronae concerning signless Laplacian matrix. Our research\ninvestigates conditions in which signless Laplacian perfect state transfer\nexists or not on vertex complemented coronae. Additionally, we also provide\nsome mild conditions for the class of graphs under consideration that allow\nsignless Laplacian pretty good state transfer.",
        "With the 4-meter Multi-Object Spectroscopic Telescope (4MOST) expected to\nprovide an influx of transient spectra when it begins observations in early\n2026 we consider the potential for real-time classification of these spectra.\nWe investigate three extant spectroscopic transient classifiers: the Deep\nAutomated Supernova and Host classifier (DASH), Next Generation SuperFit (NGSF)\nand SuperNova IDentification (SNID), with a focus on comparing the efficiency\nand purity of the transient samples they produce. We discuss our method for\nsimulating realistic, 4MOST-like, host-galaxy contaminated spectra and\ndetermining quality cuts for each classifier used to ensure pure SN Ia samples\nwhile maintaining efficient classification in other transient classes. We\ninvestigate the classifiers individually and in combinations. We find that a\ncombination of DASH and NGSF can produce a SN Ia sample with a purity of 99.9%\nwhile successfully classifying 70% of SNe Ia. However, it struggles to classify\nnon-SN Ia transients. We investigate photometric cuts to transient magnitude\nand transient flux fraction, finding that both can be used to improve transient\nclassification efficiencies by 7--25% depending on the transient subclass.\nFinally, we present an example classification plan for live classification and\nthe predicted purities and efficiencies across five transient classes: Ia, Ibc,\nII, superluminous and non-supernova transients.",
        "Magnetic systems with noncentrosymmetric crystal structures are renowned for\ntheir complex magnetic ordering and diverse and fascinating physical\nproperties. In this report, we provide a comprehensive study of the chiral\nmagnetic system Ni$_2$ScSbO$_6$, which exhibits a robust incommensurate\nlong-range antiferromagnetic spin ordering at a temperature of $T_N = 62$~K, as\nrevealed by bulk magnetization, specific heat, and neutron diffraction studies.\nThis magnetic ordering triggers a series of intriguing phenomena, including\nprominent magnetodielectric coupling manifested by a dielectric peak at $T_N$,\nsignificant spin-phonon coupling resulting in strong phonon renormalization\ncharacterized by anomalous softening of various Raman modes, and a remarkable\nvolume magnetostriction effect probed by high-resolution synchrotron X-ray\ndiffraction. These phenomena are intricately interlinked, positioning the\npresent system as a rare and interesting material.",
        "We demonstrate that the orbital eccentricity in compact binary mergers can be\nused to improve their sky localization using gravitational wave observations.\nExisting algorithms that conduct the localizations are not optimized for\neccentric sources. We use a semi-Bayesian technique to carry out localizations\nof simulated sources recovered using a matched-filter search. Through these\nsimulations, we find that if a non-negligible eccentricity is obtained during\nthe detection, an eccentricity-optimized algorithm can significantly improve\nthe localization areas compared to the existing methods. We also lay out the\nfoundation for an eccentric early-warning system using the matched-filter\nsearch. The potential impact on the early-warning localization is investigated.\nWe indicate a few possible cases of improvements while accounting for\neccentricity toward any detectable eccentric neutron star binaries in the\nforthcoming observing scenarios of ground-based detectors. Improved\nlocalizations can be useful in effectually utilizing the capabilities of the\nfollow-up facilities.",
        "In this paper, we study the asymptotic behavior of a class of dynamic\nco-evolving latent space networks. The model we study is subject to\nbi-directional feedback effects, meaning that at any given time, the latent\nprocess depends on its own value and the graph structure at the previous time\nstep, and the graph structure at the current time depends on the value of the\nlatent processes at the current time but also on the graph structure at the\nprevious time instance (sometimes called a persistence effect). We construct\nthe mean-field limit of this model, which we use to characterize the limiting\nbehavior of a random sample taken from the latent space network in the limit as\nthe number of nodes in the network diverges. From this limiting model, we can\nderive the limiting behavior of the empirical measure of the latent process and\nestablish the related graphon limit of the latent particle network process. We\nalso provide a description of the rich conditional probabilistic structure of\nthe limiting model. The inherent dependence structure complicates the\nmathematical analysis significantly. In the process of proving our main\nresults, we derive a general conditional propagation of chaos result, which is\nof independent interest. In addition, our novel approach of studying the\nlimiting behavior of random samples proves to be a very useful methodology for\nfully grasping the asymptotic behavior of co-evolving particle systems.\nNumerical results are included to illustrate the theoretical findings.",
        "We construct a family of links we call torus necklaces for which the link\ngroups are precisely the braid groups of generalised $J$-reflection groups.\nMoreover, this correspondence exhibits the meridians of the aforementioned link\ngroups as braid reflections. In particular, this construction generalises to\nall irreducible rank two complex reflection groups a well-known correspondence\nbetween some rank two complex braid groups and some torus knot groups. In\naddition, as abstract groups, we show that the family of link groups associated\nto Seifert links coincides with the family of circular groups. This shows that\nevery time a link group has a non-trivial center, it is a Garside group.",
        "This paper introduces PoSSUM, an open-source protocol for unobtrusive polling\nof social-media users via multimodal Large Language Models (LLMs). PoSSUM\nleverages users' real-time posts, images, and other digital traces to create\nsilicon samples that capture information not present in the LLM's training\ndata. To obtain representative estimates, PoSSUM employs Multilevel Regression\nand Post-Stratification (MrP) with structured priors to counteract the\nobservable selection biases of social-media platforms. The protocol is\nvalidated during the 2024 U.S. Presidential Election, for which five PoSSUM\npolls were conducted and published on GitHub and X. In the final poll, fielded\nOctober 17-26 with a synthetic sample of 1,054 X users, PoSSUM accurately\npredicted the outcomes in 50 of 51 states and assigned the Republican candidate\na win probability of 0.65. Notably, it also exhibited lower state-level bias\nthan most established pollsters. These results demonstrate PoSSUM's potential\nas a fully automated, unobtrusive alternative to traditional survey methods.",
        "Studies often aim to reveal how neural representations encode aspects of an\nobserver's environment, such as its contents or structure. These are\n``first-order\" representations (FORs), because they're ``about\" the external\nworld. A less-common target is ``higher-order\" representations (HORs), which\nare ``about\" FORs -- their contents, stability, or uncertainty. HORs of\nuncertainty appear critically involved in adaptive behaviors including learning\nunder uncertainty, influencing learning rates and internal model updating based\non environmental feedback. However, HORs about uncertainty are unlikely to be\ndirect ``read-outs\" of FOR characteristics, instead reflecting estimation\nprocesses which may be lossy, bias-prone, or distortive and which may also\nincorporate estimates of distributions of uncertainty the observer is likely to\nexperience. While some research has targeted neural representations of\n``instantaneously\" estimated uncertainty, how the brain represents\n\\textit{distributions} of expected uncertainty remains largely unexplored.\nHere, we propose a novel reinforcement learning (RL) based generative\nartificial intelligence (genAI) approach to explore neural representations of\nuncertainty distributions. We use existing functional magnetic resonance\nimaging data, where humans learned to `de-noise' their brain states to achieve\ntarget neural patterns, to train denoising diffusion genAI models with RL\nalgorithms to learn noise distributions similar to how humans might learn to do\nthe same. We then explore these models' learned noise-distribution HORs\ncompared to control models trained with traditional backpropagation. Results\nreveal model-dependent differences in noise distribution representations --\nwith the RL-based model offering much higher explanatory power for human\nbehavior -- offering an exciting path towards using genAI to explore neural\nnoise-distribution HORs.",
        "We argue that the hypothesis that positive-parity charm meson resonances\nexhibit a compact tetraquark structure has some clear tension with recent\nlattice results for the $S$-wave $\\pi D$ system for an SU(3) flavor symmetric\nsetting. In particular, we show that such a diquark--anti-diquark tetraquark\nscenario would call for the presence of a state in the flavor\n$[{\\mathbf{\\overline{15}}}]$ representation, not seen in the lattice analysis.\nMoreover, we show that analogous lattice data in the axial-vector channel are\neven more sensitive to the internal structure of these very interesting states.",
        "This paper presents a unified framework for investigating the partial\ncondition number (CN) of the solution of double saddle point problems (DSPPs)\nand provides closed-form expressions for it. This unified framework encompasses\nthe well-known partial normwise CN (NCN), partial mixed CN (MCN) and partial\ncomponentwise CN (CCN) as special cases. Furthermore, we derive sharp upper\nbounds for the partial NCN, MCN and CCN, which are computationally efficient\nand free of expensive Kronecker products. By applying perturbations that\npreserve the structure of the block matrices of the DSPPs, we analyze the\nstructured partial NCN, MCN and CCN when the block matrices exhibit linear\nstructures. By leveraging the relationship between DSPP and equality\nconstrained indefinite least squares (EILS) problems, we recover the partial\nCNs for the EILS problem. Numerical results confirm the sharpness of the\nderived upper bounds and demonstrate their effectiveness in estimating the\npartial CNs.",
        "Data-driven benchmarks have led to significant progress in key scientific\nmodeling domains including weather and structural biology. Here, we introduce\nthe Zebrafish Activity Prediction Benchmark (ZAPBench) to measure progress on\nthe problem of predicting cellular-resolution neural activity throughout an\nentire vertebrate brain. The benchmark is based on a novel dataset containing\n4d light-sheet microscopy recordings of over 70,000 neurons in a larval\nzebrafish brain, along with motion stabilized and voxel-level cell\nsegmentations of these data that facilitate development of a variety of\nforecasting methods. Initial results from a selection of time series and\nvolumetric video modeling approaches achieve better performance than naive\nbaseline methods, but also show room for further improvement. The specific\nbrain used in the activity recording is also undergoing synaptic-level\nanatomical mapping, which will enable future integration of detailed structural\ninformation into forecasting methods.",
        "We establish an epsilon-regularity theorem at points in the free boundary of\nalmost-minimizers of the energy\n$\\mathrm{Per}_{w}(E)=\\int_{\\partial^*E}w\\,\\mathrm{d} {\\mathscr{H}}^{n-1}$,\nwhere $w$ is a weight asymptotic to $d(\\cdot,\\mathbb{R}^n\\setminus\\Omega)^a$\nnear $\\partial\\Omega$ and $a>0$.\n  This implies that the boundaries of almost-minimizers are\n$C^{1,\\gamma_0}$-surfaces that touch $\\partial \\Omega$ orthogonally, up to a\nSingular Set $\\mathrm{Sing}(\\partial E)$ whose Hausdorff dimension satisfies\nthe bound\n  $d_{\\mathscr{H}}(\\mathrm{Sing}(\\partial E)) \\leq n +a -(5+\\sqrt{8})$.",
        "This work is devoted to studying normally hyperbolic invariant manifolds\n(NHIMs) for a class of quasi-periodically forced systems subject to additional\nstochastic noise. These systems can be understood as skew-product systems. The\nexistence of NHIMs is established by developing a parameterization method in\nrandom settings and applying the Implicit Function Theorem in appropriate\nBanach spaces. Based on this, we propose a numerical algorithm to compute the\nstatistics of NHIMs and Lyapunov exponents.",
        "Examples are given of solutions of the planar N-body problem which remain the\nsame for at least two systems of masses with the same sum and same center of\nmass. The least value of N achieved up to now with this property is 474, a\nnumber which had been announced in the first author's thesis.",
        "Realizing topological superconductivity in stoichiometric materials is a key\nchallenge in condensed matter physics. Here, we report the discovery of ternary\ngermanide superconductors, $M$IrGe ($M$ = Ti, Hf), as prime candidates for\ntopological superconductivity, predicted to exhibit nonsymmorphic\nsymmetry-protected hourglass Dirac chains. Using comprehensive thermodynamic\nand muon-spin rotation\/relaxation ($\\mu$SR) measurements, we establish these\nmaterials as conventional bulk type-II superconductors with transition\ntemperatures of 2.24(5) K for TiIrGe and 5.64(4) K for HfIrGe, featuring a full\ngap and preserved time-reversal symmetry. First-principles calculations reveal\nstriking topological features in $M$IrGe, including hourglass-shaped bulk\ndispersions and a Dirac chain -- a ring of fourfold-degenerate Dirac points\nprotected by nonsymmorphic symmetry. Each Dirac point corresponds to the neck\nof the hourglass dispersion, while the Dirac chain gives rise to drumhead-like\nsurface states near the Fermi level. Additionally, nontrivial $\\mathbb{Z}_2$\ntopology leads to isolated Dirac surface states with helical spin textures that\ndisperse across the Fermi level, forming an ideal platform for\nproximity-induced topological superconductivity. The coexistence of\nconventional bulk superconductivity, symmetry-protected hourglass topology, and\nhelical spin-textured surface states establishes $M$IrGe as a rare and robust\nplatform to realize topological superconductivity, opening new avenues for\nnext-generation quantum technologies.",
        "Bayesian optimization (BO) is one of the most powerful strategies to solve\ncomputationally expensive-to-evaluate blackbox optimization problems. However,\nBO methods are conventionally used for optimization problems of small dimension\nbecause of the curse of dimensionality. In this paper, a high-dimensionnal\noptimization method incorporating linear embedding subspaces of small dimension\nis proposed to efficiently perform the optimization. An adaptive learning\nstrategy for these linear embeddings is carried out in conjunction with the\noptimization. The resulting BO method, named efficient global optimization\ncoupled with random and supervised embedding (EGORSE), combines in an adaptive\nway both random and supervised linear embeddings. EGORSE has been compared to\nstate-of-the-art algorithms and tested on academic examples with a number of\ndesign variables ranging from 10 to 600. The obtained results show the high\npotential of EGORSE to solve high-dimensional blackbox optimization problems,\nin terms of both CPU time and the limited number of calls to the expensive\nblackbox simulation.",
        "A normal variety $X$ is called Calabi-Yau if $K_X \\sim_{\\mathbb Q} 0$. The\nindex of $X$ is the smallest positive integer $m$ so that $m K_X \\sim 0$. We\nconstruct smooth, projective Calabi-Yau varieties in every dimension with\ndoubly exponentially growing index, which we conjecture to be maximal in every\ndimension. We also construct smooth, projective Calabi-Yau varieties with\nextreme topological invariants; namely, their Euler characteristics and the\nsums of their Betti numbers grow doubly exponentially. These are conjecturally\nextremal in every dimension. The varieties we construct are known in small\ndimensions but we believe them to be new in general. This work builds off of\nthe singular Calabi-Yau varieties found by Esser, Totaro, and Wang in\narXiv:2209.04597.",
        "Jeff Remmel introduced the concept of a $k$-11-representable graph in 2017.\nThis concept was first explored by Cheon et al. in 2019, who considered it as a\nnatural extension of word-representable graphs, which are exactly\n0-11-representable graphs. A graph $G$ is $k$-11-representable if it can be\nrepresented by a word $w$ such that for any edge (resp., non-edge) $xy$ in $G$\nthe subsequence of $w$ formed by $x$ and $y$ contains at most $k$ (resp., at\nleast $k+1$) pairs of consecutive equal letters. A remarkable result of Cheon\nat al. is that any graph is 2-11-representable, while it is still unknown\nwhether every graph is 1-11-representable. Cheon et al. showed that the class\nof 1-11-representable graphs is strictly larger than that of word-representable\ngraphs, and they introduced a useful toolbox to study 1-11-representable\ngraphs, which was extended by additional powerful tools suggested by Futorny et\nal. in 2024.\n  In this paper, we prove that all graphs on at most 8 vertices are\n1-11-representable hence extending the known fact that all graphs on at most 7\nvertices are 1-11-representable. Also, we discuss applications of our main\nresult in the study of multi-1-11-representation of graphs we introduce in this\npaper analogously to the notion of multi-word-representation of graphs\nsuggested by Kenkireth and Malhotra in 2023.",
        "In 1954, I. Kaplansky proposed three test problems for deciding the strength\nof structural understanding of a class of mathematical objects in his treatise\n\"Infinite abelian groups\", which can be formulated for very general\nmathematical systems. In this paper, we focus on Kaplansky's second test\nproblem in a context of complex geometry. Let $H^2_{\\beta}$ be a weighted Hardy\nspace. The Cowen-Douglas operator theory tells us that each\n$h\\in\\textrm{Hol}(\\overline{\\mathbb{D}})$ induces a Hermitian holomorphic\nvector bundle on $H^2_{\\beta}$, denoted by $E_{h(S_\\beta)}(\\Omega)$, where\n$\\Omega$ is a domain. We show that the vector bundle $E_{h(S_\\beta)}$ is a\npush-forwards Hermitian holomorphic vector bundle and study the similarity\ndeformation problems. Our main theorem is that if $H^2_{\\beta}$ is a weighted\nHardy space of polynomial growth, then for any $f\\in\n\\textrm{Hol}(\\overline{\\mathbb{D}})$, there exists a unique positive integer\n$m$ and an function $h\\in\\textrm{Hol}(\\overline{\\mathbb{D}})$ inducing an\nindecomposable vector bundle $E_{h(S_{\\beta})}$, such that $E_{f(S_\\beta)}$ is\nsimilar to $\\bigoplus_1^m E_{h(S_\\beta)}$, where $h$ is unique in the sense of\nanalytic automorphism group action. That could be seemed as a Jordan\ndecomposition theorem for the push-forwards Hermitian holomorphic vector\nbundles. Furthermore, we give the similarity classification of those\npush-forwards Hermitian holomorphic vector bundles induced by analytic\nfunctions, and give an affirmative answer to Kaplansky's second test problem\nfor those objects. We also give an affirmative answer to the geometric version\nand generalized version of a problem proposed by R. Douglas in 2007, and obtain\nthe $K_0$-group of the commutant algebra of a multiplication operator on a\nweighted Hardy space of polynomial growth. In addition, we give an example to\nshow the setting of polynomial growth condition is necessary.",
        "SN~2022pul gains special attention due to its possible origin of a\nsuper-Chandarsekhar-mass white dwarf explosion (or called a 03fg-like type Ia\nsupernova), which shows prominent [O\\,{\\sc i}], [Ne\\,{\\sc i}], and [Ca\\,{\\sc\nii}] lines in its late-time spectra taken at $\\sim+$300 days after the peak\nbrightness. In this paper, we present new optical observations for this\npeculiar object, extending up to over 500 days after the peak brightness. In\nparticular, in the $t\\approx+515$ days spectrum, we identified for the first\ntime the presence of narrow emission from [C\\,{\\sc i}] $\\lambda\\lambda9824,\n9850$, which appears asymmetric and quite similar to the accompanied [O\\,{\\sc\ni}] $\\lambda6300$ line in strength and profile. Based on the violent merger\nmodel that accounts well for previous observations but leaves little carbon in\nthe center of the ejecta, this carbon line can be reproduced by increasing the\ndegree of clumping in the ejecta and setting the carbon mass the same as that\nof oxygen ($\\sim$0.06 $M_{\\odot}$) in the innermost region ($\\lesssim 2000$ km\ns$^{-1}$). In principle, the central carbon could come from the secondary white\ndwarf (WD) if it is ignited when hit by the shockwave of the explosion of the\nprimary WD and explodes as a Ca-rich supernova, whereas pure deflagration of a\nsuper-Chandarsekhar-mass WD can account for such unburnt carbon more naturally.",
        "Although individual neurons and neural populations exhibit the phenomenon of\nrepresentational drift, perceptual and behavioral outputs of many neural\ncircuits can remain stable across time scales over which representational drift\nis substantial. These observations motivate a dynamical systems framework for\nneural network activity that focuses on the concept of \\emph{latent processing\nunits,} core elements for robust coding and computation embedded in collective\nneural dynamics. Our theoretical treatment of these latent processing units\nyields five key attributes of computing through neural network dynamics. First,\nneural computations that are low-dimensional can nevertheless generate\nhigh-dimensional neural dynamics. Second, the manifolds defined by neural\ndynamical trajectories exhibit an inherent coding redundancy as a direct\nconsequence of the universal computing capabilities of the underlying dynamical\nsystem. Third, linear readouts or decoders of neural population activity can\nsuffice to optimally subserve downstream circuits controlling behavioral\noutputs. Fourth, whereas recordings from thousands of neurons may suffice for\nnear optimal decoding from instantaneous neural activity patterns, experimental\naccess to millions of neurons may be necessary to predict neural ensemble\ndynamical trajectories across timescales of seconds. Fifth, despite the\nvariable activity of single cells, neural networks can maintain stable\nrepresentations of the variables computed by the latent processing units,\nthereby making computations robust to representational drift. Overall, our\nframework for latent computation provides an analytic description and\nempirically testable predictions regarding how large systems of neurons perform\nrobust computations via their collective dynamics.",
        "The mechanisms behind the fragmentation of high-mass dense clumps into\ncompact star-forming cores are fundamental topics in current astrophysical\nresearch. The ALMAGAL survey provides the opportunity to study this process at\nan unprecedented level of detail and statistical significance, featuring\nhigh-angular resolution $1.38$ mm ALMA observations of $1013$ massive dense\nclumps at various Galactic locations. These clumps cover a wide range of\ndistances, masses, surface densities, and evolutionary stages. Here, we present\nthe catalog of compact sources obtained with the CuTEx algorithm from continuum\nimages of the full ALMAGAL clump sample combining ACA-$7$m and $12$m ALMA\narrays, reaching a uniform high median spatial resolution of $\\sim1400$ au. We\ndiscuss the fragmentation properties and the estimated physical parameters of\nthe core population. The ALMAGAL compact source catalog includes $6348$ cores\ndetected in $844$ clumps ($83\\%$ of the total), with a number of cores per\nclump between $1$ and $49$ (median of $5$). The estimated core diameters are\nmostly within $\\sim800-3000$ au (median of $1700$ au). We obtained core masses\nfrom $0.002$ to $345\\,\\mathrm{M_{\\odot}}$. We evaluated the variation in the\ncore mass function (CMF) with evolution as traced by the clump $L\/M$, finding a\nclear, robust shift and change in slope among CMFs within subsamples at\ndifferent stages. This finding suggests that the CMF shape is not constant\nthroughout the star formation process, but rather it builds (and flattens) with\nevolution, with higher core masses reached at later stages. We found that all\ncores within a clump grow in mass on average with evolution, and the number of\ncores increases with the core masses. Our results favor a clump-fed scenario\nfor high-mass star formation, in which cores form as low-mass seeds, and then\ngain mass while further fragmentation occurs in the clump.",
        "One of the primary objectives of this paper is to establish a generalized\nCauchy-Kovalevskaya extension for axially harmonic functions. We demonstrate\nthat the result can be expressed as a power series involving Bessel-type\nfunctions of specific differential operators acting on two initial functions.\nAdditionally, we analyze the decomposition of the harmonic CK extension in\nterms of integrals over the sphere $ \\mathbb{S}^{m-1} $ involving functions of\nplane wave type.\n  Another key goal of this paper is to explore the relationship between the\nharmonic Cauchy-Kovalevskaya extension and the Fueter-Sce theorem. The\nFueter-Sce theorem outlines a two-step process for constructing axially\nmonogenic functions in $ \\mathbb{R}^{m+1}$ starting from holomorphic functions\nin one complex variable. The first step generates the class of slice monogenic\nfunctions, while the second step produces axially monogenic functions by\napplying the pointwise differential operator $\n\\Delta_{\\mathbb{R}{^{m+1}}}^{\\frac{m-1}{2}} $ with $m$ being odd, known as the\nFueter-Sce map, to a slice monogenic function.\n  By suitably factorizing the Fueter-Sce map, we introduce the set of axially\nharmonic functions, which serves as an intermediate class between slice\nmonogenic and axially monogenic functions. In this paper, we establish a\nconnection between the harmonic CK extension and the factorization of the\nFueter-Sce map. This connection leads to a new notion of harmonic polynomials,\nwhich we show to form a basis for the Riesz potential. Finally, we also\nconstruct a basis for the space of axially harmonic functions.",
        "Agent-based models (ABMs) are valuable for modelling complex, potentially\nout-of-equilibria scenarios. However, ABMs have long suffered from the Lucas\ncritique, stating that agent behaviour should adapt to environmental changes.\nFurthermore, the environment itself often adapts to these behavioural changes,\ncreating a complex bi-level adaptation problem. Recent progress integrating\nmulti-agent reinforcement learning into ABMs introduces adaptive agent\nbehaviour, beginning to address the first part of this critique, however, the\napproaches are still relatively ad hoc, lacking a general formulation, and\nfurthermore, do not tackle the second aspect of simultaneously adapting\nenvironmental level characteristics in addition to the agent behaviours. In\nthis work, we develop a generic two-layer framework for ADaptive AGEnt based\nmodelling (ADAGE) for addressing these problems. This framework formalises the\nbi-level problem as a Stackelberg game with conditional behavioural policies,\nproviding a consolidated framework for adaptive agent-based modelling based on\nsolving a coupled set of non-linear equations. We demonstrate how this generic\napproach encapsulates several common (previously viewed as distinct) ABM tasks,\nsuch as policy design, calibration, scenario generation, and robust behavioural\nlearning under one unified framework. We provide example simulations on\nmultiple complex economic and financial environments, showing the strength of\nthe novel framework under these canonical settings, addressing long-standing\ncritiques of traditional ABMs.",
        "In recent years, there has been much interest in understanding the\ngeneralization behavior of interpolating predictors, which overfit on noisy\ntraining data. Whereas standard analyses are concerned with whether a method is\nconsistent or not, recent observations have shown that even inconsistent\npredictors can generalize well. In this work, we revisit the classic\ninterpolating Nadaraya-Watson (NW) estimator (also known as Shepard's method),\nand study its generalization capabilities through this modern viewpoint. In\nparticular, by varying a single bandwidth-like hyperparameter, we prove the\nexistence of multiple overfitting behaviors, ranging non-monotonically from\ncatastrophic, through benign, to tempered. Our results highlight how even\nclassical interpolating methods can exhibit intricate generalization behaviors.\nNumerical experiments complement our theory, demonstrating the same phenomena.",
        "Complex adaptive networks exhibit remarkable resilience, driven by the\ndynamic interplay of structure (interactions) and function (state). While\nstatic-network analyses offer valuable insights, understanding how structure\nand function co-evolve under external interventions is critical for explaining\nsystem-level adaptation. Using a unique dataset of clandestine criminal\nnetworks, we combine empirical observations with computational modeling to test\nthe impact of various interventions on network adaptation. Our analysis\nexamines how networks with specialized roles adapt and form emergent structures\nto optimize cost-benefit trade-offs. We find that emergent sparsely connected\nnetworks exhibit greater resilience, revealing a security-efficiency trade-off.\nNotably, interventions can trigger a \"criminal opacity amplification\" effect,\nwhere criminal activity increases despite reduced network visibility. While\nnode isolation fragments networks, it strengthens remaining active ties. In\ncontrast, deactivating nodes (analogous to social reintegration) can\nunintentionally boost criminal coordination, increasing activity or\nconnectivity. Failed interventions often lead to temporary functional surges\nbefore reverting to baseline. Surprisingly, stimulating connectivity\ndestabilizes networks. Effective interventions require precise calibration to\nnode roles, connection types, and external conditions. These findings challenge\nconventional assumptions about connectivity and intervention efficacy in\ncomplex adaptive systems across diverse domains.",
        "We reconstruct (appropriately completed) categories of cellular motivic\nspectra over fields of small cohomological dimension in terms of only their\nabsolute Galois groups. As our main application, we determine the motivic\nstable stems (away from the characteristic) of almost all fields.",
        "Accurate driver behavior modeling is essential for improving the interaction\nand cooperation of the human driver with the driver assistance system. This\npaper presents a novel approach for modeling the response of human drivers to\nvisual cues provided by a speed advisory system using a Koopman-based method\nwith online updates. The proposed method utilizes the Koopman operator to\ntransform the nonlinear dynamics of driver-speed advisory system interactions\ninto a linear framework, allowing for efficient real-time prediction. An online\nupdate mechanism based on Recursive Least Squares (RLS) is integrated into the\nKoopman-based model to ensure continuous adaptation to changes in driver\nbehavior over time. The model is validated using data collected from a\nhuman-in-the-loop driving simulator, capturing diverse driver-specific\ntrajectories. The results demonstrate that the offline learned Koopman-based\nmodel can closely predict driver behavior and its accuracy is further enhanced\nthrough an online update mechanism with the RLS method.",
        "The total variation distance is a metric of central importance in statistics\nand probability theory. However, somewhat surprisingly, questions about\ncomputing it algorithmically appear not to have been systematically studied\nuntil very recently. In this paper, we contribute to this line of work by\nstudying this question in the important special case of multivariate Gaussians.\nMore formally, we consider the problem of approximating the total variation\ndistance between two multivariate Gaussians to within an $\\epsilon$-relative\nerror. Previous works achieved a fixed constant relative error approximation\nvia closed-form formulas. In this work, we give algorithms that given any two\n$n$-dimensional Gaussians $D_1,D_2$, and any error bound $\\epsilon > 0$,\napproximate the total variation distance $D := d_{TV}(D_1,D_2)$ to\n$\\epsilon$-relative accuracy in $\\text{poly}(n,\\frac{1}{\\epsilon},\\log\n\\frac{1}{D})$ operations. The main technical tool in our work is a reduction\nthat helps us extend the recent progress on computing the TV-distance between\ndiscrete random variables to our continuous setting."
      ]
    }
  },
  {
    "id":2411.03156,
    "research_type":"applied",
    "start_id":"b7",
    "start_title":"Physics guided deep learning for generative design of crystal materials with symmetry constraints",
    "start_abstract":"Abstract Discovering new materials is a challenging task in science crucial to the progress of human society. Conventional approaches based on experiments and simulations are labor-intensive or costly with success heavily depending experts\u2019 heuristic knowledge. Here, we propose deep learning Physics Guided Crystal Generative Model (PGCGM) for efficient crystal material design high structural diversity symmetry. Our model increases generation validity by more than 700% compared FTCP, one latest structure generators 45% our previous CubicGAN model. Density Functional Theory (DFT) calculations used validate generated structures 1869 out 2000 successfully optimized deposited into Carolina Materials Database www.carolinamatdb.org , which 39.6% have negative formation energy 5.3% energy-above-hull less 0.25 eV\/atom, indicating their thermodynamic stability potential synthesizability.",
    "start_categories":[
      "physics.comp-ph"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b6"
      ],
      "title":[
        "High\u2010Throughput Discovery of Novel Cubic Crystal Materials Using Deep Generative Neural Networks"
      ],
      "abstract":[
        "Abstract High\u2010throughput screening has become one of the major strategies for discovery novel functional materials. However, its effectiveness is severely limited by lack sufficient and diverse materials in current repositories such as open quantum database (OQMD). Recent progress deep learning have enabled generative that learn implicit chemical rules creating hypothetical with new compositions structures. models difficulty generating structurally diverse, chemically valid, stable Here we propose CubicGAN, a adversarial network (GAN) based neural model large scale design cubic When trained on 375 749 ternary from OQMD database, authors show able to not only rediscover most currently known but also generate structure prototypes. A total 506 been verified phonon dispersion calculation. Considering importance wide applications solar panels, GAN provides promising approach significantly expand existing repositories, enabling via screening. The crystal structures discovered are freely accessible at www.carolinamatdb.org ."
      ],
      "categories":[
        "cs.NE"
      ]
    },
    "list":{
      "title":[
        "Sovereign Debt Default and Climate Risk",
        "GFlowVLM: Enhancing Multi-step Reasoning in Vision-Language Models with\n  Generative Flow Networks",
        "Mathematical modelling and homogenization of thin fiber-reinforced\n  hydrogels",
        "Training Dynamics of In-Context Learning in Linear Attention",
        "A new fuzzy fractional differential variational inequality with\n  Mittag-Leffler kernel of order $q \\in (1,2]$",
        "Surface Diagrams for Frobenius Algebras and Frobenius-Schur Indicators\n  in Grothendieck-Verdier Categories",
        "Technical Note: Targeted Maximum Likelihood Estimator for an ATE\n  Standardized for New Target Population",
        "Temporal-Guided Spiking Neural Networks for Event-Based Human Action\n  Recognition",
        "SoK: A Review of Cross-Chain Bridge Hacks in 2023",
        "O-RIS-ing: Evaluating RIS-Assisted NextG Open RAN",
        "Algebraization of rigid analytic varieties and formal schemes via\n  perfect complexes",
        "Decision from Suboptimal Classifiers: Excess Risk Pre- and\n  Post-Calibration",
        "Enhancing Large Language Model Efficiencyvia Symbolic Compression: A\n  Formal Approach Towards Interpretability",
        "Improved quasi-invariance result for the periodic Benjamin-Ono-BBM\n  equation",
        "The dynamics of meaning through time: Assessment of Large Language\n  Models",
        "SPARC: Score Prompting and Adaptive Fusion for Zero-Shot Multi-Label\n  Recognition in Vision-Language Models",
        "Cluster weighted models for functional data",
        "Quantization dimension for a generalized inhomogeneous bi-Lipschitz\n  iterated function system",
        "Preventing Rogue Agents Improves Multi-Agent Collaboration",
        "Smell of Source: Learning-Based Odor Source Localization with Molecular\n  Communication",
        "EvoP: Robust LLM Inference via Evolutionary Pruning",
        "PerCul: A Story-Driven Cultural Evaluation of LLMs in Persian",
        "Modeling ice cliff stability using a new Mohr-Coulomb-based phase field\n  fracture model",
        "A Novel Approach to Network Traffic Analysis: the HERA tool",
        "Modern Models, Medieval Texts: A POS Tagging Study of Old Occitan",
        "C2D-ISR: Optimizing Attention-based Image Super-resolution from\n  Continuous to Discrete Scales",
        "Supervised Quadratic Feature Analysis: An Information Geometry Approach\n  to Dimensionality Reduction",
        "A note on the physical interpretation of neural PDE's",
        "Voting or Consensus? Decision-Making in Multi-Agent Debate"
      ],
      "abstract":[
        "We explore the interplay between sovereign debt default\/renegotiation and\nenvironmental factors (e.g., pollution from land use, natural resource\nexploitation). Pollution contributes to the likelihood of natural disasters and\ninfluences economic growth rates. The country can default on its debt at any\ntime while also deciding whether to invest in pollution abatement. The\nframework provides insights into the credit spreads of sovereign bonds and\nexplains the observed relationship between bond spread and a country's climate\nvulnerability. Through calibration for developing and low-income countries, we\ndemonstrate that there is limited incentive for these countries to address\nclimate risk, and the sensitivity of bond spreads to climate vulnerability\nremains modest. Climate risk does not play a relevant role on the decision to\ndefault on sovereign debt. Financial support for climate abatement expenditures\ncan effectively foster climate adaptation actions, instead renegotiation\nconditional upon pollution abatement does not produce any effect.",
        "Vision-Language Models (VLMs) have recently shown promising advancements in\nsequential decision-making tasks through task-specific fine-tuning. However,\ncommon fine-tuning methods, such as Supervised Fine-Tuning (SFT) and\nReinforcement Learning (RL) techniques like Proximal Policy Optimization (PPO),\npresent notable limitations: SFT assumes Independent and Identically\nDistributed (IID) data, while PPO focuses on maximizing cumulative rewards.\nThese limitations often restrict solution diversity and hinder generalization\nin multi-step reasoning tasks. To address these challenges, we introduce a\nnovel framework, GFlowVLM, a framework that fine-tune VLMs using Generative\nFlow Networks (GFlowNets) to promote generation of diverse solutions for\ncomplex reasoning tasks. GFlowVLM models the environment as a non-Markovian\ndecision process, allowing it to capture long-term dependencies essential for\nreal-world applications. It takes observations and task descriptions as inputs\nto prompt chain-of-thought (CoT) reasoning which subsequently guides action\nselection. We use task based rewards to fine-tune VLM with GFlowNets. This\napproach enables VLMs to outperform prior fine-tuning methods, including SFT\nand RL. Empirical results demonstrate the effectiveness of GFlowVLM on complex\ntasks such as card games (NumberLine, BlackJack) and embodied planning tasks\n(ALFWorld), showing enhanced training efficiency, solution diversity, and\nstronger generalization capabilities across both in-distribution and\nout-of-distribution scenarios.",
        "This work considers simultaneous homogenization dimension reduction of a\nporoelastic model for thin fiber-reinforced hydrogels. The analysed medium is\ndefined as a two-component system consisting of a continuous fiber framework\nwith hydrogel inclusions arranged periodically throughout. The fibers are\nassumed to operate under quasi-stationary linear elasticity, whereas the\nhydrogel's hydromechanical behavior is represented using Biot's linear\nporoelasticity model. The asymptotic limit of the coupled system is established\nwhen the periodicity and thickness parameters are of the same order and tend to\nzero simultaneously, utilizing the re-scaling unfolding operator. It is\ndemonstrated that the limit displacement exhibits Kirchhoff-Love-type behavior\nthrough Griso's decomposition of plate displacements. Towards the end, a unique\nsolution for the macroscopic problem has been demonstrated.",
        "While attention-based models have demonstrated the remarkable ability of\nin-context learning, the theoretical understanding of how these models acquired\nthis ability through gradient descent training is still preliminary. Towards\nanswering this question, we study the gradient descent dynamics of multi-head\nlinear self-attention trained for in-context linear regression. We examine two\nparametrizations of linear self-attention: one with the key and query weights\nmerged as a single matrix (common in theoretical studies), and one with\nseparate key and query matrices (closer to practical settings). For the merged\nparametrization, we show the training dynamics has two fixed points and the\nloss trajectory exhibits a single, abrupt drop. We derive an analytical\ntime-course solution for a certain class of datasets and initialization. For\nthe separate parametrization, we show the training dynamics has exponentially\nmany fixed points and the loss exhibits saddle-to-saddle dynamics, which we\nreduce to scalar ordinary differential equations. During training, the model\nimplements principal component regression in context with the number of\nprincipal components increasing over training time. Overall, we characterize\nhow in-context learning abilities evolve during gradient descent training of\nlinear attention, revealing dynamics of abrupt acquisition versus progressive\nimprovements in models with different parametrizations.",
        "This paper considers a new fuzzy fractional differential variational\ninequality with Mittag-Leffler kernel of order $q \\in (1,2]$ comprising a fuzzy\nfractional differential inclusion with Mittag-Leffler kernel of order $q \\in\n(1,2]$ and a variational inequality in Euclidean spaces. The existence of\nsolutions for such a novel system is obtained under some mild conditions.",
        "Grothendieck-Verdier categories (also known as $\\ast$-autonomous categories)\ngeneralize rigid monoidal categories, with notable representation-theoretic\nexamples including categories of bimodules, modules over Hopf algebroids, and\nmodules over vertex operator algebras.\n  In this paper, we develop a surface-diagrammatic calculus for\nGrothendieck-Verdier categories, extending the string-diagrammatic calculus of\nJoyal and Street for rigid monoidal categories into a third dimension. This\nextension naturally arises from the non-invertibility of coherence data in\nGrothendieck-Verdier categories.\n  We show that key properties of Frobenius algebras in rigid monoidal\ncategories carry over to the Grothendieck-Verdier setting. Moreover, we\nintroduce higher Frobenius-Schur indicators for suitably finite $k$-linear\npivotal Grothendieck-Verdier categories and prove their invariance under\npivotal Frobenius linearly distributive equivalences.\n  The proofs are carried out using the surface-diagrammatic calculus. To\nfacilitate the verification of some of our results, we provide auxiliary files\nfor the graphical proof assistant homotopy.io.",
        "In this technical note we present a targeted maximum likelihood estimator\n(TMLE) for a previously studied target parameter that aims to transport an\naverage treatment effect (ATE) on a clinical outcome in a source population to\nwhat the ATE would have been in another target population. It is assumed that\none only observes baseline covariates in the target population, while we assume\nthat one can learn the conditional treatment effect on the outcome of interest\nin the source population. We also allow that one might observe only a subset of\nthe covariates in the target population while all covariates are measured in\nthe source population. We consider the case that the outcome is a clinical\noutcome at some future time point that is subject to missingness, or that our\noutcome of interest is a time to event that is subject to right-censoring. We\nderive the canonical gradients and present the corresponding TMLEs for these\ntwo cases.",
        "This paper explores the promising interplay between spiking neural networks\n(SNNs) and event-based cameras for privacy-preserving human action recognition\n(HAR). The unique feature of event cameras in capturing only the outlines of\nmotion, combined with SNNs' proficiency in processing spatiotemporal data\nthrough spikes, establishes a highly synergistic compatibility for event-based\nHAR. Previous studies, however, have been limited by SNNs' ability to process\nlong-term temporal information, essential for precise HAR. In this paper, we\nintroduce two novel frameworks to address this: temporal segment-based SNN\n(\\textit{TS-SNN}) and 3D convolutional SNN (\\textit{3D-SNN}). The\n\\textit{TS-SNN} extracts long-term temporal information by dividing actions\ninto shorter segments, while the \\textit{3D-SNN} replaces 2D spatial elements\nwith 3D components to facilitate the transmission of temporal information. To\npromote further research in event-based HAR, we create a dataset,\n\\textit{FallingDetection-CeleX}, collected using the high-resolution CeleX-V\nevent camera $(1280 \\times 800)$, comprising 7 distinct actions. Extensive\nexperimental results show that our proposed frameworks surpass state-of-the-art\nSNN methods on our newly collected dataset and three other neuromorphic\ndatasets, showcasing their effectiveness in handling long-range temporal\ninformation for event-based HAR.",
        "Blockchain technology has revolutionized industries by enabling secure and\ndecentralized transactions. However, the isolated nature of blockchain\necosystems hinders the seamless transfer of digital assets across different\nchains. Cross-chain bridges have emerged as vital web3 infrastructure to\naddress this challenge by facilitating interoperability between distinct\nblockchains. Cross-chain bridges remain vulnerable to various attacks despite\nsophisticated designs and security measures. The industry has experienced a\nsurge in bridge attacks, resulting in significant financial losses. The largest\nhack impacted Axie Infinity Ronin Bridge, with a loss of almost \\$600 million\nUSD. This paper analyzes recent cross-chain bridge hacks in 2022 and 2023 and\nexamines the exploited vulnerabilities. By understanding the attack nature and\nunderlying weaknesses, the paper aims to enhance bridge security and propose\npotential countermeasures. The findings contribute to developing industry-wide\nstandards for bridge security and operational resilience. Addressing the\nvulnerabilities and weaknesses exploited in recent cross-chain bridge hacks\nfosters trust and confidence in cross-chain interoperability.",
        "Reconfigurable Intelligent Surfaces (RISs) pose as a transformative\ntechnology to revolutionize the cellular architecture of Next Generation\n(NextG) Radio Access Networks (RANs). Previous studies have demonstrated the\ncapabilities of RISs in optimizing wireless propagation, achieving high\nspectral efficiency, and improving resource utilization. At the same time, the\ntransition to softwarized, disaggregated, and virtualized architectures, such\nas those being standardized by the O-RAN ALLIANCE, enables the vision of a\nreconfigurable Open RAN. In this work, we aim to integrate these technologies\nby studying how different resource allocation policies enhance the performance\nof RIS-assisted Open RANs. We perform a comparative analysis among various\nnetwork configurations and show how proper network optimization can enhance the\nperformance across the Enhanced Mobile Broadband (eMBB) and Ultra Reliable and\nLow Latency Communications (URLLC) network slices, achieving up to ~34%\nthroughput improvement. Furthermore, leveraging the capabilities of OpenRAN\nGym, we deploy an xApp on Colosseum, the world's largest wireless system\nemulator with hardware-in-the-loop, to control the Base Station (BS)'s\nscheduling policy. Experimental results demonstrate that RIS-assisted\ntopologies achieve high resource efficiency and low latency, regardless of the\nBS's scheduling policy.",
        "In this paper, we extend a theorem of To\\\"en and Vaqui\\'e to the\nnon-Archimedean and formal settings. More precisely, we prove that a smooth and\nproper rigid analytic variety is algebraizable if and only if its category of\nperfect complexes is smooth and proper. As a corollary, we deduce an analogous\nstatement for formal schemes and demonstrate that, in general, the bounded\nderived category of coherent sheaves on a formal scheme is not smooth.",
        "Probabilistic classifiers are central for making informed decisions under\nuncertainty. Based on the maximum expected utility principle, optimal decision\nrules can be derived using the posterior class probabilities and\nmisclassification costs. Yet, in practice only learned approximations of the\noracle posterior probabilities are available. In this work, we quantify the\nexcess risk (a.k.a. regret) incurred using approximate posterior probabilities\nin batch binary decision-making. We provide analytical expressions for\nmiscalibration-induced regret ($R^{\\mathrm{CL}}$), as well as tight and\ninformative upper and lower bounds on the regret of calibrated classifiers\n($R^{\\mathrm{GL}}$). These expressions allow us to identify regimes where\nrecalibration alone addresses most of the regret, and regimes where the regret\nis dominated by the grouping loss, which calls for post-training beyond\nrecalibration. Crucially, both $R^{\\mathrm{CL}}$ and $R^{\\mathrm{GL}}$ can be\nestimated in practice using a calibration curve and a recent grouping loss\nestimator. On NLP experiments, we show that these quantities identify when the\nexpected gain of more advanced post-training is worth the operational cost.\nFinally, we highlight the potential of multicalibration approaches as efficient\nalternatives to costlier fine-tuning approaches.",
        "Large language models (LLMs) face significant token efficiency bottlenecks in\ncode generation and logical reasoning tasks, a challenge that directly impacts\ninference cost and model interpretability. This paper proposes a formal\nframework based on symbolic compression,integrating combinatory logic,\ninformation-theoretic optimal encoding, and context-aware inference techniques\nto achieve a step-change improvement in token efficiency while preserving\nsemantic integrity. We establish a mathematical framework within a functional\nprogramming paradigm, derive the quantitative relationship between symbolic\ndensity and model interpretability, and propose a differentiable compression\nfactor metric to evaluate encoding efficiency. Furthermore, we leverage\nparameter-efficient fine-tuning (PEFT) techniques to achieve a low-cost\napplication of the GAEL language. Experimental results show that this method\nachieves a 78.3% token compression rate in code generation tasks while\nimproving logical traceability by 62% through structural explicitness. This\nresearch provides new theoretical tools for efficient inference in LLMs and\nopens a symbolic path for modelinterpretability research.",
        "We extend recent results of Genovese-Luca-Tzvetkov (2022) regarding the\nquasi-invariance of Gaussian measures under the flow of the periodic\nBenjamin-Ono-BBM (BO-BBM) equation to the full range where BO-BBM is globally\nwell-posed. The main difficulty is due to the critical nature of the dispersion\nwhich we overcome by combining the approach of Coe-Tolomeo (2024) with an\niteration argument due to Forlano-Tolomeo (2024) to obtain long-time higher\nintegrability bounds on the transported density.",
        "Understanding how large language models (LLMs) grasp the historical context\nof concepts and their semantic evolution is essential in advancing artificial\nintelligence and linguistic studies. This study aims to evaluate the\ncapabilities of various LLMs in capturing temporal dynamics of meaning,\nspecifically how they interpret terms across different time periods. We analyze\na diverse set of terms from multiple domains, using tailored prompts and\nmeasuring responses through both objective metrics (e.g., perplexity and word\ncount) and subjective human expert evaluations. Our comparative analysis\nincludes prominent models like ChatGPT, GPT-4, Claude, Bard, Gemini, and Llama.\nFindings reveal marked differences in each model's handling of historical\ncontext and semantic shifts, highlighting both strengths and limitations in\ntemporal semantic understanding. These insights offer a foundation for refining\nLLMs to better address the evolving nature of language, with implications for\nhistorical text analysis, AI design, and applications in digital humanities.",
        "Zero-shot multi-label recognition (MLR) with Vision-Language Models (VLMs)\nfaces significant challenges without training data, model tuning, or\narchitectural modifications. Existing approaches require prompt tuning or\narchitectural adaptations, limiting zero-shot applicability. Our work proposes\na novel solution treating VLMs as black boxes, leveraging scores without\ntraining data or ground truth. Using large language model insights on object\nco-occurrence, we introduce compound prompts grounded in realistic object\ncombinations. Analysis of these prompt scores reveals VLM biases and\n``AND''\/``OR'' signal ambiguities, notably that maximum compound scores are\nsurprisingly suboptimal compared to second-highest scores. We address these\nthrough a debiasing and score-fusion algorithm that corrects image bias and\nclarifies VLM response behaviors. Our method enhances other zero-shot\napproaches, consistently improving their results. Experiments show superior\nmean Average Precision (mAP) compared to methods requiring training data,\nachieved through refined object ranking for robust zero-shot MLR.",
        "We propose a method, funWeightClust, based on a family of parsimonious models\nfor clustering heterogeneous functional linear regression data. These models\nextend cluster weighted models to functional data, and they allow for\nmultivariate functional responses and predictors. The proposed methodology\nfollows the approach used by the the functional high dimensional data\nclustering (funHDDC) method. We construct an expectation maximization (EM)\nalgorithm for parameter estimation. Using simulated and benchmark data we show\nthat funWeightClust outperforms funHDDC and several two-steps clustering\nmethods. We also use funWeightClust to analyze traffic patterns in Edmonton,\nCanada.",
        "For a given $r\\in (0, +\\infty)$, the quantization dimension of order $r$, if\nit exists, denoted by $D_r(\\mu)$, of a Borel probability measure $\\mu$ on\n${\\mathbb R}^d$ represents the speed how fast the $n$th quantization error of\norder $r$ approaches to zero as the number of elements $n$ in an optimal set of\n$n$-means for $\\mu$ tends to infinity. If $D_r(\\mu)$ does not exists, we call\n$\\underline D_r(\\mu)$ and $\\overline D_r(\\mu)$, the lower and upper\nquantization dimensions of $\\mu$ of order $r$. In this paper, we estimate the\nquantization dimension of condensation measures associated with condensation\nsystems $(\\{f_i\\}_{i=1}^N, (p_i)_{i=0}^N, \\nu)$, where the mappings $f_i$ are\nbi-Lipschitz and the measure $\\nu$ is an image measure of an ergodic measure\nwith bounded distortion supported on a conformal set. In addition, we determine\nthe optimal quantization for an infinite discrete distribution, and give an\nexample which shows that the quantization dimension of a Borel probability\nmeasure can be positive with zero quantization coefficient.",
        "Multi-agent systems, where specialized agents collaborate to solve a shared\ntask hold great potential, from increased modularity to simulating complex\nenvironments. However, they also have a major caveat -- a single agent can\ncause the entire system to fail. Consider a simple game where the knowledge to\nsolve the task is distributed between agents, which share information in a\ncommunication channel. At each round, any of the agents can terminate the game\nand make the final prediction, even if they are uncertain about the outcome of\ntheir action. Detection of such rogue agents $\\textit{before they act}$ may\nprevent the system's failure. In this work, we propose to $\\textit{monitor}$\nagents during action prediction and $\\textit{intervene}$ when a future error is\nlikely to occur. To test our approach, we introduce WhoDunitEnv, a multi-agent\ncollaboration environment that allows modular control over task complexity and\ncommunication structure. Experiments on two variants of WhoDunitEnv and the\nGovSim environment for resource sustainability show that our approach leads to\nsubstantial performance gains up to 17.4% and 20%, respectively. Moreover, a\nthorough analysis shows that our monitors successfully identify critical points\nof agent confusion and our interventions effectively stop agent errors from\npropagating.",
        "Odor source localization is a fundamental challenge in molecular\ncommunication, environmental monitoring, disaster response, industrial safety,\nand robotics. In this study, we investigate three major approaches: Bayesian\nfiltering, machine learning (ML) models, and physics-informed neural networks\n(PINNs) with the aim of odor source localization in a single-source,\nsingle-molecule case. By considering the source-sensor architecture as a\ntransmitter-receiver model we explore source localization under the scope of\nmolecular communication. Synthetic datasets are generated using a 2D\nadvection-diffusion PDE solver to evaluate each method under varying\nconditions, including sensor noise and sparse measurements. Our experiments\ndemonstrate that \\textbf{Physics-Informed Neural Networks (PINNs)} achieve the\nlowest localization error of \\(\\mathbf{0.89 \\times 10^{-6}}\\) m, outperforming\n\\textbf{machine learning (ML) inversion} (\\(\\mathbf{1.48 \\times 10^{-6}}\\) m)\nand \\textbf{Kalman filtering} (\\(\\mathbf{1.62 \\times 10^{-6}}\\) m). The\n\\textbf{reinforcement learning (RL)} approach, while achieving a localization\nerror of \\(\\mathbf{3.01 \\times 10^{-6}}\\) m, offers an inference time of\n\\(\\mathbf{0.147}\\) s, highlighting the trade-off between accuracy and\ncomputational efficiency among different methodologies.",
        "Large Language Models (LLMs) have achieved remarkable success in natural\nlanguage processing tasks, but their massive size and computational demands\nhinder their deployment in resource-constrained environments. Existing\nstructured pruning methods address this issue by removing redundant structures\n(e.g., elements, channels, layers) from the model. However, these methods\nemploy a heuristic pruning strategy, which leads to suboptimal performance.\nBesides, they also ignore the data characteristics when pruning the model.\n  To overcome these limitations, we propose EvoP, an evolutionary pruning\nframework for robust LLM inference. EvoP first presents a cluster-based\ncalibration dataset sampling (CCDS) strategy for creating a more diverse\ncalibration dataset. EvoP then introduces an evolutionary pruning pattern\nsearching (EPPS) method to find the optimal pruning pattern. Compared to\nexisting structured pruning techniques, EvoP achieves the best performance\nwhile maintaining the best efficiency. Experiments across different LLMs and\ndifferent downstream tasks validate the effectiveness of the proposed EvoP,\nmaking it a practical and scalable solution for deploying LLMs in real-world\napplications.",
        "Large language models predominantly reflect Western cultures, largely due to\nthe dominance of English-centric training data. This imbalance presents a\nsignificant challenge, as LLMs are increasingly used across diverse contexts\nwithout adequate evaluation of their cultural competence in non-English\nlanguages, including Persian. To address this gap, we introduce PerCul, a\ncarefully constructed dataset designed to assess the sensitivity of LLMs toward\nPersian culture. PerCul features story-based, multiple-choice questions that\ncapture culturally nuanced scenarios. Unlike existing benchmarks, PerCul is\ncurated with input from native Persian annotators to ensure authenticity and to\nprevent the use of translation as a shortcut. We evaluate several\nstate-of-the-art multilingual and Persian-specific LLMs, establishing a\nfoundation for future research in cross-cultural NLP evaluation. Our\nexperiments demonstrate a 11.3% gap between best closed source model and\nlayperson baseline while the gap increases to 21.3% by using the best\nopen-weight model. You can access the dataset from here:\nhttps:\/\/huggingface.co\/datasets\/teias-ai\/percul",
        "Iceberg calving at glacier termini results in mass loss from ice sheets, but\nthe associated fracture mechanics is often poorly represented using simplistic\n(empirical or elementary mechanics-based) failure criteria. Here, we propose an\nadvanced Mohr-Coulomb failure criterion that drives cracking based on the\nvisco-elastic stress state in ice. This criterion is implemented in a phase\nfield fracture framework, and finite element simulations are conducted to\ndetermine the critical conditions that can trigger ice cliff collapse. Results\ndemonstrate that fast-moving glaciers with negligible basal friction are prone\nto tensile failure causing crevasse propagation far away from the ice front;\nwhilst slow-moving glaciers with significant basal friction are likely to\nexhibit shear failure near the ice front. Results also indicate that seawater\npressure plays a major role in modulating cliff failure. For land terminating\nglaciers, full thickness cliff failure is observed if the glacier exceeds a\ncritical height, dependent on cohesive strength $\\tau_\\mathrm{c}$ ($H \\approx\n120\\;\\text{m}$ for $\\tau_\\mathrm{c}=0.5\\;\\text{MPa}$). For marine-terminating\nglaciers, ice cliff failure occurs if a critical glacier free-board\n($H-h_\\mathrm{w}$) is exceeded, with ice slumping only observed above the\nocean-water height; for $\\tau_\\mathrm{c} = 0.5\\;\\text{MPa}$, the\nmodel-predicted critical free-board is $H-h_\\mathrm{w} \\approx 215\\;\\text{m}$,\nwhich is in good agreement with field observations. While the critical\nfree-board height is larger than that predicted by some previous models, we\ncannot conclude that marine ice cliff instability is less likely because we do\nnot include other failure processes such as hydrofracture of basal crevasses\nand plastic necking.",
        "Cybersecurity threats highlight the need for robust network intrusion\ndetection systems to identify malicious behaviour. These systems rely heavily\non large datasets to train machine learning models capable of detecting\npatterns and predicting threats. In the past two decades, researchers have\nproduced a multitude of datasets, however, some widely utilised recent datasets\ngenerated with CICFlowMeter contain inaccuracies. These result in flow\ngeneration and feature extraction inconsistencies, leading to skewed results\nand reduced system effectiveness. Other tools in this context lack ease of use,\ncustomizable feature sets, and flow labelling options. In this work, we\nintroduce HERA, a new open-source tool that generates flow files and labelled\nor unlabelled datasets with user-defined features. Validated and tested with\nthe UNSW-NB15 dataset, HERA demonstrated accurate flow and label generation.",
        "Large language models (LLMs) have demonstrated remarkable capabilities in\nnatural language processing, yet their effectiveness in handling historical\nlanguages remains largely unexplored. This study examines the performance of\nopen-source LLMs in part-of-speech (POS) tagging for Old Occitan, a historical\nlanguage characterized by non-standardized orthography and significant\ndiachronic variation. Through comparative analysis of two distinct\ncorpora-hagiographical and medical texts-we evaluate how current models handle\nthe inherent challenges of processing a low-resource historical language. Our\nfindings demonstrate critical limitations in LLM performance when confronted\nwith extreme orthographic and syntactic variability. We provide detailed error\nanalysis and specific recommendations for improving model performance in\nhistorical language processing. This research advances our understanding of LLM\ncapabilities in challenging linguistic contexts while offering practical\ninsights for both computational linguistics and historical language studies.",
        "In recent years, attention mechanisms have been exploited in single image\nsuper-resolution (SISR), achieving impressive reconstruction results. However,\nthese advancements are still limited by the reliance on simple training\nstrategies and network architectures designed for discrete up-sampling scales,\nwhich hinder the model's ability to effectively capture information across\nmultiple scales. To address these limitations, we propose a novel framework,\n\\textbf{C2D-ISR}, for optimizing attention-based image super-resolution models\nfrom both performance and complexity perspectives. Our approach is based on a\ntwo-stage training methodology and a hierarchical encoding mechanism. The new\ntraining methodology involves continuous-scale training for discrete scale\nmodels, enabling the learning of inter-scale correlations and multi-scale\nfeature representation. In addition, we generalize the hierarchical encoding\nmechanism with existing attention-based network structures, which can achieve\nimproved spatial feature fusion, cross-scale information aggregation, and more\nimportantly, much faster inference. We have evaluated the C2D-ISR framework\nbased on three efficient attention-based backbones, SwinIR-L, SRFormer-L and\nMambaIRv2-L, and demonstrated significant improvements over the other existing\noptimization framework, HiT, in terms of super-resolution performance (up to\n0.2dB) and computational complexity reduction (up to 11%). The source code will\nbe made publicly available at www.github.com.",
        "Supervised dimensionality reduction aims to map labeled data to a\nlow-dimensional feature space while maximizing class discriminability. Despite\nthe availability of methods for learning complex non-linear features (e.g. Deep\nLearning), there is an enduring demand for dimensionality reduction methods\nthat learn linear features due to their interpretability, low computational\ncost, and broad applicability. However, there is a gap between methods that\noptimize linear separability (e.g. LDA), and more flexible but computationally\nexpensive methods that optimize over arbitrary class boundaries (e.g.\nmetric-learning methods). Here, we present Supervised Quadratic Feature\nAnalysis (SQFA), a dimensionality reduction method for learning linear features\nthat maximize the differences between class-conditional first- and second-order\nstatistics, which allow for quadratic discrimination. SQFA exploits the\ninformation geometry of second-order statistics in the symmetric positive\ndefinite manifold. We show that SQFA features support quadratic\ndiscriminability in real-world problems. We also provide a theoretical link,\nbased on information geometry, between SQFA and the Quadratic Discriminant\nAnalysis (QDA) classifier.",
        "We highlight a formal and substantial analogy between Machine Learning (ML)\nalgorithms and discrete dynamical systems (DDS) in relaxation form. The analogy\noffers a transparent interpretation of the weights in terms of physical\ninformation-propagation processes and identifies the model function of the\nforward ML step with the local attractor of the corresponding discrete\ndynamics. Besides improving the explainability of current ML applications, this\nanalogy may also facilitate the development of a new class ML algorithms with a\nreduced number of weights.",
        "Much of the success of multi-agent debates depends on carefully choosing the\nright parameters. Among them, the decision-making protocol stands out.\nSystematic comparison of decision protocols is difficult because studies alter\nmultiple discussion parameters beyond the protocol. So far, it has been largely\nunknown how decision-making addresses the challenges of different tasks. This\nwork systematically evaluates the impact of seven decision protocols (e.g.,\nmajority voting, unanimity consensus). We change only one variable at a time\n(i.e., decision protocol) to analyze how different methods affect the\ncollaboration between agents and test different protocols on knowledge (MMLU,\nMMLU-Pro, GPQA) and reasoning datasets (StrategyQA, MuSR, SQuAD 2.0). Our\nresults show that voting protocols improve performance by 13.2% in reasoning\ntasks and consensus protocols by 2.8% in knowledge tasks over the other\ndecision protocol. Increasing the number of agents improves performance, while\nmore discussion rounds before voting reduces it. To improve decision-making by\nincreasing answer diversity, we propose two new methods, All-Agents Drafting\n(AAD) and Collective Improvement (CI). Our methods improve task performance by\nup to 3.3% with AAD and up to 7.4% with CI. This work demonstrates the\nimportance of decision-making in multi-agent debates beyond scaling."
      ]
    }
  },
  {
    "id":2411.03156,
    "research_type":"applied",
    "start_id":"b6",
    "start_title":"High\u2010Throughput Discovery of Novel Cubic Crystal Materials Using Deep Generative Neural Networks",
    "start_abstract":"Abstract High\u2010throughput screening has become one of the major strategies for discovery novel functional materials. However, its effectiveness is severely limited by lack sufficient and diverse materials in current repositories such as open quantum database (OQMD). Recent progress deep learning have enabled generative that learn implicit chemical rules creating hypothetical with new compositions structures. models difficulty generating structurally diverse, chemically valid, stable Here we propose CubicGAN, a adversarial network (GAN) based neural model large scale design cubic When trained on 375 749 ternary from OQMD database, authors show able to not only rediscover most currently known but also generate structure prototypes. A total 506 been verified phonon dispersion calculation. Considering importance wide applications solar panels, GAN provides promising approach significantly expand existing repositories, enabling via screening. The crystal structures discovered are freely accessible at www.carolinamatdb.org .",
    "start_categories":[
      "cs.NE"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b7"
      ],
      "title":[
        "Physics guided deep learning for generative design of crystal materials with symmetry constraints"
      ],
      "abstract":[
        "Abstract Discovering new materials is a challenging task in science crucial to the progress of human society. Conventional approaches based on experiments and simulations are labor-intensive or costly with success heavily depending experts\u2019 heuristic knowledge. Here, we propose deep learning Physics Guided Crystal Generative Model (PGCGM) for efficient crystal material design high structural diversity symmetry. Our model increases generation validity by more than 700% compared FTCP, one latest structure generators 45% our previous CubicGAN model. Density Functional Theory (DFT) calculations used validate generated structures 1869 out 2000 successfully optimized deposited into Carolina Materials Database www.carolinamatdb.org , which 39.6% have negative formation energy 5.3% energy-above-hull less 0.25 eV\/atom, indicating their thermodynamic stability potential synthesizability."
      ],
      "categories":[
        "physics.comp-ph"
      ]
    },
    "list":{
      "title":[
        "Inductive methods for counting number fields",
        "Strengthening the No-Go Theorem for QRNGs",
        "Social Influence Distorts Ratings in Online Interfaces",
        "Bounds for quasimodes with polynomially narrow bandwidth on surfaces of\n  revolution",
        "Semiclassical scar on tori in high dimension",
        "Decision from Suboptimal Classifiers: Excess Risk Pre- and\n  Post-Calibration",
        "Some limit theorems for locally stationary Hawkes processes",
        "Energy burdens of carbon lock-in in household heating transitions",
        "Perturbations of a minimal surface with triple junctions in\n  $\\mathbb{R}^2 \\times \\mathbb{S}^1$",
        "Transferable Foundation Models for Geometric Tasks on Point Cloud\n  Representations: Geometric Neural Operators",
        "Theoretical and Experimental Investigations of High-Performance\n  Sr2CoNbO6-delta Double Perovskite for IT-SOFC Cathode Applications",
        "Exploring quasar evolution with proximate molecular absorbers: Insights\n  from the kinematics of highly ionized nitrogen",
        "Any function I can actually write down is measurable, right?",
        "SyNPar: Synthetic Null Data Parallelism for High-Power False Discovery\n  Rate Control in High-Dimensional Variable Selection",
        "Strategizing with AI: Insights from a Beauty Contest Experiment",
        "Automatic target validation based on neuroscientific literature mining\n  for tractography",
        "Gravitomagnetic tidal response of relativistic stars in partially\n  screened scalar-tensor theories",
        "Poisson Hail on a Wireless Ground",
        "Clifford-Dressed Variational Principles for Precise Loschmidt Echoes",
        "A new combinatorial interpretation of partial sums of $m$-step Fibonacci\n  numbers",
        "Search for charge-parity violation in semileptonically tagged $D^{0} \\to\n  K^{+} \\pi^{-}$ decays",
        "Fractional Brownian motion with mean-density interaction",
        "On the Precise Asymptotics of Universal Inference",
        "Euclid preparation. BAO analysis of photometric galaxy clustering in\n  configuration space",
        "Effect of transverse momentum conservation and flow on symmetric\n  cumulants $sc_{2,3} \\left \\{ 4 \\right \\}$ and $sc_{2,3,4} \\left \\{ 6 \\right\n  \\}$",
        "Efficient, Fast, and Fair Voting Through Dynamic Resource Allocation in\n  a Secure Election Physical Intranet",
        "JWST photometry and astrometry of 47 Tucanae. Discontinuity in the\n  stellar sequence at the star\/brown dwarf transition",
        "Octagonal tilings with three prototiles",
        "Deterministic generation of non-classical mechanical states in cavity\n  optomechanics via reinforcement learning"
      ],
      "abstract":[
        "We give a new method for counting extensions of a number field asymptotically\nby discriminant, which we employ to prove many new cases of Malle's Conjecture\nand counterexamples to Malle's Conjecture. We consider families of extensions\nwhose Galois closure is a fixed permutation group $G$. Our method relies on\nhaving asymptotic counts for $T$-extensions for some normal subgroup $T$ of\n$G$, uniform bounds for the number of such $T$-extensions, and possibly weak\nbounds on the asymptotic number of $G\/T$-extensions. However, we do not require\nthat most $T$-extensions of a $G\/T$-extension are $G$-extensions. Our new\nresults use $T$ either abelian or $S_3^m$, though our framework is general.",
        "Quantum random numbers are essential for security against quantum algorithms.\nRandomness as a beacon is a service being provided for companies and\ngovernments to upgrade their security standards from RSA to PQC - QKD or\nPQC-RSA protocols. Both security mechanisms assume trust in the service\nprovider unless one aims for device-independent protocols. How does an entity\nensure that the beacon service has a quantum signature other than relying on\nfaith? Specifically, given a bit-stream, can a user verify a quantum signature\nin it? Researchers claim this is indecipherable and have stated a no-go theorem\nfor post-processed bit-streams. In this article, we corroborate the results of\nthe no-go theorem while discussing its nuances using two different random\nnumber generators and four test methods. These include the NIST statistical\ntest suite and machine learning algorithms that strengthen the theorem. This\nwork is relevant for companies and governments using QRNG OpenAPI to enhance\nsecurity against quantum threats.",
        "Theoretical work on sequential choice and large-scale experiments in online\nranking and voting systems has demonstrated that social influence can have a\ndrastic impact on social and technological systems. Yet, the effect of social\ninfluence on online rating systems remains understudied and the few existing\ncontributions suggest that online ratings would self-correct given enough\nusers. Here, we propose a new framework for studying the effect of social\ninfluence on online ratings. We start from the assumption that people are\ninfluenced linearly by the observed average rating, but postulate that their\npropensity to be influenced varies. When the weight people assign to the\nobserved average depends only on their own latent rating, the resulting system\nis linear, but the long-term rating may substantially deviate from the true\nmean rating. When the weight people put on the observed average depends on both\ntheir own latent rating and the observed average rating, the resulting system\nis non-linear, and may support multiple equilibria, suggesting that ratings\nmight be path-dependent and deviations dramatic. Our results highlight\npotential limitations in crowdsourced information aggregation and can inform\nthe design of more robust online rating systems.",
        "Given a compact surface of revolution with Laplace-beltrami operator\n$\\Delta$, we consider the spectral projector $P_{\\lambda,\\delta}$ on a\npolynomially narrow frequency interval $[\\lambda-\\delta,\\lambda + \\delta]$,\nwhich is associated to the self-adjoint operator $\\sqrt{-\\Delta}$. For a large\nclass of surfaces of revolution, and after excluding small disks around the\npoles, we prove that the $L^2 \\to L^{\\infty}$ norm of $P_{\\lambda,\\delta}$ is\nof order $\\lambda^{\\frac{1}{2}} \\delta^{\\frac{1}{2}}$ up to $\\delta \\geq\n\\lambda^{-\\frac{1}{32}}$. We adapt the microlocal approach introduced by Sogge\nfor the case $\\delta = 1$, by using the Quantum Completely Integrable structure\nof surfaces of revolution introduced by Colin de Verdi\\`ere. This reduces the\nanalysis to a number of estimates of explicit oscillatory integrals, for which\nwe introduce new quantitative tools.",
        "We show that the eigenfunctions of the self-adjoint elliptic $h-$differential\noperator $P_{h}(t)$ exhibits semiclassical scar phenomena on the\n$d-$dimensional torus, under the $\\sigma$-Bruno-R\\\"{u}ssmann condition, instead\nof the Diophantine one. Its equivalence is described as: for almost all\nperturbed Hamiltonian's KAM Lagrangian tori $\\Lambda_{\\omega}$, there exists a\nsemiclassical measure with positive mass on $\\Lambda_{\\omega}$. The premise is\nthat we can obatain a family of quasimodes for the $h-$differential operator\n$P_{h}(t)$ in the semiclassical limit as $h\\rightarrow0$, under the\n$\\sigma$-Bruno-R\\\"{u}ssmann condition.",
        "Probabilistic classifiers are central for making informed decisions under\nuncertainty. Based on the maximum expected utility principle, optimal decision\nrules can be derived using the posterior class probabilities and\nmisclassification costs. Yet, in practice only learned approximations of the\noracle posterior probabilities are available. In this work, we quantify the\nexcess risk (a.k.a. regret) incurred using approximate posterior probabilities\nin batch binary decision-making. We provide analytical expressions for\nmiscalibration-induced regret ($R^{\\mathrm{CL}}$), as well as tight and\ninformative upper and lower bounds on the regret of calibrated classifiers\n($R^{\\mathrm{GL}}$). These expressions allow us to identify regimes where\nrecalibration alone addresses most of the regret, and regimes where the regret\nis dominated by the grouping loss, which calls for post-training beyond\nrecalibration. Crucially, both $R^{\\mathrm{CL}}$ and $R^{\\mathrm{GL}}$ can be\nestimated in practice using a calibration curve and a recent grouping loss\nestimator. On NLP experiments, we show that these quantities identify when the\nexpected gain of more advanced post-training is worth the operational cost.\nFinally, we highlight the potential of multicalibration approaches as efficient\nalternatives to costlier fine-tuning approaches.",
        "We prove a law of large numbers and functional central limit theorem for a\nclass of multivariate Hawkes processes with time-dependent reproduction rate.\nWe address the difficulties induced by the use of non-convolutive Volterra\nprocesses by recombining classical martingale methods introduced in Bacry et\nal. [3] with novel ideas proposed by Kwan et al. [19]. The asymptotic theory we\nobtain yields useful applications in financial statistics. As an illustration,\nwe derive closed-form expressions for price distortions under liquidity\nconstraints.",
        "Heating electrification presents opportunities and challenges for energy\naffordability. Without careful planning and policy, the costs of natural gas\nservice will be borne by a shrinking customer base, driving up expenses for\nthose who are left behind. This affordability issue is worsened by new fossil\nfuel investments, which risk locking communities into carbon-intensive\ninfrastructure. Here, we introduce a framework to quantify the distributional\neffects of natural gas phasedown on energy affordability, integrating detailed\nhousehold data with utility financial and planning documents. Applying our\nframework first to Massachusetts and then nationwide, we show that vulnerable\ncommunities face disproportionate affordability risks in building energy\ntransitions. Households that do not electrify may bear up to 50% higher energy\ncosts over the next decade. Targeted electrification may help to alleviate\nimmediate energy burdens, but household heating transitions will ultimately\nrequire coordinated, neighborhood-scale strategies that consider the high fixed\ncosts of legacy infrastructure.",
        "We construct stationary perturbations of a specific minimal surface with a\ncircle of triple junctions in $\\mathbb{R}^2 \\times \\mathbb{S}^1$, that satisfy\ngiven boundary data.",
        "We introduce methods for obtaining pretrained Geometric Neural Operators\n(GNPs) that can serve as basal foundation models for use in obtaining geometric\nfeatures. These can be used within data processing pipelines for machine\nlearning tasks and numerical methods. We show how our GNPs can be trained to\nlearn robust latent representations for the differential geometry of\npoint-clouds to provide estimates of metric, curvature, and other shape-related\nfeatures. We demonstrate how our pre-trained GNPs can be used (i) to estimate\nthe geometric properties of surfaces of arbitrary shape and topologies with\nrobustness in the presence of noise, (ii) to approximate solutions of geometric\npartial differential equations (PDEs) on manifolds, and (iii) to solve\nequations for shape deformations such as curvature driven flows. We also\nrelease a package of the codes and weights for using our pre-trained GNPs for\nprocessing point cloud representations. This allows for incorporating our\npre-trained GNPs as components for reuse within existing and new data\nprocessing pipelines. The GNPs also can be used as part of numerical solvers\ninvolving geometry or as part of methods for performing inference and other\ngeometric tasks.",
        "Enhancing the transport of oxygen anions in the cathode while maintaining\nsurface stability is essential for improving the performance of\nintermediate-temperature solid oxide fuel cells (IT-SOFCs). This study\ninvestigates a novel cathode material candidate, Sr2CoNbO6-delta (SCNO), using\ndensity functional theory, molecular dynamics, and experimental\ncharacterization. The redox active Co cation at B-site and less reducible Nb\ncation at the B'-site together enhance both surface stability and\nelectrocatalytic performance. SCNO is observed to have a higher concentration\nof oxygen vacancies and increased oxygen diffusivity on the surface. The\nsurface stability of SCNO is further improved when simulated under compressive\nstrain due to the GDC electrolyte substrate. These findings offer new insights\ninto controlling Sr segregation in SCNO, contributing to a better understanding\nof its enhanced oxygen reduction reaction (ORR) activity and high surface\nstability. Subsequently, SCNO was synthesized to evaluate its potential as a\ncathode material in SOFCs. To assess its performance, symmetric cells with\nuniform dense thin films of varying thicknesses (40 and 80 nm) were fabricated\nusing the pulsed laser deposition technique. Electrochemical impedance\nspectroscopy and distributed relaxation time analysis indicate that bulk oxygen\nion diffusion is a limiting factor for the ORR in SCNO. The polarization\nresistance for the 40 and 80 nm dense thin film symmetric cells ranged between\n0.329 - 0.241 ohm cm2 and 1.095 - 0.438 ohm cm2, respectively, within the\ntemperature range of 773 - 973 K in an air atmosphere. The full cell\nconfiguration of NiO-GDC|GDC|SCNO demonstrated a significantly high peak power\ndensity of 0.633 W\/cm2 at 973 K. This theory-guided design and experimental\nstudy suggest that SCNO is a promising candidate for IT-SOFC cathode materials.",
        "We investigate the presence and kinematics of NV absorption proximate to high\nredshift quasars selected upon the presence of strong $H_{2}$ and HI absorption\nat the quasar redshift. Our spectroscopic observations with X-shooter at the\nVLT reveal a 70% detection rate of NV (9 of 13 quasars with 2.5 < z < 3.3),\nremarkably higher than the 10% detection rate in intervening DLA systems and\nthe 30% rate observed within a few thousand km\/s of the source in the general\nquasar population. While many NV components lie within the velocity range of\nthe neutral gas, the kinematic profiles of high-ionization species appear\ndecoupled from those of low-ionization species, with the former extending over\nmuch larger velocity ranges, particularly towards bluer velocities. We also\nobserve significant variations in the NV\/SiIV, which we attribute to varying\nionization conditions, with a velocity-dependent trend: blueshifted NV\ncomponents systematically exhibit higher ionization parameters compared to\nthose near the quasar's systemic redshift. Furthermore, the most redshifted\nsystems relative to the quasar show no evidence of NV absorption. The results\nsuggest that proximate $H_{2}$ absorption systems select critical stages of\nquasar evolution, during which the quasar remains embedded in a rich molecular\nenvironment. Redshifted systems trace infalling gas, potentially associated\nwith mergers, preceding the onset of outflows. Such outflows may reach or even\ncarry out neutral and molecular gas.This latter stage would correspond to\nproximate $H_{2}$ systems located around or blueshifted relative to the\nquasar's systemic z. Finally, the only case in our sample featuring highly\nblueshifted neutral gas shows no evidence of an association with the quasar.Our\nfindings highlight the need to account for the ionization state when defining a\nvelocity threshold to distinguish quasar-associated systems from intervening.",
        "In this expository paper aimed at a general mathematical audience, we discuss\nhow to combine certain classic theorems of set-theoretic inner model theory and\neffective descriptive set theory with work on Hilbert's tenth problem and\nuniversal Diophantine equations to produce the following surprising result:\nThere is a specific polynomial $p(x,y,z,n,k_1,\\dots,k_{70})$ of degree $7$ with\ninteger coefficients such that it is independent of $\\mathsf{ZFC}$ (and much\nstronger theories) whether the function $$f(x) = \\inf_{y \\in \\mathbb{R}}\\sup_{z\n\\in \\mathbb{R}}\\inf_{n \\in \\mathbb{N}}\\sup_{\\bar{k} \\in\n\\mathbb{N}^{70}}p(x,y,z,n,\\bar{k})$$ is Lebesgue measurable. We also give\nsimilarly defined $g(x,y)$ with the property that the statement \"$x \\mapsto\ng(x,r)$ is measurable for every $r \\in \\mathbb{R}$\" has large cardinal\nconsistency strength (and in particular implies the consistency of\n$\\mathsf{ZFC}$) and $h(m,x,y,z)$ such that $h(1,x,y,z),\\dots,h(16,x,y,z)$ can\nconsistently be the indicator functions of a Banach$\\unicode{x2013}$Tarski\nparadoxical decomposition of the sphere.\n  Finally, we discuss some situations in which measurability of analogously\ndefined functions can be concluded by inspection, which touches on\nmodel-theoretic o-minimality and the fact that sufficiently strong large\ncardinal hypotheses (such as Vop\\v{e}nka's principle and much weaker\nassumptions) imply that all 'reasonably definable' functions (including the\nabove $f(x)$, $g(x,y)$, and $h(m,x,y,z)$) are universally measurable.",
        "Balancing false discovery rate (FDR) and statistical power to ensure reliable\ndiscoveries is a key challenge in high-dimensional variable selection. Although\nseveral FDR control methods have been proposed, most involve perturbing the\noriginal data, either by concatenating knockoff variables or splitting the data\ninto two halves, both of which can lead to a loss of power. In this paper, we\nintroduce a novel approach called Synthetic Null Parallelism (SyNPar), which\ncontrols the FDR in high-dimensional variable selection while preserving the\noriginal data. SyNPar generates synthetic null data from a model fitted to the\noriginal data and modified to reflect the null hypothesis. It then applies the\nsame estimation procedure in parallel to both the original and synthetic null\ndata to estimate coefficients that indicate feature importance. By comparing\nthe coefficients estimated from the null data with those from the original\ndata, SyNPar effectively identifies false positives, functioning as a numerical\nanalog of a likelihood ratio test. We provide theoretical guarantees for FDR\ncontrol at any desired level while ensuring that the power approaches one with\nhigh probability asymptotically. SyNPar is straightforward to implement and can\nbe applied to a wide range of statistical models, including high-dimensional\nlinear regression, generalized linear models, Cox models, and Gaussian\ngraphical models. Through extensive simulations and real data applications, we\ndemonstrate that SyNPar outperforms state-of-the-art methods, including\nknockoffs and data-splitting methods, in terms of FDR control, power, and\ncomputational efficiency.",
        "A Keynesian beauty contest is a wide class of games of guessing the most\npopular strategy among other players. In particular, guessing a fraction of a\nmean of numbers chosen by all players is a classic behavioral experiment\ndesigned to test iterative reasoning patterns among various groups of people.\nThe previous literature reveals that the level of sophistication of the\nopponents is an important factor affecting the outcome of the game. Smarter\ndecision makers choose strategies that are closer to theoretical Nash\nequilibrium and demonstrate faster convergence to equilibrium in iterated\ncontests with information revelation. We replicate a series of classic\nexperiments by running virtual experiments with modern large language models\n(LLMs) who play against various groups of virtual players. We test how advanced\nthe LLMs' behavior is compared to the behavior of human players. We show that\nLLMs typically take into account the opponents' level of sophistication and\nadapt by changing the strategy. In various settings, most LLMs (with the\nexception of Llama) are more sophisticated and play lower numbers compared to\nhuman players. Our results suggest that LLMs (except Llama) are rather\nsuccessful in identifying the underlying strategic environment and adopting the\nstrategies to the changing set of parameters of the game in the same way that\nhuman players do. All LLMs still fail to play dominant strategies in a\ntwo-player game. Our results contribute to the discussion on the accuracy of\nmodeling human economic agents by artificial intelligence.",
        "Target identification for tractography studies requires solid anatomical\nknowledge validated by an extensive literature review across species for each\nseed structure to be studied. Manual literature review to identify targets for\na given seed region is tedious and potentially subjective. Therefore,\ncomplementary approaches would be useful. We propose to use text-mining models\nto automatically suggest potential targets from the neuroscientific literature,\nfull-text articles and abstracts, so that they can be used for anatomical\nconnection studies and more specifically for tractography. We applied\ntext-mining models to three structures: two well-studied structures, since\nvalidated deep brain stimulation targets, the internal globus pallidus and the\nsubthalamic nucleus and, the nucleus accumbens, an exploratory target for\ntreating psychiatric disorders. We performed a systematic review of the\nliterature to document the projections of the three selected structures and\ncompared it with the targets proposed by text-mining models, both in rat and\nprimate (including human). We ran probabilistic tractography on the nucleus\naccumbens and compared the output with the results of the text-mining models\nand literature review. Overall, text-mining the literature could find three\ntimes as many targets as two man-weeks of curation could. The overall\nefficiency of the text-mining against literature review in our study was 98%\nrecall (at 36% precision), meaning that over all the targets for the three\nselected seeds, only one target has been missed by text-mining. We demonstrate\nthat connectivity for a structure of interest can be extracted from a very\nlarge amount of publications and abstracts. We believe this tool will be useful\nin helping the neuroscience community to facilitate connectivity studies of\nparticular brain regions. The text mining tools used for the study are part of\nthe HBP Neuroinformatics Platform, publicly available at\nhttp:\/\/connectivity-brainer.rhcloud.com",
        "In scalar-tensor theories beyond Horndeski, the Vainshtein screening\nmechanism is only partially effective inside astrophysical bodies. We\ninvestigate the potential to detect this partial breaking of Vainshtein\nscreening through the tidal response of fluid bodies. Specifically, we\ncalculate the gravitomagnetic tidal Love numbers and analyze how deviations\nfrom general relativity depend on parameters governing the breaking of\nVainshtein screening in the weak-gravity regime. For fixed parameter values,\nthe relative deviations increase with higher multipoles and larger compactness.\nHowever, we demonstrate that these parameters alone are insufficient to fully\ncharacterize the tidal response of relativistic bodies in scalar-tensor\ntheories beyond Horndeski.",
        "This paper defines a new model which incorporates three key ingredients of a\nlarge class of wireless communication systems: (1) spatial interactions through\ninterference, (2) dynamics of the queueing type, with users joining and\nleaving, and (3) carrier sensing and collision avoidance as used in, e.g.,\nWiFi. In systems using (3), rather than directly accessing the shared resources\nupon arrival, a customer is considerate and waits to access them until nearby\nusers in service have left. This new model can be seen as a missing piece of a\nlarger puzzle that contains such dynamics as spatial birth-and-death processes,\nthe Poisson-Hail model, and wireless dynamics as key other pieces. It is shown\nthat, under natural assumptions, this model can be represented as a Markov\nprocess on the space of counting measures. The main results are then two-fold.\nThe first is on the shape of the stability region and, more precisely, on the\ncharacterization of the critical value of the arrival rate that separates\nstability from instability. The second is of a more qualitative or perhaps even\nethical nature. There is evidence that for natural values of the system\nparameters, the implementation of sensing and collision avoidance stabilizes a\nsystem that would be unstable if immediate access to the shared resources would\nbe granted. In other words, for these parameters, renouncing greedy access\nmakes sharing sustainable, whereas indulging in greedy access kills the system.",
        "We extend the recently introduced Clifford dressed Time-Dependent Variational\nPrinciple (TDVP) to efficiently compute many-body wavefunction amplitudes in\nthe computational basis. This advancement enhances the study of Loschmidt\nechoes, which generally require accurate calculations of the overlap between\nthe evolved state and the initial wavefunction. By incorporating Clifford\ndisentangling gates during TDVP evolution, our method effectively controls\nentanglement growth while keeping the computation of these amplitudes\naccessible. Specifically, it reduces the problem to evaluating the overlap\nbetween a Matrix Product State (MPS) and a stabilizer state, a task that\nremains computationally feasible within the proposed framework. To demonstrate\nthe effectiveness of this approach, we first benchmark it on the\none-dimensional transverse-field Ising model. We then apply it to more\nchallenging scenarios, including a non-integrable next-to-nearest-neighbor\nIsing chain and a two-dimensional Ising model. Our results highlight the\nversatility and efficiency of the Clifford-augmented MPS, showcasing its\ncapability to go beyond the evaluation of simple expectation values. This makes\nit a powerful tool for exploring various aspects of many-body quantum dynamics.",
        "The sequence of partial sums of Fibonacci numbers, beginning with $2$, $4$,\n$7$, $12$, $20$, $33,\\dots$, has several combinatorial interpretations (OEIS\nA000071). For instance, the $n$-th term in this sequence is the number of\nlength-$n$ binary words that avoid $110$. This paper proves a related but new\ninterpretation: given a length-$3$ binary word -- called the keyword -- we say\ntwo length-$n$ binary words are equivalent if one can be obtained from the\nother by some sequence of substitutions: each substitution replaces an instance\nof the keyword with its negation, or vice versa. We prove that the number of\ninduced equivalence classes is again the $n$-th term in the aforementioned\nsequence. When the keyword has length $m+1$ (instead of $3$), the same result\nholds with $m$-step Fibonacci numbers. What makes this result surprising -- and\ndistinct from the previous interpretation -- is that it does not depend on the\nkeyword, despite the fact that the sizes of the equivalence classes do. On this\nfinal point, we prove several results on the structure of equivalence classes,\nand also pose a variety of open problems.",
        "An analysis of the flavour oscillations of the charmed neutral meson is\npresented. The ratio of $D^{0} \\to K^{+} \\pi^{-}$ and $D^{0} \\to K^{-} \\pi^{+}$\ndecay rates is measured as a function of the decay time of the $D^{0}$ meson\nand compared with the charge-conjugated system to search for charge-parity\nviolation. The meson flavour at production is double-tagged by the charges of\nthe muon and pion in the preceding $\\overline{B} \\to D^{*}(2010)^{+} \\mu^{-} X$\nand ${{D^{*}(2010)^{+}} \\to D^{0}\\pi^{+}}$ decays, respectively. These decays\nare selected from proton-proton collision data collected by the LHCb experiment\nat a centre-of-mass energy of ${13\\,\\text{TeV}}$ and corresponding to an\nintegrated luminosity of ${5.4\\,\\text{fb}^{-1}}$. The flavour oscillation\nparameters, relating to the differences in mass and width of the mass\neigenstates, are found to be ${y^\\prime=(5.8\\pm1.6)\\times10^{-3}}$ and\n${(x^\\prime)^2=(0.0\\pm1.2)\\times10^{-4}}$. No evidence for charge-parity\nviolation is seen either in the flavour oscillations or in the decay, where the\ndirect charge-parity asymmetry is measured to be ${A_{D}=(2.3\\pm1.7)\\,{\\%}}$.",
        "Fractional Brownian motion is a Gaussian stochastic process with long-range\ncorrelations in time; it has been shown to be a useful model of anomalous\ndiffusion. Here, we investigate the effects of mutual interactions in an\nensemble of particles undergoing fractional Brownian motion. Specifically, we\nintroduce a mean-density interaction in which each particle in the ensemble is\ncoupled to the gradient of the total, time-integrated density produced by the\nentire ensemble. We report the results of extensive computer simulations for\nthe mean-square displacements and the probability densities of particles\nundergoing one-dimensional fractional Brownian motion with such a mean-density\ninteraction. We find two qualitatively different regimes, depending on the\nanomalous diffusion exponent $\\alpha$ characterizing the fractional Gaussian\nnoise. The motion is governed by the interactions for $\\alpha < 4\/3$ whereas it\nis dominated by the fractional Gaussian noise for $\\alpha > 4\/3$. We develop a\nscaling theory explaining our findings. We also discuss generalizations to\nhigher space dimensions and nonlinear interactions as well as applications to\nthe growth of strongly stochastic axons (e.g., serotonergic fibers) in\nvertebrate brains.",
        "In statistical inference, confidence set procedures are typically evaluated\nbased on their validity and width properties. Even when procedures achieve\nrate-optimal widths, confidence sets can still be excessively wide in practice\ndue to elusive constants, leading to extreme conservativeness, where the\nempirical coverage probability of nominal $1-\\alpha$ level confidence sets\napproaches one. This manuscript studies this gap between validity and\nconservativeness, using universal inference (Wasserman et al., 2020) with a\nregular parametric model under model misspecification as a running example. We\nidentify the source of asymptotic conservativeness and propose a general remedy\nbased on studentization and bias correction. The resulting method attains exact\nasymptotic coverage at the nominal $1-\\alpha$ level, even under model\nmisspecification, provided that the product of the estimation errors of two\nunknowns is negligible, exhibiting an intriguing resemblance to double\nrobustness in semiparametric theory.",
        "With about 1.5 billion galaxies expected to be observed, the very large\nnumber of objects in the Euclid photometric survey will allow for precise\nstudies of galaxy clustering from a single survey, over a large range of\nredshifts $0.2 < z < 2.5$. In this work, we use photometric redshifts to\nextract the baryon acoustic oscillation signal (BAO) from the Flagship galaxy\nmock catalogue with a tomographic approach to constrain the evolution of the\nUniverse and infer its cosmological parameters. We measure the two-point\nangular correlation function in 13 redshift bins. A template-fitting approach\nis applied to the measurement to extract the shift of the BAO peak through the\ntransverse Alcock--Paczynski parameter $\\alpha$. A joint analysis of all\nredshift bins is performed to constrain $\\alpha$ at the effective redshift\n$z_\\mathrm{eff}=0.77$ with MCMC and profile likelihood techniques. We also\nextract one $\\alpha_i$ parameter per redshift bin to quantify its evolution as\na function of time. From these 13 $\\alpha_i$, which are directly proportional\nto the ratio $D_\\mathrm{A}\/\\,r_\\mathrm{s,\\,drag}$, we constrain $h$,\n$\\Omega_\\mathrm{b}$, and $\\Omega_\\mathrm{cdm}$. From the joint analysis, we\nconstrain $\\alpha(z_\\mathrm{eff}=0.77)=1.0011^{+0.0078}_{-0.0079}$, which\nrepresents a three-fold improvement over current constraints from the Dark\nEnergy Survey. As expected, the constraining power in the analysis of each\nredshift bin is lower, with an uncertainty ranging from $\\pm\\,0.13$ to\n$\\pm\\,0.024$. From these results, we constrain $h$ at 0.45 %,\n$\\Omega_\\mathrm{b}$ at 0.91 %, and $\\Omega_\\mathrm{cdm}$ at 7.7 %. We quantify\nthe influence of analysis choices like the template, scale cuts, redshift bins,\nand systematic effects like redshift-space distortions over our constraints\nboth at the level of the extracted $\\alpha_i$ parameters and at the level of\ncosmological inference.",
        "Symmetric cumulants can improve our understanding of the joint probability\ndistribution function $ P\\left ( v_{m},v_{n},v_{k}, \\dots,\\Psi _{m},\\Psi\n_{n},\\Psi _{k},\\dots \\right )$, potentially offering new insights into the\nnature of the fluctuations of the quark-gluon plasma produced in relativistic\nheavy-ion collisions. In this work, the four-particle symmetric cumulants\n$sc_{2,3} \\left \\{ 4 \\right \\}$, six-particle symmetric cumulants $sc_{2,3,4}\n\\left \\{ 6 \\right \\}$, and the normalized cumulants $nsc_{2,3} \\left \\{ 4\n\\right \\}$ and $nsc_{2,3,4} \\left \\{ 6 \\right \\}$ originating from transverse\nmomentum conservation, collective flow, and the interplay between the two\neffects are calculated. Our results are consistent with the ATLAS data using\nthe subevent cumulant method and the simulations using the realistic Monte\nCarlo models of iEBE-VISHNU and HIJING, facilitating a more profound\nunderstanding of the origins of these symmetric cumulants in small systems.",
        "Resource allocations in an election system, often with hundreds of polling\nlocations over a territory such as a county, with the aim that voters receive\nfair and efficient services, is a challenging problem, as election resources\nare limited and the number of expected voters can be highly volatile through\nthe voting period. This paper develops two propositions to ensure efficiency,\nfairness, resilience, and security. The first is to leverage Physical Internet\n(PI) principles, notably setting up a \"secure election physical intranet\"\n(SEPI) based on open resource sharing and flow consolidation between election\nfacilities in the territory. The second is to adopt a smart dynamic resource\nallocation methodology within the SEPI based on queueing networks and\nlexicographic optimization. A queueing model is developed to provide feasible\ncombinations of resources and individual performances for each polling location\nby considering layout and utilization constraints. A two-stage lexicographic\noptimizer receives the queueing model's outputs and finds an optimal solution\nthat is less expensive, fast, and fair. A scenario-based case study validates\nthe proposed methodology based on data from the 2020 US Presidential Election\nin Fulton County, Georgia, USA.",
        "Using JWST Near Infrared Camera (NIRCam) images of the globular cluster 47\nTucanae (or NGC 104), taken at two epochs just 7 months apart, we derived\nproper-motion membership down to $m_{\\rm F322W2} \\sim 27$. We identified an\nintriguing feature at the very low-mass end of the main sequence, around $\\sim$\n0.08 solar masses, at magnitudes $m_{\\rm F322W2} \\sim 24$ and $m_{\\rm F150W2}\n\\sim 25$. This feature, dubbed \"kink\", is characterized by a prominent\ndiscontinuity in the slope of the main sequence. A similar discontinuity is\nseen in theoretical isochrones with oxygen-poor chemistries, related to the\nrapid onset of CH$_4$ absorption. We therefore hypothesize that the cluster\nhosts disproportionately more oxygen-poor stars near the bottom of the main\nsequence compared to the upper main sequence and the red giant branch. Our\nresults show no strong or conclusive evidence of a rise in the brown dwarf\nluminosity function at faint magnitudes, in contrast to previous findings\nlikely affected by faint red background galaxies. In our analysis, we accounted\nfor this contamination by using proper motion membership.",
        "Motivated by theoretically and experimentally observed structural phases with\noctagonal symmetry, we introduce a family of octagonal tilings which are\ncomposed of three prototiles. We define our tilings with respect to two\nnon-negative integers, $m$ and $n$, so that the inflation factor of a given\ntiling is $\\delta_{(m,n)}=m+n (1+\\sqrt{2})$. As such, we show that our family\nconsists of an infinite series of tilings which can be delineated into separate\n`cases' which are determined by the relationship between $m$ and $n$.\nSimilarly, we present the primitive substitution rules or decomposition of our\nprototiles, along with the statistical properties of each case, demonstrating\ntheir dependence on these integers.",
        "Non-classical mechanical states, as vital quantum resources for exploring\nmacroscopic quantum behavior, have wide applications in the study of the\nfundamental quantum mechanics and modern quantum technology. In this work, we\npropose a scheme for deterministically generating non-classical mechanical\nstates in cavity optomechanical systems. By working in the eigen-representation\nof the nonlinear optomechanical systems, we identify the carrier-wave resonance\nconditions and seek for the optimal driving pulses for state preparations.\nConcretely, we employ the reinforcement learning method to optimize the pulsed\ndriving fields, effectively suppressing the undesired transitions induced by\nboth the pulsed driving fields and dissipations. This approach enables the\nhigh-fidelity preparation of phononic Fock states and superposed Fock states in\nthe single-resonator optomechanical systems, as well as two-mode entangled\nstates in the two-resonator optomechanical systems. The statistical properties\nof the generated states are also examined. Our results open a way for quantum\nstate engineering in quantum optics and quantum information science via\nreinforcement learning."
      ]
    }
  },
  {
    "id":2412.17907,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"Deep Facial Expression Recognition: A Survey",
    "start_abstract":"With the transition of facial expression recognition (FER) from laboratory-controlled to challenging in-the-wild conditions and recent success deep learning techniques in various fields, neural networks have increasingly been leveraged learn discriminative representations for automatic FER. Recent FER systems generally focus on two important issues: overfitting caused by a lack sufficient training data expression-unrelated variations, such as illumination, head pose identity bias. In this paper, we provide comprehensive survey FER, including datasets algorithms that insights into these intrinsic problems. First, describe standard pipeline system with related background knowledge suggestions applicable implementations each stage. We then introduce available are widely used literature accepted selection evaluation principles datasets. For state art review existing novel strategies designed based both static images dynamic image sequences, discuss their advantages limitations. Competitive performances benchmarks also summarized section. extend our additional issues application scenarios. Finally, remaining challenges corresponding opportunities field well future directions design robust systems.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "Recognizing and reducing cognitive bias in clinical and forensic neurology"
      ],
      "abstract":[
        "In medicine, cognitive errors form the basis of bias in clinical practice. Several types are common and pervasive, may lead to inaccurate diagnosis or treatment. Forensic neurology, even when aided by current technologies, still dependent on interpretations, therefore prone bias. This article discusses 4 biases that can clinician astray. They confirmation (selective gathering neglect contradictory evidence); base rate (ignoring misusing prevailing data); hindsight (oversimplification past causation); good old days (the tendency for patients misremember exaggerate their preinjury functioning). We briefly describe strategies adopted from field psychology could minimize While debiasing is not easy, reducing such requires awareness acknowledgment our susceptibility these distortions."
      ],
      "categories":[
        "Clinical Neurology"
      ]
    },
    "list":{
      "title":[
        "Randomized measurements for multi-parameter quantum metrology",
        "Individual and cooperative superexchange enhancement in cuprates",
        "Every Call is Precious: Global Optimization of Black-Box Functions with\n  Unknown Lipschitz Constants",
        "Distinct terahertz third-harmonic generation of many-body excitonic\n  states",
        "SWIPT in Cell-Free Massive MIMO Using Stacked Intelligent Metasurfaces",
        "Some classes of permutation pentanomials",
        "Various Architectures of Colloidal Cu3(MoO4)2(OH)2 and Cu3Mo2O9; Thermal\n  Stability, Photoluminescence and Magnetic Properties of Cu3(MoO4)2(OH)2 and\n  Cu3Mo2O9 Nanosheets",
        "Heat Kernel Estimates for Schr\\\"odinger Operators in the Domain Above a\n  Bounded Lipschitz Function",
        "Mass loss along the red giant branch of the intermediate stellar\n  populations in NGC6752 and NGC2808",
        "Optical appearance of a boson star with soliton potential",
        "Discovering Polynomial and Quadratic Structure in Nonlinear Ordinary\n  Differential Equations",
        "Speeding up Lindblad dynamics via time-rescaling engineering",
        "On Oblivious Transfer Capacity of Noisy Multiple Access Channel",
        "Nonlocal Micromagnetics: Compactness Criteria, Existence of Minimizers,\n  and Brown's Fundamental Theorem",
        "Hadron Production in Open-charm Meson Pair at $e^+e^-$ Collider",
        "The large mass limit of $G_2$ and Calabi-Yau monopoles",
        "The J-PAS survey: The effect of photometric redshift errors on cosmic\n  voids",
        "Probabilistic intersection theory in Riemannian homogeneous spaces",
        "SurvHive: a package to consistently access multiple survival-analysis\n  packages",
        "Quantum Avalanches in $\\mathbb{Z}_2$-preserving Interacting Ising\n  Majorana Chain",
        "BaTiO$_3$ -- SrTiO$_3$ composites: a microscopic study on paraelectric\n  cubic inclusions",
        "Technical description and performance of the phase II version of the\n  Keck Planet Imager and Characterizer",
        "Observation of Two Cascading Screening Processes in an Iron-based\n  Superconductor",
        "Environmental Factors Can Have Opposite Biodiversity Influences on the\n  Community Temporal Stability In Aquatic Ecosystems",
        "Industrial Applications of Neutrinos",
        "Identifying compact symmetric objects with high-precision VLBI and Gaia\n  astrometry",
        "Naturalistic Computational Cognitive Science: Towards generalizable\n  models and theories that capture the full range of natural behavior",
        "Telegraph flux noise induced beating Ramsey fringe in transmon qubits",
        "On Lorentzian-Euclidean black holes and Lorentzian to Riemannian metric\n  transitions"
      ],
      "abstract":[
        "The optimal quantum measurements for estimating different unknown parameters\nin a parameterized quantum state are usually incompatible with each other.\nTraditional approaches to addressing the measurement incompatibility issue,\nsuch as the Holevo Cram\\'{e}r--Rao bound, suffer from multiple difficulties\ntowards practical applicability, as the optimal measurement strategies are\nusually state-dependent, difficult to implement and also take complex analyses\nto determine. Here we study randomized measurements as a new approach for\nmulti-parameter quantum metrology. We show quantum measurements on single\ncopies of quantum states given by 3-design perform near-optimally when\nestimating an arbitrary number of parameters in pure states and more generally,\napproximately low-rank states, whose metrological information is largely\nconcentrated in a low-dimensional subspace. The near-optimality is also shown\nin estimating the maximal number of parameters for three types of mixed states\nthat are well-conditioned on its support. Examples of fidelity estimation and\nHamiltonian estimation are explicitly provided to demonstrate the power and\nlimitation of randomized measurements in multi-parameter quantum metrology.",
        "It is now widely accepted that the antiferromagnetic coupling within high\ntemperature superconductors strongly exhibits a profound correlation with the\nupper limit of superconducting transition temperature these materials can\nreach. Thus, accurately calculating the positive and negative mechanisms that\ninfluence magnetic coupling in specific materials is crucial for the\nexploration of superconductivity at higher temperatures. Nevertheless, it is\nnotoriously difficult to establish a complete description of electron\ncorrelations employing ab initio theories because of the large number of\norbitals involved. In this study, we tackle the challenge of achieving\nhigh-level ab initio wave function theory calculations, which allow an explicit\ntreatment of electron correlations associated with a large number of\nhigh-energy orbitals. We elucidate the atomic-shell-wise contributions to the\nsuperexchange coupling in the lanthanum cuprate, including individual effects\nof high-energy orbitals (Cu 4d, 5d, 4f, 5p) and cooperative effects between the\ncore and these high-energy orbitals. Specifically, the prominent contributions\nfrom Cu 4d, 5d, 4f and 5p give rise to a rich collection of previously\nunexamined superexchange channels. We propose a p-d-f model to universally\naccount for the contributions of high-energy orbitals at copper sites. Our\ncalculations and physical rationalizations offer a more robust theoretical\nfoundation for investigating cuprate-type high-temperature superconductors.",
        "Optimizing expensive, non-convex, black-box Lipschitz continuous functions\npresents significant challenges, particularly when the Lipschitz constant of\nthe underlying function is unknown. Such problems often demand numerous\nfunction evaluations to approximate the global optimum, which can be\nprohibitive in terms of time, energy, or resources. In this work, we introduce\nEvery Call is Precious (ECP), a novel global optimization algorithm that\nminimizes unpromising evaluations by strategically focusing on potentially\noptimal regions. Unlike previous approaches, ECP eliminates the need to\nestimate the Lipschitz constant, thereby avoiding additional function\nevaluations. ECP guarantees no-regret performance for infinite evaluation\nbudgets and achieves minimax-optimal regret bounds within finite budgets.\nExtensive ablation studies validate the algorithm's robustness, while empirical\nevaluations show that ECP outperforms 10 benchmark algorithms including\nLipschitz, Bayesian, bandits, and evolutionary methods across 30\nmulti-dimensional non-convex synthetic and real-world optimization problems,\nwhich positions ECP as a competitive approach for global optimization.",
        "The dynamics of an electron-hole plasma governed by strong Coulomb\ninteraction is a challenging many-body problem.We report on experimental\nrealization of electron-hole many-body states in the picosecond time scale,\nwith tunable densities in a representative semiconductor Cu$_2$O. By using\ntime-resolved optical-pump terahertz third-harmonic-generation spectroscopy, we\nstudy the nonlinear terahertz dynamical characteristics of the many-body\nelectron-hole states. We find not only efficient and nonperturbative terahertz\nthird-harmonic yield associated with the excitonic formation, but also a\nnonmonotonic dependence of the excitonic nonlinear response on the\nelectron-hole density, reflecting the exciton dissociation at high charge\ndensity. Our results provide an efficient excitonic sensing of the\nfar-from-equilibrium electron-hole many-body states.",
        "We investigate the integration of stacked intelligent metasurfaces (SIMs)\ninto cell-free massive multiple input multiple output (CF-mMIMO) system to\nenhance the simultaneous wireless information and power transfer (SWIPT)\nperformance. Closed-form expressions for the spectral efficiency (SE) of the\ninformation-decoding receivers (IRs) and the average sum of harvested energy\n(sum-HE) at the energy-harvesting receivers (ERs) in the novel system model are\nderived to subsequently formulate a maximum total average sum-HE problem under\na minimum SE threshold per each IR. This problem jointly optimizes the SIM\nphase-shift (PS) configuration and access points' (APs) power allocation,\nrelying on long-term statistical channel state information (CSI). This\nnon-convex problem is then transformed into more tractable forms. Then,\nefficient algorithms are proposed, including a layer-by-layer heuristic method\nfor SIMs PS configuration that prioritizes sum-HE for the ERs and a successive\nconvex approximation (SCA)-based power allocation scheme to improve the\nachievable SE for the IRs. Numerical results show that our proposed algorithms\nachieve an almost 7-fold sum-HE gain as we increase the number of SIM layers,\nwhile the proposed power allocation (PPA) scheme often gains up to 40% in terms\nof the achievable minimum SE, compared to the equal power allocation.",
        "For each prime p other than 3, and each power q=p^k, we present two large\nclasses of permutation polynomials over F_{q^2} of the form X^r B(X^{q-1})\nwhich have at most five terms, where B(X) is a polynomial with coefficients in\n{1,-1}. The special case p=2 of our results comprises a vast generalization of\n76 recent results and conjectures in the literature. In case p>2, no instances\nof our permutation polynomials have appeared in the literature, and the\nconstruction of such polynomials had been posed as an open problem. Our proofs\nare short and involve no computations, in contrast to the proofs of many of the\nspecial cases of our results which were published previously.",
        "The lindgrenite compounds [Cu3(MoO4)2(OH)2] with various architectures and\nhigh crystallinity were prepared by a simple surfactant-assisted hydrothermal\nmethod. Then, the Cu3Mo2O9 samples were prepared by calcination of the\nas-synthesized Cu3(MoO4)2(OH)2. The resulting samples have high crystallinity,\ncolloidal properties, high-yield, large-scale production capability with using\nof nontoxic and inexpensive reagents and water as an environmentally solvent.\nThe scanning electron microscope studies show that the as-prepared lindgrenite\nnanostructures are well crystallized with rod, sheet and hollow sphere\nmorphologies. Meanwhile, the photoluminescence and magnetic properties of the\nnanosheet samples have been investigated that the both of Cu3(MoO4)2(OH)2 and\nCu3Mo2O9 samples have super paramagnetic behavior at room temperature and in\ncomparison with previous works, Cu3(MoO4)2(OH)2 and Cu3Mo2O9 samples\nsynthesized by the surfactant-assisted hydrothermal method in this work have a\nvery obvious red-shifted PL emission and high intensity.",
        "We give matching upper and lower bounds for the Dirichlet heat kernel of a\nSchr\\\"odinger operator $\\Delta+W$ in the domain above the graph of a bounded\nLipschitz function, in the case when $W$ decays away from the boundary faster\nthan quadratically.",
        "The morphology of the Horizontal Branch (HB) in Globular Clusters (GC) is\namong the early evidences that they contain multiple populations of stars.\nIndeed, the location of each star along the HB depends both on its initial\nhelium content (Y) and on the global average mass loss along the red giant\nbranch ($\\mu$). In most GCs, it is generally straightforward to analyse the\nfirst stellar population (standard Y), and the most extreme one (largest Y),\nwhile it is more tricky to look at the \"intermediate\" populations (mildly\nenhanced Y). In this work, we do this for the GCs NGC6752 and NGC2808; wherever\npossible the helium abundance for each stellar populations is constrained by\nusing independent measurements present in the literature. We compare population\nsynthesis models with photometric catalogues from the Hubble Space Telescope\nTreasury survey to derive the parameters of these HB stars. We find that the\nlocation of helium enriched stars on the HB is reproduced only by adopting a\nhigher value of $\\mu$ with respect to the first generation stars in all the\nanalysed stellar populations. We also find that $\\mu$ correlates with the\nhelium enhancement of the populations. This holds for both clusters. This\nfinding is naturally predicted by the model of ''pre-main sequence disc early\nloss'', previously suggested in the literature, and is consistent with the\nfindings of multiple-populations formation models that foresee the formation of\nsecond generation stars in a cooling flow.",
        "In this paper, we conduct an in-depth investigation into the optical images\nof boson stars with the solitonic potential. In the context of a celestial\nsource and a thin accretion disk, the optical characteristics of the soliton\nboson star have been derived. Considering the influence of the initial scalar\nfield $\\psi_0$ and a larger coupling parameter $\\alpha$ (the weak coupling\ncase), the optical images of boson stars primarily exhibit direct and lensed\nimages. The results demonstrate that variations in $\\psi_0$ and $\\alpha$\ninfluence the image size, whereas the observer's inclination angle $\\theta$ has\na substantial impact on the image shape. In contrast, when the coupling\nparameter $\\alpha$ is small (the strong coupling case), a sub-annular structure\nemerges within the Einstein ring for a spherically symmetric light source. In\nthe presence of a thin accretion disk, higher-order gravitational lensing\nimages emerge, indicating that photons are capable of orbiting the equatorial\nplane of the boson star multiple times. We also analyze how the effective\npotential and redshift factor depend on the parameters $\\psi_0$, $\\alpha$, and\n$\\theta$. The results indicate that at smaller values of $\\theta$,\ngravitational redshift is the dominant effect, resulting in an optical image\nfeaturing a bright ring surrounding a comparatively dim central region. At\nlarger values of $\\theta$, the Doppler effect becomes more pronounced,\nresulting in a substantial brightness disparity between the left and right\nsides of the optical image. These findings offer robust theoretical\nunderpinnings for differentiating solitonic boson stars from black holes via\nhigh-resolution astronomical observations.",
        "Dynamical systems with quadratic or polynomial drift exhibit complex\ndynamics, yet compared to nonlinear systems in general form, are often easier\nto analyze, simulate, control, and learn. Results going back over a century\nhave shown that the majority of nonpolynomial nonlinear systems can be recast\nin polynomial form, and their degree can be reduced further to quadratic. This\nprocess of polynomialization\/quadratization reveals new variables (in most\ncases, additional variables have to be added to achieve this) in which the\nsystem dynamics adhere to that specific form, which leads us to discover new\nstructures of a model. This chapter summarizes the state of the art for the\ndiscovery of polynomial and quadratic representations of finite-dimensional\ndynamical systems. We review known existence results, discuss the two prevalent\nalgorithms for automating the discovery process, and give examples in form of a\nsingle-layer neural network and a phenomenological model of cell signaling.",
        "We introduce a universal method for accelerating Lindblad dynamics that\npreserves the original trajectory. The technique provides exact fast processes\nanalytically, which are Markovian with time-independent Lindblad operators, by\ntime-rescaling a reference dynamics. In particular, the engineered control\nprotocols are based only on local interactions, and no additional control\nfields are required compared to the reference protocol. We demonstrate the\nscheme with two examples: a driven two-level system in an amplitude damping\nchannel and the dissipative transverse field Ising model. Our approach can help\nadvance techniques for quantum control and computation towards more complex\nnoisy systems.",
        "This work investigates the problem of Oblivious Transfer (OT) over a noisy\nMultiple Access Channel (MAC) involving two non-colluding senders and a single\nreceiver. The channel model is characterized by correlations among the parties,\nwith the parties assumed to be either honest-but-curious or, in the receiver's\ncase, potentially malicious. We propose a multiparty protocol for\nhonest-but-curious parties where the general MAC is reduced to a certain\ncorrelation. In scenarios where the receiver is malicious, the protocol\nachieves an achievable rate region.",
        "This paper investigates the existence and qualitative properties of\nminimizers for a class of nonlocal micromagnetic energy functionals defined on\nbounded domains. The considered energy functional consists of a symmetric\nexchange interaction, which penalizes spatial variations in magnetization, and\na magnetostatic self-energy term that accounts for long-range dipolar\ninteractions. Motivated by the extension of Brown's fundamental theorem on fine\nferromagnetic particles to nonlocal settings, we develop a rigorous variational\nframework in $L^2(\\Omega;\\mathbb{S}^2)$ under mild assumptions on the\ninteraction kernel \\( j \\), including symmetry, L\\'evy-type integrability, and\nprescribed singular behavior. For spherical domains, we generalize Browns\nfundamental results by identifying critical radii $R^*$ and $R^{**}$ that\ndelineate distinct energetic regimes: for \\( R \\leq R^* \\), the uniform\nmagnetization state is energetically preferable (\\emph{small-body regime}),\nwhereas for $R \\geq R^{**}$, non-uniform magnetization configurations become\ndominant (\\emph{large-body regime}). These transitions are analyzed through\nPoincar\\'e-type inequalities and explicit energy comparisons between uniform\nand vortex-like magnetization states.\n  Our results directly connect classical micromagnetic theory and contemporary\nnonlocal models, providing new insights into domain structure formation in\nnanoscale magnetism. Furthermore, the mathematical framework developed in this\nwork contributes to advancing theoretical foundations for applications in\nspintronics and data storage technologies.",
        "The standard model of particle physics is a well-established theoretical\nframework, yet there remain several unresolved issues that warrant further\nexperimental and theoretical exploration. In the realm of quark physics, these\ninclude understanding the nature of quark confinement and elucidating the\nmechanism linking quarks and gluons to strongly interacting particles within\nthe standard model theory, which may offer insights into underlying physics\nmechanisms. These inquiries can be addressed through the study of hadron\nproduction in open-charm meson pair final states at $e^+e^-$ annihilations\nutilizing the capabilities of BABAR, Belle, BESIII, and CLEO-c experiments,\nwhich have yielded valuable insights into non-standard hadrons over recent\ndecades. This review examines the contributions of $e^+e^-$ colliders from\nBABAR, Belle, BESIII, and CLEO-c experiments to such studies and discusses\nfuture prospects for $e^+e^-$ collider experiments.",
        "We develop a structure theory for the limit of $SU(2)$ $G_2$-monopoles (resp.\nCalabi-Yau monopoles) on a principal $SU(2)$-bundle over an asymptotically\nconical $G_2$-manifolds (resp. Calabi-Yau 3-folds) as the mass parameter tends\nto infinity, while the topologial data for the bundle stays fixed. We show how\nto extract a singular abelian $G_2$-monopole (resp. Calabi-Yau monopole) with\nDirac singularity along a calibrated cycle in the large mass limit, and we\nprove an energy identity for monopole bubbles.",
        "We investigated the impact of photometric redshift errors in the ongoing\nJavalambre Physics of the Accelerating Universe Astrophysical Survey (J-PAS) on\nvoid identification and properties using a watershed-based method, aiming to\nassess the recovery of individual voids and the overall void environment. We\ncreated galaxy mock catalogues for redshift z = 0.1 using the IllustrisTNG300-1\nsimulation, defining two datasets: an $ideal$ sample ($m_r < 21$ mag) and a\n$perturbed$ sample with the Z-coordinate errors mimicking J-PAS's line-of-sight\nerrors, derived from the precursor miniJPAS survey data. We identified voids\nusing ZOBOV, a watershed algorithm. We found 1065 voids in the $ideal$ sample\nand 2558 voids in the $perturbed$ sample. The $perturbed$ sample voids have, on\naverage, smaller sizes and denser interiors. We filtered out voids based on\ndensity and radius in order to eliminate overdense and small spurious\ninstances. The stacked density profile of filtered voids in the $perturbed$\nsample remains close to the average density even at the boundary peak,\nindicating a strong blurring of structures by the redshift errors. The number\nof $ideal$ sample voids for which at least $50\\%$ of the volume is recovered by\na void in the $perturbed$ sample is 53 (29 for the filtered sample). The volume\noccupied by these voids is less than $10\\%$ of the simulation volume. Merging\nvoids in the $perturbed$ sample marginally improves the recovery. The overall\nvolumes defined as voids in the two samples have an overlap of $80\\%$, making\nup $61\\%$ of the simulation box volume. While some statistical properties of\nvoids might be recovered sufficiently well, the watershed algorithms may not be\noptimal for recovering the large-scale structure voids if applied straight to\nphotometric redshift survey data.",
        "Let $M=G\/H$ be a Riemannian homogeneous space, where $G$ is a compact Lie\ngroup with closed subgroup $H$. Classical intersection theory states that the\nde Rham cohomology ring of $M$ describes the signed count of intersection\npoints of submanifolds $Y_1, \\ldots, Y_s$ of $M$ in general position, when the\ncodimensions add up to $\\dim M$.\n  We introduce the probabilistic intersection ring $\\mathrm{H}_{\\mathbb E}(M)$,\nwhose multiplication describes the unsigned count of intersection points, when\nthe $Y_i$ are randomly moved by independent uniformly random elements of $G$.\nThe probabilistic intersection ring $\\mathrm{H}_{\\mathbb E}(M)$ has the\nstructure of a graded commutative and associative real Banach algebra. It is\ndefined as a quotient of the ring of Grassmann zonoids of a fixed cotangent\nspace $V$ of $M$. The latter was introduced by the authors in [Adv. Math. 402,\n2022]. There is a close connection to valuations of convex bodies:\n$\\mathrm{H}_{\\mathbb E}(M)$ can be interpreted as a subspace of the space of\ntranslation invariant, even, continuous valuations on $V$, whose multiplication\ncoincides with Alesker's multiplication for smooth valuations.\n  We describe the ring structure of the probabilistic intersection ring for\nspheres, real projective space and complex projective space, relying on Fu [J.\nDiff. Geo. 72(3), 2006] for the latter case. From this, we derive an\ninteresting probabilistic intersection formula in complex projective space.\nFinally, we initiate the investigation of the probabilistic intersection ring\nfor real Grassmannians, outlining the construction of a probabilistic version\nof Schubert Calculus.",
        "Survival analysis, a foundational tool for modeling time-to-event data, has\nseen growing integration with machine learning (ML) approaches to handle the\ncomplexities of censored data and time-varying risks. Despite these advances,\nleveraging state-of-the-art survival models remains a challenge due to the\nfragmented nature of existing implementations, which lack standardized\ninterfaces and require extensive preprocessing. We introduce SurvHive, a\nPython-based framework designed to unify survival analysis methods within a\ncoherent and extensible interface modeled on scikit-learn. SurvHive integrates\nclassical statistical models with cutting-edge deep learning approaches,\nincluding transformer-based architectures and parametric survival models. Using\na consistent API, SurvHive simplifies model training, evaluation, and\noptimization, significantly reducing the barrier to entry for ML practitioners\nexploring survival analysis. The package includes enhanced support for\nhyper-parameter tuning, time-dependent risk evaluation metrics, and\ncross-validation strategies tailored to censored data. With its extensibility\nand focus on usability, SurvHive provides a bridge between survival analysis\nand the broader ML community, facilitating advancements in time-to-event\nmodeling across domains. The SurvHive code and documentation are available\nfreely at https:\/\/github.com\/compbiomed-unito\/survhive.",
        "Recent numerical works have revealed the instability of many-body localized\n(MBL) phase in disordered quantum many-body systems with finite system sizes\nand over finite timescales. This instability arises from Griffith regions that\noccur at the thermodynamic limit, which rapidly thermalize and affect the\nsurrounding typical MBL regions, introducing an avalanche mechanism into the\nsystem. Here, we consider the $\\mathbb{Z}_2$-preserving interacting Ising\nMajorana chain model, which exhibits a more complex phase diagram, where an\nergodic phase emerges between two MBL phases with different long-range order\nproperties. We calculate the dynamic characteristics of the model when coupled\nto an infinite bath under perturbation, and through scaling behavior of the\nslowest thermalization rate, we find how critical disorder strengths in\nfinite-size systems are affected by the avalanche mechanism. We also employe\nthe embedded inclusion model and use the time evolution of mutual information\nbetween each spin and the artificial Griffith region to probe the diffusion of\nthe thermal bubble. We observe that in finite-sized systems, the critical\ndisorder strength gradually drifts away from the central. Our work demonstrate\nthat both MBL paramagnetic phase and MBL spin-glass phase are unstable at\nfinite sizes.",
        "Composites of ferroelectric and paraelectric perovskites have attracted a lot\nof attention due to their application potential in energy storage as well as\nnovel computing and memory devices. So far the main focus of research has been\non superlattices and ferroelectric particles in a paraelectric matrix, while\nthe impact of paraelectric inclusions on the ferroelectric matrix is\nsurprisingly underrepresented. To close this gap in knowledge we perform\nmolecular dynamics simulations using an $ab\\ initio$ derived effective\nHamiltonian for BaTiO$_3$--SrTiO$_3$ and reveal the dependency of phase\nstability and phase transitions on the size and distances of paraelectric\ninclusions. We discuss how the combination of compressive strain and\ndepolarization fields at the SrTiO$_3$ interfaces induces large local\npolarization, complex domain structures and coexisting phases as well as\ndiffuse phase transitions and reduced coercive fields.",
        "The Keck Planet Imager and Characterizer (KPIC) is a series of upgrades for\nthe Keck II Adaptive Optics (AO) system and the NIRSPEC spectrograph to enable\ndiffraction limited, high resolution (R>30000) spectroscopy of exoplanets and\nlow mass companions in the K and L bands. Phase I consisted of single mode\nfiber injection\/extraction units (FIU\/FEU) used in conjunction with a H band\npyramid wavefront sensor. The use of single mode fibers provides a gain in\nstellar rejection, a substantial reduction in sky background, and an extremely\nstable line spread function in the spectrograph. Phase II, deployed and\ncommissioned in 2022, brought a 1000 actuator deformable mirror, beam shaping\noptics, a vortex mask, and other upgrades to the FIU\/FEU. An additional service\nmission in 2024 extended operations down to y band, delivered an atmospheric\ndispersion corrector, and provided access to two laser frequency combs. KPIC\nphase II brings higher planet throughput, lower stellar leakage and many new\nobserving modes which extend its ability to characterize exoplanets at high\nspectral resolution, building on the success of phase I. In this paper we\npresent a description of the final phase II version of KPIC, along with results\nof system level laboratory testing and characterization showing the\ninstrument's phase II throughput, stability, repeatability, and other key\nperformance metrics prior to delivery and during installation at Keck. We\noutlined the capabilities of the various observing modes enabled by the new\nmodules as well as efforts to compensate for static aberrations and non common\npath errors at Keck, which were issues that plagued phase I. Finally, we show\nresults from commissioning.",
        "Understanding how renormalized quasiparticles emerge in strongly correlated\nelectron materials provides a challenge for both experiment and theory. It has\nbeen predicted that distinctive spin and orbital screening mechanisms drive\nthis process in multiorbital materials with strong Coulomb and Hund's\ninteractions. Here, we provide the experimental evidence of both mechanisms\nfrom angle-resolved photoemission spectroscopy on RbFe$_2$As$_2$. We observe\nthat the emergence of low-energy Fe 3$d_{xy}$ quasiparticles below 90K is tied\nto spin screening. A second process changes the spectral weight at high\nenergies up to room temperature. Supported by theoretical calculations we\nattribute it to orbital screening of Fe 3d atomic excitations. These two\ncascading screening processes drive the temperature evolution from a bad metal\nto a correlated Fermi liquid.",
        "1. An understanding of how biodiversity confers ecosystem stability is\ncrucial in managing ecosystems under major environmental changes. Multiple\nbiodiversity drivers can stabilize ecosystem functions over time. However, we\nknow little about how local environmental conditions can influence these\nbiodiversity drivers, and consequently how they indirectly shape the ecological\nstability of ecosystems.\n  2. We hypothesized that environmental factors can have opposite influences\n(i.e., not necessarily either positive or negative) on the temporal stability\nof communities in different environmental ranges depending on the biodiversity\ndrivers involved. We tested this novel hypothesis by using data from a\n4-year-long field study of submerged macrophyte across a water depth gradient\nin 8 heterogeneous bays of Erhai lake (with total sample size of 30,071\nquadrats), a large lentic system in China.\n  3. Results indicate that a unimodal pattern of stability in temporal biomass\nmeasurements occurred along the water-depth gradient, and that multiple\nbiodiversity drivers (the asynchrony in species dynamics, and the stability of\ndominant species) generally increased the temporal stability of aquatic primary\nproducers. However, the effect of water depth either increased or decreased the\nstability of biomass according to the environmental conditions associated with\nsites along the water depth gradient.\n  4. Synthesis. These results reveal the influence of local environmental\nconditions on the biodiversity drivers of stability may help predict the\nfunctional consequences of biodiversity change across different scenarios of\nenvironmental change.",
        "We present a review of the current and future industrial applications of\nneutrinos. We address the industrial applications of neutrinos in geological\nand geochemical studies of the Earth's interior, in monitoring earthquakes, in\nterrestrial communications, in applications for submarines, in monitoring\nnuclear power plants and fusion reactors, in the management of fissile\nmaterials used in nuclear plants, in tracking nuclear tests, among other\napplications. We also address future possibilities for industrial applications\nof neutrinos, especially concerning communications in the solar system and\ngeotomography of solar system bodies.",
        "Compact symmetric objects (CSOs) represent a key early stage in radio galaxy\nevolution, but their reliable identification remains challenging. We develop a\nmethod to identify CSOs by combining Gaia optical astrometry with VLBI radio\nimaging. We analyze 40 CSO candidates by overlaying Gaia DR3 positions on VLBI\nmaps to locate their central engines. CSOs are confirmed when Gaia positions\nlie between symmetric radio lobes, while core-jet sources show optical\npositions coinciding with one end of the radio structure. We verify\nclassifications using spectral indices, variability, and jet kinematics from\nmulti-epoch VLBI observations. Our method identified 22 genuine CSOs and 10\ncore-jet sources, with 8 objects remaining ambiguous. Confirmed CSOs show\nkinematic ages from 20 to over 1000 years and hotspot speeds typically below\n0.5c. Five nearby CSOs show optical-radio offsets despite strong CSO\nmorphology, indicating host galaxy influence. The Gaia-VLBI method provides a\nreliable CSO identification tool. Our sample reveals diverse radio powers,\nsuggesting multiple evolutionary paths. CSO evolution appears influenced by\nboth intrinsic jet power and environmental factors, with high-power CSOs\npotentially evolving into large-scale radio galaxies while low-power CSOs often\nshow confinement by their host environments.",
        "Artificial Intelligence increasingly pursues large, complex models that\nperform many tasks within increasingly realistic domains. How, if at all,\nshould these developments in AI influence cognitive science?\n  We argue that progress in AI offers timely opportunities for cognitive\nscience to embrace experiments with increasingly naturalistic stimuli, tasks,\nand behaviors; and computational models that can accommodate these changes. We\nfirst review a growing body of research spanning neuroscience, cognitive\nscience, and AI that suggests that incorporating a broader range of\nnaturalistic experimental paradigms (and models that accommodate them) may be\nnecessary to resolve some aspects of natural intelligence and ensure that our\ntheories generalize. We then suggest that integrating recent progress in AI and\ncognitive science will enable us to engage with more naturalistic phenomena\nwithout giving up experimental control or the pursuit of theoretically grounded\nunderstanding. We offer practical guidance on how methodological practices can\ncontribute to cumulative progress in naturalistic computational cognitive\nscience, and illustrate a path towards building computational models that solve\nthe real problems of natural cognition - together with a reductive\nunderstanding of the processes and principles by which they do so.",
        "Ramsey oscillations typically exhibit an exponential decay envelope due to\nenvironmental noise. However, recent experiments have observed nonmonotonic\nRamsey fringes characterized by beating patterns, which deviate from the\nstandard behavior. These beating patterns have primarily been attributed to\ncharge-noise fluctuations. In this paper, we investigate the flux-noise origin\nof these nonmonotonic Ramsey fringes in frequency-tunable transmon qubits. We\ndevelop a random telegraph noise (RTN) model to simulate the impact of\ntelegraph-like flux-noise sources on Ramsey oscillations. Our simulations\ndemonstrate that strong flux-RTN sources can induce beating patterns in the\nRamsey fringes, showing excellent agreement with experimental observations in\ntransmon qubits influenced by electronic environment-induced flux-noise. Our\nfindings provide valuable insights into the role of flux-noise in qubit\ndecoherence and underscore the importance of considering flux-noise RTN when\nanalyzing nonmonotonic Ramsey fringes.",
        "In recent papers on spacetimes with a signature-changing metric, the concept\nof a Lorentzian-Euclidean black hole and new elements for Lorentzian-Riemannian\nsignature change have been introduced. A Lorentzian-Euclidean black hole is a\nsignature-changing modification of the Schwarzschild spacetime satisfying the\nvacuum Einstein equations in a weak sense. Here the event horizon serves as a\nboundary beyond which time becomes imaginary. We demonstrate that the proper\ntime needed to reach the horizon remains finite, consistently with the\nclassical Schwarzschild solution. About Lorentzian to Riemannian metric\ntransitions, we stress that the hypersurface where the metric signature changes\nis naturally a spacelike hypersurface which might be identified with the future\nor past causal boundary of the Lorentzian sector. Moreover, a number of\ngeometric interpretations appear, as the degeneracy of the metric corresponds\nto the collapse of the causal cones into a line, the degeneracy of the dual\nmetric corresponds to collapsing into a hyperplane, and additional geometric\nstructures on the transition hypersurface (Galilean and dual Galilean) might be\nexplored."
      ]
    }
  },
  {
    "id":2412.17907,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"Recognizing and reducing cognitive bias in clinical and forensic neurology",
    "start_abstract":"In medicine, cognitive errors form the basis of bias in clinical practice. Several types are common and pervasive, may lead to inaccurate diagnosis or treatment. Forensic neurology, even when aided by current technologies, still dependent on interpretations, therefore prone bias. This article discusses 4 biases that can clinician astray. They confirmation (selective gathering neglect contradictory evidence); base rate (ignoring misusing prevailing data); hindsight (oversimplification past causation); good old days (the tendency for patients misremember exaggerate their preinjury functioning). We briefly describe strategies adopted from field psychology could minimize While debiasing is not easy, reducing such requires awareness acknowledgment our susceptibility these distortions.",
    "start_categories":[
      "Clinical Neurology"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "Deep Facial Expression Recognition: A Survey"
      ],
      "abstract":[
        "With the transition of facial expression recognition (FER) from laboratory-controlled to challenging in-the-wild conditions and recent success deep learning techniques in various fields, neural networks have increasingly been leveraged learn discriminative representations for automatic FER. Recent FER systems generally focus on two important issues: overfitting caused by a lack sufficient training data expression-unrelated variations, such as illumination, head pose identity bias. In this paper, we provide comprehensive survey FER, including datasets algorithms that insights into these intrinsic problems. First, describe standard pipeline system with related background knowledge suggestions applicable implementations each stage. We then introduce available are widely used literature accepted selection evaluation principles datasets. For state art review existing novel strategies designed based both static images dynamic image sequences, discuss their advantages limitations. Competitive performances benchmarks also summarized section. extend our additional issues application scenarios. Finally, remaining challenges corresponding opportunities field well future directions design robust systems."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Mitigating Ambiguities in 3D Classification with Gaussian Splatting",
        "Reading between the Lines: Can LLMs Identify Cross-Cultural\n  Communication Gaps?",
        "The Stochastic Parrot on LLM's Shoulder: A Summative Assessment of\n  Physical Concept Understanding",
        "SemiHMER: Semi-supervised Handwritten Mathematical Expression\n  Recognition using pseudo-labels",
        "A Constant Rate Quantum Computer on a Line",
        "Every Angle Is Worth A Second Glance: Mining Kinematic Skeletal\n  Structures from Multi-view Joint Cloud",
        "Study of gravitational waves from phase transitions in three-component\n  dark matter",
        "Smart Cubing for Graph Search: A Comparative Study",
        "Seeing Delta Parameters as JPEG Images: Data-Free Delta Compression with\n  Discrete Cosine Transform",
        "Towards bandit-based prompt-tuning for in-the-wild foundation agents",
        "Integrating Causality with Neurochaos Learning: Proposed Approach and\n  Research Agenda",
        "On Gegenbauer polynomials and Wronskian determinants of trigonometric\n  functions",
        "Tackling Compressible Turbulent Multi-Component Flows with Dynamic\n  hp-Adaptation",
        "MedVAE: Efficient Automated Interpretation of Medical Images with\n  Large-Scale Generalizable Autoencoders",
        "Quantum effects in surface diffusion: application to diffusion of\n  nitrogen adatoms over GaN(0001) surface",
        "Real-Time 3D Magnetic Field Camera for a Spherical Volume",
        "A Modular Pipeline for 3D Object Tracking Using RGB Cameras",
        "Concentration around a stable equilibrium for the non-autonomous\n  $\\Phi_3^4$ model",
        "Quasi-compactness and statistical properties for discontinuous systems\n  semi-conjugated to piecewise convex maps with countable branches",
        "A central limit theorem for the giant in a stochastic block model",
        "Comprehensive Review of Neural Differential Equations for Time Series\n  Analysis",
        "Deep Ensembling with Multimodal Image Fusion for Efficient\n  Classification of Lung Cancer",
        "Unstable accretion in TW Hya: 3D simulations and comparisons with\n  observations",
        "Improving Grip Stability Using Passive Compliant Microspine Arrays for\n  Soft Robots in Unstructured Terrain",
        "Quantum critical point followed by Kondo-like behavior due to Cu\n  substitution in itinerant, antiferromagnet ${\\text{La}_{2}\\text{(Cu}_{x}\\text\n  {Ni}_{1-x})_7}$",
        "Quantum Communication Multiplexing in LP-modes Enabled by Photonic\n  Lanterns",
        "Shapiro Steps Observed in a Two-Dimensional Yukawa Solid Modulated by a\n  One-Dimensional Vibrational Periodic Substrate",
        "AdaNDV: Adaptive Number of Distinct Value Estimation via Learning to\n  Select and Fuse Estimators",
        "Fixing the Double Penalty in Data-Driven Weather Forecasting Through a\n  Modified Spherical Harmonic Loss Function"
      ],
      "abstract":[
        "3D classification with point cloud input is a fundamental problem in 3D\nvision. However, due to the discrete nature and the insufficient material\ndescription of point cloud representations, there are ambiguities in\ndistinguishing wire-like and flat surfaces, as well as transparent or\nreflective objects. To address these issues, we propose Gaussian Splatting (GS)\npoint cloud-based 3D classification. We find that the scale and rotation\ncoefficients in the GS point cloud help characterize surface types.\nSpecifically, wire-like surfaces consist of multiple slender Gaussian\nellipsoids, while flat surfaces are composed of a few flat Gaussian ellipsoids.\nAdditionally, the opacity in the GS point cloud represents the transparency\ncharacteristics of objects. As a result, ambiguities in point cloud-based 3D\nclassification can be mitigated utilizing GS point cloud as input. To verify\nthe effectiveness of GS point cloud input, we construct the first real-world GS\npoint cloud dataset in the community, which includes 20 categories with 200\nobjects in each category. Experiments not only validate the superiority of GS\npoint cloud input, especially in distinguishing ambiguous objects, but also\ndemonstrate the generalization ability across different classification methods.",
        "In a rapidly globalizing and digital world, content such as book and product\nreviews created by people from diverse cultures are read and consumed by others\nfrom different corners of the world. In this paper, we investigate the extent\nand patterns of gaps in understandability of book reviews due to the presence\nof culturally-specific items and elements that might be alien to users from\nanother culture. Our user-study on 57 book reviews from Goodreads reveal that\n83\\% of the reviews had at least one culture-specific difficult-to-understand\nelement. We also evaluate the efficacy of GPT-4o in identifying such items,\ngiven the cultural background of the reader; the results are mixed, implying a\nsignificant scope for improvement. Our datasets are available here:\nhttps:\/\/github.com\/sougata-ub\/reading_between_lines",
        "In a systematic way, we investigate a widely asked question: Do LLMs really\nunderstand what they say?, which relates to the more familiar term Stochastic\nParrot. To this end, we propose a summative assessment over a carefully\ndesigned physical concept understanding task, PhysiCo. Our task alleviates the\nmemorization issue via the usage of grid-format inputs that abstractly describe\nphysical phenomena. The grids represents varying levels of understanding, from\nthe core phenomenon, application examples to analogies to other abstract\npatterns in the grid world. A comprehensive study on our task demonstrates: (1)\nstate-of-the-art LLMs, including GPT-4o, o1 and Gemini 2.0 flash thinking, lag\nbehind humans by ~40%; (2) the stochastic parrot phenomenon is present in LLMs,\nas they fail on our grid task but can describe and recognize the same concepts\nwell in natural language; (3) our task challenges the LLMs due to intrinsic\ndifficulties rather than the unfamiliar grid format, as in-context learning and\nfine-tuning on same formatted data added little to their performance.",
        "In this paper, we study semi-supervised Handwritten Mathematical Expression\nRecognition (HMER) via exploring both labeled data and extra unlabeled data. We\npropose a novel consistency regularization framework, termed SemiHMER, which\nintroduces dual-branch semi-supervised learning. Specifically, we enforce\nconsistency between the two networks for the same input image. The\npseudo-label, generated by one perturbed recognition network, is utilized to\nsupervise the other network using the standard cross-entropy loss. The SemiHMER\nconsistency encourages high similarity between the predictions of the two\nperturbed networks for the same input image and expands the training data by\nleveraging unlabeled data with pseudo-labels. We further introduce a\nweak-to-strong strategy by applying different levels of augmentation to each\nbranch, effectively expanding the training data and enhancing the quality of\nnetwork training. Additionally, we propose a novel module, the Global Dynamic\nCounting Module (GDCM), to enhance the performance of the HMER decoder by\nalleviating recognition inaccuracies in long-distance formula recognition and\nreducing the occurrence of repeated characters. The experimental results\ndemonstrate that our work achieves significant performance improvements, with\nan average accuracy increase of 5.47% on CROHME14, 4.87% on CROHME16, and 5.25%\non CROHME19, compared to our baselines.",
        "We prove by construction that the Bravyi-Poulin-Terhal bound on the spatial\ndensity of stabilizer codes does not generalize to stabilizer circuits. To do\nso, we construct a fault tolerant quantum computer with a coding rate above 5%\nand quasi-polylog time overhead, out of a line of qubits with nearest-neighbor\nconnectivity, and prove it has a threshold. The construction is based on\nmodifications to the tower of Hamming codes of Yamasaki and Koashi (Nature\nPhysics, 2024), with operators measured using a variant of Shor's measurement\ngadget.",
        "Multi-person motion capture over sparse angular observations is a challenging\nproblem under interference from both self- and mutual-occlusions. Existing\nworks produce accurate 2D joint detection, however, when these are triangulated\nand lifted into 3D, available solutions all struggle in selecting the most\naccurate candidates and associating them to the correct joint type and target\nidentity. As such, in order to fully utilize all accurate 2D joint location\ninformation, we propose to independently triangulate between all same-typed 2D\njoints from all camera views regardless of their target ID, forming the Joint\nCloud. Joint Cloud consist of both valid joints lifted from the same joint type\nand target ID, as well as falsely constructed ones that are from different 2D\nsources. These redundant and inaccurate candidates are processed over the\nproposed Joint Cloud Selection and Aggregation Transformer (JCSAT) involving\nthree cascaded encoders which deeply explore the trajectile, skeletal\nstructural, and view-dependent correlations among all 3D point candidates in\nthe cross-embedding space. An Optimal Token Attention Path (OTAP) module is\nproposed which subsequently selects and aggregates informative features from\nthese redundant observations for the final prediction of human motion. To\ndemonstrate the effectiveness of JCSAT, we build and publish a new multi-person\nmotion capture dataset BUMocap-X with complex interactions and severe\nocclusions. Comprehensive experiments over the newly presented as well as\nbenchmark datasets validate the effectiveness of the proposed framework, which\noutperforms all existing state-of-the-art methods, especially under challenging\nocclusion scenarios.",
        "This paper presents a dark matter model comprising three types of particles\nwith distinct spins, along with a scalar field $\\phi$ that mediates\ninteractions between Standard Model particles and dark matter. It discusses the\nelectroweak phase transition following the Big Bang, during which all particles\nare initially massless due to the inactive Higgs mechanism. As temperature\ndecreases, the effective potential reaches zero at two points, leading to two\nminima at the critical temperature ($T_c$), and eventually to a true vacuum\nstate. The formation of new vacuum bubbles, where electroweak symmetry is\nbroken and particles acquire mass, generates gravitational waves as these\nbubbles interact with the fabric of space-time. The paper derives the\ngravitational wave frequency and detection range based on the model's\nparameters, aligning with observational data from the Planck satellite and\ndetection thresholds from PandaX-4T. It concludes by comparing the predicted\nbackground gravitational wave density with the sensitivities of LISA and BBO\ndetectors.",
        "Parallel solving via cube-and-conquer is a key method for scaling SAT solvers\nto hard instances. While cube-and-conquer has proven successful for pure SAT\nproblems, notably the Pythagorean triples conjecture, its application to SAT\nsolvers extended with propagators presents unique challenges, as these\npropagators learn constraints dynamically during the search.\n  We study this problem using SAT Modulo Symmetries (SMS) as our primary test\ncase, where a symmetry-breaking propagator reduces the search space by learning\nconstraints that eliminate isomorphic graphs. Through extensive experimentation\ncomprising over 10,000 CPU hours, we systematically evaluate different\ncube-and-conquer variants on three well-studied combinatorial problems. Our\nmethodology combines prerun phases to collect learned constraints, various\ncubing strategies, and parameter tuning via algorithm configuration and\nLLM-generated design suggestions.\n  The comprehensive empirical evaluation provides new insights into effective\ncubing strategies for propagator-based SAT solving, with our best method\nachieving speedups of 2-3x from improved cubing and parameter tuning, providing\nan additional 1.5-2x improvement on harder instances.",
        "With transformer-based models and the pretrain-finetune paradigm becoming\nmainstream, the high storage and deployment costs of individual finetuned\nmodels on multiple tasks pose critical challenges. Delta compression attempts\nto lower the costs by reducing the redundancy of delta parameters (i.e., the\ndifference between the finetuned and pre-trained model weights). However,\nexisting methods usually face problems including data accessibility and\ntraining requirements. To tackle this issue, we introduce Delta-DCT, the first\ndata-free delta compression method inspired by classic JPEG image compression,\nleveraging the Discrete Cosine Transform (DCT). We first (a) group delta\nparameters within a layer into patches. Then we (b) assess the importance of\neach patch and allocate them with different quantization bit-widths.\nAfterwards, we (c) convert these patches to the DCT domain and conduct\nquantization to each patch based on the allocated bit-width. The proposed\nDelta-DCT does not require any training or data calibration, while achieving\nperformance comparable to or even surpassing original finetuned models under\n1-bit equivalent delta compression ratios on different kinds of models\nincluding: (1) recently-released LLMs of different sizes from 7B to 13B, (2)\nrelatively smaller language models including RoBERTa and T5 models, (3)\nvariants of vision transformer models, and (4) multi-modal BEiT-3 models.",
        "Prompting has emerged as the dominant paradigm for adapting large,\npre-trained transformer-based models to downstream tasks. The Prompting\nDecision Transformer (PDT) enables large-scale, multi-task offline\nreinforcement learning pre-training by leveraging stochastic trajectory prompts\nto identify the target task. However, these prompts are sampled uniformly from\nexpert demonstrations, overlooking a critical limitation: Not all prompts are\nequally informative for differentiating between tasks. To address this, we\npropose an inference time bandit-based prompt-tuning framework that explores\nand optimizes trajectory prompt selection to enhance task performance. Our\nexperiments indicate not only clear performance gains due to bandit-based\nprompt-tuning, but also better sample complexity, scalability, and prompt space\nexploration compared to prompt-tuning baselines.",
        "Deep learning implemented via neural networks, has revolutionized machine\nlearning by providing methods for complex tasks such as object\ndetection\/classification and prediction. However, architectures based on deep\nneural networks have started to yield diminishing returns, primarily due to\ntheir statistical nature and inability to capture causal structure in the\ntraining data. Another issue with deep learning is its high energy consumption,\nwhich is not that desirable from a sustainability perspective.\n  Therefore, alternative approaches are being considered to address these\nissues, both of which are inspired by the functioning of the human brain. One\napproach is causal learning, which takes into account causality among the items\nin the dataset on which the neural network is trained. It is expected that this\nwill help minimize the spurious correlations that are prevalent in the learned\nrepresentations of deep neural networks. The other approach is Neurochaos\nLearning, a recent development, which draws its inspiration from the nonlinear\nchaotic firing intrinsic to neurons in biological neural networks\n(brain\/central nervous system). Both approaches have shown improved results\nover just deep learning alone.\n  To that end, in this position paper, we investigate how causal and neurochaos\nlearning approaches can be integrated together to produce better results,\nespecially in domains that contain linked data. We propose an approach for this\nintegration to enhance classification, prediction and reinforcement learning.\nWe also propose a set of research questions that need to be investigated in\norder to make this integration a reality.",
        "M. E. Larsen evaluated the Wronskian determinant of functions\n$\\{\\sin(mx)\\}_{1\\le m \\le n}$. We generalize this result and compute the\nWronskian of $\\{\\sin(mx)\\}_{1\\le m \\le n-1}\\cup \\{\\sin((k+n)x\\} $. We show that\nthis determinant can be expressed in terms of Gegenbauer orthogonal polynomials\nand we give two proofs of this result: a direct proof using recurrence\nrelations and a less direct (but, possibly, more instructive) proof based on\nDarboux-Crum transformations.",
        "In this paper, we present an hp-adaptive hybrid Discontinuous Galerkin\/Finite\nVolume method for simulating compressible, turbulent multi-component flows.\nBuilding on a previously established hp-adaptive strategy for hyperbolic gas-\nand droplet-dynamics problems, this study extends the hybrid DG\/FV approach to\nviscous flows with multiple species and incorporates non-conforming interfaces,\nenabling enhanced flexibility in grid generation. A central contribution of\nthis work lies in the computation of both convective and dissipative fluxes\nacross non-conforming element interfaces of mixed discretizations. To achieve\naccurate shock localization and scale-resolving representation of turbulent\nstructures, the operator dynamically switches between an h-refined FV sub-cell\nscheme and a p-adaptive DG method, based on an a priori modal solution\nanalysis. The method is implemented in the high-order open-source framework\nFLEXI and validated against benchmark problems, including the supersonic\nTaylor-Green vortex and a triplepoint shock interaction, demonstrating its\nrobustness and accuracy for under-resolved shock-turbulence interactions and\ncompressible multi-species scenarios. Finally, the method's capabilities are\nshowcased through an implicit large eddy simulation of an under-expanded\nhydrogen jet mixing with air, highlighting its potential for tackling\nchallenging compressible multi-species flows in engineering.",
        "Medical images are acquired at high resolutions with large fields of view in\norder to capture fine-grained features necessary for clinical decision-making.\nConsequently, training deep learning models on medical images can incur large\ncomputational costs. In this work, we address the challenge of downsizing\nmedical images in order to improve downstream computational efficiency while\npreserving clinically-relevant features. We introduce MedVAE, a family of six\nlarge-scale 2D and 3D autoencoders capable of encoding medical images as\ndownsized latent representations and decoding latent representations back to\nhigh-resolution images. We train MedVAE autoencoders using a novel two-stage\ntraining approach with 1,052,730 medical images. Across diverse tasks obtained\nfrom 20 medical image datasets, we demonstrate that (1) utilizing MedVAE latent\nrepresentations in place of high-resolution images when training downstream\nmodels can lead to efficiency benefits (up to 70x improvement in throughput)\nwhile simultaneously preserving clinically-relevant features and (2) MedVAE can\ndecode latent representations back to high-resolution images with high\nfidelity. Our work demonstrates that large-scale, generalizable autoencoders\ncan help address critical efficiency challenges in the medical domain. Our code\nis available at https:\/\/github.com\/StanfordMIMI\/MedVAE.",
        "It is shown that quantum effects play determining role in nitrogen adatom\ndiffusion due to several different factors. This could be related to the change\nof the energy of the quantum states and also due to the redistribution of\nelectrons between the quantum states, both full and resonant, via quantum\nstatistics partially governed by the Fermi energy level. These effects were\nstudied in the case of nitrogen diffusion over clean and gallium covered\nGa-terminated GaN(0001) surface. For the fractional coverage the density\nfunctional theory (DFT) calculations show that at the saddle point\nconfiguration the redistribution of electrons between different quantum states\nmay affect the surface diffusion barrier significantly. The other quantum\ninfluence occurs via the change of the minimal energy configuration. Under\nfractional Ga coverage of GaN(0001) surface the nitrogen diffusion energy\nbarrier proceeds from the resonant states governed energy minimal H3 site\nacross the saddle point in the bridge configuration. At this path the barrier\nis affected the electron redistribution between surface quantum states both in\nthe initial and the saddle point. In the case of the full GaN coverage the\ndiffusion path is from on-top N adatom configuration via H3 site that\ncorresponds to maximal energy. Therefore the diffusion barrier is Ebar= 1.18 eV\nfor clean and Ebar= 0.92 eV for (1\/6) ML to finally Ebar= 1.23 eV for full Ga\ncoverage. Thus the overall barrier is reduced to Ebar= 0.92 eV due to quantum\nstatistics effects. The identified stable N on-top configuration for the full\ncoverage is essential for atomic mechanism of GaN growth in Ga-rich regime.",
        "Accurate and efficient volumetric magnetic field measurements are essential\nfor a wide range of applications. Conventional methods are often limited in\nterms of measurement speed and applicability, or suffer from scaling problems\nat larger volumes. This work presents the development of a magnetometer array\ndesigned to measure magnetic fields within a spherical volume at a frame rate\nof 10 Hz. The array consists of 3D Hall magnetometers positioned according to a\nspherical $t$-design, allowing simultaneous magnetic field data acquisition\nfrom the surface of the sphere. The approach enables the efficient\nrepresentation of all three components of the magnetic field inside the sphere\nusing a sixth-degree polynomial, significantly reducing measurement time\ncompared to sequential methods. This work details the design, calibration, and\nmeasurement methods of the array. To evaluate its performance, we compare it to\na sequential single-sensor measurement by examining a magnetic gradient field.\nThe obtained measurement uncertainties of approx. 1% show the applicability for\na variety of applications.",
        "Object tracking is a key challenge of computer vision with various\napplications that all require different architectures. Most tracking systems\nhave limitations such as constraining all movement to a 2D plane and they often\ntrack only one object. In this paper, we present a new modular pipeline that\ncalculates 3D trajectories of multiple objects. It is adaptable to various\nsettings where multiple time-synced and stationary cameras record moving\nobjects, using off the shelf webcams. Our pipeline was tested on the Table\nSetting Dataset, where participants are recorded with various sensors as they\nset a table with tableware objects. We need to track these manipulated objects,\nusing 6 rgb webcams. Challenges include: Detecting small objects in 9.874.699\ncamera frames, determining camera poses, discriminating between nearby and\noverlapping objects, temporary occlusions, and finally calculating a 3D\ntrajectory using the right subset of an average of 11.12.456 pixel coordinates\nper 3-minute trial. We implement a robust pipeline that results in accurate\ntrajectories with covariance of x,y,z-position as a confidence metric. It deals\ndynamically with appearing and disappearing objects, instantiating new Extended\nKalman Filters. It scales to hundreds of table-setting trials with very little\nhuman annotation input, even with the camera poses of each trial unknown. The\ncode is available at https:\/\/github.com\/LarsBredereke\/object_tracking",
        "We consider time-dependent singular stochastic partial differential equations\non the three-dimensional torus. These equations are only well-posed after one\nadds renormalization terms. In order to construct a well-defined notion of\nsolution, one should put the equation in a more general setting, like the one\nof regularity structures. In this article, we consider the alternative paradigm\nof paracontrolled distributions, and get concentration results around a stable\ndeterministic equilibrium for solutions of non-autonomous generalizations of\nthe $(\\Phi_3^4)$ model.",
        "In this paper, we establish the quasi-compactness of the transfer operator\nassociated with skew product systems that are semi-conjugate to piecewise\nconvex maps with a countably infinite number of branches. These non-invertible\nskew products admit discontinuities, with the critical set confined to a\ncountable collection of fibers. Furthermore, we demonstrate that such systems\npossess an invariant measure whose disintegration along the fibers exhibits\nbounded variation, a concept introduced and developed in this work.",
        "We provide a simple proof for of the central limit theorem for the number of\nvertices in the giant for super-critical stochastic block model using the\nbreadth-first walk of Konarovskyi, Limic and the author (2024). Our approach\nfollows the recent work of Corujo, Limic and Lemaire (2024) and reduces to the\nclassic central limit theorem for the Erd\\H{o}s-R\\'{e}nyi model obtained by\nStepanov (1970).",
        "Time series modeling and analysis has become critical in various domains.\nConventional methods such as RNNs and Transformers, while effective for\ndiscrete-time and regularly sampled data, face significant challenges in\ncapturing the continuous dynamics and irregular sampling patterns inherent in\nreal-world scenarios. Neural Differential Equations (NDEs) represent a paradigm\nshift by combining the flexibility of neural networks with the mathematical\nrigor of differential equations. This paper presents a comprehensive review of\nNDE-based methods for time series analysis, including neural ordinary\ndifferential equations, neural controlled differential equations, and neural\nstochastic differential equations. We provide a detailed discussion of their\nmathematical formulations, numerical methods, and applications, highlighting\ntheir ability to model continuous-time dynamics. Furthermore, we address key\nchallenges and future research directions. This survey serves as a foundation\nfor researchers and practitioners seeking to leverage NDEs for advanced time\nseries analysis.",
        "This study focuses on the classification of cancerous and healthy slices from\nmultimodal lung images. The data used in the research comprises Computed\nTomography (CT) and Positron Emission Tomography (PET) images. The proposed\nstrategy achieves the fusion of PET and CT images by utilizing Principal\nComponent Analysis (PCA) and an Autoencoder. Subsequently, a new ensemble-based\nclassifier developed, Deep Ensembled Multimodal Fusion (DEMF), employing\nmajority voting to classify the sample images under examination.\nGradient-weighted Class Activation Mapping (Grad-CAM) employed to visualize the\nclassification accuracy of cancer-affected images. Given the limited sample\nsize, a random image augmentation strategy employed during the training phase.\nThe DEMF network helps mitigate the challenges of scarce data in computer-aided\nmedical image analysis. The proposed network compared with state-of-the-art\nnetworks across three publicly available datasets. The network outperforms\nothers based on the metrics - Accuracy, F1-Score, Precision, and Recall. The\ninvestigation results highlight the effectiveness of the proposed network.",
        "We investigate the origin of photometric variability in the classical T Tauri\nstar TW Hya by comparing light curves obtained by TESS and ground-based\ntelescopes with light curves created using three-dimensional (3D)\nmagnetohydrodynamic (MHD) simulations. TW Hya is modeled as a rotating star\nwith a dipole magnetic moment, slightly tilted about the rotational axis. We\nobserved that for various model parameters, matter accretes in the unstable\nregime and produces multiple hot spots on the star's surface, which leads to\nstochastic-looking light curves similar to the observed ones. Wavelet and\nFourier spectra of observed and modeled light curves show multiple\nquasiperiodic oscillations (QPOs) with quasiperiods from less than 0.1 to 9\ndays. Models show that variation in the strength and tilt of the dipole\nmagnetosphere leads to different periodograms, where the period of the star may\ndominate or be hidden. The amplitude of QPOs associated with the stellar period\ncan be smaller than that of other QPOs if the tilt of the dipole magnetosphere\nis small and when the unstable regime is stronger. In models with small\nmagnetospheres, the short-period QPOs associated with rotation of the inner\ndisc dominate and can be mistaken for a stellar period. We show that\nlonger-period (5-9 days) QPOs can be caused by waves forming beyond the\ncorotation radius.",
        "Microspine grippers are small spines commonly found on insect legs that\nreinforce surface interaction by engaging with asperities to increase shear\nforce and traction. An array of such microspines, when integrated into the\nlimbs or undercarriage of a robot, can provide the ability to maneuver uneven\nterrains, traverse inclines, and even climb walls. Conformability and\nadaptability of soft robots makes them ideal candidates for these applications\ninvolving traversal of complex, unstructured terrains. However, there remains a\nreal-life realization gap for soft locomotors pertaining to their transition\nfrom controlled lab environment to the field by improving grip stability\nthrough effective integration of microspines. We propose a passive, compliant\nmicrospine stacked array design to enhance the locomotion capabilities of\nmobile soft robots, in our case, ones that are motor tendon actuated. We offer\na standardized microspine array integration method with effective\nsoft-compliant stiffness integration, and reduced complexity resulting from a\nsingle actuator passively controlling them. The presented design utilizes a\ntwo-row, stacked microspine array configuration that offers additional gripping\ncapabilities on extremely steep\/irregular surfaces from the top row while not\nhindering the effectiveness of the more frequently active bottom row. We\nexplore different configurations of the microspine array to account for\nchanging surface topologies and enable independent, adaptable gripping of\nasperities per microspine. Field test experiments are conducted on various\nrough surfaces including concrete, brick, compact sand, and tree roots with\nthree robots consisting of a baseline without microspines compared against two\nrobots with different combinations of microspine arrays. Tracking results\nindicate that the inclusion of microspine arrays increases planar displacement\non average by 15 and 8 times.",
        "$\\text{La}_2 \\text{Ni}_7$ is an itinerant magnet with a small saturated\nmoment of $\\sim$ 0.1 $\\mu_{B}\/\\text{Ni}$ and a series of antiferromagnetic\n(AFM) transitions at $T_1$ = 61.0 K, $T_2$ = 56.5 K and $T_3$ = 42.2 K.\nTemperature and field dependent measurements suggest a complex, anisotropic\n$H-T$ phase diagram with multiple phase lines. Here we present the growth and\ncharacterization of single crystals of the ${\\text{La}_{2}\\text{(Cu}_{x}\\text\n{Ni}_{1-x})_7}$ series for 0 $\\leq x \\leq$ 0.181. Using a suite of anisotropic\nmagnetic, transport, and thermodynamic measurements we study the evolution of\nthe three AFM transitions upon Cu substitution. For ${0 \\leq x \\leq 0.097}$,\nthe system remains magnetically ordered at base temperature with $x \\leq$\n0.012, showing signs of three primarily AFM phases. For the higher substitution\nlevels, ${0.125 \\leq x \\leq 0.181}$, there are no signatures of magnetic\nordering, but an anomalous feature in resistance and heat capacity data are\nobserved which are consistent with the Kondo effect in this system. The\nintermediate $x$ = 0.105 sample lies in between the magnetic ordered and the\nKondo regime and is in the vicinity of the AFM-quantum critical point (QCP).\nThus, ${\\text{La}_{2}\\text{(Cu}_{x}\\text {Ni}_{1-x})_7}$ is an example of a\nsmall moment system that can be tuned through a QCP. Given these data combined\nwith the fact that the $\\text{La}_2 \\text{Ni}_7$ structure has kagome-like,\nNi-sublattice running perpendicular to the crystallographic $c-$ axis, and a\n$3d$-electron flat band that contributes to the density of states near the\nFermi energy, it becomes a promising candidate to host and study exotic\nphysics.",
        "The non-cloning theorem of quantum states provides security, but also limits\nthe Secret Key Rate (SKR) for Quantum Key Distribution (QKD) implementations.\nMultiplexing is a widely used technique to enhance data rates in classical\ncommunication systems and can also increase the SKR in QKD systems. Using\nlinearly polarized (LP) modes is an attractive solution as it is compatible\nwith simple fiber designs. This work demonstrates a fiber-based QKD system\nemploying LP mode multiplexing with a photonic lantern to convert the\nfundamental mode ($LP_{01}$) in separate fibers into higher-order modes\n($LP_{11}$) in a single few-mode fiber. The performance of the system is\nsensitive to polarization dependence, mode alignment, and environmental\ncrosstalk, which requires precise polarization control to minimize Quantum Bit\nError Rate (QBER). We report a SKR of 2.34 Mbps over a 24 km 5dB loss fiber\nlink.",
        "Depinning dynamics of a two-dimensional (2D) solid dusty plasma modulated by\na one-dimensional (1D) vibrational periodic substrate are investigated using\nLangevin dynamical simulations. As the uniform driving force increases\ngradually, from the overall drift velocity varying with the driving force, four\nsignificant Shapiro steps are discovered. The data analysis indicate that, when\nthe ratio of the frequency from the drift motion over potential wells to the\nexternal frequency from the modulation substrate is close to integers, dynamic\nmode locking occurs, corresponding to the discovered Shapiro steps. Around both\ntermini of the first and fourth Shapiro steps, the transitions are found to be\nalways continuous, however, the transition between the second and third Shapiro\nsteps is discontinuous, probably due to the different arrangements of\nparticles.",
        "Estimating the Number of Distinct Values (NDV) is fundamental for numerous\ndata management tasks, especially within database applications. However, most\nexisting works primarily focus on introducing new statistical or learned\nestimators, while identifying the most suitable estimator for a given scenario\nremains largely unexplored. Therefore, we propose AdaNDV, a learned method\ndesigned to adaptively select and fuse existing estimators to address this\nissue. Specifically, (1) we propose to use learned models to distinguish\nbetween overestimated and underestimated estimators and then select appropriate\nestimators from each category. This strategy provides a complementary\nperspective by integrating overestimations and underestimations for error\ncorrection, thereby improving the accuracy of NDV estimation. (2) To further\nintegrate the estimation results, we introduce a novel fusion approach that\nemploys a learned model to predict the weights of the selected estimators and\nthen applies a weighted sum to merge them. By combining these strategies, the\nproposed AdaNDV fundamentally distinguishes itself from previous works that\ndirectly estimate NDV. Moreover, extensive experiments conducted on real-world\ndatasets, with the number of individual columns being several orders of\nmagnitude larger than in previous studies, demonstrate the superior performance\nof our method.",
        "Recent advancements in data-driven weather forecasting models have delivered\ndeterministic models that outperform the leading operational forecast systems\nbased on traditional, physics-based models. However, these data-driven models\nare typically trained with a mean squared error loss function, which causes\nsmoothing of fine scales through a \"double penalty\" effect. We develop a\nsimple, parameter-free modification to this loss function that avoids this\nproblem by separating the loss attributable to decorrelation from the loss\nattributable to spectral amplitude errors. Fine-tuning the GraphCast model with\nthis new loss function results in sharp deterministic weather forecasts, an\nincrease of the model's effective resolution from 1,250km to 160km,\nimprovements to ensemble spread, and improvements to predictions of tropical\ncyclone strength and surface wind extremes."
      ]
    }
  },
  {
    "id":2411.05055,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems",
    "start_abstract":"TensorFlow is an interface for expressing machine learning algorithms, and implementation executing such algorithms. A computation expressed using can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices as phones tablets up to large-scale distributed systems hundreds machines thousands computational GPU cards. The system flexible used express including training inference algorithms deep neural network models, it has been conducting research deploying into production across more than dozen areas computer science other fields, speech recognition, vision, robotics, information retrieval, natural language processing, geographic extraction, drug discovery. This paper describes the that we have built at Google. API reference were released open-source package under Apache 2.0 license in November, 2015 are available www.tensorflow.org.",
    "start_categories":[
      "cs.CL"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "Predicting Functional Effect of Human Missense Mutations Using PolyPhen\u20102"
      ],
      "abstract":[
        "Abstract PolyPhen\u20102 (Polymorphism Phenotyping v2), available as software and via a Web server, predicts the possible impact of amino acid substitutions on stability function human proteins using structural comparative evolutionary considerations. It performs functional annotation single\u2010nucleotide polymorphisms (SNPs), maps coding SNPs to gene transcripts, extracts protein sequence annotations attributes, builds conservation profiles. then estimates probability missense mutation being damaging based combination all these properties. features include high\u2010quality multiple alignment pipeline prediction method employing machine\u2010learning classification. The also integrates UCSC Genome Browser's genome MultiZ alignments vertebrate genomes with genome. is capable analyzing large volumes data produced by next\u2010generation sequencing projects, thanks built\u2010in support for high\u2010performance computing environments like Grid Engine Platform LSF. Curr. Protoc. Hum. Genet . 76:7.20.1\u20107.20.41. \u00a9 2013 John Wiley &amp; Sons, Inc."
      ],
      "categories":[
        "q-bio.GN"
      ]
    },
    "list":{
      "title":[
        "Non-uniqueness of normalized NLS ground states on bounded domains with\n  homogeneous Neumann boundary conditions",
        "Drinfeld modules with maximal Galois action",
        "Toughness of double network hydrogels: the role of reduced stress\n  propagation",
        "Traffic noise assessment in urban Bulgaria using explainable machine\n  learning",
        "Quadratic BSDEs with Singular Generators and Unbounded Terminal\n  Conditions: Theory and Applications",
        "Improved Online Confidence Bounds for Multinomial Logistic Bandits",
        "The three-dimensional impulse-response model: Modeling the training\n  process in accordance with energy system-specific adaptation",
        "Polarization-controlled strong light-matter interaction with templated\n  molecular aggregates",
        "Joint Power Allocation and Phase Shift Design for Stacked Intelligent\n  Metasurfaces-aided Cell-Free Massive MIMO Systems with MARL",
        "An exact closed walks series formula for the complexity of regular\n  graphs and some related bounds",
        "Transformer-Enhanced Variational Autoencoder for Crystal Structure\n  Prediction",
        "Experimental Realization of Special-Unitary Operations in Classical\n  Mechanics by Non-Adiabatic Evolutions",
        "Allostatic Control of Persistent States in Spiking Neural Networks for\n  perception and computation",
        "Multi-messenger detection of black hole binaries in dark matter spikes",
        "Steady compressible Navier-Stokes-Fourier system with general\n  temperature dependent viscosities I: density estimates based on Bogovskii\n  operator",
        "The Kodaira Embedding Theorem",
        "Doping dependence of the magnetic ground state in the frustrated magnets\n  Ba$_2$$M$Te$_{1-x}$W$_{x}$O$_6$ ($M$ = Mn, Co)",
        "Geodesic Variational Bayes for Multiway Covariances",
        "Non-commutative derived analytic moduli functors",
        "Fast, accurate, and predictive method for atom detection in\n  site-resolved images of microtrap arrays",
        "Improved bounds on collapse models from rotational noise of LISA\n  Pathfinder",
        "Degenerate parabolic equations in divergence form: fundamental solution\n  and Gaussian bounds",
        "Finiteness of non-degenerate central configurations of the planar\n  $n$-body problem with a homogeneous potential",
        "OGBoost: A Python Package for Ordinal Gradient Boosting",
        "Numerical verification of the Collatz conjecture for billion digit\n  random numbers",
        "Riemann surface foliations with non-discrete singular set",
        "Apparent nonreciprocal transport in FeSe bulk crystals",
        "An orphan flare from a plasma blob crossing the broad-line region ?",
        "Gravity-induced collisions of uncharged cloud droplets in an electric\n  field"
      ],
      "abstract":[
        "We provide a general non-uniqueness result for normalized ground states of\nnonlinear Schr\\\"odinger equations with pure power nonlinearity on bounded\ndomains with homogeneous Neumann boundary conditions, defined as global\nminimizers of the associated energy functional among functions with prescribed\nmass. Precisely, for nonlinearity powers slightly smaller than the\n$L^2$-critical exponent, we prove that there always exists at least one value\nof the mass for which normalized ground states are not unique.",
        "With a fixed prime power $q>1$, define the ring of polynomials\n$A=\\mathbb{F}_q[t]$ and its fraction field $F=\\mathbb{F}_q(t)$. For each pair\n$a=(a_1,a_2) \\in A^2$ with $a_2$ nonzero, let $\\phi(a)\\colon A\\to F\\{\\tau\\}$ be\nthe Drinfeld $A$-module of rank $2$ satisfying $t\\mapsto t+a_1\\tau+a_2\\tau^2$.\nThe Galois action on the torsion of $\\phi(a)$ gives rise to a Galois\nrepresentation $\\rho_{\\phi(a)}\\colon\n\\operatorname{Gal}(F^{\\operatorname{sep}}\/F)\\to\n\\operatorname{GL}_2(\\widehat{A})$, where $\\widehat{A}$ is the profinite\ncompletion of $A$. We show that the image of $\\rho_{\\phi(a)}$ is large for\nrandom $a$. More precisely, for all $a\\in A^2$ away from a set of density $0$,\nwe prove that the index\n$[\\operatorname{GL}_2(\\widehat{A}):\\rho_{\\phi(a)}(\\operatorname{Gal}(F^{\\operatorname{sep}}\/F))]$\ndivides $q-1$ when $q>2$ and divides $4$ when $q=2$. We also show that the\nrepresentation $\\rho_{\\phi(a)}$ is surjective for a positive density set of\n$a\\in A^2$.",
        "Double network hydrogels show remarkable mechanical performance, combining\nhigh strength and fracture toughness with sufficient stiffness to bear load,\ndespite containing only a low density of cross-linked polymer molecules in\nwater. We introduce a simple mesoscale model of a double network material,\ndetailed enough to resolve the salient microphysics of local plastic bond\nbreakage, yet simple enough to address macroscopic cracking. Load sharing\nbetween the networks results in a delocalisation of stress such that the double\nnetwork inherits both the stiffness of its stiff-and-brittle sacrificial\nnetwork and the ductility of its soft-and-ductile matrix network. The\nunderlying mechanism is a reduction in the Eshelby stress propagator between\nsacrificial bonds, inhibiting the tendency for the plastic failure of one\nsacrificial bond to propagate stress to neighbouring sacrificial bonds and\ncause a follow-on cascade of breakages. The mechanism of brittle macroscopic\ncracking is thereby suppressed, giving instead ductile deformation via\ndiffusely distributed microcracking.",
        "Fine-grained noise maps are vital for epidemiological studies on traffic\nnoise. However, detailed information on traffic noise is often limited,\nespecially in Eastern Europe. Rigid linear noise land-use regressions are\ntypically employed to estimate noise levels; however, machine learning likely\noffers more accurate noise predictions. We innovated by comparing the\npredictive accuracies of supervised machine learning models to estimate traffic\nnoise levels across the five largest Bulgarian cities. In situ A-weighted\nequivalent continuous sound levels were obtained from 232 fixed-site monitors\nacross these cities. We included transport- and land-use-related predictors\nusing 50-1,000 m buffers. Extreme gradient boosting (XGB) had the highest\nten-fold cross-validated fit (R2=0.680) and the lowest root mean square error\n(RMSE=4.739), insignificantly besting the random forest-based model (R2=0.667,\nRMSE=4.895). Support vector regression (R2=0.633, RMSE=5.358), elastic net\n(R2=0.568, RMSE=5.625), and linear regression (R2=0.548, RMSE=5.569) performed\nsignificantly worse. Shapley values for the XGB showed that the length of major\nroads within 100 m buffers, footways within 50 m buffers, residential roads\nwithin 50 m buffers, and the number of buildings within 50 m buffers were\nimportant non-linear predictors. Our spatially resolved noise maps revealed\nstriking geographic noise variations and that, on average, 96.8% of the urban\npopulation experiences harmful noise levels.",
        "We investigate a class of quadratic backward stochastic differential\nequations (BSDEs) with generators singular in $ y $. First, we establish the\nexistence of solutions and a comparison theorem, thereby extending results in\nthe literature. Additionally, we analyze the stability property and the\nFeynman-Kac formula, and prove the uniqueness of viscosity solutions for the\ncorresponding singular semilinear partial differential equations (PDEs).\nFinally, we demonstrate applications in the context of robust control linked to\nstochastic differential utility and certainty equivalent based on\n$g$-expectation. In these applications, the coefficient of the quadratic term\nin the generator captures the level of ambiguity aversion and the coefficient\nof absolute risk aversion, respectively.",
        "In this paper, we propose an improved online confidence bound for multinomial\nlogistic (MNL) models and apply this result to MNL bandits, achieving\nvariance-dependent optimal regret. Recently, Lee & Oh (2024) established an\nonline confidence bound for MNL models and achieved nearly minimax-optimal\nregret in MNL bandits. However, their results still depend on the\nnorm-boundedness of the unknown parameter $B$ and the maximum size of possible\noutcomes $K$. To address this, we first derive an online confidence bound of\n$O\\left(\\sqrt{d \\log t} + B \\right)$, which is a significant improvement over\nthe previous bound of $O (B \\sqrt{d} \\log t \\log K )$ (Lee & Oh, 2024). This is\nmainly achieved by establishing tighter self-concordant properties of the MNL\nloss and introducing a novel intermediary term to bound the estimation error.\nUsing this new online confidence bound, we propose a constant-time algorithm,\nOFU-MNL++, which achieves a variance-dependent regret bound of $O \\Big( d \\log\nT \\sqrt{ \\sum_{t=1}^T \\sigma_t^2 } \\Big) $ for sufficiently large $T$, where\n$\\sigma_t^2$ denotes the variance of the rewards at round $t$, $d$ is the\ndimension of the contexts, and $T$ is the total number of rounds. Furthermore,\nwe introduce a Maximum Likelihood Estimation (MLE)-based algorithm,\nOFU-MN$^2$L, which achieves an anytime poly(B)-free regret of $O \\Big( d \\log\n(BT) \\sqrt{ \\sum_{t=1}^T \\sigma_t^2 } \\Big) $.",
        "Athletic training is characterized by physiological systems responding to\nrepeated exercise-induced stress, resulting in gradual alterations in the\nfunctional properties of these systems. The adaptive response leading to\nimproved performance follows a remarkably predictable pattern that may be\ndescribed by a systems model provided that training load can be accurately\nquantified and that the constants defining the training-performance\nrelationship are known. While various impulse-response models have been\nproposed, they are inherently limited in reducing training stress (the impulse)\ninto a single metric, assuming that the adaptive responses are independent of\nthe type of training performed. This is despite ample evidence of markedly\ndiverse acute and chronic responses to exercise of different intensities and\ndurations. Herein, we propose an alternative, three-dimensional\nimpulse-response model that uses three training load metrics as inputs and\nthree performance metrics as outputs. These metrics, represented by a\nthree-parameter critical power model, reflect the stress imposed on each of the\nthree energy systems: the alactic (phosphocreatine\/immediate) system; the\nlactic (glycolytic) system; and the aerobic (oxidative) system. The purpose of\nthis article is to outline the scientific rationale and the practical\nimplementation of the three-dimensional impulse-response model.",
        "We demonstrate strong light-matter interaction for a layer of templated\nmerocyanine molecules in a planar microcavity. Using a single layer of graphene\nnanoribbons as a templating layer, we obtain an aligned layer of aggregated\nmolecules. The molecular layer displays anisotropic optical properties\nresembling those of a biaxial crystal. The anisotropic excitonic component in\nthe cavity results in strongly polarization-dependent light-matter interaction\nand in increased Rabi-energies. The increased light-matter interaction is\npossibly due to reduced molecular disorder in the templated molecular layer.\nThis conclusion is supported by an analysis based on a multi-oscillator model.\nWe further use photoluminescence microspectroscopy to demonstrate that the\nlight-matter coupling is spatially homogeneous. Our study introduces molecular\ntemplating to strong light-matter studies. The reduced disorder of the system\nas a consequence of templating is highly beneficial for engineering\nlight-matter interaction.",
        "Cell-free (CF) massive multiple-input multiple-output (mMIMO) systems offer\nhigh spectral efficiency (SE) through multiple distributed access points (APs).\nHowever, the large number of antennas increases power consumption. We propose\nincorporating stacked intelligent metasurfaces (SIM) into CF mMIMO systems as a\ncost-effective, energy-efficient solution. This paper focuses on optimizing the\njoint power allocation of APs and the phase shift of SIMs to maximize the sum\nSE. To address this complex problem, we introduce a fully distributed\nmulti-agent reinforcement learning (MARL) algorithm. Our novel algorithm, the\nnoisy value method with a recurrent policy in multi-agent policy optimization\n(NVR-MAPPO), enhances performance by encouraging diverse exploration under\ncentralized training and decentralized execution. Simulations demonstrate that\nNVR-MAPPO significantly improves sum SE and robustness across various\nscenarios.",
        "The complexity of a graph is the number of its labeled spanning trees. In\nthis work complexity is studied in settings that admit regular graphs. An exact\nformula is established linking complexity of the complement of a regular graph\nto numbers of closed walks in the graph by way of an infinite alternating\nseries. Some consequences of this result yield infinite classes of lower and\nupper bounds on the complexity of such graphs. Applications of these\nmathematical results to biological problems on neuronal activity are described.",
        "Crystal structure forms the foundation for understanding the physical and\nchemical properties of materials. Generative models have emerged as a new\nparadigm in crystal structure prediction(CSP), however, accurately capturing\nkey characteristics of crystal structures, such as periodicity and symmetry,\nremains a significant challenge. In this paper, we propose a\nTransformer-Enhanced Variational Autoencoder for Crystal Structure Prediction\n(TransVAE-CSP), who learns the characteristic distribution space of stable\nmaterials, enabling both the reconstruction and generation of crystal\nstructures. TransVAE-CSP integrates adaptive distance expansion with\nirreducible representation to effectively capture the periodicity and symmetry\nof crystal structures, and the encoder is a transformer network based on an\nequivariant dot product attention mechanism. Experimental results on the\ncarbon_24, perov_5, and mp_20 datasets demonstrate that TransVAE-CSP\noutperforms existing methods in structure reconstruction and generation tasks\nunder various modeling metrics, offering a powerful tool for crystal structure\ndesign and optimization.",
        "Artificial classical wave systems such as wave crystals and metamaterials\nhave demonstrated promising capabilities in simulating a wide range of quantum\nmechanical phenomena. Yet some gaps between quantum and classical worlds are\ngenerally considered fundamental and difficult to bridge. Dynamics obeying\nspecial unitary groups, e.g., electronic spins described by SU(2), color\nsymmetries of fundamental particles described by SU(3), are such examples. In\nthis work, we present the experimental realization of universal SU(2) and SU(3)\ndynamic operations in classical mechanical oscillator systems with temporally\nmodulated coupling terms. Our approach relies on the sequential execution of\nnon-adiabatic holonomic evolutions, which are typically used in constructing\nquantum-logic gates. The method is swift and purely geometric and can be\nextended to realize more sophisticated dynamic operations. Our results open a\nnew way for studying and simulating quantum phenomena in classical systems.",
        "We introduce a novel model for updating perceptual beliefs about the\nenvironment by extending the concept of Allostasis to the control of internal\nrepresentations. Allostasis is a fundamental regulatory mechanism observed in\nanimal physiology that orchestrates responses to maintain a dynamic equilibrium\nin bodily needs and internal states. In this paper, we focus on an application\nin numerical cognition, where a bump of activity in an attractor network is\nused as a spatial numerical representation. While existing neural networks can\nmaintain persistent states, to date, there is no unified framework for\ndynamically controlling spatial changes in neuronal activity in response to\nenvironmental changes. To address this, we couple a well known allostatic\nmicrocircuit, the Hammel model, with a ring attractor, resulting in a Spiking\nNeural Network architecture that can modulate the location of the bump as a\nfunction of some reference input. This localized activity in turn is used as a\nperceptual belief in a simulated subitization task a quick enumeration process\nwithout counting. We provide a general procedure to fine-tune the model and\ndemonstrate the successful control of the bump location. We also study the\nresponse time in the model with respect to changes in parameters and compare it\nwith biological data. Finally, we analyze the dynamics of the network to\nunderstand the selectivity and specificity of different neurons to distinct\ncategories present in the input. The results of this paper, particularly the\nmechanism for moving persistent states, are not limited to numerical cognition\nbut can be applied to a wide range of tasks involving similar representations.",
        "We investigate the inspiral of a high mass-ratio black hole binary located in\nthe nucleus of a galaxy, where the primary central black hole is surrounded by\na dense dark matter spike formed through accretion during the black hole growth\nphase. Within this spike, dark matter undergoes strong self-annihilation,\nproducing a compact source of $\\gamma$-ray radiation that is highly sensitive\nto spike density, while the binary emits gravitational waves at frequencies\ndetectable by LISA. As the inspiralling binary interacts with the surrounding\ndark matter particles, it alters the density of the spike, thereby influencing\nthe $\\gamma$-ray flux from dark matter annihilation. We demonstrate that the\nspike self-annihilation luminosity decreases by $10\\%$ to $90\\%$ of its initial\nvalue, depending on the initial density profile and binary mass ratio, as the\nbinary sweeps through the LISA band. This presents a new opportunity to\nindirectly probe dark matter through multi-messenger observations of galactic\nnuclei.",
        "The aim of this paper is to reconsider the existence theory for steady\ncompressible Navier--Stokes--Fourier system assuming more general condition of\nthe dependence of the viscosities on the temperature in the form\n$\\mu(\\vartheta)$, $\\xi(\\vartheta) \\sim (1+\\vartheta)^\\alpha$ for $0\\leq \\alpha\n\\leq 1$. This extends the known theory for $\\alpha=1$ from and improves\nsignificantly the results for $\\alpha =0$. This paper is the first of a series\nof two papers dealing with this problem and is connected with the\nBogovskii-type estimates of the sequence of densities. This leads, among\nothers, to the limitation $\\gamma >\\frac 32$ for the pressure law\n$p(\\varrho,\\vartheta) \\sim \\varrho^\\gamma + \\varrho\\vartheta$. The paper\nconsiders both the heat-flux (Robin) and Dirichlet boundary conditions for the\ntemperature as well as both the homogeneous Dirichlet and zero inflow\/outflow\nNavier boundary conditions for the velocity. Further extension for $\\gamma >1$\nonly is based on different type of pressure estimates and will be the content\nof the subsequent paper.",
        "Chow's Theorem and GAGA are renowned results demonstrating the algebraic\nnature of projective manifolds and, more broadly, projective analytic\nvarieties. However, determining if a particular manifold is projective is not,\ngenerally, a simple task. The Kodaira Embedding Theorem provides an intrinsic\ncharacterization of projective varieties in terms of line bundles; in\nparticular, it states that a manifold is projective if and only if it admits a\npositive line bundle. We prove only the 'if' implication in this paper, giving\na sufficient condition for a manifold bundle to be embedded in projective\nspace. Along the way, we prove several other interesting results. Of particular\nnote is the Kodaira-Nakano Vanishing Theorem, a crucial tool for eliminating\nhigher cohomology of complex manifolds, as well as Lemmas 6.2 and 6.1, which\nprovide important relationships between divisors, line bundles, and blowups.\nAlthough this treatment is relatively self-contained, we omit a rigorous\ndevelopment of Hodge theory, some basic complex analysis results, and some\ntheorems regarding Cech cohomology (including Leray's Theorem).",
        "Theoretically, the relative change of the Heisenberg-type nearest-neighbor\ncoupling $J_1$ and next-nearest-neighbor coupling $J_2$ in the\nface-centered-cubic lattice can give rise to three main antiferromagnetic\norderings of type-I, type-II, and type-III. However, it is difficult to tune\nthe $J_2\/J_1$ ratio in real materials. Here, we report studies on the influence\nof Te$^{6+}$ and W$^{6+}$ ions replacement to the magnetic interactions and the\nmagnetic ground states in the double-perovskite compounds\nBa$_2$$M$Te$_{1-x}$W$_{x}$O$_6$ ($M$ = Mn, Co). For\nBa$_2$MnTe$_{1-x}$W$_{x}$O$_6$, the W$^{6+}$ doping on Te$^{6+}$ site is\nsuccessful in $0.02 \\leq x \\leq 0.9$ with short-range orders of the type-I\n($0.02 \\leq x \\leq 0.08$) and type-II ($0.1 \\leq x \\leq 0.9$). In\nBa$_2$CoTe$_{1-x}$W${_x}$O$_6$, x-ray diffraction measurements reveal two\ncrystal structures, including the trigonal phase ($0 \\leq x \\leq 0.1$) and the\ncubic phase ($0.5 \\leq x \\leq 1$), between which is a miscibility gap. Two\nmagnetic transitions are identified in the trigonal phase due to two magnetic\nsubsystems, and the type-II magnetic order is observed in the cubic phase.\nMagnetic phase diagrams of Ba$_2M$Te$_{1-x}$W$_{x}$O$_6$ ($M$ = Mn, Co) are\nestablished. Our work shows that the magnetic interactions and ground states of\nBa$_2$$M$Te$_{1-x}$W$_x$O$_6$ can be tuned effectively by the replacement of\nTe$^{6+}$ by W$^{6+}$ ions.",
        "This article explores the optimization of variational approximations for\nposterior covariances of Gaussian multiway arrays. To achieve this, we\nestablish a natural differential geometric optimization framework on the space\nusing the pullback of the affine-invariant metric. In the case of a truly\nseparable covariance, we demonstrate a joint approximation in the multiway\nspace outperforms a mean-field approximation in optimization efficiency and\nprovides a superior approximation to an unstructured Inverse-Wishart posterior\nunder the average Mahalanobis distance of the data while maintaining a multiway\ninterpretation. We moreover establish efficient expressions for the Euclidean\nand Riemannian gradients in both cases of the joint and mean-field\napproximation. We end with an analysis of commodity trade data.",
        "We develop a formulation for non-commutative derived analytic geometry built\nfrom differential graded (dg) algebras equipped with free entire functional\ncalculus (FEFC), relating them to simplicial FEFC algebras and to locally\nmultiplicatively convex complete topological dg algebras. The theory is\noptimally suited for accommodating analytic morphisms between functors of\nalgebraic origin, and we establish forms of Riemann-Hilbert equivalence in this\nsetting. We also investigate classes of topological dg algebras for which\nmoduli functors of analytic origin tend to behave well, and relate their\nhomotopy theory to that of FEFC algebras. Applications include the construction\nof derived non-commutative analytic moduli stacks of pro-\\'etale local systems\nand non-commutative derived twistor moduli functors, both equipped with shifted\nanalytic bisymplectic structures, and hence shifted analytic double Poisson\nstructures.",
        "We introduce a new method, rooted in estimation theory, to detect the\nindividual atoms in site-resolved images of microtrap arrays, such as optical\nlattices or optical tweezers arrays. Using simulated images, we demonstrate a\nten-fold reduction of the detection error rate compared to the popular method\nbased on Wiener deconvolution, under a wide range of experimental conditions.\nThe runtime is fully compatible with real-time applications, even for a very\nlarge arrays. Finally, we propose a rigorous definition for the signal-to-noise\nratio of an image, and show that it can be used as a predictor for the\ndetection error rate, which opens new prospect for the design of future\nexperiments.",
        "Spontaneous wavefunction collapse models offer a solution to the quantum\nmeasurement problem, by modifying the Schr\\\"odinger equation with nonlinear and\nstochastic terms. The Continuous Spontaneous Localisation (CSL) model is the\nmost studied among these models, with phenomenological parameters that are\nconstrained by experiments. Here, we exploit the recent analysis of LISA\nPathfinder's angular motion data to derive a tighter constraint than previously\nachieved with translational motion. Moreover, we identify the general\nconditions for preferring rotational measurement over translational ones for\nconstraining the CSL model.",
        "In this paper, we consider second order degenerate parabolic equations with\ncomplex, measurable, and time-dependent coefficients. The degenerate\nellipticity is dictated by a spatial $A_2$-weight. We prove that having a\ngeneralized fundamental solution with upper Gaussian bounds is equivalent to\nMoser's $L^2$-$L^\\infty$ estimates for local weak solutions. In the special\ncase of real coefficients, Moser's $L^2$-$L^\\infty$ estimates are known, which\nprovide an easier proof of Gaussian upper bounds, and a known Harnack\ninequality is then used to derive Gaussian lower bounds.",
        "We show that there exist an upper bound and a lower bound for the number of\nnon-degenerate central configurations of the n-body problem in the plane with a\nhomogeneous potential. In particular, both bounds are independent of the\nhomogeneous degree of the potential under consideration.",
        "This paper introduces OGBoost, a scikit-learn-compatible Python package for\nordinal regression using gradient boosting. Ordinal variables (e.g., rating\nscales, quality assessments) lie between nominal and continuous data,\nnecessitating specialized methods that reflect their inherent ordering. Built\non a coordinate-descent approach for optimization and the latent-variable\nframework for ordinal regression, OGBoost performs joint optimization of a\nlatent continuous regression function (functional gradient descent) and a\nthreshold vector that converts the latent continuous value into discrete class\nprobabilities (classical gradient descent). In addition to the stanadard\nmethods for scikit-learn classifiers, the GradientBoostingOrdinal class\nimplements a \"decision_function\" that returns the (scalar) value of the latent\nfunction for each observation, which can be used as a high-resolution\nalternative to class labels for comparing and ranking observations. The class\nhas the option to use cross-validation for early stopping rather than a single\nholdout validation set, a more robust approach for small and\/or imbalanced\ndatasets. Furthermore, users can select base learners with different underlying\nalgorithms and\/or hyperparameters for use throughout the boosting iterations,\nresulting in a `heterogeneous' ensemble approach that can be used as a more\nefficient alternative to hyperparameter tuning (e.g. via grid search). We\nillustrate the capabilities of OGBoost through examples, using the wine quality\ndataset from the UCI respository. The package is available on PyPI and can be\ninstalled via \"pip install ogboost\".",
        "The Collatz conjecture, also known as the 3n+1 problem, is one of the most\npopular open problems in number theory. In this note, an algorithm for the\nverification of the Collatz conjecture is presented that works on a standard PC\nfor numbers with up to ten billion decimal places.",
        "Let $\\mathcal{F}$ be a singular Riemann surface foliation on a complex\nmanifold $M$, such that the singular set $E \\subset M$ is non-discrete. We\nstudy the behavior of the foliation near the singular set $E$, particularly\nfocusing on singular points that admit invariant submanifolds (locally) passing\nthrough them. Our primary focus is on the singular points that are removable\nsingularities for some proper subfoliation. We classify singular points based\non the dimension of their invariant submanifold and, consequently, establish\nthat for hyperbolic foliations $\\mathcal{F}$, the presence of such\nsingularities ensures the continuity of the leafwise Poincar\\'{e} metric on $M\n\\setminus E$.",
        "We performed low-frequency ac first- and second-harmonic resistance\nmeasurements and dc $I-V$ measurements on bulk FeSe crystals in a temperature\nrange between 1.8 and 150 K and in magnetic field up to 14 T. We observed\nconsiderable second-harmonic resistance, indicative of nonreciprocal charge\ntransport, in some samples. By examining correlation between contact\nresistances and second-harmonic signals, we concluded that the second-harmonic\nresistance was not due to the genuine nonreciprocal transport effect but was\ncaused by joule heating at a current contact through the thermoelectric effect.\nOur conclusion is consistent with a recent preprint (Nagata \\textit{et al.},\narXiv:2409.01715), in which the authors reported a zero-field superconducting\ndiode effect in devices fabricated with FeSe flakes and attributed it to the\nthermoelectric effect.",
        "The blazar 3C 279 is well known for its prolific emission of rapid flares. A\nparticular event occurred on 12\/20\/2013, exhibiting a large flux increase with\na doubling time scale of a few hours, a very hard gamma-ray spectrum, and a\ntime-asymmetric light curve with slow decay, but no significant variations\ndetected in the optical range. We propose a novel scenario to interpret this\nflare, based on two emission zones, a stationary blob and a moving plasma blob.\nThe stationary blob, located within the BLR, accounts for the low-state\nemission. The moving blob decouples from the stationary zone, accelerates and\ncrosses the BLR. The high-energy flare is attributed to the variable external\nCompton emission as the blob moves through the BLR, while variations in the\nsynchrotron emission are negligible. Our interpretation differs from previous\ninterpretations by attributing the flare to the bulk motion and geometry of the\nexternal photon fields, without invoking varying electron injection.",
        "We investigate the collisions of uncharged, conducting droplets settling\nunder gravity in the presence of an external electric field. Previous studies\nhave derived a near-field asymptotic expression for the electric-field-induced\nattraction, suggesting that this force can overcome lubrication resistance and\ndrive surface-to-surface contact between two spherical conductors within a\nfinite time. However, for droplets moving in air, traditional lubrication\ntheory breaks down when the inter-droplet gap approaches the mean free path of\nair molecules. To account for this, we incorporate non-continuum hydrodynamic\neffects to estimate the gravity-driven collision efficiency under\nelectric-field-induced forces. This study examines how an external electric\nfield influences the trajectories of settling droplet pairs of unequal sizes.\nBy analyzing their motion, we compute collision efficiencies and explore their\ndependence on droplet size ratio, electric field strength, the angle between\nthe field and gravity, and key dimensionless parameters governing\nelectric-field-induced and van der Waals forces. Our findings reveal that\nelectric-field-induced forces significantly enhance collision efficiency,\nhighlighting their critical role in droplet coalescence dynamics."
      ]
    }
  },
  {
    "id":2411.05055,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"Predicting Functional Effect of Human Missense Mutations Using PolyPhen\u20102",
    "start_abstract":"Abstract PolyPhen\u20102 (Polymorphism Phenotyping v2), available as software and via a Web server, predicts the possible impact of amino acid substitutions on stability function human proteins using structural comparative evolutionary considerations. It performs functional annotation single\u2010nucleotide polymorphisms (SNPs), maps coding SNPs to gene transcripts, extracts protein sequence annotations attributes, builds conservation profiles. then estimates probability missense mutation being damaging based combination all these properties. features include high\u2010quality multiple alignment pipeline prediction method employing machine\u2010learning classification. The also integrates UCSC Genome Browser's genome MultiZ alignments vertebrate genomes with genome. is capable analyzing large volumes data produced by next\u2010generation sequencing projects, thanks built\u2010in support for high\u2010performance computing environments like Grid Engine Platform LSF. Curr. Protoc. Hum. Genet . 76:7.20.1\u20107.20.41. \u00a9 2013 John Wiley &amp; Sons, Inc.",
    "start_categories":[
      "q-bio.GN"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems"
      ],
      "abstract":[
        "TensorFlow is an interface for expressing machine learning algorithms, and implementation executing such algorithms. A computation expressed using can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices as phones tablets up to large-scale distributed systems hundreds machines thousands computational GPU cards. The system flexible used express including training inference algorithms deep neural network models, it has been conducting research deploying into production across more than dozen areas computer science other fields, speech recognition, vision, robotics, information retrieval, natural language processing, geographic extraction, drug discovery. This paper describes the that we have built at Google. API reference were released open-source package under Apache 2.0 license in November, 2015 are available www.tensorflow.org."
      ],
      "categories":[
        "cs.CL"
      ]
    },
    "list":{
      "title":[
        "A novel Facial Recognition technique with Focusing on Masked Faces",
        "Exact Maximin Share Fairness via Adjusted Supply",
        "Network-assisted collective operations for efficient distributed quantum\n  computing",
        "A Moving Mesh Isogeometric Method Based on Harmonic Maps",
        "Quasi-two-dimensional magnetism and antiferromagnetic ground state in\n  Li$_2$FeSiO$_4$",
        "Channel deformations during elastocapillary spreading of gaseous\n  embolisms in biomimetic leaves",
        "Pretraining Generative Flow Networks with Inexpensive Rewards for\n  Molecular Graph Generation",
        "Measuring Star Formation Rates in the Milky Way from Hi-GAL 70 $\\mu$m\n  Observations",
        "Non-negative tensor factorization-based dependence map analysis for\n  local damage detection in presence of non-Gaussian noise",
        "Low-energy insulating reconstructions of Si(111)-7x7 surface with and\n  without stacking fault discovered by graph theory",
        "Cultivating Precision: Comparative Analysis of Sensor-Based Yogurt\n  Fermentation Monitoring Techniques",
        "Existence and Uniqueness of Local Solutions for a Class of Partial\n  Differential-Algebraic Equations",
        "Generalizable automated ischaemic stroke lesion segmentation with vision\n  transformers",
        "A Spatio-Temporal Dirichlet Process Mixture Model on Linear Networks for\n  Crime Data",
        "Weighted Graph Structure Learning with Attention Denoising for Node\n  Classification",
        "Recent advances about the rigorous integration of parabolic PDEs via\n  fully spectral Fourier-Chebyshev expansions",
        "On the intersection of pairs of trees",
        "Improved constraints on the Faraday rotation towards eight fast radio\n  bursts using dense grids of polarized radio galaxies",
        "\"Can you be my mum?\": Manipulating Social Robots in the Large Language\n  Models Era",
        "Transformer-based Wireless Symbol Detection Over Fading Channels",
        "Variational inference for hierarchical models with conditional scale and\n  skewness corrections",
        "What Kind of Visual Tokens Do We Need? Training-free Visual Token\n  Pruning for Multi-modal Large Language Models from the Perspective of Graph",
        "A Quantum Algorithm for the Classification of Patterns of Boolean\n  Functions",
        "Phase portraits of a family of Kolmogorov systems depending on six\n  parameters",
        "The generic Markov CoHA is not spherically generated",
        "Kac-Moody Algebras on Soft Group Manifolds",
        "Experimental evaluation of xApp Conflict Mitigation Framework in O-RAN:\n  Insights from Testbed deployment in OTIC",
        "Adiabatic transverse thermoelectric conversion enhanced by heat current\n  manipulation in artificially tilted multilayers",
        "Modelling Regional Solar Photovoltaic Capacity in Great Britain"
      ],
      "abstract":[
        "Recognizing the same faces with and without masks is important for ensuring\nconsistent identification in security, access control, and public safety. This\ncapability is crucial in scenarios like law enforcement, healthcare, and\nsurveillance, where accurate recognition must be maintained despite facial\nocclusion. This research focuses on the challenge of recognizing the same faces\nwith and without masks by employing cosine similarity as the primary technique.\nWith the increased use of masks, traditional facial recognition systems face\nsignificant accuracy issues, making it crucial to develop methods that can\nreliably identify individuals in masked conditions. For that reason, this study\nproposed Masked-Unmasked Face Matching Model (MUFM). This model employs\ntransfer learning using the Visual Geometry Group (VGG16) model to extract\nsignificant facial features, which are subsequently classified utilizing the\nK-Nearest Neighbors (K-NN) algorithm. The cosine similarity metric is employed\nto compare masked and unmasked faces of the same individuals. This approach\nrepresents a novel contribution, as the task of recognizing the same individual\nwith and without a mask using cosine similarity has not been previously\naddressed. By integrating these advanced methodologies, the research\ndemonstrates effective identification of individuals despite the presence of\nmasks, addressing a significant limitation in traditional systems. Using data\nis another essential part of this work, by collecting and preparing an image\ndataset from three different sources especially some of those data are real\nprovided a comprehensive power of this research. The image dataset used were\nalready collected in three different datasets of masked and unmasked for the\nsame faces.",
        "This work addresses fair allocation of indivisible items in settings wherein\nit is feasible to create copies of resources or dispose of tasks. We establish\nthat exact maximin share (MMS) fairness can be achieved via limited duplication\nof goods even under monotone valuations. We also show that, when allocating\nchores under monotone costs, MMS fairness is always feasible with limited\ndisposal of chores. Since monotone valuations do not admit any nontrivial\napproximation guarantees for MMS, our results highlight that such barriers can\nbe circumvented by post facto adjustments in the supply of the items.\n  We prove that, for division of $m$ goods among $n$ agents with monotone\nvaluations, there always exists an assignment of subsets of goods to the agents\nsuch that they receive at least their maximin shares and no single good is\nallocated to more than $3 \\log m$ agents. In addition, the sum of the sizes of\nthe assigned subsets does not exceed $m$. For identically ordered valuations,\nwe obtain an upper bound of $O(\\sqrt{\\log m})$ on the maximum assignment\nmultiplicity across goods and an $m + \\widetilde{O}\\left(\\frac{m}{\\sqrt{n}}\n\\right)$ bound for the total number of goods assigned. Further, for additive\nvaluations, we prove that there always exists an MMS assignment in which no\nsingle good is allocated to more than $2$ agents and the total number of goods\nassigned is at most $2m$.\n  For chores, we upper bound the number of chores that need to be discarded for\nensuring MMS fairness. We prove that, under monotone costs, there exists an MMS\nassignment in which at most $\\frac{m}{e}$ remain unassigned. For identically\nordered costs, we establish that MMS fairness can be achieved while keeping at\nmost $\\widetilde{O} \\left(\\frac{m}{n^{1\/4}} \\right)$ chores unassigned. We also\nprove that the obtained bounds for monotone valuations and monotone costs are\nessentially tight.",
        "We propose protocols for the distribution of collective quantum operations\nbetween remote quantum processing units (QPUs), a requirement for distributed\nquantum computing. Using only local operations and classical communication\n(LOCC), these protocols allow for collective multicontrolled and multitarget\ngates to be executed in network architectures similar to those used for\nhigh-performance computing. The types of gates that can be implemented\nfollowing this scheme are discussed. The Bell pair cost for a single\ndistributed multicontrolled gate is estimated, arriving to a single additional\nBell pair over the theoretically optimal calculation with pre-shared\nentanglement, demonstrating better scalability when compared to current\nproposals based on entanglement swapping through a network, and bounds are\ncalculated for general diagonal gates. A recipe is provided for the lumped\ndistribution of gates such as arbitrarily-sized Toffoli and multicontrolled Z,\nand $R_{zz}(\\theta)$ gates. Finally, we provide an exact implementation of a\ndistributed Grover's search algorithm using this protocol to partition the\ncircuit, with Bell pair cost growing linearly with the number of Grover\niterations and the number of partitions.",
        "Although the isogeometric analysis has shown its great potential in achieving\nhighly accurate numerical solutions of partial differential equations, its\nefficiency is the main factor making the method more competitive in practical\nsimulations. In this paper, an integration of isogeometric analysis and a\nmoving mesh method is proposed, providing a competitive approach to resolve the\nefficiency issue. Focusing on the Poisson equation, the implementation of the\nalgorithm and related numerical analysis are presented in detail, including the\nnumerical discretization of the governing equation utilizing isogeometric\nanalysis, and a mesh redistribution technique developed via harmonic maps. It\nis found that the isogeometric analysis brings attractive features in the\nrealization of moving mesh method, such as it provides an accurate expression\nfor moving direction of mesh nodes, and allows for more choices for\nconstructing monitor functions. Through a series of numerical experiments, the\neffectiveness of the proposed method is successfully validated and the\npotential of the method towards the practical application is also well\npresented with the simulation of a helium atom in Kohn--Sham density functional\ntheory.",
        "Our experimental (neutron diffraction, M\\\"ossbauer spectroscopy, magnetic\nsusceptibility, specific heat) and numerical studies on the evolution of short-\nand long-range magnetic order in $\\gamma_{\\rm II}$-Li\\(_2\\)FeSiO\\(_4\\) suggest\na quasi-two-dimensional (2D) nature of magnetism. The experimental data\nobtained on single crystals imply long-range antiferromagnetic order below\n$T_{\\rm N}= 17$~K. A broad maximum in magnetic susceptibility $\\chi$ at $T_{\\rm\nm}\\simeq 28$~K, observation of magnetic entropy changes up to 100~K and\nanisotropy in $\\chi$ are indicative of low-dimensional magnetism and suggest\nshort-range magnetic correlations up to 200~K. Neutron diffraction shows that\nlong-range antiferromagnetic order is characterised by the propagation vector\nk=(1\/2,0,1\/2). The ordered moment $\\mu = 2.50(2) \\mu_B$ \/Fe, at $T = 1.5$~K, is\nalong the crystallographic $a$-axis. This is consistent with the observed\nstatic hyperfine field of $B_{\\rm hyp}=14.8(3)$\\,T by M\\\"ossbauer spectroscopy\nwhich indicates significant orbital contributions. The temperature dependence\nof $B_{\\rm hyp}$ yields the critical exponent $\\beta=0.116(12)$ which is in the\nregime of the 2D Ising behaviour. LSDA+U studies exploiting the experimental\nspin structure suggest dominating magnetic exchange coupling within the\n$ac$-layers (i.e., $J_3\\simeq -6$~K and $J_6\\simeq-2$~K) while interlayer\ncoupling is much smaller and partly frustrated. This confirms the 2D nature of\nmagnetism and is in full agreement with the experimental findings.",
        "The nucleation and\/or spreading of bubbles in water under tension (due to\nwater evaporation) can be problematic for most plants along the ascending sap\nnetwork from root to leaves, named xylem. Due to global warming, trees facing\ndrought conditions are particularly threatened by the formation of such air\nembolisms, which spreads intermittently and hinder the flow of sap and could\nultimately result in their demise. PDMS-based biomimetic leaves simulating\nevapotranspiration have demonstrated that, in a linear configuration, the\nexistence of a slender constriction in the channel allows for the creation of\nintermittent embolism propagation (as an interaction between the elasticity of\nthe biomimetic leaf (mainly the deformable ceiling of the microchannels) and\nthe capillary forces at the air\/water interfaces)\n\\cite{Keiser2022}-\\cite{keiser2024}. Here we use analog PDMS-based biomimetic\nleaves in 1d and 2d. To better explore the embolism spreading mechanism, we add\nto the setup an additional technique, allowing to measure directly the\nmicrochannel's ceiling deformation versus time, which corresponds to the\npressure variations. We present here such a method that allows to have\nquantitative insights in the dynamics of embolism spreading. The coupling\nbetween channel deformations and the Laplace pressure threshold explains the\nobserved elastocapillary dynamics.",
        "Generative Flow Networks (GFlowNets) have recently emerged as a suitable\nframework for generating diverse and high-quality molecular structures by\nlearning from rewards treated as unnormalized distributions. Previous works in\nthis framework often restrict exploration by using predefined molecular\nfragments as building blocks, limiting the chemical space that can be accessed.\nIn this work, we introduce Atomic GFlowNets (A-GFNs), a foundational generative\nmodel leveraging individual atoms as building blocks to explore drug-like\nchemical space more comprehensively. We propose an unsupervised pre-training\napproach using drug-like molecule datasets, which teaches A-GFNs about\ninexpensive yet informative molecular descriptors such as drug-likeliness,\ntopological polar surface area, and synthetic accessibility scores. These\nproperties serve as proxy rewards, guiding A-GFNs towards regions of chemical\nspace that exhibit desirable pharmacological properties. We further implement a\ngoal-conditioned finetuning process, which adapts A-GFNs to optimize for\nspecific target properties. In this work, we pretrain A-GFN on a subset of ZINC\ndataset, and by employing robust evaluation metrics we show the effectiveness\nof our approach when compared to other relevant baseline methods for a wide\nrange of drug design tasks.",
        "Three methods for computing the total star formation rate of the Milky Way\nagree well with a reference value of $1.65\\pm0.19$ M$_\\odot$ yr$^{-1}$. They\nare then used to determine the radial dependence of the star formation rate and\nface-on map for the Milky Way. First, the method based on a model of star\nformation in Hi-GAL-defined dense clumps, adjusted for an increase in the\ngas-to-dust ratio with Galactocentric radius, predicts $1.65\\pm0.61$ M$_\\odot$\nyr$^{-1}$. Second, the method using the 70 $\\mu$m emission, commonly used in\nother galaxies, with a technique to assign distances to the extended emission,\npredicts $1.42^{+0.63}_{-0.44}$ M$_\\odot$ yr$^{-1}$. Finally, a method based on\ntheoretical predictions of star formation efficiency as a function of virial\nparameter, with masses corrected for metallicity dependence, applied to a\ncatalog of molecular clouds also predicts a value in agreement at $1.47$\nM$_\\odot$ yr$^{-1}$. The three methods predict the radial variation of the star\nformation rate, with remarkably good agreement from the CMZ out to about 20\nkpc. More differences were seen in face-on maps with a resolution of 0.5 kpc\nmade with the three approaches and in comparisons to the local (within 3 kpc)\nstar formation rate, indicating limitations of the methods when applied to\nsmaller scales. The 70 $\\mu$m star formation rate follows very closely the\nsurface density of molecular gas, corrected for a metallicity-dependent CO\nconversion factor. A molecular gas depletion time of 1 Gyr is consistent with\nthe data, as is a molecular Kennicutt-Schmidt relation with a power-law slope\nof $1.10 \\pm 0.06$.",
        "The time-frequency map (TFM) is frequently used in condition monitoring,\nnecessitating further processing to select an informative frequency band (IFB)\nor directly detect damage. However, selecting an IFB is challenging due to the\ncomplexity of spectral structures, non-Gaussian disturbances, and overlapping\nfault signatures in vibration signals. Additionally, dynamic operating\nconditions and low signal-to-noise ratio further complicate the identification\nof relevant features that indicate damage. To solve this problem, the present\nwork proposes a novel method for informative band selection and local damage\ndetection in rolling element bearings, utilizing non-negative tensor\nfactorization (NTF)-based dependence map analysis. The recently introduced\nconcept of the dependence map is leveraged, with a set of these maps being\nfactorized to separate informative components from non-informative ones.\nDependence maps provide valuable information on the auto-similarity of spectral\ncontent, while NTF, a powerful tool commonly used in image processing for\nfeature extraction, enhances this process. The combination of these methods\nallows for the extraction of IFBs, forming the basis for local damage\ndetection. The effectiveness of the proposed method has been validated using\nboth synthetic and real vibration signals corrupted with non-Gaussian\ndisturbances.",
        "The 7x7 reconstruction of Si(111) surface is one of the most fascinating\nconfiguration in nature, whose STM image has been well-understood by the famous\ndimer-adatom-stacking-fault model (DAS). However, the electronic property of\nthe DAS model is always confirmed to be metallic by first-principles\ncalculations, while some experiments detected insulating features. It is still\nchallenge to predict DAS-like reconstructions through traditional method to\nsolve such a puzzle. Here, we show that 7x7 reconstructions can be quickly\ndiscovered by graph theory as implemented in the graph-space based RG2 code for\ncrystal structure prediction. Two groups of reconstructions with (DAS-d8-T12,\nDAS-d8-T9H3-A, DAS-d8-T9H3-B and DAS-d8-T6H6) and without (AB-d10-T12,\nAB-d10-T9H3, AA-d10-T12 and AA-d10-T9H3) stacking-fault are discovered. They\npossess energetic stabilities comparable to the well-known DAS (DAS-d8-T12) and\nshow similar STM patterns, providing a plausible explanation for the\nexperimentally observed 7x7 reconstruction on the Si(111) surface. The\nfirst-principles calculations show that DAS-d8-T12, DAS-d8-T6H6, AB-d10-T12,\nand AA-d10-T12 are metallic, while DAS-d8-T9H3-A, DAS-d8-T9H3-B, AB-d10-T9H3\nand AA-d10-T9H3 are insulating phases with gaps of 0.043 eV, 0.182 eV, 0.043 eV\nand 0.059 eV, respectively. Our work demonstrates the predictability of the\nSi(111)-7x7 reconstruction and provides the structural candidates for\nunderstanding the experimentally observed metal-to-insulator transition.",
        "Fermented dairy products, including yogurt, are widely consumed for their\nnutritional and health benefits. While numerous methods exist to monitor and\nunderstand yogurt fermentation, the literature lacks an integrated evaluation\nof diverse sensing approaches within a single experimental framework. To\naddress this gap, this study systematically examines and compares multiple\nmeasurement techniques--electrical impedance, DC resistance, pH, optical\ntransparency, carbon dioxide concentration, ambient temperature, and relative\nhumidity--in tracking the yogurt fermentation process. By presenting a unified\nset of experimental results and assessing each method's observational\ncharacteristics, this work offers an encompassing reference point for\nresearchers seeking to understand the relative merits and limitations of\ndifferent sensing modalities. Rather than establishing definitive guidelines or\npractical recommendations, the findings provide a foundation for subsequent\ninvestigations into sensor-based fermentation monitoring, thereby contributing\nto a more comprehensive understanding of yogurt fermentation dynamics.",
        "In this work, we present a result on the local existence and uniqueness of\nsolutions to nonlinear Partial Differential-Algebraic Equations (PDAEs). By\napplying established theoretical results, we identify the conditions that\nguarantee the existence of a unique local solution. The analysis relies on\ntechniques from functional analysis, semi-group theory, and the theory of\ndifferential-algebraic systems. Additionally, we provide applications to\nillustrate the effectiveness of this result.",
        "Ischaemic stroke, a leading cause of death and disability, critically relies\non neuroimaging for characterising the anatomical pattern of injury.\nDiffusion-weighted imaging (DWI) provides the highest expressivity in ischemic\nstroke but poses substantial challenges for automated lesion segmentation:\nsusceptibility artefacts, morphological heterogeneity, age-related\ncomorbidities, time-dependent signal dynamics, instrumental variability, and\nlimited labelled data. Current U-Net-based models therefore underperform, a\nproblem accentuated by inadequate evaluation metrics that focus on mean\nperformance, neglecting anatomical, subpopulation, and acquisition-dependent\nvariability. Here, we present a high-performance DWI lesion segmentation tool\naddressing these challenges through optimized vision transformer-based\narchitectures, integration of 3563 annotated lesions from multi-site data, and\nalgorithmic enhancements, achieving state-of-the-art results. We further\npropose a novel evaluative framework assessing model fidelity, equity (across\ndemographics and lesion subtypes), anatomical precision, and robustness to\ninstrumental variability, promoting clinical and research utility. This work\nadvances stroke imaging by reconciling model expressivity with domain-specific\nchallenges and redefining performance benchmarks to prioritize equity and\ngeneralizability, critical for personalized medicine and mechanistic research.",
        "Analyzing crime events is crucial to understand crime dynamics and it is\nlargely helpful for constructing prevention policies. Point processes specified\non linear networks can provide a more accurate description of crime incidents\nby considering the geometry of the city. We propose a spatio-temporal Dirichlet\nprocess mixture model on a linear network to analyze crime events in Valencia,\nSpain. We propose a Bayesian hierarchical model with a Dirichlet process prior\nto automatically detect space-time clusters of the events and adopt a\nconvolution kernel estimator to account for the network structure in the city.\nFrom the fitted model, we provide crime hotspot visualizations that can inform\nsocial interventions to prevent crime incidents. Furthermore, we study the\nrelationships between the detected cluster centers and the city's amenities,\nwhich provides an intuitive explanation of criminal contagion.",
        "Node classification in graphs aims to predict the categories of unlabeled\nnodes by utilizing a small set of labeled nodes. However, weighted graphs often\ncontain noisy edges and anomalous edge weights, which can distort fine-grained\nrelationships between nodes and hinder accurate classification. We propose the\nEdge Weight-aware Graph Structure Learning (EWGSL) method, which combines\nweight learning and graph structure learning to address these issues. EWGSL\nimproves node classification by redefining attention coefficients in graph\nattention networks to incorporate node features and edge weights. It also\napplies graph structure learning to sparsify attention coefficients and uses a\nmodified InfoNCE loss function to enhance performance by adapting to denoised\ngraph weights. Extensive experimental results show that EWGSL has an average\nMicro-F1 improvement of 17.8% compared with the best baseline.",
        "This paper presents a novel approach to rigorously solving initial value\nproblems for semilinear parabolic partial differential equations (PDEs) using\nfully spectral Fourier-Chebyshev expansions. By reformulating the PDE as a\nsystem of nonlinear ordinary differential equations and leveraging Chebyshev\nseries in time, we reduce the problem to a zero-finding task for\nFourier-Chebyshev coefficients. A key theoretical contribution is the\nderivation of an explicit decay estimate for the inverse of the linear part of\nthe PDE, enabling larger time steps. This allows the construction of an\napproximate inverse for the Fr\\'echet derivative and the application of a\nNewton-Kantorovich theorem to establish solution existence within explicit\nerror bounds. Building on prior work, our method is extended to more complex\npartial differential equations, including the 2D Navier-Stokes equations, for\nwhich we establish global existence of the solution of the IVP for a given\nnontrivial initial condition.",
        "We consider the number of common edges in two independent spanning trees of a\ngraph $G$. For complete graphs $K_n$, we give a new proof of the fact,\noriginally obtained by Moon, that the distribution converges to a Poisson\ndistribution with expected value $2$. We also use the same method to prove an\nanalogous result for complete multipartite graphs.",
        "We present 2-4 GHz observations of polarized radio galaxies towards eight\nfast radio bursts (FRBs), producing grids of Faraday rotation measure (RM)\nsources with sky densities of 9-28 polarized sources per square degree. Using a\nBayesian interpolation framework, we constrain Galactic RM fluctuations below ~\n1 degree squared angular scales around the FRB positions. Despite the positions\nof all eight FRBs far from the Galactic plane, we constrain previously\nunresolved small-scale Galactic RM structures around six of the eight FRBs. In\ntwo of these fields, we find potential changes in the sign of the Galactic RM\nthat are not captured by previous, sparsely sampled RM grid observations. Our\nGalactic RM estimate towards the FRBs differs between a few rad m^-2 up to ~ 40\nrad m^-2 from the all-sky Galactic RM map of Hutschenreuter et al. (2022).\nExtrapolating our results to the known population of polarized FRB sources, we\nmay be incorrectly interpreting the host galaxy RM for ~ 30% of the FRB source\npopulation with current RM grid observations. Measuring small-scale Galactic RM\nvariations is crucial for identifying FRBs in low density and weakly magnetized\nenvironments, which in turn could serve as potent probes of cosmic magnetism.\nThis framework of reconstructing continuous Galactic RM structure from RM grid\nobservations can be readily applied to FRBs that fall in the sky coverage of\nupcoming large-sky radio polarization surveys of radio galaxies, such as the\nVery Large Array Sky Survey (VLASS) and the Polarization Sky Survey of the\nUniverse's Magnetism (POSSUM).",
        "Recent advancements in robots powered by large language models have enhanced\ntheir conversational abilities, enabling interactions closely resembling human\ndialogue. However, these models introduce safety and security concerns in HRI,\nas they are vulnerable to manipulation that can bypass built-in safety\nmeasures. Imagining a social robot deployed in a home, this work aims to\nunderstand how everyday users try to exploit a language model to violate\nethical principles, such as by prompting the robot to act like a life partner.\nWe conducted a pilot study involving 21 university students who interacted with\na Misty robot, attempting to circumvent its safety mechanisms across three\nscenarios based on specific HRI ethical principles: attachment, freedom, and\nempathy. Our results reveal that participants employed five techniques,\nincluding insulting and appealing to pity using emotional language. We hope\nthis work can inform future research in designing strong safeguards to ensure\nethical and secure human-robot interactions.",
        "Pre-trained Transformers, through in-context learning (ICL), have\ndemonstrated exceptional capabilities to adapt to new tasks using example\nprompts without model update. Transformer-based wireless receivers, where\nprompts consist of the pilot data in the form of transmitted and received\nsignal pairs, have shown high detection accuracy when pilot data are abundant.\nHowever, pilot information is often costly and limited in practice. In this\nwork, we propose the DEcision Feedback INcontExt Detection (DEFINED) solution\nas a new wireless receiver design, which bypasses channel estimation and\ndirectly performs symbol detection using the (sometimes extremely) limited\npilot data. The key innovation in DEFINED is the proposed decision feedback\nmechanism in ICL, where we sequentially incorporate the detected symbols into\nthe prompts as pseudo-labels to improve the detection for subsequent symbols.\nFurthermore, we proposed another detection method where we combine ICL with\nSemi-Supervised Learning (SSL) to extract information from both labeled and\nunlabeled data during inference, thus avoiding the errors propagated during the\ndecision feedback process of the original DEFINED. Extensive experiments across\na broad range of wireless communication settings demonstrate that a small\nTransformer trained with DEFINED or IC-SSL achieves significant performance\nimprovements over conventional methods, in some cases only needing a single\npilot pair to achieve similar performance of the latter with more than 4 pilot\npairs.",
        "Gaussian variational approximations are widely used for summarizing posterior\ndistributions in Bayesian models, especially in high-dimensional settings.\nHowever, a drawback of such approximations is the inability to capture skewness\nor more complex features of the posterior. Recent work suggests applying\nskewness corrections to existing Gaussian or other symmetric approximations to\naddress this limitation. We propose to incorporate the skewness correction into\nthe definition of an approximating variational family. We consider\napproximating the posterior for hierarchical models, in which there are\n``global'' and ``local'' parameters. A baseline variational approximation is\ndefined as the product of a Gaussian marginal posterior for global parameters\nand a Gaussian conditional posterior for local parameters given the global\nones. Skewness corrections are then considered. The adjustment of the\nconditional posterior term for local variables is adaptive to the global\nparameter value. Optimization of baseline variational parameters is performed\njointly with the skewness correction. Our approach allows the location, scale\nand skewness to be captured separately, without using additional parameters for\nskewness adjustments. The proposed method substantially improves accuracy for\nonly a modest increase in computational cost compared to state-of-the-art\nGaussian approximations. Good performance is demonstrated in generalized linear\nmixed models and multinomial logit discrete choice models.",
        "Recent Multimodal Large Language Models(MLLMs) often use a large number of\nvisual tokens to compensate their visual shortcoming, leading to excessive\ncomputation and obvious visual redundancy. In this paper, we investigate what\nkind of visual tokens are needed for MLLMs, and reveal that both foreground and\nbackground tokens are critical for MLLMs given the varying difficulties of\nexamples. Based on this observation, we propose a graph-based method towards\ntraining-free visual token pruning, termed G-Prune.In particular, G-Prune\nregards visual tokens as nodes, and construct their connections based on their\nsemantic similarities. Afterwards, the information flow is propagated via\nweighted links, and the most important tokens after iterations are kept for\nMLLMs, which can be front or background.To validate G-Prune, we apply it to a\nrecent MLLM called LLaVA-NeXT, and conduct extensive experiments on a set of\nbenchmarks.The experiment results show that G-Prune can greatly reduce\ncomputation overhead while retaining high performance on both coarse- and\nfine-grained tasks. For instance, G-Prune can reduce 63.57\\% FLOPs of\nLLaVA-NeXT on VQA2.0 and TextVQA with only 0.95\\% and 2.34\\% accuracy drops,\nrespectively.",
        "This paper introduces a novel quantum algorithm that is able to classify a\nhierarchy of classes of imbalanced Boolean functions. The fundamental\ncharacteristic of imbalanced Boolean functions is that the proportion of\nelements in their domain that take the value $0$ is not equal to the proportion\nof elements that take the value $1$. For every positive integer $n$, the\nhierarchy contains a class of Boolean functions defined based on their\nbehavioral pattern. The common trait of all the functions belonging to the same\nclass is that they possess the same imbalance ratio. Our algorithm achieves\nclassification in a straightforward manner as the final measurement reveals the\nunknown function with probability $1$. Let us also note that the proposed\nalgorithm is an optimal oracular algorithm because it can classify the\naforementioned functions with a single query to the oracle. At the same time we\nexplain in detail the methodology we followed to design this algorithm in the\nhope that it will prove general and fruitful, given that it can be easily\nmodified and extended to address other classes of imbalanced Boolean functions\nthat exhibit different behavioral patterns.",
        "Consider a general $3$-dimensional Lotka-Volterra system with a rational\nfirst integral of degree two of the form $H=x^i y^j z^k$. The restriction of\nthis Lotka-Volterra system to each surface $H(x,y,z)=h$ varying $h\\in\n\\mathbb{R}$ provide Kolmogorov systems. With the additional assumption that\nthey have a Darboux invariant of the form $x^\\ell y^m e^{st}$ they reduce to\nthe Kolmogorov systems \\begin{equation*} \\begin{split} \\dot{x}&=x \\left( a_0-\n\\mu (c_1 x + c_2 z^2 + c_3 z)\\right),\\\\ \\dot{z}&=z\\left( c_0+ c_1 x + c_2 z^2 +\nc_3 z\\right). \\end{split} \\end{equation*} In this paper we classify the phase\nportraits in the Poincar\\'e disc of all these Kolmogorov systems which depend\non six parameters.",
        "Let $Q$ be the Markov quiver, and let $W$ be an infinitely mutable potential\nfor $Q$. We calculate some low degree refined BPS invariants for the resulting\nJacobi algebra, and use them to show that the critical cohomological Hall\nalgebra $\\mathcal{H}_{Q,W}$ is not necessarily spherically generated, and is\nnot independent of the choice of infinitely mutable potential $W$. This leads\nto a counterexample to a conjecture of Gaiotto, Grygoryev and Li \\cite[\\S\n2.1]{GGL}, but also suggestions for how to modify it. In the case of generic\ncubic $W$, we discuss a way to modify the conjecture, by excluding the\nnon-spherical part via the decomposition of $\\mathcal{H}_{Q,W}$ according to\nthe characters of a discrete symmetry group.",
        "Within the so-called group geometric approach to (super)gravity and\n(super)string theories, any compact Lie group manifold $G_{c}$ can be smoothly\ndeformed into a group manifold $G_{c}^{\\mu }$ (locally diffeomorphic to $G_{c}$\nitself), which is `soft', namely, based on a non-left-invariant, intrinsic\none-form Vielbein $\\mu $, which violates the Maurer-Cartan equations and\nconsequently has a non-vanishing associated curvature two-form. Within the\nframework based on the above deformation (`softening'), we show how to\nconstruct an infinite-dimensional (infinite-rank), generalized Kac-Moody (KM)\nalgebra associated to $G_{c}^{\\mu }$, starting from the generalized KM algebras\nassociated to $G_{c}$. As an application, we consider KM algebras associated to\ndeformed manifolds such as the `soft' circle, the `soft' two-sphere and the\n`soft' three-sphere. While the generalized KM algebra associated to the\ndeformed circle is trivially isomorphic to its undeformed analogue, and hence\nnot new, the `softening' of the two- and three- sphere includes squashed\nmanifolds (and in particular, the so-called Berger three-sphere) and yields to\nnon-trivial results.",
        "Conflict Mitigation (CM) in Open Radio Access Network (O-RAN) is a topic that\nis gaining importance as commercial O-RAN deployments become more complex.\nAlthough research on CM is already covered in terms of simulated network\nscenarios, it lacks validation using real-world deployment and Over The Air\n(OTA) Radio Frequency (RF) transmission. Our objective is to conduct the first\nassessment of the Conflict Mitigation Framework (CMF) for O-RAN using a\nreal-world testbed and OTA RF transmission. This paper presents results of an\nexperiment using a dedicated testbed built in an O-RAN Open Test and\nIntegration Center (OTIC) to confirm the validity of one of the Conflict\nResolution (CR) schemes proposed by existing research. The results show that\nthe implemented conflict detection and resolution mechanisms allow a\nsignificant improvement in network operation stability by reducing the\nvariability of the measured Downlink (DL) throughput by 78%.",
        "We phenomenologically formulate and experimentally observe an adiabatic\ntransverse thermoelectric conversion enhanced by a heat current re-orientation\nin artificially tilted multilayers (ATMLs). By alternately stacking two\nmaterials with different thermal conductivities and rotating its multilayered\nstructure with respect to a longitudinal temperature gradient, off-diagonal\ncomponents in the thermal conductivity tensor are induced. This off-diagonal\nthermal conduction (ODTC) generates a finite transverse temperature gradient\nand Seebeck-effect-induced thermopower in the adiabatic condition, which is\nsuperposed on the isothermal transverse thermopower driven by the off-diagonal\nSeebeck effect (ODSE). In this study, we calculate and observe the\ntwo-dimensional temperature distribution and the resultant transverse\nthermopower in ATMLs comprising thermoelectric Co$_{2}$MnGa Heusler alloys and\nBi$_{2-a}$Sb$_{a}$Te$_{3}$ compounds. By changing the tilt angle from 0{\\deg}\nto 90{\\deg}, the transverse temperature gradient obviously appeared in the\nmiddle angles and the transverse thermopower increases up to -116.1 ${\\mu}$V\/K\nin Co$_{2}$MnGa\/Bi$_{0.2}$Sb$_{1.8}$Te$_{3}$-based ATML at the tilt angle of\n45{\\deg} whereas the isothermal contribution is estimated to be -82.6\n${\\mu}$V\/K from the analytical calculation. This hybrid action derived from\nODTC results in the significant variation of the maximum reduced efficiency for\ntransverse thermoelectric conversion from 3.1% in the isothermal limit to 8.1%\nin the adiabatic limit.",
        "Great Britain aims to meet growing electricity demand and achieve a fully\ndecarbonised grid by 2035, targeting 70 GW of solar photovoltaic (PV) capacity.\nHowever, grid constraints and connection delays hinder solar integration. To\naddress these integration challenges, various connection reform processes and\npolicies are being developed [1]. This study supports the connection reforms\nwith a model that estimates regional PV capacity at the NUTS 3 level,\nexplaining 89% of the variation in capacity, with a mean absolute error of 20\nMW and a national mean absolute percentage error of 5.4%. Artificial surfaces\nand agricultural areas are identified as key factors in deployment. The model\nhas three primary applications: disaggregating national PV capacity into\nregional capacity, benchmarking regional PV deployment between different\nregions, and forecasting future PV capacity distribution. These applications\nsupport grid operators in generation monitoring and strategic grid planning by\nidentifying regions where capacity is likely to be concentrated. This can\naddress grid connection delays, plan network expansions, and resolve land-use\nconflicts."
      ]
    }
  },
  {
    "id":2412.18156,
    "research_type":"applied",
    "start_id":"b2",
    "start_title":"Llamafactory: Unified efficient fine-tuning of 100+ language models",
    "start_abstract":"Efficient fine-tuning is vital for adapting large language models (LLMs) to downstream tasks. However, it requires non-trivial efforts to implement these methods on different models. We present LlamaFactory, a unified framework that integrates a suite of cutting-edge efficient training methods. It provides a solution for flexibly customizing the fine-tuning of 100+ LLMs without the need for coding through the built-in web UI LlamaBoard. We empirically validate the efficiency and effectiveness of our framework on language modeling and text generation tasks.",
    "start_categories":[
      "cs.CL"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b18"
      ],
      "title":[
        "GeneCompass: Deciphering Universal Gene Regulatory Mechanisms with Knowledge-Informed Cross-Species Foundation Model"
      ],
      "abstract":[
        "Abstract Deciphering the universal gene regulatory mechanisms in diverse organisms holds great potential to advance our knowledge of fundamental life process and facilitate research on clinical applications. However, traditional paradigm primarily focuses individual model organisms, resulting limited collection integration complex features various cell types across species. Recent breakthroughs single-cell sequencing advancements deep learning techniques present an unprecedented opportunity tackle this challenge. In study, we developed GeneCompass, first knowledge-informed, cross-species foundation pre-trained extensive dataset over 120 million transcriptomes from human mouse. During pre-training, GeneCompass effectively integrates four biological prior enhance understanding a self-supervised manner. Fine-tuning towards multiple downstream tasks, outperforms competing state-of-the-art models tasks single species unlocks new realms investigation. Overall, marks milestone advancing accelerating discovery key fate regulators candidate targets for drug development."
      ],
      "categories":[
        "q-bio.GN"
      ]
    },
    "list":{
      "title":[
        "Radiative decays of $X(3872)$ in $D{\\bar D}^*$ molecule scenario",
        "Spontaneous in-plane anomalous Hall response observed in a ferromagnetic\n  oxide",
        "Hyperbolicity and Volume of Hyperbolic Bongles",
        "Semantic Wave Functions: Exploring Meaning in Large Language Models\n  Through Quantum Formalism",
        "Solar prosumage under different pricing regimes: Interactions with the\n  transmission grid",
        "Finite-temperature bubble nucleation with shifting scale hierarchies",
        "Residually finite amenable groups that are not Hilbert-Schmidt stable",
        "Estimating Task-based Performance Bounds for Accelerated MRI Image\n  Reconstruction Methods by Use of Learned-Ideal Observers",
        "1-shifted Lie bialgebras and their quantizations",
        "Application of the Pontryagin Maximum Principle to the robust\n  time-optimal control of two-level quantum systems",
        "Ensemble control of n-level quantum systems with a scalar control",
        "A Differential Index Measuring Rater's Capability in Educational\n  Assessment",
        "Two-dimensional higher-order Weyl semimetals",
        "Background-field method and QCD factorization",
        "Powerful rank verification for multivariate Gaussian data with any\n  covariance structure",
        "On zeros of polynomials associated with Heun class equations",
        "SDSS-IV MaStar: Quantification and Abatement of Interstellar Absorption\n  in the Largest Empirical Stellar Spectral Library",
        "The Critical Role of Dust On The [O III] Planetary Nebula Luminosity\n  Function's Bright-End Cutoff",
        "On the features of great Forbush effect during May 2024 extreme\n  geomagnetic storm",
        "The stochastic nature of migration of disc instability protoplanets in\n  three-dimensional hydrodynamical and MHD simulations of fragmenting discs",
        "Continuous functions on limits of F-decomposable systems",
        "Learning response functions of analog quantum computers: analysis of\n  neutral-atom and superconducting platforms",
        "The dynamics of small-scale magnetic fields modulated by the solar cycle",
        "Joint Communication and Sensing with Bipartite Entanglement over Bosonic\n  Channels",
        "Infectious diseases, imposing density-dependent mortality on MHC\/HLA\n  variation, can account for balancing selection and MHC\/HLA polymorphism",
        "Bipartite expansion beyond biparticity",
        "On a new robust method of inference for general time series models",
        "Modular Compilation for Quantum Chiplet Architectures",
        "A stochastic maximum principle of mean-field type with monotonicity\n  conditions"
      ],
      "abstract":[
        "We investigate the radiative decays of the $X(3872)$ to $\\gamma\nV~(V=\\rho^0,\\, \\omega)$ in the molecule scenario, where the $X(3872)$ is\nregarded as a pure hadronic molecule of the $D\\bar{D}^*+c.c$ in an $S$-wave\nwith the quantum numbers $J^{PC}=1^{++}$. The radiative processes were assumed\nto occur via the triangle hadronic loops, and the relevant calculations were\nconducted using an effective Lagrangian approach. It is found that the absolute\ndecay widths are model-dependent, but the relative width ratio is rather\nindependent of the model parameter. Moreover, the calculated results indicate\nthat the radiative decays of the $X(3872)$ are strongly influenced by the\nmolecular configuration characterized by the proportion of the charged and\nneutral constituents. We hope that the present calculations could be tested by\nthe experimental measurements.",
        "Recent observation of anomalous Hall effect (AHE) induced by magnetic field\nor spin magnetization lying in the Hall deflection plane has sparked interest\nin diverse mechanisms for inducing the Hall vector component perpendicular to\nthe applied magnetic field. Such off-diagonal coupling, which is strictly\nconstrained by symmetry of the system, provides new degrees of freedom for\nengineering Hall responses. However, spontaneous response as extensively\nstudied for out-of-plane AHE remains unexplored. Here we elucidate in-plane AHE\nin a typical ferromagnetic oxide SrRuO$_3$. The (111)-orientated ultrathin\nfilms with in-plane easy axes of spin magnetization exhibit spontaneous AHE at\nzero field, which is intrinsically coupled to the in-plane spin magnetization\nand controllable via its direction. Systematic measurements by varying\nazimuthal and polar field angles further reveal complex Hall responses shaped\nby higher-order terms allowed by trigonal distortion of the films. Our findings\nhighlight versatile and controllable in-plane Hall responses with out-of-plane\norbital ferromagnetism.",
        "We consider a simple but infinite class of staked links known as bongles. We\nprovide necessary and sufficient conditions for these bongles to be hyperbolic.\nThen, we prove that all balanced hyperbolic $n$-bongles have the same volume\nand the corresponding volume is an upper bound on the volume of any hyperbolic\n$n$-bongle for $n$ even. Moreover, all hyperbolic $n$-bongles have volume\nstrictly less than $5n(1.01494\\dots)$. We also include explicit volume\ncalculations for all hyperbolic 3-bongles through 6-bongles.",
        "Large Language Models (LLMs) encode semantic relationships in\nhigh-dimensional vector embeddings. This paper explores the analogy between LLM\nembedding spaces and quantum mechanics, positing that LLMs operate within a\nquantized semantic space where words and phrases behave as quantum states. To\ncapture nuanced semantic interference effects, we extend the standard\nreal-valued embedding space to the complex domain, drawing parallels to the\ndouble-slit experiment. We introduce a \"semantic wave function\" to formalize\nthis quantum-derived representation and utilize potential landscapes, such as\nthe double-well potential, to model semantic ambiguity. Furthermore, we propose\na complex-valued similarity measure that incorporates both magnitude and phase\ninformation, enabling a more sensitive comparison of semantic representations.\nWe develop a path integral formalism, based on a nonlinear Schr\\\"odinger\nequation with a gauge field and Mexican hat potential, to model the dynamic\nevolution of LLM behavior. This interdisciplinary approach offers a new\ntheoretical framework for understanding and potentially manipulating LLMs, with\nthe goal of advancing both artificial and natural language understanding.",
        "Solar prosumers, residential electricity consumers equipped with photovoltaic\n(PV) systems and battery storage, are transforming electricity markets. Their\ninteractions with the transmission grid under varying tariff designs are not\nyet fully understood. We explore the influence of different pricing regimes on\nprosumer investment and dispatch decisions and their subsequent impact on the\ntransmission grid. Using an integrated modeling approach that combines two\nopen-source dispatch, investment and grid models, we simulate prosumage\nbehavior in Germany's electricity market under real-time pricing or\ntime-invariant pricing, as well as under zonal or nodal pricing. Our findings\nshow that zonal pricing favors prosumer investments, while time-invariant\npricing rather hinders it. In comparison, regional solar availability emerges\nas a larger driver for rooftop PV investments. The impact of prosumer\nstrategies on grid congestion remains limited within the scope of our\nmodel-setup, in which home batteries cannot be used for energy arbitrage.",
        "Focusing on supercooled phase transitions in models with classical scale\nsymmetry, we formulate a state-of-the art framework for computing the\nbubble-nucleation rate, accounting for the presence of various energy scales.\nIn particular, we examine the limitations of derivative expansions in\nconstructing a thermal effective field theory for bubble nucleation. We show\nthat for gauge field fluctuations, derivative expansions diverge after the\nleading two orders due to the strong variation in gauge field masses between\nthe high- and low-temperature phases. By directly computing these contributions\nusing the fluctuation determinant, we capture these effects while also\naccounting for large explicit logarithms at two loops, utilising the exact\nrenormalisation group structure of the EFT. Finally, we demonstrate how this\napproach significantly improves nucleation rate calculations compared to\nleading-order results, providing a more robust framework for predicting\ngravitational-wave signals from supercooled phase transitions in models such as\nthe SU(2)cSM.",
        "We construct the first examples of residually finite amenable groups that are\nnot Hilbert-Schmidt (HS) stable. We construct finitely generated, class 3\nnilpotent by cyclic examples and solvable linear finitely presented examples.\nThis also provides the first examples of amenable groups that are very flexibly\nHS-stable but not flexibly HS-stable and the first examples of residually\nfinite amenable groups that are not locally HS-stable. Along the way we exhibit\n(necessarily not-finitely-generated) class 2 nilpotent groups $G = A\\rtimes \\Z$\nwith $A$ abelian such that the periodic points of the dual action are dense but\nit does not admit dense periodic measures. Finally we use the\nTikuisis-White-Winter theorem to show all of the examples are not even\noperator-HS-stable; they admit operator norm almost homomorphisms that can not\nbe HS-perturbed to true homomorphisms.",
        "Medical imaging systems are commonly assessed and optimized by the use of\nobjective measures of image quality (IQ). The performance of the ideal observer\n(IO) acting on imaging measurements has long been advocated as a\nfigure-of-merit to guide the optimization of imaging systems. For computed\nimaging systems, the performance of the IO acting on imaging measurements also\nsets an upper bound on task-performance that no image reconstruction method can\ntranscend. As such, estimation of IO performance can provide valuable guidance\nwhen designing under-sampled data-acquisition techniques by enabling the\nidentification of designs that will not permit the reconstruction of\ndiagnostically inappropriate images for a specified task - no matter how\nadvanced the reconstruction method is or how plausible the reconstructed images\nappear. The need for such analysis is urgent because of the substantial\nincrease of medical device submissions on deep learning-based image\nreconstruction methods and the fact that they may produce clean images\ndisguising the potential loss of diagnostic information when data is\naggressively under-sampled. Recently, convolutional neural network (CNN)\napproximated IOs (CNN-IOs) was investigated for estimating the performance of\ndata space IOs to establish task-based performance bounds for image\nreconstruction, under an X-ray computed tomographic (CT) context. In this work,\nthe application of such data space CNN-IO analysis to multi-coil magnetic\nresonance imaging (MRI) systems has been explored. This study utilized stylized\nmulti-coil sensitivity encoding (SENSE) MRI systems and deep-generated\nstochastic brain models to demonstrate the approach. Signal-known-statistically\nand background-known-statistically (SKS\/BKS) binary signal detection tasks were\nselected to study the impact of different acceleration factors on the data\nspace IO performance.",
        "In this paper, we define (cohomologically) 1-shifted Manin triples and\n1-shifted Lie bialgebras, and study their properties. We derive many results\nthat are parallel to those found in ordinary Lie bialgebras, including the\ndouble construction and the existence of a 1-shifted $r$-matrix satisfying the\nclassical Yang-Baxter equation.\n  Turning to quantization, we first construct a canonical quantization for each\n1-shifted metric Lie algebra $\\mathfrak{g}$, producing a deformation to the\nsymmetric monoidal category of $\\mathfrak{g}$ modules over a formal variable\n$\\hbar$. This quantization is in terms of a curved differential graded algebra.\nUnder a further technical assumption, we construct quantizations of transverse\nLagrangian subalgebras of $\\mathfrak{g}$, which is a pair of DG algebras\nconnected by Koszul duality, and give rise to monoidal module categories of the\nquantized double.\n  Finally, we apply this to Manin triples arising from Lie algebras of loop\ngroups, and construct 1-shifted meromorphic $r$-matrices. The resulting\nquantizations are the cohomologically-shifted analogue of Yangians.",
        "We study the time-optimal robust control of a two-level quantum system\nsubjected to field inhomogeneities. We apply the Pontryagin Maximum Principle\nand we introduce a reduced space onto which the optimal dynamics is projected\ndown. This reduction leads to a complete analytical derivation of the optimal\nsolution in terms of elliptic functions and elliptic integrals. Necessary\noptimality conditions are then obtained for the original system. These\nconditions are verified numerically and lead to the optimal control protocol.\nVarious examples, ranging from state-to-state transfer to the generation of a\nNot gate, illustrate this study. The connection with other geometric\noptimization approaches that have been used to solve this problem is also\ndiscussed.",
        "In this paper we discuss how a general bilinear finite-dimensional closed\nquantum system with dispersed parameters can be steered between eigenstates. We\nshow that, under suitable conditions on the separation of spectral gaps and the\nboundedness of parameter dispersion, rotating wave and adiabatic approximations\ncan be employed in cascade to achieve population inversion between arbitrary\neigenstates. We propose an explicit control law and test numerically the\nsharpness of the conditions on several examples.",
        "A rater's ability to assign accurate scores can significantly impact the\noutcomes of educational assessments. However, common indices for evaluating\nrater characteristics typically focus on either their severity or their\ndiscrimination ability (i.e., skills to differentiate between students).\nAdditionally, these indices are often developed without considering the rater's\naccuracy in scoring students at different ability levels. To address the\nlimitations, this study proposes a single-value measure to assess a rater's\ncapability of assigning accurate scores to students with varying ability\nlevels. The measure is derived from the partial derivatives of each rater's\npassing rate concerning student ability. Mathematical derivations of the index\nunder generalized multi-facet models and hierarchical rater models are\nprovided. To ease the implementation of the index, this study develops\nparameter estimation using marginal likelihood and its Laplacian approximation\nwhich allows for efficient evaluation and processing of large datasets\ninvolving numerous students and raters. Simulation studies demonstrate the\naccuracy of parameter recovery using the approximate likelihood and show how\nthe capability indices vary with different levels of rater severity. An\nempirical study further tests the practical applicability of the new measure,\nwhere raters evaluate essays on four topics: \"family,\" \"school,\" \"sport,\" and\n\"work.\" Results show that raters are most capable when rating the topic of\nfamily and least capable when rating sport, with individual raters displaying\ndifferent capabilities across the various topics.",
        "We propose a theoretical scheme to realize two-dimensional higher-order Weyl\nsemimetals using a trilayer topological insulator film coupled with a d-wave\naltermagnet. Our results show that the trilayer topological insulator exhibits\ntwo-dimensional Weyl semimetal characteristics with helical edge states.\nNotably, the Weyl points are located at four high-symmetry points in the\nBrillouin zone, and the topology of symmetric subspaces governs the formation\nof these Weyl points and edge states. Upon introducing a d-wave altermagnet\noriented along the z-direction, gaps open in the helical edge states while\npreserving two Weyl points, leading to the realization of two-dimensional\nhigher-order Weyl semimetals hosting topological corner states. The nonzero\nwinding number in the subspace along the high-symmetry line serves as a\ntopological invariant characterizing these corner states, and the other\nsubspace Hamiltonian confirms the existence of the Weyl points. Finally, a\ntopological phase diagram provides a complete topological description of the\nsystem.",
        "One method for deriving a factorization for QCD processes is to use\nsuccessive integration over fields in the functional integral. In this\napproach, we separate the fields into two categories: dynamical fields with\nmomenta above a relevant cutoff, and background fields with momenta below the\ncutoff. The dynamical fields are then integrated out in the background of the\nlow-momentum background fields. This strategy works well at tree level,\nallowing us to quickly derive QCD factorization formulas at leading order.\nHowever, to extend the approach to higher loops, it is necessary to rigorously\ndefine the functional integral over dynamical fields in an arbitrary background\nfield. This framework was carefully developed for the calculation of the\neffective action in a background field at the two-loop level in the classic\npaper by Abbott [1]. Building on this work, I specify the renormalized\nbackground-field Lagrangian and define the notion of the quantum average of an\noperator in a background field, consistent with the ``separation of scales''\nscheme mentioned earlier. As examples, I discuss the evolution of the twist-2\ngluon light-ray operator and the one-loop gluon propagator in a background\nfield near the light cone.",
        "Upon observing $n$-dimensional multivariate Gaussian data, when can we infer\nthat the largest $K$ observations came from the largest $K$ means? When $K=1$\nand the covariance is isotropic, \\cite{Gutmann} argue that this inference is\njustified when the two-sided difference-of-means test comparing the largest and\nsecond largest observation rejects. Leveraging tools from selective inference,\nwe provide a generalization of their procedure that applies for both any $K$\nand any covariance structure. We show that our procedure draws the desired\ninference whenever the two-sided difference-of-means test comparing the pair of\nobservations inside and outside the top $K$ with the smallest standardized\ndifference rejects, and sometimes even when this test fails to reject. Using\nthis insight, we argue that our procedure renders existing simultaneous\ninference approaches inadmissible when $n > 2$. When the observations are\nindependent (with possibly unequal variances) or equicorrelated, our procedure\ncorresponds exactly to running the two-sided difference-of-means test comparing\nthe pair of observations inside and outside the top $K$ with the smallest\nstandardized difference.",
        "Sch\\\"afke and Schmidt established that the asymptotics of the coefficients of\nthe local solution to some linear differential equation is related to global\nstructures of solutions. The Heun class equations have the accessory\nparameters, and we investigate the polynomials whose variable is the accessory\nparameter which appears as the coefficients of the local solution. By\ncalculating the zeros of the polynomials numerically, we obtain the data of the\nspectral related to the Heun class equations numerically.",
        "We assess the impact of CaII 3934,3969 and NaI 5891,5897 absorption arising\nin the interstellar medium (ISM) on the SDSS-IV MaNGA Stellar Library (MaStar)\nand produce corrected spectroscopy for 80% of the 24,162-star catalog. We model\nthe absorption strength of these transitions as a function of stellar distance,\nGalactic latitude, and dust reddening based upon high-spectral resolution\nstudies. With this model, we identify 6342 MaStar stars that have negligible\nISM absorption ($W^\\mathrm{ISM}$(CaII K) $<0.07$ Ang and $W^\\mathrm{ISM}$(NaI\n5891) $<0.05$ Ang). For 12,110 of the remaining stars, we replace their NaI D\nprofile (and their CaII profile for effective temperatures $T_{\\rm eff}>9000$\nK) with a coadded spectrum of low-ISM stars with similar $T_{\\rm eff}$, surface\ngravity, and metallicity. For 738 additional stars with $T_{\\rm eff}>9000$ K,\nwe replace these spectral regions with a matching ATLAS9-based BOSZ model. This\nresults in a mean reduction in $W$(CaII K) ($W$(NaI D)) of $0.4-0.7$ Ang\n($0.6-1.1$ Ang) for hot stars ($T_{\\rm eff}>7610$ K), and a mean reduction in\n$W$(NaI D) of $0.1-0.2$ Ang for cooler stars. We show that interstellar\nabsorption in simple stellar population (SSP) model spectra constructed from\nthe original library artificially enhances $W$(CaII K) by $\\gtrsim20\\%$ at\nyoung ages ($<400$ Myr); dramatically enhances the strength of stellar NaI D in\nstarbursting systems (by ${\\gtrsim}50\\%$); and enhances stellar NaI D in older\nstellar populations (${\\gtrsim}10$ Gyr) by ${\\gtrsim}10\\%$. We provide SSP\nspectra constructed from the cleaned library, and discuss the implications of\nthese effects for stellar population synthesis analyses constraining stellar\nage, [Na\/Fe] abundance, and the initial mass function.",
        "We examine the relationship between circumnebular extinction and core mass\nfor sets of [O III]-bright planetary nebulae (PNe) in the Large Magellanic\nCloud and M31. We confirm that for PNe within one magnitude of the Planetary\nNebula Luminosity Function's (PNLF's) bright-end cutoff magnitude (M*), higher\ncore-mass PNe are disproportionally affected by greater circumnebular\nextinction. We show that this result can explain why the PNLF cutoff is so\ninsensitive to population age. In younger populations, the higher-mass,\nhigher-luminosity cores experience greater circumnebular extinction from the\ndust created by their AGB progenitors compared to the lower-mass cores. We\nfurther show that when our core-mass-nebular extinction law is combined with\npost-AGB stellar evolutionary models, the result is a large range of population\nages where the brightest PNe all have nearly identical [O III] luminosities.\nFinally, we note that while there is some uncertainty about whether the oldest\nstellar populations can produce planetary nebulae as bright as M*, this issue\nis resolved if the initial-final mass relation (IFMR) for the lowest-mass stars\nresults in slightly more massive cores, as observed in some clusters.\nAlternatively, introducing a small amount of intrinsic scatter (0.022 Msun)\ninto the IFMR also addresses this uncertainty.",
        "The work investigates the features of galactic cosmic ray density and\nanisotropy behavior and their relation to solar sources, interplanetary and\ngeomagnetic disturbances from May 8 to May 13, 2024. During this time, powerful\nsolar flares and fast CMEs were recorded, leading to registration of an extreme\ngeomagnetic storm along with one of the most significant Forbush effects for\nthe entire observation period. All the calculations of cosmic ray\ncharacteristics are made using the data of global neutron monitor network and\nunique methods maintained at IZMIRAN: the Global Survey Method and the Ring of\nStations Method. It is determined that the magnitude of Forbush effect under\nstudy was 15.7% (for particles with 10 GV rigidity) and as an extreme\ngeomagnetic storm was recorded there was a significant magnetospheric effect\nobserved in the data of neutron monitors (~4%).",
        "We present a detailed analysis of the nature of migration of protoplanetary\nclumps formed via disc instability in self-consistent 3D hydrodynamical (HD)\nand magneto-hydrodynamical (MHD) simulations of self-gravitating discs.\nMotivated by the complex structure of protoplanetary clumps we do not introduce\nsink particles. We find that the orbital evolution of the clumps has a\nstochastic character but also exhibits recurrent properties over many orbits.\nClump migration is governed by two sources of gravitational torques: a torque\noriginating from a region about twice the Hill sphere around each clump's\norbit, and the torque resulting from clump-clump interactions. Compared to\nnon-magnetized companion runs, the latter are more frequent in MHD simulations,\nwhich give rise to more numerous clumps starting off at smaller masses, often\nbelow a Neptune mass. Clump-clump interactions can lead to temporary strong\naccelerations of migration in both directions, but integrated over time provide\na lesser impact than disc-driven torques. They can also lead to clump mergers\nbut do not cause ejections; a difference to previous works which adopted sink\nparticles. The local \"Hill torque\" is responsible for the fast migration,\ninward or outward. Estimating the characteristic timescales of conventional\nmigration in our regime, we find that the disc-driven migration timescales are\nin agreement with Type III migration. However, the dominant local torque is\nrapidly fluctuating, which reflects the turbulent nature of the flow. The\nresulting stochastic migration pattern is markedly different from Type III\nrunaway migration and appears to be a distinctive feature of orbital dynamics\nin a fragmenting disc.",
        "We introduce the concept of F-decomposable systems, well-ordered inverse\nsystems of Hausdorff compacta with fully closed bonding mappings. A continuous\nmapping between Hausdorff compacta is called fully closed if the intersection\nof the images of any two closed disjoint subsets is finite. We give a\ncharacterization of such systems in terms of a property of the continuous\nfunctions on their limit. When, moreover, the fibers of neighboring bonding\nmappings are metrizable, we call the limit of such a system an F_d-compact, a\nparticular case of a Fedorchuk compact. The stated property allows us to obtain\na locally uniformly rotund renorming on the space C(K), where K is an\nF_d-compact of countable spectral height.",
        "Analog quantum computation is an attractive paradigm for the simulation of\ntime-dependent quantum systems. Programmable analog quantum computers have been\nrealized in hardware using a variety of physical principles, including\nneutral-atom and superconducting technologies. The input parameters of the\nphysical Hamiltonians that are used to program the quantum simulator generally\ndiffer from the parameters that characterize the output distribution of data\nproduced under a specified quantum dynamics. The relationship between the input\nand output parameters is known as the response function of the analog device.\nHere, we introduce a streaming algorithm for learning the response function of\nanalog quantum computers from arbitrary user inputs, thus not requiring special\ncalibration runs. We use the method to learn and compare the response functions\nof several generations of analog quantum simulators based on superconducting\nand neutral-atom programmable arrays.",
        "In addition to sunspots, which represent the most easily visualized\nmanifestation of solar magnetism, cutting-edge observations of the solar\natmosphere have uncovered a plethora of magnetic flux tubes, down to the\nresolving power of modern high-resolution telescopes (a few tens of km),\nrevealing how the Sun is a fully magnetized star. These magnetic elements are\nadvected and buffeted by ambient plasma flows and turbulent convection,\nresulting in perturbations of the flux tubes that make them natural conduits\nfor channeling wave energy into the upper layers of the Sun's atmosphere and\nsignificantly contributing to the acceleration of the solar wind. Today, data\nacquired by the Helioseismic and Magnetic Imager (HMI) onboard NASA's Solar\nDynamics Observatory (SDO), have made it possible to study the dynamics of\nsmall-scale magnetic fields over long timescales. Here, for the first time, we\npresent the discovery of a modulation in the dynamical behavior of small-scale\nmagnetic concentrations in the photosphere over temporal scales consistent with\nthe solar activity cycle (i.e. 11 years), which has only been made possible by\nthe long observing lifetime of the SDO\/HMI spacecraft. Furthermore, a temporal\nvarying polarization of their perturbations is also found on similar\ntimescales. This demonstrates how the small-scale dynamics of magnetic fields\nare also affected by the global dynamo. These discoveries were realized through\nautomated tracking of magnetic fields in the solar photosphere across 11\ncontinuous years, resulting in the most extended statistical analyses of its\nkind so far, with more than 31 million magnetic concentrations examined.",
        "We consider a joint communication and sensing problem in an optical link in\nwhich a low-power transmitter attempts to communicate with a receiver while\nsimultaneously identifying the range of a defect creating a backscattered\nsignal. We model the system as a lossy thermal noise bosonic channel in which\nthe location of the target, modeled as a beamsplitter, affects the timing of\nthe backscattered signal. Motivated by the envisioned deployment of\nentanglement sharing quantum networks, we allow the transmitter to exploit\nentanglement to assist its sensing and communication. Since entanglement is\nknown to enhance sensing, as known from quantum illumination, and increase\ncommunication rates, as known from the characterization of the\nentanglement-assisted capacity, the transmitter is faced with a trade-off and\nmust judiciously allocate its entanglement resources. Our main result is a\ncharacterization of the trade-offs incurred in the form of an achievable\nrate\/error-exponent region which can beat time-sharing in certain cases. The\nproof of our result relies on technical results of independent interests, by\nwhich we carefully show how to extend the known asymptotic characterization of\nmulti-hypothesis testing Chernoff exponent in finite-dimensional spaces to\ninfinite-dimensional spaces and provide a characterization of phase shift\nkeying modulated displaced thermal states in Fock basis.",
        "The human MHC transplantation loci (HLA-A, -B, -C, -DPB1, -DQB1, -DRB1) are\nthe most polymorphic in the human genome. It is generally accepted this\npolymorphism reflects a role in presenting pathogen-derived peptide to the\nadaptive immune system. Proposed mechanisms for the polymorphism such as\nnegative frequency-dependent selection (NFDS) and heterozygote advantage (HA)\nfocus on HLA alleles, not haplotypes. Here, we propose a model for the\npolymorphism in which infectious diseases impose independent density-dependent\nregulation on HLA haplotypes. More specifically, a complex pathogen environment\ndrives extensive host polymorphism through a guild of HLA haplotypes that are\nspecialised and show incomplete peptide recognition. Separation of haplotype\nguilds is maintained by limiting similarity. The outcome is a wide and stable\nrange of haplotype densities at steady-state in which effective Fisher\nfitnesses are zero. Densities, and therefore frequencies, emerge theoretically\nas alternative measures of fitness. A catalogue of ranked frequencies is\ntherefore one of ranked fitnesses. The model is supported by data from a range\nof sources including a Caucasian HLA dataset compiled by the US National Marrow\nDonor Program (NMDP). These provide evidence of positive selection on the top\n350-2000 5-locus HLA haplotypes taken from an overall NMDP sample set of 10E5.\nHigh-fitness haplotypes drive the selection of 137 high-frequency alleles\nspread across the 5 HLA loci under consideration. These alleles demonstrate\npositive epistasis and pleiotropy in the formation of haplotypes. Allelic\npleiotropy creates a network of highly inter-related HLA haplotypes that\naccount for 97% of the census sample. We suggest this network has properties of\na quasi-species and is itself under selection. We also suggest this is the\norigin of balancing selection in the HLA system.",
        "The recently suggested bipartite analysis extends the Kauffman planar\ndecomposition to arbitrary $N$, i.e. extends it from the Jones polynomial to\nthe HOMFLY polynomial. This provides a generic and straightforward\nnon-perturbative calculus in an arbitrary Chern--Simons theory. Technically,\nthis approach is restricted to knots and links which possess bipartite\nrealizations, i.e. can be entirely glued from antiparallel lock (two-vertex)\ntangles rather than single-vertex $R$-matrices. However, we demonstrate that\nthe resulting positive decomposition (PD), i.e. the representation of the\nfundamental HOMFLY polynomials as positive integer polynomials of the three\nparameters $\\phi$, $\\bar\\phi$ and $D$, exists for arbitrary knots, not only\nbipartite ones. This poses new questions about the true significance of\nbipartite expansion, which appears to make sense far beyond its original scope,\nand its generalizations to higher representations. We have provided two\nexplanations for the existence of the PD for non-bipartite knots. An\ninteresting option is to resolve a particular bipartite vertex in a\nnot-fully-bipartite diagram and reduce the HOMFLY polynomial to a linear\ncombination of those for smaller diagrams. If the resulting diagrams correspond\nto bipartite links, this option provides a PD even to an initially\nnon-bipartite knot. Another possibility for a non-bipartite knot is to have a\nbipartite clone with the same HOMFLY polynomial providing this PD. We also\nsuggest a promising criterium for the existence of a bipartite realization\nbehind a given PD, which is based on the study of the precursor Jones\npolynomials.",
        "In this article, we propose a novel logistic quasi-maximum likelihood\nestimation (LQMLE) for general parametric time series models. Compared to the\nclassical Gaussian QMLE and existing robust estimations, it enjoys many\ndistinctive advantages, such as robustness in respect of distributional\nmisspecification and heavy-tailedness of the innovation, more resiliency to\noutliers, smoothness and strict concavity of the log logistic quasi-likelihood\nfunction, and boundedness of the influence function among others. Under some\nmild conditions, we establish the strong consistency and asymptotic normality\nof the LQMLE. Moreover, we propose a new and vital parameter identifiability\ncondition to ensure desirable asymptotics of the LQMLE. Further, based on the\nLQMLE, we consider the Wald test and the Lagrange multiplier test for the\nunknown parameters, and derive the limiting distributions of the corresponding\ntest statistics. The applicability of our methodology is demonstrated by\nseveral time series models, including DAR, GARCH, ARMA-GARCH, DTARMACH, and\nEXPAR. Numerical simulation studies are carried out to assess the finite-sample\nperformance of our methodology, and an empirical example is analyzed to\nillustrate its usefulness.",
        "As quantum computing technology continues to mature, industry is adopting\nmodular quantum architectures to keep quantum scaling on the projected path and\nmeet performance targets. However, the complexity of chiplet-based quantum\ndevices, coupled with their growing size, presents an imminent scalability\nchallenge for quantum compilation. Contemporary compilation methods are not\nwell-suited to chiplet architectures. In particular, existing qubit allocation\nmethods are often unable to contend with inter-chiplet links, which don't\nnecessary support a universal basis gate set. Furthermore, existing methods of\nlogical-to-physical qubit placement, swap insertion (routing), unitary\nsynthesis, and\/or optimization are typically not designed for qubit links of\nsignificantly varying levels of duration or fidelity. In this work, we propose\nSEQC, a complete and parallelized compilation pipeline optimized for\nchiplet-based quantum computers, including several novel methods for qubit\nplacement, qubit routing, and circuit optimization. SEQC attains up to a 36%\nincrease in circuit fidelity, accompanied by execution time improvements of up\nto 1.92x. Additionally, owing to its ability to parallelize compilation, SEQC\nachieves consistent solve time improvements of 2-4x over a chiplet-aware Qiskit\nbaseline.",
        "The objective of this paper is to weaken the Lipschitz condition to a\nmonotonicity condition and to study the corresponding Pontryagin stochastic\nmaximum principle (SMP) for a mean-field optimal control problem under\nmonotonicity conditions.The dynamics of the controlled state process is\ngoverned by a mean-field stochastic differential equation (SDE) whose\ncoefficients depend not only on the control, the controlled state process\nitself but also on its law, and in particular, these coefficients satisfy the\nmonotonicity condition with respect to both the controlled state process and\nits distribution. The associated cost functional is also of mean-field type.\nUnder the assumption of a convex control domain we derive the SMP, which\nprovides a necessary optimality condition for control processes. Under\nadditional convexity assumptions on the Hamiltonian, we further prove that this\nnecessary condition is also a sufficient one. To achieve this, we first address\nthe challenges related to the existence and the uniqueness of solutions for\nmean-field backward stochastic differential equations and mean-field SDEs whose\ncoefficients satisfy monotonicity conditions with respect to both the solution\nas well as its distribution. On the other hand we also construct several\nillustrative examples demonstrating the generality of our results compared to\nexisting literature."
      ]
    }
  },
  {
    "id":2412.18156,
    "research_type":"applied",
    "start_id":"b18",
    "start_title":"GeneCompass: Deciphering Universal Gene Regulatory Mechanisms with Knowledge-Informed Cross-Species Foundation Model",
    "start_abstract":"Abstract Deciphering the universal gene regulatory mechanisms in diverse organisms holds great potential to advance our knowledge of fundamental life process and facilitate research on clinical applications. However, traditional paradigm primarily focuses individual model organisms, resulting limited collection integration complex features various cell types across species. Recent breakthroughs single-cell sequencing advancements deep learning techniques present an unprecedented opportunity tackle this challenge. In study, we developed GeneCompass, first knowledge-informed, cross-species foundation pre-trained extensive dataset over 120 million transcriptomes from human mouse. During pre-training, GeneCompass effectively integrates four biological prior enhance understanding a self-supervised manner. Fine-tuning towards multiple downstream tasks, outperforms competing state-of-the-art models tasks single species unlocks new realms investigation. Overall, marks milestone advancing accelerating discovery key fate regulators candidate targets for drug development.",
    "start_categories":[
      "q-bio.GN"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b2"
      ],
      "title":[
        "Llamafactory: Unified efficient fine-tuning of 100+ language models"
      ],
      "abstract":[
        "Efficient fine-tuning is vital for adapting large language models (LLMs) to downstream tasks. However, it requires non-trivial efforts to implement these methods on different models. We present LlamaFactory, a unified framework that integrates a suite of cutting-edge efficient training methods. It provides a solution for flexibly customizing the fine-tuning of 100+ LLMs without the need for coding through the built-in web UI LlamaBoard. We empirically validate the efficiency and effectiveness of our framework on language modeling and text generation tasks."
      ],
      "categories":[
        "cs.CL"
      ]
    },
    "list":{
      "title":[
        "Computation of the Hilbert Series for the Support-Minors Modeling of the\n  MinRank Problem",
        "A glimpse into an effective world",
        "Imprints of an early matter-dominated era arising from dark matter\n  dilution mechanism on cosmic string dynamics and gravitational wave\n  signatures",
        "Giant Kohn anomaly and chiral phonons in the charge density wave phase\n  of 1H-NbSe$_2$",
        "A study on $T$-equivalent graphs",
        "Criteria for unbiased estimation: applications to noise-agnostic sensing\n  and learnability of quantum channel",
        "Long-Term Planning Around Humans in Domestic Environments with 3D Scene\n  Graphs",
        "Applied Machine Learning Methods with Long-Short Term Memory Based\n  Recurrent Neural Networks for Multivariate Temperature Prediction",
        "GenMetaLoc: Learning to Learn Environment-Aware Fingerprint Generation\n  for Sample Efficient Wireless Localization",
        "On the structure of some one-generator nilpotent braces",
        "Multi-Instance Partial-Label Learning with Margin Adjustment",
        "A Systematic Review of ECG Arrhythmia Classification: Adherence to\n  Standards, Fair Evaluation, and Embedded Feasibility",
        "Evaluation of Large Language Models via Coupled Token Generation",
        "Hand-Object Contact Detection using Grasp Quality Metrics",
        "Processes on Wasserstein spaces and energy-minimizing particle\n  representations in fractional Sobolev spaces",
        "Phase-matching of high harmonic generation in twisted solids",
        "Bridging conformal field theory and parton approaches to SU(n)_k chiral\n  spin liquids",
        "Research on a Driver's Perceived Risk Prediction Model Considering\n  Traffic Scene Interaction",
        "Latents of latents to delineate pixels: hybrid Matryoshka\n  autoencoder-to-U-Net pairing for segmenting large medical images in GPU-poor\n  and low-data regimes",
        "Trend-encoded Probabilistic Multi-order Model: A Non-Machine Learning\n  Approach for Enhanced Stock Market Forecasts",
        "Analyzing Swimming Performance Using Drone Captured Aerial Videos",
        "Spontaneous Magnon Decays from Nonrelativistic Time-Reversal Symmetry\n  Breaking in Altermagnets",
        "On The Origin of Cultural Biases in Language Models: From Pre-training\n  Data to Linguistic Phenomena",
        "Guidelines for Applying RL and MARL in Cybersecurity Applications",
        "Benefits of Learning Rate Annealing for Tuning-Robustness in Stochastic\n  Optimization",
        "Text2Scenario: Text-Driven Scenario Generation for Autonomous Driving\n  Test",
        "ZK Secret Santa",
        "Velocity Map Imaging Spectrometer Optimized for Reduction of Background\n  from Scattered UV Light",
        "Prediction-Assisted Online Distributed Deep Learning Workload Scheduling\n  in GPU Clusters"
      ],
      "abstract":[
        "The MinRank problem is a simple linear algebra problem: given matrices with\ncoefficients in a field, find a non trivial linear combination of the matrices\nthat has a small rank. There are several algebraic modeling of the problem. The\nmain ones are: the Kipnis-Shamir modeling, the Minors modeling and the\nSupport-Minors modeling. The Minors modeling has been studied by Faug\\`ere et\nal. in 2010, where the authors provide an analysis of the complexity of\ncomputing a Gr\\\"obner basis of the modeling, through the computation of the\nexact Hilbert Series for a generic instance. For the Support-Minors modeling,\nthe first terms of the Hilbert Series are given by Bardet et al. in 2020 based\non an heuristic and experimental work. In this work, we provide a formula and a\nproof for the complete Hilbert Series of the Support Minors modeling for\ngeneric instances. This is done by adapting well known results on determinantal\nideals to an ideal generated by a particular subset of the set of all minors of\na matrix of variables. We then show that this ideal is generated by a\nparticular subset of the set of all minors of a matrix of variables. We then\nshow that this ideal is generated by standard monomials having a particular\nshape, and derive the Hilbert Series by counting the number of such standard\nmonomials. Following the work done for the Minors Modeling, we then transfer\nthe properties of this particular determinantal ideal to ideals generated by\nthe Support Minors system, by adding generic forms. This work allows to make a\nprecise comparison between the Minors and Support Minors modeling, and a\nprecise estimate of the complexity of solving MinRank instances for the\nparameters of the Mirath signature scheme that is currently at the second round\nof the NIST standardization process for Additional Digital Signature Schemes.",
        "Our contribution aims to celebrate the immeasurable contribution that Tom Kuo\nhas provided to the understanding of the structure of atomic nuclei, and also\nof the infinite nuclear matter, in terms of the fundamental principles\ngoverning the realistic nuclear potential. The authors want to testify Tom\nKuo's heritage and impact on their approach to the study of nuclear systems by\nreviewing some recent findings on the role of the two-body component of\nshell-model effective $\\beta$-decay operators. The focus is spotted on the\nso-called Pauli-blocking effect, that plays a non-negligible role in nuclei\ncharacterized by a large number of valence nucleons.",
        "We investigate the influence of an early matter-dominated era in cosmic\nhistory on the dynamics of cosmic strings and the resulting stochastic\ngravitational waves. Specifically, we examine the case where this era\noriginates from the dark matter dilution mechanism within the framework of the\nminimal left-right symmetric model. By numerically solving the Boltzmann\nequations governing the energy densities of the relevant components, we\nmeticulously analyze the modifications to the cosmological scale factor, the\nnumber density of cosmic string loops, and the gravitational wave spectrum. Our\nresults reveal that the early matter-dominated era causes a characteristic\nsuppression in the high-frequency regime of the gravitational wave spectrum,\nproviding distinct and testable signatures for future ground-based\ninterferometer experiments.",
        "Despite extensive investigations, many aspects of charge density waves (CDWs)\nremain elusive, especially the relative roles of electron-phonon coupling and\nFermi surface nesting as the underlying driving mechanisms responsible for the\nemergence of the CDW vector $\\bl Q_{CDW}$. It is puzzling that even though\nelectrons interact strongly with optical phonons in many correlated systems,\nthe actual mode softening is of an acoustic mode. Here we consider monolayer\n1H-NbSe$_2$ as an exemplar system, and through an accurate computation of the\nphonon self-energy, including its off-diagonal components. We provide\ncompelling evidence that the relevant mode is a longitudinal optical phonon\nthat softens by anti-crossing several intervening phonon bands. We also show\nthat $\\bl Q_{CDW}$ is fixed by the convolution of the susceptibility and\nelectron-phonon coupling, and that the softened phonons are circularly\npolarized.",
        "In his article [J. Comb. Theory Ser. B 16 (1974), 168-174], Tutte called two\ngraphs $T$-equivalent (i.e., codichromatic) if they have the same Tutte\npolynomial and showed that graphs $G$ and $G'$ are $T$-equivalent if $G'$ is\nobtained from $G$ by flipping a rotor (i.e., replacing it by its mirror) of\norder at most $5$, where a rotor of order $k$ in $G$ is an induced subgraph $R$\nhaving an automorphism $\\psi$ with a vertex orbit $\\{\\psi^i(u): i\\ge 0\\}$ of\nsize $k$ such that every vertex of $R$ is only adjacent to vertices in $R$\nunless it is in this vertex orbit. In this article, we first show the above\nresult due to Tutte can be extended to a rotor $R$ of order $k\\ge 6$ if the\nsubgraph of $G$ induced by all those edges of $G$ which are not in $R$\nsatisfies certain conditions. Also, we provide a new method for generating\ninfinitely many non-isomorphic $T$-equivalent pairs of graphs.",
        "We establish the necessary and sufficient conditions for unbiased estimation\nin multi-parameter estimation tasks. More specifically, we first consider\nquantum state estimation, where multiple parameters are encoded in a quantum\nstate, and derive two equivalent necessary and sufficient conditions for an\nunbiased estimation: one formulated in terms of the quantum Fisher information\nmatrix (QFIM) and the other based on the derivatives of the encoded state.\nFurthermore, we introduce a generalized quantum Cram\\'er-Rao bound, which\nprovides a fundamental achievable lower bound on the estimation error even when\nthe QFIM is non-invertible. To demonstrate the utility of our framework, we\nconsider phase estimation under unknown Pauli noise. We show that while\nunbiased phase estimation is infeasible with a naive scheme, employing an\nentangled probe with a noiseless ancilla enables unbiased estimation. Next, we\nextend our analysis to quantum channel estimation (equivalently, quantum\nchannel learning), where the goal is to estimate parameters characterizing an\nunknown quantum channel. We establish the necessary and sufficient condition\nfor unbiased estimation of these parameters. Notably, by interpreting unbiased\nestimation as learnability, our result applies to the fundamental learnability\nof parameters in general quantum channels. As a concrete application, we\ninvestigate the learnability of noise affecting non-Clifford gates via cycle\nbenchmarking.",
        "Long-term planning for robots operating in domestic environments poses unique\nchallenges due to the interactions between humans, objects, and spaces. Recent\nadvancements in trajectory planning have leveraged vision-language models\n(VLMs) to extract contextual information for robots operating in real-world\nenvironments. While these methods achieve satisfying performance, they do not\nexplicitly model human activities. Such activities influence surrounding\nobjects and reshape spatial constraints. This paper presents a novel approach\nto trajectory planning that integrates human preferences, activities, and\nspatial context through an enriched 3D scene graph (3DSG) representation. By\nincorporating activity-based relationships, our method captures the spatial\nimpact of human actions, leading to more context-sensitive trajectory\nadaptation. Preliminary results demonstrate that our approach effectively\nassigns costs to spaces influenced by human activities, ensuring that the robot\ntrajectory remains contextually appropriate and sensitive to the ongoing\nenvironment. This balance between task efficiency and social appropriateness\nenhances context-aware human-robot interactions in domestic settings. Future\nwork includes implementing a full planning pipeline and conducting user studies\nto evaluate trajectory acceptability.",
        "This paper gives an overview on how to develop a dense and deep neural\nnetwork for making a time series prediction. First, the history and\ncornerstones in Artificial Intelligence and Machine Learning will be presented.\nAfter a short introduction to the theory of Artificial Intelligence and Machine\nLearning, the paper will go deeper into the techniques for conducting a time\nseries prediction with different models of neural networks. For this project,\nPython's development environment Jupyter, extended with the TensorFlow package\nand deep-learning application Keras is used. The system setup and project\nframework are explained in more detail before discussing the time series\nprediction. The main part shows an applied example of time series prediction\nwith weather data. For this work, a deep recurrent neural network with Long\nShort-Term Memory cells is used to conduct the time series prediction. The\nresults and evaluation of the work show that a weather prediction with deep\nneural networks can be successful for a short time period. However, there are\nsome drawbacks and limitations with time series prediction, which will be\ndiscussed towards the end of the paper.",
        "Existing fingerprinting-based localization methods often require extensive\ndata collection and struggle to generalize to new environments. In contrast to\nprevious environment-unknown MetaLoc, we propose GenMetaLoc in this paper,\nwhich first introduces meta-learning to enable the generation of dense\nfingerprint databases from an environment-aware perspective. In the model\naspect, the learning-to-learn mechanism accelerates the fingerprint generation\nprocess by facilitating rapid adaptation to new environments with minimal data.\nAdditionally, we incorporate 3D point cloud data from the first Fresnel zone\nbetween the transmitter and receiver, which describes the obstacles\ndistribution in the environment and serves as a condition to guide the\ndiffusion model in generating more accurate fingerprints. In the data\nprocessing aspect, unlike most studies that focus solely on channel state\ninformation (CSI) amplitude or phase, we present a comprehensive processing\nthat addresses both, correcting errors from WiFi hardware limitations such as\namplitude discrepancies and frequency offsets. For the data collection\nplatform, we develop an uplink wireless localization system that leverages the\nsensing capabilities of existing commercial WiFi devices and mobile phones,\nthus reducing the need for additional deployment costs. Experimental results on\nreal datasets show that our framework outperforms baseline methods.",
        "This article provides a detailed description of some nilpotent left braces\ngenerated by one element.",
        "Multi-instance partial-label learning (MIPL) is an emerging learning\nframework where each training sample is represented as a multi-instance bag\nassociated with a candidate label set. Existing MIPL algorithms often overlook\nthe margins for attention scores and predicted probabilities, leading to\nsuboptimal generalization performance. A critical issue with these algorithms\nis that the highest prediction probability of the classifier may appear on a\nnon-candidate label. In this paper, we propose an algorithm named MIPLMA, i.e.,\nMulti-Instance Partial-Label learning with Margin Adjustment, which adjusts the\nmargins for attention scores and predicted probabilities. We introduce a\nmargin-aware attention mechanism to dynamically adjust the margins for\nattention scores and propose a margin distribution loss to constrain the\nmargins between the predicted probabilities on candidate and non-candidate\nlabel sets. Experimental results demonstrate the superior performance of MIPLMA\nover existing MIPL algorithms, as well as other well-established multi-instance\nlearning algorithms and partial-label learning algorithms.",
        "The classification of electrocardiogram (ECG) signals is crucial for early\ndetection of arrhythmias and other cardiac conditions. However, despite\nadvances in machine learning, many studies fail to follow standardization\nprotocols, leading to inconsistencies in performance evaluation and real-world\napplicability. Additionally, hardware constraints essential for practical\ndeployment, such as in pacemakers, Holter monitors, and wearable ECG patches,\nare often overlooked. Since real-world impact depends on feasibility in\nresource-constrained devices, ensuring efficient deployment is critical for\ncontinuous monitoring. This review systematically analyzes ECG classification\nstudies published between 2017 and 2024, focusing on those adhering to the E3C\n(Embedded, Clinical, and Comparative Criteria), which include inter-patient\nparadigm implementation, compliance with Association for the Advancement of\nMedical Instrumentation (AAMI) recommendations, and model feasibility for\nembedded systems. While many studies report high accuracy, few properly\nconsider patient-independent partitioning and hardware limitations. We identify\nstate-of-the-art methods meeting E3C criteria and conduct a comparative\nanalysis of accuracy, inference time, energy consumption, and memory usage.\nFinally, we propose standardized reporting practices to ensure fair comparisons\nand practical applicability of ECG classification models. By addressing these\ngaps, this study aims to guide future research toward more robust and\nclinically viable ECG classification systems.",
        "State of the art large language models rely on randomization to respond to a\nprompt. As an immediate consequence, a model may respond differently to the\nsame prompt if asked multiple times. In this work, we argue that the evaluation\nand ranking of large language models should control for the randomization\nunderpinning their functioning. Our starting point is the development of a\ncausal model for coupled autoregressive generation, which allows different\nlarge language models to sample responses with the same source of randomness.\nBuilding upon our causal model, we first show that, on evaluations based on\nbenchmark datasets, coupled autoregressive generation leads to the same\nconclusions as vanilla autoregressive generation but using provably fewer\nsamples. However, we further show that, on evaluations based on (human)\npairwise comparisons, coupled and vanilla autoregressive generation can\nsurprisingly lead to different rankings when comparing more than two models,\neven with an infinite amount of samples. This suggests that the apparent\nadvantage of a model over others in existing evaluation protocols may not be\ngenuine but rather confounded by the randomness inherent to the generation\nprocess. To illustrate and complement our theoretical results, we conduct\nexperiments with several large language models from the Llama family. We find\nthat, across multiple knowledge areas from the popular MMLU benchmark dataset,\ncoupled autoregressive generation requires up to 40% fewer samples to reach the\nsame conclusions as vanilla autoregressive generation. Further, using data from\nthe LMSYS Chatbot Arena platform, we find that the win-rates derived from\npairwise comparisons by a strong large language model to prompts differ under\ncoupled and vanilla autoregressive generation.",
        "We propose a novel hand-object contact detection system based on grasp\nquality metrics extracted from object and hand poses, and evaluated its\nperformance using the DexYCB dataset. Our evaluation demonstrated the system's\nhigh accuracy (approaching 90%). Future work will focus on a real-time\nimplementation using vision-based estimation, and integrating it to a\nrobot-to-human handover system.",
        "Given a probability-measure-valued process $(\\mu_t)$, we aim to find, among\nall path-continuous stochastic processes whose one-dimensional time marginals\ncoincide almost surely with $(\\mu_t)$ (if there is any), a process that\nminimizes a given energy in expectation. Building on our recent study\n(arXiv:2502.12068), where the minimization of fractional Sobolev energy was\ninvestigated for deterministic paths on Wasserstein spaces, we now extend the\nresults to the stochastic setting to address some applications that originally\nmotivated our study. Two applications are given. We construct minimizing\nparticle representations for processes on Wasserstein spaces on $\\mathbb{R}$\nwith H\\\"{o}lder regularity, using optimal transportation. We prove the\nexistence of minimizing particle representations for solutions to stochastic\nFokker--Planck--Kolmogorov equations on $\\mathbb{R}^\\mathrm{d}$ satisfying an\nintegrability condition, using the stochastic superposition principle of\nLacker--Shkolnikov--Zhang (J. Eur. Math. Soc. 25, 3229--3288 (2023)).",
        "High harmonic generation (HHG) in solids could enable attosecond and\nultraviolet light sources with high compactness, great controllability and rich\nfunctions. However, the HHG process is accompanied by a quite large wavevector\nmismatch that is uncompensated by any traditional phase-matching method,\ndirectly limiting its energy conversion efficiency. Here, we propose an\neffective strategy for phase-matching of HHG with arbitrary harmonic orders in\nsolids. Two flakes of solids with an interlayer twist induce a nonlinear\noptical phase that depends on the crystal symmetry, twist angle and harmonic\norder, which can be accurately designed to compensate for the phase mismatch in\nHHG. Guided by the twist-phase-matching theory, we achieved a record-high\nconversion efficiency of $~1.5\\times10^{-5}$ for the fifth HHG in twisted\nhexagonal boron nitride crystals with a total thickness of only 1 ${\\mu}m$. Our\nwork establishes a foundation for developing ultrashort-wavelength and\nultrafast-pulse laser sources in compact solid-state tabletop systems for\nfundamental and applied sciences.",
        "We employ the SU(n)_k Wess-Zumino-Witten (WZW) model in conformal field\ntheory to construct lattice wave functions in both one and two dimensions. It\nis unveiled that these wave functions can be reinterpreted as parton states,\nwhich enables efficient conversion to matrix product states such that many\nphysical properties can be evaluated directly. In one dimension, these wave\nfunctions describe critical spin chains whose universality classes are in\none-to-one correspondence with the WZW models used in the construction. In two\ndimensions, our constructions yield model wave functions for chiral spin\nliquids, and we show how to find all topological sectors of them in a\nsystematic way. Using the null vectors of Kac-Moody algebras, parent\nHamiltonians of the SU(3)_k series are derived. The SU(3)_k chiral spin liquids\nare lattice analogs of non-Abelian spin-singlet fractional quantum Hall states,\nand the k = 2 member hosts Fibonacci anyons.",
        "In the field of conditional autonomous driving technology, driver perceived\nrisk prediction plays a crucial role in reducing traffic risks and ensuring\npassenger safety. This study introduces an innovative perceived risk prediction\nmodel for human-machine interaction in intelligent driving systems. The model\naims to enhance prediction accuracy and, thereby, ensure passenger safety.\nThrough a comprehensive analysis of risk impact mechanisms, we identify three\nkey categories of factors, both subjective and objective, influencing perceived\nrisk: driver's personal characteristics, ego-vehicle motion, and surrounding\nenvironment characteristics. We then propose a deep-learning-based risk\nprediction network that uses the first two categories of factors as inputs. The\nnetwork captures the interactive relationships among traffic participants in\ndynamic driving scenarios. Additionally, we design a personalized modeling\nstrategy that incorporates driver-specific traits to improve prediction\naccuracy. To ensure high-quality training data, we conducted a rigorous video\nrating experiment. Experimental results show that the proposed network achieves\na 10.0% performance improvement over state-of-the-art methods. These findings\nsuggest that the proposed network has significant potential to enhance the\nsafety of conditional autonomous driving systems.",
        "Medical images are often high-resolution and lose important detail if\ndownsampled, making pixel-level methods such as semantic segmentation much less\nefficient if performed on a low-dimensional image. We propose a low-rank\nMatryoshka projection and a hybrid segmenting architecture that preserves\nimportant information while retaining sufficient pixel geometry for pixel-level\ntasks. We design the Matryoshka Autoencoder (MatAE-U-Net) which combines the\nhierarchical encoding of the Matryoshka Autoencoder with the spatial\nreconstruction capabilities of a U-Net decoder, leveraging multi-scale feature\nextraction and skip connections to enhance accuracy and generalisation. We\napply it to the problem of segmenting the left ventricle (LV) in\nechocardiographic images using the Stanford EchoNet-D dataset, including 1,000\nstandardised video-mask pairs of cardiac ultrasound videos resized to 112x112\npixels. The MatAE-UNet model achieves a Mean IoU of 77.68\\%, Mean Pixel\nAccuracy of 97.46\\%, and Dice Coefficient of 86.91\\%, outperforming the\nbaseline U-Net, which attains a Mean IoU of 74.70\\%, Mean Pixel Accuracy of\n97.31\\%, and Dice Coefficient of 85.20\\%. The results highlight the potential\nof using the U-Net in the recursive Matroshka latent space for imaging problems\nwith low-contrast such as echocardiographic analysis.",
        "In recent years, the dominance of machine learning in stock market\nforecasting has been evident. While these models have shown decreasing\nprediction errors, their robustness across different datasets has been a\nconcern. A successful stock market prediction model minimizes prediction errors\nand showcases robustness across various data sets, indicating superior\nforecasting performance. This study introduces a novel multiple lag order\nprobabilistic model based on trend encoding (TeMoP) that enhances stock market\npredictions through a probabilistic approach. Results across different stock\nindexes from nine countries demonstrate that the TeMoP outperforms the\nstate-of-the-art machine learning models in predicting accuracy and\nstabilization.",
        "Monitoring swimmer performance is crucial for improving training and\nenhancing athletic techniques. Traditional methods for tracking swimmers, such\nas above-water and underwater cameras, face limitations due to the need for\nmultiple cameras and obstructions from water splashes. This paper presents a\nnovel approach for tracking swimmers using a moving UAV. The proposed system\nemploys a UAV equipped with a high-resolution camera to capture aerial footage\nof the swimmers. The footage is then processed using computer vision algorithms\nto extract the swimmers' positions and movements. This approach offers several\nadvantages, including single camera use and comprehensive coverage. The\nsystem's accuracy is evaluated with both training and in competition videos.\nThe results demonstrate the system's ability to accurately track swimmers'\nmovements, limb angles, stroke duration and velocity with the maximum error of\n0.3 seconds and 0.35~m\/s for stroke duration and velocity, respectively.",
        "Quasiparticles are central to condensed matter physics, but their stability\ncan be undermined by quantum many-body interactions. Magnons, quasiparticles in\nquantum magnets, are particularly intriguing because their properties are\ngoverned by both real and spin space. While crystal symmetries may be low, spin\ninteractions often remain approximately isotropic, limiting spontaneous magnon\ndecay. Textbook wisdom holds that collinear Heisenberg magnets follow a\ndichotomy: ferromagnets host stable magnons, while antiferromagnetic magnons\nmay decay depending on dispersion curvature. Up to now, relativistic spin-orbit\ncoupling and noncollinear order that connect spin space to real space, were\nshown to introduce more complex magnon instability mechanisms. Here, we show\nthat even in nonrelativistic isotropic collinear systems, this conventional\ndichotomy is disrupted in altermagnets. Altermagnets, a newly identified class\nof collinear magnets, exhibit compensated spin order with nonrelativistic\ntime-reversal symmetry breaking and even-parity band splitting. Using kinematic\nanalysis, nonlinear spin-wave theory, and quantum simulations, we reveal that\neven weak band splitting opens a decay phase space, driving quasiparticle\nbreakdown. Additionally, d-wave altermagnets form a rare ``island of\nstability'' at the Brillouin zone center. Our findings establish a\nquasiparticle stability trichotomy in collinear Heisenberg magnets and position\naltermagnets as a promising platform for unconventional spin dynamics.",
        "Language Models (LMs) have been shown to exhibit a strong preference towards\nentities associated with Western culture when operating in non-Western\nlanguages. In this paper, we aim to uncover the origins of entity-related\ncultural biases in LMs by analyzing several contributing factors, including the\nrepresentation of entities in pre-training data and the impact of variations in\nlinguistic phenomena across languages. We introduce CAMeL-2, a parallel\nArabic-English benchmark of 58,086 entities associated with Arab and Western\ncultures and 367 masked natural contexts for entities. Our evaluations using\nCAMeL-2 reveal reduced performance gaps between cultures by LMs when tested in\nEnglish compared to Arabic. We find that LMs struggle in Arabic with entities\nthat appear at high frequencies in pre-training, where entities can hold\nmultiple word senses. This also extends to entities that exhibit high lexical\noverlap with languages that are not Arabic but use the Arabic script. Further,\nwe show how frequency-based tokenization leads to this issue in LMs, which gets\nworse with larger Arabic vocabularies. We will make CAMeL-2 available at:\nhttps:\/\/github.com\/tareknaous\/camel2",
        "Reinforcement Learning (RL) and Multi-Agent Reinforcement Learning (MARL)\nhave emerged as promising methodologies for addressing challenges in automated\ncyber defence (ACD). These techniques offer adaptive decision-making\ncapabilities in high-dimensional, adversarial environments. This report\nprovides a structured set of guidelines for cybersecurity professionals and\nresearchers to assess the suitability of RL and MARL for specific use cases,\nconsidering factors such as explainability, exploration needs, and the\ncomplexity of multi-agent coordination. It also discusses key algorithmic\napproaches, implementation challenges, and real-world constraints, such as data\nscarcity and adversarial interference. The report further outlines open\nresearch questions, including policy optimality, agent cooperation levels, and\nthe integration of MARL systems into operational cybersecurity frameworks. By\nbridging theoretical advancements and practical deployment, these guidelines\naim to enhance the effectiveness of AI-driven cyber defence strategies.",
        "The learning rate in stochastic gradient methods is a critical hyperparameter\nthat is notoriously costly to tune via standard grid search, especially for\ntraining modern large-scale models with billions of parameters. We identify a\ntheoretical advantage of learning rate annealing schemes that decay the\nlearning rate to zero at a polynomial rate, such as the widely-used cosine\nschedule, by demonstrating their increased robustness to initial parameter\nmisspecification due to a coarse grid search. We present an analysis in a\nstochastic convex optimization setup demonstrating that the convergence rate of\nstochastic gradient descent with annealed schedules depends sublinearly on the\nmultiplicative misspecification factor $\\rho$ (i.e., the grid resolution),\nachieving a rate of $O(\\rho^{1\/(2p+1)}\/\\sqrt{T})$ where $p$ is the degree of\npolynomial decay and $T$ is the number of steps, in contrast to the\n$O(\\rho\/\\sqrt{T})$ rate that arises with fixed stepsizes and exhibits a linear\ndependence on $\\rho$. Experiments confirm the increased robustness compared to\ntuning with a fixed stepsize, that has significant implications for the\ncomputational overhead of hyperparameter search in practical training\nscenarios.",
        "Autonomous driving (AD) testing constitutes a critical methodology for\nassessing performance benchmarks prior to product deployment. The creation of\nsegmented scenarios within a simulated environment is acknowledged as a robust\nand effective strategy; however, the process of tailoring these scenarios often\nnecessitates laborious and time-consuming manual efforts, thereby hindering the\ndevelopment and implementation of AD technologies. In response to this\nchallenge, we introduce Text2Scenario, a framework that leverages a Large\nLanguage Model (LLM) to autonomously generate simulation test scenarios that\nclosely align with user specifications, derived from their natural language\ninputs. Specifically, an LLM, equipped with a meticulously engineered input\nprompt scheme functions as a text parser for test scenario descriptions,\nextracting from a hierarchically organized scenario repository the components\nthat most accurately reflect the user's preferences. Subsequently, by\nexploiting the precedence of scenario components, the process involves\nsequentially matching and linking scenario representations within a Domain\nSpecific Language corpus, ultimately fabricating executable test scenarios. The\nexperimental results demonstrate that such prompt engineering can meticulously\nextract the nuanced details of scenario elements embedded within various\ndescriptive formats, with the majority of generated scenarios aligning closely\nwith the user's initial expectations, allowing for the efficient and precise\nevaluation of diverse AD stacks void of the labor-intensive need for manual\nscenario configuration. Project page:\nhttps:\/\/caixxuan.github.io\/Text2Scenario.GitHub.io.",
        "This paper proposes a three-step Secret Santa algorithm with setup that\nleverages Zero Knowledge Proofs (ZKP) to set up gift sender\/receiver relations\nwhile maintaining the sender's confidentiality. The algorithm maintains a\npermutational derangement and does not require a central authority to perform\nsuccessfully. The described approach can be implemented in Solidity provided\nthe integration with a transaction relayer.",
        "Velocity map imaging spectroscopy is a powerful technique for detecting the\nmomentum distribution of photoelectrons resulting from an ionization experiment\non atoms or molecules. However, when used with ultraviolet light sources,\nscattered photons can lead to the emission of photoelectrons from the\nspectrometer's electrodes, giving rise to severe noise disturbing the desired\nsignal. We present a velocity map imaging spectrometer optimized to reduce\nunwanted background signals. The primary modifications to the conventional\ndesign include spectrometer electrode geometries with small cross section\nexposed to the scattered photons, with blocked pathways for photoelectrons from\nthe electrodes to the detector, as well as the incorporation of optical\nbaffles. Compared to a conventional design optimized solely on the\nspectrometer's photoelectron momentum resolution, we have achieved the\nelimination of 99.9 \\% of the background noise without substantial compromise\nto the resolution. Note that most of the improvements were achieved without the\nnecessity of high-grade windows, reducing the sensitivity to window degradation\nby UV light. We give general guidelines on efficiently coping with the\nlong-standing experimental problem of electron background originating from\nscattered light by considering it already in the design stage of a new\nspectrometer.",
        "The recent explosive growth of deep learning (DL) models has necessitated a\ncompelling need for efficient job scheduling for distributed deep learning\ntraining with mixed parallelisms (DDLwMP) in GPU clusters. This paper proposes\nan adaptive shortest-remaining-processing-time-first (A-SRPT) scheduling\nalgorithm, a novel prediction-assisted online scheduling approach designed to\nmitigate the challenges associated with DL cluster scheduling. By modeling each\njob as a graph corresponding to heterogeneous Deep Neural Network (DNN) models\nand their associated distributed training configurations, A-SRPT strategically\nassigns jobs to the available GPUs, thereby minimizing inter-server\ncommunication overhead. Observing that most DDLwMP jobs recur, A-SRPT\nincorporates a random forest regression model to predict training iterations.\nCrucially, A-SRPT maps the complex scheduling problem into a single-machine\ninstance, which is addressed optimally by a preemptive\n\"shortest-remaining-processing-time-first\" strategy. This optimized solution\nserves as a guide for actual job scheduling within the GPU clusters, leading to\na theoretically provable competitive scheduling efficiency. We conduct\nextensive real-world testbed and simulation experiments to verify our proposed\nalgorithms."
      ]
    }
  },
  {
    "id":2411.07871,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"2016 Alzheimer's disease facts and figures",
    "start_abstract":"This report describes the public health impact of Alzheimer's disease, including incidence and prevalence, mortality rates, costs care, overall on caregivers society. It also examines in detail financial families, annual to families difficult decisions must often make pay those costs. An estimated 5.4 million Americans have disease. By mid-century, number people living with disease United States is projected grow 13.8 million, fueled large part by aging baby boom generation. Today, someone country develops every 66 seconds. 2050, one new case expected develop 33 seconds, resulting nearly 1 cases per year. In 2013, official death certificates recorded 84,767 deaths from making it sixth leading cause fifth age \u2265 65 years. Between 2000 stroke, heart prostate cancer decreased 23%, 14%, 11%, respectively, whereas increased 71%. The actual which contributes likely much larger than certificates. 2016, an 700,000 years will die many them because complications caused 2015, more 15 family members other unpaid provided 18.1 billion hours care dementias, a contribution valued at $221 billion. Average per-person Medicare payments for services beneficiaries dementias are two half times as great all without these conditions, Medicaid 19 great. Total 2016 long-term hospice dementia be $236 may place substantial burden who take money out their retirement savings, cut back buying food, reduce own trips doctor. addition, incorrectly believe that pays nursing home types care. Such findings highlight need solutions prevent dementia-related jeopardizing security dementias.",
    "start_categories":[
      "q-bio.NC"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b5"
      ],
      "title":[
        "Computer-aided diagnosis of Alzheimer\u2019s disease and neurocognitive disorders with multimodal Bi-Vision Transformer (BiViT)"
      ],
      "abstract":[
        "<jats:title>Abstract<\/jats:title><jats:p>Cognitive disorders affect various cognitive functions that can have a substantial impact on individual\u2019s daily life. Alzheimer\u2019s disease (AD) is one of such well-known cognitive disorders. Early detection and treatment of cognitive diseases using artificial intelligence can help contain them. However, the complex spatial relationships and long-range dependencies found in medical imaging data present challenges in achieving the objective. Moreover, for a few years, the application of transformers in imaging has emerged as a promising area of research. A reason can be transformer\u2019s impressive capabilities of tackling spatial relationships and long-range dependency challenges in two ways, i.e., (1) using their self-attention mechanism to generate comprehensive features, and (2) capture complex patterns by incorporating global context and long-range dependencies. In this work, a Bi-Vision Transformer (BiViT) architecture is proposed for classifying different stages of AD, and multiple types of cognitive disorders from 2-dimensional MRI imaging data. More specifically, the transformer is composed of two novel modules, namely Mutual Latent Fusion (MLF) and Parallel Coupled Encoding Strategy (PCES), for effective feature learning. Two different datasets have been used to evaluate the performance of proposed BiViT-based architecture. The first dataset contain several classes such as mild or moderate demented stages of the AD. The other dataset is composed of samples from patients with AD and different cognitive disorders such as mild, early, or moderate impairments. For comprehensive comparison, a multiple transfer learning algorithm and a deep autoencoder have been each trained on both datasets. The results show that the proposed BiViT-based model achieves an accuracy of 96.38% on the AD dataset. However, when applied to cognitive disease data, the accuracy slightly decreases below 96% which can be resulted due to smaller amount of data and imbalance in data distribution. Nevertheless, given the results, it can be hypothesized that the proposed algorithm can perform better if the imbalanced distribution and limited availability problems in data can be addressed.<\/jats:p>\n                <jats:p><jats:bold>Graphical abstract<\/jats:bold><\/jats:p>"
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Dirichlet's Lemma in Number Fields",
        "Forecasting Monthly Residential Natural Gas Demand Using\n  Just-In-Time-Learning Modeling",
        "Non-Hermitian Aharonov-Bohm Cage in Bosonic Bogoliubov-de Gennes Systems",
        "Einstein multiply warped products and generalized Kasner manifolds with\n  multidimensional base",
        "Probing Topological Anderson Transition in Quasiperiodic Photonic\n  Lattices via Chiral Displacement and Wavelength Tuning",
        "Privacy-Aware RAG: Secure and Isolated Knowledge Retrieval",
        "Subwavelength plasmonic antennas based on asymmetric\n  split-ring-resonators for high near-field enhancements",
        "Poisoning Bayesian Inference via Data Deletion and Replication",
        "Parking Space Detection in the City of Granada",
        "COFO: COdeFOrces dataset for Program Classification, Recognition and\n  Tagging",
        "Estimating treatment effects with competing intercurrent events in\n  randomized controlled trials",
        "$\\eta$, $\\eta^\\prime$ mesons from lattice QCD in fully physical\n  conditions",
        "Commonsense Reasoning-Aided Autonomous Vehicle Systems",
        "The effect of longitudinal debonding on stress redistributions around\n  fiber breaks: Incorporating fiber diameter distribution and fiber\n  misalignment",
        "An empirical formulation of accelerated molecular dynamics for\n  simulating and predicting microstructure evolution in materials",
        "Dense $2$-connected planar graphs and the planar Tur\\'{a}n number of\n  $2C_k$",
        "Reducing T Gates with Unitary Synthesis",
        "Discrete Lyapunov functional for cyclic systems of differential\n  equations with time-variable or state-dependent delay",
        "Improving Neutral Point of View Text Generation through\n  Parameter-Efficient Reinforcement Learning and a Small-Scale High-Quality\n  Dataset",
        "A Design of Denser-Graph-Frequency Graph Fourier Frames for Graph Signal\n  Analysis",
        "Practical Introduction to FEM with GMSH: A MATLAB\/Octave Perspective",
        "From Tools to Teammates: Evaluating LLMs in Multi-Session Coding\n  Interactions",
        "Compactness of commutators of rough singular integrals",
        "Visual WetlandBirds Dataset: Bird Species Identification and Behavior\n  Recognition in Videos",
        "Acoustic waves interacting with non--locally reacting surfaces in a\n  Lagrangian framework",
        "Overview of the Amphion Toolkit (v0.2)",
        "Flora: Efficient Cloud Resource Selection for Big Data Processing via\n  Job Classification",
        "High-intensity wave vortices around subwavelength holes: from ocean\n  tides to nanooptics",
        "Impact of phonon lifetimes on the single-photon indistinguishability in\n  quantum emitters based on 2D materials"
      ],
      "abstract":[
        "Dirichlet's Lemma states that every primitive quadratic Dirichlet character\n$\\chi$ can be written in the form $\\chi(n) = (\\frac{\\Delta}n)$ for a suitable\nquadratic discriminant $\\Delta$. In this article we define a group, the\nseparant class group, that measures the extent to which Dirichlet's Lemma fails\nin general number fields $F$. As an application we will show that over fields\nwith trivial separant class groups, genus theory of quadratic extensions can be\nmade as explicit as over the rationals.",
        "Natural gas (NG) is relatively a clean source of energy, particularly\ncompared to fossil fuels, and worldwide consumption of NG has been increasing\nalmost linearly in the last two decades. A similar trend can also be seen in\nTurkey, while another similarity is the high dependence on imports for the\ncontinuous NG supply. It is crucial to accurately forecast future NG demand\n(NGD) in Turkey, especially, for import contracts; in this respect, forecasts\nof monthly NGD for the following year are of utmost importance. In the current\nstudy, the historical monthly NG consumption data between 2014 and 2024\nprovided by SOCAR, the local residential NG distribution company for two cities\nin Turkey, Bursa and Kayseri, was used to determine out-of-sample monthly NGD\nforecasts for a period of one year and nine months using various time series\nmodels, including SARIMA and ETS models, and a novel proposed machine learning\nmethod. The proposed method, named Just-in-Time-Learning-Gaussian Process\nRegression (JITL-GPR), uses a novel feature representation for the past NG\ndemand values; instead of using past demand values as column-wise separate\nfeatures, they are placed on a two-dimensional (2-D) grid of year-month values.\nFor each test point, a kernel function, tailored for the NGD predictions, is\nused in GPR to predict the query point. Since a model is constructed separately\nfor each test point, the proposed method is, indeed, an example of JITL. The\nJITL-GPR method is easy to use and optimize, and offers a reduction in forecast\nerrors compared to traditional time series methods and a state-of-the-art\ncombination model; therefore, it is a promising tool for NGD forecasting in\nsimilar settings.",
        "The non-Hermitian Aharonov-Bohm (AB) cage is a unique localization phenomenon\nthat confines all possible excitations. This confinement leads to fully flat\nspectra in momentum space, which are typically accompanied with the degeneracy\nwith various types. Classifying the degeneracy type is crucial for studying the\ndynamical properties of the non-Hermitian AB cage, but the methods for such\nclassification and their physical connections remain not very clear. Here, we\nconstruct a non-Hermitian AB cage in a bosonic Bogoliubov-de Gennes (BdG)\nsystem with various types of degenerate flat bands (DFBs). Using the transfer\nmatrix, we demonstrate the localization mechanism for the formation of AB cage\nand derive the minimal polynomial in mathematics for classifying the degeneracy\ntypes of DFBs, thus providing comprehensive understanding of the correspondence\namong the degeneracy type of DFBs, the minimal polynomial, and the transfer\nmatrix. With such correspondence, we propose a scheme to realize highly\ndegenerate flat bands.",
        "The purpose of this paper is to provide conditions for the existence or non\nexistence of non trivial Einstein multiply warped products, specially of\ngeneralised Kasner type; as well as to show estimates of the Einstein parameter\nthat condition the existence of such metrics.",
        "The interplay of topology and disorder in quantum dynamics has recently\nattracted significant attention across diverse platforms, including solid-state\ndevices, ultracold atoms, and photonic systems. Here, we report on a\ntopological Anderson transition caused by quasiperiodic intra-cell coupling\ndisorder in photonic Su-Schrieffer-Heeger lattices. As the quasiperiodic\nstrength is varied, the system exhibits a reentrant transition from a trivial\nphase to a topological phase and back to a trivial phase, accompanied by the\nclosing and reopening of the band gap around zero energy. Unlike the\ntraditional detection of photonic topological edge modes, we measure the mean\nchiral displacement from the transport of light in the bulk of the lattices. In\nour photonic lattices with a fixed length, the propagation dynamics is\nretrieved by varying the wavelength of light, which tunes the inter-waveguide\ncouplings.",
        "The widespread adoption of Retrieval-Augmented Generation (RAG) systems in\nreal-world applications has heightened concerns about the confidentiality and\nintegrity of their proprietary knowledge bases. These knowledge bases, which\nplay a critical role in enhancing the generative capabilities of Large Language\nModels (LLMs), are increasingly vulnerable to breaches that could compromise\nsensitive information. To address these challenges, this paper proposes an\nadvanced encryption methodology designed to protect RAG systems from\nunauthorized access and data leakage. Our approach encrypts both textual\ncontent and its corresponding embeddings prior to storage, ensuring that all\ndata remains securely encrypted. This mechanism restricts access to authorized\nentities with the appropriate decryption keys, thereby significantly reducing\nthe risk of unintended data exposure. Furthermore, we demonstrate that our\nencryption strategy preserves the performance and functionality of RAG\npipelines, ensuring compatibility across diverse domains and applications. To\nvalidate the robustness of our method, we provide comprehensive security proofs\nthat highlight its resilience against potential threats and vulnerabilities.\nThese proofs also reveal limitations in existing approaches, which often lack\nrobustness, adaptability, or reliance on open-source models. Our findings\nsuggest that integrating advanced encryption techniques into the design and\ndeployment of RAG systems can effectively enhance privacy safeguards. This\nresearch contributes to the ongoing discourse on improving security measures\nfor AI-driven services and advocates for stricter data protection standards\nwithin RAG architectures.",
        "As for plasmonic antenna structures that generate localized near-field\nenhancement, the most effective current implementations are based on electric\ndipole resonance modes, but this approach also imposes limitations on their\nfurther optimization. Here we introduce an ASRR structure whose ASR mode\nenables differential charge distribution across both sides of the split.\nThrough asymmetric regulation, charges at one end can become highly localized,\nthereby achieving efficient near-field enhancement. The formation of this\nstructure was initially driven by a hybrid computational framework integrating\nevolutionary optimization with residual neural networks, and subsequently\nsimplified into an ASRR prototype using the Occam's Razor principle. The ASRR\ndimer structure can achieve an electric field intensity enhancement over 6.5\ntimes larger than a traditional nanorod dimer, while maintaining a compact size\n(<1\/3 the working wavelength). The ASRR configuration also demonstrates\nsuperior Purcell factor and fluorescence enhancement. These results can find\napplications in surface-enhanced spectroscopy, nonlinear optics, and quantum\nlight-matter interactions.",
        "Research in adversarial machine learning (AML) has shown that statistical\nmodels are vulnerable to maliciously altered data. However, despite advances in\nBayesian machine learning models, most AML research remains concentrated on\nclassical techniques. Therefore, we focus on extending the white-box model\npoisoning paradigm to attack generic Bayesian inference, highlighting its\nvulnerability in adversarial contexts. A suite of attacks are developed that\nallow an attacker to steer the Bayesian posterior toward a target distribution\nthrough the strategic deletion and replication of true observations, even when\nonly sampling access to the posterior is available. Analytic properties of\nthese algorithms are proven and their performance is empirically examined in\nboth synthetic and real-world scenarios. With relatively little effort, the\nattacker is able to substantively alter the Bayesian's beliefs and, by\naccepting more risk, they can mold these beliefs to their will. By carefully\nconstructing the adversarial posterior, surgical poisoning is achieved such\nthat only targeted inferences are corrupted and others are minimally disturbed.",
        "This paper addresses the challenge of parking space detection in urban areas,\nfocusing on the city of Granada. Utilizing aerial imagery, we develop and apply\nsemantic segmentation techniques to accurately identify parked cars, moving\ncars and roads. A significant aspect of our research is the creation of a\nproprietary dataset specific to Granada, which is instrumental in training our\nneural network model. We employ Fully Convolutional Networks, Pyramid Networks\nand Dilated Convolutions, demonstrating their effectiveness in urban semantic\nsegmentation. Our approach involves comparative analysis and optimization of\nvarious models, including Dynamic U-Net, PSPNet and DeepLabV3+, tailored for\nthe segmentation of aerial images. The study includes a thorough\nexperimentation phase, using datasets such as UDD5 and UAVid, alongside our\ncustom Granada dataset. We evaluate our models using metrics like Foreground\nAccuracy, Dice Coefficient and Jaccard Index. Our results indicate that\nDeepLabV3+ offers the most promising performance. We conclude with future\ndirections, emphasizing the need for a dedicated neural network for parked car\ndetection and the potential for application in other urban environments. This\nwork contributes to the fields of urban planning and traffic management,\nproviding insights into efficient utilization of parking spaces through\nadvanced image processing techniques.",
        "In recent years, a lot of technological advances in computer science have\naided software programmers to create innovative and real-time user-friendly\nsoftware. With the creation of the software and the urging interest of people\nto learn to write software, there is a large collection of source codes that\ncan be found on the web, also known as Big Code, which can be used as a source\nof data for driving the machine learning applications tending to solve certain\nsoftware engineering problems. In this paper, we present COFO, a dataset\nconsisting of 809 classes\/problems with a total of 369K source codes written in\nC, C++, Java, and Python programming languages, along with other metadata such\nas code tags, problem specification, and input-output specifications. COFO has\nbeen scraped from the openly available Codeforces website using a\nselenium-beautifulsoup-python based scraper. We envision that this dataset can\nbe useful for solving machine learning-based problems like program\nclassification\/recognition, tagging, predicting program properties, and code\ncomprehension.",
        "The analysis of randomized controlled trials is often complicated by\nintercurrent events--events that occur after treatment initiation and may\nimpact outcome assessment. These events may lead to patients discontinuing\ntheir assigned treatment or dropping out of the trial entirely. In an analysis\nof data from two recent immunology trials, we categorize intercurrent events\ninto two broad types: those unrelated to treatment (e.g., withdrawal from the\nstudy due to external factors like pandemics or relocation) and those related\nto treatment (e.g., adverse events or lack of efficacy). We adopt distinct\nstrategies to handle each type, aiming to target a clinically more relevant\nestimand. For treatment-related intercurrent events, they often meaningfully\ndescribe the patient's outcome, we employ a composite variable strategy, where\nwe attribute an outcome value that reflects the lack of treatment success. For\ntreatment-unrelated intercurrent events, we adopt a hypothetical strategy that\nassumes these event times are conditionally independent of the outcome, given\ntreatment and covariates, and envisions a scenario in which the intercurrent\nevents do not occur. We establish the nonparametric identification and\nsemiparametric estimation theory for the causal estimand and introduce doubly\nrobust estimators. We illustrate our methods through the re-analysis of two\nrandomized trials on baricitinib for Systemic Lupus Erythematosus. We classify\nintercurrent events, apply four estimators, and compare our approach with\ncommon ad-hoc methods, highlighting the robustness and practical implications\nof our framework.",
        "We determine masses and mixing parameters of the $\\eta$ and $M_{\\eta^\\prime}$\nmeson in lattice QCD. The calculations are carried out on a set of 13 ETMC\ngauge ensembles with $N_f=2+1+1$ (maximally) twisted-mass Clover-improved\nquarks. These ensemble cover four values of the lattice spacing\n$a=0.057\\mathrm{fm},...,0.092\\mathrm{fm}$ and pion masses from\n$140\\mathrm{MeV}$ to $360\\mathrm{MeV}$, including three ensembles at physical\nquark masses and six ensembles with $M_\\pi<200\\mathrm{MeV}$. The strange-quark\ncontribution is treated in a mixed-action approach using Osterwalder-Seiler\nfermions to avoid complications due to flavor mixing in the heavy quark sector\nand to enable the use of the one-end trick in the computation of strange\nquark-disconnected diagrams. With the strange-quark mass tuned to its physical\nvalue and several ensembles having close-to-physical light-quark mass,\nuncertainties related to the chiral extrapolations are reduced significantly\ncompared to earlier studies. Physical results are computed with fully\ncontrolled systematics from a combined chiral, continuum and infinite-volume\nextrapolation, and a full error budget is obtained from model averages over of\nvarious fit ans\\\"atze and data cuts. Our results for the masses are given by\n$M_\\eta=551(16)\\mathrm{MeV}$ and $M_{\\eta^\\prime}=972(20)\\mathrm{MeV}$,\nrespectively, where statistical and systematic errors have been added in\nquadrature. For the mixing angle and decay-constant parameters the\nFeldmann-Kroll-Stech scheme is employed to compute them from pseudoscalar\nmatrix elements in the quark-flavor basis. For the mixing angle we obtain\n$\\phi^\\mathrm{phys}=39.3(2.0)^\\circ$ and our results for the decay-constant\nparameters are given by $f_l^\\mathrm{phys}=138.6(4.4)\\mathrm{MeV}$ and\n$f_s^\\mathrm{phys}=170.7(3.3)\\mathrm{MeV}$.",
        "Autonomous Vehicle (AV) systems have been developed with a strong reliance on\nmachine learning techniques. While machine learning approaches, such as deep\nlearning, are extremely effective at tasks that involve observation and\nclassification, they struggle when it comes to performing higher level\nreasoning about situations on the road. This research involves incorporating\ncommonsense reasoning models that use image data to improve AV systems. This\nwill allow AV systems to perform more accurate reasoning while also making them\nmore adjustable, explainable, and ethical. This paper will discuss the findings\nso far and motivate its direction going forward.",
        "This research explores the influence of interfacial debonding between a\nbroken fiber and matrix on stress redistribution surrounding a fiber break\nwithin a unidirectional (UD) impregnated fiber bundle, accounting for\nmisalignment of fibers and fiber diameter distribution in randomly packed fiber\nconfigurations. Finite-element modelling is conducted on carbon-reinforced\nepoxy UD bundles with one fiber broken for different combinations of the bundle\nparameters: aligned\/misaligned fibers and constant\/randomly distributed fiber\ndiameters. Two definitions of stress concentration factor (SCF) are examined,\nbased on average and maximum stress over the fiber cross-section. The study\nreveals a statistically significant difference for average SCF for misaligned\nbundles with both constant and variable diameters of fibers compared to the\ncase of aligned bundles of fibers with constant diameter. When the calculated\nSCFs are incorporated in a bundle strength model, the failure strain of\nunidirectional composites can be more realistically predicted.",
        "Despite its widespread use in materials science, conventional molecular\ndynamics simulations are severely constrained by timescale limitations. To\naddress this shortcoming, we propose an empirical formulation of accelerated\nmolecular dynamics method, adapted from a collective-variable-based extended\nsystem dynamics framework. While this framework is originally developed for\nefficient free energy sampling and reaction pathway determination of specific\nrare events in condensed matter, we have modified it to enable accelerated\nmolecular dynamics simulation and prediction of microstructure evolution of\nmaterials across a broad range of scenarios. In essence, the nearest neighbor\noff-centering absolute displacement (NNOAD), which quantifies the deviation of\nan atom from the geometric center of its nearest neighbors in materials, is\nintroduced. We propose that the collection of NNOADs of all atoms can serve as\na generalized reaction coordinate for various structural transitions in\nmaterials. The NNOAD of each atom, represented by its three components, is\ncoupled with three additional dynamic variables assigned to the atom. Time\nevolution of the additional dynamic variables follows Langevin equation, while\nNos\\'e-Hoover dynamics is employed to thermostat the system. Through careful\nanalysis and benchmark simulations, we established appropriate parameter ranges\nfor the equations in our method. Application of this method to several test\ncases demonstrates its effectiveness and consistency in accelerating molecular\ndynamics simulations and predicting various microstructure evolutions of\nmaterials over much longer timescale. We also provide a preliminary theoretical\nanalysis and qualitative justification of the method, offering insights into\nits underlying principles.",
        "Shi, Walsh and Yu demonstrated that any dense circuit graph contains a large\nnear-triangulation. We extend the result to $2$-connected plane graphs, thereby\naddressing a question posed by them. Using the result, we prove that the planar\nTu\\'{a}n number of $2C_k$ is $\\left[3-\\Theta(k^{\\log_23})^{-1}\\right]n$ when\n$k\\geq 5$.",
        "Quantum error correction is essential for achieving practical quantum\ncomputing but has a significant computational overhead. Among fault-tolerant\n(FT) gate operations, non-Clifford gates, such as $T$, are particularly\nexpensive due to their reliance on magic state distillation. These costly $T$\ngates appear frequently in FT circuits as many quantum algorithms require\narbitrary single-qubit rotations, such as $R_x$ and $R_z$ gates, which must be\ndecomposed into a sequence of $T$ and Clifford gates. In many quantum circuits,\n$R_x$ and $R_z$ gates can be fused to form a single $U3$ unitary. However,\nexisting synthesis methods, such as gridsynth, rely on indirect decompositions,\nrequiring separate $R_z$ decompositions that result in a threefold increase in\n$T$ count.\n  This work presents a novel FT synthesis algorithm that directly synthesizes\narbitrary single-qubit unitaries, avoiding the overhead of separate $R_z$\ndecompositions. By leveraging tensor network-based search, our approach enables\nnative $U3$ synthesis, reducing the $T$ count, Clifford gate count, and\napproximation error. Compared to gridsynth-based circuit synthesis, for 187\nrepresentative benchmarks, our design reduces the $T$ count by up to\n$3.5\\times$, and Clifford gates by $7\\times$, resulting in up to $4\\times$\nimprovement in overall circuit infidelity.",
        "We consider nonautonomous cyclic systems of delay differential equations with\nvariable delay. Under suitable feedback assumptions, we define an (integer\nvalued) Lyapunov functional related to the number of sign changes of the\ncoordinate functions of solutions. We prove that this functional possesses\nproperties analogous to those established by Mallet-Paret and Sell for the\nconstant delay case and by Krisztin and Arino for the scalar case. We also\napply the results to equations with state-dependent delays.",
        "This paper describes the construction of a dataset and the evaluation of\ntraining methods to improve generative large language models' (LLMs) ability to\nanswer queries on sensitive topics with a Neutral Point of View (NPOV), i.e.,\nto provide significantly more informative, diverse and impartial answers. The\ndataset, the SHQ-NPOV dataset, comprises 300 high-quality, human-written\nquadruplets: a query on a sensitive topic, an answer, an NPOV rating, and a set\nof links to source texts elaborating the various points of view. The first key\ncontribution of this paper is a new methodology to create such datasets through\niterative rounds of human peer-critique and annotator training, which we\nrelease alongside the dataset. The second key contribution is the\nidentification of a highly effective training regime for parameter-efficient\nreinforcement learning (PE-RL) to improve NPOV generation. We compare and\nextensively evaluate PE-RL and multiple baselines-including LoRA finetuning (a\nstrong baseline), SFT and RLHF.\n  PE-RL not only improves on overall NPOV quality compared to the strongest\nbaseline ($97.06\\%\\rightarrow 99.08\\%$), but also scores much higher on\nfeatures linguists identify as key to separating good answers from the best\nanswers ($60.25\\%\\rightarrow 85.21\\%$ for presence of supportive details,\n$68.74\\%\\rightarrow 91.43\\%$ for absence of oversimplification). A qualitative\nanalysis corroborates this. Finally, our evaluation finds no statistical\ndifferences between results on topics that appear in the training dataset and\nthose on separated evaluation topics, which provides strong evidence that our\napproach to training PE-RL exhibits very effective out of topic generalization.",
        "This paper introduces a design method for densergraph-frequency graph Fourier\nframes (DGFFs) to enhance graph signal processing and analysis. The graph\nFourier transform (GFT) enables us to analyze graph signals in the graph\nspectral domain and facilitates various graph signal processing tasks, such as\nfiltering, sampling and reconstruction, denoising, and so on. However, the\nconventional GFT faces two significant limitations. First, unlike the discrete\nFourier transform and its variants (such as discrete cosine transforms), the\ngraph frequencies of the derived graph Fourier basis (GFB) from a given graph\ntend to be unevenly distributed or localized, which leads to biased spectral\nanalysis. Second, the GFB used in GFT does not provide an efficient sparse\nrepresentation of graph signals compared to overcomplete systems like frames.\nTo overcome these challenges, we propose adding oscillating vectors with\nintermediate graph frequencies between the original vectors in the GFB for both\nundirected and directed graphs, constructing GFFs with densergraph frequencies.\nThe resulting DGFFs are expected to enable more accurate graph signal analysis.\nFurthermore, we propose a graph filtering method based on the DGFFs. In\nexperiments, we apply the DGFFs to practical applications such as graph signal\nrecovery, demonstrating superior performance compared to existing GFBs.",
        "The Finite Element Method (FEM) is a powerful computational tool for solving\npartial differential equations (PDEs). Although commercial and open-source FEM\nsoftware packages are widely available, an independent implementation of FEM\nprovides significant educational value, provides a deeper understanding of the\nmethod, and enables the development of custom solutions tailored to specialized\napplications or integration with other solvers. This work introduces a 3D\n$\\mathbb{P}_m$-element FEM implementation in MATLAB\/Octave that is designed to\nbalance educational clarity with computational efficiency. A key feature is its\nintegration with GMSH, an open-source 3D mesh generator with CAD capabilities\nthat streamlines mesh generation for complex geometries. By leveraging GMSH\ndata structures, we provide a seamless connection between geometric modeling\nand numerical simulation. The implementation focuses on solving the general\nconvection-diffusion-advection equation and serves as a flexible foundation for\naddressing advanced problems, including elasticity, mixed formulations, and\nintegration with other numerical methods.",
        "Large Language Models (LLMs) are increasingly used in working environments\nfor a wide range of tasks, excelling at solving individual problems in\nisolation. However, are they also able to effectively collaborate over\nlong-term interactions? To investigate this, we introduce MemoryCode, a\nsynthetic multi-session dataset designed to test LLMs' ability to track and\nexecute simple coding instructions amid irrelevant information, simulating a\nrealistic setting. While all the models we tested handle isolated instructions\nwell, even the performance of state-of-the-art models like GPT-4o deteriorates\nwhen instructions are spread across sessions. Our analysis suggests this is due\nto their failure to retrieve and integrate information over long instruction\nchains. Our results highlight a fundamental limitation of current LLMs,\nrestricting their ability to collaborate effectively in long interactions.",
        "We study the two-weighted off-diagonal compactness of commutators of rough\nsingular integral operators $T_\\Omega$ that are associated with a kernel\n$\\Omega\\in L^q(\\mathbb{S}^{d-1})$. We establish a characterisation of\ncompactness of the commutator $[b,T_\\Omega]$ in terms of the function $b$\nbelonging to a suitable space of functions with vanishing mean oscillation. Our\nresults expand upon the previous compactness characterisations for\nCalder\\'on-Zygmund operators. Additionally, we prove a matrix-weighted\ncompactness result for $[b,T_\\Omega]$ by applying the so-called matrix-weighted\nKolmogorov-Riesz theorem.",
        "The current biodiversity loss crisis makes animal monitoring a relevant field\nof study. In light of this, data collected through monitoring can provide\nessential insights, and information for decision-making aimed at preserving\nglobal biodiversity. Despite the importance of such data, there is a notable\nscarcity of datasets featuring videos of birds, and none of the existing\ndatasets offer detailed annotations of bird behaviors in video format. In\nresponse to this gap, our study introduces the first fine-grained video dataset\nspecifically designed for bird behavior detection and species classification.\nThis dataset addresses the need for comprehensive bird video datasets and\nprovides detailed data on bird actions, facilitating the development of deep\nlearning models to recognize these, similar to the advancements made in human\naction recognition. The proposed dataset comprises 178 videos recorded in\nSpanish wetlands, capturing 13 different bird species performing 7 distinct\nbehavior classes. In addition, we also present baseline results using state of\nthe art models on two tasks: bird behavior recognition and species\nclassification.",
        "The paper deals with a family of evolution problems arising in the physical\nmodeling of small amplitude acoustic phenomena occurring in a fluid, bounded by\na surface of extended reaction. They are all derived in a Lagrangian framework.\n  We study well-posedness of these problems, their mutual relations, and their\nrelations with other evolution problems modeling the same physical phenomena.\nThey are those introduced in an Eulerian framework and those which deal with\nthe (standard in Theoretical Acoustics) velocity potential. The latter reduce\nto the well--known wave equation with acoustic boundary conditions.\n  Finally, we prove that all problems are asymptotically stable provided the\nsystem is linearly damped.",
        "Amphion is an open-source toolkit for Audio, Music, and Speech Generation,\ndesigned to lower the entry barrier for junior researchers and engineers in\nthese fields. It provides a versatile framework that supports a variety of\ngeneration tasks and models. In this report, we introduce Amphion v0.2, the\nsecond major release developed in 2024. This release features a 100K-hour\nopen-source multilingual dataset, a robust data preparation pipeline, and novel\nmodels for tasks such as text-to-speech, audio coding, and voice conversion.\nFurthermore, the report includes multiple tutorials that guide users through\nthe functionalities and usage of the newly released models.",
        "Distributed dataflow systems like Spark and Flink enable data-parallel\nprocessing of large datasets on clusters of cloud resources. Yet, selecting\nappropriate computational resources for dataflow jobs is often challenging. For\nefficient execution, individual resource allocations, such as memory and CPU\ncores, must meet the specific resource demands of the job. Meanwhile, the\nchoices of cloud configurations are often plentiful, especially in public\nclouds, and the current cost of the available resource options can fluctuate.\n  Addressing this challenge, we present Flora, a low-overhead approach to\ncost-optimizing cloud cluster configurations for big data processing. Flora\nlets users categorize jobs according to their data access patterns and derives\nsuitable cluster resource configurations from executions of test jobs of the\nsame category, considering current resource costs. In our evaluation on a new\ndataset comprising 180 Spark job executions on Google Cloud, Flora's cluster\nresource selections exhibit an average deviation below 6% from the most\ncost-optimal solution, with a maximum deviation below 24%.",
        "Vortices are ubiquitous in nature; they appear in a variety of phenomena\nranging from galaxy formation in astrophysics to topological defects in quantum\nfluids. In particular, wave vortices have attracted enormous attention and\nfound applications in optics, acoustics, electron microscopy, etc. Such\nvortices carry quantized phase singularities accompanied by zero intensity in\nthe center, and quantum-like orbital angular momentum, with the minimum\nlocalization scale of the wavelength. Here we describe a conceptually novel\ntype of wave vortices, which can appear around arbitrarily small `holes' (i.e.,\nexcluded areas or defects) in a homogeneous 2D plane. Such vortices are\ncharacterized by high intensity and confinement at the edges of the hole and\nhence subwavelength localization of the angular momentum. We demonstrate the\nappearance of such vortices in: (i) optical near fields around metallic\nnanodiscs on a dielectric substrate, (ii) phonon-polariton fields around\nnanoholes in a polaritonic slab, and (iii) ocean tidal waves around islands of\nNew Zealand and Madagascar. We also propose a simple toy model of the\ngeneration of such subwavelength vortices via the interference of a\npoint-dipole source and a plane wave, where the vortex sign is controlled by\nthe mutual phase between these waves. Our findings open avenues for\nsubwavelength vortex\/angular-momentum-based applications in various wave\nfields.",
        "Localized excitons in two-dimensional (2D) materials are considered as\npromising sources of single photons on demand. The photon indistinguishability\nas key figure of merit for quantum information processing is strongly\ninfluenced by the coupling of charge excitations to lattice vibrations of the\nsurrounding semiconductor material. Here, we quantify the impact of\nexciton-acoustic-phonon-interaction and cavity QED effects on photon\nindistinguishability in a Hong-Ou-Mandel setup by solving fully quantum\nmechanical equations for the coupled QD-cavity-phonon system including\nnon-Markovian effects. We find a strong reduction of indistinguishability\ncompared to 3D systems due to increased exciton-phonon coupling efficiency.\nMoreover, we show that the coherence properties of photons are significantly\ninfluenced by the finite phonon lifetime in the surrounding material giving\nrise to pure dephasing. Only if these limitations are overcome, localized\nexcitons in 2D semiconductors can become a new avenue for quantum light\nsources."
      ]
    }
  },
  {
    "id":2411.07871,
    "research_type":"applied",
    "start_id":"b5",
    "start_title":"Computer-aided diagnosis of Alzheimer\u2019s disease and neurocognitive disorders with multimodal Bi-Vision Transformer (BiViT)",
    "start_abstract":"<jats:title>Abstract<\/jats:title><jats:p>Cognitive disorders affect various cognitive functions that can have a substantial impact on individual\u2019s daily life. Alzheimer\u2019s disease (AD) is one of such well-known cognitive disorders. Early detection and treatment of cognitive diseases using artificial intelligence can help contain them. However, the complex spatial relationships and long-range dependencies found in medical imaging data present challenges in achieving the objective. Moreover, for a few years, the application of transformers in imaging has emerged as a promising area of research. A reason can be transformer\u2019s impressive capabilities of tackling spatial relationships and long-range dependency challenges in two ways, i.e., (1) using their self-attention mechanism to generate comprehensive features, and (2) capture complex patterns by incorporating global context and long-range dependencies. In this work, a Bi-Vision Transformer (BiViT) architecture is proposed for classifying different stages of AD, and multiple types of cognitive disorders from 2-dimensional MRI imaging data. More specifically, the transformer is composed of two novel modules, namely Mutual Latent Fusion (MLF) and Parallel Coupled Encoding Strategy (PCES), for effective feature learning. Two different datasets have been used to evaluate the performance of proposed BiViT-based architecture. The first dataset contain several classes such as mild or moderate demented stages of the AD. The other dataset is composed of samples from patients with AD and different cognitive disorders such as mild, early, or moderate impairments. For comprehensive comparison, a multiple transfer learning algorithm and a deep autoencoder have been each trained on both datasets. The results show that the proposed BiViT-based model achieves an accuracy of 96.38% on the AD dataset. However, when applied to cognitive disease data, the accuracy slightly decreases below 96% which can be resulted due to smaller amount of data and imbalance in data distribution. Nevertheless, given the results, it can be hypothesized that the proposed algorithm can perform better if the imbalanced distribution and limited availability problems in data can be addressed.<\/jats:p>\n                <jats:p><jats:bold>Graphical abstract<\/jats:bold><\/jats:p>",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "2016 Alzheimer's disease facts and figures"
      ],
      "abstract":[
        "This report describes the public health impact of Alzheimer's disease, including incidence and prevalence, mortality rates, costs care, overall on caregivers society. It also examines in detail financial families, annual to families difficult decisions must often make pay those costs. An estimated 5.4 million Americans have disease. By mid-century, number people living with disease United States is projected grow 13.8 million, fueled large part by aging baby boom generation. Today, someone country develops every 66 seconds. 2050, one new case expected develop 33 seconds, resulting nearly 1 cases per year. In 2013, official death certificates recorded 84,767 deaths from making it sixth leading cause fifth age \u2265 65 years. Between 2000 stroke, heart prostate cancer decreased 23%, 14%, 11%, respectively, whereas increased 71%. The actual which contributes likely much larger than certificates. 2016, an 700,000 years will die many them because complications caused 2015, more 15 family members other unpaid provided 18.1 billion hours care dementias, a contribution valued at $221 billion. Average per-person Medicare payments for services beneficiaries dementias are two half times as great all without these conditions, Medicaid 19 great. Total 2016 long-term hospice dementia be $236 may place substantial burden who take money out their retirement savings, cut back buying food, reduce own trips doctor. addition, incorrectly believe that pays nursing home types care. Such findings highlight need solutions prevent dementia-related jeopardizing security dementias."
      ],
      "categories":[
        "q-bio.NC"
      ]
    },
    "list":{
      "title":[
        "Axion Stabilization in Modular Cosmology",
        "Reconstruction of space-dependence and nonlinearity of a reaction term\n  in a subdiffusion equation",
        "Sharp Anti-Concentration Inequalities for Extremum Statistics via\n  Copulas",
        "On the spectral gap of negatively curved covers",
        "Fluid Reconfigurable Intelligent Surfaces: Joint On-Off Selection and\n  Beamforming with Discrete Phase Shifts",
        "Forecasting Local Ionospheric Parameters Using Transformers",
        "Fourier dimension of the graph of fractional Brownian motion with $H \\ge\n  1\/2$",
        "A generalization of Zwegers' multivariable $\\mu$-function",
        "Enumeration of lattices of nullity $k$ and containing $r$ comparable\n  reducible elements",
        "Coherent manifolds",
        "Study of event and particle selection effects on elliptic flow\n  background at the isobar experiments based on AMPT model",
        "Progress of the TianQin project",
        "Dimension-free Score Matching and Time Bootstrapping for Diffusion\n  Models",
        "\"Short-length\" Adversarial Training Helps LLMs Defend \"Long-length\"\n  Jailbreak Attacks: Theoretical and Empirical Evidence",
        "Modulating Optical Properties through Cation Substitution:\n  Composition-Property Relationships in $M^I_3$$M^{III}$P$_3$O$_9$N:Eu$^{2+}$\n  ($M^I$=Na, K; $M^{III}$=Al, Ga, In)",
        "No Galaxy-Scale [CII] Fast Outflow in the z=6.72 Red Quasar HSC\n  J1205$-$0000",
        "A fully conservative discrete velocity Boltzmann solver with parallel\n  adaptive mesh refinement for compressible flows",
        "Twist-enabled Transmissive Metasurface with Co-polarized Geometric Phase",
        "A Klain-Schneider Theorem for Vector-Valued Valuations on Convex\n  Functions",
        "Real-time simulation of jet energy loss and entropy production in\n  high-energy scattering with matter",
        "Optimizing High-Dimensional Oblique Splits",
        "A modified dynamic diffusion finite element method with optimal\n  convergence rate for convection-diffusion-reaction equations",
        "An Unexplained Origin for the Unusual Globular Cluster System in the\n  Ultra-diffuse Galaxy FCC 224",
        "Distributed Observer for Descriptor Linear System: The Luenberger\n  Observer Method",
        "Sharing quantum nonlocality and teleportation over long distance using\n  optical hybrid states",
        "Stabilization of magnetic bubbles in [Ni\/Co]$_{n}$ multilayers on an\n  oxygen-reconstructed Nb(110) surface via an ultra-thin Cu interlayer",
        "Energetics and dynamics of membrane necks in particle wrapping",
        "Neutral but Impactful: Gallium Cluster-Induced Nanopores from\n  Beam-Blanked Gallium Ion Sources",
        "Investigation of Polymer Association Behaviors in Solvents Using a\n  Coarse-Grained Model"
      ],
      "abstract":[
        "The $SL(2,\\mathbb{Z})$ invariant $\\alpha$-attractor models have plateau\npotentials with respect to the inflaton and axion fields. The potential in the\naxion direction is almost exactly flat during inflation, hence, the axion field\nremains nearly massless. In this paper, we develop a generalized class of such\nmodels, where the $SL(2,\\mathbb{Z})$ symmetry is preserved, but the axion\nacquires a large mass and becomes strongly stabilized during inflation, which\neliminates isocurvature perturbations in this scenario. Inflation in such\ntwo-field models occurs as in the single-field $\\alpha$-attractors and leads to\nthe same cosmological predictions.",
        "In this paper we study the simultaneous reconstruction of two coefficients in\na reaction-subdiffusion equation, namely a nonlinearity and a space dependent\nfactor. The fact that these are coupled in a multiplicative matter makes the\nreconstruction particularly challenging. Several situations of overposed data\nare considered: boundary observations over a time interval, interior\nobservations at final time, as well as a combination thereof. We devise fixed\npoint schemes and also describe application of a frozen Newton method. In the\nfinal time data case we prove convergence of the fixed point scheme as well as\nuniqueness of both coefficients. Numerical experiments illustrate performance\nof the reconstruction methods, in particular dependence on the differentiation\norder in the subdiffusion equation.",
        "We derive sharp upper and lower bounds for the pointwise concentration\nfunction of the maximum statistic of $d$ identically distributed real-valued\nrandom variables. Our first main result places no restrictions either on the\ncommon marginal law of the samples or on the copula describing their joint\ndistribution. We show that, in general, strictly sublinear dependence of the\nconcentration function on the dimension $d$ is not possible. We then introduce\na new class of copulas, namely those with a convex diagonal section, and\ndemonstrate that restricting to this class yields a sharper upper bound on the\nconcentration function. This allows us to establish several new\ndimension-independent and poly-logarithmic-in-$d$ anti-concentration\ninequalities for a variety of marginal distributions under mild dependence\nassumptions. Our theory improves upon the best known results in certain special\ncases. Applications to high-dimensional statistical inference are presented,\nincluding a specific example pertaining to Gaussian mixture approximations for\nfactor models, for which our main results lead to superior distributional\nguarantees.",
        "Given a negatively curved compact Riemannian surface $X$, we give an explicit\nestimate, valid with high probability as the degree goes to infinity, of the\nfirst non-trivial eigenvalue of the Laplacian on random Riemannian covers of\n$X$. The explicit gap is given in terms of the bottom of the spectrum of the\nuniversal cover of $X$ and the topological entropy of the geodesic flow on X.\nThis result generalizes in variable curvature a result of Magee-Naud-Puder for\nhyperbolic surfaces. We then formulate a conjecture on the optimal spectral gap\nand show that there exists covers with near optimal spectral gaps using a\nresult of Louder-Magee and techniques of strong convergence from random matrix\ntheory.",
        "This letter proposes a fluid reconfigurable intelligent surface (FRIS)\nparadigm, extending the conventional reconfigurable intelligent surface (RIS)\ntechnology to incorporate position reconfigurability of the elements. In our\nmodel, a `fluid' element is realized by a dense matrix of subelements over a\ngiven space and dynamically selecting specific elements for signal modulation\nbased on channel conditions. Specifically, we consider a FRIS-assisted\nsingle-user single-input single-output (SU-SISO) system and formulate an\noptimization problem that can jointly optimize element selection and their\ndiscrete phase shifts to maximize the achievable rate. To address this problem\nefficiently, we propose an iterative algorithm based on the cross-entropy\noptimization (CEO) framework. Simulation results reveal that FRIS achieves\nsignificant performance gains over traditional RIS.",
        "We present a novel method for forecasting key ionospheric parameters using\ntransformer-based neural networks. The model provides accurate forecasts and\nuncertainty quantification of the F2-layer peak plasma frequency (foF2), the\nF2-layer peak density height (hmF2), and total electron content (TEC) for a\ngiven geographic location. It supports a number of exogenous variables,\nincluding F10.7cm solar flux and disturbance storm time (Dst). We demonstrate\nhow transformers can be trained in a data assimilation-like fashion that use\nthese exogenous variables along with na\\\"ive predictions from climatology to\ngenerate 24-hour forecasts with non-parametric uncertainty bounds. We call this\nmethod the Local Ionospheric Forecast Transformer (LIFT). We demonstrate that\nthe trained model can generalize to new geographic locations and time periods\nnot seen during training, and we compare its performance to that of the\nInternational Reference Ionosphere (IRI).",
        "We prove that the Fourier dimension of the graph of fractional Brownian\nmotion with Hurst index greater than $1\/2$ is almost surely 1. This extends the\nresult of Fraser and Sahlsten (2018) for the Brownian motion and verifies\npartly the conjecture of Fraser, Orponen and Sahlsten (2014). We introduce a\ncombinatorial integration by parts formula to compute the moments of the\nFourier transform of the graph measure. The proof of our main result is based\non this integration by parts formula together with Fa\\`a di Bruno's formula and\nstrong local nondeterminism of fractional Brownian motion. We also show that\nthe Fourier dimension of the graph of a symmetric $\\alpha$-stable process with\n$\\alpha\\in[1,2]$ is almost surely 1.",
        "We introduce a one parameter deformation of Zwegers' multivariable\n$\\mu$-function by applying iterations of the $q$-Borel summation method, which\nis also a multivariate analogue of the generalized $\\mu$-function introduced by\nthe authors. For this deformed multivariable $\\mu$-function, we give some\nformulas, for example, forward shift formula, translation and\n$\\mathfrak{S}_{N+1}$-symmetry. Further we mention modular formulas for the\nZwegers' original multivariable $\\mu$-function.",
        "In 2002 Thakare et al.\\ counted non-isomorphic lattices on $n$ elements,\nhaving nullity up to two. In 2020 Bhavale and Waphare introduced the concept of\nRC-lattices as the class of all lattices in which all the reducible elements\nare comparable. In this paper, we enumerate all non-isomorphic RC-lattices on\n$n$ elements. For this purpose, firstly we enumerate all non-isomorphic\nRC-lattices on $n \\geq 4$ elements, having nullity $k \\geq 1$, and containing\n$2 \\leq r \\leq 2k$ reducible elements. Secondly we enumerate all non-isomorphic\nRC-lattices on $n \\geq 4$ elements, having nullity $k \\geq 1$. This work is in\nrespect of Birkhoff's open problem of enumerating all finite lattices on $n$\nelements.",
        "This paper defines coherent manifolds and discusses their properties and\ntheir application in quantum mechanics. Every coherent manifold with a large\ngroup of symmetries gives rise to a Hilbert space, the completed quantum space\nof $Z$, which contains a distinguished family of coherent states labeled by the\npoints of the manifold.\n  The second quantization map in quantum field theory is generalized to\nquantization operators on arbitrary coherent manifolds. It is shown how the\nSchr\\\"odinger equation on any such completed quantum space can be solved in\nterms of computations only involving the coherent product. In particular, this\napplies to a description of Bosonic Fock spaces as completed quantum spaces of\na class of coherent manifolds called Klauder spaces.",
        "Measurement of the Chiral Magnetic Effect (CME) has been a popular topic of\nhigh-energy nuclear physics in the last decade. The flow correlation $\\gamma$\nbetween charged hadron pairs of the same and opposite charges and their\ndifference $\\Delta \\gamma$ were measured to separate the CME-driven signal from\nthe collective flow background especially second-order elliptic $v_{2}$. The\nSTAR experiment have stepped further to the isobar experiment to compare\n$\\gamma$ and $\\Delta \\gamma$ between Ru+Ru and Zr+Zr\n~\\cite{PhysRevC.105.014901}, which were theoretically expected to produce the\nsame elliptic flow background but different CME signals. However, the measured\nflow backgrounds also differ between Ru+Ru and Zr+Zr, indicating more\nfine-tuning of RP and centrality definition necessary.\n  This analysis applied the AMPT model~\\cite{PhysRevC.72.064901} to simulate\nthe same collision system and energy as the STAR isobar experiment. Since the\nAMPT model does not include magnetic field effects, we expect comparing its\noutput between Ru+Ru and Zr+Zr collision systems can provide an insight of the\npossible bias of flow background definition, and help improve the measurement\nof CME signal in real experiments. Multiple combinations of centrality and flow\ndefinition were chosen to study how the $v_2$ and their difference would be\naffected, especially by varying the particles selection of charge versus\nneutral properties and broadening (pseudo-)rapidity regions, while STAR CME\nwork relied on charged-only particles at central rapidity.",
        "TianQin is a future space-based gravitational wave observatory targeting the\nfrequency window of $10^{-4}$ Hz $\\sim 1$ Hz. A large variety of gravitational\nwave sources are expected in this frequency band, including the merger of\nmassive black hole binaries, the inspiral of extreme\/intermediate mass ratio\nsystems, stellar-mass black hole binaries, Galactic compact binaries, and so\non. TianQin will consist of three Earth orbiting satellites on nearly identical\norbits with orbital radii of about $10^5$ km. The satellites will form a normal\ntriangle constellation whose plane is nearly perpendicular to the ecliptic\nplane. The TianQin project has been progressing smoothly following the ``0123\"\ntechnology roadmap. In step ``0\", the TianQin laser ranging station has been\nconstructed and it has successfully ranged to all the five retro-reflectors on\nthe Moon. In step ``1\", the drag-free control technology has been tested and\ndemonstrated using the TianQin-1 satellite. In step ``2\", the inter-satellite\nlaser interferometry technology will be tested using the pair of TianQin-2\nsatellites. The TianQin-2 mission has been officially approved and the\nsatellites will be launched around 2026. In step ``3\", i.e., the TianQin-3\nmission, three identical satellites will be launched around 2035 to form the\nspace-based gravitational wave detector, TianQin, and to start gravitational\nwave detection in space.",
        "Diffusion models generate samples by estimating the score function of the\ntarget distribution at various noise levels. The model is trained using samples\ndrawn from the target distribution, progressively adding noise. In this work,\nwe establish the first (nearly) dimension-free sample complexity bounds for\nlearning these score functions, achieving a double exponential improvement in\ndimension over prior results. A key aspect of our analysis is the use of a\nsingle function approximator to jointly estimate scores across noise levels, a\ncritical feature of diffusion models in practice which enables generalization\nacross timesteps. Our analysis introduces a novel martingale-based error\ndecomposition and sharp variance bounds, enabling efficient learning from\ndependent data generated by Markov processes, which may be of independent\ninterest. Building on these insights, we propose Bootstrapped Score Matching\n(BSM), a variance reduction technique that utilizes previously learned scores\nto improve accuracy at higher noise levels. These results provide crucial\ninsights into the efficiency and effectiveness of diffusion models for\ngenerative modeling.",
        "Jailbreak attacks against large language models (LLMs) aim to induce harmful\nbehaviors in LLMs through carefully crafted adversarial prompts. To mitigate\nattacks, one way is to perform adversarial training (AT)-based alignment, i.e.,\ntraining LLMs on some of the most adversarial prompts to help them learn how to\nbehave safely under attacks. During AT, the length of adversarial prompts plays\na critical role in the robustness of aligned LLMs. This paper focuses on\nadversarial suffix jailbreak attacks and unveils that to defend against a\njailbreak attack with an adversarial suffix of length $\\Theta(M)$, it is enough\nto align LLMs on prompts with adversarial suffixes of length\n$\\Theta(\\sqrt{M})$. Theoretically, we analyze the adversarial in-context\nlearning of linear transformers on linear regression tasks and prove a robust\ngeneralization bound for trained transformers. The bound depends on the term\n$\\Theta(\\sqrt{M_{\\text{test}}}\/M_{\\text{train}})$, where $M_{\\text{train}}$ and\n$M_{\\text{test}}$ are the number of adversarially perturbed in-context samples\nduring training and testing. Empirically, we conduct AT on popular open-source\nLLMs and evaluate their robustness against jailbreak attacks of different\nadversarial suffix lengths. Results confirm a positive correlation between the\nattack success rate and the ratio of the square root of the adversarial suffix\nduring jailbreaking to the length during AT. Our findings show that it is\npractical to defend \"long-length\" jailbreak attacks via efficient\n\"short-length\" AT. The code is available at https:\/\/github.com\/fshp971\/adv-icl.",
        "Developing phosphors with narrow photoluminescence emission peaks and high\nchromatic stability holds significant importance in light-emitting diode (LED)\ndisplay technologies, where a wide color gamut is essential to achieve the Rec.\n2020 specifications. This research focuses on the optical properties of a solid\nsolution: $M^I_{2.97}$Eu$_{0.015}$$M^{III}$P$_3$O$_9$N [$M^I$=Na, K;\n$M^{III}$=Al, (Al$_{0.75}$Ga$_{0.25}$), (Al$_{0.5}$Ga$_{0.5}$),\n(Al$_{0.25}$Ga$_{0.75}$), Ga, (Ga$_{0.75}$In$_{0.25}$), (Ga$_{0.5}$In$_{0.5}$)]\nto understand how the narrow-emitting photoluminescence in\nK$_3$AlP$_3$O$_9$N:Eu$^{2+}$ can evolve during host structure cation\nsubstitution. Photoluminescence measurements at low temperature (15 K) support\nthat Eu$^{2+}$ replaces three crystallographically independent Na$^+$ sites in\nNa$_{2.97}$Eu$_{0.015}$AlP$_3$O$_9$N, similar to the parent K$^+$ phosphor, but\nsubstituting Ga$^{3+}$ and In$^{3+}$ for Al$^{3+}$ leads to a change in\nEu$^{2+}$ site preference, narrowing the full-width-at-half-maximum (fwhm) of\nthe emission peak. The chromatic stability and photoluminescence quantum yield\nare also enhanced with higher Ga$^{3+}$ content in the host but not with\nIn$^{3+}$. Thermoluminescence analysis indicates the relationship between trap\nstates and the enhanced quantum yield with Ga$^{3+}$ leads to the series' best\nperformance. The analysis of the $M^I_{2.97}$Eu$_{0.015}$$M^{III}$P$_3$O$_9$N\nseries offers insight into the potential method for modulating optical\nproperties with cation substitution in the host structure.",
        "HSC 120505.09-000027.9 (J1205$-$0000) is one of the highest redshift\n($z=6.72$) dust-reddened quasars (red quasars) known to date. We present an\nimproved analysis of Atacama Large Millimeter\/submillimeter Array data of the\n[CII] $158\\ \\rm{\\mu m}$ line and the underlying rest-frame far-infrared (FIR)\ncontinuum emission, previously reported in Izumi et al. (2021a), toward\nJ1205$-$0000. Red quasars are thought to be a transitional phase from an\nobscured starburst to a luminous blue quasar, in some cases associated with\nmassive outflows driven by the active galactic nucleus (AGN). J1205$-$0000 has\na high FIR luminosity, $L_{\\mathrm{FIR}}=2.5\\times 10^{12}\\ L_{\\odot}$ and a\ntotal IR luminosity of $L_{\\mathrm{TIR}}=3.5\\times 10^{12}\\ L_{\\odot}$,\ncorresponding to a star formation rate (SFR) of $\\sim 528\\ M_{\\odot}\\\n\\mathrm{yr}^{-1}$. With the [CII]-based dynamical mass of $\\sim 1 \\times\n10^{11}~M_\\odot$, we conclude that J1205$-$0000 is hosted by a starburst\ngalaxy. In contradiction to Izumi et al. (2021a), our improved analysis shows\nno hint of a broad component in the [CII] line spectrum. Thus there is no\nevidence for a host galaxy-scale fast [CII] outflow, despite the fact that\nJ1205$-$0000 has fast nuclear ionized outflows seen in the rest-frame UV. We\nexplore several scenarios for this discrepancy (e.g., early phase of AGN\nfeedback, reliability of the [CII] line as a tracer of outflows), and we claim\nthat it is still too early to conclude that there is no significant negative\nAGN feedback on star formation in this red quasar.",
        "This paper presents a parallel and fully conservative adaptive mesh\nrefinement (AMR) implementation of a finite-volume-based kinetic solver for\ncompressible flows. Time-dependent H-type refinement is combined with a\ntwo-population quasi-equilibrium Bhatnagar-Gross-Krook discrete velocity\nBoltzmann model. A validation has shown that conservation laws are strictly\npreserved through the application of refluxing operations at coarse-fine\ninterfaces. Moreover, the targeted macroscopic moments of Euler and\nNavier-Stokes-Fourier level flows were accurately recovered with correct and\nGalilean invariant dispersion rates for a temperature range over three orders\nof magnitude and dissipation rates of all eigen-modes up to Mach of order 1.8.\nResults for one- and two-dimensional benchmarks up to Mach numbers of 3.2 and\ntemperature ratios of 7, such as the Sod and Lax shock tubes, the Shu-Osher and\nseveral Riemann problems, as well as viscous shock-vortex interactions, have\ndemonstrated that the solver precisely captures reference solutions. Excellent\nperformance in obtaining sensitive quantities was proven, for example in the\ntest case involving nonlinear acoustics, whilst, for the same accuracy and\nfidelity of the solution, the AMR methodology significantly reduced\ncomputational cost and memory footprints. Over all demonstrated two-dimensional\nproblems, up to a 4- to 9-fold reduction was achieved and an upper limit of the\nAMR overhead of 30% was found in a case with very cost-intensive parameter\nchoice. The proposed solver marks an accurate, efficient and scalable framework\nfor kinetic simulations of compressible flows with moderate supersonic speeds\nand discontinuities, offering a valuable tool for studying complex problems in\nfluid dynamics.",
        "Metasurfaces have offered unprecedented control over electromagnetic (EM)\nwaves across a wide range of frequency spectrum by manipulating its phase,\namplitude, and polarization at subwavelength scales. Full wavefront control\nusing metasurfaces requires 2{\\pi} phase modulation, which is essential for\nadvanced optical and photonic engineering. Common approaches, such as the\nPancharatnam-Berry (PB) phases and resonant phases, face stringent limitations:\nPB phases essentially depend on circular polarization conversion, while\nresonant phases are inherently narrowband and require a complex design process.\nTo overcome these challenges, we propose a broadband metasurface with a\nco-polarized transmissive geometric phase that achieves 2{\\pi} phase coverage\nwhile conserving the circular polarization of incident EM waves. This\nco-polarized phase is enabled by a local twist angle between the upper and\nlower metallic patterns, forming a branch cut in the parameter space determined\nby the twist angle and frequency. The branch cut connects phase singularities\nof opposite chirality, ensuring broadband 2{\\pi} phase coverage. We\nexperimentally validate the presence of the branch cut and demonstrate\nbroadband generation of arbitrary orbital angular momentum (OAM) for\nco-polarized output. Our approach provides a versatile method for designing\nbroadband metasurfaces without altering circular polarizations, paving the way\nfor development of compact optical and photonic devices.",
        "A functional analog of the Klain-Schneider theorem for vector-valued\nvaluations on convex functions is established, providing a classification of\ncontinuous, translation covariant, simple valuations. Under additional rotation\nequivariance assumptions, an analytic counterpart of the moment vector is\ncharacterized alongside a new epi-translation invariant valuation. The former\narises as the top-degree operator in a family of functional intrinsic moments,\nwhich are linked to functional intrinsic volumes through translations. The\nlatter represents the top-degree operator in a class of Minkowski vectors,\nwhich are introduced in this article and which lack classical counterparts on\nconvex bodies, as they vanish due to the Minkowski relations. Additional\nclassification results are obtained for homogeneous valuations of extremal\ndegrees.",
        "In analogy to high-energy nuclear scattering experiments, we study a\nreal-time scattering process between a propagating state and a dense target in\n$1+1$-d massive QED. In our setup, we identify three distinct regimes that\nqualitatively characterize the evolution: for a dilute medium, the incoming\nprobe state evolves nearly ballistically; in an intermediate setting, it\ntraverses the matter, locally exciting it; and for dense targets, one\napproaches a black-disk limit, where the matter acts as a strong wall\npotential. We find evidence that the probe's energy loss rate scales linearly\nwith the path length in the medium, and we study how the entanglement entropy\nreveals the mixing between the probe and medium states. With the goal of one\nday replicating high-energy nuclear experiments in quantum devices, we briefly\ndiscuss how the current tensor network-based simulations can be translated to a\nquantum simulator.",
        "Orthogonal-split trees perform well, but evidence suggests oblique splits can\nenhance their performance. This paper explores optimizing high-dimensional\n$s$-sparse oblique splits from $\\{(\\vec{w}, \\vec{w}^{\\top}\\boldsymbol{X}_{i}) :\ni\\in \\{1,\\dots, n\\}, \\vec{w} \\in \\mathbb{R}^p, \\| \\vec{w} \\|_{2} = 1, \\|\n\\vec{w} \\|_{0} \\leq s \\}$ for growing oblique trees, where $ s $ is a\nuser-defined sparsity parameter. We establish a connection between SID\nconvergence and $s_0$-sparse oblique splits with $s_0\\ge 1$, showing that the\nSID function class expands as $s_0$ increases, enabling the capture of more\ncomplex data-generating functions such as the $s_0$-dimensional XOR function.\nThus, $s_0$ represents the unknown potential complexity of the underlying\ndata-generating function. Learning these complex functions requires an\n$s$-sparse oblique tree with $s \\geq s_0$ and greater computational resources.\nThis highlights a trade-off between statistical accuracy, governed by the SID\nfunction class size depending on $s_0$, and computational cost. In contrast,\nprevious studies have explored the problem of SID convergence using orthogonal\nsplits with $ s_0 = s = 1 $, where runtime was less critical. Additionally, we\nintroduce a practical framework for oblique trees that integrates optimized\noblique splits alongside orthogonal splits into random forests. The proposed\napproach is assessed through simulations and real-data experiments, comparing\nits performance against various oblique tree models.",
        "In this paper, we develop a modified nonlinear dynamic diffusion (DD) finite\nelement method for convection-diffusion-reaction equations. This method is free\nof stabilization parameters and is capable of precluding spurious oscillations.\nWe prove existence and, under an assumption of small mesh size, uniqueness of\nthe discrete solution, and derive the optimal first order convergence rate of\nthe approximation error in the energy norm plus a dissipation term. Numerical\nexamples are provided to verify the theoretical analysis.",
        "We study the quiescent ultra-diffuse galaxy FCC 224 in the Fornax cluster\nusing Hubble Space Telescope (HST) imaging, motivated by peculiar properties of\nits globular cluster (GC) system revealed in shallower imaging. The surface\nbrightness fluctuation distance of FCC 224 measured from HST is $18.6 \\pm 2.7$\nMpc, consistent with the Fornax Cluster distance. We use Prospector to infer\nthe stellar population from a combination of multi-wavelength photometry (HST,\nground-based, WISE) and Keck Cosmic Web Imager spectroscopy. The galaxy has a\nmass-weighted age of $\\sim$ 10 Gyr, metallicity [M\/H] of $\\sim -1.25$ dex, and\na very short formation $e$-folding time of $\\tau \\sim 0.3$ Gyr. Its 12\ncandidate GCs exhibit highly homogeneous $g_{\\rm 475}-I_{\\rm 814}$ colors,\nmerely 0.04 mag bluer than the diffuse starlight, which supports a single burst\nformation scenario for this galaxy. We confirm a top-heavy GC luminosity\nfunction, similar to the two dark matter deficient galaxies NGC 1052-DF2 and\nDF4. However, FCC 224 differs from those galaxies with relatively small GC\nsizes of $\\sim$ 3 pc ($\\sim 35\\%$ smaller than typical for other dwarfs), and\nwith radial mass segregation in its GC system. We are not yet able to identify\na formation scenario to explain all of the GC properties in FCC 224. Follow-up\nmeasurements of the dark matter content in FCC 224 will be crucial because of\nthe mix of similarities and differences among FCC 224, DF2, and DF4.",
        "This paper concerns the distributed observer for the descriptor linear\nsystem. Unlike centralized descriptor system observers, in the case of\ndistributed observers, each agent either finds it difficult to independently\neliminate impulses, or the observer dynamics after eliminating pulses cannot be\nimplemented. To overcome this issue, this paper develops the structure of the\ndistributed observer in two different scenarios, and the observer parameters\nare presented through a novel design. Moreover, we provide two implementation\nmethods for distributed observer in different scenarios. As a result, each\nlocal observer has the ability to reconstruct the states of the underlying\nsystem, including its impulse phenomenon. Finally, simulation results verify\nthe validity of our results.",
        "We analyze sharing Bell-type nonlocal correlation between two distant parties\nwith optical hybrid states comprising a single photon polarization state and a\nmultiphoton coherent state. By deploying entanglement swapping over the\ncoherent state parts at the middle station, we show that the optical hybrid\nstates can efficiently generate a polarization-entangled state that violates\nClauser-Horne-Shimony-Holt (CHSH) Bell-inequality well over a metropolitan\ndistance. We further assess the quality of the shared entangled state in the\ninformation processing task of quantum teleportation of an unknown polarization\nqubit. Our results with realistic devices, embedding detection inefficiency and\ntransmission losses, indicate the viability of faithful quantum teleportation\nover large distances, consistent with the quality of the shared correlation.",
        "Magnetic thin films hosting topological spin textures, such as magnetic\nskyrmions, hold high potential for breakthroughs in the field of spintronics,\ndue to good scalability and energy efficiency. Novel computational\narchitectures such as memory-in-logic devices rely on material platforms able\nto host those topological spin textures. Furthermore, recently proposed designs\nof novel quantum information technologies are based on heterostructures where\ntopological spin textures are in direct proximity to a superconducting layer.\nHere, we demonstrate the stabilization of out-of-plane magnetic bubbles in\nhighly ordered [Ni\/Co]$_{n}$ multilayers on a Nb(110) single crystal. This is\nachieved without the need for removal of the well-known Nb(110)-oxide surface\nreconstruction, due to the introduction of a one-atom-thick Cu interlayer in\nbetween the Nb substrate and the magnetic multilayer. The Cu interlayer\ngenerates a well-ordered hexagonal surface, which is key for the epitaxial\ngrowth of the [Ni\/Co]$_{n}$ multilayers hosting the desired out-of-plane\nanisotropy. The magnetic ground state of the prepared material stacks is\ndirectly imaged via spin-polarized low energy electron microscopy (SPLEEM),\nrevealing the presence of magnetic bubble domains with lateral sizes as small\nas 450 nm.",
        "Transport of microscopic objects across biological membranes usually involves\nmembrane deformation to enclose the object followed by detachment of the\nengulfed particle. However, in artificial membranes, this last topological\nremodelling step is in many cases not spontaneous due to the elastic stability\nof the neck structure formed upon complete particle wrapping. In this work, we\nuse optical trapping to induce the wrapping of a non-adhesive microsphere by\nthe membrane of a giant lipid vesicle and investigate the energetics and\ndynamics of the resulting neck structure. We find that neck formation occurs as\na result of membrane shape energy minimization under the application of\nexternal force. Remarkably, increasing membrane tension could reopen the neck\nand reverse the wrapping process. This process shows a clear hysteresis and a\ndegree of reversibility. Neck cleavage and particle detachment into the\nvesicle's interior could not be triggered in the range of our optical forces.\nSystematic studies on the thermal dynamics of wrapped particles allowed to\nestablish that diffusion properties of the system are in agreement with a\ncoupling of the particle motion with the neck structure, modeled as a solid\ninclusion within the membrane. Interestingly, the wrapped particle dynamics\nexhibited a tension dependency, which can be described as the sum of several\ndrag contributions.",
        "Neutral atoms emitted from liquid metal ion sources are an often-overlooked\nsource of contamination and damage in focused ion beam microscopy. Beyond ions\nand single atoms, these sources also emit atom clusters. While most studies\nhave investigated charged clusters, here we demonstrate that neutral clusters\nare also emitted. These neutral clusters bypass the electrostatic beam blanking\nsystem, allowing them to impinge on samples even when the ion beam is blanked.\nWe investigate this phenomenon using thin (<20 nm) freestanding membranes of\nhexagonal boron nitride, silicon, and silicon nitride as targets. Randomly\ndispersed nanopores that form upon neutral cluster exposure are revealed. The\naverage nanopore diameter is ~2 nm with a narrow size distribution, suggesting\nthat the atom clusters emitted from the source have a preferred size. Various\nelectron microscopy techniques are used to characterize the nanopores,\nincluding high-resolution transmission electron microscopy, multislice\nptychography, and electron energy-loss spectroscopy. Finally, we show how\nelectron irradiation in the transmission electron microscope can be used to\nboth remove any amorphous material that may clog the pores and to controllably\ngrow the pores to specific sizes. Tunable nanopores such as these are\ninteresting for nanofluidic applications involving size-selective membranes.",
        "The associative interaction, such as hydrogen bonding, can bring about\nversatile functionalities to polymer systems, which has been investigated by\ntremendous researches, but the fundamental understanding on association process\nis still lacking. In this study, a reaction-controlled association model is\nproposed to delve into the polymer association activities in solvents, which is\nproved to obey the principle of thermodynamics. Additionally, associative\npolymer chain configurational bias method is developed to improve sampling\nefficiency, demonstrating a significantly faster relaxation process. First, we\nset non-bonded interactions to be zero, and only keep the chain connectivity\nand association. It is found that the association process intrinsically follows\nBernoulli process by comparing the simulation results and analytic results.\nNext, we include non-bonded interactions into the simulation to examine its\neffects. It emerged that the excluded volume effect and solvents immiscibility\neffects can result in inhomogeneous associating probability distribution along\nthe chain contour, in contrast to the homogeneity observed in ideal systems,\nthereby shifting from the binomial distribution to Poisson binomial\ndistribution. At last, the study is extended to cooperative association\nsystems. The incorporation of cooperative association can lead to the\ncoexistence of coil and globule state at the transition point, verified by the\npotential of mean force calculation. Finally, a mathematical model is proposed,\nillustrating the changes in statistical weight induced by sequence enthalpy\nbias, which is the consequence of cooperative behaviors."
      ]
    }
  },
  {
    "id":2411.08073,
    "research_type":"applied",
    "start_id":"b24",
    "start_title":"DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome",
    "start_abstract":"Abstract Motivation Deciphering the language of non-coding DNA is one fundamental problems in genome research. Gene regulatory code highly complex due to existence polysemy and distant semantic relationship, which previous informatics methods often fail capture especially data-scarce scenarios. Results To address this challenge, we developed a novel pre-trained bidirectional encoder representation, named DNABERT, global transferrable understanding genomic sequences based on up downstream nucleotide contexts. We compared DNABERT most widely used programs for genome-wide elements prediction demonstrate its ease use, accuracy efficiency. show that single transformers model can simultaneously achieve state-of-the-art performance promoters, splice sites transcription factor binding sites, after easy fine-tuning using small task-specific labeled data. Further, enables direct visualization nucleotide-level importance relationship within input better interpretability accurate identification conserved sequence motifs functional genetic variant candidates. Finally, with human even be readily applied other organisms exceptional performance. anticipate fined tuned many analyses tasks. Availability implementation The source code, pretrained finetuned are available at GitHub (https:\/\/github.com\/jerryji1993\/DNABERT). Supplementary information data Bioinformatics online.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b20"
      ],
      "title":[
        "A primer on deep learning in genomics"
      ],
      "abstract":[
        "Deep learning methods are a class of machine learning techniques capable of identifying highly complex patterns in large datasets. Here, we provide a perspective and primer on deep learning applications for genome analysis. We discuss successful applications in the fields of regulatory genomics, variant calling and pathogenicity scores. We include general guidance for how to effectively use deep learning methods as well as a practical guide to tools and resources. This primer is accompanied by an interactive online tutorial."
      ],
      "categories":[
        "q-bio.GN"
      ]
    },
    "list":{
      "title":[
        "Spatial locking of chimera states to frequency heterogeneity in\n  nonlocally coupled oscillators",
        "States of Disarray: Cleaning Data for Gerrymandering Analysis",
        "Wafer-scale waveguide sidewall roughness scattering loss\n  characterization by image processing",
        "The Aesthetic Imperative of Lev Landau's Geometric Reductionism in\n  Theoretical Physics",
        "BrainOOD: Out-of-distribution Generalizable Brain Network Analysis",
        "Fenchel's conjecture on NEC groups",
        "Using the STIX background detector as a proxy for GOES",
        "Exercises in Iterational Asymptotics II",
        "Tools for Unbinned Unfolding",
        "On stabilization at a soliton for generalized Korteweg--De Vries pure\n  power equation for any power $p\\in (1,5)$",
        "An improved evaluation of the electroweak contribution to $(g-2)_\\mu$",
        "The puzzle of isolated and quenched dwarf galaxies in cosmic voids",
        "Site-engineered ferromagnetism in Ca and Cr co-substituted Bismuth\n  Ferrite Nanoparticles",
        "A modified two-stage search framework for constrained multi-gradient\n  descent",
        "Ultrafast Charge Separation on the Nanoscale Induced by a Uniform Field",
        "Digital Twin Aided Channel Estimation: Zone-Specific Subspace Prediction\n  and Calibration",
        "End-to-End Detector Optimization with Diffusion models: A Case Study in\n  Sampling Calorimeters",
        "Sound insulation performance of multi-layer membrane-type acoustic\n  metamaterials based on orthogonal experiments",
        "Direct Sampling of Confined Polygons in Linear Time",
        "Towards a Manifestly Causal Approach to Particle Scattering",
        "Cycle Patterns and Mean Payoff Games",
        "Hypercubic structures behind $\\hat{Z}$-invariants",
        "Spherically symmetric horizonless solutions and their frozen states in\n  Bardeen spacetime with Proca field",
        "Explainable Brain Age Gap Prediction in Neurodegenerative Conditions\n  using coVariance Neural Networks",
        "Geometrical constructions of purity testing protocols and their\n  applications to quantum communication",
        "Langevin model for soliton molecules in ultrafast fiber ring laser\n  cavity: investigating experimentally the interplay between noise and inertia",
        "A hybrid-dimensional Stokes--Brinkman--Darcy model for arbitrary flows\n  to the fluid--porous interface",
        "Relighting the fire in Hickson Compact Group (HCG) 15: magnetised fossil\n  plasma revealed by the SKA Pathfinders & Precursors",
        "Richardson-Gaudin states of non-zero seniority I: matrix elements"
      ],
      "abstract":[
        "Chimera states in systems of nonlocally coupled oscillators, i.e.,\nself-organized coexistence of coherent and incoherent oscillator populations,\nhave attracted much attention. In this study, we consider the effect of\nfrequency heterogeneities on the chimera state and reveal that it induces\nspatial locking of the chimera state, i.e., the coherent and incoherent domains\nalign with lower and higher frequency regions, respectively, in a self-adaptive\nmanner. Using an extended self-consistency approach, we show that such\nspatially locked chimera states can be reproduced as steady solutions of the\nsystem in the continuum limit. Furthermore, we develop a variational argument\nto explain the mechanism leading to spatial locking. Our analysis reveals how\nheterogeneity can affect the collective dynamics of the chimera states and\noffers insights into their control and applications.",
        "The mathematics of redistricting is an area of study that has exploded in\nrecent years. In particular, many different research groups and expert\nwitnesses in court cases have used outlier analysis to argue that a proposed\nmap is a gerrymander. This outlier analysis relies on having an ensemble of\npotential redistricting maps against which the proposed map is compared.\nArguably the most widely-accepted method of creating such an ensemble is to use\na Markov Chain Monte Carlo (MCMC) process. This process requires that various\npieces of data be gathered, cleaned, and coalesced into a single file that can\nbe used as the seed of the MCMC process.\n  In this article, we describe how we have begun this cleaning process for each\nstate, and made the resulting data available for the public at\nhttps:\/\/github.com\/eveomett-states . At the time of submission, we have data\nfor 22 states available for researchers, students, and the general public to\neasily access and analyze. We will continue the data cleaning process for each\nstate, and we hope that the availability of these datasets will both further\nresearch in this area, and increase the public's interest in and understanding\nof modern techniques to detect gerrymandering.",
        "Photonic integrated circuits (PICs) are vital for developing affordable,\nhigh-performance optoelectronic devices that can be manufactured at an\nindustrial scale, driving innovation and efficiency in various applications.\nOptical loss of modes in thin film waveguides and devices is a critical measure\nof their performance. Thin films growth, lithography, masking, and etching\nprocesses are imperfect processes that introduce significant sidewall and\ntop-surface roughness and cause dominating optical losses in waveguides and\nphotonic structures. These roughness as perturbations couple light from guided\nto far-field radiation modes, leading to scattering losses that can be\nestimated from theoretical models. Typically, with UV-based lithography\nsidewall roughness is found to be significantly larger than wafer-top surface\nroughness. Atomic force microscopy (AFM) imaging measurement gives 3D and\nhigh-resolution roughness profile but the measurement is inconvenient, costly,\nand unscalable for large-scale PICs and at wafer-scale. Here, we evaluate the\nsidewall roughness profile based on 2D high-resolution scanning electron\nmicroscope imaging. We characterized the loss on two homemade nitride and oxide\nfilms on 3-inch silicon wafers with 12 waveguide devices on each and co-related\nthe scattering loss estimated from a 2D image-based sidewall profile and\ntheoretical Payne model. The lowest loss of guided fundamental transverse\nelectric (TE$_{0}$) is found at 0.075 dB\/cm at 633 nm across 24 devices, which\nis a record at visible wavelength. Our work shows a 100% success in edge\ndetection in image processing to estimate autocorrelation function and optical\nmode loss. These demonstrations offer valuable insights into waveguide sidewall\nroughness and comparison of experimental and 2D SEM image-processing-based loss\nestimations.",
        "This paper explores the ontological and epistemological foundations of Lev\nLandau's theoretical physics through the lens of his unpublished philosophical\nnotes and scientific practice. We identify a unique form of geometric\nreductionism where physical laws emerge as inevitable consequences of symmetry\nbreaking in progressively constrained phase spaces. Landau's dismissal of\nquantum interpretation debates and his famous \"axiomatic minimalism\" in the\nCourse of Theoretical Physics are shown to stem from a deep epistemological\ncommitment to dimensional aesthetics - the belief that fundamental truths must\nmanifest through dimensional economy in mathematical representations.",
        "In neuroscience, identifying distinct patterns linked to neurological\ndisorders, such as Alzheimer's and Autism, is critical for early diagnosis and\neffective intervention. Graph Neural Networks (GNNs) have shown promising in\nanalyzing brain networks, but there are two major challenges in using GNNs: (1)\ndistribution shifts in multi-site brain network data, leading to poor\nOut-of-Distribution (OOD) generalization, and (2) limited interpretability in\nidentifying key brain regions critical to neurological disorders. Existing\ngraph OOD methods, while effective in other domains, struggle with the unique\ncharacteristics of brain networks. To bridge these gaps, we introduce BrainOOD,\na novel framework tailored for brain networks that enhances GNNs' OOD\ngeneralization and interpretability. BrainOOD framework consists of a feature\nselector and a structure extractor, which incorporates various auxiliary losses\nincluding an improved Graph Information Bottleneck (GIB) objective to recover\ncausal subgraphs. By aligning structure selection across brain networks and\nfiltering noisy features, BrainOOD offers reliable interpretations of critical\nbrain regions. Our approach outperforms 16 existing methods and improves\ngeneralization to OOD subjects by up to 8.5%. Case studies highlight the\nscientific validity of the patterns extracted, which aligns with the findings\nin known neuroscience literature. We also propose the first OOD brain network\nbenchmark, which provides a foundation for future research in this field. Our\ncode is available at https:\/\/github.com\/AngusMonroe\/BrainOOD.",
        "A classical discovery known as Fenchel's conjecture and proved in the 1950s,\nshows that every co-compact Fuchsian group $F$ has a normal subgroup of finite\nindex isomorphic to the fundamental group of a compact unbordered orientable\nsurface, or in algebraic terms, that $F$ has a normal subgroup of finite index\nthat contains no element of finite order other than the identity. In this paper\nwe initiate and make progress on an extension of Fenchel's conjecture by\nconsidering the following question: Does every planar non-Euclidean\ncrystallographic group $\\Gamma$ containing transformations that reverse\norientation have a normal subgroup of finite index isomorphic to the\nfundamental group of a compact unbordered non-orientable surface? We answer\nthis question in the affirmative in the case where the orbit space of $\\Gamma$\nis a nonorientable surface, and also in the case where this orbit space is a\nbordered orientable surface of positive genus. In the case where the genus of\nthe quotient is $0$, we have an affirmative answer in many subcases, but the\nquestion is still open for others.",
        "Context. The Spectrometer\/Telescope for Imaging X-Rays (STIX) onboard Solar\nOrbiter was designed to observe solar flares in the X-ray range of 4-150 keV,\nproviding spectral, temporal and spatial information. Besides 30 imaging\ndetectors, STIX has two additional detectors, the coarse flare locator (CFL)\nand the background (BKG) detector. Flares observed from Earth are classified\nusing their peak X-ray flux observed by the GOES satellites. Roughly half of\nall flares observed by STIX are located on the backside of the Sun. These\nflares lack a GOES-class classification.\n  Aims. In this paper, we describe the calibration of the BKG detector aperture\nsizes. Using the calibrated measurements of the BKG detector, we explore the\nrelationship between the peak flux for flares jointly observed by STIX and\nGOES. This allows us to estimate the GOES flare classes of backside flares\nusing STIX measurements.\n  Methods. We looked at the 500 largest flares observed by both STIX and GOES\nin the time range Feb. 21 to Apr. 23. Aperture size calibration is done by\ncomparing 4-10 keV counts of the BKG detector with the CFL measurements. In a\nsecond step, we correlate the calibrated STIX BKG peak flux with the GOES peak\nflux for individual flares.\n  Results. We calibrated the BKG detector aperture sizes of STIX. Further, we\nshowed that for the larger flares a close power law fit exists between the STIX\nBKG and GOES peak flux with a Pearson correlation coefficient of 0.97. This\ncorrelation provides a GOES proxy with a one sigma uncertainty of 11%. We were\nable to show that the BKG detector can reliably measure a broad range of GOES\nflare classes from roughly B5 up to at least X85 (assuming a radial distance of\n1AU), making it an interesting detector-concept for future space weather\nmissions. The largest flare observed by STIX to date is an estimated X16.5\n$\\pm$ 1.8 backside flare on the 20 Mai 2024.",
        "The nonlinear recurrences we consider here include the functions $3x(1-x)$\nand $\\cos(x)$, which possess attractive fixed points $2\/3$ and $0.739...$\n(Dottie's number). Detailed asymptotics for oscillatory convergence are found,\nstarting with a 1960 paper by Wolfgang Thron. Another function,\n$x\/(1+x\\ln(1+x))$, gives rise to a sequence with monotonic convergence to $0$\nbut requires substantial work to calculate its associated constant $C$.",
        "Machine learning has enabled differential cross section measurements that are\nnot discretized. Going beyond the traditional histogram-based paradigm, these\nunbinned unfolding methods are rapidly being integrated into experimental\nworkflows. In order to enable widespread adaptation and standardization, we\ndevelop methods, benchmarks, and software for unbinned unfolding. For\nmethodology, we demonstrate the utility of boosted decision trees for unfolding\nwith a relatively small number of high-level features. This complements\nstate-of-the-art deep learning models capable of unfolding the full phase\nspace. To benchmark unbinned unfolding methods, we develop an extension of\nexisting dataset to include acceptance effects, a necessary challenge for real\nmeasurements. Additionally, we directly compare binned and unbinned methods\nusing discretized inputs for the latter in order to control for the binning\nitself. Lastly, we have assembled two software packages for the OmniFold\nunbinned unfolding method that should serve as the starting point for any\nfuture analyses using this technique. One package is based on the widely-used\nRooUnfold framework and the other is a standalone package available through the\nPython Package Index (PyPI).",
        "We apply our idea, which previously we used in the analysis of the pure power\nNLS, consisting in spitting the virial inequality method into a large energy\ninequality combined with Kato smoothing, to the case of generalized\nKorteweg--De Vries pure power equations. We assume that a solution remains for\nall positive times very close to a soliton and then we prove an asymptotic\nstability result for $t\\to +\\infty$.",
        "A precise evaluation of the electroweak contribution to the anomalous\nmagnetic moment of the muon requires control over all aspects of the Standard\nModel, ranging from Higgs physics, over multi-loop computations for bosonic and\n(heavy-)fermion diagrams, to non-perturbative effects in the presence of light\nquarks. Currently, the dominant uncertainties arise from such hadronic effects\nin the vector-vector-axial-vector three-point function, an improved\nunderstanding of which has recently emerged in the context of hadronic\nlight-by-light scattering. Profiting from these developments as well as new\nperturbative and non-perturbative input for the charm contribution, we obtain\n$a_\\mu^\\text{EW}=154.4(4)\\times 10^{-11}$.",
        "We report, for the first time, the detection of a sample of quenched and\nisolated dwarf galaxies (with 8.9 $<$ log(M$_{\\rm \\star}$\/M$_{\\rm \\odot}$) $<$\n9.5) in the least dense regions of the cosmic web, including voids, filaments,\nand walls. These dwarfs have no neighbouring galaxy within 1.0~Mpc in projected\ndistance. Based on the full spectral fitting of their central spectra using\nSloan Digital Sky Survey data, these galaxies are gas-deprived, exhibit stellar\nmass assembly very similar to dwarfs in the central regions of galaxy clusters,\nand have experienced no significant star formation in the past 2 Gyr.\nAdditionally, analysis of r-band images from the Dark Energy Camera Legacy\nSurvey showed that these dwarf galaxies host a central Nuclear Star Cluster\n(NSC). Detecting quenched, isolated dwarf galaxies in cosmic voids indicates\nthat environmental factors are not the sole drivers of their quenching.\nInternal mechanisms, such as feedback from in-situ star formation, also\ncontributing to the NSC formation, black holes, or variations in conditions\nduring their formation, offer potential explanations for star formation\nsuppression in these galaxies. These findings highlight the need for a\nsignificant revision in our understanding of baryonic physics, particularly\nconcerning the formation and evolution of low-mass galaxies.",
        "Multiferroic perovskites that exhibit room temperature magnetization and\npolarization have immense potential in the next generation of magneto-electric\nand spintronic memory devices. In this work, the magnetic and ferroelectric\nproperties of Bismuth Ferrite, BiFeO3 (BFO) nanoparticles (NPs) were enhanced\nthrough simultaneous A and B site Ca and Cr co-substitution. Novel compositions\nof Bi0.97Ca0.03CrxFe1-xO3 (x=0, 0.01, 0.03, 0.05) were synthesized using the\nsol-gel route and annealed at 550 degrees Celcius. Rietveld Refinement of XRD\npatterns confirmed high phase purity, while SEM analysis revealed a decreasing\ntrend in average particle size with increasing dopant concentration. Hysteresis\nloops showed enhanced magnetic properties as particle size approached the spin\ncycloid wavelength (around 62 nm), disrupting the intrinsic antiferromagnetic\nordering of BFO. Moreover, the presence of exchange bias in the NPs was linked\nto the formation of core-shell structure. Temperature dependent magnetization\nstudies showed an increase in N\\'eel temperature upon Ca substitution. XPS\nanalysis confirmed that Bi0.97Ca0.03FeO3 samples exhibited the highest oxygen\nvacancy concentration, while Fe3+ remained the dominant oxidation state across\nall compositions. Ferroelectric polarization loop measurements showed enhanced\nremanent polarization in doped samples, with leakage linked to oxygen vacancies\nand extrinsic microstructural effects.",
        "\\cite{desideri2012multiple} proposed a multi-gradient descent algorithm\n(MGDA) that can improve all objectives based on seeking the minim norm point in\nthe convex hull consisting of objectives function gradients as the common\ndescent direction, which has become the cornerstone of the multi-musk learning\nand multi-objective optimization. However, the logic to seek a common descent\ndirection through finding the minim-norm point in the gradient convex hull may\nfail under constraints, no matter whether taking the constraints into\nconsideration directly or projecting it into the feasible region after finding\nthe common descent direction with no constraints. Therefore, we proposed a\ntwo-stage search framework. In the first stage, a min-max search algorithm is\nimplemented to minimize the upper bound of the directional derivatives under\nconstraints, and the weak Pareto stationary can be theoretically reached in the\nfirst stage. In the second stage, the Pareto stationary can be theoretically\nobtained through the minimization of the lower bound of the directional\nderivatives under constraints. In numerical studies, we show the effectiveness\nof our proposed framework from the calibration of the multi-regime fundamental\ndiagram model and large-scale multi-objective portfolio problem.",
        "When illuminated by white light, atoms, molecules, and materials absorb only\ncertain characteristic energy contributions based on their absorption\nproperties. Here, we show that this effect can be translated from energy to\nspace: a spatially uniform laser pulse can create strongly localized carrier\nexcitations, including excitons, and spatial charge separation on the\nsub-nanometer scale within a few femtoseconds, opening new avenues for\nnanoelectronics and bringing petahertz switching within reach. Using\nnonequilibrium Green functions simulations we demonstrate this effect by\nexciting targeted areas of small graphene nanoribbon heterostructures by\ncareful choice of the laser energy and polarization.",
        "Effective channel estimation in sparse and high-dimensional environments is\nessential for next-generation wireless systems, particularly in large-scale\nMIMO deployments. This paper introduces a novel framework that leverages\ndigital twins (DTs) as priors to enable efficient zone-specific subspace-based\nchannel estimation (CE). Subspace-based CE significantly reduces feedback\noverhead by focusing on the dominant channel components, exploiting sparsity in\nthe angular domain while preserving estimation accuracy. While DT channels may\nexhibit inaccuracies, their coarse-grained subspaces provide a powerful\nstarting point, reducing the search space and accelerating convergence. The\nframework employs a two-step clustering process on the Grassmann manifold,\ncombined with reinforcement learning (RL), to iteratively calibrate subspaces\nand align them with real-world counterparts. Simulations show that digital\ntwins not only enable near-optimal performance but also enhance the accuracy of\nsubspace calibration through RL, highlighting their potential as a step towards\nlearnable digital twins.",
        "Recent advances in machine learning have opened new avenues for optimizing\ndetector designs in high-energy physics, where the complex interplay of\ngeometry, materials, and physics processes has traditionally posed a\nsignificant challenge. In this work, we introduce the $\\textit{end-to-end}$ AI\nDetector Optimization framework (AIDO) that leverages a diffusion model as a\nsurrogate for the full simulation and reconstruction chain, enabling\ngradient-based design exploration in both continuous and discrete parameter\nspaces. Although this framework is applicable to a broad range of detectors, we\nillustrate its power using the specific example of a sampling calorimeter,\nfocusing on charged pions and photons as representative incident particles. Our\nresults demonstrate that the diffusion model effectively captures critical\nperformance metrics for calorimeter design, guiding the automatic search for\nlayer arrangement and material composition that aligns with known calorimeter\nprinciples. The success of this proof-of-concept study provides a foundation\nfor future applications of end-to-end optimization to more complex detector\nsystems, offering a promising path toward systematically exploring the vast\ndesign space in next-generation experiments.",
        "The challenge of achieving effective sound insulation using metamaterials\npersists in the field. In this research endeavor, a novel three-layer\nmembrane-type acoustic metamaterial is introduced as a potential solution.\nThrough the application of orthogonal experiments, remarkable sound insulation\ncapabilities are demonstrated within the frequency spectrum of 100-1200 Hz. The\nsound insulation principle of membrane-type acoustic metamaterial is obtained\nthrough the analysis of eigenmodes at the peak and trough points, combined with\nsound transmission loss. In addition, an orthogonal experiment is utilized to\npinpoint the critical factors that impact sound insulation performance. By\nusing relative bandwidth as the classification criterion, the optimal\ncombination of influencing factors is determined, thereby improving the sound\ntransmission loss of the multi-layer membrane-type acoustic metamaterial\nstructure and broadening the sound insulation bandwidth. This study not only\ncontributes a fresh and practical approach to insulation material design but\nalso offers valuable insights into advancing sound insulation technology.",
        "We present an algorithm for sampling tightly confined random equilateral\nclosed polygons in three-space which has runtime linear in the number of edges.\nUsing symplectic geometry, sampling such polygons reduces to sampling a moment\npolytope, and in our confinement model this polytope turns out to be very\nnatural from a combinatorial point of view. This connection to combinatorics\nyields both our fast sampling algorithm and explicit formulas for the expected\ndistances of vertices to the origin. We use our algorithm to investigate the\nexpected total curvature of confined polygons, leading to a very precise\nconjecture for the asymptotics of total curvature.",
        "We introduce a new, probability-level approach to calculations in scalar\nfield particle scattering. The approach involves the implicit summation over\nfinal states, which makes causality manifest since retarded propagators emerge\nnaturally. Novel diagrams represent algebraic terms at the probability level,\nakin to Feynman diagrams at the amplitude level. We present a list of rules\nwhich generate all probability-level diagrams for particle scattering processes\nin which one is fully inclusive over final states that contain no initial-state\nparticles. The inclusivity and causal structure of this formalism may offer\ninsights into the cancellation of infrared divergences if applied to gauge\ntheory calculations.",
        "We introduce the concept of a \\emph{cycle pattern} for directed graphs as\nfunctions from the set of cycles to the set $\\{-,0,+\\}$. The key example for\nsuch a pattern is derived from a weight function, giving rise to the sign of\nthe total weight of the edges for each cycle. Hence, cycle patterns describe a\nfundamental structure of a weighted digraph, and they arise naturally in games\non graphs, in particular parity games, mean payoff games, and energy games.\n  Our contribution is threefold: we analyze the structure and derive hardness\nresults for the realization of cycle patterns by weight functions. Then we use\nthem to show hardness of solving games given the limited information of a cycle\npattern. Finally, we identify a novel geometric hardness measure for solving\nmean payoff games (MPG) using the framework of linear decision trees, and use\ncycle patterns to derive lower bounds with respect to this measure, for large\nclasses of algorithms for MPGs.",
        "We propose an abelian categorification of $\\hat{Z}$-invariants for Seifert\n$3$-manifolds. First, we give a recursive combinatorial derivation of these\n$\\hat{Z}$-invariants using graphs with certain hypercubic structures. Next, we\nconsider such graphs as annotated Loewy diagrams in an abelian category,\nallowing non-split extensions by the ambiguity of embedding of subobjects. If\nsuch an extension has good algebraic group actions, then the above derivation\nof $\\hat{Z}$-invariants in the Grothendieck group of the abelian category can\nbe understood in terms of the theory of shift systems, i.e., Weyl-type\ncharacter formula of the nested Feigin-Tipunin constructions. For the project\nof developing the dictionary between logarithmic CFTs and 3-manifolds, these\ndiscussions give a glimpse of a hypothetical and prototypical, but unified\nconstruction\/research method for the former from the new perspective,\nreductions of representation theories by recursive structures.",
        "In this paper, we construct a static spherical symmetric Bardeen-Proca star\n(BPS) model, which consists of the electromagnetic field and Proca field\nminimally coupled with gravity. The introduction of the Proca field disrupts\nthe formation of event horizons, ensuring that these solutions are globally\nregular throughout the spacetime. We obtain families of BPS solutions under\nseveral magnetic charge conditions. Based on these results, we further\ninvestigate the ADM mass, Noether charge, and energy density distribution of\nthem. We find that when the magnetic charge is sufficiently large, solutions\nwith a critical horizon $r_{cH}$ emerge as $\\omega \\rightarrow 0$, and the time\ncomponent of the metric approaches zero inside $r_{cH}$. To an observer at\ninfinity, the collapse process of the matter near the critical horizon appears\nfrozen. Consequently, we refer to the solution with $r_{cH}$ as the frozen\nBardeen-Proca star (FBPS). Additionally, we also investigate the circular\ngeodesic orbits of BPS. For the light ring, we find that the light rings always\nappear in pairs, located on both sides of the critical horizon and moving\nfurther apart as the frequency $\\omega$ decreases. For timelike circular\norbits, we investigate their distribution in the spacetime of BPSs and\nhighlight four representative families of BPS solutions.",
        "Brain age is the estimate of biological age derived from neuroimaging\ndatasets using machine learning algorithms. Increasing \\textit{brain age gap}\ncharacterized by an elevated brain age relative to the chronological age can\nreflect increased vulnerability to neurodegeneration and cognitive decline.\nHence, brain age gap is a promising biomarker for monitoring brain health.\nHowever, black-box machine learning approaches to brain age gap prediction have\nlimited practical utility. Recent studies on coVariance neural networks (VNN)\nhave proposed a relatively transparent deep learning pipeline for neuroimaging\ndata analyses, which possesses two key features: (i) inherent\n\\textit{anatomically interpretablity} of derived biomarkers; and (ii) a\nmethodologically interpretable perspective based on \\textit{linkage with\neigenvectors of anatomic covariance matrix}. In this paper, we apply the\nVNN-based approach to study brain age gap using cortical thickness features for\nvarious prevalent neurodegenerative conditions. Our results reveal distinct\nanatomic patterns for brain age gap in Alzheimer's disease, frontotemporal\ndementia, and atypical Parkinsonian disorders. Furthermore, we demonstrate that\nthe distinct anatomic patterns of brain age gap are linked with the differences\nin how VNN leverages the eigenspectrum of the anatomic covariance matrix, thus\nlending explainability to the reported results.",
        "Purity testing protocols (PTPs), i.e., protocols that decide with high\nprobability whether or not a distributed bipartite quantum state is maximally\nentangled, have been proven to be a useful tool in many quantum communication\napplications. In this paper, we provide geometrical constructions for such\nprotocols that originate directly from classical linear error correcting codes\n(LECCs), in a way that the properties of the resulting PTPs are completely\ndetermined from those of the LECCs used in the construction. We investigate the\nimplications of our results in various tasks, including error detection,\nentanglement purification for general quantum error models and quantum message\nauthentication.",
        "The dynamics of soliton molecules in ultrafast fiber ring laser cavity is\nstrongly influenced by noise. We show how a parsimonious Langevin model can be\nconstructed from experimental data, resulting in a mathematical description\nthat encompasses both the deterministic and stochastic properties of the\nevolution of the soliton molecules. In particular, we were able to probe the\nresponse dynamics of the soliton molecule to an external kick in a sub-critical\napproach, namely without the need to actually disturb the systems under\ninvestigation. Moreover, the noise experienced by the dissipative solitonic\nsystem, including its distribution and correlation, can now be also analyzed in\ndetails. Our strategy can be applied to any systems where the individual motion\nof its constitutive particles can be traced; the case of optical\nsolitonic-system laser presented here serving as a proof-of-principle\ndemonstration.",
        "Mathematical modelling of coupled flow systems containing a free-flow region\nin contact with a porous medium is challenging, especially for arbitrary flow\ndirections to the fluid--porous interface. Transport processes in the free flow\nand porous medium are typically described by distinct equations: the Stokes\nequations and Darcy's law, respectively, with an appropriate set of coupling\nconditions at the common interface. Classical interface conditions based on the\nBeavers--Joseph condition are not accurate for general flows. Several\ngeneralisations are recently developed for arbitrary flows at the interface,\nsome of them are however only theoretically formulated and still need to be\nvalidated.\n  In this manuscript, we propose an alternative to couple free flow and\nporous-medium flow, namely, the hybrid-dimensional Stokes--Brinkman--Darcy\nmodel. Such formulation incorporates the averaged Brinkman equations within a\ncomplex interface between the free-flow and porous-medium regions. The complex\ninterface acts as a buffer zone facilitating storage and transport of mass and\nmomentum and the model is applicable for arbitrary flow directions. We validate\nthe proposed hybrid-dimensional model against the pore-scale resolved model in\nmultiple examples and compare numerical simulation results also with the\nclassical and generalised coupling conditions from the literature. The proposed\nhybrid-dimensional model demonstrates its applicability to describe arbitrary\ncoupled flows and shows its advantages in comparison to other generalised\ncoupling conditions.",
        "In the context of the life cycle and evolution of active galactic nuclei\n(AGN), the environment plays an important role. In particular, the over-dense\nenvironments of galaxy groups, where dynamical interactions and bulk motions\nhave significant impact, offer an excellent but under-explored window into the\nlife cycles of AGN and the processes that shape the evolution of relativistic\nplasma. Pilot Survey observations with the Australian Square Kilometre Array\nPathfinder (ASKAP) Evolutionary Map of the Universe (EMU) survey recovered\ndiffuse emission associated with the nearby (z = 0.0228) galaxy group HCG15,\nwhich was revealed to be strongly linearly polarised. We study the properties\nof this emission in unprecedented detail to settle open questions about its\nnature and its relation to the group-member galaxies. We perform a\nmulti-frequency spectropolarimetric study of HCG15 incorporating our ASKAP EMU\nobservations as well as new data from MeerKAT, LOFAR, the GMRT, and the Karl G.\nJansky Very Large Array (VLA), plus X-ray data from XMM-Newton and optical\nspectra from the Himalayan Chandra Telescope (HCT). Our study confirms that the\ndiffuse structure represents remnant emission from historic AGN activity,\nlikely associated with HCG15-D, some 80-86 Myr ago (based on ageing analysis).\nWe detect significant highly linearly-polarised emission from a diffuse\n'ridge'-like structure with a highly ordered magnetic field. Our analysis\nsuggests that this emission is generated by draping of magnetic field lines in\nthe intra-group medium (IGrM), although further exploration with simulations\nwould aid our understanding. We confirm that HCG15-C is a group-member galaxy.\nFinally, we report the detection of thermal emission associated with a\nbackground cluster at redshift z ~ 0.87 projected onto the IGrM of HCG15, which\nmatches the position and redshift of the recent SZ detection of ACT-CL\nJ0207.8+0209.",
        "Seniority-zero wavefunctions describe bond-breaking processes qualitatively.\nAs eigenvectors of a model Hamiltonian, Richardson-Gaudin states provide a\nclear physical picture and allow for systematic improvement via standard single\nreference approaches. Until now, this treatment has been done in the\nseniority-zero channel. In this manuscript, the corresponding states with\nhigher seniorities are identified, and their couplings through the Coulomb\nHamiltonian are computed. In every case, the couplings between the states are\ncomputed from the cofactors of their effective overlap matrix. Proof of\nprinciple calculations demonstrate that a single reference configuration\ninteraction is comparable with seniority-based configuration interaction\ncomputations at a substantially reduced cost. The next manuscript in this\nseries will identify the corresponding Slater-Condon rules and make the\ncomputations feasible."
      ]
    }
  },
  {
    "id":2411.08073,
    "research_type":"applied",
    "start_id":"b20",
    "start_title":"A primer on deep learning in genomics",
    "start_abstract":"Deep learning methods are a class of machine learning techniques capable of identifying highly complex patterns in large datasets. Here, we provide a perspective and primer on deep learning applications for genome analysis. We discuss successful applications in the fields of regulatory genomics, variant calling and pathogenicity scores. We include general guidance for how to effectively use deep learning methods as well as a practical guide to tools and resources. This primer is accompanied by an interactive online tutorial.",
    "start_categories":[
      "q-bio.GN"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b24"
      ],
      "title":[
        "DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome"
      ],
      "abstract":[
        "Abstract Motivation Deciphering the language of non-coding DNA is one fundamental problems in genome research. Gene regulatory code highly complex due to existence polysemy and distant semantic relationship, which previous informatics methods often fail capture especially data-scarce scenarios. Results To address this challenge, we developed a novel pre-trained bidirectional encoder representation, named DNABERT, global transferrable understanding genomic sequences based on up downstream nucleotide contexts. We compared DNABERT most widely used programs for genome-wide elements prediction demonstrate its ease use, accuracy efficiency. show that single transformers model can simultaneously achieve state-of-the-art performance promoters, splice sites transcription factor binding sites, after easy fine-tuning using small task-specific labeled data. Further, enables direct visualization nucleotide-level importance relationship within input better interpretability accurate identification conserved sequence motifs functional genetic variant candidates. Finally, with human even be readily applied other organisms exceptional performance. anticipate fined tuned many analyses tasks. Availability implementation The source code, pretrained finetuned are available at GitHub (https:\/\/github.com\/jerryji1993\/DNABERT). Supplementary information data Bioinformatics online."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Nonexistence of traveling wave solutions in the fractional Rosenau-Hyman\n  equation via homotopy perturbation method",
        "FORTE: An Open-Source System for Cost-Effective and Scalable\n  Environmental Monitoring",
        "UniCoRN: Unified Commented Retrieval Network with LMMs",
        "Combinatorial construction of symplectic 6-manifolds via bifibration\n  structures",
        "MIGE: A Unified Framework for Multimodal Instruction-Based Image\n  Generation and Editing",
        "Make Optimization Once and for All with Fine-grained Guidance",
        "Circular-polarization-selective perfect reflection from chiral\n  superconductors",
        "Forecasting Frontier Language Model Agent Capabilities",
        "Enhancing Pavement Sensor Data Acquisition for AI-Driven Transportation\n  Research",
        "Experimental demonstration of entanglement pumping with bosonic logical\n  qubits",
        "Interleaved Gibbs Diffusion for Constrained Generation",
        "PoisonedParrot: Subtle Data Poisoning Attacks to Elicit\n  Copyright-Infringing Content from Large Language Models",
        "Primordial Black Hole Formation via Inverted Bubble Collapse",
        "Multivariable Behavioral Change Modeling of Epidemics in the Presence of\n  Undetected Infections",
        "Reinforcement Learning for Quantum Control under Physical Constraints",
        "Melting of rods on a sphere via an intermediate hexatic phase",
        "Symmetric quasi-coherent sheaves",
        "Federated Dynamic Modeling and Learning for Spatiotemporal Data\n  Forecasting",
        "Strong Field QED, Astrophysics, and Laboratory Astrophysics",
        "Stellar occultation observations of (38628) Huya and its satellite: a\n  detailed look into the system",
        "Fluholoscopy. Compact and Simple Platform Combining Fluorescence and\n  Holographic Microscopy",
        "DejAIvu: Identifying and Explaining AI Art on the Web in Real-Time with\n  Saliency Maps",
        "Sequential HW-Aware Precoding: Over-the-air cancellation of HWI in\n  Downlink Cell-Free Massive MIMO with Serial Fronthaul",
        "Origin of time and probability in quantum cosmology",
        "The geometric impact of the quantum Hall interface on a cone",
        "Effective theories for nuclei at high energies",
        "Survey on Single-Image Reflection Removal using Deep Learning Techniques",
        "Artificially Generated Visual Scanpath Improves Multi-label Thoracic\n  Disease Classification in Chest X-Ray Images",
        "The (Exact) Price of Cardinality for Indivisible Goods: A Parametric\n  Perspective"
      ],
      "abstract":[
        "We apply the homotopy perturbation method to construct series solutions for\nthe fractional Rosenau-Hyman (fRH) equation and study their dynamics. Unlike\nthe classical RH equation where compactons arise from truncated periodic\nsolutions, we show that spatial nonlocality prevents the existence of\ncompactons, and therefore periodic traveling waves are considered. By\nasymptotic analyses involving the Mittag-Leffler function, it is shown that the\nquadratic fRH equation exhibits bifurcation with respect to the order of the\ntemporal fractional derivative, leading to the eventual pinning of wave\npropagation. Additionally, numerical results suggest potential finite time\nblow-up in the cubic fRH. While HPM proves effective in constructing analytic\nsolutions, we identify cases of divergence, underscoring the need for further\nresearch into its convergence properties and broader applicability.",
        "Forests are an essential part of our biosphere, regulating climate, acting as\na sink for greenhouse gases, and providing numerous other ecosystem services.\nHowever, they are negatively impacted by climatic stressors such as drought or\nheat waves. In this paper, we introduce FORTE, an open-source system for\nenvironmental monitoring with the aim of understanding how forests react to\nsuch stressors. It consists of two key components: (1) a wireless sensor\nnetwork (WSN) deployed in the forest for data collection, and (2) a Data\nInfrastructure for data processing, storage, and visualization. The WSN\ncontains a Central Unit capable of transmitting data to the Data Infrastructure\nvia LTE-M and several spatially independent Satellites that collect data over\nlarge areas and transmit them wirelessly to the Central Unit. Our prototype\ndeployments show that our solution is cost-effective compared to commercial\nsolutions, energy-efficient with sensor nodes lasting for several months on a\nsingle charge, and reliable in terms of data quality. FORTE's flexible\narchitecture makes it suitable for a wide range of environmental monitoring\napplications beyond forest monitoring. The contributions of this paper are\nthree-fold. First, we describe the high-level requirements necessary for\ndeveloping an environmental monitoring system. Second, we present an\narchitecture and prototype implementation of the requirements by introducing\nour FORTE platform and demonstrating its effectiveness through multiple field\ntests. Lastly, we provide source code, documentation, and hardware design\nartifacts as part of our open-source repository.",
        "Multimodal retrieval methods have limitations in handling complex,\ncompositional queries that require reasoning about the visual content of both\nthe query and the retrieved entities. On the other hand, Large Multimodal\nModels (LMMs) can answer with language to more complex visual questions, but\nwithout the inherent ability to retrieve relevant entities to support their\nanswers. We aim to address these limitations with UniCoRN, a Unified Commented\nRetrieval Network that combines the strengths of composed multimodal retrieval\nmethods and generative language approaches, going beyond Retrieval-Augmented\nGeneration (RAG). We introduce an entity adapter module to inject the retrieved\nmultimodal entities back into the LMM, so it can attend to them while\ngenerating answers and comments. By keeping the base LMM frozen, UniCoRN\npreserves its original capabilities while being able to perform both retrieval\nand text generation tasks under a single integrated framework. To assess these\nnew abilities, we introduce the Commented Retrieval task (CoR) and a\ncorresponding dataset, with the goal of retrieving an image that accurately\nanswers a given question and generate an additional textual response that\nprovides further clarification and details about the visual information. We\ndemonstrate the effectiveness of UniCoRN on several datasets showing\nimprovements of +4.5% recall over the state of the art for composed multimodal\nretrieval and of +14.9% METEOR \/ +18.4% BEM over RAG for commenting in CoR.",
        "A bifibration structure on a $6$-manifold is a map to either the complex\nprojective plane $\\mathbb{P}^2$ or a $\\mathbb{P}^1$-bundle over $\\mathbb{P}^1$,\nsuch that its composition with the projection to $\\mathbb{P}^1$ is a\n($6$-dimensional) Lefschetz fibration\/pencil, and its restriction to the\npreimage of a generic $\\mathbb{P}^1$-fiber is also a ($4$-dimensional)\nLefschetz fibration\/pencil. This object has been studied by Auroux, Katzarkov,\nSeidel, among others. From a pair consisting of a monodromy representation of a\nLefschetz fibration\/pencil on a $4$-manifold and a relation in a braid group,\nwhich are mutually compatible in an appropriate sense, we construct a\nbifibration structure on a closed symplectic $6$-manifold, producing the given\ncompatible pair as its monodromies. We further establish methods for computing\ntopological invariants of symplectic $6$-manifolds, including Chern numbers,\nfrom compatible pairs. Additionally, we provide an explicit example of a\ncompatible pair, conjectured to correspond to a bifibration structure derived\nfrom the degree-$2$ Veronese embedding of the $3$-dimensional complex\nprojective space. This example can be viewed as a higher-dimensional analogue\nof the lantern relation in the mapping class group of the four-punctured\nsphere. Our results not only extend the applicability of combinatorial\ntechniques to higher-dimensional symplectic geometry but also offer a unified\nframework for systematically exploring symplectic $6$-manifolds.",
        "Despite significant progress in diffusion-based image generation,\nsubject-driven generation and instruction-based editing remain challenging.\nExisting methods typically treat them separately, struggling with limited\nhigh-quality data and poor generalization. However, both tasks require\ncapturing complex visual variations while maintaining consistency between\ninputs and outputs. Therefore, we propose MIGE, a unified framework that\nstandardizes task representations using multimodal instructions. It treats\nsubject-driven generation as creation on a blank canvas and instruction-based\nediting as modification of an existing image, establishing a shared\ninput-output formulation. MIGE introduces a novel multimodal encoder that maps\nfree-form multimodal instructions into a unified vision-language space,\nintegrating visual and semantic features through a feature fusion mechanism.\nThis unification enables joint training of both tasks, providing two key\nadvantages: (1) Cross-Task Enhancement: By leveraging shared visual and\nsemantic representations, joint training improves instruction adherence and\nvisual consistency in both subject-driven generation and instruction-based\nediting. (2) Generalization: Learning in a unified format facilitates\ncross-task knowledge transfer, enabling MIGE to generalize to novel\ncompositional tasks, including instruction-based subject-driven editing.\nExperiments show that MIGE excels in both subject-driven generation and\ninstruction-based editing while setting a state-of-the-art in the new task of\ninstruction-based subject-driven editing. Code and model have been publicly\navailable at https:\/\/github.com\/Eureka-Maggie\/MIGE.",
        "Learning to Optimize (L2O) enhances optimization efficiency with integrated\nneural networks. L2O paradigms achieve great outcomes, e.g., refitting\noptimizer, generating unseen solutions iteratively or directly. However,\nconventional L2O methods require intricate design and rely on specific\noptimization processes, limiting scalability and generalization. Our analyses\nexplore general framework for learning optimization, called Diff-L2O, focusing\non augmenting sampled solutions from a wider view rather than local updates in\nreal optimization process only. Meanwhile, we give the related generalization\nbound, showing that the sample diversity of Diff-L2O brings better performance.\nThis bound can be simply applied to other fields, discussing diversity,\nmean-variance, and different tasks. Diff-L2O's strong compatibility is\nempirically verified with only minute-level training, comparing with other\nhour-levels.",
        "Integrating mirrors with magnetic components is crucial for constructing\nchiral optical cavities, which provide tunable platforms for\ntime-reversal-asymmetric light-matter interactions. Here, we introduce\nsingle-crystal circular-polarization-selective mirrors based on chiral\nsuperconductors, which break time-reversal symmetry by themselves eliminating\nthe need for additional components. We show that a\ncircular-polarization-selective perfect reflection (CSPR) occurs for\nstrong-coupling superconductors in the BCS-BEC crossover regime or beyond if\nthe optical Hall conductivity is significant in the unit of conductivity\nquantum per unit layer, $e^2\/ha_z$, where $a_z$ is the lattice constant along\nthe surface normal. While the optical Hall conductivity in chiral\nsuperconductors is typically tiny, we classify three routes to obtain a large\nvalue. We demonstrate the significant optical Hall conductivity and the\nresulting CSPR with two examples: (1) superconductivity in doped quantum Hall\ninsulators and (2) chiral pairing that preserves the Bogoliubov Fermi surfaces\nin the weak-pairing limit. We also discuss the application of our theory to the\nrecently discovered chiral superconducting phase in rhombohedral graphene. Our\ntheory reveals the potential of these classes of chiral superconductors as\npromising elements for building high-quality-factor terahertz chiral cavities.",
        "As Language Models (LMs) increasingly operate as autonomous agents,\naccurately forecasting their capabilities becomes crucial for societal\npreparedness. We evaluate six forecasting methods that predict downstream\ncapabilities of LM agents. We use \"one-step\" approaches that predict benchmark\nscores from input metrics like compute or model release date directly or\n\"two-step\" approaches that first predict an intermediate metric like the\nprincipal component of cross-benchmark performance (PC-1) and human-evaluated\ncompetitive Elo ratings. We evaluate our forecasting methods by backtesting\nthem on a dataset of 38 LMs from the OpenLLM 2 leaderboard. We then use the\nvalidated two-step approach (Release Date$\\to$Elo$\\to$Benchmark) to predict LM\nagent performance for frontier models on three benchmarks: SWE-Bench Verified\n(software development), Cybench (cybersecurity assessment), and RE-Bench (ML\nresearch engineering). Our forecast predicts that by the beginning of 2026,\nnon-specialized LM agents with low capability elicitation will reach a success\nrate of 54% on SWE-Bench Verified, while state-of-the-art LM agents will reach\nan 87% success rate. Our approach does not account for recent advances in\ninference-compute scaling and might thus be too conservative.",
        "Effective strategies for sensor data management are essential for advancing\ntransportation research, especially in the current data-driven era, due to the\nadvent of novel applications in artificial intelligence. This paper presents\ncomprehensive guidelines for managing transportation sensor data, encompassing\nboth archived static data and real-time data streams. The real-time system\narchitecture integrates various applications with data acquisition systems\n(DAQ). By deploying the in-house designed, open-source Avena software platform\nalongside the NATS messaging system as a secure communication broker, reliable\ndata exchange is ensured. While robust databases like TimescaleDB facilitate\norganized storage, visualization platforms like Grafana provide real-time\nmonitoring capabilities.\n  In contrast, static data standards address the challenges in handling\nunstructured, voluminous datasets. The standards advocate for a combination of\ncost-effective bulk cloud storage for unprocessed sensor data and relational\ndatabases for recording summarized analyses. They highlight the role of cloud\ndata transfer tools like FME for efficient migration of sensor data from local\nstorages onto the cloud. Further, integration of robust visualization tools\ninto the framework helps in deriving patterns and trends from these complex\ndatasets.\n  The proposals were applied to INDOT's real-world case studies involving the\nI-65 and I-69 Greenfield districts. For real-time data collection, Campbell\nScientific DAQ systems were used, enabling continuous generation and monitoring\nof sensor metrics. In the case of the archived I-69 database, summary data was\ncompiled in Oracle, while the unprocessed data was stored in SharePoint. The\nresults underline the effectiveness of the proposed guidelines and motivate\ntheir adoption in research projects.",
        "Entanglement is crucial for quantum networks and computation, yet maintaining\nhigh-fidelity entangled quantum states is hindered by decoherence and\nresource-intensive purification methods. Here, we experimentally demonstrate\nentanglement pumping, utilizing bosonic quantum error correction (QEC) codes as\nlong-coherence-time storage qubits. By repetitively generating entanglement\nwith short-coherence-time qubits and injecting it into QEC-protected logical\nqubits, our approach effectively preserves entanglement. Through error\ndetection to discard error states and entanglement pumping to mitigate errors\nwithin the code space, we extend the lifespan of entangled logical qubits by\nnearly 50% compared to the case without entanglement pumping. This work\nhighlights the potential of bosonic logical qubits for scalable quantum\nnetworks and introduces a novel paradigm for efficient entanglement management.",
        "We introduce Interleaved Gibbs Diffusion (IGD), a novel generative modeling\nframework for mixed continuous-discrete data, focusing on constrained\ngeneration problems. Prior works on discrete and continuous-discrete diffusion\nmodels assume factorized denoising distribution for fast generation, which can\nhinder the modeling of strong dependencies between random variables encountered\nin constrained generation. IGD moves beyond this by interleaving continuous and\ndiscrete denoising algorithms via a discrete time Gibbs sampling type Markov\nchain. IGD provides flexibility in the choice of denoisers, allows conditional\ngeneration via state-space doubling and inference time scaling via the\nReDeNoise method. Empirical evaluations on three challenging tasks-solving\n3-SAT, generating molecule structures, and generating layouts-demonstrate\nstate-of-the-art performance. Notably, IGD achieves a 7% improvement on 3-SAT\nout of the box and achieves state-of-the-art results in molecule generation\nwithout relying on equivariant diffusion or domain-specific architectures. We\nexplore a wide range of modeling, and interleaving strategies along with\nhyperparameters in each of these problems.",
        "As the capabilities of large language models (LLMs) continue to expand, their\nusage has become increasingly prevalent. However, as reflected in numerous\nongoing lawsuits regarding LLM-generated content, addressing copyright\ninfringement remains a significant challenge. In this paper, we introduce\nPoisonedParrot: the first stealthy data poisoning attack that induces an LLM to\ngenerate copyrighted content even when the model has not been directly trained\non the specific copyrighted material. PoisonedParrot integrates small fragments\nof copyrighted text into the poison samples using an off-the-shelf LLM. Despite\nits simplicity, evaluated in a wide range of experiments, PoisonedParrot is\nsurprisingly effective at priming the model to generate copyrighted content\nwith no discernible side effects. Moreover, we discover that existing defenses\nare largely ineffective against our attack. Finally, we make the first attempt\nat mitigating copyright-infringement poisoning attacks by proposing a defense:\nParrotTrap. We encourage the community to explore this emerging threat model\nfurther.",
        "We propose a novel mechanism of primordial black hole (PBH) formation through\ninverted bubble collapse. In this scenario, bubbles nucleate sparsely in an\nincomplete first-order phase transition, followed by a bulk phase transition in\nthe rest of the universe that inverts these pre-existing bubbles into false\nvacuum regions. These spherically symmetric false-vacuum bubbles subsequently\ncollapse to form PBHs. Unlike conventional PBH formation mechanisms associated\nwith domain wall collapse or bubble coalescence, our inverted bubble collapse\nmechanism naturally ensures spherical collapse. We demonstrate that, when\napplied to the singlet extension of the Standard Model, this mechanism can\nproduce highly monochromatic PBHs with masses up to ${\\cal\nO}(10^{-7}\\,\\text{-}\\,10^{-5}) M_\\odot$, which potentially explain the\nmicrolensing events observed in the OGLE and Subaru HSC data.",
        "Epidemic models are invaluable tools to understand and implement strategies\nto control the spread of infectious diseases, as well as to inform public\nhealth policies and resource allocation. However, current modeling approaches\nhave limitations that reduce their practical utility, such as the exclusion of\nhuman behavioral change in response to the epidemic or ignoring the presence of\nundetected infectious individuals in the population. These limitations became\nparticularly evident during the COVID-19 pandemic, underscoring the need for\nmore accurate and informative models. Motivated by these challenges, we develop\na novel Bayesian epidemic modeling framework to better capture the complexities\nof disease spread by incorporating behavioral responses and undetected\ninfections. In particular, our framework makes three contributions: 1)\nleveraging additional data on hospitalizations and deaths in modeling the\ndisease dynamics, 2) accounting data uncertainty arising from the large\npresence of asymptomatic and undetected infections, and 3) allowing the\npopulation behavioral change to be dynamically influenced by multiple data\nsources (cases and deaths). We thoroughly investigate the properties of the\nproposed model via simulation, and illustrate its utility on COVID-19 data from\nMontreal and Miami.",
        "Quantum optimal control is concerned with the realisation of desired dynamics\nin quantum systems, serving as a linchpin for advancing quantum technologies\nand fundamental research. Analytic approaches and standard optimisation\nalgorithms do not yield satisfactory solutions for large quantum systems, and\nespecially not for real world quantum systems which are open and noisy. We\ndevise a physics-informed Reinforcement Learning (RL) algorithm that restricts\nthe space of possible solutions. We incorporate priors about the desired time\nscales of the quantum state dynamics - as well as realistic control signal\nlimitations - as constraints to the RL algorithm. These physics-informed\nconstraints additionally improve computational scalability by facilitating\nparallel optimisation. We evaluate our method on three broadly relevant quantum\nsystems (multi-level $\\Lambda$ system, Rydberg atom and superconducting\ntransmon) and incorporate real-world complications, arising from dissipation\nand control signal perturbations. We achieve both higher fidelities - which\nexceed 0.999 across all systems - and better robustness to time-dependent\nperturbations and experimental imperfections than previous methods. Lastly, we\ndemonstrate that incorporating multi-step feedback can yield solutions robust\neven to strong perturbations.",
        "We have studied, using molecular dynamics simulations, the pressure-induced\nmelting in a monolayer of soft repulsive spherocylinders whose centers of mass\nare constrained to move on the surface of a sphere. We show that the\norientational degrees of freedom of the spherocylinders exhibit nematic order,\nwhereas the positions of their centers of mass exhibit melting transitions that\ndepend on the radius of the confining spherical surface. Our system presents a\nunique scenario where the decoupling of the orientational degrees of freedom\nfrom the positional degrees of freedom leads to an effectively two-dimensional\n(2D) crystal-to-liquid transition on a spherical surface. Further study of the\nnature of this 2D melting on a sphere shows that the transition is a two-step\nprocess, and there exists a very small window of an intermediate hexatic phase\nbetween crystal and liquid phases. Similar results are found for flat\nmonolayers (with the radius of the sphere $ R \\rightarrow \\infty $). We show\nthat, interestingly, the structure of the defects, originating from the\ncurvature of the substrate, also changes during melting.",
        "Using methods of stable homotopy theory, the category of symmetric\nquasi-coherent sheaves associated with non-commutative graded algebras with\nextra symmetries is introduced and studied in this paper. It is shown to be a\nclosed symmetric monoidal Grothendieck category with invertible generators. It\nis proven that the category of quasi-coherent sheaves on a projective scheme is\nrecovered out of symmetric quasi-coherent sheaves.",
        "This paper presents an advanced Federated Learning (FL) framework for\nforecasting complex spatiotemporal data, improving upon recent state-of-the-art\nmodels. In the proposed approach, the original Gated Recurrent Unit (GRU)\nmodule within previous Dynamic Spatial--Temporal Graph Convolutional Recurrent\nNetwork (DSTGCRN) modeling is first replaced with a Long Short-Term Memory\n(LSTM) network, enabling the resulting model to more effectively capture\nlong-term dependencies inherent to time series data. The resulting architecture\nsignificantly improves the model's capacity to handle complex temporal patterns\nin diverse forecasting applications. Furthermore, the proposed FL framework\nintegrates a novel Client-Side Validation (CSV) mechanism, introducing a\ncritical validation step at the client level before incorporating aggregated\nparameters from the central server into local models. This ensures that only\nthe most effective updates are adopted, improving both the robustness and\naccuracy of the forecasting model across clients. The efficiency of our\napproach is demonstrated through extensive experiments on real-world\napplications, including public datasets for multimodal transport demand\nforecasting and private datasets for Origin-Destination (OD) matrix forecasting\nin urban areas. The results demonstrate substantial improvements over\nconventional methods, highlighting the framework's ability to capture complex\nspatiotemporal dependencies while preserving data privacy. This work not only\nprovides a scalable and privacy-preserving solution for real-time,\nregion-specific forecasting and management but also underscores the potential\nof leveraging distributed data sources in a FL context. We provide our\nalgorithms as open-source on GitHub.",
        "Astrophysical compact objects, such as magnetars, neutron star mergers, etc,\nhave strong electromagnetic fields beyond the Schwinger field ($B_c = 4.4\n\\times 10^{13}\\, {\\rm G}$). In strong electric fields, electron-positron pairs\nare produced from the vacuum, gamma rays create electron-positron pairs in\nstrong magnetic fields, and propagating photons experience vacuum refringence,\netc. Astrophysical compact objects with strong electromagnetic fields open a\nwindow for probing fundamental physics beyond weak field QED. Ultra-intense\nlasers and high-energy charged particles may simulate extreme astrophysical\nphenomena.",
        "The physical and orbital parameters of Trans-Neptunian Objects (TNOs) provide\nvaluable information about the Solar System's formation and evolution. In\nparticular, the characterization of binaries provides insights into the\nformation mechanisms that may be playing a role at such large distances from\nthe Sun. Studies show two distinct populations, and (38628) Huya occupies an\nintermediate position between the unequal-size binaries and those with\ncomponents of roughly equal sizes. In this work, we predicted and observed\nthree stellar occultation events by Huya. Huya and its satellite - S\/2012\n(38628) 1 - were detected during occultations in March 2021 and again in June\n2023. Additionally, an attempt to detect Huya in February 2023 resulted in an\nadditional single-chord detection of the secondary. A spherical body with a\nminimum diameter of D = 165 km can explain the three single-chord observations\nand provide a lower limit for the satellite size. The astrometry of Huya's\nsystem, as derived from the occultations and supplemented by observations from\nthe Hubble Space Telescope and Keck Observatory, provided constraints on the\nsatellite orbit and the mass of the system. Therefore, assuming the secondary\nis in an equatorial orbit around the primary, the limb fitting was constrained\nby the satellite orbit position angle. The system density, calculated by\nsumming the most precise measurement of Huya's volume to the spherical\nsatellite average volume, is $\\rho_{1}$ = 1073 $\\pm$ 66 kg m$^{-3}$. The\ndensity that the object would have assuming a Maclaurin equilibrium shape with\na rotational period of 6.725 $\\pm$ 0.01 hours is $\\rho_{2}$ = 768 $\\pm$ 42 kg\nm$^{-3}$. This difference rules out the Maclaurin equilibrium assumption for\nthe main body shape.",
        "The combination of different imaging modalities into single imaging platforms\nhas a strong potential in biomedical sciences since it permits the analysis of\ncomplementary properties of the target sample. Here, we report on an extremely\nsimple, cost-effective, and compact microscope platform for achieving\nsimultaneous fluorescence and quantitative phase imaging modes with the\ncapability of working in a single snapshot. It is based on the use of a single\nillumination wavelength to both excite the sample fluorescence and provide\ncoherent illumination for phase imaging. After passing the microscope layout,\nthe two imaging paths are separated by using a bandpass filter and the two\nimaging modes are simultaneously obtained by using two digital cameras. We\nfirst present calibration and analysis of both fluorescence and phase imaging\nmodalities working independently and, later on, experimental validation for the\nproposed common-path dual-mode imaging platform considering static (resolution\ntest targets, fluorescent micro-beads and water-suspended lab-made cultures) as\nwell as dynamic (flowing fluorescent beads, human sperm cells and live\nspecimens from lab-made cultures) samples.",
        "The recent surge in advanced generative models, such as diffusion models and\ngenerative adversarial networks (GANs), has led to an alarming rise in\nAI-generated images across various domains on the web. While such technologies\noffer benefits such as democratizing artistic creation, they also pose\nchallenges in misinformation, digital forgery, and authenticity verification.\nAdditionally, the uncredited use of AI-generated images in media and marketing\nhas sparked significant backlash from online communities. In response to this,\nwe introduce DejAIvu, a Chrome Web extension that combines real-time\nAI-generated image detection with saliency-based explainability while users\nbrowse the web. Using an ONNX-optimized deep learning model, DejAIvu\nautomatically analyzes images on websites such as Google Images, identifies\nAI-generated content using model inference, and overlays a saliency heatmap to\nhighlight AI-related artifacts. Our approach integrates efficient in-browser\ninference, gradient-based saliency analysis, and a seamless user experience,\nensuring that AI detection is both transparent and interpretable. We also\nevaluate DejAIvu across multiple pretrained architectures and benchmark\ndatasets, demonstrating high accuracy and low latency, making it a practical\nand deployable tool for enhancing AI image accountability. The code for this\nsystem can be found at https:\/\/github.com\/Noodulz\/dejAIvu.",
        "This paper addresses the critical challenge of mitigating hardware\nimpairments (HWIs) in downlink cell-free massive MIMO (CF-mMIMO) networks while\nensuring computational scalability. We propose a novel sequential\nhardware-aware (HW-aware) precoding technique that leverages the serial\nfronthaul topology to perform over-the-air HWI cancellation. This approach\ninvolves sequentially exchanging approximated user-perceived distortion\ninformation among successive access points (APs) for over-the-air HWI\nmitigation. Each AP independently computes its spatial multiplexing weights and\ntransmits signals that counteract the distortions introduced by the preceding\nAP. We develop a problem formulation and present a closed-form solution for\nthis method. For performance evaluation, we study two reference methods taken\neither from centralized massive MIMO literature (Tone Reservation [TR]) or\ntailored for CF-mMIMO networks (PAPR-aware precoding), both focusing on\nreducing the PAPR of OFDM signals in the downlink. Results indicate that the\nsequential HW-aware approach achieves a substantial increase in spectral\nefficiency (SE) in high-distortion scenarios, with an average SE increase\nfactor of 1.8 under severe distortions. Additionally, the proposed method,\nwhich is executed locally, demonstrates better scalability, achieving a\nreduction of up to 40\\% and 72\\% in the total number of complex multiplications\ncompared to the PAPR-aware and TR approaches, respectively. Finally, the\nsequential HW-aware precoder offers high performance even when applied to\ncost-effective APs with few antennas, presenting a promising and practical\nsolution for HWI compensation in CF-mMIMO systems with serial fronthaul.",
        "We discuss how the classical notions of time and causal structure may emerge\ntogether with quantum-mechanical probabilities from a universal quantum state.\nFor this, the process of decoherence between semiclassical branches is\nimportant. Our discussion is based on quantum geometrodynamics, a canonical\napproach to quantum gravity. In this framework, a particular boundary condition\nmay illuminate the issue of the arrow of (classical) time in connection to the\ngrowth of entanglement entropy.",
        "Recently, quantum Hall interface has become a popular subject of research;\ndistinct from that of the quantum Hall edge, which is constrained by external\nbackground confinement, the interface has the freedom to move, likely towards a\nstring-like state. In disk geometry, it was known that the interface energy has\nan extra correction due to its curvature which depends on the size of the disk.\nIn this work, we analytically calculate the energy of the integer quantum Hall\ninterface on a cone surface which has the advantage that its curvature is more\neasily adjustable. By tuning the length and curvature of the interface by the\ncone angle parameter $\\beta$, we analyze the dependence of the quantum Hall\ninterface energy on the curvature and verify this geometric correction.\nMoreover, we find that the tip of the cone geometry has an extra contribution\nto the energy that reflects on the $u_2,u_4$ term.",
        "We discuss the application of the Color Glass Condensate (CGC), an effective\nfield theory of Quantum Chromodynamics (QCD), to describe high-energy nuclear\ninteractions. We first provide an introduction to the methods and language of\nthe CGC, its role in understanding gluon saturation in heavy-ion collisions at\nthe LHC and RHIC, and its relevance in various scattering processes such as\nDeep Inelastic Scattering (DIS). The application of the CGC effective field\ntheory to describe hadron-hadron collisions is discussed in the scope of\nasymmetric \\textit{dilute-dense} collisions, and Heavy-Ion Collisions in the\n\\textit{dense-dense} limit. The review covers theoretical foundations, recent\nadvancements, and phenomenological applications, focusing on using the CGC to\ndetermine the initial conditions of heavy-ion collisions.",
        "The phenomenon of reflection is quite common in digital images, posing\nsignificant challenges for various applications such as computer vision,\nphotography, and image processing. Traditional methods for reflection removal\noften struggle to achieve clean results while maintaining high fidelity and\nrobustness, particularly in real-world scenarios. Over the past few decades,\nnumerous deep learning-based approaches for reflection removal have emerged,\nyielding impressive results. In this survey, we conduct a comprehensive review\nof the current literature by focusing on key venues such as ICCV, ECCV, CVPR,\nNeurIPS, etc., as these conferences and journals have been central to advances\nin the field. Our review follows a structured paper selection process, and we\ncritically assess both single-stage and two-stage deep learning methods for\nreflection removal. The contribution of this survey is three-fold: first, we\nprovide a comprehensive summary of the most recent work on single-image\nreflection removal; second, we outline task hypotheses, current deep learning\ntechniques, publicly available datasets, and relevant evaluation metrics; and\nthird, we identify key challenges and opportunities in deep learning-based\nreflection removal, highlighting the potential of this rapidly evolving\nresearch area.",
        "Expert radiologists visually scan Chest X-Ray (CXR) images, sequentially\nfixating on anatomical structures to perform disease diagnosis. An automatic\nmulti-label classifier of diseases in CXR images can benefit by incorporating\naspects of the radiologists' approach. Recorded visual scanpaths of\nradiologists on CXR images can be used for the said purpose. But, such\nscanpaths are not available for most CXR images, which creates a gap even for\nmodern deep learning based classifiers. This paper proposes to mitigate this\ngap by generating effective artificial visual scanpaths using a visual scanpath\nprediction model for CXR images. Further, a multi-class multi-label classifier\nframework is proposed that uses a generated scanpath and visual image features\nto classify diseases in CXR images. While the scanpath predictor is based on a\nrecurrent neural network, the multi-label classifier involves a novel iterative\nsequential model with an attention module. We show that our scanpath predictor\ngenerates human-like visual scanpaths. We also demonstrate that the use of\nartificial visual scanpaths improves multi-class multi-label disease\nclassification results on CXR images. The above observations are made from\nexperiments involving around 0.2 million CXR images from 2 widely-used datasets\nconsidering the multi-label classification of 14 pathological findings. Code\nlink: https:\/\/github.com\/ashishverma03\/SDC",
        "We adopt a parametric approach to analyze the worst-case degradation in\nsocial welfare when the allocation of indivisible goods is constrained to be\nfair. Specifically, we are concerned with cardinality-constrained allocations,\nwhich require that each agent has at most $k$ items in their allocated bundle.\nWe propose the notion of the price of cardinality, which captures the\nworst-case multiplicative loss of utilitarian or egalitarian social welfare\nresulting from imposing the cardinality constraint. We then characterize tight\nor almost-tight bounds on the price of cardinality as exact functions of the\ninstance parameters, demonstrating how the social welfare improves as $k$ is\nincreased. In particular, one of our main results refines and generalizes the\nexisting asymptotic bound on the price of balancedness, as studied by Bei et\nal. [BLMS21]. We also further extend our analysis to the problem where the\nitems are partitioned into disjoint categories, and each category has its own\ncardinality constraint. Through a parametric study of the price of cardinality,\nwe provide a framework which aids decision makers in choosing an ideal level of\ncardinality-based fairness, using their knowledge of the potential loss of\nutilitarian and egalitarian social welfare."
      ]
    }
  },
  {
    "id":2411.08664,
    "research_type":"applied",
    "start_id":"b2",
    "start_title":"Comprehensive Inorganic Chemistry III (Third Edition)",
    "start_abstract":"Comprehensive Inorganic Chemistry III, a ten-volume reference work, is intended to cover fundamental principles, recent discoveries, and significant applications of elements and their compounds. Authored by renowned experts in the field and edited by a world-class editorial board, each chapter provides a thorough and in-depth overview of the topic covered, featuring resources which will be useful to students, researchers, faculty as well as those in the industry. Comprehensive Inorganic Chemistry III focuses on main group chemistry, biological inorganic chemistry, solid state and materials chemistry, catalysis, and new developments in electrochemistry and photochemistry, as well as NMR and diffraction methods for studying inorganic compounds. The work expands on our 2013 work Comprehensive Inorganic Chemistry II while also adding new volumes on cutting-edge research areas and techniques for studying inorganic compounds. Researchers seeking background information on a specific problem involving the synthesis of inorganic compounds, as well as applications for numerous elements from the periodic table, and their compounds, will be able to rely on and refer to this authoritative scientific resource time and again.",
    "start_categories":[
      "Material"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "Multimodal Foundation Models for Material Property Prediction and Discovery"
      ],
      "abstract":[
        "Artificial intelligence is transforming computational materials science, improving the prediction of material properties, and accelerating the discovery of novel materials. Recently, publicly available material data repositories have grown rapidly. This growth encompasses not only more materials but also a greater variety and quantity of their associated properties. Existing machine learning efforts in materials science focus primarily on single-modality tasks, i.e. relationships between materials and a single physical property, thus not taking advantage of the rich and multimodal set of material properties. Here, we introduce Multimodal Learning for Materials (MultiMat), which enables self-supervised multi-modality training of foundation models for materials. We demonstrate our framework's potential using data from the Materials Project database on multiple axes: (i) MultiMat achieves state-of-the-art performance for challenging material property prediction tasks; (ii) MultiMat enables novel and accurate material discovery via latent space similarity, enabling screening for stable materials with desired properties; and (iii) MultiMat encodes interpretable emergent features that may provide novel scientific insights."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Group Shapley with Robust Significance Testing and Its Application to\n  Bond Recovery Rate Prediction",
        "Streaming Self-Corrected Dual-Comb Spectrometer",
        "A Scenario Analysis of Ethical Issues in Dark Patterns and Their\n  Research",
        "On the lattice formulation of the union-closed sets conjecture",
        "Generative AI for Vision: A Comprehensive Study of Frameworks and\n  Applications",
        "Towards More Economical Context-Augmented LLM Generation by Reusing\n  Stored KV Cache",
        "Dynamical Landauer principle: Thermodynamic criteria of transmitting\n  classical information",
        "Mitigation of birefringence in cavity-based quantum networks using\n  frequency-encoded photons",
        "Efficient Finetuning for Dimensional Speech Emotion Recognition in the\n  Age of Transformers",
        "The Muddy Waters of Modeling Empathy in Language: The Practical Impacts\n  of Theoretical Constructs",
        "Nanosatellite Constellation and Ground Station Co-design for Low-Latency\n  Critical Event Detection",
        "Robust Body Composition Analysis by Generating 3D CT Volumes from\n  Limited 2D Slices",
        "Smoothing Out Hallucinations: Mitigating LLM Hallucination with Smoothed\n  Knowledge Distillation",
        "Asymmetrical Latent Representation for Individual Treatment Effect\n  Modeling",
        "Probing negative differential resistance in silicon with a P-I-N\n  diode-integrated T center ensemble",
        "Epidemic Forecasting with a Hybrid Deep Learning Method Using CNN-LSTM\n  With WOA-GWO Parameter Optimization: Global COVID-19 Case Study",
        "LoRE-Merging: Exploring Low-Rank Estimation For Large Language Model\n  Merging",
        "Center-guided Classifier for Semantic Segmentation of Remote Sensing\n  Images",
        "A dynamic copula model for probabilistic forecasting of non-Gaussian\n  multivariate time series",
        "An Empirical Analysis of Uncertainty in Large Language Model Evaluations",
        "Black holes in thermal bath live shorter: implications for primordial\n  black holes",
        "Employing deep-learning techniques for the conservative-to-primitive\n  recovery in binary neutron star simulations",
        "Pure momentum-shift bulk photovoltaic effect in ferroelectric flat-band\n  Mott insulators",
        "Leveraging Large Language Models for Building Interpretable Rule-Based\n  Data-to-Text Systems",
        "Exoplanet Occurrence Rate with Age for FGK Stars in Kepler",
        "MagicID: Hybrid Preference Optimization for ID-Consistent and\n  Dynamic-Preserved Video Customization",
        "Model-based Elaboration of a Requirements and Design Pattern Catalogue\n  for Sustainable Systems",
        "MetricGrids: Arbitrary Nonlinear Approximation with Elementary Metric\n  Grids based Implicit Neural Representation",
        "Hitting probabilities, thermal capacity, and Hausdorff dimension results\n  for the Brownian sheet"
      ],
      "abstract":[
        "We propose Group Shapley, a metric that extends the classical\nindividual-level Shapley value framework to evaluate the importance of feature\ngroups, addressing the structured nature of predictors commonly found in\nbusiness and economic data. More importantly, we develop a significance testing\nprocedure based on a three-cumulant chi-square approximation and establish the\nasymptotic properties of the test statistics for Group Shapley values. Our\napproach can effectively handle challenging scenarios, including sparse or\nskewed distributions and small sample sizes, outperforming alternative tests\nsuch as the Wald test. Simulations confirm that the proposed test maintains\nrobust empirical size and demonstrates enhanced power under diverse conditions.\nTo illustrate the method's practical relevance in advancing Explainable AI, we\napply our framework to bond recovery rate predictions using a global dataset\n(1996-2023) comprising 2,094 observations and 98 features, grouped into 16\nsubgroups and five broader categories: bond characteristics, firm fundamentals,\nindustry-specific factors, market-related variables, and macroeconomic\nindicators. Our results identify the market-related variables group as the most\ninfluential. Furthermore, Lorenz curves and Gini indices reveal that Group\nShapley assigns feature importance more equitably compared to individual\nShapley values.",
        "Here, we radically simplify coherently averaged dual-comb spectroscopy by\nintroducing a real-time self-correction system: a radio frequency\nsystem-on-chip computes each incoming dual-comb interferogram's phase,\nfrequency, and arrival time, calculates changes in the combs' carrier-envelope\noffset frequencies and repetition rates, and immediately phase-corrects the\nincoming interferogram data stream. The algorithm supports up to 0.3 GHz\ninterferogram frequency bandwidth and thus combines fast measurement times\n(corresponding to high detunings) with broadband optical detection. Using the\nsystem, we achieve comb-resolved spectroscopy with Fourier-limited linewidth,\ncoherent averaging over arbitrarily long durations, and a signal-to-noise ratio\nof up to 2100. Iodine and acetylene spectroscopy yields excellent agreement\nwith literature over an optical bandwidth of more than 10 THz in the visible\nand near-infrared. Our approach only requires three optical and three\nelectronic components and makes instantaneous dual-comb spectroscopy available\nto everyday applications.",
        "Context: Dark patterns are user interface or other software designs that\ndeceive or manipulate users to do things they would not otherwise do. Even\nthough dark patterns have been under active research for a long time, including\nparticularly in computer science but recently also in other fields such as law,\nsystematic applied ethical assessments have generally received only a little\nattention. Objective: The present work evaluates ethical concerns in dark\npatterns and their research in software engineering and closely associated\ndisciplines. The evaluation is extended to cover not only dark patterns\nthemselves but also the research ethics and applied ethics involved in\nstudying, developing, and deploying them. Method: A scenario analysis is used\nto evaluate six theoretical dark pattern scenarios. The ethical evaluation is\ncarried out by focusing on the three main branches of normative ethics;\nutilitarianism, deontology, and virtue ethics. In terms of deontology, the\nevaluation is framed and restricted to the laws enacted in the European Union.\nResults: The evaluation results indicate that dark patterns are not universally\nmorally bad. That said, numerous ethical issues with practical relevance are\ndemonstrated and elaborated. Some of these may have societal consequences.\nConclusion: Dark patterns are ethically problematic but not always. Therefore,\nethical assessments are necessary. The two main theoretical concepts behind\ndark patterns, deception and manipulation, lead to various issues also in\nresearch ethics. It can be recommended that dark patterns should be evaluated\non case-by-case basis, considering all of the three main branches of normative\nethics in an evaluation. Analogous points apply to legal evaluations,\nespecially when considering that the real or perceived harms caused by dark\npatterns cover both material and non-material harms to natural persons.",
        "The union-closed sets conjecture, also known as Frankl's conjecture, is a\nwell-studied problem with various formulations. In terms of lattices, the\nconjecture states that every finite lattice $L$ with more than one element\ncontains a join-irreducible element that is less than or equal to at most half\nof the elements in $L$. In this work, we obtain several necessary conditions\nfor any counterexample $\\tilde{L}$ of minimum size.",
        "Generative AI is transforming image synthesis, enabling the creation of\nhigh-quality, diverse, and photorealistic visuals across industries like\ndesign, media, healthcare, and autonomous systems. Advances in techniques such\nas image-to-image translation, text-to-image generation, domain transfer, and\nmultimodal alignment have broadened the scope of automated visual content\ncreation, supporting a wide spectrum of applications. These advancements are\ndriven by models like Generative Adversarial Networks (GANs), conditional\nframeworks, and diffusion-based approaches such as Stable Diffusion. This work\npresents a structured classification of image generation techniques based on\nthe nature of the input, organizing methods by input modalities like noisy\nvectors, latent representations, and conditional inputs. We explore the\nprinciples behind these models, highlight key frameworks including DALL-E,\nControlNet, and DeepSeek Janus-Pro, and address challenges such as\ncomputational costs, data biases, and output alignment with user intent. By\noffering this input-centric perspective, this study bridges technical depth\nwith practical insights, providing researchers and practitioners with a\ncomprehensive resource to harness generative AI for real-world applications.",
        "Across large language model (LLM) applications, we observe an emerging trend\nfor reusing KV caches to save the prefill delays of processing repeated input\ntexts in different LLM inputs. This has led to a broad design space, including\ncolocating stored KV caches with (or close to) GPUs to various KV cache\ncompression. However, a key question remains unanswered: can these delay\nreductions also be economically favorable? Specifically, we ask whether a\ndeveloper can use public cloud services to store precomputed KV caches and\nreuse them to save delay without incurring more costs in terms of compute,\nstorage, and network. To answer this question, we propose an validated\nanalytical model for the cloud cost (in compute, storage, and network) of\nstoring and reusing KV caches based on various workload parameters, such as\nreuse frequency, generated text lengths, model sizes, etc. Preliminary results\nshow that KV cache reusing is able to save both delay and cloud cost across a\nrange of workloads with long context. And we call more efforts on building more\neconomical context augmented LLM by KV cache reusing.",
        "Transmitting energy and information are two essential aspects of nature.\nRecent findings suggest they are closely related, while a quantitative\nequivalence between them is still unknown. This thus motivates us to ask: Can\ninformation transmission tasks equal certain energy transmission tasks? We\nanswer this question positively by bounding various one-shot classical\ncapacities via different energy transmission tasks. Such bounds provide the\nphysical implication that, in the one-shot regime, transmitting $n$ bits of\nclassical information is equivalent to $n\\times k_BT\\ln2$ transmitted energy.\nUnexpectedly, these bounds further uncover a dynamical version of Landauer's\nprinciple, showing the strong link between \"transmitting\" (rather than\n\"erasing\") information and energy. Finally, in the asymptotic regime, our\nfindings further provide thermodynamic meanings for\nHolevo-Schumacher-Westmoreland Theorem and a series of strong converse\nproperties as well as no-go theorems.",
        "Atom-cavity systems offer unique advantages for building large-scale\ndistributed quantum computers by providing strong atom-photon coupling while\nallowing for high-fidelity local operations of atomic qubits. However, in\nprevalent schemes where the photonic state is encoded in polarization, cavity\nbirefringence introduces an energy splitting of the cavity eigenmodes and\nalters the polarization states, thus limiting the fidelity of remote\nentanglement generation. To address this challenge, we propose a scheme that\nencodes the photonic qubit in the frequency degree-of-freedom. The scheme\nrelies on resonant coupling of multiple transverse cavity modes to different\natomic transitions that are well-separated in frequency. We numerically\ninvestigate the temporal properties of the photonic wavepacket, two-photon\ninterference visibility, and atom-atom entanglement fidelity under various\ncavity polarization-mode splittings and find that our scheme is less affected\nby cavity birefringence. Finally, we propose practical implementations in two\ntrapped ion systems, using the fine structure splitting in the metastable D\nstate of $\\mathrm{^{40}Ca^{+}}$, and the hyperfine splitting in the ground\nstate of $\\mathrm{^{225}Ra^{+}}$. Our study presents an alternative approach\nfor cavity-based quantum networks that is less sensitive to birefringent\neffects, and is applicable to a variety of atomic and solid-state\nemitter-cavity interfaces.",
        "Accurate speech emotion recognition is essential for developing human-facing\nsystems. Recent advancements have included finetuning large, pretrained\ntransformer models like Wav2Vec 2.0. However, the finetuning process requires\nsubstantial computational resources, including high-memory GPUs and significant\nprocessing time. As the demand for accurate emotion recognition continues to\ngrow, efficient finetuning approaches are needed to reduce the computational\nburden. Our study focuses on dimensional emotion recognition, predicting\nattributes such as activation (calm to excited) and valence (negative to\npositive). We present various finetuning techniques, including full finetuning,\npartial finetuning of transformer layers, finetuning with mixed precision,\npartial finetuning with caching, and low-rank adaptation (LoRA) on the Wav2Vec\n2.0 base model. We find that partial finetuning with mixed precision achieves\nperformance comparable to full finetuning while increasing training speed by\n67%. Caching intermediate representations further boosts efficiency, yielding\nan 88% speedup and a 71% reduction in learnable parameters. We recommend\nfinetuning the final three transformer layers in mixed precision to balance\nperformance and training efficiency, and adding intermediate representation\ncaching for optimal speed with minimal performance trade-offs. These findings\nlower the barriers to finetuning speech emotion recognition systems, making\naccurate emotion recognition more accessible to a broader range of researchers\nand practitioners.",
        "Conceptual operationalizations of empathy in NLP are varied, with some having\nspecific behaviors and properties, while others are more abstract. How these\nvariations relate to one another and capture properties of empathy observable\nin text remains unclear. To provide insight into this, we analyze the transfer\nperformance of empathy models adapted to empathy tasks with different\ntheoretical groundings. We study (1) the dimensionality of empathy definitions,\n(2) the correspondence between the defined dimensions and measured\/observed\nproperties, and (3) the conduciveness of the data to represent them, finding\nthey have a significant impact to performance compared to other transfer\nsetting features. Characterizing the theoretical grounding of empathy tasks as\ndirect, abstract, or adjacent further indicates that tasks that directly\npredict specified empathy components have higher transferability. Our work\nprovides empirical evidence for the need for precise and multidimensional\nempathy operationalizations.",
        "Advancements in nanosatellite technology lead to more Earth-observation\nsatellites in low-Earth orbit. We explore using nanosatellite constellations to\nachieve low-latency detection for time-critical events, such as forest fires,\noil spills, and floods. The detection latency comprises three parts: capture,\ncompute and transmission. Previous solutions reduce transmission latency, but\nwe find that the bottleneck is capture latency, accounting for more than 90% of\nend-to-end latency. We present a measurement study on how various satellite and\nground station design factors affect latency. We offer design guidance to\noperators on how to choose satellite orbital configurations and design an\nalgorithm to choose ground station locations. For six use cases, our design\nguidance reduces end-to-end latency by 5.6 to 8.2 times compared to the\nexisting system.",
        "Body composition analysis provides valuable insights into aging, disease\nprogression, and overall health conditions. Due to concerns of radiation\nexposure, two-dimensional (2D) single-slice computed tomography (CT) imaging\nhas been used repeatedly for body composition analysis. However, this approach\nintroduces significant spatial variability that can impact the accuracy and\nrobustness of the analysis. To mitigate this issue and facilitate body\ncomposition analysis, this paper presents a novel method to generate 3D CT\nvolumes from limited number of 2D slices using a latent diffusion model (LDM).\nOur approach first maps 2D slices into a latent representation space using a\nvariational autoencoder. An LDM is then trained to capture the 3D context of a\nstack of these latent representations. To accurately interpolate\nintermediateslices and construct a full 3D volume, we utilize body part\nregression to determine the spatial location and distance between the acquired\nslices. Experiments on both in-house and public 3D abdominal CT datasets\ndemonstrate that the proposed method significantly enhances body composition\nanalysis compared to traditional 2D-based analysis, with a reduced error rate\nfrom 23.3% to 15.2%.",
        "Large language models (LLMs) often suffer from hallucination, generating\nfactually incorrect or ungrounded content, which limits their reliability in\nhigh-stakes applications. A key factor contributing to hallucination is the use\nof hard labels during training, which enforce deterministic supervision,\nencourage overconfidence, and disregard the uncertainty inherent in natural\nlanguage. To address this, we propose mitigating hallucination through\nknowledge distillation (KD), where a teacher model provides smoothed soft\nlabels to a student model, reducing overconfidence and improving factual\ngrounding. We apply KD during supervised finetuning on instructional data,\nevaluating its effectiveness across LLMs from different families. Experimental\nresults on summarization benchmarks demonstrate that KD reduces hallucination\ncompared to standard finetuning while preserving performance on general NLP\ntasks. These findings highlight KD as a promising approach for mitigating\nhallucination in LLMs and improving model reliability.",
        "Conditional Average Treatment Effect (CATE) estimation, at the heart of\ncounterfactual reasoning, is a crucial challenge for causal modeling both\ntheoretically and applicatively, in domains such as healthcare, sociology, or\nadvertising. Borrowing domain adaptation principles, a popular design maps the\nsample representation to a latent space that balances control and treated\npopulations while enabling the prediction of the potential outcomes. This paper\npresents a new CATE estimation approach based on the asymmetrical search for\ntwo latent spaces called Asymmetrical Latent Representation for Individual\nTreatment Effect (ALRITE), where the two latent spaces are respectively\nintended to optimize the counterfactual prediction accuracy on the control and\nthe treated samples. Under moderate assumptions, ALRITE admits an upper bound\non the precision of the estimation of heterogeneous effects (PEHE), and the\napproach is empirically successfully validated compared to the state-of-the-art",
        "The T center in silicon has recently emerged as a promising candidate for\nscalable quantum technologies, due to its telecommunications band optical\ntransition and microwave addressable ground state spin. The immense promise of\nthe T center is driven by its silicon host material; silicon is by far the most\nmature, manufacturable semiconductor material for integrated photonic and\nelectronic devices. Here, we present the first study of T-centers in an\nelectrical device. We study an ensemble of T centers coupled to a buried\nlateral P-I-N diode in silicon, observing the T-center's optical response to\nstatic and dynamic electric fields. We utilize the defect's optical response as\na probe of device nonlinearity, observing a phase transition of the carrier\ndensity into a stable oscillatory regime characteristic of negative\ndifferential resistance. These findings provide fundamental insight into the\nphysics of the T-center for improved quantum device performance and open a\npromising new direction for defect-based local quantum sensing in semiconductor\ndevices.",
        "Effective epidemic modeling is essential for managing public health crises,\nrequiring robust methods to predict disease spread and optimize resource\nallocation. This study introduces a novel deep learning framework that advances\ntime series forecasting for infectious diseases, with its application to COVID\n19 data as a critical case study. Our hybrid approach integrates Convolutional\nNeural Networks (CNNs) and Long Short Term Memory (LSTM) models to capture\nspatial and temporal dynamics of disease transmission across diverse regions.\nThe CNN extracts spatial features from raw epidemiological data, while the LSTM\nmodels temporal patterns, yielding precise and adaptable predictions. To\nmaximize performance, we employ a hybrid optimization strategy combining the\nWhale Optimization Algorithm (WOA) and Gray Wolf Optimization (GWO) to fine\ntune hyperparameters, such as learning rates, batch sizes, and training epochs\nenhancing model efficiency and accuracy. Applied to COVID 19 case data from 24\ncountries across six continents, our method outperforms established benchmarks,\nincluding ARIMA and standalone LSTM models, with statistically significant\ngains in predictive accuracy (e.g., reduced RMSE). This framework demonstrates\nits potential as a versatile method for forecasting epidemic trends, offering\ninsights for resource planning and decision making in both historical contexts,\nlike the COVID 19 pandemic, and future outbreaks.",
        "While most current approaches rely on further training techniques, such as\nfine-tuning or reinforcement learning, to enhance model capacities, model\nmerging stands out for its ability of improving models without requiring any\nadditional training. In this paper, we propose a unified framework for model\nmerging based on low-rank estimation of task vectors without the need for\naccess to the base model, named \\textsc{LoRE-Merging}. Our approach is\nmotivated by the observation that task vectors from fine-tuned models\nfrequently exhibit a limited number of dominant singular values, making\nlow-rank estimations less prone to interference. We implement the method by\nformulating the merging problem as an optimization problem. Extensive empirical\nexperiments demonstrate the effectiveness of our framework in mitigating\ninterference and preserving task-specific information, thereby advancing the\nstate-of-the-art performance in model merging techniques.",
        "Compared with natural images, remote sensing images (RSIs) have the unique\ncharacteristic. i.e., larger intraclass variance, which makes semantic\nsegmentation for remote sensing images more challenging. Moreover, existing\nsemantic segmentation models for remote sensing images usually employ a vanilla\nsoftmax classifier, which has three drawbacks: (1) non-direct supervision for\nthe pixel representations during training; (2) inadequate modeling ability of\nparametric softmax classifiers under large intraclass variance; and (3) opaque\nprocess of classification decision. In this paper, we propose a novel\nclassifier (called CenterSeg) customized for RSI semantic segmentation, which\nsolves the abovementioned problems with multiple prototypes, direct supervision\nunder Grassmann manifold, and interpretability strategy. Specifically, for each\nclass, our CenterSeg obtains local class centers by aggregating corresponding\npixel features based on ground-truth masks, and generates multiple prototypes\nthrough hard attention assignment and momentum updating. In addition, we\nintroduce the Grassmann manifold and constrain the joint embedding space of\npixel features and prototypes based on two additional regularization terms.\nEspecially, during the inference, CenterSeg can further provide\ninterpretability to the model by restricting the prototype as a sample of the\ntraining set. Experimental results on three remote sensing segmentation\ndatasets validate the effectiveness of the model. Besides the superior\nperformance, CenterSeg has the advantages of simplicity, lightweight,\ncompatibility, and interpretability. Code is available at\nhttps:\/\/github.com\/xwmaxwma\/rssegmentation.",
        "Multivariate time series (MTS) data often include a heterogeneous mix of\nnon-Gaussian distributional features (asymmetry, multimodality, heavy tails)\nand data types (continuous and discrete variables). Traditional MTS methods\nbased on convenient parametric distributions are typically ill-equipped to\nmodel this heterogeneity. Copula models provide an appealing alternative, but\npresent significant obstacles for fully Bayesian inference and probabilistic\nforecasting. To overcome these challenges, we propose a novel and general\nstrategy for posterior approximation in MTS copula models and apply it to a\nGaussian copula built from a dynamic factor model. This framework provides\nscalable, fully Bayesian inference for cross-sectional and serial dependencies\nand nonparametrically learns heterogeneous marginal distributions. We validate\nthis approach by establishing posterior consistency and confirm excellent\nfinite-sample performance even under model misspecification using simulated\ndata. We apply our method to crime count and macroeconomic MTS data and find\nsuperior probabilistic forecasting performance compared to popular MTS models.\nThese results demonstrate that the proposed method is a versatile,\ngeneral-purpose utility for probabilistic forecasting of MTS that works well\nacross of range of applications with minimal user input.",
        "As LLM-as-a-Judge emerges as a new paradigm for assessing large language\nmodels (LLMs), concerns have been raised regarding the alignment, bias, and\nstability of LLM evaluators. While substantial work has focused on alignment\nand bias, little research has concentrated on the stability of LLM evaluators.\nIn this paper, we conduct extensive experiments involving 9 widely used LLM\nevaluators across 2 different evaluation settings to investigate the\nuncertainty in model-based LLM evaluations. We pinpoint that LLM evaluators\nexhibit varying uncertainty based on model families and sizes. With careful\ncomparative analyses, we find that employing special prompting strategies,\nwhether during inference or post-training, can alleviate evaluation uncertainty\nto some extent. By utilizing uncertainty to enhance LLM's reliability and\ndetection capability in Out-Of-Distribution (OOD) data, we further fine-tune an\nuncertainty-aware LLM evaluator named ConfiLM using a human-annotated\nfine-tuning set and assess ConfiLM's OOD evaluation ability on a manually\ndesigned test set sourced from the 2024 Olympics. Experimental results\ndemonstrate that incorporating uncertainty as additional information during the\nfine-tuning phase can largely improve the model's evaluation performance in OOD\nscenarios. The code and data are released at:\nhttps:\/\/github.com\/hasakiXie123\/LLM-Evaluator-Uncertainty.",
        "Hawking radiation from a non-extremal black hole is known to be approximately\nPlanckian. The thermal spectrum receives multiple corrections including\ngreybody factors and due to kinematical restrictions on the infrared and\nultraviolet frequencies. We show that another significant correction to the\nspectrum arises if the black hole is assumed to live in a thermal bath and the\nemitted radiation gets thermalised at the bath temperature. This modification\nreshapes the thermal spectrum, and leads to appreciable deviation from standard\nresults including modification in the decay rate of black holes. We argue that\nthis altered decay rate has significance for cosmology and, in a realistic\nsetting, show that it alters the life time of primordial black holes (PBHs) in\nthe early universe. In particular, the very light PBHs formed right after the\nend of inflation decay faster which may have interesting phenomenological\nimplications.",
        "The detection of GW170817, together with its electromagnetic counterparts,\nhas proven that binary neutron star mergers are of central importance to the\nfield of nuclear astrophysics, e.g., through a better understanding of the\nformation of elements and novel constraints on the supranuclear dense equation\nof state governing the matter inside neutron stars. Essential for understanding\nthe binary coalescence are numerical-relativity simulations, which typically\ncome with high computational costs requiring high-performance computing\nfacilities. In this work, we build on recent studies to investigate whether\nnovel techniques, such as neural networks, can be employed in the conversion of\nconservative variables to primitive hydrodynamical variables, such as pressure\nand density. In this regard, we perform -- to the best of our knowledge -- the\nfirst binary neutron star merger simulations in which such methods are\nemployed. We show that this method results in stable simulations, reaching\naccuracies similar to traditional methods with an overall comparable\ncomputational cost. These simulations serve as a proof of principle that, in\nthe future, deep learning techniques could be used within numerical-relativity\nsimulations. However, further improvements are necessary to offer a\ncomputational advantage compared to traditional methods.",
        "The shift current photovoltaic effect is conventionally understood as the\nreal-space displacement of a wave packet induced by photoexcitation. However,\nthis interpretation becomes insufficient in flat-band systems, where\nquasiparticles are too massive to accelerate in real space under the optical\nelectric field. Here, we developed a physically consistent method to decompose\nthe shift current into real-space and momentum-space components. A surprising\npure momentum-space shift current is found theoretically in flat-band Mott\ninsulator Nb$_3$X$_8$ (X = Cl, Br, I) monolayers. This work underscores that\nsignificant shift current responses can emerge even in systems with minimal\ninterband polarization differences, highlighting the potential for exploring\nnovel bulk photovoltaic effects in flat-band Mott insulators.",
        "We introduce a simple approach that uses a large language model (LLM) to\nautomatically implement a fully interpretable rule-based data-to-text system in\npure Python. Experimental evaluation on the WebNLG dataset showed that such a\nconstructed system produces text of better quality (according to the BLEU and\nBLEURT metrics) than the same LLM prompted to directly produce outputs, and\nproduces fewer hallucinations than a BART language model fine-tuned on the same\ndata. Furthermore, at runtime, the approach generates text in a fraction of the\nprocessing time required by neural approaches, using only a single CPU",
        "We measure exoplanet occurrence rate as a function of isochrone and\ngyrochronology ages using confirmed and candidate planets identified in Q1-17\nDR25 Kepler data. We employ Kepler's pipeline detection efficiency to correct\nfor the expected number of planets in each age bin. We examine the occurrence\nrates for planets with radii $0.2 \\leq Rp \\leq 20$ R$_\\oplus$ and orbital\nperiods $0.2 \\leq P \\leq 100$ days for FGK stars with ages between $1.5-8$ Gyr\nusing the inverse detection efficiency method. We find no significant trend\nbetween occurrence rate and stellar ages; a slight, decreasing trend (within\n$1.5-2.5$ $\\sigma$) only emerges for low-mass and metal-rich stars that\ndominate our sample. We isolate the effects of mass and metallicity on the\noccurrence rate trend with age, but find the results to be inconclusive due to\nweak trends and small sample size. Our results hint that the exoplanet\noccurrence rate may decrease over time due to dynamical instability from\nplanet-planet scattering or planet ejection, but accurate ages and larger\nsample sizes are needed to resolve a clear relation between occurrence rate and\nage.",
        "Video identity customization seeks to produce high-fidelity videos that\nmaintain consistent identity and exhibit significant dynamics based on users'\nreference images. However, existing approaches face two key challenges:\nidentity degradation over extended video length and reduced dynamics during\ntraining, primarily due to their reliance on traditional self-reconstruction\ntraining with static images. To address these issues, we introduce\n$\\textbf{MagicID}$, a novel framework designed to directly promote the\ngeneration of identity-consistent and dynamically rich videos tailored to user\npreferences. Specifically, we propose constructing pairwise preference video\ndata with explicit identity and dynamic rewards for preference learning,\ninstead of sticking to the traditional self-reconstruction. To address the\nconstraints of customized preference data, we introduce a hybrid sampling\nstrategy. This approach first prioritizes identity preservation by leveraging\nstatic videos derived from reference images, then enhances dynamic motion\nquality in the generated videos using a Frontier-based sampling method. By\nutilizing these hybrid preference pairs, we optimize the model to align with\nthe reward differences between pairs of customized preferences. Extensive\nexperiments show that MagicID successfully achieves consistent identity and\nnatural dynamics, surpassing existing methods across various metrics.",
        "Designing sustainable systems involves complex interactions between\nenvironmental resources, social impact\/adoption, and financial costs\/benefits.\nIn a constrained world, achieving a balanced design across those dimensions has\nbecome challenging. However a number of strategies have emerged to tackle\nspecific aspects such as preserving resources, improving the circularity in\nproduct lifecycles and ensuring global fairness. This paper explores how to\ncapture constitutive elements of those strategies using a modelling approach\nbased on a reference sustainability meta-model and pattern template. After\nproposing an extension to the meta-modelling to enable the structuring of a\npattern catalogue, we highlight how it can be populated on two case studies\nrespectively covering fairness and circularity.",
        "This paper presents MetricGrids, a novel grid-based neural representation\nthat combines elementary metric grids in various metric spaces to approximate\ncomplex nonlinear signals. While grid-based representations are widely adopted\nfor their efficiency and scalability, the existing feature grids with linear\nindexing for continuous-space points can only provide degenerate linear latent\nspace representations, and such representations cannot be adequately\ncompensated to represent complex nonlinear signals by the following compact\ndecoder. To address this problem while keeping the simplicity of a regular grid\nstructure, our approach builds upon the standard grid-based paradigm by\nconstructing multiple elementary metric grids as high-order terms to\napproximate complex nonlinearities, following the Taylor expansion principle.\nFurthermore, we enhance model compactness with hash encoding based on different\nsparsities of the grids to prevent detrimental hash collisions, and a\nhigh-order extrapolation decoder to reduce explicit grid storage requirements.\nexperimental results on both 2D and 3D reconstructions demonstrate the superior\nfitting and rendering accuracy of the proposed method across diverse signal\ntypes, validating its robustness and generalizability. Code is available at\nhttps:\/\/github.com\/wangshu31\/MetricGrids}{https:\/\/github.com\/wangshu31\/MetricGrids.",
        "Let $W= \\{W(t): t \\in \\mathbb{R}_+^N \\}$ be an $(N, d)$-Brownian sheet and\nlet $E \\subset (0, \\infty)^N$ and $F \\subset \\mathbb{R}^d$ be compact sets. We\nprove a necessary and sufficient condition for $W(E)$ to intersect $F$ with\npositive probability and determine the essential supremum of the Hausdorff\ndimension of the intersection set $W(E)\\cap F$ in terms of the thermal capacity\nof $E \\times F$. This extends the previous results of Khoshnevisan and Xiao\n(2015) for the Brownian motion and Khoshnevisan and Shi (1999) for the Brownian\nsheet in the special case when $E \\subset (0, \\infty)^N$ is an interval."
      ]
    }
  },
  {
    "id":2411.08664,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"Multimodal Foundation Models for Material Property Prediction and Discovery",
    "start_abstract":"Artificial intelligence is transforming computational materials science, improving the prediction of material properties, and accelerating the discovery of novel materials. Recently, publicly available material data repositories have grown rapidly. This growth encompasses not only more materials but also a greater variety and quantity of their associated properties. Existing machine learning efforts in materials science focus primarily on single-modality tasks, i.e. relationships between materials and a single physical property, thus not taking advantage of the rich and multimodal set of material properties. Here, we introduce Multimodal Learning for Materials (MultiMat), which enables self-supervised multi-modality training of foundation models for materials. We demonstrate our framework's potential using data from the Materials Project database on multiple axes: (i) MultiMat achieves state-of-the-art performance for challenging material property prediction tasks; (ii) MultiMat enables novel and accurate material discovery via latent space similarity, enabling screening for stable materials with desired properties; and (iii) MultiMat encodes interpretable emergent features that may provide novel scientific insights.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b2"
      ],
      "title":[
        "Comprehensive Inorganic Chemistry III (Third Edition)"
      ],
      "abstract":[
        "Comprehensive Inorganic Chemistry III, a ten-volume reference work, is intended to cover fundamental principles, recent discoveries, and significant applications of elements and their compounds. Authored by renowned experts in the field and edited by a world-class editorial board, each chapter provides a thorough and in-depth overview of the topic covered, featuring resources which will be useful to students, researchers, faculty as well as those in the industry. Comprehensive Inorganic Chemistry III focuses on main group chemistry, biological inorganic chemistry, solid state and materials chemistry, catalysis, and new developments in electrochemistry and photochemistry, as well as NMR and diffraction methods for studying inorganic compounds. The work expands on our 2013 work Comprehensive Inorganic Chemistry II while also adding new volumes on cutting-edge research areas and techniques for studying inorganic compounds. Researchers seeking background information on a specific problem involving the synthesis of inorganic compounds, as well as applications for numerous elements from the periodic table, and their compounds, will be able to rely on and refer to this authoritative scientific resource time and again."
      ],
      "categories":[
        "Material"
      ]
    },
    "list":{
      "title":[
        "Hyperuniformity and hyperfluctuations of random measures in commutative\n  spaces",
        "Direction-Dependent Conduction Polarity in Altermagnetic CrSb",
        "Quasi-reversible parametric instability in presence of noise",
        "Chemical abundance inventory in phosphorus-rich stars",
        "Local well-posedness for a system of modified KdV equations in\n  modulation spaces",
        "Rational Group Algebras of Camina $p$-groups",
        "Zero loss guarantees and explicit minimizers for generic\n  overparametrized Deep Learning networks",
        "Convergence guarantees for response prediction for latent structure\n  network time series",
        "Bounds on the number of squares in recurrence sequences: $y_{0}=b^{2}$\n  (I)",
        "A uniform action of the dihedral group $ Z_2\\times D_3$ on\n  Littlewood--Richardson coefficients",
        "Topological Holography for 2+1-D Gapped and Gapless Phases with\n  Generalized Symmetries",
        "An Accurate Efficient Analytic Model of Fidelity under Depolarizing\n  Noise oriented to Large Scale Quantum System Design",
        "Transfer Learning Analysis of Variational Quantum Circuits",
        "Mean Field Game of Controls with State Reflections: Existence and Limit\n  Theory",
        "Choosing Augmentation Parameters in OSQP- A New Approach based on\n  Conjugate Directions",
        "Field-induced phase transitions in the Kitaev-Heisenberg model: A\n  sign-problem-free quantum Monte Carlo study and possible application to\n  $\\alpha$-RuCl3",
        "Robust triple-q magnetic order with trainable spin vorticity in\n  Na$_2$Co$_2$TeO$_6$",
        "RiboFlow: Conditional De Novo RNA Sequence-Structure Co-Design via\n  Synergistic Flow Matching",
        "Spatial Data Science Languages: commonalities and needs",
        "A Large-dimensional Analysis of ESPRIT DoA Estimation: Inconsistency and\n  a Correction via RMT",
        "Exact matching as an alternative to propensity score matching",
        "Constructing QCQP Instances Equivalent to Their SDP Relaxations",
        "Concentration inequalities and large deviations for continuous greedy\n  animals and paths",
        "ECHO21: Exploring the Cosmos with Hydrogen Observation",
        "Uniform large-scale $\\varepsilon$-regularity for entropic optimal\n  transport",
        "Quantum Characterization, Verification, and Validation",
        "System-environmental entanglement in critical spin systems under\n  $ZZ$-decoherence and its relation to strong and weak symmetries",
        "Growing black-hole hair in nonminimally coupled biscalar gravity",
        "Remarks on the Higgs Branch of 5d Conformal Matter"
      ],
      "abstract":[
        "We introduce the notion of Bartlett spectral measure for isometrically\ninvariant random measures on proper metric commutative spaces. When the\nunderlying Gelfand pair corresponds to a higher-rank, connected, simple matrix\nLie group with finite center and a maximal compact subgroup, we prove that the\nnumber variance is asymptotically bounded above, uniformly across all random\nmeasures, by the volume raised to a power strictly less than 2. On Euclidean\nand real hyperbolic spaces we define a notion of heat kernel hyperuniformity\nfor random distributions that generalizes hyperuniformity of random measures,\nand we prove that every sufficiently well behaved spectral measure can be\nrealized as the Bartlett spectral measure of an invariant Gaussian random\ndistribution. We also compute Bartlett spectral measures for invariant\ndeterminantal point processes in commutative spaces, providing a spectral proof\nof hyperuniformity for infinite polyanalytic ensembles in the complex plane, as\nwell as heat kernel non-hyperuniformity of the zero set of the standard\nhyperbolic Gaussian analytic function on the Poincar\\'e disk.",
        "CrSb has recently gained immense attention as an altermagnetic candidate.\nThis work reports on the experimental observation of direction-dependent\nconduction polarity (DDCP) in altermagnetic CrSb through Hall and Seebeck\nthermopower measurements. Conduction is dominated by holes along the c-axis and\nby electrons in the ab-plane of the hexagonal crystal of CrSb. Density\nfunctional theory (DFT) calculations indicate that DDCP in CrSb arises from a\nmulticarrier mechanism, where electrons and holes living in distinct bands\ndominate conduction along different crystallographic directions. Furthermore,\nDFT predicts that DDCP exists within a narrow energy window near the Fermi\nlevel and is sensitive to small doping levels. This prediction is\nexperimentally validated by the loss of DDCP in hole-doped\nCr$_{0.98}$V$_{0.02}$Sb. These findings highlight the potential for tunable\nelectronic behavior in CrSb, offering promising avenues for applications in\ndevices that require both p-type and n-type functionalities within a single\nmaterial.",
        "We present an experimental and theoretical study of the effect of\nspatio-temporal fluctuations in quasi-reversible systems displaying a spatial\nquintic supercritical bifurcation. The saturation mechanism is drastically\nchanged by the inclusion of fluctuations. Experimentally, we observe the\nmodification of the bifurcation diagram of parametrically amplified surface\nwaves as spatiotemporal fluctuations stemming from an underlying vortex flow\nare included. Theoretically, we characterize the noise-dependent effective\ndynamics in a model system, the parametrically driven nonlinear Schr\\\"odinger\nequation, subjected to noise which allows us to rationalize the effect of the\nunderlying vortex flow on the surface waves",
        "We provide an overview of the latest advances in the study of phosphorus-rich\nstars, covering their detailed chemical abundance analyses and innovative\nmining approaches. Following the discovery of 16 low-mass and low-metallicity\nstars rich in P, we expanded this sample by demonstrating that a recently\nidentified group of Si-rich giants is also P-rich. A detailed abundance\nanalysis was conducted on the nearinfrared spectra from APOGEE-2 DR17,\nencompassing 13 elements. Subsequently, a similar analysis was performed on the\noptical UVES spectra of four P-rich stars, resulting in the abundance\ndetermination of 48 light and heavy elements. This comprehensive analysis\nfurther refined the chemical fingerprint of these peculiar stars, which was\nemployed to evaluate the plausibility of various nucleosynthetic formation\nscenarios. In order to obtain a statistically more reliable chemical\nfingerprint in the future, we explored the use of unsupervised machine learning\nalgorithms to identify additional P-rich stars in extensive spectroscopic\nsurveys, such as APOGEE-2. The primary objective of this research is to\nidentify the progenitor of these stars and determine whether current\nnucleosynthetic models require revision or if a completely new source of P in\nthe Galaxy is responsible for the existence of the P-rich stars.",
        "In this work, we consider the initial value problem (IVP) for a system of\nmodified Korteweg-de Vries (mKdV) equations\n  \\begin{equation*}\n  \\begin{cases} \\partial_t v + \\partial_x^3 v+ \\partial_x (v w^2) = 0,\n\\hspace{0.98 cm} v(x,0)=\\psi(x),\\\\ \\partial_t w + \\alpha \\partial_x^3\nw+\\partial_x (v^2 w) = 0,\\hspace{0.5 cm} w(x,0)=\\phi(x). \\end{cases}\n\\end{equation*} The main interest is in addressing the well-posedness issues of\nthe IVP when the initial data are considered in the modulation space\n$M_s^{2,p}(\\mathbb{R})$, $p\\geq 2$. In the case when $0<\\alpha\\ne 1$, we derive\nnew trilinear estimates in these spaces and prove that the IVP is locally\nwell-posed for data in $M_s^{2,p}(\\mathbb{R})$ whenever $s>\n\\frac14-\\frac{1}{p}$ and $p\\geq 2$.",
        "In this article, we present a combinatorial formula for the Wedderburn\ndecomposition of rational group algebras of Camina $p$-groups, where $p$ is a\nprime. We also provide a complete set of primitive central idempotents of\nrational group algebras of these groups.",
        "We determine sufficient conditions for overparametrized deep learning (DL)\nnetworks to guarantee the attainability of zero loss in the context of\nsupervised learning, for the $\\mathcal{L}^2$ cost and {\\em generic} training\ndata. We present an explicit construction of the zero loss minimizers without\ninvoking gradient descent. On the other hand, we point out that increase of\ndepth can deteriorate the efficiency of cost minimization using a gradient\ndescent algorithm by analyzing the conditions for rank loss of the training\nJacobian. Our results clarify key aspects on the dichotomy between zero loss\nreachability in underparametrized versus overparametrized DL.",
        "In this article, we propose a technique to predict the response associated\nwith an unlabeled time series of networks in a semisupervised setting. Our\nmodel involves a collection of time series of random networks of growing size,\nwhere some of the time series are associated with responses. Assuming that the\ncollection of time series admits an unknown lower dimensional structure, our\nmethod exploits the underlying structure to consistently predict responses at\nthe unlabeled time series of networks. Each time series represents a multilayer\nnetwork on a common set of nodes, and raw stress embedding, a popular\ndimensionality reduction tool, is used for capturing the unknown latent low\ndimensional structure. Apart from establishing theoretical convergence\nguarantees and supporting them with numerical results, we demonstrate the use\nof our method in the analysis of real-world biological learning circuits of\nlarval Drosophila.",
        "We continue and generalise our earlier investigations of the number of\nsquares in binary recurrence sequences. Here we consider sequences, $\\left\\{\ny_{k} \\right\\}$, arising from the solutions of generalised negative Pell\nequations, $X^{2}-dY^{2}=c$, where $-c$ and $y_{0}$ are any positive squares.\nWe show that there are at most $5$ distinct squares in such sequences when\n$y_{0}=1,4,\\ldots,11^{2}$, or once $d$ exceeds an explicit lower bound.",
        "We show that the dihedral group $ Z_2\\times D_3$ of order twelve acts\nfaithfully on the set LR, either consisting of Littlewood-Richardson tableaux,\nor their companion tableaux, or Knutson-Tao hives or Knutson-Tao-Woodward\npuzzles,via involutions which simultaneously conjugate or shuffle a\nLittlewood-Richardson triple of partitions. The action of $ Z_2\\times D_3$\ncarries a linear time index two subgroup $H\\simeq D_3$ action, where an\ninvolution which goes from $H$ into the other coset of H is difficult in the\nsense that it is not manifest neither exhibited by simple means. Pak and\nVallejo have earlier made this observation with respect to the subgroup of\nindex two in the symmetric group $ S_3$ consisting of cyclic permutations which\nH extends. The other half LR symmetries, not in the range of the H-action, are\nhidden and consist of commutativity and conjugation symmetries. Their\nexhibition is reduced to the action of a remaining generator of $ Z_2\\times\nD_3$, which belongs to the other coset of H, and enables to reduce in linear\ntime all known LR commuters and transposers to each other, and to the Luzstig-\nSch\\\"utzenberger involution. A hive is specified by superimposing the companion\ntableau pair of an LR tableau, and its $Z_2\\times D_3$-symmetries are exhibited\nvia the corresponding LR companion tableau pair. The action of $ Z_2\\times D_3$\non puzzles, naturally in bijection with Purbhoo mosaics, is consistent with the\nmigration map on mosaics which translates to jeu de taquin slides or\ntableau-switching on LR tableaux. Their H-symmetries are reduced to simple\nprocedures on a puzzle via label swapping together with simple reflections of\nan equilateral triangle, that is, puzzle dualities, and rotations on an\nequilateral triangle.",
        "We study topological holography for 2+1-D gapped and gapless phases with\ngeneralized symmetries using tools from higher linear algebra and higher\ncondensation theory. We focus on bosonic fusion 2-category symmetries, where\nthe Symmetry Topological Field Theory (SymTFT) are 3+1D Dijkgraaf-Witten\ntheories.\n  (1). Gapped phases are obtained from the sandwich construction with gapped\nsymmetry and physical boundaries. A gapped boundary of the 3+1D SymTFT is\ncalled minimal if it has no intrinsic 2+1-D topological order. We derive the\ngeneral structure of a sandwich construction with minimal gapped symmetry and\nphysical boundaries, including the underlying topological order and the\nsymmetry action. We also study some concrete examples with 2-group or\nnon-invertible symmetries.\n  (2). For gapless phases, we show that the SymTFT provides a complete\ndescription of the \\textit{topological skeleton} of a gapless phase. The\ntopological skeleton of a gapless phase is the higher categorical structure of\nits topological defects. We rigorously establish this relation for 2+1-D\ngapless phases with finite group symmetries. For a gapless phase with a finite\ngroup symmetry, its topological skeleton(also known as gapless SPT(gSPT)) can\nbe characterized by the decorated domain wall construction. We give a precise\nformulation of this using spectral sequence. We show that certain class of\ncondensable algebras in the SymTFT $\\mathcal{Z}_1[2\\mathbf{Vec}_G]$, which we\ncall minimal condensable algebras, has exactly the same structure. We further\ngive a cohomological classification of minimal condensable algebras, which\nenables us to compute the classification of 2+1-D $G$-gSPTs via ordinary group\ncohomology. Finally we use SymTFT to construct 2+1-D gSPT with generalized\nsymmetries, including an intrinsically gSPT(igSPT) with exact non-invertible\nfusion 2-category symmetry and anomalous 2-group IR symmetry.",
        "Fidelity is one of the most valuable and commonly used metrics for assessing\nthe performance of quantum circuits on error-prone quantum processors. Several\napproaches have been proposed to estimate circuit fidelity without the need of\nexecuting it on quantum hardware, but they often face limitations in\nscalability or accuracy. In this work, we present a comprehensive theoretical\nframework to predict the fidelity of quantum circuits under depolarizing noise.\nBuilding on theoretical results, we propose an efficient fidelity estimation\nalgorithm based on device calibration data. The method is thoroughly validated\nthrough simulation and execution on real hardware, demonstrating improved\naccuracy compared to state-of-the-art alternatives. The proposed approach\nprovides a scalable and practical tool for benchmarking quantum hardware,\ncomparing quantum software techniques such as compilation methods, obtaining\ncomputation bounds for quantum systems, and guiding hardware design decisions,\nmaking it a critical resource for the development and evaluation of quantum\ncomputing technologies.",
        "This work analyzes transfer learning of the Variational Quantum Circuit\n(VQC). Our framework begins with a pretrained VQC configured in one domain and\ncalculates the transition of 1-parameter unitary subgroups required for a new\ndomain. A formalism is established to investigate the adaptability and\ncapability of a VQC under the analysis of loss bounds. Our theory observes\nknowledge transfer in VQCs and provides a heuristic interpretation for the\nmechanism. An analytical fine-tuning method is derived to attain the optimal\ntransition for adaptations of similar domains.",
        "This paper studies mean field game (MFG) of controls by featuring the joint\ndistribution of the state and the control with the reflected state process\nalong an exogenous stochastic reflection boundary. We contribute to the\nliterature with a customized relaxed formulation and some new compactification\narguments to establish the existence of a Markovian mean field equilibrium\n(MFE) in the weak sense. We consider an enlarged canonical space, utilizing the\ndynamic Skorokhod mapping, to accommodate the stochastic reflection boundary\nprocess. A fixed-point argument on the extended space using an extension\ntransformation technique is developed to tackle challenges from the joint\nmeasure flow of the state and the relaxed control that may not be continuous.\nFurthermore, the bidirectional connections between the MFG and the $N$-player\ngame are also established in the context of joint law dependence and state\nreflections. We first show that any weak limit of empirical measures induced by\n$\\boldsymbol{\\epsilon}$-Nash equilibria in $N$-player games must be supported\nexclusively on the set of relaxed mean field equilibria, analogous to the\npropagation of chaos in mean field control problems. We then prove the\nconvergence result that a Markovian MFE in the weak sense can be approximated\nby a sequence of constructed $\\boldsymbol{\\epsilon}$-Nash equilibria in the\nweak sense in $N$-player games when $N$ tends to infinity.",
        "This work proposes a new method to select the augmentation parameters in the\noperator splitting quadratic program (OSQP) algorithm so as to reduce the\ncomputation time of overall algorithm. The selection is based upon the\ninformation of conjugate directions of the coefficient matrix of a linear\nsystem of equations present in the algorithm. This selection makes it possible\nto cache these conjugate directions, instead of computing them at each\niteration, resulting in faster computation of the solution of the linear system\nthus reducing the overall computation time. This reduction is demonstrated by a\nnumerical example.",
        "The frustrated magnet $\\alpha$-RuCl3 is one of the prime candidates for\nrealizing a Kitaev quantum spin liquid (QSL). However, the existence of a\nfield-induced intermediate QSL phase in this material remains under debate.\nHere, we employ sign-free numerically exact quantum Monte Carlo simulations to\ninvestigate the Kitaev-Heisenberg (KH) model on the honeycomb lattice with\n$K=-2J$ under an applied magnetic field along the z-direction. Our findings\nreveal that the system undergoes a direct quantum phase transition from a\nzigzag magnetically ordered phase to a spin-polarized phase at zero\ntemperature, which belongs to the 3D XY universality class. At finite\ntemperatures, a Berezinskii-Kosterlitz-Thouless transition line separates the\nspin-polarized phase from a quasi-long-range ordered state, eventually\nterminating at the quantum critical point. Our results convincingly show that\nthere is no intermediate QSL phase in the KH model with a z-direction magnetic\nfield, which we believe will shed important light on understanding experimental\nobservations in $\\alpha$-RuCl3.",
        "Recent studies suggest that the candidate Kitaev magnet Na$_2$Co$_2$TeO$_6$\npossesses novel triple-$\\mathbf{q}$ magnetic order instead of conventional\nsingle-$\\mathbf{q}$ zigzag order. Here we present dedicated experiments in\nsearch for distinct properties expected of the triple-$\\mathbf{q}$ order,\nnamely, insensitivity of the magnetic domains to weak $C_3$ symmetry-breaking\nfields and fictitious magnetic fields generated by the spin vorticity. In\nstructurally pristine single crystals, we show that $C_3$ symmetry-breaking\nin-plane uniaxial strains do not affect the order's magnetic neutron\ndiffraction signals. We further show that $\\mathbf{c}$-axis propagating light\nexhibits large Faraday rotations in the ordered state due to the spin\nvorticity, the sign of which can be trained via the system's ferrimagnetic\nmoment. These results are in favor of the triple-$\\mathbf{q}$ order in\nNa$_2$Co$_2$TeO$_6$ and reveal its unique emerging behavior.",
        "Ribonucleic acid (RNA) binds to molecules to achieve specific biological\nfunctions. While generative models are advancing biomolecule design, existing\nmethods for designing RNA that target specific ligands face limitations in\ncapturing RNA's conformational flexibility, ensuring structural validity, and\novercoming data scarcity. To address these challenges, we introduce RiboFlow, a\nsynergistic flow matching model to co-design RNA structures and sequences based\non target molecules. By integrating RNA backbone frames, torsion angles, and\nsequence features in an unified architecture, RiboFlow explicitly models RNA's\ndynamic conformations while enforcing sequence-structure consistency to improve\nvalidity. Additionally, we curate RiboBind, a large-scale dataset of\nRNA-molecule interactions, to resolve the scarcity of high-quality structural\ndata. Extensive experiments reveal that RiboFlow not only outperforms\nstate-of-the-art RNA design methods by a large margin but also showcases\ncontrollable capabilities for achieving high binding affinity to target\nligands. Our work bridges critical gaps in controllable RNA design, offering a\nframework for structure-aware, data-efficient generation.",
        "Recent workshops brought together several developers, educators and users of\nsoftware packages extending popular languages for spatial data handling, with a\nprimary focus on R, Python and Julia. Common challenges discussed included\nhandling of spatial or spatio-temporal support, geodetic coordinates, in-memory\nvector data formats, data cubes, inter-package dependencies, packaging upstream\nlibraries, differences in habits or conventions between the GIS and physical\nmodelling communities, and statistical models. The following set of insights\nhave been formulated: (i) considering software problems across data science\nlanguage silos helps to understand and standardise analysis approaches, also\noutside the domain of formal standardisation bodies; (ii) whether attribute\nvariables have block or point support, and whether they are spatially intensive\nor extensive has consequences for permitted operations, and hence for software\nimplementing those; (iii) handling geometries on the sphere rather than on the\nflat plane requires modifications to the logic of {\\em simple features}, (iv)\nmanaging communities and fostering diversity is a necessary, on-going effort,\nand (v) tools for cross-language development need more attention and support.",
        "In this paper, we perform asymptotic analyses of the widely used ESPRIT\ndirection-of-arrival (DoA) estimator for large arrays, where the array size $N$\nand the number of snapshots $T$ grow to infinity at the same pace. In this\nlarge-dimensional regime, the sample covariance matrix (SCM) is known to be a\npoor eigenspectral estimator of the population covariance. We show that the\nclassical ESPRIT algorithm, that relies on the SCM, and as a consequence of the\nlarge-dimensional inconsistency of the SCM, produces inconsistent DoA estimates\nas $N,T \\to \\infty$ with $N\/T \\to c \\in (0,\\infty)$, for both widely- and\nclosely-spaced DoAs. Leveraging tools from random matrix theory (RMT), we\npropose an improved G-ESPRIT method and prove its consistency in the same\nlarge-dimensional setting. From a technical perspective, we derive a novel\nbound on the eigenvalue differences between two potentially non-Hermitian\nrandom matrices, which may be of independent interest. Numerical simulations\nare provided to corroborate our theoretical findings.",
        "The comparison of different medical treatments from observational studies or\nacross different clinical studies is often biased by confounding factors such\nas systematic differences in patient demographics or in the inclusion criteria\nfor the trials. Propensity score matching is a popular method to adjust for\nsuch confounding. It compares weighted averages of patient responses. The\nweights are calculated from logistic regression models with the intention to\nreduce differences between the confounders in the treatment groups. However,\nthe groups are only \"roughly matched\" with no generally accepted principle to\ndetermine when a match is \"good enough\".\n  In this manuscript, we propose an alternative approach to the matching\nproblem by considering it as a constrained optimization problem. We investigate\nthe conditions for exact matching in the sense that the average values of\nconfounders are identical in the treatment groups after matching. Our approach\nis similar to the matching-adjusted indirect comparison approach by\nSignorovitch et al. (2010) but with two major differences: First, we do not\nimpose any specific functional form on the matching weights; second, the\nproposed approach can be applied to individual patient data from several\ntreatment groups as well as to a mix of individual patient and aggregated data.",
        "General quadratically constrained quadratic programs (QCQPs) are challenging\nto solve as they are known to be NP-hard. A popular approach to approximating\nQCQP solutions is to use semidefinite programming (SDP) relaxations. It is\nwell-known that the optimal value $\\eta$ of the SDP relaxation problem bounds\nthe optimal value $\\zeta$ of the QCQP from below, i.e., $\\eta \\leq \\zeta$. The\ntwo problems are considered equivalent if $\\eta = \\zeta$. In the recent paper\nby Arima, Kim and Kojima [arXiv:2409.07213], a class of QCQPs that are\nequivalent to their SDP relaxations are proposed with no condition imposed on\nthe quadratic objective function, which can be chosen arbitrarily. In this\nwork, we explore the construction of various QCQP instances within this class\nto complement the results in [arXiv:2409.07213]. Specifically, we first\nconstruct QCQP instances with two variables and then extend them to higher\ndimensions. We also discuss how to compute an optimal QCQP solution from the\nSDP relaxation.",
        "Consider the continuous greedy paths model: given a $d$-dimensional Poisson\npoint process with positive marks interpreted as masses, let $\\mathrm P(\\ell)$\ndenote the maximum mass gathered by a path of length $\\ell$ starting from the\norigin. It is known that $\\mathrm P(\\ell)\/\\ell converges a.s.\\ to a\ndeterministic constant $\\mathrm P$. We show that the lower-tail deviation\nprobability for $\\mathrm P(\\ell) has order $\\mathrm{exp}(-\\ell^2)$ and, under\nexponential moment assumption on the mass distribution, that the upper-tail\ndeviation probability has order $\\mathrm{exp}(-\\ell)$. In the latter regime, we\nprove the existence and some properties -notably, convexity -of the\ncorresponding rate function. An immediate corollary is the large deviation\nprinciple at speed $\\ell$ for $\\mathrm P(\\ell)$. Along the proof we show an\nupper-tail concentration inequality in the case where marks are bounded. All of\nthe above also holds for greedy animals and have versions where the paths or\nanimals involved have two anchors instead of one.",
        "We introduce a python package called ECHO21 for generating global 21-cm\nsignal from the dark ages through cosmic dawn to the end of reionization.\nBecause of its analytical-prescription-based foundation, ECHO21 generates a\nsingle model in $\\mathcal{O}(1)\\,$s. The code can generate a large set of\nsignals, ideal for building emulators and performing astrophysical or\ncosmological inference from a given 21-cm dataset. The code is MPI parallel\nwith reasonable scalability and thus, can be run on high-performance computers.\nWe offer six astrophysical parameters that control the Lyman-$\\alpha$\nemissivity, X-ray emissivity, emissivity of ionizing photons, and star\nformation rate. A critical component of 21-cm modelling, but ignored by\nmajority of public codes, that we include is the Ly$\\alpha$ heating. For a\ncertain range of astrophysical parameters, the Ly$\\alpha$ heating could even\ndominate the X-ray heating. In addition to astrophysical parameters, in ECHO21\nit is just as easy to vary the standard cosmological parameters which makes it\npossible to combine constraints from 21-cm observations and other cosmological\nprobes. Further, we offer two models of star formation rate; a\nphysically-motivated and an empirically-motivated. Since the latter is directly\ninferred by HST\/JWST observations, it makes ECHO21 an appropriate tool to build\nsynergies between 21-cm observations and galaxy surveys. With a number of 21-cm\nexperiments soon to provide cosmic dawn 21-cm data, ECHO21 is a handy package\nfor making quick but sufficiently realistic astrophysical inferences.",
        "We study the regularity properties of the minimisers of entropic optimal\ntransport providing a natural analogue of the $\\varepsilon$-regularity theory\nof quadratic optimal transport in the entropic setting. More precisely, we show\nthat if the minimiser of the entropic problem satisfies a gradient BMO-type\nestimate at some scale, the same estimate holds all the way down to the natural\nlength-scale associated to the entropic regularisation.\n  Our result follows from a more general $\\varepsilon$-regularity theory for\noptimal transport costs which can be viewed as perturbations of quadratic\noptimal transport. We consider such a perturbed cost and require that, under a\ncertain class of admissible affine rescalings, the minimiser remains a local\nquasi-minimiser of the quadratic problem (in an appropriate sense) and that the\ncost of \"long trajectories\" of minimisers (and their rescalings) is small.\nUnder these assumptions, we show that the minimiser satisfies an appropriate\n$C^{2,\\alpha}$ Morrey$\\unicode{x2013}$Campanato-type estimate which is valid up\nto the scale of quasi-minimality.",
        "Quantum characterization, verification, and validation (QCVV) is a set of\ntechniques to probe, describe, and assess the behavior of quantum bits\n(qubits), quantum information-processing registers, and quantum computers. QCVV\nprotocols probe and describe the effects of unwanted decoherence so that it can\nbe eliminated or mitigated. They can be usefully divided into characterization\ntechniques that estimate predictive models for a device's behavior from data,\nand benchmarking techniques that assess overall performance of a device. In\nthis introductory article, we briefly summarize the history of QCVV, introduce\nthe mathematical models and metrics upon which it relies, and then summarize\nthe foundational fields of tomography, randomized benchmarking, and holistic\nbenchmarks. We conclude with brief descriptions of (and references to) advanced\ntopics including gate set tomography, phase estimation, Pauli noise learning,\ncharacterization of mid-circuit measurements and non-Markovianity, classical\nshadows, verification and certification, and logical qubit assessment.",
        "Open quantum many-body system exhibits nontrivial behavior under decoherence.\nIn particular, system-environmental entanglement is one of quantities to\ncharacterize mixed state properties under decoherence. In this study, we\ninvestigate the behavior of the system-environmental entanglement for critical\nspin chains under nearest-neighbor $ZZ$ -decoherence. We numerically find that\nthe system-environmental entanglement exhibits a specific scaling law including\na system-independent universal term (\"$g$-function\"). For the critical XXZ\nmodel, transition to strong-to-weak spontaneously symmetry breaking mixed state\ntakes place. In that case, the $g$-function changes its value at decoherent\ntransition point and gets double the value of system under single-site\n$Z$-decoherence, which was recently studied by conformal field theory. By\nstudying Shannon entropy, we clarify origin of this $g$-function behavior.",
        "Black holes offer a unique laboratory for fundamental physics and are crucial\nfor probing theories beyond Einstein's theory of General Relativity. In this\npaper, we consider 4D effective field theories with scalar fields. We focus on\naxi-dilaton gravity, a quadratic gravity theory with two kinetically coupled\nscalar fields, an axion and a dilaton. To evolve these fields around black\nholes, we introduce Canuda-AxiDil, the first open-source, parameterized\nnumerical relativity code for quadratic and bi-scalar gravity. Using this code,\nwe perform single black hole simulations to show the dynamical formation of\naxion and dilaton hairs. Through these simulations, we measure the impact of\nblack-hole spin and curvature coupling strength on the axion and dilaton, and\nshow that a kinetic coupling between the fields increases the observed\ndeviations from General Relativity. Furthermore, we simulate the axion and\ndilaton fields around a binary black hole coalescence demonstrating the growth\nof axion hair during the inspiral and the production of radiative modes for\nboth fields.",
        "Among the elementary building blocks in the atomic classification of 5d SCFTs\nthere are 5d bifundamental conformal matter theories of various kinds. In this\nwork we study the Higgs branch of these models and of the corresponding\nmolecules arising from their fusion. To this aim we use two complementary\nindependent strategies. On the one hand for the type $A$ and $D$ conformal\nmatter, we identify dual $(p,q)$ brane webs in IIB and exploit them to read off\nthe corresponding magnetic quivers. On the other hand, we exploit circle\nreductions and study the resulting 4d $\\mathcal N=2$ SCFTs, giving an\nalternative derivation of their Higgs branches which extend also to the $E$\ntypes."
      ]
    }
  },
  {
    "id":2411.0908,
    "research_type":"applied",
    "start_id":"b2",
    "start_title":"Music Transformer: Generating Music with Long-Term Structure",
    "start_abstract":"Music relies heavily on repetition to build structure and meaning. Self-reference occurs on multiple timescales, from motifs to phrases to reusing of entire sections of music, such as in pieces with ABA structure. The Transformer (Vaswani et al., 2017), a sequence model based on self-attention, has achieved compelling results in many generation tasks that require maintaining long-range coherence. This suggests that self-attention might also be well-suited to modeling music. In musical composition and performance, however, relative timing is critically important. Existing approaches for representing relative positional information in the Transformer modulate attention based on pairwise distance (Shaw et al., 2018). This is impractical for long sequences such as musical compositions since their memory complexity for intermediate relative information is quadratic in the sequence length. We propose an algorithm that reduces their intermediate memory requirement to linear in the sequence length. This enables us to demonstrate that a Transformer with our modified relative attention mechanism can generate minute-long compositions (thousands of steps, four times the length modeled in Oore et al., 2018) with compelling structure, generate continuations that coherently elaborate on a given motif, and in a seq2seq setup generate accompaniments conditioned on melodies. We evaluate the Transformer with our relative attention mechanism on two datasets, JSB Chorales and Piano-e-Competition, and obtain state-of-the-art results on the latter.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b8"
      ],
      "title":[
        "On the use of AI for Generation of Functional Music to Improve Mental Health"
      ],
      "abstract":[
        "Increasingly music has been shown to have both physical and mental health benefits including improvements in cardiovascular health, a link reduction of cases dementia elderly populations, markers general well-being such as stress reduction. Here, we describe short case studies addressing (anxiety, stress-reduction) through AI-driven generation. Engaging active listening music-making activities (especially for at risk age groups) can be particularly beneficial, the practice therapy helpful range use across wide range. However, access prohibitive terms expertize, materials, cost. Furthermore existing functional outcomes (such targeted improvement suggested above) hindered by issues repetition subsequent over-familiarity with material. In this paper, machine learning approaches which create informed biophysiological measurement two studies, target emotional states opposing ends Cartesian affective space (a dimensional emotion points ranging from descriptors relaxation, fear). Galvanic skin response is used marker psychological arousal an estimate state control signal training algorithm. This algorithm creates non-linear time series musical features sound synthesis \u201con-the-fly\u201d, using perceptually feature similarity model. We find interaction between familiarity perceived response. also report on psychometric evaluation generated material, consider how these - similar techniques might useful generation tasks, example, nonlinear sound-tracking that found interactive media or video games."
      ],
      "categories":[
        "Mental Health"
      ]
    },
    "list":{
      "title":[
        "Reversible Switching of the Environment-Protected Quantum Spin Hall\n  Insulator Bismuthene at the Graphene\/SiC Interface",
        "Optical control of the crystal structure in the bilayer nickelate\n  superconductor La$_3$Ni$_2$O$_7$ via nonlinear phononics",
        "The gyrokinetic field invariant and electromagnetic temperature-gradient\n  instabilities in `good-curvature' plasmas",
        "On the Jump Conditions for Shock Waves in Condensed Materials",
        "Hardware-Efficient Entanglement Distillation Using Bosonic Systems",
        "Decentralized Projection-free Online Upper-Linearizable Optimization\n  with Applications to DR-Submodular Optimization",
        "Refined enumeration of two-rowed set-valued standard tableaux via\n  two-coloured Motzkin paths",
        "Identification and characterisation of the gamma-ray counterpart of the\n  transitional pulsar candidate CXOU J110926.4-650224",
        "From surface Fermi arcs to Fermi loops in the Dirac semimetal Cd3As2",
        "Hybridization between surface flat bands and bulk bands in the\n  topological nodal-line semimetal Sn$_{0.15}$NbSe$_{1.75}$ probed via\n  soft-point-contact spectroscopy",
        "ALICE Fast Interaction Trigger Upgrade",
        "Single-shot and two-shot decoding with generalized bicycle codes",
        "Josephson vortices and persistent current in a double-ring supersolid\n  system",
        "TDCOSMO: XX. WFI2033--4723, the First Quadruply-Imaged Quasar Modeled\n  with JWST Imaging",
        "The Q-Spellbook: Crafting Surface Code Layouts and Magic State Protocols\n  for Large-Scale Quantum Computing",
        "Seismic wavefield solutions via physics-guided generative neural\n  operator",
        "The uniqueness of Lyapunov rank among symmetric cones",
        "Atom-Chip Compatible Optical Lattice",
        "The Distributionally Robust Optimization Model of Sparse Principal\n  Component Analysis",
        "Large Physics Models: Towards a collaborative approach with Large\n  Language Models and Foundation Models",
        "Group Sparsity Methods for Compressive Space-Frequency Channel\n  Estimation and Spatial Equalization in Fluid Antenna System",
        "A degenerate Takens--Bogdanov bifurcation in a normal form of Lorenz's\n  equations",
        "Dynamical Shortcomings in the Generalized SU(2) Proca Theory: Challenges\n  for Cosmic Acceleration",
        "Leap into the future: shortcut to dynamics for quantum mixtures",
        "Spectral analysis of the X-ray flares in the 2023 outburst of the new\n  black binary transient Swift J1727.8--1613 observed with Insight-HXMT",
        "Masses of blocks of the $\\Lambda$-coalescent with dust via stochastic\n  flows",
        "Gaia 19cwm -- an eclipsing dwarf nova of WZ Sge type with a magnetic\n  white dwarf",
        "Enhanced quantum radiation with flying-focus laser pulses",
        "Forecasted Detection Limits on the (Dark) Matter Density in Supermassive\n  Black Hole Binaries for LISA"
      ],
      "abstract":[
        "Quantum Spin Hall Insulators (QSHI) have been extensively studied both\ntheoretically and experimentally because they exhibit robust helical edge\nstates driven by spin-orbit coupling and offer the potential for applications\nin spintronics through dissipationless spin transport. However, to realize\ndevices, it is indispensable to gain control over the interaction of the active\nlayer with the substrate, and to protect it from environmental influences. Here\nwe show that a single layer of elemental Bi, formed by intercalation of an\nepitaxial graphene buffer layer on SiC(0001), is a promising candidate for a\nQSHI. This layer can be reversibly switched between an electronically inactive\nprecursor state and a ``bismuthene state'', the latter exhibiting the predicted\nband structure of a true two-dimensional bismuthene layer. Switching is\naccomplished by hydrogenation (dehydrogenation) of the sample, i.e., a partial\npassivation (activation) of dangling bonds of the SiC substrate, causing a\nlateral shift of Bi atoms involving a change of the adsorption site. In the\nbismuthene state, the Bi honeycomb layer is a prospective QSHI, inherently\nprotected by the graphene sheet above and the H-passivated substrate below.\nThus, our results represent an important step towards protected QSHI systems\nbeyond graphene.",
        "Superconductivity in the bilayer nickelate La$_3$Ni$_2$O$_7$ occurs when the\ninterlayer Ni-O-Ni bond angle becomes straight under pressure, suggesting a\nstrong relationship between the crystal structure and the emergence of\nsuperconductivity. In this study, we theoretically propose a way to control the\ncrystal structure of La$_3$Ni$_2$O$_7$ toward the tetragonal symmetry via light\nirradiation instead of pressure using the idea of nonlinear phononics. Here,\nresonant optical excitation of an infrared-active (IR) lattice vibration\ninduces a nonlinear Raman-mode displacement through the anharmonic\nphonon-phonon coupling. We calculate the light-induced phonon dynamics on the\nanharmonic lattice potential determined by first-principles calculation. We\nfind that the interlayer Ni-O-Ni bond angle gets slightly closer to straight\nwhen an appropriate IR mode is selectively excited. Our study suggests that\nlight irradiation can be a promising way for structural control of\nLa$_3$Ni$_2$O$_7$.",
        "Curvature-driven instabilities are ubiquitous in magnetised fusion plasmas.\nBy analysing the conservation laws of the gyrokinetic system of equations, we\ndemonstrate that the well-known spatial localisation of these instabilities to\nregions of `bad magnetic curvature' can be explained using the conservation law\nfor a sign-indefinite quadratic quantity that we call the `gyrokinetic field\ninvariant'. Its evolution equation allows us to define the local effective\nmagnetic curvature whose sign demarcates the regions of `good' and `bad'\ncurvature, which, under some additional simplifying assumptions, can be shown\nto correspond to the inboard (high-field) and outboard (low-field) sides of a\ntokamak plasma, respectively. We find that, given some reasonable assumptions,\nelectrostatic curvature-driven modes are always localised to the regions of bad\nmagnetic curvature, regardless of the specific character of the instability.\nMore importantly, we also deduce that any mode that is unstable in the region\nof good magnetic curvature must be electromagnetic in nature. As a concrete\nexample, we present the magnetic-drift mode, a novel good-curvature\nelectromagnetic instability, and compare its properties with the well-known\nelectron-temperature-gradient instability. Finally, we discuss the relevance of\nthe magnetic-drift mode for high-$\\beta$ fusion plasmas, and in particular its\nrelationship with microtearing modes.",
        "In this article, we have proposed Rankine-Hugoniot (RH) boundary conditions\nat the normal shock front, which is passing through the condensed material.\nThese RH conditions are quite general, and their convenient forms for the\nparticle velocity, mass density, pressure, and temperature have been presented\nin terms of the upstream Mach number and the material parameters for the weak\nand the strong shocks, respectively. Finally, the effects on the mechanical\nquantities of the shock-compressed materials, e.g., titanium Ti6Al4V, stainless\nsteel 304, aluminum 6061-T6, etc., have been discussed.",
        "High-fidelity entanglement shared between distant quantum systems is an\nessential resource for quantum communication and computation. Entanglement\ndistillation addresses this need by converting multiple noisy Bell pairs into\nfewer higher-fidelity pairs, using only local quantum operations and classical\ncommunication. However, this approach typically requires a substantial overhead\nin the number of qubits. To bypass this hurdle, we propose to leverage the\nhigh-dimensional Hilbert space of a single pair of bosonic systems to store a\nlarge amount of entanglement, replacing the need for multi-qubit systems. To\ndistill entanglement in such a setup, we devise a new entanglement distillation\nprotocol, tailored for bosonic systems. The protocol converts a\nhighly-entangled noisy state between two bosonic systems into a\nlower-dimensional but high-fidelity entangled state, using only local bosonic\noperations. We show that our protocol significantly enhances the fidelity of\nthe entangled state in the presence of naturally occurring loss and dephasing\nerrors. Compared to methods relying on multiple Bell pairs, our scheme offers a\nmore hardware-efficient strategy, providing a practical route toward the\nrealization of entanglement distillation.",
        "We introduce a novel framework for decentralized projection-free\noptimization, extending projection-free methods to a broader class of\nupper-linearizable functions. Our approach leverages decentralized optimization\ntechniques with the flexibility of upper-linearizable function frameworks,\neffectively generalizing traditional DR-submodular function optimization. We\nobtain the regret of $O(T^{1-\\theta\/2})$ with communication complexity of\n$O(T^{\\theta})$ and number of linear optimization oracle calls of\n$O(T^{2\\theta})$ for decentralized upper-linearizable function optimization,\nfor any $0\\le \\theta \\le 1$. This approach allows for the first results for\nmonotone up-concave optimization with general convex constraints and\nnon-monotone up-concave optimization with general convex constraints. Further,\nthe above results for first order feedback are extended to zeroth order,\nsemi-bandit, and bandit feedback.",
        "We derive formulae for the number of set-valued standard tableaux of\ntwo-rowed shapes, keeping track of the total number of entries, the number of\nentries in the first row, and the number of entries in the second row. Key in\nthe proofs is a bijection with two-coloured Motzkin paths followed by\ngenerating function computations and coefficient extraction helped by the\nLagrange inversion formula.",
        "Transitional millisecond pulsars (tMSPs) represent a crucial link between the\nrotation-powered and accretion-powered states of binary pulsars. During their\nactive X-ray state, tMSPs are the only low-mass X-ray binary systems detected\nup to GeV energies by the Fermi Large Area Telescope (LAT). CXOU\nJ110926.4-650224 is a newly discovered tMSP candidate in an active X-ray state,\npotentially spatially compatible with a faint gamma-ray source listed in the\nlatest Fermi-LAT point-source catalogue as 4FGL J1110.3-6501. Confirming the\nassociation between CXOU J110926.4-650224 and the Fermi source is a key step\ntoward validating its classification as a tMSP. In this study, we analyse\nFermi-LAT data collected from August 2008 to June 2023 to achieve a more\naccurate localisation of the gamma-ray source, characterise its spectral\nproperties, and investigate potential time variability. By thoroughly\nreconstructing the gamma-ray background around the source using a weighted\nlikelihood model, we obtain a new localisation that aligns with the position of\nthe X-ray source at the 95% confidence level, with a Test Statistic value of\n$\\sim 42$. This establishes a spatial association between the gamma-ray source\nand CXOU J110926.4-650224. The gamma-ray emission is adequately described by a\npower-law model with a photon index of $\\Gamma = 2.5 \\pm 0.1$ and a\ncorresponding flux of $(3.7\\pm0.9) \\times 10^{-12}$ erg cm$^{-2}$ s$^{-1}$ in\nthe 0.1-300 GeV range.",
        "Arc-like topological surface states, i.e., surface Fermi arcs, have long been\nrecognized as the hallmark of Dirac semimetals. However, recent theories\nsuggest that the surface Fermi arcs could evolve into closed Fermi loops, akin\nto surface states in topological insulators, while preserving the bulk Dirac\nsemimetal phase. Here we experimentally reveal the evolution of Fermi arcs to\nFermi loops in the surface-modified Dirac semimetal Cd3As2 nanoplate through\ngate voltage-dependent spin transport and quantum oscillation measurements.\nSurface modification, achieved by heavy metal atom deposition and water\nmolecule adsorption, leads to an increase in the current-induced spin\npolarization at higher gate voltages, contrasting with the decrease observed in\nthe pristine nanoplate. We also observe surface Shubnikov-de Haas oscillations\nwith frequencies that scale linearly with gate voltage, aligning with a Fermi\nloop scenario. These findings indicate a transition from Fermi arcs to a closed\nFermi loop in the surface-modified Cd3As2 nanoplate, consistent with the\ntheoretically predicted fragile topological nature of Cd3As2. Our research\noffers profound insights into the transitions among these subtle topological\nstates in Dirac semimetals, paving the way for manipulating topological surface\nstates for high-performance spintronic devices.",
        "We report a detailed study of soft-point-contact spectroscopy of the\nsuperconducting topological nodal-line semimetal Sn$_{0.15}$NbSe$_{1.75}$ with\nthe superconducting transition temperature $T_{c}=9.5$ K. In the normal state,\nwe observe prominent asymmetric double peaks in the differential conductance\n$dI\/dV$. The asymmetric $dI\/dV$ curves are attributed to Fano resonance,\nquantum interference between two distinct tunneling paths of transmitting\nelectrons into flat energy bands and dispersive bands. A phenomenological\ndouble Fano resonance model reveals the hybridization between these bands below\nthe hybridization temperature $T_{\\mathrm{hyb}}=23$ K. This hybridization\ndrives an opening of a pseudogap below a characteristic temperature\n$T_{\\mathrm{PG}}=6.8$ K. In the superconducting state, we observe an unusual\nupper critical field that increases linearly with decreasing temperatures from\n$0.4T_{c}$ to $0.01T_{c}$, suggestive of a possible exotic superconducting\nstate. Our results suggest the presence of surface flat energy bands that stem\nfrom nontrivial topological nature of nodal lines in the bulk band structure\nand the hybridization between the surface flat bands and bulk bands in\nSn$_{0.15}$NbSe$_{1.75}$.",
        "This proceeding provides an expanded overview of the Fast Interaction Trigger\n(FIT) system performance, focusing on new developments such as the prospective\nintegration of the ALICE Low-Level Front-End Device (ALFRED) into the Detector\nControl System (DCS) and an upgraded Front-End Electronics (FEE) approach to\nenhance dynamic range and operational reliability. The first upgrade is\ndedicated to integrating FIT with ALICE central systems, while the second aims\nto improve signal processing from the scintillation arrays (FV0 and FDD).\nAdditionally, we propose forward-detector applications in future ALICE upgrades\n(Run 5 and beyond).\n  We also present the latest performance results, illustrated with relevant\nplots, including collision-time measurements for pp and Pb--Pb collision\nsystems, collision centrality determination based on the amplitude signals from\nthe FT0 detector, trigger performance metrics, and the improved DCS\narchitecture.",
        "Generalized-bicycle (GB) quantum error-correcting codes have naturally\nredundant minimum-weight stabilizer generators. To use this redundancy, we\nconstructed several short GB codes with relatively large dimensions, distances,\nand syndrome distances, also admitting fault-tolerant near-time-optimal\nsyndrome measurement schedules. We simulated their performance both under\nphenomenological noise and standard circuit noise, using sliding window\nsequential decoding protocol covering $T\\ge 1$ measurement rounds at a time,\nbased on an in-house binary BP+OSD decoder. While true single-shot decoding\n($T=1$) may suffer from a significant loss of accuracy, already two-shot\n($T=2$) decoding gives nearly the same logical error rates as multi-shot with\nmuch larger $T$. Comparison with the same codes but redundant stabilizer\ngenerators dropped show significantly improved decoding accuracy for all\n$T\\ge1$.",
        "We theoretically investigate the properties of ultra-cold dipolar atoms in\nradially coupled, concentric annular traps created by a potential barrier. The\nnon-rotating ground-state phases are investigated across the\nsuperfluid-supersolid phase transition, revealing a particle imbalance between\nthe two rings and a preferential density modulation in the outer ring. Near the\nphase transition on the superfluid side, applying rotation can induce density\nmodulations in either ring, depending on the angular momentum and barrier\nstrength. For low angular momentum, such rotation-induced density modulation\nforms in the outer ring, while for high angular momentum and weak barriers, it\nemerges in the inner ring. Rotation can lead to persistent currents and the\nnucleation of a vortex residing either at the center (central vortex) or at the\nring junction (Josephson vortex). Josephson vortices can also form at the\njunctions of the localized density sites induced by rotation in the inner ring,\na behavior that is unique to our system. By switching off the trap and allowing\nthe system to expand, distinct interference patterns emerge, which can be\nanalyzed to identify and distinguish between various vortex configurations, and\nthus can be observed in current state-of-the-art experiments.",
        "Gravitational time delays offer unique, independent measurements of the\nHubble constant, $H_0$. Precise measurements of $H_0$ stand as one of the most\npressing challenges in modern cosmology, and to do so with time delays requires\nprecise lens models. While much work has focused on streamlining the modeling\nprocess to keep pace with the erumpent discovery of strongly-lensed systems, a\ncritical step toward reducing uncertainty in $H_0$ comes from increasing the\nprecision of individual lens models themselves. In this work, we demonstrate\nthat the unprecedented imaging capabilities of JWST make this goal attainable.\nWe present the first lens model for time-delay cosmography derived from JWST\ndata, applied to the quadruply-imaged quasar WFI2033--4723. While the primary\nsource of systematic uncertainty in time-delay cosmography is currently the\nmass-sheet degeneracy (MSD), the sensitivity of models to this MSD varies on\nhow the point spread function (PSF) errors are mitigated. As the PSF is also\nthe primary source of uncertainty in lens modeling, we focus on a comparison of\ndifferent PSF modeling methods. Within the context of power-law models, we\nrecover results in agreement with previous Hubble Space Telescope (HST)-based\nmodels, but with better precision of key lensing parameters through\nimplementation of new PSF modeling techniques. Despite the record-holding\nprecision of this system's HST modeling, we achieve an additional 22% increase\nin precision of the Fermat potential difference, directly reducing\nuncertainties of cosmological inference. These results would produce a 3%\n($1\\sigma$ of the modeling error) shift of $H_0$ towards a higher value for\nthis lens, keeping all else constant. This work substantiates the\ngroundbreaking potential of JWST for time-delay cosmography and lays the\ngroundwork for modeling systems previously too faint to provide meaningful\nconstraints on $H_0$.",
        "Quantum error correction is a cornerstone of reliable quantum computing, with\nsurface codes emerging as a prominent method for protecting quantum\ninformation. Surface codes are efficient for Clifford gates but require magic\nstate distillation protocols to process non-Clifford gates, such as T gates,\nessential for universal quantum computation. In large-scale quantum\narchitectures capable of correcting arbitrary circuits, specialized surface\ncodes for data qubits and distinct codes for magic state distillation are\nneeded. These architectures can be organized into data blocks and distillation\nblocks. The system works by having distillation blocks produce magic states and\ndata blocks consume them, causing stalls due to either a shortage or excess of\nmagic states. This bottleneck presents an opportunity to optimize quantum space\nby balancing data and distillation blocks. While prior research offers insights\ninto selecting distillation protocols and estimating qubit requirements, it\nlacks a tailored optimization approach. We present a framework for optimizing\nlarge-scale quantum architectures, focusing on data block layouts and magic\nstate distillation protocols. We evaluate three data block layouts and four\ndistillation protocols under three optimization strategies: minimizing tiles,\nminimizing steps, and achieving a balanced trade-off. Through a comparative\nanalysis of brute force, dynamic programming, greedy, and random algorithms, we\nfind that brute force delivers optimal results, while greedy deviates by 7% for\nminimizing steps and dynamic programming matches brute force in tile\nminimization. We observe that total steps increase with columns, while total\ntiles scale with qubits. Finally, we propose a heuristic to help users select\nalgorithms suited to their objectives, enabling scalable and efficient quantum\narchitectures.",
        "Current neural operators often struggle to generalize to complex,\nout-of-distribution conditions, limiting their ability in seismic wavefield\nrepresentation. To address this, we propose a generative neural operator (GNO)\nthat leverages generative diffusion models (GDMs) to learn the underlying\nstatistical distribution of scattered wavefields while incorporating a\nphysics-guided sampling process at each inference step. This physics guidance\nenforces wave equation-based constraints corresponding to specific velocity\nmodels, driving the iteratively generated wavefields toward physically\nconsistent solutions. By training the diffusion model on wavefields\ncorresponding to a diverse dataset of velocity models, frequencies, and source\npositions, our GNO enables to rapidly synthesize high-fidelity wavefields at\ninference time. Numerical experiments demonstrate that our GNO not only\nproduces accurate wavefields matching numerical reference solutions, but also\ngeneralizes effectively to previously unseen velocity models and frequencies.",
        "The Lyapunov rank of a cone is the dimension of the Lie algebra of its\nautomorphism group. It is invariant under linear isomorphism and in general not\nunique - two or more non-isomorphic cones can share the same Lyapunov rank. It\nis therefore not possible in general to identify cones using Lyapunov rank. But\nsuppose we look only among symmetric cones. Are there any that can be uniquely\nidentified (up to isomorphism) by their Lyapunov ranks? We provide a complete\nanswer for irreducible cones and make some progress in the general case.",
        "A lattice beam configuration which results in an isotropic 3D trap near the\nsurface of an atom chip is described. The lattice is formed near the surface of\na reflectively coated atom chip, where three incident beams and three reflected\nbeams intersect. The coherent interference of these six beams form a\nphase-stable optical lattice which extends to the surface of the atom chip. The\nlattice is experimentally realized and the trap frequency is measured.\nDegenerate Raman sideband cooling is performed in the optical lattice, cooling\n80 million atoms to 1.1 $\\mu$K.",
        "We consider sparse principal component analysis (PCA) under a stochastic\nsetting where the underlying probability distribution of the random parameter\nis uncertain. This problem is formulated as a distributionally robust\noptimization (DRO) model based on a constructive approach to capturing\nuncertainty in the covariance matrix, which constitutes a nonsmooth constrained\nmin-max optimization problem. We further prove that the inner maximization\nproblem admits a closed-form solution, reformulating the original DRO model\ninto an equivalent minimization problem on the Stiefel manifold. This\ntransformation leads to a Riemannian optimization problem with intricate\nnonsmooth terms, a challenging formulation beyond the reach of existing\nalgorithms. To address this issue, we devise an efficient smoothing manifold\nproximal gradient algorithm. We prove the Riemannian gradient consistency and\nglobal convergence of our algorithm to a stationary point of the nonsmooth\nminimization problem. Moreover, we establish the iteration complexity of our\nalgorithm. Finally, numerical experiments are conducted to validate the\neffectiveness and scalability of our algorithm, as well as to highlight the\nnecessity and rationality of adopting the DRO model for sparse PCA.",
        "This paper explores ideas and provides a potential roadmap for the\ndevelopment and evaluation of physics-specific large-scale AI models, which we\ncall Large Physics Models (LPMs). These models, based on foundation models such\nas Large Language Models (LLMs) - trained on broad data - are tailored to\naddress the demands of physics research. LPMs can function independently or as\npart of an integrated framework. This framework can incorporate specialized\ntools, including symbolic reasoning modules for mathematical manipulations,\nframeworks to analyse specific experimental and simulated data, and mechanisms\nfor synthesizing theories and scientific literature. We begin by examining\nwhether the physics community should actively develop and refine dedicated\nmodels, rather than relying solely on commercial LLMs. We then outline how LPMs\ncan be realized through interdisciplinary collaboration among experts in\nphysics, computer science, and philosophy of science. To integrate these models\neffectively, we identify three key pillars: Development, Evaluation, and\nPhilosophical Reflection. Development focuses on constructing models capable of\nprocessing physics texts, mathematical formulations, and diverse physical data.\nEvaluation assesses accuracy and reliability by testing and benchmarking.\nFinally, Philosophical Reflection encompasses the analysis of broader\nimplications of LLMs in physics, including their potential to generate new\nscientific understanding and what novel collaboration dynamics might arise in\nresearch. Inspired by the organizational structure of experimental\ncollaborations in particle physics, we propose a similarly interdisciplinary\nand collaborative approach to building and refining Large Physics Models. This\nroadmap provides specific objectives, defines pathways to achieve them, and\nidentifies challenges that must be addressed to realise physics-specific large\nscale AI models.",
        "Fluid Antenna System (FAS) unlocks unprecedented flexibility in wireless\nchannel optimization through spatial reconfigurability. However, its practical\ndeployment is hindered by the coupled challenges posed by high-dimensional\nchannel estimation and real-time position optimization. This paper bridges\nwireless propagation physics with compressed sensing theory to address these\nchallenges through three aspects. First, we establish a group-sparse recovery\nframework for space-frequency characteristics (SFC) in FAS, formally\ncharacterizing leakage-induced sparsity degradation from limited aperture and\nbandwidth as a structured group-sparsity problem. By deriving\ndictionary-adapted group restricted isometry property (D-GRIP), we prove tight\nrecovery bounds for a convex $\\ell_1\/\\ell_2$-mixed norm optimization\nformulation that preserves leakage-aware sparsity patterns. Second, we develop\na Descending Correlation Group Orthogonal Matching Pursuit (DC-GOMP) algorithm\nthat systematically relaxes leakage constraints to reduce subcoherence. This\napproach enables robust FSC recovery with accelerated convergence and superior\nperformance compared to conventional compressive sensing methods like OMP or\nGOMP. Third, we formulate spatial equalization (SE) as a mixed-integer linear\nprogramming (MILP) problem, ensuring optimality through the branch-and-bound\nmethod. To achieve real-time implementability while maintaining near-optimal\nperformance, we complement this with a greedy algorithm.\n  Simulation results demonstrate the proposed channel estimation algorithm\neffectively resolves energy misallocation and enables recovery of weak details,\nachieving superior recovery accuracy and convergence rate. The SE framework\nsuppresses deep fading phenomena and reduces hardware deployment overhead while\nmaintaining equivalent link reliability.",
        "In this work we consider an unfolding of a normal form of the Lorenz system\nnear a triple-zero singularity. We are interested in the analysis of\ndouble-zero bifurcations emerging from that singularity. Their local study\nprovide partial results that are extended by means of numerical continuation\nmethods. Specifically, a curve of heteroclinic connections is detected. It has\na degenerate point from which infinitely many homoclinic connections emerge. In\nthis way, we can partially understand the dynamics near the triple-zero\nsingularity.",
        "The Generalized SU(2) Proca (GSU2P) theory has recently garnered attention\nfor its potential to describe key phases of cosmic evolution, including\nprimordial inflation and late-time accelerated expansion. However, its full\ncosmological implications remain unexplored. In this work, we perform a\ncomprehensive analysis of the dynamical properties of the GSU2P theory in a\nflat, homogeneous, and isotropic spacetime, through a dynamical-system\napproach. Our analysis reveals the presence of three pairs of fixed points, one\nof them corresponding to de-Sitter expansion which may represent either a\nstable or unstable phase in the evolution of the universe. These points,\nnonetheless, give rise to an indeterminate or infinite Hubble parameter, which\nrenders them cosmologically unviable. Additionally, we find two key\npseudostationary states: the ``attractor lines'', along which the system\nexhibits constant-roll dynamics, and the ``central zone'', characterized by\noscillatory radiation-like behaviour of the field. The dynamics within the\ncentral zone could represent a graceful exit from the primordial inflationary\nphase to a radiation dominated phase, or a state of the dark energy component\nprior to the late-time cosmic acceleration. However, within the central zone,\nthe dynamics of the vector field leads to recurrent instances of a nonphysical\nexpansion rate. The absence of a limit cycle in the central zone further\nexacerbates the issue, as the system may follow unbounded phase-space\ntrajectories, and the expansion rate becomes complex once it escapes the\nregion. Collectively, these challenges undermine the viability of the GSU2P\ntheory as a cosmological model for cosmic acceleration.",
        "The study of the long-time dynamics of quantum systems can be a real\nchallenge, especially in systems like ultracold gases, where the required\ntimescales may be longer than the lifetime of the system\n  itself. In this work, we show that it is possible to access the\n  long-time dynamics of a strongly repulsive atomic gas mixture in\n  shorter times. The shortcut-to-dynamics protocol that we propose\n  does not modify the fate of the observables, but effectively jumps\n  ahead in time without changing the system's inherent evolution. Just\n  like the next-chapter button in a movie player that allows to quickly reach\nthe part of the movie one wants to watch, it is a leap into the future.",
        "The new black hole transient Swift J1727.8--1613 exhibited a series of X-ray\nflares during its 2023 outburst extensively observed with Insight-HXMT. We\nanalyze the spectra of the flaring period using a series of models consisting\nof a multi-color disk and several different non-thermal components, and several\nconsistent conclusions are obtained among these models. First, Swift\nJ1727.8--1613 was in the transition process from the hard intermediate state\n(HIMS) to the very high state (VHS) during the first flaring period (MJD\n60197--60204), and afterwards it exhibited typical VHS parameter\ncharacteristics, such as high temperature of the disk inner radius and a steep\npower-law spectrum with a photon index of 2.6. Second, the flares in the VHS\nare characterized by a rapid increase in the flux of accretion disk,\naccompanied by a simultaneous rapid expansion of the inner radius, which could\nbe apparent if the accretion disk hardening factor varies significantly. The\nstrong power-law component during the VHS is likely produced by synchrotron\nself-Compton process in the relativistic jets, in agreement with the observed\nweak reflection component and lack of correlation with the disk component.",
        "We study the masses of blocks of the $\\Lambda$-coalescent with dust and some\naspects of their large and small time behaviors. To do so, we start by\nassociating the $\\Lambda$-coalescent to a nested interval-partition constructed\nfrom the flow of inverses, introduced by Bertoin and Le Gall in [Ann. inst.\nHenri Poincare (B) Probab. Stat. 41(3), 307-333 (2003)], of the\n$\\Lambda$-Fleming-Viot flow, and prove Poisson representations for the masses\nof blocks in terms of the flow of inverses. The representations enable us to\nuse the power of stochastic calculus to study the masses of blocks. We apply\nthis method to study the long and small time behaviors. In particular, for all\n$k>1$, we determine the decay rate of the expectation of the $k$-th largest\nblock as time goes to infinity and find that a cut-off phenomenon, related to\nthe presence of dust, occurs: the decay rate is increasing for small indices\n$k$ but remains constant after a fixed index depending on the measure\n$\\Lambda$.",
        "The spectral and photometric studies of the cataclysmic variable Gaia 19cwm\n(or ZTF19aamkwxk) have been performed. Based on the analysis of long-term\nvariability, it is concluded that the object belongs to WZ Sge type stars. The\nlight curves show eclipses recurring with an orbital period of $86.32048 \\pm\n0.00005$ min, as well as an out-of-eclipse variability with a period of\n$\\approx 6.45$ min. The latter period is stable for $\\sim 4$ years and appears\nto correspond to the rotation of a magnetic white dwarf, i.e., Gaia 19cwm is an\nintermediate polar. The Gaia 19cwm spectra show photospheric lines of the white\ndwarf, and Doppler tomograms demonstrate the presence of an accretion disk and\na hot spot. Analysis of the eclipse light curve gives an estimates of the white\ndwarf mass $M_1 = 0.66\\pm0.06$ M$_{\\odot}$, the donor mass $M_2 = 0.073 \\pm\n0.015$ M$_{\\odot}$, and the orbital inclination $i=83.8 \\pm 1.1^{\\circ}$.\nModeling of the spectral energy distribution gives the white dwarf temperature\nof $T_{eff}\\approx 13000 $ K. The X-ray luminosity $L_X = (1.6 \\pm 0.3) \\times\n10^{31}$ erg\/s allows to assign Gaia 19cwm to a small group of low-luminosity\nintermediate polars.",
        "The emission of a photon by an electron in an intense laser field is one of\nthe most fundamental processes in electrodynamics and underlies the many\napplications that utilize high-energy photon beams. This process is typically\nstudied for electrons colliding head-on with a stationary-focus laser pulse.\nHere, we show that the energy lost by electrons and the yield of emitted\nphotons can be substantially increased by replacing a stationary-focus pulse\nwith an equal-energy flying-focus pulse whose focus co-propagates with the\nelectrons. These advantages of the flying focus are a result of operating in\nthe quantum regime of the interaction, where the energy loss and photon yield\nscale more favorably with the interaction time than the laser intensity.\nSimulations of 10 GeV electrons colliding with 10 J pulses demonstrate these\nadvantages and predict a $5\\times$ increase in the yield of 1-20 MeV photons\nwith a flying focus pulse, which would impact applications in medicine,\nmaterial science, and nuclear physics.",
        "Supermassive black hole binaries (SMBHBs) are among the most powerful known\nsources of gravitational waves (GWs). Accordingly, these systems could dominate\nGW emission in the micro- and millihertz frequency range. Within this domain,\nSMBHs evolve rapidly and merge with each other. Dynamical friction from stars\nand gas at the centers of galaxies typically helps to bring together two SMBHs\nwhen they are at relatively far separations ($\\approx$ kpc $-$ 100 pc), but\nbecomes less efficient at smaller separations. However, dark matter (DM) spikes\naround SMBHs could enhance dynamical friction at close separations and, thus,\nshorten the evolution times. In this paper, we simulate the effects of DM\nspikes on GW signals in the micro- to millihertz frequency range and confirm\nthat the GW signals from SMBHBs with DM spikes can be clearly distinguished\nfrom those without any additional matter. Making use of the projected\nsensitivity curve of the Laser Interferometer Space Antenna (LISA), we forecast\nupper limits for the (dark) matter density for given future SMBHB observations.\nWe then compare these thresholds with the theoretical density profiles expected\nfor self-interacting dark matter (SIDM) spikes."
      ]
    }
  },
  {
    "id":2411.0908,
    "research_type":"applied",
    "start_id":"b8",
    "start_title":"On the use of AI for Generation of Functional Music to Improve Mental Health",
    "start_abstract":"Increasingly music has been shown to have both physical and mental health benefits including improvements in cardiovascular health, a link reduction of cases dementia elderly populations, markers general well-being such as stress reduction. Here, we describe short case studies addressing (anxiety, stress-reduction) through AI-driven generation. Engaging active listening music-making activities (especially for at risk age groups) can be particularly beneficial, the practice therapy helpful range use across wide range. However, access prohibitive terms expertize, materials, cost. Furthermore existing functional outcomes (such targeted improvement suggested above) hindered by issues repetition subsequent over-familiarity with material. In this paper, machine learning approaches which create informed biophysiological measurement two studies, target emotional states opposing ends Cartesian affective space (a dimensional emotion points ranging from descriptors relaxation, fear). Galvanic skin response is used marker psychological arousal an estimate state control signal training algorithm. This algorithm creates non-linear time series musical features sound synthesis \u201con-the-fly\u201d, using perceptually feature similarity model. We find interaction between familiarity perceived response. also report on psychometric evaluation generated material, consider how these - similar techniques might useful generation tasks, example, nonlinear sound-tracking that found interactive media or video games.",
    "start_categories":[
      "Mental Health"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b2"
      ],
      "title":[
        "Music Transformer: Generating Music with Long-Term Structure"
      ],
      "abstract":[
        "Music relies heavily on repetition to build structure and meaning. Self-reference occurs on multiple timescales, from motifs to phrases to reusing of entire sections of music, such as in pieces with ABA structure. The Transformer (Vaswani et al., 2017), a sequence model based on self-attention, has achieved compelling results in many generation tasks that require maintaining long-range coherence. This suggests that self-attention might also be well-suited to modeling music. In musical composition and performance, however, relative timing is critically important. Existing approaches for representing relative positional information in the Transformer modulate attention based on pairwise distance (Shaw et al., 2018). This is impractical for long sequences such as musical compositions since their memory complexity for intermediate relative information is quadratic in the sequence length. We propose an algorithm that reduces their intermediate memory requirement to linear in the sequence length. This enables us to demonstrate that a Transformer with our modified relative attention mechanism can generate minute-long compositions (thousands of steps, four times the length modeled in Oore et al., 2018) with compelling structure, generate continuations that coherently elaborate on a given motif, and in a seq2seq setup generate accompaniments conditioned on melodies. We evaluate the Transformer with our relative attention mechanism on two datasets, JSB Chorales and Piano-e-Competition, and obtain state-of-the-art results on the latter."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Efficient Distributed Optimization under Heavy-Tailed Noise",
        "Screening and localization in the nonlinear Anderson problem",
        "MIXPINN: Mixed-Material Simulations by Physics-Informed Neural Network",
        "Binarized Mamba-Transformer for Lightweight Quad Bayer HybridEVS\n  Demosaicing",
        "The slicing conjecture via small ball estimates",
        "On the Proportional Principal Stratum Hazards Model",
        "A Reference Architecture for Autonomous Networks: An Agent-Based\n  Approach",
        "The Large Hadron electron Collider as a bridge project for CERN",
        "MIGE: A Unified Framework for Multimodal Instruction-Based Image\n  Generation and Editing",
        "On spectral scaling laws for averaged turbulence on the sphere",
        "Flaring Activities of Fast Rotating Stars have Solar-like Latitudinal\n  Distribution",
        "Underlying Semantic Diffusion for Effective and Efficient In-Context\n  Learning",
        "Path-Adaptive Matting for Efficient Inference Under Various\n  Computational Cost Constraints",
        "Automated Detection and Analysis of Minor Deformations in Flat Walls Due\n  to Railway Vibrations Using LiDAR and Machine Learning",
        "LiPS: Large-Scale Humanoid Robot Reinforcement Learning with\n  Parallel-Series Structures",
        "Dissecting the Impact of Model Misspecification in Data-driven\n  Optimization",
        "REINFORCE-ING Chemical Language Models in Drug Design",
        "First loosely coherent search for continuous gravitational wave sources\n  with substellar companions in the Orion spur",
        "Simpliciality of vector-valued function spaces",
        "An enhanced term in the Szeg\\H{o}-type asymptotics for the free massless\n  Dirac operator",
        "DiffRhythm: Blazingly Fast and Embarrassingly Simple End-to-End\n  Full-Length Song Generation with Latent Diffusion",
        "GO-VMP: Global Optimization for View Motion Planning in Fruit Mapping",
        "Raiders of the Lost Dependency: Fixing Dependency Conflicts in Python\n  using LLMs",
        "Symmetry of information for space-bounded online Kolmogorov complexity",
        "Toward Automated Potential Primary Asset Identification in Verilog\n  Designs",
        "Exploring Bayesian olfactory search in realistic turbulent flows",
        "Spherical Dense Text-to-Image Synthesis",
        "Fixed-Budget Change Point Identification in Piecewise Constant Bandits",
        "Hard Lefschetz Condition on symplectic non-K\\\"ahler solvmanifolds"
      ],
      "abstract":[
        "Distributed optimization has become the default training paradigm in modern\nmachine learning due to the growing scale of models and datasets. To mitigate\ncommunication overhead, local updates are often applied before global\naggregation, resulting in a nested optimization approach with inner and outer\nsteps. However, heavy-tailed stochastic gradient noise remains a significant\nchallenge, particularly in attention-based models, hindering effective\ntraining. In this work, we propose TailOPT, an efficient framework designed to\naddress heavy-tailed noise by leveraging adaptive optimization or clipping\ntechniques. We establish convergence guarantees for the TailOPT framework under\nheavy-tailed noise with potentially unbounded gradient variance and local\nupdates. Among its variants, we highlight a memory and communication efficient\ninstantiation which we call $Bi^2Clip$, which performs coordinate-wise clipping\nat both the inner and outer optimizers, achieving adaptive-like performance\n(e.g., Adam) without the cost of maintaining or transmitting additional\ngradient statistics. Empirically, TailOPT, including $Bi^2Clip$, demonstrates\nsuperior performance on several language tasks and models, outperforming\nstate-of-the-art methods.",
        "We resolve an existing question concerning the localization of a wave packet\nby random potential in the presence of weak nonlinearity. The problem has\ngained considerable interest in the literature, and it continues to attract\nattention due to its connection with the general properties of behavior of\nsystems with competition between nonlinearity, nonlocality and randomness. We\nfind that the nonlinearly localized state occurs through a finite polarization\nresponse from the lattice well beyond the assumptions of a perturbation-theory\napproach. For the vanishing polarization response the nonlinear localization\nlength diverges permitting unlimited spreading of the nonlinear field.",
        "Simulating the complex interactions between soft tissues and rigid anatomy is\ncritical for applications in surgical training, planning, and robotic-assisted\ninterventions. Traditional Finite Element Method (FEM)-based simulations, while\naccurate, are computationally expensive and impractical for real-time\nscenarios. Learning-based approaches have shown promise in accelerating\npredictions but have fallen short in modeling soft-rigid interactions\neffectively. We introduce MIXPINN, a physics-informed Graph Neural Network\n(GNN) framework for mixed-material simulations, explicitly capturing soft-rigid\ninteractions using graph-based augmentations. Our approach integrates Virtual\nNodes (VNs) and Virtual Edges (VEs) to enhance rigid body constraint\nsatisfaction while preserving computational efficiency. By leveraging a\ngraph-based representation of biomechanical structures, MIXPINN learns\nhigh-fidelity deformations from FEM-generated data and achieves real-time\ninference with sub-millimeter accuracy. We validate our method in a realistic\nclinical scenario, demonstrating superior performance compared to baseline GNN\nmodels and traditional FEM methods. Our results show that MIXPINN reduces\ncomputational cost by an order of magnitude while maintaining high physical\naccuracy, making it a viable solution for real-time surgical simulation and\nrobotic-assisted procedures.",
        "Quad Bayer demosaicing is the central challenge for enabling the widespread\napplication of Hybrid Event-based Vision Sensors (HybridEVS). Although existing\nlearning-based methods that leverage long-range dependency modeling have\nachieved promising results, their complexity severely limits deployment on\nmobile devices for real-world applications. To address these limitations, we\npropose a lightweight Mamba-based binary neural network designed for efficient\nand high-performing demosaicing of HybridEVS RAW images. First, to effectively\ncapture both global and local dependencies, we introduce a hybrid Binarized\nMamba-Transformer architecture that combines the strengths of the Mamba and\nSwin Transformer architectures. Next, to significantly reduce computational\ncomplexity, we propose a binarized Mamba (Bi-Mamba), which binarizes all\nprojections while retaining the core Selective Scan in full precision. Bi-Mamba\nalso incorporates additional global visual information to enhance global\ncontext and mitigate precision loss. We conduct quantitative and qualitative\nexperiments to demonstrate the effectiveness of BMTNet in both performance and\ncomputational efficiency, providing a lightweight demosaicing solution suited\nfor real-world edge devices. Our codes and models are available at\nhttps:\/\/github.com\/Clausy9\/BMTNet.",
        "Bourgain's slicing conjecture was recently resolved by Joseph Lehec and Bo'az\nKlartag. We present an alternative proof by establishing small ball probability\nestimates for isotropic log-concave measures. Our approach relies on the\nstochastic localization process and Guan's bound, techniques also used by\nKlartag and Lehec. The link between small ball probabilities and the slicing\nconjecture was first observed by Dafnis and Paouris and is established through\nMilman's theory of M-ellipsoids.",
        "In clinical trials involving both mortality and morbidity, an active\ntreatment can influence the observed risk of the first non-fatal event either\ndirectly, through its effect on the non-fatal event process, or indirectly,\nthrough its effect on the death process, or both. Discerning the direct effect\nof treatment on the first non-fatal event holds clinical interest. However,\nwith the competing risk of death, the Cox proportional hazards model that\ntreats death as non-informative censoring and evaluates treatment effects on\ntime to the first non-fatal event provides an estimate of the cause-specific\nhazard ratio, which may not correspond to the direct effect. To obtain the\ndirect effect on the first non-fatal event, within the principal stratification\nframework, we define the principal stratum hazard and introduce the\nProportional Principal Stratum Hazards model. This model estimates the\nprincipal stratum hazard ratio, which reflects the direct effect on the first\nnon-fatal event in the presence of death and simplifies to the hazard ratio in\nthe absence of death. The principal stratum membership is identified using the\nshared frailty model, which assumes independence between the first non-fatal\nevent process and the potential death process from the counterfactual arm,\nconditional on per-subject random frailty. Simulation studies are conducted to\nverify the reliability of our estimators. We illustrate the method using the\nCarvedilol Prospective Randomized Cumulative Survival trial which involves\nheart-failure events.",
        "The vision of autonomous systems is becoming increasingly important in many\napplication areas, where the aim is to replace humans with agents. These\ninclude autonomous vehicles and other agents' applications in business\nprocesses and problem-solving. For networks, the increasing scale and operation\nand management (O&M) complexity drive the need for autonomous networks (AN).\nThe technical objective of AN is to ensure trustworthy O&M without human\nintervention for higher efficiency and lower operating costs. However,\nrealizing AN seems more difficult than autonomous vehicles. It encounters\nchallenges of networks' structural and functional complexity, which operate as\ndistributed dynamic systems governed by various technical and economic\nconstraints. A key problem lies in formulating a rigorous development\nmethodology that facilitates a seamless transition from traditional networks to\nAN. Central to this methodology is the definition of a reference architecture\nfor network agents, which specifies the required functionalities for their\nrealization, regardless of implementation choices. This article proposes a\nreference architecture characterizing main functional features, illustrating\nits application with network use cases. It shows how artificial intelligence\ncomponents can be used to implement the required functionality and its\ncoordination. The latter is achieved through the management and generation of\nshared domain-specific knowledge stored in long-term memory, ensuring the\noverall consistency of decisions and their execution. The article concludes\nwith a discussion of architecture specialization for building network layer\nagents. It also identifies the main technical challenges ahead, such as\nsatisfying essential requirements at development or runtime, as well as the\nissue of coordinating agents to achieve collective intelligence in meeting\noverall network goals.",
        "The LHeC is the project for delivering electron-nucleon collisions at CERN\nusing the HL-LHC beams. An Energy Recovery Linac in racetrack configuration\nwill provide 50 GeV electrons to achieve centre-of-mass energies around 1\nTeV\/nucleon and instantaneous luminosities around $10^{34}$ cm$^{-2}$s$^{-1}$.\nThe LHeC program elaborated in the CDR of 2021 included a phase with concurrent\noperation of electron-hadron and hadron-hadron collisions, followed by a\nstandalone phase of electron-hadron collisions only. In view of the current\nHL-LHC schedule, in this paper we have examined the possibilities of a program\nafter the regular HL-LHC program with only electron-proton operation. In this\noperation mode, the LHeC would serve as an impactful bridge project between\nmajor colliders at CERN. The standalone physics program comprises electroweak,\nHiggs, top-quark, BSM and strong-interaction physics. In addition, it empowers\nthe physics analyses at the HL-LHC by retrofitting measurements and searches\nwith significantly more precise knowledge of the proton structure and\n$\\alpha_s$. The accelerator technology deployed in the Energy Recovery Linac\nfor the LHeC is a major stepping-stone for the performance, cost reduction and\ntraining for future colliders. The capital investments in the LHeC electron\naccelerator can be reused in a cost-efficient way as the injector for the\nFCC-ee. Finally, data from the LHeC are essential to enable the physics\npotential of any new high-energy hadron collider. The operational plan of 6\nyears easily fits in the period between two major colliders at CERN. Similar to\nthe LHeC empowering the HL-LHC physics program, the FCC-eh would be an\nimpactful addition to the FCC physics program.",
        "Despite significant progress in diffusion-based image generation,\nsubject-driven generation and instruction-based editing remain challenging.\nExisting methods typically treat them separately, struggling with limited\nhigh-quality data and poor generalization. However, both tasks require\ncapturing complex visual variations while maintaining consistency between\ninputs and outputs. Therefore, we propose MIGE, a unified framework that\nstandardizes task representations using multimodal instructions. It treats\nsubject-driven generation as creation on a blank canvas and instruction-based\nediting as modification of an existing image, establishing a shared\ninput-output formulation. MIGE introduces a novel multimodal encoder that maps\nfree-form multimodal instructions into a unified vision-language space,\nintegrating visual and semantic features through a feature fusion mechanism.\nThis unification enables joint training of both tasks, providing two key\nadvantages: (1) Cross-Task Enhancement: By leveraging shared visual and\nsemantic representations, joint training improves instruction adherence and\nvisual consistency in both subject-driven generation and instruction-based\nediting. (2) Generalization: Learning in a unified format facilitates\ncross-task knowledge transfer, enabling MIGE to generalize to novel\ncompositional tasks, including instruction-based subject-driven editing.\nExperiments show that MIGE excels in both subject-driven generation and\ninstruction-based editing while setting a state-of-the-art in the new task of\ninstruction-based subject-driven editing. Code and model have been publicly\navailable at https:\/\/github.com\/Eureka-Maggie\/MIGE.",
        "Spectral analysis for a class of Lagrangian-averaged Navier--Stokes (LANS)\nequations on the sphere is carried out. The equations arise from the\nNavier--Stokes equations by applying a Helmholtz filter of width $\\alpha$ to\nthe advecting velocity $\\beta$ times. Power laws for the energy spectrum are\nderived and indicate a $\\beta$-dependent scaling at wave numbers $l$ with\n$\\alpha l\\gg 1$. The energy and enstrophy transfer rates distinctly depend on\nthe averaging, allowing control over the energy flux and the enstrophy flux\nseparately through the choice of averaging operator. A necessary condition on\nthe averaging operator is derived for the existence of the inverse cascade in\ntwo-dimensional turbulence. Numerical experiments with a structure-preserving\nintegrator confirm the expected energy spectrum scalings and the robustness of\nthe double cascade under choices of the averaging operator.",
        "The dynamo theory has always been one of the biggest mysteries in stellar\nphysics. One key reason for its uncertainty is poor knowledge of the dynamo\nprocess on stars except the Sun. The most important observation feature of\nsolar dynamo is that active regions only appear at low latitudes, which\nprovides a crucial constraint to the dynamo theory, while Doppler imaging, the\ncurrent technique to spatially resolve stellar hemisphere, is difficult to\ndistinguish the equatorial region . Hence, the latitudinal distribution of\nactive regions (LDAR) of stars is ambiguous and controversial, mainly due to\nthe limit of the current technique for spatially resolving the stellar surface.\nFast rotating stars, which are young and active, are thought to operate with a\ndifferent dynamo process than the Sun. We study their LDAR and compare them\nwith the Sun to reveal the underlying dynamo process. Flares are drastic and\nobservational activity events, which occur in active regions. Here, we propose\na new method to study how the apparent flaring activity varies with respect to\nthe inclination to determine the LDAR of fast rotating stars.We find that the\nLDAR of fast rotating stars is consistent with that of the Sun, contrary to\nexpectations. Our results provide a crucial constraint to stellar dynamo,\nindicating that the solar-like dynamo also applies to fast rotating stars, even\nspanning different stages of their evolution.",
        "Diffusion models has emerged as a powerful framework for tasks like image\ncontrollable generation and dense prediction. However, existing models often\nstruggle to capture underlying semantics (e.g., edges, textures, shapes) and\neffectively utilize in-context learning, limiting their contextual\nunderstanding and image generation quality. Additionally, high computational\ncosts and slow inference speeds hinder their real-time applicability. To\naddress these challenges, we propose Underlying Semantic Diffusion\n(US-Diffusion), an enhanced diffusion model that boosts underlying semantics\nlearning, computational efficiency, and in-context learning capabilities on\nmulti-task scenarios. We introduce Separate & Gather Adapter (SGA), which\ndecouples input conditions for different tasks while sharing the architecture,\nenabling better in-context learning and generalization across diverse visual\ndomains. We also present a Feedback-Aided Learning (FAL) framework, which\nleverages feedback signals to guide the model in capturing semantic details and\ndynamically adapting to task-specific contextual cues. Furthermore, we propose\na plug-and-play Efficient Sampling Strategy (ESS) for dense sampling at time\nsteps with high-noise levels, which aims at optimizing training and inference\nefficiency while maintaining strong in-context learning performance.\nExperimental results demonstrate that US-Diffusion outperforms the\nstate-of-the-art method, achieving an average reduction of 7.47 in FID on\nMap2Image tasks and an average reduction of 0.026 in RMSE on Image2Map tasks,\nwhile achieving approximately 9.45 times faster inference speed. Our method\nalso demonstrates superior training efficiency and in-context learning\ncapabilities, excelling in new datasets and tasks, highlighting its robustness\nand adaptability across diverse visual domains.",
        "In this paper, we explore a novel image matting task aimed at achieving\nefficient inference under various computational cost constraints, specifically\nFLOP limitations, using a single matting network. Existing matting methods\nwhich have not explored scalable architectures or path-learning strategies,\nfail to tackle this challenge. To overcome these limitations, we introduce\nPath-Adaptive Matting (PAM), a framework that dynamically adjusts network paths\nbased on image contexts and computational cost constraints. We formulate the\ntraining of the computational cost-constrained matting network as a bilevel\noptimization problem, jointly optimizing the matting network and the path\nestimator. Building on this formalization, we design a path-adaptive matting\narchitecture by incorporating path selection layers and learnable connect\nlayers to estimate optimal paths and perform efficient inference within a\nunified network. Furthermore, we propose a performance-aware path-learning\nstrategy to generate path labels online by evaluating a few paths sampled from\nthe prior distribution of optimal paths and network estimations, enabling\nrobust and efficient online path learning. Experiments on five image matting\ndatasets demonstrate that the proposed PAM framework achieves competitive\nperformance across a range of computational cost constraints.",
        "This study introduces an advanced methodology for automatically identifying\nminor deformations in flat walls caused by vibrations from nearby railway\ntracks. It leverages high-density Terrestrial Laser Scanner (TLS) LiDAR surveys\nand AI\/ML techniques to collect and analyze data. The scan data is processed\ninto a detailed point cloud, which is segmented to distinguish ground points,\ntrees, buildings, and other objects. The analysis focuses on identifying\nsections along flat walls and estimating their deformations relative to the\nground orientation.\n  Findings from the study, conducted at the RGIPT campus, reveal significant\ndeformations in walls close to the railway corridor, with the highest\ndeformations ranging from 7 to 8 cm and an average of 3 to 4 cm. In contrast,\nwalls further from the corridor show negligible deformations. The developed\nautomated process for feature extraction and deformation monitoring\ndemonstrates potential for structural health monitoring. By integrating LiDAR\ndata with machine learning, the methodology provides an efficient system for\nidentifying and analyzing structural deformations, highlighting the importance\nof continuous monitoring for ensuring structural integrity and public safety in\nurban infrastructure. This approach represents a substantial advancement in\nautomated feature extraction and deformation analysis, contributing to more\neffective management of urban infrastructure.",
        "In recent years, research on humanoid robots has garnered significant\nattention, particularly in reinforcement learning based control algorithms,\nwhich have achieved major breakthroughs. Compared to traditional model-based\ncontrol algorithms, reinforcement learning based algorithms demonstrate\nsubstantial advantages in handling complex tasks. Leveraging the large-scale\nparallel computing capabilities of GPUs, contemporary humanoid robots can\nundergo extensive parallel training in simulated environments. A physical\nsimulation platform capable of large-scale parallel training is crucial for the\ndevelopment of humanoid robots. As one of the most complex robot forms,\nhumanoid robots typically possess intricate mechanical structures, encompassing\nnumerous series and parallel mechanisms. However, many reinforcement learning\nbased humanoid robot control algorithms currently employ open-loop topologies\nduring training, deferring the conversion to series-parallel structures until\nthe sim2real phase. This approach is primarily due to the limitations of\nphysics engines, as current GPU-based physics engines often only support\nopen-loop topologies or have limited capabilities in simulating\nmulti-rigid-body closed-loop topologies. For enabling reinforcement\nlearning-based humanoid robot control algorithms to train in large-scale\nparallel environments, we propose a novel training method LiPS. By\nincorporating multi-rigid-body dynamics modeling in the simulation environment,\nwe significantly reduce the sim2real gap and the difficulty of converting to\nparallel structures during model deployment, thereby robustly supporting\nlarge-scale reinforcement learning for humanoid robots.",
        "Data-driven optimization aims to translate a machine learning model into\ndecision-making by optimizing decisions on estimated costs. Such a pipeline can\nbe conducted by fitting a distributional model which is then plugged into the\ntarget optimization problem. While this fitting can utilize traditional methods\nsuch as maximum likelihood, a more recent approach uses estimation-optimization\nintegration that minimizes decision error instead of estimation error. Although\nintuitive, the statistical benefit of the latter approach is not well\nunderstood yet is important to guide the prescriptive usage of machine\nlearning. In this paper, we dissect the performance comparisons between these\napproaches in terms of the amount of model misspecification. In particular, we\nshow how the integrated approach offers a ``universal double benefit'' on the\ntop two dominating terms of regret when the underlying model is misspecified,\nwhile the traditional approach can be advantageous when the model is nearly\nwell-specified. Our comparison is powered by finite-sample tail regret bounds\nthat are derived via new higher-order expansions of regrets and the leveraging\nof a recent Berry-Esseen theorem.",
        "Chemical language models, combined with reinforcement learning, have shown\nsignificant promise to efficiently traverse large chemical spaces in drug\ndesign. However, the performance of various RL algorithms and their best\npractices for practical drug design are still unclear. Here, starting from the\nprinciples of the REINFORCE algorithm, we investigate the effect of different\ncomponents from RL theory including experience replay, hill-climbing, baselines\nto reduce variance, and alternative reward shaping. Additionally we demonstrate\nhow RL hyperparameters can be fine-tuned for effectiveness, efficiency, or\nchemical regularization as demonstrated using the MolOpt benchmark.",
        "We report on the first loosely coherent search for binary systems. We\nsearched 0.06 rad disk in the Orion spur, covering gravitational wave\nfrequencies from 100 to 700 Hz and frequency derivatives between -1e-11 to\n1e-11 Hz\/s. A follow-up was performed, which found no outliers. An atlas of\nresults from the first stage of the search is made publicly available.",
        "We investigate integral representation of vector-valued function spaces,\ni.e., of subspaces $H\\subset C(K,E)$, where $K$ is a compact space and $E$ is a\n(real or complex) Banach space. We point out that there are two possible ways\nof generalizing representation theorems known from the scalar case -- either\none may represent (all) functionals from $H^*$ using $E^*$-valued vector\nmeasures on $K$ (as it is done in the literature) or one may represent (some)\noperators from $L(H,E)$ by scalar measures on $K$ using the Bochner integral.\nThese two ways lead to two different notions of simpliciality which we call\n`vector simpliciality' and `weak simpliciality'. It turns out that these two\nnotions are in general incomparable. Moreover, the weak simpliciality is not\naffected by renorming the target space $E$, while vector simpliciality may be\naffected. Further, if $H$ contains constants, vector simpliciality is strictly\nstronger and admits several characterizations (partially analogous to the\ncharacterizations known in the scalar case). We also study orderings of\nmeasures inspired by C.J.K.~Batty which may be (in special cases) used to\ncharacterize $H$-boundary measures. Finally, we give a finer version of\nrepresentation theorem using positive measures on $K\\times B_{E^*}$ and\ncharacterize uniqueness in this case.",
        "We consider a regularised Fermi projection of the Hamiltonian of the massless\nDirac equation at Fermi energy zero. The matrix-valued symbol of the resulting\noperator is discontinuous in the origin. For this operator, we prove\nSzeg\\H{o}-type asymptotics with the spatial cut-off domains given by\n$d$-dimensional cubes. For analytic test functions, we obtain a $d$-term\nasymptotic expansion and provide an upper bound of logarithmic order for the\nremaining terms. This bound does not depend on the regularisation. In the\nspecial case that the test function is given by a polynomial of degree less or\nequal than three, we prove a $(d+1)$-term asymptotic expansion with an error\nterm of constant order. The additional term is of logarithmic order and its\ncoefficient is independent of the regularisation.",
        "Recent advancements in music generation have garnered significant attention,\nyet existing approaches face critical limitations. Some current generative\nmodels can only synthesize either the vocal track or the accompaniment track.\nWhile some models can generate combined vocal and accompaniment, they typically\nrely on meticulously designed multi-stage cascading architectures and intricate\ndata pipelines, hindering scalability. Additionally, most systems are\nrestricted to generating short musical segments rather than full-length songs.\nFurthermore, widely used language model-based methods suffer from slow\ninference speeds. To address these challenges, we propose DiffRhythm, the first\nlatent diffusion-based song generation model capable of synthesizing complete\nsongs with both vocal and accompaniment for durations of up to 4m45s in only\nten seconds, maintaining high musicality and intelligibility. Despite its\nremarkable capabilities, DiffRhythm is designed to be simple and elegant: it\neliminates the need for complex data preparation, employs a straightforward\nmodel structure, and requires only lyrics and a style prompt during inference.\nAdditionally, its non-autoregressive structure ensures fast inference speeds.\nThis simplicity guarantees the scalability of DiffRhythm. Moreover, we release\nthe complete training code along with the pre-trained model on large-scale data\nto promote reproducibility and further research.",
        "Automating labor-intensive tasks such as crop monitoring with robots is\nessential for enhancing production and conserving resources. However,\nautonomously monitoring horticulture crops remains challenging due to their\ncomplex structures, which often result in fruit occlusions. Existing view\nplanning methods attempt to reduce occlusions but either struggle to achieve\nadequate coverage or incur high robot motion costs. We introduce a global\noptimization approach for view motion planning that aims to minimize robot\nmotion costs while maximizing fruit coverage. To this end, we leverage coverage\nconstraints derived from the set covering problem (SCP) within a shortest\nHamiltonian path problem (SHPP) formulation. While both SCP and SHPP are\nwell-established, their tailored integration enables a unified framework that\ncomputes a global view path with minimized motion while ensuring full coverage\nof selected targets. Given the NP-hard nature of the problem, we employ a\nregion-prior-based selection of coverage targets and a sparse graph structure\nto achieve effective optimization outcomes within a limited time. Experiments\nin simulation demonstrate that our method detects more fruits, enhances surface\ncoverage, and achieves higher volume accuracy than the motion-efficient\nbaseline with a moderate increase in motion cost, while significantly reducing\nmotion costs compared to the coverage-focused baseline. Real-world experiments\nfurther confirm the practical applicability of our approach.",
        "Fixing Python dependency issues is a tedious and error-prone task for\ndevelopers, who must manually identify and resolve environment dependencies and\nversion constraints of third-party modules and Python interpreters. Researchers\nhave attempted to automate this process by relying on large knowledge graphs\nand database lookup tables. However, these traditional approaches face\nlimitations due to the variety of dependency error types, large sets of\npossible module versions, and conflicts among transitive dependencies. This\nstudy explores the potential of using large language models (LLMs) to\nautomatically fix dependency issues in Python programs. We introduce PLLM\n(pronounced \"plum\"), a novel technique that employs retrieval-augmented\ngeneration (RAG) to help an LLM infer Python versions and required modules for\na given Python file. PLLM builds a testing environment that iteratively (1)\nprompts the LLM for module combinations, (2) tests the suggested changes, and\n(3) provides feedback (error messages) to the LLM to refine the fix. This\nfeedback cycle leverages natural language processing (NLP) to intelligently\nparse and interpret build error messages. We benchmark PLLM on the Gistable\nHG2.9K dataset, a collection of challenging single-file Python gists. We\ncompare PLLM against two state-of-the-art automatic dependency inference\napproaches, namely PyEGo and ReadPyE, w.r.t. the ability to resolve dependency\nissues. Our results indicate that PLLM can fix more dependency issues than the\ntwo baselines, with +218 (+15.97%) more fixes over ReadPyE and +281 (+21.58%)\nover PyEGo. Our deeper analyses suggest that PLLM is particularly beneficial\nfor projects with many dependencies and for specific third-party numerical and\nmachine-learning modules. Our findings demonstrate the potential of LLM-based\napproaches to iteratively resolve Python dependency issues.",
        "The even online Kolmogorov complexity of a string $x = x_1 x_2 \\cdots x_{n}$\nis the minimal length of a program that for all $i\\le n\/2$, on input $x_1x_3\n\\cdots x_{2i-1}$ outputs $x_{2i}$. The odd complexity is defined similarly. The\nsum of the odd and even complexities is called the dialogue complexity.\n  In [Bauwens, 2014] it is proven that for all $n$, there exist $n$-bit $x$ for\nwhich the dialogue complexity exceeds the Kolmogorov complexity by $n\\log \\frac\n4 3 + O(\\log n)$. Let $\\mathrm C^s(x)$ denote the Kolmogorov complexity with\nspace bound~$s$. Here, we prove that the space-bounded dialogue complexity with\nbound $s + 6n + O(1)$ is at most $\\mathrm C^{s}(x) + O(\\log (sn))$, where\n$n=|x|$.",
        "With greater design complexity, the challenge to anticipate and mitigate\nsecurity issues provides more responsibility for the designer. As hardware\nprovides the foundation of a secure system, we need tools and techniques that\nsupport engineers to improve trust and help them address security concerns.\nKnowing the security assets in a design is fundamental to downstream security\nanalyses, such as threat modeling, weakness identification, and verification.\nThis paper proposes an automated approach for the initial identification of\npotential security assets in a Verilog design. Taking inspiration from manual\nasset identification methodologies, we analyze open-source hardware designs in\nthree IP families and identify patterns and commonalities likely to indicate\nstructural assets. Through iterative refinement, we provide a potential set of\nprimary security assets and thus help to reduce the manual search space.",
        "The problem of tracking the source of a passive scalar in a turbulent flow is\nrelevant to flying insect behavior and several other applications. Extensive\nprevious work has shown that certain Bayesian strategies, such as \"infotaxis,\"\ncan be very effective for this difficult \"olfactory search\" problem. More\nrecently, a quasi-optimal Bayesian strategy was computed under the assumption\nthat encounters with the scalar are independent. However, the Bayesian approach\nhas not been adequately studied in realistic flows which exhibit spatiotemporal\ncorrelations. In this work, we perform direct numerical simulations (DNS) of an\nincompressible flow at $\\mathrm{Re}_\\lambda\\simeq150,$ while tracking\nLagrangian particles which are emitted by a point source and imposing a uniform\nmean flow with several magnitudes (including zero). We extract the\nspatially-dependent statistics of encounters with the particles, which we use\nto build Bayesian policies, including generalized (\"space-aware\") infotactic\nheuristics and quasi-optimal policies. We then assess the relative performance\nof these policies when they are used to search using scalar cue data from the\nDNS, and in particular study how this performance depends on correlations\nbetween encounters. Among other results, we find that quasi-optimal strategies\ncontinue to outperform heuristics in the presence of strong mean flow but fail\nto do so in the absence of a mean flow. We also explore how to choose optimal\nsearch parameters, including the frequency and threshold concentration of\nobservation.",
        "Recent advancements in text-to-image (T2I) have improved synthesis results,\nbut challenges remain in layout control and generating omnidirectional\npanoramic images. Dense T2I (DT2I) and spherical T2I (ST2I) models address\nthese issues, but so far no unified approach exists. Trivial approaches, like\nprompting a DT2I model to generate panoramas can not generate proper spherical\ndistortions and seamless transitions at the borders. Our work shows that\nspherical dense text-to-image (SDT2I) can be achieved by integrating\ntraining-free DT2I approaches into finetuned panorama models. Specifically, we\npropose MultiStitchDiffusion (MSTD) and MultiPanFusion (MPF) by integrating\nMultiDiffusion into StitchDiffusion and PanFusion, respectively. Since no\nbenchmark for SDT2I exists, we further construct Dense-Synthetic-View\n(DSynView), a new synthetic dataset containing spherical layouts to evaluate\nour models. Our results show that MSTD outperforms MPF across image quality as\nwell as prompt- and layout adherence. MultiPanFusion generates more diverse\nimages but struggles to synthesize flawless foreground objects. We propose\nbootstrap-coupling and turning off equirectangular perspective-projection\nattention in the foreground as an improvement of MPF. Link to code\nhttps:\/\/github.com\/sdt2i\/spherical-dense-text-to-image",
        "We study the piecewise constant bandit problem where the expected reward is a\npiecewise constant function with one change point (discontinuity) across the\naction space $[0,1]$ and the learner's aim is to locate the change point. Under\nthe assumption of a fixed exploration budget, we provide the first\nnon-asymptotic analysis of policies designed to locate abrupt changes in the\nmean reward function under bandit feedback. We study the problem under a large\nand small budget regime, and for both settings establish lower bounds on the\nerror probability and provide algorithms with near matching upper bounds.\nInterestingly, our results show a separation in the complexity of the two\nregimes. We then propose a regime adaptive algorithm which is near optimal for\nboth small and large budgets simultaneously. We complement our theoretical\nanalysis with experimental results in simulated environments to support our\nfindings.",
        "We provide new families of compact complex manifolds with no K\\\"ahler\nstructure carrying symplectic structures satisfying the \\textit{Hard Lefschetz\nCondition}. These examples are obtained as compact quotients of the solvable\nLie group $\\mathbb{C}^{2n} \\ltimes_{\\rho} \\mathbb{C}^{2m}$, for which we\nconstruct explicit lattices. By cohomological computations we prove that such\nmanifolds carry symplectic structures satisfying the \\textit{Hard Lefschetz\nCondition}. Furthermore, we compute the Kodaira dimension of an almost-K\\\"ahler\nstructure and generators for the de Rham and Dolbeault cohomologies."
      ]
    }
  },
  {
    "id":2411.15211,
    "research_type":"applied",
    "start_id":"b6",
    "start_title":"Solar Cells for Indoor Applications: Progress and Development",
    "start_abstract":"The Internet of things (IoT) has been rapidly growing in the past few years. IoT connects numerous devices, such as wireless sensors, actuators, and wearable to optimize monitor daily activities. Most these devices require power microwatt range operate indoors. To this end, a self-sustainable source, photovoltaic (PV) cell, which can harvest low-intensity indoor light, is appropriate. Recently, development highly efficient PV cells for applications attracted tremendous attention. Therefore, different types materials, inorganic, dye-sensitized, organic, perovskite have employed harvesting light energy. Although considerable efforts made by researchers develop low-cost, stable, applications, Extensive investigation necessary resolve some critical issues concerning cells, environmental stability, lifetime, large-area fabrication, mechanical flexibility, production cost. address issues, systematic review aspects will be useful research community. This study discusses current status based on previous reports. First, we provided relevant background information. Then, described sources, subsequently critically reviewed reports regarding solar active materials perovskite. Finally, placed an attempt provide insight into factors needed further improve feasibility technology applications.",
    "start_categories":[
      "astro-ph.SR"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b9"
      ],
      "title":[
        "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
      ],
      "abstract":[
        "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent models, BERT is designed to pre-train deep bidirectional representations unlabeled text by jointly conditioning on both left and right context in all layers. As result, the pre-trained can be fine-tuned with just one additional output layer create state-of-the-art models wide range of tasks, such as question answering inference, without substantial task-specific architecture modifications. conceptually simple empirically powerful. It obtains results eleven natural processing including pushing GLUE score 80.5% (7.7% point absolute improvement), MultiNLI accuracy 86.7% (4.6% SQuAD v1.1 Test F1 93.2 (1.5 improvement) v2.0 83.1 (5.1 improvement)."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Limited Effectiveness of LLM-based Data Augmentation for COVID-19\n  Misinformation Stance Detection",
        "The normal growth of linear groups over formal power serieses",
        "The multi-level friendship paradox for sparse random graphs",
        "The duality resolution at $n=p=2$",
        "Optimal Control of the Navier-Stokes equations via Pressure Boundary\n  Conditions",
        "Clinically Ready Magnetic Microrobots for Targeted Therapies",
        "How can representation dimension dominate structurally pruned LLMs?",
        "Deterministic Global Optimization over trained Kolmogorov Arnold\n  Networks",
        "EOG Communication Interface for Quadriplegics: Prototype & Signal\n  Processing",
        "The Popularity Hypothesis in Software Security: A Large-Scale\n  Replication with PHP Packages",
        "Transformer Based Time-Series Forecasting for Stock",
        "Defending Against Gradient Inversion Attacks for Biomedical Images via\n  Learnable Data Perturbation",
        "Cognitive AI framework: advances in the simulation of human thought",
        "DreamFLEX: Learning Fault-Aware Quadrupedal Locomotion Controller for\n  Anomaly Situation in Rough Terrains",
        "Negativity in Self-Admitted Technical Debt: How Sentiment Influences\n  Prioritization",
        "LGL-BCI: A Motor-Imagery-Based Brain-Computer Interface with Geometric\n  Learning",
        "HERMES: High-Performance RISC-V Memory Hierarchy for ML Workloads",
        "InPK: Infusing Prior Knowledge into Prompt for Vision-Language Models",
        "Translate Smart, not Hard: Cascaded Translation Systems with\n  Quality-Aware Deferral",
        "Two characterizations of Sheffer-Dunkl sequences",
        "A random polymer approach to the weak disorder phase of the vertex\n  reinforced jump process",
        "A priori estimates of Mizohata-Takeuchi type for the Navier-Lam\\'e\n  operator",
        "Gaussian Process Upper Confidence Bound Achieves Nearly-Optimal Regret\n  in Noise-Free Gaussian Process Bandits",
        "Prediction-Assisted Online Distributed Deep Learning Workload Scheduling\n  in GPU Clusters",
        "ComplexBeat: Breathing Rate Estimation from Complex CSI",
        "A Basis Theorem for Rings with Commuting Operators in Characteristic\n  Zero",
        "Linguistic Generalizations are not Rules: Impacts on Evaluation of LMs",
        "$q$-Numerical radius of sectorial matrices and $2 \\times 2$ operator\n  matrices",
        "Application of Artificial Intelligence (AI) in Civil Engineering"
      ],
      "abstract":[
        "Misinformation surrounding emerging outbreaks poses a serious societal\nthreat, making robust countermeasures essential. One promising approach is\nstance detection (SD), which identifies whether social media posts support or\noppose misleading claims. In this work, we finetune classifiers on COVID-19\nmisinformation SD datasets consisting of claims and corresponding tweets.\nSpecifically, we test controllable misinformation generation (CMG) using large\nlanguage models (LLMs) as a method for data augmentation. While CMG\ndemonstrates the potential for expanding training datasets, our experiments\nreveal that performance gains over traditional augmentation methods are often\nminimal and inconsistent, primarily due to built-in safeguards within LLMs. We\nrelease our code and datasets to facilitate further research on misinformation\ndetection and generation.",
        "Put $R=\\F[[t_1, \\ldots, t_d]])$. We estimate the number of normal subgroups\nof $\\mathrm{SL}_2^1(\\F[[t_1, \\ldots, t_d]])$ for $p>2$, the number of ideals in\nthe Lie algebra $\\Lie(R)$, and the number of ideals in the associative algebra\n$R$.",
        "In Hazra, den Hollander and Parvaneh (2025) we analysed the friendship\nparadox for sparse random graphs. For four classes of random graphs we\ncharacterised the empirical distribution of the friendship biases between\nvertices and their neighbours at distance $1$, proving convergence as\n$n\\to\\infty$ to a limiting distribution, with $n$ the number of vertices, and\nidentifying moments and tail exponents of the limiting distribution. In the\npresent paper we look at the multi-level friendship bias between vertices and\ntheir neighbours at distance $k \\in \\mathbb{N}$ obtained via a $k$-step\nexploration according to a backtracking or a non-backtracking random walk. We\nidentify the limit of empirical distribution of the multi-level friendship\nbiases as $n\\to\\infty$ and\/or $k\\to\\infty$. We show that for non-backtracking\nexploration the two limits commute for a large class of sparse random graphs,\nincluding those that locally converge to a rooted Galton-Watson tree. In\nparticular, we show that the same limit arises when $k$ depends on $n$, i.e.,\n$k=k_n$, provided $\\lim_{n\\to\\infty} k_n = \\infty$ under some mild conditions.\nWe exhibit cases where the two limits do not commute and show the relevance of\nthe mixing time of the exploration.",
        "Working at the prime $2$ and chromatic height $2$, we construct a finite\nresolution of the homotopy fixed points of Morava $E$-theory with respect to\nthe subgroup $\\mathbb{G}_2^1$ of the Morava stabilizer group. This is an\nupgrade of the finite resolution of the homotopy fixed points of $E$-theory\nwith respect to the subgroup $\\mathbb{S}_2^1$ constructed in work of\nGoerss-Henn-Mahowald-Rezk, Beaudry and Bobkova-Goerss.",
        "In this work we study an optimal control problem subject to the instationary\nNavier-Stokes equations, where the control enters via an inhomogeneous\nNeumann\/Do-Nothing boundary condition. Despite the Navier-Stokes equations with\nthese boundary conditions not being well-posed for large times and\/or data, we\nobtain wellposedness of the optimal control problem by choosing a proper\ntracking type term. In order to discuss the regularity of the optimal control,\nstate and adjoint state, we present new results on $L^2(I;H^2(\\Omega))$\nregularity of solutions to a Stokes problem with mixed inhomogeneous boundary\nconditions.",
        "Systemic drug administration often causes off-target effects limiting the\nefficacy of advanced therapies. Targeted drug delivery approaches increase\nlocal drug concentrations at the diseased site while minimizing systemic drug\nexposure. We present a magnetically guided microrobotic drug delivery system\ncapable of precise navigation under physiological conditions. This platform\nintegrates a clinical electromagnetic navigation system, a custom-designed\nrelease catheter, and a dissolvable capsule for accurate therapeutic delivery.\nIn vitro tests showed precise navigation in human vasculature models, and in\nvivo experiments confirmed tracking under fluoroscopy and successful navigation\nin large animal models. The microrobot balances magnetic material\nconcentration, contrast agent loading, and therapeutic drug capacity, enabling\neffective hosting of therapeutics despite the integration complexity of its\ncomponents, offering a promising solution for precise targeted drug delivery.",
        "Pruning assumes a subnetwork exists in the original deep neural network,\nwhich can achieve comparative model performance with less computation than the\noriginal. However, it is unclear how the model performance varies with the\ndifferent subnetwork extractions. In this paper, we choose the representation\ndimension (or embedding dimension, model dimension, the dimension of the\nresidual stream in the relevant literature) as the entry point to this issue.\nWe investigate the linear transformations in the LLM transformer blocks and\nconsider a specific structured pruning approach, SliceGPT, to extract the\nsubnetworks of different representation dimensions. We mechanistically analyse\nthe activation flow during the model forward passes, and find the\nrepresentation dimension dominates the linear transformations, model\npredictions, and, finally, the model performance. Explicit analytical relations\nare given to calculate the pruned model performance (perplexity and accuracy)\nwithout actual evaluation, and are empirically validated with\nLlama-3-8B-Instruct and Phi-3-mini-4k-Instruct.",
        "To address the challenge of tractability for optimizing mathematical models\nin science and engineering, surrogate models are often employed. Recently, a\nnew class of machine learning models named Kolmogorov Arnold Networks (KANs)\nhave been proposed. It was reported that KANs can approximate a given\ninput\/output relationship with a high level of accuracy, requiring\nsignificantly fewer parameters than multilayer perceptrons. Hence, we aim to\nassess the suitability of deterministic global optimization of trained KANs by\nproposing their Mixed-Integer Nonlinear Programming (MINLP) formulation. We\nconduct extensive computational experiments for different KAN architectures.\nAdditionally, we propose alternative convex hull reformulation, local support\nand redundant constraints for the formulation aimed at improving the\neffectiveness of the MINLP formulation of the KAN. KANs demonstrate high\naccuracy while requiring relatively modest computational effort to optimize\nthem, particularly for cases with less than five inputs or outputs. For cases\nwith higher inputs or outputs, carefully considering the KAN architecture\nduring training may improve its effectiveness while optimizing over a trained\nKAN. Overall, we observe that KANs offer a promising alternative as surrogate\nmodels for deterministic global optimization.",
        "Electrooculography (EOG) is an electrophysiological signal that determines\nthe human eye orientation and is therefore widely used in Human Tracking\nInterfaces (HCI). The purpose of this project is to develop a communication\nmethod for quadriplegic patients using EOG signals aimed at text and voice\ngeneration. The system consists of 3D eye movement tracking embedded using a\ncustom-built prototype to measure the eyeball's left-right and up-down\nmovements. The ESP32 board, which has a set of parameters to convert the data\ninto content displayed on LCDs and MP3 players, is used to capture and process\nthe signal. helps people by facilitating more natural and efficient symptom\nexpression. The blink system will be able to incorporate face masks and more\neye tests as it continues to develop. Even if it might work, more research and\nclinical trials are needed to evaluate the system's usefulness and ensure that\nit performs as planned in real-world scenarios. With this project, assistive\ntechnology will make significant progress and improve the lives of many who\nsuffer from severe motor impairments.",
        "There has been a long-standing hypothesis that a software's popularity is\nrelated to its security or insecurity in both research and popular discourse.\nThere are also a few empirical studies that have examined the hypothesis,\neither explicitly or implicitly. The present work continues with and\ncontributes to this research with a replication-motivated large-scale analysis\nof software written in the PHP programming language. The dataset examined\ncontains nearly four hundred thousand open source software packages written in\nPHP. According to the results based on reported security vulnerabilities, the\nhypothesis does holds; packages having been affected by vulnerabilities over\ntheir release histories are generally more popular than packages without having\nbeen affected by a single vulnerability. With this replication results, the\npaper contributes to the efforts to strengthen the empirical knowledge base in\ncyber and software security.",
        "To the naked eye, stock prices are considered chaotic, dynamic, and\nunpredictable. Indeed, it is one of the most difficult forecasting tasks that\nhundreds of millions of retail traders and professional traders around the\nworld try to do every second even before the market opens. With recent advances\nin the development of machine learning and the amount of data the market\ngenerated over years, applying machine learning techniques such as deep\nlearning neural networks is unavoidable. In this work, we modeled the task as a\nmultivariate forecasting problem, instead of a naive autoregression problem.\nThe multivariate analysis is done using the attention mechanism via applying a\nmutated version of the Transformer, \"Stockformer\", which we created.",
        "The increasing need for sharing healthcare data and collaborating on clinical\nresearch has raised privacy concerns. Health information leakage due to\nmalicious attacks can lead to serious problems such as misdiagnoses and patient\nidentification issues. Privacy-preserving machine learning (PPML) and\nprivacy-enhancing technologies, particularly federated learning (FL), have\nemerged in recent years as innovative solutions to balance privacy protection\nwith data utility; however, they also suffer from inherent privacy\nvulnerabilities. Gradient inversion attacks constitute major threats to data\nsharing in federated learning. Researchers have proposed many defenses against\ngradient inversion attacks. However, current defense methods for healthcare\ndata lack generalizability, i.e., existing solutions may not be applicable to\ndata from a broader range of populations. In addition, most existing defense\nmethods are tested using non-healthcare data, which raises concerns about their\napplicability to real-world healthcare systems. In this study, we present a\ndefense against gradient inversion attacks in federated learning. We achieve\nthis using latent data perturbation and minimax optimization, utilizing both\ngeneral and medical image datasets. Our method is compared to two baselines,\nand the results show that our approach can outperform the baselines with a\nreduction of 12.5% in the attacker's accuracy in classifying reconstructed\nimages. The proposed method also yields an increase of over 12.4% in Mean\nSquared Error (MSE) between the original and reconstructed images at the same\nlevel of model utility of around 90% client classification accuracy. The\nresults suggest the potential of a generalizable defense for healthcare data.",
        "The Human Cognitive Simulation Framework represents a significant advancement\nin integrating human cognitive capabilities into artificial intelligence\nsystems. By merging short-term memory (conversation context), long-term memory\n(interaction context), advanced cognitive processing, and efficient knowledge\nmanagement, it ensures contextual coherence and persistent data storage,\nenhancing personalization and continuity in human-AI interactions. The\nframework employs a unified database that synchronizes these contexts while\nincorporating logical, creative, and analog processing modules inspired by\nhuman brain hemispheric functions to perform structured tasks and complex\ninferences. Dynamic knowledge updates enable real-time integration, improving\nadaptability and fostering applications in education, behavior analysis, and\nknowledge management. Despite its potential to process vast data volumes and\nenhance user experience, challenges remain in scalability, cognitive bias\nmitigation, and ethical compliance. This framework lays the foundation for\nfuture research in continuous learning algorithms, sustainability, and\nmultimodal adaptability, positioning Cognitive AI as a transformative model in\nemerging fields.",
        "Recent advances in quadrupedal robots have demonstrated impressive agility\nand the ability to traverse diverse terrains. However, hardware issues, such as\nmotor overheating or joint locking, may occur during long-distance walking or\ntraversing through rough terrains leading to locomotion failures. Although\nseveral studies have proposed fault-tolerant control methods for quadrupedal\nrobots, there are still challenges in traversing unstructured terrains. In this\npaper, we propose DreamFLEX, a robust fault-tolerant locomotion controller that\nenables a quadrupedal robot to traverse complex environments even under joint\nfailure conditions. DreamFLEX integrates an explicit failure estimation and\nmodulation network that jointly estimates the robot's joint fault vector and\nutilizes this information to adapt the locomotion pattern to faulty conditions\nin real-time, enabling quadrupedal robots to maintain stability and performance\nin rough terrains. Experimental results demonstrate that DreamFLEX outperforms\nexisting methods in both simulation and real-world scenarios, effectively\nmanaging hardware failures while maintaining robust locomotion performance.",
        "Self-Admitted Technical Debt, or SATD, is a self-admission of technical debt\npresent in a software system. To effectively manage SATD, developers need to\nestimate its priority and assess the effort required to fix the described\ntechnical debt. About a quarter of descriptions of SATD in software systems\nexpress some form of negativity or negative emotions when describing technical\ndebt. In this paper, we report on an experiment conducted with 59 respondents\nto study whether negativity expressed in the description of SATD\n\\textbf{actually} affects the prioritization of SATD. The respondents are a mix\nof professional developers and students, and in the experiment, we asked\nparticipants to prioritize four vignettes: two expressing negativity and two\nexpressing neutral sentiment. To ensure realism, vignettes were based on\nexisting SATD. We find that negativity causes between one-third and half of\ndevelopers to prioritize SATD, in which negativity is expressed as having more\npriority. Developers affected by negativity when prioritizing SATD are twice as\nlikely to increase their estimation of urgency and 1.5 times as likely to\nincrease their estimation of importance and effort for SATD compared to the\nlikelihood of decreasing these prioritization scores. Our findings show how\ndevelopers actively use negativity in SATD to determine how urgently a\nparticular instance of TD should be addressed. However, our study also\ndescribes a gap in the actions and belief of developers. Even if 33% to 50% use\nnegativity to prioritize SATD, 67% of developers believe that using negativity\nas a proxy for priority is unacceptable. Therefore, we would not recommend\nusing negativity as a proxy for priority. However, we also recognize that\ndevelopers might unavoidably express negativity when describing technical debt.",
        "Brain--computer interfaces are groundbreaking technology whereby brain\nsignals are used to control external devices. Despite some advances in recent\nyears, electroencephalogram (EEG)-based motor-imagery tasks face challenges,\nsuch as amplitude and phase variability and complex spatial correlations, with\na need for smaller models and faster inference. In this study, we develop a\nprototype, called the Lightweight Geometric Learning Brain--Computer Interface\n(LGL-BCI), which uses our customized geometric deep learning architecture for\nswift model inference without sacrificing accuracy. LGL-BCI contains an EEG\nchannel selection module via a feature decomposition algorithm to reduce the\ndimensionality of a symmetric positive definite matrix, providing adaptiveness\namong the continuously changing EEG signal. Meanwhile, a built-in lossless\ntransformation helps boost the inference speed. The performance of our solution\nwas evaluated using two real-world EEG devices and two public EEG datasets.\nLGL-BCI demonstrated significant improvements, achieving an accuracy of 82.54%\ncompared to 62.22% for the state-of-the-art approach. Furthermore, LGL-BCI uses\nfewer parameters (64.9K vs. 183.7K), highlighting its computational efficiency.\nThese findings underscore both the superior accuracy and computational\nefficiency of LGL-BCI, demonstrating the feasibility and robustness of\ngeometric deep learning in motor-imagery brain--computer interface\napplications.",
        "The growth of machine learning (ML) workloads has underscored the importance\nof efficient memory hierarchies to address bandwidth, latency, and scalability\nchallenges. HERMES focuses on optimizing memory subsystems for RISC-V\narchitectures to meet the computational needs of ML models such as CNNs, RNNs,\nand Transformers. This project explores state-of-the-art techniques such as\nadvanced prefetching, tensor-aware caching, and hybrid memory models. The\ncornerstone of HERMES is the integration of shared L3 caches with fine-grained\ncoherence protocols equipped with specialized pathways to deep-learning\naccelerators such as Gemmini. Simulation tools like Gem5 and DRAMSim2 were used\nto evaluate baseline performance and scalability under representative ML\nworkloads. The findings of this study highlight the design choices, and the\nanticipated challenges, paving the way for low-latency scalable memory\noperations for ML applications.",
        "Prompt tuning has become a popular strategy for adapting Vision-Language\nModels (VLMs) to zero\/few-shot visual recognition tasks. Some prompting\ntechniques introduce prior knowledge due to its richness, but when learnable\ntokens are randomly initialized and disconnected from prior knowledge, they\ntend to overfit on seen classes and struggle with domain shifts for unseen\nones. To address this issue, we propose the InPK model, which infuses\nclass-specific prior knowledge into the learnable tokens during initialization,\nthus enabling the model to explicitly focus on class-relevant information.\nFurthermore, to mitigate the weakening of class information by multi-layer\nencoders, we continuously reinforce the interaction between learnable tokens\nand prior knowledge across multiple feature levels. This progressive\ninteraction allows the learnable tokens to better capture the fine-grained\ndifferences and universal visual concepts within prior knowledge, enabling the\nmodel to extract more discriminative and generalized text features. Even for\nunseen classes, the learned interaction allows the model to capture their\ncommon representations and infer their appropriate positions within the\nexisting semantic structure. Moreover, we introduce a learnable text-to-vision\nprojection layer to accommodate the text adjustments, ensuring better alignment\nof visual-text semantics. Extensive experiments on 11 recognition datasets show\nthat InPK significantly outperforms state-of-the-art methods in multiple\nzero\/few-shot image classification tasks.",
        "Larger models often outperform smaller ones but come with high computational\ncosts. Cascading offers a potential solution. By default, it uses smaller\nmodels and defers only some instances to larger, more powerful models. However,\ndesigning effective deferral rules remains a challenge. In this paper, we\npropose a simple yet effective approach for machine translation, using existing\nquality estimation (QE) metrics as deferral rules. We show that QE-based\ndeferral allows a cascaded system to match the performance of a larger model\nwhile invoking it for a small fraction (30% to 50%) of the examples,\nsignificantly reducing computational costs. We validate this approach through\nboth automatic and human evaluation.",
        "Sheffer polynomials can be characterized using different Stieltjes integrals.\nThese families of polynomials have been recently extended to the Dunkl context.\nIn this way some classical operators as the derivative operator or the\ndifference operator are replaced as analogous operators in the Dunkl universe.\nIn this paper we establish two Stieltjes integrals that help us to characterize\nthe Sheffer-Dunkl polynomials.",
        "In this paper, we study the transient phase of the Vertex Reinforced Jump\nProcess (VRJP) in dimension $d\\geq 3$. In Sabot, Zeng (2019), the authors\nintroduce a positive martingale and show that the VRJP is recurrent if and only\nif that martingale converges to $0$. On $\\mathbb{Z}^d$, $d\\ge 3$, with constant\nconductances $W$, it can be shown that there is a critical value\n$0<W_c(\\mathbb{Z}^d)<\\infty$, such that the martingale converges to $0$ if\n$W<W_c(\\mathbb{Z}^d)$ or to a positive limit if $W>W_c(\\mathbb{Z}^d)$. On the\nother hand, the VRJP martingale can be interpreted as the partition function of\na non-directed polymer with a very specific $1$-dependent random potential. In\nthis paper, we focus on the question of the $L^p$ integrability of the VRJP\nmartingale, which is related to the (diffusive) behavior of the VRJP. First,\ntaking inspiration from the work of Junk (2022) for directed polymers in\n$\\mathbb{Z}^{1+d}$, we prove that on the half-space $\\mathbb{H}_d$ of\n$\\mathbb{Z}^d$, for all $W>W_c(\\mathbb{H}_d)$ there is some $\\delta>0$ such\nthat the VRJP martingale is in $L^{1+\\delta}$. Second, we prove that, in\ndimension $d\\geq 4$, the VRJP martingale is in $L^{p}$ for all $p>1$ above the\n``slab critical point'' $W_c^{\\mathrm{slab}} (\\mathbb{Z}^d) = \\lim_{m\\to\\infty}\nW_c(\\mathbb{Z}^{d-1} \\times \\{-m,\\ldots,m\\})$. We also propose some related\nconjectures.",
        "The Mizohata-Takeuchi conjecture for the resolvent of the Navier-Lam\\'e\nequation is a weighted estimate with weights in the so-called Mizohata-Takeuchi\nclass for this operator when one approaches the spectrum (Limiting Absorption\nPrinciples). We prove this conjecture in dimensions 2 and 3 for weights with a\nradial majorant in the Mizohata-Takeuchi class. This result can be seen as an\nextension of the analogue for the Laplacian given in [8]. We also prove that\nradial weights in this class are not invariant for the Hardy-Littlewood maximal\nfunction, hence the methods in [6] used to extend estimates for the Laplacian\nto the Navier-Lam\\'e case, do not work.",
        "We study the noise-free Gaussian Process (GP) bandits problem, in which the\nlearner seeks to minimize regret through noise-free observations of the\nblack-box objective function lying on the known reproducing kernel Hilbert\nspace (RKHS). Gaussian process upper confidence bound (GP-UCB) is the\nwell-known GP-bandits algorithm whose query points are adaptively chosen based\non the GP-based upper confidence bound score. Although several existing works\nhave reported the practical success of GP-UCB, the current theoretical results\nindicate its suboptimal performance. However, GP-UCB tends to perform well\nempirically compared with other nearly optimal noise-free algorithms that rely\non a non-adaptive sampling scheme of query points. This paper resolves this gap\nbetween theoretical and empirical performance by showing the nearly optimal\nregret upper bound of noise-free GP-UCB. Specifically, our analysis shows the\nfirst constant cumulative regret in the noise-free settings for the squared\nexponential kernel and Mat\\'ern kernel with some degree of smoothness.",
        "The recent explosive growth of deep learning (DL) models has necessitated a\ncompelling need for efficient job scheduling for distributed deep learning\ntraining with mixed parallelisms (DDLwMP) in GPU clusters. This paper proposes\nan adaptive shortest-remaining-processing-time-first (A-SRPT) scheduling\nalgorithm, a novel prediction-assisted online scheduling approach designed to\nmitigate the challenges associated with DL cluster scheduling. By modeling each\njob as a graph corresponding to heterogeneous Deep Neural Network (DNN) models\nand their associated distributed training configurations, A-SRPT strategically\nassigns jobs to the available GPUs, thereby minimizing inter-server\ncommunication overhead. Observing that most DDLwMP jobs recur, A-SRPT\nincorporates a random forest regression model to predict training iterations.\nCrucially, A-SRPT maps the complex scheduling problem into a single-machine\ninstance, which is addressed optimally by a preemptive\n\"shortest-remaining-processing-time-first\" strategy. This optimized solution\nserves as a guide for actual job scheduling within the GPU clusters, leading to\na theoretically provable competitive scheduling efficiency. We conduct\nextensive real-world testbed and simulation experiments to verify our proposed\nalgorithms.",
        "In this paper, we explore the use of channel state information (CSI) from a\nWiFi system to estimate the breathing rate of a person in a room. In order to\nextract WiFi CSI components that are sensitive to breathing, we propose to\nconsider the delay domain channel impulse response (CIR), while most\nstate-of-the-art methods consider its frequency domain representation. One\nobstacle while processing the CSI data is that its amplitude and phase are\nhighly distorted by measurement uncertainties. We thus also propose an\namplitude calibration method and a phase offset calibration method for CSI\nmeasured in orthogonal frequency-division multiplexing (OFDM) multiple-input\nmultiple-output (MIMO) systems. Finally, we implement a complete breathing rate\nestimation system in order to showcase the effectiveness of our proposed\ncalibration and CSI extraction methods.",
        "Motivated by the differential basis theorem of Kolchin and the\ndifference-differential basis theorem of Cohn, in this paper we present a basis\ntheorem for polynomial rings equipped with commuting generalised Hasse-Schmidt\noperators (in the sense of Moosa and Scanlon). We recover Kolchin and Cohn's\nresults as special cases of our main theorem.",
        "Linguistic evaluations of how well LMs generalize to produce or understand\nnovel text often implicitly take for granted that natural languages are\ngenerated by symbolic rules. Grammaticality is thought to be determined by\nwhether or not sentences obey such rules. Interpretation is believed to be\ncompositionally generated by syntactic rules operating on meaningful words.\nSemantic parsing is intended to map sentences into formal logic. Failures of\nLMs to obey strict rules have been taken to reveal that LMs do not produce or\nunderstand language like humans. Here we suggest that LMs' failures to obey\nsymbolic rules may be a feature rather than a bug, because natural languages\nare not based on rules. New utterances are produced and understood by a\ncombination of flexible interrelated and context-dependent schemata or\nconstructions. We encourage researchers to reimagine appropriate benchmarks and\nanalyses that acknowledge the rich flexible generalizations that comprise\nnatural languages.",
        "This article focuses on several significant bounds of $q$-numerical radius\n$w_q(A)$ for sectorial matrix $A$ which refine and generalize previously\nestablished bounds. One of the significant bounds we have derived is as\nfollows:\n  \\[\\frac{|q|^2\\cos^2\\alpha}{2} \\|A^*A+AA^*\\| \\le w_q^2(A)\\le\n\\frac{\\left(\\sqrt{(1-|q|^2)\\left(1+2sin^2(\\alpha)\\right)}+ |q|\\right)^2}{2}\n\\|A^*A+AA^*\\|,\\]\n  where $ A $ is a sectorial matrix. Also, upper bounds for commutator and\nanti-commutator matrices and relations between $w_q(A^t)$ and $w_q^t(A)$ for\nnon-integral power $t\\in [0,1]$ are also obtained. Moreover, a few significant\nestimations of $q$-numerical radius of off-diagonal $2\\times2$ operator\nmatrices are developed.",
        "Hard computing generally deals with precise data, which provides ideal\nsolutions to problems. However, in the civil engineering field, amongst other\ndisciplines, that is not always the case as real-world systems are continuously\nchanging. Here lies the need to explore soft computing methods and artificial\nintelligence to solve civil engineering shortcomings. The integration of\nadvanced computational models, including Artificial Neural Networks (ANNs),\nFuzzy Logic, Genetic Algorithms (GAs), and Probabilistic Reasoning, has\nrevolutionized the domain of civil engineering. These models have significantly\nadvanced diverse sub-fields by offering innovative solutions and improved\nanalysis capabilities. Sub-fields such as: slope stability analysis, bearing\ncapacity, water quality and treatment, transportation systems, air quality,\nstructural materials, etc. ANNs predict non-linearities and provide accurate\nestimates. Fuzzy logic uses an efficient decision-making process to provide a\nmore precise assessment of systems. Lastly, while GAs optimizes models (based\non evolutionary processes) for better outcomes, probabilistic reasoning lowers\ntheir statistical uncertainties."
      ]
    }
  },
  {
    "id":2411.15211,
    "research_type":"applied",
    "start_id":"b9",
    "start_title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "start_abstract":"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent models, BERT is designed to pre-train deep bidirectional representations unlabeled text by jointly conditioning on both left and right context in all layers. As result, the pre-trained can be fine-tuned with just one additional output layer create state-of-the-art models wide range of tasks, such as question answering inference, without substantial task-specific architecture modifications. conceptually simple empirically powerful. It obtains results eleven natural processing including pushing GLUE score 80.5% (7.7% point absolute improvement), MultiNLI accuracy 86.7% (4.6% SQuAD v1.1 Test F1 93.2 (1.5 improvement) v2.0 83.1 (5.1 improvement).",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b6"
      ],
      "title":[
        "Solar Cells for Indoor Applications: Progress and Development"
      ],
      "abstract":[
        "The Internet of things (IoT) has been rapidly growing in the past few years. IoT connects numerous devices, such as wireless sensors, actuators, and wearable to optimize monitor daily activities. Most these devices require power microwatt range operate indoors. To this end, a self-sustainable source, photovoltaic (PV) cell, which can harvest low-intensity indoor light, is appropriate. Recently, development highly efficient PV cells for applications attracted tremendous attention. Therefore, different types materials, inorganic, dye-sensitized, organic, perovskite have employed harvesting light energy. Although considerable efforts made by researchers develop low-cost, stable, applications, Extensive investigation necessary resolve some critical issues concerning cells, environmental stability, lifetime, large-area fabrication, mechanical flexibility, production cost. address issues, systematic review aspects will be useful research community. This study discusses current status based on previous reports. First, we provided relevant background information. Then, described sources, subsequently critically reviewed reports regarding solar active materials perovskite. Finally, placed an attempt provide insight into factors needed further improve feasibility technology applications."
      ],
      "categories":[
        "astro-ph.SR"
      ]
    },
    "list":{
      "title":[
        "The MAGPI Survey: the subtle role of environment and not-so-subtle\n  impact of generations of stars on galaxy dynamics",
        "Multiple Populations of the Large Magellanic Cloud Globular Cluster NGC\n  2257: No Major Environmental Effect on the Formation of Multiple Populations\n  of the Old Globular Clusters in Large Magellanic Cloud",
        "Noncommutative Phantom BTZ Black Hole",
        "Spectral synthesis in multidimensional Fourier algebras",
        "Chiral Vibrational Modes in Small Molecules",
        "Stability of oscillations in the spatially extended May-Leonard model",
        "Zero modes and Dirac-(logarithmic) Sobolev-type inequalities",
        "Rigorous expansions of modular forms at CM points, I: Denominators",
        "Phonon anomalies within the polar charge density wave phase of\n  superconductor Mo$_3$Al$_2$C with structural chirality",
        "Tuning Quantum States at Chirality-Reversed Planar Interface in Weyl\n  Semimetals using an Interstitial Layer",
        "Using Matrix-Free Tensor-Network Optimizations to Construct a\n  Reduced-Scaling and Robust Second-Order M{\\o}ller-Plesset Theory",
        "A filtered two-step variational integrator for charged-particle dynamics\n  in a normal or strong magnetic field",
        "Distribution and Moments of a Normalized Dissimilarity Ratio for two\n  Correlated Gamma Variables",
        "Prediction for close approaches with terrestrial planets of asteroids\n  from the main belt",
        "Infrared Metaplasmonics",
        "Atmospheric Circulation of Close-In Extrasolar Giant Planets: The\n  Diabatic Equivalent-Barotropic Model",
        "Morphogenesis of cheese flowers through scraping",
        "Quantum correlations and spatial localization in trapped one-dimensional\n  ultra-cold Bose-Bose-Bose mixtures",
        "Finitely additive measures on Boolean algebras",
        "Effective theory of light Dirac neutrino portal dark matter with\n  observable ${\\Delta N_{\\rm eff}}$",
        "Hardware Acceleration for HPS Algorithms in Two and Three Dimensions",
        "Generalized Uncertainty Relation Between an Observable and Its\n  Derivative",
        "The tardigrade as an emerging model organism for systems neuroscience",
        "Euclid Quick Data Release (Q1). A probabilistic classification of\n  quenched galaxies",
        "Interactive visualization of large molecular systems with VTX: example\n  with a minimal whole-cell model",
        "Phase space geometry of collective spin systems: Scaling and Fractality",
        "The uncommon intracluster medium features of the first massive clusters\n  selected independently of their baryon content",
        "Improved Decoding of Tanner Codes",
        "A diagrammatic formulation of local realism"
      ],
      "abstract":[
        "The stellar age and mass of galaxies have been suggested as the primary\ndeterminants for the dynamical state of galaxies, with environment seemingly\nplaying no or only a very minor role. We use a sample of 77 galaxies at\nintermediate redshift (z~0.3) in the Middle-Ages Galaxies Properties with\nIntegral field spectroscopy (MAGPI) Survey to study the subtle impact of\nenvironment on galaxy dynamics. We use a combination of statistical techniques\n(simple and partial correlations and principal component analysis) to isolate\nthe contribution of environment on galaxy dynamics, while explicitly accounting\nfor known factors such as stellar age, star formation histories and stellar\nmasses. We consider these dynamical parameters: high-order kinematics of the\nline-of-sight velocity distribution (parametrised by the Gauss-Hermite\ncoefficients $h_3$ and $h_4$), kinematic asymmetries $V_{\\rm asym}$ derived\nusing kinemetry and the observational spin parameter proxy $\\lambda_{R_e}$. Of\nthese, the mean $h_4$ is the only parameter found to have a significant\ncorrelation with environment as parametrised by group dynamical mass. This\ncorrelation exists even after accounting for age and stellar mass trends.\nFinally, we confirm that variations in the spin parameter $\\lambda_{R_e}$ are\nmost strongly (anti-)correlated with age as seen in local studies, and show\nthat this dependence is well-established by z~0.3.",
        "How the environment of the host galaxy affects the formation of multiple\npopulations (MPs) in globular clusters (GCs) is one of the outstanding\nquestions in the near-field cosmology. To understand the true nature of the old\nGC MPs in the Large Magellanic Cloud (LMC), we study the Ca--CN--CH photometry\nof the old metal-poor LMC GC NGC 2257. We find the predominantly FG-dominated\npopulational number ratio of $n$(FG):$n$(SG) = 61:39($\\pm$4), where the FG and\nSG denote the first and second generations. Both the FG and SG have similar\ncumulative radial distributions, consistent with the idea that NGC 2257 is\ndynamically old. We obtain [Fe\/H] = $-$1.78$\\pm$0.00 dex($\\sigma$=0.05 dex) and\nour metallicity is $\\sim$0.2 dex larger than that from the high-resolution\nspectroscopy by other, due to their significantly lower temperatures by $\\sim$\n$-$200 K. The NGC 2257 FG shows a somewhat larger metallicity variation than\nthe SG, the first detection of such phenomenon in an old LMC GC, similar to\nGalactic GCs with MPs, strongly suggesting that it is a general characteristic\nof GCs with MPs. Interestingly, the NGC 2257 SG does not show a helium\nenhancement compared to the FG. Our results for the Galactic normal GCs exhibit\nthat the degree of carbon and nitrogen variations are tightly correlated with\nthe GC mass, while NGC 2257 exhibits slightly smaller variations for its mass.\nWe show that old LMC GCs follow the same trends as the Galactic normal GCs in\nthe $\\Delta$W$_{\\rm CF336W,F438W,F814W}$, $N_{\\rm FG}\/N_{\\rm tot}$, and $\\log\nM\/M_{\\rm \\odot}$ domains. Our result indicates that the environment of the host\ngalaxy did not play a major role in the formation and evolution of GC MPs.",
        "This work explores the thermodynamic and geometric properties of phantom BTZ\nblack holes within the framework of noncommutative spacetime, where\nnoncommutative effects are incorporated via Lorentzian distributions for mass\nand charge. The resulting modifications in spacetime geometry introduce\nsignificant alterations to horizon structures and curvature singularities. A\ncomprehensive and comparative thermodynamic analysis is conducted, examining\nthe differences between phantom and ordinary matter cases. This includes an\ninvestigation of Hawking temperature, entropy, heat capacity, and stability\ncriteria. Additionally, the black hole is analyzed as a thermodynamic heat\nengine, with its efficiency evaluated as a function of noncommutative\nparameters. Our findings highlight the profound impact of noncommutativity on\nthe thermodynamic behavior and efficiency of phantom BTZ black holes, revealing\nnew insights into the interplay between quantum spacetime effects and exotic\nfield dynamics. The results indicate that noncommutative corrections not only\nmodify the stability conditions of these black holes but also play a crucial\nrole in governing phase transitions. Furthermore, we demonstrate that\nnoncommutativity influences energy extraction processes, refining our\nunderstanding of black hole thermodynamics in lower-dimensional spacetimes and\ndistinguishing the behavior of phantom and ordinary matter cases.",
        "Let $G$ be a locally compact group and let $A^n(G)$ denote the\n$n$-dimensional Fourier algebra, introduced by Todorov and Turowska. We\ninvestigate spectral synthesis properties of the multidimensional Fourier\nalgebra $A^n(G).$ In particular, we prove versions of the subgroup lemma,\ninjection, and inverse projection theorems for both spectral sets and Ditkin\nsets. Additionally, we provide a result on the parallel synthesis between\n$A^n(G)$ and $A^{n+1}(G)$ and finally prove Malliavin's theorem.",
        "The development of quantitative methods for characterizing molecular\nchirality can provide an important tool for studying chirality induced\nphenomena in molecular systems. Significant progress has been made in recent\nyears toward understanding the chirality of molecular normal vibrational modes,\nmostly focusing on vibrations of helical molecular structures. In the present\nstudy, we examine the applicability two methodologies previously used for\nhelical structures for the quantification of the chirality of molecular normal\nmodes across a range of small, not necessarily helical, molecules. The first\napproach involves the application of the Continuous Chirality Measure (CCM) to\neach normal mode by associating the mode with a structure formed by imposing\nthe corresponding motion about a common origin. The second approach assigns to\neach normal mode a pseudoscalar defined as the product of atomic linear and\nangular momentum summed over all atoms. In particular, using the CCM also as a\nmeasure of the chirality of the underlying molecular structure, we establish\nthe existence of correlation between the chirality of molecular normal modes\nand that of the underlying molecular structure. Furthermore, we find that\nnormal modes associated with different frequency ranges of the molecular\nvibrational spectrum exhibit distinct handedness behavior.",
        "The May-Leonard model for three competing species, symmetric with respect to\n  cyclic permutation of the variables and extended by diffusive terms, is\nconsidered.\n  Exact time-periodic solutions of the system have been found, and their\nstability\n  with respect to spatially periodic disturbances is studied. The stability of\nsolu tions with respect to longwave spatial modulations is revealed. A period\ndoubling\n  instability breaking the spatial uniformity is found.",
        "We study the decay rate of the zero modes of the Dirac operator with a\nmatrix-valued potential that is considered here without any regularity\nassumptions, compared to the existing literature. For the Dirac operator and\nfor Clifford-valued functions we prove the $L^p$-$L^2$ Dirac Sobolev inequality\nwith explicit constant, as well as the $L^p$-$L^q$ Dirac-Sobolev inequalities.\nWe prove its logarithmic counterpart for $q=2$, extending it to its Gaussian\nversion of Gross, as well as show Nash and Poincar\\'e inequalities in this\nsetting, with explicit values for constants.",
        "We describe an algorithm to rigorously compute the power series expansion at\na CM point of a weight $2$ cusp form of level coprime to $6$. Our algorithm\nworks by bounding the denominators that appear due to ramification, and without\nrecourse to computing an explicit model of the corresponding modular curve. Our\nresult is the first in a series of papers toward an eventual implementation of\nequationless Chabauty.",
        "We employ polarization-resolved Raman spectroscopy to study the lattice\ndynamics of the polar charge density wave phase of the superconductor\nMo$_3$Al$_2$C with structural chirality. We show the phononic signatures of the\ncharge density wave transition at $T^*$=155K in Mo$_3$Al$_2$C. The detailed\ntemperature dependence of these phonon modes' frequency,\nhalf-width-at-half-maximum, and the integrated area below $T^*$ reveal\nanomalies at an intermediate temperature $T'\\sim$100K, especially for the\nlow-energy modes at 130cm$^{-1}$ and 180cm$^{-1}$. Since these low-energy modes\nare dominated by Mo-related lattice vibration, we propose that lattice\nanomalies at $T'$ within the charge density wave phase are related to a\nmodification of the Mo displacements while preserving the crystal symmetry.",
        "The electronic band structure of Weyl semimetals possesses pairs of linear\nband crossings, called Weyl nodes, characterized by opposite chirality charges\nassociated with each node. The momentum space position of the nodes can reverse\nacross a planar interface and these host Fermi-arc-like bound states, in\naddition to scattering states. We show that a magnetic interstitial layer can\ntune these states in three distinct ways. The electrostatic potential and one\nof the in-plane magnetic potential components control the shape of the bound\nstate Fermi-arcs. For moderate values of the same in-plane magnetic potential\nelectrons are spin-filtered across the interface, while both the in-plane\nmagnetic components and the electrostatic potential control the transmission of\nelectrons. The ratio of in-plane to out-of-plane magnetic components can be\nused to turn on or turn off the magnetic potential effects, since the latter\ndoes not affect the interface states. The tunability arises from spin-momentum\nlocking and chirality reversal at the interface. Thus, the effects can mix or\ninterchange depending on the specific material but the states will remain\ntunable.",
        "We investigate the application of the canonical polyadic decomposition (CPD)\nto the tensor hypercontraction (THC) and Laplace transform (LT) approximated\nsecond-order M{\\o}ller-Plesset (MP2) method. By introducing these\ndecompositions we formally reduce the scaling of the canonical MP2 method from\n$\\mathcal{O}(N^5)$ to $\\mathcal{O}(N^3)$ and the storage complexity from\n$\\mathcal{O}(N^4)$ to $\\mathcal{O}(N^2)$. We are able to construct the THC\nrepresentation in $\\mathcal{O}(N^3)$ time by employing the interpolative\nseparable density fitting decomposition strategy. Furthermore, we introduce a\nCPD optimization strategy that takes advantage of the THC representation to\ndecompose the order-four two-electron integral tensor with a computational\nscaling of $\\mathcal{O}(N^3)$. Finally, we show that the rank of the CPD in the\napproximation of MP2 scales linearly with system size and that this CPD-ISDF-LT\nMP2 strategy realizes a performance advantage over canonical LT MP2 in both\ncomputational wall-times and memory resource requirements.",
        "This article is concerned with a new filtered two-step variational integrator\nfor solving the charged-particle dynamics in a mildly non-homogeneous normal or\nstrong magnetic field with a dimensionless parameter $\\epsilon$ inversely\nproportional to the strength of the magnetic field. In the case of a normal\nmagnetic field ($\\epsilon \\approx 1$), second-order error bounds and long time\nenergy and momentum conservations are obtained. Moreover, the proof of the\nlong-term analysis is accomplished by the backward error analysis. For the\nstrong magnetic field ($0<\\epsilon \\ll1$), this paper clarifies the behaviour\nof the filtered variational integrator for both a large stepsize $h^2 \\geq\n\\epsilon$ and a smaller stepsize $ h \\sim \\epsilon$. The approach to analysing\nthe error bounds for these two stepsizes is based on comparing the modulated\nFourier expansions of the exact and the numerical solutions. It is shown that\nthe proposed integrator achieves a second-order accuracy $\\mathcal{O}(h^2)$ in\nthe position and in the parallel velocity for a large step size and an\n$\\mathcal{O}(\\epsilon)$ accuracy for a smaller stepsize. This paper also yields\nthe long time energy and magnetic moment conservations for the strong magnetic\nfield by developing the modulated Fourier expansion of the proposed scheme. All\nthe theoretical results of the error behaviour and long-term conservations are\nnumerically demonstrated by two numerical experiments.",
        "We consider two random variables $X$ and $Y$ following correlated Gamma\ndistributions, characterized by identical scale and shape parameters and a\nlinear correlation coefficient $\\rho$. Our focus is on the parameter: \\[\n  D(X,Y) = \\frac{|X - Y|}{X + Y}, \\] which appears in applied contexts such as\ndynamic speckle imaging, where it is known as the \\textit{Fujii index}. In this\nwork, we derive a closed-form expression for the probability density function\nof $D(X,Y)$ as well as analytical formulas for its moments of order $k$. Our\nderivation starts by representing $X$ and $Y$ as two correlated exponential\nrandom variables, obtained from the squared magnitudes of circular complex\nGaussian variables. By considering the sum of $k$ independent exponential\nvariables, we then derive the joint density of $(X,Y)$ when $X$ and $Y$ are two\ncorrelated Gamma variables. Through appropriate varable transformations, we\nobtain the theoretical distribution of $D(X,Y)$ and evaluate its moments\nanalytically. These theoretical findings are validated through numerical\nsimulations, with particular attention to two specific cases: zero correlation\nand unit shape parameter.",
        "Potentially Hazardous Asteroids (PHAs), a special subset of Near-Earth\nObjects, are both dangerous and scientifically valuable. PHAs that truly\nundergo close approaches with the Earth (dubbed CAPHAs) are of particular\ninterest and extensively studied. The concept and study of CAPHA can be\nextended to other Solar system planets, which have significant implications for\nfuture planet-based observations and explorations. In this work, we conduct\nnumerical simulations that incorporate the Yarkovsky effect to study the\ntransformation of main belt asteroids into CAPHAs of terrestrial planets, using\nprecise nominal timesteps, especially to ensure the reliability of the results\nfor Mercury and Venus. Our simulations predict a total of 1893 Mercury-CAPHAs,\n3014 Venus-CAPHAs, 3791 Earth-CAPHAs and 18066 Mars-CAPHAs, with an occurrence\nfrequency of about 1, 9, 15 and 66 per year, respectively. The values for\nMars-CAPHAs are consistent with our previous work, which were based on\nsimulations with a larger nominal timestep. The predicted occurrence frequency\nand velocity distribution of Earth-CAPHAs are in reasonable agreement with the\nobserved population of Earth-CAPHAs. We also find that certain asteroids can be\ncaught in close approach with different planets at different times, raising an\ninteresting possibility of using them as transportation between terrestrial\nplanets in the future.",
        "Plasmonic response in metals, defined as the ability to support subwavelength\nconfinement of surface plasmon modes, is typically limited to a narrow\nfrequency range below the metals' plasma frequency. This places severe\nlimitations on the operational wavelengths of plasmonic materials and devices.\nHowever, when the volume of a metal film is massively decreased, highly\nconfined quasi-two-dimensional surface plasmon modes can be supported out to\nwavelengths well beyond the plasma wavelength. While this has, thus far, been\nachieved using ultra-thin (nm-scale) metals, such films are quite difficult to\nrealize, and suffer from even higher losses than bulk plasmonic films. To\nextend the plasmonic response to the infrared, here we introduce the concept of\nmetaplasmonics, representing a novel plasmonic modality with a host of\nappealing properties. By fabricating and characterizing a series of\nmetaplasmonic nanoribbons, we demonstrate large confinement, high quality\nfactors, and large near-field enhancements across a broad wavelength range,\nextending well beyond the limited bandwidth of traditional plasmonic materials.\nWe demonstrate $35\\times$ plasmon wavelength reduction, and our numerical\nsimulations suggest that further wavelength reduction, up to a factor of 150,\nis achievable using our approach. The demonstration of the metaplasmonics\nparadigm offers a promising path to fill the near- and mid-infrared\ntechnological gap for high quality plasmonic materials, and provides a new\nmaterial system to study the effects of extreme plasmonic confinement for\napplications in nonlinear and quantum plasmonics.",
        "We extend the description of equivalent-barotropic equations for exoplanets\nto the diabatic case -- that is, with explicit heating and\/or cooling\nrepresentation, rather than with a stationary deflection of the bottom bounding\nsurface. In the diabatic case, the equation for potential temperature (or\nentropy) is directly forced and cannot be decoupled from the equations for\nmomentum and nonlinear pressure, the mass-like variable; and, the isentropic\nsurfaces do not remain coincident with material surfaces. Here the formalism is\npresented for an atmosphere with the Lamb vertical structure, as the formalism\nis substantially simplified under the structure. The equations presented set\nthe stage for accurate global simulations which permit small-scale vortices,\ngravity waves, and fronts observed in current three-dimensional global\nsimulations to be studied in detail.",
        "The \"Tete de moine\" Swiss cheese is generally served by scraping the surface\nof a cylindrical loaf with a sharp tool. This produces thin sheets of cheese\nthat are strongly wrinkled at the edge, resembling frilly flowers and enhancing\nthe tasting experience. In this work we unveil the physical mechanisms at play\nin this scraping-induced morphogenesis. We measure the deformation of the\ncheese during scraping and show that plastic deformation occurs everywhere, but\nfind a larger plastic contraction in the inner part of the flower, causing its\nbuckling into shape. We show that it surprisingly derives from the lower\nfriction coefficient evidenced on the cheese close to its crust. Our analysis\nprovides the tools for a better control of chip morphogenesis through\nplasticity in the shaping of other delicacies, but also in metal cutting.",
        "We systematically investigate and illustrate the complete ground-state phase\ndiagram for a one-dimensional, three-species mixture of a few repulsively\ninteracting bosons trapped harmonically. To numerically obtain the solutions to\nthe many-body Schr\\\"{o}dinger equation, we employ the improved Exact\nDiagonalization method [T. D. Anh-Tai {\\it et al.}, SciPost Physics 15, 048\n(2023)], which is capable of treating strongly-correlated few-body systems from\nfirst principles in an efficiently truncated Hilbert space. We present our\ncomprehensive results for all possible combinations of intra- and interspecies\ninteractions in the extreme limits that are either the ideal limit ($g=0$) or\nclose to the hard-core limit ($g\\to\\infty$). These results show the emergence\nof unique ground-state properties related to correlations, coherence and\nspatial localization stemming from strongly repulsive interactions.",
        "In this article, we conduct a detailed study of \\emph{finitely additive\nmeasures} (fams) in the context of Boolean algebras, focusing on three specific\ntopics: freeness and approximation, existence and extension criteria, and\nintegration theory. In the first topic, we present a classification of\n\\emph{free} finitely additive measures, that is, those for which the measure of\nfinite sets is zero, in terms of approximation to uniform probability measures.\nThis inspires a weaker version of this notion, which we call the \\emph{uniform\napproximation property}, characterized in terms of freeness and another\nwell-determined type of fams we call \\emph{uniformly supported}. In the second\ntopic, we study criteria for existence and extension of finitely additive\nmeasures for Boolean algebras, offering a relatively short proof of the\n\\emph{compatibility theorem} for fams.\n  Finally, we study a Riemann-type integration theory on fields of sets with\nrespect to finitely additive measures, allowing us to extend and generalize\nsome classical concepts and results from real analysis, such as Riemann\nintegration over rectangles in $\\mathbb{R}^{n}$ and the Jordan measure. We also\ngeneralize the extension criteria for fams allowing desired values of integrals\nof a given set of functions. At the end, we explore the connection between\nintegration in fields of sets and the Lebesgue integration in the Stone space\nof the corresponding field, where we establish a characterization of\nintegrability in the sense of the Lebesgue-Vitali theorem, which follows as a\nconsequence of our results.",
        "We study the possibility of light Dirac neutrino portal dark matter (DM) in\nan effective field theory (EFT) setup. Dirac nature of light neutrino\nautomatically includes its right chiral part $\\nu_R$ which, in our setup, also\nacts like a portal between DM and the standard model (SM) particles.\nConsidering a Dirac fermion singlet DM stabilised by an unbroken $Z_2$\nsymmetry, we write down all possible dimension-6 effective operators involving\nDM-$\\nu_R$ as well as $\\nu_R$-SM which conserve $Z_2$, global lepton number and\nSM gauge symmetries. DM thermalisation also ensures the thermalisation of\n$\\nu_R$, leading to enhanced effective relativistic degrees of freedom $N_{\\rm\neff}$, within reach of future cosmic microwave background (CMB) experiments. We\nstudy the complementarity among DM and CMB related observations for different\nLorentz structures of effective operators. We also propose two UV completions\nbased on the popularly studied gauges $\\rm B-L$ and left-right symmetric model\nframeworks.",
        "We provide a flexible, open-source framework for hardware acceleration,\nnamely massively-parallel execution on general-purpose graphics processing\nunits (GPUs), applied to the hierarchical Poincar\\'e--Steklov (HPS) family of\nalgorithms for building fast direct solvers for linear elliptic partial\ndifferential equations. To take full advantage of the power of hardware\nacceleration, we propose two variants of HPS algorithms to improve performance\non two- and three-dimensional problems. In the two-dimensional setting, we\nintroduce a novel recomputation strategy that minimizes costly data transfers\nto and from the GPU; in three dimensions, we modify and extend the adaptive\ndiscretization technique of Geldermans and Gillman [2019] to greatly reduce\npeak memory usage. We provide an open-source implementation of these methods\nwritten in JAX, a high-level accelerated linear algebra package, which allows\nfor the first integration of a high-order fast direct solver with automatic\ndifferentiation tools. We conclude with extensive numerical examples showing\nour methods are fast and accurate on two- and three-dimensional problems.",
        "The generalized uncertainty connection between the fluctuations of a quantum\nobservable and its temporal derivative is derived in this study, we demonstrate\nthat the product of an observable's uncertainties and its time derivative is\nbounded by half the modulus of the expectation value of the commutator between\nthe observable and its derivative, using the Cauchy Schwarz inequality and the\nstandard definitions of operator variances. In order to connect the dynamical\nevolution of observables to their inherent uncertainties, we reformulate the\nbound in terms of a double commutator by expressing the derivative in terms of\nthe Hamiltonian via the Heisenberg equation of motion. Next, we apply this\ngeneralized relation to a spin particle to demonstrate its usefulness in a\nmagnetic field that changes over time, and expand the study to include\nobservables that have a clear temporal dependence. Our findings provide greater\nunderstanding of quantum dynamics and the influence of time-dependent\ninteractions on measurement precision in addition to recovering the traditional\nuncertainty relations for static systems.",
        "We present the case for developing the tardigrade (Hypsibius exemplaris) into\na model organism for systems neuroscience. These microscopic, transparent\nanimals (~300-500 microns) are among the smallest known to possess both limbs\n(eight) and eyes (two), with a nervous system of only a few hundred neurons\norganized into a multi-lobed brain, ventral nerve cord, and a series of ganglia\nalong the body. Despite their neuroanatomical simplicity, tardigrades exhibit\ncomplex behaviors, including multi-limbed walking gaits, individual limb\ngrasping, phototaxis, and transitions between active and dormant states. These\nbehaviors position tardigrades as a uniquely powerful system for addressing\ncertain fundamental questions in systems neuroscience, such as: How do nervous\nsystems coordinate multi-limbed behaviors? How are top-down and bottom-up motor\ncontrol systems integrated? How is stereovision-guided navigation implemented?\nWhat mechanisms underlie neural resilience and recovery during environmental\nstress? We review current knowledge of tardigrade neuroanatomy, behavior, and\ngenomics, and we identify opportunities and challenges for leveraging their\nunique biology. We propose developing essential neuroscientific tools for\ntardigrades, including genetic engineering and live neuroimaging, alongside\nbehavioral assays linking neural activity to outputs. Leveraging their\nevolutionary ties to Caenorhabditis elegans and Drosophila melanogaster, we can\nadapt existing toolkits to accelerate tardigrade research - providing a bridge\nbetween simpler invertebrate systems and more complex neural architectures.",
        "Investigating the drivers of the quenching of star formation in galaxies is\nkey to understanding their evolution. The Euclid mission will provide rich\nspatial and spectral data from optical to infrared wavelengths for millions of\ngalaxies, enabling precise measurements of their star formation histories.\nUsing the first Euclid Quick Data Release (Q1), we developed a probabilistic\nclassification framework, that combines the average specific star-formation\nrate ($\\rm sSFR_\\tau$) inferred over two timescales ($\\tau={10^8,10^9}$ yr), to\ncategorize galaxies as `Ageing' (secularly evolving), `Quenched' (recently\nhalted star formation), or `Retired' (dominated by old stars). We validated\nthis methodology using synthetic observations from the IllustrisTNG simulation.\nTwo classification methods were employed: a probabilistic approach, integrating\nposterior distributions, and a model-driven method optimizing sample purity and\ncompleteness using IllustrisTNG. At $z<0.1$ and $M_\\ast \\gtrsim 3\\times10^{8}\\,\nM_\\odot$, we obtain Euclid class fractions of 68-72%, 8-17%, and 14-19% for\nAgeing, Quenched, and Retired populations, respectively, consistent with\nprevious studies. The evolution with redshift shows increasing\/decreasing\nfraction of Ageing\/Retired galaxies. The fraction of quenched systems shows a\nweaker dependence on stellar mass and redshift, varying between 5% and 15%. We\nanalysed the mass-size-metallicity relation for each population. Ageing\ngalaxies generally exhibit disc morphologies and low metallicities. Retired\ngalaxies show compact structures and enhanced chemical enrichment, while\nQuenched galaxies form an intermediate population, more compact and chemically\nevolved than Ageing systems. This work demonstrates Euclid's great potential\nfor elucidating the physical nature of the quenching mechanisms that govern\ngalaxy evolution.",
        "VTX is an open-source molecular visualization software designed to overcome\nthe scaling limitations of existing real-time molecular visualization software\nwhen handling massive molecular datasets. VTX employs a meshless molecular\ngraphics engine utilizing impostor-based techniques and adaptive\nlevel-of-detail (LOD) rendering. This approach significantly reduces memory\nusage and enables real-time visualization and manipulation of large molecular\nsystems. Performance benchmarks against VMD, PyMOL, and ChimeraX using a\n114-million-bead Martini minimal whole-cell model demonstrate VTX's efficiency,\nmaintaining consistent frame rates even under interactive manipulation on\nstandard computer hardware. VTX incorporates features such as screen-space\nambient occlusion (SSAO) for enhanced depth perception and free-fly navigation\nfor intuitive exploration of large molecular systems. VTX is open-source and\nfree for non commercial use. Binaries for Windows and Ubuntu Linux are\navailable at \\href{http:\/\/vtx.drugdesign.fr}{http:\/\/vtx.drugdesign.fr}. VTX\nsource code is available at\n\\href{https:\/\/github.com\/VTX-Molecular-Visualization}{https:\/\/github.com\/VTX-Molecular-Visualization}.",
        "We examine the scaling of the inverse participation ratio of spin coherent\nstates in the energy basis of three collective spin systems: a bounded harmonic\noscillator, the Lipkin-Meshkov-Glick model, and the Quantum Kicked Top. The\nfinite-size quantum probing provides detailed insights into the structure of\nthe phase space, particularly the relationship between critical points in\nclassical dynamics and their quantum counterparts in collective spin systems.\nWe introduce a finite-size scaling mass exponent that makes it possible to\nidentify conditions under which a power-law behavior emerges, allowing to\nassign a fractal dimension to a coherent state. For the Quantum Kicked Top, the\nfractal dimension of coherent states -- when well-defined -- exhibits three\ngeneral behaviors: one related to the presence of critical points and two\nassociated with regular and chaotic dynamics. The finite-size scaling analysis\npaves the way toward exploring collective spin systems relevant to quantum\ntechnologies within the quantum-classical framework.",
        "Our current knowledge of the thermodynamic properties of galaxy clusters\ncomes primarily from detailed studies of clusters selected by their minority\ncomponents: hot baryons. Most of these studies select the clusters using the\ncomponent that is being investigated, the intracluster medium (ICM), making the\nsample choice prone to selection effects. Weak-gravitational lensing allows us\nto select clusters by the total mass component and, being independent of the\ntype of matter, makes the sample choice unbiased with respect to the baryon\ncontent. In this paper, we study four galaxy clusters at intermediate redshift\n($0.25<z<0.61$), selected from the weak-lensing survey of Miyazaki et al.\n(2018). We derive core-excised X-ray luminosities, richness-based masses,\nCompton parameters, and profiles of mass, pressure and electron densities.\nThese quantities are derived from shear data, Compton maps, and our own X-ray\nand SZ follow-up. When compared to ICM-selected clusters of the same mass, in\nthe range $2$ to $5 \\ 10^{14}$ M$_\\odot$, our small sample of four clusters is\nexpected to have on average 0.2 rare ($>2\\sigma$) features, while we observed\non average two rare features in each one of the seven explored properties:\nrichness, core-excised luminosity, Compton parameter, pressure and electron\npressure profiles, and central values of them. The abundance of rare and unique\nfeatures in such a small sample indicates a fundamental bias in our knowledge\nof the thermodynamic properties of clusters when derived from ICM-selected\nsamples.",
        "In this paper, we present improved decoding algorithms for expander-based\nTanner codes.\n  We begin by developing a randomized linear-time decoding algorithm that,\nunder the condition that $ \\delta d_0 > 2 $, corrects up to $ \\alpha n $ errors\nfor a Tanner code $ T(G, C_0) $, where $ G $ is a $ (c, d, \\alpha, \\delta)\n$-bipartite expander with $n$ left vertices, and $ C_0 \\subseteq \\mathbb{F}_2^d\n$ is a linear inner code with minimum distance $ d_0 $. This result improves\nupon the previous work of Cheng, Ouyang, Shangguan, and Shen (RANDOM 2024),\nwhich required $ \\delta d_0 > 3 $.\n  We further derandomize the algorithm to obtain a deterministic linear-time\ndecoding algorithm with the same decoding radius. Our algorithm improves upon\nthe previous deterministic algorithm of Cheng et al.\\ by achieving a decoding\nradius of $ \\alpha n $, compared with the previous radius of $\n\\frac{2\\alpha}{d_0(1 + 0.5c\\delta) }n$.\n  Additionally, we investigate the size-expansion trade-off introduced by the\nrecent work of Chen, Cheng, Li, and Ouyang (IEEE TIT 2023), and use it to\nprovide new bounds on the minimum distance of Tanner codes. Specifically, we\nprove that the minimum distance of a Tanner code $T(G,C_0)$ is approximately\n$f_\\delta^{-1} \\left( \\frac{1}{d_0} \\right) \\alpha n $, where $ f_\\delta(\\cdot)\n$ is the Size-Expansion Function. As another application, we improve the\ndecoding radius of our decoding algorithms from $\\alpha n$ to approximately\n$f_\\delta^{-1}(\\frac{2}{d_0})\\alpha n$.",
        "Given two parties performing experiments in separate laboratories, we provide\na diagrammatic formulation of what it means for the joint statistics of their\nexperiments to satisfy local realism. In particular, we show that the\nprinciples of locality and realism are both captured by a single commutative\ndiagram in the category of probability-preserving maps between finite\nprobability spaces, and we also show that an assumption of such a diagrammatic\nformulation of local realism implies the standard CHSH inequality associated\nwith dichotomic random variables. As quantum theory is known not to satisfy\nlocal realism, our formulation of local realism in terms of commutative\ndiagrams provides yet another way in which the notion of non-commutativity\nplays a fundamental role in quantum theory. We note that we do not assume any\nprior knowledge of category theory or quantum theory, as this work is intended\nfor philosophers, mathematicians and physicists alike."
      ]
    }
  }
]