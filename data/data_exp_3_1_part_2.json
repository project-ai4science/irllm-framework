[
  {
    "id":2411.00922,
    "research_type":"applied",
    "start_id":"b12",
    "start_title":"nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation",
    "start_abstract":"Biomedical imaging is a driver of scientific discovery and a core component of medical care and is being stimulated by the field of deep learning. While semantic segmentation algorithms enable image analysis and quantification in many applications, the design of respective specialized solutions is non-trivial and highly dependent on dataset properties and hardware conditions. We developed nnU-Net, a deep learning-based segmentation method that automatically configures itself, including preprocessing, network architecture, training and post-processing for any new task. The key design choices in this process are modeled as a set of fixed parameters, interdependent rules and empirical decisions. Without manual intervention, nnU-Net surpasses most existing approaches, including highly specialized solutions on 23 public datasets used in international biomedical segmentation competitions. We make nnU-Net publicly available as an out-of-the-box tool, rendering state-of-the-art segmentation accessible to a broad audience by requiring neither expert knowledge nor computing resources beyond standard network training.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b16"
      ],
      "title":[
        "Deep learning model for automatic segmentation of lungs and pulmonary metastasis in small animal MR images"
      ],
      "abstract":[
        "Lungs are the most frequent site of metastases growth. The amount and size pulmonary acquired from MRI imaging data important criteria to assess efficacy new drugs in preclinical models. While efficient solutions both for MR downstream automatic segmentation have been proposed human patients, lung animal models remains challenging due physiological motion (respiratory cardiac movements), low protons this organ particular challenge precise metastases. As a consequence post-mortem analysis is currently required obtain information on metastatic volume. In work, we developed complete methodological pipeline automated lungs mice, consisting an sequence image acquisition deep learning method On one hand, optimized mouse with high contrast detection sensitivity. other hand DeepMeta, multiclass U-Net 3+ model automatically segment images. To if able provide accurate metastases, longitudinally imaged mice fast- slow-growing metastasis. Fifty-five balb\/c were injected two different derivatives renal carcinoma cells. Mice SG-bSSFP (self-gated balanced steady state free precession) at time points after injection cancer Both segmentations manually performed by experts. DeepMeta was trained perform based resulting ground truth annotations. Volumes as well number per measured separate test dataset Thanks SG method, 3D bSSFP images artifact-free, enabling serial follow-up Moreover, accurately soon they reached volume \u223c0.02mm3 . Thus distinguish groups terms slow versus fast patterns growth We shown that our methodology combining learning, enables processing whole thus viable alternative histology alone."
      ],
      "categories":[
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "Sharp stability for critical points of the Sobolev inequality in the\n  absence of bubbling",
        "Free energy profiles for chemical reactions in solution from\n  high-dimensional neural network potentials: The case of the Strecker\n  synthesis",
        "Efficient Multivariate Robust Mean Estimation Under Mean-Shift\n  Contamination",
        "Mathematical Modelling of Mechanotransduction via RhoA Signalling\n  Pathways",
        "Nonparametric Smoothing of Directional and Axial Data",
        "The EFT Bootstrap at Finite $M_{PL}$",
        "Preconditioning for a Cahn-Hilliard-Navier-Stokes model for morphology\n  formation in organic solar cells",
        "Four-quark scatterings in QCD III",
        "Vacuum stress between conducting plates: the curved spacetime version",
        "Euler--Poincar\\'e reduction and the Kelvin--Noether theorem for discrete\n  mechanical systems with advected parameters and additional dynamics",
        "On the Prescribed Ricci Curvature of Noncompact Homogeneous Spaces with\n  Two Isotropy Summands",
        "Active bacterial baths in droplets",
        "Seeing Stereotypes",
        "Supercell environments using GridRad-Severe and the HRRR: Addressing\n  discrepancies between prior tornado datasets",
        "Indigenous Mathematics I. Smoke Telegraphy",
        "Calibration of the Polarimetric GNSS-R Sensor in the Rongowai Mission",
        "Nonlinear Spectroscopy as a Magnon Breakdown Diagnosis and its Efficient\n  Simulation",
        "Perspectives on Quantum Friction, Self-Propulsion, and Self-Torque",
        "Towards Quantitative Interpretation of 3D Atomic Force Microscopy at\n  Solid-Liquid Interfaces",
        "The Simons Observatory: Validation of reconstructed power spectra from\n  simulated filtered maps for the Small Aperture Telescope survey",
        "The Southern Twenty-centimetre All-sky Polarization Survey (STAPS):\n  survey description and maps",
        "Visualization of Organ Movements Using Automatic Region Segmentation of\n  Swallowing CT",
        "Pad\\'e metrics for black hole perturbations and light rings",
        "Bottomonium meson spectrum with quenched and unquenched quark models",
        "Sparse Hyperparametric Itakura-Saito NMF via Bi-Level Optimization",
        "Bounded powers of edge ideals: regularity and linear quotients",
        "Elastic Plateau-Rayleigh instability in soft cylinders: Surface\n  elasticity and periodic beading",
        "Efficient evaluation of real-time path integrals",
        "MolSpectra: Pre-training 3D Molecular Representation with Multi-modal\n  Energy Spectra"
      ],
      "abstract":[
        "When $u$ is close to a single Talenti bubble $v$ of the $p$-Sobolev\ninequality, we show that\n  \\begin{equation*}\n  \\|Du-Dv\\|_{L^p(\\mathbb{R}^n)}^{\\max\\{1,p-1\\}}\\le C \\|-{\\rm\ndiv}(|Du|^{p-2}Du)-|u|^{p^*-2}u\\|_{W^{-1,q}(\\mathbb{R}^n)}, \\end{equation*}\nwhere $C=C(n,p)>0$. This estimate provides a sharp stability estimate for the\nStruwe-type decomposition in the single bubble case, generalizing the result of\nCiraolo, Figalli, and Maggi \\cite{CFM2018} (focusing on the case $p=2$) to the\narbitrary $p$. Also, in the Sobolev setting, this answers an open problem\nraised by Zhou and Zou in \\cite[Remark 1.17]{ZZ2023}.",
        "Machine learning potentials (MLPs) have become a popular tool in chemistry\nand materials science as they combine the accuracy of electronic structure\ncalculations with the high computational efficiency of analytic potentials.\nMLPs are particularly useful for computationally demanding simulations such as\nthe determination of free energy profiles governing chemical reactions in\nsolution, but to date such applications are still rare. In this work we show\nhow umbrella sampling simulations can be combined with active learning of\nhigh-dimensional neural network potentials (HDNNPs) to construct free energy\nprofiles in a systematic way. For the example of the first step of Strecker\nsynthesis of glycine in aqueous solution we provide a detailed analysis of the\nimproving quality of HDNNPs for datasets of increasing size. We find that next\nto the typical quantification of energy and force errors with respect to the\nunderlying density functional theory data also the long-term stability of the\nsimulations and the convergence of physical properties should be rigorously\nmonitored to obtain reliable and converged free energy profiles of chemical\nreactions in solution.",
        "We study the algorithmic problem of robust mean estimation of an identity\ncovariance Gaussian in the presence of mean-shift contamination. In this\ncontamination model, we are given a set of points in $\\mathbb{R}^d$ generated\ni.i.d. via the following process. For a parameter $\\alpha<1\/2$, the $i$-th\nsample $x_i$ is obtained as follows: with probability $1-\\alpha$, $x_i$ is\ndrawn from $\\mathcal{N}(\\mu, I)$, where $\\mu \\in \\mathbb{R}^d$ is the target\nmean; and with probability $\\alpha$, $x_i$ is drawn from $\\mathcal{N}(z_i, I)$,\nwhere $z_i$ is unknown and potentially arbitrary. Prior work characterized the\ninformation-theoretic limits of this task. Specifically, it was shown that, in\ncontrast to Huber contamination, in the presence of mean-shift contamination\nconsistent estimation is possible. On the other hand, all known robust\nestimators in the mean-shift model have running times exponential in the\ndimension. Here we give the first computationally efficient algorithm for\nhigh-dimensional robust mean estimation with mean-shift contamination that can\ntolerate a constant fraction of outliers. In particular, our algorithm has\nnear-optimal sample complexity, runs in sample-polynomial time, and\napproximates the target mean to any desired accuracy. Conceptually, our result\ncontributes to a growing body of work that studies inference with respect to\nnatural noise models lying in between fully adversarial and random settings.",
        "We derive and simulate a mathematical model for mechanotransduction related\nto the Rho GTPase signalling pathway. The model addresses the bidirectional\ncoupling between signalling processes and cell mechanics. A numerical method\nbased on bulk-surface finite elements is proposed for the approximation of the\ncoupled system of nonlinear reaction-diffusion equations, defined inside the\ncell and on the cell membrane, and the equations of elasticity. Our simulation\nresults illustrate novel emergent features such as the strong dependence of the\ndynamics on cell shape, a threshold-like response to changes in substrate\nstiffness, and the fact that coupling mechanics and signalling can lead to the\nrobustness of cell deformation to larger changes in substrate stiffness,\nensuring mechanical homeostasis in agreement with experiments.",
        "We discuss generalized linear models for directional data where the\nconditional distribution of the response is a von Mises-Fisher distribution in\narbitrary dimension or a Bingham distribution on the unit circle. To do this\nproperly, we parametrize von Mises-Fisher distributions by Euclidean parameters\nand investigate computational aspects of this parametrization. Then we modify\nthis approach for local polynomial regression as a means of nonparametric\nsmoothing of distributional data. The methods are illustrated with simulated\ndata and a data set from planetary sciences involving covariate vectors on a\nsphere with axial response.",
        "We explore the impact of loop effects on positivity in effective field\ntheories emerging in the infrared from unitary and causal microscopic dynamics.\nFocusing on massless particles coupled to gravity, we address the treatment of\nforward-limit divergences from loop discontinuities and establish necessary\nconditions for maintaining computational control in perturbation theory. While\nloop effects remain small, ensuring consistency in our approach leads to a\nsignificant impact on bounds, even at tree level.",
        "We present a model for the morphology evolution of printed organic solar\ncells which occurs during the drying of a mixture of polymer, the non-fullerene\nacceptor and the solvent. Our model uses a phase field approach coupled to a\nNavier-Stokes equation describing the macroscopic movement of the fluid.\nAdditionally, we incorporate the evaporation process of the solvent using an\nAllen-Cahn equation.\n  The model is discretized using a finite-element approach with a semi-implicit\ndiscretization in time. The resulting (non)linear systems are coupled and of\nlarge dimensionality. We present a preconditioned iterative scheme to solve\nthem robustly with respect to changes in the discretization parameters. We\nillustrate that the preconditioned solver shows parameter-robust iteration\nnumbers and that the model qualitatively captures the behavior of the film\nmorphology during drying.",
        "We study the full infrared dynamics of 2+1 flavour QCD with the functional\nrenormalisation group approach. We resolve self-consistently the glue dynamics\nas well as the dynamics of chiral symmetry breaking. The computation hosts no\nphenomenological parameter or external input. The only ultraviolet input\nparameters are the physical ones in QCD: the light and strange quark masses.\nThey are adjusted to the physical ratios of the pion and kaon masses, divided\nby the pion decay constant. The results for other observables of current\nfirst-principles computations are in quantitative agreement with the physical\nones. This work completes the series of papers, initiated and furthered in\n[1,2], on dynamical chiral symmetry breaking and the emergence of mesonic bound\nstates within the functional renormalisation group. As a first application we\ndiscuss the formation of light mesonic bound states. Amongst other applications\nsuch as the phase structure of QCD, the current work paves the way for studying\nQCD parton distribution functions within the functional renormalisation group\napproach to first-principles QCD.",
        "Brown and Maclay \\cite{Brown} found the energy-momentum tensor for the\nCasimir effect of parallel plates in 1969. We find its curved spacetime version\nin a static background using the point splitting regularization method.\nPrevious results in the literature are reinforced and some consequences\ndiscussed.",
        "The Euler--Poincar\\'e equations, firstly introduced by Henri Poincar\\'e in\n1901, arise from the application of Lagrangian mechanics to systems on Lie\ngroups that exhibit symmetries, particularly in the contexts of classical\nmechanics and fluid dynamics. These equations have been extended to various\nsettings, such as semidirect products, advected parameters, and field theory,\nand have been widely applied to mechanics and physics. In this paper, we\nintroduce the discrete Euler--Poincar\\'e reduction for discrete Lagrangian\nsystems on Lie groups with advected parameters and additional dynamics,\nutilizing the group difference map technique. Specifically, the group\ndifference map is defined using either the Cayley transform or the matrix\nexponential. The continuous and discrete Kelvin--Noether theorems are extended\naccordingly, that account for Kelvin--Noether quantities of the corresponding\ncontinuous and discrete Euler--Poincar\\'e equations. As an application, we show\nboth continuous and discrete Euler--Poincar\\'e formulations about the dynamics\nof underwater vehicles, followed by numerical simulations. Numerical results\nillustrate the scheme's ability to preserve geometric properties over extended\ntime intervals, highlighting its potential for practical applications in the\ncontrol and navigation of underwater vehicles, as well as in other domains.",
        "This work studies simply connected, noncompact $G\/H$ in which $G$ is\nsemi-simple, $H$ is connected, and $G\/H$ has two irreducible summands. Here, we\nclassify all such spaces and we provide solutions to the so-called Prescribed\nRicci Curvature problem for all such spaces.",
        "Suspensions of self-propelled objects represent a novel paradigm in colloidal\nscience. In such active baths traditional concepts, such as Brownian motion,\nfluctuation-dissipation relations, and work extraction from heat reservoirs,\nmust be extended beyond the conventional framework of thermal baths. Unlike\nthermal baths, which are characterized by a single parameter, the temperature,\nthe fundamental descriptors of an active bath remain elusive, especially in\nconfined environments. In this study, buoyant, passive tracers are employed as\ngeneralized probes to investigate an active bath comprising motile bacteria\nconfined within a droplet. We demonstrate that momentum transfer from the bath\nto the tracer can be effectively described as colored noise, characterized by\ntemporal memory and an enhanced effective diffusivity significantly larger\ncompared to thermal Brownian motion values. Using a stochastic analytical\nframework, we extract the temporal memory and diffusivity parameters that\ndefine such an active bath. Notably, the diffusivity scales linearly with\nbacterial concentration, modulated by a factor representing the role of\nconfinement, expressed as the ratio of the confining radius to the probe\nradius. This finding, while still awaiting a complete theoretical explanation,\noffers new insights into the transport properties of confined active baths and\npaves the way for a deeper understanding of active emulsions driven by confined\nactive matter.",
        "Reliance on stereotypes is a persistent feature of human decision-making and\nhas been extensively documented in educational settings, where it can shape\nstudents' confidence, performance, and long-term human capital accumulation.\nWhile effective techniques exist to mitigate these negative effects, a crucial\nfirst step is to establish whether teachers can recognize stereotypes in their\nprofessional environment. We introduce the Stereotype Identification Test\n(SIT), a novel survey tool that asks teachers to evaluate and comment on the\npresence of stereotypes in images randomly drawn from school textbooks. Their\nresponses are systematically linked to established measures of implicit bias\n(Implicit Association Test, IAT) and explicit bias (survey scales on teaching\nstereotypes and social values). Our findings demonstrate that the SIT is a\nvalid and reliable measure of stereotype recognition. Teachers' ability to\nrecognize stereotypes is linked to trainable traits such as implicit bias\nawareness and inclusive teaching practices. Moreover, providing personalized\nfeedback on implicit bias improves SIT scores by 0.25 standard deviations,\nreinforcing the idea that stereotype recognition is malleable and can be\nenhanced through targeted interventions.",
        "Storm-relative helicity (SRH) is an important ingredient in supercell\ndevelopment, as well as mesocyclone intensity, and is linked to tornadogenesis\nand tornado potential. Derived from the storm-relative wind profile, SRH is\ncomposed of both the vertical wind shear and storm-relative flow. Recent\nstudies have come to conflicting findings regarding whether shallower or deeper\nlayers of SRH have more skill in tornado forecasting. Possible causes of this\ndiscrepancy include the use of observed versus model-based proximity soundings,\nas well as whether the storm-relative wind profile is determined via observed\nversus estimated storm motions. This study uses a new dataset of objectively\nidentified supercells, with observed storm motions, paired with high-resolution\nmodel analyses to address the discrepancies among prior studies. Unlike in\nprevious model-based tornado environmental datasets, the present approach\nreveals substantive differences in storm-relative flow, vertical wind shear,\nand SRH within the low-to-mid-levels between nontornadic and tornadic\nsupercells. Using observed storm motions for storm-relative variables further\nmagnifies differences in the low-to-mid-level storm-relative winds between\nnontornadic and tornadic supercells, ultimately leading to deeper layers of SRH\nhaving more forecast skill than near-ground SRH. Thus, the combination of a\nhigher-resolution model analyses, which better represents the near-storm\nenvironment, with observed storm motions appears to explain why many past\ntornado climatologies using model-based environmental analyses have failed to\nfind significant differences in the storm-relative wind profile. These results\nhelp bridge the gap between previous studies that employed coarser model-based\nanalyses with those that aggregated observed soundings from field projects.",
        "This article is the first in an occasional series for the Australian\nMathematical Society Gazette on diverse aspects and topics of Indigenous\nmathematical knowledge. This is an important, but neglected, part of the\nmathematical heritage of humankind, and as such is the concern of the\nmathematics community as a whole. It is hoped that this and future articles may\nhelp to inspire mathematics researchers, students, and educators at tertiary\nand school levels who are seeking to widen their mathematical horizons and\ndevelop course and research materials of broad cultural relevance.\n  I would like to honour the Mithaka peoples of the Kurrawoolben and\nKirrenderri (Diamantina) and Nooroondinna (Georgina) river channel country of\nsouth-western Qld, Australia. The material in this article does not involve\nculturally restricted knowledge or images, and is shared with respect for the\nMithaka ancestors and their descendants.",
        "Polarimetric GNSS-R systems, equipped with an additional polarization\nchannel, offer enhanced capabilities for separating vegetation and surface\nscattering effects, thereby improving GNSS-R land remote sensing applications\nsuch as soil moisture retrieval in vegetated and forested areas and biomass\nestimation. However, the effectiveness of these applications relies on accurate\ncalibration of the polarimetric GNSS-R sensor. In the Rongowai mission, a newly\ndeveloped Next Generation GNSS-R Receiver (NGRx) is installed on a domestic Air\nNew Zealand airplane to collect data during its commercial flights. The NGRx\nprocesses multi-GNSS satellite signals simultaneously and utilizes a\ndual-channel (LHCP and RHCP) antenna, thereby improving spatial coverage and\nretrieval accuracy. The dual-polarized antenna also provides the possibility to\nexamine the polarimetric GNSS-R system. In this article, a new methodology is\ndeveloped to calibrate the Level-1 power measurement and the on-board antenna\ncross-pol gain by comparing measurements from inland lakes and ocean with\nmodeled results. The calibration results in a 34% decrease in the uncertainty\nin co-pol reflectivity retrieval. The retrieved cross-pol and co-pol\nreflectivity after calibration are examined by their statistical distribution\nand spatial mapping with 1.5 km resolution, with multi-land surface types and\nincidence angles. These results validate the effectiveness of the calibration\nmethod and pave the way for future terrestrial science applications.",
        "Identifying quantum spin liquids, magnon breakdown, or fractionalized\nexcitations in quantum magnets is an ongoing challenge due to the ambiguity of\npossible origins of excitation continua occurring in linear response probes.\nRecently, it was proposed that techniques measuring higher-order response, such\nas two-dimensional coherent spectroscopy (2DCS), could resolve such\nambiguities. Numerically simulating nonlinear response functions can, however,\nbe computationally very demanding. We present an efficient Lanczos-based method\nto compute second-order susceptibilities $\\chi^{2}\\omega_t,\\omega_\\tau)$\ndirectly in the frequency domain. Applying this to extended Kitaev models\ndescribing $\\alpha$-RuCl$_3$, we find qualitatively different nonlinear\nresponses between intermediate magnetic field strengths and the high-field\nregime. To put these results into context, we derive the general 2DCS response\nof partially-polarized magnets within the linear spin-wave approximation,\nestablishing that $\\chi^2(\\omega_t,\\omega_\\tau)$ is restricted to a distinct\nuniversal form if the excitations are conventional magnons. Deviations from\nthis form, as predicted in our (Lanczos-based) simulations for\n$\\alpha$-RuCl$_3$, can hence serve in 2DCS experiments as direct criteria to\ndetermine whether an observed excitation continuum is of conventional\ntwo-magnon type or of different nature.",
        "This paper provides an overview of the nonequilibrium fluctuational forces\nand torques acting on a body either in motion or at rest relative to another\nbody or the thermal vacuum blackbody radiation. For a moving body, a retarding\nforce emerges, called quantum or Casimir friction, which in vacuum was first\npredicted by Einstein and Hopf in 1907. Moreover, if a stationary body is not\nin thermal equilibrium with the blackbody vacuum, a self-propulsive force or\ntorque can appear, resulting in a potentially observable linear or angular\nterminal velocity, even after thermalization.",
        "Three-dimensional atomic force microscopy (3D-AFM) has been a powerful tool\nto probe the atomic-scale structure of solid-liquid interfaces. As a nanoprobe\nmoves along the 3D volume of interfacial liquid, the probe-sample interaction\nforce is sensed and mapped, providing information on not only the solid\nmorphology, but also the liquid density distribution. To date 3D-AFM force maps\nof a diverse set of solid-liquid interfaces have been recorded, revealing\nremarkable force oscillations that are typically attributed to solvation layers\nor electrical double layers. However, despite the high resolution down to\nsub-angstrom level, quantitative interpretation of the 3D force maps has been\nan outstanding challenge. Here we will review the technical details of 3D-AFM\nand the existing approaches for quantitative data interpretation. Based on\nevidences in recent literature, we conclude that the perturbation-induced AFM\nforce paradoxically represents the intrinsic, unperturbed liquid density\nprofile. We will further discuss how the oscillatory force profiles can be\nattributed to the probe-modulation of the liquid configurational entropy, and\nhow the quantitative, atomic-scale liquid density distribution can be derived\nfrom the force maps.",
        "We present a transfer function-based method to estimate angular power spectra\nfrom filtered maps for cosmic microwave background (CMB) surveys. This is\nespecially relevant for experiments targeting the faint primordial\ngravitational wave signatures in CMB polarisation at large scales, such as the\nSimons Observatory (SO) small aperture telescopes. While timestreams can be\nfiltered to mitigate the contamination from low-frequency noise, usual methods\nthat calculate the mode coupling at individual multipoles can be challenging\nfor experiments covering large sky areas or reaching few-arcminute resolution.\nThe method we present here, although approximate, is more practical and faster\nfor larger data volumes. We validate it through the use of simulated\nobservations approximating the first year of SO data, going from half-wave\nplate-modulated timestreams to maps, and using simulations to estimate the\nmixing of polarisation modes induced by an example of time-domain filtering. We\nshow its performance through an example null test and with an end-to-end\npipeline that performs inference on cosmological parameters, including the\ntensor-to-scalar ratio $r$. The performance demonstration uses simulated\nobservations at multiple frequency bands. We find that the method can recover\nunbiased parameters for our simulated noise levels.",
        "We present data processing and verification of the Southern Twenty-centimetre\nAll-sky Polarization Survey (STAPS) conducted with Murriyang, the Parkes 64-m\ntelescope. The survey covers the sky area of -89<Dec<0 and the frequency range\nof 1.3-1.8 GHz split into 1-MHz channels. STAPS was observed commensally with\nthe S-band Polarization All-Sky Survey (S-PASS). The survey is composed of long\nazimuth scans, which allows us to absolutely calibrate Stokes Q and U with the\ndata processing procedure developed for S-PASS. We obtain I, Q, and U maps in\nboth flux density scale (Jy\/beam) and main beam brightness temperature scale\n(K), for the 301 frequency channels with sufficiently good data. The\ntemperature scale is tied to the Global Magneto-ionic Medium Survey (GMIMS)\nhigh-band north sky survey conducted with the Dominion Radio Astrophysical\nObservatory 26-m telescope. All the STAPS maps are smoothed to a common\nresolution of 20 arcmin. The rms noise per channel ranges from about 16 mK to 8\nmK for I, and from about 8 mK to 5 mK for Q and U at frequencies from 1.3 to\n1.8 GHz. The rms noise in Q and U varies with declination and reaches minimum\nat declination of -89 degree. We also run rotation measure (RM) synthesis and\nRM clean to obtain peak polarized intensity and Faraday depth maps. The whole\nSTAPS data processing is validated by comparing flux densities of compact\nsources, pixel flux density versus pixel flux density for Cen A, pixel\ntemperature versus pixel temperature for the entire survey area, and RMs of\nextragalactic sources between STAPS and other measurements. The uncertainty of\nthe flux density scale is less than 10%. STAPS delivers an L-band (20 cm)\nmulti-frequency polarization view of the Galaxy, and will help advance our\nunderstanding of the Galactic magnetic field and magnetized interstellar\nmedium.",
        "This study presents the first report on the development of an artificial\nintelligence (AI) for automatic region segmentation of four-dimensional\ncomputer tomography (4D-CT) images during swallowing. The material consists of\n4D-CT images taken during swallowing. Additionally, data for verifying the\npracticality of the AI were obtained from 4D-CT images during mastication and\nswallowing. The ground truth data for the region segmentation for the AI were\ncreated from five 4D-CT datasets of swallowing. A 3D convolutional model of\nnnU-Net was used for the AI. The learning and evaluation method for the AI was\nleave-one-out cross-validation. The number of epochs for training the nnU-Net\nwas 100. The Dice coefficient was used as a metric to assess the AI's region\nsegmentation accuracy. Regions with a median Dice coefficient of 0.7 or higher\nincluded the bolus, bones, tongue, and soft palate. Regions with a Dice\ncoefficient below 0.7 included the thyroid cartilage and epiglottis. Factors\nthat reduced the Dice coefficient included metal artifacts caused by dental\ncrowns in the bolus and the speed of movement for the thyroid cartilage and\nepiglottis. In practical verification of the AI, no significant misrecognition\nwas observed for facial bones, jaw bones, or the tongue. However, regions such\nas the hyoid bone, thyroid cartilage, and epiglottis were not fully delineated\nduring fast movement. It is expected that future research will improve the\naccuracy of the AI's region segmentation, though the risk of misrecognition\nwill always exist. Therefore, the development of tools for efficiently\ncorrecting the AI's segmentation results is necessary. AI-based visualization\nis expected to contribute not only to the deepening of motion analysis of\norgans during swallowing but also to improving the accuracy of swallowing CT by\nclearly showing the current state of its precision.",
        "Most distinguishing features of black holes and their mimickers are\nconcentrated near the horizon. In contrast, astrophysical observations and\ntheoretical considerations primarily constrain the far-field geometry. In this\nwork we develop tools to effectively describe both, using the two-point Pad\\'e\napproximation to construct interpolating metrics connecting the near and\nfar-field. We extend our previous work by computing the quasinormal modes of\ngravitational perturbations for static, spherically symmetric metrics that\ndeviate from Schwarzschild spacetime. Even at the lowest order, this approach\ncompares well with existing methods in both accuracy and applicability.\nAdditionally, we show that the lowest-order interpolating metric reliably\npredicts light ring locations. It closely matches exact results, even when\nunsuitable for quasinormal frequency calculations.",
        "An open question in hadronic phenomenology concerns the ``unquenching\"\neffects of higher Fock space components on the leading Fock space description\nof hadrons. We address this by making a comparison of the bottomonium spectrum\nas computed with the relativized Godfrey-Isgur quark model and an unquenched\ncoupled channel model driven by the ``$^3P_0$\" mechanism of hadronic decay. Our\nresults show that both models can describe the spectrum well, indicating that\nthe influence of coupled channel effects can be largely absorbed into the\nparameters of the quenched quark model. This conclusion is reinforced by a\nperturbative calculation that shows that the spin-dependence of mass splittings\ndue to mixing with the continuum recapitulates quenched quark model\nspin-dependent interactions. We also show that softening of the quark-antiquark\nwavefunction due to continuum mixing improves the description of vector\nbottomonium decay constants. Together, these results illustrate and\nsubstantiate the surprising robustness of simple constituent quark model\ndescriptions of hadrons.",
        "The selection of penalty hyperparameters is a critical aspect in Nonnegative\nMatrix Factorization (NMF), since these values control the trade-off between\nthe reconstruction accuracy and the adherence to desired constraints. In this\nwork, we focus on an NMF problem involving the Itakura-Saito (IS) divergence,\neffective for extracting low spectral density components from spectrograms of\nmixed signals, enhanced with sparsity constraints. We propose a new algorithm\ncalled SHINBO, which introduces a bi-level optimization framework to\nautomatically and adaptively tune the row-dependent penalty hyperparameters,\nenhancing the ability of IS-NMF to isolate sparse, periodic signals against\nnoise. Experimental results showed SHINBO ensures precise spectral\ndecomposition and demonstrates superior performance in both synthetic and\nreal-world applications. For the latter, SHINBO is particularly useful, as\nnoninvasive vibration-based fault detection in rolling bearings, where the\ndesired signal components often reside in high-frequency subbands but are\nobscured by stronger, spectrally broader noise. By addressing the critical\nissue of hyperparameter selection, SHINBO advances the state-of-the-art in\nsignal recovery for complex, noise-dominated environments.",
        "Let $S=K[x_1, \\ldots,x_n]$ denote the polynomial ring in $n$ variables over a\nfield $K$ and let $I \\subset S$ be a monomial ideal. For a vector\n$\\mathfrak{c}\\in\\mathbb{N}^n$, we set $I_{\\mathfrak{c}}$ to be the ideal\ngenerated by monomials belonging to $I$ whose exponent vectors are\ncomponentwise bounded above by $\\mathfrak{c}$. Also, let\n$\\delta_{\\mathfrak{c}}(I)$ be the largest integer $k$ such that\n$(I^k)_{\\mathfrak{c}}\\neq 0$. It is shown that for every graph $G$ with edge\nideal $I(G)$, the ideal $(I(G)^{\\delta_{\\mathfrak{c}}(I)})_{\\mathfrak{c}}$ is a\npolymatroidal ideal. Moreover, we show that for each integer $s=1, \\ldots\n\\delta_{\\mathfrak{c}}(I(G))$, the Castelnuovo--Mumford regularity of\n$(I(G)^s)_{\\mathfrak{c}}$ is bounded above by $\\delta_{\\mathfrak{c}}(I(G))+s$.",
        "The Plateau-Rayleigh instability shows that a cylindrical fluid flow can be\ndestabilized by surface tension. Similarly, capillary forces can make an\nelastic cylinder unstable when the elastocapillary length is comparable to the\ncylinder's radius. While existing models predict a single isolated bulge as the\nresult of an instability, experiments reveal a periodic sequence of bulges\nspaced out by thinned regions, a phenomenon known as beading instability. Most\nmodels assume that surface tension is independent of the deformation of the\nsolid, neglecting variations due to surface stretch.\n  In this work, we assume that surface tension arises from the deformation of\nmaterial particles near the free surface, treating it as a pre-stretched\nelastic surface surrounding the body. Using the theoretical framework proposed\nby Gurtin and Murdoch, we show that a cylindrical solid can undergo a\nmechanical instability with a finite critical wavelength if the body is\nsufficiently soft or axially stretched. Post-buckling numerical simulations\nreveal a morphology in qualitative agreement with experimental observations.\nPeriod-halving secondary bifurcations are also observed. The results of this\nresearch have broad implications for soft materials, biomechanics, and\nmicrofabrication applications where surface tension plays a crucial role.",
        "The Feynman path integral has revolutionized modern approaches to quantum\nphysics. Although the path integral formalism has proven very successful and\nspawned several approximation schemes, the direct evaluation of real-time path\nintegrals is still extremely expensive and numerically delicate due to its\nhigh-dimensional and oscillatory nature. We propose an efficient method for the\nnumerical evaluation of the real-time world-line path integral for theories\nwhere the potential is dominated by a quadratic at infinity. This is done by\nrewriting the high-dimensional oscillatory integral in terms of a series of\nlow-dimensional oscillatory integrals, that we efficiently evaluate with\nPicard-Lefschetz theory or approximate with the eikonal approximation.\nSubsequently, these integrals are stitched together with a series of fast\nFourier transformations to recover the lattice regularized Feynman path\nintegral. Our method directly applies to problems in quantum mechanics, the\nword-line quantization of quantum field theory, and quantum gravity.",
        "Establishing the relationship between 3D structures and the energy states of\nmolecular systems has proven to be a promising approach for learning 3D\nmolecular representations. However, existing methods are limited to modeling\nthe molecular energy states from classical mechanics. This limitation results\nin a significant oversight of quantum mechanical effects, such as quantized\n(discrete) energy level structures, which offer a more accurate estimation of\nmolecular energy and can be experimentally measured through energy spectra. In\nthis paper, we propose to utilize the energy spectra to enhance the\npre-training of 3D molecular representations (MolSpectra), thereby infusing the\nknowledge of quantum mechanics into the molecular representations.\nSpecifically, we propose SpecFormer, a multi-spectrum encoder for encoding\nmolecular spectra via masked patch reconstruction. By further aligning outputs\nfrom the 3D encoder and spectrum encoder using a contrastive objective, we\nenhance the 3D encoder's understanding of molecules. Evaluations on public\nbenchmarks reveal that our pre-trained representations surpass existing methods\nin predicting molecular properties and modeling dynamics."
      ]
    }
  },
  {
    "id":2412.02695,
    "research_type":"applied",
    "start_id":"b10",
    "start_title":"EEG data for ADHD \/ Control children",
    "start_abstract":"Participants were 61 children with ADHD and 60 healthy controls (boys and girls, ages 7-12). The ADHD children were diagnosed by an experienced psychiatrist to DSM-IV criteria, and have taken Ritalin for up to 6 months. None of the children in the control group had a history of psychiatric disorders, epilepsy, or any report of high-risk behaviors. EEG recording was performed based on 10-20 standard by 19 channels (Fz, Cz, Pz, C3, T3, C4, T4, Fp1, Fp2, F3, F4, F7, F8, P3, P4, T5, T6, O1, O2) at 128 Hz sampling frequency. The A1 and A2 electrodes were the references located on earlobes. Since one of the deficits in ADHD children is visual attention, the EEG recording protocol was based on visual attention tasks. In the task, a set of pictures of cartoon characters was shown to the children and they were asked to count the characters. The number of characters in each image was randomly selected between 5 and 16, and the size of the pictures was large enough to be easily visible and countable by children. To have a continuous stimulus during the signal recording, each image was displayed immediately and uninterrupted after the child\u2019s response. Thus, the duration of EEG recording throughout this cognitive visual task was dependent on the child\u2019s performance (i.e. response speed).",
    "start_categories":[
      "Neurotherapeutics"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b11"
      ],
      "title":[
        "Deep Residual Learning for Image Recognition"
      ],
      "abstract":[
        "Deeper neural networks are more difficult to train. We present a residual learning framework ease the training of that substantially deeper than those used previously. explicitly reformulate layers as functions with reference layer inputs, instead unreferenced functions. provide comprehensive empirical evidence showing these easier optimize, and can gain accuracy from considerably increased depth. On ImageNet dataset we evaluate nets depth up 152 - 8\u00d7 VGG [40] but still having lower complexity. An ensemble achieves 3.57% error on test set. This result won 1st place ILSVRC 2015 classification task. also analysis CIFAR-10 100 1000 layers. The representations is central importance for many visual recognition tasks. Solely due our extremely deep representations, obtain 28% relative improvement COCO object detection dataset. Deep foundations submissions & competitions1, where places tasks detection, localization, segmentation."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "De Sitter Horizon Edge Partition Functions",
        "Reducing Simulation Effort for RIS Optimization using an Efficient\n  Far-Field Approximation",
        "Minerva: A Programmable Memory Test Benchmark for Language Models",
        "Connecting the Unconnectable through Feedback",
        "Language Models for Automated Classification of Brain MRI Reports and\n  Growth Chart Generation",
        "Spider's webs and sharp $L^p$ bounds for the Hardy--Littlewood maximal\n  operator on Gromov hyperbolic spaces",
        "Polarisation conversion and optical meron topologies in anisotropic\n  epsilon-near-zero metamaterials",
        "Censor Resistant Instruction Independent Obfuscation for Multiple\n  Programs",
        "Bell Inequality Violation of Light Quarks in Back-to-Back Dihadron Pair\n  Production at Lepton Colliders",
        "PRISMe: A Novel LLM-Powered Tool for Interactive Privacy Policy\n  Assessment",
        "CoLMDriver: LLM-based Negotiation Benefits Cooperative Autonomous\n  Driving",
        "Intelligent Framework for Human-Robot Collaboration: Safety, Dynamic\n  Ergonomics, and Adaptive Decision-Making",
        "X-Dyna: Expressive Dynamic Human Image Animation",
        "One Model to Train them All: Hierarchical Self-Distillation for Enhanced\n  Early Layer Embeddings",
        "Parametric Hypersensitivity and Transport in the Steady-State\n  Open-System Holstein Model",
        "Exploring strong electronic correlations in the breathing kagome metal\n  Fe$_3$Sn",
        "DEAL: Data-Efficient Adversarial Learning for High-Quality Infrared\n  Imaging",
        "Multi-agent coordination via communication partitions",
        "Hall Coefficient of the Intercalated Graphite CaC$_6$ in the Uniaxial\n  CDW Ground State",
        "The MAGPI Survey: the kinematic morphology-density relation (or lack\n  thereof) and the Hubble sequence at $z\\sim0.3$",
        "Egoistic MDS-based Rigid Body Localization",
        "Collision Risk Quantification and Conflict Resolution in Trajectory\n  Tracking for Acceleration-Actuated Multi-Robot Systems",
        "SWE-Lancer: Can Frontier LLMs Earn $1 Million from Real-World Freelance\n  Software Engineering?",
        "Se-HiLo: Noise-Resilient Semantic Communication with High-and-Low\n  Frequency Decomposition",
        "Embedding of a Discrete Lattice Structure in a Smooth Manifold",
        "Nontrapping Tunable Topological Photonic Memory",
        "Exploring the Finite-Temperature Behavior of Rydberg Atom Arrays: A\n  Tensor Network Approach",
        "CopyJudge: Automated Copyright Infringement Identification and\n  Mitigation in Text-to-Image Diffusion Models",
        "Causality Enhanced Origin-Destination Flow Prediction in Data-Scarce\n  Cities"
      ],
      "abstract":[
        "One-loop $S^{d+1}$ path integrals were shown to factorize into two parts: a\nbulk thermal ideal gas partition function in a $dS_{d+1}$ static patch and an\nedge partition function associated with degrees of freedom living on $S^{d-1}$.\nHere, we analyze the $\\mathfrak{so}(d)$ structure of the edge partition\nfunctions for massive and massless totally symmetric tensors of arbitrary rank\nin any $d\\geq 3$. For linearized Einstein gravity on $S^{d+1}$, we find that\nthe edge partition function receives contributions from shift-symmetric vector\nand scalar fields on $S^{d-1}$ that nonlinearly realize the isometry group\n$SO(d+2)$ of $S^{d+1}$, suggesting a possible interpretation in terms of an\nembedded $S^{d-1}$ brane.",
        "Optimization of Reconfigurable Intelligent Surfaces (RIS) via a previously\nintroduced method is effective, but time-consuming, because multiport impedance\nor scatter matrices are required for each transmitter and receiver position,\nwhich generally must be obtained through full-wave simulation. Herein, a simple\nand efficient far-field approximation is introduced, to extrapolate scatter\nmatrices for arbitrary receiver and transmitter positions from only a single\nsimulation while still maintaining high accuracy suitable for optimization\npurposes. This is demonstrated through comparisons of the optimized capacitance\nvalues and further supported by empirical measurements.",
        "How effectively can LLM-based AI assistants utilize their memory (context) to\nperform various tasks? Traditional data benchmarks, which are often manually\ncrafted, suffer from several limitations: they are static, susceptible to\noverfitting, difficult to interpret, and lack actionable insights--failing to\npinpoint the specific capabilities a model lacks when it does not pass a test.\nIn this paper, we present a framework for automatically generating a\ncomprehensive set of tests to evaluate models' abilities to use their memory\neffectively. Our framework extends the range of capability tests beyond the\ncommonly explored (passkey, key-value, needle in the haystack) search, a\ndominant focus in the literature. Specifically, we evaluate models on atomic\ntasks such as searching, recalling, editing, matching, comparing information in\ncontext memory, and performing basic operations when inputs are structured into\ndistinct blocks, simulating real-world data. Additionally, we design composite\ntests to investigate the models' ability to maintain state while operating on\nmemory. Our benchmark enables an interpretable, detailed assessment of memory\ncapabilities of LLMs.",
        "Reliable uplink connectivity remains a persistent challenge for IoT devices,\nparticularly those at the cell edge, due to their limited transmit power and\nsingle-antenna configurations. This paper introduces a novel framework aimed at\nconnecting the unconnectable, leveraging real-time feedback from access points\n(APs) to enhance uplink coverage without increasing the energy consumption of\nIoT devices. At the core of this approach are feedback channel codes, which\nenable IoT devices to dynamically adapt their transmission strategies based on\nAP decoding feedback, thereby reducing the critical uplink SNR required for\nsuccessful communication. Analytical models are developed to quantify the\ncoverage probability and the number of connectable APs, providing a\ncomprehensive understanding of the system's performance. Numerical results\nvalidate the proposed method, demonstrating substantial improvements in\ncoverage range and connectivity, particularly for devices at the cell edge,\nwith up to a 51% boost in connectable APs. Our approach offers a robust and\nenergy-efficient solution to overcoming uplink coverage limitations, enabling\nIoT networks to connect devices in challenging environments.",
        "Clinically acquired brain MRIs and radiology reports are valuable but\nunderutilized resources due to the challenges of manual analysis and data\nheterogeneity. We developed fine-tuned language models (LMs) to classify brain\nMRI reports as normal (reports with limited pathology) or abnormal, fine-tuning\nBERT, BioBERT, ClinicalBERT, and RadBERT on 44,661 reports. We also explored\nthe reasoning capabilities of a leading LM, Gemini 1.5-Pro, for normal report\ncategorization. Automated image processing and modeling generated brain growth\ncharts from LM-classified normal scans, comparing them to human-derived charts.\nFine-tuned LMs achieved high classification performance (F1-Score >97%), with\nunbalanced training mitigating class imbalance. Performance was robust on\nout-of-distribution data, with full text outperforming summary (impression)\nsections. Gemini 1.5-Pro showed a promising categorization performance,\nespecially with clinical inference. LM-derived brain growth charts were nearly\nidentical to human-annotated charts (r = 0.99, p < 2.2e-16). Our LMs offer\nscalable analysis of radiology reports, enabling automated classification of\nbrain MRIs in large datasets. One application is automated generation of brain\ngrowth charts for benchmarking quantitative image features. Further research is\nneeded to address data heterogeneity and optimize LM reasoning.",
        "In this paper we prove that if $1<a\\leq b<a^2$ and $X$ is a locally doubling\n$\\delta$-hyperbolic complete connected length metric measure space with\n$(a,b)$-pinched exponential growth at infinity, then the centred\nHardy--Littlewood maximal operator $\\mathcal M$ is bounded on $L^p(X)$ for all\n$p>\\tau$, and it is of weak type $(\\tau,\\tau)$, where $\\tau := \\log_ab$. A key\nstep in the proof is a new structural theorem for Gromov hyperbolic spaces with\n$(a,b)$-pinched exponential growth at infinity, consisting in a discretisation\nof $X$ by means of certain graphs, introduced in this paper and called spider's\nwebs, with ``good connectivity properties\". Our result applies to trees with\nbounded geometry, and Cartan--Hadamard manifolds of pinched negative curvature,\nproviding new boundedness results in these settings. The index $\\tau$ is\noptimal in the sense that if $p<\\tau$, then there exists $X$ satisfying the\nassumptions above such that $\\mathcal M$ is not of weak type $(p,p)$.\nFurthermore, if $b>a^2$, then there are examples of spaces $X$ satisfying the\nassumptions above such that $\\mathcal M$ bounded on $L^p(X)$ if and only if\n$p=\\infty$.",
        "Plasmonic metamaterials provide a flexible platform for light manipulation\nand polarisation management, thanks to their engineered optical properties with\nexotic dispersion regimes. Here, we exploit the enhanced spin-orbit coupling\ninduced by the strong anisotropy of plasmonic nanorod metamaterials to control\nthe polarisation of vector vortex beams and generate complex field structures\nwith meron topology. Modifying the degree of ellipticity of the input\npolarisation, we show how the observed meron topology can be additionally\nmanipulated. Flexible control of the state of polarisation of vortex beams is\nimportant in optical manipulation, communications, metrology and quantum\ntechnologies.",
        "This work builds upon and optimizes our prior research on obfuscation as\ninstruction decorrelation which achieves multiple program obfuscation.\nLeveraging this infrastructure, we further achieve the property of\nsensor-resistant computation.",
        "Spin correlations between particles produced at colliders provide valuable\ninsights for quantum information studies. While traditional studies of quantum\ninformation at colliders are typically limited to massive particles with\nperturbative decay, we propose an innovative method to explore the Bell\ninequality in massless quark pair systems by analyzing the azimuthal\ncorrelations in back-to-back $\\pi^+\\pi^-$ dihadron pair production at lepton\ncolliders. Revisiting the Belle data, we have shown the potential to detect\nBell inequality violation of light quarks by introducing an additional angular\ncut, achieving a significance of 2.5 $\\sigma$ even in the worst-case scenario\nof 100% correlated systematic uncertainties in each bins. The significance\nsubstantially exceeds $5\\sigma$ when considering uncorrelated systematic\nuncertainties. Our approach opens avenues for exploring spin quantum\ninformation in the non-perturbative aspect and leverages existing data for\nquantum information research.",
        "Protecting online privacy requires users to engage with and comprehend\nwebsite privacy policies, but many policies are difficult and tedious to read.\nWe present PRISMe (Privacy Risk Information Scanner for Me), a novel Large\nLanguage Model (LLM)-driven privacy policy assessment tool, which helps users\nto understand the essence of a lengthy, complex privacy policy while browsing.\nThe tool, a browser extension, integrates a dashboard and an LLM chat. One\nmajor contribution is the first rigorous evaluation of such a tool. In a\nmixed-methods user study (N=22), we evaluate PRISMe's efficiency, usability,\nunderstandability of the provided information, and impacts on awareness. While\nour tool improves privacy awareness by providing a comprehensible quick\noverview and a quality chat for in-depth discussion, users note issues with\nconsistency and building trust in the tool. From our insights, we derive\nimportant design implications to guide future policy analysis tools.",
        "Vehicle-to-vehicle (V2V) cooperative autonomous driving holds great promise\nfor improving safety by addressing the perception and prediction uncertainties\ninherent in single-agent systems. However, traditional cooperative methods are\nconstrained by rigid collaboration protocols and limited generalization to\nunseen interactive scenarios. While LLM-based approaches offer generalized\nreasoning capabilities, their challenges in spatial planning and unstable\ninference latency hinder their direct application in cooperative driving. To\naddress these limitations, we propose CoLMDriver, the first full-pipeline\nLLM-based cooperative driving system, enabling effective language-based\nnegotiation and real-time driving control. CoLMDriver features a parallel\ndriving pipeline with two key components: (i) an LLM-based negotiation module\nunder an actor-critic paradigm, which continuously refines cooperation policies\nthrough feedback from previous decisions of all vehicles; and (ii) an\nintention-guided waypoint generator, which translates negotiation outcomes into\nexecutable waypoints. Additionally, we introduce InterDrive, a CARLA-based\nsimulation benchmark comprising 10 challenging interactive driving scenarios\nfor evaluating V2V cooperation. Experimental results demonstrate that\nCoLMDriver significantly outperforms existing approaches, achieving an 11%\nhigher success rate across diverse highly interactive V2V driving scenarios.\nCode will be released on https:\/\/github.com\/cxliu0314\/CoLMDriver.",
        "The integration of collaborative robots into industrial environments has\nimproved productivity, but has also highlighted significant challenges related\nto operator safety and ergonomics. This paper proposes an innovative framework\nthat integrates advanced visual perception technologies, real-time ergonomic\nmonitoring, and Behaviour Tree (BT)-based adaptive decision-making. Unlike\ntraditional methods, which often operate in isolation or statically, our\napproach combines deep learning models (YOLO11 and SlowOnly), advanced tracking\n(Unscented Kalman Filter) and dynamic ergonomic assessments (OWAS), offering a\nmodular, scalable and adaptive system. Experimental results show that the\nframework outperforms previous methods in several aspects: accuracy in\ndetecting postures and actions, adaptivity in managing human-robot\ninteractions, and ability to reduce ergonomic risk through timely robotic\ninterventions. In particular, the visual perception module showed superiority\nover YOLOv9 and YOLOv8, while real-time ergonomic monitoring eliminated the\nlimitations of static analysis. Adaptive role management, made possible by the\nBehaviour Tree, provided greater responsiveness than rule-based systems, making\nthe framework suitable for complex industrial scenarios. Our system\ndemonstrated a 92.5\\% accuracy in grasping intention recognition and\nsuccessfully classified ergonomic risks with real-time responsiveness (average\nlatency of 0.57 seconds), enabling timely robotic",
        "We introduce X-Dyna, a novel zero-shot, diffusion-based pipeline for\nanimating a single human image using facial expressions and body movements\nderived from a driving video, that generates realistic, context-aware dynamics\nfor both the subject and the surrounding environment. Building on prior\napproaches centered on human pose control, X-Dyna addresses key shortcomings\ncausing the loss of dynamic details, enhancing the lifelike qualities of human\nvideo animations. At the core of our approach is the Dynamics-Adapter, a\nlightweight module that effectively integrates reference appearance context\ninto the spatial attentions of the diffusion backbone while preserving the\ncapacity of motion modules in synthesizing fluid and intricate dynamic details.\nBeyond body pose control, we connect a local control module with our model to\ncapture identity-disentangled facial expressions, facilitating accurate\nexpression transfer for enhanced realism in animated scenes. Together, these\ncomponents form a unified framework capable of learning physical human motion\nand natural scene dynamics from a diverse blend of human and scene videos.\nComprehensive qualitative and quantitative evaluations demonstrate that X-Dyna\noutperforms state-of-the-art methods, creating highly lifelike and expressive\nanimations. The code is available at https:\/\/github.com\/bytedance\/X-Dyna.",
        "Deploying language models often requires handling model size vs. performance\ntrade-offs to satisfy downstream latency constraints while preserving the\nmodel's usefulness. Model distillation is commonly employed to reduce model\nsize while maintaining acceptable performance. However, distillation can be\ninefficient since it involves multiple training steps. In this work, we\nintroduce MODULARSTARENCODER, a modular multi-exit encoder with 1B parameters,\nuseful for multiple tasks within the scope of code retrieval.\nMODULARSTARENCODER is trained with a novel self-distillation mechanism that\nsignificantly improves lower-layer representations-allowing different portions\nof the model to be used while still maintaining a good trade-off in terms of\nperformance. Our architecture focuses on enhancing text-to-code and\ncode-to-code search by systematically capturing syntactic and semantic\nstructures across multiple levels of representation. Specific encoder layers\nare targeted as exit heads, allowing higher layers to guide earlier layers\nduring training. This self-distillation effect improves intermediate\nrepresentations, increasing retrieval recall at no extra training cost. In\naddition to the multi-exit scheme, our approach integrates a repository-level\ncontextual loss that maximally utilizes the training context window, further\nenhancing the learned representations. We also release a new dataset\nconstructed via code translation, seamlessly expanding traditional text-to-code\nbenchmarks with code-to-code pairs across diverse programming languages.\nExperimental results highlight the benefits of self-distillation through\nmulti-exit supervision.",
        "We demonstrate that the nonequilibrium steady state (NESS) of an open-system\nHolstein model with linear bias displays extreme sensitivity to the closed\nsystem parameters. This sensitivity is shown to correspond to avoided crossings\nin the closed system spectrum, as previously demonstrated in the Rabi model. We\nthen develop a kinetic model to analyze the effects of environmental parameters\non NESS hypersensitivity. This reveals that hypersensitivity only exists in\nintermediate environmental parameter regimes, a prediction that is verified\nnumerically. The inherent spatial character of the Holstein model offers a\nnatural connection to transport, revealing that transport properties in the\nsteady-state regime can be optimized by simultaneously coordinating the closed-\nand open-system parameters.",
        "Kagome metals have emerged as pivotal materials in condensed matter physics\ndue to their unique geometric arrangement and intriguing electronic properties.\nUnderstanding the origin of magnetism in these materials, particularly in iron\nrich Fe-Sn binary compounds like Fe$_3$Sn, holds a significant importance, as\nthey represent potential candidates for permanent magnets with a high Curie\ntemperature and a strong magnetic anisotropy. In the present study, we employ\ndensity-functional theory and dynamical mean-field theory to analyze the\nelectronic structure and magnetic properties of Fe$_3$Sn. Our investigation\nreveals the presence of several nearly-flat bands and Weyl nodes at low\nexcitation energies. The inclusion of local correlation effects is shown to\npush these features even closer to the Fermi energy, which may be important for\ntheir manipulation via external stimuli. Regarding magnetism, the Hubbard-like\ninteraction leads to an increase of orbital polarization at the expenses of a\nminor reduction of the spin moment. The magnetic anisotropy energy exhibits a\nstrong dependence on the particular choice of the Coulomb interaction\nparameters. Additionally, our detailed analysis of the interatomic exchange\ninteractions indicates a significant contribution from the antisymmetric\nexchange, i.e. the Dzyaloshinskii-Moriya interaction, which showcases the\nexistence of magnetic chirality in the system. Overall, our investigation\nhighlights a strong interplay between the flat bands near the Fermi level, the\nlocal Coulomb interaction and the triangular geometry of the lattice, which\nplays a crucial role in driving the magnetic properties of this material.",
        "Thermal imaging is often compromised by dynamic, complex degradations caused\nby hardware limitations and unpredictable environmental factors. The scarcity\nof high-quality infrared data, coupled with the challenges of dynamic,\nintricate degradations, makes it difficult to recover details using existing\nmethods. In this paper, we introduce thermal degradation simulation integrated\ninto the training process via a mini-max optimization, by modeling these\ndegraded factors as adversarial attacks on thermal images. The simulation is\ndynamic to maximize objective functions, thus capturing a broad spectrum of\ndegraded data distributions. This approach enables training with limited data,\nthereby improving model performance.Additionally, we introduce a\ndual-interaction network that combines the benefits of spiking neural networks\nwith scale transformation to capture degraded features with sharp spike signal\nintensities. This architecture ensures compact model parameters while\npreserving efficient feature representation. Extensive experiments demonstrate\nthat our method not only achieves superior visual quality under diverse single\nand composited degradation, but also delivers a significant reduction in\nprocessing when trained on only fifty clear images, outperforming existing\ntechniques in efficiency and accuracy. The source code will be available at\nhttps:\/\/github.com\/LiuZhu-CV\/DEAL.",
        "Coordinating the behaviour of self-interested agents in the presence of\nmultiple Nash equilibria is a major research challenge for multi-agent systems.\nPre-game communication between all the players can aid coordination in cases\nwhere the Pareto-optimal payoff is unique, but can lead to deadlocks when there\nare multiple payoffs on the Pareto frontier. We consider a communication\npartition, where only players within the same coalition can communicate with\neach other, and they can establish an agreement (a coordinated joint-action) if\nit is envy-free, credible, and Pareto-optimal. We show that under a natural\nassumption about symmetry, certain communication partitions can induce social\noptimal outcomes in singleton congestion games. This game is a reasonable model\nfor a decentralised, anonymous system where players are required to choose from\na range of identical resources, and incur costs that are increasing and convex\nin the total number of players sharing the same resource. The communication\npartition can be seen as a mechanism for inducing efficient outcomes in this\ncontext.",
        "We evaluate the Hall coefficient characterising magnetotransport in an\nintercalated graphite CaC$_6$ with the Fermi surface reconstructed by an\nuniaxial charge density wave from closed pockets to open sheets. As the typical\norder parameter, corresponding to the pseudo-gap in electronic spectrum and\nconsequently to spacing between electron trajectories in reciprocal space, is\nof the order of $10^2$K, magnetic breakdown in strong experimentally achievable\nfields of the order of 10T is inevitable. The classical expressions for the\ncomponents of the magnetoconductivity tensor are strongly modified by magnetic\nfield-assisted over-gap tunneling causing quantum interference. Due to magnetic\nbreakdown, all magnetoconductivity components undergo strong quantum\noscillations reflected in the Hall coefficient. In their nature, these are\ndifferent than standard Shubnikov de Haas oscillations which would not appear\nin a system with an open Fermi surface.",
        "This work presents visual morphological and dynamical classifications for 637\nspatially resolved galaxies, most of which are at intermediate redshift\n($z\\sim0.3$), in the Middle-Ages Galaxy Properties with Integral field\nspectroscopy (MAGPI) Survey. For each galaxy, we obtain a minimum of 11\nindependent visual classifications by knowledgeable classifiers. We use an\nextension of the standard Dawid-Skene bayesian model introducing\nclassifier-specific confidence parameters and galaxy-specific difficulty\nparameters to quantify classifier confidence and infer reliable statistical\nconfidence estimates. Selecting sub-samples of 86 bright ($r<20$ mag)\nhigh-confidence ($>0.98$) morphological classifications at redshifts ($0.2 \\le\nz \\le0.4$), we confirm the full range of morphological types is represented in\nMAGPI as intended in the survey design. Similarly, with a sub-sample of 82\nbright high-confidence stellar kinematic classifications, we find that the\nrotating and non-rotating galaxies seen at low redshift are already in place at\nintermediate redshifts. We \\textit{do not} find evidence that the kinematic\nmorphology-density relation seen at $z\\sim0$ is established at $z\\sim0.3$. We\nsuggest that galaxies without obvious stellar rotation are dynamically\npre-processed sometime before $z\\sim0.3$ within lower mass groups before\njoining denser environments.",
        "We consider a novel anchorless rigid body localization (RBL) suitable for\napplication in autonomous driving (AD), in so far as the algorithm enables a\nrigid body to egoistically detect the location (relative translation) and\norientation (relative rotation) of another body, without knowledge of the shape\nof the latter, based only on a set of measurements of the distances between\nsensors of one vehicle to the other. A key point of the proposed method is that\nthe translation vector between the two-bodies is modeled using the\ndouble-centering operator from multidimensional scaling (MDS) theory, enabling\nthe method to be used between rigid bodies regardless of their shapes, in\ncontrast to conventional approaches which require both bodies to have the same\nshape. Simulation results illustrate the good performance of the proposed\ntechnique in terms of root mean square error (RMSE) of the estimates in\ndifferent setups.",
        "One of the pivotal challenges in a multi-robot system is how to give\nattention to accuracy and efficiency while ensuring safety. Prior arts cannot\nstrictly guarantee collision-free for an arbitrarily large number of robots or\nthe results are considerably conservative. Smoothness of the avoidance\ntrajectory also needs to be further optimized. This paper proposes an\naccelerationactuated simultaneous obstacle avoidance and trajectory tracking\nmethod for arbitrarily large teams of robots, that provides a nonconservative\ncollision avoidance strategy and gives approaches for deadlock avoidance. We\npropose two ways of deadlock resolution, one involves incorporating an\nauxiliary velocity vector into the error function of the trajectory tracking\nmodule, which is proven to have no influence on global convergence of the\ntracking error. Furthermore, unlike the traditional methods that they address\nconflicts after a deadlock occurs, our decision-making mechanism avoids the\nnear-zero velocity, which is much more safer and efficient in crowed\nenvironments. Extensive comparison show that the proposed method is superior to\nthe existing studies when deployed in a large-scale robot system, with minimal\ninvasiveness.",
        "We introduce SWE-Lancer, a benchmark of over 1,400 freelance software\nengineering tasks from Upwork, valued at \\$1 million USD total in real-world\npayouts. SWE-Lancer encompasses both independent engineering tasks--ranging\nfrom \\$50 bug fixes to \\$32,000 feature implementations--and managerial tasks,\nwhere models choose between technical implementation proposals. Independent\ntasks are graded with end-to-end tests triple-verified by experienced software\nengineers, while managerial decisions are assessed against the choices of the\noriginal hired engineering managers. We evaluate model performance and find\nthat frontier models are still unable to solve the majority of tasks. To\nfacilitate future research, we open-source a unified Docker image and a public\nevaluation split, SWE-Lancer Diamond\n(https:\/\/github.com\/openai\/SWELancer-Benchmark). By mapping model performance\nto monetary value, we hope SWE-Lancer enables greater research into the\neconomic impact of AI model development.",
        "Semantic communication has emerged as a transformative paradigm in\nnext-generation communication systems, leveraging advanced artificial\nintelligence (AI) models to extract and transmit semantic representations for\nefficient information exchange. Nevertheless, the presence of unpredictable\nsemantic noise, such as ambiguity and distortions in transmitted\nrepresentations, often undermines the reliability of received information.\nConventional approaches primarily adopt adversarial training with noise\ninjection to mitigate the adverse effects of noise. However, such methods\nexhibit limited adaptability to varying noise levels and impose additional\ncomputational overhead during model training. To address these challenges, this\npaper proposes Noise-Resilient \\textbf{Se}mantic Communication with\n\\textbf{Hi}gh-and-\\textbf{Lo}w Frequency Decomposition (Se-HiLo) for image\ntransmission. The proposed Se-HiLo incorporates a Finite Scalar Quantization\n(FSQ) based noise-resilient module, which bypasses adversarial training by\nenforcing encoded representations within predefined spaces to enhance noise\nresilience. While FSQ improves robustness, it compromise representational\ndiversity. To alleviate this trade-off, we adopt a transformer-based\nhigh-and-low frequency decomposition module that decouples image\nrepresentations into high-and-low frequency components, mapping them into\nseparate FSQ representation spaces to preserve representational diversity.\nExtensive experiments demonstrate that Se-HiLo achieves superior noise\nresilience and ensures accurate semantic communication across diverse noise\nenvironments.",
        "I propose a mathematical framework for embedding an unshaped discrete lattice\n$L$ on a smooth manifold $M$. This framework simplifies complex concepts in\npure mathematics and physics by connecting discrete lattice structures with\ncontinuous geometric interpretations through practical embeddings.",
        "We propose a novel topological photonic memory that encodes information\nthrough dynamically controllable Chern numbers in a two-band topological\nphotonic system. Utilizing a honeycomb lattice photonic crystal, the memory\nleverages topologically protected edge states that remain robust against\nfabrication imperfections and environmental perturbations. By applying a\nsynthetic time-dependent magnetic field, we achieve real-time tunability of the\nChern number, enabling rapid and efficient memory switching without the need\nfor light-trapping mechanisms. Our computational study evaluates critical\nperformance metrics, including write speed, read stabilization time, energy gap\nstability, and nonadiabatic transition probabilities. The results demonstrate\nthat the system supports GHz-range write speeds (approximately 1-10 GHz), with\nstable data retention due to the large energy gap between bands. The system\nenables scalable multi-bit memory encoding based on quantized Chern numbers and\nexhibits superior speed, fault tolerance, and robustness compared to\nconventional photonic memory architectures. This work introduces a scalable,\nhigh-speed, and nontrapping optical memory paradigm, paving the way for future\napplications in quantum information processing and optical communication\ntechnologies.",
        "Rydberg atom arrays have emerged as a powerful platform for experimental\nresearch and a challenging subject for theoretical investigation in quantum\nscience. In this study, we investigate the finite-temperature properties of\ntwo-dimensional square-lattice Rydberg atom arrays using the projected\nentangled pair states (PEPS) method. By analyzing the thermal behavior of\nsystems in the checkerboard and striated phases, we extract critical exponents\nand identify phase transition characteristics. Our results confirm that the\ncheckerboard phase transition belongs to the 2D Ising universality class, while\nthe striated phase exhibits critical exponents that deviate from known\nuniversality classes, possibly due to finite-size effects. These findings\nprovide theoretical insights into the thermal stability of quantum phases in\nRydberg atom arrays and offer valuable guidance for future experimental\nefforts.",
        "Assessing whether AI-generated images are substantially similar to\ncopyrighted works is a crucial step in resolving copyright disputes. In this\npaper, we propose CopyJudge, an automated copyright infringement identification\nframework that leverages large vision-language models (LVLMs) to simulate\npractical court processes for determining substantial similarity between\ncopyrighted images and those generated by text-to-image diffusion models.\nSpecifically, we employ an abstraction-filtration-comparison test framework\nwith multi-LVLM debate to assess the likelihood of infringement and provide\ndetailed judgment rationales. Based on the judgments, we further introduce a\ngeneral LVLM-based mitigation strategy that automatically optimizes infringing\nprompts by avoiding sensitive expressions while preserving the non-infringing\ncontent. Besides, our approach can be enhanced by exploring non-infringing\nnoise vectors within the diffusion latent space via reinforcement learning,\neven without modifying the original prompts. Experimental results show that our\nidentification method achieves comparable state-of-the-art performance, while\noffering superior generalization and interpretability across various forms of\ninfringement, and that our mitigation method could more effectively mitigate\nmemorization and IP infringement without losing non-infringing expressions.",
        "Accurate origin-destination (OD) flow prediction is of great importance to\ndeveloping cities, as it can contribute to optimize urban structures and\nlayouts. However, with the common issues of missing regional features and\nlacking OD flow data, it is quite daunting to predict OD flow in developing\ncities. To address this challenge, we propose a novel Causality-Enhanced OD\nFlow Prediction (CE-OFP), a unified framework that aims to transfer urban\nknowledge between cities and achieve accuracy improvements in OD flow\npredictions across data-scarce cities. In specific, we propose a novel\nreinforcement learning model to discover universal causalities among urban\nfeatures in data-rich cities and build corresponding causal graphs. Then, we\nfurther build Causality-Enhanced Variational Auto-Encoder (CE-VAE) to\nincorporate causal graphs for effective feature reconstruction in data-scarce\ncities. Finally, with the reconstructed features, we devise a knowledge\ndistillation method with a graph attention network to migrate the OD prediction\nmodel from data-rich cities to data-scare cities. Extensive experiments on two\npairs of real-world datasets validate that the proposed CE-OFP remarkably\noutperforms state-of-the-art baselines, which can reduce the RMSE of OD flow\nprediction for data-scarce cities by up to 11%."
      ]
    }
  },
  {
    "id":2412.02695,
    "research_type":"applied",
    "start_id":"b11",
    "start_title":"Deep Residual Learning for Image Recognition",
    "start_abstract":"Deeper neural networks are more difficult to train. We present a residual learning framework ease the training of that substantially deeper than those used previously. explicitly reformulate layers as functions with reference layer inputs, instead unreferenced functions. provide comprehensive empirical evidence showing these easier optimize, and can gain accuracy from considerably increased depth. On ImageNet dataset we evaluate nets depth up 152 - 8\u00d7 VGG [40] but still having lower complexity. An ensemble achieves 3.57% error on test set. This result won 1st place ILSVRC 2015 classification task. also analysis CIFAR-10 100 1000 layers. The representations is central importance for many visual recognition tasks. Solely due our extremely deep representations, obtain 28% relative improvement COCO object detection dataset. Deep foundations submissions & competitions1, where places tasks detection, localization, segmentation.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b10"
      ],
      "title":[
        "EEG data for ADHD \/ Control children"
      ],
      "abstract":[
        "Participants were 61 children with ADHD and 60 healthy controls (boys and girls, ages 7-12). The ADHD children were diagnosed by an experienced psychiatrist to DSM-IV criteria, and have taken Ritalin for up to 6 months. None of the children in the control group had a history of psychiatric disorders, epilepsy, or any report of high-risk behaviors. EEG recording was performed based on 10-20 standard by 19 channels (Fz, Cz, Pz, C3, T3, C4, T4, Fp1, Fp2, F3, F4, F7, F8, P3, P4, T5, T6, O1, O2) at 128 Hz sampling frequency. The A1 and A2 electrodes were the references located on earlobes. Since one of the deficits in ADHD children is visual attention, the EEG recording protocol was based on visual attention tasks. In the task, a set of pictures of cartoon characters was shown to the children and they were asked to count the characters. The number of characters in each image was randomly selected between 5 and 16, and the size of the pictures was large enough to be easily visible and countable by children. To have a continuous stimulus during the signal recording, each image was displayed immediately and uninterrupted after the child\u2019s response. Thus, the duration of EEG recording throughout this cognitive visual task was dependent on the child\u2019s performance (i.e. response speed)."
      ],
      "categories":[
        "Neurotherapeutics"
      ]
    },
    "list":{
      "title":[
        "Classical and Deep Reinforcement Learning Inventory Control Policies for\n  Pharmaceutical Supply Chains with Perishability and Non-Stationarity",
        "Void spin distribution as a powerful probe of $\\sigma_{8}$",
        "Social Choice Rules with Responsibility for Individual Skills",
        "Hadron Identification Prospects With Granular Calorimeters",
        "Mie resonances in optical trapping: Their role in kinematics and\n  back-action",
        "The typical structure of dense claw-free graphs",
        "Granulation and Convectional Driving on Stellar Surfaces",
        "11 New Transiting Brown Dwarfs and Very Low Mass Stars from TESS",
        "Any theory that admits a Wigner's Friend type multi-agent paradox is\n  logically contextual",
        "A structural instability drives the VO2 metal-insulator transition",
        "The large time asymptotics of nonlinear multichannel Schroedinger\n  equations",
        "White Gaussian Noise Generation with a Vacuum State Quantum Entropy\n  Source Chip",
        "Non-Reversible Langevin Algorithms for Constrained Sampling",
        "Approche non-invariante de la correspondance de Jacquet-Langlands :\n  analyse spectrale",
        "A simple method to enlarge a basin of attraction using a memristive\n  function",
        "Simulating quantum circuits with arbitrary local noise using Pauli\n  Propagation",
        "Zeptosecond to attosecond dynamics in atoms and possibility of\n  generating a zeptosecond light source",
        "The MeerKAT Fornax Survey V. H i kinematics and Fornax cluster\n  membership of the dwarf galaxy ESO 358-60",
        "Giant Topological Hall Effect Across Wide Temperature in Pt\/NiCo2O4\n  Heterostructure",
        "Black hole accretion and radiation variability in GRMHD simulations with\n  Rezzolla-Zhidenko spacetime",
        "Perturbative solutions for compact objects in (2+1)-dimensional\n  Bopp-Podolsky electrodynamics",
        "Three-dimensional flat band in ultra-thin Kagome metal Mn3Sn film",
        "SIGGRAPH: G: Improved Projective Dynamics Global Using Snapshots-based\n  Reduced Bases",
        "Mathematical analysis of the gradients in deep learning",
        "Continuously tunable anomalous Hall crystals in rhombohedral heptalayer\n  graphene",
        "Spherical black hole perturbations in EFT of scalar-tensor gravity with\n  timelike scalar profile",
        "Background processes in Higgs decay to Z gamma",
        "Late Time Cosmic Acceleration in Modified Gravity and Gauss-Bonnet\n  Cosmology",
        "3D Electron Diffraction as GIWAXS Alternative for Quantitative\n  Structural Characterization of Organic Solar Cells"
      ],
      "abstract":[
        "We study inventory control policies for pharmaceutical supply chains,\naddressing challenges such as perishability, yield uncertainty, and\nnon-stationary demand, combined with batching constraints, lead times, and lost\nsales. Collaborating with Bristol-Myers Squibb (BMS), we develop a realistic\ncase study incorporating these factors and benchmark three\npolicies--order-up-to (OUT), projected inventory level (PIL), and deep\nreinforcement learning (DRL) using the proximal policy optimization (PPO)\nalgorithm--against a BMS baseline based on human expertise. We derive and\nvalidate bounds-based procedures for optimizing OUT and PIL policy parameters\nand propose a methodology for estimating projected inventory levels, which are\nalso integrated into the DRL policy with demand forecasts to improve\ndecision-making under non-stationarity. Compared to a human-driven policy,\nwhich avoids lost sales through higher holding costs, all three implemented\npolicies achieve lower average costs but exhibit greater cost variability.\nWhile PIL demonstrates robust and consistent performance, OUT struggles under\nhigh lost sales costs, and PPO excels in complex and variable scenarios but\nrequires significant computational effort. The findings suggest that while DRL\nshows potential, it does not outperform classical policies in all numerical\nexperiments, highlighting 1) the need to integrate diverse policies to manage\npharmaceutical challenges effectively, based on the current state-of-the-art,\nand 2) that practical problems in this domain seem to lack a single policy\nclass that yields universally acceptable performance.",
        "We present a numerical proof of the concept that the void spin distributions\ncan in principle provide a tight constraint on the amplitude of matter density\nfluctuation on the scale of $8\\,h^{-1}{\\rm Mpc}$ ($\\sigma_{8}$) without being\nseverely deteriorated by the degeneracies of $\\sigma_{8}$ with cold dark matter\ndensity parameter multiplied by the dimensionless Hubble parameter square\n($\\Omega_{\\rm cdm}h^{2}$), total neutrino mass ($M_{\\nu}$) and dark energy\nequation of state ($w$). Applying the Void-Finder algorithm to a total of $15$\nAbacusSummit $N$-body simulations of $15$ different cosmological models, we\nidentify the giant voids to measure their spins, defined as the magnitudes of\nrescaled specific angular momenta of void halos. The $15$ cosmologies include\nthe Planck $\\Lambda$CDM and $14$ non-Planck models, each of which differs among\none another only in one of $\\{\\sigma_{8},\\ \\Omega_{\\rm cdm}h^{2},\\ M_{\\nu},\\\nw\\}$. The probability density distribution of void spins is determined for each\nmodel and found to be well approximated by the generalized Gamma distribution\nwith two characteristic parameters, $k$ and $\\theta$. It turns out that the\nbest-fit values of $k$ and $\\theta$ exhibit very sensitive dependences only on\n$\\sigma_{8}$, being almost insensitive to $\\Omega_{\\rm cdm}h^{2}$, $M_{\\nu}$,\n$w$. This exclusive $\\sigma_{8}$-dependence of the void spin distributions is\nconfirmed to be robust against the variation of the mass and number cuts of\nvoid halos. We also test an observational feasibility of estimating the void\nspins from real data on the galaxy redshifts.",
        "This paper examines normatively acceptable criteria for evaluating social\nstates when individuals are responsible for their skills or productivity and\nthese factors should be accounted for. We consider social choice rules over\nsets of feasible utility vectors \\`a la Nash's (1950) bargaining problem.\nFirst, we identify necessary and sufficient conditions for choice rules to be\nrationalized by welfare orderings or functions over ability-normalized utility\nvectors. These general results provide a foundation for exploring novel choice\nrules with the normalization and providing their axiomatic foundations. By\nadding natural axioms, we propose and axiomatize a new class of choice rules,\nwhich can be viewed as combinations of three key principles: distribution\naccording to individuals' abilities, utilitarianism, and egalitarianism.\nFurthermore, we show that at the axiomatic level, this class of choice rules is\nclosely related to the classical bargaining solution introduced by Kalai and\nSmorodinsky (1975).",
        "In this work we consider the problem of determining the identity of hadrons\nat high energies based on the topology of their energy depositions in dense\nmatter, along with the time of the interactions. Using GEANT4 simulations of a\nhomogeneous lead tungstate calorimeter with high transverse and longitudinal\nsegmentation, we investigated the discrimination of protons, positive pions,\nand positive kaons at 100 GeV. The analysis focuses on the impact of\ncalorimeter granularity by progressively merging detector cells and extracting\nfeatures like energy deposition patterns andtiming information. Two machine\nlearning approaches, XGBoost and fully connected deep neural networks, were\nemployed to assess the classification performance across particle pairs. The\nresults indicate that fine segmentation improves particle discrimination, with\nhigher granularity yielding more detailed characterization of energy showers.\nAdditionally, the results highlight the importance of shower radius, energy\nfractions, and timing variables in distinguishing particle types. The XGBoost\nmodel demonstrated computational efficiency and interpretability advantages\nover deep learning for tabular data structures, while achieving similar\nclassification performance. This motivates further work required to combine\nhigh- and low-level feature analysis, e.g., using convolutional and graph-based\nneural networks, and extending the study to a broader range of particle\nenergies and types.",
        "The heating rate plays a crucial role in the decoherence of the harmonic\nmotion of an optically levitated nanoparticle. The values of this rate vary\ndepending on both the scattering photon rate and the kinetic energy acquired\nthrough individual photon recoils. While the combined roles of these factors\nhave been extensively studied, the energy transfer per recoil has not been\nexplicitly examined. This energy transfer is often approximated using a linear\ndipole model with coefficients $\\{1\/5, 2\/5, 7\/5\\}$ which applies in the\nRayleigh limit. In this work, we analyze the evolution of energy transfer per\nphoton recoil for low-absorption dielectric nanospheres with diameters ranging\nfrom 2 nm to 500 nm. Using a far-field approximation, we demonstrate that the\nKerker condition, which enhances the alignment between incident and scattered\nwavevectors, may significantly reduce the energy transferred per recoil.\nAlthough this reduction is counterbalanced by the increasing scattering rate,\nfor an individual scattering event, the reduction of recoil suggests an\nintrinsic suppression of back-action. Our results reveal a potential\nenhancement in the accuracy of estimations in tabletop experiments involving\nMie particles of the considered sizes and provide guidance for the selection of\noptimal probe sizes and materials. Our interpretation of recoil reduction as a\nmanifestation of back-action suppression indicates the potential for quantum\nnondemolition (QND) measurements by tailoring scattered radiation patterns with\nmetamaterials.",
        "We analyze the asymptotic number and typical structure of claw-free graphs at\nconstant edge densities. The first of our main results is a formula for the\nasymptotics of the logarithm of the number of claw-free graphs of edge density\n$\\gamma \\in (0,1)$. We show that the problem exhibits a second-order phase\ntransition at edge density $\\gamma^\\ast=\\frac{5-\\sqrt{5}}{4}$. The asymptotic\nformula arises by solving a variational problem over graphons. For\n$\\gamma\\geq\\gamma^\\ast$ there is a unique optimal graphon, while for\n$\\gamma<\\gamma^\\ast$ there is an infinite set of optimal graphons. By analyzing\nmore detailed structure, we prove that for $\\gamma<\\gamma^\\ast$, there is in\nfact a unique graphon $W$ such that almost all claw-free graphs at edge density\n$\\gamma$ are close in cut metric to $W$. We also analyze the probability of\nclaw-freeness in the Erd\\H{o}s-R\\'enyi random graph $G(n,p)$ for constant $p$,\nobtaining a formula for the large-deviation rate function for claw-freeness. In\nthis case, the problem exhibits a first-order phase transition at\n$p^\\ast=\\frac{3-\\sqrt{5}}{2}$, separating distinct structural regimes. At the\ncritical point $p^\\ast$, the corresponding graphon variational problem has\ninfinitely many solutions, and we again pinpoint a unique optimal graphon that\ndescribes the typical structure of $G(n,p^\\ast)$ conditioned on being\nclaw-free.",
        "Surface convection is important for the presence of magnetic activity at\nstars. So far, this convection is thought to be a result of heating from below,\nwhere convection cells rise and break up. New models reveal that surface\nconvection is instead strongly driven by cooling from above. We compare two\nsimulations of surface convection, one with a significant heating from below\nand one without. We obtain surface convection in both cases, and they show\nsimilar granulation patterns. The deep convection driven by heating from below\nis still evolving and asymptotically approaches a steady-state solution. We\nfind that convection from below is not needed at all to form typical\nphotospheric granulation. This indicates the possibility of a surface dynamo\nacting on stars without a convecting envelope. Even stars without a convecting\nenvelope could therefore exhibit stronger magnetic and coronal activity than\nexpected so far.",
        "We present the discovery of 11 new transiting brown dwarfs and low-mass\nM-dwarfs from NASA's TESS mission: TOI-2844, TOI-3122, TOI-3577, TOI-3755,\nTOI-4462, TOI-4635, TOI-4737, TOI-4759, TOI-5240, TOI-5467, and TOI-5882. They\nconsist of 5 brown dwarf companions and 6 very low mass stellar companions\nranging in mass from $25 M_{\\rm J}$ to $128 M_{\\rm J}$. We used a combination\nof photometric time-series, spectroscopic, and high resolution imaging\nfollow-up as a part of the TESS Follow-up Observing Program (TFOP) in order to\ncharacterize each system. With over 50 transiting brown dwarfs confirmed, we\nnow have a large enough sample to directly test different formation and\nevolutionary scenarios. We provide a renewed perspective on the transiting\nbrown dwarf desert and its role in differentiating between planetary and\nstellar formation mechanisms. Our analysis of the eccentricity distribution for\nthe transiting brown dwarf sample does not support previous claims of a\ntransition between planetary and stellar formation at $\\sim42$ $M_{\\rm J}$. We\nalso contribute a first look into the metallicity distribution of transiting\ncompanions in the range $7 - 150$ $M_{\\rm J}$, showing that this too does not\nsupport a $\\sim42$ $M_{\\rm J}$ transition. Finally, we also detect a\nsignificant lithium absorption feature in one of the brown dwarf hosts\n(TOI-5882) but determine that the host star is likely old based on rotation,\nkinematic, and photometric measurements. We therefore claim that TOI-5882 may\nbe a candidate for planetary engulfment.",
        "Wigner's Friend scenarios push the boundaries of quantum theory by modeling\nagents, along with their memories storing measurement outcomes, as physical\nquantum systems. Extending these ideas beyond quantum theory, we ask: in which\nphysical theories, and under what assumptions, can agents who are reasoning\nlogically about each other's measurement outcomes encounter apparent paradoxes?\nTo address this, we prove a link between Wigner's Friend type multi-agent\nparadoxes and contextuality in general theories: if agents who are modeled\nwithin a physical theory come to a contradiction when reasoning using that\ntheory (under certain assumptions on how they reason and describe\nmeasurements), then the theory must admit contextual correlations of a logical\nform. This also yields a link between the distinct fundamental concepts of\nHeisenberg cuts and measurement contexts in general theories, and in\nparticular, implies that the quantum Frauchiger-Renner paradox is a proof of\nlogical contextuality. Moreover, we identify structural properties of such\nparadoxes in general theories and specific to quantum theory. For instance, we\ndemonstrate that theories admitting behaviors corresponding to extremal\nvertices of n-cycle contextuality scenarios admit Wigner's Friend type\nparadoxes without post-selection, and that any quantum Wigner's Friend paradox\nbased on the n-cycle scenario must necessarily involve post-selection. Further,\nwe construct a multi-agent paradox based on a genuine contextuality scenario\ninvolving sequential measurements on a single system, showing that Bell\nnon-local correlations between distinct subsystems are not necessary for\nWigner's Friend paradoxes. Our work offers an approach to investigate the\nstructure of physical theories and their information-theoretic resources by\nmeans of deconstructing the assumptions underlying multi-agent physical\nparadoxes.",
        "VO2 features concomitant structural and metal-insulator transitions. This\nposes a challenge for understanding the underlying mechanism: is the transition\ntriggered by a structural or by an electronic instability? Here, we address\nthis question by studying pre-transitional fluctuations in the metallic state.\nBy measuring resonant diffuse X-ray scattering we find no evidence that spatial\nfluctuations of d-electrons are any different from those of vanadium ion cores.\nThat is, charge and lattice remain coupled as they fluctuate jointly towards\nthe insulating phase, strongly supporting the case that the VO2 metal-insulator\ntransition is triggered by a structural instability. Our work offers a novel\napproach to solve similar problems in other strongly correlated systems.",
        "We consider the Schroedinger equation with a general interaction term, which\nis localized in space. The interaction may be x, t dependent and non-linear.\nPurely non-linear parts of the interaction are localized via the radial Sobolev\nembedding. Under the assumption of radial symmetry and boundedness in H1(R3) of\nthe solution, uniformly in time. we prove it is asymptotic in L2 (and H1) in\nthe strong sense, to a free wave and a weakly localized solution. The general\nproperties of the localized solutions are derived. The proof is based on the\nintroduction of phase-space analysis of the nonlinear dispersive dynamics and\nrelies on a new class of (exterior) a priory propagation estimates. This\napproach allows a unified analysis of general linear time-dependent potentials\nand non-linear interactions.",
        "White Gaussian noise (WGN) is widely used in communication system testing,\nphysical modeling, Monte Carlo simulations, and electronic countermeasures. WGN\ngeneration relies heavily on random numbers. In this work, we present an\nimplementation of WGN generation utilizing a quantum entropy source chip for\nthe first time. A photonic integrated chip based on the vacuum state scheme\ngenerates quantum random numbers at a real-time output rate of up to 6.4 Gbps.\nA hardware-based inversion method converts uniform quantum random numbers into\nGaussian random numbers using the inverse cumulative distribution function.\nSubsequently, the WGN signal is generated through a digital-to-analog converter\nand amplifiers. The WGN generator is characterized by a bandwidth of 230 MHz, a\ncrest factor as high as 6.2, and an adjustable peak-to-peak range of 2.5 V.\nThis work introduces a novel approach to WGN generation with information-theory\nprovable quantum random numbers to enhance system security.",
        "We consider the constrained sampling problem where the goal is to sample from\na target distribution on a constrained domain. We propose skew-reflected\nnon-reversible Langevin dynamics (SRNLD), a continuous-time stochastic\ndifferential equation with skew-reflected boundary. We obtain non-asymptotic\nconvergence rate of SRNLD to the target distribution in both total variation\nand 1-Wasserstein distances. By breaking reversibility, we show that the\nconvergence is faster than the special case of the reversible dynamics. Based\non the discretization of SRNLD, we propose skew-reflected non-reversible\nLangevin Monte Carlo (SRNLMC), and obtain non-asymptotic discretization error\nfrom SRNLD, and convergence guarantees to the target distribution in\n1-Wasserstein distance. We show better performance guarantees than the\nprojected Langevin Monte Carlo in the literature that is based on the\nreversible dynamics. Numerical experiments are provided for both synthetic and\nreal datasets to show efficiency of the proposed algorithms.",
        "This is the second article in a two-part series presenting a new proof\ncomparing the non-invariant trace formula for a general linear group with that\nof one of its inner forms. In this article, we focus on the spectral side of\nthe trace formula. We complete the proof of the global Jacquet-Langlands\ncorrespondence using the non-invariant trace formula and examine its arithmetic\nimplications. Furthermore, we define the notion of non-invariant spectral\ntransfer of a test function and show that it coincides with the non-invariant\ngeometric transfer introduced in our first article. This provides a positive\nanswer to a conjecture of Arthur and extends a well-known theorem of Kazhdan\nwithin our framework.",
        "This study presents an innovative approach to chaotic attractor stabilization\nintroducing a memristor in discrete dynamical systems. Using the H\\'enon map as\na test case, we replace a system parameter with a memristive function governed\nby a sigmoid activation function. The method relies on leveraging attractors\nwith larger basins of attraction to attract the orbits and guide them towards\nthe desired chaotic attractor. The effectiveness of the method is confirmed\nthrough numerical simulations, showing substantial enhancement in attractor\nstability without requiring explicit parameter control.",
        "We present a polynomial-time classical algorithm for estimating expectation\nvalues of arbitrary observables on typical quantum circuits under any\nincoherent local noise, including non-unital or dephasing. Although previous\nresearch demonstrated that some carefully designed quantum circuits affected by\nnon-unital noise cannot be efficiently simulated, we show that this does not\napply to average-case circuits, as these can be efficiently simulated using\nPauli-path methods. Specifically, we prove that, with high probability over the\ncircuit gates choice, Pauli propagation algorithms with tailored truncation\nstrategies achieve an inversely polynomially small simulation error. This\nresult holds for arbitrary circuit topologies and for any local noise, under\nthe assumption that the distribution of each circuit layer is invariant under\nsingle-qubit random gates. Under the same minimal assumptions, we also prove\nthat most noisy circuits can be truncated to an effective logarithmic depth for\nthe task of {estimating} expectation values of observables, thus generalizing\nprior results to a significantly broader class of circuit ensembles. We further\nnumerically validate our algorithm with simulations on a $6\\times6$ lattice of\nqubits under the effects of amplitude damping and dephasing noise, as well as\nreal-time dynamics on an $11\\times11$ lattice of qubits affected by amplitude\ndamping.",
        "In nuclear collisions, nuclear bremsstrahlung can cause nuclear Coulomb\nexcitation via photon exchange in the projectile as well as the target nuclei.\nSuch a process originating in nuclear timescales (zeptoseconds) can also\ninfluence the atomic phenomenon, which can be observed if it is delayed at\nleast by a few attoseconds as atomic timescales $\\ge$ an attosecond. We have\nfound that this may happen due to a mechanism called the Eisenbud-Wigner-Smith\n(EWS)time delay process. We have estimated EWS time delays in atomic collisions\nutilizing the non-relativistic version of random phase approximation with\nexchange as well as Hartree-Fock methods. We present three representative\ncollision systems through which one can experimentally observe the phenomena in\nattosecond timescales even though they originate from nuclear bremsstrahlung\nradiation occurring in zeptoseconds. Thus the present work represents an\ninvestigation of parallels between two neighboring areas of physics: atomic and\nnuclear physics. Furthermore the present work suggests the possibilities for\natomic physics research near the Coulomb barrier energies, where the nuclear\nbremsstrahlung can be used as a zeptosecond x-ray source.",
        "The MeerKAT Fornax Survey (MFS) is a large survey project mapping the HI in\nthe Fornax cluster. Most of the cluster members detected in HI show significant\nsigns of interaction with the intra-cluster medium or other galaxies. The\ngalaxy ESO 358-60 however stands out as its large HI disk appears regular and\nundisturbed. A possible explanation for this undisturbed disk is that the\ngalaxy is not in Fornax.\n  We analyze the HI distribution within and around ESO 358-60 based on the MFS\nobservations. We visually inspect the low resolution data in order to study the\nHI disk from the center to its outskirts and look for low column density gas\nthat could reveal recent interactions. We then construct a detailed\nparameterization of the HI disk by fitting a tilted ring model to the high\nresolution data cube. We use the fitted rotational velocity to place the galaxy\non the baryonic Tully-Fisher relationship. By equating the galaxy's HI and 3.6\n$\\mu$m fluxes to the thus retrieved baryonic mass, we obtain a redshift\nindependent distance.\n  Our modeling confirms the regularity of the HI disk in ESO 358-60 but also\nshows that the galaxy contains a significant line of sight warp and contains\nradial motions, of the order of 10 km s$^{-1}$, that cover the extent of the\noptical disk. From the modeling we obtain a velocity V$_{\\rm flat} = $48.1\n$\\pm$ 1.4 km s$^{-1}$ for the best fit rotation curve. This leads to a distance\nfrom the baryonic Tully-Fisher relation of 9.4 $\\pm$ 2.5 Mpc,$\\sim$ 10 Mpc less\nthan the distance to the Fornax cluster. This distance not only fits better\nwith V$_{\\rm flat}$ but also with the overall HI distribution of low mass\ngalaxies and the fact that the galaxy appears undisturbed and reasonably\nsymmetric. At 9.4 Mpc ESO 358-60 cannot be a member of the Fornax cluster but\nis a foreground field galaxy.",
        "Topological Hall effect (THE), a quantum phenomenon arising from emergent\nmagnetic field generated by topological spin texture, is a key method for\ndetecting non-coplanar spin structures like skyrmions in magnetic materials.\nHere, we investigate a bilayer structure of Pt and conducting ferrimagnet\nNiCo2O4 (NCO) of perpendicular magnetic anisotropy and demonstrate giant THE\nacross a temperature range 2 - 350 K. The absence of THE in single-layer Pt and\nNCO, as well as in Pt\/Cu\/NCO, suggests its interfacial origin. The maximum THE\noccurring just before the NCO coercive field indicates its connection to\nmagnetic nucleation centers, which are topologically equivalent to skyrmions.\nThe large normalized THE, based on the emergent-field model, points to a high\npopulation density of small nucleation centers. This aligns with the\nunresolvable domain structures during magnetization reversal, even though clear\ndomain structures are detected after zero-field cooling. These results\nestablish heavy metal\/NCO as a promising system for exploring topological spin\nstructures.",
        "The Event Horizon Telescope (EHT) has revealed the horizon-scale radiation of\nSagittarius A* (Sgr A*), our galaxy's central supermassive black hole, offering\na new platform to test gravitational theories. The next step involves studying\naccretion flows and spacetime structures near black holes using EHT time\nvariability data and GRMHD simulations. We study accretion dynamics in\nspherically symmetric black hole spacetimes deviating from general relativity,\nusing 2D GRMHD simulations with Rezzolla-Zhidenko spacetime. This study\nsystematically investigates how light curve variability amplitudes from\nnon-Kerr GRMHD simulations depend on Schwarzschild spacetime deviations, based\non the constraints from weak gravitational fields and Sgr A*'s shadow size. We\nfind that the dynamics of accretion flows systematically depend on the\ndeviation. In spacetimes with a deeper gravitational potential, fluid and\nAlfv\\'en velocities consistently decrease relative to the Schwarzschild metric,\nindicating weaker dynamical behavior. We also examine the influence of\nspacetime deviations on radiation properties by computing luminosity\nfluctuations at 230 GHz using general relativistic radiative transfer\nsimulations, in line with EHT observations. The amplitude of these fluctuations\nexhibits a systematic dependence on the deviation parameters, decreasing for\ndeeper gravitational potentials compared to the Schwarzschild metric. These\nfeatures are validated using one of the theoretically predicted metrics, the\nHayward metric, a model that describes nonsingular black holes. This\ncharacteristic is expected to have similar effects in future comprehensive\nsimulations that include more realistic accretion disk models and electron\ncooling in the future, potentially aiding in distinguishing black hole\nsolutions that explain the variability of Sgr A*.",
        "We investigate the space-time geometry generated by compact objects in\n(2+1)-dimensional Bopp-Podolsky electrodynamics. Inspired by previous studies\nwhere the Bopp-Podolsky field acts as a source for spherically symmetric\nsolutions, we revisit this question within the lower-dimensional (2+1)\nframework. Using a perturbative approach, we derive a charged BTZ-like black\nhole solution and compute corrections up to second order in a perturbative\nexpansion valid far from the horizon. Our analysis suggests that the\nnear-horizon and inner structure of the solution remain unaltered, indicating\nthat no new non-black hole objects emerge in this regime. In particular, we do\nnot find evidence of wormhole solutions in the (2+1)-dimensional version of\nthis theory.",
        "Flat bands with small energy dispersion can give rise to strongly correlated\nelectronic and topological phases, especially when located at the Fermi level.\nWhilst flat bands have been experimentally realized in two-dimensional (2D)\ntwisted van der Waals heterostructures, they are highly sensitive to twist\nangle, necessitating complex fabrication techniques. Geometrically frustrated\nkagome lattices have emerged as an attractive platform as they natively host\nflat bands that have been observed experimentally in quasi-2D bulk-crystal\nkagome metals. An outstanding experimental question is whether flat bands can\nbe realized in atomically thin metals, with opportunities for stronger\nelectron-electron interactions through tuning of the surrounding dielectric\nenvironment. Here we use angle-resolved photoelectron spectroscopy, scanning\ntunnelling microscopy and band structure calculations to show that ultra-thin\nfilms of the kagome metal Mn3Sn host a robust dispersionless flat band with a\nbandwidth of 50 meV. Furthermore, we demonstrate chemical tuning of the flat\nband to near the Fermi level via manganese defect engineering. The realization\nof tunable kagome-derived flat bands in an ultra-thin kagome metal, represents\na promising platform to study strongly correlated and topological phenomena,\nwith applications in quantum computing, spintronics and low-energy electronics.",
        "We propose a snapshots-based method to compute reduction subspaces for\nphysics-based simulations. Our method is applicable to any mesh with some\nartistic prior knowledge of the solution and only requires a record of existing\nsolutions during, for instance, the range-of-motion test that is required\nbefore approving a mesh character for an application. Our subspaces span a\nwider range of motion, especially large deformations, and rotations by default.\nCompared to the state-of-the-art, we achieve improved numerical stability,\ncomputational efficiency, and more realistic simulations with a smaller\nsub-space.",
        "Deep learning algorithms -- typically consisting of a class of deep\nartificial neural networks (ANNs) trained by a stochastic gradient descent\n(SGD) optimization method -- are nowadays an integral part in many areas of\nscience, industry, and also our day to day life. Roughly speaking, in their\nmost basic form, ANNs can be regarded as functions that consist of a series of\ncompositions of affine-linear functions with multidimensional versions of\nso-called activation functions. One of the most popular of such activation\nfunctions is the rectified linear unit (ReLU) function $\\mathbb{R} \\ni x\n\\mapsto \\max\\{ x, 0 \\} \\in \\mathbb{R}$. The ReLU function is, however, not\ndifferentiable and, typically, this lack of regularity transfers to the cost\nfunction of the supervised learning problem under consideration. Regardless of\nthis lack of differentiability issue, deep learning practioners apply SGD\nmethods based on suitably generalized gradients in standard deep learning\nlibraries like {\\sc TensorFlow} or {\\sc Pytorch}. In this work we reveal an\naccurate and concise mathematical description of such generalized gradients in\nthe training of deep fully-connected feedforward ANNs and we also study the\nresulting generalized gradient function analytically. Specifically, we provide\nan appropriate approximation procedure that uniquely describes the generalized\ngradient function, we prove that the generalized gradients are limiting\nFr\\'echet subgradients of the cost functional, and we conclude that the\ngeneralized gradients must coincide with the standard gradient of the cost\nfunctional on every open sets on which the cost functional is continuously\ndifferentiable.",
        "The interplay of electronic interactions and nontrivial topology can give\nrise to a wealth of exotic quantum states. A notable example is the formation\nof Wigner crystals driven by strong electron-electron interactions. When these\nelectronic crystals emerge in a parent band carrying a large Berry curvature,\nthey can exhibit topologically nontrivial properties as anomalous Hall\ncrystals, spontaneously breaking both continuous translational symmetry and\ntime-reversal symmetry. Here, we report the experimental observation of tunable\nanomalous Hall crystals in rhombohedral heptalayer graphene moir\\'e\nsuperlattices. At filling factors near one electron per moir\\'e unit cell\n(v=1), we identify a series of incommensurate Chern insulators with a Chern\nnumber of C=1. Furthermore, we observe spontaneous time-reversal symmetry\nbreaking spanning the entire filling range from v=1 to v=2, manifesting as\nanomalous Hall effects with pronounced magnetic hysteresis. Notably, anomalous\nHall crystals with a high Chern number C=3 are observed over generic fillings\nranging from v=1.5 to v=2. These anomalous Hall crystals are incommensurate\nwith the moir\\'e superlattice and exhibit dispersive fan diagrams consistent\nwith the Streda formula, with their positions continuously tunable through\ndisplacement fields. Remarkably, these partially filled Chern insulators\ndisplay Chern numbers distinct from their parent bands. Our findings\ndemonstrate the rich variety of electronic crystalline states in rhombohedral\ngraphene moir\\'e superlattices, offering valuable insights into the strongly\ncorrelated topological phases.",
        "We study linear even-parity perturbations of static and spherically symmetric\nblack holes with a timelike scalar profile by use of the effective field theory\n(EFT) approach. For illustrative purposes, we consider a simple subclass of the\nEFT that accommodates ghost condensate, namely the k-essence model along with\nthe so-called scordatura term, and focus on the spherical (monopole)\nperturbations about an approximately stealth Schwarzschild solution. The\nscordatura effect is introduced to avoid the strong coupling problem that\ntypically happens in the scalar sector around stealth solutions with a timelike\nscalar profile. We argue that the scalar perturbation is decoupled from the\nmetric perturbations at the leading order in the scordatura effect under a\nparticular gauge choice. We stress that this is an important step in\nunderstanding the dynamics of even-parity perturbations, paving the way towards\nderiving a set of master equations -- the generalized Zerilli and the\nscalar-field equations -- for generic multipoles.",
        "The ATLAS and CMS Collaborations reported that the observed number of Higgs\nboson decays into a $Z$ boson and a photon is $\\mu = 2.2 \\pm 0.7$ times higher\nthan predicted by the Standard Model. Initially, this discrepancy was\nattributed to a modification of the $HZ\\gamma$ vertex. In the $H \\to Z\\gamma$\nprocess, this decay is reconstructed from $H \\to \\ell\\ell\\gamma$, where $\\ell$\nrepresents either an electron or a muon. In this study, an investigation is\nconducted to examine this anomaly by exploring potential additional background\ncontributions to $H \\to \\ell\\ell\\gamma$ from various subprocesses within and\nbeyond the Standard Model.",
        "This thesis focuses on late-time cosmic acceleration within modified theories\nof gravity, using various observational data sets and statistical analysis. The\nUniverse is assumed to be spatially homogeneous and isotropic and is described\nby the Friedmann Lema\\^{i}tre Robertson Walker metric. The late-time\nacceleration of the Universe has posed a significant challenge to contemporary\ncosmology. General relativity addresses this by introducing the cosmological\nconstant, forming the basis of the standard cosmological model ($\\Lambda$CDM).\nHowever, this model has limitations, leading cosmologists to explore\nalternative explanations for late-time acceleration. These alternatives range\nfrom models involving a dynamic dark fluid known as dark energy, to large-scale\nmodifications of gravitational interaction, known as modified gravity. The\nformulation of general relativity fundamentally changed our understanding of\ngravitation, redefining gravity as a manifestation of the curvature of\nspacetime rather than a force as described by Newton. Despite its success,\ngeneral relativity has shown incompatibilities with observations, necessitating\nthe introduction of dark matter and dark energy....",
        "We demonstrate elastically filtered 3D Electron Diffraction (3D ED) as a\npowerful alternative technique to Grazing Incidence Wide-Angle X-ray Scattering\n(GIWAXS) for quantitatively characterizing the structure of organic\nsemiconductor films. Using a model material system of solvent vapor annealed\nDRCN5T:PC71BM thin film, which is employed in organic solar cells (OSCs), we\nextract the structural data obtained from 3D ED and compare with that from\nGIWAXS, utilizing both laboratory and synchrotron X-ray sources. Quantitative\nevaluation of the datasets in terms of peak positions, peak widths and\nmosaicity revealed good agreement between both techniques, qualifying 3D ED as\nan alternative tool for analyzing highly beam-sensitive organic thin films.\nFurthermore, the respective advantages and limitations of 3D ED and GIWAXS are\ndiscussed, emphasizing the unique capability of 3D ED to integrate seamlessly\nwith the diverse imaging and spectroscopic modalities in modern TEM. This\nintegration enriches the techniques of structural characterization of OSCs,\npaving the way for deeper insights into their structural properties and\nultimately their performance."
      ]
    }
  },
  {
    "id":2411.03522,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"Language Models are Few-Shot Learners",
    "start_abstract":"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training a large corpus of text followed fine-tuning specific task. While typically task-agnostic in architecture, this method still requires task-specific datasets thousands or tens examples. By contrast, humans can generally perform new language task from only few examples simple instructions - something which current systems largely struggle to do. Here we show that scaling up models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art approaches. Specifically, train GPT-3, an autoregressive model 175 billion parameters, 10x more than any previous non-sparse model, test its performance the setting. For all tasks, GPT-3 is applied without gradient updates fine-tuning, demonstrations specified purely via interaction model. achieves strong datasets, including translation, question-answering, cloze as well several require on-the-fly reasoning domain adaptation, such unscrambling words, using novel word sentence, performing 3-digit arithmetic. At same time, also identify some where GPT-3's learning struggles, faces methodological issues related training web corpora. Finally, find generate samples news articles human evaluators have difficulty distinguishing written humans. We discuss broader societal impacts finding general.",
    "start_categories":[
      "cs.CL"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b20"
      ],
      "title":[
        "Long non-coding RNAs: definitions, functions, challenges and recommendations"
      ],
      "abstract":[
        "Genes specifying long non-coding RNAs (lncRNAs) occupy a large fraction of the genomes of complex organisms. The term \u2018lncRNAs\u2019 encompasses RNA polymerase I (Pol I), Pol II and Pol III transcribed RNAs, and RNAs from processed introns. The various functions of lncRNAs and their many isoforms and interleaved relationships with other genes make lncRNA classification and annotation difficult. Most lncRNAs evolve more rapidly than protein-coding sequences, are cell type specific and regulate many aspects of cell differentiation and development and other physiological processes. Many lncRNAs associate with chromatin-modifying complexes, are transcribed from enhancers and nucleate phase separation of nuclear condensates and domains, indicating an intimate link between lncRNA expression and the spatial control of gene expression during development. lncRNAs also have important roles in the cytoplasm and beyond, including in the regulation of translation, metabolism and signalling. lncRNAs often have a modular structure and are rich in repeats, which are increasingly being shown to be relevant to their function. In this Consensus Statement, we address the definition and nomenclature of lncRNAs and their conservation, expression, phenotypic visibility, structure and functions. We also discuss research challenges and provide recommendations to advance the understanding of the roles of lncRNAs in development, cell biology and disease."
      ],
      "categories":[
        "q-bio.CB"
      ]
    },
    "list":{
      "title":[
        "Secure Quantum Key Distribution with Room-Temperature Quantum Emitter",
        "Plebanski complex",
        "Extended string-net models with all anyons at finite temperature",
        "Geodesic cycles on the Sphere: $t$-designs and Marcinkiewicz-Zygmund\n  Inequalities",
        "Minimax Optimality of Classical Scaling Under General Noise Conditions",
        "The Effect of the Non-Abelian Quantum Metric on Superfluidity",
        "Global well-posedness and optimal time-decay of 3D full compressible\n  Navier-Stokes system",
        "Origin of switchable quasiparticle-interference chirality in\n  loop-current phase of kagome metals measured by scanning-tunneling-microscopy",
        "Learning the P2D Model for Lithium-Ion Batteries with SOH Detection",
        "Dot to dot: high-$z$ little red dots in $M_{\\rm bh}$-$M_{\\rm \\star}$\n  diagrams with galaxy-morphology-specific scaling relations and nuclear star\n  clusters",
        "Knoop: Practical Enhancement of Knockoff with Over-Parameterization for\n  Variable Selection",
        "Spectral properties of bottomonium at high temperature: a systematic\n  investigation",
        "Cyclical accretion regime change in the slow X-ray pulsar 4U 0114+65\n  observed with Chandra",
        "Bayesian Parameter Shift Rule in Variational Quantum Eigensolvers",
        "Ground state and magnetic transitions of the orthorhombic\n  antiferromagnet CaCo$_2$TeO$_6$",
        "Clustered Flexible Calibration Plots For Binary Outcomes Using Random\n  Effects Modeling",
        "A Theory of Chaordic Economics: How Artificial Intelligence and\n  Blockchain Transform Businesses, Economies and Societies",
        "Absence of dehydration due to superionic transition at Earth's\n  core-mantle boundary",
        "Optimizing realistic continuous-variable quantum teleportation with\n  non-Gaussian resources",
        "On the origin of compressive turbulence in protoclumps in high redshift\n  disks",
        "A pioneering experiment combining single-antenna and aperture-synthesis\n  data to measure Faraday rotation with GMIMS and the CGPS",
        "Thermal conductance at superradiant phase transition in quantum Rabi\n  model",
        "Science of a coffee cup: a physicist walks into a bar...",
        "Unfolding the Six-Dimensional Tensor Multiplet",
        "Quantum Electrodynamics from Quantum Cellular Automata, and the Tension\n  Between Symmetry, Locality and Positive Energy",
        "Integrative Analysis of High-dimensional RCT and RWD Subject to\n  Censoring and Hidden Confounding",
        "Diffusion Models for Inverse Problems in the Exponential Family",
        "Comparative study on radiation resistance of WTaCrV high-entropy alloy\n  and tungsten in helium-containing conditions",
        "Movement Dynamics in Elite Female Soccer Athletes: The Quantile Cube\n  Approach"
      ],
      "abstract":[
        "On-demand generation of single photons from solid-state quantum emitters is a\nkey building block for future quantum networks, particularly quantum key\ndistribution (QKD) systems, by enabling higher secure key rates (SKR) and lower\nquantum bit error rates (QBER). In this work, we demonstrate the B92 protocol\nbased on single photons from defects in hexagonal boron nitride (hBN). The\nresults show a sifted key rate (SiKR) of 17.5 kbps with a QBER of 6.49 % at a\ndynamic polarization encoding rate of 40 MHz. Finite-key analysis yields a SKR\nof 7 kbps, as one of the highest SKR obtained for any room-temperature single\nphoton source. Our results highlight the potential of hBN defects in advancing\nquantum communication technologies.",
        "As is very well-known, linearisation of the instanton equations on a\n4-manifold gives rise to an elliptic complex of differential operators, the\ntruncated (twisted) Hodge complex $\\Lambda^0(\\mathfrak{g}) \\to\n\\Lambda^1(\\mathfrak{g})\\to \\Lambda^2_+(\\mathfrak{g})$. Moreover, the\nlinearisation of the full YM equations also fits into this framework, as it is\ngiven by the second map followed by its adjoint. We define and study properties\nof what we call the Pleba\\'nski complex. This is a differential complex that\narises by linearisation of the equations implying that a Riemannian 4-manifold\nis hyper-K\\\"ahler. We recall that these are most naturally stated as the\ncondition that there exists a perfect $\\Sigma^i\\wedge \\Sigma^j\\sim\\delta^{ij}$\ntriple $\\Sigma^i, i=1,2,3$ of 2-forms that are closed $d\\Sigma^i=0$. The\nRiemannian metric is encoded by the 2-forms $\\Sigma^i$. We show that what\nresults is an elliptic differential complex $TM \\to S\\to E\\times \\Lambda^1 \\to\nE$, where $S$ is the tangent space to the space of perfect triples, and\n$E=\\mathbb{R}^3$. We also show that, as in the case with instanton equations,\nthe full Einstein equations $Ric=0$ also fit into this framework, their\nlinearisation being given by the second map followed by its adjoint. Our second\nresult concerns the elliptic operator that the Pleba\\'nski complex defines. In\nthe case of the instanton complex, operators appearing in the complex\nsupplemented with their adjoints assemble to give the Dirac operator. We show\nhow the same holds true for the Pleba\\'nski complex. Supplemented by suitable\nadjoints, operators assemble into an elliptic operator that squares to the\nLaplacian and is given by the direct sum of two Dirac operators.",
        "String-net models describe a vast family of topological orders in two spatial\ndimensions, but fail to produce all the expected anyonic excitations. Following\narXiv:1502.03433, we consider an extended string-net model by attaching one\ntail to each plaquette of the lattice, allowing all anyons to emerge as\nelementary plaquette excitations for arbitrary input categories. The\ncorresponding tube algebra is the mathematical tool needed to construct the\nanyons from the input category and to obtain their internal multiplicities. We\nuse them to compute the energy level degeneracies and the partition function.\nIn the thermodynamic limit, the latter is dominated by the trivial (vacuum)\nanyon, so that the topological order is destroyed at any non-zero temperature.\nIn a finite-size system, order survives up to a finite temperature, similarly\nto the one-dimensional classical Ising model. We confirm this by computing\nthermal averages of topological projectors, Wegner-Wilson loops and the\ntopological mutual information. The results are also generalized to models with\nmultiple tails per plaquette.",
        "A geodesic cycle is a closed curve that connects finitely many points along\ngeodesics. We study geodesic cycles on the sphere in regard to their role in\nequal-weight quadrature rules and approximation.",
        "We establish the consistency of classical scaling under a broad class of\nnoise models, encompassing many commonly studied cases in literature. Our\napproach requires only finite fourth moments of the noise, significantly\nweakening standard assumptions. We derive convergence rates for classical\nscaling and establish matching minimax lower bounds, demonstrating that\nclassical scaling achieves minimax optimality in recovering the true\nconfiguration even when the input dissimilarities are corrupted by noise.",
        "The quantum geometric tensor, which encodes the full geometric information of\nquantum states in projective Hilbert space, plays a crucial role in condensed\nmatter physics. In this work, we examine the effect of the non-Abelian quantum\nmetric -- the real part of the non-Abelian quantum geometric tensor -- on the\nsuperfluid weight in time-reversal symmetric systems. For conventional $s$-wave\npairing, we demonstrate that the superfluid weight includes a contribution\nproportional to the trace of the non-Abelian quantum metric. Notably, this\ncontribution remains significant even when the total Chern number of a set of\ndegenerate bands is zero and can exceed the conventional contribution, as\nconfirmed using lattice models. Ab initio density functional theory (DFT)\ncalculations for MoS$_2$ and TiSe$_2$ further corroborate these findings,\nrevealing that the non-Abelian quantum metric accounts for up to 20% of the\nsuperfluid weight in MoS$_2$ and 50% in TiSe$_2$. Our results provide new\ninsights into the nontrivial relationship between the geometric properties of\nquantum states and superconductivity, opening avenues for further exploration\nin topological and superconducting materials.",
        "In this paper, we investigate the global well-posedness and optimal\ntime-decay of classical solutions for the 3-D full compressible Navier-Stokes\nsystem, which is given by the motion of the compressible viscous and\nheat-conductive gases. First of all, we study the global well-posedness of the\nCauchy problem to the system when the initial data is small enough. Secondly,\nwe show the optimal decay rates of the higher-order spatial derivatives of the\n$\\dot{H}^{-s}$ $\\left(0\\leq s<\\frac{3}{2}\\right)$ negative Sobolev norms.\nFinally, under the assumption that the initial data is bounded in $L^{1}$-norm,\nwe establish the upper and lower bounds of the optimal decay rates for the\nclassical solutions.",
        "The chiral loop-current (LC) phase in kagome metals AV3Sb5 (A = Cs, Rb, K)\nhas attracted considerable attention as a novel quantum state driven by\nelectron correlations. Scanning tunneling microscopy (STM) experiments have\nprovided strong evidence for the chiral LC phase through the detection of\nchirality in the quasiparticle interference (QPI) signal. However, the\nfundamental relationship between ``QPI chirality'' and ``LC chirality'' remains\nunexplored. For instance, the QPI signal is unchanged even when all LC orders\nare inverted. Furthermore, only the chiral LC order cannot induce QPI\nchirality. At present, the true essence of kagome metals that we should learn\nfrom the remarkable QPI experiments remains elusive. To address this, we\ninvestigate the origin of the QPI signal in the LC phase using a large\nunit-cell tight-binding model for kagome metals. The LC phase gives rise to a\n$Z_3$ nematic phase, characterized by three distinct directors, under the\nStar-of-David bond order. Our findings demonstrate that the QPI chirality\ninduced by a single impurity at site Z, denoted as $\\chi_Z$, can take values of\n$\\pm1$ (chiral) or 0 (achiral), depending on the direction of the $Z_3$ nematic\norder. Prominent QPI chirality originates from extremely dilute impurities\n($\\lesssim$0.1%) in the present mechanism. Notably, $\\chi_Z$ ($=\\pm1$, 0)\nchanges smoothly with minimal free-energy barriers by applying a small magnetic\nfield $B_z$, accompanied by a switching of the $Z_3$ nematic director. This\nstudy provides a comprehensive explanation for the observed ``$B_z$-switchable\nQPI chirality'' in regions with dilute impurities, offering fundamental insight\ninto the chiral LC in kagome metals.",
        "Lithium ion batteries are widely used in many applications. Battery\nmanagement systems control their optimal use and charging and predict when the\nbattery will cease to deliver the required output on a planned duty or driving\ncycle. Such systems use a simulation of a mathematical model of battery\nperformance. These models can be electrochemical or data-driven.\nElectrochemical models for batteries running at high currents are\nmathematically and computationally complex. In this work, we show that a\nwell-regarded electrochemical model, the Pseudo Two Dimensional (P2D) model,\ncan be replaced by a computationally efficient Convolutional Neural Network\n(CNN) surrogate model fit to accurately simulated data from a class of random\ndriving cycles. We demonstrate that a CNN is an ideal choice for accurately\ncapturing Lithium ion concentration profiles. Additionally, we show how the\nneural network model can be adjusted to correspond to battery changes in State\nof Health (SOH).",
        "High-redshift little red dots (LRDs) detected with the James Webb Space\nTelescope are considered the cores of emerging galaxies. For the first time, we\ncompare LRDs in $M_{\\rm bh}$-$M_{\\star}$ diagrams with an array of $z=0$\ngalaxy-morphology-dependent scaling relations, along with the $M_{\\rm\nbh}$-$M_{\\rm \\star,nsc}$ relation for nuclear star clusters. The $M_{\\rm\nbh}$-$M_{\\rm \\star,sph}$ relations for spheroidal stellar systems are\ncharacterised by a nearly parallel set of quasi-quadratic (or steeper)\ndistributions that are known to trace the `punctuated equilibrium' of galaxies,\nreflecting their stepwise growth in black hole mass and merger-built\nbulge\/spheroid mass. We show that LRDs are not equivalent to nuclear star\nclusters, with the latter having higher $M_{\\rm bh}\/M_{\\star}$ ratios. However,\nthe least massive LRDs exhibit similar $M_{\\rm bh}$ and $M_{\\rm \\star,gal}$\nvalues as ultracompact dwarf (UCD) galaxies. We show that the LRDs span the\n$M_{\\rm bh}$-$M_{\\rm \\star,gal}$ diagram from UCD galaxies to primaeval\nlenticular galaxies. In contrast, spiral galaxies and the subset of\nmajor-merger-built early-type galaxies define offset relations. Additionally,\nwe observe that low-$z$ galaxies with active galactic nuclei align with the\nsteep black hole scaling relations for disc galaxies defined by primarily\ninactive galaxies with directly measured black hole masses. Collectively, this\nhighlights the benefits of considering galaxy morphology, which reflects their\naccretion and merger history, to understand the coevolution of galaxies and\ntheir black holes.",
        "Variable selection plays a crucial role in enhancing modeling effectiveness\nacross diverse fields, addressing the challenges posed by high-dimensional\ndatasets of correlated variables. This work introduces a novel approach namely\nKnockoff with over-parameterization (Knoop) to enhance Knockoff filters for\nvariable selection. Specifically, Knoop first generates multiple knockoff\nvariables for each original variable and integrates them with the original\nvariables into an over-parameterized Ridgeless regression model. For each\noriginal variable, Knoop evaluates the coefficient distribution of its\nknockoffs and compares these with the original coefficients to conduct an\nanomaly-based significance test, ensuring robust variable selection. Extensive\nexperiments demonstrate superior performance compared to existing methods in\nboth simulation and real-world datasets. Knoop achieves a notably higher Area\nunder the Curve (AUC) of the Receiver Operating Characteristic (ROC) Curve for\neffectively identifying relevant variables against the ground truth by\ncontrolled simulations, while showcasing enhanced predictive accuracy across\ndiverse regression and classification tasks. The analytical results further\nbackup our observations.",
        "We investigate spectral features of bottomonium at high temperature, in\nparticular the thermal mass shift and width of ground state S-wave and P-wave\nstate. We employ and compare a range of methods for determining these features\nfrom lattice NRQCD correlators, including direct correlator analyses\n(multi-exponential fits and moments of spectral functions), linear methods\n(Backus-Gilbert, Tikhonov and HLT methods), and Bayesian methods for spectral\nfunction reconstruction (MEM and BR). We comment on the reliability and\nlimitations of the various methods.",
        "4U 0114+65 is a high-mass X-ray binary system formed by the luminous\nsupergiant B1Ia, known as V{*} V662 Cas, and one of the slowest rotating\nneutron stars (NS) with a spin period of about 2.6 hours. This fact provides a\nrare opportunity to study interesting details of the accretion within each\nindividual pulse of the compact object. In this paper, we analyze 200 ks of\nChandra grating data, divided into 9 uninterrupted observations around the\norbit. The changes in the circumstellar absorption column through the orbit\nsuggest an orbital inclination of $\\sim$ $40^{\\circ}$ with respect to the\nobserver and a companion mass-loss rate of $\\sim$ 8.6 10$^{-7}$ solar masses\nyr$^{-1}$. The peaks of the NS pulse show a large pulse-to-pulse variability.\nThree of them show an evolution from a brighter regime to a weaker one. We\npropose that the efficiency of Compton cooling in this source fluctuates\nthroughout an accumulation cycle. After significant depletion of matter within\nthe magnetosphere, since the settling velocity is $\\sim \\times$ 2 times lower\nthan the free-fall velocity, the source gradually accumulates matter until the\ndensity exceeds a critical threshold. This increase in density triggers a\ntransition to a more efficient Compton cooling regime, leading to a higher mass\naccretion rate and consequently to an increased brightness.",
        "Parameter shift rules (PSRs) are key techniques for efficient gradient\nestimation in variational quantum eigensolvers (VQEs). In this paper, we\npropose its Bayesian variant, where Gaussian processes with appropriate kernels\nare used to estimate the gradient of the VQE objective. Our Bayesian PSR offers\nflexible gradient estimation from observations at arbitrary locations with\nuncertainty information and reduces to the generalized PSR in special cases. In\nstochastic gradient descent (SGD), the flexibility of Bayesian PSR allows the\nreuse of observations in previous steps, which accelerates the optimization\nprocess. Furthermore, the accessibility to the posterior uncertainty, along\nwith our proposed notion of gradient confident region (GradCoRe), enables us to\nminimize the observation costs in each SGD step. Our numerical experiments show\nthat the VQE optimization with Bayesian PSR and GradCoRe significantly\naccelerates SGD and outperforms the state-of-the-art methods, including\nsequential minimal optimization.",
        "We report the systematic synthesis, crystal structure, magnetization, and\npowder neutron diffraction of single crystalline and polycrystalline\nCaCo$_2$TeO$_6$ samples. CaCo$_2$TeO$_6$ crystallizes in an orthorhombic\nstructure with $Pnma$ space group, featuring chains of edge-shared CoO$_6$\noctahedra arranged in a honeycomb pattern. Two antiferromagnetic transitions\nare observed at $T$$_{N1}$ = 14.4 K and $T$$_{N2}$ = 16.2 K, corresponding to\ntwo long-range magnetic orders with propagation vectors of $\\bf{k}$$_1$ = (0,\n0, 0) and $\\bf{k}$$_2$ = (0.125, 0, 0.25), respectively. The ground state is\ndetermined as a canted up-up-down-down zigzag spin configuration along the $c$\naxis, wherein the magnetic moments of Co1 and Co2 ions are 3.4(1) and\n2.1(1)$\\mu$$_B$, respectively. Successive spin-flop transitions appear with the\nincreasing magnetic field applied along the easy axis ($c$ axis), accompanied\nby depression of the antiferromagnetic orders and enhancement of residual\nmagnetic entropy. The field-induced spin-disordered state suggests that\nCaCo$_2$TeO$_6$ may be an ideal candidate for studying frustrated magnetism.",
        "Evaluation of clinical prediction models across multiple clusters, whether\ncenters or datasets, is becoming increasingly common. A comprehensive\nevaluation includes an assessment of the agreement between the estimated risks\nand the observed outcomes, also known as calibration. Calibration is of utmost\nimportance for clinical decision making with prediction models and it may vary\nbetween clusters. We present three approaches to take clustering into account\nwhen evaluating calibration. (1) Clustered group calibration (CG-C), (2) two\nstage meta-analysis calibration (2MA-C) and (3) mixed model calibration (MIX-C)\ncan obtain flexible calibration plots with random effects modelling and\nproviding confidence and prediction intervals. As a case example, we externally\nvalidate a model to estimate the risk that an ovarian tumor is malignant in\nmultiple centers (N = 2489). We also conduct a simulation study and synthetic\ndata study generated from a true clustered dataset to evaluate the methods. In\nthe simulation and the synthetic data analysis MIX-C gave estimated curves\nclosest to the true overall and center specific curves. Prediction interval was\nbest for 2MA-C with splines. Standard flexible calibration worked likewise in\nterms of calibration error when sample size is limited. We recommend using\n2MA-C (splines) to estimate the curve with the average effect and the 95% PI\nand MIX-C for the cluster specific curves, specially when sample size per\ncluster is limited. We provide ready-to-use code to construct summary flexible\ncalibration curves with confidence and prediction intervals to assess\nheterogeneity in calibration across datasets or centers.",
        "Dee Hock, the founder of Visa, coined the term 'chaordic' to describe\nsimultaneously chaotic and ordered systems. Based on his reasoning, we\nintroduce the Theory of Chaordic Economics to explain how economic systems are\ntransformed by two disruptive technologies: namely Artificial Intelligence and\nBlockchain. Artificial intelligence can generate novel output through\nalgorithmic yet rather unpredictable processes. Blockchain creates\ndeterministic results without central authorities and relies on elaborated\nprotocols that prescribe how consensus can be reached within a network of\npeers. The amalgamation of chaos and order produces chaordic economic systems\nand can yield hitherto unthinkable economic structures.",
        "The properties and stability of hydrous phases are key to unraveling the\nmysteries of the water cycle in Earth's interior. Under the deep lower mantle\nconditions, hydrous phases transition into a superionic state. However, the\ninfluence of the superionic effect on their stability and dehydration processes\nremains poorly understood. Using ab initio calculations and deep-learning\npotential molecular dynamics simulations, we discovered a doubly superionic\ntransition in delta-AlOOH, characterized by the highly diffusive behavior of\nionic hydrogen and aluminum within the oxygen sub-lattice. These highly\ndiffusive elements contribute significant external entropy into the system,\nresulting in exceptional thermostability. Free energy calculations indicate\nthat dehydration is energetically and kinetically unfavorable when water exists\nin a superionic state under core-mantle boundary (CMB) conditions.\nConsequently, water can accumulate in the deep lower mantle over Earth's\nhistory. This deep water reservoir plays a crucial role in the global deep\nwater and hydrogen cycles.",
        "In this work, we investigate the performance of non-Gaussian entangled\nresources in continuous-variable quantum teleportation within a realistic\nsetting. We describe the characteristic functions of three distinct entangled\nresources, a two-mode squeezed vacuum state, a two-mode photon-subtracted\nsqueezed state, and a two-mode photon-added squeezed state. We extend the\ntheoretical analysis by Yang et al. to include the realistic experimental\nconditions such as photon losses, imperfect measurements which typically affect\ncontinuous-variable quantum teleportation. Our results demonstrate that even in\nnon-ideal situations, the photon-subtracted squeezed state outperforms the\nother two resources in the low squeezing regime, keeping fidelity above the\nclassical threshold that suggests the robustness of photon-subtracted squeezed\nstate in practical teleportation applications. We further analyze the EPR\ncorrelations of these entangled resources, revealing that the photon-subtracted\nsqueezed state exhibits stronger EPR correlations than the original two-mode\nsqueezed vacuum state and the two-mode photon-added squeezed state. This study\nmerges theoretical models with realistic imperfections and utilizes\nnon-Gaussian entanglement into high-fidelity quantum teleportation.",
        "The giant, star forming clumps in gas-rich, high redshift disks are commonly\nassumed to form due to gravitational instabilities, in which protoclumps have a\nToomre-$Q$ parameter less than unity. However, some cosmological simulations\nshow that clumps can form in regions where $Q\\gg1$. In these simulations, there\nis an excess of compressive modes of turbulence that lead to gravitational\ncollapse of regions that were not supposed to gravitationally collapse,\naccording to linear theory. In contrast, sites of clump formation in isolated\nsimulations do not show this excess, hinting that the origin may be external.\nWe explore two external mechanisms that can induce compressive modes of disk\nturbulence in protoclumps, namely, compressive tides exerted by the\ncosmological environment and the direct driving by inflowing streams. We\ncorrelate the local strength of compressive tides and the amount of fresh\nstream material with protoclump regions in zoom-in cosmological simulations.\nThe local strength of compressive tides is derived from the tidal tensor. The\nlocal strength of incoming streams is derived from the fractional presence of\nthe stream compared to the average. We find that the tidal field in protoclumps\ntends to be over-compressive while random patches in the disk show diverging\ntides. In particular, in $25\\%$ of the protoclumps, the tidal field is fully\ncompressive, while no random patch resides in regions of fully compressive\ntides. In addition, protoclumps tend to reside in regions where the fraction of\nincoming stream mass is 2-10 times larger than the average at the same\ngalactocentric radius. Both compressive tides and inflowing streams are\ncorrelated with the protoclumps and can thus serve as the drivers of excessive\ncompressive turbulence that can initiate clump formation. This constitutes a\nnew, non-linear mode of violent disk instabilities in high-$z$ galaxies.",
        "Structures in the magneto-ionic medium exist across a wide range of angular\nsizes due to large-scale magnetic fields coherent over the Galactic spiral arms\ncombined with small-scale fluctuations in the magnetic field and electron\ndensity resulting from energy injection processes such as supernovae. For the\nfirst time, we produce diffuse Galactic synchrotron emission Faraday rotation\nmaps covering all spatial scales down to $3'$ resolution for Galactic magnetic\nfield studies. These maps complement standard total and polarized intensity\nmaps combining single-antenna and interferometric data that have been produced,\nfor example, for the Canadian Galactic Plane Survey (CGPS). Such combined maps\nhave sensitivity to large scales from the single-antenna component, and the\nresolution from the interferometric component. We combine Global Magneto-Ionic\nMedium Survey High-Band-North (GMIMS-HBN) single-antenna and CGPS\naperture-synthesis polarization data after spatial filtering, producing Stokes\n$Q$ and $U$ maps individually for the four CGPS frequency channels. We\ncalculate Rotation Measures (RM) for all pixels using a linear fit to\npolarization angle versus wavelength squared. The RM maps show magnetic field\nstructures on the full range of scales probed by this dataset. Regions of\nsmooth polarized emission require the large-scale sensitivity of the\nsingle-antenna data to illuminate the Faraday depths, while the\naperture-synthesis data reveal small-scale variability in the Faraday rotation.\nDespite the limitation of the 35 MHz bandwidth of the CGPS, we demonstrate that\nuseful information on Faraday rotation structures can be obtained by combining\nsingle-antenna and aperture-synthesis data, highlighting the important synergy\nbetween future broadband interferometric and single-antenna polarization\nsurveys.",
        "The quantum Rabi model exhibits a superradiant phase transition when the\ncoupling becomes strong, even though it involves only two components: a\ntwo-level atom and a single bosonic mode. This phase transition is referred to\nas a few-body quantum phase transition, in contrast to conventional phase\ntransitions in many-body systems. In this work, we investigate heat transport\nacross an atom embedded in bosonic modes, modeled by the quantum Rabi model,\nbetween two thermal baths. We found a manifestation of the superradiant phase\ntransition in the thermal conductance, which represents the linear response to\na temperature bias. Our work can be helpful for the development of quantum heat\ndevices utilizing controllable few-body phase transitions.",
        "... and annoys everyone with unsolicited experiments. The present paper\nproposes a short pedagogical review of the various phenomena that can be\nobserved in a coffee cup with little to no equipment. The physical domains\nspanned include acoustics, optics and, of course, fluid mechanics. The variety\nof experimental and theoretical techniques introduced throughout the paper\nmakes it suitable for a broad audience. For each topic, we first propose an\nexperimental realization before introducing a minimal model to explain the\nobservations. We end each section by discussing more advanced works existing in\nthe literature as well as related applications. We provide detailed\nexperimental procedures and videos of the experiments that can be freely used\nfor teaching purposes. The phenomena presented here also show remarkable\nefficiency as icebreakers for morning coffee in laboratories or conferences.",
        "We derive a manifestly superconformally covariant unfolded formulation of the\nfree (2,0) tensor multiplet. The unfolded system consists of an abelian\ntwo-form and an infinite-dimensional, chiral Weyl zero-form realized using\nsuperoscillators. The construction of the cocycle gluing these forms on a\ngeneral superconformal background goes one step beyond previous results in\nsuper-Poincar\\'e backgrounds.",
        "We show that free QED is equivalent to the continuous-space-and-time limit of\nFermi and Bose lattice quantum cellular automata theories derived from quantum\nrandom walks satisfying simple symmetry and unitarity conditions. In doing so\nwe define the Fermi and Bose theories in a unified manner using the usual\nfermion internal space but a boson internal space that is six-dimensional. We\nshow that the reduction to a two-dimensional boson internal space (two helicity\nstates arising from spin-1 plus the photon transversality condition) comes from\nrestricting the quantum cellular automaton theory to positive energies. We\nbriefly examine common symmetries of quantum cellular automata, and how\ntime-reversal symmetry demands the existence of negative-energy solutions.\nThese solutions produce a tension in coupling the Fermi and Bose theories, in\nwhich the strong locality of quantum cellular automata seems to require a\nnonzero amplitude to produce negative-energy states, leading to an unphysical\ncascade of negative-energy particles. However, we show in a 1D model that by\nextending interactions over a larger (but finite) range it is possible to\nexponentially suppress the production of negative-energy particles to the point\nwhere they can be neglected.",
        "In this study, we focus on estimating the heterogeneous treatment effect\n(HTE) for survival outcome. The outcome is subject to censoring and the number\nof covariates is high-dimensional. We utilize data from both the randomized\ncontrolled trial (RCT), considered as the gold standard, and real-world data\n(RWD), possibly affected by hidden confounding factors. To achieve a more\nefficient HTE estimate, such integrative analysis requires great insight into\nthe data generation mechanism, particularly the accurate characterization of\nunmeasured confounding effects\/bias. With this aim, we propose a\npenalized-regression-based integrative approach that allows for the\nsimultaneous estimation of parameters, selection of variables, and\nidentification of the existence of unmeasured confounding effects. The\nconsistency, asymptotic normality, and efficiency gains are rigorously\nestablished for the proposed estimate.\n  Finally, we apply the proposed method to estimate the HTE of lobar\/sublobar\nresection on the survival of lung cancer patients. The RCT is a multicenter\nnon-inferiority randomized phase 3 trial, and the RWD comes from a clinical\noncology cancer registry in the United States. The analysis reveals that the\nunmeasured confounding exists and the integrative approach does enhance the\nefficiency for the HTE estimation.",
        "Diffusion models have emerged as powerful tools for solving inverse problems,\nyet prior work has primarily focused on observations with Gaussian measurement\nnoise, restricting their use in real-world scenarios. This limitation persists\ndue to the intractability of the likelihood score, which until now has only\nbeen approximated in the simpler case of Gaussian likelihoods. In this work, we\nextend diffusion models to handle inverse problems where the observations\nfollow a distribution from the exponential family, such as a Poisson or a\nBinomial distribution. By leveraging the conjugacy properties of exponential\nfamily distributions, we introduce the evidence trick, a method that provides a\ntractable approximation to the likelihood score. In our experiments, we\ndemonstrate that our methodology effectively performs Bayesian inference on\nspatially inhomogeneous Poisson processes with intensities as intricate as\nImageNet images. Furthermore, we demonstrate the real-world impact of our\nmethodology by showing that it performs competitively with the current\nstate-of-the-art in predicting malaria prevalence estimates in Sub-Saharan\nAfrica.",
        "W and W-based high-entropy alloys (HEAs) are promising candidates for\nplasma-facing materials in fusion reactors. While irradiation studies on W have\nrevealed a tendency for helium (He) bubble formation and radiation-induced\ndefects, investigations of WTaCrV HEA have demonstrated superior radiation\nresistance, whether under He+ irradiation or heavy ion irradiation. To assess\nmaterial performance under conditions relevant to fusion reactors -\ncharacterized by fast neutrons and gas production from transmutation reactions\n- complex irradiation environments need to be modeled. Using molecular dynamics\nsimulations, we examined defect evolution in W and equimolar WTaCrV HEA with\nand without preexisting He atoms under cascade overlap conditions up to 0.2 dpa\nat 300 K. In W, dislocation loops and large interstitial clusters formed\nreadily, with increasing He content leading to higher dislocation densities and\nthe formation of polygonal interstitial networks. In contrast, the WTaCrV alloy\nexhibited strong resistance to the formation of dislocation loops and large\ninterstitial clusters but was more susceptible to the formation of bubbles at\nhigher He concentrations. Bubble growth was driven by helium trapping at\nvacancy sites and the coalescence of smaller bubbles. Larger bubbles remained\nstable against cascade overlap, limiting further growth by coalescence.",
        "This paper presents an innovative adaptation of existing methodology to\ninvestigate external load in elite female soccer athletes using GPS-derived\nmovement data from 23 matches. We developed a quantitative framework to examine\nvelocity, acceleration, and movement angle across game halves, enabling\ntransparent and meaningful performance insights. By constructing a quantile\ncube to quantify movement patterns, we segmented athletes' movements into\ndistinct velocity, acceleration, and angle quantiles. Statistical analysis\nrevealed significant differences in movement distributions between match halves\nfor individual athletes. Principal Component Analysis (PCA) identified\nanomalous games with unique movement dynamics, particularly at the start and\nend of the season. Dirichlet-multinomial regression further explored how\nfactors like athlete position, playing time, and game characteristics\ninfluenced movement profiles. This approach provides a structured method for\nanalyzing movement dynamics, revealing external load variations over time and\noffering insights into performance optimization. The integration of these\nstatistical techniques demonstrates the potential of data-driven strategies to\nenhance athlete monitoring in soccer."
      ]
    }
  },
  {
    "id":2411.03522,
    "research_type":"applied",
    "start_id":"b20",
    "start_title":"Long non-coding RNAs: definitions, functions, challenges and recommendations",
    "start_abstract":"Genes specifying long non-coding RNAs (lncRNAs) occupy a large fraction of the genomes of complex organisms. The term \u2018lncRNAs\u2019 encompasses RNA polymerase I (Pol I), Pol II and Pol III transcribed RNAs, and RNAs from processed introns. The various functions of lncRNAs and their many isoforms and interleaved relationships with other genes make lncRNA classification and annotation difficult. Most lncRNAs evolve more rapidly than protein-coding sequences, are cell type specific and regulate many aspects of cell differentiation and development and other physiological processes. Many lncRNAs associate with chromatin-modifying complexes, are transcribed from enhancers and nucleate phase separation of nuclear condensates and domains, indicating an intimate link between lncRNA expression and the spatial control of gene expression during development. lncRNAs also have important roles in the cytoplasm and beyond, including in the regulation of translation, metabolism and signalling. lncRNAs often have a modular structure and are rich in repeats, which are increasingly being shown to be relevant to their function. In this Consensus Statement, we address the definition and nomenclature of lncRNAs and their conservation, expression, phenotypic visibility, structure and functions. We also discuss research challenges and provide recommendations to advance the understanding of the roles of lncRNAs in development, cell biology and disease.",
    "start_categories":[
      "q-bio.CB"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "Language Models are Few-Shot Learners"
      ],
      "abstract":[
        "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training a large corpus of text followed fine-tuning specific task. While typically task-agnostic in architecture, this method still requires task-specific datasets thousands or tens examples. By contrast, humans can generally perform new language task from only few examples simple instructions - something which current systems largely struggle to do. Here we show that scaling up models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art approaches. Specifically, train GPT-3, an autoregressive model 175 billion parameters, 10x more than any previous non-sparse model, test its performance the setting. For all tasks, GPT-3 is applied without gradient updates fine-tuning, demonstrations specified purely via interaction model. achieves strong datasets, including translation, question-answering, cloze as well several require on-the-fly reasoning domain adaptation, such unscrambling words, using novel word sentence, performing 3-digit arithmetic. At same time, also identify some where GPT-3's learning struggles, faces methodological issues related training web corpora. Finally, find generate samples news articles human evaluators have difficulty distinguishing written humans. We discuss broader societal impacts finding general."
      ],
      "categories":[
        "cs.CL"
      ]
    },
    "list":{
      "title":[
        "Adaptive Drift Compensation for Soft Sensorized Finger Using Continual\n  Learning",
        "Video-DPRP: A Differentially Private Approach for Visual\n  Privacy-Preserving Video Human Activity Recognition",
        "A Survey of Internet Censorship and its Measurement: Methodology,\n  Trends, and Challenges",
        "Unifying Perplexing Behaviors in Modified BP Attributions through\n  Alignment Perspective",
        "UL-UNAS: Ultra-Lightweight U-Nets for Real-Time Speech Enhancement via\n  Network Architecture Search",
        "DDAT: Diffusion Policies Enforcing Dynamically Admissible Robot\n  Trajectories",
        "Density-Functional Perturbation Theory with Numeric Atom-Centered\n  Orbitals",
        "Real-Time Streaming Telemetry Based Detection and Mitigation of OOK and\n  Power Interference in Multi-User OSaaS Networks",
        "Towards Interpretable Protein Structure Prediction with Sparse\n  Autoencoders",
        "Ab Initio theory of Electron-phonon-coupling-induced Giant Magnetic\n  Moments of Chiral Phonons in Magnetic Materials",
        "Terahertz Integrated Sensing and Communication-Empowered UAVs in 6G: A\n  Transceiver Design Perspective",
        "Study of Nucleon Charge-Exchange Processes at $^{12}$C Fragmentation\n  with an Energy of 300 MeV\/Nucleon",
        "Global Picard Spectra and Borel Parametrized Algebra",
        "Radar Pulse Deinterleaving with Transformer Based Deep Metric Learning",
        "Microscopic investigation of wobbling motion in even-even nuclei",
        "Improvement of Data Analytics Techniques in Reflection High Energy\n  Electron Diffraction to Enable Machine Learning",
        "Scalar behavior for a complex multi-soliton arising in blow-up for a\n  semilinear wave equation",
        "Multimodal MRI-Ultrasound AI for Prostate Cancer Detection Outperforms\n  Radiologist MRI Interpretation: A Multi-Center Study",
        "3D-grids are not transducible from planar graphs",
        "RIS-Aided Fluid Antenna Array-Mounted UAV Networks",
        "Behavioral Homophily in Social Media via Inverse Reinforcement Learning:\n  A Reddit Case Study",
        "Public Access Defibrillator Deployment for Cardiac Arrests: A\n  Learn-Then-Optimize Approach with SHAP-based Interpretable Analytics",
        "ProtoGS: Efficient and High-Quality Rendering with 3D Gaussian\n  Prototypes",
        "Potential Contribution of Young Pulsar Wind Nebulae to Galactic\n  High-Energy Neutrino Emission",
        "Estimating Parameters of Structural Models Using Neural Networks",
        "Big-Math: A Large-Scale, High-Quality Math Dataset for Reinforcement\n  Learning in Language Models",
        "HoloGest: Decoupled Diffusion and Motion Priors for Generating\n  Holisticly Expressive Co-speech Gestures",
        "Orthogonal Representation Learning for Estimating Causal Quantities",
        "AI-Powered Noisy Quantum Emulation: Generalized Gate-Based Protocols for\n  Hardware-Agnostic Simulation"
      ],
      "abstract":[
        "Strain sensors are gaining popularity in soft robotics for acquiring tactile\ndata due to their flexibility and ease of integration. Tactile sensing plays a\ncritical role in soft grippers, enabling them to safely interact with\nunstructured environments and precisely detect object properties. However, a\nsignificant challenge with these systems is their high non-linearity,\ntime-varying behavior, and long-term signal drift. In this paper, we introduce\na continual learning (CL) approach to model a soft finger equipped with\npiezoelectric-based strain sensors for proprioception. To tackle the\naforementioned challenges, we propose an adaptive CL algorithm that integrates\na Long Short-Term Memory (LSTM) network with a memory buffer for rehearsal and\nincludes a regularization term to keep the model's decision boundary close to\nthe base signal while adapting to time-varying drift. We conduct nine different\nexperiments, resetting the entire setup each time to demonstrate signal drift.\nWe also benchmark our algorithm against two other methods and conduct an\nablation study to assess the impact of different components on the overall\nperformance.",
        "Considerable effort has been made in privacy-preserving video human activity\nrecognition (HAR). Two primary approaches to ensure privacy preservation in\nVideo HAR are differential privacy (DP) and visual privacy. Techniques\nenforcing DP during training provide strong theoretical privacy guarantees but\noffer limited capabilities for visual privacy assessment. Conversely methods,\nsuch as low-resolution transformations, data obfuscation and adversarial\nnetworks, emphasize visual privacy but lack clear theoretical privacy\nassurances. In this work, we focus on two main objectives: (1) leveraging DP\nproperties to develop a model-free approach for visual privacy in videos and\n(2) evaluating our proposed technique using both differential privacy and\nvisual privacy assessments on HAR tasks. To achieve goal (1), we introduce\nVideo-DPRP: a Video-sample-wise Differentially Private Random Projection\nframework for privacy-preserved video reconstruction for HAR. By using random\nprojections, noise matrices and right singular vectors derived from the\nsingular value decomposition of videos, Video-DPRP reconstructs DP videos using\nprivacy parameters ($\\epsilon,\\delta$) while enabling visual privacy\nassessment. For goal (2), using UCF101 and HMDB51 datasets, we compare\nVideo-DPRP's performance on activity recognition with traditional DP methods,\nand state-of-the-art (SOTA) visual privacy-preserving techniques. Additionally,\nwe assess its effectiveness in preserving privacy-related attributes such as\nfacial features, gender, and skin color, using the PA-HMDB and VISPR datasets.\nVideo-DPRP combines privacy-preservation from both a DP and visual privacy\nperspective unlike SOTA methods that typically address only one of these\naspects.",
        "Internet censorship limits the access of nodes residing within a specific\nnetwork environment to the public Internet, and vice versa. During the last\ndecade, techniques for conducting Internet censorship have been developed\nfurther. Consequently, methodology for measuring Internet censorship had been\nimproved as well. In this paper, we firstly provide a survey of Internet\ncensorship techniques. Secondly, we survey censorship measurement methodology,\nincluding a coverage of available datasets. In cases where it is beneficial, we\nbridge the terminology and taxonomy of Internet censorship with related\ndomains, namely traffic obfuscation and information hiding. We cover both,\ntechnical and human aspects, as well as recent trends, and challenges.",
        "Attributions aim to identify input pixels that are relevant to the\ndecision-making process. A popular approach involves using modified\nbackpropagation (BP) rules to reverse decisions, which improves\ninterpretability compared to the original gradients. However, these methods\nlack a solid theoretical foundation and exhibit perplexing behaviors, such as\nreduced sensitivity to parameter randomization, raising concerns about their\nreliability and highlighting the need for theoretical justification. In this\nwork, we present a unified theoretical framework for methods like GBP,\nRectGrad, LRP, and DTD, demonstrating that they achieve input alignment by\ncombining the weights of activated neurons. This alignment improves the\nvisualization quality and reduces sensitivity to weight randomization. Our\ncontributions include: (1) Providing a unified explanation for multiple\nbehaviors, rather than focusing on just one. (2) Accurately predicting novel\nbehaviors. (3) Offering insights into decision-making processes, including\nlayer-wise information changes and the relationship between attributions and\nmodel decisions.",
        "Lightweight models are essential for real-time speech enhancement\napplications. In recent years, there has been a growing trend toward developing\nincreasingly compact models for speech enhancement. In this paper, we propose\nan Ultra-Lightweight U-net optimized by Network Architecture Search (UL-UNAS),\nwhich is suitable for implementation in low-footprint devices. Firstly, we\nexplore the application of various efficient convolutional blocks within the\nU-Net framework to identify the most promising candidates. Secondly, we\nintroduce two boosting components to enhance the capacity of these\nconvolutional blocks: a novel activation function named affine PReLU and a\ncausal time-frequency attention module. Furthermore, we leverage neural\narchitecture search to discover an optimal architecture within our carefully\ndesigned search space. By integrating the above strategies, UL-UNAS not only\nsignificantly outperforms the latest ultra-lightweight models with the same or\nlower computational complexity, but also delivers competitive performance\ncompared to recent baseline models that require substantially higher\ncomputational resources.",
        "Diffusion models excel at creating images and videos thanks to their\nmultimodal generative capabilities. These same capabilities have made diffusion\nmodels increasingly popular in robotics research, where they are used for\ngenerating robot motion. However, the stochastic nature of diffusion models is\nfundamentally at odds with the precise dynamical equations describing the\nfeasible motion of robots. Hence, generating dynamically admissible robot\ntrajectories is a challenge for diffusion models. To alleviate this issue, we\nintroduce DDAT: Diffusion policies for Dynamically Admissible Trajectories to\ngenerate provably admissible trajectories of black-box robotic systems using\ndiffusion models. A sequence of states is a dynamically admissible trajectory\nif each state of the sequence belongs to the reachable set of its predecessor\nby the robot's equations of motion. To generate such trajectories, our\ndiffusion policies project their predictions onto a dynamically admissible\nmanifold during both training and inference to align the objective of the\ndenoiser neural network with the dynamical admissibility constraint. The\nauto-regressive nature of these projections along with the black-box nature of\nrobot dynamics render these projections immensely challenging. We thus enforce\nadmissibility by iteratively sampling a polytopic under-approximation of the\nreachable set of a state onto which we project its predicted successor, before\niterating this process with the projected successor. By producing accurate\ntrajectories, this projection eliminates the need for diffusion models to\ncontinually replan, enabling one-shot long-horizon trajectory planning. We\ndemonstrate that our framework generates higher quality dynamically admissible\nrobot trajectories through extensive simulations on a quadcopter and various\nMuJoCo environments, along with real-world experiments on a Unitree GO1 and\nGO2.",
        "This paper represents one contribution to a larger Roadmap article reviewing\nthe current status of the FHI-aims code. In this contribution, the\nimplementation of density-functional perturbation theory in a numerical\natom-centered framework is summarized. Guidelines on usage and links to\ntutorials are provided.",
        "We present a framework to identify and mitigate rogue OOK signals and\nuser-generated power interference in a multi-user Optical-Spectrum-as-a-Service\nnetwork. Experimental tests on the OpenIreland-testbed achieve up to 89%\ndetection rate within 10 seconds of an interference event.",
        "Protein language models have revolutionized structure prediction, but their\nnonlinear nature obscures how sequence representations inform structure\nprediction. While sparse autoencoders (SAEs) offer a path to interpretability\nhere by learning linear representations in high-dimensional space, their\napplication has been limited to smaller protein language models unable to\nperform structure prediction. In this work, we make two key advances: (1) we\nscale SAEs to ESM2-3B, the base model for ESMFold, enabling mechanistic\ninterpretability of protein structure prediction for the first time, and (2) we\nadapt Matryoshka SAEs for protein language models, which learn hierarchically\norganized features by forcing nested groups of latents to reconstruct inputs\nindependently. We demonstrate that our Matryoshka SAEs achieve comparable or\nbetter performance than standard architectures. Through comprehensive\nevaluations, we show that SAEs trained on ESM2-3B significantly outperform\nthose trained on smaller models for both biological concept discovery and\ncontact map prediction. Finally, we present an initial case study demonstrating\nhow our approach enables targeted steering of ESMFold predictions, increasing\nstructure solvent accessibility while fixing the input sequence. To facilitate\nfurther investigation by the broader community, we open-source our code,\ndataset, pretrained models https:\/\/github.com\/johnyang101\/reticular-sae , and\nvisualizer https:\/\/sae.reticular.ai .",
        "Chiral phonons, characterized by nonzero angular momenta and magnetic\nmoments, have attracted extensive attention. However, a long-standing critical\nissue in this field is the lack of ab initio methods to accurately calculate\nphonon magnetic moments resulting from electron-phonon coupling (EPC). Here, we\nresolve this challenge by developing an ab initio theory for calculating\nEPC-induced phonon magnetic properties, applicable to both insulating and\nmetallic materials. Based on this theory, we demonstrate EPC-induced giant\nphonon magnetic moments and resulting phonon Zeeman splittings in magnetic\nmetals, which are orders of magnitude larger than classical predictions from\npoint-charge models. Interestingly, these splittings could open observable\ntopological gaps in phonon spectra of magnets, generating intrinsic phonon\nChern states. Through first-principles calculations, we first propose candidate\nmaterials with such intrinsic phonon Chern states hosting robust edge phonon\ncurrents which may be applied to detecting neutral particles such as dark\nmatter particles. Our work not only establishes a theoretical foundation for\nEPC-induced phonon magnetic properties, but also enables the ab initio\ncalculation of long-sought TRS-breaking phonon spectra throughout the Brillouin\nzone in realistic materials.",
        "Due to their high maneuverability, flexible deployment, and low cost,\nunmanned aerial vehicles (UAVs) are expected to play a pivotal role in not only\ncommunication, but also sensing. Especially by exploiting the ultra-wide\nbandwidth of terahertz (THz) bands, integrated sensing and communication\n(ISAC)-empowered UAV has been a promising technology of 6G space-air-ground\nintegrated networks. In this article, we systematically investigate the key\ntechniques and essential obstacles for THz-ISAC-empowered UAV from a\ntransceiver design perspective, with the highlight of its major challenges and\nkey technologies. Specifically, we discuss the THz-ISAC-UAV wireless\npropagation environment, based on which several channel characteristics for\ncommunication and sensing are revealed. We point out the transceiver payload\ndesign peculiarities for THz-ISAC-UAV from the perspective of antenna design,\nradio frequency front-end, and baseband signal processing. To deal with the\nspecificities faced by the payload, we shed light on three key technologies,\ni.e., hybrid beamforming for ultra-massive MIMO-ISAC, power-efficient THz-ISAC\nwaveform design, as well as communication and sensing channel state information\nacquisition, and extensively elaborate their concepts and key issues. More\nimportantly, future research directions and associated open problems are\npresented, which may unleash the full potential of THz-ISAC-UAV for 6G wireless\nnetworks.",
        "The search of reactions with nucleon charge-exchange was performed on the\nFRAGM fragment-separator of the TWAC accelerator complex at fragmentation of\ncarbon nuclei with an energy of 300 MeV\/nucleon on a thin beryllium target. The\nexperimental setup, located at an angle of 3.5 degrees to the incident beam,\nhad high momentum resolution. Differential cross sections were measured for\n$^{11}$Be, $^{12}$B and $^{12}$Be as function of the nuclei momentum. The\nexperimental data were compared with theoretical predictions of various models\nof nucleus-nucleus interactions and other experimental results. Measurements of\nnucleon charge exchange processes in this energy region was carried out for the\nfirst time. New results were obtained to test theoretical models of\nnucleus-nucleus interactions.",
        "We answer a question of Schwede on the existence of global Picard spectra\nassociated to his ultra-commutative global ring spectra; given an\nultra-commutative global ring spectrum $R$, we show there exists a global\nspectrum $\\mathrm{pic}_\\mathrm{eq}(R)$ assembling the Picard spectra of all\nunderlying $G$-equivariant ring spectra $\\mathrm{res}_G R$ of $R$ into one\nobject, in that for all finite groups $G$, the genuine fixed points are given\nby $\\mathrm{pic}_\\mathrm{eq}(R)^G \\simeq\n\\mathrm{pic}(\\mathrm{Mod}_{\\mathrm{res}_G R}(\\mathrm{Sp}_G))$.\n  Along the way, we develop a generalization of Borel-equivariant objects in\nthe setting of parametrized higher algebra. We use this to assemble the\nsymmetric monoidal categories of $G$-spectra for all finite groups $G$ together\nwith all restrictions and norms into a single `normed global category', and\nbuild a comparison functor which allows us to import ultra-commutative\n$G$-equivariant or global ring spectra into the setting of parametrized higher\nalgebra.",
        "When receiving radar pulses it is common for a recorded pulse train to\ncontain pulses from many different emitters. The radar pulse deinterleaving\nproblem is the task of separating out these pulses by the emitter from which\nthey originated. Notably, the number of emitters in any particular recorded\npulse train is considered unknown. In this paper, we define the problem and\npresent metrics that can be used to measure model performance. We propose a\nmetric learning approach to this problem using a transformer trained with the\ntriplet loss on synthetic data. This model achieves strong results in\ncomparison with other deep learning models with an adjusted mutual information\nscore of 0.882.",
        "The possibility of observing wobbling mode in the even-even systems of 76Ge,\n112Ru, 188,192Os, 192Pt and 232Th is explored using the triaxial projected\nshell model approach. These nuclei are known to have {\\gamma}-bands whose\nodd-spin members are lower than the average of the neighbouring even-spin\nstates. It is shown through a detailed analysis of the excitation energies and\nthe electromagnetic transition probabilities that the observed band structures\nin these nuclei except for 232Th can be characterised as originating from the\nwobbling motion. It is further demonstrated that quasiparticle alignment is\nresponsible for driving the systems to the wobbling mode.",
        "Perovskite oxides such as LaFeO$_3$ are a well-studied family of materials\nthat possess a wide range of useful and novel properties. Successfully\nsynthesizing perovskite oxide samples usually requires a significant number of\ngrowth attempts and a detailed film characterization on each sample to find the\noptimal growth window of a material. The most common real-time \\textit{in situ}\ndiagnostic technique available during molecular beam epitaxy (MBE) synthesis is\nreflection high-energy electron diffraction (RHEED). Conventional use of RHEED\nallows a highly experienced operator to determine growth rate by monitoring\nintensity osciallations and make some qualitative observations during growth,\nsuch as recognizing the sample has become amorphous or recognizing that large\nislands have formed on the surface. However, due to a lack of theoretical\nunderstanding of the diffraction patterns, finer, more precise levels of\nobservations are challenging. To address these limitations, we implement new\ndata analytics techniques in the growth of three LaFeO$_3$ samples on Nb-doped\nSrTiO$_3$ by MBE. These techniques improve our ability to perform unsupervised\nmachine learning using principal component analysis (PCA) and k-means\nclustering by using drift correction to overcome sample or stage motion during\ngrowth and intensity transformations that highlight more subtle features in the\nimages such as Kikuchi bands. With this approach, we enable the first\ndemonstration of PCA and k-means across multiple samples, allowing for\nquantitative comparison of RHEED videos for two LaFeO$_3$ film samples. These\ncapabilities set the stage for real-time processing of RHEED data during growth\nto enable machine learning-accelerated film synthesis.",
        "This paper deals with blow-up for the complex-valued semilinear wave equation\nwith power nonlinearity in dimension 1. Up to a rotation of the solution in the\ncomplex plane, we show that near a characteristic blow-up point, the solution\nbehaves exactly as in the real-valued case. Namely, up to a rotation in the\ncomplex plane, the solution decomposes into a sum of a finite number of\ndecoupled solitons with alternate signs. The main novelty of our proof is a\nresolution of a complex-valued first order Toda system governing the evolution\nof the positions and the phases of the solitons.",
        "Pre-biopsy magnetic resonance imaging (MRI) is increasingly used to target\nsuspicious prostate lesions. This has led to artificial intelligence (AI)\napplications improving MRI-based detection of clinically significant prostate\ncancer (CsPCa). However, MRI-detected lesions must still be mapped to\ntransrectal ultrasound (TRUS) images during biopsy, which results in missing\nCsPCa. This study systematically evaluates a multimodal AI framework\nintegrating MRI and TRUS image sequences to enhance CsPCa identification. The\nstudy included 3110 patients from three cohorts across two institutions who\nunderwent prostate biopsy. The proposed framework, based on the 3D UNet\narchitecture, was evaluated on 1700 test cases, comparing performance to\nunimodal AI models that use either MRI or TRUS alone. Additionally, the\nproposed model was compared to radiologists in a cohort of 110 patients. The\nmultimodal AI approach achieved superior sensitivity (80%) and Lesion Dice\n(42%) compared to unimodal MRI (73%, 30%) and TRUS models (49%, 27%). Compared\nto radiologists, the multimodal model showed higher specificity (88% vs. 78%)\nand Lesion Dice (38% vs. 33%), with equivalent sensitivity (79%). Our findings\ndemonstrate the potential of multimodal AI to improve CsPCa lesion targeting\nduring biopsy and treatment planning, surpassing current unimodal models and\nradiologists; ultimately improving outcomes for prostate cancer patients.",
        "We prove that the class of 3D-grids is cannot be transduced from planar\ngraphs, and more generally, from any class of graphs of bounded Euler genus. To\nprove our result, we introduce a new structural tool called slice\ndecompositions, and show that every graph class transducible from a class of\ngraphs of bounded Euler genus is a perturbation of a graph class that admits\nslice decompositions.",
        "This paper investigates reconfigurable intelligent surface (RIS)-assisted\nunmanned aerial vehicle (UAV) downlink networks with fluid antennas (FA), where\nRIS enables non-line-of-sight (NLoS) transmissions. Moreover, the FA is\nequipped on the UAV offering dynamic antenna position adjustment, enhancing\nspatial diversity besides UAV deployment. We aim at total downlink rate\nmaximization while ensuring minimum user rate requirement. We consider joint\noptimization of active UAV beamforming, passive RIS beamforming, UAV deployment\nand FA position adjustment. To address the complex problem, we propose\nbeamfomring for RIS\/UAV and FA-UAV deployment (BRAUD) scheme by employing\nalternative optimization, successive convex approximation (SCA) and sequential\nrank-one constraint relaxation (SROCR) method for the decomposed subproblems.\nSimulation results demonstrate the effectiveness of RIS-FA-UAV, achieving the\nhighest rate among existing architectures without FA\/UAV\/RIS deployment and\nwithout proper beamforming. Moreover, BRAUD achieves the highest rate among\nbenchmarks of drop-rank method, heuristic optimizations and conventional\nzero-forcing beamforming as well as random method.",
        "Online communities play a critical role in shaping societal discourse and\ninfluencing collective behavior in the real world. The tendency for people to\nconnect with others who share similar characteristics and views, known as\nhomophily, plays a key role in the formation of echo chambers which further\namplify polarization and division. Existing works examining homophily in online\ncommunities traditionally infer it using content- or adjacency-based\napproaches, such as constructing explicit interaction networks or performing\ntopic analysis. These methods fall short for platforms where interaction\nnetworks cannot be easily constructed and fail to capture the complex nature of\nuser interactions across the platform. This work introduces a novel approach\nfor quantifying user homophily. We first use an Inverse Reinforcement Learning\n(IRL) framework to infer users' policies, then use these policies as a measure\nof behavioral homophily. We apply our method to Reddit, conducting a case study\nacross 5.9 million interactions over six years, demonstrating how this approach\nuncovers distinct behavioral patterns and user roles that vary across different\ncommunities. We further validate our behavioral homophily measure against\ntraditional content-based homophily, offering a powerful method for analyzing\nsocial media dynamics and their broader societal implications. We find, among\nothers, that users can behave very similarly (high behavioral homophily) when\ndiscussing entirely different topics like soccer vs e-sports (low topical\nhomophily), and that there is an entire class of users on Reddit whose purpose\nseems to be to disagree with others.",
        "Out-of-hospital cardiac arrest (OHCA) survival rates remain extremely low due\nto challenges in the timely accessibility of medical devices. Therefore,\neffective deployment of automated external defibrillators (AED) can\nsignificantly increase survival rates. Precise and interpretable predictions of\nOHCA occurrences provide a solid foundation for efficient and robust AED\ndeployment optimization. This study develops a novel learn-then-optimize\napproach, integrating three key components: a machine learning prediction\nmodel, SHAP-based interpretable analytics, and a SHAP-guided integer\nprogramming (SIP) model. The machine learning model is trained utilizing only\ngeographic data as inputs to overcome data availability obstacles, and its\nstrong predictive performance validates the feasibility of interpretation.\nFurthermore, the SHAP model elaborates on the contribution of each geographic\nfeature to the OHCA occurrences. Finally, an integer programming model is\nformulated for optimizing AED deployment, incorporating SHAP-weighted OHCA\ndensities. Various numerical experiments are conducted across different\nsettings. Based on comparative and sensitive analysis, the optimization effect\nof our approach is verified and valuable insights are derived to provide\nsubstantial support for theoretical extension and practical implementation.",
        "3D Gaussian Splatting (3DGS) has made significant strides in novel view\nsynthesis but is limited by the substantial number of Gaussian primitives\nrequired, posing challenges for deployment on lightweight devices. Recent\nmethods address this issue by compressing the storage size of densified\nGaussians, yet fail to preserve rendering quality and efficiency. To overcome\nthese limitations, we propose ProtoGS to learn Gaussian prototypes to represent\nGaussian primitives, significantly reducing the total Gaussian amount without\nsacrificing visual quality. Our method directly uses Gaussian prototypes to\nenable efficient rendering and leverage the resulting reconstruction loss to\nguide prototype learning. To further optimize memory efficiency during\ntraining, we incorporate structure-from-motion (SfM) points as anchor points to\ngroup Gaussian primitives. Gaussian prototypes are derived within each group by\nclustering of K-means, and both the anchor points and the prototypes are\noptimized jointly. Our experiments on real-world and synthetic datasets prove\nthat we outperform existing methods, achieving a substantial reduction in the\nnumber of Gaussians, and enabling high rendering speed while maintaining or\neven enhancing rendering fidelity.",
        "Pulsar wind nebulae (PWNe), especially the young ones, are among the most\nenergetic astrophysical sources in the Galaxy. It is usually believed that the\nspin-down energy injected from the pulsars is converted into magnetic field and\nrelativistic electrons, but the possible presence of proton acceleration inside\nPWNe cannot be ruled out. Previous works have estimated the neutrino emission\nfrom PWNe using various source catalogs measured in gamma-rays. However, such\nresults rely on the sensitivity of TeV gamma-ray observations and may omit the\ncontribution by unresolved sources. Here we estimate the potential neutrino\nemission from a synthetic population of PWNe in the Galaxy with a focus on the\nones that are still in the free expansion phase. In the calculation, we model\nthe temporal evolution of the free-expanding PWNe and consider the transport of\nprotons inside the PWNe. The Crab nebula is treated as a standard template for\nyoung PWNe to evaluate some model parameters, such as the energy conversion\nfraction of relativistic protons and the target gas density for the hadronic\nprocess, which are relevant to neutrino production. In the optimistic case, the\nneutrino flux from the simulated young PWNe may constitute to 5% of the\nmeasured flux by IceCube around 100 TeV. At higher energy around 1 PeV, the\nneutrino emission from the population highly depends on the injection spectral\nshape, and also on the emission of the nearby prominent sources.",
        "We study an alternative use of machine learning. We train neural nets to\nprovide the parameter estimate of a given (structural) econometric model, for\nexample, discrete choice or consumer search. Training examples consist of\ndatasets generated by the econometric model under a range of parameter values.\nThe neural net takes the moments of a dataset as input and tries to recognize\nthe parameter value underlying that dataset. Besides the point estimate, the\nneural net can also output statistical accuracy. This neural net estimator\n(NNE) tends to limited-information Bayesian posterior as the number of training\ndatasets increases. We apply NNE to a consumer search model. It gives more\naccurate estimates at lighter computational costs than the prevailing approach.\nNNE is also robust to redundant moment inputs. In general, NNE offers the most\nbenefits in applications where other estimation approaches require very heavy\nsimulation costs. We provide code at: https:\/\/nnehome.github.io.",
        "Increasing interest in reasoning models has led math to become a prominent\ntesting ground for algorithmic and methodological improvements. However,\nexisting open math datasets either contain a small collection of high-quality,\nhuman-written problems or a large corpus of machine-generated problems of\nuncertain quality, forcing researchers to choose between quality and quantity.\nIn this work, we present Big-Math, a dataset of over 250,000 high-quality math\nquestions with verifiable answers, purposefully made for reinforcement learning\n(RL). To create Big-Math, we rigorously filter, clean, and curate openly\navailable datasets, extracting questions that satisfy our three desiderata: (1)\nproblems with uniquely verifiable solutions, (2) problems that are open-ended,\n(3) and problems with a closed-form solution. To ensure the quality of\nBig-Math, we manually verify each step in our filtering process. Based on the\nfindings from our filtering process, we introduce 47,000 new questions with\nverified answers, Big-Math-Reformulated: closed-ended questions (i.e. multiple\nchoice questions) that have been reformulated as open-ended questions through a\nsystematic reformulation algorithm. Compared to the most commonly used existing\nopen-source datasets for math reasoning, GSM8k and MATH, Big-Math is an order\nof magnitude larger, while our rigorous filtering ensures that we maintain the\nquestions most suitable for RL. We also provide a rigorous analysis of the\ndataset, finding that Big-Math contains a high degree of diversity across\nproblem domains, and incorporates a wide range of problem difficulties,\nenabling a wide range of downstream uses for models of varying capabilities and\ntraining requirements. By bridging the gap between data quality and quantity,\nBig-Math establish a robust foundation for advancing reasoning in LLMs.",
        "Animating virtual characters with holistic co-speech gestures is a\nchallenging but critical task. Previous systems have primarily focused on the\nweak correlation between audio and gestures, leading to physically unnatural\noutcomes that degrade the user experience. To address this problem, we\nintroduce HoleGest, a novel neural network framework based on decoupled\ndiffusion and motion priors for the automatic generation of high-quality,\nexpressive co-speech gestures. Our system leverages large-scale human motion\ndatasets to learn a robust prior with low audio dependency and high motion\nreliance, enabling stable global motion and detailed finger movements. To\nimprove the generation efficiency of diffusion-based models, we integrate\nimplicit joint constraints with explicit geometric and conditional constraints,\ncapturing complex motion distributions between large strides. This integration\nsignificantly enhances generation speed while maintaining high-quality motion.\nFurthermore, we design a shared embedding space for gesture-transcription text\nalignment, enabling the generation of semantically correct gesture actions.\nExtensive experiments and user feedback demonstrate the effectiveness and\npotential applications of our model, with our method achieving a level of\nrealism close to the ground truth, providing an immersive user experience. Our\ncode, model, and demo are are available at\nhttps:\/\/cyk990422.github.io\/HoloGest.github.io\/.",
        "Representation learning is widely used for estimating causal quantities\n(e.g., the conditional average treatment effect) from observational data. While\nexisting representation learning methods have the benefit of allowing for\nend-to-end learning, they do not have favorable theoretical properties of\nNeyman-orthogonal learners, such as double robustness and quasi-oracle\nefficiency. Also, such representation learning methods often employ additional\nconstraints, like balancing, which may even lead to inconsistent estimation. In\nthis paper, we propose a novel class of Neyman-orthogonal learners for causal\nquantities defined at the representation level, which we call OR-learners. Our\nOR-learners have several practical advantages: they allow for consistent\nestimation of causal quantities based on any learned representation, while\noffering favorable theoretical properties including double robustness and\nquasi-oracle efficiency. In multiple experiments, we show that, under certain\nregularity conditions, our OR-learners improve existing representation learning\nmethods and achieve state-of-the-art performance. To the best of our knowledge,\nour OR-learners are the first work to offer a unified framework of\nrepresentation learning methods and Neyman-orthogonal learners for causal\nquantities estimation.",
        "Quantum computer emulators model the behavior and error rates of specific\nquantum processors. Without accurate noise models in these emulators, it is\nchallenging for users to optimize and debug executable quantum programs prior\nto running them on the quantum device, as device-specific noise is not properly\naccounted for. To overcome this challenge, we introduce a general protocol to\napproximate device-specific emulators without requiring pulse-level control. By\napplying machine learning to data obtained from gate set tomography, we\nconstruct a device-specific emulator by predicting the noise model input\nparameters that best match the target device. We demonstrate the effectiveness\nof our protocol's emulator in estimating the unitary coupled cluster energy of\nthe H$_2$ molecule and compare the results with those from actual quantum\nhardware. Remarkably, our noise model captures device noise with high accuracy,\nachieving a mean absolute difference of just 0.3\\% in expectation value\nrelative to the state-vector simulation."
      ]
    }
  },
  {
    "id":2411.17702,
    "research_type":"basic",
    "start_id":"b5",
    "start_title":"Patient Contrastive Learning: a Performant, Expressive, and Practical Approach to ECG Modeling.",
    "start_abstract":"Supervised machine learning applications in health care are often limited due to a scarcity of labeled training data. To mitigate this effect small sample size, we introduce pre-training approach, Patient Contrastive Learning Representations (PCLR), which creates latent representations ECGs from large number unlabeled examples. The resulting expressive, performant, and practical across wide spectrum clinical tasks. We develop PCLR using system with over 3.2 million 12-lead ECGs, demonstrate substantial improvements multiple new tasks when there fewer than 5,000 labels.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b2"
      ],
      "title":[
        "Screening for cardiac contractile dysfunction using an artificial intelligence-enabled electrocardiogram"
      ],
      "abstract":[
        "Asymptomatic left ventricular dysfunction (ALVD) is present in 3-6% of the general population, is associated with reduced quality of life and longevity, and is treatable when found1-4. An inexpensive, noninvasive screening tool for ALVD in the doctor's office is not available. We tested the hypothesis that application of artificial intelligence (AI) to the electrocardiogram (ECG), a routine method of measuring the heart's electrical activity, could identify ALVD. Using paired 12-lead ECG and echocardiogram data, including the left ventricular ejection fraction (a measure of contractile function), from 44,959 patients at the Mayo Clinic, we trained a convolutional neural network to identify patients with ventricular dysfunction, defined as ejection fraction \u226435%, using the ECG data alone. When tested on an independent set of 52,870 patients, the network model yielded values for the area under the curve, sensitivity, specificity, and accuracy of 0.93, 86.3%, 85.7%, and 85.7%, respectively. In patients without ventricular dysfunction, those with a positive AI screen were at 4 times the risk (hazard ratio, 4.1; 95% confidence interval, 3.3 to 5.0) of developing future ventricular dysfunction compared with those with a negative screen. Application of AI to the ECG-a ubiquitous, low-cost test-permits the ECG to serve as a powerful screening tool in asymptomatic individuals to identify ALVD."
      ],
      "categories":[
        "Cardio"
      ]
    },
    "list":{
      "title":[
        "On singular problems associated with mixed operators under mixed\n  boundary conditions",
        "Effects of altermagnetic order, strain and doping on the optical and\n  vibrational properties of RuO$_2$",
        "Rethinking the Bias of Foundation Model under Long-tailed Distribution",
        "Depth-Bounds for Neural Networks via the Braid Arrangement",
        "Ducci Matrices in $p$-adic Context",
        "Einstein's Cat -- A Thought Experiment Against Anti-Relativist Claims",
        "Involutions of spherical 3-manifolds",
        "Book I of Euclid's Elements and application of areas",
        "Maximal $L_p$-regularity for fractional problem driven by non-autonomous\n  forms",
        "Logarithmic Non-Abelian Hodge Theory for curves in prime characteristic",
        "Nonlinear Einstein-Power-Yang-Mills AdS Black Holes: From Quantum\n  Tunneling to Aschenbach Effect",
        "Observation of the Dirac Dispersions in Co-doped CaFe2As2",
        "Anticoncentration in Clifford Circuits and Beyond: From Random Tensor\n  Networks to Pseudo-Magic States",
        "Phases and critical transport of the SU(N) Hofstadter-Hubbard model on\n  the triangular lattice",
        "Nonrelativistic spin-splitting multiferroic antiferromagnet and\n  compensated ferrimagnet with zero net magnetization",
        "IQPopt: Fast optimization of instantaneous quantum polynomial circuits\n  in JAX",
        "Physics-informed neural networks for phase-resolved data assimilation\n  and prediction of nonlinear ocean waves",
        "Intervals in Dyck paths and the wreath conjecture",
        "Quantifying Quantumness in (A)dS spacetimes with Unruh-DeWitt Detector",
        "The two filter formula reconsidered: Smoothing in partially observed\n  Gauss--Markov models without information parametrization",
        "Dynamic Structures of Knowledge Production: Citation Rates in Hydrogen\n  Technologies",
        "Spin nematic order and superconductivity in $J_1$-$J_2$ Kondo lattice\n  model on square lattice",
        "Sabotage and Free Riding in Contests with a Group-Specific\n  Public-Good\/Bad Prize",
        "Space-Dependent Fractional Evolution Equations: A New Approach",
        "Leveraging LLMS for Top-Down Sector Allocation In Automated Trading",
        "Assessing the value of advanced computing infrastructure for supporting\n  research: new tools to inform research policy",
        "Revisiting the outer-weakly convex domination number in graph products",
        "Enhanced Electromechanical Properties of Solution-Processed\n  K$_{0.5}$Na$_{0.5}$NbO$_{3}$ Thin Films",
        "Premixed flame quenching distance between cold walls: effects of flow\n  and Lewis number"
      ],
      "abstract":[
        "In this paper, we study the following singular problem associated with mixed\noperators (the combination of the classical Laplace operator and the fractional\nLaplace operator) under mixed boundary conditions \\begin{equation*} \\label{1}\n  \\left\\{\n  \\begin{aligned}\n  \\mathcal{L}u &= g(u), \\quad u > 0 \\quad \\text{in} \\quad \\Omega,\n  u &= 0 \\quad \\text{in} \\quad U^c,\n  \\mathcal{N}_s(u) &= 0 \\quad \\text{in} \\quad \\mathcal{N},\n  \\frac{\\partial u}{\\partial \\nu} &= 0 \\quad \\text{in} \\quad \\partial \\Omega\n\\cap \\overline{\\mathcal{N}},\n  \\end{aligned}\n  \\right.\n  \\tag{$P_\\lambda$} \\end{equation*}\n  where $U= (\\Omega \\cup {\\mathcal{N}} \\cup\n(\\partial\\Omega\\cap\\overline{\\mathcal{N}}))$, $\\Omega \\subseteq \\mathbb{R}^N$\nis a non empty open set, $\\mathcal{D}$, $\\mathcal{N}$ are open subsets of\n$\\mathbb{R}^N\\setminus{\\bar{\\Omega }}$ such that ${\\mathcal{D}} \\cup\n{\\mathcal{N}}= \\mathbb{R}^N\\setminus{\\bar{\\Omega}}$, $\\mathcal{D} \\cap\n{\\mathcal{N}}= \\emptyset $ and $\\Omega\\cup \\mathcal{N}$ is a bounded set with\nsmooth boundary, $\\lambda >0$ is a real parameter and\n  $\\mathcal{L}= -\\Delta+(-\\Delta)^{s},~ \\text{for}~s \\in (0, 1).$\n  Here $g(u)=u^{-q}$ or $g(u)= \\lambda u^{-q}+ u^p$ with $0<q<1<p\\leq 2^*-1$.\nWe study $(P_\\lambda)$ to derive the existence of weak solutions along with its\n$L^\\infty$-regularity. Moreover, some Sobolev-type variational inequalities\nassociated with these weak solutions are established.",
        "RuO$_2$, one of the most widely studied transition metal oxides, was recently\npredicted to host a novel form of collinear magnetic order referred to as\naltermagnetism. In this study we combine experiment (reflectance,\ntransmittance, ellipsometry and Raman measurements) and first-principles\ncalculations to elucidate the potential role of altermagnetic order, strain and\ndoping on the optical and vibrational properties of RuO$_2$ grown on TiO$_2$\n(001), (101) and (110) substrates. The combination of experiment and theory in\nthis study surprisingly indicates RuO$_2$ is in fact best described if one\nassumes the nonmagnetic state. Calculations of the altermagnetic state leads to\npoor agreement with the measured optical and vibrational properties of RuO$_2$.",
        "Long-tailed learning has garnered increasing attention due to its practical\nsignificance. Among the various approaches, the fine-tuning paradigm has gained\nconsiderable interest with the advent of foundation models. However, most\nexisting methods primarily focus on leveraging knowledge from these models,\noverlooking the inherent biases introduced by the imbalanced training data they\nrely on. In this paper, we examine how such imbalances from pre-training affect\nlong-tailed downstream tasks. Specifically, we find the imbalance biases\ninherited in foundation models on downstream task as parameter imbalance and\ndata imbalance. During fine-tuning, we observe that parameter imbalance plays a\nmore critical role, while data imbalance can be mitigated using existing\nre-balancing strategies. Moreover, we find that parameter imbalance cannot be\neffectively addressed by current re-balancing techniques, such as adjusting the\nlogits, during training, unlike data imbalance. To tackle both imbalances\nsimultaneously, we build our method on causal learning and view the incomplete\nsemantic factor as the confounder, which brings spurious correlations between\ninput samples and labels. To resolve the negative effects of this, we propose a\nnovel backdoor adjustment method that learns the true causal effect between\ninput samples and labels, rather than merely fitting the correlations in the\ndata. Notably, we achieve an average performance increase of about $1.67\\%$ on\neach dataset.",
        "We contribute towards resolving the open question of how many hidden layers\nare required in ReLU networks for exactly representing all continuous and\npiecewise linear functions on $\\mathbb{R}^d$. While the question has been\nresolved in special cases, the best known lower bound in general is still 2. We\nfocus on neural networks that are compatible with certain polyhedral complexes,\nmore precisely with the braid fan. For such neural networks, we prove a\nnon-constant lower bound of $\\Omega(\\log\\log d)$ hidden layers required to\nexactly represent the maximum of $d$ numbers. Additionally, under our\nassumption, we provide a combinatorial proof that 3 hidden layers are necessary\nto compute the maximum of 5 numbers; this had only been verified with an\nexcessive computation so far. Finally, we show that a natural generalization of\nthe best known upper bound to maxout networks is not tight, by demonstrating\nthat a rank-3 maxout layer followed by a rank-2 maxout layer is sufficient to\nrepresent the maximum of 7 numbers.",
        "In this paper, we mutuate the concept of Ducci matrices to the $p$-adic\nsetting, generalizing the classical Ducci sequences to the framework of\n$p$-adic numbers. The classical Ducci operator, which iteratively computes the\nabsolute differences of neighboring elements in a sequence or matrix, is\nredefined using the $p$-adic absolute value $| \\cdot |_p$. We investigate the\ndynamics of $p$-adic Ducci sequences for matrices over $\\mathbb{Q}_p$, focusing\non their convergence and periodicity properties.",
        "When faced with overwhelming evidence supporting the reality of time\ndilation, confirmed in particular by the Hafele-Keating experiment, some\nanti-relativists reluctantly concede that time dilation applies to light\nclocks. However, they argue that the Theory of Relativity remains flawed,\nclaiming that time dilation applies to light clocks only, not to massive\nobjects. They assert that atomic clocks, which operate based on microwave\nradiation, merely create the illusion that the Hafele-Keating experiment\nconfirms the theory. To refute this misconception, we introduce a thought\nexperiment inspired by Schrodinger's cat, in which the fate of Einstein's cat\ndepends on a \"Sync-or-Die clock\", an imaginary device that tests the\nsynchronization between a light clock and a mechanical clock, potentially\ntriggering the release of poison. By analyzing this scenario from both the\ninertial frame where the device is at rest and another in which it moves at\nconstant velocity, we demonstrate that time dilation must apply to the\nmechanical clock in exactly the same way as it does to the light clock,\nhighlighting the universality of relativistic time dilation.",
        "We classify involutions acting on spherical 3-manifolds up to conjugacy. Our\ngeometric approach provides insights into numerous topological properties of\nthese involutions.",
        "We work through Book I of Euclid's Elements with our focus on application of\nareas (I.42, I.44, I.45). We summarize alternate constructions from medieval\neditions of Euclid's elements and ancient and medieval commentaries. We remark\nthat Euclid's proof of I.44 involves a seldom commented on use of\nsuperposition, but that several medieval editions of Euclid give constructions\nthat avoid the use of superposition. This use of superposition is also avoided\nin Ralph Abraham's \"VCE: The Visual Constructions of Euclid\" C#12, C#12B at\nhttp:\/\/www.visual-euclid.org\/vce\/contents.html\n  We collate the figures with the digitized editions of Euclid at (P)\nBiblioteca Apostolica Vaticana (BAV), Vat. gr. 190, (F) Florence, Biblioteca\nMedicea Laurenziana, Plut. 28.03, (B) Bodleian, MS. D'Orville 301, (V)\n\\\"Osterreichische Nationalbibliothek, Cod. Phil. gr. 31 Han",
        "We investigate the maximal $L_p$-regularity in J.L. Lions' problem involving\na time-fractional derivative and a non-autonomous form $a(t;\\cdot,\\cdot)$ on a\nHilbert space $H$. This problem says whether the maximal $L_p$-regularity in\n$H$ hold when $t \\mapsto a(t ; u, v)$ is merely continuous or even merely\nmeasurable. We prove the maximal $L_p$-regularity results when the coefficients\nsatisfy general Dini-type continuity conditions. In particular, we construct a\ncounterexample to negatively answer this problem, indicating the minimal\nH\\\"{o}lder-scale regularity required for positive results.",
        "For a curve C and a reductive group G in prime characteristic, we relate the\nde Rham moduli of logarithmic G-connections on C to the Dolbeault moduli of\nlogarithmic G-Higgs bundles on the Frobenius twist of C. We name this result\nthe Log-p-NAHT. It is a logarithmic version of Chen-Zhu's characteristic p Non\nAbelian Hodge Theorem (p-NAHT). In contrast to the no pole case, the two moduli\nstacks in the log case are not isomorphic etale locally over the Hitchin base.\nInstead, they differ by an Artin-Schreier type Galois cover of the base. In\ncontrast to the case over the complex numbers, where some parabolic\/parahoric\ndata are needed to specify the boundary behavior of the tame harmonic metrics,\nno parabolic\/parahoric data are needed in Log-p-NAHT. We also establish a\nsemistable version of the Log-p-NAHT, and deduce several geometric and\ncohomological consequences. In particular, when G=GL_r, the Log-p-NAHT induces\nan embedding of the intersection cohomology of the degree d Dolbeault moduli to\nthat of the degree pd de Rham moduli, and the embedding is an isomorphism when\nr is coprime to d and p>r.",
        "This study investigates the thermodynamic and quantum properties of\nEinstein-Power-Yang-Mills (EPYM) black holes in an Anti-de Sitter background,\nfocusing on the effects of the nonlinear Yang-Mills charge parameter $\\gamma$.\nWe derive the metric function, analyze Hawking radiation through boson\ntunneling, and calculate thermodynamic properties including temperature and\nphase transitions. The quantum tunneling of $W^+$ bosons is examined using the\nWKB approximation and Hamilton-Jacobi formalism, revealing how nonlinearity\nmodifies the radiation spectrum. We compute the effective potential governing\nphoton orbits and null geodesics, demonstrating significant alterations in\nlight behavior in strong gravitational fields. Additionally, we explore the\nAschenbach effect, showing that this phenomenon, which is typically associated\nwith rotating black holes, can emerge in spherically symmetric EPYM spacetimes\nbecause of non-linear field interactions. Our results may yield observational\nmarkers that can be identified with instruments such as the Event Horizon\nTelescope and upcoming gravitational wave detectors.",
        "We performed an angle-resolved photoemission spectroscopy (ARPES) study of\nthe electronic structure of the CaFe$_2$As$_2$ 122-iron pnictide, a parent\ncompound, and two iron-based superconductors CaFe$_{2-x}$Co$_x$As$_2$ ($x =\n0.07$ and 0.15). We studied the band structure of this system across the phase\ndiagram with the transition from the orthorhombic spin density wave (SDW) phase\nto the tetragonal paramagnetic phase. We observed characteristic features of\nthe electronic structures corresponding to the antiferromagnetic phase in the\nparent compound and the samples with low cobalt concentration ($x = 0.07$). For\nhighly doped systems ($x = 0.15$), the measurements revealed the concentric\nbranches of the Fermi surface, which are associated with paramagnetic and\nsuperconducting 122-iron pnictides. We found the existence of Dirac cones\nlocated at 30 meV below Fermi energy for nonsuperconducting CaFe$_2$As$_2$ and\nsuperconducting CaFe$_{1.93}$Co$_{0.07}$As$_2$ orthorhombic SDW systems.",
        "Anticoncentration describes how an ensemble of quantum states spreads over\nthe allowed Hilbert space, leading to statistically uniform output probability\ndistributions. In this work, we investigate the anticoncentration of random\nClifford circuits toward the overlap distribution of random stabilizer states.\nUsing exact analytical techniques and extensive numerical simulations based on\nClifford replica tensor networks, we demonstrate that random Clifford circuits\nfully anticoncentrate in logarithmic circuit depth, namely higher-order moments\nof the overlap distribution converge to those of random stabilizer states.\nMoreover, we investigate the effect of introducing a controlled number of\nnon-Clifford (magic) resources into Clifford circuits. We show that inserting a\npolylogarithmic in qudit number of $T$-states is sufficient to drive the\noverlap distribution toward the Porter-Thomas statistics, effectively\nrecovering full quantum randomness. In short, this fact presents doped tensor\nnetworks and shallow Clifford circuits as pseudo-magic quantum states. Our\nresults clarify the interplay between Clifford dynamics, magic resource\ninjection, and quantum complexity, with implications for quantum circuit\nsampling and benchmarking of computational quantum advantage.",
        "We study phases and transitions of a triangular Hubbard model subject to\ncommensurate magnetic field, called the Hofstadter-Hubbard model. At filling\none fermion per site, for the number of fermion flavors 2 <= N <= 4, we\nidentify three distinct phases and calculate critical interaction strength from\nself-consistent mean-field approximation. Integer quantum Hall, chiral spin\nliquid, and valence bond solid (or stripe) states could be realized upon\nvarying the Hubbard interaction U. We study the critical transport behavior\nusing quantum Boltzmann equations for general N for the putative continuous\ntransition from quantum Hall states to chiral spin liquid. The critical\nbehavior serves as strong signatures of the critical theory and consequently of\nthe existence of chiral spin liquid as the proximate phase.",
        "Spin-splitting antiferromagnets with spin-polarized band structures in\nmomentum space have garnered intensive research attention due to their zero net\nmagnetic moments, ultras fast spin dynamics as conventional antiferromagnets,\nand spin-polarized transport properties akin to ferromagnets, making them\npromising candidates for antiferromagnetic spintronics. However, unlike\nspin-torque switching of ferromagnets by electric current, efficient electric\ncontrol of spin-splitting antiferromagnetic order remains challenges. In this\nwork, we identify prototypes of multiferroic spin-splitting antiferromagnets,\nincluding BiFeO3, Fe2Mo3O8 and compensated ferrimagnet GaFeO3 with\nferroelectric polarization as well as spin-polarized electronic structures. We\nestablish design principles for the spin-splitting multiferroic\nantiferromagnets and compensated ferrimagnets, elucidating the band symmetry\nfeatures in Brillouin zone. We demonstrate that the spin polarization in\nspin-splitting magnets, despite of zero net magnetic moment, can be switched by\nferroelectric polarization, providing an efficient means of controlling the\nantiferromagnetic order. Our work may inspire future development of novel\nmultiferroic functional magnets with zero magnetic moments and pave the way for\ntheir applications in magnetoelectric spintronic devices.",
        "IQPopt is a software package designed to optimize large-scale instantaneous\nquantum polynomial circuits on classical hardware. By exploiting an efficient\nclassical simulation algorithm for expectation value estimation, circuits with\nthousands of qubits and millions of gates can be optimized, provided the\nrelevant objective function has an efficient description in terms of Pauli-Z\ntype observables. Since sampling from instantaneous quantum polynomial circuits\nis widely believed to be hard for classical computers, this provides a method\nto identify powerful circuit instances before deployment and sampling on\nquantum hardware, where computational advantages may exist. The package\nleverages automatic differentiation in JAX, can be accelerated with access to\nhardware accelerators such as graphics processing units, and contains a\ndedicated module that can be used to train and evaluate quantum generative\nmodels via the maximum mean discrepancy.",
        "The assimilation and prediction of phase-resolved surface gravity waves are\ncritical challenges in ocean science and engineering. Potential flow theory\n(PFT) has been widely employed to develop wave models and numerical techniques\nfor wave prediction. However, traditional wave prediction methods are often\nlimited. For example, most simplified wave models have a limited ability to\ncapture strong wave nonlinearity, while fully nonlinear PFT solvers often fail\nto meet the speed requirements of engineering applications. This computational\ninefficiency also hinders the development of effective data assimilation\ntechniques, which are required to reconstruct spatial wave information from\nsparse measurements to initialize the wave prediction. To address these\nchallenges, we propose a novel solver method that leverages physics-informed\nneural networks (PINNs) that parameterize PFT solutions as neural networks.\nThis provides a computationally inexpensive way to assimilate and predict wave\ndata. The proposed PINN framework is validated through comparisons with\nanalytical linear PFT solutions and experimental data collected in a laboratory\nwave flume. The results demonstrate that our approach accurately captures and\npredicts irregular, nonlinear, and dispersive wave surface dynamics. Moreover,\nthe PINN can infer the fully nonlinear velocity potential throughout the entire\nfluid volume solely from surface elevation measurements, enabling the\ncalculation of fluid velocities that are difficult to measure experimentally.",
        "Let $\\iota_{k}(m,l)$ denote the total number of intervals of length $m$\nacross all Dyck paths of semilength $k$ such that each interval contains\nprecisely $l$ falls. We give the formula for $\\iota_{k}(m,l)$ and show that\n$\\iota_{k}(k,l)=\\binom{k}{l}^2$. Motivated by this, we propose two stronger\nvariants of the wreath conjecture due to Baranyai for $n=2k+1$.",
        "Probing quantumness in curved spacetime is regarded as one of fundamental and\nimportant topics in the framework of relativistic quantum information. In this\nwork, we focus on the theoretical feasibility of probing quantum properties in\nde Sitter (dS) and Anti-de Sitter (AdS) spacetimes via detectors. By employing\nthe Unruh-DeWitt detector coupled with a massless scalar field, which is\ntreated as an open system, quantum uncertainty and quantum coherence in both dS\nand AdS spacetimes are investigated. Our analysis reveals that the acceleration\nin dS spacetime and the boundary conditions in AdS spacetime significantly\nimpact the detector's evolution in the initial stage. Notably, both of the\nuncertainty and coherence will oscillate with the initial state being in a\nsuperposition state, however the high temperature is able to suppress their\noscillation. Interestingly, it is found that the constant values of the final\nuncertainty and coherence are identical as those in dS and AdS spacetimes,\nwhich are determined by the ratio of energy gap to temperature. Hence, the\ncurrent exploration offers insight into quantumness in dS and AdS spacetimes,\nand might be helpful to facilitate the curved-spacetime-based quantum\ninformation processing.",
        "In this article, the two filter formula is re-examined in the setting of\npartially observed Gauss--Markov models. It is traditionally formulated as a\nfilter running backward in time, where the Gaussian density is parametrized in\n``information form''. However, the quantity in the backward recursion is\nstrictly speaking not a distribution, but a likelihood. Taking this observation\nseriously, a recursion over log-quadratic likelihoods is formulated instead,\nwhich obviates the need for ``information'' parametrization. In particular, it\ngreatly simplifies the square-root formulation of the algorithm. Furthermore,\nformulae are given for producing the forward Markov representation of the a\nposteriori distribution over paths from the proposed likelihood representation.",
        "We explore a dynamic patent citation network model to explain the established\nlink between network structure and technological improvement rate. This model,\na type of survival model, posits that the *dynamic* network structure\ndetermines the *constant* improvement rate, requiring consistent structural\nreproduction over time. The model's hazard rate, the probability of a patent\nbeing cited, represents \"knowledge production,\" reflecting the output of new\npatents given existing ones. Analyzing hydrogen technology patents, we find\ndistinct subdomain knowledge production rates, but consistent development\nacross subdomains. \"Distribution\" patents show the lowest production rate,\nsuggesting dominant \"distribution\" costs in $H_2$ pricing. Further modeling\nshows Katz-centrality predicts knowledge production, outperforming subdomain\nclassification. Lower Katz centrality in \"distribution\" suggests inherent\norganizational differences in invention. Exploitative learning\n(within-subdomain citations) correlates with higher patenting opportunity\ncosts, potentially explaining slower \"distribution\" development, as high\ninvestment needs may incentivize monopolization over knowledge sharing.",
        "We investigate competition and cooperation of magnetic frustration and the\nKondo effect in the $J_1$-$J_2$ Kondo lattice model on the square lattice at\nzero temperature. In this model, the frustrated interactions $J_1,J_2$ between\nthe localized spins stabilize spin nematic orders, while the Kondo coupling\nfavors local spin singlets. Using the slave boson mean field approximation, we\nfind that the spin nematic order remains stable against small Kondo coupling,\nand the localized spins and the conduction electrons are effectively decoupled.\nOn the other hand, a standard Fermi liquid state is formed for sufficiently\nstrong Kondo interactions. Furthermore, in an intermediate region with moderate\nKondo coupling, the spin nematic order and the Kondo effect coexist, and\nsuperconducting pairing of the conduction electrons is induced by the spinon\npairing. We discuss the ground state phase diagram and nature of the quantum\nphase transitions between the different superconducting states.",
        "We study contests in which two groups compete to win (or not to win) a\ngroup-specific public-good\/bad prize. Each player in the groups can exert two\ntypes of effort: one to help her own group win the prize, and one to sabotage\nher own group's chances of winning it. The players in the groups choose their\neffort levels simultaneously and independently. We introduce a specific form of\ncontest success function that determines each group's probability of winning\nthe prize, taking into account players' sabotage activities. We show that two\ntypes of purestrategy Nash equilibrium occur, depending on parameter values:\none without sabotage activities and one with sabotage activities. In the first\ntype, only the highest-valuation player in each group expends positive effort,\nwhereas, in the second type, only the lowest-valuation player in each group\nexpends positive effort.",
        "Inspired by the works of \\cite{baz2} and \\cite{kian}, this study develops an\nabstract framework for analyzing differential equations with space-dependent\nfractional time derivatives and bounded operators. Within this framework, we\nestablish existence and uniqueness results for solutions in both linear and\nsemilinear settings. Our findings provide deeper insights into how spatially\nvarying fractional derivatives influence the behavior of differential\nequations, shedding light on their mathematical properties and potential\napplications.",
        "This paper introduces a methodology leveraging Large Language Models (LLMs)\nfor sector-level portfolio allocation through systematic analysis of\nmacroeconomic conditions and market sentiment. Our framework emphasizes\ntop-down sector allocation by processing multiple data streams simultaneously,\nincluding policy documents, economic indicators, and sentiment patterns.\nEmpirical results demonstrate superior risk-adjusted returns compared to\ntraditional cross momentum strategies, achieving a Sharpe ratio of 2.51 and\nportfolio return of 8.79% versus -0.61 and -1.39% respectively. These results\nsuggest that LLM-based systematic macro analysis presents a viable approach for\nenhancing automated portfolio allocation decisions at the sector level.",
        "Purpose: How much to invest in research facilities has long been a question\nin research policy and practice in higher education. This matter is\ntime-sensitive due to critical financial challenges at institutions in the USA,\nwith signs of significant problems in Europe. The purpose of this report is to\npresent new techniques for assessment of one particular type of research\ninfrastructure - computing facilities and staff that support research. These\nnew approaches are timely because of the ongoing financial crises which may\nmake it essential for institutions of higher education to make difficult\ndecisions regarding research infrastructure.\n  Principal results: We present recently developed methods for assessment of\nthe economic and scientific value of investment in advanced computing\nfacilities and services. Existing examples of these tools in use show that\ninvestment in advanced computing facilities and services contributes\nimportantly to positive financial and academic outcomes for institutions of\nhigher education. We present a format based on the Balanced Scorecard concept\nfor summarizing such information.\n  Conclusion: The methods presented here enable quantitative assessment of the\nrelationship between investment in computing facilities and research and\neducation outcomes. These methods should be of interest to research policy\ninvestigators and practitioners. The analysis methods described may be applied\nretroactively, making this report of potentially immediate value in setting\nresearch policies.",
        "Let $G = (V, E)$ be a simple undirected graph. A set $C \\subseteq V(G)$ is\nweakly convex of graph $G$ if for every two vertices $u,v\\in G$, there exists a\n$u-v$ geodesic whose vertices are in $C$. A set $C \\subseteq V$ is an\nouter-weakly convex dominating set if it is dominating set and every vertex not\nin $C$ is adjacent to some vertex in $C$ and a set $V(G)\\setminus C$ is weakly\nconvex. The outer-weakly convex domination number of graph $G$, denoted by\n$\\widetilde{ \\gamma}_{wcon}(G)$, is the minimum cardinality of an outer-weakly\nconvex dominating vertex set of graph $G$. In this paper, we determined the\nouter-weakly convex domination number of two graphs under the cartesian, strong\nand lexicographic products, and discuss some important combinatorial findings.",
        "K$_{0.5}$Na$_{0.5}$NbO$_{3}$ is among the most promising lead-free\npiezoelectrics. While its sputtered films match the performance of the champion\npiezoelectric Pb(Zr,Ti)O$_{3}$, processing of high-quality, reproducible, and\ntime-stable solution-processed K$_{0.5}$Na$_{0.5}$NbO$_{3}$ films remains\nchallenging. Here, we report 1 $\\mu$m-thick Mn-doped\nK$_{0.5}$Na$_{0.5}$NbO$_{3}$ films prepared through a chemical solution\ndeposition process, which have perfectly dense microstructure and uniform\ncomposition across their thickness. The films exhibit a high transverse\npiezoelectric coefficient (e$_{31,f}$ = -14.8 C\/m$^{2}$), high dielectric\npermittivity (${\\epsilon}_{r}$ = 920), low dielectric losses (tan${\\delta}$ =\n0.05) and can withstand electric fields up to at least 1 MV\/cm. The functional\nproperties show excellent stability over time, and the synthesis process is\nreproducible. The results demonstrate the high potential of Mn-doped\nK$_{0.5}$Na$_{0.5}$NbO$_{3}$ films to become a replacement for lead-based\nPb(Zr,Ti)O$_{3}$ films in piezoelectric applications.",
        "This study investigates the critical conditions for flame propagation in\nchannels with cold walls. We analyze the impact of the Lewis number and flow\namplitude ($A$) on the minimum channel width required to sustain a premixed\nflame. Our results span a wide range of Lewis numbers, encompassing both aiding\nand opposing flow conditions. Results are presented for both variable and\nconstant density models. A combined numerical approach, involving stationary\nand time-dependent simulations, is employed to determine quenching distances\nand solution stability. We find that smaller Lewis numbers and aiding flows ($A\n< 0$) facilitate flame propagation in narrower channels, while opposing flows\n($A > 0$) tend to destabilize the flame, promoting asymmetric solutions. For\nsufficiently large positive values of $A$, the quenching distance is determined\nby asymmetric solutions, rather than the typical symmetric ones."
      ]
    }
  },
  {
    "id":2411.17702,
    "research_type":"basic",
    "start_id":"b11",
    "start_title":"CLOCS: Contrastive Learning of Cardiac Signals Across Space, Time, and Patients",
    "start_abstract":"The healthcare industry generates troves of unlabelled physiological data. This data can be exploited via contrastive learning, a self-supervised pre-training method that encourages representations instances to similar one another. We propose family learning methods, CLOCS, across space, time, \\textit{and} patients show CLOCS consistently outperforms the state-of-the-art BYOL and SimCLR, when performing linear evaluation of, fine-tuning on, downstream tasks. also achieves strong generalization performance with only 25\\% labelled training Furthermore, our procedure naturally patient-specific used quantify patient-similarity.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b2"
      ],
      "title":[
        "Screening for cardiac contractile dysfunction using an artificial intelligence-enabled electrocardiogram"
      ],
      "abstract":[
        "Asymptomatic left ventricular dysfunction (ALVD) is present in 3-6% of the general population, is associated with reduced quality of life and longevity, and is treatable when found1-4. An inexpensive, noninvasive screening tool for ALVD in the doctor's office is not available. We tested the hypothesis that application of artificial intelligence (AI) to the electrocardiogram (ECG), a routine method of measuring the heart's electrical activity, could identify ALVD. Using paired 12-lead ECG and echocardiogram data, including the left ventricular ejection fraction (a measure of contractile function), from 44,959 patients at the Mayo Clinic, we trained a convolutional neural network to identify patients with ventricular dysfunction, defined as ejection fraction \u226435%, using the ECG data alone. When tested on an independent set of 52,870 patients, the network model yielded values for the area under the curve, sensitivity, specificity, and accuracy of 0.93, 86.3%, 85.7%, and 85.7%, respectively. In patients without ventricular dysfunction, those with a positive AI screen were at 4 times the risk (hazard ratio, 4.1; 95% confidence interval, 3.3 to 5.0) of developing future ventricular dysfunction compared with those with a negative screen. Application of AI to the ECG-a ubiquitous, low-cost test-permits the ECG to serve as a powerful screening tool in asymptomatic individuals to identify ALVD."
      ],
      "categories":[
        "Cardio"
      ]
    },
    "list":{
      "title":[
        "On singular problems associated with mixed operators under mixed\n  boundary conditions",
        "Effects of altermagnetic order, strain and doping on the optical and\n  vibrational properties of RuO$_2$",
        "Rethinking the Bias of Foundation Model under Long-tailed Distribution",
        "Depth-Bounds for Neural Networks via the Braid Arrangement",
        "Ducci Matrices in $p$-adic Context",
        "Einstein's Cat -- A Thought Experiment Against Anti-Relativist Claims",
        "Involutions of spherical 3-manifolds",
        "Book I of Euclid's Elements and application of areas",
        "Maximal $L_p$-regularity for fractional problem driven by non-autonomous\n  forms",
        "Logarithmic Non-Abelian Hodge Theory for curves in prime characteristic",
        "Nonlinear Einstein-Power-Yang-Mills AdS Black Holes: From Quantum\n  Tunneling to Aschenbach Effect",
        "Observation of the Dirac Dispersions in Co-doped CaFe2As2",
        "Anticoncentration in Clifford Circuits and Beyond: From Random Tensor\n  Networks to Pseudo-Magic States",
        "Phases and critical transport of the SU(N) Hofstadter-Hubbard model on\n  the triangular lattice",
        "Nonrelativistic spin-splitting multiferroic antiferromagnet and\n  compensated ferrimagnet with zero net magnetization",
        "IQPopt: Fast optimization of instantaneous quantum polynomial circuits\n  in JAX",
        "Physics-informed neural networks for phase-resolved data assimilation\n  and prediction of nonlinear ocean waves",
        "Intervals in Dyck paths and the wreath conjecture",
        "Quantifying Quantumness in (A)dS spacetimes with Unruh-DeWitt Detector",
        "The two filter formula reconsidered: Smoothing in partially observed\n  Gauss--Markov models without information parametrization",
        "Dynamic Structures of Knowledge Production: Citation Rates in Hydrogen\n  Technologies",
        "Spin nematic order and superconductivity in $J_1$-$J_2$ Kondo lattice\n  model on square lattice",
        "Sabotage and Free Riding in Contests with a Group-Specific\n  Public-Good\/Bad Prize",
        "Space-Dependent Fractional Evolution Equations: A New Approach",
        "Leveraging LLMS for Top-Down Sector Allocation In Automated Trading",
        "Assessing the value of advanced computing infrastructure for supporting\n  research: new tools to inform research policy",
        "Revisiting the outer-weakly convex domination number in graph products",
        "Enhanced Electromechanical Properties of Solution-Processed\n  K$_{0.5}$Na$_{0.5}$NbO$_{3}$ Thin Films",
        "Premixed flame quenching distance between cold walls: effects of flow\n  and Lewis number"
      ],
      "abstract":[
        "In this paper, we study the following singular problem associated with mixed\noperators (the combination of the classical Laplace operator and the fractional\nLaplace operator) under mixed boundary conditions \\begin{equation*} \\label{1}\n  \\left\\{\n  \\begin{aligned}\n  \\mathcal{L}u &= g(u), \\quad u > 0 \\quad \\text{in} \\quad \\Omega,\n  u &= 0 \\quad \\text{in} \\quad U^c,\n  \\mathcal{N}_s(u) &= 0 \\quad \\text{in} \\quad \\mathcal{N},\n  \\frac{\\partial u}{\\partial \\nu} &= 0 \\quad \\text{in} \\quad \\partial \\Omega\n\\cap \\overline{\\mathcal{N}},\n  \\end{aligned}\n  \\right.\n  \\tag{$P_\\lambda$} \\end{equation*}\n  where $U= (\\Omega \\cup {\\mathcal{N}} \\cup\n(\\partial\\Omega\\cap\\overline{\\mathcal{N}}))$, $\\Omega \\subseteq \\mathbb{R}^N$\nis a non empty open set, $\\mathcal{D}$, $\\mathcal{N}$ are open subsets of\n$\\mathbb{R}^N\\setminus{\\bar{\\Omega }}$ such that ${\\mathcal{D}} \\cup\n{\\mathcal{N}}= \\mathbb{R}^N\\setminus{\\bar{\\Omega}}$, $\\mathcal{D} \\cap\n{\\mathcal{N}}= \\emptyset $ and $\\Omega\\cup \\mathcal{N}$ is a bounded set with\nsmooth boundary, $\\lambda >0$ is a real parameter and\n  $\\mathcal{L}= -\\Delta+(-\\Delta)^{s},~ \\text{for}~s \\in (0, 1).$\n  Here $g(u)=u^{-q}$ or $g(u)= \\lambda u^{-q}+ u^p$ with $0<q<1<p\\leq 2^*-1$.\nWe study $(P_\\lambda)$ to derive the existence of weak solutions along with its\n$L^\\infty$-regularity. Moreover, some Sobolev-type variational inequalities\nassociated with these weak solutions are established.",
        "RuO$_2$, one of the most widely studied transition metal oxides, was recently\npredicted to host a novel form of collinear magnetic order referred to as\naltermagnetism. In this study we combine experiment (reflectance,\ntransmittance, ellipsometry and Raman measurements) and first-principles\ncalculations to elucidate the potential role of altermagnetic order, strain and\ndoping on the optical and vibrational properties of RuO$_2$ grown on TiO$_2$\n(001), (101) and (110) substrates. The combination of experiment and theory in\nthis study surprisingly indicates RuO$_2$ is in fact best described if one\nassumes the nonmagnetic state. Calculations of the altermagnetic state leads to\npoor agreement with the measured optical and vibrational properties of RuO$_2$.",
        "Long-tailed learning has garnered increasing attention due to its practical\nsignificance. Among the various approaches, the fine-tuning paradigm has gained\nconsiderable interest with the advent of foundation models. However, most\nexisting methods primarily focus on leveraging knowledge from these models,\noverlooking the inherent biases introduced by the imbalanced training data they\nrely on. In this paper, we examine how such imbalances from pre-training affect\nlong-tailed downstream tasks. Specifically, we find the imbalance biases\ninherited in foundation models on downstream task as parameter imbalance and\ndata imbalance. During fine-tuning, we observe that parameter imbalance plays a\nmore critical role, while data imbalance can be mitigated using existing\nre-balancing strategies. Moreover, we find that parameter imbalance cannot be\neffectively addressed by current re-balancing techniques, such as adjusting the\nlogits, during training, unlike data imbalance. To tackle both imbalances\nsimultaneously, we build our method on causal learning and view the incomplete\nsemantic factor as the confounder, which brings spurious correlations between\ninput samples and labels. To resolve the negative effects of this, we propose a\nnovel backdoor adjustment method that learns the true causal effect between\ninput samples and labels, rather than merely fitting the correlations in the\ndata. Notably, we achieve an average performance increase of about $1.67\\%$ on\neach dataset.",
        "We contribute towards resolving the open question of how many hidden layers\nare required in ReLU networks for exactly representing all continuous and\npiecewise linear functions on $\\mathbb{R}^d$. While the question has been\nresolved in special cases, the best known lower bound in general is still 2. We\nfocus on neural networks that are compatible with certain polyhedral complexes,\nmore precisely with the braid fan. For such neural networks, we prove a\nnon-constant lower bound of $\\Omega(\\log\\log d)$ hidden layers required to\nexactly represent the maximum of $d$ numbers. Additionally, under our\nassumption, we provide a combinatorial proof that 3 hidden layers are necessary\nto compute the maximum of 5 numbers; this had only been verified with an\nexcessive computation so far. Finally, we show that a natural generalization of\nthe best known upper bound to maxout networks is not tight, by demonstrating\nthat a rank-3 maxout layer followed by a rank-2 maxout layer is sufficient to\nrepresent the maximum of 7 numbers.",
        "In this paper, we mutuate the concept of Ducci matrices to the $p$-adic\nsetting, generalizing the classical Ducci sequences to the framework of\n$p$-adic numbers. The classical Ducci operator, which iteratively computes the\nabsolute differences of neighboring elements in a sequence or matrix, is\nredefined using the $p$-adic absolute value $| \\cdot |_p$. We investigate the\ndynamics of $p$-adic Ducci sequences for matrices over $\\mathbb{Q}_p$, focusing\non their convergence and periodicity properties.",
        "When faced with overwhelming evidence supporting the reality of time\ndilation, confirmed in particular by the Hafele-Keating experiment, some\nanti-relativists reluctantly concede that time dilation applies to light\nclocks. However, they argue that the Theory of Relativity remains flawed,\nclaiming that time dilation applies to light clocks only, not to massive\nobjects. They assert that atomic clocks, which operate based on microwave\nradiation, merely create the illusion that the Hafele-Keating experiment\nconfirms the theory. To refute this misconception, we introduce a thought\nexperiment inspired by Schrodinger's cat, in which the fate of Einstein's cat\ndepends on a \"Sync-or-Die clock\", an imaginary device that tests the\nsynchronization between a light clock and a mechanical clock, potentially\ntriggering the release of poison. By analyzing this scenario from both the\ninertial frame where the device is at rest and another in which it moves at\nconstant velocity, we demonstrate that time dilation must apply to the\nmechanical clock in exactly the same way as it does to the light clock,\nhighlighting the universality of relativistic time dilation.",
        "We classify involutions acting on spherical 3-manifolds up to conjugacy. Our\ngeometric approach provides insights into numerous topological properties of\nthese involutions.",
        "We work through Book I of Euclid's Elements with our focus on application of\nareas (I.42, I.44, I.45). We summarize alternate constructions from medieval\neditions of Euclid's elements and ancient and medieval commentaries. We remark\nthat Euclid's proof of I.44 involves a seldom commented on use of\nsuperposition, but that several medieval editions of Euclid give constructions\nthat avoid the use of superposition. This use of superposition is also avoided\nin Ralph Abraham's \"VCE: The Visual Constructions of Euclid\" C#12, C#12B at\nhttp:\/\/www.visual-euclid.org\/vce\/contents.html\n  We collate the figures with the digitized editions of Euclid at (P)\nBiblioteca Apostolica Vaticana (BAV), Vat. gr. 190, (F) Florence, Biblioteca\nMedicea Laurenziana, Plut. 28.03, (B) Bodleian, MS. D'Orville 301, (V)\n\\\"Osterreichische Nationalbibliothek, Cod. Phil. gr. 31 Han",
        "We investigate the maximal $L_p$-regularity in J.L. Lions' problem involving\na time-fractional derivative and a non-autonomous form $a(t;\\cdot,\\cdot)$ on a\nHilbert space $H$. This problem says whether the maximal $L_p$-regularity in\n$H$ hold when $t \\mapsto a(t ; u, v)$ is merely continuous or even merely\nmeasurable. We prove the maximal $L_p$-regularity results when the coefficients\nsatisfy general Dini-type continuity conditions. In particular, we construct a\ncounterexample to negatively answer this problem, indicating the minimal\nH\\\"{o}lder-scale regularity required for positive results.",
        "For a curve C and a reductive group G in prime characteristic, we relate the\nde Rham moduli of logarithmic G-connections on C to the Dolbeault moduli of\nlogarithmic G-Higgs bundles on the Frobenius twist of C. We name this result\nthe Log-p-NAHT. It is a logarithmic version of Chen-Zhu's characteristic p Non\nAbelian Hodge Theorem (p-NAHT). In contrast to the no pole case, the two moduli\nstacks in the log case are not isomorphic etale locally over the Hitchin base.\nInstead, they differ by an Artin-Schreier type Galois cover of the base. In\ncontrast to the case over the complex numbers, where some parabolic\/parahoric\ndata are needed to specify the boundary behavior of the tame harmonic metrics,\nno parabolic\/parahoric data are needed in Log-p-NAHT. We also establish a\nsemistable version of the Log-p-NAHT, and deduce several geometric and\ncohomological consequences. In particular, when G=GL_r, the Log-p-NAHT induces\nan embedding of the intersection cohomology of the degree d Dolbeault moduli to\nthat of the degree pd de Rham moduli, and the embedding is an isomorphism when\nr is coprime to d and p>r.",
        "This study investigates the thermodynamic and quantum properties of\nEinstein-Power-Yang-Mills (EPYM) black holes in an Anti-de Sitter background,\nfocusing on the effects of the nonlinear Yang-Mills charge parameter $\\gamma$.\nWe derive the metric function, analyze Hawking radiation through boson\ntunneling, and calculate thermodynamic properties including temperature and\nphase transitions. The quantum tunneling of $W^+$ bosons is examined using the\nWKB approximation and Hamilton-Jacobi formalism, revealing how nonlinearity\nmodifies the radiation spectrum. We compute the effective potential governing\nphoton orbits and null geodesics, demonstrating significant alterations in\nlight behavior in strong gravitational fields. Additionally, we explore the\nAschenbach effect, showing that this phenomenon, which is typically associated\nwith rotating black holes, can emerge in spherically symmetric EPYM spacetimes\nbecause of non-linear field interactions. Our results may yield observational\nmarkers that can be identified with instruments such as the Event Horizon\nTelescope and upcoming gravitational wave detectors.",
        "We performed an angle-resolved photoemission spectroscopy (ARPES) study of\nthe electronic structure of the CaFe$_2$As$_2$ 122-iron pnictide, a parent\ncompound, and two iron-based superconductors CaFe$_{2-x}$Co$_x$As$_2$ ($x =\n0.07$ and 0.15). We studied the band structure of this system across the phase\ndiagram with the transition from the orthorhombic spin density wave (SDW) phase\nto the tetragonal paramagnetic phase. We observed characteristic features of\nthe electronic structures corresponding to the antiferromagnetic phase in the\nparent compound and the samples with low cobalt concentration ($x = 0.07$). For\nhighly doped systems ($x = 0.15$), the measurements revealed the concentric\nbranches of the Fermi surface, which are associated with paramagnetic and\nsuperconducting 122-iron pnictides. We found the existence of Dirac cones\nlocated at 30 meV below Fermi energy for nonsuperconducting CaFe$_2$As$_2$ and\nsuperconducting CaFe$_{1.93}$Co$_{0.07}$As$_2$ orthorhombic SDW systems.",
        "Anticoncentration describes how an ensemble of quantum states spreads over\nthe allowed Hilbert space, leading to statistically uniform output probability\ndistributions. In this work, we investigate the anticoncentration of random\nClifford circuits toward the overlap distribution of random stabilizer states.\nUsing exact analytical techniques and extensive numerical simulations based on\nClifford replica tensor networks, we demonstrate that random Clifford circuits\nfully anticoncentrate in logarithmic circuit depth, namely higher-order moments\nof the overlap distribution converge to those of random stabilizer states.\nMoreover, we investigate the effect of introducing a controlled number of\nnon-Clifford (magic) resources into Clifford circuits. We show that inserting a\npolylogarithmic in qudit number of $T$-states is sufficient to drive the\noverlap distribution toward the Porter-Thomas statistics, effectively\nrecovering full quantum randomness. In short, this fact presents doped tensor\nnetworks and shallow Clifford circuits as pseudo-magic quantum states. Our\nresults clarify the interplay between Clifford dynamics, magic resource\ninjection, and quantum complexity, with implications for quantum circuit\nsampling and benchmarking of computational quantum advantage.",
        "We study phases and transitions of a triangular Hubbard model subject to\ncommensurate magnetic field, called the Hofstadter-Hubbard model. At filling\none fermion per site, for the number of fermion flavors 2 <= N <= 4, we\nidentify three distinct phases and calculate critical interaction strength from\nself-consistent mean-field approximation. Integer quantum Hall, chiral spin\nliquid, and valence bond solid (or stripe) states could be realized upon\nvarying the Hubbard interaction U. We study the critical transport behavior\nusing quantum Boltzmann equations for general N for the putative continuous\ntransition from quantum Hall states to chiral spin liquid. The critical\nbehavior serves as strong signatures of the critical theory and consequently of\nthe existence of chiral spin liquid as the proximate phase.",
        "Spin-splitting antiferromagnets with spin-polarized band structures in\nmomentum space have garnered intensive research attention due to their zero net\nmagnetic moments, ultras fast spin dynamics as conventional antiferromagnets,\nand spin-polarized transport properties akin to ferromagnets, making them\npromising candidates for antiferromagnetic spintronics. However, unlike\nspin-torque switching of ferromagnets by electric current, efficient electric\ncontrol of spin-splitting antiferromagnetic order remains challenges. In this\nwork, we identify prototypes of multiferroic spin-splitting antiferromagnets,\nincluding BiFeO3, Fe2Mo3O8 and compensated ferrimagnet GaFeO3 with\nferroelectric polarization as well as spin-polarized electronic structures. We\nestablish design principles for the spin-splitting multiferroic\nantiferromagnets and compensated ferrimagnets, elucidating the band symmetry\nfeatures in Brillouin zone. We demonstrate that the spin polarization in\nspin-splitting magnets, despite of zero net magnetic moment, can be switched by\nferroelectric polarization, providing an efficient means of controlling the\nantiferromagnetic order. Our work may inspire future development of novel\nmultiferroic functional magnets with zero magnetic moments and pave the way for\ntheir applications in magnetoelectric spintronic devices.",
        "IQPopt is a software package designed to optimize large-scale instantaneous\nquantum polynomial circuits on classical hardware. By exploiting an efficient\nclassical simulation algorithm for expectation value estimation, circuits with\nthousands of qubits and millions of gates can be optimized, provided the\nrelevant objective function has an efficient description in terms of Pauli-Z\ntype observables. Since sampling from instantaneous quantum polynomial circuits\nis widely believed to be hard for classical computers, this provides a method\nto identify powerful circuit instances before deployment and sampling on\nquantum hardware, where computational advantages may exist. The package\nleverages automatic differentiation in JAX, can be accelerated with access to\nhardware accelerators such as graphics processing units, and contains a\ndedicated module that can be used to train and evaluate quantum generative\nmodels via the maximum mean discrepancy.",
        "The assimilation and prediction of phase-resolved surface gravity waves are\ncritical challenges in ocean science and engineering. Potential flow theory\n(PFT) has been widely employed to develop wave models and numerical techniques\nfor wave prediction. However, traditional wave prediction methods are often\nlimited. For example, most simplified wave models have a limited ability to\ncapture strong wave nonlinearity, while fully nonlinear PFT solvers often fail\nto meet the speed requirements of engineering applications. This computational\ninefficiency also hinders the development of effective data assimilation\ntechniques, which are required to reconstruct spatial wave information from\nsparse measurements to initialize the wave prediction. To address these\nchallenges, we propose a novel solver method that leverages physics-informed\nneural networks (PINNs) that parameterize PFT solutions as neural networks.\nThis provides a computationally inexpensive way to assimilate and predict wave\ndata. The proposed PINN framework is validated through comparisons with\nanalytical linear PFT solutions and experimental data collected in a laboratory\nwave flume. The results demonstrate that our approach accurately captures and\npredicts irregular, nonlinear, and dispersive wave surface dynamics. Moreover,\nthe PINN can infer the fully nonlinear velocity potential throughout the entire\nfluid volume solely from surface elevation measurements, enabling the\ncalculation of fluid velocities that are difficult to measure experimentally.",
        "Let $\\iota_{k}(m,l)$ denote the total number of intervals of length $m$\nacross all Dyck paths of semilength $k$ such that each interval contains\nprecisely $l$ falls. We give the formula for $\\iota_{k}(m,l)$ and show that\n$\\iota_{k}(k,l)=\\binom{k}{l}^2$. Motivated by this, we propose two stronger\nvariants of the wreath conjecture due to Baranyai for $n=2k+1$.",
        "Probing quantumness in curved spacetime is regarded as one of fundamental and\nimportant topics in the framework of relativistic quantum information. In this\nwork, we focus on the theoretical feasibility of probing quantum properties in\nde Sitter (dS) and Anti-de Sitter (AdS) spacetimes via detectors. By employing\nthe Unruh-DeWitt detector coupled with a massless scalar field, which is\ntreated as an open system, quantum uncertainty and quantum coherence in both dS\nand AdS spacetimes are investigated. Our analysis reveals that the acceleration\nin dS spacetime and the boundary conditions in AdS spacetime significantly\nimpact the detector's evolution in the initial stage. Notably, both of the\nuncertainty and coherence will oscillate with the initial state being in a\nsuperposition state, however the high temperature is able to suppress their\noscillation. Interestingly, it is found that the constant values of the final\nuncertainty and coherence are identical as those in dS and AdS spacetimes,\nwhich are determined by the ratio of energy gap to temperature. Hence, the\ncurrent exploration offers insight into quantumness in dS and AdS spacetimes,\nand might be helpful to facilitate the curved-spacetime-based quantum\ninformation processing.",
        "In this article, the two filter formula is re-examined in the setting of\npartially observed Gauss--Markov models. It is traditionally formulated as a\nfilter running backward in time, where the Gaussian density is parametrized in\n``information form''. However, the quantity in the backward recursion is\nstrictly speaking not a distribution, but a likelihood. Taking this observation\nseriously, a recursion over log-quadratic likelihoods is formulated instead,\nwhich obviates the need for ``information'' parametrization. In particular, it\ngreatly simplifies the square-root formulation of the algorithm. Furthermore,\nformulae are given for producing the forward Markov representation of the a\nposteriori distribution over paths from the proposed likelihood representation.",
        "We explore a dynamic patent citation network model to explain the established\nlink between network structure and technological improvement rate. This model,\na type of survival model, posits that the *dynamic* network structure\ndetermines the *constant* improvement rate, requiring consistent structural\nreproduction over time. The model's hazard rate, the probability of a patent\nbeing cited, represents \"knowledge production,\" reflecting the output of new\npatents given existing ones. Analyzing hydrogen technology patents, we find\ndistinct subdomain knowledge production rates, but consistent development\nacross subdomains. \"Distribution\" patents show the lowest production rate,\nsuggesting dominant \"distribution\" costs in $H_2$ pricing. Further modeling\nshows Katz-centrality predicts knowledge production, outperforming subdomain\nclassification. Lower Katz centrality in \"distribution\" suggests inherent\norganizational differences in invention. Exploitative learning\n(within-subdomain citations) correlates with higher patenting opportunity\ncosts, potentially explaining slower \"distribution\" development, as high\ninvestment needs may incentivize monopolization over knowledge sharing.",
        "We investigate competition and cooperation of magnetic frustration and the\nKondo effect in the $J_1$-$J_2$ Kondo lattice model on the square lattice at\nzero temperature. In this model, the frustrated interactions $J_1,J_2$ between\nthe localized spins stabilize spin nematic orders, while the Kondo coupling\nfavors local spin singlets. Using the slave boson mean field approximation, we\nfind that the spin nematic order remains stable against small Kondo coupling,\nand the localized spins and the conduction electrons are effectively decoupled.\nOn the other hand, a standard Fermi liquid state is formed for sufficiently\nstrong Kondo interactions. Furthermore, in an intermediate region with moderate\nKondo coupling, the spin nematic order and the Kondo effect coexist, and\nsuperconducting pairing of the conduction electrons is induced by the spinon\npairing. We discuss the ground state phase diagram and nature of the quantum\nphase transitions between the different superconducting states.",
        "We study contests in which two groups compete to win (or not to win) a\ngroup-specific public-good\/bad prize. Each player in the groups can exert two\ntypes of effort: one to help her own group win the prize, and one to sabotage\nher own group's chances of winning it. The players in the groups choose their\neffort levels simultaneously and independently. We introduce a specific form of\ncontest success function that determines each group's probability of winning\nthe prize, taking into account players' sabotage activities. We show that two\ntypes of purestrategy Nash equilibrium occur, depending on parameter values:\none without sabotage activities and one with sabotage activities. In the first\ntype, only the highest-valuation player in each group expends positive effort,\nwhereas, in the second type, only the lowest-valuation player in each group\nexpends positive effort.",
        "Inspired by the works of \\cite{baz2} and \\cite{kian}, this study develops an\nabstract framework for analyzing differential equations with space-dependent\nfractional time derivatives and bounded operators. Within this framework, we\nestablish existence and uniqueness results for solutions in both linear and\nsemilinear settings. Our findings provide deeper insights into how spatially\nvarying fractional derivatives influence the behavior of differential\nequations, shedding light on their mathematical properties and potential\napplications.",
        "This paper introduces a methodology leveraging Large Language Models (LLMs)\nfor sector-level portfolio allocation through systematic analysis of\nmacroeconomic conditions and market sentiment. Our framework emphasizes\ntop-down sector allocation by processing multiple data streams simultaneously,\nincluding policy documents, economic indicators, and sentiment patterns.\nEmpirical results demonstrate superior risk-adjusted returns compared to\ntraditional cross momentum strategies, achieving a Sharpe ratio of 2.51 and\nportfolio return of 8.79% versus -0.61 and -1.39% respectively. These results\nsuggest that LLM-based systematic macro analysis presents a viable approach for\nenhancing automated portfolio allocation decisions at the sector level.",
        "Purpose: How much to invest in research facilities has long been a question\nin research policy and practice in higher education. This matter is\ntime-sensitive due to critical financial challenges at institutions in the USA,\nwith signs of significant problems in Europe. The purpose of this report is to\npresent new techniques for assessment of one particular type of research\ninfrastructure - computing facilities and staff that support research. These\nnew approaches are timely because of the ongoing financial crises which may\nmake it essential for institutions of higher education to make difficult\ndecisions regarding research infrastructure.\n  Principal results: We present recently developed methods for assessment of\nthe economic and scientific value of investment in advanced computing\nfacilities and services. Existing examples of these tools in use show that\ninvestment in advanced computing facilities and services contributes\nimportantly to positive financial and academic outcomes for institutions of\nhigher education. We present a format based on the Balanced Scorecard concept\nfor summarizing such information.\n  Conclusion: The methods presented here enable quantitative assessment of the\nrelationship between investment in computing facilities and research and\neducation outcomes. These methods should be of interest to research policy\ninvestigators and practitioners. The analysis methods described may be applied\nretroactively, making this report of potentially immediate value in setting\nresearch policies.",
        "Let $G = (V, E)$ be a simple undirected graph. A set $C \\subseteq V(G)$ is\nweakly convex of graph $G$ if for every two vertices $u,v\\in G$, there exists a\n$u-v$ geodesic whose vertices are in $C$. A set $C \\subseteq V$ is an\nouter-weakly convex dominating set if it is dominating set and every vertex not\nin $C$ is adjacent to some vertex in $C$ and a set $V(G)\\setminus C$ is weakly\nconvex. The outer-weakly convex domination number of graph $G$, denoted by\n$\\widetilde{ \\gamma}_{wcon}(G)$, is the minimum cardinality of an outer-weakly\nconvex dominating vertex set of graph $G$. In this paper, we determined the\nouter-weakly convex domination number of two graphs under the cartesian, strong\nand lexicographic products, and discuss some important combinatorial findings.",
        "K$_{0.5}$Na$_{0.5}$NbO$_{3}$ is among the most promising lead-free\npiezoelectrics. While its sputtered films match the performance of the champion\npiezoelectric Pb(Zr,Ti)O$_{3}$, processing of high-quality, reproducible, and\ntime-stable solution-processed K$_{0.5}$Na$_{0.5}$NbO$_{3}$ films remains\nchallenging. Here, we report 1 $\\mu$m-thick Mn-doped\nK$_{0.5}$Na$_{0.5}$NbO$_{3}$ films prepared through a chemical solution\ndeposition process, which have perfectly dense microstructure and uniform\ncomposition across their thickness. The films exhibit a high transverse\npiezoelectric coefficient (e$_{31,f}$ = -14.8 C\/m$^{2}$), high dielectric\npermittivity (${\\epsilon}_{r}$ = 920), low dielectric losses (tan${\\delta}$ =\n0.05) and can withstand electric fields up to at least 1 MV\/cm. The functional\nproperties show excellent stability over time, and the synthesis process is\nreproducible. The results demonstrate the high potential of Mn-doped\nK$_{0.5}$Na$_{0.5}$NbO$_{3}$ films to become a replacement for lead-based\nPb(Zr,Ti)O$_{3}$ films in piezoelectric applications.",
        "This study investigates the critical conditions for flame propagation in\nchannels with cold walls. We analyze the impact of the Lewis number and flow\namplitude ($A$) on the minimum channel width required to sustain a premixed\nflame. Our results span a wide range of Lewis numbers, encompassing both aiding\nand opposing flow conditions. Results are presented for both variable and\nconstant density models. A combined numerical approach, involving stationary\nand time-dependent simulations, is employed to determine quenching distances\nand solution stability. We find that smaller Lewis numbers and aiding flows ($A\n< 0$) facilitate flame propagation in narrower channels, while opposing flows\n($A > 0$) tend to destabilize the flame, promoting asymmetric solutions. For\nsufficiently large positive values of $A$, the quenching distance is determined\nby asymmetric solutions, rather than the typical symmetric ones."
      ]
    }
  },
  {
    "id":2411.17702,
    "research_type":"basic",
    "start_id":"b2",
    "start_title":"Screening for cardiac contractile dysfunction using an artificial intelligence-enabled electrocardiogram",
    "start_abstract":"Asymptomatic left ventricular dysfunction (ALVD) is present in 3-6% of the general population, is associated with reduced quality of life and longevity, and is treatable when found1-4. An inexpensive, noninvasive screening tool for ALVD in the doctor's office is not available. We tested the hypothesis that application of artificial intelligence (AI) to the electrocardiogram (ECG), a routine method of measuring the heart's electrical activity, could identify ALVD. Using paired 12-lead ECG and echocardiogram data, including the left ventricular ejection fraction (a measure of contractile function), from 44,959 patients at the Mayo Clinic, we trained a convolutional neural network to identify patients with ventricular dysfunction, defined as ejection fraction \u226435%, using the ECG data alone. When tested on an independent set of 52,870 patients, the network model yielded values for the area under the curve, sensitivity, specificity, and accuracy of 0.93, 86.3%, 85.7%, and 85.7%, respectively. In patients without ventricular dysfunction, those with a positive AI screen were at 4 times the risk (hazard ratio, 4.1; 95% confidence interval, 3.3 to 5.0) of developing future ventricular dysfunction compared with those with a negative screen. Application of AI to the ECG-a ubiquitous, low-cost test-permits the ECG to serve as a powerful screening tool in asymptomatic individuals to identify ALVD.",
    "start_categories":[
      "Cardio"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b5",
        "b11"
      ],
      "title":[
        "Patient Contrastive Learning: a Performant, Expressive, and Practical Approach to ECG Modeling.",
        "CLOCS: Contrastive Learning of Cardiac Signals Across Space, Time, and Patients"
      ],
      "abstract":[
        "Supervised machine learning applications in health care are often limited due to a scarcity of labeled training data. To mitigate this effect small sample size, we introduce pre-training approach, Patient Contrastive Learning Representations (PCLR), which creates latent representations ECGs from large number unlabeled examples. The resulting expressive, performant, and practical across wide spectrum clinical tasks. We develop PCLR using system with over 3.2 million 12-lead ECGs, demonstrate substantial improvements multiple new tasks when there fewer than 5,000 labels.",
        "The healthcare industry generates troves of unlabelled physiological data. This data can be exploited via contrastive learning, a self-supervised pre-training method that encourages representations instances to similar one another. We propose family learning methods, CLOCS, across space, time, \\textit{and} patients show CLOCS consistently outperforms the state-of-the-art BYOL and SimCLR, when performing linear evaluation of, fine-tuning on, downstream tasks. also achieves strong generalization performance with only 25\\% labelled training Furthermore, our procedure naturally patient-specific used quantify patient-similarity."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "A Hadamard theorem in transversely affine geometry with applications to\n  affine orbifolds",
        "Aligning Instance-Semantic Sparse Representation towards Unsupervised\n  Object Segmentation and Shape Abstraction with Repeatable Primitives",
        "A non-degeneracy theorem for interacting electrons in one dimension",
        "RiskHarvester: A Risk-based Tool to Prioritize Secret Removal Efforts in\n  Software Artifacts",
        "RSRefSeg: Referring Remote Sensing Image Segmentation with Foundation\n  Models",
        "Can LLM Agents Maintain a Persona in Discourse?",
        "Measuring $A_\\text{FB}^b$ and $R_b$ with exclusive $b$-hadron decays at\n  the FCC-ee",
        "Topological phase transition through tunable nearest-neighbor\n  interactions in a one-dimensional lattice",
        "VicSim: Enhancing Victim Simulation with Emotional and Linguistic\n  Fidelity",
        "Vision-Driven Prompt Optimization for Large Language Models in\n  Multimodal Generative Tasks",
        "Evaluating IOMMU-Based Shared Virtual Addressing for RISC-V Embedded\n  Heterogeneous SoCs",
        "MAD-BA: 3D LiDAR Bundle Adjustment -- from Uncertainty Modelling to\n  Structure Optimization",
        "Uniform bounds in excellent $\\mathbf{F}_p$-algebras and applications to\n  semi-continuity",
        "QuantumBind-RBFE: Accurate Relative Binding Free Energy Calculations\n  Using Neural Network Potentials",
        "A nonlocal degenerate macroscopic model of traffic dynamics with\n  saturated diffusion: modeling and calibration theory",
        "Constraints on Evolutions of Fundamental Constants from Clustering of\n  Fast Radio Burst Dispersion Measure",
        "Smart Sampling Strategies for Wireless Industrial Data Acquisition",
        "Spin-charge Kondo effect for a quantum dot with side coupled Majorana\n  zero mode",
        "Augmented Intelligence for Multimodal Virtual Biopsy in Breast Cancer\n  Using Generative Artificial Intelligence",
        "Relative Tur\\'an densities of ordered graphs",
        "PhaseMO: Future-Proof, Energy-efficient, Adaptive Massive MIMO",
        "AdvSwap: Covert Adversarial Perturbation with High Frequency\n  Info-swapping for Autonomous Driving Perception",
        "Inference Scaling Reshapes AI Governance",
        "Bayesian inference from time series of allele frequency data using exact\n  simulation techniques",
        "Tensor parametric Hamiltonian operator inference",
        "Human-in-the-Loop Annotation for Image-Based Engagement Estimation:\n  Assessing the Impact of Model Reliability on Annotation Accuracy",
        "Global existence for multi-dimensional partially diffusive systems",
        "As Confidence Aligns: Exploring the Effect of AI Confidence on Human\n  Self-confidence in Human-AI Decision Making"
      ],
      "abstract":[
        "We introduce and investigate a novel notion of transversely affine foliation,\ncomparing and contrasting it to the previous ones in the literature. We then\nuse it to give an extension of the classic Hadamard's theorem from Riemannian\ngeometry to this setting. Our main result is a transversely affine version of a\nwell-known \"Hadamard-like\" theorem by J. Hebda for Riemannian foliations.\nAlternatively, our result can be viewed as a foliation-theoretic analogue of\nthe Hadamard's theorem for affine manifolds proven by Beem and Parker. Namely,\nwe show that under the transverse analogs of pseudoconvexity and disprisonment\nfor the family of geodesics in the transverse affine geometry, together with an\nabsence of transverse conjugate points, the universal cover of a manifold\nendowed with a transversely affine foliation whose leaves are compact and with\nfinite holonomy is diffeomorphic to the product of a contractible manifold with\nthe universal cover of a leaf. This also leads to a Beem-Parker-type\nHadamard-like theorem for affine orbifolds.",
        "Understanding 3D object shapes necessitates shape representation by object\nparts abstracted from results of instance and semantic segmentation. Promising\nshape representations enable computers to interpret a shape with meaningful\nparts and identify their repeatability. However, supervised shape\nrepresentations depend on costly annotation efforts, while current unsupervised\nmethods work under strong semantic priors and involve multi-stage training,\nthereby limiting their generalization and deployment in shape reasoning and\nunderstanding. Driven by the tendency of high-dimensional semantically similar\nfeatures to lie in or near low-dimensional subspaces, we introduce a one-stage,\nfully unsupervised framework towards semantic-aware shape representation. This\nframework produces joint instance segmentation, semantic segmentation, and\nshape abstraction through sparse representation and feature alignment of object\nparts in a high-dimensional space. For sparse representation, we devise a\nsparse latent membership pursuit method that models each object part feature as\na sparse convex combination of point features at either the semantic or\ninstance level, promoting part features in the same subspace to exhibit similar\nsemantics. For feature alignment, we customize an attention-based strategy in\nthe feature space to align instance- and semantic-level object part features\nand reconstruct the input shape using both of them, ensuring geometric\nreusability and semantic consistency of object parts. To firm up semantic\ndisambiguation, we construct cascade unfrozen learning on geometric parameters\nof object parts.",
        "In this paper, we show that the ground-state of many-body Schr\\\"odinger\noperators for electrons in one dimension is non-degenerate. More precisely, we\nconsider Schr\\\"odinger operators of the form $H_N(v,w) = -\\Delta + \\sum_{i\\neq\nj}^N w(x_i,x_j) + \\sum_{j=1}^N v(x_i)$ acting on $\\wedge^N\n\\mathrm{L}^2([0,1])$, where the external and interaction potentials $v$ and $w$\nbelong to a large class of distributions. In this setting, we show that the\nground-state of the system with Fermi statistics and local boundary conditions\nis non-degenerate and does not vanish on a set of positive measure. In the case\nof periodic and anti-periodic (or more general non-local) boundary conditions,\nwe show that the same result holds whenever the number of particles is odd and\neven, respectively. This non-degeneracy result seems to be new even for regular\npotentials $v$ and $w$. As an immediate application of this result, we prove\neigenvalue inequalities and the strong unique continuation property for\neigenfunctions of the single-particle one-dimensional operators $h(v) = -\\Delta\n+v$. In addition, we prove strict inequalities between the lowest eigenvalues\nof different self-adjoint realizations of $H_N(v,w)$.",
        "Since 2020, GitGuardian has been detecting checked-in hard-coded secrets in\nGitHub repositories. During 2020-2023, GitGuardian has observed an upward\nannual trend and a four-fold increase in hard-coded secrets, with 12.8 million\nexposed in 2023. However, removing all the secrets from software artifacts is\nnot feasible due to time constraints and technical challenges. Additionally,\nthe security risks of the secrets are not equal, protecting assets ranging from\nobsolete databases to sensitive medical data. Thus, secret removal should be\nprioritized by security risk reduction, which existing secret detection tools\ndo not support. The goal of this research is to aid software practitioners in\nprioritizing secrets removal efforts through our security risk-based tool. We\npresent RiskHarvester, a risk-based tool to compute a security risk score based\non the value of the asset and ease of attack on a database. We calculated the\nvalue of asset by identifying the sensitive data categories present in a\ndatabase from the database keywords in the source code. We utilized data flow\nanalysis, SQL, and ORM parsing to identify the database keywords. To calculate\nthe ease of attack, we utilized passive network analysis to retrieve the\ndatabase host information. To evaluate RiskHarvester, we curated RiskBench, a\nbenchmark of 1,791 database secret-asset pairs with sensitive data categories\nand host information manually retrieved from 188 GitHub repositories.\nRiskHarvester demonstrates precision of (95%) and recall (90%) in detecting\ndatabase keywords for the value of asset and precision of (96%) and recall\n(94%) in detecting valid hosts for ease of attack. Finally, we conducted a\nsurvey (52 respondents) to understand whether developers prioritize secret\nremoval based on security risk score. We found that 86% of the developers\nprioritized the secrets for removal with descending security risk scores.",
        "Referring remote sensing image segmentation is crucial for achieving\nfine-grained visual understanding through free-format textual input, enabling\nenhanced scene and object extraction in remote sensing applications. Current\nresearch primarily utilizes pre-trained language models to encode textual\ndescriptions and align them with visual modalities, thereby facilitating the\nexpression of relevant visual features. However, these approaches often\nstruggle to establish robust alignments between fine-grained semantic concepts,\nleading to inconsistent representations across textual and visual information.\nTo address these limitations, we introduce a referring remote sensing image\nsegmentation foundational model, RSRefSeg. RSRefSeg leverages CLIP for visual\nand textual encoding, employing both global and local textual semantics as\nfilters to generate referring-related visual activation features in the latent\nspace. These activated features then serve as input prompts for SAM, which\nrefines the segmentation masks through its robust visual generalization\ncapabilities. Experimental results on the RRSIS-D dataset demonstrate that\nRSRefSeg outperforms existing methods, underscoring the effectiveness of\nfoundational models in enhancing multimodal task comprehension. The code is\navailable at \\url{https:\/\/github.com\/KyanChen\/RSRefSeg}.",
        "Large Language Models (LLMs) are widely used as conversational agents,\nexploiting their capabilities in various sectors such as education, law,\nmedicine, and more. However, LLMs are often subjected to context-shifting\nbehaviour, resulting in a lack of consistent and interpretable\npersonality-aligned interactions. Adherence to psychological traits lacks\ncomprehensive analysis, especially in the case of dyadic (pairwise)\nconversations. We examine this challenge from two viewpoints, initially using\ntwo conversation agents to generate a discourse on a certain topic with an\nassigned personality from the OCEAN framework (Openness, Conscientiousness,\nExtraversion, Agreeableness, and Neuroticism) as High\/Low for each trait. This\nis followed by using multiple judge agents to infer the original traits\nassigned to explore prediction consistency, inter-model agreement, and\nalignment with the assigned personality. Our findings indicate that while LLMs\ncan be guided toward personality-driven dialogue, their ability to maintain\npersonality traits varies significantly depending on the combination of models\nand discourse settings. These inconsistencies emphasise the challenges in\nachieving stable and interpretable personality-aligned interactions in LLMs.",
        "This paper presents a novel tagging technique to measure the beauty-quark\npartial decay-width ratio $R_b$ and its forward-backward asymmetry\n$A_\\text{FB}^b$ at the FCC-ee, using $\\mathcal{O}(10^{12})$ $Z$-boson decays.\nThe method is based on the exclusive reconstruction of a selected list of\n$b$-hadron decay modes in $Z\\to b\\bar{b}$ events at the $Z$ pole, which can\nprovide the flavour and possibly the charge of the hemisphere. This approach\neffectively eliminates the contamination from light-quark physics events and\nreduces the leading systematic uncertainties arising from background\ncontamination, tagging-efficiency correlations, and gluon-radiation corrections\nby exploiting the geometric and kinematic properties of beauty hadrons. This\nresults in a total relative uncertainty of the order of $0.01\\,\\%$ for both\nobservables. Furthermore, this precision allows to obtain a commensurate\nprecision on the weak mixing angle $\\sin^2(\\theta_W^\\text{eff})$ compared to\nthe muon forward-backward asymmetry on the order of $0.002\\,\\%$.",
        "We investigate the phase diagram of a one-dimensional model of hardcore\nbosons or spinless fermions with tunable nearest-neighbor interactions. By\nintroducing alternating repulsive and attractive interactions on consecutive\nbonds, we show that the system undergoes a transition from a bond-ordered (BO)\nphase to a charge-density wave-II (CDW-II) phase as the attractive interaction\nstrength increases at a fixed repulsive interaction. For a specific interaction\npattern, the BO phase exhibits topological properties, which vanish when the\npattern is altered, leading to a transition from a topological BO phase to a\ntrivial BO phase through a gap-closing point where both interactions vanish. We\nidentify these phases using a combination of order parameters, topological\ninvariants, edge-state analysis and Thouless charge pumping. By extending our\nanalysis beyond half-filling, we explore the phase diagram across all densities\nand identify the superfluid (SF) and the pair-superfluid (PSF) phases,\ncharacterized by single-particle and bound-pair excitations at incommensurate\ndensities. The proposed model is experimentally realizable in platforms such as\nRydberg excited or ultracold atoms in optical lattices, offering a versatile\nframework to study such interplay between topology and interactions in\nlow-dimensional systems.",
        "Scenario-based training has been widely adopted in many public service\nsectors. Recent advancements in Large Language Models (LLMs) have shown promise\nin simulating diverse personas to create these training scenarios. However,\nlittle is known about how LLMs can be developed to simulate victims for\nscenario-based training purposes. In this paper, we introduce VicSim (victim\nsimulator), a novel model that addresses three key dimensions of user\nsimulation: informational faithfulness, emotional dynamics, and language style\n(e.g., grammar usage). We pioneer the integration of scenario-based victim\nmodeling with GAN-based training workflow and key-information-based prompting,\naiming to enhance the realism of simulated victims. Our adversarial training\napproach teaches the discriminator to recognize grammar and emotional cues as\nreliable indicators of synthetic content. According to evaluations by human\nraters, the VicSim model outperforms GPT-4 in terms of human-likeness.",
        "Vision generation remains a challenging frontier in artificial intelligence,\nrequiring seamless integration of visual understanding and generative\ncapabilities. In this paper, we propose a novel framework, Vision-Driven Prompt\nOptimization (VDPO), that leverages Large Language Models (LLMs) to dynamically\ngenerate textual prompts from visual inputs, guiding high-fidelity image\nsynthesis. VDPO combines a visual embedding prompt tuner, a textual instruction\ngenerator, and a vision generation module to achieve state-of-the-art\nperformance in diverse vision generation tasks. Extensive experiments on\nbenchmarks such as COCO and Sketchy demonstrate that VDPO consistently\noutperforms existing methods, achieving significant improvements in FID, LPIPS,\nand BLEU\/CIDEr scores. Additional analyses reveal the scalability, robustness,\nand generalization capabilities of VDPO, making it a versatile solution for\nin-domain and out-of-domain tasks. Human evaluations further validate the\npractical superiority of VDPO in generating visually appealing and semantically\ncoherent outputs.",
        "Embedded heterogeneous systems-on-chip (SoCs) rely on domain-specific\nhardware accelerators to improve performance and energy efficiency. In\nparticular, programmable multi-core accelerators feature a cluster of\nprocessing elements and tightly coupled scratchpad memories to balance\nperformance, energy efficiency, and flexibility. In embedded systems running a\ngeneral-purpose OS, accelerators access data via dedicated, physically\naddressed memory regions. This negatively impacts memory utilization and\nperformance by requiring a copy from the virtual host address to the physical\naccelerator address space. Input-Output Memory Management Units (IOMMUs)\novercome this limitation by allowing devices and hosts to use a shared virtual\npaged address space. However, resolving IO virtual addresses can be\nparticularly costly on high-latency memory systems as it requires up to three\nsequential memory accesses on IOTLB miss. In this work, we present a\nquantitative evaluation of shared virtual addressing in RISC-V heterogeneous\nembedded systems. We integrate an IOMMU in an open-source heterogeneous RISC-V\nSoC consisting of a 64-bit host with a 32-bit accelerator cluster. We evaluated\nthe system performance by emulating the design on FPGA and implementing compute\nkernels from the RajaPERF benchmark suite using heterogeneous OpenMP\nprogramming. We measure the transfers and computation time on the host and\naccelerators for systems with different DRAM access latencies. We first show\nthat IO virtual address translation can account for 4.2% up to 17.6% of the\naccelerator's runtime for gemm (General Matrix Multiplication) at low and high\nmemory bandwidth. Then, we show that in systems containing a last-level cache,\nthis IO address translation cost falls to 0.4% and 0.7% under the same\nconditions, making shared virtual addressing and zero-copy offloading suitable\nfor such RISC-V heterogeneous SoCs.",
        "The joint optimization of sensor poses and 3D structure is fundamental for\nstate estimation in robotics and related fields. Current LiDAR systems often\nprioritize pose optimization, with structure refinement either omitted or\ntreated separately using representations like signed distance functions or\nneural networks. This paper introduces a framework for simultaneous\noptimization of sensor poses and 3D map, represented as surfels. A generalized\nLiDAR uncertainty model is proposed to address degraded or less reliable\nmeasurements in varying scenarios. Experimental results on public datasets\ndemonstrate improved performance over most comparable state-of-the-art methods.\nThe system is provided as open-source software to support further research.",
        "We study two important numerical invariants, Hilbert-Kunz multiplicity and\n$F$-signature, on the spectrum of a Noetherian $\\mathbf{F}_p$-algebra $R$ that\nis not necessarily $F$-finite. When $R$ is excellent, we show that the limits\ndefining the invariants are uniform. As a consequence, we show that the\n$F$-signature is lower semi-continuous, and the Hilbert-Kunz multiplicity is\nupper semi-continuous provided $R$ is locally equidimensional. Uniform\nconvergence is achieved via a uniform version of Cohen-Gabber theorem. We prove\nthe results under weaker conditions than excellence.",
        "Accurate prediction of protein-ligand binding affinities is crucial in drug\ndiscovery, particularly during hit-to-lead and lead optimization phases,\nhowever, limitations in ligand force fields continue to impact prediction\naccuracy. In this work, we validate relative binding free energy (RBFE)\naccuracy using neural network potentials (NNPs) for the ligands. We utilize a\nnovel NNP model, AceForce 1.0, based on the TensorNet architecture for small\nmolecules that broadens the applicability to diverse drug-like compounds,\nincluding all important chemical elements and supporting charged molecules.\nUsing established benchmarks, we show overall improved accuracy and correlation\nin binding affinity predictions compared with GAFF2 for molecular mechanics and\nANI2-x for NNPs. Slightly less accuracy but comparable correlations with OPLS4.\nWe also show that we can run the NNP simulations at 2 fs timestep, at least two\ntimes larger than previous NNP models, providing significant speed gains. The\nresults show promise for further evolutions of free energy calculations using\nNNPs while demonstrating its practical use already with the current generation.\nThe code and NNP model are publicly available for research use.",
        "In this work, we introduce a novel first-order nonlocal partial differential\nequation with saturated diffusion to describe the macroscopic behavior of\ntraffic dynamics. We show how the proposed model is better in comparison with\nexisting models in explaining the underlying driver behavior in real traffic\ndata. In doing so, we introduce a methodology for adjusting the parameters of\nthe proposed PDE with respect to the distribution of real datasets. In\nparticular, we conceptually and analytically elaborate on how such calibration\nconnects the solution of the PDE to the probability transition kernel proposed\nby the datasets.\n  The performance of the model is thoroughly investigated with respect to\nseveral metrics. More precisely, we study the capability of the model in\ncapturing the probability distribution realized by the datasets in the form of\nthe fundamental diagram. We show that the model is capable of approximating the\ndynamics of the evolution of the probability distribution. To this end, we\nevaluate the performance of the model with regard to the congestion formation\nand dissipation scenarios from various datasets.",
        "Constrained measurements of fundamental physical constants using astronomical\nobservational data represent a powerful method for investigating potential new\nphysics. In particular, the dispersion measure (DM) of fast radio bursts\n(FRBs), which probes the electron density along their propagation paths, may be\ninfluenced by the space-time variation of the fine-structure constant\n\\(\\alpha\\). In this study, we analyze the cross-correlation signal between\nforeground galaxies and the DM of background FRBs to constrain the evolution of\n\\(\\alpha\\). Assuming large-scale structure (LSS) galaxy surveys with the\ncapabilities of the China Space Station Telescope (CSST) at \\(z=0.15\\) and { a\nmock FRB survey with \\(N_{\\text{FRB}}=10^5\\) at \\(z=0.4\\), we test how well\n\\(\\alpha\\) variation can be constrained}, with a standard deviation of\n\\(\\sigma(\\Delta \\alpha \/ \\alpha) = 0.0007\\) at \\(z=0.15\\). Furthermore, taking\ninto account the nonminimal coupling between the scalar field and the\nelectromagnetic field, the variation in \\(\\alpha\\) can lead to the\nnon-conservation of photon number along geodesics. This would result in a\nviolation of the CDDR and affect the evolution of the Cosmic Microwave\nBackground (CMB) temperature. In this work, we { obtain constraints results} on\nthe CDDR parameter \\(\\eta\\) and the parameter \\(\\beta\\) governing CMB\ntemperature evolution at \\(z=0.15\\), yielding \\(\\sigma(\\eta) = 0.0004\\) and\n\\(\\sigma(\\beta) = 0.0006\\), respectively. Finally, we relate the variation in\n\\(\\alpha\\) to the time evolution of the proton-to-electron mass ratio, {\nreporting a standard deviation} of \\(\\sigma(\\Delta \\mu\/\\mu) = 0.002\\) at\n$z=0.15$. Future FRB surveys hold significant potential for advancing our\nunderstanding of the evolution of fundamental physical constants.",
        "In industrial environments, data acquisition accuracy is crucial for process\ncontrol and optimization. Wireless telemetry has proven to be a valuable tool\nfor improving efficiency in well-testing operations, enabling bidirectional\ncommunication and real-time control of downhole tools. However, high sampling\nfrequencies present challenges in telemetry, including data storage,\ntransmission, computational resource consumption, and battery life of wireless\ndevices. This study explores how optimizing data acquisition strategies can\nreduce aliasing effects and systematic errors while improving sampling rates\nwithout compromising measurement accuracy. A reduction of 80% in sampling\nfrequency was achieved without degrading measurement quality, demonstrating the\npotential for resource optimization in industrial environments.",
        "We investigate a minimal system consisting of a quantum dot coupled to a\nMajorana zero mode and a normal lead. We identify the underlying screening\nprocess as a novel spin-charge Kondo effect, where the low-energy spin and\ncharge degrees of freedom of the Majorana zero mode-quantum dot subsystem are\nfully screened by those in the normal lead, resulting in the formation of a\nspin-charge singlet. An effective low-energy model is derived, with charge\nfluctuations appropriately accounted for. This spin-charge Kondo effect is\nfound to be consistent with the spin-dependent Andreev\/normal boundary\nconditions induced by the Majorana zero mode. We demonstrate that the anomalous\nsubstructure in the spectrum and thermodynamic properties is closely tied to\nthe proportion of the charge component in the screening cloud. The spin-charge\nscreening cloud exhibits scaling behavior analogous to that of traditional\nKondo systems, though the sub-leading even-odd effect is subtly modified by the\nboundary conditions. These findings enhance our understanding of Kondo physics\nand resolve key debates on quantum dot nanostructures with Majorana zero modes.",
        "Full-Field Digital Mammography (FFDM) is the primary imaging modality for\nroutine breast cancer screening; however, its effectiveness is limited in\npatients with dense breast tissue or fibrocystic conditions. Contrast-Enhanced\nSpectral Mammography (CESM), a second-level imaging technique, offers enhanced\naccuracy in tumor detection. Nonetheless, its application is restricted due to\nhigher radiation exposure, the use of contrast agents, and limited\naccessibility. As a result, CESM is typically reserved for select cases,\nleaving many patients to rely solely on FFDM despite the superior diagnostic\nperformance of CESM. While biopsy remains the gold standard for definitive\ndiagnosis, it is an invasive procedure that can cause discomfort for patients.\nWe introduce a multimodal, multi-view deep learning approach for virtual\nbiopsy, integrating FFDM and CESM modalities in craniocaudal and mediolateral\noblique views to classify lesions as malignant or benign. To address the\nchallenge of missing CESM data, we leverage generative artificial intelligence\nto impute CESM images from FFDM scans. Experimental results demonstrate that\nincorporating the CESM modality is crucial to enhance the performance of\nvirtual biopsy. When real CESM data is missing, synthetic CESM images proved\neffective, outperforming the use of FFDM alone, particularly in multimodal\nconfigurations that combine FFDM and CESM modalities. The proposed approach has\nthe potential to improve diagnostic workflows, providing clinicians with\naugmented intelligence tools to improve diagnostic accuracy and patient care.\nAdditionally, as a contribution to the research community, we publicly release\nthe dataset used in our experiments, facilitating further advancements in this\nfield.",
        "We introduce a modification of the Tur\\'an density of ordered graphs and\ninvestigate this graph parameter.",
        "The rapid proliferation of devices and increasing data traffic in cellular\nnetworks necessitate advanced solutions to meet these escalating demands.\nMassive MIMO (Multiple Input Multiple Output) technology offers a promising\napproach, significantly enhancing throughput, coverage, and spatial\nmulti-plexing. Despite its advantages, massive MIMO systems often lack flexible\nsoftware controls over hardware, limiting their ability to optimize operational\nexpenditure (OpEx) by reducing power consumption while maintaining performance.\nCurrent software-controlled methods, such as antenna muting combined with\ndigital beamforming and hybrid beamforming, have notable limitations. Antenna\nmuting struggles to maintain throughput and coverage, while hybrid beamforming\nfaces hardware constraints that restrict scalability and future-proofing. This\nwork presents PhaseMO, a versatile approach that adapts to varying network\nloads. PhaseMO effectively reduces power consumption in low-load scenarios\nwithout sacrificing coverage and overcomes the hardware limitations of hybrid\nbeamforming, offering a scalable and future-proof solution. We will show that\nPhaseMO can achieve up to 30% improvement in energy efficiency while avoiding\nabout 10% coverage reduction and 5dB increase in UE transmit power.",
        "Perception module of Autonomous vehicles (AVs) are increasingly susceptible\nto be attacked, which exploit vulnerabilities in neural networks through\nadversarial inputs, thereby compromising the AI safety. Some researches focus\non creating covert adversarial samples, but existing global noise techniques\nare detectable and difficult to deceive the human visual system. This paper\nintroduces a novel adversarial attack method, AdvSwap, which creatively\nutilizes wavelet-based high-frequency information swapping to generate covert\nadversarial samples and fool the camera. AdvSwap employs invertible neural\nnetwork for selective high-frequency information swapping, preserving both\nforward propagation and data integrity. The scheme effectively removes the\noriginal label data and incorporates the guidance image data, producing\nconcealed and robust adversarial samples. Experimental evaluations and\ncomparisons on the GTSRB and nuScenes datasets demonstrate that AdvSwap can\nmake concealed attacks on common traffic targets. The generates adversarial\nsamples are also difficult to perceive by humans and algorithms. Meanwhile, the\nmethod has strong attacking robustness and attacking transferability.",
        "The shift from scaling up the pre-training compute of AI systems to scaling\nup their inference compute may have profound effects on AI governance. The\nnature of these effects depends crucially on whether this new inference compute\nwill primarily be used during external deployment or as part of a more complex\ntraining programme within the lab. Rapid scaling of inference-at-deployment\nwould: lower the importance of open-weight models (and of securing the weights\nof closed models), reduce the impact of the first human-level models, change\nthe business model for frontier AI, reduce the need for power-intense data\ncentres, and derail the current paradigm of AI governance via training compute\nthresholds. Rapid scaling of inference-during-training would have more\nambiguous effects that range from a revitalisation of pre-training scaling to a\nform of recursive self-improvement via iterated distillation and amplification.",
        "A central statistical problem in population genetics is to infer evolutionary\nand biological parameters such as the strength of natural selection and allele\nage from DNA samples extracted from a contemporary population. That all samples\ncome only from the present-day has long been known to limit statistical\ninference; there is potentially more information available if one also has\naccess to ancient DNA so that inference is based on a time-series of historical\nchanges in allele frequencies. We introduce a Markov Chain Monte Carlo (MCMC)\nmethod for Bayesian inference from allele frequency time-series data based on\nan underlying Wright--Fisher diffusion model of evolution, through which one\ncan infer the parameters of essentially any selection model including those\nwith frequency-dependent effects. The chief novelty is that we show this method\nto be exact in the sense that it is possible to augment the state space\nexplored by MCMC with the unobserved diffusion trajectory, even though the\ntransition function of this diffusion is intractable. Through careful design of\na proposal distribution, we describe an efficient method in which updates to\nthe trajectory and accept\/reject decisions are calculated without error. We\nillustrate the method on data capturing changes in coat colour over the past\n20,000 years, and find evidence to support previous findings that the mutant\nalleles ASIP and MC1R responsible for changes in coat color have experienced\nvery strong, possibly overdominant, selection and further provide estimates for\nthe ages of these genes.",
        "This work presents a tensor-based approach to constructing data-driven\nreduced-order models corresponding to semi-discrete partial differential\nequations with canonical Hamiltonian structure. By expressing parameter-varying\noperators with affine dependence as contractions of a generalized parameter\nvector against a constant tensor, this method leverages the operator inference\nframework to capture parametric dependence in the learned reduced-order model\nvia the solution to a convex, least-squares optimization problem. This leads to\na concise and straightforward implementation which compactifies previous\nparametric operator inference approaches and directly extends to learning\nparametric operators with symmetry constraints, a key feature required for\nconstructing structure-preserving surrogates of Hamiltonian systems. The\nproposed approach is demonstrated on both a (non-Hamiltonian) heat equation\nwith variable diffusion coefficient as well as a Hamiltonian wave equation with\nvariable wave speed.",
        "Human-in-the-loop (HITL) frameworks are increasingly recognized for their\npotential to improve annotation accuracy in emotion estimation systems by\ncombining machine predictions with human expertise. This study focuses on\nintegrating a high-performing image-based emotion model into a HITL annotation\nframework to evaluate the collaborative potential of human-machine interaction\nand identify the psychological and practical factors critical to successful\ncollaboration. Specifically, we investigate how varying model reliability and\ncognitive framing influence human trust, cognitive load, and annotation\nbehavior in HITL systems. We demonstrate that model reliability and\npsychological framing significantly impact annotators' trust, engagement, and\nconsistency, offering insights into optimizing HITL frameworks. Through three\nexperimental scenarios with 29 participants--baseline model reliability (S1),\nfabricated errors (S2), and cognitive bias introduced by negative framing\n(S3)--we analyzed behavioral and qualitative data. Reliable predictions in S1\nyielded high trust and annotation consistency, while unreliable outputs in S2\nled to increased critical evaluations but also heightened frustration and\nresponse variability. Negative framing in S3 revealed how cognitive bias\ninfluenced participants to perceive the model as more relatable and accurate,\ndespite misinformation regarding its reliability. These findings highlight the\nimportance of both reliable machine outputs and psychological factors in\nshaping effective human-machine collaboration. By leveraging the strengths of\nboth human oversight and automated systems, this study establishes a scalable\nHITL framework for emotion annotation and lays the foundation for broader\napplications in adaptive learning and human-computer interaction.",
        "In this work, we explore the global existence of strong solutions for a class\nof partially diffusive hyperbolic systems within the framework of critical\nhomogeneous Besov spaces. Our objective is twofold: first, to extend our recent\nfindings on the local existence presented in J.-P. Adogbo and R. Danchin. Local\nwell-posedness in the critical regularity setting for hyperbolic systems with\npartial diffusion. arXiv:2307.05981, 2024, and second, to refine and enhance\nthe analysis of Kawashima (S. Kawashima. Systems of a hyperbolic parabolic type\nwith applications to the equations of magnetohydrodynamics. PhD thesis, Kyoto\nUniversity, 1983).\n  To address the distinct behaviors of low and high frequency regimes, we\nemploy a hybrid Besov norm approach that incorporates different regularity\nexponents for each regime. This allows us to meticulously analyze the\ninteractions between these regimes, which exhibit fundamentally different\ndynamics.\n  A significant part of our methodology is based on the study of a Lyapunov\nfunctional, inspired by the work of Beauchard and Zuazua (K. Beauchard and E.\nZuazua. Large time asymptotics for partially dissipative hyperbolic system.\nArch. Rational Mech. Anal, 199:177-227, 2011.) and recent contributions (T.\nCrin-Barat and R. Danchin. Partially dissipative hyperbolic systems in the\ncritical regularity setting: the multi-dimensional case. J. Math. Pures Appl.\n(9), 165:1-41, 2022). To effectively handle the high-frequency components, we\nintroduce a parabolic mode with better smoothing properties, which plays a\ncentral role in our analysis.\n  Our results are particularly relevant for important physical systems, such as\nthe magnetohydrodynamics (MHD) system and the Navier-Stokes-Fourier equations.",
        "Complementary collaboration between humans and AI is essential for human-AI\ndecision making. One feasible approach to achieving it involves accounting for\nthe calibrated confidence levels of both AI and users. However, this process\nwould likely be made more difficult by the fact that AI confidence may\ninfluence users' self-confidence and its calibration. To explore these\ndynamics, we conducted a randomized behavioral experiment. Our results indicate\nthat in human-AI decision-making, users' self-confidence aligns with AI\nconfidence and such alignment can persist even after AI ceases to be involved.\nThis alignment then affects users' self-confidence calibration. We also found\nthe presence of real-time correctness feedback of decisions reduced the degree\nof alignment. These findings suggest that users' self-confidence is not\nindependent of AI confidence, which practitioners aiming to achieve better\nhuman-AI collaboration need to be aware of. We call for research focusing on\nthe alignment of human cognition and behavior with AI."
      ]
    }
  },
  {
    "id":2412.20007,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning",
    "start_abstract":"Deep learning tools have gained tremendous attention in applied machine learning. However such for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about uncertainty, but usually come with prohibitive computational cost. this paper we develop new theoretical casting dropout training deep neural networks (NNs) as approximate inference Gaussian processes. A direct result of theory gives us uncertainty NNs -- extracting information from existing that has been thrown away so far. This mitigates the problem representing without sacrificing either complexity or test accuracy. We perform an extensive study properties dropout's Various network architectures non-linearities are assessed on tasks classification, using MNIST example. show considerable improvement predictive log-likelihood RMSE compared state-of-the-art methods, finish by reinforcement",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b6"
      ],
      "title":[
        "The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions"
      ],
      "abstract":[
        "Training of neural networks for automated diagnosis pigmented skin lesions is hampered by the small size and lack diversity available datasets dermatoscopic images. We tackle this problem releasing HAM10000 (\"Human Against Machine with 10000 training images\") dataset. collected images from different populations acquired stored modalities. Given we had to apply acquisition cleaning methods developed semi-automatic workflows utilizing specifically trained networks. The final dataset consists 10015 which are released as a set academic machine learning purposes publicly through ISIC archive. This benchmark can be used comparisons human experts. Cases include representative collection all important diagnostic categories in realm lesions. More than 50% have been confirmed pathology, while ground truth rest cases was either follow-up, expert consensus, or confirmation in-vivo confocal microscopy."
      ],
      "categories":[
        "Data"
      ]
    },
    "list":{
      "title":[
        "On the origin of heating-induced stiffening and enthalpic reinforcement\n  in elastomeric nanocomposites",
        "General Classification, Invariance and Conservation Laws Analyses of\n  Nonlinear Fourth Order Wave and Nerve Membrane Equations with Dissipation",
        "Energy density and stress fields in quantum systems",
        "Beam splitters as controlled-Z gate for hybrid state",
        "On the zero capillarity limit for the Euler-Korteweg system",
        "Combinatorial construction of symplectic 6-manifolds via bifibration\n  structures",
        "Fiber-based mid-infrared frequency-swept laser at 50 MScans\/s via\n  frequency down-conversion of time-stretched pulses",
        "Geometric Flavours of Quantum Field Theory on a Cauchy Hypersurface:\n  Gaussian Analysis for the Hamiltonian Formalism and Applications to Cosmology",
        "Asymptotic Freedom in Parton Language: the Birth of Perturbative QCD",
        "From Target Tracking to Targeting Track -- Part III: Stochastic Process\n  Modeling and Online Learning",
        "Contextual Similarity Distillation: Ensemble Uncertainties with a Single\n  Model",
        "High-throughput computational screening of Heusler compounds with phonon\n  considerations for enhanced material discovery",
        "The self-interaction effects on the Kerr black hole superradiance and\n  their observational implications",
        "Constitutive Kolmogorov-Arnold Networks (CKANs): Combining Accuracy and\n  Interpretability in Data-Driven Material Modeling",
        "Dynamic Imprints of Colliding-wind Dust Formation from WR140",
        "Spectral gaps on thick part of moduli spaces",
        "Rewinding the byte trail of the White Whale",
        "Emergence of Fermi's Golden Rule in the Probing of a Quantum Many-Body\n  System",
        "Theory of Quantum-Enhanced Stimulated Raman Scattering",
        "Density fluctuation in the solar corona and solar wind: A comparative\n  analysis of radio-occultation observations and magnetohydrodynamic simulation",
        "A Milli-Kelvin Atomic Force Microscope Made of Glass",
        "Atomistic insights into solid solution strengthening: size misfit versus\n  stiffness misfit",
        "Enhancement of Electric Drive in Silicon Quantum Dots with Electric\n  Quadrupole Spin Resonance",
        "A strong-coupling effective-field theory for asymmetrically charged\n  plates with counterions only",
        "Skyrmions in Nanotechnology: Fundamental Properties, Experimental\n  Advances, and Emerging Applications",
        "Bayesian Selection for Efficient MLIP Dataset Selection",
        "Mapping strain and structural heterogeneities around bubbles in\n  amorphous ionically conductive Bi$_2$O$_3$",
        "Properties of Turbulent Convection and Large-Scale Flows in a Rotating\n  F-type Star Revealed by 3D Realistic Radiative Hydrodynamic Simulations",
        "Quantum metric induced magneto-optical effects in\n  $\\mathcal{PT}$-symmetric antiferromagnets"
      ],
      "abstract":[
        "Despite a century of use, the mechanism of nanoparticle-driven mechanical\nreinforcement of elastomers is unresolved. A major hypothesis attributes it to\nglassy interparticle bridges, supported by an observed inversion of the\nvariation of the modulus E(T) on heating -- from entropic stiffening in\nelastomers to enthalpic softening in nanocomposites. Here, molecular\nsimulations reveal that this instead arises from a bulk-modulus mediated\ncompetition between elastomer and nanoparticulate networks over preferred\nnonequilibrium densities under deformation. A theory accounting for softening\nof the bulk modulus on heating quantitatively predicts the simulated E(T)\ninversion, suggesting that reinforcement is driven by a volume-competition\nmechanism unique to co-continuous systems of soft and rigid networks.",
        "We study the nonlinear wave equation for arbitrary function with fourth order\ndissipation. A special case that is analysed exclusively is the model of nerve\nmembranes; we consider this model, both, in the presence and absence of the\nfourth order dissipation. The equivalence transformations, Lie symmetries and a\ncomplete classification is presented. We also discuss the one dimensional\noptimal system in each case obtained via classification. The reduction of the\npartial differential equations (PDEs) is carried out and the forms of invariant\nsolutions are presented. The study also include the construction of\nconservation laws using the direct method. The invariant solutions and some\nspecial type of solutions including solitions are presented with their\ngraphical illustrations.",
        "There has been an enduring interest and controversy about whether or not one\ncan define physically meaningful energy density and stress fields, $e({\\bf r})$\nand $\\sigma_{\\alpha \\beta}({\\bf r})$ in quantum systems. A key issue is kinetic\nenergy since $\\frac{1}{2}|\\nabla \\Psi|^2$ and $-\\frac{1}{2}\\Psi\\nabla^2 \\Psi$\nlead to different densities, and analogous issues arise interaction energies.\nThis paper presents a resolution to the problems in two steps: 1) All effects\nof exchange and correlation are shown to be unique functions defined at each\npoint ${\\bf r}$; all issues of non-uniqueness involve only the density $n({\\bf\nr})$ and are equivalent to a single-particle problem with wavefunction $s({\\bf\nr}) = \\sqrt{n({\\bf r})\/N}$. 2) Forms for the latter terms are proposed based on\nthe nature of energy and stress, where energy has two distinct roles. Because\nthe energy determines the ground state itself through the variational\nprinciple, the appropriate density involves the terms in the hamiltonian:\n$-\\frac{1}{2}\\Psi\\nabla^2 \\Psi$ and interactions in terms of potentials acting\non the particles. This leads naturally to density functional theory with the\ninterpretation that the energy density $e({\\bf r})$ is equilibrated to minimize\nfluctuations with the same chemical potential throughout the system. On the\nother hand, the energy and stress (derivative of the energy with respect to\nstrain) are properties of the ground state, and simple examples to show that\nthe only acceptable expressions involve the combination $\\frac{1}{4}[|\\nabla\n\\Psi|^2 - \\Psi\\nabla^2 \\Psi]$, as derived by Schrodinger, Pauli and others, and\nCoulomb interactions in the Maxwell form in terms of electric fields, not\npotentials. Together these results lead to well-defined formulations of energy\ndensity and stress fields that are physically motivated and based on a clear\nset of arguments.",
        "We explore a scheme based on adding a nonlocal photon and subtracting some\nnumber of photons to entangle the initial single-mode squeezed vacuum (SMSV)\nstate with the photon state. In a realistic model of interaction of the SMSV\nstate with the photonic state on a beam splitter (BS) with changeable\ntransmissivity or reflectivity the hybrid entanglement is realized for any\nvalues of the squeezing of input SMSV state. Maximum hybrid entanglement is\nachieved at certain values of the squeezing and BS parameter, which can mean\nimplementation of a two-qubit controlled-Z (CZ-) operation using the BS with\nthe appropriate initialization of the input states. The success probability of\nthe gate, taking into account multiphoton outcomes in the measuring mode of the\nBS, is more than 0.3. We also propose to use new continuous variable (CV)\nstates of definite parity that could increase the success probability of\ngenerating maximal hybrid entanglement. We show sufficient robustness of the\ngenerated entanglement under photon number resolving detection with practical\nquantum efficiency.",
        "We study the Euler-Korteweg equations with a weak capillarity tensor. It\nformally converges to the Euler equations in the zero capillarity limit. Our\naim is two-fold : first we prove rigorously this limit in R d , d $\\ge$ 1, and\nobtain a more precise BKW expansion of the solution, second we initiate the\nstudy of the problem on the half space. In this case we obtain a priori\nestimates for the solutions that degenerate as the capillary coefficient\nconverges to zero, and we explain this degeneracy with the construction of a\n(formal) BKW expansion that exhibits boundary layers. The results on the full\nspace extend and improve a classical result of Grenier (1998) on the\nsemi-classical limit of nonlinear Schr{\\\"o}dinger equations. The analysis on\nthe half space is restricted to the case of quantum fluids with irrotational\nvelocity.",
        "A bifibration structure on a $6$-manifold is a map to either the complex\nprojective plane $\\mathbb{P}^2$ or a $\\mathbb{P}^1$-bundle over $\\mathbb{P}^1$,\nsuch that its composition with the projection to $\\mathbb{P}^1$ is a\n($6$-dimensional) Lefschetz fibration\/pencil, and its restriction to the\npreimage of a generic $\\mathbb{P}^1$-fiber is also a ($4$-dimensional)\nLefschetz fibration\/pencil. This object has been studied by Auroux, Katzarkov,\nSeidel, among others. From a pair consisting of a monodromy representation of a\nLefschetz fibration\/pencil on a $4$-manifold and a relation in a braid group,\nwhich are mutually compatible in an appropriate sense, we construct a\nbifibration structure on a closed symplectic $6$-manifold, producing the given\ncompatible pair as its monodromies. We further establish methods for computing\ntopological invariants of symplectic $6$-manifolds, including Chern numbers,\nfrom compatible pairs. Additionally, we provide an explicit example of a\ncompatible pair, conjectured to correspond to a bifibration structure derived\nfrom the degree-$2$ Veronese embedding of the $3$-dimensional complex\nprojective space. This example can be viewed as a higher-dimensional analogue\nof the lantern relation in the mapping class group of the four-punctured\nsphere. Our results not only extend the applicability of combinatorial\ntechniques to higher-dimensional symplectic geometry but also offer a unified\nframework for systematically exploring symplectic $6$-manifolds.",
        "Increasing the sweep rate of mid-infrared (MIR) frequency-swept sources\noffers significant potential for various high-speed spectroscopy-based\napplications. While continuous-wave frequency-swept lasers have achieved sweep\nrates up to 1 MHz, a recently demonstrated time-stretched ultrashort pulsed\nlaser has reached a significantly higher sweep rate, up to tens of MHz.\nHowever, the previous system relied on a bulky femtosecond optical parametric\noscillator and produced only ~30 discrete spectral elements due to the use of a\nfree-space time stretcher. In this work, we present a fiber-based\nfrequency-swept MIR source that utilizes the frequency down-conversion of\ntime-stretched near-infrared pulses, employing a compact mode-locked fiber\nlaser and telecommunication fiber. As a proof-of-concept demonstration, we\nperformed MIR spectroscopy of methane gas around 3.4 um at a rate of 50\nMSpectra\/s, capturing 220 spectral elements over a range of 19.0 cm-1. This\ncompact and robust high-speed MIR frequency-swept laser system holds the\npotential for deployment in field applications.",
        "This thesis explores Quantum Field Theory (QFT) on curved spacetimes using a\ngeometric Hamiltonian approach to the Schr\\\"odinger-like representation. In\nparticular it studies the theory of the scalar field described through its\nconfigurations over a Cauchy hypersurface. It is focused on mathematical\nconsistency based on analytic and geometric tools.\n  The mathematical aspects of Gaussian integration theory in\ninfinite-dimensional Topological Vector Spaces (TVS) are thoroughly reviewed.\nIt also reviews the complex and holomorphic versions of important results and\nconcepts of Gaussian integration. For example, the Wiener-It\\^o decomposition\ntheorem or the definition of Hida test functions.\n  The physical framework builds upon three interconnected levels: classical\nGeneral Relativity (GR), Classical Statistical Field Theory (CSFT), and QFT.\nThe work begins by extending the Koopman-van Hove (KvH) formalism of classical\nstatistical mechanics to CSFT. This description is based upon prequantization\ntheory. It reveals features inherent to both CSFT and QFT, that help delineate\nthe genuine quantum features of a theory.\n  Upon the prequantum program, the QFT of the scalar field is built mixing\nGeometric Quantization with the choice of Wick and Weyl orderings. Various\nquantum representations are introduced: the holomorphic, Schr\\\"odinger,\nfield-momentum, and antiholomorphic. The relation among them is studied using\nintegral transforms, including novel infinite-dimensional Fourier transforms.\nFrom a geometrical analysis, it is argued that a covariant time derivative that\nmodifies the evolution equations should be added to the Schr\\\"odinger equation.\nThis connection is unique and required by the geometrodynamical description\nofthe coupling of QFT and GR.\n  Finally, studying the free model on cosmological spacetimes it obtains\nparticle creation effects on a dynamical equation.",
        "I review the contributions of Giorgio Parisi to perturbative QCD.\nConcentrated in a decade, they mark the transition of the theory of strong\ninteractions from a set of loosely connected ideas based on models, to a\nquantum field theory that is now an integral part of the standard model of\nfundamental interactions. Parisi's contributions have established at a very\nearly stage ideas, methods and tools that are now standard, and in several\ncases anticipated results that only became prominent in the XXIst century.",
        "This is the third part of a series of studies that model the target\ntrajectory, which describes the target state evolution over continuous time, as\na sample path of a stochastic process (SP). By adopting a\ndeterministic-stochastic decomposition framework, we decompose the learning of\nthe trajectory SP into two sequential stages: the first fits the deterministic\ntrend of the trajectory using a curve function of time, while the second\nestimates the residual stochastic component through parametric learning of\neither a Gaussian process (GP) or Student's-$t$ process (StP). This leads to a\nMarkov-free data-driven tracking approach that produces the continuous-time\ntrajectory with minimal prior knowledge of the target dynamics. Notably, our\napproach explicitly models both the temporal correlations of the state sequence\nand of measurement noises through the SP framework. It does not only take\nadvantage of the smooth trend of the target but also makes use of the long-term\ntemporal correlation of both the data noise and the model fitting error.\nSimulations in four maneuvering target tracking scenarios have demonstrated its\neffectiveness and superiority in comparison with existing approaches.",
        "Uncertainty quantification is a critical aspect of reinforcement learning and\ndeep learning, with numerous applications ranging from efficient exploration\nand stable offline reinforcement learning to outlier detection in medical\ndiagnostics. The scale of modern neural networks, however, complicates the use\nof many theoretically well-motivated approaches such as full Bayesian\ninference. Approximate methods like deep ensembles can provide reliable\nuncertainty estimates but still remain computationally expensive. In this work,\nwe propose contextual similarity distillation, a novel approach that explicitly\nestimates the variance of an ensemble of deep neural networks with a single\nmodel, without ever learning or evaluating such an ensemble in the first place.\nOur method builds on the predictable learning dynamics of wide neural networks,\ngoverned by the neural tangent kernel, to derive an efficient approximation of\nthe predictive variance of an infinite ensemble. Specifically, we reinterpret\nthe computation of ensemble variance as a supervised regression problem with\nkernel similarities as regression targets. The resulting model can estimate\npredictive variance at inference time with a single forward pass, and can make\nuse of unlabeled target-domain data or data augmentations to refine its\nuncertainty estimates. We empirically validate our method across a variety of\nout-of-distribution detection benchmarks and sparse-reward reinforcement\nlearning environments. We find that our single-model method performs\ncompetitively and sometimes superior to ensemble-based baselines and serves as\na reliable signal for efficient exploration. These results, we believe,\nposition contextual similarity distillation as a principled and scalable\nalternative for uncertainty quantification in reinforcement learning and\ngeneral deep learning.",
        "High-throughput (HTP) $ab$ $initio$ calculations are performed on 27,865\nHeusler compositions, covering a broad range of regular, inverse, and\nhalf-Heusler compounds in both cubic and tetragonal phases. In addition to\nconventional stability metrics, such as formation energy, Hull distance, and\nmagnetic critical temperature $T_{\\mathrm{c}}$, phonon stability is assessed by\nsystematically conducting $ab$ $initio$ phonon calculations for over 8,000\ncompounds. The performance of $ab$ $initio$ stability criteria is\nsystematically assessed against 189 experimentally synthesized compounds, and\nmagnetic critical temperature calculations are validated using 59 experimental\ndata points. As a result, we identify 631 stable compounds as promising\ncandidates for further functional material exploration. Notably, 47 low-moment\nferrimagnets are identified, with their spin polarization and anomalous\nHall\/Nernst conductivity calculated to provide insights into potential\napplications in spintronics and energy harvesting. Furthermore, our analyses\nreveal linear relationship between $T_{\\mathrm{c}}$ and magnetization in 14\nsystems and correlations between stability and atomic properties such as atomic\nradius and ionization energy. The regular\/inverse structures preference in\n$X_2YZ$ compound and tetragonal distortion are also investigated for a broad\nHeusler family.",
        "Through the black hole (BH) superradiance, ultralight bosons can form dense\nclouds around rotating Kerr BHs. Certain ultralight bosons, such as axions and\naxion-like particles (promising dark matter candidates), naturally possess\nself-interactions, and thus may significantly modify the dynamics of the\nsuperradiance process. Previous studies on the detection or constraint of\nultralight bosons through superradiance have largely neglected the\nself-interaction effects of bosons. In this work, we investigate the formation\nand evolution of self-interacting boson clouds in the full Kerr spacetime\nduring BH superradiance. Using numerical methods, we compute the superradiant\ngrowth rate of boson clouds with self-interactions around Kerr BHs and\nquantitatively evaluate how the self-interaction strength of scalar bosons\naffects the growth rate. We also assess the evolution of the BH's mass and\nspin. Our results reveal that, in addition to the superradiance-imposed upper\nbound on the boson cloud mass, self-interactions of ultralight bosons introduce\na new, lower critical mass limit, beyond which the growth rate of the boson\ncloud approaches zero. This implies that the superradiance process terminates\nearlier when self-interactions are considered. Furthermore, we explore how\nself-interactions affect both the oscillation frequency of boson clouds in\ngravitational atoms and the frequency of gravitational wave (GW) emitted\nthrough cloud annihilation. The anticipated frequency shift could be detectable\nby the GW observatories. Given that self-interactions substantially alter the\nevolution of BH superradiance, their effects can significantly relax existing\nconstraints on scalar boson models derived from superradiance. Taking the spin\nmeasurements from GW190412 and GW190517 as examples, we discuss the impact of\nself-interactions on constraint results in details.",
        "Hybrid constitutive modeling integrates two complementary approaches for\ndescribing and predicting a material's mechanical behavior: purely data-driven\nblack-box methods and physically constrained, theory-based models. While\nblack-box methods offer high accuracy, they often lack interpretability and\nextrapolability. Conversely, physics-based models provide theoretical insight\nand generalizability but may not capture complex behaviors with the same\naccuracy. Traditionally, hybrid modeling has required a trade-off between these\naspects. In this paper, we show how recent advances in symbolic machine\nlearning, specifically Kolmogorov-Arnold Networks (KANs), help to overcome this\nlimitation. We introduce Constitutive Kolmogorov-Arnold Networks (CKANs) as a\nnew class of hybrid constitutive models. By incorporating a post-processing\nsymbolification step, CKANs combine the predictive accuracy of data-driven\nmodels with the interpretability and extrapolation capabilities of symbolic\nexpressions, bridging the gap between machine learning and physical modeling.",
        "Carbon-rich Wolf-Rayet binaries are a prominent source of carbonaceous dust\nthat contribute to the dust budget of galaxies. The \"textbook\" example of an\nepisodic dust producing WR binary, WR140 (HD193793), provides us with an ideal\nlaboratory for investigating the dust physics and kinematics in an extreme\nenvironment. This study is among the first to utilize two separate JWST\nobservations, from Cycle 1 ERS (July 2022) and Cycle 2 (Sept. 2023), to measure\nWR140's dust kinematics and confirm its morphology. To measure the proper\nmotions and projected velocities of the dust shells, we performed a novel PSF\nsubtraction to reduce the effects of the bright diffraction spikes and\ncarefully aligned the Cycle 2 to the Cycle 1 images. At 7.7 $\\mu$m, through the\nbright feature common to 16 dust shells (C1), we find an average dust shell\nproper motion of $390\\pm29$ mas yr$^{-1}$, which equates to a projected\nvelocity of $2714\\pm188$ km s$^{-1}$ at a distance of 1.64 kpc. Our measured\nspeeds are constant across all visible shells and consistent with previously\nreported dust expansion velocities. Our observations not only prove that these\ndusty shells are astrophysical (i.e., not associated with any PSF artifact) and\noriginate from WR140, but also confirm the \"clumpy\" morphology of the dust\nshells, in which identifiable substructures within certain shells persist for\nat least 14 months from one cycle to the next. These results support the\nhypothesis that clumping in the wind collision region is required for dust\nproduction in WR binaries.",
        "In this paper, we study spectral gaps of closed hyperbolic surfaces for large\ngenus. We show that for any fixed $k\\geq 1$, as the genus goes to infinity, the\nmaximum of $\\lambda_k-\\lambda_{k-1}$ over any thick part of the moduli space of\nclosed Riemann surfaces approaches the limit $\\frac{1}{4}$.",
        "Motivated by a popular code golf challenge, we review some key ideas from\ninformation theory and discuss how to efficiently compress a streaming file\nwith an acceptable error rate.",
        "Fermi's Golden Rule (FGR) is one of the most impactful formulas in quantum\nmechanics, providing a link between easy-to-measure observables - such as\ntransition rates - and fundamental microscopic properties - such as density of\nstates or spectral functions. Its validity relies on three key assumptions: the\nexistence of a continuum, an appropriate time window, and a weak coupling.\nUnderstanding the regime of validity of FGR is critical for the proper\ninterpretation of most spectroscopic experiments. While the assumptions\nunderlying FGR are straightforward to analyze in simple models, their\napplicability is significantly more complex in quantum many-body systems. Here,\nwe observe the emergence and breakdown of FGR, using a strongly interacting\nhomogeneous spin-$1\/2$ Fermi gas coupled to a radio-frequency (rf) field.\nMeasuring the transition probability into an outcoupled internal state, we map\nthe system's dynamical response diagram versus the rf-pulse duration $t$ and\nRabi frequency $\\Omega_0$. For weak drives, we identify three regimes: an\nearly-time regime where the transition probability takes off as $t^2$, an\nintermediate-time FGR regime, and a long-time non-perturbative regime. Beyond a\nthreshold Rabi frequency, Rabi oscillations appear. Our results provide a\nblueprint for the applicability of linear response theory to the spectroscopy\nof quantum many-body systems.",
        "Stimulated Raman scattering (SRS) is a powerful method for label-free imaging\nand spectroscopy of materials. Recent experiments have shown that\nquantum-enhanced Raman scattering can surpass the shot noise limit and improve\nthe sensitivity substantially. Here, we introduce a full theory of\nquantum-enhanced SRS based on the framework of quantum metrology. Our results\nenable the assessment of quantum-enhancements of arbitrary measurement\nstrategies and identify optimal measurement observables that extract maximal\ninformation about the signal. We use this to identify the optimal employment of\nsqueezed states in SRS, highlighting the potential to improve quantum gains\nbeyond those observed in recent experiments. Our work establishes the\ntheoretical foundation for understanding and approaching the quantum limits of\nprecision in SRS, and provide a tool to discuss nonlinear spectroscopy and\nimaging more broadly.",
        "Recent in-situ observations and numerical models indicated various types of\nmagnetohydrodynamic (MHD) waves contributing to the solar wind acceleration.\nAmong them is an MHD wave decomposition at distances closer than 50 $R_{\\odot}$\nusing data taken by the first perihelion pass of Parker Solar Probe (PSP).\nHowever, the underlying physical processes responsible for the formation of the\nsolar wind have not yet been observationally confirmed at distances closer than\n10 $R_{\\odot}$. We aim to infer the mode population of density fluctuations\nobserved by radio occultation, which has all been attributed to slow\nmagnetoacoustic waves. We compare the radio occultation observations conducted\nin 2016 using the JAXA's Venus orbiter Akatsuki with the MHD simulation. The\ntime-frequency analysis was applied to the density fluctuations observed by the\nradio occultation and those reproduced in the MHD model. The time-spatial\nspectrum of the density fluctuation in the model exhibits two components that\nare considered to be fast and slow magnetoacoustic waves. The fast\nmagnetoacoustic waves in the model tend to have periods shorter than the slow\nmagnetoacoustic waves, and the superposition of these modes has a broadened\nspectrum extending in the range of approximately 20$-$1000 s, which resembles\nthat of the observed waves. Based on this comparison, it is probable that the\ndensity oscillations observed by radio occultation include fast and slow\nmagnetoacoustic waves, and that fast magnetoacoustic waves are predominant at\nshort periods and slow magnetoacoustic waves are prevalent at long periods.\nThis is qualitatively similar to the results of the mode decomposition obtained\nfrom the PSP's first perihelion at more distance regions.",
        "Milli-Kelvin atomic force microscopy (mK-AFM) presents an ongoing\nexperimental challenge due to the intense vibrations in a cryogen-free dilution\nrefrigerator and the low cooling power available at mK temperatures. A viable\napproach is to make the system exceptionally rigid and thermally insulating to\ndecouple external vibrations and isolate heat dissipation from the piezo\nelements. Here, we present a low-cost and large scan-range mK-AFM that operates\nbelow 100 mK. All the essential parts of our mK-AFM, including the scanners,\ntip assembly, and microscope body, are custom-made of fused silica glass by\ntaking advantage of its high specific modulus, extremely low thermal expansion\ncoefficient, and excellent thermal insulation properties. We carefully balance\nthe scan range (25 ${\\mu}$m $\\times$ 25 ${\\mu}$m), heat dissipation, and\nstiffness of the system to reach optimal performance at mK temperatures.",
        "Used for centuries to enhance mechanical properties of materials, solid\nsolution strengthening (SSS) is a classical metallurgical method in which small\namounts of impurity elements are added to a base metal. Developed for dilute\nalloys, classical theories of SSS are presently challenged by the ongoing\nexplosive development of complex concentrated alloys (CCA) in which all\ncomponent elements are present in nearly equal fractions. Here we develop a\nmethod of computational alchemy in which interatomic interactions are modified\nto continuously and systematically vary two key parameters defining SSS, atomic\nsize misfit and elastic stiffness misfit, over a maximally wide range of misfit\nvalues. The resulting model alloys are subjected to massive Molecular Dynamics\nsimulations reproducing full complexity of plastic strength response in\nconcentrated single-phase body-centered cubic solid solutions. At variance with\nviews prevailing in the literature, our computational experiments show that\nstiffness misfit can contribute to SSS on par if not more than size misfit.\nFurthermore, depending on exactly how they are combined, the two misfits can\nresult in synergistic or antagonistic effect on alloy strengthening. In\ncontrast to real CCAs in which every constituent element comes with its\nspecific combination of atomic size and elastic stiffness, our alchemical model\nalloys sample the space of misfit parameters continuously thus augmenting the\nmuch more constrained and inevitably spotty experimental exploration of the CCA\ndesign space. Taking advantage of unique to our approach ability to define\nalloy misfit parameters, our computational study demonstrates how useful\ninsights can be gained from intentionally unrealistic alchemical models. Rather\nthan practical recommendation for alloy design, our computational experiments\nshould be regarded as a proving ground for further SSS theory development.",
        "Quantum computation with electron spin qubits requires coherent and efficient\nmanipulation of these spins, typically accomplished through the application of\nalternating magnetic or electric fields for electron spin resonance (ESR). In\nparticular, electrical driving allows us to apply localized fields on the\nelectrons, which benefits scale-up architectures. However, we have found that\nElectric Dipole Spin Resonance (EDSR) is insufficient for modeling the Rabi\nbehavior in recent experimental studies. Therefore, we propose that the\nelectron spin is being driven by a new method of electric spin qubit control\nwhich generalizes the spin dynamics by taking into account a quadrupolar\ncontribution of the quantum dot: electric quadrupole spin resonance (EQSR). In\nthis work, we explore the electric quadrupole driving of a quantum dot in\nsilicon, specifically examining the cases of 5 and 13 electron occupancies.",
        "We are interested in rationalizing the phenomenon of like-charge attraction\nbetween charged bodies, such as a pair of colloids, in the strong coupling\nregime. The two colloids are modelled as uniformly charged parallel plates,\nneutralized by mobile counterions. In an earlier work [Palaia et al., J. Phys.\nChem. B 126, 3143 (2022)], we developed an effective-field theory for symmetric\nplates, stemming from the ground-state description that holds at infinite\ncouplings. Here, we generalize the approach to the asymmetric case, where the\nplates bear charges of the same sign, but of different values. In the symmetric\nsituation, the mobile ions, which are localized in the vicinity of the two\nplates, share equally between both of them. Here, the sharing is non-trivial,\ndepending both on the coupling parameter and the distance between the plates.\nWe thus introduce a counterion occupation parameter, that is determined\nvariationally to ensure minimum of the free energy. The resulting analytical\nresults for the pressure as a function of the plate-plate distance $d$ agree\nwell with our Monte Carlo data, in a large interval of strong and intermediate\ncoupling constants $\\Xi$. We show in particular that within this description,\nthere exists a range of large distances at which the attractive pressure\nfeatures a $1\/d^2$ behavior.",
        "Skyrmions, topologically protected textures, have been observed in different\nfields of nanotechnology and have emerged as promising candidates for different\napplications due to their topological stability, low-power operation, and\ndynamic response to external stimuli. First introduced in particle physics,\nskyrmions have since been observed in different condensed matter fields,\nincluding magnetism, ferroelectricity, photonics, and acoustics. Their unique\ntopological properties enable robust manipulation and detection, paving the way\nfor innovative applications in room temperature sensing, storage, and\ncomputing. Recent advances in materials engineering and device integration have\ndemonstrated several strategies for an efficient manipulation of skyrmions,\naddressing key challenges in their practical implementation. In this review, we\nsummarize the state-of-the-art research on skyrmions across different\nplatforms, highlighting their fundamental properties and characteristics,\nrecent experimental breakthroughs, and technological potential. We present\nfuture perspectives and remaining challenges, emphasizing the interdisciplinary\nimpact of skyrmions on nanotechnology.",
        "The problem of constructing a dataset for MLIP development which gives the\nmaximum quality in the minimum amount of compute time is complex, and can be\napproached in a number of ways. We introduce a ``Bayesian selection\" approach\nfor selecting from a candidate set of structures, and compare the effectiveness\nof this method against other common approaches in the task of constructing\nideal datasets targeting Silicon surface energies. We show that the Bayesian\nselection method performs much better than Simple Random Sampling at this task\n(for example, the error on the (100) surface energy is 4.3x lower in the low\ndata regime), and is competitive with a variety of existing selection methods,\nusing ACE and MACE features.",
        "While amorphous materials are often approximated to have a statistically\nhomogeneous atomic structure, they frequently exhibit localized structural\nheterogeneity that challenges simplified models. This study uses 4D scanning\ntransmission electron microscopy to investigate the strain and structural\nmodifications around gas bubbles in amorphous Bi$_2$O$_3$ induced by argon\nirradiation. We present a method for determining strain fields surrounding\nbubbles that can be used to measure the internal pressure of the gas.\nCompressive strain is observed around the cavities, with higher-order\ncrystalline symmetries emerging near the cavity interfaces, suggesting\nparacrystalline ordering as a result of bubble coarsening. This ordering, along\nwith a compressive strain gradient, indicates that gas bubbles induce\nsignificant localized changes in atomic packing. By analyzing strain fields\nwith maximum compressive strains of 3\\%, we estimate a lower bound on the\ninternal pressure of the bubbles at 2.5 GPa. These findings provide insight\ninto the complex structural behavior of amorphous materials under stress,\nparticularly in systems with gas inclusions, and offer new methods for probing\nthe local atomic structure in disordered materials. Although considering\nstructural heterogeneity in amorphous systems is non-trivial, these features\nhave crucial impacts on material functionalities, such as mechanical strength,\nionic conductivity, and electronic mobility.",
        "The nonlinear coupling between stellar convection and rotation is of great\ninterest because it relates to understanding both stellar evolution and\nactivity. We investigated the influence of rotation and the Coriolis force on\nthe dynamics and thermodynamic structure of an F-type main-sequence star with a\nshallow outer convection zone. We performed a series of 3D radiative\nhydrodynamic simulations of a 1.47Msun star for different rotation rates\n(periods of rotation 1 and 14 days) and with computational domains placed at\nlatitudes of 0degrees (equator), 30degrees, and 60degrees. Because the star has\na relatively shallow convection zone (28.5 Mm thick or about 2.81% R*), we\nmodel its dynamics from the upper layers of the radiative zone, the whole\nconvection zone, and the low atmosphere. The simulation results show a weak\nshift of the ionization zones to the photosphere and a decrease of the stellar\nradius by about 29 km at the equator and about 58 km at higher latitudes in the\npresence of rotation with a period of 1 day. The models presented reveal the\nformation of radial differential rotation, meridional flows, latitude-dependent\nroll-like structures of convection, a tachocline, the presence of a\ngravity-darkening effect, and others. In this paper, we primarily discuss the\nproperties of the outer convection zone for different rotation rates. Detailed\nanalysis of the properties of the tachocline, the overshoot layer, and\nsmall-scale turbulence will be discussed in follow-on papers.",
        "The magneto-optical effects (MOEs), as a fundamental physical phenomenon, can\nreveal the electronic structures of materials. The related probing methods are\nwidely used in the study of magnetic materials. However, space-time inversion\n($\\mathcal{PT}$) symmetric antiferromagnets were previously believed to be\nmagneto-optically inactive. Here, we point out that this traditional\nunderstanding is incorrect. Based on our generic formulas and symmetry\nanalysis, we find that in $\\mathcal{PT}$-symmetric antiferromagnets, it is the\nquantum metric, i.e., the real part of the quantum geometry, that induces MOEs.\nCombining a tight-binding model and first-principles calculations, we confirm\nthis observation by showing MOEs in the $\\mathcal{PT}$-symmetric\nantiferromagnet. Our work demonstrates that $\\mathcal{PT}$-symmetric\nantiferromagnets previously thought to lack MOEs can indeed exhibit MOEs and\ngreatly broaden the research on MOEs."
      ]
    }
  },
  {
    "id":2412.20007,
    "research_type":"applied",
    "start_id":"b6",
    "start_title":"The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions",
    "start_abstract":"Training of neural networks for automated diagnosis pigmented skin lesions is hampered by the small size and lack diversity available datasets dermatoscopic images. We tackle this problem releasing HAM10000 (\"Human Against Machine with 10000 training images\") dataset. collected images from different populations acquired stored modalities. Given we had to apply acquisition cleaning methods developed semi-automatic workflows utilizing specifically trained networks. The final dataset consists 10015 which are released as a set academic machine learning purposes publicly through ISIC archive. This benchmark can be used comparisons human experts. Cases include representative collection all important diagnostic categories in realm lesions. More than 50% have been confirmed pathology, while ground truth rest cases was either follow-up, expert consensus, or confirmation in-vivo confocal microscopy.",
    "start_categories":[
      "Data"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning"
      ],
      "abstract":[
        "Deep learning tools have gained tremendous attention in applied machine learning. However such for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about uncertainty, but usually come with prohibitive computational cost. this paper we develop new theoretical casting dropout training deep neural networks (NNs) as approximate inference Gaussian processes. A direct result of theory gives us uncertainty NNs -- extracting information from existing that has been thrown away so far. This mitigates the problem representing without sacrificing either complexity or test accuracy. We perform an extensive study properties dropout's Various network architectures non-linearities are assessed on tasks classification, using MNIST example. show considerable improvement predictive log-likelihood RMSE compared state-of-the-art methods, finish by reinforcement"
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "$U(1)_A$ Breaking in Hot QCD in the Chiral Limit",
        "An approach for improving the distorted structured light in holographic\n  optical tweezers",
        "ChaosEater: Fully Automating Chaos Engineering with Large Language\n  Models",
        "Cute-Lock: Behavioral and Structural Multi-Key Logic Locking Using Time\n  Base Keys",
        "Exo-MerCat v2.0.0: updates and open-source release of the Exoplanet\n  Merged Catalog software",
        "CMB-S4: Foreground-Cleaning Pipeline Comparison for Measuring Primordial\n  Gravitational Waves",
        "DobLIX: A Dual-Objective Learned Index for Log-Structured Merge Trees",
        "Random quantum Ising model with three-spin couplings",
        "Online Housing Market",
        "Global existence of weak solutions to a cell migration and\n  (de)differentiation model with double haptotaxis in the context of tissue\n  regeneration",
        "Electronic structures and multi-orbital models of La$_3$Ni$_2$O$_7$ thin\n  films",
        "Amortized In-Context Bayesian Posterior Estimation",
        "CAT Pruning: Cluster-Aware Token Pruning For Text-to-Image Diffusion\n  Models",
        "Multiple-gain Estimation for Running Time of Evolutionary Combinatorial\n  Optimization",
        "Uncovering the Hidden Threat of Text Watermarking from Users with\n  Cross-Lingual Knowledge",
        "Fine-Grained Appropriate Reliance: Human-AI Collaboration with a\n  Multi-Step Transparent Decision Workflow for Complex Task Decomposition",
        "Bayesian Multifractal Image Segmentation",
        "Momentum tunnelling between nanoscale liquid flows",
        "Weihrauch problems as containers",
        "Homological stability for Hurwitz spaces and applications",
        "XTS mode revisited: high hopes for key scopes?",
        "Modality-Composable Diffusion Policy via Inference-Time\n  Distribution-level Composition",
        "Inverse Flow and Consistency Models",
        "Probing Spin-2 Ultralight Dark Matter with Space-based Gravitational\n  Wave Detectors in Millihertz",
        "Joint graphical model estimation using Stein-type shrinkage for fast\n  large scale network inference in scRNAseq data",
        "Predicting Air Temperature from Volumetric Urban Morphology with Machine\n  Learning",
        "Noise disturbance and lack of privacy: Modeling acoustic dissatisfaction\n  in open-plan offices",
        "RG-Attn: Radian Glue Attention for Multi-modality Multi-agent\n  Cooperative Perception",
        "Private Federated Learning In Real World Application -- A Case Study"
      ],
      "abstract":[
        "We propose a simple instanton-based random matrix model of hot QCD that in\nthe quenched case precisely reproduces the distribution of the lowest lattice\noverlap Dirac eigenvalues. Even after including dynamical quarks the model can\nbe easily simulated in volumes and for quark masses that will be out of reach\nfor direct lattice simulations in the foreseeable future. Our simulations show\nthat quantities connected to the $U(1)_A$ and $SU(N_f)_A$ chiral symmetry are\ndominated by eigenvalues in a peak of the spectral density that becomes\nsingular at zero in the thermodynamic limit. This spectral peak turns out to be\nproduced by an ideal instanton gas. By generalizing Banks-Casher type integrals\nfor the singular spectral density, definite predictions can be given for\nphysical quantities that are essential to test chiral symmetry breaking, but\npresently impossible to compute reliably with direct lattice simulations.",
        "Optical tweezers have been widely used for optical manipulation of various\nparticles. At present, there are different type of optical tweezers. Among\nthem, holographic optical tweezers have attracted growing attention as a\npowerful tools for optical trapping, optical transportation and optical sorting\nin many fields, due to its excellent properties including great flexibility and\nhigh convenience. Experimentally, however, the structured light has been easily\ndistorted, which would lead to serious degradation of optical manipulation\nperformance. In this work, the distortion of structured light is theoretically\nanalyzed. In the following, the distortion of structured light are numerically\nsimulated and experimentally measured. It shows that the simulated results are\nin consistent with the experimental ones. Then, an approach for decreasing its\noptical distortion is proposed, and the results reveal that the distortion of\nstructured light can be effectively corrected. Accordingly, our study provides\na way for improving the distorted structured light, which is useful for\noptically manipulating various particles in optical tweezers.",
        "Chaos Engineering (CE) is an engineering technique aimed at improving the\nresiliency of distributed systems. It involves artificially injecting specific\nfailures into a distributed system and observing its behavior in response.\nBased on the observation, the system can be proactively improved to handle\nthose failures. Recent CE tools realize the automated execution of predefined\nCE experiments. However, defining these experiments and reconfiguring the\nsystem after the experiments still remain manual. To reduce the costs of the\nmanual operations, we propose \\textsc{ChaosEater}, a \\textit{system} for\nautomating the entire CE operations with Large Language Models (LLMs). It\npre-defines the general flow according to the systematic CE cycle and assigns\nsubdivided operations within the flow to LLMs. We assume systems based on\nInfrastructure as Code (IaC), wherein the system configurations and artificial\nfailures are managed through code. Hence, the LLMs' operations in our\n\\textit{system} correspond to software engineering tasks, including requirement\ndefinition, code generation and debugging, and testing. We validate our\n\\textit{system} through case studies on both small and large systems. The\nresults demonstrate that our \\textit{system} significantly reduces both time\nand monetary costs while completing reasonable single CE cycles.",
        "The outsourcing of semiconductor manufacturing raises security risks, such as\npiracy and overproduction of hardware intellectual property. To overcome this\nchallenge, logic locking has emerged to lock a given circuit using additional\nkey bits. While single-key logic locking approaches have demonstrated serious\nvulnerability to a wide range of attacks, multi-key solutions, if carefully\ndesigned, can provide a reliable defense against not only oracle-guided logic\nattacks, but also removal and dataflow attacks. In this paper, using time base\nkeys, we propose, implement and evaluate a family of secure multi-key logic\nlocking algorithms called Cute-Lock that can be applied both in RTL-level\nbehavioral and netlist-level structural representations of sequential circuits.\nOur extensive experimental results under a diverse range of attacks confirm\nthat, compared to vulnerable state-of-the-art methods, employing the Cute-Lock\nfamily drives attacking attempts to a dead end without additional overhead.",
        "Exoplanet research is at the forefront of contemporary astronomy\nrecommendations. As more and more exoplanets are discovered and vetted,\ndatabases and catalogs are built to collect information. Various resources are\navailable to scientists for this purpose, though every one of them has\ndifferent scopes and notations. In Alei et al. (2020) we described Exo-MerCat,\na script that collects information from multiple sources and creates a\nhomogenized table. In this manuscript, we announce the release of the\nExo-MerCat v2.0.0 script as an upgraded, tested, documented and open-source\nsoftware to produce catalogs. The main upgrades on the script concern: 1) the\naddition of the TESS Input Catalog and the K2 Input Catalog as input sources;\n2) the optimization of the main identifier queries; 3) a more complex merging\nof the entries from the input sources into the final catalog; 4) some\nquality-of-life improvements such as informative flags, more user-friendly\ncolumn headers, and log files; 5) the refactoring of the code in modules. We\ncompare the performance of Exo-MerCat v2.0.0 with the previous version and\nnotice a substantial improvement in the completeness of the sample, thanks to\nthe addition of new input sources, and its accuracy, because of the\noptimization of the script.",
        "We compare multiple foreground-cleaning pipelines for estimating the\ntensor-to-scalar ratio, $r$, using simulated maps of the planned CMB-S4\nexperiment within the context of the South Pole Deep Patch. To evaluate\nrobustness, we analyze bias and uncertainty on $r$ across various foreground\nsuites using map-based simulations. The foreground-cleaning methods include: a\nparametric maximum likelihood approach applied to auto- and cross-power spectra\nbetween frequency maps; a map-based parametric maximum-likelihood method; and a\nharmonic-space internal linear combination using frequency maps. We summarize\nthe conceptual basis of each method to highlight their similarities and\ndifferences. To better probe the impact of foreground residuals, we implement\nan iterative internal delensing step, leveraging a map-based pipeline to\ngenerate a lensing $B$-mode template from the Large Aperture Telescope\nfrequency maps. Our results show that the performance of the three approaches\nis comparable for simple and intermediate-complexity foregrounds, with\n$\\sigma(r)$ ranging from 3 to 5 $\\times 10^{-4}$. However, biases at the\n$1-2\\sigma$ level appear when analyzing more complex forms of foreground\nemission. By extending the baseline pipelines to marginalize over foreground\nresiduals, we demonstrate that contamination can be reduced to within\nstatistical uncertainties, albeit with a pipeline-dependent impact on\n$\\sigma(r)$, which translates to a detection significance between 2 and\n4$\\sigma$ for an input value of $r = 0.003$. These findings suggest varying\nlevels of maturity among the tested pipelines, with the auto- and\ncross-spectra-based approach demonstrating the best stability and overall\nperformance. Moreover, given the extremely low noise levels, mutual validation\nof independent foreground-cleaning pipelines is essential to ensure the\nrobustness of any potential detection.",
        "In this paper, we introduce DobLIX, a dual-objective learned index\nspecifically designed for Log-Structured Merge(LSM) tree-based key-value\nstores. Although traditional learned indexes focus exclusively on optimizing\nindex lookups, they often overlook the impact of data access from storage,\nresulting in performance bottlenecks. DobLIX addresses this by incorporating a\nsecond objective, data access optimization, into the learned index training\nprocess. This dual-objective approach ensures that both index lookup efficiency\nand data access costs are minimized, leading to significant improvements in\nread performance while maintaining write efficiency in real-world LSM-tree\nsystems. Additionally, DobLIX features a reinforcement learning agent that\ndynamically tunes the system parameters, allowing it to adapt to varying\nworkloads in real-time. Experimental results using real-world datasets\ndemonstrate that DobLIX reduces indexing overhead and improves throughput by\n1.19 to 2.21 times compared to state-of-the-art methods within RocksDB, a\nwidely used LSM-tree-based storage engine.",
        "We apply a real-space block renormalization group approach to study the\ncritical properties of the random transverse-field Ising spin chain with\nmultispin interactions. First we recover the known properties of the\ntraditional model with two-spin interactions by applying the renormalization\napproach for arbitrary size of the block. For the model with three-spin\ncouplings we calculate the critical point and demonstrate that the phase\ntransition is controlled by an infinite disorder fixed point. We have\ndetermined the typical correlation-length critical exponent, which seems to be\ndifferent from that of the random transverse Ising chain with nearest-neighbor\ncouplings. Thus this model represents a new infinite disorder universality\nclass.",
        "This paper studies an online variant of the celebrated housing market\nproblem, where each agent has a single house and seeks to exchange it for\nanother based on her preferences. In this online setting, agents may arrive and\ndepart at any time, meaning that not all agents are present on the housing\nmarket simultaneously. I extend the well known serial dictatorship and Gale s\ntop trading cycle mechanisms to this online scenario, aiming to retain their\ndesirable properties such as Pareto efficiency, individual rationality, and\nstrategy proofness. These extensions also seek to prevent agents from\nstrategically delaying their arrival or advancing their departure. I\ndemonstrate that achieving all of these properties simultaneously is impossible\nin the online context, and I present several variants that achieve different\nsubsets of these properties.",
        "We study a model for the spread and (de)differentiation of mesenchymal stem\ncells and chondrocytes in a scaffold whose fibers are coated with hyaluron. The\nchondrocytes produce new extracellular matrix, which, together with hyaluron,\nserves as haptotactic cue for the stem cell migration. We prove global\nexistence of weak solutions of the corresponding cross-diffusion system with\ndouble haptotaxis.",
        "The discovery of superconductivity with $T_c$ exceeding 40 K in\nLa$_3$Ni$_2$O$_7$ and (La,Pr)$_{3}$Ni$_2$O$_7$ thin films at ambient pressure\nmarks a significant breakthrough in nickelate superconductors. Using density\nfunctional theory (DFT), we propose the double-stacked two-orbital effective\nmodels for La$_3$Ni$_2$O$_7$ thin films, based on the Ni$-e_g$ orbitals. Our\nanalysis reveals the presence of three electron pockets\n$\\alpha,\\alpha^{\\prime},\\beta$ and two hole pockets $\\gamma,\\gamma^{\\prime}$ on\nthe Fermi surface, where the additional pockets $\\alpha^{\\prime}$ and\n$\\gamma^{\\prime}$ emerge due to inter-stack interactions. Furthermore, we\nconstruct higher-energy models incorporating O-$p$ orbitals to facilitate\nfurther investigations. The spin susceptibility, calculated within the random\nphase approximation (RPA), indicates enhanced magnetic correlations primarily\ndriven by nesting effects of the $\\gamma$ pocket, which is predominantly\ncontributed by the Ni$-d_{z^2}$ orbital. These models provide fundamental\nframework for further theoretical and experimental studies, offering critical\ninsights into the superconducting mechanism of La$_3$Ni$_2$O$_7$ thin films.",
        "Bayesian inference provides a natural way of incorporating prior beliefs and\nassigning a probability measure to the space of hypotheses. Current solutions\nrely on iterative routines like Markov Chain Monte Carlo (MCMC) sampling and\nVariational Inference (VI), which need to be re-run whenever new observations\nare available. Amortization, through conditional estimation, is a viable\nstrategy to alleviate such difficulties and has been the guiding principle\nbehind simulation-based inference, neural processes and in-context methods\nusing pre-trained models. In this work, we conduct a thorough comparative\nanalysis of amortized in-context Bayesian posterior estimation methods from the\nlens of different optimization objectives and architectural choices. Such\nmethods train an amortized estimator to perform posterior parameter inference\nby conditioning on a set of data examples passed as context to a sequence model\nsuch as a transformer. In contrast to language models, we leverage permutation\ninvariant architectures as the true posterior is invariant to the ordering of\ncontext examples. Our empirical study includes generalization to\nout-of-distribution tasks, cases where the assumed underlying model is\nmisspecified, and transfer from simulated to real problems. Subsequently, it\nhighlights the superiority of the reverse KL estimator for predictive problems,\nespecially when combined with the transformer architecture and normalizing\nflows.",
        "Diffusion models have revolutionized generative tasks, especially in the\ndomain of text-to-image synthesis; however, their iterative denoising process\ndemands substantial computational resources. In this paper, we present a novel\nacceleration strategy that integrates token-level pruning with caching\ntechniques to tackle this computational challenge. By employing noise relative\nmagnitude, we identify significant token changes across denoising iterations.\nAdditionally, we enhance token selection by incorporating spatial clustering\nand ensuring distributional balance. Our experiments demonstrate reveal a\n50%-60% reduction in computational costs while preserving the performance of\nthe model, thereby markedly increasing the efficiency of diffusion models. The\ncode is available at https:\/\/github.com\/ada-cheng\/CAT-Pruning",
        "The running-time analysis of evolutionary combinatorial optimization is a\nfundamental topic in evolutionary computation. Its current research mainly\nfocuses on specific algorithms for simplified problems due to the challenge\nposed by fluctuating fitness values. This paper proposes a multiple-gain model\nto estimate the fitness trend of population during iterations. The proposed\nmodel is an improved version of the average gain model, which is the approach\nto estimate the running time of evolutionary algorithms for numerical\noptimization. The improvement yields novel results of evolutionary\ncombinatorial optimization, including a briefer proof for the time complexity\nupper bound in the case of (1+1) EA for the Onemax problem, two tighter time\ncomplexity upper bounds than the known results in the case of (1+$\\lambda$) EA\nfor the knapsack problem with favorably correlated weights and a closed-form\nexpression of time complexity upper bound in the case of (1+$\\lambda$) EA for\ngeneral $k$-MAX-SAT problems. The results indicate that the practical running\ntime aligns with the theoretical results, verifying that the multiple-gain\nmodel is more general for running-time analysis of evolutionary combinatorial\noptimization than state-of-the-art methods.",
        "In this study, we delve into the hidden threats posed to text watermarking by\nusers with cross-lingual knowledge. While most research focuses on watermarking\nmethods for English, there is a significant gap in evaluating these methods in\ncross-lingual contexts. This oversight neglects critical adversary scenarios\ninvolving cross-lingual users, creating uncertainty regarding the effectiveness\nof cross-lingual watermarking. We assess four watermarking techniques across\nfour linguistically rich languages, examining watermark resilience and text\nquality across various parameters and attacks. Our focus is on a realistic\nscenario featuring adversaries with cross-lingual expertise, evaluating the\nadequacy of current watermarking methods against such challenges.",
        "In recent years, the rapid development of AI systems has brought about the\nbenefits of intelligent services but also concerns about security and\nreliability. By fostering appropriate user reliance on an AI system, both\ncomplementary team performance and reduced human workload can be achieved.\nPrevious empirical studies have extensively analyzed the impact of factors\nranging from task, system, and human behavior on user trust and appropriate\nreliance in the context of one-step decision making. However, user reliance on\nAI systems in tasks with complex semantics that require multi-step workflows\nremains under-explored. Inspired by recent work on task decomposition with\nlarge language models, we propose to investigate the impact of a novel\nMulti-Step Transparent (MST) decision workflow on user reliance behaviors. We\nconducted an empirical study (N = 233) of AI-assisted decision making in\ncomposite fact-checking tasks (i.e., fact-checking tasks that entail multiple\nsub-fact verification steps). Our findings demonstrate that human-AI\ncollaboration with an MST decision workflow can outperform one-step\ncollaboration in specific contexts (e.g., when advice from an AI system is\nmisleading). Further analysis of the appropriate reliance at fine-grained\nlevels indicates that an MST decision workflow can be effective when users\ndemonstrate a relatively high consideration of the intermediate steps. Our work\nhighlights that there is no one-size-fits-all decision workflow that can help\nobtain optimal human-AI collaboration. Our insights help deepen the\nunderstanding of the role of decision workflows in facilitating appropriate\nreliance. We synthesize important implications for designing effective means to\nfacilitate appropriate reliance on AI systems in composite tasks, positioning\nopportunities for the human-centered AI and broader HCI communities.",
        "Multifractal analysis (MFA) provides a framework for the global\ncharacterization of image textures by describing the spatial fluctuations of\ntheir local regularity based on the multifractal spectrum. Several works have\nshown the interest of using MFA for the description of homogeneous textures in\nimages. Nevertheless, natural images can be composed of several textures and,\nin turn, multifractal properties associated with those textures. This paper\nintroduces a Bayesian multifractal segmentation method to model and segment\nmultifractal textures by jointly estimating the multifractal parameters and\nlabels on images. For this, a computationally and statistically efficient\nmultifractal parameter estimation model for wavelet leaders is firstly\ndeveloped, defining different multifractality parameters to different regions\nof an image. Then, a multiscale Potts Markov random field is introduced as a\nprior to model the inherent spatial and scale correlations between the labels\nof the wavelet leaders. A Gibbs sampling methodology is employed to draw\nsamples from the posterior distribution of the parameters. Numerical\nexperiments are conducted on synthetic multifractal images to evaluate the\nperformance of the proposed segmentation approach. The proposed method achieves\nsuperior performance compared to traditional unsupervised segmentation\ntechniques as well as modern deep learning-based approaches, showing its\neffectiveness for multifractal image segmentation.",
        "The world of nanoscales in fluidics is the frontier where the continuum of\nfluid mechanics meets the atomic, and even quantum, nature of matter. While\nwater dynamics remains largely classical under extreme confinement, several\nexperiments have recently reported coupling between water transport and the\nelectronic degrees of freedom of the confining materials. This avenue prompts\nus to reconsider nanoscale hydrodynamic flows under the perspective of\ninteracting excitations, akin to condensed matter frameworks. Here we show,\nusing a combination of many-body theory and molecular simulations, that the\nflow of a liquid can induce the flow of another liquid behind a separating\nwall, at odds with the prediction of continuum hydrodynamics. We further show\nthat the range of this 'flow tunnelling' can be tuned through the solid's\nelectronic excitations, with a maximum occurring when these are at resonance\nwith the liquid's charge density fluctuations. Flow tunnelling is expected to\nplay a role in global transport across nanoscale fluidic networks, such as\nlamellar graphene oxide or MXene membranes. It further suggests exploiting the\nelectronic properties of the confining walls for manipulating liquids via their\ndielectric spectra, beyond the nature and characteristics of individual\nmolecules.",
        "We note that Weihrauch problems can be regarded as containers over the\ncategory of projective represented spaces and that Weihrauch reductions\ncorrespond exactly to container morphisms. We also show that Bauer's extended\nWeihrauch degrees and the posetal reflection of containers over partition\nassemblies are equivalent. Using this characterization, we show how a number of\noperators over Weihrauch degrees, such as the composition product, also arise\nnaturally from the abstract theory of polynomial functors.",
        "We show the homology of the Hurwitz space associated to an arbitrary finite\nrack stabilizes integrally in a suitable sense. We also compute the dominant\npart of its stable homology after inverting finitely many primes. This proves a\nconjecture of Ellenberg--Venkatesh--Westerland and improves upon our previous\nresults for non-splitting racks. We obtain applications to Malle's conjecture,\nthe Picard rank conjecture, and the Cohen--Lenstra--Martinet heuristics.",
        "This paper concisely summarizes the XTS block encryption mode for storage\nsector-based encryption applications and clarifies its limitations. In\nparticular, we aim to provide a unified basis for much needed discussions about\nthe newly proposed key scope change to the IEEE 1619 standard.",
        "Diffusion Policy (DP) has attracted significant attention as an effective\nmethod for policy representation due to its capacity to model\nmulti-distribution dynamics. However, current DPs are often based on a single\nvisual modality (e.g., RGB or point cloud), limiting their accuracy and\ngeneralization potential. Although training a generalized DP capable of\nhandling heterogeneous multimodal data would enhance performance, it entails\nsubstantial computational and data-related costs. To address these challenges,\nwe propose a novel policy composition method: by leveraging multiple\npre-trained DPs based on individual visual modalities, we can combine their\ndistributional scores to form a more expressive Modality-Composable Diffusion\nPolicy (MCDP), without the need for additional training. Through extensive\nempirical experiments on the RoboTwin dataset, we demonstrate the potential of\nMCDP to improve both adaptability and performance. This exploration aims to\nprovide valuable insights into the flexible composition of existing DPs,\nfacilitating the development of generalizable cross-modality, cross-domain, and\neven cross-embodiment policies. Our code is open-sourced at\nhttps:\/\/github.com\/AndyCao1125\/MCDP.",
        "Inverse generation problems, such as denoising without ground truth\nobservations, is a critical challenge in many scientific inquiries and\nreal-world applications. While recent advances in generative models like\ndiffusion models, conditional flow matching, and consistency models achieved\nimpressive results by casting generation as denoising problems, they cannot be\ndirectly used for inverse generation without access to clean data. Here we\nintroduce Inverse Flow (IF), a novel framework that enables using these\ngenerative models for inverse generation problems including denoising without\nground truth. Inverse Flow can be flexibly applied to nearly any continuous\nnoise distribution and allows complex dependencies. We propose two algorithms\nfor learning Inverse Flows, Inverse Flow Matching (IFM) and Inverse Consistency\nModel (ICM). Notably, to derive the computationally efficient, simulation-free\ninverse consistency model objective, we generalized consistency training to any\nforward diffusion processes or conditional flows, which have applications\nbeyond denoising. We demonstrate the effectiveness of IF on synthetic and real\ndatasets, outperforming prior approaches while enabling noise distributions\nthat previous methods cannot support. Finally, we showcase applications of our\ntechniques to fluorescence microscopy and single-cell genomics data,\nhighlighting IF's utility in scientific problems. Overall, this work expands\nthe applications of powerful generative models to inversion generation\nproblems.",
        "Spin-2 ultralight dark matter (ULDM) is a viable dark matter candidate and it\ncan be constrained using gravitational wave (GW) observations. In this paper,\nwe investigate the detectability of spin-2 ULDM by space-based GW\ninterferometers. By considering a direct coupling between spin-2 ULDM and\nordinary matter, we derive the corresponding response functions and sensitivity\ncurves for various time-delay interferometry channels and calculate the optimal\nsensitivity curves for future millihertz GW detectors. Our results demonstrate\nthat the space-based detectors can place stringent constraints on the coupling\nconstant of spin-2 ULDM, reaching $\\alpha \\sim 10^{-10}$ around a mass of $m\n\\sim 10^{-17} \\rm eV$, surpassing current limits from ground-based detectors\nand pulsar timing arrays. Thus, the space-based GW detectors can serve as\npowerful tools not only for detecting GWs but also for probing fundamental\nproperties of ultralight dark matter.",
        "Graphical modeling is a widely used tool for analyzing conditional\ndependencies between variables and traditional methods may struggle to capture\nshared and distinct structures in multi-group or multi-condition settings.\nJoint graphical modeling (JGM) extends this framework by simultaneously\nestimating network structures across multiple related datasets, allowing for a\ndeeper understanding of commonalities and differences. This capability is\nparticularly valuable in fields such as genomics and neuroscience, where\nidentifying variations in network topology can provide critical biological\ninsights. Existing JGM methodologies largely fall into two categories:\nregularization-based approaches, which introduce additional penalties to\nenforce structured sparsity, and Bayesian frameworks, which incorporate prior\nknowledge to improve network inference. In this study, we explore an\nalternative method based on two-target linear covariance matrix shrinkage.\nFormula for optimal shrinkage intensities is proposed which leads to the\ndevelopment of JointStein framework. Performance of JointStein framework is\nproposed through simulation benchmarking which demonstrates its effectiveness\nfor large-scale single-cell RNA sequencing (scRNA-seq) data analysis. Finally,\nwe apply our approach to glioblastoma scRNA-seq data, uncovering dynamic shifts\nin T cell network structures across disease progression stages. The result\nhighlights potential of JointStein framework in extracting biologically\nmeaningful insights from high-dimensional data.",
        "In this study, we firstly introduce a method that converts CityGML data into\nvoxels which works efficiently and fast in high resolution for large scale\ndatasets such as cities but by sacrificing some building details to overcome\nthe limitations of previous voxelization methodologies that have been\ncomputationally intensive and inefficient at transforming large-scale urban\nareas into voxel representations for high resolution. Those voxelized 3D city\ndata from multiple cities and corresponding air temperature data are used to\ndevelop a machine learning model. Before the model training, Gaussian blurring\nis implemented on input data to consider spatial relationships, as a result the\ncorrelation rate between air temperature and volumetric building morphology is\nalso increased after the Gaussian blurring. After the model training, the\nprediction results are not just evaluated with Mean Square Error (MSE) but some\nimage similarity metrics such as Structural Similarity Index Measure (SSIM) and\nLearned Perceptual Image Patch Similarity (LPIPS) that are able to detect and\nconsider spatial relations during the evaluation process. This trained model is\ncapable of predicting the spatial distribution of air temperature by using\nbuilding volume information of corresponding pixel as input. By doing so, this\nresearch aims to assist urban planners in incorporating environmental\nparameters into their planning strategies, thereby facilitating more\nsustainable and inhabitable urban environments.",
        "Open-plan offices are well-known to be adversely affected by acoustic issues.\nThis study aims to model acoustic dissatisfaction using measurements of room\nacoustics, sound environment during occupancy, and occupant surveys (n = 349)\nin 28 offices representing a diverse range of workplace parameters. As latent\nfactors, the contribution of $\\textit{lack of privacy}$ (LackPriv) was 25%\nhigher than $\\textit{noise disturbance}$ (NseDstrb) in predicting\n$\\textit{acoustic dissatisfaction}$ (AcDsat). Room acoustic metrics based on\nsound pressure level (SPL) decay of speech ($L_{\\text{p,A,s,4m}}$ and\n$r_{\\text{C}}$) were better in predicting these factors than distraction\ndistance ($r_{\\text{D}}$) based on speech transmission index. This contradicts\nprevious findings, and the trends for SPL-based metrics in predicting AcDsat\nand LackPriv go against expectations based on ISO 3382-3. For sound during\noccupation, $L_{\\text{A,90}}$ and psychoacoustic loudness ($N_{\\text{90}}$)\npredicted AcDsat, and a SPL fluctuation metric ($M_{\\text{A,eq}}$) predicted\nLackPriv. However, these metrics were weaker predictors than ISO 3382-3\nmetrics. Medium-sized offices exhibited higher dissatisfaction than larger\n($\\geq$50 occupants) offices. Dissatisfaction varied substantially across\nparameters including ceiling heights, number of workstations, and years of\nwork, but not between offices with fixed seating compared to more flexible and\nactivity-based working configurations. Overall, these findings highlight the\ncomplexities in characterizing occupants' perceptions using instrumental\nacoustic measurements.",
        "Cooperative perception offers an optimal solution to overcome the perception\nlimitations of single-agent systems by leveraging Vehicle-to-Everything (V2X)\ncommunication for data sharing and fusion across multiple agents. However, most\nexisting approaches focus on single-modality data exchange, limiting the\npotential of both homogeneous and heterogeneous fusion across agents. This\noverlooks the opportunity to utilize multi-modality data per agent, restricting\nthe system's performance. In the automotive industry, manufacturers adopt\ndiverse sensor configurations, resulting in heterogeneous combinations of\nsensor modalities across agents. To harness the potential of every possible\ndata source for optimal performance, we design a robust LiDAR and camera\ncross-modality fusion module, Radian-Glue-Attention (RG-Attn), applicable to\nboth intra-agent cross-modality fusion and inter-agent cross-modality fusion\nscenarios, owing to the convenient coordinate conversion by transformation\nmatrix and the unified sampling\/inversion mechanism. We also propose two\ndifferent architectures, named Paint-To-Puzzle (PTP) and\nCo-Sketching-Co-Coloring (CoS-CoCo), for conducting cooperative perception. PTP\naims for maximum precision performance and achieves smaller data packet size by\nlimiting cross-agent fusion to a single instance, but requiring all\nparticipants to be equipped with LiDAR. In contrast, CoS-CoCo supports agents\nwith any configuration-LiDAR-only, camera-only, or LiDAR-camera-both,\npresenting more generalization ability. Our approach achieves state-of-the-art\n(SOTA) performance on both real and simulated cooperative perception datasets.\nThe code will be released at GitHub in early 2025.",
        "This paper presents an implementation of machine learning model training\nusing private federated learning (PFL) on edge devices. We introduce a novel\nframework that uses PFL to address the challenge of training a model using\nusers' private data. The framework ensures that user data remain on individual\ndevices, with only essential model updates transmitted to a central server for\naggregation with privacy guarantees. We detail the architecture of our app\nselection model, which incorporates a neural network with attention mechanisms\nand ambiguity handling through uncertainty management. Experiments conducted\nthrough off-line simulations and on device training demonstrate the feasibility\nof our approach in real-world scenarios. Our results show the potential of PFL\nto improve the accuracy of an app selection model by adapting to changes in\nuser behavior over time, while adhering to privacy standards. The insights\ngained from this study are important for industries looking to implement PFL,\noffering a robust strategy for training a predictive model directly on edge\ndevices while ensuring user data privacy."
      ]
    }
  },
  {
    "id":2411.05188,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"Limitations of conventional magnetic resonance imaging as a predictor of death or disability following neonatal hypoxic-ischemic encephalopathy in the late hypothermia trial",
    "start_abstract":"Objective: To investigate if magnetic resonance imaging (MRI) is an accurate predictor for death or moderate-severe disability at 18-22 months of age among infants with neonatal encephalopathy in a trial of cooling initiated at 6-24 hours. Study design: Subgroup analysis of infants \u226536 weeks of gestation with moderate-severe neonatal encephalopathy randomized at 6-24 postnatal hours to hypothermia or usual care in a multicenter trial of late hypothermia. MRI scans were performed per each center's practice and interpreted by 2 central readers using the Eunice Kennedy Shriver National Institute of Child Health and Human Development injury score (6 levels, normal to hemispheric devastation). Neurodevelopmental outcomes were assessed at 18-22 months of age. Results: Of 168 enrollees, 128 had an interpretable MRI and were seen in follow-up (n = 119) or died (n = 9). MRI findings were predominantly acute injury and did not differ by cooling treatment. At 18-22 months, death or severe disability occurred in 20.3%. No infant had moderate disability. Agreement between central readers was moderate (weighted kappa 0.56, 95% CI 0.45-0.67). The adjusted odds of death or severe disability increased 3.7-fold (95% CI 1.8-7.9) for each increment of injury score. The area under the curve for severe MRI patterns to predict death or severe disability was 0.77 and the positive and negative predictive values were 36% and 100%, respectively. Conclusions: MRI injury scores were associated with neurodevelopmental outcome at 18-22 months among infants in the Late Hypothermia Trial. However, the results suggest caution when using qualitative interpretations of MRI images to provide prognostic information to families following perinatal hypoxia-ischemia.",
    "start_categories":[
      "Pediatrics"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b7"
      ],
      "title":[
        "What is being transferred in transfer learning?"
      ],
      "abstract":[
        "One desired capability for machines is the ability to transfer their knowledge of one domain another where data (usually) scarce. Despite ample adaptation learning in various deep applications, we yet do not understand what enables a successful and which part network responsible that. In this paper, provide new tools analyses address these fundamental questions. Through series on transferring block-shuffled images, separate effect feature reuse from low-level statistics show that some benefit comes latter. We present when training pre-trained weights, model stays same basin loss landscape different instances such are similar space close parameter space."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "Single-Ferroelectric Memcapacitor-Based Time-Domain Content-Addressable\n  Memory for Highly Precise Distance Function Computation",
        "Brain-inspired sparse training enables Transformers and LLMs to perform\n  as fully connected",
        "Exploring the encoding of linguistic representations in the\n  Fully-Connected Layer of generative CNNs for Speech",
        "A Note on Mixed Cages of Girth 5",
        "Modeling the impact of hospitalization-induced behavioral changes on\n  SARS-COV-2 spread in New York City",
        "Doped resonating valence bond states: How robust are the spin ice phases\n  in 3D Rydberg arrays",
        "FlakeRanker: Automated Identification and Prioritization of Flaky Job\n  Failure Categories",
        "$K_{2,3}$-induced minor-free graphs admit quasi-isometry with additive\n  distortion to graphs of tree-width at most two",
        "Cryogenic operation of silicon photomultiplier arrays",
        "GLaM-Sign: Greek Language Multimodal Lip Reading with Integrated Sign\n  Language Accessibility",
        "Invitation to the subpath number",
        "Mono-lepton Signature of a Neutrino-philic Dark Fermion at Hadron\n  Colliders",
        "Learning Accurate Models on Incomplete Data with Minimal Imputation",
        "Eruptive YSOs in Cygnus-X: a mid-infrared variability study with NEOWISE\n  and SPICY",
        "Declarative Application Management in the Fog. A bacteria-inspired\n  decentralised approach",
        "Chiral and deconfinement thermal transitions at finite quark spin\n  polarization in lattice QCD simulations",
        "Variational Quantum Optimization with Continuous Bandits",
        "AD-L-JEPA: Self-Supervised Spatial World Models with Joint Embedding\n  Predictive Architecture for Autonomous Driving with LiDAR Data",
        "The Massive and Distant Clusters of $WISE$ Survey. XII. Exploring X-ray\n  AGN in Dynamically Active Massive Galaxy Clusters at z~1",
        "Spectral proper orthogonal decomposition using sub-Nyquist rate data",
        "Robotic CBCT Meets Robotic Ultrasound",
        "Finding Needles in Emb(a)dding Haystacks: Legal Document Retrieval via\n  Bagging and SVR Ensembles",
        "On Beating $2^n$ for the Closest Vector Problem",
        "A Supplement to the anticanonical Volumes of weak $\\mathbb{Q}$-Fano\n  threefolds of Picard rank two",
        "Interaction of Electromagnetic Radiation with Cometary Dust",
        "Various approaches to solving nonlinear equations",
        "Matrix weighted inequalities for fractional type integrals associated to\n  operators with new classes of weights",
        "Are Multilingual Language Models an Off-ramp for Under-resourced\n  Languages? Will we arrive at Digital Language Equality in Europe in 2030?",
        "Effects of particle elongation on dense granular flows down a rough\n  inclined plane"
      ],
      "abstract":[
        "Single ferroelectric memcapacitor-based time-domain (TD) content-addressable\nmemory (CAM) is proposed and experimentally demonstrated for high reliability\nand density. The proposed TD CAM features the symmetric capacitance-voltage\ncharacteristics of a ferroelectric memcapacitor with a gated p-i-n diode\nstructure. This CAM performs search operations based on the variable\ncapacitance of cells. The propagation delay of the TD CAM output signal is\nlinearly correlated with the Hamming distance (HD) between input and output\nvectors. The proposed TD CAM array exhibits exceptional reliability in HD\ncomputation and in-memory search tasks owing to this linearity, considerably\noutperforming the conventional nonlinear voltage-domain CAM.",
        "This study aims to enlarge our current knowledge on application of\nbrain-inspired network science principles for training artificial neural\nnetworks (ANNs) with sparse connectivity. Dynamic sparse training (DST) can\nreduce the computational demands in ANNs, but faces difficulties to keep peak\nperformance at high sparsity levels. The Cannistraci-Hebb training (CHT) is a\nbrain-inspired method for growing connectivity in DST. CHT leverages a\ngradient-free, topology-driven link regrowth, which has shown ultra-sparse (1%\nconnectivity or lower) advantage across various tasks compared to fully\nconnected networks. Yet, CHT suffers two main drawbacks: (i) its time\ncomplexity is O(Nd^3) - N node network size, d node degree - hence it can apply\nonly to ultra-sparse networks. (ii) it selects top link prediction scores,\nwhich is inappropriate for the early training epochs, when the network presents\nunreliable connections. We propose a GPU-friendly approximation of the CH link\npredictor, which reduces the computational complexity to O(N^3), enabling a\nfast implementation of CHT in large-scale models. We introduce the\nCannistraci-Hebb training soft rule (CHTs), which adopts a strategy for\nsampling connections in both link removal and regrowth, balancing the\nexploration and exploitation of network topology. To improve performance, we\nintegrate CHTs with a sigmoid gradual density decay (CHTss). Empirical results\nshow that, using 1% of connections, CHTs outperforms fully connected networks\nin MLP on visual classification tasks, compressing some networks to < 30%\nnodes. Using 5% of the connections, CHTss outperforms fully connected networks\nin two Transformer-based machine translation tasks. Using 30% of the\nconnections, CHTss achieves superior performance compared to other dynamic\nsparse training methods in language modeling, and it surpasses the fully\nconnected counterpart in zero-shot evaluations.",
        "Interpretability work on the convolutional layers of CNNs has primarily\nfocused on computer vision, but some studies also explore correspondences\nbetween the latent space and the output in the audio domain. However, it has\nnot been thoroughly examined how acoustic and linguistic information is\nrepresented in the fully connected (FC) layer that bridges the latent space and\nconvolutional layers. The current study presents the first exploration of how\nthe FC layer of CNNs for speech synthesis encodes linguistically relevant\ninformation. We propose two techniques for exploration of the fully connected\nlayer. In Experiment 1, we use weight matrices as inputs into convolutional\nlayers. In Experiment 2, we manipulate the FC layer to explore how\nsymbolic-like representations are encoded in CNNs. We leverage the fact that\nthe FC layer outputs a feature map and that variable-specific weight matrices\nare temporally structured to (1) demonstrate how the distribution of learned\nweights varies between latent variables in systematic ways and (2) demonstrate\nhow manipulating the FC layer while holding constant subsequent model\nparameters affects the output. We ultimately present an FC manipulation that\ncan output a single segment. Using this technique, we show that lexically\nspecific latent codes in generative CNNs (ciwGAN) have shared lexically\ninvariant sublexical representations in the FC-layer weights, showing that\nciwGAN encodes lexical information in a linguistically principled manner.",
        "A mixed regular graph is a graph where every vertex has $z$ incoming arcs,\n$z$ outgoing arcs, and $r$ edges; furthermore, if it has girth $g$, we say that\nthe graph is a \\emph{$[z,r;g]$-mixed graph}. A \\emph{$[z,r;g]$-mixed cage} is a\n$[z,r;g]$-mixed graph with the smallest possible order. In this note, we give a\nfamily of $[z,q;5]$-mixed graphs for $q\\geq 7$ power of prime and $q-1\\leq\n4z+R$ with $z\\geq 1$ and $R \\in \\{1,\\ldots,5\\}$. This provides better upper\nbounds on the order of mixed cages until this moment.",
        "A novel behavior-epidemiology model, which considers $n$ heterogeneous\nbehavioral groups based on level of risk tolerance and distinguishes behavioral\nchanges by social and disease-related motivations (such as peer-influence and\nfear of disease-related hospitalizations), is developed. In addition to\nrigorously analyzing the basic qualitative features of this model, a special\ncase is considered where the total population is stratified into two groups:\nrisk-averse (Group 1) and risk-tolerant (Group 2). The two-group behavior model\nhas three disease-free equilibria in the absence of disease, and their\nstability is analyzed using standard linearization and the properties of\nMetzler-stable matrices. Furthermore, the two-group model was calibrated and\nvalidated using daily hospitalization data for New York City during the first\nwave, and the calibrated model was used to predict the data for the second\nwave. Numerical simulations of the calibrated two-group behavior model showed\nthat while the dynamics of the SARS-CoV-2 pandemic during the first wave was\nlargely influenced by the behavior of the risk-tolerant individuals, the\ndynamics during the second wave was influenced by the behavior of individuals\nin both groups. It was also shown that disease-motivated behavioral changes had\ngreater influence in significantly reducing SARS-CoV-2 morbidity and mortality\nthan behavior changes due to the level of peer or social influence or pressure.\nFinally, it is shown that the initial proportion of members in the community\nthat are risk-averse (i.e., the proportion of individuals in Group 1 at the\nbeginning of the pandemic) and the early and effective implementation of\nnon-pharmaceutical interventions have major impacts in reducing the size and\nburden of the pandemic (particularly the total SARS-CoV-2 mortality in New York\nCity during the second wave).",
        "Rydberg blockade effect provides a convenient platform for simulating locally\nconstrained many-body systems, such as quantum dimer models and quantum loop\nmodels, especially their novel phases like topological orders and gapless\nquantum spin ice (QSI) phases. To discuss the possible phase diagram containing\ndifferent QSIs in 3D Rydberg arrays, here, we have constructed an extended\nRokhsar-Kivelson (RK) Hamiltonian with equal-weight-superposition ground state\nin different fillings at the RK point. Therefore, both the perfect QSIs with\nfixed local dimer filling and their monomer-doped states can be simulated\ndirectly by Monte Carlo sampling. Using single mode approximation, the\nexcitations of dimers and monomers have also been explored in different\nfillings. We find that, in the thermodynamical limit, even doping a small\namount of monomers can disrupt the topological structure and lead to the\nexistence of off-diagonal long-range order. However, in a finite size (as in\ncold-atom experiment), the property of QSI will be kept in a certain region\nlike a crossover after doping. The phase diagram containing different QSIs and\noff-diagonal order phases is proposed.",
        "This document presents the artifact associated with the ICSE SEIP 25 paper\ntitled On the Diagnosis of Flaky Job Failures: Understanding and Prioritizing\nFailure Categories. The original paper identifies and analyzes 46 distinct\ncategories of flaky job failures that developers encounter, using Recency (R),\nFrequency (F), and Monetary (M) measures. In addition, it uses an RFM\nclustering model to identify and prioritize the most wasteful and persistent.\nThe original paper only discusses the rankings and evolution of the top 20\ncategories in the results. This artifact contains (1) the regex and scripts\nused to automate the labeling process for RQ1, (2) complete analysis results,\nincluding the ranking of all 46 categories by cost in RQ2 and the evolution of\nthese categories over time in RQ3, and (3) the RFM dataset and scripts used to\ncreate the RFM clustering model for prioritization in RQ4. In addition, we\nengineered the labeling tool and the RFM-based prioritization methodology in a\ncommand-line interface (CLI) called FLAKERANKER to facilitate reuse and\nrepurposing in future studies.",
        "A graph $H$ is an induced minor of a graph $G$ if $H$ can be obtained from\n$G$ by a sequence of edge contractions and vertex deletions. Otherwise, $G$ is\n$H$-induced minor-free. In this paper, we prove that $K_{2,3}$-induced\nminor-free graphs admit a quasi-isometry with additive distortion to graphs\nwith tree-width at most two. Our result implies that a recent conjecture of\nNguyen et al. [Coarse tree-width (2025)] holds for $K_{2,3}$-induced minor-free\ngraphs.",
        "The LHCb experiment at CERN has been upgraded for the Run 3 operation of the\nLarge Hadron Collider (LHC). A new concept of tracking detector based on\nScintillating Fibres (SciFi) read out with multichannel silicon\nphotomultipliers (SiPMs) was installed during its upgrade. One of the main\nchallenges the SciFi tracker will face during the Run 4 operation of the LHC is\nthe higher radiation environment due to fast neutrons, where the SiPMs are\nlocated. To cope with the increase in radiation, cryogenic cooling with liquid\nnitrogen is being investigated as a possible solution to mitigate the\nperformance degradation of the SiPMs induced by radiation damage. Thus, a\ndetailed performance study of different layouts of SiPM arrays produced by\nFondazione Bruno Kessler (FBK) and Hamamatsu Photonics K.K. is being carried\nout. These SiPMs have been designed to operate at cryogenic temperatures.\nSeveral SiPMs have been tested in a dedicated cryogenic setup down to 100 K.\nKey performance parameters such as breakdown voltage, dark count rate, photon\ndetection efficiency, gain and direct cross-talk are characterized as a\nfunction of the temperature. The main results of this study are going to be\npresented here.",
        "The Greek Language Multimodal Lip Reading with Integrated Sign Language\nAccessibility (GLaM-Sign) [1] is a groundbreaking resource in accessibility and\nmultimodal AI, designed to support Deaf and Hard-of-Hearing (DHH) individuals.\nDeveloped from the FEELIT project [2], it integrates high-resolution audio,\nvideo, textual transcriptions, and Greek Sign Language translations for\napplications like real-time sign language translation and enhanced subtitle\nsynchronization. While its primary focus is on promoting inclusivity in the\nGreek tourism sector, its adaptability extends to education, healthcare, and\npublic services. Future advancements will enhance word-level precision and\nscalability to additional languages, supported by advanced AI methodologies and\ncollaborations with diverse stakeholders. This dataset underscores the\ntransformative potential of multimodal resources in bridging communication\ngaps, fostering innovation, and setting a benchmark for ethical AI and\ninclusive technologies.",
        "In this paper we count all the subpaths of a given graph G; including the\nsubpaths of length zero, and we call this quantity the subpath number of G. The\nsubpath number is related to the extensively studied number of subtrees, as it\ncan be considered as counting subtrees with the additional requirement of\nmaximum degree being two. We first give the explicit formula for the subpath\nnumber of trees and unicyclic graphs. We show that among connected graphs on\nthe same number of vertices, the minimum of the subpath number is attained for\nany tree and the maximum for the complete graph. Further, we show that the\ncomplete bipartite graph with partite sets of almost equal size maximizes the\nsubpath number among all bipartite graphs. The explicit formula for cycle\nchains, i.e. graphs in which two consecutive cycles share a single edge, is\nalso given. This family of graphs includes the unbranched catacondensed\nbenzenoids which implies a possible application of the result in chemistry. The\npaper is concluded with several directions for possible further research where\nseveral conjectures are provided.",
        "Searching for dark matter at high-energy colliders and direct detection\nexperiments can effectively cover nearly the entire mass range from the MeV to\nthe TeV scale. In this paper, we focus on four-fermion contact interactions\nformulated within the framework of Effective Field Theory. Specifically, we\npresent a detailed analysis of mono-lepton production at the LHC. Our results\ndemonstrate that tensor operators exhibit superior sensitivity in the\nmono-lepton channel, constraining energy scales up to 3\\,TeV for a nearly\nmassless dark fermion using current LHC data. Moreover, these operators mediate\nboth spin-independent and spin-dependent absorption processes in nuclear\ntargets. A systematic comparison of constraints between direct detection\nexperiments and collider measurements reveals the LHC's distinct advantage in\nexploring sub-GeV dark matter candidates while maintaining competitive\nsensitivity at the TeV scale. Notably, direct detection experiments such as\nSuper-Kamiokande and Borexino achieve complementary constraints in the\n10-100\\,TeV mass range through their unique capabilities: utilization of light\nnuclei targets, large exposure volumes, and distinctive features of the recoil\nenergy spectra.",
        "Missing data often exists in real-world datasets, requiring significant time\nand effort for imputation to learn accurate machine learning (ML) models. In\nthis paper, we demonstrate that imputing all missing values is not always\nnecessary to achieve an accurate ML model. We introduce the concept of minimal\ndata imputation, which ensures accurate ML models trained over the imputed\ndataset. Implementing minimal imputation guarantees both minimal imputation\neffort and optimal ML models. We propose algorithms to find exact and\napproximate minimal imputation for various ML models. Our extensive experiments\nindicate that our proposed algorithms significantly reduce the time and effort\nrequired for data imputation.",
        "The mass accretion process controls pre-main-sequence evolution, although its\nintrinsic instability has yet to be fully understood, especially towards the\nprotostellar stage. In this work, we have undertaken a thorough examination of\nthe mid-infrared variability of Spitzer-selected YSOs in the Cygnus-X\nstar-forming region over the last decade, using the NEOWISE time series. This\nwork compares two groups of young stars: embedded Class I objects, and the more\nevolved flat-spectrum\/Class II sources. We report on 48 candidate eruptive\nvariables within these groups, including 14 with characteristics that resemble\nthe photometric behaviour of FUors. We also include an additional 20 YSOs,\nwhich are of a less certain categorisation. We find the candidate FUors to be\nan order of magnitude more common among the younger Class I systems than more\nevolved objects. A large number of the identified short-duration eruptive YSOs\ndisplay mid-infrared colour behaviour that is redder-when-brighter, which\ncontrasts with optically bright outbursts seen in YSOs. Finally, we note the\nunusual long-term rising behaviours of four Class I YSOs, with rise timescales\nlonger than five years, which is far slower than 6-12 month timescale for the\nmajority of optically discovered FUors. Additionally, our broader investigation\nof MIR variability for embedded class I YSOs shows that there is a higher\nincidence of high amplitude variability for these stars, than is seen in class\nII sources. This holds true for all variable class I YSOs, not just the\neruptive sources.",
        "Orchestrating next gen applications over hterogeneous resources along the\nCloud-IoT continuum calls for new strategies and tools to enable scalable and\napplication-specific managements. Inspired by the self-organisation\ncapabilities of bacteria colonies, we propose a declarative, fully\ndecentralised application management solution, targeting pervasive\nopportunistic Cloud-IoT infrastructures. We present acustomisable declarative\nimplementation of the approach and validate its scalability through simulation\nover motivating scenarios, also considering end-user's mobility and the\npossibility to enforce application-specific management policies for different\nclasses of applications.",
        "We study the effect of finite spin quark density on the chiral and\ndeconfinement thermal transitions using numerical simulations of lattice QCD\nwith two dynamical light quarks. The finite spin density is introduced by the\nquark spin potential in the canonical formulation of the spin operator. We show\nthat both chiral and deconfinement temperatures are decreasing functions of the\nspin potential. We determine the parabolic curvatures of transition\ntemperatures in a limit of physical quark masses.",
        "We introduce a novel approach to variational Quantum algorithms (VQA) via\ncontinuous bandits. VQA are a class of hybrid Quantum-classical algorithms\nwhere the parameters of Quantum circuits are optimized by classical algorithms.\nPrevious work has used zero and first order gradient based methods, however\nsuch algorithms suffer from the barren plateau (BP) problem where gradients and\nloss differences are exponentially small. We introduce an approach using\nbandits methods which combine global exploration with local exploitation. We\nshow how VQA can be formulated as a best arm identification problem in a\ncontinuous space of arms with Lipschitz smoothness. While regret minimization\nhas been addressed in this setting, existing methods for pure exploration only\ncover discrete spaces. We give the first results for pure exploration in a\ncontinuous setting and derive a fixed-confidence, information-theoretic,\ninstance specific lower bound. Under certain assumptions on the expected\npayoff, we derive a simple algorithm, which is near-optimal with respect to our\nlower bound. Finally, we apply our continuous bandit algorithm to two VQA\nschemes: a PQC and a QAOA quantum circuit, showing that we significantly\noutperform the previously known state of the art methods (which used gradient\nbased methods).",
        "As opposed to human drivers, current autonomous driving systems still require\nvast amounts of labeled data to train. Recently, world models have been\nproposed to simultaneously enhance autonomous driving capabilities by improving\nthe way these systems understand complex real-world environments and reduce\ntheir data demands via self-supervised pre-training. In this paper, we present\nAD-L-JEPA (aka Autonomous Driving with LiDAR data via a Joint Embedding\nPredictive Architecture), a novel self-supervised pre-training framework for\nautonomous driving with LiDAR data that, as opposed to existing methods, is\nneither generative nor contrastive. Our method learns spatial world models with\na joint embedding predictive architecture. Instead of explicitly generating\nmasked unknown regions, our self-supervised world models predict Bird's Eye\nView (BEV) embeddings to represent the diverse nature of autonomous driving\nscenes. Our approach furthermore eliminates the need to manually create\npositive and negative pairs, as is the case in contrastive learning. AD-L-JEPA\nleads to simpler implementation and enhanced learned representations. We\nqualitatively and quantitatively demonstrate high-quality of embeddings learned\nwith AD-L-JEPA. We furthermore evaluate the accuracy and label efficiency of\nAD-L-JEPA on popular downstream tasks such as LiDAR 3D object detection and\nassociated transfer learning. Our experimental evaluation demonstrates that\nAD-L-JEPA is a plausible approach for self-supervised pre-training in\nautonomous driving applications and is the best available approach\noutperforming SOTA, including most recently proposed Occupancy-MAE [1] and ALSO\n[2]. The source code of AD-L-JEPA is available at\nhttps:\/\/github.com\/HaoranZhuExplorer\/AD-L-JEPA-Release.",
        "We present an analysis of the cluster X-ray morphology and active galactic\nnucleus (AGN) activity in nine $z\\sim1$ galaxy clusters from the Massive and\nDistant Clusters of $WISE$ Survey (MaDCoWS) observed with $Chandra$. Using\nphoton asymmetry ($A_{\\text{phot}}$) to quantify X-ray morphologies, we find\nevidence that the four most dynamically disturbed clusters are likely to be\nmergers. Employing a luminosity cut of $7.6\\times10^{42}$ erg\/s to identify AGN\nin the 0.7-7.0 keV, we show that the majority of these clusters host excess AGN\ncompared to the local field. We use the cumulative number-count ($\\log N-\\log\nS$) model to predict AGN incidence in cluster isophotes under this luminosity\ncut. Our analysis finds evidence (at $> 2\\sigma$) of a positive correlation\nbetween AGN surface densities and photon asymmetry, suggesting that a disturbed\ncluster environment plays a pivotal role in regulating AGN triggering. Studying\nAGN incidence in cluster X-ray isophotes equivalent in area to $1.0r_{500}$, we\nfind that the AGN space density inversely scales with cluster mass as $\\sim\nM^{-0.5^{+0.18}_{-0.18}}$ at the 3.18$\\sigma$ level. Finally, when we\nseparately explore the cluster mass dependence of excess AGN surface density in\ndisturbed and relaxed clusters, we see tentative evidence that the two\nmorphologically distinct sub-populations exhibit diverging trends, especially\nnear the outskirts, likely due to cluster merger-driven AGN\ntriggering\/suppression.",
        "Modal decomposition methods are important for characterizing the\nlow-dimensional dynamics of complex systems, including turbulent flows.\nDifferent methods have varying data requirements and produce modes with\ndifferent properties. Spectral proper orthogonal decomposition (SPOD) produces\northogonal, energy-ranked spatial modes at discrete temporal frequencies for\nstatistically stationary flows. However, SPOD requires long stretches of\nsequential, uniformly sampled, time-resolved data. These data requirements\nlimit SPOD's use in experimental settings where the maximum capture rate of a\ncamera is often slower than the Nyquist sampling rate required to resolve the\nhighest turbulent frequencies. However, if two PIV systems operate in tandem,\npairs of data can be acquired that are arbitrarily close in time. The dynamic\nmode decomposition (DMD) uses this pairwise data to resolve frequencies up to\nthe Nyquist frequency associated with the small time step within a pair.\nHowever, these modes do not form an orthonormal basis and have no set ranking.\nThe present work attempts to compute SPOD modes from pairwise data with a small\ntime step but with large gaps between pairs. We use DMD on pairwise data to\nestimate segment-wise, uniformly sampled series that can then be used to\nestimate the SPOD modes, intending to resolve frequencies between the gap and\npair Nyquist limits. The method is tested on numerically obtained data of the\nlinearized complex Ginzburg-Landau equation, as well as a Mach 0.4 isothermal\nturbulent jet. For the jet, pairwise SPOD can accurately de-alias the SPOD\nspectrum and estimate mode shapes at frequencies up to St = 1.0 while using\nover 90% less data.",
        "The multi-modality imaging system offers optimal fused images for safe and\nprecise interventions in modern clinical practices, such as computed tomography\n- ultrasound (CT-US) guidance for needle insertion. However, the limited\ndexterity and mobility of current imaging devices hinder their integration into\nstandardized workflows and the advancement toward fully autonomous intervention\nsystems. In this paper, we present a novel clinical setup where robotic cone\nbeam computed tomography (CBCT) and robotic US are pre-calibrated and\ndynamically co-registered, enabling new clinical applications. This setup\nallows registration-free rigid registration, facilitating multi-modal guided\nprocedures in the absence of tissue deformation. First, a one-time\npre-calibration is performed between the systems. To ensure a safe insertion\npath by highlighting critical vasculature on the 3D CBCT, SAM2 segments vessels\nfrom B-mode images, using the Doppler signal as an autonomously generated\nprompt. Based on the registration, the Doppler image or segmented vessel masks\nare then mapped onto the CBCT, creating an optimally fused image with\ncomprehensive detail. To validate the system, we used a specially designed\nphantom, featuring lesions covered by ribs and multiple vessels with simulated\nmoving flow. The mapping error between US and CBCT resulted in an average\ndeviation of 1.72+-0.62 mm. A user study demonstrated the effectiveness of\nCBCT-US fusion for needle insertion guidance, showing significant improvements\nin time efficiency, accuracy, and success rate. Needle intervention performance\nimproved by approximately 50% compared to the conventional US-guided workflow.\nWe present the first robotic dual-modality imaging system designed to guide\nclinical applications. The results show significant performance improvements\ncompared to traditional manual interventions.",
        "We introduce a retrieval approach leveraging Support Vector Regression (SVR)\nensembles, bootstrap aggregation (bagging), and embedding spaces on the German\nDataset for Legal Information Retrieval (GerDaLIR). By conceptualizing the\nretrieval task in terms of multiple binary needle-in-a-haystack subtasks, we\nshow improved recall over the baselines (0.849 > 0.803 | 0.829) using our\nvoting ensemble, suggesting promising initial results, without training or\nfine-tuning any deep learning models. Our approach holds potential for further\nenhancement, particularly through refining the encoding models and optimizing\nhyperparameters.",
        "The Closest Vector Problem (CVP) is a computational problem in lattices that\nis central to modern cryptography. The study of its fine-grained complexity has\ngained momentum in the last few years, partly due to the upcoming deployment of\nlattice-based cryptosystems in practice. A main motivating question has been if\nthere is a $(2-\\varepsilon)^n$ time algorithm on lattices of rank $n$, or\nwhether it can be ruled out by SETH.\n  Previous work came tantalizingly close to a negative answer by showing a\n$2^{(1-o(1))n}$ lower bound under SETH if the underlying distance metric is\nchanged from the standard $\\ell_2$ norm to other $\\ell_p$ norms. Moreover,\nbarriers toward proving such results for $\\ell_2$ (and any even $p$) were\nestablished.\n  In this paper we show \\emph{positive results} for a natural special case of\nthe problem that has hitherto seemed just as hard, namely\n$(0,1)$-$\\mathsf{CVP}$ where the lattice vectors are restricted to be sums of\nsubsets of basis vectors (meaning that all coefficients are $0$ or $1$). All\nprevious hardness results applied to this problem, and none of the previous\nalgorithmic techniques could benefit from it. We prove the following results,\nwhich follow from new reductions from $(0,1)$-$\\mathsf{CVP}$ to weighted\nMax-SAT and minimum-weight $k$-Clique.\n  1. An $O(1.7299^n)$ time algorithm for exact $(0,1)$-$\\mathsf{CVP}_2$ in\nEuclidean norm, breaking the natural $2^n$ barrier, as long as the absolute\nvalue of all coordinates in the input vectors is $2^{o(n)}$.\n  2. A computational equivalence between $(0,1)$-$\\mathsf{CVP}_p$ and\nMax-$p$-SAT for all even $p$.\n  3. The minimum-weight-$k$-Clique conjecture from fine-grained complexity and\nits numerous consequences (which include the APSP conjecture) can now be\nsupported by the hardness of a lattice problem, namely\n$(0,1)$-$\\mathsf{CVP}_2$.",
        "We show that for a weak $\\mathbb{Q}$-Fano threefold $X$\n($\\mathbb{Q}$-factorial with terminal singularities and $-K_X$ is nef and big)\nof Picard rank $\\rho(X)\\leq 2$, either $-K_X^3\\leq 64$ or $-K_X^3=72$ and\n$X=\\mathbb{P}_{\\mathbb{P}^2}(\\mathcal{O}_{\\mathbb{P}^2}\\oplus\\mathcal{O}_{\\mathbb{P}^2}(3))$.\nThis is supplementary to the previous work in arXiv:2501.12555.",
        "The chapter overviews the recent developments in the remote sensing of\ncometary dust using visible, near-infrared, and thermal-infrared radiation, as\nwell as interaction of the dust with electromagnetic radiation, which affects\nthe dynamics of dust particles. It considers photometric, polarimetric, and\nspectral studies of cometary dust, focusing on those observables and\ncorrelations between them that allow revealing the composition, size, and\nstructure of the dust particles. The analysis includes the observed brightness\nand polarization phase curves, color and polarimetric color of the cometary\ndust, and near- and thermal-infrared spectra. Special attention is paid to the\nrole of gas contamination in the polarimetric and photometric data. A review of\nmodeling efforts to interpret the observational results is also provided,\ndescribing the most popular (and some novel) techniques used in the computer\nmodeling of light scattering by dust particles with a focus on modeling the\nmost complex type of cometary particles: fluffy and porous agglomerates. The\nchapter also considers how properties of the dust particles affect their\nphotoelectric emission and their response to the radiation pressure and\nradiative torque, including alignment and fragmentation of particles. Results\nof computer and some laboratory modeling are analyzed for their consistency\nwith the observational and in situ data. Also discussed is how the modeling\nresults can be combined with in situ data for better characterization of the\ncometary dust.",
        "Modelling real world systems frequently requires the solution of systems of\nnonlinear equations. A number of approaches have been suggested and developed\nfor this computational problem. However, it is also possible to attempt\nsolutions using more general nonlinear least squares or function minimization\ntechniques. There are concerns, nonetheless, that we may fail to find\nsolutions, or that the process will be inefficient. Examples are presented with\nR with the goal of providing guidance on the solution of nonlinear equations\nproblems.",
        "Let $e^{-tL}$ be a analytic semigroup generated by $-L$, where $L$ is a\nnon-negative self-adjoint operator on $L^2(\\mathbb{R}^d)$. Assume that the\nkernels of $e^{-tL}$, denoted by $p_t(x,y)$, only satisfy the upper bound: for\nall $N>0$, there are constants $c,C>0$ such that \\begin{align}\\label{upper\nbound}\n|p_t(x,y)|\\leq\\frac{C}{t^{d\/2}}e^{-\\frac{|x-y|^2}{ct}}\\Big(1+\\frac{\\sqrt{t}}{\\rho(x)}+\n\\frac{\\sqrt{t}}{\\rho(y)}\\Big)^{-N} \\end{align} holds for all\n$x,y\\in\\mathbb{R}^d$ and $t>0$. We first establish the quantitative matrix\nweighted inequalities for fractional type integrals associated to $L$ with new\nclasses of matrix weights, which are nontrivial extension of the results\nestablished by Li, Rahm and Wick [23]. Next, we give new two-weight bump\nconditions with Young functions satisfying wider conditions for fractional type\nintegrals associated to $L$, which cover the result obtained by Cruz-Uribe,\nIsralowitz and Moen [6]. We point out that the new classes of matrix weights\nand bump conditions are larger and weaker than the classical ones given in [17]\nand [6], respectively. As applications, our results can be applied to settings\nof magnetic Schr\\\"{o}dinger operator, Laguerre operators, etc.",
        "Large language models (LLMs) demonstrate unprecedented capabilities and\ndefine the state of the art for almost all natural language processing (NLP)\ntasks and also for essentially all Language Technology (LT) applications. LLMs\ncan only be trained for languages for which a sufficient amount of pre-training\ndata is available, effectively excluding many languages that are typically\ncharacterised as under-resourced. However, there is both circumstantial and\nempirical evidence that multilingual LLMs, which have been trained using data\nsets that cover multiple languages (including under-resourced ones), do exhibit\nstrong capabilities for some of these under-resourced languages. Eventually,\nthis approach may have the potential to be a technological off-ramp for those\nunder-resourced languages for which \"native\" LLMs, and LLM-based technologies,\ncannot be developed due to a lack of training data. This paper, which\nconcentrates on European languages, examines this idea, analyses the current\nsituation in terms of technology support and summarises related work. The\narticle concludes by focusing on the key open questions that need to be\nanswered for the approach to be put into practice in a systematic way.",
        "Granular materials in nature are nearly always non-spherical, but particle\nshape effects in granular flow remain largely elusive. This study uses discrete\nelement method simulations to investigate how elongated particle shapes affect\nthe mobility of dense granular flows down a rough incline. For a range of\nsystematically varied particle length-to-diameter aspect ratios (AR), we run\nsimulations with various flow thicknesses $h$ and slope angles $\\theta$ to\nextract the well-known $h_\\textrm{stop}(\\theta)$ curves (below which the flow\nceases) and the $Fr$-$h\/h_\\textrm{stop}$ relations following Pouliquen's\napproach, where $Fr=u\/\\sqrt{gh}$ is the Froude number, $u$ is the mean flow\nvelocity, and $g$ is the gravitational acceleration. The slope $\\beta$ of the\n$Fr$-$h\/h_\\textrm{stop}$ relations shows an intriguing S-shaped dependence on\nAR, with two plateaus at small and large AR, respectively, transitioning with a\nsharp increase. We understand this S-shaped dependence by examining statistics\nof particle orientation, alignment, and hindered rotation. We find that the\nrotation ability of weakly elongated particles ($\\textrm{AR}\\lesssim1.3$)\nremains similar to spheres, leading to the first plateau in the $\\beta$-AR\nrelation, whereas the effects of particle orientation saturates beyond\n$\\textrm{AR}\\approx2.0$, explaining the second plateau. An empirical sigmoidal\nfunction is proposed to capture this non-linear dependence. The findings are\nexpected to enhance our understanding of how particle shape affects the flow of\ngranular materials from both the flow- and particle-scale perspectives."
      ]
    }
  },
  {
    "id":2411.05188,
    "research_type":"applied",
    "start_id":"b7",
    "start_title":"What is being transferred in transfer learning?",
    "start_abstract":"One desired capability for machines is the ability to transfer their knowledge of one domain another where data (usually) scarce. Despite ample adaptation learning in various deep applications, we yet do not understand what enables a successful and which part network responsible that. In this paper, provide new tools analyses address these fundamental questions. Through series on transferring block-shuffled images, separate effect feature reuse from low-level statistics show that some benefit comes latter. We present when training pre-trained weights, model stays same basin loss landscape different instances such are similar space close parameter space.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "Limitations of conventional magnetic resonance imaging as a predictor of death or disability following neonatal hypoxic-ischemic encephalopathy in the late hypothermia trial"
      ],
      "abstract":[
        "Objective: To investigate if magnetic resonance imaging (MRI) is an accurate predictor for death or moderate-severe disability at 18-22 months of age among infants with neonatal encephalopathy in a trial of cooling initiated at 6-24 hours. Study design: Subgroup analysis of infants \u226536 weeks of gestation with moderate-severe neonatal encephalopathy randomized at 6-24 postnatal hours to hypothermia or usual care in a multicenter trial of late hypothermia. MRI scans were performed per each center's practice and interpreted by 2 central readers using the Eunice Kennedy Shriver National Institute of Child Health and Human Development injury score (6 levels, normal to hemispheric devastation). Neurodevelopmental outcomes were assessed at 18-22 months of age. Results: Of 168 enrollees, 128 had an interpretable MRI and were seen in follow-up (n = 119) or died (n = 9). MRI findings were predominantly acute injury and did not differ by cooling treatment. At 18-22 months, death or severe disability occurred in 20.3%. No infant had moderate disability. Agreement between central readers was moderate (weighted kappa 0.56, 95% CI 0.45-0.67). The adjusted odds of death or severe disability increased 3.7-fold (95% CI 1.8-7.9) for each increment of injury score. The area under the curve for severe MRI patterns to predict death or severe disability was 0.77 and the positive and negative predictive values were 36% and 100%, respectively. Conclusions: MRI injury scores were associated with neurodevelopmental outcome at 18-22 months among infants in the Late Hypothermia Trial. However, the results suggest caution when using qualitative interpretations of MRI images to provide prognostic information to families following perinatal hypoxia-ischemia."
      ],
      "categories":[
        "Pediatrics"
      ]
    },
    "list":{
      "title":[
        "Known Unknowns: Out-of-Distribution Property Prediction in Materials and\n  Molecules",
        "Overcoming experimental obstacles in two-dimensional spectroscopy of a\n  single molecule",
        "Benchmarking Selected Density Functionals and Dispersion Corrections for\n  MOF-5 and its Derivatives",
        "Quantum-Corrected Hawking Radiation from Near-Extremal Kerr-Newman Black\n  Holes",
        "Current-linear emergent induction of pinned skyrmion textures in an\n  oxide bilayer",
        "Superadditivity at Large Charge",
        "Mapping strain and structural heterogeneities around bubbles in\n  amorphous ionically conductive Bi$_2$O$_3$",
        "Enumeration of polyhedra with triangular and hexagonal faces and three\n  faces around each vertex",
        "Universal Semantic Embeddings of Chemical Elements for Enhanced\n  Materials Inference and Discovery",
        "Extending the Bridge Connecting Chiral Lagrangians and QCD Gaussian\n  Sum-Rules for Low-Energy Hadronic Physics",
        "The Pseudo-Dimension of Contracts",
        "Nonlinear eigenvalue problems for a class of quasilinear operator on\n  complete Riemannian manifolds",
        "Statistical Inference of the Matthews Correlation Coefficient for\n  Multiclass Classification",
        "Study of Mass Transport in the Anode of a Proton Exchange Membrane Fuel\n  Cell with a New Hydrogen Flow-Rate Modulation Technique",
        "ATP requirements for growth reveal the bioenergetic impact of\n  mitochondrial symbiosis",
        "On Volume Minimization in Conformal Regression",
        "Low-Complexity Detection of Multiple Preambles in the Presence of\n  Mobility and Delay Spread",
        "Constraints on fast radio burst population from the first CHIME\/FRB\n  catalog from Hierarchical Bayesian Inference",
        "A Multi-Modal AI Copilot for Single-Cell Analysis with Instruction\n  Following",
        "Benchmarking Quantum Reinforcement Learning",
        "Magnetically Induced Current Density from Numerical Positional\n  Derivatives of Nucleus Independent Chemical Shifts",
        "Kovacs-like memory effect in a sheared colloidal glass: role of\n  non-affine flows",
        "Bridging Critical Gaps in Convergent Learning: How Representational\n  Alignment Evolves Across Layers, Training, and Distribution Shifts",
        "Caribou -- A versatile data acquisition system for silicon pixel\n  detector prototyping",
        "Elimination of substrate-induced FMR linewidth broadening in the\n  epitaxial system YIG-GGG by microstructuring",
        "Optical Nuclear Electric Resonance in LiNa: Selective Addressing of\n  Nuclear Spins Through Pulsed Lasers",
        "Post-Hoc Uncertainty Quantification in Pre-Trained Neural Networks via\n  Activation-Level Gaussian Processes",
        "How far are two symmetric matrices from commuting? With an application\n  to object characterisation and identification in metal detection",
        "On the Hilbert depth of the quotient ring of the edge ideal of a star\n  graph"
      ],
      "abstract":[
        "Discovery of high-performance materials and molecules requires identifying\nextremes with property values that fall outside the known distribution.\nTherefore, the ability to extrapolate to out-of-distribution (OOD) property\nvalues is critical for both solid-state materials and molecular design. Our\nobjective is to train predictor models that extrapolate zero-shot to higher\nranges than in the training data, given the chemical compositions of solids or\nmolecular graphs and their property values. We propose using a transductive\napproach to OOD property prediction, achieving improvements in prediction\naccuracy. In particular, the True Positive Rate (TPR) of OOD classification of\nmaterials and molecules improved by 3x and 2.5x, respectively, and precision\nimproved by 2x and 1.5x compared to non-transductive baselines. Our method\nleverages analogical input-target relations in the training and test sets,\nenabling generalization beyond the training target support, and can be applied\nto any other material and molecular tasks.",
        "Two-dimensional electronic spectroscopy provides information on coupling and\nenergy transfer between excited states on ultrafast timescales. Only recently,\nincoherent fluorescence detection has made it possible to combine this method\nwith single-molecule optical spectroscopy to reach the ultimate limit of\nsensitivity. The main obstacle has been the low number of photons detected due\nto limited photostability. Here we discuss the key experimental choices that\nallowed us to overcome these obstacles: broadband acousto-optic modulation,\naccurate phase-locked loops, photon-counting lock-in detection, delay stage\nlinearization, and detector dead-time compensation. We demonstrate how the\nacquired photon stream data can be used to post-select detection events\naccording to specific criteria.",
        "Accurate computational predictions of metal-organic frameworks (MOFs) and\ntheir properties is crucial for discovering optimal compositions and applying\nthem in relevant technological areas. This work benchmarks density functional\ntheory (DFT) approaches, including semi-local, meta-GGA, and hybrid functionals\nwith various dispersion corrections, on MOF-5 and three of its computationally\npredicted derivatives, analyzing structural, electronic, and vibrational\nproperties. Our results underline the importance of explicitly treating van der\nWaals interactions for an accurate description of structural and vibrational\nproperties, and indicate the meta-GGA functional R2SCAN as the best balance\nbetween accuracy and efficiency for characterizing the electronic structure of\nthese systems, in view of future high-throughput screening studies on MOFs.",
        "Near-extremal black holes have a long AdS$_2$ throat in their near-horizon\nregion. Quantum fluctuations in the throat region are effectively governed by a\nquantum version of Jackiw-Teitelboim gravity with matter and are strongly\ncoupled at low temperatures. We investigate how these quantum fluctuations\naffect the spectrum of emission of particles during Hawking radiation. We\nsystematically consider the cases of Kerr and Kerr-Newman black holes for\nemission of scalar particles and discuss photon and graviton emission from the\nKerr background. We find that at very low temperatures the quantum fluctuations\nradically change the nature of particle emission. Unlike the generic\nsuppression of particle emission in the spherically symmetric\nReissner-Nordstr\\\"om case, we uncover that for particles with non-vanishing\nangular momentum, the quantum-corrected emission can be substantially enhanced\nwith respect to the standard semiclassical result.",
        "Emergent electromagnetic induction (EEMI) induced through current-driven spin\ndynamics was recently predicted and subsequently observed in helical spin\nmagnets, opening a new direction in spintronics and paving the way towards\nfurther miniaturization of electronic circuit elements. In contrast to\nconventional inductors consisting of coil-like structures whose inductance $L$\nshows a linear dependence on the cross-section $A$, emergent inductors exhibit\nan inverse ($\\propto {A}^{-1}$) proportionality, favorable for the\nminiaturization of electrical devices. However, the expected current-linear\nresponse of the EEMI voltage has not been demonstrated. Magnetic skyrmions hold\npromise as a simple platform to study the conceptual foundations of EEMI from\ncurrent-driven spin dynamics. We fabricated devices of thin film bilayers of\nferromagnetic SrRuO$_3$ and paramagnetic SrIrO$_3$, which are known to host\ninterfacial N\\'eel skyrmions detected by the appearance of a topological\nHall-effect (THE). A large, positive and current-linear inductive response is\nfound to accompany the THE. In our experiment, the current-induced dynamics of\npinned magnetic skyrmions creates a voltage both parallel and perpendicular to\nthe applied electric current flow, corresponding to longitudinal and transverse\ninduction, respectively. This is the first report of transverse EEMI,\nindicating an angle of $80^{\\circ}$ between skyrmion motion and the applied\ncurrent. Our observation of current-linear longitudinal and transverse EEMI is\na hallmark of pinned dynamics of magnetic skyrmion textures in oxide\nheterostructures.",
        "The weak gravity conjecture has been invoked to conjecture that the\ndimensions of charged operators in a CFT should obey a superadditivity relation\n(sometimes referred to as convexity). In this paper, we study superadditivity\nof the operator spectrum in theories expanded about the semi-classical saddle\npoint that dominates correlators of large charge operators. We explore this in\ntwo contexts. The first is a model with two scalar fields that carry different\ncharges, at a non-trivial Wilson-Fisher fixed point. A careful analysis of the\nsemi-classics for this two field model demonstrates that 'quantum' violations\nof superadditivity (those not forbidden by the conjecture) persist in the large\ncharge regime. We then turn to study the general properties of CFTs at large\ncharge as bottom-up EFTs. By a trial and error procedure we come up with a\nseemingly consistent family of examples violating the conjecture. In so doing\nthe presence of a genuine dilaton field appears necessary. On the one hand our\nresult demonstrates that the superadditivity conjecture cannot be proven purely\non the basis of a bottom-up analysis. On the other hand, the need for a\ndilaton, with the corresponding infinite fine tuning, indicates the\nconjecture-violating EFTs are unlikely to be UV completable.",
        "While amorphous materials are often approximated to have a statistically\nhomogeneous atomic structure, they frequently exhibit localized structural\nheterogeneity that challenges simplified models. This study uses 4D scanning\ntransmission electron microscopy to investigate the strain and structural\nmodifications around gas bubbles in amorphous Bi$_2$O$_3$ induced by argon\nirradiation. We present a method for determining strain fields surrounding\nbubbles that can be used to measure the internal pressure of the gas.\nCompressive strain is observed around the cavities, with higher-order\ncrystalline symmetries emerging near the cavity interfaces, suggesting\nparacrystalline ordering as a result of bubble coarsening. This ordering, along\nwith a compressive strain gradient, indicates that gas bubbles induce\nsignificant localized changes in atomic packing. By analyzing strain fields\nwith maximum compressive strains of 3\\%, we estimate a lower bound on the\ninternal pressure of the bubbles at 2.5 GPa. These findings provide insight\ninto the complex structural behavior of amorphous materials under stress,\nparticularly in systems with gas inclusions, and offer new methods for probing\nthe local atomic structure in disordered materials. Although considering\nstructural heterogeneity in amorphous systems is non-trivial, these features\nhave crucial impacts on material functionalities, such as mechanical strength,\nionic conductivity, and electronic mobility.",
        "We give an exact count of the number of trivalent graphs whose faces all have\n3 or 6 sides, or equivalently, the number of polyhedra with triangular and\nhexagonal faces and three faces around each vertex. The count is given in terms\nof the prime factorization of the number of vertices. We also enumerate graphs\nof this type with mirror symmetry, with 3-fold rotational symmetry, and with\nboth types of symmetry.",
        "We present a framework for generating universal semantic embeddings of\nchemical elements to advance materials inference and discovery. This framework\nleverages ElementBERT, a domain-specific BERT-based natural language processing\nmodel trained on 1.29 million abstracts of alloy-related scientific papers, to\ncapture latent knowledge and contextual relationships specific to alloys. These\nsemantic embeddings serve as robust elemental descriptors, consistently\noutperforming traditional empirical descriptors with significant improvements\nacross multiple downstream tasks. These include predicting mechanical and\ntransformation properties, classifying phase structures, and optimizing\nmaterials properties via Bayesian optimization. Applications to titanium\nalloys, high-entropy alloys, and shape memory alloys demonstrate up to 23%\ngains in prediction accuracy. Our results show that ElementBERT surpasses\ngeneral-purpose BERT variants by encoding specialized alloy knowledge. By\nbridging contextual insights from scientific literature with quantitative\ninference, our framework accelerates the discovery and optimization of advanced\nmaterials, with potential applications extending beyond alloys to other\nmaterial classes.",
        "It has previously been demonstrated that the mesonic fields in chiral\nLagrangians can be related to the quark-level operators of QCD sum-rules via\nenergy-independent (constant) scale factor matrices constrained by chiral\nsymmetry. This leads to universal scale factors for each type of chiral nonet\nrelated to quark-antiquark ($q\\bar q$) operators and four-quark ($qq\\bar q\\bar\nq$) operators. Motivated by these successful demonstrations of scale-factor\nuniversality for the $K_0^*$ isodoublet and $a_0$ isotriplet scalar mesons, a\nrevised Gaussian QCD sum-rule methodology is developed that enables the\nextension to higher-dimensional isospin sectors, including the possibility of\nmixing with glueball components. Moreover, to extract non-perturbative\ninformation about a resonance stemming from the final state interactions of its\ndecay products, a background-resonance interference approximation is developed\nand shown to provide an excellent description of both $\\pi K$ scattering\namplitude data and $\\pi\\eta$ scattering calculations. This background-resonance\ninterference approximation inspires new resonance models as ingredients in the\nscale-factor analysis connecting chiral Lagrangians and QCD Gaussian sum-rules.\nUsing the revised Gaussian QCD sum-rule methodology, key properties of the\nscale factors are examined for the $K_0^*$ isodoublet and $a_0$ isotriplet\nscalar mesons for a sequence of increasingly sophisticated resonance models.\nGaussian sum-rules are demonstrated to have sufficient resolution to\ndistinguish between different resonance models, and it is shown that the\nbackground-resonance interference approximation not only describes $\\{\\pi\nK,\\pi\\eta\\}$ scattering, but leads to the best universality and\nenergy-independence properties of the scale factors.",
        "Algorithmic contract design studies scenarios where a principal incentivizes\nan agent to exert effort on her behalf. In this work, we focus on settings\nwhere the agent's type is drawn from an unknown distribution, and formalize an\noffline learning framework for learning near-optimal contracts from sample\nagent types. A central tool in our analysis is the notion of pseudo-dimension\nfrom statistical learning theory. Beyond its role in establishing upper bounds\non the sample complexity, pseudo-dimension measures the intrinsic complexity of\na class of contracts, offering a new perspective on the tradeoffs between\nsimplicity and optimality in contract design. Our main results provide\nessentially optimal tradeoffs between pseudo-dimension and representation error\n(defined as the loss in principal's utility) with respect to linear and bounded\ncontracts. Using these tradeoffs, we derive sample- and time-efficient learning\nalgorithms, and demonstrate their near-optimality by providing almost matching\nlower bounds on the sample complexity. Conversely, for unbounded contracts, we\nprove an impossibility result showing that no learning algorithm exists.\n  Finally, we extend our techniques in three important ways. First, we provide\nrefined pseudo-dimension and sample complexity guarantees for the combinatorial\nactions model, revealing a novel connection between the number of critical\nvalues and sample complexity. Second, we extend our results to menus of\ncontracts, showing that their pseudo-dimension scales linearly with the menu\nsize. Third, we adapt our algorithms to the online learning setting, where we\nshow that, a polynomial number of type samples suffice to learn near-optimal\nbounded contracts. Combined with prior work, this establishes a formal\nseparation between expert advice and bandit feedback for this setting.",
        "In this manuscript, we study the nonlinear eigenvalue problem on complete\nRiemannian manifolds with Ricci curvature bounded from below, to find the\nunknowns $\\lambda$ and $u$, such that\n  $$\n  Qu + \\lambda f(u) = 0\n  $$\n  where $\\lambda$ is an eigenvalue of $u$, with respect to the quasilinear\noperator $Qu = \\operatorname{div} (\\mathcal{F}(u^2, |\\nabla u|^2)\\nabla u)$ and\nnonlinar function $f(\\cdot)\\neq 0$. We generalize the Cheng--Yau gradient\nestimate in \\cite{shen2025feasibilitynashmoseriterationchengyautype} and\ndemonstrate that under certain conditions, a non-zero eigenvalue gives rise to\nunbounded eigenfunction $u$. Our new result also covers more quasilinear\nequations like $p$-porous medium equation (\\textit{i.e.} $\\Delta_p u^q =\n\\lambda u^r$), and generally,\n$\\Delta_{p}\\left(\\sum_{i=1}^{m}a_iu^{q_i}\\right)+\\lambda u^r = 0$.",
        "Classification problems are essential statistical tasks that form the\nfoundation of decision-making across various fields, including patient\nprognosis and treatment strategies for critical conditions. Consequently,\nevaluating the performance of classification models is of significant\nimportance, and numerous evaluation metrics have been proposed. Among these,\nthe Matthews correlation coefficient (MCC), also known as the phi coefficient,\nis widely recognized as a reliable metric that provides balanced measurements\neven in the presence of class imbalance. However, with the increasing\nprevalence of multiclass classification problems involving three or more\nclasses, macro-averaged and micro-averaged extensions of MCC have been\nemployed, despite a lack of clear definitions or established references for\nthese extensions. In the present study, we propose a formal framework for MCC\ntailored to multiclass classification problems using macro-averaged and\nmicro-averaged approaches. Moreover, discussions on the use of these extended\nMCCs for multiclass problems often rely solely on point estimates, potentially\noverlooking the statistical significance and reliability of the results. To\naddress this gap, we introduce several methods for constructing asymptotic\nconfidence intervals for the proposed metrics. Furthermore, we extend these\nmethods to include the construction of asymptotic confidence intervals for\ndifferences in the proposed metrics, specifically for paired study designs. The\nutility of our methods is evaluated through comprehensive simulations and\nreal-world data analyses.",
        "Hydrogen transport in the anode of a proton-exchange membrane fuel cell\n(PEMFC) has been studied with a modulation technique relating the hydrogen\nflow-rate $(\\tilde{Q}_{H2})$ and the faradaic current $(\\tilde{I})$, called\n$\\textit{Current-modulated Hydrogen flow-rate Spectroscopy}$ (CH2S). A simple\nanalytical expression for the transfer function, $H(j{\\omega})=n \\, F \\,\n\\tilde{Q}_{H2} \\mathbin{\/} \\tilde{I}$, is provided, showing a skewed semicircle\nin Nyquist representation ($-H^{''}$ vs. $H^{'}$), extending from $H^{'}=0\\:$\nto $H^{'}=1$, and with the maximum frequency at\n${\\omega}_{max}=2.33\\,(D_{H2}\\mathbin{\/}{L_{i}}^{2})$, where $D_{H2}$ is the\neffective hydrogen diffusivity and $L_i$ the thickness of the anode gas\ndiffusion layer (GDL). The expression for CH2S is also calculated with an\nexisting reversible chemical reaction in the GDL. Experimental results under\ndifferent operation conditions show two transport processes limiting the anode\nreaction, one attributed to molecular diffusion through the partially saturated\nGDL, and the other to the microporous layer (MPL), or its interfaces with GDL\nor with the catalyst layer (CL). CH2S provides the hydrogen diffusivities\n$(D_{H2,i})$ associated to each process under the different conditions. Current\ndensity decreases slightly the diffusivity of the GDL, while it becomes\nactivated in the MPL; using two GDLs in the anode improves both GDL and MPL\ndiffusivities; humidification decreases the diffusivity in both, GDL and MPL;\nfinally, a superhydrophobic anodic CL prepared by electrospray improves\nhydrogen diffusivity in GDL and MPL.",
        "Studies by microbiologists from the 1970s provided robust estimates for the\nenergy supply and demand of a prokaryotic cell. The amount of ATP needed to\nsupport growth was calculated from the chemical composition of the cell and\nknown enzymatic pathways that synthesize its constituents from known substrates\nin culture. Starting in 2015, geneticists and evolutionary biologists began\ninvestigating the bioenergetic role of mitochondria at eukaryote origin and\nenergy in metazoan evolution using their own, widely trusted but hitherto\nunvetted model for the costs of growth in terms of ATP per cell. The more\nrecent model contains, however, a severe and previously unrecognized error that\nsystematically overestimates the ATP cost of amino acid synthesis up to 200\nfold. The error applies to all organisms studied by such models and leads to\nconspicuously false inferences, for example that the synthesis of an average\namino acid in humans requires 30 ATP, which no biochemistry textbook will\nconfirm. Their ATP cost calculations would require that Escherichia coli\nobtains roughly 100 ATP per glucose and that mammals obtain roughly 240 ATP per\nglucose, propositions that invalidate evolutionary inferences so based. By\ncontrast, established methods for estimating the ATP cost of microbial growth\nshow that the first mitochondrial endosymbionts could have easily doubled the\nhosts available ATP pool, provided that genes for growth on environmental amino\nacids were transferred from the mitochondrial symbiont to the archaeal host and\nthat the host for mitochondrial origin was an autotroph using the acetyl-CoA\npathway.",
        "We study the question of volume optimality in split conformal regression, a\ntopic still poorly understood in comparison to coverage control. Using the fact\nthat the calibration step can be seen as an empirical volume minimization\nproblem, we first derive a finite-sample upper-bound on the excess volume loss\nof the interval returned by the classical split method. This important quantity\nmeasures the difference in length between the interval obtained with the split\nmethod and the shortest oracle prediction interval. Then, we introduce EffOrt,\na methodology that modifies the learning step so that the base prediction\nfunction is selected in order to minimize the length of the returned intervals.\nIn particular, our theoretical analysis of the excess volume loss of the\nprediction sets produced by EffOrt reveals the links between the learning and\ncalibration steps, and notably the impact of the choice of the function class\nof the base predictor. We also introduce Ad-EffOrt, an extension of the\nprevious method, which produces intervals whose size adapts to the value of the\ncovariate. Finally, we evaluate the empirical performance and the robustness of\nour methodologies.",
        "Current wireless infrastructure is optimized to support downlink\napplications. This paper anticipates the emergence of applications where\nengineering focus shifts from downlink to uplink. The current paradigm of\nscheduling users on reserved uplink resources is not able to deal efficiently\nwith unpredictable traffic patterns. As a result, 3GPP introduced the 2-step\nRACH as a mechanism to enable grant-free (random) initial access. The first of\nthe two steps is preamble detection in a RACH slot, and in this paper we\ndescribe a low-complexity algorithm for simultaneous detection of multiple\npreambles in the presence of mobility and delay spread. We provide a pathway to\nstandards adoption by choosing ZC sequences as preambles, as ZC sequences\nalready appear in 5G standards. We construct preambles by using the discrete\nZak transform to pass from a ZC sequence of length MN in the TD to a\nquasi-periodic MxN array in the DD domain. There are MN quasi-periodic Dirac\npulses, each corresponding to a Zak-OTFS carrier waveform, and the ZC preamble\nis simply the corresponding sum of Zak-OTFS carrier waveforms. We detect\nmultiple preambles in the presence of mobility and delay spread by sampling the\nreceived signal on the MxN period grid in the DD domain. We approach detection\nas a compressed sensing problem. We represent a preamble as a column of length\nMN in the DD domain and apply discrete shifts in delay and Doppler to produce a\nblock with O(MN) columns in the compressed sensing matrix. The superposition of\nmultiple preambles determines a block sparse sum of columns in the sensing\nmatrix. The correlation properties of ZC sequences result in a highly\nstructured compressed sensing matrix, making it possible to identify\nconstituent preambles using OST, which has complexity O(M^3N^3). In this paper,\nwe describe an algorithm with complexity that is O(M^2N^2) in the size of an\nindividual column.",
        "Fast Radio Bursts (FRBs) have emerged as one of the most dynamic areas of\nresearch in astronomy and cosmology. Despite increasing number of FRBs have\nbeen reported, the exact origin of FRBs remains elusive. Investigating the\nintrinsic distributions of FRBs could provide valuable insights into their\npossible origins and enhance the power of FRBs a cosmological probe. In this\npaper, we propose a hierarchical Bayesian inference approach combining with\nseveral viable models to investigate the population information of FRBs\nreleased in the CHIME catalog 1. By utilizing this method, we aim to uncover\nthe underlying patterns and characteristics of the FRB population. Taking into\naccount the uncertainties and complex relationships within the data. We find\nthat the distribution of FRBs does not trace the history of star formation, and\nthere is evidence that the FRB population has time delay with respect to the\nhistory of star formation.",
        "Large language models excel at interpreting complex natural language\ninstructions, enabling them to perform a wide range of tasks. In the life\nsciences, single-cell RNA sequencing (scRNA-seq) data serves as the \"language\nof cellular biology\", capturing intricate gene expression patterns at the\nsingle-cell level. However, interacting with this \"language\" through\nconventional tools is often inefficient and unintuitive, posing challenges for\nresearchers. To address these limitations, we present InstructCell, a\nmulti-modal AI copilot that leverages natural language as a medium for more\ndirect and flexible single-cell analysis. We construct a comprehensive\nmulti-modal instruction dataset that pairs text-based instructions with\nscRNA-seq profiles from diverse tissues and species. Building on this, we\ndevelop a multi-modal cell language architecture capable of simultaneously\ninterpreting and processing both modalities. InstructCell empowers researchers\nto accomplish critical tasks-such as cell type annotation, conditional\npseudo-cell generation, and drug sensitivity prediction-using straightforward\nnatural language commands. Extensive evaluations demonstrate that InstructCell\nconsistently meets or exceeds the performance of existing single-cell\nfoundation models, while adapting to diverse experimental conditions. More\nimportantly, InstructCell provides an accessible and intuitive tool for\nexploring complex single-cell data, lowering technical barriers and enabling\ndeeper biological insights.",
        "Quantum Reinforcement Learning (QRL) has emerged as a promising research\nfield, leveraging the principles of quantum mechanics to enhance the\nperformance of reinforcement learning (RL) algorithms. However, despite its\ngrowing interest, QRL still faces significant challenges. It is still uncertain\nif QRL can show any advantage over classical RL beyond artificial problem\nformulations. Additionally, it is not yet clear which streams of QRL research\nshow the greatest potential. The lack of a unified benchmark and the need to\nevaluate the reliance on quantum principles of QRL approaches are pressing\nquestions. This work aims to address these challenges by providing a\ncomprehensive comparison of three major QRL classes: Parameterized Quantum\nCircuit based QRL (PQC-QRL) (with one policy gradient (QPG) and one Q-Learning\n(QDQN) algorithm), Free Energy based QRL (FE-QRL), and Amplitude Amplification\nbased QRL (AA-QRL). We introduce a set of metrics to evaluate the QRL\nalgorithms on the widely applicable benchmark of gridworld games. Our results\nprovide a detailed analysis of the strengths and weaknesses of the QRL classes,\nshedding light on the role of quantum principles in QRL and paving the way for\nfuture research in this field.",
        "Instead of computing magneticallly induced (MI) current densities (CD) via\nthe wave function and their quatum mechanical definition one can also use the\ndifferential form of the Amp\\`ere-Maxwell law to obtain them from spatial\nderivatives of the induced magnetic field. In magnetic molecular response\ncalculations, the latter can be done by numerical derivativation of the so\ncalled ``nucleus-independent chemical shifts'' (NICS) which are avaialable to\nmany standard quantum chemical programs. The resulting numerical MICD data is\nin contrast to other numerically obtained MICDs computed via the wave function\nroute, virtually divergence-free.",
        "Memory effect reflects a system's ability to encode, retain and retrieve\ninformation about its past. Such effects are essentially an out-of-equilibrium\nphenomenon providing insight into the complex structural and dynamical behavior\nof the system. Kovacs effect is one such memory effect that is traditionally\nassociated with thermal history. Although studies on the Kovacs-like memory\neffect have been extended to mechanical perturbations such as\ncompression-decompression, whether such effects can also be observed under\nvolume-conserving perturbations like shear, remains unclear. Combining\nexperiments, simulations and linear response theory we demonstrate Kovacs-like\nmemory effect in a sheared colloidal glass. Moreover, we explore the influence\nof non-linear perturbations and establish a correlation between the deviation\nfrom linear response prediction and microscopic non-affine flows generated due\nto such large deformations in affecting the memory effect. Our study not only\nextends Kovacs-like memory effect in the domain of volume-conserving mechanical\nperturbations, it also highlights the importance of the nature of underlying\nmicroscopic flows in controlling the memory effect in amorphous matter.",
        "Understanding convergent learning -- the extent to which artificial and\nbiological neural networks develop similar representations -- is crucial for\nneuroscience and AI, as it reveals shared learning principles and guides\nbrain-like model design. While several studies have noted convergence in early\nand late layers of vision networks, key gaps remain. First, much existing work\nrelies on a limited set of metrics, overlooking transformation invariances\nrequired for proper alignment. We compare three metrics that ignore specific\nirrelevant transformations: linear regression (ignoring affine\ntransformations), Procrustes (ignoring rotations and reflections), and\npermutation\/soft-matching (ignoring unit order). Notably, orthogonal\ntransformations align representations nearly as effectively as more flexible\nlinear ones, and although permutation scores are lower, they significantly\nexceed chance, indicating a robust representational basis. A second critical\ngap lies in understanding when alignment emerges during training. Contrary to\nexpectations that convergence builds gradually with task-specific learning, our\nfindings reveal that nearly all convergence occurs within the first epoch --\nlong before networks achieve optimal performance. This suggests that shared\ninput statistics, architectural biases, or early training dynamics drive\nconvergence rather than the final task solution. Finally, prior studies have\nnot systematically examined how changes in input statistics affect alignment.\nOur work shows that out-of-distribution (OOD) inputs consistently amplify\ndifferences in later layers, while early layers remain aligned for both\nin-distribution and OOD inputs, suggesting that this alignment is driven by\ngeneralizable features stable across distribution shifts. These findings fill\ncritical gaps in our understanding of representational convergence, with\nimplications for neuroscience and AI.",
        "Caribou is a versatile data acquisition system used in multiple collaborative\nframeworks (CERN EP R&D, DRD3, AIDAinnova, Tangerine) for laboratory and\ntest-beam qualification of novel silicon pixel detector prototypes. The system\nis built around a common hardware, firmware and software stack shared accross\ndifferent projects, thereby drastically reducing the development effort and\ncost. It consists of a custom Control and Readout (CaR) board and a commercial\nXilinx Zynq System-on-Chip (SoC) platform. The SoC platform runs a full Yocto\ndistribution integrating the custom software framework (Peary) and a custom\nFPGA firmware built within a common firmware infrastructure (Boreal). The CaR\nboard provides a hardware environment featuring various services such as\npowering, slow-control, and high-speed data links for the target detector\nprototype. Boreal and Peary, in turn, offer firmware and software architectures\nthat enable seamless integration of control and readout for new devices. While\nthe first version of the system used a SoC platform based on the ZC706\nevaluation board, migration to a Zynq UltraScale+ architecture is progressing\ntowards the support of the ZCU102 board and the ultimate objective of\nintegrating the SoC functionality directly into the CaR board, eliminating the\nneed for separate evaluation boards. This paper describes the Caribou system,\nfocusing on the latest project developments and showcasing progress and future\nplans across its hardware, firmware, and software components.",
        "Modern quantum technologies and hybrid quantum systems offer the opportunity\nto utilize magnons on the level of single excitations. Long lifetimes, low\ndecoherence rates, and a strong coupling rate to other subsystems propose the\nferrimagnet yttrium iron garnet (YIG), grown on a gadolinium gallium garnet\n(GGG) substrate, as a suitable platform to host magnonic quantum states.\nHowever, the magnetic damping at cryogenic temperatures significantly increases\ndue to the paramagnetic character and the highly inhomogeneous stray field of\nGGG, as recent experiments and simulations pointed out. Here, we report on\ntemperature dependent ferromagnetic resonance (FMR) spectroscopy studies in\nYIG-GGG thin-films with different sample geometries. We experimentally\ndemonstrate how to eliminate the asymmetric stray field-induced linewidth\nbroadening via microstructuring of the YIG film. Additionally, our experiments\nreveal evidence of a non-Gilbert like behavior of the linewidth at cryogenic\ntemperatures, independent of the inhomogeneous GGG stray field.",
        "Optical nuclear electric resonance (ONER), a recently proposed protocol for\nnuclear spin manipulation in atomic systems via short laser pulses with MHz\nrepetition rate, exploits the coupling between the nuclear quadrupole moment of\na suitable atom and the periodic modulations of the electric field gradient\ngenerated by an optically stimulated electronic excitation. In this theory\npaper, we extend the scope of ONER from atomic to molecular systems and show\nthat molecular vibrations do not interfere with our protocol. Exploring the\ndiatomic molecule LiNa as a first benchmark system, our investigation showcases\nthe robustness with respect to molecular vibration, and the ability to address\nand manipulate each of the two nuclear spins independently, simply by adjusting\nthe repetition rate of a pulsed laser. Our findings suggest that it might be\npossible to shift complicated spin manipulation tasks required for quantum\ncomputing into the time domain by pulse-duration encoded laser signals.",
        "Uncertainty quantification in neural networks through methods such as\nDropout, Bayesian neural networks and Laplace approximations is either prone to\nunderfitting or computationally demanding, rendering these approaches\nimpractical for large-scale datasets. In this work, we address these\nshortcomings by shifting the focus from uncertainty in the weight space to\nuncertainty at the activation level, via Gaussian processes. More specifically,\nwe introduce the Gaussian Process Activation function (GAPA) to capture\nneuron-level uncertainties. Our approach operates in a post-hoc manner,\npreserving the original mean predictions of the pre-trained neural network and\nthereby avoiding the underfitting issues commonly encountered in previous\nmethods. We propose two methods. The first, GAPA-Free, employs empirical kernel\nlearning from the training data for the hyperparameters and is highly efficient\nduring training. The second, GAPA-Variational, learns the hyperparameters via\ngradient descent on the kernels, thus affording greater flexibility. Empirical\nresults demonstrate that GAPA-Variational outperforms the Laplace approximation\non most datasets in at least one of the uncertainty quantification metrics.",
        "Examining the extent to which measurements of rotation matrices are close to\neach other is challenging due measurement noise. To overcome this, data is\ntypically smoothed and Riemannian and Euclidean metrics are applied. However,\nif rotation matrices are not directly measured and are instead formed by\neigenvectors of measured symmetric matrices, this can be problematic if the\nassociated eigenvalues are close. In this work, we propose novel semi-metrics\nthat can be used to approximate the Riemannian metric for small angles. Our new\nresults do not require eigenvector information and are beneficial for measured\ndatasets. There are also issues when using comparing rotational data arising\nfrom computational simulations and it is important that the impact of the\napproximations on the computed outputs is properly assessed to ensure that the\napproximations made and the finite precision arithmetic are not unduly\npolluting the results. In this work, we examine data arising from object\ncharacterisation in metal detection using the complex symmetric rank two\nmagnetic polarizability tensor (MPT) description, we rigorously analyse the\neffects of our numerical approximations and apply our new approximate measures\nof distance to the commutator of the real and imaginary parts of the MPT to\nthis application. Our new approximate measures of distance provide additional\nfeature information, which is invariant of the object orientation, to aid with\nobject identification using machine learning classifiers. We present Bayesian\nclassification examples to demonstrate the success of our approach.",
        "Let $S_n=K[x_1,\\ldots,x_n,y]$ and $I_n=(x_1y,x_2y,\\ldots,x_ny)\\subset S_n$ be\nthe edge ideal of star graph. We prove that $\\operatorname{hdepth}(S_n\/I_n)\\geq\n\\left\\lceil \\frac{n}{2} \\right\\rceil + \\left\\lfloor \\sqrt{n} \\right\\rfloor -\n2$. Also, we show that for any $\\varepsilon>0$, there exists some integer\n$A=A(\\varepsilon)\\geq 0$ such that $\\operatorname{hdepth}(S_n\/I_n)\\leq\n\\left\\lceil \\frac{n}{2} \\right\\rceil + \\left\\lfloor \\varepsilon n \\right\\rfloor\n+ A - 2$. We deduce that $\\lim\\limits_{n\\to\\infty}\n\\frac{1}{n}\\operatorname{hdepth}(S_n\/I_n) = \\frac{1}{2}$."
      ]
    }
  },
  {
    "id":2411.19345,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"On the use of preclinical imaging to explore the principles of brain function in rodent models and their relevance for illnesses of the human mind",
    "start_abstract":"Dear Editor, We recently published in Translational Psychiatry an article that examine the strategies for evaluating brain function at the wholebrain level [1]. In this review, we covered several methods, from functional MRI to functional ultrasound to calcium imaging. For each technique, we wrote a brief history of its development, the physical notion, some key applications, its potentials, and its limitations. We concluded that methods for imaging the rodent brain at the network level are growing and will advance our understanding of brain function. A commentary by Zhuo and colleagues further enhances the complexity of addressing the issue of \u201ctranslation\u201d from animal models to patients for the discipline of psychiatry [2]. They propose that the approaches employed to develop an animal model for a psychiatric disease need to be thoroughly scrutinized and, perhaps, revised. For example, most rodent models of mental diseases are to-date established using a simple pharmacological infusion [3] and\/or psychosocial stimulation [4]. The key concern posed, however, is how these manipulations change the brain\u2019s structure and function, and whether these models genuinely reflect the pathophysiology of human mental illnesses. Especially since it is difficult to evaluate whether one can speak of inverse inference from rodents to humans. This is a true and acceptable statement. However, this is exactly what preclinical imaging aims to deliver. By mapping the dynamic responses of brain networks in animal models and compare them, if possible, with those reported in clinical studies, we can obtain quantitative data and parameters to establish whether our models are effectively translational [5]. If these metrics demonstrate temporal and spatial similarity in network-level modifications as those observed in humans, we can pursue further inquiry utilizing more intrusive and more specific methods for brain recordings in animal models. Otherwise, we must have the confidence and the correctness to move forward and attempt other solutions. Two recent examples. In 2019 we established a causal association between activity of the noradrenergic nucleus locus coeruleus (LC) and the engagement of numerous large-scale brain networks in mice, in particular of the salience and amygdala networks [6]. In addition, we could link network-changes with direct markers of norepinephrine (NE) turnover and with the distribution of NE receptors over the entire brain. The hypothesis that specific brain networks dynamics are related to LC activity and to NE receptor density derives from stress-research and pharmacological studies in humans [7, 8]. However, since it is impossible to selectively stimulate LC in people, it has remained a hypothesis for more than a decade. Our preclinical work helped confirm this causal relationship and this has direct implications for interpreting the results of clinical imaging studies on stress and anxiety behavior. More recently, the Gozzi lab described how chronic local neuronal suppression via overexpression of a potassium channel or acute silencing via chemogenetics result in a paradoxical hyperconnectivity [9]; an intriguing finding often reported in humans after stroke [10] and in early stages of Alzheimer\u2019s disease [11], but never truly understood. Using in vivo electrophysiology, they showed local inhibition improves low frequency (0.1\u20134 Hz) oscillatory power via suppression of neuronal activity not phaselocked to slow rhythms, resulting in increased slow and \u03b4 band coherence between areas that display fMRI overconnectivity. These data present causal evidence that cortical inactivation can counterintuitively augment fMRI connectivity via greater, lesslocalized slow oscillatory processes. Once again, this could be only achieved by combining functional MRI and electrophysiology with neuromodulation in animal models. These and other examples give a peek of what the future of preclinical imaging might look like: a field of research capable of delivering causal explanations to the hypotheses presented by human neuroscience, neurology and psychiatry. Lastly, I would argue against statements like \u201cthe computational complexity of human brains is billions of times that of mouse brain\u201d. While this may be true from a numerical standpoint of mere neuronal counts, preclinical neuroimaging\u2019s objective should not be per se to map every single neuron in real time but of identifying the general neural and cellular principles governing the assembly of brain networks and its breakdown in brain disorders. The field is relatively new but is moving fast and has already produced some important insights. The future is challenging and will require time, devotion and an optimal synergy between engineering, chemistry, biology, and computer science. If the community will be patient and supportive enough, there will be further important discoveries in the future.",
    "start_categories":[
      "Psychiatry"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b8"
      ],
      "title":[
        "GAN\uff08Generative Adversarial Nets\uff09"
      ],
      "abstract":[
        "We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: model G that captures the data distribution, and discriminative D estimates probability sample came from training rather than G. The procedure is to maximize of making mistake. This corresponds minimax two-player game. In space arbitrary functions D, unique solution exists, with recovering distribution equal \u00bd everywhere. case where are defined by multilayer perceptrons, entire system can be trained backpropagation. There no need any Markov chains or unrolled approximate inference networks during either generation samples. Experiments demonstrate potential through qualitative quantitative evaluation generated"
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "Multi-messenger observations in the Einstein Telescope era: binary\n  neutron star and black hole - neutron star mergers",
        "On invex functions with same {\\eta} in single and multivalued nonsmooth\n  optimization with Clarke's subdifferential",
        "MagicComp: Training-free Dual-Phase Refinement for Compositional Video\n  Generation",
        "Probing LLMs for Multilingual Discourse Generalization Through a Unified\n  Label Set",
        "Emulating Retrieval Augmented Generation via Prompt Engineering for\n  Enhanced Long Context Comprehension in LLMs",
        "Flexible and Efficient Grammar-Constrained Decoding",
        "Efficient Risk-sensitive Planning via Entropic Risk Measures",
        "CER: Confidence Enhanced Reasoning in LLMs",
        "Quasimodular Asymptotics of Spherical Integrals",
        "An Improved Deep Learning Model for Word Embeddings Based Clustering for\n  Large Text Datasets",
        "Transformer-Based Nonlinear Transform Coding for Multi-Rate CSI\n  Compression in MIMO-OFDM Systems",
        "ACF-Monotonicity Formula on RCD(0,N) Metric Measure Cones",
        "Generative AI for Vision: A Comprehensive Study of Frameworks and\n  Applications",
        "From Natural Language to Extensive-Form Game Representations",
        "Spherically symmetric electrically counterpoised dust either collapses\n  or disperses",
        "Sensing-based Robustness Challenges in Agricultural Robotic Harvesting",
        "Dictionary-Learning-Based Data Pruning for System Identification",
        "Initial Guess Generation for Low-Thrust Trajectory Design with\n  Robustness to Missed-Thrust-Events",
        "Machine Learning for Health symposium 2024 -- Findings track",
        "Quantum Regular Black Holes and Complete Monotonicity",
        "Encrypted Qubits can be Cloned",
        "On Some Fundamental Problems for Multi-Agent Systems Over Multilayer\n  Networks",
        "Semantic Shift Estimation via Dual-Projection and Classifier\n  Reconstruction for Exemplar-Free Class-Incremental Learning",
        "FloPE: Flower Pose Estimation for Precision Pollination",
        "Ancient Greek Technology: An Immersive Learning Use Case Described Using\n  a Co-Intelligent Custom ChatGPT Assistant",
        "TPC: Cross-Temporal Prediction Connection for Vision-Language Model\n  Hallucination Reduction",
        "A search for close binary systems in the SALT survey of\n  hydrogen-deficient stars using {\\it TESS}",
        "Spectral properties of bottomonium at high temperature: a systematic\n  investigation",
        "Radii of light nuclei from the Jacobi No-Core Shell Model"
      ],
      "abstract":[
        "The Einstein Telescope (ET), a proposed next-generation gravitational wave\n(GW) observatory, will expand the reach of GW astronomy of stellar-mass compact\nobject binaries to unprecedented distances, enhancing opportunities for\nmulti-messenger observations. Here we investigate multi-messenger emission\nproperties of binary neutron star (NSNS) and black hole-neutron star (BHNS)\nmergers detectable by ET, providing projections to optimize observational\nstrategies and maximize scientific insights from these sources. Using a\nsynthetic population of compact binary mergers, we model each source's GW\nsignal-to-noise ratio, sky localization uncertainty, kilonova (KN) light curves\nin optical and near-infrared bands, fluence of the relativistic jet gamma-ray\nburst (GRB) prompt emission and afterglow light curves across radio, optical,\nX-ray and very high energy wavelengths. We analyze multi-messenger\ndetectability prospects for ET as a standalone observatory with two different\nconfigurations and within a network of next-generation GW detectors. ET will\ndetect over $10^4$ NSNS mergers annually, enabling potential observation of\ntens to hundreds of electromagnetic (EM) counterparts. BHNS mergers have more\nlimited multi-messenger prospects, but joint GW-EM rates will increase by an\norder of magnitude compared to current-generation instruments. We quantify\nuncertainties due to the NS equation of state (EoS) and mass distribution of\nNSNSs, as well as the NS EoS and BH spin for BHNSs. While a single ET will\nachieve an impressive GW detection rate, the fraction of well-localized events\nis orders of magnitude lower than in a network with additional detectors. This\nsignificantly limits efficient EM follow-up and science cases requiring\nwell-characterized counterparts or early observations. The challenge is even\ngreater for BHNS mergers due to their low EM rate.",
        "In this paper, a finite family of nonsmooth locally Lipschitz continuous\nfunctions that are invex with respect to the same function {\\eta} are\ncharacterized in terms of their scalarized counterparts.",
        "Text-to-video (T2V) generation has made significant strides with diffusion\nmodels. However, existing methods still struggle with accurately binding\nattributes, determining spatial relationships, and capturing complex action\ninteractions between multiple subjects. To address these limitations, we\npropose MagicComp, a training-free method that enhances compositional T2V\ngeneration through dual-phase refinement. Specifically, (1) During the\nConditioning Stage: We introduce the Semantic Anchor Disambiguation to\nreinforces subject-specific semantics and resolve inter-subject ambiguity by\nprogressively injecting the directional vectors of semantic anchors into\noriginal text embedding; (2) During the Denoising Stage: We propose Dynamic\nLayout Fusion Attention, which integrates grounding priors and model-adaptive\nspatial perception to flexibly bind subjects to their spatiotemporal regions\nthrough masked attention modulation. Furthermore, MagicComp is a model-agnostic\nand versatile approach, which can be seamlessly integrated into existing T2V\narchitectures. Extensive experiments on T2V-CompBench and VBench demonstrate\nthat MagicComp outperforms state-of-the-art methods, highlighting its potential\nfor applications such as complex prompt-based and trajectory-controllable video\ngeneration. Project page: https:\/\/hong-yu-zhang.github.io\/MagicComp-Page\/.",
        "Discourse understanding is essential for many NLP tasks, yet most existing\nwork remains constrained by framework-dependent discourse representations. This\nwork investigates whether large language models (LLMs) capture discourse\nknowledge that generalizes across languages and frameworks. We address this\nquestion along two dimensions: (1) developing a unified discourse relation\nlabel set to facilitate cross-lingual and cross-framework discourse analysis,\nand (2) probing LLMs to assess whether they encode generalizable discourse\nabstractions. Using multilingual discourse relation classification as a\ntestbed, we examine a comprehensive set of 23 LLMs of varying sizes and\nmultilingual capabilities. Our results show that LLMs, especially those with\nmultilingual training corpora, can generalize discourse information across\nlanguages and frameworks. Further layer-wise analyses reveal that language\ngeneralization at the discourse level is most salient in the intermediate\nlayers. Lastly, our error analysis provides an account of challenging relation\nclasses.",
        "This paper addresses the challenge of comprehending very long contexts in\nLarge Language Models (LLMs) by proposing a method that emulates Retrieval\nAugmented Generation (RAG) through specialized prompt engineering and\nchain-of-thought (CoT) reasoning. While recent LLMs support over 100,000 tokens\nin a single prompt, simply enlarging context windows has not guaranteed robust\nmulti-hop reasoning when key details are scattered across massive input. Our\napproach treats the model as both the retriever and the reasoner: it first tags\nrelevant segments within a long passage, then employs a stepwise CoT workflow\nto integrate these pieces of evidence. This single-pass method thereby reduces\nreliance on an external retriever, yet maintains focus on crucial segments. We\nevaluate our approach on selected tasks from BABILong, which interleaves\nstandard bAbI QA problems with large amounts of distractor text. Compared to\nbaseline (no retrieval) and naive RAG pipelines, our approach more accurately\nhandles multi-fact questions such as object location tracking, counting, and\nindefinite knowledge. Furthermore, we analyze how prompt structure, including\nthe order of question, relevant-text tags, and overall instructions,\nsignificantly affects performance. These findings underscore that optimized\nprompt engineering, combined with guided reasoning, can enhance LLMs'\nlong-context comprehension and serve as a lightweight alternative to\ntraditional retrieval pipelines.",
        "Large Language Models (LLMs) are often asked to generate structured outputs\nthat obey precise syntactic rules, such as code snippets or formatted data.\nGrammar-constrained decoding (GCD) can guarantee that LLM outputs matches such\nrules by masking out tokens that will provably lead to outputs that do not\nbelong to a specified context-free grammar (CFG). To guarantee soundness, GCD\nalgorithms have to compute how a given LLM subword tokenizer can align with the\ntokens used\n  by a given context-free grammar and compute token masks based on this\ninformation. Doing so efficiently is challenging and existing GCD algorithms\nrequire tens of minutes to preprocess common grammars. We present a new GCD\nalgorithm together with an implementation that offers 17.71x faster offline\npreprocessing than existing approaches while preserving state-of-the-art\nefficiency in online mask computation.",
        "Risk-sensitive planning aims to identify policies maximizing some\ntail-focused metrics in Markov Decision Processes (MDPs). Such an optimization\ntask can be very costly for the most widely used and interpretable metrics such\nas threshold probabilities or (Conditional) Values at Risk. Indeed, previous\nwork showed that only Entropic Risk Measures (EntRM) can be efficiently\noptimized through dynamic programming, leaving a hard-to-interpret parameter to\nchoose. We show that the computation of the full set of optimal policies for\nEntRM across parameter values leads to tight approximations for the metrics of\ninterest. We prove that this optimality front can be computed effectively\nthanks to a novel structural analysis and smoothness properties of entropic\nrisks. Empirical results demonstrate that our approach achieves strong\nperformance in a variety of decision-making scenarios.",
        "Ensuring the reliability of Large Language Models (LLMs) in complex reasoning\ntasks remains a formidable challenge, particularly in scenarios that demand\nprecise mathematical calculations and knowledge-intensive open-domain\ngeneration. In this work, we introduce an uncertainty-aware framework designed\nto enhance the accuracy of LLM responses by systematically incorporating model\nconfidence at critical decision points. We propose an approach that encourages\nmulti-step reasoning in LLMs and quantify the confidence of intermediate\nanswers such as numerical results in mathematical reasoning and proper nouns in\nopen-domain generation. Then, the overall confidence of each reasoning chain is\nevaluated based on confidence of these critical intermediate steps. Finally, we\naggregate the answer of generated response paths in a way that reflects the\nreliability of each generated content (as opposed to self-consistency in which\neach generated chain contributes equally to majority voting). We conducted\nextensive experiments in five datasets, three mathematical datasets and two\nopen-domain datasets, using four LLMs. The results consistently validate the\neffectiveness of our novel confidence aggregation method, leading to an\naccuracy improvement of up to 7.4% and 5.8% over baseline approaches in math\nand open-domain generation tasks, respectively. Code is publicly available at\nhttps:\/\/github.com\/ Aquasar11\/CER.",
        "We show that the spherical integral of the Circular Unitary Ensemble\nconverges in expectation to Euler's generating function for integer partitions,\nand that subleading corrections to this high-dimensional limit are quasimodular\nforms.",
        "In this paper, an improved clustering technique for large textual datasets by\nleveraging fine-tuned word embeddings is presented. WEClustering technique is\nused as the base model. WEClustering model is fur-ther improvements\nincorporating fine-tuning contextual embeddings, advanced dimensionality\nreduction methods, and optimization of clustering algorithms. Experimental\nresults on benchmark datasets demon-strate significant improvements in\nclustering metrics such as silhouette score, purity, and adjusted rand index\n(ARI). An increase of 45% and 67% of median silhouette score is reported for\nthe proposed WE-Clustering_K++ (based on K-means) and WEClustering_A++ (based\non Agglomerative models), respec-tively. The proposed technique will help to\nbridge the gap between semantic understanding and statistical robustness for\nlarge-scale text-mining tasks.",
        "We propose a novel approach for channel state information (CSI) compression\nin multiple-input multiple-output orthogonal frequency division multiplexing\n(MIMO-OFDM) systems, where the frequency-domain channel matrix is treated as a\nhigh-dimensional complex-valued image. Our method leverages transformer-based\nnonlinear transform coding (NTC), an advanced deep-learning-driven image\ncompression technique that generates a highly compact binary representation of\nthe CSI. Unlike conventional autoencoder-based CSI compression, NTC optimizes a\nnonlinear mapping to produce a latent vector while simultaneously estimating\nits probability distribution for efficient entropy coding. By exploiting the\nstatistical independence of latent vector entries, we integrate a\ntransformer-based deep neural network with a scalar nested-lattice uniform\nquantization scheme, enabling low-complexity, multi-rate CSI feedback that\ndynamically adapts to varying feedback channel conditions. The proposed\nmulti-rate CSI compression scheme achieves state-of-the-art rate-distortion\nperformance, outperforming existing techniques with the same number of neural\nnetwork parameters. Simulation results further demonstrate that our approach\nprovides a superior rate-distortion trade-off, requiring only 6% of the neural\nnetwork parameters compared to existing methods, making it highly efficient for\npractical deployment.",
        "The ACF-monotonicity formula is a powerful tool in the study of two-phase\nfree boundary problems, which was introduced by Alt, Caffarelli, and\nFriedman[1]. In this paper, we extend it to RCD(0,N) metric measure cones. As\nan application, we give a rigidity result for RCD(0,N) metric measure cones.",
        "Generative AI is transforming image synthesis, enabling the creation of\nhigh-quality, diverse, and photorealistic visuals across industries like\ndesign, media, healthcare, and autonomous systems. Advances in techniques such\nas image-to-image translation, text-to-image generation, domain transfer, and\nmultimodal alignment have broadened the scope of automated visual content\ncreation, supporting a wide spectrum of applications. These advancements are\ndriven by models like Generative Adversarial Networks (GANs), conditional\nframeworks, and diffusion-based approaches such as Stable Diffusion. This work\npresents a structured classification of image generation techniques based on\nthe nature of the input, organizing methods by input modalities like noisy\nvectors, latent representations, and conditional inputs. We explore the\nprinciples behind these models, highlight key frameworks including DALL-E,\nControlNet, and DeepSeek Janus-Pro, and address challenges such as\ncomputational costs, data biases, and output alignment with user intent. By\noffering this input-centric perspective, this study bridges technical depth\nwith practical insights, providing researchers and practitioners with a\ncomprehensive resource to harness generative AI for real-world applications.",
        "We introduce a framework for translating game descriptions in natural\nlanguage into extensive-form representations in game theory, leveraging Large\nLanguage Models (LLMs) and in-context learning. Given the varying levels of\nstrategic complexity in games, such as perfect versus imperfect information,\ndirectly applying in-context learning would be insufficient. To address this,\nwe introduce a two-stage framework with specialized modules to enhance\nin-context learning, enabling it to divide and conquer the problem effectively.\nIn the first stage, we tackle the challenge of imperfect information by\ndeveloping a module that identifies information sets along and the\ncorresponding partial tree structure. With this information, the second stage\nleverages in-context learning alongside a self-debugging module to produce a\ncomplete extensive-form game tree represented using pygambit, the Python API of\na recognized game-theoretic analysis tool called Gambit. Using this python\nrepresentation enables the automation of tasks such as computing Nash\nequilibria directly from natural language descriptions. We evaluate the\nperformance of the full framework, as well as its individual components, using\nvarious LLMs on games with different levels of strategic complexity. Our\nexperimental results show that the framework significantly outperforms baseline\nmodels in generating accurate extensive-form games, with each module playing a\ncritical role in its success.",
        "We explore the dynamical evolution of spherically symmetric objects made of\nelectrically counterpoised dust in general relativity. It has been claimed that\nthese objects are in neutral equilibrium and, therefore, that black hole\nmimickers made of electrically counterpoised dust are feasible. Here we show\nthat if a velocity is imparted to the fluid elements, no matter how small, the\nevolution leads either to a black hole or to the dispersion of the fluid.\nFurthermore, in the case of collapse, the resulting object is necessarily an\nextremal Reissner-Nordstr\\\"om black hole.",
        "This paper presents the challenges agricultural robotic harvesters face in\ndetecting and localising fruits under various environmental disturbances. In\ncontrolled laboratory settings, both the traditional HSV (Hue Saturation Value)\ntransformation and the YOLOv8 (You Only Look Once) deep learning model were\nemployed. However, only YOLOv8 was utilised in outdoor experiments, as the HSV\ntransformation was not capable of accurately drawing fruit contours.\nExperiments include ten distinct fruit patterns with six apples and six\noranges. A grid structure for homography (perspective) transformation was\nemployed to convert detected midpoints into 3D world coordinates. The\nexperiments evaluated detection and localisation under varying lighting and\nbackground disturbances, revealing accurate performance indoors, but\nsignificant challenges outdoors. Our results show that indoor experiments using\nYOLOv8 achieved 100% detection accuracy, while outdoor conditions decreased\nperformance, with an average accuracy of 69.15% for YOLOv8 under direct\nsunlight. The study demonstrates that real-world applications reveal\nsignificant limitations due to changing lighting, background disturbances, and\ncolour and shape variability. These findings underscore the need for further\nrefinement of algorithms and sensors to enhance the robustness of robotic\nharvesters for agricultural use.",
        "System identification is normally involved in augmenting time series data by\ntime shifting and nonlinearisation (via polynomial basis), which introduce\nredundancy both feature-wise and sample-wise. Many research works focus on\nreducing redundancy feature-wise, while less attention is paid to sample-wise\nredundancy. This paper proposes a novel data pruning method, called\n(mini-batch) FastCan, to reduce sample-wise redundancy based on dictionary\nlearning. Time series data is represented by some representative samples,\ncalled atoms, via dictionary learning. The useful samples are selected based on\ntheir correlation with the atoms. The method is tested on one simulated dataset\nand two benchmark datasets. The R-squared between the coefficients of models\ntrained on the full and the coefficients of models trained on pruned datasets\nis adopted to evaluate the performance of data pruning methods. It is found\nthat the proposed method significantly outperforms the random pruning method.",
        "The growing interest in cislunar space exploration in recent years has driven\nan increasing demand for efficient low-thrust missions to key cislunar orbits.\nThese missions, typically possessing long thrust arcs, are particularly\nsusceptible to operational uncertainties such as missed thrust events.\nAddressing these challenges requires efficient robust trajectory design\nframeworks during the preliminary mission design phase, where it is necessary\nto explore the solution space at a rapid cadence under evolving operational\nconstraints. However, existing methods for missed thrust design rely on solving\nhigh-dimensional nonlinear programs, where generating effective initial guesses\nbecomes challenging. To enhance computational efficiency, quality, and depth of\nrobustness of solutions from global search, we compare two initial guess\nstrategies: a baseline non-conditional global search, which samples from a\nstatic distribution with global support, and a conditional global search, which\ngenerates initial guesses conditioned on solutions to problems with less depth\nof robustness. The conditional search provides a sequential procedure for\nsolving increasingly robust problems. We validate the improvements in the\nconditional approach using a low-thrust case study for the Lunar Gateway Power\nand Propulsion Element, where our results demonstrate that it significantly\nimproves convergence rate and solution quality, highlighting its potential in\npreliminary robust trajectory design.",
        "A collection of the accepted Findings papers that were presented at the 4th\nMachine Learning for Health symposium (ML4H 2024), which was held on December\n15-16, 2024, in Vancouver, BC, Canada. ML4H 2024 invited high-quality\nsubmissions describing innovative research in a variety of health-related\ndisciplines including healthcare, biomedicine, and public health. Works could\nbe submitted to either the archival Proceedings track, or the non-archival\nFindings track. The Proceedings track targeted mature, cohesive works with\ntechnical sophistication and high-impact relevance to health. The Findings\ntrack promoted works that would spark new insights, collaborations, and\ndiscussions at ML4H. Both tracks were given the opportunity to share their work\nthrough the in-person poster session. All the manuscripts submitted to ML4H\nSymposium underwent a double-blind peer-review process.",
        "We examine the conjecture for the complete monotonicity of certain curvature\ninvariants for quantum black holes. In this note, we study a class of quantum\nregular black holes that are static, spherically symmetric, and characterized\nonly by their mass. Additionally, this class of black holes reduces to the\nSchwarzschild solution in the classical limit $\\hbar\\to 0$. We provide evidence\nsupporting the non-perturbativity conjecture that perturbative corrections\ncannot falsify complete monotonicity. We demonstrate that these quantum black\nholes cannot be generated by perturbative quantum corrections to the Einstein\nequations. We then investigate the thermodynamics of these black holes and\nderive a bound on their entropy, showing that the entropy is always greater\nthan the horizon area divided by 4G. We Also demonstrate that these black holes\nexhibit a bounded temperature, with a maximum temperature scaling as\n$T\\sim\\frac{1}{L_p}$ and a critical mass scale where the temperature vanishes",
        "We show that encrypted cloning of unknown quantum states is possible. Any\nnumber of encrypted clones of a qubit can be created through a unitary\ntransformation, and each of the encrypted clones can be decrypted through a\nunitary transformation. The decryption of an encrypted clone consumes the\ndecryption key, i.e., only one decryption is possible, in agreement with the\nno-cloning theorem. A possible application of encrypted cloning is to enable\nencrypted quantum multi-cloud storage. Beyond applications in cryptography,\nencrypted cloning could provide a form of redundancy, parallelism, fault\ntolerance or scalability where direct duplication is forbidden by the\nno-cloning theorem.",
        "Many researchers have considered multi-agent systems over single-layer\nnetworks as models for studying diffusion phenomena. Since real-world networks\ninvolve connections between agents with different semantics (e.g., family\nmember, friend, colleague), the study of multi-agent systems over multilayer\nnetworks has assumed importance. Our focus is on one class of multi-agent\nsystem models over multilayer networks, namely multilayer synchronous dynamical\nsystems (MSyDSs). We study several fundamental problems for this model. We\nestablish properties of the phase spaces of MSyDSs and bring out interesting\ndifferences between single-layer and multilayer dynamical systems. We show\nthat, in general, the problem of determining whether two given MSyDSs are\ninequivalent is NP-complete. This hardness result holds even when the only\ndifference between the two systems is the local function at just one node in\none layer. We also present efficient algorithms for the equivalence problem for\nrestricted versions of MSyDSs (e.g., systems where each local function is a\nbounded-threshold function, systems where the number of layers is fixed and\neach local function is symmetric). In addition, we investigate the expressive\npower of MSyDSs based on the number of layers. In particular, we examine\nconditions under which a system with k >= 2 layers has an equivalent system\nwith k-1 or fewer layers.",
        "Exemplar-Free Class-Incremental Learning (EFCIL) aims to sequentially learn\nfrom distinct categories without retaining exemplars but easily suffers from\ncatastrophic forgetting of learned knowledge. While existing EFCIL methods\nleverage knowledge distillation to alleviate forgetting, they still face two\ncritical challenges: semantic shift and decision bias. Specifically, the\nembeddings of old tasks shift in the embedding space after learning new tasks,\nand the classifier becomes biased towards new tasks due to training solely with\nnew data, thereby hindering the balance between old and new knowledge. To\naddress these issues, we propose the Dual-Projection Shift Estimation and\nClassifier Reconstruction (DPCR) approach for EFCIL. DPCR effectively estimates\nsemantic shift through a dual-projection, which combines a learnable\ntransformation with a row-space projection to capture both task-wise and\ncategory-wise shifts. Furthermore, to mitigate decision bias, DPCR employs\nridge regression to reformulate classifier training as a reconstruction\nprocess. This reconstruction exploits previous information encoded in\ncovariance and prototype of each class after calibration with estimated shift,\nthereby reducing decision bias. Extensive experiments demonstrate that, across\nvarious datasets, DPCR effectively balances old and new tasks, outperforming\nstate-of-the-art EFCIL methods.",
        "This study presents Flower Pose Estimation (FloPE), a real-time flower pose\nestimation framework for computationally constrained robotic pollination\nsystems. Robotic pollination has been proposed to supplement natural\npollination to ensure global food security due to the decreased population of\nnatural pollinators. However, flower pose estimation for pollination is\nchallenging due to natural variability, flower clusters, and high accuracy\ndemands due to the flowers' fragility when pollinating. This method leverages\n3D Gaussian Splatting to generate photorealistic synthetic datasets with\nprecise pose annotations, enabling effective knowledge distillation from a\nhigh-capacity teacher model to a lightweight student model for efficient\ninference. The approach was evaluated on both single and multi-arm robotic\nplatforms, achieving a mean pose estimation error of 0.6 cm and 19.14 degrees\nwithin a low computational cost. Our experiments validate the effectiveness of\nFloPE, achieving up to 78.75% pollination success rate and outperforming prior\nrobotic pollination techniques.",
        "Achieving consistency in immersive learning case descriptions is essential\nbut challenging due to variations in research focus, methodology, and\nresearchers' background. We address these challenges by leveraging the\nImmersive Learning Case Sheet (ILCS), a methodological instrument to\nstandardize case descriptions, that we applied to an immersive learning case on\nancient Greek technology in VRChat. Research team members had differing levels\nof familiarity with the ILCS and the case content, so we developed a custom\nChatGPT assistant to facilitate consistent terminology and process alignment\nacross the team. This paper constitutes an example of how structured case\nreports can be a novel contribution to immersive learning literature. Our\nfindings demonstrate how the ILCS supports structured reflection and\ninterpretation of the case. Further we report that the use of a ChatGPT\nassistant significantly sup-ports the coherence and quality of the team members\ndevelopment of the final ILCS. This exposes the potential of employing\nAI-driven tools to enhance collaboration and standardization of research\npractices in qualitative educational research. However, we also discuss the\nlimitations and challenges, including reliance on AI for interpretive tasks and\nmanaging varied levels of expertise within the team. This study thus provides\ninsights into the practical application of AI in standardizing immersive\nlearning research processes.",
        "Vision-language models (VLMs) have achieved remarkable advancements,\ncapitalizing on the impressive capabilities of large language models (LLMs)\nacross diverse tasks. Despite this, a critical challenge known as hallucination\noccurs when models overconfidently describe objects or attributes absent from\nthe image, a problem exacerbated by the tendency of VLMs to rely on linguistic\npriors. This limitation reduces model reliability in high-stakes applications.\nIn this work, we have observed the characteristic of logits' continuity\nconsistency enhancement and introduced a straightforward and efficient method,\nCross-Temporal Prediction Connection (TPC), designed to enhance the semantic\nconsistency of logits by connecting them temporally across timesteps. TPC\namplifies information flow and improves coherence, effectively reducing\nhallucination. Extensive experiments show that TPC surpasses existing\nrepresentatives, delivering superior performance in both accuracy and\nefficiency while maintaining robustness in open-ended text generation tasks.",
        "The {\\it TESS} periodograms of the SALT survey catalogue of\nhydrogen-deficient stars were searched for evidence of short-period\nvariability. Periodic light curve variations were identified in 16 stars out of\n153 catalogue objects, of which 10 were false positives. From the remaining 6\nidentified variables, Ton S 415 is a known close binary system and the sixth\nclose binary containing a hydrogen-deficient hot subdwarf. Radial velocity and\nSED analyses ruled out the remaining 5 as close binary systems; the causes of\ntheir variability remain uncertain. With one or more K-type companions, BPS CS\n22956-0094 may be a wide binary or triple. From this SALT+{\\it TESS} sample,\nthe fraction of close binaries stands at $ 1\/29 \\approx 3.5\\%$ for intermediate\nhelium hot subdwarfs and $0\/124 = 0\\%$ for extreme helium subdwarfs.",
        "We investigate spectral features of bottomonium at high temperature, in\nparticular the thermal mass shift and width of ground state S-wave and P-wave\nstate. We employ and compare a range of methods for determining these features\nfrom lattice NRQCD correlators, including direct correlator analyses\n(multi-exponential fits and moments of spectral functions), linear methods\n(Backus-Gilbert, Tikhonov and HLT methods), and Bayesian methods for spectral\nfunction reconstruction (MEM and BR). We comment on the reliability and\nlimitations of the various methods.",
        "Accurately determining the size of the atomic nucleus with realistic nuclear\nforces is a long outstanding issue of nuclear physics. The no-core shell model\n(NCSM), one of the powerful ab initio methods for nuclear structure, can\nachieve accurate energies of light nuclei. The extraction of converged radii is\nmore difficult. In this work, we present a novel method to effectively extract\nthe radius of light nuclei by restoring the long-range behavior of densities\nfrom NCSM calculations. The correct large distance asymptotic of two-body\nrelative densities are deduced based on the NCSM densities in limited basis\nsize. The resulting radii using the corrected densities show a nice\nconvergence. The root-mean-square matter and charge radii of $^{4,6,8}$He and\n$^{6,7,8}$Li can be accurately obtained based on Jacobi-NCSM calculations with\nthe high-precision chiral two-nucleon and three-nucleon forces combined with\nthis new method. Our method can be straightforwardly extended to other ab\ninitio calculations, potentially providing a better description of nuclear\nsizes with realistic nuclear forces."
      ]
    }
  },
  {
    "id":2411.19345,
    "research_type":"applied",
    "start_id":"b8",
    "start_title":"GAN\uff08Generative Adversarial Nets\uff09",
    "start_abstract":"We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: model G that captures the data distribution, and discriminative D estimates probability sample came from training rather than G. The procedure is to maximize of making mistake. This corresponds minimax two-player game. In space arbitrary functions D, unique solution exists, with recovering distribution equal \u00bd everywhere. case where are defined by multilayer perceptrons, entire system can be trained backpropagation. There no need any Markov chains or unrolled approximate inference networks during either generation samples. Experiments demonstrate potential through qualitative quantitative evaluation generated",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "On the use of preclinical imaging to explore the principles of brain function in rodent models and their relevance for illnesses of the human mind"
      ],
      "abstract":[
        "Dear Editor, We recently published in Translational Psychiatry an article that examine the strategies for evaluating brain function at the wholebrain level [1]. In this review, we covered several methods, from functional MRI to functional ultrasound to calcium imaging. For each technique, we wrote a brief history of its development, the physical notion, some key applications, its potentials, and its limitations. We concluded that methods for imaging the rodent brain at the network level are growing and will advance our understanding of brain function. A commentary by Zhuo and colleagues further enhances the complexity of addressing the issue of \u201ctranslation\u201d from animal models to patients for the discipline of psychiatry [2]. They propose that the approaches employed to develop an animal model for a psychiatric disease need to be thoroughly scrutinized and, perhaps, revised. For example, most rodent models of mental diseases are to-date established using a simple pharmacological infusion [3] and\/or psychosocial stimulation [4]. The key concern posed, however, is how these manipulations change the brain\u2019s structure and function, and whether these models genuinely reflect the pathophysiology of human mental illnesses. Especially since it is difficult to evaluate whether one can speak of inverse inference from rodents to humans. This is a true and acceptable statement. However, this is exactly what preclinical imaging aims to deliver. By mapping the dynamic responses of brain networks in animal models and compare them, if possible, with those reported in clinical studies, we can obtain quantitative data and parameters to establish whether our models are effectively translational [5]. If these metrics demonstrate temporal and spatial similarity in network-level modifications as those observed in humans, we can pursue further inquiry utilizing more intrusive and more specific methods for brain recordings in animal models. Otherwise, we must have the confidence and the correctness to move forward and attempt other solutions. Two recent examples. In 2019 we established a causal association between activity of the noradrenergic nucleus locus coeruleus (LC) and the engagement of numerous large-scale brain networks in mice, in particular of the salience and amygdala networks [6]. In addition, we could link network-changes with direct markers of norepinephrine (NE) turnover and with the distribution of NE receptors over the entire brain. The hypothesis that specific brain networks dynamics are related to LC activity and to NE receptor density derives from stress-research and pharmacological studies in humans [7, 8]. However, since it is impossible to selectively stimulate LC in people, it has remained a hypothesis for more than a decade. Our preclinical work helped confirm this causal relationship and this has direct implications for interpreting the results of clinical imaging studies on stress and anxiety behavior. More recently, the Gozzi lab described how chronic local neuronal suppression via overexpression of a potassium channel or acute silencing via chemogenetics result in a paradoxical hyperconnectivity [9]; an intriguing finding often reported in humans after stroke [10] and in early stages of Alzheimer\u2019s disease [11], but never truly understood. Using in vivo electrophysiology, they showed local inhibition improves low frequency (0.1\u20134 Hz) oscillatory power via suppression of neuronal activity not phaselocked to slow rhythms, resulting in increased slow and \u03b4 band coherence between areas that display fMRI overconnectivity. These data present causal evidence that cortical inactivation can counterintuitively augment fMRI connectivity via greater, lesslocalized slow oscillatory processes. Once again, this could be only achieved by combining functional MRI and electrophysiology with neuromodulation in animal models. These and other examples give a peek of what the future of preclinical imaging might look like: a field of research capable of delivering causal explanations to the hypotheses presented by human neuroscience, neurology and psychiatry. Lastly, I would argue against statements like \u201cthe computational complexity of human brains is billions of times that of mouse brain\u201d. While this may be true from a numerical standpoint of mere neuronal counts, preclinical neuroimaging\u2019s objective should not be per se to map every single neuron in real time but of identifying the general neural and cellular principles governing the assembly of brain networks and its breakdown in brain disorders. The field is relatively new but is moving fast and has already produced some important insights. The future is challenging and will require time, devotion and an optimal synergy between engineering, chemistry, biology, and computer science. If the community will be patient and supportive enough, there will be further important discoveries in the future."
      ],
      "categories":[
        "Psychiatry"
      ]
    },
    "list":{
      "title":[
        "Tight Bounds on the Binomial CDF, and the Minimum of i.i.d Binomials, in\n  terms of KL-Divergence",
        "Euclid Quick Data Release (Q1). Extending the quest for little red dots\n  to z<4",
        "Energy-variational structure in evolution equations",
        "Classification of Electron and Muon Neutrino Events for the ESS$\\nu$SB\n  Near Water Cherenkov Detector using Graph Neural Networks",
        "Modified scattering for small data solutions to the Vlasov-Maxwell\n  system: a short proof",
        "Prompt and Conventional High-Energy Muon Spectra from a full Monte Carlo\n  Simulation via $\\texttt{CORSIKA7}$",
        "Enhancing Diffusion Models Efficiency by Disentangling Total-Variance\n  and Signal-to-Noise Ratio",
        "Spin 1 Transverse Momentum Dependent Tensor Structure Functions in\n  CLAS12",
        "A lower bound on the Ulrich complexity of hypersurfaces",
        "Oscillations of Solitonic Galactic Cores in Ultralight Dark Matter",
        "First detection of variable radio emission originating from the infant\n  planetary system V1298 Tau",
        "Regularized Sparse Optimal Discriminant Clustering",
        "Number of spanning trees in a wheel graph with two identified vertices\n  via hitting times",
        "Benchmarking low-power flopping-mode spin qubit fidelities in Si\/SiGe\n  devices with alloy disorder",
        "Successive Interference Cancellation-aided Diffusion Models for Joint\n  Channel Estimation and Data Detection in Low Rank Channel Scenarios",
        "Coding methods for string reconstruction from erroneous prefix-suffix\n  compositions",
        "Linear enhanced dissipation for the 2D Taylor-Couette flow in the\n  exterior region: A supplementary example for Gearhart-Pr\\\"uss type lemma",
        "The SPHERE infrared survey for exoplanets (SHINE). V. Complete\n  observations, data reduction and analysis, detection performances, and final\n  results",
        "Bridging Voting and Deliberation with Algorithms: Field Insights from\n  vTaiwan and Kultur Komitee",
        "$\\textit{In situ}$ time-resolved X-ray absorption spectroscopy of\n  shock-loaded magnesiosiderite",
        "Incomplete Data Multi-Source Static Computed Tomography Reconstruction\n  with Diffusion Priors and Implicit Neural Representation",
        "From Policy to Practice. Upper Bound Cost Estimates of Europes Green\n  Hydrogen Ambitions",
        "Existence of dynamical fluctuation in AMPT generated data for Au+Au\n  collisions at 10 AGeV",
        "Sensing Rate Optimization for Multi-Band Cooperative ISAC Systems",
        "Extreme Ultraviolet High-Harmonic Interferometry of Excitation-Induced\n  Bandgap Dynamics in Solids",
        "Magnetic Bloch States at Integer Flux Quanta Induced by Super-moir\\'e\n  Potential in Graphene Aligned with Twisted Boron Nitride",
        "Below the Schwinger critical magnetic field value, quantum vacuum and\n  gamma-ray bursts delay",
        "Synchronization in the complexified Kuramoto model",
        "Relations amongst the distances between $C^{*}$-subalgebras and some\n  canonically associated operator algebras"
      ],
      "abstract":[
        "We provide finite sample upper and lower bounds on the Binomial tail\nprobability which are a direct application of Sanov's theorem. We then use\nthese to obtain high probability upper and lower bounds on the minimum of\ni.i.d. Binomial random variables. Both bounds are finite sample, asymptotically\ntight, and expressed in terms of the KL-divergence.",
        "Recent James Webb Space Telescope (JWST) observations have revealed a\npopulation of sources with a compact morphology and a `v-shaped' continuum,\nnamely blue at rest-frame $\\lambda<4000$A and red at longer wavelengths. The\nnature of these sources, called `little red dots' (LRDs), is still debated,\nsince it is unclear if they host active galactic nuclei (AGN) and their number\nseems to drastically drop at z<4. We utilise the 63 $deg^2$ covered by the\nquick Euclid Quick Data Release (Q1) to extend the search for LRDs to brighter\nmagnitudes and to lower z than what has been possible with JWST to have a\nbroader view of the evolution of this peculiar galaxy population. The selection\nis done by fitting the available photometric data (Euclid, Spitzer\/IRAC, and\nground-based griz data) with two power laws, to retrieve the rest-frame optical\nand UV slopes consistently over a large redshift range (i.e, z<7.6). We exclude\nextended objects and possible line emitters, and perform a visual inspection to\nremove imaging artefacts. The final selection includes 3341 LRD candidates from\nz=0.33 to z=3.6, with 29 detected in IRAC. Their rest-frame UV luminosity\nfunction, in contrast with previous JWST studies, shows that the number density\nof LRD candidates increases from high-z down to z=1.5-2.5 and decreases at even\nlower z. Less evolution is apparent focusing on the subsample of more robust\nLRD candidates having IRAC detections, which is affected by low statistics and\nlimited by the IRAC resolution. The comparison with previous quasar UV\nluminosity functions shows that LRDs are not the dominant AGN population at\nz<4. Follow-up studies of these LRD candidates are key to confirm their nature,\nprobe their physical properties and check for their compatibility with JWST\nsources, since the different spatial resolution and wavelength coverage of\nEuclid and JWST could select different samples of compact sources.",
        "We consider different measure-valued solvability concepts from the literature\nand show that they could be simplified by using the energy-variational\nstructure of the underlying system of partial differential equations. In the\nconsidered examples, we prove that a certain class of improved measure-valued\nsolutions can be equivalently expressed as an energy-variational solution. The\nfirst concept represents the solution as a high-dimensional Young measure,\nwhether for the second concept, only a scalar auxiliary variable is introduced\nand the formulation is relaxed to an energy-variational inequality. We\ninvestigate four examples: the two-phase Navier--Stokes equations, a\nquasilinear wave equation, a system stemming from polyconvex elasticity, and\nthe Ericksen--Leslie equations equipped with the Oseen--Frank energy. The wide\nrange of examples suggests that this is a recurrent feature in evolution\nequations in general.",
        "In the effort to obtain a precise measurement of leptonic CP-violation with\nthe ESS$\\nu$SB experiment, accurate and fast reconstruction of detector events\nplays a pivotal role. In this work, we examine the possibility of replacing the\ncurrently proposed likelihood-based reconstruction method with an approach\nbased on Graph Neural Networks (GNNs). As the likelihood-based reconstruction\nmethod is reasonably accurate but computationally expensive, one of the\nbenefits of a Machine Learning (ML) based method is enabling fast event\nreconstruction in the detector development phase, allowing for easier\ninvestigation of the effects of changes to the detector design. Focusing on\nclassification of flavour and interaction type in muon and electron events and\nmuon- and electron neutrino interaction events, we demonstrate that the GNN\nreconstructs events with greater accuracy than the likelihood method for events\nwith greater complexity, and with increased speed for all events. Additionally,\nwe investigate the key factors impacting reconstruction performance, and\ndemonstrate how separation of events by pion production using another GNN\nclassifier can benefit flavour classification.",
        "We prove that for any global solution to the Vlasov-Maxwell system arising\nfrom compactly supported data, and such that the electromagnetic field decays\nfast enough, the distribution function exhibits a modified scattering dynamic.\nIn particular, our result applies to every small data solution constructed by\nGlassey-Strauss.",
        "Extensive air showers produce high-energy muons that can be utilized to probe\nhadronic interaction models in cosmic ray interactions. Most muons originate\nfrom pion and kaon decays, called $\\textit{conventional}$ muons, while a\nsmaller fraction, referred to as $\\textit{prompt}$ muons, arises from the decay\nof heavier, short-lived hadrons. The $\\texttt{EHISTORY}$ option of the air\nshower simulation tool $\\texttt{CORSIKA7}$ is used in this work to investigate\nthe prompt and conventional muon flux in the energy range of 100 TeV to 100\nPeV, utilizing the newly developed open-source python software\n$\\texttt{PANAMA}$. Identifying the muon parent particles allows for scaling the\ncontribution of prompt particles, which can be leveraged by future experimental\nanalyses to measure the normalization of the prompt muon flux. Obtained prompt\nmuon spectra from $\\texttt{CORSIKA7}$ are compared to $\\texttt{MCEq}$ results.\nThe relevance to large-volume neutrino detectors, such as IceCube and KM3NeT,\nand the connection to hadronic interaction models is discussed.",
        "The long sampling time of diffusion models remains a significant bottleneck,\nwhich can be mitigated by reducing the number of diffusion time steps. However,\nthe quality of samples with fewer steps is highly dependent on the noise\nschedule, i.e., the specific manner in which noise is introduced and the signal\nis reduced at each step. Although prior work has improved upon the original\nvariance-preserving and variance-exploding schedules, these approaches\n$\\textit{passively}$ adjust the total variance, without direct control over it.\nIn this work, we propose a novel total-variance\/signal-to-noise-ratio\ndisentangled (TV\/SNR) framework, where TV and SNR can be controlled\nindependently. Our approach reveals that different existing schedules, where\nthe TV explodes exponentially, can be $\\textit{improved}$ by setting a constant\nTV schedule while preserving the same SNR schedule. Furthermore, generalizing\nthe SNR schedule of the optimal transport flow matching significantly improves\nthe performance in molecular structure generation, achieving few step\ngeneration of stable molecules. A similar tendency is observed in image\ngeneration, where our approach with a uniform diffusion time grid performs\ncomparably to the highly tailored EDM sampler.",
        "We propose to analyze CLAS12 RG-C data to study the tensor\ntransverse-momentum-dependent parton distribution functions (TMDs) on deuteron\ndata. The deuteron is the lightest nucleus with spin-1, in essence a weakly\nbound system of two spin-1\/2 nucleons. However, one of the most intriguing\ncharacteristics of the deuteron is that the tensor polarized structure provides\ndirect access to the quark and gluon distribution of light nuclear system,\nwhich cannot be naively constructed from the proton and neutron. We will study\nthe tensor polarized structure functions with the Semi-inclusive Deep Inelastic\nScattering (SIDIS) $eD \\arrow eP_{h}X$ and Inclusive processes in the available\ndata on deuterated ammonia (ND3) target. We will perform the first ever SIDIS\nanalysis extraction of the tensor structure functions, which can be interpreted\nin term of completely unexplored tensor polarized TMDs. Our analysis will focus\non the extraction of the tensor structure functions b1 from inclusive process,\nand $F_{U(LL),T}$ and $F^{cos 2\\phi_{h}}_{U(LL)}$ from SIDIS. These last two\nstructure functions carry information related to two tensor-polarized TMDs,\n$f_{1LL}$ and $h^{\\perp}_{1LL}$. These initial exploratory measurements of\ntensor-polarized structure functions will enable the first extraction of spin-1\nTMDs and motivate more precise future measurements.",
        "We give a lower bound on the Ulrich complexity of hypersurfaces of dimension\n$n \\ge 6$.",
        "A remarkable feature of dark matter consisting of ultralight bosonic\nparticles is the emergence of superfluid Bose-Einstein condensate structures on\ngalactic scales. We investigate the oscillations of the solitonic dark matter\nstructure in the central galactic region by numerically solving the\nBogoliubov-de Gennes problem, accounting for perturbations in the gravitational\npotential and local self-interactions. Our findings reveal that the central\nsolitonic core, formed by the balance of gravitational attraction, quantum\npressure, and repulsive interactions, exhibits significant oscillatory\nbehaviour. These oscillations, characterized by distinct eigenmodes, provide\ninsights into the dynamical properties of solitonic dark matter structures and\ntheir observational implications and contributions to galactic structure\nformation and evolution.",
        "V1298 Tau is a very young and magnetically active star which hosts a\nbenchmark multi-planetary system to study planet formation and evolutionary\nhistory at the earliest stages. We selected V1298 Tau for a first targeted\nfollow-up at radio frequencies with the Karl G. Jansky Very Large Telescope\n(JVLA), the upgraded Giant Metrewave Radio Telescope (uGMRT), and the Sardinia\nRadio Telescope (SRT), to search for emission in the overall frequency range\n0.55-7.2 GHz. Detecting radio emission from such a very active star is key to\ncharacterise its magnetosphere, allowing in principle to probe the strength of\nthe coronal magnetic field and plasma density. Observations were carried out\nbetween Oct 2023 and Sept 2024: three epochs with uGMRT band-4 (0.55-0.75 GHz),\n12 epochs with the JVLA using L (1-2 GHz) and C (4.5-6.5 GHz) bands, and three\nepochs with SRT using C-high band (6-7.2 GHz). We report the first detection of\nradio emission from V1298 Tau at different epochs using the JVLA. The emission\nhas maximum peak flux densities of 91$\\pm$10 and 177$\\pm$6 $\\mu$Jy\/beam in the\nL- and C-band, respectively. From a comparison with contemporary optical\nphotometry, we found that the detected emission with the highest fluxes are\nlocated around a phase of minimum of the photospheric light curve. Although the\nuGMRT and SRT observations could not detect the source, we measured 3$\\sigma$\nflux density upper limits in the range ~41-56 $\\mu$Jy\/beam using uGMRT, while\nwith SRT we reached upper limits down to 13 mJy. The lack of a significant\nfraction of circular polarisation indicates that the observed flux is not due\nto electron cyclotron maser emission from star-planet interaction, and it is\nlikely produced by gyrosynchroton\/cyclotron emission from the corona triggered\nby stellar magnetic activity, although we cannot exclude thermal emission due\nto a lack of constraints on the brightness temperature.",
        "We propose a new method based on sparse optimal discriminant clustering\n(SODC), by a penalty term to scoring matrix based on convex clustering. With\nthe addition of this penalty term, it is expected to improve the accuracy of\ncluster identification by attaching points from the same cluster closer\ntogether and points from different clusters further apart. Moreover, we develop\na novel algorithm to derive the updated formula of this scoring matrix using\nmajorizing function. It solves the difficulty to satisfy both constraint and\ncontaining the clustering structure to the scoring matrix. We have demonstrated\nthe numerical simulations and its an application to real data to assess the\nperformance of the proposed method.",
        "In this paper, we provide an exact formula for the average hitting times in a\nwheel graph $W_{N+1}$ using a combinatorial approach. For this wheel graph, the\naverage hitting times can be expressed using Fibonacci numbers when the number\nof surrounding vertices is odd and Lucas numbers when it is even. Furthermore,\ncombining the exact formula for the average hitting times with the general\nformula for the effective resistance of the graph allows determination of the\nnumber of spanning trees of the graph with two identified vertices.",
        "In the \"flopping-mode\" regime of electron spin resonance, a single electron\nconfined in a double quantum dot is electrically driven in the presence of a\nmagnetic field gradient. The increased dipole moment of the charge in the\nflopping mode significantly reduces the amount of power required to drive spin\nrotations. However, the susceptibility of flopping-mode spin qubits to charge\nnoise, and consequently their overall performance, has not been examined in\ndetail. In this work, we simulate single-qubit gate fidelities of electrically\ndriven spin rotations in an ensemble of devices configured to operate in both\nthe single-dot and flopping-mode regimes. Our model accounts for the valley\nphysics of conduction band electrons in silicon and realistic alloy disorder in\nthe SiGe barrier layers, allowing us to investigate device-to-device\nvariability. We include charge and magnetic noise, as well as spin relaxation\nprocesses arising from charge noise and electron-phonon coupling. We find that\nthe two operating modes exhibit significantly different susceptibilities to the\nvarious noise sources, with valley splitting and spin relaxation times also\nplaying a role in their relative performance. For realistic noise strengths, we\nfind that single-dot gate fidelities are limited by magnetic noise while\nflopping-mode fidelities are primarily limited by charge noise and spin\nrelaxation. For sufficiently long spin relaxation times, flopping-mode spin\noperation is feasible with orders-of-magnitude lower drive power and gate\nfidelities that are on par with conventional single-dot electric dipole spin\nresonance.",
        "This paper proposes a novel joint channel-estimation and source-detection\nalgorithm using successive interference cancellation (SIC)-aided generative\nscore-based diffusion models. Prior work in this area focuses on massive MIMO\nscenarios, which are typically characterized by full-rank channels, and fail in\nlow-rank channel scenarios. The proposed algorithm outperforms existing methods\nin joint source-channel estimation, especially in low-rank scenarios where the\nnumber of users exceeds the number of antennas at the access point (AP). The\nproposed score-based iterative diffusion process estimates the gradient of the\nprior distribution on partial channels, and recursively updates the estimated\nchannel parts as well as the source. Extensive simulation results show that the\nproposed method outperforms the baseline methods in terms of normalized mean\nsquared error (NMSE) and symbol error rate (SER) in both full-rank and low-rank\nchannel scenarios, while having a more dominant effect in the latter, at\nvarious signal-to-noise ratios (SNR).",
        "The number of zeros and the number of ones in a binary string are referred to\nas the composition of the string, and the prefix-suffix compositions of a\nstring are a multiset formed by the compositions of the prefixes and suffixes\nof all possible lengths of the string. In this work, we present binary codes of\nlength n in which every codeword can be efficiently reconstructed from its\nerroneous prefix-suffix compositions with at most t composition errors. All our\nconstructions have decoding complexity polynomial in n and the best of our\nconstructions has constant rate and can correct $t = \\Theta(n)$ errors. As a\ncomparison, no prior constructions can afford to efficiently correct $t =\n\\Theta(n)$ arbitrary composition errors.\n  Additionally, we propose a method of encoding h arbitrary strings of the same\nlength so that they can be reconstructed from the multiset union of their\nerror-free prefix-suffix compositions, at the expense of h-fold coding\noverhead. In contrast, existing methods can only recover h distinct strings,\nalbeit with code rate asymptotically equal to 1\/h. Building on the top of the\nproposed method, we also present a coding scheme that enables efficient\nrecovery of h strings from their erroneous prefix-suffix compositions with $t =\n\\Theta(n)$ errors.",
        "From the perspective of asymptotic stability at high Reynolds numbers,\nTaylor-Couette flow, as a typical rotating shear flow, exhibits rich decay\nbehaviors. Previously, for the extensively studied Couette flow or the\nTaylor-Couette flow in bounded annular domains, methods based on resolvent\nestimates could derive exponential decay asymptotic for the solutions of the\nlinearized system. However, unlike the Couette flow or the Taylor-Couette flow\nin bounded annular domains, the Taylor-Couette flow in exterior regions\nexhibits degeneration of derivatives of any order at infinity. In this paper,\nwe present in Theorem 1.1 that the linearized system of the 2D Taylor-Couette\nflow in the exterior region exhibits space-time coupled polynomial decay\nasymptotics. We also prove that the solution to this system, when it contains\ninhomogeneous terms, cannot be expected to exhibit space-time coupled\nexponential decay, as detailed in Theorem 1.2. The result of Theorem 1.2\nindicates that, even if we can obtain sharp resolvent estimates in different\nweighted spaces, the Gearhart-Pr\\\"uss type lemma no longer applies. This\nsuggests that resolvent estimates may not be very effective for handling\ndegenerate shear flows. Furthermore, Theorem 1.2 also implies that, for the\ntransition threshold problem of the 2D Taylor-Couette flow in exterior regions,\nwe can at most expect the solution to exhibit long-time behavior with\nspace-time coupled polynomial decay. Finally, we present a generalization of\nTheorem 1.2, as detailed in Theorem 1.3.",
        "During the past decade, state-of-the-art planet-finder instruments like\nSPHERE@VLT, coupling coronagraphic devices and extreme adaptive optics systems,\nunveiled, thanks to large surveys, around 20 planetary mass companions at\nsemi-major axis greater than 10 astronomical units. Direct imaging being the\nonly detection technique to be able to probe this outer region of planetary\nsystems, the SPHERE infrared survey for exoplanets (SHINE) was designed and\nconducted from 2015 to 2021 to study the demographics of such young gas giant\nplanets around 400 young nearby solar-type stars. In this paper, we present the\nobserving strategy, the data quality, and the point sources analysis of the\nfull SHINE statistical sample as well as snapSHINE. Both surveys used the\nSPHERE@VLT instrument with the IRDIS dual band imager in conjunction with the\nintegral field spectrograph IFS and the angular differential imaging observing\ntechnique. All SHINE data (650 datasets), corresponding to 400 stars, including\nthe targets of the F150 survey, are processed in a uniform manner with an\nadvanced post-processing algorithm called PACO ASDI. An emphasis is put on the\nclassification and identification of the most promising candidate companions.\nCompared to the previous early analysis SHINE F150, the use of advanced\npost-processing techniques significantly improved by one or 2 magnitudes\n(x3-x6) the contrast detection limits, which will allow us to put even tighter\nconstraints on the radial distribution of young gas giants. This increased\nsensitivity directly places SHINE as the largest and deepest direct imaging\nsurvey ever conducted. We detected and classified more than 3500 physical\nsources. One additional substellar companion has been confirmed during the\nsecond phase of the survey (HIP 74865 B), and several new promising candidate\ncompanions are awaiting second epoch confirmations.",
        "Democratic processes increasingly aim to integrate large-scale voting with\nface-to-face deliberation, addressing the challenge of reconciling individual\npreferences with collective decision-making. This work introduces new methods\nthat use algorithms and computational tools to bridge online voting with\nface-to-face deliberation, tested in two real-world scenarios: Kultur Komitee\n2024 (KK24) and vTaiwan. These case studies highlight the practical\napplications and impacts of the proposed methods.\n  We present three key contributions: (1) Radial Clustering for Preference\nBased Subgroups, which enables both in-depth and broad discussions in\ndeliberative settings by computing homogeneous and heterogeneous group\ncompositions with balanced and adjustable group sizes; (2) Human-in-the-loop\nMES, a practical method that enhances the Method of Equal Shares (MES)\nalgorithm with real-time digital feedback. This builds algorithmic trust by\ngiving participants full control over how much decision-making is delegated to\nthe voting aggregation algorithm as compared to deliberation; and (3) the\nReadTheRoom deliberation method, which uses opinion space mapping to identify\nagreement and divergence, along with spectrum-based preference visualisation to\ntrack opinion shifts during deliberation. This approach enhances transparency\nby clarifying collective sentiment and fosters collaboration by encouraging\nparticipants to engage constructively with differing perspectives.\n  By introducing these actionable frameworks, this research extends in-person\ndeliberation with scalable digital methods that address the complexities of\nmodern decision-making in participatory processes.",
        "Carbonate minerals are important in Earth's system sciences and have been\nfound on Mars and in meteorites and asteroids, highlighting the importance of\nimpacts in planetary processes. While extensively studied under static\ncompression, the behavior of carbonates under shock compression remains\nunderexplored, with no $\\textit{in situ}$ X-ray investigations reported so far.\nHere we investigate natural magnesiosiderite (Fe$_{0.6}$Mg$_{0.4}$CO$_{3}$)\nunder nanosecond laser-driven shock compression at pressures up to 150 GPa,\ncoupled with $\\textit{in situ}$ ultrafast synchrotron X-ray absorption\nspectroscopy (XAS). The interpretation of the experimental spectra is\ncomplemented using first-principles absorption cross-section calculations\nperformed on crystalline phases at different pressures and on a dense liquid\nphase obtained using density functional theory-based molecular dynamics\n(DFT-MD) simulations. Under laser-driven shock compression, the\nmagnesiosiderite crystal phase remains unchanged up to the melt. Under shock\nreverberation, the absorption spectra show changes similar to those attributed\nto a high-spin to low-spin transition observed under static compression. At\nhigher pressures, the laser shock induces the formation of CO$_4$ tetrahedral\nunits in the melt. Upon unloading from the shocked state, only a few\nnanoseconds later, the original magnesiosiderite phase is recovered.",
        "The dose of X-ray radiation and the scanning time are crucial factors in\ncomputed tomography (CT) for clinical applications. In this work, we introduce\na multi-source static CT imaging system designed to rapidly acquire sparse view\nand limited angle data in CT imaging, addressing these critical factors. This\nlinear imaging inverse problem is solved by a conditional generation process\nwithin the denoising diffusion image reconstruction framework. The noisy volume\ndata sample generated by the reverse time diffusion process is projected onto\nthe affine set to ensure its consistency to the measured data. To enhance the\nquality of the reconstruction, the 3D phantom's orthogonal space projector is\nparameterized implicitly by a neural network. Then, a self-supervised learning\nalgorithm is adopted to optimize the implicit neural representation. Through\nthis multistage conditional generation process, we obtain a new approximate\nposterior sampling strategy for MSCT volume reconstruction. Numerical\nexperiments are implemented with various imaging settings to verify the\neffectiveness of our methods for incomplete data MSCT volume reconstruction.",
        "As the European countries strive to meet their ambitious climate goals,\nrenewable hydrogen has emerged to aid in decarbonizing energy-intensive sectors\nand support the overall energy transition. To ensure that hydrogen production\naligns with these goals, the European Commission has introduced criteria for\nadditionality, temporal correlation, and geographical correlation. These\ncriteria are designed to ensure that hydrogen production from renewable sources\nsupports the growth of renewable energy. This study assesses the impact of\nthese criteria on green hydrogen production, focusing on production costs and\ntechnology impacts. The European energy market is simulated up to 2048 using\nstochastic programming, applying these requirements exclusively to green\nhydrogen production without the phased-in compliance period outlined in the EU\nregulations. The findings show that meeting the criteria will increase expected\nsystem costs by 82 EUR billion from 2024 to 2048, largely due to the rapid\nshift from fossil fuels to renewable energy. The additionality requirement,\nwhich mandates the use of new renewable energy installations for electrolysis,\nproves to be the most expensive, but also the most effective in accelerating\nrenewable energy adoption.",
        "In this study, the intermittency behavior of emitted particles produced in\nheavy ion collisions has been studied using both modes (default $\\&$ string\nmelting) of A Multi Phase Transport (AMPT) model-generated data. We adopted one\nof the most conventional and successful techniques, the Scaled Factorial Moment\n(SFM) method, using Monte Carlo (MC) data for 10 AGeV Au+Au collisions in\nsearch of intermittency in the model-generated data. Our interest is to search\nfor intermittency behavior of particles that leads to multiplicity fluctuations\nand that would reveal a phase transition from hadronic matter to QGP. In this\narticle, the intermittency values for both modes of AMPT data are presented.\nThe results obtain some insight into the dynamics of heavy ion collisions and\nthe formation of QGP.",
        "Integrated sensing and communication (ISAC) has been recognized as one of the\nkey technologies for future wireless networks, which potentially need to\noperate in multiple frequency bands to satisfy ever-increasing demands for both\ncommunication and sensing services. Motivated by this, we consider the sum\nsensing rate (SR) optimization for a cooperative ISAC system with linear\nprecoding, where each base station (BS) works in a different frequency band.\nWith this aim, we propose an optimization algorithm based on the semi-definite\nrank relaxation that introduces covariance matrices as optimization variables,\nand we apply the inner approximation (IA) method to deal with the nonconvexity\nof the resulting problem. Simulation results show that the proposed algorithm\nincreases the SR by approximately 25 % and 40 % compared to the case of equal\npower distribution in a cooperative ISAC system with two and three BSs,\nrespectively. Additionally, the algorithm converges in only a few iterations,\nwhile its most optimal implementation scenario is in the low power regime.",
        "Interferometry is a fundamental technique in physics, enabling precise\nmeasurements through the interference of waves. High-harmonic generation (HHG)\nin solids has emerged as a powerful method for probing ultrafast electronic\ndynamics within crystalline structures.\n  In this study, we employed extreme ultraviolet (XUV) high-harmonic\ninterferometry with phase-locked XUV pulse pairs to investigate\nexcitation-induced bandgap dynamics in solids. Our experiments on amorphous\nSiO2 and crystalline MgO, complemented by analytical modeling and semiconductor\nBloch equation simulations, reveal a correlation between transient bandgap\nmodifications and variations in the phase of harmonic emission. These findings\nsuggest a potential pathway for sub-cycle, all-optical control of band\nstructure modifications, advancing prospects for petahertz-scale electronic\napplications and attosecond diagnostics of carrier dynamics.",
        "Two-dimensional electron systems in both magnetic fields and periodic\npotentials are described by Hofstadter butterfly, a fundamental problem of\nsolid-state physics. While moir\\'e systems provide a powerful method to realize\nthis spectrum, previous experiments, however, have been limited to fractional\nflux quanta regime due to the difficulty of building ~ 50 nm periodic\nmodulations. Here, we demonstrate a super-moir\\'e strategy to overcome this\nchallenge. By aligning monolayer graphene (G) with 1.0{\\deg} twisted hexagonal\nboron nitride (t-hBN), a 63.2 nm bichromatic G\/t-hBN super-moir\\'e is\nconstructed, made possible by exploiting the electrostatic nature of t-hBN\npotential. Under magnetic field B, magnetic Bloch states at integer flux quanta\n(1-9) are achieved and observed as integer Brown-Zak oscillations, expanding\nthe flux quanta from factions to integers. Theoretical analysis reproduces\nthese experimental findings. This work opens new avenues to study unexplored\nHofstadter butterfly, explore emergent topological order at integer flux quanta\nand engineer long-wavelength periodic modulations.",
        "A magnetic field above the Schwinger critical value $B_{\\rm crit} = 10^9$\nTesla is much higher than any magnetic field known by now in the interstellar\nbulk except in the vicinity of observed magnetars with magnetic fields between\n$10^9$ and $10^{11}~$Tesla. Above the critical magnetic field, calculated by\nSchwinger in the lowest order perturbation in quantum electrodynamics (QED),\none reaches the threshold for electron-positron pair creation, which has\ninteresting consequences. Therefore, finding out whether one could encounter\nsome consequences of interest also for the values of the magnetic field below\nthe Schwinger critical point, we invoke the next higher-order effect in QED,\nwhich is emerging from the Quantum Vacuum Effect. The latter is equivalent to\nthe use of the Euler-Heisenberg effective theory in nonlinear electrodynamics,\nwhere the Lagrangian has a term with a higher power, $B^4$. In this case, in\nthe region $B<B_{\\rm crit}$, we show that interesting effects appear, among\nthem the Cherenkov radiation and the reduction in the speed of light. The\nlatter effects appear because of the quantum vacuum mimicking a medium. We also\npresent quantitative arguments for such a close analogy. As a rough estimate,\nwe show that the time delay $\\tau$ of gamma-ray bursts (GRB) having traveled\nthrough the entire cosmological distances in an average strong magnetic field\nsuch as $10^6~$Tesla, reaches an experimentally considerable value of $\\tau =\n2.4$ hours. In the vicinity of magnetars, the magnetic field is much stronger,\nof the order of $10^9-10^{11}$ Tesla. However, in this case the linear scale of\nGRB trajectory through such regions would be much smaller. For the latter, we\ngive an estimate for the number of the magnetars along the trajectory and also\nfor the delay. Finally, we shall dwell on the recently raised issue in the\nliterature, namely the Lorentz invariance violation (LIV).",
        "In this paper, we consider an $N$-oscillators complexified Kuramoto model.\nWhen the coupling strength $\\lambda$ is strong, sufficient conditions for\nvarious types of synchronization are established for general $N \\geq 2$. On the\nother hand, we analyze the case when the coupling strength is weak. For $N=2$,\nwhen the coupling strength is below a critical coupling strength $\\lambda_c$,\nwe show that periodic orbits emerge near each equilibrium point, and hence full\nphase-locking state exists. This phenomenon significantly differentiates the\ncomplexified Kuramoto model from the real Kuramoto system, as synchronization\nnever occurs when $\\lambda<\\lambda_c$ in the latter. For $N=3$, we demonstrate\nthat if the natural frequencies are in arithmetic progression, non-trivial\nsynchronization states can be achieved for certain initial conditions even when\nthe coupling strength is weak. In particular, we characterize the critical\ncoupling strength ($\\lambda\/\\lambda_c = 0.85218915...$) such that a semistable\nequilibrium point in the real Kuramoto model bifurcates into a pair of stable\nand unstable equilibria, marking a new phenomenon in complexified Kuramoto\nmodels.",
        "We prove that the Christensen distance (resp., the Kadison-Kastler distance)\nbetween two $C^*$-subalgebras $\\mathcal{A}$ and $\\mathcal{B}$ of a\n$C^*$-algebra $\\mathcal{C}$ is equal to that between their enveloping von\nNeumann algebras $\\mathcal{A}^{**}$ and $\\mathcal{B}^{**}$ (resp., the tensor\nproduct algebras $\\mathcal{A} \\otimes^{\\min} \\mathcal{D}$ and $\\mathcal{B}\n\\otimes^{\\min} \\mathcal{D}$, for any unital commutative $C^*$-algebra\n$\\mathcal{D}$)."
      ]
    }
  },
  {
    "id":2411.00688,
    "research_type":"applied",
    "start_id":"b19",
    "start_title":"Parameter-Free FISTA by Adaptive Restart and Backtracking",
    "start_abstract":"We consider a combined restarting and adaptive backtracking strategy for the\npopular Fast Iterative Shrinking-Thresholding Algorithm frequently employed for\naccelerating the convergence speed of large-scale structured convex\noptimization problems. Several variants of FISTA enjoy a provable linear\nconvergence rate for the function values $F(x_n)$ of the form $\\mathcal{O}(\ne^{-K\\sqrt{\\mu\/L}~n})$ under the prior knowledge of problem conditioning, i.e.\nof the ratio between the (\\L ojasiewicz) parameter $\\mu$ determining the growth\nof the objective function and the Lipschitz constant $L$ of its smooth\ncomponent. These parameters are nonetheless hard to estimate in many practical\ncases. Recent works address the problem by estimating either parameter via\nsuitable adaptive strategies. In our work both parameters can be estimated at\nthe same time by means of an algorithmic restarting scheme where, at each\nrestart, a non-monotone estimation of $L$ is performed. For this scheme,\ntheoretical convergence results are proved, showing that a $\\mathcal{O}(\ne^{-K\\sqrt{\\mu\/L}n})$ convergence speed can still be achieved along with\nquantitative estimates of the conditioning. The resulting Free-FISTA algorithm\nis therefore parameter-free. Several numerical results are reported to confirm\nthe practical interest of its use in many exemplar problems.",
    "start_categories":[
      "math.OC"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "Convex generalizations of total variation based on the structure tensor with applications to inverse problems"
      ],
      "abstract":[
        "We introduce a generic convex energy functional that is suitable for both grayscale and vector-valued images. Our functional is based on the eigenvalues of the structure tensor, therefore it penalizes image variation at every point by taking into account the information from its neighborhood. It generalizes several existing variational penalties, such as the Total Variation and vectorial extensions of it. By introducing the concept of patch-based Jacobian operator, we derive an equivalent formulation of the proposed regularizer that is based on the Schatten norm of this operator. Using this new formulation, we prove convexity and develop a dual definition for the proposed energy, which gives rise to an efficient and parallelizable minimization algorithm. Moreover, we establish a connection between the minimization of the proposed convex regularizer and a generic type of nonlinear anisotropic diffusion that is driven by a spatially regularized and adaptive diffusion tensor. Finally, we perform extensive experiments with image denoising and deblurring for grayscale and color images. The results show the effectiveness of the proposed approach as well as its improved performance compared to Total Variation and existing vectorial extensions of it."
      ],
      "categories":[
        "eess.IV"
      ]
    },
    "list":{
      "title":[
        "Decay rates of $\\Lambda_b^0 \\to \\Lambda_c^+ \\ell^- \\bar\\nu_\\ell$ using\n  helicity analysis and phase-moment parametrization",
        "Ultraviolet Renormalization of Spin Boson Models I. Normal and\n  2-Nilpotent Interactions",
        "Development of a high-rate capable DLC-RPC based on a current evacuation\n  pattern",
        "Non-Lorentzian model for strong exciton-plasmon coupling",
        "Transformer Based Self-Context Aware Prediction for Few-Shot Anomaly\n  Detection in Videos",
        "PrivilegedDreamer: Explicit Imagination of Privileged Information for\n  Rapid Adaptation of Learned Policies",
        "Are you a DePIN? A Decision Tree to Classify Decentralized Physical\n  Infrastructure Networks",
        "Bridging HCI and AI Research for the Evaluation of Conversational SE\n  Assistants",
        "Hidden Darkness in LLM-Generated Designs: Exploring Dark Patterns in\n  Ecommerce Web Components Generated by LLMs",
        "Blast waves and reverse shocks: from ultra-relativistic GRBs to\n  moderately relativistic X-ray binaries",
        "How Low Can You Go? Searching for the Intrinsic Dimensionality of\n  Complex Networks using Metric Node Embeddings",
        "Native antisite defects in h-BN",
        "Black Box Causal Inference: Effect Estimation via Meta Prediction",
        "Exploring the energy spectrum of a four-terminal Josephson junction:\n  Towards topological Andreev band structures",
        "Near-Field ISAC: Synergy of Dual-Purpose Codebooks and Space-Time\n  Adaptive Processing",
        "Designing Guidance for Multiple Valley-based Topological States Driven\n  by Magnetic Substrates: Potential Applications at High Temperatures",
        "Euclid Quick Data Release (Q1). Galaxy shapes and alignments in the\n  cosmic web",
        "Hierarchical Deep Reinforcement Learning for Adaptive Resource\n  Management in Integrated Terrestrial and Non-Terrestrial Networks",
        "Stochastic Method for Delayed Neutron Precursors Transport in Liquid\n  Fuel",
        "Nonlinear bubble behaviours of compressible Rayleigh-Taylor instability\n  with isothermal stratification in cylindrical geometry",
        "Efficient License Plate Recognition in Videos Using Visual Rhythm and\n  Accumulative Line Analysis",
        "Understanding User Preference -- Comparison between Linear and\n  Directional Top-K Query results",
        "Stellar Population and Metal Production in AGN Disks",
        "A comparison of data filtering techniques for English-Polish LLM-based\n  machine translation in the biomedical domain",
        "QAOA in Quantum Datacenters: Parallelization, Simulation, and\n  Orchestration",
        "Efficient cavity-mediated energy transfer between photosynthetic light\n  harvesting complexes from strong to weak coupling regime",
        "Convergence Rates of GMM Estimators with Nonsmooth Moments under\n  Misspecification",
        "Learning to Negotiate via Voluntary Commitment",
        "A Unified Knowledge-Distillation and Semi-Supervised Learning Framework\n  to Improve Industrial Ads Delivery Systems"
      ],
      "abstract":[
        "Based on the helicity method, formulae for the semileptonic transition of\n$\\Lambda_b^0 \\to \\Lambda_c^+ \\ell^- \\bar \\nu_\\ell$ including lepton mass\neffects are derived. In order to calculate the form factors of the $\\Lambda_b$\nbaryon transition matrix element, we employ the phase-moment parameterization\nand perform fits to the Lattice QCD data. With the help of the obtained form\nfactors, six helicity amplitudes and the differential decay widths are\nevaluated. Through appropriate angular integrations, we express the helicity\nflip, helicity nonflip integrated decay rates, and the lepton-side\nforward-backward asymmetry. We present a numerical analysis of these physical\nobservables. We obtain the mentioned physical quantities by performing fits to\nthe Lattice QCD data using the well-known Boyd-Grinstein-Lebed parametrization.\nComparisons with other experimental and theoretical data are also discussed.",
        "We study the ultraviolet problem for models of a finite-dimensional quantum\nmechanical system linearly coupled to a bosonic quantum field, such as the\n(many-)spin boson model or its rotating-wave approximation. If the state change\nof the system upon emission or absorption of a boson is either given by a\nnormal matrix or by a 2-nilpotent one, which is the case for the previously\nnamed examples, we prove an optimal renormalization result. We complement it,\nby proving the norm resolvent convergence of appropriately regularized models\nto the renormalized one. Our method consists of a dressing transformation\nargument in the normal case and an appropriate interior boundary condition for\nthe 2-nilpotent case.",
        "A Resistive Plate Chamber using Diamond-Like Carbon electrodes (DLC-RPC) has\nbeen developed as a background tagging detector in the MEG$~$II experiment. The\nDLC-RPC is planned to be installed in a high-intensity and low-momentum muon\nbeam. This detector is required to have a detection efficiency above 90 % with\nfour active gaps in the muon beam due to the limitation of the material budget.\nIn such an environment, the high current flowing through the resistive\nelectrodes causes a voltage drop, which reduces the performance of the DLC-RPC.\nThis voltage drop can be suppressed by implementing a current evacuation\npattern, though discharges are more likely to occur near the pattern. Therefore\nthe pattern must be covered by a protection cover made of an insulator. In this\nstudy, electrode samples with a current evacuation pattern and different widths\nof protection cover (0.2 mm and 0.8 mm) have been produced, and their\nperformance and stability were measured. The detection efficiency of a\nsingle-gap chamber for $\\beta$-rays from a $^{90}$Sr source was measured to be\nup to approximately 60 % in both electrode samples. The target efficiency can\nbe achieved even with a drop of 100 $-$ 150 V. On the other hand, after more\nthan a dozen hours of operation, discharges suddenly occurred and the detector\nwas prevented from further operation. These discharges created current paths on\nthe spacing pillars. This serious problem must be investigated and solved in\nthe future.",
        "We develop a non-Lorentzian approach for quantum emitters (QE) resonantly\ncoupled to localized surface plasmons (LSP) in metal-dielectric structures.\nUsing the exact LSP Green function, we derive non-Lorentzian version of\nMaxwell-Bloch equations which describe LSP in terms of metal complex dielectric\nfunction rather than via Lorentzian resonances. For a single QE coupled to the\nLSP, we obtain an explicit expression for the system effective optical\npolarizability which, in the Lorentzian approximation, recovers the classical\ncoupled oscillator (CO) model. We demonstrate that non-Lorentzian effects\noriginating from the temporal dispersion of metal dielectric function affect\ndramatically the optical spectra as the system transitions to the strong\ncoupling regime. Specifically, in contrast to Lorentzian models, the main\nspectral weight is shifted towards the lower energy polaritonic band,\nconsistent with the experiment.",
        "Anomaly detection in videos is a challenging task as anomalies in different\nvideos are of different kinds. Therefore, a promising way to approach video\nanomaly detection is by learning the non-anomalous nature of the video at hand.\nTo this end, we propose a one-class few-shot learning driven transformer based\napproach for anomaly detection in videos that is self-context aware. Features\nfrom the first few consecutive non-anomalous frames in a video are used to\ntrain the transformer in predicting the non-anomalous feature of the subsequent\nframe. This takes place under the attention of a self-context learned from the\ninput features themselves. After the learning, given a few previous frames, the\nvideo-specific transformer is used to infer if a frame is anomalous or not by\ncomparing the feature predicted by it with the actual. The effectiveness of the\nproposed method with respect to the state-of-the-art is demonstrated through\nqualitative and quantitative results on different standard datasets. We also\nstudy the positive effect of the self-context used in our approach.",
        "Numerous real-world control problems involve dynamics and objectives affected\nby unobservable hidden parameters, ranging from autonomous driving to robotic\nmanipulation, which cause performance degradation during sim-to-real transfer.\nTo represent these kinds of domains, we adopt hidden-parameter Markov decision\nprocesses (HIP-MDPs), which model sequential decision problems where hidden\nvariables parameterize transition and reward functions. Existing approaches,\nsuch as domain randomization, domain adaptation, and meta-learning, simply\ntreat the effect of hidden parameters as additional variance and often struggle\nto effectively handle HIP-MDP problems, especially when the rewards are\nparameterized by hidden variables. We introduce Privileged-Dreamer, a\nmodel-based reinforcement learning framework that extends the existing\nmodel-based approach by incorporating an explicit parameter estimation module.\nPrivilegedDreamer features its novel dual recurrent architecture that\nexplicitly estimates hidden parameters from limited historical data and enables\nus to condition the model, actor, and critic networks on these estimated\nparameters. Our empirical analysis on five diverse HIP-MDP tasks demonstrates\nthat PrivilegedDreamer outperforms state-of-the-art model-based, model-free,\nand domain adaptation learning algorithms. Additionally, we conduct ablation\nstudies to justify the inclusion of each component in the proposed\narchitecture.",
        "Decentralized physical infrastructure networks (DePINs) are an emerging\nvertical within \"Web3\" replacing the traditional method that physical\ninfrastructures are constructed. Yet, the boundaries between DePIN and\ntraditional method of building crowd-sourced infrastructures such as citizen\nscience initiatives or other Web3 verticals are not always so clear cut. In\nthis work, we systematically analyze the differences between DePIN and other\nWeb2 and Web3 verticals. For this, the study proposes a novel decision tree for\nclassifying systems as DePIN. This tree is informed by prior studies and\ndifferentiates DePIN from related concepts using criteria such as the presence\nof a three-sided market, token-based incentives for supply, and the requirement\nfor physical asset placement in those systems.\n  The paper demonstrates the application of the decision tree to various\nblockchain systems, including Helium and Bitcoin, showcasing its practical\nutility in differentiating DePIN systems.\n  This research offers significant contributions towards establishing a more\nobjective and systematic approach to identifying and categorizing DePIN\nsystems. It lays the groundwork for creating a comprehensive and unbiased\ndatabase of DePIN systems, which will inform future research and development\nwithin this emerging sector.",
        "As Large Language Models (LLMs) are increasingly adopted in software\nengineering, recently in the form of conversational assistants, ensuring these\ntechnologies align with developers' needs is essential. The limitations of\ntraditional human-centered methods for evaluating LLM-based tools at scale\nraise the need for automatic evaluation. In this paper, we advocate combining\ninsights from human-computer interaction (HCI) and artificial intelligence (AI)\nresearch to enable human-centered automatic evaluation of LLM-based\nconversational SE assistants. We identify requirements for such evaluation and\nchallenges down the road, working towards a framework that ensures these\nassistants are designed and deployed in line with user needs.",
        "Recent work has highlighted the risks of LLM-generated content for a wide\nrange of harmful behaviors, including incorrect and harmful code. In this work,\nwe extend this by studying whether LLM-generated web design contains dark\npatterns. This work evaluated designs of ecommerce web components generated by\nfour popular LLMs: Claude, GPT, Gemini, and Llama. We tested 13 commonly used\necommerce components (e.g., search, product reviews) and used them as prompts\nto generate a total of 312 components across all models. Over one-third of\ngenerated components contain at least one dark pattern. The majority of dark\npattern strategies involve hiding crucial information, limiting users' actions,\nand manipulating them into making decisions through a sense of urgency. Dark\npatterns are also more frequently produced in components that are related to\ncompany interests. These findings highlight the need for interventions to\nprevent dark patterns during front-end code generation with LLMs and emphasize\nthe importance of expanding ethical design education to a broader audience.",
        "Blast wave models are commonly used to model relativistic outflows from\nultra-relativistic gamma-ray bursts (GRBs), but are also applied to lower\nLorentz factor ejections from X-ray binaries (XRBs). Here we revisit the\nphysics of blast waves and reverse shocks in these systems and explore the\nsimilarities and differences between the ultra-relativistic ($\\Gamma \\gg 1$)\nand moderately relativistic ($\\Gamma \\sim$ a few) regimes. We first demonstrate\nthat the evolution of the blast wave radius as a function of the observer frame\ntime is recovered in the on-axis ultra-relativistic limit from a general energy\nand radius blast wave evolution, emphasizing that XRB ejections are off-axis,\nmoderately relativistic cousins of GRB afterglows. We show that, for fixed\nblast wave or ejecta energy, reverse shocks cross the ejecta much later\n(earlier) on in the evolution for less (more) relativistic systems, and find\nthat reverse shocks are much longer-lived in XRBs and off-axis GRBs compared to\non-axis GRBs. Reverse shock crossing should thus typically finish after\n$\\sim10-100$ days (in the observer frame) in XRB ejections. This\ncharacteristic, together with their moderate Lorentz factors and resolvable\ncore separations, makes XRB ejections unique laboratories for shock and\nparticle acceleration physics. We discuss the impact of geometry and lateral\nspreading on our results, explore how to distinguish between different shock\ncomponents, and comment on the implications for GRB and XRB environments.\nAdditionally, we argue that identification of reverse shock signatures in XRBs\ncould provide an independent constraint on the ejecta Lorentz factor.",
        "Low-dimensional embeddings are essential for machine learning tasks involving\ngraphs, such as node classification, link prediction, community detection,\nnetwork visualization, and network compression. Although recent studies have\nidentified exact low-dimensional embeddings, the limits of the required\nembedding dimensions remain unclear. We presently prove that lower dimensional\nembeddings are possible when using Euclidean metric embeddings as opposed to\nvector-based Logistic PCA (LPCA) embeddings. In particular, we provide an\nefficient logarithmic search procedure for identifying the exact embedding\ndimension and demonstrate how metric embeddings enable inference of the exact\nembedding dimensions of large-scale networks by exploiting that the metric\nproperties can be used to provide linearithmic scaling. Empirically, we show\nthat our approach extracts substantially lower dimensional representations of\nnetworks than previously reported for small-sized networks. For the first time,\nwe demonstrate that even large-scale networks can be effectively embedded in\nvery low-dimensional spaces, and provide examples of scalable, exact\nreconstruction for graphs with up to a million nodes. Our approach highlights\nthat the intrinsic dimensionality of networks is substantially lower than\npreviously reported and provides a computationally efficient assessment of the\nexact embedding dimension also of large-scale networks. The surprisingly low\ndimensional representations achieved demonstrate that networks in general can\nbe losslessly represented using very low dimensional feature spaces, which can\nbe used to guide existing network analysis tasks from community detection and\nnode classification to structure revealing exact network visualizations.",
        "Hexagonal boron nitride (hBN) is an excellent host for solid-state single\nphonon emitters. Experimental observed emission ranges from infrared to\nultraviolet. The emission centers are generally attributed to either intrinsic\nor extrinsic point defects embedded into hBN. Nevertheless, the microscopic\nstructure of most of these defect emitters is uncertain. Here, through\ndensity-functional theory calculations we studied the native antisite defects\nin hBN. We find that the neutral boron antisite might be a nonmagnetic single\nphoton source with zero-phonon-line (ZPL) at 1.58 eV and such a lineshape that\nis often observed in experiments. Furthermore, the positively charged nitrogen\nantisite might be associated with a dim color center recently observed as a\nblue emitter with ZPL at 2.63 eV. These simple single substitution defects\nindicate the existence of out-of-plane phonon mode which significantly affects\nthe optical properties. Our results could provide useful information for\nidentification of quantum emitters in hBN.",
        "Causal inference and the estimation of causal effects plays a central role in\ndecision-making across many areas, including healthcare and economics.\nEstimating causal effects typically requires an estimator that is tailored to\neach problem of interest. But developing estimators can take significant effort\nfor even a single causal inference setting. For example, algorithms for\nregression-based estimators, propensity score methods, and doubly robust\nmethods were designed across several decades to handle causal estimation with\nobserved confounders. Similarly, several estimators have been developed to\nexploit instrumental variables (IVs), including two-stage least-squares (TSLS),\ncontrol functions, and the method-of-moments. In this work, we instead frame\ncausal inference as a dataset-level prediction problem, offloading algorithm\ndesign to the learning process. The approach we introduce, called black box\ncausal inference (BBCI), builds estimators in a black-box manner by learning to\npredict causal effects from sampled dataset-effect pairs. We demonstrate\naccurate estimation of average treatment effects (ATEs) and conditional average\ntreatment effects (CATEs) with BBCI across several causal inference problems\nwith known identification, including problems with less developed estimators.",
        "Hybrid multiterminal Josephson junctions (JJs) are expected to harbor a novel\nclass of Andreev bound states (ABSs), including topologically nontrivial states\nin four-terminal devices. In these systems, topological phases emerge when ABSs\ndepend on at least three superconducting phase differences, resulting in a\nthree-dimensional (3D) energy spectrum characterized by Weyl nodes at zero\nenergy. Here, we realize a four-terminal JJ in a hybrid Al\/InAs\nheterostructure, where ABSs form a synthetic 3D band structure. We probe the\nenergy spectrum using tunneling spectroscopy and identify spectral features\nassociated with the formation of a tri-Andreev molecule, a bound state whose\nenergy depends on three superconducting phases and, therefore, is able to host\ntopological ABSs. The experimental observations are well described by a\nnumerical model. The calculations predict the appearance of four Weyl nodes at\nzero energy within a gap smaller than the experimental resolution. These\ntopological states are theoretically predicted to remain stable within an\nextended region of the parameter space, well accessible by our device. These\nfindings establish an experimental foundation to study high-dimensional\nsynthetic band structures in multiterminal JJs, and to realize topological\nAndreev bands.",
        "Integrated sensing and communication (ISAC) has emerged as a transformative\nparadigm, enabling situationally aware and perceptive next-generation wireless\nnetworks through the co-design of shared network resources. With the adoption\nof millimeter-wave (mmWave) and terahertz (THz) frequency bands, ultra-massive\nMIMO (UM-MIMO) systems and holographic surfaces unlock the potential of\nnear-field (NF) propagation, characterized by spherical wavefronts that\nfacilitate beam manipulation in both angular and range domains. This paper\npresents a unified approach to near-field beam-training and sensing,\nintroducing a dual-purpose codebook design that employs discrete Fourier\ntransform (DFT)-based codebooks for coarse estimation of sensing parameters and\npolar codebooks for parameter refinement. Leveraging these range and angle\nestimates, a customized low-complexity space-time adaptive processing (STAP)\ntechnique is proposed for NF-ISAC to detect slow-moving targets and efficiently\nmitigate clutter. The interplay between codebooks and NF-STAP framework offers\nthree key advantages: reduced communication beam training overhead, improved\nestimation accuracy, and minimal STAP computational complexity. Simulation\nresults show that the proposed framework can reduce STAP complexity by three\norders of magnitude, validating efficacy, and highlighting the potential of the\nproposed approach to seamlessly integrate NF communication and sensing\nfunctionalities in future wireless networks.",
        "Valley-based topological phases offer a wealth of exotic quantum states with\ntunable functionalities, driven by the valley degree of freedom. In this work,\nby constructing heterostructures of germanene (silicene, stanene) on various\nmagnetic substrates, we address key tuning factors such as the spin-orbit\ncoupling (SOC) strength of the substrate, magnetic orientations, and stacking\norders, all of which govern multiple valley-based topological features. We\npresent a comprehensive guiding principle for the efficient manipulation of\nthese features, achieved simply by designing and modulating the magnetic\nproperties of the underlying substrates. Specifically, increasing the SOC\nstrength of the magnetic substrate acilitates a range of topological phase\ntransitions characterized by different Chern numbers, with many systems\nexhibiting a transition from quantum valley Hall to quantum anomalous Hall\n(QAH) states. Additionally, rotating the in-plane magnetic orientation of the\nsubstrate enables tunability of the Chern number and chirality, within a\nmoderate range of SOC strength. Furthermore, the antiferromagnetic coupling of\nthe magnetic substrate can induce valley-based QAH states with substantial\nvalley gaps, leveraging its high Curie temperature (TC) to enable the\nrealization of multiple tunable magnetic topologies at elevated temperatures.\nOur findings provide a straightforward strategy for the design and manipulation\nof spintronic and valleytronic devices that can potentially operate under\nhigh-temperature conditions.",
        "Galaxy morphologies and shape orientations are expected to correlate with\ntheir large-scale environment, since they grow by accreting matter from the\ncosmic web and are subject to interactions with other galaxies. Cosmic\nfilaments are extracted in projection from the Euclid Quick Data Release 1\n(covering 63.1 $\\mathrm{deg}^2$) at $0.5<z<0.9$ in tomographic slices of 170\ncomoving $h^{-1}\\mathrm{Mpc}$ using photometric redshifts. Galaxy morphologies\nare accurately retrieved thanks to the excellent resolution of VIS data. The\ndistribution of massive galaxies ($M_* > 10^{10} M_\\odot$) in the projected\ncosmic web is analysed as a function of morphology measured from VIS data.\nSpecifically, the 2D alignment of galaxy shapes with large-scale filaments is\nquantified as a function of S\\'ersic indices and masses. We find the known\ntrend that more massive galaxies are closer to filament spines. At fixed\nstellar masses, morphologies correlate both with densities and distances to\nlarge-scale filaments. In addition, the large volume of this data set allows us\nto detect a signal indicating that there is a preferential alignment of the\nmajor axis of massive early-type galaxies along projected cosmic filaments.\nOverall, these results demonstrate our capabilities to carry out detailed\nstudies of galaxy environments with Euclid, which will be extended to higher\nredshift and lower stellar masses with the future Euclid Deep Survey.",
        "Efficient spectrum allocation has become crucial as the surge in\nwireless-connected devices demands seamless support for more users and\napplications, a trend expected to grow with 6G. Innovations in satellite\ntechnologies such as SpaceX's Starlink have enabled non-terrestrial networks\n(NTNs) to work alongside terrestrial networks (TNs) and allocate spectrum based\non regional demands. Existing spectrum sharing approaches in TNs use machine\nlearning for interference minimization through power allocation and spectrum\nsensing, but the unique characteristics of NTNs like varying orbital dynamics\nand coverage patterns require more sophisticated coordination mechanisms. The\nproposed work uses a hierarchical deep reinforcement learning (HDRL) approach\nfor efficient spectrum allocation across TN-NTN networks. DRL agents are\npresent at each TN-NTN hierarchy that dynamically learn and allocate spectrum\nbased on regional trends. This framework is 50x faster than the exhaustive\nsearch algorithm while achieving 95\\% of optimum spectral efficiency. Moreover,\nit is 3.75x faster than multi-agent DRL, which is commonly used for spectrum\nsharing, and has a 12\\% higher overall average throughput.",
        "This paper presents a novel stochastic method for modeling the transport of\nDelayed Neutron Precursors (DNPs) in liquid nuclear fuel. The method\nincorporates advection and diffusion effects into the Monte Carlo solution of\nthe neutron balance equation by leveraging the Green's function of the\nadvection-diffusion-reaction (ADR) equation. For a 1D system, we demonstrate\nthat the Green's function can be interpreted as the Probability Density\nFunction (PDF) of the position increment of a Brownian motion with drift. Using\nthis interpretation, the position of DNPs is sampled via a time-of-flight\nprocess combined with a drift and diffusion model. The method is validated on a\nmodified 1D rod problem, where results from the Monte Carlo implementation are\ncompared against those obtained using a deterministic approach. The comparison\nconfirms that the method accurately captures the impact of fuel velocity and\ndiffusion on neutron flux. As expected, the fuel velocity shifts the neutron\nflux. Reactivity decreases as a function of speed while diffusion can\ncounteract this decrease under certain conditions. While the current study is\nlimited to 1D systems, the approach could be extended to higher dimensions and\nmore complex geometries by replacing the Green's function with the Stochastic\nDifferential Equation (SDE) associated with the ADR equation.",
        "Nonlinear evolutions of two-dimensional single-mode compressible\nRayleigh--Taylor instability (RTI) with isothermal stratification are\ninvestigated in cylindrical geometry via direct numerical simulation for\ndifferent Atwood numbers ($A_T=0.1-0.9$) and Mach numbers ($Ma=0.1-0.9$). It is\nfound that the nonlinear bubble growth involves the effects of density\nstratification, vorticity accumulation and flow compressibility and shows\nconsiderable differences between convergent (acceleration acting radially\ninward) and divergent (acceleration acting radially outward) cases.\nSpecifically, the density stratification leads to non-acceleration at low $A_T$\nand high $Ma$. The accelerations in convergent cases are dominated by vorticity\naccumulation at low $A_T$ and low $Ma$ and by flow compressibility at high\n$A_T$ and high $Ma$ whereas the accelerations in divergent cases are purely\ninduced by flow compressibility at high $A_T$ and high $Ma$. Based on the\nnonlinear theory of incompressible cylindrical RTI with uniform-density\nbackground~(Zhao et al., J. Fluid Mech., vol. 900, 2020, A24), an improved\nmodel is proposed by taking the density variation, vorticity accumulation and\nflow compressibility into consideration. This model is verified by numerical\nresults and well reproduces the bubble evolution for different $A_T$ and $Ma$\nfrom linear to highly nonlinear regimes.",
        "Video-based Automatic License Plate Recognition (ALPR) involves extracting\nvehicle license plate text information from video captures. Traditional systems\ntypically rely heavily on high-end computing resources and utilize multiple\nframes to recognize license plates, leading to increased computational\noverhead. In this paper, we propose two methods capable of efficiently\nextracting exactly one frame per vehicle and recognizing its license plate\ncharacters from this single image, thus significantly reducing computational\ndemands. The first method uses Visual Rhythm (VR) to generate time-spatial\nimages from videos, while the second employs Accumulative Line Analysis (ALA),\na novel algorithm based on single-line video processing for real-time\noperation. Both methods leverage YOLO for license plate detection within the\nframe and a Convolutional Neural Network (CNN) for Optical Character\nRecognition (OCR) to extract textual information. Experiments on real videos\ndemonstrate that the proposed methods achieve results comparable to traditional\nframe-by-frame approaches, with processing speeds three times faster.",
        "This paper investigates user preferences for Linear Top-k Queries and\nDirectional Top-k Queries, two methods for ranking results in multidimensional\ndatasets. While Linear Queries prioritize weighted sums of attributes,\nDirectional Queries aim to deliver more balanced results by incorporating the\nspatial relationship between data points and a user-defined preference line.\nThe study explores how preferences for these methods vary across different\ncontexts by focusing on two real-world topics: used cars (e-commerce domain)\nand football players (personal interest domain). A user survey involving 106\nparticipants was conducted to evaluate preferences, with results visualized as\nscatter plots for comparison. The findings reveal a significant preference for\ndirectional queries in the used cars topic, where balanced results align better\nwith user goals. In contrast, preferences in the football players topic were\nmore evenly distributed, influenced by user expertise and familiarity with the\ndomain. Additionally, the study demonstrates that the two specific topics\nselected for this research exhibit significant differences in their impact on\nuser preferences. This research reveals authentic user preferences,\nhighlighting the practical utility of Directional Queries for lifestyle-related\napplications and the subjective nature of preferences in specialized domains.\nThese insights contribute to advancing personalized database technologies,\nguiding the development of more user-centric ranking systems.",
        "As gravitational wave detections increase the number of observed compact\nbinaries (consisting of neutron stars or blacks), we begin to probe the\ndifferent conditions producing these binaries. Most studies of compact remnant\nformation focus either on stellar collapse from the evolution of field binary\nstars in gas-free environments or the formation of stars in clusters where\ndynamical interactions capture the compact objects, forming binaries. But a\nthird scenario exists. In this paper, we study the fate of massive stars\nformed, accrete gas, and evolve in the dense disks surrounding supermassive\nblack holes. We calculate the explosions produced and compact objects formed by\nthe collapse of these massive stars. Nucleosynthetic yields may provide an\nideal, directly observable, diagnostic of the formation and fate of these stars\nin active galactic nuclei. We present a first study of the explosive yields\nfrom these stars, comparing these yields with the observed nucleosynthetic\nsignatures in the disks around supermassive stars with quasars. We show that,\neven though these stars tend to form black holes, their rapid rotation leads to\ndisks that can eject a considerable amount of iron during the collapse of the\nstar. The nucleosynthetic yields from these stars can produce constraints on\nthe number of systems formed in this manner, but further work is needed to\nexploit variations from the initial models presented in this paper.",
        "Large Language Models (LLMs) have become state-of-the-art in Machine\nTranslation (MT), often trained on massive bilingual parallel corpora scraped\nfrom the web, that contain low-quality entries and redundant information,\nleading to significant computational challenges. Various data filtering methods\nexist to reduce dataset sizes, but their effectiveness largely varies based on\nspecific language pairs and domains. This paper evaluates the impact of\ncommonly used data filtering techniques, such as LASER, MUSE, and LaBSE, on\nEnglish-Polish translation within the biomedical domain. By filtering the UFAL\nMedical Corpus, we created varying dataset sizes to fine-tune the mBART50\nmodel, which was then evaluated using the SacreBLEU metric on the Khresmoi\ndataset, having the quality of translations assessed by bilingual speakers. Our\nresults show that both LASER and MUSE can significantly reduce dataset sizes\nwhile maintaining or even enhancing performance. We recommend the use of LASER,\nas it consistently outperforms the other methods and provides the most fluent\nand natural-sounding translations.",
        "Scaling quantum computing requires networked systems, leveraging HPC for\ndistributed simulation now and quantum networks in the future. Quantum\ndatacenters will be the primary access point for users, but current approaches\ndemand extensive manual decisions and hardware expertise. Tasks like algorithm\npartitioning, job batching, and resource allocation divert focus from quantum\nprogram development. We present a massively parallelized, automated QAOA\nworkflow that integrates problem decomposition, batch job generation, and\nhigh-performance simulation. Our framework automates simulator selection,\noptimizes execution across distributed, heterogeneous resources, and provides a\ncloud-based infrastructure, enhancing usability and accelerating quantum\nprogram development. We find that QAOA partitioning does not significantly\ndegrade optimization performance and often outperforms classical solvers. We\nintroduce our software components -- Divi, Maestro, and our cloud platform --\ndemonstrating ease of use and superior performance over existing methods.",
        "Excitation energy transfer between photosynthetic light-harvesting complexes\nis vital for highly efficient primary photosynthesis. Controlling this process\nis the key for advancing the emerging artificial photosynthetic systems. Here,\nwe experimentally demonstrate the enhanced excitation energy transfer between\nphotosynthetic light-harvesting 2 complexes (LH2) mediated through the\nFabry-Perot optical microcavity. Using intensity-dependent pump-probe\nspectroscopy, we analyse the exciton-exciton annihilation (EEA) due to\ninter-LH2 energy transfer. Comparing EEA in LH2 within cavity samples and the\nbare LH2 films, we observe enhanced EEA in cavities indicating improved\nexcitation energy transfer via coupling to a common cavity mode. Surprisingly,\nthe effect remains even in the weak coupling regime. The enhancement is\nattributed to the additional connectivity between LH2s introduced by the\nresonant optical microcavity. Our results suggest that optical microcavities\ncan be a strategic tool for modifying excitation energy transfer between\nmolecular complexes, offering a promising approach towards efficient artificial\nlight harvesting.",
        "The asymptotic behavior of GMM estimators depends critically on whether the\nunderlying moment condition model is correctly specified. Hong and Li (2023,\nEconometric Theory) showed that GMM estimators with nonsmooth\n(non-directionally differentiable) moment functions are at best\n$n^{1\/3}$-consistent under misspecification. Through simulations, we verify the\nslower convergence rate of GMM estimators in such cases. For the two-step GMM\nestimator with an estimated weight matrix, our results align with theory.\nHowever, for the one-step GMM estimator with the identity weight matrix, the\nconvergence rate remains $\\sqrt{n}$, even under severe misspecification.",
        "The partial alignment and conflict of autonomous agents lead to mixed-motive\nscenarios in many real-world applications. However, agents may fail to\ncooperate in practice even when cooperation yields a better outcome. One well\nknown reason for this failure comes from non-credible commitments. To\nfacilitate commitments among agents for better cooperation, we define Markov\nCommitment Games (MCGs), a variant of commitment games, where agents can\nvoluntarily commit to their proposed future plans. Based on MCGs, we propose a\nlearnable commitment protocol via policy gradients. We further propose\nincentive-compatible learning to accelerate convergence to equilibria with\nbetter social welfare. Experimental results in challenging mixed-motive tasks\ndemonstrate faster empirical convergence and higher returns for our method\ncompared with its counterparts. Our code is available at\nhttps:\/\/github.com\/shuhui-zhu\/DCL.",
        "Industrial ads ranking systems conventionally rely on labeled impression\ndata, which leads to challenges such as overfitting, slower incremental gain\nfrom model scaling, and biases due to discrepancies between training and\nserving data. To overcome these issues, we propose a Unified framework for\nKnowledge-Distillation and Semi-supervised Learning (UKDSL) for ads ranking,\nempowering the training of models on a significantly larger and more diverse\ndatasets, thereby reducing overfitting and mitigating training-serving data\ndiscrepancies. We provide detailed formal analysis and numerical simulations on\nthe inherent miscalibration and prediction bias of multi-stage ranking systems,\nand show empirical evidence of the proposed framework's capability to mitigate\nthose. Compared to prior work, UKDSL can enable models to learn from a much\nlarger set of unlabeled data, hence, improving the performance while being\ncomputationally efficient. Finally, we report the successful deployment of\nUKDSL in an industrial setting across various ranking models, serving users at\nmulti-billion scale, across various surfaces, geological locations, clients,\nand optimize for various events, which to the best of our knowledge is the\nfirst of its kind in terms of the scale and efficiency at which it operates."
      ]
    }
  },
  {
    "id":2411.00688,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"Convex generalizations of total variation based on the structure tensor with applications to inverse problems",
    "start_abstract":"We introduce a generic convex energy functional that is suitable for both grayscale and vector-valued images. Our functional is based on the eigenvalues of the structure tensor, therefore it penalizes image variation at every point by taking into account the information from its neighborhood. It generalizes several existing variational penalties, such as the Total Variation and vectorial extensions of it. By introducing the concept of patch-based Jacobian operator, we derive an equivalent formulation of the proposed regularizer that is based on the Schatten norm of this operator. Using this new formulation, we prove convexity and develop a dual definition for the proposed energy, which gives rise to an efficient and parallelizable minimization algorithm. Moreover, we establish a connection between the minimization of the proposed convex regularizer and a generic type of nonlinear anisotropic diffusion that is driven by a spatially regularized and adaptive diffusion tensor. Finally, we perform extensive experiments with image denoising and deblurring for grayscale and color images. The results show the effectiveness of the proposed approach as well as its improved performance compared to Total Variation and existing vectorial extensions of it.",
    "start_categories":[
      "eess.IV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b19"
      ],
      "title":[
        "Parameter-Free FISTA by Adaptive Restart and Backtracking"
      ],
      "abstract":[
        "We consider a combined restarting and adaptive backtracking strategy for the\npopular Fast Iterative Shrinking-Thresholding Algorithm frequently employed for\naccelerating the convergence speed of large-scale structured convex\noptimization problems. Several variants of FISTA enjoy a provable linear\nconvergence rate for the function values $F(x_n)$ of the form $\\mathcal{O}(\ne^{-K\\sqrt{\\mu\/L}~n})$ under the prior knowledge of problem conditioning, i.e.\nof the ratio between the (\\L ojasiewicz) parameter $\\mu$ determining the growth\nof the objective function and the Lipschitz constant $L$ of its smooth\ncomponent. These parameters are nonetheless hard to estimate in many practical\ncases. Recent works address the problem by estimating either parameter via\nsuitable adaptive strategies. In our work both parameters can be estimated at\nthe same time by means of an algorithmic restarting scheme where, at each\nrestart, a non-monotone estimation of $L$ is performed. For this scheme,\ntheoretical convergence results are proved, showing that a $\\mathcal{O}(\ne^{-K\\sqrt{\\mu\/L}n})$ convergence speed can still be achieved along with\nquantitative estimates of the conditioning. The resulting Free-FISTA algorithm\nis therefore parameter-free. Several numerical results are reported to confirm\nthe practical interest of its use in many exemplar problems."
      ],
      "categories":[
        "math.OC"
      ]
    },
    "list":{
      "title":[
        "Introduction of the G$_2$-Ricci Flow: Geometric Implications for\n  Spontaneous Symmetry Breaking and Gauge Boson Masses",
        "A bang-bang optimal control for a nonlinear system modeling the Gate\n  Control Theory of Pain",
        "Universality of Packing Dimension Estimates for Spectral Measures of\n  Quasiperiodic Operators: Monotone Potentials",
        "Major-Minor Mean Field Game of Stopping: An Entropy Regularization\n  Approach",
        "TOI-512: Super-Earth transiting a K-type star discovered by TESS and\n  ESPRESSO",
        "Chung-Graham and Zeckendorf representations",
        "The Layered Catalan Monoids: Structure and Determinants",
        "Multi-Channel Currency: A Secure Method Using Semi-Quantum Tokens",
        "High Resolution {\\it BOES} Spectroscopy of Raman-scattered\n  He~II$\\lambda$6545 in Young Planetary Nebulae",
        "Reducing Simulation Effort for RIS Optimization using an Efficient\n  Far-Field Approximation",
        "Learning in Markets with Heterogeneous Agents: Dynamics and Survival of\n  Bayesian vs. No-Regret Learners",
        "Large thermoelectric spin-valve effect with a superconductor",
        "High Harmonic Generation with Orbital Angular Momentum Beams:\n  Beyond-dipole Corrections",
        "Polynomial sequences with the same recurrence relation as Chebyshev\n  polynomials and the minimal polynomial of $2\\cos (2\\pi \/n)$",
        "Preference learning made easy: Everything should be understood through\n  win rate",
        "Classical Attack on Bell Inequalities",
        "Topological altermagnetic Josephson junctions",
        "On the Khovanov homology of 3-braids",
        "Bayesian estimation of Unit-Weibull distribution based on dual\n  generalized order statistics with application to the Cotton Production Data",
        "The feasibility of multi-graph alignment: a Bayesian approach",
        "Tate's question, Standard conjecture D, semisimplicity and Dynamical\n  degree comparison conjecture",
        "Multiplicity $=$ Volume formula and Newton non-degenerate ideals in\n  regular local rings",
        "Fast and Precise Spectral Analysis for Dark Matter Searches with LIGO",
        "Quantum effects in near-extremal charged black hole spacetimes",
        "Exact Stark analytical function for H{\\alpha} line based on the FFM\n  Model related with plasma parameters",
        "Well-Posedness of Contact Discontinuity Solutions and Vanishing Pressure\n  Limit for the Aw-Rascle Traffic Flow Model",
        "Slit-Slide-Sew bijections for planar bipartite maps with prescribed\n  degree",
        "Reinforced Galton--Watson processes III: Empirical offspring\n  distributions",
        "Integrated Computation and Communication with Fiber-optic Transmissions"
      ],
      "abstract":[
        "This work introduces the G$_2$-Ricci flow on seven-dimensional manifolds with\nnon-zero torsion and explores its physical implications. By extending the Ricci\nflow to manifolds with G$_2$ structures, we study the evolution of solitonic\nsolutions and their role in spontaneous symmetry breaking in gauge theories. In\nparticular, this model proposes that the masses of the W and Z bosons are\ndetermined not by an external scalar field, as in the Higgs mechanism, but by\nthe intrinsic geometric torsion of the manifold. Furthermore, a possible\nconnection between the geometry of extra dimensions and the curvature of our\nspacetime is explored, with implications for the experimentally observed\npositive cosmological constant. This approach provides an innovative\ninterpretation of fundamental interactions in theoretical physics, opening new\npossibilities for studying extra dimensions and the geometry of\nG$_2$-manifolds.",
        "We consider a nonlinear system of coupled ordinary differential equations\n(representing the excitatory, inhibitory, and T-cell potentials) based on the\nGate Control Theory of Pain, initially proposed by R. Melzack and P.D. Wall in\n1965, and later mathematically modeled by N.F. Britton and S.M. Skevington in\n1988.",
        "Let $H$ be a quasiperiodic Schr\\\"{o}dinger operator generated by a monotone\npotential, as defined in [16]. Following [20], we study the connection between\nthe Lyapunov exponent $L\\left(E\\right)$, arithmetic properties of the frequency\n$\\alpha$, and certain fractal-dimensional properties of the spectral measures\nof $H$.",
        "This paper studies a discrete-time major-minor mean field game of stopping\nwhere the major player can choose either an optimal control or stopping time.\nWe look for the relaxed equilibrium as a randomized stopping policy, which is\nformulated as a fixed point of a set-valued mapping, whose existence is\nchallenging by direct arguments. To overcome the difficulties caused by the\npresence of a major player, we propose to study an auxiliary problem by\nconsidering entropy regularization in the major player's problem while\nformulating the minor players' optimal stopping problems as linear programming\nover occupation measures. We first show the existence of regularized equilibria\nas fixed points of some simplified set-valued operator using the\nKakutani-Fan-Glicksberg fixed-point theorem. Next, we prove that the\nregularized equilibrium converges as the regularization parameter $\\lambda$\ntends to 0, and the limit corresponds to a fixed point of the original\noperator, thereby confirming the existence of a relaxed equilibrium in the\noriginal mean field game problem.",
        "One of the goals of the ESPRESSO guaranteed time observations (GTOs) at the\nESO 8.2m telescope is to follow up on candidate planets from transit surveys\nsuch as the TESS mission. High-precision radial velocities are required to\ncharacterize small exoplanets. Aims. We intend to confirm the existence of a\ntransiting super-Earth around the bright (V=9.74) K0-type star TOI-512 (TIC\n119292328) and provide a characterization. Combining photometric data from TESS\nand 37 high-resolution spectroscopic observations from ESPRESSO in a joint\nMarkov chain Monte Carlo analysis, we determined the planetary parameters of\nTOI-512b and characterized its internal structure. We find that TOI-512b is a\nsuper-Earth, with a radius of $1.54 \\pm 0.10$ R$_\\oplus$ and mass of\n$3.57_{-0.55}^{+0.53}$~M$_\\oplus$, on a $7.19_{-6.1\\cdot 10^{-5}}^{+7\\cdot\n10^{-5}}$ day orbit. This corresponds to a bulk density of\n$5.62_{-1.28}^{+1.59}$ g cm$^{-3}$. Our interior structure analysis presents a\nsmall inner core representing $0.13^{+0.13}_{-0.11}$ of the solid mass fraction\nfor the planet, surrounded by a mantle with a mass fraction of\n$0.69^{+0.20}_{-0.22}$, and an upper limit of the water layer of $0.16$. The\ngas mass below $10^{-8.93}$ indicates a very small amount of gas on the planet.\nWe find no evidence of the second candidate found by the TESS pipeline,\nTOI-512.02, neither in TESS photometry, nor in the ESPRESSO radial velocities.\nThe low stellar activity makes it an interesting transmission spectroscopy\ncandidate for future-generation instruments.",
        "We examine the relationship between the Chung-Graham and Zeckendorf\nrepresentations of an integer using the software package {\\tt Walnut}.",
        "In this paper, we introduce and study a class of monoids, called Layered\nCatalan Monoids (\\( {LC}_n \\)), which satisfy the structural conditions for\n$\\ll$-smoothness as defined in~\\cite{Sha-Det2}. These monoids are defined by\nspecific identities inspired by Catalan monoids. We establish their canonical\nforms and compute their determinant, proving that it is non-zero for \\(1 \\leq n\n\\leq 7\\) but vanishes for \\(n \\geq 8\\).",
        "Digital currencies primarily operate online, but there is growing interest in\nenabling offline transactions to improve digital inclusion. Existing offline\nmethods struggle with double-spending risks, often limiting transaction\namounts. In this work, we propose a quantum-state-based currency system that\nuses the non-cloning theorem to enable secure, multi-channel transactions\nwithout the risk of double spending. We demonstrate this system's\nimplementation with experimental results, including use cases for currency\ntransfers and swaps. To mitigate credit risks in swaps, we also integrate\nblockchain to show its wide applicability. Our approach paves the way for\nquantum-secure digital currencies and opens new possibilities for optimizing\nmulti-channel tokens.",
        "Young planetary nebulae (PNe) are characterized by their hot central stars\nand the presence of abundant neutral and molecular components, which result\nfrom significant mass loss during the asymptotic giant branch (AGB) phase of\nstellar evolution. Far-UV \\ion{He}{2}$\\lambda$1025 line photons produced near\nthe central star can undergo Raman scattering by hydrogen atoms, creating a\nbroad emission feature centered at $\\sim$ 6545~\\AA. We conducted\nhigh-resolution spectroscopy of 12 young PNe from April 2019 to March 2020\nusing the Bohyunsan Observatory Echelle Spectrograph ({\\it BOES}). Building on\nthe study by Choi and Lee, who identified Raman-scattered \\ion{He}{2} at\n6545~\\AA\\ in NGC~6881 and NGC~6886, we report new detections of this feature in\nNGC~6741 and NGC~6884. Profile fitting reveals that the velocity of the\n\\ion{H}{1} component relative to the \\ion{He}{2} emission region ranges from\n$26-33~{\\rm km~s^{-1}}$ in these PNe. Using photoionization modeling, we\nestimate the line flux of \\ion{He}{2}$\\lambda$1025 and derive Raman conversion\nefficiencies of 0.39, 0.21, 0.24, and 0.07 for NGC~6881, NGC~6741, NGC~6886,\nand NGC~6884, respectively. These results, combined with radiative transfer\nmodeling, suggest the presence of \\ion{H}{1} components with masses around\n$10^{-2}~M_\\odot$, moving outward from the central \\ion{He}{2} emission region\nat speeds characteristic of the slow stellar wind from a mass-losing giant\nstar.",
        "Optimization of Reconfigurable Intelligent Surfaces (RIS) via a previously\nintroduced method is effective, but time-consuming, because multiport impedance\nor scatter matrices are required for each transmitter and receiver position,\nwhich generally must be obtained through full-wave simulation. Herein, a simple\nand efficient far-field approximation is introduced, to extrapolate scatter\nmatrices for arbitrary receiver and transmitter positions from only a single\nsimulation while still maintaining high accuracy suitable for optimization\npurposes. This is demonstrated through comparisons of the optimized capacitance\nvalues and further supported by empirical measurements.",
        "We analyze the performance of heterogeneous learning agents in asset markets\nwith stochastic payoffs. Our agents aim to maximize the expected growth rate of\ntheir wealth but have different theories on how to learn this best. We focus on\ncomparing Bayesian and no-regret learners in market dynamics. Bayesian learners\nwith a prior over a finite set of models that assign positive prior probability\nto the correct model have posterior probabilities that converge exponentially\nto the correct model. Consequently, they survive even in the presence of agents\nwho invest according to the correct model of the stochastic process. Bayesians\nwith a continuum prior converge to the correct model at a rate of $O((\\log\nT)\/T)$. Online learning theory provides no-regret algorithms for maximizing the\nlog of wealth in this setting, achieving a worst-case regret bound of $O(\\log\nT)$ without assuming a steady underlying stochastic process but comparing to\nthe best fixed investment rule. This regret, as we observe, is of the same\norder of magnitude as that of a Bayesian learner with a continuum prior.\nHowever, we show that even such low regret may not be sufficient for survival\nin asset markets: an agent can have regret as low as $O(\\log T)$, but still\nvanish in market dynamics when competing against agents who invest according to\nthe correct model or even against a perfect Bayesian with a finite prior. On\nthe other hand, we show that Bayesian learning is fragile, while no-regret\nlearning requires less knowledge of the environment and is therefore more\nrobust. Any no-regret learner will drive out of the market an imperfect\nBayesian whose finite prior or update rule has even small errors. We formally\nestablish the relationship between notions of survival, vanishing, and market\ndomination studied in economics and the framework of regret minimization, thus\nbridging these theories.",
        "Recent studies have revealed magnetically controllable thermoelectric effects\nin superconductor\/ferromagnet (S\/F) structures. A tunable cryogenic\nthermoelectric generator needs not only a high conversion factor between\nelectricity and heat, but also a large change in the thermoelectric output when\nswitching the magnetic state of the device. Here, we experimentally measure and\nnumerically model thermoelectric effects in fully epitaxial F\/S\/F junctions\nbased on commercially available, easily grown materials, as well as their\ndependence on the magnetic configuration of the F electrodes. We observe\nsizeable Seebeck coefficients for the parallel alignment of the ferromagnetic\nelectrodes, reaching values of about $100$~$\\mu$V\/K. Importantly, we find a\ndecrease of the thermoelectric signal of more than an order of magnitude when\nswitching from a parallel to an antiparallel configuration, constituting a\nlarge thermoelectric spin-valve effect. Theoretical modeling based on a\nself-consistent non-equilibrium Keldysh-Usadel Green function theory, combined\nwith micromagnetic simulations, qualitatively reproduce the experimental\nfindings. These findings pave the way for the development of efficient and\nversatile cryogenic thermoelectric heat engines.",
        "We study the high harmonic generation with vortex beams beyond the dipole\napproximation. To do so we employ the full minimal coupling approach to account\nfor multipolar coupling without truncation and describe the full\nspatio-temporal properties of the electromagnetic field. This allows us to\ninvestigate the beyond-dipole deviations in electron trajectories and the\nemitted power, where the influence of the orbital angular momentum contains\nboth magnetic and quadrupolar effects. In contrast to the system driven by\nplane-wave light, we show that the non-linear dipole dynamics induced by the\nvortex beams are not confined to the polarization or propagation directions,\nbut also have a component in the orthogonal direction. We identify the effects\nof the resulting symmetry breaking via increased beyond dipole corrections\nwhich are particularly apparent in even harmonics.",
        "In this paper we consider the minimal polynomial $\\psi_n(x)$ of $2\\cos (2\\pi\n\/n)$. We introduce some polynomial sequences with the same recurrence relation\nas the rescaled Chebyshev polynomials $t_n(x)=2\\, T_n(x\/2)$ of the first kind,\nwhich turn out to be related to those of various kinds, all coming from those\nof the second kind. We see that $t_n(x)\\pm 2=2(T_n(x\/2)\\pm 1)$ are divisible by\nthe square of either of these polynomials. Then by appropriately removing\nunnecessary factors from these polynomials, we can easily calculate\n$\\psi_n(x)$, which improves Barnes' result in 1977. As an appendix, we give a\ncompact list of the minimal polynomials $\\psi_n(x)$ of $2\\cos (2\\pi \/n)$ for\n$n\\leqslant 120$.",
        "Preference learning, or the task of aligning generative models to preference\ncomparison data, has yet to reach the conceptual maturity of classification,\ndensity estimation, etc. To close this gap, this work presents a framework to\nunderstand preference learning starting from the sampling distribution of\npairwise preference data. First, we prove that the only evaluation of a\ngenerative model that respects both preferences and prevalences in the data\ndistribution is a form of win rate, justifying win rate as the focal point to\nunderstand preference learning. We then analyze preference learning methods as\nwin rate optimization (WRO) or non-WRO. We present novel instances of WRO\nbeyond existing examples (RLHF, NLHF) and identify two key theoretical benefits\nof all such methods. We prove that common non-WRO methods like DPO and SFT on\npreferred samples lack these properties and suggest ways to mitigate such\ntheoretical limitations. We also show that WRO underperforms in practice due\noptimization difficulties and that optimization success predicts performance\nbetter than choices which affect the objective's solution. Our analysis\nhighlights best practices for existing methods and provides recommendations for\nfuture research, guided by the principle that one should either align non-WRO\nmethods more closely with WRO or improve the optimization of WRO objectives.",
        "Representing multi-mode squeezed light with a Gaussian random vector, our\nlocally deterministic detection model challenges the CHSH game, achieving\nfidelities exceeding 96\\%. Squeezing strength, detector threshold, and\nefficiency influence the security of the quantum bound.",
        "Planar Josephson junctions are pivotal for engineering topological\nsuperconductivity, yet are severely hindered by orbital effects induced by\nin-plane magnetic fields. In this work, we introduce the generic topological\naltermagnetic Josephson junctions (TAJJs) by leveraging the intrinsic\nspin-polarized band splitting and zero net magnetization attributes of\naltermagnets. Our proposed TAJJs effectively mitigate the detrimental orbital\neffects while robustly hosting Majorana end modes (MEMs) at both ends of the\njunction. Specifically, we demonstrate that MEMs emerge in $d_{x^2-y^2}$-wave\nTAJJs but vanish in the $d_{xy}$-wave configuration, thereby establishing the\ncrystallographic orientation angle $\\theta$ of the altermagnet as a novel\ncontrol parameter of topology. The distinct spin-polarization of the MEMs\nprovides an unambiguous experimental signature for the spin-resolved\nmeasurement. Furthermore, by harnessing the synergy between the\n$d_{x^2-y^2}$-wave altermagnet and its superconducting counterpart, our\nproposal extends to high-$T_c$ platforms naturally. Overall, this work\nestablishes altermagnets as a versatile paradigm for realizing topological\nsuperconductivity, bridging conceptual innovations with scalable quantum\narchitectures devoid of orbital effects and stray fields.",
        "We prove the conjecture of Przytycki and Sazdanovic that the Khovanov\nhomology of the closure of a 3-stranded braid only contains torsion of order 2.\nThis conjecture has been known for six out of seven classes in the\nMurasugi-classification of 3-braids and we show it for the remaining class. Our\nproof also works for the other classes and relies on Bar-Natan's version of\nKhovanov homology for tangles as well as his delooping and cancellation\ntechniques, and the reduced integral Bar-Natan--Lee--Turner spectral sequence.\nWe also show that the Knight-move conjecture holds for 3-braids.",
        "The Unit Weibull distribution with parameters $\\alpha$ and $\\beta$ is\nconsidered to study in the context of dual generalized order statistics. For\nthe analysis purpose, Bayes estimators based on symmetric and asymmetric loss\nfunctions are obtained. The methods which are utilized for Bayesian estimation\nare approximation and simulation tools such as Lindley, Tierney-Kadane and\nMarkov chain Monte Carlo methods. The authors have considered squared error\nloss function as symmetric and LINEX and general entropy loss function as\nasymmetric loss functions. After presenting the mathematical results, a\nsimulation study is conducted to exhibit the performances of various derived\nestimators. As this study is considered for the dual generalized order\nstatistics that is unification of models based distinct ordered random variable\nsuch as order statistics, record values, etc. This provides flexibility in our\nresults and in continuation of this, the cotton production data of USA is\nanalyzed for both submodels of ordered random variables: order statistics and\nrecord values.",
        "We establish thresholds for the feasibility of random multi-graph alignment\nin two models. In the Gaussian model, we demonstrate an \"all-or-nothing\"\nphenomenon: above a critical threshold, exact alignment is achievable with high\nprobability, while below it, even partial alignment is statistically\nimpossible. In the sparse Erd\\H{o}s-R\\'enyi model, we rigorously identify a\nthreshold below which no meaningful partial alignment is possible and\nconjecture that above this threshold, partial alignment can be achieved. To\nprove these results, we develop a general Bayesian estimation framework over\nmetric spaces, which provides insight into a broader class of high-dimensional\nstatistical problems.",
        "Let $X$ be a smooth projective variety of dimension $n$ over the algebraic\nclosure of a finite field $\\mathbb{F}_p$.\n  Assuming the standard conjecture $D$, we prove\n  a weaker form of the Dynamical Degree Comparison conjecture;\n  equivalence of semisimplicity of Frobenius endomorphism and of any polarized\nendomorphism (a more general result, in terms of the biggest size of Jordan\nblocks, holds).\n  We illustrate these results through examples, including varieties dominated\nby rational maps from Abelian varieties and suitable products of $K3$ surfaces.\n  Using the same idea, we provide a new proof of the main result in a recent\npaper by the third author, including Tate's question\/Serre's conjecture that\nfor a polarized endomorphism $f:X\\rightarrow X$, all eigenvalues of the action\nof $f$ on $H^k(X)$ have the same absolute value.",
        "We develop the notions of Newton non-degenerate (NND) ideals and Newton\npolyhedra for regular local rings. These concepts were first defined in the\ncontext of complex analysis. We show that the characterization of NND ideals\nvia their integral closures known in the analytical setting extends to regular\nlocal rings. We use the limiting body $\\mathcal{C}(\\mathcal{I})$ associated to\na graded family $\\mathcal{I}$ of ideals to provide a new understanding of the\ncelebrated \"Multiplicity $=$ Volume\" formula. Particularly, we prove that, for\na Noetherian graded family $\\mathcal{I}$ of $\\mathfrak{m}$-primary ideals in a\nregular local ring $(R,\\mathfrak{m})$ of dimension $d$, the equality\n$$e(\\mathcal{I}) = d!\\text{co-vol}_d(\\mathcal{C}(\\mathcal{I}))$$ holds if and\nonly if $\\mathcal{I}$ contains certain subfamily of NND ideals.",
        "We introduce a novel logarithmic spectral estimation method for dark matter\nsearches using gravitational-wave detectors, integrating established dark\nmatter search techniques with insights from computer music analysis. By\nleveraging symmetries between the time and frequency domains, this method\nmatches the computational efficiency of FFT based algorithms without, unlike\nsuch algorithms, compromising precision. We apply this approach to data from\nLIGO's third observing run, directly comparing its performance with that of a\nprevious search. Our results show a consistent 15 percent improvement across\nnearly the entire frequency range, without additional computational costs. With\npotential for further refinements, this method already offers a solution\ncapable of maximizing the scientific potential of current and future\ngravitational-wave observatories.",
        "We compute the semiclassical current and stress-energy fluxes both at the\nevent and Cauchy horizon of a near-extremal Reissner-Nordstr\\\"om black hole. We\nconsider a minimally-coupled, massless, charged scalar field in the Unruh\nstate, describing an evaporating black hole. The near-extremal domain allows\nfor an analytical treatment of the scattering problem of the Boulware modes\nboth in the interior and exterior regions. We present this and explicit\nanalytical expressions for $\\langle j_v\\rangle $, $\\langle T_{vv}\\rangle$ at\nthe horizons, as well as estimates for $\\langle j_u\\rangle$ and $\\langle\nT_{uu}\\rangle $. We cross-check the analytical results numerically by bringing\nthe radial Klein-Gordon equation into the form of the confluent Heun equation.\nInserting these expectation values as sources to the Einstein-Maxwell\nequations, we find that at least in the near-extremal regime of small field\ncharge, quantum effects drive the black hole interior away from extremality.\nOur work generalizes the known results for the real scalar field [Phys. Rev. D\n104, 024066] and is in agreement with recent work on charged fields in\nexpanding Reissner-Nordstr\\\"om deSitter universes [Phys. Rev. Lett. 127,\n231301, Phys. Rev. D 104, 025009].",
        "Optical Emission Spectroscopy is a widely used technique for plasma\ndiagnosis, with particular interest in hydrogen atomic emission due to its\nprevalence in plasmas. However, accurately determining plasma parameters like\nelectron density, electron temperature, and gas temperature starting from the\nexperimental profiles remains a challenge. This paper introduces a\ncomprehensive model for Stark broadening of the H{\\alpha} line in a wide range\nof plasma conditions, addressing the limitations of existing analytical\nexpressions for line shapes. The proposed model encompasses the full splitting\nof the transition into fifteen Lorentzian profiles and electric micro-field\nfluctuations surrounding the emitting atoms due to collisions with charged\nparticles. Starting from accurate spectral data obtained from realistic\ncomputer simulations, fitting parameters of the model, have been obtained by\nusing an optimization method based on a genetic algorithm. The set of\nparameters of the model are reported for a wide range of plasma conditions. The\nbehavior of these parameters is analyzed to understand their dependence in\nterms of the electron density and temperature and gas density of the plasma.\nThe model parameters here obtained constitute a useful tool in plasma diagnosis\nto obtaining the values of the physical parameters of the plasma starting from\nthe experimental profiles.",
        "This paper investigates the well-posedness of contact discontinuity solutions\nand the vanishing pressure limit for the Aw-Rascle traffic flow model with\ngeneral pressure functions. The well-posedness problem is formulated as a free\nboundary problem, where initial discontinuities propagate along linearly\ndegenerate characteristics. By mollifying initial data in Lagrangian\ncoordinates, the problem is reduced to analyzing the limit of classical\nsolutions. To avoid equation degeneracy due to vacuum states, we establish a\nuniform lower bound for density by categorizing discontinuity types at singular\npoints. Using level sets for velocity derivatives, we derive uniform estimates\nfor density and velocity gradients. Through equivalence of coordinate\ntransformations, the well-posedness of contact discontinuity solutions in\noriginal coordinates is rigorously proven. Results show that compressive\ninitial data induce finite-time singularity formation, while rarefactive\ninitial data ensure global existence. Furthermore, we demonstrate that\nsolutions of the Aw-Rascle model converge to those of the pressureless model\nunder vanishing pressure, with matching convergence rates for velocity and\ncharacteristic triangles. By enhancing regularity in non-discontinuous regions,\nwe prove convergence of the Aw-Rascle model's blow-up time to the pressureless\nmodel's blow-up time. Finally, analogous results are extended to the Chaplygin\ngas pressure case.",
        "We present a bijective proof for the planar case of Louf's counting formula\non bipartite planar maps with prescribed face degree, that arises from the Toda\nhierarchy. We actually show that his formula hides two simpler formulas, both\nof which can be rewritten as equations on trees using duality and Schaeffer's\nbijection for eulerian maps. We prove them bijectively and show that the\nconstructions we provide for trees can also be interpreted as \"slit-slide-sew\"\noperations on maps. As far as we know, this is the first bijection for a\nformula arising from an integrable hierarchy with infinitely many parameters.",
        "Reinforced Galton--Watson processes describe the dynamics of a population\nwhere reproduction events are reinforced, in the sense that offspring numbers\nof forebears can be repeated randomly by descendants. More specifically, the\nevolution depends on the empirical offspring distribution of each individual\nalong its ancestral lineage. We are interested here in asymptotic properties of\nthe empirical distributions observed in the population, such as concentration,\nevanescence and persistence. For this, we incorporate tools from the theory of\nlarge deviations to our preceding analysis [arXiv:2306.02476,arXiv:2310.19030].",
        "Fiber-optic transmission systems are leveraged not only as high-speed\ncommunication channels but also as nonlinear kernel functions for machine\nlearning computations, enabling the seamless integration of computational\nintelligence and communication."
      ]
    }
  },
  {
    "id":2412.00036,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"On the Distribution of the Two-Sample Cramer-von Mises Criterion",
    "start_abstract":"The Cramer-von Mises $\\omega^2$ criterion for testing that a sample, $x_1, \\cdots, x_N$, has been drawn from specified continuous distribution $F(x)$ is \\begin{equation*}\\tag{1}\\omega^2 = \\int^\\infty_{-\\infty} \\lbrack F_N(x) - F(x)\\rbrack^2 dF(x),\\end{equation*} where $F_N(x)$ the empirical function of sample; is, $F_N(x) k\/N$ if exactly $k$ observations are less than or equal to $x(k 0, 1, N)$. If there second $y_1, y_M$, test hypothesis two samples come same (unspecified) can be based on analogue $N\\omega^2$, namely \\begin{equation*}\\tag{2} T NM\/(N + M)\\rbrack G_M(x)\\rbrack^2 dH_{N+M}(x),\\end{equation*} $G_M(x)$ sample and $H_{N+M}(x)$ together [that $(N M)H_{N+M}(x) NF_N(x) MG_M(x)\\rbrack$. limiting $N\\omega^2$ as $N \\rightarrow \\infty$ tabulated [2], it shown ([3], [4a], [7]) $T$ \\infty, M \\infty$, $N\/M \\lambda$, $\\lambda$ any finite positive constant. In this note we consider small values $N$ $M$ present tables permit use at some conventional significance levels $M$. seems surprisingly good approximation exact moderate sizes (corresponding feature [6]). accuracy better in case two-sample Kolmogorov-Smirnov statistic studied by Hodges [4].",
    "start_categories":[
      "q-fin.GN"
    ],
    "start_fields":[
      "Economics and Quantitative Finance"
    ],
    "target_paper":{
      "id":[
        "b24"
      ],
      "title":[
        "Quant GANs: deep generation of financial time series"
      ],
      "abstract":[
        "Modeling financial time series by stochastic processes is a challenging task and a central area of research in financial mathematics. As an alternative, we introduce Quant GANs, a data-driven model which is inspired by the recent success of generative adversarial networks (GANs). Quant GANs consist of a generator and discriminator function, which utilize temporal convolutional networks (TCNs) and thereby achieve to capture long-range dependencies such as the presence of volatility clusters. The generator function is explicitly constructed such that the induced stochastic process allows a transition to its risk-neutral distribution. Our numerical results highlight that distributional properties for small and large lags are in an excellent agreement and dependence properties such as volatility clusters, leverage effects, and serial autocorrelations can be generated by the generator function of Quant GANs, demonstrably in high fidelity."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Performance of the ALICE Inner Tracking System 2",
        "Empowering the Future Workforce: Prioritizing Education for the\n  AI-Accelerated Job Market",
        "Magnetic Field Structures In and Around Seyfert Galaxy Outflows",
        "Human Re-ID Meets LVLMs: What can we expect?",
        "Providing Machine Learning Potentials with High Quality Uncertainty\n  Estimates",
        "Causal AI-based Root Cause Identification: Research to Practice at Scale",
        "Non-local modular flows across deformed null-cuts",
        "CCJA: Context-Coherent Jailbreak Attack for Aligned Large Language\n  Models",
        "2D transmons with lifetimes and coherence times exceeding 1 millisecond",
        "Validation of the DESI DR2 Measurements of Baryon Acoustic Oscillations\n  from Galaxies and Quasars",
        "Fusion DeepONet: A Data-Efficient Neural Operator for Geometry-Dependent\n  Hypersonic Flows on Arbitrary Grids",
        "On the Effectiveness of Random Weights in Graph Neural Networks",
        "3+1 neutrino mixings model with $A_4$ triplet Majorana neutrino",
        "Mapping AI Avant-Gardes in Time: Posthumanism, Transhumanism,\n  Genhumanism",
        "regMMD: a package for parametric estimation and regression with maximum\n  mean discrepancy",
        "A proof of the multi-component $q$-Baker--Forrester conjecture",
        "Post-Newtonian theory-inspired framework for characterizing eccentricity\n  in gravitational waveforms",
        "Programs Versus Finite Tree-Programs",
        "Uniform stability for the inverse Sturm-Liouville problem with\n  eigenparameter-dependent boundary conditions",
        "Evidence for a hot galactic halo around the Andromeda Galaxy using fast\n  radio bursts",
        "Abelian varieties analogs of two results about algebraic curves",
        "Existence of Constant Mean Curvature Surfaces in Asymptotically Flat and\n  Asymptotically Hyperbolic Manifolds",
        "Around the Merino--Welsh conjecture: improving Jackson's inequality",
        "Predictive Performance of Photonic SRAM-based In-Memory Computing for\n  Tensor Decomposition",
        "Training Data Provenance Verification: Did Your Model Use Synthetic Data\n  from My Generative Model for Training?",
        "Dephasing-tolerant quantum sensing for transverse magnetic fields with\n  spin qudits",
        "A three-family supersymmetric Pati-Salam model from rigid intersecting\n  D6-branes",
        "Harnessing Generative Pre-Trained Transformer for Datacenter Packet\n  Trace Generation",
        "Revisiting the $\\phi^6$ Theory in Three Dimensions at Large $N$"
      ],
      "abstract":[
        "The upgraded Inner Tracking System (ITS2) of the ALICE experiment at the CERN\nLarge Hadron Collider is based on Monolithic Active Pixel Sensors (MAPS). With\na sensitive area of about 10 $m^2$ and 12.5 billion pixels, ITS2 represents the\nlargest pixel detector in high-energy physics. The detector consists of seven\nconcentric layers equipped with ALPIDE pixel sensors manufactured in the\nTowerJazz 180 nm CMOS Imaging Sensor process. The high spatial resolution and\nlow material budget, in combination with small radial distance of the innermost\nlayer from the interaction point, make the detector well suited for secondary\nvertex reconstruction as well as for tracking at low transverse momentum.\n  This paper will present the detector performance during the LHC Run 3 and\ngive an overview on the calibration methods and running experience.",
        "AI's rapid integration into the workplace demands new approaches to workforce\neducation and training and broader AI literacy across disciplines. Coordinated\naction from government, industry, and educational institutions is necessary to\nensure workers can adapt to accelerating technological change.",
        "We present radio polarimetric images of 12 Seyfert and Low-Ionization Nuclear\nEmission-line Region (LINER) galaxies belonging to the Centre for Astrophysics\n(CfA)+12 micron sample exhibiting kiloparsec-scale radio outflows (KSRs). These\nobservations have been carried out at 10 GHz with Karl G. Jansky Very Large\nArray (VLA) in D-array and at 1.4 GHz with the BnA$\\rightarrow$A array\nconfigurations. We find signatures of organized magnetic (B-) field structures\nin the cores, jets and lobes of these galaxies. The linear polarization\nfraction varies from a few per cent in the cores to $47\\pm18$ per cent in the\nlobes. The inferred B-fields are toroidal in the cores of several sources\nmaking them consistent with the presence of either a sheath-like or a wind-like\ncomponent surrounding the jet. The in-band spectral index images typically show\nthe presence of flat\/inverted spectrum cores and steep spectrum lobes. Radio\ncores with flatter spectra are found to have lower Eddington ratios while the\nsteeper ones have higher. A strong correlation is observed between the\nSeyfert\/LINER radio outflow properties and the mass of the supermassive black\nholes (SMBHs); correlations with Eddington ratios are weaker. We find\nsignatures of jet-medium interaction and both positive and negative AGN\nfeedback in these sources. Overall, our study indicates that radio-quiet (RQ)\nAGN with KSRs possess radio outflows driven by magnetic fields anchored to\ntheir black holes - accretion disks, which significantly impact their\nenvironments.",
        "Large vision-language models (LVLMs) have been regarded as a breakthrough\nadvance in an astoundingly variety of tasks, from content generation to virtual\nassistants and multimodal search or retrieval. However, for many of these\napplications, the performance of these methods has been widely criticized,\nparticularly when compared with state-of-the-art methods and technologies in\neach specific domain. In this work, we compare the performance of the leading\nlarge vision-language models in the human re-identification task, using as\nbaseline the performance attained by state-of-the-art AI models specifically\ndesigned for this problem. We compare the results due to ChatGPT-4o,\nGemini-2.0-Flash, Claude 3.5 Sonnet, and Qwen-VL-Max to a baseline ReID\nPersonViT model, using the well-known Market1501 dataset. Our evaluation\npipeline includes the dataset curation, prompt engineering, and metric\nselection to assess the models' performance. Results are analyzed from many\ndifferent perspectives: similarity scores, classification accuracy, and\nclassification metrics, including precision, recall, F1 score, and area under\ncurve (AUC). Our results confirm the strengths of LVLMs, but also their severe\nlimitations that often lead to catastrophic answers and should be the scope of\nfurther research. As a concluding remark, we speculate about some further\nresearch that should fuse traditional and LVLMs to combine the strengths from\nboth families of techniques and achieve solid improvements in performance.",
        "Computational chemistry has come a long way over the course of several\ndecades, enabling subatomic level calculations particularly with the\ndevelopment of Density Functional Theory (DFT). Recently, machine-learned\npotentials (MLP) have provided a way to overcome the prevalent time and length\nscale constraints in such calculations. Unfortunately, these models utilise\ncomplex and high dimensional representations, making it challenging for users\nto intuit performance from chemical structure, which has motivated the\ndevelopment of methods for uncertainty quantification. One of the most common\nmethods is to introduce an ensemble of models and employ an averaging approach\nto determine the uncertainty. In this work, we introduced Bayesian Neural\nNetworks (BNNs) for uncertainty aware energy evaluation as a more principled\nand resource efficient method to achieve this goal. The richness of our\nuncertainty quantification enables a new type of hybrid workflow where\ncalculations can be offloaded to a MLP in a principled manner.",
        "Modern applications are built as large, distributed systems spanning numerous\nmodules, teams, and data centers. Despite robust engineering and recovery\nstrategies, failures and performance issues remain inevitable, risking\nsignificant disruptions and affecting end users. Rapid and accurate root cause\nidentification is therefore vital to ensure system reliability and maintain key\nservice metrics.\n  We have developed a novel causality-based Root Cause Identification (RCI)\nalgorithm that emphasizes causation over correlation. This algorithm has been\nintegrated into IBM Instana-bridging research to practice at scale-and is now\nin production use by enterprise customers. By leveraging \"causal AI,\" Instana\nstands apart from typical Application Performance Management (APM) tools,\npinpointing issues in near real-time. This paper highlights Instana's advanced\nfailure diagnosis capabilities, discussing both the theoretical underpinnings\nand practical implementations of the RCI algorithm. Real-world examples\nillustrate how our causality-based approach enhances reliability and\nperformance in today's complex system landscapes.",
        "Modular flows probe important aspects of the entanglement structures,\nespecially those of QFTs, in a dynamical framework. Despite the expected\nnon-local nature in the general cases, the majority of explicitly understood\nexamples feature local space-time trajectories under modular flows. In this\nwork, we study a particular class of non-local modular flows. They are\nassociated with the relativistic vacuum state and sub-regions whose boundaries\nlie on a planar null-surface. They satisfy a remarkable algebraic property\nknown as the half-sided modular inclusion, and as a result the modular\nHamiltonians are exactly known in terms of the stress tensor operators. To be\nexplicit, we focus on the simplest QFT of a massive or massless free scalar in\n$2+1$ dimensions. We obtain explicit expressions for the generators. They can\nbe separated into a sum of local and non-local terms showing certain universal\npattern. The preservation of von-Neumann algebra under modular flow works in a\nsubtle way for the non-local terms. We derive a differential-integral equation\nfor the finite modular flow, which can be analyzed in perturbation theory of\nsmall distance deviating from the entanglement boundary, and re-summation can\nbe performed in appropriate limits. Comparison with the general expectation of\nmodular flows in such limits are discussed.",
        "Despite explicit alignment efforts for large language models (LLMs), they can\nstill be exploited to trigger unintended behaviors, a phenomenon known as\n\"jailbreaking.\" Current jailbreak attack methods mainly focus on discrete\nprompt manipulations targeting closed-source LLMs, relying on manually crafted\nprompt templates and persuasion rules. However, as the capabilities of\nopen-source LLMs improve, ensuring their safety becomes increasingly crucial.\nIn such an environment, the accessibility of model parameters and gradient\ninformation by potential attackers exacerbates the severity of jailbreak\nthreats. To address this research gap, we propose a novel\n\\underline{C}ontext-\\underline{C}oherent \\underline{J}ailbreak\n\\underline{A}ttack (CCJA). We define jailbreak attacks as an optimization\nproblem within the embedding space of masked language models. Through\ncombinatorial optimization, we effectively balance the jailbreak attack success\nrate with semantic coherence. Extensive evaluations show that our method not\nonly maintains semantic consistency but also surpasses state-of-the-art\nbaselines in attack effectiveness. Additionally, by integrating semantically\ncoherent jailbreak prompts generated by our method into widely used black-box\nmethodologies, we observe a notable enhancement in their success rates when\ntargeting closed-source commercial LLMs. This highlights the security threat\nposed by open-source LLMs to commercial counterparts. We will open-source our\ncode if the paper is accepted.",
        "Materials improvements are a powerful approach to reducing loss and\ndecoherence in superconducting qubits because such improvements can be readily\ntranslated to large scale processors. Recent work improved transmon coherence\nby utilizing tantalum (Ta) as a base layer and sapphire as a substrate. The\nlosses in these devices are dominated by two-level systems (TLSs) with\ncomparable contributions from both the surface and bulk dielectrics, indicating\nthat both must be tackled to achieve major improvements in the state of the\nart. Here we show that replacing the substrate with high-resistivity silicon\n(Si) dramatically decreases the bulk substrate loss, enabling 2D transmons with\ntime-averaged quality factors (Q) exceeding 1.5 x 10^7, reaching a maximum Q of\n2.5 x 10^7, corresponding to a lifetime (T_1) of up to 1.68 ms. This low loss\nallows us to observe decoherence effects related to the Josephson junction, and\nwe use improved, low-contamination junction deposition to achieve Hahn echo\ncoherence times (T_2E) exceeding T_1. We achieve these material improvements\nwithout any modifications to the qubit architecture, allowing us to readily\nincorporate standard quantum control gates. We demonstrate single qubit gates\nwith 99.994% fidelity. The Ta-on-Si platform comprises a simple material stack\nthat can potentially be fabricated at wafer scale, and therefore can be readily\ntranslated to large-scale quantum processors.",
        "The Dark Energy Spectroscopic Instrument (DESI) data release 2 (DR2) galaxy\nand quasar clustering data represents a significant expansion of data from DR1,\nproviding improved statistical precision in BAO constraints across multiple\ntracers, including bright galaxies (BGS), luminous red galaxies (LRGs),\nemission line galaxies (ELGs), and quasars (QSOs). In this paper, we validate\nthe BAO analysis of DR2. We present the results of robustness tests on the\nblinded DR2 data and, after unblinding, consistency checks on the unblinded DR2\ndata. All results are compared to those obtained from a suite of mock catalogs\nthat replicate the selection and clustering properties of the DR2 sample. We\nconfirm the consistency of DR2 BAO measurements with DR1 while achieving a\nreduction in statistical uncertainties due to the increased survey volume and\ncompleteness. We assess the impact of analysis choices, including different\ndata vectors (correlation function vs. power spectrum), modeling approaches and\nsystematics treatments, and an assumption of the Gaussian likelihood, finding\nthat our BAO constraints are stable across these variations and assumptions\nwith a few minor refinements to the baseline setup of the DR1 BAO analysis. We\nsummarize a series of pre-unblinding tests that confirmed the readiness of our\nanalysis pipeline, the final systematic errors, and the DR2 BAO analysis\nbaseline. The successful completion of these tests led to the unblinding of the\nDR2 BAO measurements, ultimately leading to the DESI DR2 cosmological analysis,\nwith their implications for the expansion history of the Universe and the\nnature of dark energy presented in the DESI key paper.",
        "Designing re-entry vehicles requires accurate predictions of hypersonic flow\naround their geometry. Rapid prediction of such flows can revolutionize vehicle\ndesign, particularly for morphing geometries. We evaluate advanced neural\noperator models such as Deep Operator Networks (DeepONet),\nparameter-conditioned U-Net, Fourier Neural Operator (FNO), and MeshGraphNet,\nwith the objective of addressing the challenge of learning geometry-dependent\nhypersonic flow fields with limited data. Specifically, we compare the\nperformance of these models for two grid types: uniform Cartesian and irregular\ngrids. To train these models, we use 36 unique elliptic geometries for\ngenerating high-fidelity simulations with a high-order entropy-stable DGSEM\nsolver, emphasizing the challenge of working with a scarce dataset. We evaluate\nand compare the four operator-based models for their efficacy in predicting\nhypersonic flow field around the elliptic body. Moreover, we develop a novel\nframework, called Fusion DeepONet, which leverages neural field concepts and\ngeneralizes effectively across varying geometries. Despite the scarcity of\ntraining data, Fusion DeepONet achieves performance comparable to\nparameter-conditioned U-Net on uniform grids while it outperforms MeshGraphNet\nand vanilla DeepONet on irregular, arbitrary grids. Fusion DeepONet requires\nsignificantly fewer trainable parameters as compared to U-Net, MeshGraphNet,\nand FNO, making it computationally efficient. We also analyze the basis\nfunctions of the Fusion DeepONet model using Singular Value Decomposition. This\nanalysis reveals that Fusion DeepONet generalizes effectively to unseen\nsolutions and adapts to varying geometries and grid points, demonstrating its\nrobustness in scenarios with limited training data.",
        "Graph Neural Networks (GNNs) have achieved remarkable success across diverse\ntasks on graph-structured data, primarily through the use of learned weights in\nmessage passing layers. In this paper, we demonstrate that random weights can\nbe surprisingly effective, achieving performance comparable to end-to-end\ntraining counterparts, across various tasks and datasets. Specifically, we show\nthat by replacing learnable weights with random weights, GNNs can retain strong\npredictive power, while significantly reducing training time by up to 6$\\times$\nand memory usage by up to 3$\\times$. Moreover, the random weights combined with\nour construction yield random graph propagation operators, which we show to\nreduce the problem of feature rank collapse in GNNs. These understandings and\nempirical results highlight random weights as a lightweight and efficient\nalternative, offering a compelling perspective on the design and training of\nGNN architectures.",
        "We study a 3+1 active-sterile neutrino mixings model using an $A_4$ triplet\nright-handed neutrino $\\nu_R$ and a singlet eV-scale sterile neutrino under\n$A_4\\times Z_3 \\times Z_2$ discrete symmetry. Four scalar flavons are\nconsidered to reproduce neutrino oscillation parameters within the experimental\n3$\\sigma$ range. The model also studies the effective mass parameter in\nneutrinoless double beta decay experiments. Deviation from $\\mu-\\tau$ symmetry\nin the active neutrino mass matrix is generated through an antisymmetric\ninteraction of $\\nu_R$. This model successfully explains active-sterile\nneutrino mixings consistent with the cosmological upper bound on the sum of\nactive neutrino mass $\\sum m_i < 0.113$ eV (0.145 eV) in NH(IH).",
        "Three directions for the AI avant-garde are sketched against the background\nof time. Posthumanism changes what we are, and belongs to the radical future.\nTranshumanism changes how we are, and corresponds with the radical past.\nGenhumanism changes who we are, and exists in the radical present. While\ndeveloping the concepts, this essay intersects in two ways with theoretical\ndebates about humanism in the face of technological advance. First, it\ndescribes how temporal divisions may cleanly differentiate post- and\ntranshumanism. Second, the essay introduces generative humanism, which\ncontributes to discussions about AI and society by delineating a novel\nhumanistic response to contemporary technology. Finally, grounds are provided\nfor a practical project, one where philosophers work with AI engineers in the\narea of genhumanism. Contemporary AI research into serendipity in\nrecommendation engines provides natural support for the shared research.",
        "The Maximum Mean Discrepancy (MMD) is a kernel-based metric widely used for\nnonparametric tests and estimation. Recently, it has also been studied as an\nobjective function for parametric estimation, as it has been shown to yield\nrobust estimators. We have implemented MMD minimization for parameter inference\nin a wide range of statistical models, including various regression models,\nwithin an R package called regMMD. This paper provides an introduction to the\nregMMD package. We describe the available kernels and optimization procedures,\nas well as the default settings. Detailed applications to simulated and real\ndata are provided.",
        "The Selberg integral, an $n$-dimensional generalization of the Euler beta\nintegral, plays a central role in random matrix theory, Calogero--Sutherland\nquantum many body systems, Knizhnik--Zamolodchikov equations, and multivariable\northogonal polynomial theory. The Selberg integral is known to be equivalent to\nthe Morris constant term identity. In 1998, Baker and Forrester conjectured a\n$(p+1)$-component generalization of the $q$-Morris identity. It in turn yields\na generalization of the Selberg integral. The $p=1$ case of Baker and\nForrester's conjecture was proved by K\\'{a}rolyi, Nagy, Petrov and Volkov in\n2015. In this paper, we give a proof of the $(p+1)$-component\n$q$-Baker--Forrester conjecture, thereby settling this 26-year-old conjecture.",
        "Characterizing eccentricity in gravitational waveforms in a consistent manner\nis crucial to facilitate parameter estimation, astrophysical population\nstudies, as well as searches for these rare systems. We present a framework to\ncharacterize eccentricity directly from gravitational waveforms for\nnon-precessing eccentric binary black hole (BBH) mergers using common\nmodulations that eccentricity induces in all spherical harmonic modes of the\nsignals. Our framework is in the spirit of existing methods that use frequency\nmodulations in the waveforms, but we refine the approach by connecting it to\nstate-of-the-art post-Newtonian calculations of the time evolution of the\neccentricity. Using 39 numerical relativity (NR) simulations from the SXS and\nRIT catalogs, as well as waveforms obtained from the post-Newtonian\napproximation and effective-one-body (EOB) formalism, we show that our\nframework provides eccentricity estimates that connect smoothly into the\nrelativistic regime (even up to $\\sim 2M$ before merger). We also find that it\nis necessary to carry existing post-Newtonian calculations to an extra $0.5$PN\norder to adequately characterize existing NR simulations, and provide fits to\nthe extra coefficient for existing simulations. We make the framework publicly\navailable through the Python-based \\texttt{gwModels} package.",
        "In this paper, we study classes of structures and individual structures for\nwhich programs implementing functions defined everywhere are equivalent to\nfinite tree-programs. The programs under consideration may have cycles and at\nmost countably many nodes. We start with programs in which arbitrary terms of a\ngiven signature may be used in function nodes and arbitrary formulas of this\nsignature may be used in predicate nodes. We then extend our results to\nprograms that are close in nature to computation trees: if such a program is a\nfinite tree-program, then it is an ordinary computation tree.",
        "We consider a class of self-adjoint Sturm-Liouville problems with rational\nfunctions of the spectral parameter in the boundary conditions. The uniform\nstability for direct and inverse spectral problems is proved for the first time\nfor Sturm-Liouville operator pencils with boundary conditions depending on the\neigenparameter. Furthermore, we obtain stability estimates for finite data\napproximations, which are important from the practical viewpoint. Our method is\nbased on Darboux-type transforms and proving of their Lipschitz continuity.",
        "Fast Radio Bursts (FRBs) are millisecond-duration radio transients that serve\nas unique probes of extragalactic matter. We report on the discovery and\nlocalization of two FRBs piercing the Andromeda Galaxy (M31) by the realfast\nfast transient detection system at the Very Large Array. Their unique\nsightlines allow constraints on M31's electron density distribution. We\nlocalized FRB 20230903A to its host galaxy at a redshift $z=0.09$ and FRB\n20230506C to a host galaxy at a redshift $z=0.39$. After accounting for the\ndispersion contribution from the Milky Way, the host galaxy and the\nintergalactic medium along the line of sight of the FRBs, we estimate that M31\nalone will likely contribute between 21-217 $\\mathrm{pc~cm^{-3}}$ along FRB\n20230903A and between 43-338 $\\mathrm{pc~cm^{-3}}$ along FRB 20230506C with a\n90% confidence. We also modeled the M31 disk's contribution to the DM to\ndetermine the halo contribution. We find that the halo of M31 will contribute\nbetween 9-145 $\\mathrm{pc~cm^{-3}}$ along FRB 20230903A and between 28-286\n$\\mathrm{pc~cm^{-3}}$ along FRB 20230506C with 90% confidence. The measured\nvalues of $\\rm DM_{M31,halo}$ are consistent with the predictions from the\nmodified Navarro-Frenk-White profile of M31's halo for a given impact\nparameter. The ions of the cool halo alone cannot account for the calculated\n$\\rm DM_{M31,halo}$ and therefore this measurement presents indirect evidence\nof the hot halo of M31. We also suggest a possible intersection of the line of\nsight of FRB 20230506C with a hot baryon bridge between M31 and the Milky Way",
        "We characterize decomposable principally polarized abelian varieties of the\nform $E\\times B$, with $E$ an elliptic curve, in two different ways. The first\none is by the non-surjectivity of a certain multiplication map of global\nsections, i.e. by the non-vanishing of a certain $0$-th Koszul cohomology\ngroup. The second one is by the non-surjectivity of a certain second order\ngaussian map. Both results are analogous to well known characterizations of\nhyperelliptic curves among all smooth curves of given genus. We also show that,\naccording to previous work of the first author, the second characterization can\nbe seen as an effective version of a theorem of Nakamaye characterizing the\nabove decomposable abelian varieties as those of minimal Seshadri constant.",
        "We prove the existence of compact surfaces with prescribed constant mean\ncurvature in asymptotically flat and asymptotically hyperbolic manifolds. More\nprecisely, let $(M^3,g)$ be an asymptotically flat manifold with scalar\ncurvature $R\\ge 0$. Then, for each constant $c>0$, there exists a compact,\nalmost-embedded, free boundary constant mean curvature surface $\\Sigma \\subset\nM$ with mean curvature $c$. Likewise, let $(M^3,g)$ be an asymptotically\nhyperbolic manifold with scalar curvature $R\\ge -6$. Then, for each constant\n$c>2$, there exists a compact, almost-embedded, free boundary constant mean\ncurvature surface $\\Sigma \\subset M$ with mean curvature $c$. The proof\ncombines min-max theory with the following fact about inverse mean curvature\nflow which is of independent interest: for any $T$ the inverse mean curvature\nflow emerging out of a point $p$ far enough out in an asymptotically flat (or\nasymptotically hyperbolic) end will remain smooth for all times $t\\in\n(-\\infty,T]$.",
        "The Merino-Welsh conjecture states that for a graph $G$ without loops and\nbridges we have $$\\max(T_G(2,0),T_G(0,2))\\geq T_G(1,1).$$ Later Jackson proved\nthat for any matroid $M$ without loop and coloop we have $$T_M(3,0)T_M(0,3)\\geq\nT_M(1,1)^2.$$ The value $3$ in this statement was improved to $2.9242$ by Beke,\nCs\\'aji, Csikv\\'ari and Pituk. In this paper, we further improve on this result\nby showing that $$T_M(2.355,0)T_M(0,2.355)\\geq T_M(1,1)^2.$$ We also prove that\nthe Merino--Welsh conjecture is true for matroids $M$, where all circuits of\n$M$ and its dual $M^*$ have length between $\\ell$ and $(\\ell-2)^4$ for some\n$\\ell\\geq 6$.",
        "Photonics-based in-memory computing systems have demonstrated a significant\nspeedup over traditional transistor-based systems because of their ultra-fast\noperating frequencies and high data bandwidths. Photonic static random access\nmemory (pSRAM) is a crucial component for achieving the objective of ultra-fast\nphotonic in-memory computing systems. In this work, we model and evaluate the\nperformance of a novel photonic SRAM array architecture in development.\nAdditionally, we examine hyperspectral operation through wavelength division\nmultiplexing (WDM) to enhance the throughput of the pSRAM array. We map\nMatricized Tensor Times Khatri-Rao Product (MTTKRP), a computational kernel\ncommonly used in tensor decomposition, to the proposed pSRAM array\narchitecture. We also develop a predictive performance model to estimate the\nsustained performance of different configurations of the pSRAM array. Using the\npredictive performance model, we demonstrate that the pSRAM array achieves 17\nPetaOps while performing MTTKRP in a practical hardware configuration.",
        "High-quality open-source text-to-image models have lowered the threshold for\nobtaining photorealistic images significantly, but also face potential risks of\nmisuse. Specifically, suspects may use synthetic data generated by these\ngenerative models to train models for specific tasks without permission, when\nlacking real data resources especially. Protecting these generative models is\ncrucial for the well-being of their owners. In this work, we propose the first\nmethod to this important yet unresolved issue, called Training data Provenance\nVerification (TrainProVe). The rationale behind TrainProVe is grounded in the\nprinciple of generalization error bound, which suggests that, for two models\nwith the same task, if the distance between their training data distributions\nis smaller, their generalization ability will be closer. We validate the\nefficacy of TrainProVe across four text-to-image models (Stable Diffusion v1.4,\nlatent consistency model, PixArt-$\\alpha$, and Stable Cascade). The results\nshow that TrainProVe achieves a verification accuracy of over 99\\% in\ndetermining the provenance of suspicious model training data, surpassing all\nprevious methods. Code is available at https:\/\/github.com\/xieyc99\/TrainProVe.",
        "We propose a dephasing-tolerant protocol for quantum sensing of transverse\nmagnetic fields which exploits spin qudit sensors with embedded fault-tolerant\n(FT) quantum error correction. By exploiting longitudinal drives, the\ntransverse field induces logical Rabi oscillations between encoded states,\nwhose frequency is linear in the transverse field to be probed. Numerical\nsimulations show that the present FT protocol enables the detection of very\nsmall fields, orders of magnitudes below the limit imposed by the coherence\ntime.",
        "We for the first time construct a three-family ${\\cal N}=1$ supersymmetric\nPati-Salam model from rigid intersecting D6-branes on a factorizable\n$\\mathbb{T}^6\/(\\mathbb{Z}_2\\times \\mathbb{Z}_2')$ orientifold with discrete\ntorsion. We can break the Pati-Salam gauge symmetry down to the Standard Model\n(SM) gauge symmetry via the supersymmetry preserving Higgs mechanism, generate\nthe SM fermion masses and mixings, and break the supersymmetry via gaugino\ncondensations in the hidden sector.",
        "Today, the rapid growth of applications reliant on datacenters calls for new\nadvancements to meet the increasing traffic and computational demands. Traffic\ntraces from datacenters are essential for further development and optimization\nof future datacenters. However, traces are rarely released to the public.\nResearchers often use simplified mathematical models that lack the depth needed\nto recreate intricate traffic patterns and, thus, miss optimization\nopportunities found in realistic traffic. In this preliminary work, we\nintroduce DTG-GPT, a packet-level Datacenter Traffic Generator (DTG), based on\nthe generative pre-trained transformer (GPT) architecture used by many\nstate-of-the-art large language models. We train our model on a small set of\navailable traffic traces from different domains and offer a simple methodology\nto evaluate the fidelity of the generated traces to their original\ncounterparts. We show that DTG-GPT can synthesize novel traces that mimic the\nspatiotemporal patterns found in real traffic traces. We further demonstrate\nthat DTG-GPT can generate traces for networks of different scales while\nmaintaining fidelity. Our findings indicate the potential that, in the future,\nsimilar models to DTG-GPT will allow datacenter operators to release traffic\ninformation to the research community via trained GPT models.",
        "We investigate the $O(N)$--symmetric $\\phi^6$ theory in three spacetime\ndimensions using dimensional regularisation and minimal subtraction. The\npredictions of other methods are scrutinised in a large-$N$ expansion. We show\nhow the tricritical line of fixed point emerges in a strict $N\\to\\infty$ limit\nbut argue that it is not a physical manifestation. For the first time in this\nexplicit manner, we compute the effective potential at next-to-leading order in\nthe $1\/N$-expansion and discuss its stability. The Bardeen-Moshe-Bander\nphenomenon is also analysed at next-to-leading order, and we demonstrate that\nit disappears without breaking the scale invariance spontaneously. Our findings\nindicate that the UV fixed point found by Pisarski persists at large $N$."
      ]
    }
  },
  {
    "id":2412.00036,
    "research_type":"applied",
    "start_id":"b24",
    "start_title":"Quant GANs: deep generation of financial time series",
    "start_abstract":"Modeling financial time series by stochastic processes is a challenging task and a central area of research in financial mathematics. As an alternative, we introduce Quant GANs, a data-driven model which is inspired by the recent success of generative adversarial networks (GANs). Quant GANs consist of a generator and discriminator function, which utilize temporal convolutional networks (TCNs) and thereby achieve to capture long-range dependencies such as the presence of volatility clusters. The generator function is explicitly constructed such that the induced stochastic process allows a transition to its risk-neutral distribution. Our numerical results highlight that distributional properties for small and large lags are in an excellent agreement and dependence properties such as volatility clusters, leverage effects, and serial autocorrelations can be generated by the generator function of Quant GANs, demonstrably in high fidelity.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "On the Distribution of the Two-Sample Cramer-von Mises Criterion"
      ],
      "abstract":[
        "The Cramer-von Mises $\\omega^2$ criterion for testing that a sample, $x_1, \\cdots, x_N$, has been drawn from specified continuous distribution $F(x)$ is \\begin{equation*}\\tag{1}\\omega^2 = \\int^\\infty_{-\\infty} \\lbrack F_N(x) - F(x)\\rbrack^2 dF(x),\\end{equation*} where $F_N(x)$ the empirical function of sample; is, $F_N(x) k\/N$ if exactly $k$ observations are less than or equal to $x(k 0, 1, N)$. If there second $y_1, y_M$, test hypothesis two samples come same (unspecified) can be based on analogue $N\\omega^2$, namely \\begin{equation*}\\tag{2} T NM\/(N + M)\\rbrack G_M(x)\\rbrack^2 dH_{N+M}(x),\\end{equation*} $G_M(x)$ sample and $H_{N+M}(x)$ together [that $(N M)H_{N+M}(x) NF_N(x) MG_M(x)\\rbrack$. limiting $N\\omega^2$ as $N \\rightarrow \\infty$ tabulated [2], it shown ([3], [4a], [7]) $T$ \\infty, M \\infty$, $N\/M \\lambda$, $\\lambda$ any finite positive constant. In this note we consider small values $N$ $M$ present tables permit use at some conventional significance levels $M$. seems surprisingly good approximation exact moderate sizes (corresponding feature [6]). accuracy better in case two-sample Kolmogorov-Smirnov statistic studied by Hodges [4]."
      ],
      "categories":[
        "q-fin.GN"
      ]
    },
    "list":{
      "title":[
        "Centre-of-momentum Variables in $\\nu_\\mu$CC1p1$\\pi$",
        "Is fixed-node diffusion quantum Monte Carlo reproducible?",
        "Optical signatures of noncentrosymmetric structural distortion in\n  altermagnetic MnTe",
        "Fast-response low power atomic oven for integration into an ion\n  microchip",
        "Limits on WIMP dark matter with NaI(Tl) crystals in three years of\n  COSINE-100 data",
        "Building a Software Stack for Quantum-HPC Integration",
        "Eightfold Degenerate Dirac Nodal Line in Collinear Antiferromagnet\n  Mn$_5$Si$_3$",
        "A Full AGN Feedback Prescription for Numerical Models: Negative,\n  Positive and Hot Gas-Ejection Mode",
        "All Order Classical Electromagnetic Soft Theorems",
        "Combined P-value Functions for Compatible Effect Estimation and\n  Hypothesis Testing in Drug Regulation",
        "Channel Estimation for Pinching-Antenna Systems (PASS)",
        "Rainbow Tur\\'an numbers for short brooms",
        "Influence of departures from LTE on determinations of the sulfur\n  abundances in A-K type stars",
        "Heat transport model for the transition between scaling regimes in\n  quasistatic and full magnetoconvection",
        "Structure and Skewness of the Effective Inspiral Spin Distribution of\n  Binary Black Hole Mergers",
        "Critical String Theory in a $D=4$ Robertson-Walker Background and the\n  Large Scale Structure of Spacetime",
        "Feldman-Cousins' ML Cousin: Sterile Neutrino Global Fits using\n  Simulation-Based Inference",
        "Complements of the point schemes of noncommutative projective lines",
        "Heat kernel estimates for Schr\\\"odinger operators with supercritical\n  killing potentials",
        "Depth of extensions of valuations",
        "Data-Driven Non-Parametric Model Learning and Adaptive Control of MDPs\n  with Borel spaces: Identifiability and Near Optimal Design",
        "Fast M{\\o}lmer-S{\\o}rensen gates in trapped-ion quantum processors with\n  compensated carrier transition",
        "The basic locus of ramified unitary Rapoport-Zink space at maximal\n  vertex level",
        "A fourth-order cut-cell method for solving the two-dimensional\n  advection-diffusion equation with moving boundaries",
        "Spin glass behavior in amorphous CrSiTe3 alloy",
        "Transport in integrable and perturbed easy-axis Heisenberg chain:\n  Thouless approach",
        "Constraining the Nuclear Equation of State of neutron star via\n  high-frequency quasi-periodic oscillation in short gamma-ray bursts",
        "Neutral particle analyzer for plasma diagnostics on tokamak ST40",
        "Aspects of the dilute Glasma"
      ],
      "abstract":[
        "This study introduces a novel set of variables, namely the centre-of-momentum\nvariables, $\\theta_{\\textrm{COM}}$ and $E_{\\textrm{COM}}$, designed to isolate\nfinal-state interactions (FSI) from other aspects of neutrino-nucleus\ninteractions. Through detailed simulation studies, this work demonstrates the\nability of these variables to distinguish FSI contributions with minimal\ndependence on the nuclear initial state and, practically, on the neutrino flux,\nhighlighting their potential for advancing FSI modelling. With high-purity\nneutrino-hydrogen interaction selections, $\\theta_{\\textrm{COM}}$ offers the\nfirst opportunity for a direct cross-comparison among different neutrino\ncross-section experiments.",
        "Fixed-node diffusion quantum Monte Carlo (FN-DMC) is a widely-trusted\nmany-body method for solving the Schr\\\"{o}dinger equation, known for its\nreliable predictions of material and molecular properties. Furthermore, its\nexcellent scalability with system complexity and near-perfect utilization of\ncomputational power makes FN-DMC ideally positioned to leverage new advances in\ncomputing to address increasingly complex scientific problems. Even though the\nmethod is widely used as a computational gold standard, reproducibility across\nthe numerous FN-DMC code implementations has yet to be demonstrated. This\ndifficulty stems from the diverse array of DMC algorithms and trial wave\nfunctions, compounded by the method's inherent stochastic nature. This study\nrepresents a community-wide effort to address the titular question, affirming\nthat: Yes, FN-DMC is reproducible (when handled with care). Using the\nwater-methane dimer as the canonical test case, we compare results from eleven\ndifferent FN-DMC codes and show that the approximations to treat the\nnon-locality of pseudopotentials are the primary source of the discrepancies\nbetween them. In particular, we demonstrate that, for the same choice of\ndeterminantal component in the trial wave function, reliable and reproducible\npredictions can be achieved by employing the T-move (TM), the determinant\nlocality approximation (DLA), or the determinant T-move (DTM) schemes, while\nthe older locality approximation (LA) leads to considerable variability in\nresults. This work lays the foundation to establish accurate and reproducible\nFN-DMC estimates for all future studies across applications in materials\nscience, physics, chemistry, and biology.",
        "The hexagonal MnTe is a prime material candidate for altermagnets, an\nemerging class of magnetic compounds characterized by the nontrivial interplay\nof antiparallel spin arrangements with their underlying crystal structures.\nRecognizing precise knowledge of crystal symmetry as the cornerstone of the\nspin-group classification scheme, we report here a native\ninversion-symmetry-breaking structural distortion in this compound that has\npreviously been overlooked. Through optical polarimetry experiments and\nfirst-principle calculations, we show that MnTe belongs to the\nnoncentrosymmetric $D_{3h}$ group, effectively resolving key inconsistencies in\nthe earlier interpretations of Raman spectroscopy data. Our finding impacts the\nsymmetry analysis of MnTe within the altermagnetic class and sheds light on the\nmechanism of its magneto-controllable N\\'eel order.",
        "We present a novel microfabricated neutral atom source for quantum\ntechnologies that can be easily integrated onto microchip devices using\nwell-established MEMS fabrication techniques, and contrast this to conventional\noff-chip ion loading mechanisms. The heating filament of the device is shown to\nbe as small as 90$\\times$90 $\\mu$m$^2$. Testing of the $^{171}$Yb fluorescence\nresponse is found to be in the low tens of milliseconds, two orders of\nmagnitude faster compared to previous literature at a power of milliwatts\nmaking it desirable for low-power device packages. We demonstrate how the\nevaporation material can be capped in vacuum to work with materials such as Ba\nthat oxidise easily in air, which can avoid the need for ablation lasers in the\nloading process. We calculate oven lifetimes to be over 10 years of continuous\nuse for commonly used ion species in quantum technology.",
        "We report limits on WIMP dark matter derived from three years of data\ncollected by the COSINE-100 experiment with NaI(Tl) crystals, achieving an\nimproved energy threshold of 0.7 keV. This lowered threshold enhances\nsensitivity in the sub-GeV mass range, extending the reach for direct detection\nof low-mass dark matter. Although no excess of WIMP-like events was observed,\nthe increased sensitivity enabled a model-independent comparison between the\nexpected WIMP signal rate-based on mass limits from our data-and DAMA's\nreported modulation amplitude. Our findings strongly disfavor the DAMA signal\nas originating from WIMP interactions, fully excluding DAMA\/LIBRA 3$\\sigma$\nallowed regions and providing enhanced WIMP mass limits by an order of\nmagnitude in the spin-independent model compared to previous results. In the\nspin-dependent model, cross-section upper limits were obtained in the mass\nrange [0.1-5.0] GeV\/c$^2$, with additional sensitivity to sub-GeV WIMPs through\nthe inclusion of the Migdal effect. These results represent substantial\nprogress in low-mass dark matter exploration and reinforce constraints on the\nlongstanding DAMA claim.",
        "This paper presents a comprehensive software stack architecture for\nintegrating quantum computing (QC) capabilities with High-Performance Computing\n(HPC) environments. While quantum computers show promise as specialized\naccelerators for scientific computing, their effective integration with\nclassical HPC systems presents significant technical challenges. We propose a\nhardware-agnostic software framework that supports both current noisy\nintermediate-scale quantum devices and future fault-tolerant quantum computers,\nwhile maintaining compatibility with existing HPC workflows. The architecture\nincludes a quantum gateway interface, standardized APIs for resource\nmanagement, and robust scheduling mechanisms to handle both simultaneous and\ninterleaved quantum-classical workloads. Key innovations include: (1) a unified\nresource management system that efficiently coordinates quantum and classical\nresources, (2) a flexible quantum programming interface that abstracts\nhardware-specific details, (3) A Quantum Platform Manager API that simplifies\nthe integration of various quantum hardware systems, and (4) a comprehensive\ntool chain for quantum circuit optimization and execution. We demonstrate our\narchitecture through implementation of quantum-classical algorithms, including\nthe variational quantum linear solver, showcasing the framework's ability to\nhandle complex hybrid workflows while maximizing resource utilization. This\nwork provides a foundational blueprint for integrating QC capabilities into\nexisting HPC infrastructures, addressing critical challenges in resource\nmanagement, job scheduling, and efficient data movement between classical and\nquantum resources.",
        "We study the electronic, magnetic, and spin transport properties of the\northorhombic Mn$_{5}$Si$_{3}$ compound in the $AF2$ phase using symmetry\nanalysis and ab-initio calculations. Our ground state energy calculations align\nwith experimental observations, demonstrating that the collinear\nantiferromagnetic (AFM) order, with N\\'{e}el vector in the [010] direction, is\nthe most stable magnetic configuration both with and without spin-orbit\ncoupling (SOC) in a bulk lattice geometry. We identified an unconventional\neight-fold degenerate Dirac nodal line (DNL) close to the Fermi level,\ncharacterized by negligible SOC. This DNL is robustly protected by a unique\ncombination of a pure-spin symmetry and a lattice symmetry together with\nmagnetic space group symmetries. Upon introducing SOC, this degeneracy is\nreduced to two four-fold DNLs, being protected by the combination of\ntime-reversal, partial translation and nonsymmorphic symmetries within the\nmagnetic space group. We predict also a large intrinsic spin Hall conductivity\n(SHC) which correlates with the presence of SOC-induced splitting of these\neight-fold degenerate DNLs near the Fermi level. These intriguing\ncharacteristics position collinear antiferromagnet Mn$_{5}$Si$_{3}$ as a\ncompelling candidate for spintronic applications, particularly in the\ngeneration and detection of spin currents, while remaining compatible with\nmodern silicon technology.",
        "We build upon the state-of-the-art semi-analytic model \\texttt{FEGA24}\n(Formation and Evolution of GAlaxies, \\citealt{contini2024d}), which integrates\nthe latest prescriptions relevant to galaxy formation and evolution, alongside\na comprehensive AGN feedback model. This model incorporates three modes of\nfeedback: negative (preventing excessive cooling), positive (enhancing star\nformation), and hot gas ejection (expelling gas beyond the virial radius of\nhalos). These modes operate in a coordinated manner: the negative mode\nregulates the cooling process, the positive mode promotes bursts of star\nformation, and the hot gas ejection mode expels gas beyond the virial radius\nwhen the AGN is sufficiently powerful. Our updated semi-analytic model,\n\\texttt{FEGA25}, retains the qualitative and quantitative consistency of the\nanalyses presented in \\cite{contini2024d}, while delivering more robust\nresults. Notably, \\texttt{FEGA25} provides a more detailed characterization of\nthe fraction of red galaxies as a function of stellar mass, predicts a main\nsequence of star-forming galaxies more consistent with observations, and\nestimates the fraction of hot gas in halos closer to observed values. These\nfindings underscore the importance of a physical mechanism capable of ejecting\nhot gas beyond the virialized region of dark matter halos without significantly\naltering the stellar and cold gas components. Such a mechanism is crucial to\nensure the proper functioning of other processes, such as cooling and star\nformation. Since supernova feedback is already modeled at its maximum\nefficiency, AGN feedback emerges as the natural candidate for this role.",
        "If a set of charged objects collide in space and the fragments disperse, then\nthis process will emit electromagnetic waves. Classical soft photon theorem\ndetermines the constant term and the leading power law fall-off of the\nwave-form at late and early times in terms of only the momenta and charges of\nthe incoming and outgoing objects. In this paper we determine an infinite set\nof subleading terms in the late and early time expansion of the wave-form,\nwhich also depend only on the momenta and charges of the incoming and outgoing\nparticles. For two-particle scattering, we derive a resummed low-frequency\nelectromagnetic wave-form, as well as the resummed wave-form at early and late\ntimes. In this analysis we ignore the effect of long range gravitational\ninteraction, but our result is unaffected by any other short range interactions\namong the objects.",
        "The two-trials rule in drug regulation requires statistically significant\nresults from two pivotal trials to demonstrate efficacy. However, it is unclear\nhow the effect estimates from both trials should be combined to quantify the\ndrug effect. Fixed-effect meta-analysis is commonly used but may yield\nconfidence intervals that exclude the value of no effect even when the\ntwo-trials rule is not fulfilled. We systematically address this by recasting\nthe two-trials rule and meta-analysis in a unified framework of combined\np-value functions, where they are variants of Wilkinson's and Stouffer's\ncombination methods, respectively. This allows us to obtain compatible combined\np-values, effect estimates, and confidence intervals, which we derive in\nclosed-form. Additionally, we provide new results for Edgington's, Fisher's,\nPearson's, and Tippett's p-value combination methods. When both trials have the\nsame true effect, all methods can consistently estimate it, although some show\nbias. When true effects differ, the two-trials rule and Pearson's method are\nconservative (converging to the less extreme effect), Fisher's and Tippett's\nmethods are anti-conservative (converging to the more extreme effect), and\nEdgington's method and meta-analysis are balanced (converging to a weighted\naverage). Notably, Edgington's confidence intervals asymptotically always\ninclude individual trial effects, while meta-analytic confidence intervals\nshrink to a point at the weighted average effect. We conclude that all of these\nmethods may be appropriate depending on the estimand of interest. We implement\ncombined p-value function inference for two trials in the R package twotrials,\nallowing researchers to easily perform compatible hypothesis testing and\nparameter estimation.",
        "Pinching Antennas (PAs) represent a revolutionary flexible antenna technology\nthat leverages dielectric waveguides and electromagnetic coupling to mitigate\nlarge-scale path loss. This letter is the first to explore channel estimation\nfor Pinching-Antenna SyStems (PASS), addressing their uniquely ill-conditioned\nand underdetermined channel characteristics. In particular, two efficient deep\nlearning-based channel estimators are proposed. 1) PAMoE: This estimator\nincorporates dynamic padding, feature embedding, fusion, and mixture of experts\n(MoE) modules, which effectively leverage the positional information of PAs and\nexploit expert diversity. 2) PAformer: This Transformer-style estimator employs\nthe self-attention mechanism to predict channel coefficients in a per-antenna\nmanner, which offers more flexibility to adaptively deal with dynamic numbers\nof PAs in practical deployment. Numerical results demonstrate that 1) the\nproposed deep learning-based channel estimators outperform conventional methods\nand exhibit excellent zero-shot learning capabilities, and 2) PAMoE delivers\nhigher channel estimation accuracy via MoE specialization, while PAformer\nnatively handles an arbitrary number of PAs, trading self-attention complexity\nfor superior scalability.",
        "A graph $G$ is rainbow-$F$-free if it admits a proper edge-coloring without a\nrainbow copy of $F$. The rainbow Tur\\'an number of $F$, denoted\n$\\mathrm{ex^*}(n,F)$, is the maximum number of edges in a rainbow-$F$-free\ngraph on $n$ vertices. We determine bounds on the rainbow Tur\\'an numbers of\nstars with a single edge subdivided twice; we call such a tree with $t$ total\nedges a $t$-edge \\textit{broom} with length-$3$ handle, denoted by $B_{t,3}$.\nWe improve the best known upper bounds on $\\mathrm{ex^*}(n,B_{t,3})$ in all\ncases where $t \\neq 2^s - 2$. Moreover, in the case where $t$ is odd and in a\nfew cases when $t \\equiv 0 \\mod 4$, we provide constructions asymptotically\nachieving these upper bounds. Our results also demonstrate a dependence of\n$\\mathrm{ex^*}(n,B_{t,3})$ on divisibility properties of $t$.",
        "The influence of departures from local thermodynamic equilibrium (LTE) on\nneutral sulfur lines is considered. A grid of corrections is proposed to take\ninto account the influence of departures from LTE for neutral sulfur lines in\nthe visible and infrared spectral regions, including the H-band. The grid is\ncalculated using the atomic model of sulfur incorporating the most up-to-date\ncollision rates with electrons and hydrogen. The inclusion of levels and\ntransitions of ionized sulfur in the atomic model made it possible to expand\nthe range of effective temperatures of stellar photospheres in the grid up to\n10000 K. The atomic model was tested in determining the sulfur abundance of 13\nstars and showed its adequacy in a wide range of fundamental stellar\nparameters. In the spectra of all test stars, the sulfur lines are fitted with\nsimilar abundances of the element, regardless of the degree of influence of the\neffects of deviation from LTE on a particular spectral line. For lines of\nseveral multiplets, the wavelengths and oscillator strengths were refined. A\nlist of S I lines recommended for determining sulfur abundance has been\ncreated.",
        "In magnetoconvection, the flow is governed by the interplay between\ngravitational buoyancy and the Lorentz force, with one of these forces\ndominating in different regimes. In this paper, we develop a model with a\nsingle adjustable parameter that accurately captures the smooth transition from\na buoyancy-dominated regime to one dominated by the Lorentz force. A\nperturbative extension of the model accounts for distinct transition features\nthat occur at high Prandtl numbers. We validate the model for magnetoconvection\nin both the quasistatic regime and at finite magnetic Reynolds numbers using\ndata from direct numerical simulations and existing experimental data sets. The\nmodel contains a natural extension to rotating convection and offers a\npotential generalisation to rotating magnetoconvection.",
        "The detection of gravitational waves has brought to light a population of\nbinary black holes that merge within a Hubble time. Multiple formation channels\ncan contribute to this population, making it difficult to definitively\nassociate particular population features with underlying stellar physics. Black\nhole spins are considered an important discriminator between various channels,\nbut they are less well-measured than masses, making conclusive astrophysical\nstatements using spins difficult thus far. In this paper, we consider the\ndistribution of the effective inspiral spin $\\chi_{\\rm eff}$ -- a quantity much\nbetter measured than individual component spins. We show that non-Gaussian\nfeatures like skewness, asymmetry about zero, and multimodality can naturally\narise in the $\\chi_{\\rm eff}$ distribution when multiple channels contribute to\nthe population. Searching for such features, we find signs of skewness and\nasymmetry already in the current catalogs, but no statistically significant\nsigns of bimodality. These features provide robust evidence for the presence of\na subpopulation with spins preferentially aligned to the binary's orbital\nangular momentum; and we conservatively estimate the fraction of this\nsubpopulation to be at least $12 \\% - 17\\%$ (at $90\\%$ credibility). Our models\ndo not find an excess of non-spinning systems and instead find that at least\n$\\sim 20 \\%$ of the binaries have some degree of negative $\\chi_{\\rm eff}$. The\ndata also suggest that, if preferentially aligned mergers form a significant\nfraction of the population, they must have small spins.",
        "We show that $4$-dimensional Robertson-Walker spacetimes can be constructed\nfor which all of the beta functions vanish to leading order, yielding\nconsistent string theory without extra dimensions. We find that there is a\nunique static solution, which we refer to as an anti-Einstein static universe.\nThe associated stress energy tensor can be interpreted as a perfect fluid with\na negative energy density. Interestingly, a fluid with a negative energy\ndensity was proposed in [2] as an ad hoc hypothesis to serve as a possible\nexplanation for dark energy and dark matter. Here, such a fluid spontaneously\nappears by trying to fit string theory into only $4$ dimensions. We can look at\nperturbations away from the anti-Einstein static universe. This has to be done\nnumerically and we only do it for a few choices of initial conditions. We find\nthat these solutions are very sensitive to the initial conditions and yield a\nvariety of behaviors. We hope that this behavior is rich enough to match\ncosmological observations by appropriately choosing the initial conditions. One\nof these solutions that we found is particularly interesting. It has a hubble\nparameter which is negative in the past (meaning a contracting universe), then\na point at which the hubble parameter changes sign, so the universe starts\nexpanding which could be identified with a big bang, after which the hubble\nparameter continues growing. Throughout all of this time, the acceleration is\npositive since the Hubble parameter is increasing. Finally, we comment that our\nresult does not contradict [1] as it seems that the authors overlooked the\npossibility of a purely complex axion field which allows for the\nRobertson-Walker metrics to make a contribution $c_{RW}\\geq 4$ to the central\ncharge. Thus, we can interpret dark matter and dark energy as being parts of a\nmechanism needed to keep $4$-dimensional string theory consistent.",
        "For many small-signal particle physics analyses, Wilks' theorem, a\nsimplifying assumption that presumes log-likelihood asymptotic normality, does\nnot hold. The most common alternative approach applied in particle physics is a\nhighly computationally expensive procedure put forward by Feldman and Cousins.\nWhen many experiments are combined for a global fit to data, deviations from\nWilks' theorem are exacerbated, and Feldman-Cousins becomes computationally\nintractable. We present a novel, machine learning-based procedure that can\napproximate a full-fledged Bayesian analysis 200 times faster than the\nFeldman-Cousins method. We demonstrate the utility of this novel method by\nperforming a joint analysis of electron neutrino\/antineutrino disappearance\ndata within a single sterile neutrino oscillation framework. Although we\npresent a prototypical simulation-based inference method for a sterile neutrino\nglobal fit, we anticipate that similar procedures will be useful for global\nfits of all kinds, especially those in which Feldman-Cousins is too\ncomputationally expensive to use.",
        "Recently, Chan and Nyman constructed noncommutative projective lines via a\nnoncommutative symmetric algebra for a bimodule $V$ over a pair of fields.\nThese noncommutative projective lines of contain a canonical closed subscheme\n(the point scheme) determined by a normal family of elements in the\nnoncommutative symmetric algebra. We study the complement of this subscheme\nwhen $V$ is simple, the coordinate ring of which is obtained by inverting said\nnormal family. We show that this localised ring is a noncommutative Dedekind\ndomain of Gelfand-Kirillov dimension 1. Furthermore, the question of simplicity\nof these Dedekind domains is answered by a similar dichotomy to an analogous\nopen subscheme of the noncommutative quadrics of Artin, Tate and Van den Bergh.",
        "In this paper, we study the Schr\\\"odinger operator $\\Delta-V$, where $V$ is a\nsupercritical non-negative potential belonging to a large class of functions\ncontaining functions of the form $b|x|^{-(2+2\\beta)}$, $b, \\beta>0$. We obtain\ntwo-sided estimates on the heat kernel $p(t, x, y)$ of $\\Delta-V$, along with\nestimates for the corresponding Green function. Unlike the case of the\nfractional Schr\\\"odinger operator $-(-\\Delta)^{\\alpha\/2}-V$, $\\alpha\\in (0,\n2)$, with supercritical killing potential dealt with in [11], in the present\ncase, the heat kernel $p(t, x, y)$ decays to 0 exponentially as $x$ or $y$\ntends to the origin.",
        "In this paper we develop the theory of the depth of a simple algebraic\nextension of valued fields $(L\/K,v)$. This is defined as the minimal number of\naugmentations appearing in some Mac Lane-Vaqui\\'e chain for the valuation on\n$K[x]$ determined by the choice of some generator of the extension. In the\ndefectless and unibranched case, this concept leads to a generalization of a\nclassical result of Ore about the existence of $p$-regular generators for\nnumber fields. Also, we find what valuation-theoretic conditions characterize\nthe extensions having depth one.",
        "We consider an average cost stochastic control problem with standard Borel\nspaces and an unknown transition kernel. We do not assume a parametric\nstructure on the unknown kernel. We present topologies on kernels which lead to\ntheir identifiability and ensures near optimality through robustness to\nidentifiability errors. Following this, we present two data-driven\nidentifiability results; the first one being Bayesian (with finite time\nconvergence guarantees) and the second one empirical (with asymptotic\nconvergence guarantees). The identifiability results are, then, used to design\nnear-optimal adaptive control policies which alternate between periods of\nexploration, where the agent attempts to learn the true kernel, and periods of\nexploitation where the knowledge accrued throughout the exploration phase is\nused to minimize costs. We will establish that such policies are near optimal.\nMoreover, we will show that near optimality can also be achieved through\npolicies that simultaneously explore and exploit the environment. Thus, our\nprimary contributions are to present very general conditions ensuring\nidentifiability, as well as adaptive learning and control which leads to\noptimality. Key tools facilitating our analysis are a general measurability\ntheorem, robustness to model learning, and continuity of expected average cost\nin stationary policies under Young topology.",
        "Carrier transition is one of the major factors hindering the high-speed\nimplementation of M{\\o}lmer-S{\\o}rensen gates in trapped-ion quantum\nprocessors. We present an approach to design laser pulse shapes for\nM{\\o}lmer-S{\\o}rensen gate in ion chains which accounts for the effect of\ncarrier transition on qubit-phonon dynamics. We show that the fast-oscillating\ncarrier term effectively modifies the spin-dependent forces acting on ions, and\nthis can be compensated by a simple nonlinear transformation of a laser pulse.\nUsing numerical simulations for short ion chains and perturbation theory for\nlonger chains, we demonstrate that our approach allows to reach the infidelity\nbelow $10^{-4}$ while keeping the gate duration below 100 $\\mu$s.",
        "We construct the Bruhat-Tits stratification of the ramified unitary\nRapoport-Zink space, with the level being the stabilizer of a vertex lattice.\nWe develop the local model theory for Bruhat-Tits strata, proving their\nnormality and Cohen-Macaulayness, and provide precise dimension formulas.\nAdditionally, we establish an explicit isomorphism between Bruhat-Tits strata\nand Deligne-Lusztig varieties, revealing new phenomena beyond the previously\nstudied Coxeter-type cases.",
        "We propose a fourth-order cut-cell method for solving the two-dimensional\nadvection-diffusion equation with moving boundaries on a Cartesian grid. We\nemploy the ARMS technique to give an explicit and accurate representation of\nmoving boundaries, and introduce a cell-merging technique to overcome\ndiscontinuities caused by topological changes in cut cells and the small cell\nproblem. We use a polynomial interpolation technique base on poised lattice\ngeneration to achieve fourth-order spatial discretization, and use a\nfourth-order implicit-explicit Runge-Kutta scheme for time integration.\nNumerical tests are performed on various moving regions, with advection\nvelocity both matching and differing from boundary velocity, which demonstrate\nthe fourth-order accuracy of the proposed method.",
        "Owing to the intrinsically high crystallization temperatures, layered\nphase-change materials, such as CrGeTe3 and InGeTe3, are attracting attention\nfor embedded memory applications, In addition to the electrical contrast, a\nmajor change in magnetic properties is observed in CrGeTe3 upon switching from\nthe crystalline to the amorphous state. In this work, we report a combined ab\ninitio modeling and magnetic characterization study on the isostructural\nsilicon parent compound of CrGeTe3, namely, CrSiTe3. Amorphous CrSiTe3 has\nsimilar structural properties to amorphous CrGeTe3; however, it shows a smaller\nenergy difference between the ferromagnetic configuration and the random\nmagnetic configuration, indicating a high probability of spin glass formation.\nIndeed, direct-current and alternating-current magnetic measurements show that\nthe coercive force of amorphous CrSiTe3 is higher than that of amorphous\nCrGeTe3. Therefore, the pinning effect of spins is enhanced in amorphous\nCrSiTe3, leading to a more robust spin glass state with a higher freezing\ntemperature. The large magnetic contrast between the amorphous and crystalline\nphase could make CrSiTe3 a potential candidate for phase-change magnetic\nswitching applications.",
        "We study transport in the spin chains by employing the Thouless approach\nbased on the level sensitivity to the boundary conditions, $R$. Although spin\ntransport in the integrable easy-axis XXZ model is diffusive, corresponding $R$\nis much closer to ballistic chains than to chaotic diffusive systems. In the\ncase of the grand canonical ensemble this observation can be rigorously\njustified, while in the case of the canonical ensemble it can be demonstrated\nby numerical calculations. Integrability breaking perturbation (IBP) strongly\nreduces $R$ which reveals a pronounced minimum at the crossover from anomalous\ndiffusive to normal dissipative transport. This minimum coincides with the\nonset of the universality of the random matrix theory. Results for various IBP\nsuggest a discontinuous jump of the spin conductivity in the thermodynamic\nlimit, and moreover that its value universally decreases when the strength of\nIBP decreases.",
        "The determination of the equation of state (EOS) of a neutron star (NS) and\nits maximum mass is very important for understanding the formation and\nproperties of NSs under extreme conditions, but they remain open questions.\nShort-duration gamma-ray bursts (GRBs) are believed to originate from the\nmerger of binary NSs or giant flares (GFs) of soft gamma repeaters (SGRs).\nRecently, the high-frequency quasi-periodic oscillations (QPOs) have been\nclaimed to be identified from two short GRBs (GRB 931101B and GRB 910711). In\nthis paper, we propose that the observed high-frequency QPOs in these two short\nGRBs result from torsional oscillations in the GFs of SGRs associated with cold\nNSs, or from radial oscillations of hypermassive NSs as the hot remnants of\nbinary NS mergers, and then to constrain the EOS of NSs. For torsional\noscillations, the six selected EOSs (TM1, NL3, APR, SLy4, DDME2, and GM1) of\nNSs suitable for the zero-temperature condition exhibit significant overlap in\nmass ranges, suggesting that we cannot constrain the EOS of NSs. For radial\noscillations, the six selected EOSs (IUF, TM1, TMA, FSG, BHBLp, and NL3) of NSs\nsuitable for the high-temperature condition cannot be ruled out when redshift\nis considered. However, it is found that the EOS can only be constrained if the\nredshift and temperature of the remnant can be measured.",
        "The paper presents a detailed description of a neutral particle analyzer\ndesigned and produced for plasma diagnostics on the spherical tokamak ST40. The\naim of the diagnostic is to measure both the bulk ion temperature in the range\nfrom 0.5 to 10 keV and the energy distribution of fast ions with energies up to\n40 keV, which appear in the plasma through neutral beam injection. A feature of\nthe analyzer is its ability to separate hydrogen isotopes (protium and\ndeuterium) and measure the ion distribution function of the selected isotope.",
        "The Glasma is a semiclassical nonequilibrium state describing the earliest\nstage in relativistic heavy-ion collisions predicted by the Color Glass\nCondensate effective theory. It is characterized by strong color fields, which\nare sourced by color currents pertaining to hard partons in the colliding\nnuclei. We introduce the (3+1)D dilute Glasma framework, which incorporates the\nlongitudinal and transverse structure of colliding particles and describes the\nrapidity-dependence of observables like the energy-momentum tensor. This is in\nstark contrast to the canonical picture of boost-invariance, where nuclei are\ninfinitesimally thin in longitudinal direction, and the rapidity-dependence of\nobservables is lost. We discuss the derivation of the (3+1)D dilute Glasma\nfield-strength tensor, which relies on linearizing the Yang-Mills equations in\nthe dilute approximation, i.e., assuming weak sources. The dilute Glasma\nenergy-momentum tensor can efficiently be evaluated numerically on a lattice.\nEmploying a generalized 3D McLerran-Venugopalan model, we discuss numerical\nresults for the collisions of heavy ions at energies corresponding to\nexperiments at RHIC and the LHC. We discover longitudinal flow that differs\nsignificantly from Bjorken flow and argue that this is a consequence of taking\ninto account the longitudinal extension of nuclei. Furthermore, we find\nlimiting fragmentation as a universal feature of the dilute Glasma analytically\nand numerically. Finally, we study the applicability of the dilute Glasma to\nproton-proton collisions and show the necessary modifications to reproduce\nexperimental multiplicity distributions."
      ]
    }
  },
  {
    "id":2411.0064,
    "research_type":"applied",
    "start_id":"b14",
    "start_title":"Quantifying Variance in Evaluation Benchmarks",
    "start_abstract":"Evaluation benchmarks are the cornerstone of measuring capabilities large language models (LLMs), as well driving progress in said capabilities. Originally designed to make claims about (or lack thereof) fully pretrained models, evaluation now also extensively used decide between various training choices. Despite this widespread usage, we rarely quantify variance our benchmarks, which dictates whether differences performance meaningful. Here, define and measure a range metrics geared towards including seed across initialisations, monotonicity during training. By studying number -- both openly available from scratch provide empirical estimates for variety metrics, with considerations recommendations practitioners. We evaluate utility tradeoffs continuous versus discrete measures explore options better understanding reducing variance. find that simple changes, such framing choice tasks (like MMLU) completion tasks, can often reduce smaller scale ($\\sim$7B) while more involved methods inspired human testing literature (such item analysis response theory) struggle meaningfully Overall, work provides insights into suggests LM-specific techniques variance, generally encourages practitioners carefully factor when comparing models.",
    "start_categories":[
      "stat.ME"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b6"
      ],
      "title":[
        "The Llama 3 Herd of Models"
      ],
      "abstract":[
        "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "A Supersymmetric $w_{1+\\infty}$ Symmetry, the Extended Supergravity and\n  the Celestial Holography",
        "Generalized Optimal AMG Convergence Theory for Stokes Equations Using\n  Smooth Aggregation and Vanka Relaxation Strategies",
        "CIBPU: A Conflict-Invisible Secure Branch Prediction Unit",
        "Production, Characteristics and Biological effects of Protonated Small\n  Water Clusters",
        "NitiBench: A Comprehensive Study of LLM Framework Capabilities for Thai\n  Legal Question Answering",
        "Detecting high-dimensional time-bin entanglement in fiber-loop systems",
        "Statistical Collusion by Collectives on Learning Platforms",
        "Accelerated Preference Elicitation with LLM-Based Proxies",
        "A car-following model with behavioural adaptation to road geometry",
        "Increasing the Energy-Efficiency of Wearables Using Low-Precision Posit\n  Arithmetic with PHEE",
        "Trajectory Prediction for Autonomous Driving: Progress, Limitations, and\n  Future Directions",
        "Two simple photon gauges in inflation",
        "Dress-1-to-3: Single Image to Simulation-Ready 3D Outfit with Diffusion\n  Prior and Differentiable Physics",
        "Split Adaptation for Pre-trained Vision Transformers",
        "A Detailed Analysis of Close Binary OCs",
        "An Energy-Aware RIoT System: Analysis, Modeling and Prediction in the\n  SUPERIOT Framework",
        "VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction",
        "Nuclear modification of $B_c$ mesons in relativistic heavy-ion\n  collisions based on a linear Boltzmann transport model",
        "Resilient UAV Trajectory Planning via Few-Shot Meta-Offline\n  Reinforcement Learning",
        "Metis: A Foundation Speech Generation Model with Masked Generative\n  Pre-training",
        "Contextual Speech Extraction: Leveraging Textual History as an Implicit\n  Cue for Target Speech Extraction",
        "Observational Constraints on Dark Energy Models with $\\Lambda$ as an\n  Equilibrium Point",
        "SMT-Boosted Security Types for Low-Level MPC",
        "Sub-Power Law Decay of the Wave Packet Maximum in Disordered Anharmonic\n  Chains",
        "Secure On-Device Video OOD Detection Without Backpropagation",
        "Topological Operations Around Exceptional Points via Shortcuts to\n  Adiabaticity",
        "Bulk superconductivity in pressurized trilayer nickelate Pr4Ni3O10\n  single crystals",
        "Anomalous Reynolds stress and dynamic mechanisms in two-dimensional\n  elasto-inertial turbulence of viscoelastic channel flow",
        "Robust COVID-19 Detection from Cough Sounds using Deep Neural Decision\n  Tree and Forest: A Comprehensive Cross-Datasets Evaluation"
      ],
      "abstract":[
        "We determine the ${\\cal N}=4$ supersymmetric\n$W_{1+\\infty}^{2,2}[\\lambda=\\frac{1}{4}]$ algebra which is an extension of\n${\\cal N}=4$ $SO(4)$ superconformal algebra with vanishing central charge. We\nidentify the soft current algebra between the graviton, the gravitinos, the\nvectors, the Majorana fermions, the scalar or the pseudoscalar, equivalent to\n${\\cal N}=4$ supersymmetric $w_{1+\\infty}^{2,2}[\\lambda=\\frac{1}{4}]$ algebra,\nin two dimensions with the ${\\cal N}=4$ supergravity theory with $SO(4)$ global\nsymmetry in four dimensions found by Das (at Stony Brook in 1977), via\ncelestial holography. Furthermore, the truncations of ${\\cal N}=4$\nsupersymmetric $w_{1+\\infty}^{2,2}[\\lambda=\\frac{1}{4}]$ algebra provide the\nsoft current algebras for the ${\\cal N}=2,3$ supergravity theories, the ${\\cal\nN}=2$ supergravity coupled to its Abelian vector multiplet and the ${\\cal N}=1$\nsupersymmetric Maxwell Einstein theory. For the ${\\cal N}=2$ supergravity\ntheory, the soft current algebra can be also realized by the ${\\cal N}=2$\nsupersymmetric $w_{1+\\infty}^{K,K}[\\lambda=0]$ algebra.",
        "This paper discusses our recent generalized optimal algebraic multigrid (AMG)\nconvergence theory applied to the steady-state Stokes equations discretized\nusing Taylor-Hood elements ($\\pmb{ \\mathbb{P}}_2\/\\mathbb{P}_{1}$). The\ngeneralized theory is founded on matrix-induced orthogonality of the left and\nright eigenvectors of a generalized eigenvalue problem involving the system\nmatrix and relaxation operator. This framework establishes a rigorous lower\nbound on the spectral radius of the two-grid error-propagation operator,\nenabling precise predictions of the convergence rate for symmetric indefinite\nproblems, such as those arising from saddle-point systems. We apply this theory\nto the recently developed monolithic smooth aggregation AMG (SA-AMG) solver for\nStokes, constructed using evolution-based strength of connection, standard\naggregation, and smoothed prolongation. The performance of these solvers is\nevaluated using additive and multiplicative Vanka relaxation strategies.\nAdditive Vanka relaxation constructs patches algebraically on each level,\nresulting in a nonsymmetric relaxation operator due to the partition of unity\nbeing applied on one side of the block-diagonal matrix. Although symmetry can\nbe restored by eliminating the partition of unity, this compromises\nconvergence. Alternatively, multiplicative Vanka relaxation updates velocity\nand pressure sequentially within each patch, propagating updates\nmultiplicatively across the domain and effectively addressing velocity-pressure\ncoupling, ensuring a symmetric relaxation. We demonstrate that the generalized\noptimal AMG theory consistently provides accurate lower bounds on the\nconvergence rate for SA-AMG applied to Stokes equations. These findings suggest\npotential avenues for further enhancement in AMG solver design for saddle-point\nsystems.",
        "Previous schemes for designing secure branch prediction unit (SBPU) based on\nphysical isolation can only offer limited security and significantly affect\nBPU's prediction capability, leading to prominent performance degradation.\nMoreover, encryption-based SBPU schemes based on periodic key re-randomization\nhave the risk of being compromised by advanced attack algorithms, and the\nperformance overhead is also considerable. To this end, this paper proposes a\nconflict-invisible SBPU (CIBPU). CIBPU employs redundant storage design,\nload-aware indexing, and replacement design, as well as an encryption mechanism\nwithout requiring periodic key updates, to prevent attackers' perception of\nbranch conflicts. We provide a thorough security analysis, which shows that\nCIBPU achieves strong security throughout the BPU's lifecycle. We implement\nCIBPU in a RISC-V core model in gem5. The experimental results show that CIBPU\ncauses an average performance overhead of only 1.12%-2.20% with acceptable\nhardware storage overhead, which is the lowest among the state-of-the-art SBPU\nschemes. CIBPU has also been implemented in the open-source RISC-V core,\nSonicBOOM, which is then burned onto an FPGA board. The evaluation based on the\nboard shows an average performance degradation of 2.01%, which is approximately\nconsistent with the result obtained in gem5.",
        "The production and characteristics of protonated small water clusters (PSWCs)\nwere reported in this work, where in electrospray ionization (ESI) of pure\nwater, the species obtained were singly charged molecular ions consisting of 2,\n3, 4 or 5 water molecules attached to a hydrogen ion, [(H2O)n+H]+, where n = 2,\n3, 4 or 5. We proposed a new type of PSWCs structure: 2, 3, 4, 5 water\nmolecules wrapped around a hydrogen ion which is located at the electrical and\ngeometric center, forming a very stable molecular structure. Furthermore,\nbiological tests of the PSWCs on mitochondrial function of intestinal\nepithelial cells and liver cells in mice showed the better therapeutic effect\non inflammatory bowel diseases compared to that of the biologic agent\nInfliximab.",
        "The application of large language models (LLMs) in the legal domain holds\nsignificant potential for information retrieval and question answering, yet\nThai legal QA systems face challenges due to a lack of standardized evaluation\nbenchmarks and the complexity of Thai legal structures. This paper introduces\nNitiBench, a benchmark comprising two datasets: the NitiBench-CCL, covering\ngeneral Thai financial law, and the NitiBench-Tax, which includes real-world\ntax law cases requiring advanced legal reasoning. We evaluate\nretrieval-augmented generation (RAG) and long-context LLM-based approaches to\naddress three key research questions: the impact of domain-specific components\nlike section-based chunking and cross-referencing, the comparative performance\nof different retrievers and LLMs, and the viability of long-context LLMs as an\nalternative to RAG. Our results show that section-based chunking significantly\nimproves retrieval and end-to-end performance, current retrievers struggle with\ncomplex queries, and long-context LLMs still underperform RAG-based systems in\nThai legal QA. To support fair evaluation, we propose tailored multi-label\nretrieval metrics and the use of an LLM-as-judge for coverage and contradiction\ndetection method. These findings highlight the limitations of current Thai\nlegal NLP solutions and provide a foundation for future research in the field.\nWe also open-sourced our codes and dataset to available publicly.",
        "Many quantum communication protocols rely on the distribution of entanglement\nbetween the different participating parties. One example is quantum key\ndistribution (QKD), an application that has matured to commercial use in recent\nyears. However, difficulties remain, especially with noise resilience and\nchannel capacity in long-distance communication. One way to overcome these\nproblems is to use high-dimensional entanglement, which has been shown to be\nmore robust to noise and enables higher secret-key rates. It is therefore\nimportant to have access to certifiable high-dimensional entanglement sources\nto confidently implement these advanced QKD protocols. Here, we develop a\nmethod for certifying high-dimensional time-bin entanglement in fiber-loop\nsystems. In these systems, entanglement creation and detection can utilize the\nsame physical components, and the number of time bins, and thus the\nentanglement dimension, can be adapted without making physical changes to the\nsetup. Our certification method builds on previous proposals for the\ncertification of angular-momentum entanglement in photon pairs. In particular,\nmeasurements in only two experimentally accessible bases are sufficient to\nobtain a lower bound on the entanglement dimension for both two- and\nmultiphoton quantum states. Numerical simulations show that the method is\nrobust against typical experimental noise effects and works well even with\nlimited measurement statistics, thus establishing time-bin encoded photons as a\npromising platform for high-dimensional quantum-communication protocols.",
        "As platforms increasingly rely on learning algorithms, collectives may form\nand seek ways to influence these platforms to align with their own interests.\nThis can be achieved by coordinated submission of altered data. To evaluate the\npotential impact of such behavior, it is essential to understand the\ncomputations that collectives must perform to impact platforms in this way. In\nparticular, collectives need to make a priori assessments of the effect of the\ncollective before taking action, as they may face potential risks when\nmodifying their data. Moreover they need to develop implementable coordination\nalgorithms based on quantities that can be inferred from observed data. We\ndevelop a framework that provides a theoretical and algorithmic treatment of\nthese issues and present experimental results in a product evaluation domain.",
        "Bidders in combinatorial auctions face significant challenges when describing\ntheir preferences to an auctioneer. Classical work on preference elicitation\nfocuses on query-based techniques inspired from proper learning--often via\nproxies that interface between bidders and an auction mechanism--to\nincrementally learn bidder preferences as needed to compute efficient\nallocations. Although such elicitation mechanisms enjoy theoretical query\nefficiency, the amount of communication required may still be too cognitively\ntaxing in practice.\n  We propose a family of efficient LLM-based proxy designs for eliciting\npreferences from bidders using natural language. Our proposed mechanism\ncombines LLM pipelines and DNF-proper-learning techniques to quickly\napproximate preferences when communication is limited. To validate our\napproach, we create a testing sandbox for elicitation mechanisms that\ncommunicate in natural language. In our experiments, our most promising LLM\nproxy design reaches approximately efficient outcomes with five times fewer\nqueries than classical proper learning based elicitation mechanisms.",
        "Understanding the effect of road geometry on human driving behaviour is\nessential for both road safety studies and traffic microsimulation. Research on\nthis topic is still limited, mainly focusing on free-flow traffic and not\nadequately considering the influence of curvature on car-following dynamics.\nThis work attempts to investigate this issue and model the adaptation of\ncar-following behaviour to horizontal curvature. For this purpose, the maximum\ndesired speed - which mainly determines the free-flow dynamics - is expressed\nas a parsimonious function of the curvature. A spatial anticipation mechanism\nis also included in order to realistically describe the driving behaviour when\napproaching or exiting from curves. The accuracy of the augmented model is\nevaluated using the Modified Intelligent Driver Model (M-IDM) and trajectory\ndata from free-flow and car-following traffic (Naples data and Zen Traffic\nData). The results show that a significant improvement is achieved in free-flow\ndynamics. In car-following situations, improvements are mainly observed at high\nspeed and are dependent on the observed driver. Overall, the analysis\nhighlights the lack of sufficiently spatially extended trajectory data to\ncalibrate and evaluate such driving behaviours.",
        "Wearable biomedical devices are increasingly being used for continuous\npatient health monitoring, enabling real-time insights and extended data\ncollection without the need for prolonged hospital stays. These devices must be\nenergy efficient to minimize battery size, improve comfort, and reduce\nrecharging intervals. This paper investigates the use of specialized\nlow-precision arithmetic formats to enhance the energy efficiency of biomedical\nwearables. Specifically, we explore posit arithmetic, a floating-point-like\nrepresentation, in two key applications: cough detection for chronic cough\nmonitoring and R peak detection in ECG analysis. Simulations reveal that 16-bit\nposits can replace 32-bit IEEE 754 floating point numbers with minimal accuracy\nloss in cough detection. For R peak detection, posit arithmetic achieves\nsatisfactory accuracy with as few as 10 or 8 bits, compared to the 16-bit\nrequirement for floating-point formats. To further this exploration, we\nintroduce PHEE, a modular and extensible architecture that integrates the\nCoprosit posit coprocessor within a RISC-V-based system. Using the X-HEEP\nframework, PHEE seamlessly incorporates posit arithmetic, demonstrating reduced\nhardware area and power consumption compared to a floating-point counterpart\nsystem. Post-synthesis results targeting 16nm TSMC technology show that the\nposit hardware targeting these biomedical applications can be 38% smaller and\nconsume up to 54% less energy at the functional unit level, with no performance\ncompromise. These findings establish the potential of low-precision posit\narithmetic to significantly improve the energy efficiency of wearable\nbiomedical devices.",
        "As the potential for autonomous vehicles to be integrated on a large scale\ninto modern traffic systems continues to grow, ensuring safe navigation in\ndynamic environments is crucial for smooth integration. To guarantee safety and\nprevent collisions, autonomous vehicles must be capable of accurately\npredicting the trajectories of surrounding traffic agents. Over the past\ndecade, significant efforts from both academia and industry have been dedicated\nto designing solutions for precise trajectory forecasting. These efforts have\nproduced a diverse range of approaches, raising questions about the differences\nbetween these methods and whether trajectory prediction challenges have been\nfully addressed. This paper reviews a substantial portion of recent trajectory\nprediction methods and devises a taxonomy to classify existing solutions. A\ngeneral overview of the prediction pipeline is also provided, covering input\nand output modalities, modeling features, and prediction paradigms discussed in\nthe literature. In addition, the paper discusses active research areas within\ntrajectory prediction, addresses the posed research questions, and highlights\nthe remaining research gaps and challenges.",
        "Photon propagators for power-law inflation are constructed in two\none-parameter families of noncovariant gauges, in an arbitrary number of\nspacetime dimensions. In both gauges photon propagators take relatively simple\nforms expressed in terms of scalar propagators and their derivatives. These are\nconsiderably simpler compared to their general covariant gauge counterpart.\nThis makes feasible performing dimensionally regulated loop computations\ninvolving massless vector fields in inflation.",
        "Recent advances in large models have significantly advanced image-to-3D\nreconstruction. However, the generated models are often fused into a single\npiece, limiting their applicability in downstream tasks. This paper focuses on\n3D garment generation, a key area for applications like virtual try-on with\ndynamic garment animations, which require garments to be separable and\nsimulation-ready. We introduce Dress-1-to-3, a novel pipeline that reconstructs\nphysics-plausible, simulation-ready separated garments with sewing patterns and\nhumans from an in-the-wild image. Starting with the image, our approach\ncombines a pre-trained image-to-sewing pattern generation model for creating\ncoarse sewing patterns with a pre-trained multi-view diffusion model to produce\nmulti-view images. The sewing pattern is further refined using a differentiable\ngarment simulator based on the generated multi-view images. Versatile\nexperiments demonstrate that our optimization approach substantially enhances\nthe geometric alignment of the reconstructed 3D garments and humans with the\ninput image. Furthermore, by integrating a texture generation module and a\nhuman motion generation module, we produce customized physics-plausible and\nrealistic dynamic garment demonstrations. Project page:\nhttps:\/\/dress-1-to-3.github.io\/",
        "Vision Transformers (ViTs), extensively pre-trained on large-scale datasets,\nhave become essential to foundation models, allowing excellent performance on\ndiverse downstream tasks with minimal adaptation. Consequently, there is\ngrowing interest in adapting pre-trained ViTs across various fields, including\nprivacy-sensitive domains where clients are often reluctant to share their\ndata. Existing adaptation methods typically require direct data access,\nrendering them infeasible under these constraints. A straightforward solution\nmay be sending the pre-trained ViT to clients for local adaptation, which poses\nissues of model intellectual property protection and incurs heavy client\ncomputation overhead. To address these issues, we propose a novel split\nadaptation (SA) method that enables effective downstream adaptation while\nprotecting data and models. SA, inspired by split learning (SL), segments the\npre-trained ViT into a frontend and a backend, with only the frontend shared\nwith the client for data representation extraction. But unlike regular SL, SA\nreplaces frontend parameters with low-bit quantized values, preventing direct\nexposure of the model. SA allows the client to add bi-level noise to the\nfrontend and the extracted data representations, ensuring data protection.\nAccordingly, SA incorporates data-level and model-level out-of-distribution\nenhancements to mitigate noise injection's impact on adaptation performance.\nOur SA focuses on the challenging few-shot adaptation and adopts patch\nretrieval augmentation for overfitting alleviation. Extensive experiments on\nmultiple datasets validate SA's superiority over state-of-the-art methods and\ndemonstrate its defense against advanced data reconstruction attacks while\npreventing model leakage with minimal computation cost on the client side. The\nsource codes can be found at https:\/\/github.com\/conditionWang\/Split_Adaptation.",
        "In this study, we analyzed the close binary open clusters CWNU 2666 and HSC\n224, which are in close spatial proximity, using photometric and astrometric\ndata from the {\\it Gaia} DR3 catalog. Likely member stars were identified based\non a membership probability threshold ($P \\geq 0.5$), resulting in 106 and 146\nmembers for CWNU 2666 and HSC 224, respectively. The mean proper motion\ncomponents ($\\mu_{\\alpha}\\cos\\delta$, $\\mu_{\\delta}$) were determined to be\n(0.646$\\pm$0.155, -0.769$\\pm$0.124) mas yr$^{-1}$ for CWNU 2666, and\n(0.665$\\pm$0.131, -0.728$\\pm$0.107) mas yr$^{-1}$ for HSC 224. The isochrone\ndistances ($d_{\\rm iso}$) were estimated as 1885$\\pm$44 pc for CWNU 2666 and\n1866$\\pm$29 pc for HSC 224. The corresponding cluster ages ($t$) were derived\nas 160$\\pm$15 Myr and 140$\\pm$15 Myr, respectively. The astrometric and\nfundamental astrophysical parameters derived in this study demonstrate that the\ntwo open clusters are a close pair of open clusters.",
        "This paper presents a comprehensive analysis of the energy consumption\ncharacteristics of a Silicon (Si)-based Reconfigurable IoT (RIoT) node\ndeveloped in the initial phase of the SUPERIOT project, focusing on key\noperating states, including Bluetooth Low Energy (BLE) communication,\nNarrow-Band Visible Light Communication (NBVLC), sensing, and E-ink display.\nExtensive measurements were conducted to establish a detailed energy profile,\nwhich serves as a benchmark for evaluating the effectiveness of subsequent\noptimizations and future node iterations. To minimize the energy consumption,\nmultiple optimizations were implemented at both the software and hardware\nlevels, achieving a reduction of over 60% in total energy usage through\nsoftware modifications alone. Further improvements were realized by optimizing\nthe E-ink display driving waveform and implementing a very low-power mode for\nnon-communication activities. Based on the measured data, three\nmeasurement-based energy consumption models were developed to characterize the\nenergy behavior of the node under: (i) normal, unoptimized operation, (ii)\nlow-power, software-optimized operation, and (iii) very low-power,\nhardware-optimized operation. These models, validated with new measurement\ndata, achieved an accuracy exceeding 97%, confirming their reliability for\npredicting energy consumption in diverse configurations.",
        "Recent Multimodal Large Language Models (MLLMs) have typically focused on\nintegrating visual and textual modalities, with less emphasis placed on the\nrole of speech in enhancing interaction. However, speech plays a crucial role\nin multimodal dialogue systems, and implementing high-performance in both\nvision and speech tasks remains a significant challenge due to the fundamental\nmodality differences. In this paper, we propose a carefully designed\nmulti-stage training methodology that progressively trains LLM to understand\nboth visual and speech information, ultimately enabling fluent vision and\nspeech interaction. Our approach not only preserves strong vision-language\ncapacity, but also enables efficient speech-to-speech dialogue capabilities\nwithout separate ASR and TTS modules, significantly accelerating multimodal\nend-to-end response speed. By comparing our method against state-of-the-art\ncounterparts across benchmarks for image, video, and speech tasks, we\ndemonstrate that our model is equipped with both strong visual and speech\ncapabilities, making near real-time vision and speech interaction.",
        "The nuclear modification factor ($R_\\mathrm{AA}$) of $B_c$ mesons in\nhigh-energy nuclear collisions provides a novel probe of heavy quark\ninteractions with the quark-gluon plasma (QGP). Based on a linear Boltzmann\ntransport model that incorporates both Yukawa and string types of interactions\nbetween heavy quarks and the QGP, we study the production and evolution of\nheavy quarks and $B_c$ mesons within the same framework. A $B_c$ bound state\ndissociates while one of its constituent heavy quarks scatters with the QGP\nwith momentum transfer greater than its binding energy. The medium-modified\ncharm and bottom quarks can recombine into $B_c$ mesons, and the\nmedium-modified bottom quarks can also fragment to $B_c$ mesons. We find that\nmost primordial $B_c$ mesons generated from the initial hard collisions\ndissociate inside the QGP. The production of $B_c$ mesons is primarily driven\nby the recombination mechanism at low transverse momentum and fragmentation at\nhigh transverse momentum. The string interaction dominates over the Yukawa\ninteraction in the nuclear modification of $B_c$ mesons. The participant number\ndependence of the $B_c$ meson $R_\\mathrm{AA}$ is determined by the complicated\ninterplay between the heavy quark yield, energy loss, and the QGP volume. We\nobtain a reasonable description of the $R_\\mathrm{AA}$ of $B_c$ mesons in Pb+Pb\ncollisions at $\\sqrt{s_\\mathrm{NN}}=5.02$ TeV, and provide predictions for\nAu+Au collisions at $\\sqrt{s_\\mathrm{NN}}=200$ GeV.",
        "Reinforcement learning (RL) has been a promising essence in future 5G-beyond\nand 6G systems. Its main advantage lies in its robust model-free\ndecision-making in complex and large-dimension wireless environments. However,\nmost existing RL frameworks rely on online interaction with the environment,\nwhich might not be feasible due to safety and cost concerns. Another problem\nwith online RL is the lack of scalability of the designed algorithm with\ndynamic or new environments. This work proposes a novel, resilient, few-shot\nmeta-offline RL algorithm combining offline RL using conservative Q-learning\n(CQL) and meta-learning using model-agnostic meta-learning (MAML). The proposed\nalgorithm can train RL models using static offline datasets without any online\ninteraction with the environments. In addition, with the aid of MAML, the\nproposed model can be scaled up to new unseen environments. We showcase the\nproposed algorithm for optimizing an unmanned aerial vehicle (UAV) 's\ntrajectory and scheduling policy to minimize the age-of-information (AoI) and\ntransmission power of limited-power devices. Numerical results show that the\nproposed few-shot meta-offline RL algorithm converges faster than baseline\nschemes, such as deep Q-networks and CQL. In addition, it is the only algorithm\nthat can achieve optimal joint AoI and transmission power using an offline\ndataset with few shots of data points and is resilient to network failures due\nto unprecedented environmental changes.",
        "We introduce Metis, a foundation model for unified speech generation. Unlike\nprevious task-specific or multi-task models, Metis follows a pre-training and\nfine-tuning paradigm. It is pre-trained on large-scale unlabeled speech data\nusing masked generative modeling and then fine-tuned to adapt to diverse speech\ngeneration tasks. Specifically, 1) Metis utilizes two discrete speech\nrepresentations: SSL tokens derived from speech self-supervised learning (SSL)\nfeatures, and acoustic tokens directly quantized from waveforms. 2) Metis\nperforms masked generative pre-training on SSL tokens, utilizing 300K hours of\ndiverse speech data, without any additional condition. 3) Through fine-tuning\nwith task-specific conditions, Metis achieves efficient adaptation to various\nspeech generation tasks while supporting multimodal input, even when using\nlimited data and trainable parameters. Experiments demonstrate that Metis can\nserve as a foundation model for unified speech generation: Metis outperforms\nstate-of-the-art task-specific or multi-task systems across five speech\ngeneration tasks, including zero-shot text-to-speech, voice conversion, target\nspeaker extraction, speech enhancement, and lip-to-speech, even with fewer than\n20M trainable parameters or 300 times less training data. Audio samples are are\navailable at https:\/\/metis-demo.github.io\/.",
        "In this paper, we investigate a novel approach for Target Speech Extraction\n(TSE), which relies solely on textual context to extract the target speech. We\nrefer to this task as Contextual Speech Extraction (CSE). Unlike traditional\nTSE methods that rely on pre-recorded enrollment utterances, video of the\ntarget speaker's face, spatial information, or other explicit cues to identify\nthe target stream, our proposed method requires only a few turns of previous\ndialogue (or monologue) history. This approach is naturally feasible in mobile\nmessaging environments where voice recordings are typically preceded by textual\ndialogue that can be leveraged implicitly. We present three CSE models and\nanalyze their performances on three datasets. Through our experiments, we\ndemonstrate that even when the model relies purely on dialogue history, it can\nachieve over 90 % accuracy in identifying the correct target stream with only\ntwo previous dialogue turns. Furthermore, we show that by leveraging both\ntextual context and enrollment utterances as cues during training, we further\nenhance our model's flexibility and effectiveness, allowing us to use either\ncue during inference, or combine both for improved performance. Samples and\ncode available on https:\/\/miraodasilva.github.io\/cse-project-page .",
        "We investigate a dynamical reconstruction of the dark energy equation of\nstate parameter by assuming that it satisfies a law of motion described by an\nautonomous second-order differential equation, with the limit of the\ncosmological constant as an equilibrium point. We determine the asymptotic\nsolutions of this equation and use them to construct two families of parametric\ndark energy models, employing both linear and logarithmic parametrizations with\nrespect to the scale factor. We perform observational constraints by using the\nSupernova, the Cosmic Chronometers and the Baryon Acoustic Oscillations. The\nconstraint parameters are directly related with the initial value problem for\nthe law of motion and its algebraic properties. The analysis shows that most of\nthe models fit the observational data well with a preference to the models of\nthe logarithmic parametrization. Furthermore, we introduce a new class of\nmodels as generalizations of the CPL model, for which the equilibrium point is\na constant value rather than the cosmological constant. These models fit the\ndata in a similar or better way to the CPL and the $\\Lambda$CDM cosmological\nmodels.",
        "Secure Multi-Party Computation (MPC) is an important enabling technology for\ndata privacy in modern distributed applications. We develop a new type theory\nto automatically enforce correctness,confidentiality, and integrity properties\nof protocols written in the \\emph{Prelude\/Overture} language framework.\nJudgements in the type theory are predicated on SMT verifications in a theory\nof finite fields, which supports precise and efficient analysis. Our approach\nis automated, compositional, scalable, and generalizes to arbitrary prime\nfields for data and key sizes.",
        "We show that the peak of an initially localized wave packet in\none-dimensional nonlinear disordered chains decays more slowly than any power\nlaw of time. The systems under investigation are Klein-Gordon and nonlinear\ndisordered Schr\\\"odinger-type chains, characterized by a harmonic onsite\ndisordered potential and quartic nearest-neighbor coupling. Our results apply\nin the long-time limit, hold almost surely, and are valid for arbitrary finite\nenergy values.",
        "Out-of-Distribution (OOD) detection is critical for ensuring the reliability\nof machine learning models in safety-critical applications such as autonomous\ndriving and medical diagnosis. While deploying personalized OOD detection\ndirectly on edge devices is desirable, it remains challenging due to large\nmodel sizes and the computational infeasibility of on-device training.\nFederated learning partially addresses this but still requires gradient\ncomputation and backpropagation, exceeding the capabilities of many edge\ndevices. To overcome these challenges, we propose SecDOOD, a secure\ncloud-device collaboration framework for efficient on-device OOD detection\nwithout requiring device-side backpropagation. SecDOOD utilizes cloud resources\nfor model training while ensuring user data privacy by retaining sensitive\ninformation on-device. Central to SecDOOD is a HyperNetwork-based personalized\nparameter generation module, which adapts cloud-trained models to\ndevice-specific distributions by dynamically generating local weight\nadjustments, effectively combining central and local information without local\nfine-tuning. Additionally, our dynamic feature sampling and encryption strategy\nselectively encrypts only the most informative feature channels, largely\nreducing encryption overhead without compromising detection performance.\nExtensive experiments across multiple datasets and OOD scenarios demonstrate\nthat SecDOOD achieves performance comparable to fully fine-tuned models,\nenabling secure, efficient, and personalized OOD detection on resource-limited\nedge devices. To enhance accessibility and reproducibility, our code is\npublicly available at https:\/\/github.com\/Dystopians\/SecDOOD.",
        "The existence of singularities in the spectrum of non-Hermitian Hamiltonians\nleads to a non-trivial spectral topology which can be exploited to generate\ntopological operations. However, their implementation has remained elusive due\nto the difficulty of generating a true adiabatic evolution. Here, we develop\nfast, robust control protocols that generate a desired topological operation.\nOur strategy relies on shortcuts to adiabaticity, but is not a trivial\nextension. The presence of spectral singularities renders the strategy\ndeveloped for Hermitian Hamiltonians impractical as it will lead to faulty\ncontrol protocols. Moreover, due to the dynamics sensitivity to parameter\nuncertainties, not all shortcuts to adiabaticity can be used in a realistic\nsetting. We illustrate our method in the context of a two-mode non-Hermitian\nHamiltonian and discuss why in general celebrated shortcuts to adiabaticiy like\ntransitionless driving and superadiabatic transitionless driving are not\nappropriate control protocols for non-Hermitian systems.",
        "The discovery of superconductivity in pressurized bilayer and trilayer\nnickelates has generated significant interest. However, their superconducting\nproperties are often dependent on sample quality and pressure conditions,\ncomplicating the interpretation of the underlying physics. Finding new systems\nwith optimized bulk superconducting properties is therefore important for\nadvancing our understanding of these materials. Unlike cupates, where trilayer\ncompounds typically exhibit the highest transition temperature (Tc), the\nbilayer nickelate La3Ni2O7 has thus far outperformed the trilayer La4Ni3O10 in\nreported Tc. Whether the trilayer nickelates have achieved the optimal Tc\nremains unclear, with various scenarios suggesting different possibilities.\nHere, we report the discovery of bulk superconductivity in pressurized\nPr4Ni3O10 single crystals, achieving a maximum onset Tc of 40.5 K at 80.1 GPa,\nsignificantly exceeding the 30 K observed in La4Ni3O10. The bulk nature of\nsuperconductivity is confirmed by zero resistance and a strong diamagnetic\nresponse below Tc with a superconducting volume fraction exceeding 80%. These\nfindings establish trilayer nickelates as genuine bulk high-temperature\nsuperconductors, provide new insights into the mechanisms driving\nsuperconductivity, and point to a promising route toward further enhancing\nsuperconducting properties in nickelates.",
        "Elasto-inertial turbulence (EIT) has been demonstrated to be able to sustain\nin two-dimensional (2D) channel flow; however the systematic investigations on\n2D EIT remain scare. This study addresses this gap by examining the statistical\ncharacteristics and dynamic mechanisms of 2D EIT, while exploring its\nsimilarities to and differences from three-dimensional (3D) EIT. We demonstrate\nthat the influence of elasticity on the statistical properties of 2D EIT\nfollows distinct trends compared to those observed in 3D EIT and drag-reducing\nturbulence (DRT). These differences can be attributed to variations in the\nunderlying dynamical processes. As nonlinear elasticity increases, the dominant\ndynamic evolution in 3D flows involves the gradual suppression of inertial\nturbulence (IT). In contrast, 2D flows exhibit a progressive enhancement of\nEIT. More strikingly, we identify an anomalous Reynolds stress in 2D EIT that\ncontributes negatively to flow resistance, a behavior opposite to that of IT.\nQuadrant analysis of velocity fluctuations reveals the predominance of motions\nin the first and third quadrants. These motions are closely associated with\npolymer sheet-like extension structures, which are inclined from the near-wall\nregion toward the channel center along the streamwise direction. Finally, we\npresent the dynamical budget of 2D EIT, which shows significant similarities to\nthat of 3D EIT, thereby providing compelling evidence for the objective\nexistence of the 2D nature of EIT.",
        "This research presents a robust approach to classifying COVID-19 cough sounds\nusing cutting-edge machine-learning techniques. Leveraging deep neural decision\ntrees and deep neural decision forests, our methodology demonstrates consistent\nperformance across diverse cough sound datasets. We begin with a comprehensive\nextraction of features to capture a wide range of audio features from\nindividuals, whether COVID-19 positive or negative. To determine the most\nimportant features, we use recursive feature elimination along with\ncross-validation. Bayesian optimization fine-tunes hyper-parameters of deep\nneural decision tree and deep neural decision forest models. Additionally, we\nintegrate the SMOTE during training to ensure a balanced representation of\npositive and negative data. Model performance refinement is achieved through\nthreshold optimization, maximizing the ROC-AUC score. Our approach undergoes a\ncomprehensive evaluation in five datasets: Cambridge, Coswara, COUGHVID,\nVirufy, and the combined Virufy with the NoCoCoDa dataset. Consistently\noutperforming state-of-the-art methods, our proposed approach yields notable\nAUC scores of 0.97, 0.98, 0.92, 0.93, 0.99, and 0.99 across the respective\ndatasets. Merging all datasets into a combined dataset, our method, using a\ndeep neural decision forest classifier, achieves an AUC of 0.97. Also, our\nstudy includes a comprehensive cross-datasets analysis, revealing demographic\nand geographic differences in the cough sounds associated with COVID-19. These\ndifferences highlight the challenges in transferring learned features across\ndiverse datasets and underscore the potential benefits of dataset integration,\nimproving generalizability and enhancing COVID-19 detection from audio signals."
      ]
    }
  },
  {
    "id":2411.0064,
    "research_type":"applied",
    "start_id":"b6",
    "start_title":"The Llama 3 Herd of Models",
    "start_abstract":"Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b14"
      ],
      "title":[
        "Quantifying Variance in Evaluation Benchmarks"
      ],
      "abstract":[
        "Evaluation benchmarks are the cornerstone of measuring capabilities large language models (LLMs), as well driving progress in said capabilities. Originally designed to make claims about (or lack thereof) fully pretrained models, evaluation now also extensively used decide between various training choices. Despite this widespread usage, we rarely quantify variance our benchmarks, which dictates whether differences performance meaningful. Here, define and measure a range metrics geared towards including seed across initialisations, monotonicity during training. By studying number -- both openly available from scratch provide empirical estimates for variety metrics, with considerations recommendations practitioners. We evaluate utility tradeoffs continuous versus discrete measures explore options better understanding reducing variance. find that simple changes, such framing choice tasks (like MMLU) completion tasks, can often reduce smaller scale ($\\sim$7B) while more involved methods inspired human testing literature (such item analysis response theory) struggle meaningfully Overall, work provides insights into suggests LM-specific techniques variance, generally encourages practitioners carefully factor when comparing models."
      ],
      "categories":[
        "stat.ME"
      ]
    },
    "list":{
      "title":[
        "De facto Openness to Immigration",
        "Extended $s$-wave pairing from an emergent Feshbach resonanc in bilayer\n  nickelate superconductors",
        "Optical control of the spin-Hall effect in a two-dimensional hole gas",
        "Quantum-enhanced neural networks for quantum many-body simulations",
        "A Link Between White Dwarf Pulsars and Polars: Multiwavelength\n  Observations of the 9.36-Minute Period Variable Gaia22ayj",
        "Decoding lithium's subtle phase stability with a machine learning force\n  field",
        "Decay of large solutions around shocks to multi-D viscous conservation\n  law with strictly convex flux",
        "Scalable architecture for dark photon searches: Superconducting-qubit\n  proof of principle",
        "On zero-sum Ramsey numbers modulo 3",
        "Isogenies of minimal Cantor systems: from Sturmian to Denjoy and\n  interval exchanges",
        "Modeling reflection and refraction of freeform surfaces",
        "Anisotropy can make a moving active fluid membrane rough or crumpled",
        "Tomographic identification of all molecular orbitals in a wide binding\n  energy range",
        "Star formation in interacting galaxy systems: UVIT imaging of NGC 7252\n  and NGC 5291",
        "Using Lagrangian descriptors to reveal the phase space structure of\n  dynamical systems described by fractional differential equations: Application\n  to the Duffing oscillator",
        "Evidence of Replica Symmetry Breaking under the Nishimori conditions in\n  epidemic inference on graphs",
        "Jointly Assigning Processes to Machines and Generating Plans for\n  Autonomous Mobile Robots in a Smart Factory",
        "ouladFormat R package: Preparing the Open University Learning Analytics\n  Dataset for analysis",
        "Large $\\theta$ angle in two-dimensional large $N$ $\\mathbb{CP}^{N-1}$\n  model",
        "On the learning power of Friedman-Stanley jumps",
        "Homoclinic and Heteroclinic Trajectories of Differential Equations with\n  Piecewise Constant Arguments of Generalized Type",
        "A deep BSDE approach for the simultaneous pricing and delta-gamma\n  hedging of large portfolios consisting of high-dimensional multi-asset\n  Bermudan options",
        "Impact of Data Patterns on Biotype identification Using Machine Learning",
        "Bandgap-Dependent Doping of Semiconducting Carbon Nanotube Networks by\n  Proton-Coupled Electron Transfer for Stable Thermoelectrics",
        "Evidence of the P_ccbars(4459)0 in Upsilon(1S, 2S) inclusive decays at\n  Belle",
        "Decoding FRB energetics and frequency features hidden by observational\n  incompleteness",
        "Search for the radiative leptonic decay $D^+\\to\\gamma e^+\\nu_e$ with\n  Deep Learning",
        "Observation of Giant Orbital Hall Effect in Si",
        "Best Approximations on Quasi-Cone Metric Spaces"
      ],
      "abstract":[
        "Various factors influence why some countries are more open to immigration\nthan others. Policy is only one of them. We design country-specific measures of\nopenness to immigration that aim to capture de facto levels of openness to\nimmigration, complementing existing de jure measures of immigration, based on\nenacted immigration laws and policy measures. We estimate these for 148\ncountries and three years (2000, 2010, and 2020). For a subset of countries, we\nalso distinguish between openness towards tertiary-educated migrants and less\nthan tertiary-educated migrants. Using the measures, we show that most places\nin the World today are closed to immigration, and a few regions are very open.\nThe World became more open in the first decade of the millennium, an opening\nmainly driven by the Western World and the Gulf countries. Moreover, we show\nthat other factors equal, countries that increased their openness to\nimmigration, reduced their old-age dependency ratios, and experienced slower\nreal wage growth, arguably a sign of relaxing labor and skill shortages.",
        "Since the discovery of unconventional superconductivity in cuprates,\nunraveling the pairing mechanism of charge carriers in doped antiferromagnets\nhas been a long-standing challenge. Motivated by the discovery of high-T$_c$\nsuperconductivity in nickelate bilayer La$_3$Ni$_2$O$_7$ (LNO), we study a\nminimal mixed dimensional (MixD) $t-J$ model supplemented with a repulsive\nCoulomb interaction $V$. When hole-doped, previous numerical simulations\nrevealed that the system exhibits strong binding energies, with a phenomenology\nresembling a BCS-to-BEC crossover accompanied by a Feshbach resonance between\ntwo distinct types of charge carriers. Here, we perform a mean-field analysis\nthat enables a direct observation of the BCS-to-BEC crossover as well as\nmicroscopic insights into the crossover region and the pairing symmetry for\ntwo-dimensional bilayers. We benchmark our mean-field description by comparing\nit to density-matrix renormalization group (DMRG) simulations in quasi-one\ndimensional settings and find remarkably good agreement. For the\ntwo-dimensional system relevant to LNO our mean-field calculations predict a\nBCS pairing gap with an extended $s$-wave symmetry, directly resulting from the\npairing mechanism's Feshbach-origin. Our analysis hence gives insights into\npairing in unconventional superconductors and, further, can be tested in\ncurrently available ultracold atom experiments.",
        "Relativistic effects influence the motion of charged particles in solids by\nintertwining spin and momentum. The resulting phenomena exhibit rich and\nintriguing properties that can unveil radically new quantum devices. In this\ncontext, the two-dimensional hole gas formed in group IV heterostructures is a\nparticularly promising platform, owning to a notable spin-orbit coupling.\nHowever, the exploitation of spin-momentum locking and precise manipulation of\nspin currents has remained elusive thus far. Here we use the modulation-doping\ntechnique to break inversion symmetry at novel Ge1-xSnx\/Ge interfaces and\nexplore spin-orbit phenomena in the emergent Rashba-coupled hole gases.\nMagneto-optical investigations demonstrate the unusual establishment of a\nstaggered band alignment with carrier lifetime in the ns range. Optical spin\norientation is then leveraged to directly inject spin-polarized currents in the\nRashba-split 2D gas. Spin-to-charge conversion is shown to genuinely occur at\nthe staggered gap through the inverse spin-Hall effect. This provides\nunprecedented access to low-order contributions of the spin-orbit Hamiltonian.\nMoreover, it leads to the startling demonstration that the spin Hall angle can\nbe optically controlled by modifying the Rashba coupling through the\nphotoexcitation density. Ge1-xSnx quantum wells thus offer innovative solutions\nand functionalities stemming from their unique spin-dependent properties and\nintriguing quantum phenomena at the crossroad between transport and photonic\nrealms.",
        "Neural quantum states (NQS) have gained prominence in variational quantum\nMonte Carlo methods in approximating ground-state wavefunctions. Despite their\nsuccess, they face limitations in optimization, scalability, and expressivity\nin addressing certain problems. In this work, we propose a quantum-neural\nhybrid framework that combines parameterized quantum circuits with neural\nnetworks to model quantum many-body wavefunctions. This approach combines the\nefficient sampling and optimization capabilities of autoregressive neural\nnetworks with the enhanced expressivity provided by quantum circuits. Numerical\nsimulations demonstrate the scalability and accuracy of the hybrid ansatz in\nspin systems and quantum chemistry problems. Our results reveal that the hybrid\nmethod achieves notably lower relative energy compared to standalone NQS. These\nfindings underscore the potential of quantum-neural hybrid methods for tackling\nchallenging problems in quantum many-body simulations.",
        "White dwarfs (WDs) are the most abundant compact objects, and recent surveys\nhave suggested that over a third of WDs in accreting binaries host a strong (B\n$\\gtrsim$ 1 MG) magnetic field. However, the origin and evolution of WD\nmagnetism remain under debate. Two WD pulsars, AR Sco and J191213.72-441045.1\n(J1912), have been found, which are non-accreting binaries hosting rapidly\nspinning (1.97-min and 5.30-min, respectively) magnetic WDs. The WD in AR Sco\nis slowing down on a $P\/\\dot{P}\\approx 5.6\\times 10^6$ yr timescale. It is\nbelieved they will eventually become polars, accreting systems in which a\nmagnetic WD (B $\\approx 10-240$ MG) accretes from a Roche lobe-filling donor\nspinning in sync with the orbit ($\\gtrsim 78$ min). Here, we present\nmultiwavelength data and analysis of Gaia22ayj, which outbursted in March 2022.\nWe find that Gaia22ayj is a magnetic accreting WD that is rapidly spinning down\n($P\/\\dot{P} = 6.1^{+0.3}_{-0.2}\\times 10^6$ yr) like WD pulsars, but shows\nclear evidence of accretion, like polars. Strong linear polarization (40%) is\ndetected in Gaia22ayj; such high levels have only been seen in the WD pulsar AR\nSco and demonstrate the WD is magnetic. High speed photometry reveals a\n9.36-min period accompanying a high amplitude ($\\sim 2$ mag) modulation. We\nassociate this with a WD spin or spin-orbit beat period, not an orbital period\nas was previously suggested. Fast (60-s) optical spectroscopy reveals a broad\n``hump'', reminiscent of cyclotron emission in polars, between 4000-8000\nAngstrom. We find an X-ray luminosity of $L_X = 2.7_{-0.8}^{+6.2}\\times10^{32}\n\\textrm{ erg s}^{-1}$ in the 0.3-8 keV energy range, while two VLA radio\ncampaigns resulted in a non-detection with a $F_r < 15.8\\mu\\textrm{Jy}$ 3$\n\\sigma$ upper limit. The shared properties of both WD pulsars and polars\nsuggest that Gaia22ayj is a missing link between the two classes of magnetic WD\nbinaries.",
        "Understanding the phase stability of elemental lithium (Li) is crucial for\noptimizing its performance in lithium-metal battery anodes, yet this seemingly\nsimple metal exhibits complex polymorphism that requires proper accounting for\nquantum and anharmonic effects to capture the subtleties in its flat energy\nlandscape. Here we address this challenge by developing an accurate graph\nneural network-based machine learning force field and performing efficient\nself-consistent phonon calculations for bcc-, fcc-, and 9R-Li under\nnear-ambient conditions, incorporating quantum, phonon renormalization and\nthermal expansion effects. Our results reveal the important role of\nanharmonicity in determining Li's thermodynamic properties. The free energy\ndifferences between these phases, particularly fcc- and 9R-Li are found to be\nonly a few meV\/atom, explaining the experimental challenges in obtaining\nphase-pure samples and suggesting a propensity for stacking faults and related\ndefect formation. fcc-Li is confirmed as the ground state at zero temperature\nand pressure, and the predicted bcc-fcc phase boundary qualitatively matches\nexperimental phase transition lines, despite overestimation of the transition\ntemperature and pressure slope. These findings provide crucial insights into\nLi's complex polymorphism and establish an effective computational approach for\nlarge-scale atomistic simulations of Li in more realistic settings for\npractical energy storage applications.",
        "We consider a planar viscous shock for a scalar viscous conservation law with\na strictly convex flux in multi-dimensional setting, where the transversal\ndirection is periodic. We first show the contraction property for any solutions\nevolving from a large bounded initial perturbation in $L^2$ of the viscous\nshock. The contraction holds up to a dynamical shift, and it is measured by a\nweighted relative entropy. This result for the contraction extends the existing\nresult in 1D \\cite{Kang19} to the multi-dimensional case. As a consequence, if\nthe large bounded initial $L^2$-perturbation is also in $L^1$, then the large\nperturbation decays of rate $t^{-1\/4}$ in $L^2$, up to a dynamical shift that\nis uniformly bounded in time. This is the first result for the quantitative\nestimate converging to a planar shock under large perturbations.",
        "The dark photon is a well-motivated candidate of dark matter due to its\npotential to open the window of new physics beyond the Standard Model. A\nfundamental mass-range-sensitivity dilemma is always haunting the dark photon\nsearching experiments: The resonant haloscopes have excellent sensitivity but\nare narrowband, and vice versa for the non-resonant ones. A scalable\narchitecture integrating numerous resonant haloscopes will be a desirable\nsolution to this dilemma. However, even the concept of scalable searching\nremains rarely explored, due to the size limitation of conventional haloscopes\nimposed by the dark photon wavelength. Here we propose and demonstrate a novel\narchitecture using superconducting qubits as sub-wavelength haloscope units. By\nvirtue of the scalability of superconducting qubits, it is possible to\nintegrate multiple qubits with different frequencies on a chip-scale device.\nFurthermore, the frequencies of the qubits can be tuned to extend the searching\nmass range. Thus, our architectures allow for searching for dark photons in a\nbroad mass range with high sensitivity. As a proof-of-principle experiment, we\ndesigned and fabricated a three-qubit chip and successfully demonstrated a\nscalable dark-photon searching. Our work established constraints on dark\nphotons in the mass range of 15.632 $\\mu$eV$\\sim$15.638 $\\mu$eV, 15.838\n$\\mu$eV$\\sim$15.845 $\\mu$eV, and 16.463 $\\mu$eV$\\sim$16.468 $\\mu$eV,\nsimultaneously, and the constraints are much more stringent than the cosmology\nconstraints. Our work can be scaled up in the future to boost the scrutiny of\nnew physics and extended to search for more dark matter candidates, including\ndark photons, axions and axion-like particles.",
        "We start with a systematic study of the zero-sum Ramsey numbers. For a graph\n$G$ with $0 \\ (\\!\\!\\!\\!\\mod 3)$ edges, the zero-sum Ramsey number is defined as\nthe smallest positive integer $R(G, \\mathbb{Z}_3)$ such that for every $n \\geq\nR(G, \\mathbb{Z}_3)$ and every edge-colouring $f$ of $K_n$ using $\\mathbb{Z}_3$,\nthere is a zero-sum copy of $G$ in $K_n$ coloured by $f$, that is: $\\sum_{e \\in\nE(G)} f(e) \\equiv 0 \\ (\\!\\!\\!\\!\\mod 3)$.\n  Only sporadic results are known for these Ramsey numbers, and we discover\nmany new ones. In particular we prove that for every forest $F$ on $n$ vertices\nand with $0 \\ (\\!\\!\\!\\!\\mod 3)$ edges, $R(F, \\mathbb{Z}_3) \\leq n+2$, and this\nbound is tight if all the vertices of $F$ have degrees $1 \\ (\\!\\!\\!\\!\\mod 3)$.\nWe also determine exact values of $R(T, \\mathbb{Z}_3)$ for infinite families of\ntrees.",
        "This work is motivated by the study of continued fraction expansions of real\nnumbers: we describe in dynamical terms their orbits under the action of\n$\\mathrm{PGL}_2(\\mathbb{Q})$. A real number gives rise to a Sturmian system\nencoding a rotation of the circle. It is well known that\n$\\mathrm{PGL}_2(\\mathbb{Z})$-equivalence of real numbers, characterized by the\ntails of their continued fraction expansions, amounts to flow equivalence of\nSturmian systems. We show that the multiplicative action of $m\\in \\mathbb{Z}$\non a real number corresponds to taking the $m$th-power followed by what we call\nan infinitesimal 2-asymptotic factor of its Sturmian system.\n  This leads us to introduce the notion of isogeny between zero-dimensional\nsystems: it combines virtual flow equivalences and infinitesimal asymptotic\nequivalences. We develop tools for classifying systems up to isogeny involving\ncohomological invariants and states. We then use this to give a complete\ndescription of $\\mathrm{PSL}_2(\\mathbb{Q})$-equivalence of real numbers in\nterms of Sturmian systems. We classify Denjoy systems up to isogenies within\nthis class via the action of $\\mathrm{PGL}_{2}(\\mathbb{Q})$ on their\ninvariants.\n  We also investigate eventual flow equivalence of Sturmian systems: we show\nthat for non-quadratic parameters it amounts to topological conjugacy and for\nquadratic parameters it implies total flow equivalence and other arithmetic\nconstraints.\n  In another direction, we consider interval exchanges satisfying Keane's\ncondition. We characterize flow equivalence in terms of interval-induced\nsubsystems (or the tails of their paths in the bilateral Rauzy induction\ndiagram). Finally we find rational invariants for isogeny involving the length\nmodules and SAF invariants of the associated ergodic measures. This leads to a\nconjecture for their classification up to isogeny, which we prove in the\ntotally ergodic case.",
        "In this work, we present a detailed procedure of computer implementation of\nthe laws of refraction and reflection on an arbitrary surface with rotational\nsymmetry with respect to the propagation axis. The goal is to facilitate the\nunderstanding and application of these physical principles in a computational\ncontext. This enables students and instructors alike to develop simulations and\ninteractive applications that faithfully replicate the behavior of light and\nsound propagating in a diversity of media separated by arbitrary surfaces. In\nparticular it can help to explore freeform optics. Additionally, we include a\npractical example demonstrating these implementations using either Matlab or\nopen-source Octave programming language.",
        "We present a hydrodynamic theory of anisotropic and inversion-asymmetric\nmoving active permeable fluid membranes. These are described by an anisotropic\nKardar-Parisi-Zhang equation. Depending upon the anisotropy parameters, the\nmembrane can be large-scale anisotropic and logarithmically rough with\ntranslational quasi long range order and orientational long range order,\ntogether with the relaxational dynamics being logarithmically faster than\nordinary diffusion. For other choices of the anisotropy parameters, the\nmembrane is either effectively isotropic and algebraically rough with\ntranslational short, but orientational long range order, or crumpled.",
        "In the past decade, photoemission orbital tomography (POT) has evolved into a\npowerful tool to investigate the electronic structure of organic molecules\nadsorbed on surfaces. Here we show that POT allows for the comprehensive\nexperimental identification of all molecular orbitals in a substantial binding\nenergy range, in the present case more than 10 eV. Making use of the angular\ndistribution of photoelectrons as a function of binding energy, we exemplify\nthis by extracting orbital-resolved partial densities of states (pDOS) for 15\n$\\pi$ and 23 $\\sigma$ orbitals from the experimental photoemission intensities\nof the prototypical organic molecule bisanthene (C$_{28}$H$_{14}$) on a Cu(110)\nsurface. In their entirety, these experimentally measured orbital-resolved pDOS\nfor an essentially complete set of orbitals serve as a stringent benchmark for\nelectronic structure methods, which we illustrate by performing density\nfunctional theory (DFT) calculations employing four frequently-used\nexchange-correlation functionals. By computing the respective\nmolecular-orbital-projected densities of states of the bisanthene\/Cu(110)\ninterface, a one-to-one comparison with experimental data for an unprecedented\nnumber of 38 orbital energies becomes possible. The quantitative analysis of\nour data reveals that the range-separated hybrid functional HSE performs best\nfor the investigated organic\/metal interface. At a more fundamental level, the\nremarkable agreement between the experimental and the Kohn-Sham orbital\nenergies over a binding energy range larger than 10\\,eV suggests that --\nperhaps unexpectedly -- Kohn-Sham orbitals approximate Dyson orbitals, which\nwould rigorously account for the electron extraction process in photoemission\nspectroscopy but are notoriously difficult to compute, in a much better way\nthan previously thought.",
        "Interactions play a significant role in the formation and evolution of\ngalaxies in the Universe. The galaxy systems, NGC 7252 and NGC 5291 are two\nnearby interacting systems that are hosting Tidal Dwarf Galaxies (TDGs) and\nstar-forming knots. The present work aims (a) To determine the\nattenuation-corrected star formation rate (SFR) of the interacting system NGC\n7252 (b) To compare the star formation in the NGC 7252 system with that of the\nNGC 5291 system (c) To explore the relation between surface densities of gas\nand SFR in these two systems. The study utilises high-resolution FUV and NUV\nimaging data from the Ultraviolet Imaging Telescope (UVIT) on board AstroSat.\nSix star-forming regions, including the merger remnant, were identified in the\nNGC 7252 system. The SFR corrected for attenuation of the knots in the NGC 7252\nsystem is determined using the continuum slope (\\beta) calculated from the\nFUV-NUV colour. It has been observed that the attenuation-corrected SFR values\nof the knots in this system fall within the range of SFR values determined for\nthe NGC 5291 knots. The TDGs in both systems adhere to the same\nKennicutt-Schmidt (KS) relation as regular spiral galaxies.",
        "We showcase the utility of the Lagrangian descriptors method in qualitatively\nunderstanding the underlying dynamical behavior of dynamical systems governed\nby fractional-order differential equations. In particular, we use the\nLagrangian descriptors method to study the phase space structure of the\nunforced and undamped Duffing oscillator when its time evolution is governed by\nfractional-order differential equations. In our study, we implement two types\nof fractional derivatives, namely the standard Gr\\\"unwald-Letnikov method,\nwhich is a finite difference approximation of the Riemann-Liouville fractional\nderivative, and a Gr\\\"unwald-Letnikov method with a correction term that\napproximates the Caputo fractional derivative. While there is no issue with\nforward-time integrations needed for the evaluation of Lagrangian descriptors,\nwe discuss in detail ways to perform the non-trivial task of backward-time\nintegrations and implement two methods for this purpose: a `nonlocal implicit\ninverse' technique and a `time-reverse inverse' approach. We analyze the\ndifferences in the Lagrangian descriptors results due to the two backward-time\nintegration approaches, discuss the physical significance of these differences,\nand eventually argue that the nonlocal implicit inverse implementation of the\nGr\\\"unwald-Letnikov fractional derivative manages to reveal the phase space\nstructure of fractional-order dynamical systems correctly.",
        "In Bayesian inference, computing the posterior distribution from the data is\ntypically a non-trivial problem, which usually requires approximations such as\nmean-field approaches or numerical methods, like the Monte Carlo Markov Chain.\nBeing a high-dimensional distribution over a set of correlated variables, the\nposterior distribution can undergo the notorious replica symmetry breaking\ntransition. When it happens, several mean-field methods and virtually every\nMonte Carlo scheme can not provide a reasonable approximation to the posterior\nand its marginals. Replica symmetry is believed to be guaranteed whenever the\ndata is generated with known prior and likelihood distributions, namely under\nthe so-called Nishimori conditions. In this paper, we break this belief, by\nproviding a counter-example showing that, under the Nishimori conditions,\nreplica symmetry breaking arises. Introducing a simple, geometrical model that\ncan be thought of as a patient zero retrieval problem in a highly infectious\nregime of the epidemic Susceptible-Infectious model, we show that under the\nNishimori conditions, there is evidence of replica symmetry breaking. We\nachieve this result by computing the instability of the replica symmetric\ncavity method toward the one step replica symmetry broken phase. The origin of\nthis phenomenon -- replica symmetry breaking under the Nishimori conditions --\nis likely due to the correlated disorder appearing in the epidemic models.",
        "A modern smart factory runs a manufacturing procedure using a collection of\nprogrammable machines. Typically, materials are ferried between these machines\nusing a team of mobile robots. To embed a manufacturing procedure in a smart\nfactory, a factory operator must a) assign its processes to the smart factory's\nmachines and b) determine how agents should carry materials between machines. A\ngood embedding maximizes the smart factory's throughput; the rate at which it\noutputs products. Existing smart factory management systems solve the\naforementioned problems sequentially, limiting the throughput that they can\nachieve. In this paper we introduce ACES, the Anytime Cyclic Embedding Solver,\nthe first solver which jointly optimizes the assignment of processes to\nmachines and the assignment of paths to agents. We evaluate ACES and show that\nit can scale to real industrial scenarios.",
        "Analysing educational data sets is fundamental to many fields of research\nfocusing on improving student learning. However, large educational data sets\nare complex and can involve intensive preprocessing. These obstacles can be\novercome through the development of educational tools which simplifies the\npreprocessing stages of analysis. The Open University Learning Analytics\nDataset (OULAD), available online, contains data from 32,593 students across 22\nmodule presentations at the Open University. This paper introduces the R\nsoftware package ouladFormat; which loads and formats the OULAD for data\nanalysis. The paper summarizes the ouladFormat R package and explains the\ndifferent functions within the package. In addition, two case studies are\nprovided which discuss how the OULAD and ouladFormat R package could be used\nwhen preparing for an educational study, and in the early identification of\nat-risk students. The package increases the accessibility of the OULAD for\nresearchers, practitioners, and educators, and supports reproducibility and\ncomparability of educational studies.",
        "In confining large $N$ theories with a $\\theta$ angle such as\nfour-dimensional $\\rm{SU}(N)$ pure Yang-Mills theory, there are multiple\nmetastable vacua and it makes sense to consider the parameter region of ``large\n$\\theta$ of order $N$'' despite the fact that $\\theta$ is a $2\\pi$-periodic\nparameter. We investigate this parameter region in the two-dimensional\n$\\mathbb{CP}^{N-1}$ model by computing the partition function on $T^2$. When\n$\\theta\/N$ is of order $ \\mathcal{O}(0.1) $ or less, we get perfectly sensible\nresults for the vacuum energies and decay rates of metastable vacua. However,\nwhen $\\theta\/N$ is of order $\\mathcal{O}(1) $, we encounter a problem about\nsaddle points that would give larger contributions to the partition function\nthan the true vacuum. We discuss why it might not be straightforward to resolve\nthis problem.",
        "Recently, a surprising connection between algorithmic learning of algebraic\nstructures and descriptive set theory has emerged. Following this line of\nresearch, we define the learning power of an equivalence relation $E$ on a\ntopological space as the class of isomorphism relations with countably many\nequivalence classes that are continuously reducible to $E$. In this paper, we\ndescribe the learning power of the finite Friedman-Stanley jumps of\n$=_{\\mathbb{N}}$ and $=_{\\mathbb{N}^\\mathbb{N}}$, proving that these\nequivalence relations learn the families of countable structures that are\npairwise distinguished by suitable infinitary sentences. Our proof techniques\nintroduce new ideas for assessing the continuous complexity of Borel\nequivalence relations.",
        "Quasilinear systems with piecewise constant arguments of generalized type are\nunder investigation from the asymptotic point of view. The systems have\ndiscontinuous right-hand sides which are identified via a discrete-time map. It\nis rigorously proved that homoclinic and heteroclinic solutions are generated,\nand they are taken into account in the functional sense. The Banach fixed point\ntheorem is used for the verification. The hyperbolic set of solutions is also\ndiscussed, and an example supporting the theoretical findings is provided.",
        "A deep BSDE approach is presented for the pricing and delta-gamma hedging of\nhigh-dimensional Bermudan options, with applications in portfolio risk\nmanagement. Large portfolios of a mixture of multi-asset European and Bermudan\nderivatives are cast into the framework of discretely reflected BSDEs. This\nsystem is discretized by the One Step Malliavin scheme (Negyesi et al. [2024,\n2025]) of discretely reflected Markovian BSDEs, which involves a $\\Gamma$\nprocess, corresponding to second-order sensitivities of the associated option\nprices. The discretized system is solved by a neural network regression Monte\nCarlo method, efficiently for a large number of underlyings. The resulting\noption Deltas and Gammas are used to discretely rebalance the corresponding\nreplicating strategies. Numerical experiments are presented on both\nhigh-dimensional basket options and large portfolios consisting of multiple\noptions with varying early exercise rights, moneyness and volatility. These\nexamples demonstrate the robustness and accuracy of the method up to $100$ risk\nfactors. The resulting hedging strategies significantly outperform benchmark\nmethods both in the case of standard delta- and delta-gamma hedging.",
        "Background: Patient stratification in brain disorders remains a significant\nchallenge, despite advances in machine learning and multimodal neuroimaging.\nAutomated machine learning algorithms have been widely applied for identifying\npatient subtypes (biotypes), but results have been inconsistent across studies.\nThese inconsistencies are often attributed to algorithmic limitations, yet an\noverlooked factor may be the statistical properties of the input data. This\nstudy investigates the contribution of data patterns on algorithm performance\nby leveraging synthetic brain morphometry data as an exemplar.\n  Methods: Four widely used algorithms-SuStaIn, HYDRA, SmileGAN, and SurrealGAN\nwere evaluated using multiple synthetic pseudo-patient datasets designed to\ninclude varying numbers and sizes of clusters and degrees of complexity of\nmorphometric changes. Ground truth, representing predefined clusters, allowed\nfor the evaluation of performance accuracy across algorithms and datasets.\n  Results: SuStaIn failed to process datasets with more than 17 variables,\nhighlighting computational inefficiencies. HYDRA was able to perform\nindividual-level classification in multiple datasets with no clear pattern\nexplaining failures. SmileGAN and SurrealGAN outperformed other algorithms in\nidentifying variable-based disease patterns, but these patterns were not able\nto provide individual-level classification.\n  Conclusions: Dataset characteristics significantly influence algorithm\nperformance, often more than algorithmic design. The findings emphasize the\nneed for rigorous validation using synthetic data before real-world application\nand highlight the limitations of current clustering approaches in capturing the\nheterogeneity of brain disorders. These insights extend beyond neuroimaging and\nhave implications for machine learning applications in biomedical research.",
        "Networks of semiconducting single-walled carbon nanotubes (SWNTs) are a\npromising material for thermoelectric energy harvesting due to their mechanical\nflexibility, solution processability, high Seebeck coefficients and high\nelectrical conductivities after chemical p- or n-doping. Here, we demonstrate\nthat proton-coupled electron transfer (PCET) with benzoquinone (BQ) as the\noxidant and lithium bis(trifluoromethylsulfonyl)imide (Li[TFSI]) for\nelectrolyte counterions is a promising method for p-doping of polymer-sorted\nsemiconducting SWNT networks. The achieved doping levels, as determined from\nabsorption bleaching, depend directly on both the pH of the aqueous doping\nsolutions and the bandgap (i.e., diameter) of the nanotubes within the network.\nFast screening of different nanotube networks under various doping conditions\nwas enabled by a high-throughput setup for thermoelectric measurements of five\nsamples in parallel. For small-bandgap SWNTs, PCET-doping is sufficient to\nreach the maximum thermoelectric power factors, which are equal to those\nobtained by conventional methods. In contrast to other doping methods, the\nelectrical conductivity of PCET-doped SWNTs remains stable over at least 5 days\nin air. These results confirm PCET to be a suitable approach for more\nenvironmentally friendly and stable doping of semiconducting SWNTs as promising\nthermoelectric materials.",
        "Using data samples of 102 million Upsilon(1S) events and 158 million\nUpsilon(2S) events collected by the Belle detector at the KEKB\nasymmetric-energy $e^+e^-$ collider, we search for [udsccbar] pentaquark states\ndecaying to Jpsi Lambda. Using the first observations of Upsilon(1S, 2S)\ninclusive decays to Jpsi Lambda, we find evidence of the P_ccbars(4459)0 state\nwith a significance of 3.3 standard deviations, including statistical and\nsystematic uncertainties. We measure the mass and width of the Pccbars(4459)0\nto be (4471.7 +- 4.8 +- 0.6) MeV\/c2 and (21.9 +- 13.1 +- 2.7) MeV,\nrespectively. The branching fractions for P_ccbars(4459)0 production are\nmeasured to be B[Upsilon(1S) -> P_ccbars(4459)0\/ Pbar_ccbars(4459)0 + anything]\n= (3.5 +- 2.0 +- 0.2)*10-6 and B[Upsilin(2S) -> P_ccbars(4459)0\/\nPbar_ccbars(4459)0 +anything] = (2.9 +- 1.7 +- 0.4)*10-6. The inclusive\nbranching fractions of Upsilon(1S, 2S) -> Jpsi Lambda\/Lambdabar are measured to\nbe B[Upsilin(1S) -> Jpsi Lambda\/Lambdabar + anything] = (36.9 +- 5.3 +-\n2.4)*10-6 and B[Upsilon(2S) -> Jpsi Lambda\/Lambdabar + anything] = (22.3 +- 5.7\n+- 3.1)*10-6. We measure the visible cross section $\\sigma(e^+e^- \\to J\/psi\n\\Lambda\/\\bar\\Lambda$ + anything) = (90 +- 14 +- 6) fb for the continuum\nproduction at $\\sqrt{s} = 10.52$ GeV. In all cases, the first uncertainties are\nstatistical and the second are systematic.",
        "Fast radio bursts (FRBs) are fierce radio flashes lasting for a few\nmilliseconds from the sky. Although their connection to strongly magnetized\nneutron stars has been strongly indicated, the exact triggering process and\nradiation mechanism are still unknown and highly debated. Due to their\nextremely short duration, the observation of FRBs has long been a difficult\ntask even for large radio telescopes. The difficulty results from the fact that\nthe information obtained in observations is always incomplete, since the\ntelescope always has a limited flux sensitivity and finite operating frequency\nband. A pressing challenge is to decode the intrinsic features of FRBs from the\nincomplete observations. Here we establish an efficient methodology to overcome\nthis problem, aiming at effectively correcting for the fluence and frequency\ncutoffs. Using this method, inverse modeling is performed on a large number of\nrepeating bursts from FRB 20121102A to recover their intrinsic features. It is\nfound that strong bursts intrinsically tend to concentrate their energy in a\nnarrow band, while the spectral range of weak bursts can be either narrow or\nwide. However, when a weak burst has a broad spectrum, the wing of the spectrum\ncan easily go undetected, resulting in a very narrow spectrum being observed.\nThe narrow spectrum features observed in repeating FRBs are thus an\nobservational selection effect. Underestimation of the burst energy caused by\nobservational cutoffs is also corrected for, and the intrinsic burst energy\ndistribution is re-constructed. It is also found that the bandwidth increases\nwith the increasing central frequency in the Arecibo sample (1.15-1.73 GHz),\nbut such a correlation is not observed in the FAST (1-1.5 GHz) and GBT (4-8\nGHz) sample. It indicates the emission pattern of the FRB source might vary\nacross different active periods and frequency bands.",
        "Using 20.3$~\\rm fb^{-1}$ of $e^+e^-$ annihilation data collected at a\ncenter-of-mass energy of 3.773$~\\rm GeV$ with the BESIII detector, we report an\nimproved search for the radiative leptonic decay $D^+\\to\\gamma e^+\\nu_e$. An\nupper limit on its partial branching fraction for photon energies\n$E_\\gamma>10~\\rm MeV$ is determined to be $1.2\\times10^{-5}$ at 90\\% confidence\nlevel, which excludes most current theoretical predictions. A sophisticated\ndeep learning approach with thorough validation, based on the Transformer\narchitecture, is implemented to efficiently distinguish the signal from massive\nbackgrounds.",
        "Controlling\/storing information carriers, such as electron charge and spin,\nis key for modern information society, and significant efforts have been paid\nmade to establish novel technologies at the nanoscale. The rise of Si-based\nsemiconductor technology and magnetism-based technology has been motivated by\nthe aforementioned demands. However, both technologies have been individually\ndeveloped, with little effort in fusing them. Hence, establishing a technology\nto bridge semiconductor and magnetism-based technologies that would allow\nrealization of a novel information device is strongly awaited. In line with\nthis research strategy, the creation of a magnetic device using semiconductors\nwould enable fundamental innovation. Here, we show that a mother material for\nmodern electronics, Si, gives rise to a giant room-temperature orbital Hall\neffect (OHE), enabling the creation of novel energy-efficient magnetic memory\nvia efficient torque generation. The orbital torque efficiency largely exceeds\nthat of the archetypal metallic materials used in the OHE. Our achievement\novertures the conventional understanding that nonmagnetic semiconductors cannot\nplay a pivotal role in magnetic devices and paves a new avenue for creating\nnovel information devices through the fusion of semiconductor and\nmagnetism-based technologies.",
        "This paper focuses on the best approximation in quasi-cone metric spaces, a\ncombination of quasi-metrics and cone metrics, which generalizes the notion of\ndistance by allowing it to take values in an ordered Banach space. We explore\nthe fundamental properties of best approximations in this setting, such as the\nbest approximation sets and the Chebyshev sets."
      ]
    }
  }
]