[
  {
    "id":2411.1945,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"Auto-Encoding Variational Bayes",
    "start_abstract":"How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining"
      ],
      "abstract":[
        "It is our great pleasure to welcome you the 2017 ACM Conference on Knowledge Discovery and Data Mining -- KDD 2017. We hope that content professional networking opportunities at will help succeed professionally by enabling to: identify new technology trends; learn from contributed papers, presentations, posters; discover tools, processes practices; job opportunities; hire team members. The terms \"Data Science\", Mining\" \"Big Data\" have, in last few years, grown out of research labs gained presence media everyday conversations. also hear these social decision makers various level governments corporations. impact technologies felt almost every walk life. Importantly, current rapid progress data science facilitated timely sharing newly discovered developed representations algorithms between those working interested industrial deployment. hallmark conferences past they have been bridge theory practise, facilitator catalyst for this exchange. Researchers practitioners meet person interact a meaningful way over several days. conference program, with its three parallel tracks - Research Track, Applied Science Track Invited Speakers brings two groups together. Participants are freely attend any track, events common all tracks. year continues tradition strong tutorial workshop program leading edge issues mining during first days program. devoted technical describing both novel, important contributions, deployed, innovative solutions. Three keynote talks, Cynthia Dwork, Bin Yu, Ren\u00e9e J. Miller touch some hard, emerging before field mining. With growing industry around AI assistants, Panel together experts spawn discussions an exchanges ideas. outstanding lineup speakers their experiences expertise deploying continue hands-on which participants how use practical tools. In order broaden increase participation attendees who would greatly benefit but otherwise found it financially challenging attend, we reserved substantial budget travel grants. awarded record USD 145k student set aside 25k enable smaller startups attend. \"Meet Experts\" sessions, gives researchers unique opportunity form networks share perspectives others aspects science. serve as meeting ground researchers, practitioners, funding agencies investors create commercial products."
      ],
      "categories":[
        "physics.gen-ph"
      ]
    },
    "list":{
      "title":[
        "Introduction of the G$_2$-Ricci Flow: Geometric Implications for\n  Spontaneous Symmetry Breaking and Gauge Boson Masses",
        "Preons, Braid Topology, and Representations of Fundamental Particles",
        "Reversible Imprinting and Retrieval of Quantum Information: Experimental\n  Verification of the Quantum Memory Matrix Hypothesis",
        "A New Method for Calculating the Energies Associated with Particle\n  Reactions",
        "New entropy, thermodynamics of apparent horizon and cosmology",
        "Cylindrical gravastars with Kuchowicz metric potential",
        "Exploring quaternion framework for subluminal to superluminal space\n  transformations in particle dynamics",
        "Ten Equations that Shook the Quantum World: Bose-Einstein Condensation,\n  Superfluidity, and the Quantum-Classical Transition",
        "Scattering problem for the valence electron model potential",
        "Vacuum permittivity and gravitational refractive index revisited",
        "Impossible Galaxies, the Hubble Tension and the Ricci soliton miracle,",
        "What Exactly is Antimatter (Gravitationally Speaking)? A Second Scenario",
        "Local Quantum Mechanical Prediction of the Singlet State",
        "Isometric Gelfand transforms of complete Nevanlinna-Pick spaces",
        "On extensivity of morphisms",
        "On Branch-and-Price for Project Scheduling",
        "Extreme Shape Coexistence Observed in $^{70}$Co",
        "Learning Memory and Material Dependent Constitutive Laws",
        "Exploring the Technology Landscape through Topic Modeling, Expert\n  Involvement, and Reinforcement Learning",
        "Models for the Eremenko-Lyubich class",
        "Subcode Ensemble Decoding of Linear Block Codes",
        "How does non-metricity affect particle creation and evaporation in\n  bumblebee gravity?",
        "A mesh-free hybrid Chebyshev-Tucker tensor format with applications to\n  multi-particle modelling",
        "Investigating Parameter-Efficiency of Hybrid QuGANs Based on Geometric\n  Properties of Generated Sea Route Graphs",
        "Complex potential and open system applications in heavy-ions and cold\n  atoms",
        "A Pristine-UNIONS view on the Galaxy: Kinematics of the distant spur\n  feature of the Sagittarius stream traced by Blue Horizontal Branch stars",
        "Implementation and verification of coherent error suppression using\n  randomized compiling for Grover's algorithm on a trapped-ion device",
        "Dynkin Systems and the One-Point Geometry"
      ],
      "abstract":[
        "This work introduces the G$_2$-Ricci flow on seven-dimensional manifolds with\nnon-zero torsion and explores its physical implications. By extending the Ricci\nflow to manifolds with G$_2$ structures, we study the evolution of solitonic\nsolutions and their role in spontaneous symmetry breaking in gauge theories. In\nparticular, this model proposes that the masses of the W and Z bosons are\ndetermined not by an external scalar field, as in the Higgs mechanism, but by\nthe intrinsic geometric torsion of the manifold. Furthermore, a possible\nconnection between the geometry of extra dimensions and the curvature of our\nspacetime is explored, with implications for the experimentally observed\npositive cosmological constant. This approach provides an innovative\ninterpretation of fundamental interactions in theoretical physics, opening new\npossibilities for studying extra dimensions and the geometry of\nG$_2$-manifolds.",
        "In particle phenomenology, preon models study compositional rules of standard\nmodel interactions. In spite of empirical success, mathematical underpinnings\nof preon models in terms of group representation theory have not been fully\nworked out. Here, we address this issue while clarifying the relation between\ndifferent preon models. In particular, we focus on two prominent models:\nBilson-Thompson's helon model, and Lambek's 4-vector model. We determine the\nmapping between helon model particle states and representation theory of Lie\nalgebras. Braided ribbon diagrams of the former represent on-shell states of\nspinors of the Lorentz group. Braids correspond to chirality, and twists, to\ncharges. We note that this model captures only the $SU(3)_c\\times U(1)_{em}$\nsector of the standard model. We then map the twists of helon diagrams to the\nweight polytope of $SU(3)_c \\times U(1)_{em}$. The braid structure maps to\nchiral states of fermions. We also show that Lambek's 4-vector can be recovered\nfrom helon diagrams. Alongside, we introduce a new 5-vector representation\nderived from the weight lattice. This representation contains both, the correct\ninteractions found in 4-vectors and the inclusion of chirality found in helons.\nAdditionally, we demonstrate topological analogues of CPT transformations in\nhelon diagrams. Interestingly, the braid diagrams of the helon model are the\nonly ones that are self-consistent with CPT invariance. In contrast to\nfield-theoretic approaches, the compositional character of preon models offers\nan analogous particle-centric perspective on fundamental interactions.",
        "We present a series of quantum computing experiments designed to test a\ncentral prediction of the Quantum Memory Matrix (QMM) hypothesis - that quantum\ninformation can be locally stored in finite-dimensional cells of space-time and\nlater retrieved in a fully unitary and reversible manner. Our work encompasses\nfive distinct experiments: a basic three-qubit imprint-retrieval cycle, an\nextended five-qubit model implementing two parallel cycles, and variations\nincorporating dynamic evolution and controlled error injection. In each case, a\nfield qubit is prepared in an arbitrary superposition, and its state is\nimprinted onto memory qubit(s) via controlled-R_y gates, with subsequent\ncontrolled-SWAP operations retrieving the stored information into output\nqubit(s). Execution on an IBM Quantum Processing Unit using the Qiskit Runtime\nservice yielded significant correlations between the initially prepared field\nstates and the retrieved outputs, with fidelities that, while subject to\nhardware noise and decoherence, consistently demonstrate the reversible and\nunitary nature of the process. These results not only confirm the basic\nimprint-retrieval cycle as predicted by the QMM hypothesis but also establish a\nscalable experimental methodology that may ultimately contribute to resolving\nchallenges such as the black hole information paradox and advancing our\nunderstanding of quantum gravity.",
        "A new method in which the energy and mass of elementary particles can be\ncalculated is presented. Gluon gluon interactions within a single elementary\nparticle are considered, and the number of possible interactions per particle\nis determined. This procedure can be formalized and standardized via a newly\nintroduced microcanonical partition function. The possibility of calculating\nthe energetic relationships provides new and more in-depth insights into the\nreaction possibilities of the particles. In addition, the application of this\nmethod of calculating the partition function to the known quarks themselves\nsuggests that they are composed of even more elementary particles. The\nproperties of these particles match the Rishons of the Harari Shupe preon\nmodel. In combination with the Rishon model, the method presented here for\ncalculating the energetic situation of particle reactions provides a profound\nand new understanding of the processes at an absolute elementary level; this\nopens new possibilities for the calculation and understanding of particle\nreactions and might change our understanding of particle physics in fundamental\nways.",
        "Here, we consider new nonadditive entropy of the apparent horizon\n$S_K=S_{BH}\/(1+\\gamma S_{BH})$ with $S_{BH}$ being the Bekenstein--Hawking\nentropy. This is an alternative of the R\\'{e}nyi and Tsallis entropies, that\nallow us by utilizing the holographic principle to develop of a new model of\nholographic dark energy. When $\\gamma\\rightarrow 0$ our entropy becomes the\nBekenstein--Hawking entropy $S_{BH}$. The generalized Friedmann's equations for\nFriedmann--Lema\\^{i}tre--Robertson--Walker (FLRW) space-time for the barotropic\nmatter fluid with $p=w\\rho$ were obtained. We compute the dark energy pressure\n$p_D$, density energy $\\rho_D$ and the deceleration parameter corresponding to\nour model. From the second modified Friedmann's equation a dynamical\ncosmological constant was obtained. We show that at some model parameters $w$\nand $\\beta$ there are two phases, universe acceleration and deceleration or the\nphase of the eternal inflation. Thus, our model, by virtue of the holographic\nprinciple, can describe the universe inflation and late time of the universe\nacceleration. The holographic dark energy model with the generalized entropy of\nthe apparent horizon can be of interest for new cosmology.",
        "Mazur and Mottola's gravastar model represents one of the few serious\nalternatives to the traditional understanding of the black hole. The gravastar\nis typically regarded as a theoretical alternative for the black hole. This\narticle investiagtes the creation of gravastar(gravitational vacuum star)\nwithin the realm of cylindrically symmetric space-time utilizing the Kuchowicz\nmetric potential. A stable gravastar comprises of three distinct regions,\nstarting with an interior region marked by positive energy density and negative\npressure $(p=-\\rho)$ which is followed by an intermediate thin shell, where the\ninterior negative pressure induces a outward repulsive force at each point on\nthe shell. Ultra-relativistic stiff fluid makes up the thin shell governed by\nthe equation of state(EoS) $(p=\\rho)$, which meets the Zel'dovich criteria. And\nthen comes the region exterior to it which is total vacuum. In this scenario,\nthe central singularity is eliminated and the event horizon is effectively\nsubstituted by the thin bounding shell. Employing the Kuchowicz metric\npotential we have derived the remaining metric functions for the interior\nregion and the shell regions yielding a non-singular solution for both the\nregions. Additionally, we have investigated various characteristics of this\nshell region including its proper shell length, the energy content and entropy.\nThis theoretical model successfully resolves the singularity issue inherent to\nthe black holes. Therefore, this gravastar model presents a viable alternative\nto the traditional black holes, reconciled within the context of Einstein's\ntheory of General Relativity.",
        "The present study explores the behavior of quaternionic four-space algebra\nfor subluminal and superluminal spaces. We formulate the generalized Lorentz\ntransformations for quaternionic subluminal, superluminal, and their combined\nMinkowski spaces. Furthermore, we have studied the relativistic phenomenon of\nquaternionic length contraction, time dilation, velocity addition, and the\nDoppler effect for the combination of subluminal and superluminal space. We\nclaim that the transformation between two superluminal spaces is ultimately a\nsubluminal space; the tachyonic behavior reveals itself in the consequences\nwhen the two frames are in different spaces (i.e., generalized\nsubluminal-superluminal spaces).",
        "The transition from the quantum to the classical world and its relation to\nBose-Einstein condensation and superfluidity is explained in ten equations.",
        "In the paper, in the scattering problem for the valence electron model\npotential a self-adjoint extension is performed and Rutherford formula is\nmodified. The scattering of slow particles for this potential is also discussed\nand the changes caused by the self-adjoint extension in the differential and\nintegral cross-sections of the scattering are studied.",
        "The present paper reanalyzes the problem of the refractive properties of the\nphysical vacuum and their modification under the action of the gravitational\nfield and the electromagnetic field. This problem was studied in our previous\nworks and in the subsequent works of the researchers: Leuchs, Urban, Mainland\nand their collaborators. By modeling the physical vacuum as a\nparticle-antiparticle system, we can deduce with a certain approximation, in a\nsemiclassical theory, the properties of the free vacuum and the vacuum modified\nby the interaction with a gravitational field and an electromagnetic field.\nMore precise calculation of permittivities of free vacuum and near a particle\ncan lead to a non-point model of the particle. This modeling can follow both\nthe quantum and the general relativistic path as well as the phenomenological\npath, the results complementing each other.",
        "The standard model of cosmology has begun to show signs of internal\ninconsistencies under the relentless onslaught of precision data from the James\nWebb Telescope (JWST), the Hubble Telescope (HST) and other space-based\nobservation platforms as well as those from the ground. The most statistically\nsignificant tension, the so-called Hubble Tension currently stands at between 4\n{\\sigma} to 6 {\\sigma} statistical significance. The JWST high resolution\nimages reveal mature and sometimes quenched galaxies at high redshifts (z>6)\nwhich standard cosmological models have failed to both predict and explain.\nThis is the impossible galaxies problem. These short comings of the standard\nmodel of cosmology have led to a plethora of new models attempting to explain\nthe inconsistencies between theory and observation. Thus far, none of them\nadequately address these tensions. In this article, we demonstrate that by\nincluding a Ricci soliton into Einstein's field equations both tensions can be\nadequately resolved. The Ricci soliton is sourced from gravitational energy.",
        "In arXiv:2401.10954 I investigated the consequences of regarding the\nmass-energy of the fundamental fermions (quarks and leptons) and the\nIntermediate Vector Bosons (e.g., photon) as matter, and the fundamental\nantifermions (antiquarks and antileptons) as antimatter within the context of\nan antigravity universe, one where matter and antimatter repel gravitationally.\nHere I consider an alternative scenario in which the Intermediate Vector\nBosons, which are neither particle nor antiparticle, are gravitationally\nattracted to both fundamental fermions and antifermions. This leads to a\nprediction for the free-fall acceleration of antihydrogen of\n$a_{\\bar{H}}=(0.78^{+0.11}_{-0.08})g$ (and most certainly less than $g$) as\nwell as quite different expectations for the free-fall accelerations of the\n$\\mu^+$ and positronium from those derived in arXiv:2401.10954. The cosmology\nwhich results from the premise presented here is little different from the\nstandard cosmology (i.e., the $\\Lambda$CDM model). One significant deviation is\nthat there would be an increased accelerated expansion in the early moments\nafter the Big Bang due to the gravitational repulsion between the fundamental\nfermions and antifermions.",
        "We deduce the quantum mechanical prediction of $-{\\bf a}\\cdot{\\bf b}$ for the\nsinglet spin state employing local measurement functions following Bell's\napproach. Our derivation is corroborated through a computational simulation\nconducted via the Mathematica programming environment.",
        "We show that any complete Nevanlinna-Pick space whose multiplier algebra has\nisometric Gelfand transform (or commutative C*-envelope) is essentially the\nHardy space on the open unit disk.",
        "Extensivity of a category may be described as a property of coproducts in the\ncategory, namely, that they are disjoint and universal. An alternative\nviewpoint is that it is a property of morphisms in a category. This paper\nexplores this point of view through a natural notion of extensive and\ncoextensive morphism. Through these notions, topics in universal algebra, such\nas the strict refinement and Fraser-Horn properties, take categorical form and\nthereby enjoy the benefits of categorical generalisation. On the other hand,\nthe universal algebraic theory surrounding these topics inspire categorical\nresults. One such result we establish in this paper is that a Barr-exact\ncategory is coextensive if and only if every split monomorphism in the category\nis coextensive.",
        "Integer programs for resource-constrained project scheduling problems are\nnotoriously hard to solve due to their weak linear relaxations. Several papers\nhave proposed reformulating project scheduling problems via Dantzig-Wolfe\ndecomposition to strengthen their linear relaxation and decompose large problem\ninstances. The reformulation gives rise to a master problem that has a large\nnumber of variables. Therefore, the master problem is solved by a column\ngeneration procedure embedded in a branching framework, also known as\nbranch-and-price. While branch-and-price has been successfully applied to many\nproblem classes, it turns out to be ineffective for most project scheduling\nproblems. This paper identifies drivers of the ineffectiveness by analyzing the\nstructure of the reformulated problem and the strength of different branching\nschemes. Our analysis shows that the reformulated problem has an unfavorable\nstructure for column generation: It is highly degenerate, slowing down the\nconvergence of column generation, and for many project scheduling problems, it\nyields the same or only slightly stronger linear relaxations as classical\nformulations at the expense of large increases in runtime. Our computational\nexperiments complement our theoretical findings.",
        "The shape of the atomic nucleus is a property which underpins our\nunderstanding of nuclear systems, impacts the limits of nuclear existence, and\nenables probes of physics beyond the Standard Model. Nuclei can adopt a variety\nof shapes, including spheres, axially deformed spheroids, and pear shapes. In\nsome regions of the nuclear chart where a spherical nucleus would naively be\nexpected, deformed nuclear states can result from collective action of\nconstituent protons and neutrons. In a small subset of nuclei both spherical\nand deformed nuclear states have been experimentally observed, a phenomenon\ntermed shape coexistence. We present spectroscopic evidence for the coexistence\nof $J^{\\pi}=1+$ spherical and deformed states in $^{70}$Co, separated by less\nthan 275~keV. This close degeneracy of levels with the same $J^{\\pi}$ and\ndifferent shapes demonstrates an extreme example of shape coexistence resulting\nfrom the interplay of independent particle motion and collective behavior in\nhighly unstable nuclear systems and identifies the Co isotopes as a transition\npoint between deformed ground states observed in the Cr isotopes and spherical\nconfigurations observed in the closed-shell Ni isotopes.",
        "The theory of homogenization provides a systematic approach to the derivation\nof macroscale constitutive laws, obviating the need to repeatedly resolve\ncomplex microstructure. However, the unit cell problem that defines the\nconstitutive model is typically not amenable to explicit evaluation. It is\ntherefore of interest to learn constitutive models from data generated by the\nunit cell problem. Many viscoelastic and elastoviscoplastic materials are\ncharacterized by memory-dependent constitutive laws. In order to amortize the\ncomputational investment in finding such memory-dependent constitutive laws, it\nis desirable to learn their dependence on the material microstructure. While\nprior work has addressed learning memory dependence and material dependence\nseparately, their joint learning has not been considered. This paper focuses on\nthe joint learning problem and proposes a novel neural operator framework to\naddress it.\n  In order to provide firm foundations, the homogenization problem for linear\nKelvin-Voigt viscoelastic materials is studied. The theoretical properties of\nthe cell problem in this Kelvin-Voigt setting are used to motivate the proposed\ngeneral neural operator framework; these theoretical properties are also used\nto prove a universal approximation theorem for the learned macroscale\nconstitutive model. This formulation of learnable constitutive models is then\ndeployed beyond the Kelvin-Voigt setting. Numerical experiments are presented\nshowing that the resulting data-driven methodology accurately learns history-\nand microstructure-dependent linear viscoelastic and nonlinear\nelastoviscoplastic constitutive models, and numerical results also demonstrate\nthat the resulting constitutive models can be deployed in macroscale simulation\nof material deformation.",
        "In today's rapidly evolving technological landscape, organizations face the\nchallenge of integrating external insights into their decision-making processes\nto stay competitive. To address this issue, this study proposes a method that\ncombines topic modeling, expert knowledge inputs, and reinforcement learning\n(RL) to enhance the detection of technological changes. The method has four\nmain steps: (1) Build a relevant topic model, starting with textual data like\ndocuments and reports to find key themes. (2) Create aspect-based topic models.\nExperts use curated keywords to build models that showcase key domain-specific\naspects. (3) Iterative analysis and RL driven refinement: We examine metrics\nsuch as topic magnitude, similarity, entropy shifts, and how models change over\ntime. We optimize topic selection with RL. Our reward function balances the\ndiversity and similarity of the topics. (4) Synthesis and operational\nintegration: Each iteration provides insights. In the final phase, the experts\ncheck these insights and reach new conclusions. These conclusions are designed\nfor use in the firm's operational processes. The application is tested by\nforecasting trends in quantum communication. Results demonstrate the method's\neffectiveness in identifying, ranking, and tracking trends that align with\nexpert input, providing a robust tool for exploring evolving technological\nlandscapes. This research offers a scalable and adaptive solution for\norganizations to make informed strategic decisions in dynamic environments.",
        "If $f$ is in the Eremenko-Lyubich class (transcendental entire functions with\nbounded singular set) then $\\Omega= \\{ z: |f(z)| > R\\}$ and $f|_\\Omega$ must\nsatisfy certain simple topological conditions when $R$ is sufficiently large. A\nmodel $(\\Omega, F)$ is an open set $\\Omega$ and a holomorphic function $F$ on\n$\\Omega$ that satisfy these same conditions. We show that any model can be\napproximated by an Eremenko-Lyubich function in a precise sense. In many cases,\nthis allows the construction of functions in the Eremenko-Lyubich with a\ndesired property to be reduced to the construction of a model with that\nproperty, and this is often much easier to do.",
        "Low-density parity-check (LDPC) codes together with belief propagation (BP)\ndecoding yield exceptional error correction capabilities in the large block\nlength regime. Yet, there remains a gap between BP decoding and maximum\nlikelihood decoding for short block length LDPC codes. In this context,\nensemble decoding schemes yield both reduced latency and good error rates. In\nthis paper, we propose subcode ensemble decoding (SCED), which employs an\nensemble of decodings on different subcodes of the code. To ensure that all\ncodewords are decodable, we use the concept of linear coverings and explore\napproaches for sampling suitable ensembles for short block length LDPC codes.\nMonte-Carlo simulations conducted for three LDPC codes demonstrate that SCED\nimproves decoding performance compared to stand-alone decoding and automorphism\nensemble decoding. In particular, in contrast to existing schemes, e.g.,\nmultiple bases belief propagation and automorphism ensemble decoding, SCED does\nnot require the NP-complete search for low-weight dual codewords or knowledge\nof the automorphism group of the code, which is often unknown.",
        "In this work, we analyze the impact of non-metricity on particle creation and\nthe evaporation process of black holes within the framework of bumblebee\ngravity. In general lines, we compare black holes in the metric formalism [1]\nand the metric-affine approach [2]. Initially, we focus on bosonic particle\nmodes to investigate Hawking radiation. Using the Klein-Gordon equation, we\ncompute the Bogoliubov coefficients and derive the Hawking temperature.\nSubsequently, we examine Hawking radiation as a tunneling process, resolving\ndivergent integrals through the residue method. The analysis is then extended\nto fermionic particle modes, also within the tunneling framework. Particle\ncreation densities are calculated for both bosonic and fermionic cases.\nAdditionally, greybody bounds are estimated for bosonic and fermionic\nparticles. Finally, we explore the evaporation process, considering the final\nstate of the black holes. In a general panorama, non-metricity in bumblebee\ngravity raises particle density for bosons while reducing it for fermions,\nincreases greybody factors (for both bosons and fermions), amplifies the\nemission rate, and accelerates the evaporation process.",
        "In this paper, we introduce a mesh-free two-level hybrid Tucker tensor format\nfor approximation of multivariate functions, which combines the product\nChebyshev interpolation with the ALS-based Tucker decomposition of the tensor\nof Chebyshev coefficients. It allows to avoid the expenses of the\nrank-structured approximation of function-related tensors defined on large\nspacial grids, while benefiting from the Tucker decomposition of the rather\nsmall core tensor of Chebyshev coefficients. This leads to nearly optimal\nTucker rank parameters which are close to the results for well established\nTucker-ALS algorithm applied to the large grid-based tensors. These rank\nparameters inherited from the Tucker-ALS decomposition of the coefficient\ntensor can be much less than the polynomial degrees of the initial Chebyshev\ninterpolant via function independent basis set. Furthermore, the tensor product\nChebyshev polynomials discretized on a tensor grid leads to a low-rank\ntwo-level orthogonal algebraic Tucker tensor that approximates the initial\nfunction with controllable accuracy. It is shown that our techniques could be\ngainfully applied to the long-range part of the electrostatic potential of\nmulti-particle systems approximated in the range-separated tensor format. Error\nand complexity estimates of the proposed methods are presented. We demonstrate\nthe efficiency of the suggested method numerically on examples of the\nlong-range components of multi-particle interaction potentials generated by 3D\nNewton kernel for large bio-molecule systems and lattice-type compounds.",
        "The demand for artificially generated data for the development, training and\ntesting of new algorithms is omnipresent. Quantum computing (QC), does offer\nthe hope that its inherent probabilistic functionality can be utilised in this\nfield of generative artificial intelligence. In this study, we use\nquantum-classical hybrid generative adversarial networks (QuGANs) to\nartificially generate graphs of shipping routes. We create a training dataset\nbased on real shipping data and investigate to what extent QuGANs are able to\nlearn and reproduce inherent distributions and geometric features of this data.\nWe compare hybrid QuGANs with classical Generative Adversarial Networks (GANs),\nwith a special focus on their parameter efficiency. Our results indicate that\nQuGANs are indeed able to quickly learn and represent underlying geometric\nproperties and distributions, although they seem to have difficulties in\nintroducing variance into the sampled data. Compared to classical GANs of\ngreater size, measured in the number of parameters used, some QuGANs show\nsimilar result quality. Our reference to concrete use cases, such as the\ngeneration of shipping data, provides an illustrative example and demonstrate\nthe potential and diversity in which QC can be used.",
        "Since the discovery of the complex potential of quarkonium at high\ntemperatures, quarkonium has been regarded as an open quantum system in the\nquark-gluon plasma. Recently, a similar issue regarding in-medium bound states\nof impurities has also emerged in particle physics and cold atomic physics. We\nwill provide an overview of recent advancements in understanding key quantities\nsuch as complex potential and transport coefficients for heavy impurities in\nfinite temperature QCD and cold atomic systems.",
        "Providing a detailed picture of the Sagittarius (Sgr) stream offers important\nconstraints on the build-up of the Galactic halo as well as its gravitational\npotential at large radii. While several attempts have been made to model the\nstructure of the Sgr stream, no model has yet been able to match all the\nfeatures observed for the stream. Moreover, for several of these features,\nobservational characterisation of their properties is rather limited,\nparticularly at large distances. The aim of this work is to investigate the\nkinematics of the Sgr stream outermost spur feature using blue horizontal\nbranch (BHB) stars. Candidate BHB stars were selected by combining two\napproaches; one capitalising on Pan-STARRS1 3$\\Pi$ griz and u photometry taken\nas part of UNIONS, the other using Pristine Survey CaHK and SDSS ugr\nphotometry. Follow-up optical spectra are obtained using ESO\/VLT\/FORS2 to\nconfirm their BHB nature and obtain line-of-sight (LOS) velocities. Of our 25\ncandidates, 20 stars can be confirmed as bona fide BHB stars. Their LOS\nvelocities, together with the 3D positions of these stars qualitatively match\nwell with Sgr model predictions and trace the outer apocentre of the trailing\narm and its spur feature very nicely. The quantitative offsets that are found\nbetween our data and the different models can be used to provide information\nabout the Galactic gravitational potential at large distances. We present a\nfirst, tentative, analysis in this direction, showing that the model of\nVasiliev et al. (2021) would provide better agreement with our observations if\nthe enclosed mass of the Milky Way within 100 kpc were lowered to\n$(5.3\\!\\pm\\!0.4)\\!\\times\\!10^{11}$ M$_\\odot$ (versus\n$(5.6\\!\\pm\\!0.4)\\!\\times\\!10^{11}$ M$_\\odot$). Our selection of BHB stars\nprovides a new view on the outermost structure in 3D positions and LOS\nvelocities of the Sgr debris.",
        "In near-term quantum computations that do not employ error correction, noise\ncan proliferate rapidly, corrupting the quantum state and making results\nunreliable. These errors originate from both decoherence and control\nimprecision. The latter can manifest as coherent noise that is especially\ndetrimental. Here, we study the impact of coherent errors and their mitigation\nunder standard error-reduction techniques, both theoretically and\nexperimentally on a trapped-ion quantum computer. As a representative case\nstudy, we implement a range of Grover's algorithm circuits containing up to 10\nqubits and 26 two-qubit gates. We demonstrate the effectiveness of randomized\ncompiling (RC) and algorithm error detection (ED), where the latter is realized\nvia post-selection on ancillary qubits that ideally return to the ground state\nat the end of each circuit. Our results highlight a synergetic effect:\ncombining RC and ED yields the largest reductions in errors, indicating that\nthese methods can work together to extend the capabilities of near-term quantum\ndevices for moderately deep circuits.",
        "In this note I demonstrate that the collection of Dynkin systems on finite\nsets assembles into a Connes-Consani $\\mathbb{F}_1$-module, with the collection\nof partitions of finite sets as a sub-module. The underlying simplicial set of\nthis $\\mathbb{F}_1$-module is shown to be isomorphic to the delooping of the\nKrasner hyperfield $\\mathbb{K}$, where $1+1=\\{0,1\\}$. The face and degeneracy\nmaps of the underlying simplicial set of the $\\mathbb{F}_1$-module of\npartitions correspond to merging partition blocks and introducing singleton\nblocks, respectively. I also show that the $\\mathbb{F}_1$-module of partitions\ncannot correspond to a set with a binary operation (even partially defined or\nmultivalued) under the ``Eilenberg-MacLane'' embedding. These results imply\nthat the $n$-fold sum of the Dynkin $\\mathbb{F}_1$-module with itself is\nisomorphic to the $\\mathbb{F}_1$-module of the discrete projective geometry on\n$n$ points."
      ]
    }
  },
  {
    "id":2412.09927,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Three-dimensional nanomagnetism",
    "start_abstract":"Magnetic nanostructures are being developed for use in many aspects of our daily life, spanning areas such as data storage, sensing and biomedicine. Whereas patterned nanomagnets traditionally two-dimensional planar structures, recent work is expanding nanomagnetism into three dimensions; a move triggered by the advance unconventional synthesis methods discovery new magnetic effects. In three-dimensional more complex configurations become possible, with unprecedented properties. Here we review creation these structures their implications emergence physics, development instrumentation computational methods, exploitation numerous applications. Nanoscale devices play key role modern technologies but current applications involve only 2D like discs. authors progress fabrication understanding 3D nanostructures, enabling diverse functionalities.",
    "start_categories":[
      "cond-mat.mes-hall"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b15"
      ],
      "title":[
        "MagNet: machine learning enhanced three-dimensional magnetic reconstruction"
      ],
      "abstract":[
        "Three-dimensional (3D) magnetic reconstruction is vital to the study of novel materials for 3D spintronics. Vector field electron tomography (VFET) a major in house tool achieve that. However, conventional VFET exhibits significant artefacts due unavoidable presence missing wedges. In this article, we propose deep-learning enhanced method address issue. A textures library built by micromagnetic simulations. MagNet, an U-shaped convolutional neural network, trained and tested with dataset generated from library. We demonstrate that MagNet outperforms under wedge. Quality reconstructed induction fields significantly improved."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "BANet: Bilateral Aggregation Network for Mobile Stereo Matching",
        "SNN-Driven Multimodal Human Action Recognition via Event Camera and\n  Skeleton Data Fusion",
        "Tracking Mouse from Incomplete Body-Part Observations and Deep-Learned\n  Deformable-Mouse Model Motion-Track Constraint for Behavior Analysis",
        "GCP: Guarded Collaborative Perception with Spatial-Temporal Aware\n  Malicious Agent Detection",
        "MarkushGrapher: Joint Visual and Textual Recognition of Markush\n  Structures",
        "Generative Human Geometry Distribution",
        "High-Fidelity Novel View Synthesis via Splatting-Guided Diffusion",
        "Online Dense Point Tracking with Streaming Memory",
        "UniVG: A Generalist Diffusion Model for Unified Image Generation and\n  Editing",
        "MaterialMVP: Illumination-Invariant Material Generation via Multi-view\n  PBR Diffusion",
        "Debiased Prompt Tuning in Vision-Language Model without Annotations",
        "Exploring Transferable Homogeneous Groups for Compositional Zero-Shot\n  Learning",
        "Dynamic watermarks in images generated by diffusion models",
        "The Factorizable Feigin-Frenkel center",
        "A BERT Based Hybrid Recommendation System For Academic Collaboration",
        "Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding\n  and Expert Reasoning Abilities",
        "Towards Robust and Secure Embodied AI: A Survey on Vulnerabilities and\n  Attacks",
        "Quantum Feature-Empowered Deep Classification for Fast Mangrove Mapping",
        "Antenna Position and Beamforming Optimization for Movable Antenna\n  Enabled ISAC: Optimal Solutions and Efficient Algorithms",
        "Do Unit Proofs Work? An Empirical Study of Compositional Bounded Model\n  Checking for Memory Safety Verification",
        "A New Statistical Approach to the Performance Analysis of Vision-based\n  Localization",
        "PAID: A Framework of Product-Centric Advertising Image Design",
        "How much should we care about what others know? Jump signals in optimal\n  investment under relative performance concerns",
        "Optimal PMU Placement for Kalman Filtering of DAE Power System Models",
        "CubeDiff: Repurposing Diffusion-Based Image Models for Panorama\n  Generation",
        "Bias Analysis of Experiments for Multi-Item Multi-Period Inventory\n  Control Policies",
        "Robust Phantom-Assisted Framework for Multi-Person Localization and\n  Vital Signs Monitoring Using MIMO FMCW Radar",
        "On the existence of twisted Shalika periods: the Archimedean case"
      ],
      "abstract":[
        "State-of-the-art stereo matching methods typically use costly 3D convolutions\nto aggregate a full cost volume, but their computational demands make mobile\ndeployment challenging. Directly applying 2D convolutions for cost aggregation\noften results in edge blurring, detail loss, and mismatches in textureless\nregions. Some complex operations, like deformable convolutions and iterative\nwarping, can partially alleviate this issue; however, they are not\nmobile-friendly, limiting their deployment on mobile devices. In this paper, we\npresent a novel bilateral aggregation network (BANet) for mobile stereo\nmatching that produces high-quality results with sharp edges and fine details\nusing only 2D convolutions. Specifically, we first separate the full cost\nvolume into detailed and smooth volumes using a spatial attention map, then\nperform detailed and smooth aggregations accordingly, ultimately fusing both to\nobtain the final disparity map. Additionally, to accurately identify\nhigh-frequency detailed regions and low-frequency smooth\/textureless regions,\nwe propose a new scale-aware spatial attention module. Experimental results\ndemonstrate that our BANet-2D significantly outperforms other mobile-friendly\nmethods, achieving 35.3\\% higher accuracy on the KITTI 2015 leaderboard than\nMobileStereoNet-2D, with faster runtime on mobile devices. The extended 3D\nversion, BANet-3D, achieves the highest accuracy among all real-time methods on\nhigh-end GPUs. Code: \\textcolor{magenta}{https:\/\/github.com\/gangweiX\/BANet}.",
        "Multimodal human action recognition based on RGB and skeleton data fusion,\nwhile effective, is constrained by significant limitations such as high\ncomputational complexity, excessive memory consumption, and substantial energy\ndemands, particularly when implemented with Artificial Neural Networks (ANN).\nThese limitations restrict its applicability in resource-constrained scenarios.\nTo address these challenges, we propose a novel Spiking Neural Network\n(SNN)-driven framework for multimodal human action recognition, utilizing event\ncamera and skeleton data. Our framework is centered on two key innovations: (1)\na novel multimodal SNN architecture that employs distinct backbone networks for\neach modality-an SNN-based Mamba for event camera data and a Spiking Graph\nConvolutional Network (SGN) for skeleton data-combined with a spiking semantic\nextraction module to capture deep semantic representations; and (2) a\npioneering SNN-based discretized information bottleneck mechanism for modality\nfusion, which effectively balances the preservation of modality-specific\nsemantics with efficient information compression. To validate our approach, we\npropose a novel method for constructing a multimodal dataset that integrates\nevent camera and skeleton data, enabling comprehensive evaluation. Extensive\nexperiments demonstrate that our method achieves superior performance in both\nrecognition accuracy and energy efficiency, offering a promising solution for\npractical applications.",
        "Tracking mouse body parts in video is often incomplete due to occlusions such\nthat - e.g. - subsequent action and behavior analysis is impeded. In this\nconceptual work, videos from several perspectives are integrated via global\nexterior camera orientation; body part positions are estimated by 3D\ntriangulation and bundle adjustment. Consistency of overall 3D track\nreconstruction is achieved by introduction of a 3D mouse model, deep-learned\nbody part movements, and global motion-track smoothness constraint. The\nresulting 3D body and body part track estimates are substantially more complete\nthan the original single-frame-based body part detection, therefore, allowing\nimproved animal behavior analysis.",
        "Collaborative perception significantly enhances autonomous driving safety by\nextending each vehicle's perception range through message sharing among\nconnected and autonomous vehicles. Unfortunately, it is also vulnerable to\nadversarial message attacks from malicious agents, resulting in severe\nperformance degradation. While existing defenses employ\nhypothesis-and-verification frameworks to detect malicious agents based on\nsingle-shot outliers, they overlook temporal message correlations, which can be\ncircumvented by subtle yet harmful perturbations in model input and output\nspaces. This paper reveals a novel blind area confusion (BAC) attack that\ncompromises existing single-shot outlier-based detection methods. As a\ncountermeasure, we propose GCP, a Guarded Collaborative Perception framework\nbased on spatial-temporal aware malicious agent detection, which maintains\nsingle-shot spatial consistency through a confidence-scaled spatial concordance\nloss, while simultaneously examining temporal anomalies by reconstructing\nhistorical bird's eye view motion flows in low-confidence regions. We also\nemploy a joint spatial-temporal Benjamini-Hochberg test to synthesize\ndual-domain anomaly results for reliable malicious agent detection. Extensive\nexperiments demonstrate GCP's superior performance under diverse attack\nscenarios, achieving up to 34.69% improvements in AP@0.5 compared to the\nstate-of-the-art CP defense strategies under BAC attacks, while maintaining\nconsistent 5-8% improvements under other typical attacks. Code will be released\nat https:\/\/github.com\/CP-Security\/GCP.git.",
        "The automated analysis of chemical literature holds promise to accelerate\ndiscovery in fields such as material science and drug development. In\nparticular, search capabilities for chemical structures and Markush structures\n(chemical structure templates) within patent documents are valuable, e.g., for\nprior-art search. Advancements have been made in the automatic extraction of\nchemical structures from text and images, yet the Markush structures remain\nlargely unexplored due to their complex multi-modal nature. In this work, we\npresent MarkushGrapher, a multi-modal approach for recognizing Markush\nstructures in documents. Our method jointly encodes text, image, and layout\ninformation through a Vision-Text-Layout encoder and an Optical Chemical\nStructure Recognition vision encoder. These representations are merged and used\nto auto-regressively generate a sequential graph representation of the Markush\nstructure along with a table defining its variable groups. To overcome the lack\nof real-world training data, we propose a synthetic data generation pipeline\nthat produces a wide range of realistic Markush structures. Additionally, we\npresent M2S, the first annotated benchmark of real-world Markush structures, to\nadvance research on this challenging task. Extensive experiments demonstrate\nthat our approach outperforms state-of-the-art chemistry-specific and\ngeneral-purpose vision-language models in most evaluation settings. Code,\nmodels, and datasets will be available.",
        "Realistic human geometry generation is an important yet challenging task,\nrequiring both the preservation of fine clothing details and the accurate\nmodeling of clothing-pose interactions. Geometry distributions, which can model\nthe geometry of a single human as a distribution, provide a promising\nrepresentation for high-fidelity synthesis. However, applying geometry\ndistributions for human generation requires learning a dataset-level\ndistribution over numerous individual geometry distributions. To address the\nresulting challenges, we propose a novel 3D human generative framework that,\nfor the first time, models the distribution of human geometry distributions.\nOur framework operates in two stages: first, generating the human geometry\ndistribution, and second, synthesizing high-fidelity humans by sampling from\nthis distribution. We validate our method on two tasks: pose-conditioned 3D\nhuman generation and single-view-based novel pose generation. Experimental\nresults demonstrate that our approach achieves the best quantitative results in\nterms of realism and geometric fidelity, outperforming state-of-the-art\ngenerative methods.",
        "Despite recent advances in Novel View Synthesis (NVS), generating\nhigh-fidelity views from single or sparse observations remains a significant\nchallenge. Existing splatting-based approaches often produce distorted geometry\ndue to splatting errors. While diffusion-based methods leverage rich 3D priors\nto achieve improved geometry, they often suffer from texture hallucination. In\nthis paper, we introduce SplatDiff, a pixel-splatting-guided video diffusion\nmodel designed to synthesize high-fidelity novel views from a single image.\nSpecifically, we propose an aligned synthesis strategy for precise control of\ntarget viewpoints and geometry-consistent view synthesis. To mitigate texture\nhallucination, we design a texture bridge module that enables high-fidelity\ntexture generation through adaptive feature fusion. In this manner, SplatDiff\nleverages the strengths of splatting and diffusion to generate novel views with\nconsistent geometry and high-fidelity details. Extensive experiments verify the\nstate-of-the-art performance of SplatDiff in single-view NVS. Additionally,\nwithout extra training, SplatDiff shows remarkable zero-shot performance across\ndiverse tasks, including sparse-view NVS and stereo video conversion.",
        "Dense point tracking is a challenging task requiring the continuous tracking\nof every point in the initial frame throughout a substantial portion of a\nvideo, even in the presence of occlusions. Traditional methods use optical flow\nmodels to directly estimate long-range motion, but they often suffer from\nappearance drifting without considering temporal consistency. Recent point\ntracking algorithms usually depend on sliding windows for indirect information\npropagation from the first frame to the current one, which is slow and less\neffective for long-range tracking. To account for temporal consistency and\nenable efficient information propagation, we present a lightweight and fast\nmodel with \\textbf{S}treaming memory for dense \\textbf{PO}int \\textbf{T}racking\nand online video processing. The \\textbf{SPOT} framework features three core\ncomponents: a customized memory reading module for feature enhancement, a\nsensory memory for short-term motion dynamics modeling, and a visibility-guided\nsplatting module for accurate information propagation. This combination enables\nSPOT to perform dense point tracking with state-of-the-art accuracy on the CVO\nbenchmark, as well as comparable or superior performance to offline models on\nsparse tracking benchmarks such as TAP-Vid and RoboTAP. Notably, SPOT with\n10$\\times$ smaller parameter numbers operates at least 2$\\times$ faster than\nprevious state-of-the-art models while maintaining the best performance on CVO.\nWe will release the models and codes at: https:\/\/github.com\/DQiaole\/SPOT.",
        "Text-to-Image (T2I) diffusion models have shown impressive results in\ngenerating visually compelling images following user prompts. Building on this,\nvarious methods further fine-tune the pre-trained T2I model for specific tasks.\nHowever, this requires separate model architectures, training designs, and\nmultiple parameter sets to handle different tasks. In this paper, we introduce\nUniVG, a generalist diffusion model capable of supporting a diverse range of\nimage generation tasks with a single set of weights. UniVG treats multi-modal\ninputs as unified conditions to enable various downstream applications, ranging\nfrom T2I generation, inpainting, instruction-based editing, identity-preserving\ngeneration, and layout-guided generation, to depth estimation and referring\nsegmentation. Through comprehensive empirical studies on data mixing and\nmulti-task training, we provide detailed insights into the training processes\nand decisions that inform our final designs. For example, we show that T2I\ngeneration and other tasks, such as instruction-based editing, can coexist\nwithout performance trade-offs, while auxiliary tasks like depth estimation and\nreferring segmentation enhance image editing. Notably, our model can even\noutperform some task-specific models on their respective benchmarks, marking a\nsignificant step towards a unified image generation model.",
        "Physically-based rendering (PBR) has become a cornerstone in modern computer\ngraphics, enabling realistic material representation and lighting interactions\nin 3D scenes. In this paper, we present MaterialMVP, a novel end-to-end model\nfor generating PBR textures from 3D meshes and image prompts, addressing key\nchallenges in multi-view material synthesis. Our approach leverages Reference\nAttention to extract and encode informative latent from the input reference\nimages, enabling intuitive and controllable texture generation. We also\nintroduce a Consistency-Regularized Training strategy to enforce stability\nacross varying viewpoints and illumination conditions, ensuring\nillumination-invariant and geometrically consistent results. Additionally, we\npropose Dual-Channel Material Generation, which separately optimizes albedo and\nmetallic-roughness (MR) textures while maintaining precise spatial alignment\nwith the input images through Multi-Channel Aligned Attention. Learnable\nmaterial embeddings are further integrated to capture the distinct properties\nof albedo and MR. Experimental results demonstrate that our model generates PBR\ntextures with realistic behavior across diverse lighting scenarios,\noutperforming existing methods in both consistency and quality for scalable 3D\nasset creation.",
        "Prompt tuning of Vision-Language Models (VLMs) such as CLIP, has demonstrated\nthe ability to rapidly adapt to various downstream tasks. However, recent\nstudies indicate that tuned VLMs may suffer from the problem of spurious\ncorrelations, where the model relies on spurious features (e.g. background and\ngender) in the data. This may lead to the model having worse robustness in\nout-of-distribution data. Standard methods for eliminating spurious correlation\ntypically require us to know the spurious attribute labels of each sample,\nwhich is hard in the real world. In this work, we explore improving the group\nrobustness of prompt tuning in VLMs without relying on manual annotation of\nspurious features. We notice the zero - shot image recognition ability of VLMs\nand use this ability to identify spurious features, thus avoiding the cost of\nmanual annotation. By leveraging pseudo-spurious attribute annotations, we\nfurther propose a method to automatically adjust the training weights of\ndifferent groups. Extensive experiments show that our approach efficiently\nimproves the worst-group accuracy on CelebA, Waterbirds, and MetaShift\ndatasets, achieving the best robustness gap between the worst-group accuracy\nand the overall accuracy.",
        "Conditional dependency present one of the trickiest problems in Compositional\nZero-Shot Learning, leading to significant property variations of the same\nstate (object) across different objects (states). To address this problem,\nexisting approaches often adopt either all-to-one or one-to-one representation\nparadigms. However, these extremes create an imbalance in the seesaw between\ntransferability and discriminability, favoring one at the expense of the other.\nComparatively, humans are adept at analogizing and reasoning in a hierarchical\nclustering manner, intuitively grouping categories with similar properties to\nform cohesive concepts. Motivated by this, we propose Homogeneous Group\nRepresentation Learning (HGRL), a new perspective formulates state (object)\nrepresentation learning as multiple homogeneous sub-group representation\nlearning. HGRL seeks to achieve a balance between semantic transferability and\ndiscriminability by adaptively discovering and aggregating categories with\nshared properties, learning distributed group centers that retain\ngroup-specific discriminative features. Our method integrates three core\ncomponents designed to simultaneously enhance both the visual and prompt\nrepresentation capabilities of the model. Extensive experiments on three\nbenchmark datasets validate the effectiveness of our method.",
        "High-fidelity text-to-image diffusion models have revolutionized visual\ncontent generation, but their widespread use raises significant ethical\nconcerns, including intellectual property protection and the misuse of\nsynthetic media. To address these challenges, we propose a novel multi-stage\nwatermarking framework for diffusion models, designed to establish copyright\nand trace generated images back to their source. Our multi-stage watermarking\ntechnique involves embedding: (i) a fixed watermark that is localized in the\ndiffusion model's learned noise distribution and, (ii) a human-imperceptible,\ndynamic watermark in generates images, leveraging a fine-tuned decoder. By\nleveraging the Structural Similarity Index Measure (SSIM) and cosine\nsimilarity, we adapt the watermark's shape and color to the generated content\nwhile maintaining robustness. We demonstrate that our method enables reliable\nsource verification through watermark classification, even when the dynamic\nwatermark is adjusted for content-specific variations. Source model\nverification is enabled through watermark classification. o support further\nresearch, we generate a dataset of watermarked images and introduce a\nmethodology to evaluate the statistical impact of watermarking on generated\ncontent.Additionally, we rigorously test our framework against various attack\nscenarios, demonstrating its robustness and minimal impact on image quality.\nOur work advances the field of AI-generated content security by providing a\nscalable solution for model ownership verification and misuse prevention.",
        "We prove a factorizable version of the Feigin-Frenkel theorem on the center\nof the completed enveloping algebra of the affine Kac-Moody algebra attached to\na simple Lie algebra at the critical level. On any smooth curve C we consider a\nsheaf of complete topological Lie algebras whose fiber at any point is the\nusual affine algebra at the critical level and consider its sheaf of completed\nenveloping algebras. We show that the center of this sheaf is a factorization\nalgebra and establish that it is canonically isomorphic, in a factorizable\nmanner, with the factorization algebra of functions on Opers on the pointed\ndisk.",
        "Universities serve as a hub for academic collaboration, promoting the\nexchange of diverse ideas and perspectives among students and faculty through\ninterdisciplinary dialogue. However, as universities expand in size,\nconventional networking approaches via student chapters, class groups, and\nfaculty committees become cumbersome. To address this challenge, an\nacademia-specific profile recommendation system is proposed to connect\nlike-minded stakeholders within any university community. This study evaluates\nthree techniques: Term Frequency-Inverse Document Frequency (TF-IDF),\nBidirectional Encoder Representations from Transformers (BERT), and a hybrid\napproach to generate effective recommendations. Due to the unlabelled nature of\nthe dataset, Affinity Propagation cluster-based relabelling is performed to\nunderstand the grouping of similar profiles. The hybrid model demonstrated\nsuperior performance, evidenced by its similarity score, Silhouette score,\nDavies-Bouldin index, and Normalized Discounted Cumulative Gain (NDCG),\nachieving an optimal balance between diversity and relevance in\nrecommendations. Furthermore, the optimal model has been implemented as a\nmobile application, which dynamically suggests relevant profiles based on\nusers' skills and collaboration interests, incorporating contextual\nunderstanding. The potential impact of this application is significant, as it\npromises to enhance networking opportunities within large academic institutions\nthrough the deployment of intelligent recommendation systems.",
        "Understanding and reasoning over non-speech sounds and music are crucial for\nboth humans and AI agents to interact effectively with their environments. In\nthis paper, we introduce Audio Flamingo 2 (AF2), an Audio-Language Model (ALM)\nwith advanced audio understanding and reasoning capabilities. AF2 leverages (i)\na custom CLAP model, (ii) synthetic Audio QA data for fine-grained audio\nreasoning, and (iii) a multi-stage curriculum learning strategy. AF2 achieves\nstate-of-the-art performance with only a 3B parameter small language model,\nsurpassing large open-source and proprietary models across over 20 benchmarks.\nNext, for the first time, we extend audio understanding to long audio segments\n(30 secs to 5 mins) and propose LongAudio, a large and novel dataset for\ntraining ALMs on long audio captioning and question-answering tasks.\nFine-tuning AF2 on LongAudio leads to exceptional performance on our proposed\nLongAudioBench, an expert annotated benchmark for evaluating ALMs on long audio\nunderstanding capabilities. We conduct extensive ablation studies to confirm\nthe efficacy of our approach. Project Website:\nhttps:\/\/research.nvidia.com\/labs\/adlr\/AF2\/.",
        "Embodied AI systems, including robots and autonomous vehicles, are\nincreasingly integrated into real-world applications, where they encounter a\nrange of vulnerabilities stemming from both environmental and system-level\nfactors. These vulnerabilities manifest through sensor spoofing, adversarial\nattacks, and failures in task and motion planning, posing significant\nchallenges to robustness and safety. Despite the growing body of research,\nexisting reviews rarely focus specifically on the unique safety and security\nchallenges of embodied AI systems. Most prior work either addresses general AI\nvulnerabilities or focuses on isolated aspects, lacking a dedicated and unified\nframework tailored to embodied AI. This survey fills this critical gap by: (1)\ncategorizing vulnerabilities specific to embodied AI into exogenous (e.g.,\nphysical attacks, cybersecurity threats) and endogenous (e.g., sensor failures,\nsoftware flaws) origins; (2) systematically analyzing adversarial attack\nparadigms unique to embodied AI, with a focus on their impact on perception,\ndecision-making, and embodied interaction; (3) investigating attack vectors\ntargeting large vision-language models (LVLMs) and large language models (LLMs)\nwithin embodied systems, such as jailbreak attacks and instruction\nmisinterpretation; (4) evaluating robustness challenges in algorithms for\nembodied perception, decision-making, and task planning; and (5) proposing\ntargeted strategies to enhance the safety and reliability of embodied AI\nsystems. By integrating these dimensions, we provide a comprehensive framework\nfor understanding the interplay between vulnerabilities and safety in embodied\nAI.",
        "A mangrove mapping (MM) algorithm is an essential classification tool for\nenvironmental monitoring. The recent literature shows that compared with other\nindex-based MM methods that treat pixels as spatially independent,\nconvolutional neural networks (CNNs) are crucial for leveraging spatial\ncontinuity information, leading to improved classification performance. In this\nwork, we go a step further to show that quantum features provide radically new\ninformation for CNN to further upgrade the classification results. Simply\nspeaking, CNN computes affine-mapping features, while quantum neural network\n(QNN) offers unitary-computing features, thereby offering a fresh perspective\nin the final decision-making (classification). To address the challenging MM\nproblem, we design an entangled spatial-spectral quantum feature extraction\nmodule. Notably, to ensure that the quantum features contribute genuinely novel\ninformation (unaffected by traditional CNN features), we design a separate\nnetwork track consisting solely of quantum neurons with built-in\ninterpretability. The extracted pure quantum information is then fused with\ntraditional feature information to jointly make the final decision. The\nproposed quantum-empowered deep network (QEDNet) is very lightweight, so the\nimprovement does come from the cooperation between CNN and QNN (rather than\nparameter augmentation). Extensive experiments will be conducted to demonstrate\nthe superiority of QEDNet.",
        "In this paper, we propose an integrated sensing and communication (ISAC)\nsystem enabled by movable antennas (MAs), which can dynamically adjust antenna\npositions to enhance both sensing and communication performance for future\nwireless networks. To characterize the benefits of MA-enabled ISAC systems, we\nfirst derive the Cram\\'er-Rao bound (CRB) for angle estimation error, which is\nthen minimized for optimizing the antenna position vector (APV) and beamforming\ndesign, subject to a pre-defined signal-to-noise ratio (SNR) constraint to\nensure the communication performance. In particular, for the case with receive\nMAs only, we provide a closed-form optimal antenna position solution, and show\nthat employing MAs over conventional fixed-position antennas (FPAs) can achieve\na sensing performance gain upper-bounded by 4.77 dB. On the other hand, for the\ncase with transmit MAs only, we develop a boundary traversal breadth-first\nsearch (BT-BFS) algorithm to obtain the global optimal solution in the\nline-of-sight (LoS) channel scenario, along with a lower-complexity boundary\ntraversal depth-first search (BT-DFS) algorithm to find a local optimal\nsolution efficiently. While in the scenario with non-LoS (NLoS) channels, a\nmajorization-minimization (MM) based Rosen's gradient projection (RGP)\nalgorithm with an efficient initialization method is proposed to obtain\nstationary solutions for the considered problem, which can be extended to the\ngeneral case with both transmit and receive MAs. Extensive numerical results\nare presented to verify the effectiveness of the proposed algorithms, and\ndemonstrate the superiority of the considered MA-enabled ISAC system over\nconventional ISAC systems with FPAs in terms of sensing and communication\nperformance trade-off.",
        "Memory safety defects pose a major threat to software reliability, enabling\ncyberattacks, outages, and crashes. To mitigate these risks, organizations\nadopt Compositional Bounded Model Checking (BMC), using unit proofs to formally\nverify memory safety. However, methods for creating unit proofs vary across\norganizations and are inconsistent within the same project, leading to errors\nand missed defects. In addition, unit proofing remains understudied, with no\nsystematic development methods or empirical evaluations.\n  This work presents the first empirical study on unit proofing for memory\nsafety verification. We introduce a systematic method for creating unit proofs\nthat leverages verification feedback and objective criteria. Using this\napproach, we develop 73 unit proofs for four embedded operating systems and\nevaluate their effectiveness, characteristics, cost, and generalizability. Our\nresults show unit proofs are cost-effective, detecting 74\\% of recreated\ndefects, with an additional 9\\% found with increased BMC bounds, and 19 new\ndefects exposed. We also found that embedded software requires small unit\nproofs, which can be developed in 87 minutes and executed in 61 minutes on\naverage. These findings provide practical guidance for engineers and empirical\ndata to inform tooling design.",
        "Many modern wireless devices with accurate positioning needs also have access\nto vision sensors, such as a camera, radar, and Light Detection and Ranging\n(LiDAR). In scenarios where wireless-based positioning is either inaccurate or\nunavailable, using information from vision sensors becomes highly desirable for\ndetermining the precise location of the wireless device. Specifically, vision\ndata can be used to estimate distances between the target (where the sensors\nare mounted) and nearby landmarks. However, a significant challenge in\npositioning using these measurements is the inability to uniquely identify\nwhich specific landmark is visible in the data. For instance, when the target\nis located close to a lamppost, it becomes challenging to precisely identify\nthe specific lamppost (among several in the region) that is near the target.\nThis work proposes a new framework for target localization using range\nmeasurements to multiple proximate landmarks. The geometric constraints\nintroduced by these measurements are utilized to narrow down candidate landmark\ncombinations corresponding to the range measurements and, consequently, the\ntarget's location on a map. By modeling landmarks as a marked Poisson point\nprocess (PPP), we show that three noise-free range measurements are sufficient\nto uniquely determine the correct combination of landmarks in a two-dimensional\nplane. For noisy measurements, we provide a mathematical characterization of\nthe probability of correctly identifying the observed landmark combination\nbased on a novel joint distribution of key random variables. Our results\ndemonstrate that the landmark combination can be identified using ranges, even\nwhen individual landmarks are visually indistinguishable.",
        "Creating visually appealing advertising images is often a labor-intensive and\ntime-consuming process. Is it possible to automatically generate such images\nusing only basic product information--specifically, a product foreground image,\ntaglines, and a target size? Existing methods mainly focus on parts of the\nproblem and fail to provide a comprehensive solution. To address this gap, we\npropose a novel multistage framework called Product-Centric Advertising Image\nDesign (PAID). It consists of four sequential stages to highlight product\nforegrounds and taglines while achieving overall image aesthetics: prompt\ngeneration, layout generation, background image generation, and graphics\nrendering. Different expert models are designed and trained for the first three\nstages: First, we use a visual language model (VLM) to generate background\nprompts that match the products. Next, a VLM-based layout generation model\narranges the placement of product foregrounds, graphic elements (taglines and\ndecorative underlays), and various nongraphic elements (objects from the\nbackground prompt). Following this, we train an SDXL-based image generation\nmodel that can simultaneously accept prompts, layouts, and foreground controls.\nTo support the PAID framework, we create corresponding datasets with over\n50,000 labeled images. Extensive experimental results and online A\/B tests\ndemonstrate that PAID can produce more visually appealing advertising images.",
        "We present a multi-agent and mean-field formulation of a game between\ninvestors who receive private signals informing their investment decisions and\nwho interact through relative performance concerns. A key tool in our model is\na Poisson random measure which drives jumps in both market prices and signal\nprocesses and thus captures common and idiosyncratic noise. Upon receiving a\njump signal, an investor evaluates not only the signal's implications for stock\nprice movements but also its implications for the signals received by her peers\nand for their subsequent investment decisions. A crucial aspect of this\nassessment is the distribution of investor types in the economy. These types\ndetermine their risk aversion, performance concerns, and the quality and\nquantity of their signals. We demonstrate how these factors are reflected in\nthe corresponding HJB equations, characterizing an agent's optimal response to\nher peers' signal-based strategies. The existence of equilibria in both the\nmulti-agent and mean-field game is established using Schauder's Fixed Point\nTheorem under suitable conditions on investor characteristics, particularly\ntheir signal processes. Finally, we present numerical case studies that\nillustrate these equilibria from a financial-economic perspective. This allows\nus to address questions such as how much investors should care about the\ninformation known by their peers.",
        "Optimal sensor placement is essential for minimizing costs and ensuring\naccurate state estimation in power systems. This paper introduces a novel\nmethod for optimal sensor placement for dynamic state estimation of power\nsystems modeled by differential-algebraic equations. The method identifies\noptimal sensor locations by minimizing the steady-state covariance matrix of\nthe Kalman filter, thus minimizing the error of joint differential and\nalgebraic state estimation. The problem is reformulated as a mixed-integer\nsemidefinite program and effectively solved using off-the-shelf numerical\nsolvers. Numerical results demonstrate the merits of the proposed approach by\nbenchmarking its performance in phasor measurement unit placement in comparison\nto greedy algorithms.",
        "We introduce a novel method for generating 360{\\deg} panoramas from text\nprompts or images. Our approach leverages recent advances in 3D generation by\nemploying multi-view diffusion models to jointly synthesize the six faces of a\ncubemap. Unlike previous methods that rely on processing equirectangular\nprojections or autoregressive generation, our method treats each face as a\nstandard perspective image, simplifying the generation process and enabling the\nuse of existing multi-view diffusion models. We demonstrate that these models\ncan be adapted to produce high-quality cubemaps without requiring\ncorrespondence-aware attention layers. Our model allows for fine-grained text\ncontrol, generates high resolution panorama images and generalizes well beyond\nits training set, whilst achieving state-of-the-art results, both qualitatively\nand quantitatively. Project page: https:\/\/cubediff.github.io\/",
        "Randomized experiments, or A\/B testing, are the gold standard for evaluating\ninterventions but are underutilized in the area of inventory management. This\nstudy addresses this gap by analyzing A\/B testing strategies in multi-item,\nmulti-period inventory systems with lost sales and capacity constraints. We\nexamine switchback experiments, item-level randomization, pairwise\nrandomization, and staggered rollouts, analyzing their biases theoretically and\ncomparing them through numerical experiments. Our findings provide actionable\nguidance for selecting experimental designs across various contexts in\ninventory management.",
        "With the rising prevalence of cardiovascular and respiratory disorders and an\naging global population, healthcare systems face increasing pressure to adopt\nefficient, non-contact vital sign monitoring (NCVSM) solutions. This study\nintroduces a robust framework for multi-person localization and vital signs\nmonitoring, using multiple-input-multiple-output frequency-modulated continuous\nwave radar, addressing challenges in real-world, cluttered environments. Two\nkey contributions are presented. First, a custom hardware phantom was developed\nto simulate multi-person NCVSM scenarios, utilizing recorded thoracic impedance\nsignals to replicate realistic cardiopulmonary dynamics. The phantom's design\nfacilitates repeatable and rapid validation of radar systems and algorithms\nunder diverse conditions to accelerate deployment in human monitoring. Second,\naided by the phantom, we designed a robust algorithm for multi-person\nlocalization utilizing joint sparsity and cardiopulmonary properties, alongside\nharmonics-resilient dictionary-based vital signs estimation, to mitigate\ninterfering respiration harmonics. Additionally, an adaptive signal refinement\nprocedure is introduced to enhance the accuracy of continuous NCVSM by\nleveraging the continuity of the estimates. Performance was validated and\ncompared to existing techniques through 12 phantom trials and 12 human trials,\nincluding both single- and multi-person scenarios, demonstrating superior\nlocalization and NCVSM performance. For example, in multi-person human trials,\nour method achieved average respiration rate estimation accuracies of 94.14%,\n98.12%, and 98.69% within error thresholds of 2, 3, and 4 breaths per minute,\nrespectively, and heart rate accuracies of 87.10%, 94.12%, and 95.54% within\nthe same thresholds. These results highlight the potential of this framework\nfor reliable multi-person NCVSM in healthcare and IoT applications.",
        "Let $\\K$ be an archimedean local field. We investigate the existence of the\ntwisted Shalika functionals on irreducible admissible smooth representations of\n$\\GL_{2n}(\\K)$ in terms of their L-parameters. As part of our proof, we\nestablish a Hochschild-Serre spectral sequence for nilpotent normal subgroups\nand a Kunneth formula in the framework of Schwartz homology. We also prove the\nanalogous result for twisted linear periods using theta correspondence. The\nexistence of twisted Shalika functionals on representations of\n$\\GL_{2n}^{+}(\\R)$ is also studied, which is of independent interest."
      ]
    }
  },
  {
    "id":2412.09927,
    "research_type":"applied",
    "start_id":"b15",
    "start_title":"MagNet: machine learning enhanced three-dimensional magnetic reconstruction",
    "start_abstract":"Three-dimensional (3D) magnetic reconstruction is vital to the study of novel materials for 3D spintronics. Vector field electron tomography (VFET) a major in house tool achieve that. However, conventional VFET exhibits significant artefacts due unavoidable presence missing wedges. In this article, we propose deep-learning enhanced method address issue. A textures library built by micromagnetic simulations. MagNet, an U-shaped convolutional neural network, trained and tested with dataset generated from library. We demonstrate that MagNet outperforms under wedge. Quality reconstructed induction fields significantly improved.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Three-dimensional nanomagnetism"
      ],
      "abstract":[
        "Magnetic nanostructures are being developed for use in many aspects of our daily life, spanning areas such as data storage, sensing and biomedicine. Whereas patterned nanomagnets traditionally two-dimensional planar structures, recent work is expanding nanomagnetism into three dimensions; a move triggered by the advance unconventional synthesis methods discovery new magnetic effects. In three-dimensional more complex configurations become possible, with unprecedented properties. Here we review creation these structures their implications emergence physics, development instrumentation computational methods, exploitation numerous applications. Nanoscale devices play key role modern technologies but current applications involve only 2D like discs. authors progress fabrication understanding 3D nanostructures, enabling diverse functionalities."
      ],
      "categories":[
        "cond-mat.mes-hall"
      ]
    },
    "list":{
      "title":[
        "Caroli-de Gennes-Matricon Analogs in Full-Shell Hybrid Nanowires",
        "Nonequilibrium Green's Function Formalism Applicable to Discrete\n  Impurities in Semiconductor Nanostructures",
        "Probing $k$-Space Alternating Spin Polarization via the Anomalous Hall\n  Effect",
        "Topological insulator constrictions -- Dirac particles in a\n  magneto-chiral box",
        "Klein Tunneling and Fabry-P\\'erot Resonances in Twisted Bilayer Graphene",
        "Nonvolatile Electric Control of Antiferromagnet CrSBr",
        "Longitudinal Spin Hall Magnetoresistance from Spin Fluctuations",
        "Spin-polarized STM measurement scheme for quantum geometric tensor",
        "Anomalous topological edge modes in a periodically-driven trimer lattice",
        "Gate Tunable Josephson Diode Effect in Josephson Junctions made from\n  InAs Nanosheets",
        "Single-gate tracking behavior in flat-band multilayer graphene devices",
        "Quantum dot-based device for high-performance magnetic microscopy and\n  spin filtering in the Kondo regime",
        "Quantized crystalline-electromagnetic responses in insulators",
        "A new framework for Ljusternik-Schnirelmann theory and its application\n  to planar Choquard equations",
        "Implicit Generative Modeling by Kernel Similarity Matching",
        "k-Sample inference via Multimarginal Optimal Transport",
        "A Drinfeld Presentation of the Queer Super-Yangian",
        "Implicit Bias in Matrix Factorization and its Explicit Realization in a\n  New Architecture",
        "An experimental technique for measuring radial coherence",
        "Probing prethermal nonergodicity through measurement outcomes of\n  monitored quantum dynamics",
        "Four Total Eclipsing Contact Binary Systems: The First Photometric Light\n  Curve Solutions Employing TESS and Gaia Surveys",
        "Time-Variant Vector Field Visualization for Magnetic Fields of Neutron\n  Star Simulations",
        "Quantum Birkhoff Normal Form in the $\\sigma$-Bruno-R\\\"{u}ssmann\n  non-resonant condition",
        "From de Bruijn graphs to variation graphs-relationships between\n  pangenome models",
        "High-accuracy multi-ion spectroscopy with mixed-species Coulomb crystals",
        "A Liouville-type theorem for the p-Laplacian on complete non-compact\n  Riemannian manifolds",
        "Gradient Flows and the Curvature of Theory Space",
        "Causal survival analysis, Estimation of the Average Treatment Effect\n  (ATE): Practical Recommendations"
      ],
      "abstract":[
        "We report tunneling spectroscopy of Andreev subgap states in hybrid nanowires\nwith a thin superconducting full-shell surrounding a semiconducting core. The\ncombination of the quantized fluxoid of the shell and the Andreev reflection at\nthe superconductor-semiconductor interface gives rise to analogs of Caroli-de\nGennes-Matricon (CdGM) states found in Abrikosov vortices in type-II\nsuperconductors. Unlike in metallic superconductors, CdGM analogs in full-shell\nhybrid nanowires manifest as one-dimensional van Hove singularities with energy\nspacings comparable to the superconducting gap and independent of the Fermi\nenergy, making them readily observable. Evolution of these analogs with axial\nmagnetic field, skewed within the Little-Parks lobe structure, is consistent\nwith theory and yields information about the radial distribution and angular\nmomenta of the corresponding subbands.",
        "A new theoretical framework for the nonequilibrium Green's function (NEGF)\nscheme is presented to account for the discrete nature of impurities doped in\nsemiconductor nanostructures. The short-range part of impurity potential is\nincluded as scattering potential in the self-energy due to spatially localized\nimpurity scattering, and the long-range part of impurity potential is treated\nas the self-consistent Hartree potential by coupling with the Poisson equation.\nThe position-dependent impurity scattering rate under inhomogeneous impurity\nprofiles is systematically derived so that its physical meaning is clarified.\nThe position dependence of the scattering rate turns out to be represented by\nthe `center of mass' coordinates in the Wigner coordinates, rather than the\nreal-space coordinates. Consequently, impurity scattering is intrinsically\nnonlocal in space. The proposed framework is applied to cylindrical thin wires\nunder the quasi-one-dimensional (quasi-1D) approximation. We show explicitly\nhow the discrete nature of impurities affects the transport properties such as\nelectrostatic potential, local density of states, carrier density, scattering\nrates, and mobility.",
        "Altermagnets represent a recently discovered class of collinear magnets,\ncharacterized by antiparallel neighboring magnetic moments and alternating-sign\nspin polarization in momentum-space($k$-space). However, experimental methods\nfor probing the $k$-space spin polarization in altermagnets remain limited. In\nthis work, we propose an approach to address this challenge by interfacing an\naltermagnet with the surface of a topological insulator. The massless Dirac\nfermions on the topological insulator surface acquire a mass due to the\ntime-reversal symmetry breaking. The local $k$-space magnetic moment at the\nDirac point directly determines both the sign and magnitude of this Dirac mass,\nresulting in an anomalous Hall effect. By measuring the Hall conductance, we\ncan extract the local $k$-space magnetic moment. Moreover, we can map the\nglobal magnetic moment distribution by tuning the Dirac point position using an\nin-plane magnetic field, thereby revealing the $k$-space spin density of the\naltermagnet. This work establishes the Dirac fermion on the topological\ninsulator surface as a sensitive probe for unveiling spin characters of\naltermagnets and those of other unconventional antiferromagnets.",
        "We study magneto-transport through topological insulator nanowires shaped in\nthe form of a constriction, as can be obtained by etching techniques. The\nmagnetic field is coaxial, potentially turning the nanowire into a\nmagneto-chiral junction. We show in a detailed analytical and numerical study\nthat two main transport regimes emerge, depending on the central narrow region\nbeing short or long as compared to the magnetic length at the junction entrance\nand exit. In both cases the central region hosts Dirac-particle-in-a-box states\ndue to magnetic confinement, whose conductance properties are strongly\ninfluenced by Landau levels at the ends of the constriction. Notably, in the\nlow-energy regime only chiral states with a specific handedness can transport\ncharge across the junction. Based on these properties and general symmetry\nconsiderations we argue that the shaped nanowire should exhibit strong\nmagneto-chiral non-reciprocal transport beyond linear response. We employ a\nnumerical tight-binding implementation of an effective 2D model on a\nnon-homogeneous grid, capable of simulating samples of realistic sizes, and\ntest its soundness against full simulations for scaled-down 3D topological\ninsulator wires.",
        "The paper discusses the Klein tunneling and Fabry-P\\'erot resonances of\ncharge carriers through a rectangular potential barrier in twisted bilayer\ngraphene. Within the framework of the low-energy excitations, the transmission\nprobability and the conductance are obtained depending on the parameters of the\nproblem. Owing to the different chirality in twisted bilayer graphene, the\npropagation of charge carriers exhibits an anisotropic behavior in transmission\nprobability and Fabry-P\\'erot resonances. Moreover, we show that the anisotropy\nof the charge carriers induces asymmetry and deflection in the Fabry-P\\'erot\nresonances and Klein tunneling, and they are extremely sensitive to the height\nof the potential applied. Additionally, we found that the conductance is\nstrongly sensitive to the barrier height but weakly sensitive to the barrier\nwidth. Therefore, it is possible to control the maxima and minima of the\nconductance of charge carriers in twisted bilayer graphene. With our results,\nwe gain an in-depth understanding of tunneling properties in twisted bilayer\ngraphene, which may help in the development and designing of novel electronic\nnanodevices based on anisotropic 2D materials.",
        "van der Waals magnets are emerging as a promising material platform for\nelectric field control of magnetism, offering a pathway towards the elimination\nof external magnetic fields from spintronic devices. A further step is the\nintegration of such magnets with electrical gating components which would\nenable nonvolatile control of magnetic states. However, this approach remains\nunexplored for antiferromagnets, despite their growing significance in\nspintronics. Here, we demonstrate nonvolatile electric field control of\nmagnetoelectric characteristics in van der Waals antiferromagnet CrSBr. We\nintegrate a CrSBr channel in a flash-memory architecture featuring charge\ntrapping graphene multilayers. The electrical gate operation triggers a\nnonvolatile 200 % change in the antiferromagnetic state of CrSBr resistance by\nmanipulating electron accumulation\/depletion. Moreover, the nonvolatile gate\nmodulates the metamagnetic transition field of CrSBr and the magnitude of\nmagnetoresistance. Our findings highlight the potential of manipulating\nmagnetic properties of antiferromagnetic semiconductors in a nonvolatile way.",
        "Spin Hall magnetoresistance (SMR), the variation in resistance in a heavy\nmetal (HM) with the magnetization orientation of an adjacent ferromagnet (FM),\nhas been extensively studied as a powerful tool for probing surface magnetic\nmoments in a variety of magnetic materials. However, the conventional SMR\ntheory assumes rigid magnetization of a fixed magnitude, an assumption that\nbreaks down close to the FM's Curie temperature \\(T_c\\), where the magnetic\nsusceptibility diverges. Here, we report an unconventional SMR effect arising\nfrom the magnetic-field modulation of spin fluctuations in the FM, while its\nmagnetization remaining collinear to the spin Hall accumulation in the HM. In\ncontrast to the conventional SMR, which scales with the magnetization and\nvanishes near $T_{c}$, such ``longitudinal\" SMR (LSMR), though suppressed at\nlow temperatures, becomes critically enhanced at \\(T_c\\), reaching a magnitude\ncomparable to conventional SMR amplitudes. Our findings suggest a promising\nmethod for electrically detecting enhanced spin fluctuations in magnetic\nsystems.",
        "Quantum geometric tensor (QGT) reflects the geometry of the eigenstates of a\nsystem's Hamiltonian. The full characterization of QGT is essential for various\nquantum systems. However, it is challenging to characterize the QGT of the\nsolid-state systems. Here we present a scheme by using spin-polarized STM to\nmeasure QGT of two-dimensional solid-state systems, in which the spin texture\nis extracted from geometric amplitudes of Friedel oscillations induced by the\nintentionally introduced magnetic impurity and then the QGT is derived from the\nmomentum differential of spin texture. The surface states of topological\ninsulator (TISS), as a model spin system, is promising to demonstrate the\nscheme. In a TI slab, the gapped TISS host finite quantum metric and Berry\ncurvature as the symmetric real part and the antisymmetric imaginary part of\nQGT, respectively. Thus, a detailed calculations guide the use of the developed\nscheme to measure the QGT of gapped TISS with or without an external in-plane\nmagnetic field. This study provides a feasible scheme for measuring QGT of\ntwo-dimensional solid-state systems, and hints at the great potential of the\ninformation extraction from the geometric amplitudes of STM and other\nmeasurement.",
        "Periodically driven systems have a longstanding reputation for establishing\nrich topological phenomena beyond their static counterpart. In this work, we\npropose and investigate a periodically driven extended Su-Schrieffer-Heeger\n(SSH) model with three sites per unit cell, obtained by replacing the Pauli\nmatrices with their $3\\times 3$ counterparts. The system is found to support a\nnumber of edge modes over a range of parameter windows, some of which have no\nstatic counterparts. Among these edge modes, of particular interest are those\nwhich are pinned at a specific quasienergy value. Such quasienergy-fixed edge\nmodes arise due to the interplay between topology and chiral symmetry, which\nare typically not expected in a three-band static model due to the presence of\na bulk band at the only chiral-symmetric energy value, i.e., zero. In our\ntime-periodic setting, another chiral-symmetric quasienergy value exists at\nhalf the driving frequency, which is not occupied by a bulk band and could then\nhost chiral-symmetry-protected edge modes ($\\pi$ modes). Finally, we verify the\nrobustness of all edge modes against spatial disorder and briefly discuss the\nprospect of realizing our system in experiments.",
        "We report the observation of Josephson diode effect (JDE) in hybrid devices\nmade from semiconductor InAs nanosheets and superconductor Al contacts. By\napplying an in-plane magnetic field ($B_{\\mathrm{xy}}$), we detect\nnon-reciprocal superconducting switching current as well as non-reciprocal\nsuperconducting retrapping current. The strength of the JDE depends on the\nangle between the in-plane magnetic field and the bias current\n($I_{\\mathrm{b}}$), reaching its maximum when $B_{\\mathrm{xy}} \\perp\nI_{\\mathrm{b}}$ and dropping to nearly zero when $B_{\\mathrm{xy}}\\parallel\nI_{\\mathrm{b}}$. Additionally, the diode efficiency is tunable via an\nelectrostatic gate with a complete suppression at certain gate voltages. Our\nfindings indicate that the observed JDE in InAs nanosheet-based Josephson\njunctions most likely arises from the Rashba spin-orbit interaction (SOI) in\nthe nanosheets. Such gate-tunable JDE in Josephson junctions made from\nsemiconductor material with SOI is useful not only for constructing advanced\nsuperconducting electronics but also for detecting novel superconducting\nstates.",
        "A central feature of many van der Waals (vdW) materials is the ability to\nprecisely control their charge doping, $n$, and electric displacement field,\n$D$, using top and bottom gates. For devices composed of only a few layers, it\nis commonly assumed that $D$ causes the layer-by-layer potential to drop\nlinearly across the structure. Here, we show that this assumption fails for a\nbroad class of crystalline and moir\\'e vdW structures based on Bernal- or\nrhombohedral-stacked multilayer graphene. We find that the electronic\nproperties at the Fermi level are largely dictated by special layer-polarized\nstates arising at Bernal-stacked crystal faces, which typically coexist in the\nsame band with layer-delocalized states. We uncover a novel mechanism by which\nthe layer-delocalized states completely screen the layer-polarized states from\nthe bias applied to the remote gate. This screening mechanism leads to an\nunusual scenario where voltages on either gate dope the band as expected, yet\nthe band dispersion and associated electronic properties remain primarily (and\nsometimes exclusively) governed by the gate closer to the layer-polarized\nstates. Our results reveal a novel electronic mechanism underlying the atypical\nsingle-gate-controlled transport characteristics observed across many flat-band\ngraphitic structures, and provide key theoretical insights essential for\naccurately modeling these systems.",
        "We propose a nanoscale device consisting of a double quantum dot with a full\nexchange and pair hopping interaction. In this design, the current can only\nflow through the upper dot, but is sensitive to the spin state of the lower\ndot. The system is immersed in a highly inhomogeneous magnetic field, and only\nthe bottom dot feels a substantial magnetic field, while the top dot\nexperiences only a residual one.\n  We show that our device exhibits very interesting magnetic field-dependent\ntransport properties at low temperatures. The Kondo effect partially survives\nthe presence of the magnetic field and allows to obtain conductances that\ndiffer by several orders of magnitude for the two spin types across the top\ndot.\n  Interestingly, as a function of the magnetic field, our two-dot device\nchanges from a spin singlet state to a spin triplet state, in which the\namplitudes of the spin-dependent conductances are reversed.\n  Our device is able to discriminate between positive and negative magnetic\nfields with a high sensitivity and is therefore particularly interesting for\nimaging the surface of anti-ferromagnetic (AF) insulating materials with\nalternated surface magnetic field, as well as for spin filtering applications.",
        "We introduce new classes of gapped topological phases characterized by\nquantized crystalline-electromagnetic responses, termed \"multipolar Chern\ninsulators\". These systems are characterized by nonsymmorphic momentum-space\nsymmetries and mirror symmetries, leading to quantization of momentum-weighted\nBerry curvature multipole moments. We construct lattice models for such phases\nand confirm their quantized responses through numerical calculations. These\nsystems exhibit bound charge and momentum densities at lattice and magnetic\ndefects, and currents induced by electric or time-varying strain fields. Our\nwork extends the classification of topological matter by uncovering novel\nsymmetry-protected topological phases with quantized responses.",
        "We consider the planar logarithmic Choquard equation $$- \\Delta u + a(x)u +\n(\\log|\\cdot| \\ast u^2)u = 0,\\qquad \\text{in } \\mathbb{R}^2$$ in the strongly\nindefinite and possibly degenerate setting where no sign condition is imposed\non the linear potential $a \\in L^\\infty(\\mathbb{R}^2)$. In particular, we shall\nprove the existence of a sequence of high energy solutions to this problem in\nthe case where $a$ is invariant under $\\mathbb{Z}^2$-translations.\n  The result extends to a more general $G$-equivariant setting, for which we\ndevelop a new variational approach which allows us to find critical points of\nLjusternik-Schnirelmann type. In particular, our method resolves the problem\nthat the energy functional $\\Phi$ associated with the logarithmic Choquard\nequation is only defined on a subspace $X \\subset H^1(\\mathbb{R}^2)$ with the\nproperty that $\\|\\cdot\\|_X$ is not translation invariant. The new approach is\nbased on a new $G$-equivariant version of the Cerami condition and on\ndeformation arguments adapted to a family of suitably constructed scalar\nproducts $\\langle \\cdot, \\cdot \\rangle_u$, $u \\in X$ with the $G$-equivariance\nproperty $\\langle g \\ast v , g \\ast w \\rangle_{g \\ast u} = \\langle v , w\n\\rangle_u.$",
        "Understanding how the brain encodes stimuli has been a fundamental problem in\ncomputational neuroscience. Insights into this problem have led to the design\nand development of artificial neural networks that learn representations by\nincorporating brain-like learning abilities. Recently, learning representations\nby capturing similarity between input samples has been studied to tackle this\nproblem. This approach, however, has thus far been used to only learn\ndownstream features from an input and has not been studied in the context of a\ngenerative paradigm, where one can map the representations back to the input\nspace, incorporating not only bottom-up interactions (stimuli to latent) but\nalso learning features in a top-down manner (latent to stimuli). We investigate\na kernel similarity matching framework for generative modeling. Starting with a\nmodified sparse coding objective for learning representations proposed in prior\nwork, we demonstrate that representation learning in this context is equivalent\nto maximizing similarity between the input kernel and a latent kernel. We show\nthat an implicit generative model arises from learning the kernel structure in\nthe latent space and show how the framework can be adapted to learn manifold\nstructures, potentially providing insights as to how task representations can\nbe encoded in the brain. To solve the objective, we propose a novel Alternate\nDirection Method of Multipliers (ADMM) based algorithm and discuss the\ninterpretation of the optimization process. Finally, we discuss how this\nrepresentation learning problem can lead towards a biologically plausible\narchitecture to learn the model parameters that ties together representation\nlearning using similarity matching (a bottom-up approach) with predictive\ncoding (a top-down approach).",
        "This paper proposes a Multimarginal Optimal Transport ($MOT$) approach for\nsimultaneously comparing $k\\geq 2$ measures supported on finite subsets of\n$\\mathbb{R}^d$, $d \\geq 1$. We derive asymptotic distributions of the optimal\nvalue of the empirical $MOT$ program under the null hypothesis that all $k$\nmeasures are same, and the alternative hypothesis that at least two measures\nare different. We use these results to construct the test of the null\nhypothesis and provide consistency and power guarantees of this $k$-sample\ntest. We consistently estimate asymptotic distributions using bootstrap, and\npropose a low complexity linear program to approximate the test cut-off. We\ndemonstrate the advantages of our approach on synthetic and real datasets,\nincluding the real data on cancers in the United States in 2004 - 2020.",
        "We introduce a Drinfeld presentation for the super-Yangian\n$\\mathrm{Y}(\\mathfrak{q}_n)$ associated with the queer Lie superalgebra\n$\\mathfrak{q}_n$. The Drinfeld generators of $\\mathrm{Y}(\\mathfrak{q}_n)$ are\nobtained by a block version Gauss decomposition of the generator matrix in its\nRTT presentation, and the Drinfeld relations are explicitly computed by\nutilizing a block version of its RTT relations.",
        "Gradient descent for matrix factorization is known to exhibit an implicit\nbias toward approximately low-rank solutions. While existing theories often\nassume the boundedness of iterates, empirically the bias persists even with\nunbounded sequences. We thus hypothesize that implicit bias is driven by\ndivergent dynamics markedly different from the convergent dynamics for data\nfitting. Using this perspective, we introduce a new factorization model:\n$X\\approx UDV^\\top$, where $U$ and $V$ are constrained within norm balls, while\n$D$ is a diagonal factor allowing the model to span the entire search space.\nOur experiments reveal that this model exhibits a strong implicit bias\nregardless of initialization and step size, yielding truly (rather than\napproximately) low-rank solutions. Furthermore, drawing parallels between\nmatrix factorization and neural networks, we propose a novel neural network\nmodel featuring constrained layers and diagonal components. This model achieves\nstrong performance across various regression and classification tasks while\nfinding low-rank solutions, resulting in efficient and lightweight networks.",
        "Coherence refers to correlations between field vibrations at two separate\npoints in degrees of freedom such as space, time, and polarisation. In the\ncontext of space, coherence theory has been formulated between two transverse\npositions which can be described either in the cartesian coordinates or in the\ncylindrical coordinates. When expressed in cylindrical coordinates, spatial\ncoherence is described in terms of azimuthal and radial coordinates. The\ndescription of spatial coherence in radial degree of freedom has been\nformulated only recently in JOSA A 40, 411 (2023). In the present article, we\ndemonstrate an efficient experimental technique for measuring radial coherence,\nand we report measurement of radial coherence of two different types of\nradially partially coherent optical fields.",
        "Projective measurements are a key element in quantum physics and enable rich\nphenomena in monitored quantum dynamics. Here, we show that the measurement\noutcomes, recorded during monitored dynamics, can provide crucial information\nabout the properties of the monitored dynamical system itself. We demonstrate\nthis for a Floquet model of many-body localization, where we find that the\nprethermal many-body localized regime becomes unstable against rare\nmeasurements, yielding an unusual enhancement of quantum entanglement. Through\nan unsupervised learning and mutual information analysis on the classical\ndataset of measurement outcomes, we find that the information loss in the\nsystem, reflected by the increased entanglement, is compensated by an emergent\nstructure in this classical dataset. Our findings highlight the crucial role of\nmeasurements and corresponding classical outcomes in capturing prethermal\nnonergodicity, offering a promising perspective for applications to other\nmonitored quantum dynamics.",
        "We presented the first photometric light curve solutions of four W Ursae\nMajoris (W UMa)-type contact binary systems. This investigation utilized\nphotometric data from the Transiting Exoplanet Survey Satellite (TESS) and Gaia\nData Release 3 (DR3). We used the PHysics Of Eclipsing BinariEs (PHOEBE) Python\ncode and the Markov Chain Monte Carlo (MCMC) method for these light curve\nsolutions. Only TIC 249064185 among the target systems needed a cold starspot\nto be included in the analysis. Based on the estimated mass ratios for these\ntotal eclipse systems, three of them are categorized as low mass ratio contact\nbinary stars. The absolute parameters of the systems were estimated using the\nGaia DR3 parallax method and the orbital period and semi-major axis ($P-a$)\nempirical relationship. We defined that TIC 318015356 and TIC 55522736 systems\nare A-subtypes, while TIC 249064185 and TIC 397984843 are W-subtypes, depending\non each component's effective temperature and mass. We estimated the initial\nmasses of the stars, the mass lost by the binary system, and the systems' ages.\nWe displayed star positions in the mass-radius, mass-luminosity, and total\nmass-orbital angular momentum diagrams. In addition, our findings indicate a\ngood agreement with the mass-temperature empirical parameter relationship for\nthe primary stars.",
        "We present a novel visualization application designed to explore the\ntime-dependent development of magnetic fields of neutron stars. The strongest\nmagnetic fields in the universe can be found within neutron stars, potentially\nplaying a role in initiating astrophysical jets and facilitating the outflow of\nneutron-rich matter, ultimately resulting in the production of heavy elements\nduring binary neutron star mergers. Since such effects may be dependent on the\nstrength and configuration of the magnetic field, the formation and parameters\nof such fields are part of current research in astrophysics. Magnetic fields\nare investigated using simulations in which various initial configurations are\ntested. However, the long-term configuration is an open question, and current\nsimulations do not achieve a stable magnetic field. Neutron star simulations\nproduce data quantities in the range of several terabytes, which are both\nspatially in 3D and temporally resolved. Our tool enables physicists to\ninteractively explore the generated data. We first convert the data in a\npre-processing step and then we combine sparse vector field visualization using\nstreamlines with dense vector field visualization using line integral\nconvolution. We provide several methods to interact with the data responsively.\nThis allows the user to intuitively investigate data-specific issues.\nFurthermore, diverse visualization techniques facilitate individual exploration\nof the data and enable real-time processing of specific domain tasks, like the\ninvestigation of the time-dependent evolution of the magnetic field. In a\nqualitative study, domain experts tested the tool, and the usability was\nqueried. Experts rated the tool very positively and recommended it for their\ndaily work.",
        "The aim of this paper is to construct a Gevrey quantum Birkhoff normal form\nfor the $h$-differential operator $P_{h}(t),$ where $\nt\\in(-\\frac{1}{2},\\frac{1}{2})$, in the neighborhood of the union $\\Lambda$ of\nKAM tori. This construction commences from an appropriate Birkhoff normal form\nof $H$ around $\\Lambda$ and proceeds under the $\\sigma$-Bruno-R\\\"{u}ssmann\ncondition with $\\sigma>1$.",
        "Pangenomes serve as a framework for joint analysis of genomes of related\norganisms. Several pangenome models were proposed, offering different\nfunctionalities, applications provided by available tools, their efficiency\netc. Among them, two graph-based models are particularly widely used: variation\ngraphs and de Bruijn graphs. In the current paper we propose an axiomatization\nof the desirable properties of a graph representation of a collection of\nstrings. We show the relationship between variation graphs satisfying these\ncriteria and de Bruijn graphs. This relationship can be used to efficiently\nbuild a variation graph representing a given set of genomes, transfer\nannotations between both models, compare the results of analyzes based on each\nmodel etc.",
        "Multi-ion optical clocks offer the possibility of overcoming the low\nsignal-to-noise ratio of single-ion clocks, while still providing low\nsystematic uncertainties. We present simultaneous spectroscopy of up to four\n${}^{115}$In${}^+$ clock ions in a linear Coulomb crystal, sympathetically\ncooled with ${}^{172}$Yb${}^+$ ions. In first clock comparisons, we see\nagreement below $1\\times10^{-17}$ with results obtained using a single In${}^+$\nion, for which we have evaluated the systematic uncertainty to be\n$2.5\\times10^{-18}$. Operation with four clock ions reduces the instability\nfrom $1.6\\times10^{-15}\/\\sqrt{t\/(1\\;\\mathrm{s})}$ to\n$9.2\\times10^{-16}\/\\sqrt{t\/(1\\;\\mathrm{s})}$. We derive a model for\ndecay-related dead time during state preparation, which matches the observed\nscaling of instability with clock ion number $N$, and indicates that\n$1\/\\sqrt{N}$ scaling can be achieved with the addition of a repump laser.",
        "A Liouville-type result for the p-Laplacian on complete Riemannian manifolds\nis proved. As an application are present some results concerning complete\nnon-compact hypersurfaces immersed in a suitable warped product manifold.",
        "The metric and potential associated with the gradient property of\nrenormalisation group flow in multiscalar models in $d=4-\\varepsilon$\ndimensions are studied. The metric is identified with the Zamolodchikov metric\nof nearly marginal operators on the sphere. An explicit form for the associated\nRicci scalar in $d=4-\\varepsilon$ is derived, which shows that the space of\nmultiscalar field theories is curved. The potential is identified with a\nquantity $\\widetilde{F}$ that was previously proposed as a weakly monotonic\nfunction interpolating between the $a$-theorem in four dimensions and the\n$F$-theorem in three dimensions. This implies that the $\\widetilde{F}$-theorem\ncan be extended perturbatively to a theorem about gradient flow in\n$d=4-\\varepsilon$.",
        "Causal survival analysis combines survival analysis and causal inference to\nevaluate the effect of a treatment or intervention on a time-to-event outcome,\nsuch as survival time. It offers an alternative to relying solely on Cox models\nfor assessing these effects. In this paper, we present a comprehensive review\nof estimators for the average treatment effect measured with the restricted\nmean survival time, including regression-based methods, weighting approaches,\nand hybrid techniques. We investigate their theoretical properties and compare\ntheir performance through extensive numerical experiments. Our analysis focuses\non the finite-sample behavior of these estimators, the influence of nuisance\nparameter selection, and their robustness and stability under model\nmisspecification. By bridging theoretical insights with practical evaluation,\nwe aim to equip practitioners with both state-of-the-art implementations of\nthese methods and practical guidelines for selecting appropriate estimators for\ntreatment effect estimation. Among the approaches considered, G-formula\ntwo-learners, AIPCW-AIPTW, Buckley-James estimators, and causal survival\nforests emerge as particularly promising."
      ]
    }
  },
  {
    "id":2412.19927,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"Model-free simulations of turbulent reactive flows",
    "start_abstract":"A critical review of the modern computational methods for solving the transport equations of turbulent reacting single-phase flows is presented. Primary consideration is given to those methods which lead to \u2018model-free\u2019 simulations while some attention is devoted to \u2018turbulence modeling\u2019. Emphasis is placed upon the role of supercomputers and how their increased computational capacities may be exploited to allow better simulations of the physics of turbulent reactive flows. Comparisons between the commonly employed computational schemes for simulating these flows are given, with the advantages and the limitations associated with each scheme being highlighted. Examples are presented of recent applications of model-free simulations to a variety of unsteady reacting flows, with detailed discussions on the physical phenomena captured by such simulations. Due to the nature of this review, experimental contributions are mentioned only in the context of providing empirical evidence. References are made to other contributions which are not directly related to the computational efforts in order to provide a reasonably comprehensive bibliography for those interested in pursuing various topics in greater detail. Predictions of future accomplishments, as well as some suggestions for future work, are also given.",
    "start_categories":[
      "physics.flu-dyn"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b40"
      ],
      "title":[
        "Physics guided neural networks for spatio-temporal superresolution of turbulent flows"
      ],
      "abstract":[
        "Direct numerical simulation (DNS) of turbulent flows is computationally expensive and cannot be applied to flows with large Reynolds numbers. Low-resolution large eddy simulation (LES) is a popular alternative, but it is unable to capture all of the scales of turbulent transport accurately. Reconstructing DNS from low-resolution LES is critical for large-scale simulation in many scientific and engineering disciplines, but it poses many challenges to existing super-resolution methods due to the complexity of turbulent flows and computational cost of generating frequent LES data. We propose a physics-guided neural network for reconstructing frequent DNS from sparse LES data by enhancing its spatial resolution and temporal frequency. Our proposed method consists of a partial differential equation (PDE)-based recurrent unit for capturing underlying temporal processes and a physics-guided super-resolution model that incorporates additional physical constraints. We demonstrate the effectiveness of both components in reconstructing the Taylor-Green Vortex using sparse LES data. Moreover, we show that the proposed recurrent unit can preserve the physical characteristics of turbulent flows by leveraging the physical relationships in the Navier-Stokes equation."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "FW-Shapley: Real-time Estimation of Weighted Shapley Values",
        "A Deep-Learning Iterative Stacked Approach for Prediction of Reactive\n  Dissolution in Porous Media",
        "VersaPRM: Multi-Domain Process Reward Model via Synthetic Reasoning Data",
        "Learning Versatile Optimizers on a Compute Diet",
        "Single Domain Generalization with Model-aware Parametric Batch-wise\n  Mixup",
        "Position: Curvature Matrices Should Be Democratized via Linear Operators",
        "PINT: Physics-Informed Neural Time Series Models with Applications to\n  Long-term Inference on WeatherBench 2m-Temperature Data",
        "Exploring the Potential of Bilevel Optimization for Calibrating Neural\n  Networks",
        "U-Fair: Uncertainty-based Multimodal Multitask Learning for Fairer\n  Depression Detection",
        "ColNet: Collaborative Optimization in Decentralized Federated Multi-task\n  Learning Systems",
        "Federated Learning with Reservoir State Analysis for Time Series Anomaly\n  Detection",
        "Spurious Forgetting in Continual Learning of Language Models",
        "Model-Based Exploration in Monitored Markov Decision Processes",
        "Continuous spectrum-shrinking maps and applications to preserver\n  problems",
        "Aligning LLMs with Domain Invariant Reward Models",
        "People Reduce Workers' Compensation for Using Artificial Intelligence\n  (AI)",
        "DnD Filter: Differentiable State Estimation for Dynamic Systems using\n  Diffusion Models",
        "The Commutators of $n$-dimensional Rough Fractional Hardy Operators on\n  Two Weighted Grand Herz-Morrey Spaces with Variable Exponents",
        "Hypernetwork-based approach for optimal composition design in partially\n  controlled multi-agent systems",
        "Science mapping of the Revista General de Informacion y Documentacion\n  (2005-2022)",
        "Error norm estimates for the block conjugate gradient algorithm",
        "TFBS-Finder: Deep Learning-based Model with DNABERT and Convolutional\n  Networks to Predict Transcription Factor Binding Sites",
        "Scalable Video Conferencing Using SDN Principles",
        "Mining Diamonds in labeled Transition Systems",
        "TxAgent: An AI Agent for Therapeutic Reasoning Across a Universe of\n  Tools",
        "Monte-Carlo based non-line-of-sight underwater wireless optical\n  communication channel modeling and system performance analysis under\n  turbulence",
        "AIoT-based smart traffic management system",
        "Phase space analysis of CCDM cosmologies"
      ],
      "abstract":[
        "Fair credit assignment is essential in various machine learning (ML)\napplications, and Shapley values have emerged as a valuable tool for this\npurpose. However, in critical ML applications such as data valuation and\nfeature attribution, the uniform weighting of Shapley values across subset\ncardinalities leads to unintuitive credit assignments. To address this,\nweighted Shapley values were proposed as a generalization, allowing different\nweights for subsets with different cardinalities. Despite their advantages,\nsimilar to Shapley values, Weighted Shapley values suffer from exponential\ncompute costs, making them impractical for high-dimensional datasets. To tackle\nthis issue, we present two key contributions. Firstly, we provide a weighted\nleast squares characterization of weighted Shapley values. Next, using this\ncharacterization, we propose Fast Weighted Shapley (FW-Shapley), an amortized\nframework for efficiently computing weighted Shapley values using a learned\nestimator. We further show that our estimator's training procedure is\ntheoretically valid even though we do not use ground truth Weighted Shapley\nvalues during training. On the feature attribution task, we outperform the\nlearned estimator FastSHAP by $27\\%$ (on average) in terms of Inclusion AUC.\nFor data valuation, we are much faster (14 times) while being comparable to the\nstate-of-the-art KNN Shapley.",
        "Simulating reactive dissolution of solid minerals in porous media has many\nsubsurface applications, including carbon capture and storage (CCS), geothermal\nsystems and oil & gas recovery. As traditional direct numerical simulators are\ncomputationally expensive, it is of paramount importance to develop faster and\nmore efficient alternatives. Deep-learning-based solutions, most of them built\nupon convolutional neural networks (CNNs), have been recently designed to\ntackle this problem. However, these solutions were limited to approximating one\nfield over the domain (e.g. velocity field). In this manuscript, we present a\nnovel deep learning approach that incorporates both temporal and spatial\ninformation to predict the future states of the dissolution process at a fixed\ntime-step horizon, given a sequence of input states. The overall performance,\nin terms of speed and prediction accuracy, is demonstrated on a numerical\nsimulation dataset, comparing its prediction results against state-of-the-art\napproaches, also achieving a speedup around $10^4$ over traditional numerical\nsimulators.",
        "Process Reward Models (PRMs) have proven effective at enhancing mathematical\nreasoning for Large Language Models (LLMs) by leveraging increased\ninference-time computation. However, they are predominantly trained on\nmathematical data and their generalizability to non-mathematical domains has\nnot been rigorously studied. In response, this work first shows that current\nPRMs have poor performance in other domains. To address this limitation, we\nintroduce VersaPRM, a multi-domain PRM trained on synthetic reasoning data\ngenerated using our novel data generation and annotation method. VersaPRM\nachieves consistent performance gains across diverse domains. For instance, in\nthe MMLU-Pro category of Law, VersaPRM via weighted majority voting, achieves a\n7.9% performance gain over the majority voting baseline -- surpassing\nQwen2.5-Math-PRM's gain of 1.3%. We further contribute to the community by\nopen-sourcing all data, code and models for VersaPRM.",
        "Learned optimization has emerged as a promising alternative to hand-crafted\noptimizers, with the potential to discover stronger learned update rules that\nenable faster, hyperparameter-free training of neural networks. A critical\nelement for practically useful learned optimizers, that can be used\noff-the-shelf after meta-training, is strong meta-generalization: the ability\nto apply the optimizers to new tasks. Recent state-of-the-art work in learned\noptimizers, VeLO (Metz et al., 2022), requires a large number of highly diverse\nmeta-training tasks along with massive computational resources, 4000 TPU\nmonths, to achieve meta-generalization. This makes further improvements to such\nlearned optimizers impractical. In this work, we identify several key elements\nin learned optimizer architectures and meta-training procedures that can lead\nto strong meta-generalization. We also propose evaluation metrics to reliably\nassess quantitative performance of an optimizer at scale on a set of evaluation\ntasks. Our proposed approach, Celo, makes a significant leap in improving the\nmeta-generalization performance of learned optimizers and also outperforms\ntuned state-of-the-art optimizers on a diverse set of out-of-distribution\ntasks, despite being meta-trained for just 24 GPU hours.",
        "Single Domain Generalization (SDG) remains a formidable challenge in the\nfield of machine learning, particularly when models are deployed in\nenvironments that differ significantly from their training domains. In this\npaper, we propose a novel data augmentation approach, named as Model-aware\nParametric Batch-wise Mixup (MPBM), to tackle the challenge of SDG. MPBM\ndeploys adversarial queries generated with stochastic gradient Langevin\ndynamics, and produces model-aware augmenting instances with a parametric\nbatch-wise mixup generator network that is carefully designed through an\ninnovative attention mechanism. By exploiting inter-feature correlations, the\nparameterized mixup generator introduces additional versatility in combining\nfeatures across a batch of instances, thereby enhancing the capacity to\ngenerate highly adaptive and informative synthetic instances for specific\nqueries. The synthetic data produced by this adaptable generator network,\nguided by informative queries, is expected to significantly enrich the\nrepresentation space covered by the original training dataset and subsequently\nenhance the prediction model's generalizability across diverse and previously\nunseen domains. To prevent excessive deviation from the training data, we\nfurther incorporate a real-data alignment-based adversarial loss into the\nlearning process of MPBM, regularizing any tendencies toward undesirable\nexpansions. We conduct extensive experiments on several benchmark datasets. The\nempirical results demonstrate that by augmenting the training set with\ninformative synthesis data, our proposed MPBM method achieves the\nstate-of-the-art performance for single domain generalization.",
        "Structured large matrices are prevalent in machine learning. A particularly\nimportant class is curvature matrices like the Hessian, which are central to\nunderstanding the loss landscape of neural nets (NNs), and enable second-order\noptimization, uncertainty quantification, model pruning, data attribution, and\nmore. However, curvature computations can be challenging due to the complexity\nof automatic differentiation, and the variety and structural assumptions of\ncurvature proxies, like sparsity and Kronecker factorization. In this position\npaper, we argue that linear operators -- an interface for performing\nmatrix-vector products -- provide a general, scalable, and user-friendly\nabstraction to handle curvature matrices. To support this position, we\ndeveloped $\\textit{curvlinops}$, a library that provides curvature matrices\nthrough a unified linear operator interface. We demonstrate with\n$\\textit{curvlinops}$ how this interface can hide complexity, simplify\napplications, be extensible and interoperable with other libraries, and scale\nto large NNs.",
        "This paper introduces PINT (Physics-Informed Neural Time Series Models), a\nframework that integrates physical constraints into neural time series models\nto improve their ability to capture complex dynamics. We apply PINT to the ERA5\nWeatherBench dataset, focusing on long-term forecasting of 2m-temperature data.\nPINT incorporates the Simple Harmonic Oscillator Equation as a physics-informed\nprior, embedding its periodic dynamics into RNN, LSTM, and GRU architectures.\nThis equation's analytical solutions (sine and cosine functions) facilitate\nrigorous evaluation of the benefits of incorporating physics-informed\nconstraints. By benchmarking against a linear regression baseline derived from\nits exact solutions, we quantify the impact of embedding physical principles in\ndata-driven models. Unlike traditional time series models that rely on future\nobservations, PINT is designed for practical forecasting. Using only the first\n90 days of observed data, it iteratively predicts the next two years,\naddressing challenges posed by limited real-time updates. Experiments on the\nWeatherBench dataset demonstrate PINT's ability to generalize, capture periodic\ntrends, and align with physical principles. This study highlights the potential\nof physics-informed neural models in bridging machine learning and\ninterpretable climate applications.\n  Our models and datasets are publicly available on GitHub:\nhttps:\/\/github.com\/KV-Park.",
        "Handling uncertainty is critical for ensuring reliable decision-making in\nintelligent systems. Modern neural networks are known to be poorly calibrated,\nresulting in predicted confidence scores that are difficult to use. This\narticle explores improving confidence estimation and calibration through the\napplication of bilevel optimization, a framework designed to solve hierarchical\nproblems with interdependent optimization levels. A self-calibrating bilevel\nneural-network training approach is introduced to improve a model's predicted\nconfidence scores. The effectiveness of the proposed framework is analyzed\nusing toy datasets, such as Blobs and Spirals, as well as more practical\nsimulated datasets, such as Blood Alcohol Concentration (BAC). It is compared\nwith a well-known and widely used calibration strategy, isotonic regression.\nThe reported experimental results reveal that the proposed bilevel optimization\napproach reduces the calibration error while preserving accuracy.",
        "Machine learning bias in mental health is becoming an increasingly pertinent\nchallenge. Despite promising efforts indicating that multitask approaches often\nwork better than unitask approaches, there is minimal work investigating the\nimpact of multitask learning on performance and fairness in depression\ndetection nor leveraged it to achieve fairer prediction outcomes. In this work,\nwe undertake a systematic investigation of using a multitask approach to\nimprove performance and fairness for depression detection. We propose a novel\ngender-based task-reweighting method using uncertainty grounded in how the\nPHQ-8 questionnaire is structured. Our results indicate that, although a\nmultitask approach improves performance and fairness compared to a unitask\napproach, the results are not always consistent and we see evidence of negative\ntransfer and a reduction in the Pareto frontier, which is concerning given the\nhigh-stake healthcare setting. Our proposed approach of gender-based\nreweighting with uncertainty improves performance and fairness and alleviates\nboth challenges to a certain extent. Our findings on each PHQ-8 subitem task\ndifficulty are also in agreement with the largest study conducted on the PHQ-8\nsubitem discrimination capacity, thus providing the very first tangible\nevidence linking ML findings with large-scale empirical population studies\nconducted on the PHQ-8.",
        "The integration of Federated Learning (FL) and Multi-Task Learning (MTL) has\nbeen explored to address client heterogeneity, with Federated Multi-Task\nLearning (FMTL) treating each client as a distinct task. However, most existing\nresearch focuses on data heterogeneity (e.g., addressing non-IID data) rather\nthan task heterogeneity, where clients solve fundamentally different tasks.\nAdditionally, much of the work relies on centralized settings with a server\nmanaging the federation, leaving the more challenging domain of decentralized\nFMTL largely unexplored. Thus, this work bridges this gap by proposing ColNet,\na framework designed for heterogeneous tasks in decentralized federated\nenvironments. ColNet divides models into the backbone and task-specific layers,\nforming groups of similar clients, with group leaders performing\nconflict-averse cross-group aggregation. A pool of experiments with different\nfederations demonstrated ColNet outperforms the compared aggregation schemes in\ndecentralized settings with label and task heterogeneity scenarios.",
        "With a growing data privacy concern, federated learning has emerged as a\npromising framework to train machine learning models without sharing locally\ndistributed data. In federated learning, local model training by multiple\nclients and model integration by a server are repeated only through model\nparameter sharing. Most existing federated learning methods assume training\ndeep learning models, which are often computationally demanding. To deal with\nthis issue, we propose federated learning methods with reservoir state analysis\nto seek computational efficiency and data privacy protection simultaneously.\nSpecifically, our method relies on Mahalanobis Distance of Reservoir States\n(MD-RS) method targeting time series anomaly detection, which learns a\ndistribution of reservoir states for normal inputs and detects anomalies based\non a deviation from the learned distribution. Iterative updating of statistical\nparameters in the MD-RS enables incremental federated learning (IncFed MD-RS).\nWe evaluate the performance of IncFed MD-RS using benchmark datasets for time\nseries anomaly detection. The results show that IncFed MD-RS outperforms other\nfederated learning methods with deep learning and reservoir computing models\nparticularly when clients' data are relatively short and heterogeneous. We\ndemonstrate that IncFed MD-RS is robust against reduced sample data compared to\nother methods. We also show that the computational cost of IncFed MD-RS can be\nreduced by subsampling from the reservoir states without performance\ndegradation. The proposed method is beneficial especially in anomaly detection\napplications where computational efficiency, algorithm simplicity, and low\ncommunication cost are required.",
        "Recent advancements in large language models (LLMs) reveal a perplexing\nphenomenon in continual learning: despite extensive training, models experience\nsignificant performance declines, raising questions about task alignment and\nunderlying knowledge retention. This study first explores the concept of\n\"spurious forgetting\", proposing that such performance drops often reflect a\ndecline in task alignment rather than true knowledge loss. Through controlled\nexperiments with a synthesized dataset, we investigate the dynamics of model\nperformance during the initial training phases of new tasks, discovering that\nearly optimization steps can disrupt previously established task alignments.\nOur theoretical analysis connects these shifts to orthogonal updates in model\nweights, providing a robust framework for understanding this behavior.\nUltimately, we introduce a Freezing strategy that fix the bottom layers of the\nmodel, leading to substantial improvements in four continual learning\nscenarios. Our findings underscore the critical distinction between task\nalignment and knowledge retention, paving the way for more effective strategies\nin continual learning.",
        "A tenet of reinforcement learning is that rewards are always observed by the\nagent. However, this is not true in many realistic settings, e.g., a human\nobserver may not always be able to provide rewards, a sensor to observe rewards\nmay be limited or broken, or rewards may be unavailable during deployment.\nMonitored Markov decision processes (Mon-MDPs) have recently been proposed as a\nmodel of such settings. Yet, Mon-MDP algorithms developed thus far do not fully\nexploit the problem structure, cannot take advantage of a known monitor, have\nno worst-case guarantees for ``unsolvable'' Mon-MDPs without specific\ninitialization, and only have asymptotic proofs of convergence. This paper\nmakes three contributions. First, we introduce a model-based algorithm for\nMon-MDPs that addresses all of these shortcomings. The algorithm uses two\ninstances of model-based interval estimation, one to guarantee that observable\nrewards are indeed observed, and another to learn the optimal policy. Second,\nempirical results demonstrate these advantages, showing faster convergence than\nprior algorithms in over two dozen benchmark settings, and even more dramatic\nimprovements when the monitor process is known. Third, we present the first\nfinite-sample bound on performance and show convergence to an optimal\nworst-case policy when some rewards are never observable.",
        "For a positive integer $n$ let $\\mathcal{X}_n$ be either the algebra $M_n$ of\n$n \\times n$ complex matrices, the set $N_n$ of all $n \\times n$ normal\nmatrices, or any of the matrix Lie groups $\\mathrm{GL}(n)$, $\\mathrm{SL}(n)$\nand $\\mathrm{U}(n)$. We first give a short and elementary argument that for two\npositive integers $m$ and $n$ there exists a continuous spectrum-shrinking map\n$\\phi : \\mathcal{X}_n \\to M_m$ (i.e.\\ $\\mathrm{sp}(\\phi(X))\\subseteq\n\\mathrm{sp}(X)$ for all $X \\in \\mathcal{X}_n$) if and only if $n$ divides $m$.\nMoreover, in that case we have the equality of characteristic polynomials\n$k_{\\phi(X)}(\\cdot) = k_{X}(\\cdot)^\\frac{m}{n}$ for all $X \\in \\mathcal{X}_n$,\nwhich in particular shows that $\\phi$ preserves spectra. Using this we show\nthat whenever $n \\geq 3$, any continuous commutativity preserving and\nspectrum-shrinking map $\\phi : \\mathcal{X}_n \\to M_n$ is of the form\n$\\phi(\\cdot)=T(\\cdot)T^{-1}$ or $\\phi(\\cdot)=T(\\cdot)^tT^{-1}$, for some $T\\in\n\\mathrm{GL}(n)$. The analogous results fail for the special unitary group\n$\\mathrm{SU}(n)$, and slightly more elaborate versions hold for the spaces of\nsemisimple elements in either $\\mathrm{GL}(n)$ or $\\mathrm{SL}(n)$, where a\nqualitatively new (and surprising) phenomenon arises: the map sending\n$SNS^{-1}$ to $S^{-1}NS$ for positive invertible $S$ and normal $N$ is also an\nexample. As a consequence, we also recover (a strengthened version of)\n\\v{S}emrl's influential characterization of Jordan automorphisms of $M_n$ via\npreserving properties.",
        "Aligning large language models (LLMs) to human preferences is challenging in\ndomains where preference data is unavailable. We address the problem of\nlearning reward models for such target domains by leveraging feedback collected\nfrom simpler source domains, where human preferences are easier to obtain. Our\nkey insight is that, while domains may differ significantly, human preferences\nconvey \\emph{domain-agnostic} concepts that can be effectively captured by a\nreward model. We propose \\method, a framework that trains domain-invariant\nreward models by optimizing a dual loss: a domain loss that minimizes the\ndivergence between source and target distribution, and a source loss that\noptimizes preferences on the source domain. We show \\method is a general\napproach that we evaluate and analyze across 4 distinct settings: (1)\nCross-lingual transfer (accuracy: $0.621 \\rightarrow 0.661$), (2)\nClean-to-noisy (accuracy: $0.671 \\rightarrow 0.703$), (3) Few-shot-to-full\ntransfer (accuracy: $0.845 \\rightarrow 0.920$), and (4) Simple-to-complex tasks\ntransfer (correlation: $0.508 \\rightarrow 0.556$). Our code, models and data\nare available at \\url{https:\/\/github.com\/portal-cornell\/dial}.",
        "We investigate whether and why people might reduce compensation for workers\nwho use AI tools. Across 10 studies (N = 3,346), participants consistently\nlowered compensation for workers who used AI tools. This \"AI Penalization\"\neffect was robust across (1) different types of work and worker statuses and\nworker statuses (e.g., full-time, part-time, or freelance), (2) different forms\nof compensation (e.g., required payments or optional bonuses) and their timing,\n(3) various methods of eliciting compensation (e.g., slider scale, multiple\nchoice, and numeric entry), and (4) conditions where workers' output quality\nwas held constant, subject to varying inferences, or controlled for. Moreover,\nthe effect emerged not only in hypothetical compensation scenarios (Studies\n1-5) but also with real gig workers and real monetary compensation (Study 6).\nPeople reduced compensation for workers using AI tools because they believed\nthese workers deserved less credit than those who did not use AI (Studies 3 and\n4). This effect weakened when it is less permissible to reduce worker\ncompensation, such as when employment contracts provide stricter constraints\n(Study 4). Our findings suggest that adoption of AI tools in the workplace may\nexacerbate inequality among workers, as those protected by structured contracts\nface less vulnerability to compensation reductions, while those without such\nprotections risk greater financial penalties for using AI.",
        "This paper proposes the DnD Filter, a differentiable filter that utilizes\ndiffusion models for state estimation of dynamic systems. Unlike conventional\ndifferentiable filters, which often impose restrictive assumptions on process\nnoise (e.g., Gaussianity), DnD Filter enables a nonlinear state update without\nsuch constraints by conditioning a diffusion model on both the predicted state\nand observational data, capitalizing on its ability to approximate complex\ndistributions. We validate its effectiveness on both a simulated task and a\nreal-world visual odometry task, where DnD Filter consistently outperforms\nexisting baselines. Specifically, it achieves a 25\\% improvement in estimation\naccuracy on the visual odometry task compared to state-of-the-art\ndifferentiable filters, and even surpasses differentiable smoothers that\nutilize future measurements. To the best of our knowledge, DnD Filter\nrepresents the first successful attempt to leverage diffusion models for state\nestimation, offering a flexible and powerful framework for nonlinear estimation\nunder noisy measurements.",
        "In this paper, we obtain the boundedness of $m$th order commutators generated\nby the $n$-dimensional fractional Hardy operator with rough kernel and its\nadjoint operator with BMO functions on two weighted grand Herz-Morrey spaces\nwith variable exponents. Replacing Lipschitz functions with BMO functions the\ncorresponding result is also given.",
        "Partially Controlled Multi-Agent Systems (PCMAS) are comprised of\ncontrollable agents, managed by a system designer, and uncontrollable agents,\noperating autonomously. This study addresses an optimal composition design\nproblem in PCMAS, which involves the system designer's problem, determining the\noptimal number and policies of controllable agents, and the uncontrollable\nagents' problem, identifying their best-response policies. Solving this\nbi-level optimization problem is computationally intensive, as it requires\nrepeatedly solving multi-agent reinforcement learning problems under various\ncompositions for both types of agents. To address these challenges, we propose\na novel hypernetwork-based framework that jointly optimizes the system's\ncomposition and agent policies. Unlike traditional methods that train separate\npolicy networks for each composition, the proposed framework generates policies\nfor both controllable and uncontrollable agents through a unified hypernetwork.\nThis approach enables efficient information sharing across similar\nconfigurations, thereby reducing computational overhead. Additional\nimprovements are achieved by incorporating reward parameter optimization and\nmean action networks. Using real-world New York City taxi data, we demonstrate\nthat our framework outperforms existing methods in approximating equilibrium\npolicies. Our experimental results show significant improvements in key\nperformance metrics, such as order response rate and served demand,\nhighlighting the practical utility of controlling agents and their potential to\nenhance decision-making in PCMAS.",
        "A study of the Revista General de Informacion y Documentacion, from 2005 to\n2022. The objective is aimed at qualifying the structure of the research field\nand assessing the trajectory of the thematic areas covered. Applying as\nmethodology the analysis of co-words, the construction of bibliometric networks\nand the creation of scientific maps. 514 documents are extracted from the Web\nof Science (WoS) database. The keywords assigned by the authors of the\ndocuments are selected and divided into three subperiods: 2005-2010, 2011-2016\nand 2017-2022. In the results, 1701 author keywords and 37 bibliometric\nnetworks are obtained. In the period 2005-2010, the structure of the research\nfield is represented on the scientific map with very few central and\nspecialized topics, considering an initial and underdeveloped organization. In\nthe period 2011-2016, the structure of the research field is distributed on the\nscientific map with a more varied number of central and specialized topics, but\nstill insufficient, considering an organization in the process of development.\nIn the period 2017-2022, the structure of the research field is shown on the\nmap with all kinds of family of topics (central, specialized, transversal,\nemerging or disappearing), being valued as a dynamic, complex and heterogeneous\norganization. Regarding the evolution of the thematic areas, the map shows\nsolid progress between the last two periods. The morphology of the thematic\nfield treated in RGID is outlined in three phases: foundation, process of\ndevelopment and consolidation.",
        "In the book [Meurant and Tichy, SIAM, 2024] we discussed the estimation of\nerror norms in the conjugate gradient (CG) algorithm for solving linear systems\n$Ax=b$ with a symmetric positive definite matrix $A$, where $b$ and $x$ are\nvectors. In this paper, we generalize the most important formulas for\nestimating the $A$-norm of the error to the block case. First, we discuss in\ndetail the derivation of various variants of the block CG (BCG) algorithm from\nthe block Lanczos algorithm. We then consider BCG and derive the related block\nGauss and block Gauss-Radau quadrature rules. We show how to obtain lower and\nupper bounds on the $A$-norm of the error of each system, both in terms of the\nquantities computed in BCG and in terms of the underlying block Lanczos\nalgorithm. Numerical experiments demonstrate the behavior of the bounds in\npractical computations.",
        "Transcription factors are proteins that regulate the expression of genes by\nbinding to specific genomic regions known as Transcription Factor Binding Sites\n(TFBSs), typically located in the promoter regions of those genes. Accurate\nprediction of these binding sites is essential for understanding the complex\ngene regulatory networks underlying various cellular functions. In this regard,\nmany deep learning models have been developed for such prediction, but there is\nstill scope of improvement. In this work, we have developed a deep learning\nmodel which uses pre-trained DNABERT, a Convolutional Neural Network (CNN)\nmodule, a Modified Convolutional Block Attention Module (MCBAM), a Multi-Scale\nConvolutions with Attention (MSCA) module and an output module. The pre-trained\nDNABERT is used for sequence embedding, thereby capturing the long-term\ndependencies in the DNA sequences while the CNN, MCBAM and MSCA modules are\nuseful in extracting higher-order local features. TFBS-Finder is trained and\ntested on 165 ENCODE ChIP-seq datasets. We have also performed ablation studies\nas well as cross-cell line validations and comparisons with other models. The\nexperimental results show the superiority of the proposed method in predicting\nTFBSs compared to the existing methodologies. The codes and the relevant\ndatasets are publicly available at\nhttps:\/\/github.com\/NimishaGhosh\/TFBS-Finder\/.",
        "Video-conferencing applications face an unwavering surge in traffic,\nstressing their underlying infrastructure in unprecedented ways. This paper\nrethinks the key building block for conferencing infrastructures -- selective\nforwarding units (SFUs). SFUs relay and adapt media streams between\nparticipants and, today, run in software on general-purpose servers. Our main\ninsight, discerned from dissecting the operation of production SFU servers, is\nthat SFUs largely mimic traditional packet-processing operations such as\ndropping and forwarding. Guided by this, we present Scallop, an SDN-inspired\nSFU that decouples video-conferencing applications into a hardware-based data\nplane for latency-sensitive and frequent media operations, and a software\ncontrol plane for the (infrequent) remaining tasks, such as analyzing feedback\nsignals. Our Tofino-based implementation fully supports WebRTC and delivers\n7-210 times improved scaling over a 32-core commodity server, while reaping\nperformance improvements by cutting forwarding-induced latency by 26 times.",
        "Labeled transition systems can be a great way to visualize the complex\nbehavior of parallel and communicating systems. However, if, during a\nparticular timeframe, no synchronization or communication between processes\noccurs, then multiple parallel sequences of actions are able to interleave\narbitrarily, and the resulting graph quickly becomes too complex for the human\neye to understand easily. With that in mind, we propose an exact formalization\nof these arbitrary interleavings, and an algorithm to find all said\ninterleavings in deterministic LTSs, to reduce the visual complexity of labeled\ntransition systems.",
        "Precision therapeutics require multimodal adaptive models that generate\npersonalized treatment recommendations. We introduce TxAgent, an AI agent that\nleverages multi-step reasoning and real-time biomedical knowledge retrieval\nacross a toolbox of 211 tools to analyze drug interactions, contraindications,\nand patient-specific treatment strategies. TxAgent evaluates how drugs interact\nat molecular, pharmacokinetic, and clinical levels, identifies\ncontraindications based on patient comorbidities and concurrent medications,\nand tailors treatment strategies to individual patient characteristics. It\nretrieves and synthesizes evidence from multiple biomedical sources, assesses\ninteractions between drugs and patient conditions, and refines treatment\nrecommendations through iterative reasoning. It selects tools based on task\nobjectives and executes structured function calls to solve therapeutic tasks\nthat require clinical reasoning and cross-source validation. The ToolUniverse\nconsolidates 211 tools from trusted sources, including all US FDA-approved\ndrugs since 1939 and validated clinical insights from Open Targets. TxAgent\noutperforms leading LLMs, tool-use models, and reasoning agents across five new\nbenchmarks: DrugPC, BrandPC, GenericPC, TreatmentPC, and DescriptionPC,\ncovering 3,168 drug reasoning tasks and 456 personalized treatment scenarios.\nIt achieves 92.1% accuracy in open-ended drug reasoning tasks, surpassing\nGPT-4o and outperforming DeepSeek-R1 (671B) in structured multi-step reasoning.\nTxAgent generalizes across drug name variants and descriptions. By integrating\nmulti-step inference, real-time knowledge grounding, and tool-assisted\ndecision-making, TxAgent ensures that treatment recommendations align with\nestablished clinical guidelines and real-world evidence, reducing the risk of\nadverse events and improving therapeutic decision-making.",
        "Compared with line-of-sight (LOS) communication, nonline-of-sight (NLOS)\nunderwater wireless optical communication (UWOC) systems have garnered\nextensive attention because of their heightened suitability for the intricate\nand dynamic underwater environment. In the NLOS channel, photons can reach the\nreceiver by sea surface reflection or particle scattering. However, research\nlacks comprehensive channel models that incorporate sea surface reflection and\nparticle scattering. Moreover, the presence of ocean turbulence introduces\nrandom fluctuations in the received optical signal based on the average light\nintensity. Consequently, this paper adopts the Monte Carlo simulation method\n(MCS) to solve the fading-free impulse response of the joint\nreflection-scattering channel. Furthermore, a weighted double gamma function\n(WDGF) is proposed to characterize the channel impulse response (CIR). Based on\nthe closed CIR model, the average bit error rate and the performance of the\ninterruption probability of the UWOC system under turbulence are analyzed. The\nconclusions obtained are intended to assist in the design and performance\nevaluation of NLOS UWOC systems.",
        "This paper presents a novel AI-based smart traffic management system\nde-signed to optimize traffic flow and reduce congestion in urban environments.\nBy analysing live footage from existing CCTV cameras, this approach eliminates\nthe need for additional hardware, thereby minimizing both deployment costs and\nongoing maintenance expenses. The AI model processes live video feeds to\naccurately count vehicles and assess traffic density, allowing for adaptive\nsignal control that prioritizes directions with higher traffic volumes. This\nreal-time adaptability ensures smoother traffic flow, reduces congestion, and\nminimizes waiting times for drivers. Additionally, the proposed system is\nsimulated using PyGame to evaluate its performance under various traffic\nconditions. The simulation results demonstrate that the AI-based system\nout-performs traditional static traffic light systems by 34%, leading to\nsignificant improvements in traffic flow efficiency. The use of AI to optimize\ntraffic signals can play a crucial role in addressing urban traffic challenges,\noffering a cost-effective, scalable, and efficient solution for modern cities.\nThis innovative system represents a key advancement in the field of smart city\ninfra-structure and intelligent transportation systems.",
        "We perform a detailed investigation of the CCDM (creation of cold dark\nmatter) cosmologies using the powerful techniques of qualitative analysis of\ndynamical systems. Considering a wide variety of the creation rates ranging\nfrom constant to dynamical, we examine the nature of critical points and their\nstability obtained from the individual scenario consisting of only cold dark\nmatter, or cold dark matter plus a second fluid with constant equation of\nstate. According to our analyses, these scenarios predict unstable dark matter\ndominated critical points, stable accelerating attractors dominated either by\ndark matter or the second fluid, scaling attractors in which both dark matter\nand the second fluid co-exist. Along with these critical points, these\nscenarios also indicate the possibility of decelerating attractors or\ndecelerating scaling attractors in the future which are new results in this\ndirection. These altogether suggest that CCDM cosmologies are viable\nalternatives to the mainstream cosmological models."
      ]
    }
  },
  {
    "id":2412.19927,
    "research_type":"applied",
    "start_id":"b40",
    "start_title":"Physics guided neural networks for spatio-temporal superresolution of turbulent flows",
    "start_abstract":"Direct numerical simulation (DNS) of turbulent flows is computationally expensive and cannot be applied to flows with large Reynolds numbers. Low-resolution large eddy simulation (LES) is a popular alternative, but it is unable to capture all of the scales of turbulent transport accurately. Reconstructing DNS from low-resolution LES is critical for large-scale simulation in many scientific and engineering disciplines, but it poses many challenges to existing super-resolution methods due to the complexity of turbulent flows and computational cost of generating frequent LES data. We propose a physics-guided neural network for reconstructing frequent DNS from sparse LES data by enhancing its spatial resolution and temporal frequency. Our proposed method consists of a partial differential equation (PDE)-based recurrent unit for capturing underlying temporal processes and a physics-guided super-resolution model that incorporates additional physical constraints. We demonstrate the effectiveness of both components in reconstructing the Taylor-Green Vortex using sparse LES data. Moreover, we show that the proposed recurrent unit can preserve the physical characteristics of turbulent flows by leveraging the physical relationships in the Navier-Stokes equation.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Model-free simulations of turbulent reactive flows"
      ],
      "abstract":[
        "A critical review of the modern computational methods for solving the transport equations of turbulent reacting single-phase flows is presented. Primary consideration is given to those methods which lead to \u2018model-free\u2019 simulations while some attention is devoted to \u2018turbulence modeling\u2019. Emphasis is placed upon the role of supercomputers and how their increased computational capacities may be exploited to allow better simulations of the physics of turbulent reactive flows. Comparisons between the commonly employed computational schemes for simulating these flows are given, with the advantages and the limitations associated with each scheme being highlighted. Examples are presented of recent applications of model-free simulations to a variety of unsteady reacting flows, with detailed discussions on the physical phenomena captured by such simulations. Due to the nature of this review, experimental contributions are mentioned only in the context of providing empirical evidence. References are made to other contributions which are not directly related to the computational efforts in order to provide a reasonably comprehensive bibliography for those interested in pursuing various topics in greater detail. Predictions of future accomplishments, as well as some suggestions for future work, are also given."
      ],
      "categories":[
        "physics.flu-dyn"
      ]
    },
    "list":{
      "title":[
        "Bioinspired Drone Rotors for Reduced Aeroacoustic Noise and Improved\n  Efficiency",
        "Insights of Transitions to Thermoacoustic Instability in Inverse\n  Diffusion Flame using Multifractal Detrended Fluctuation Analysis",
        "On the role of morphology and kinematics of biological swimmers to\n  spread and suppress their odors in the wake",
        "On the Separating Flow Behind a Cylinder: Insights from the Principle of\n  Minimum Pressure Gradient",
        "The origin of vorticity in viscous incompressible flows",
        "Hydrodynamically Beneficial School Configurations in Carangiform\n  Swimmers: Insights from a Flow-Physics Informed Model",
        "UV Radiation Measurement of Nitrogen in shock focusing facility",
        "Investigation of hydrogen under-expanded jets in gaseous propulsion\n  systems",
        "A toy model of turbulent shear flow using vortons",
        "An aerodynamic measurement system to improve the efficiency of wind\n  turbine rotor blades",
        "A Numerical Investigation of Particle Deposition on a Substrate",
        "Reduced Basis Model for Compressible Flow",
        "Bayesian Optimization of the GEKO Turbulence Model for Predicting Flow\n  Separation Over a Smooth Surface",
        "To Hedge or Not to Hedge: Optimal Strategies for Stochastic Trade Flow\n  Management",
        "Freelance Holography, Part II: Moving Boundary in Gauge\/Gravity\n  Correspondence",
        "Dynamically Learning to Integrate in Recurrent Neural Networks",
        "Automated DC Voltage and DC Resistance Real-time Multiple Standard for\n  Artifact Calibration of Calibrators and multimeters",
        "Two almost planetary mass survivors of common envelope evolution",
        "Electrochemically induced hyperfluorescence based on the formation of\n  charge-transfer excimers",
        "Computation of generalised magnetic coordinates asymptotically close to\n  the separatrix",
        "The diffuse extragalactic gamma-ray background radiation: star-forming\n  galaxies are not the dominant component",
        "Tight Analysis of Difference-of-Convex Algorithm (DCA) Improves\n  Convergence Rates for Proximal Gradient Descent",
        "An Open Source Validation System for Continuous Arterial Blood Pressure\n  Measuring Sensors",
        "Updated analysis of neutron magnetic form factor and the nucleon\n  transverse densities",
        "Power residue symbols and the exponential local-global principle",
        "In-medium bottomonium properties from lattice NRQCD calculations with\n  extended meson operators",
        "MXMap: A Multivariate Cross Mapping Framework for Causal Discovery in\n  Dynamical Systems",
        "Adsorption Behavior of Greenhouse Gases on Carbon Nanobelts: A\n  Semi-Empirical Tight-Binding Approach for Environmental Application"
      ],
      "abstract":[
        "The application of unmanned aerial vehicles (UAVs) is surging across several\nindustries, paralleled by growing demand for these UAVs. However, the noise\nemitted by UAVs remains a significant impediment to their widespread use even\nthough in areas such as product delivery, they can be more environmentally\nfriendly than traditional delivery methods. Nature has often been a source of\ninspiration for devices that are efficient and eco-friendly. In the current\nstudy, we leverage the previous work by Seo et al. (Bioinsp. Biomimetics, 16\n(4):046019, 2021) on the aeroacoustics of flapping wing flight in mosquitoes\nand fruit flies to propose and examine a simple strategy for reducing the\naeroacoustic noise from drone rotors. In particular, inspired by these insects,\nwe explore how an increase in the planform area of the rotor could be used to\nreduce the rotation rate and the associated aeroacoustic noise from small-scale\nrotors. The study employs a sharp-interface immersed boundary solver for the\nflow simulations and the aeroacoustic sound is predicted by the Ffowcs\nWilliams-Hawkings equation. Simulations indicate that the simple strategy of\nemploying rotors with larger planform areas could lead not just to reduced\naeroacoustic noise but improved power economy as well.",
        "The inverse diffusion flame (IDF) can experience thermoacoustic instability\ndue to variations in power input or flow conditions. However, the dynamical\ntransitions in IDF that lead to this instability when altering control\nparameters have not been thoroughly investigated. In this study, we explore the\ncontrol parameters through two different approaches and employ multifractal\ndetrended fluctuation analysis to characterize the transitions observed prior\nto the onset of thermoacoustic instability in the inverse diffusion flame. Our\nfindings reveal a loss of multifractality near the region associated with\nthermoacoustic instability, which suggests a more ordered behavior. We\ndetermine that the singularity exponent, the width of the multifractal\nspectrum, and the Hurst exponent are reliable indicators of thermoacoustic\ninstability and serve as effective classifiers of dynamical states in inverse\ndiffusion flames.",
        "Understanding the interplay between hydrodynamics and chemical sensing in\naquatic environments is crucial for unraveling biological swimmers' navigation,\nforaging, and communication strategies. This study investigates the role of\nkinematics and morphologies of fish in dispersion and suppression of odor cues\nin their wake. We employ high-fidelity three-dimensional computational fluid\ndynamics simulations, integrating a sharp-interface immersed-boundary method\nwith an odor transport model. Using carangiform and anguilliform kinematics for\na jackfish and an eel, we analyze the transport of chemical cues in the wake of\nundulatory swimmers at a Reynolds number of 3000 and Strouhal numbers of 0.25\nand 0.4. Our findings reveal that odor plumes closely align with vortex\nstructures, emphasizing a strong coupling between hydrodynamics and chemical\ndispersion. We demonstrate that kinematics, rather than morphology,\npredominantly govern odor transport, with anguilliform motion generating\nbroader, more persistent odor trails. Increasing the amplitude of undulation\nimproves the effectiveness of the odor, driven primarily by convection, while\ndiffusion plays a secondary role. These insights provide a deeper understanding\nof underwater sensing mechanisms and inform the design of bio-inspired robotic\nsystems with improved navigation and chemical detection capabilities.",
        "We study the separating flow over a circular cylinder with two objectives:\n(i) to demonstrate the validity of the condition of matching curvature, and\n(ii) to obtain a reasonable estimate of the separation angle in the subcritical\nregime (Re=10^4-10^5) without explicitly modeling the boundary layer. First, we\nstudy Roshko's free streamline model (1954); it is an ideal flow model with\nsheets of discontinuities that represent the separating shear layers in the\nnear wake region. The model fails to predict the correct separation angle over\na cylinder. Roshko attributed this discrepancy to the condition of matching\ncurvature, which asserts that the curvature of the separating streamline at the\nseparation point must match that of the cylinder. We show that such a condition\nis legitimate and is not the real culprit for the failure of Roshko's model in\npredicting separation. Second, we employ the principle of minimum pressure\ngradient (PMPG), which asserts that, an incompressible flow evolves by\nminimizing the total magnitude of the pressure gradient over the domain.\nEncouraged by the fact that the flow characteristics in the range Re=10^4-10^5\nare fairly independent of Re, we aim to predict the separation angle in this\nregime without modeling the boundary layer -- a task that may seem impossible,\nthough anticipated by Prandtl in his seminal paper (Prandtl 1904). Over the\nfamily of kinematically-admissible, equilibrium flows, we utilize the PMPG to\nsingle out the separating flow with the minimum pressure gradient cost.\nInterestingly, the obtained separation angles match experimental measurements\nover the regime Re=10^4-10^5.",
        "In inviscid, incompressible flows, the evolution of vorticity is exactly\nequivalent to that of an infinitesimal material line-element, and hence\nvorticity can be traced forward or backward in time in a Lagrangian fashion.\nThis elegant and powerful description is not possible in viscous flows due to\nthe action of diffusion. Instead, a stochastic Lagrangian interpretation is\nrequired and was recently introduced, where the origin of vorticity at a point\nis traced back in time as an expectation over the contribution from stochastic\ntrajectories. We herein introduce for the first time an Eulerian, adjoint-based\napproach to quantify the back-in-time origin of vorticity in viscous,\nincompressible flows. The adjoint variable encodes the advection, tilting and\nstretching of the earlier-in-time vorticity that ultimately leads to the target\nvalue. Precisely, the adjoint vorticity is the volume-density of the mean\nLagrangian deformation of the earlier vorticity. The formulation can also\naccount for the injection of vorticity into the domain at solid boundaries. We\ndemonstrate the mathematical equivalence of the adjoint approach and the\nstochastic Lagrangian approach. We then provide an example from turbulent\nchannel flow, where we analyze the origin of high-stress events and relate them\nto Lighthill's mechanism of stretching of near-wall vorticity.",
        "Researchers have long debated which spatial arrangements and swimming\nsynchronizations are beneficial for the hydrodynamic performance of fish in\nschools. In our previous work (Seo and Mittal, Bioinsp. Biomim., Vol. 17,\n066020, 2022), we demonstrated using direct numerical simulations that\nhydrodynamic interactions with the wake of a leading body-caudal fin\ncarangiform swimmer could significantly enhance the swimming performance of a\ntrailing swimmer by augmenting the leading-edge vortex (LEV) on its caudal fin.\nIn this study, we develop a model based on the phenomenology of LEV\nenhancement, which utilizes wake velocity data from direct numerical\nsimulations of a leading fish to predict the trailing swimmer's hydrodynamic\nperformance without additional simulations. This approach enables a\ncomprehensive analysis of the effects of relative positioning, phase\ndifference, flapping amplitude, Reynolds number, and the number of swimmers in\nthe school on thrust enhancement. The results offer several insights regarding\nthe effect of these parameters that have implications for fish schools as well\nas for bio-inspired underwater vehicle applications.",
        "A study on UV radiation in shock focused gas has been carried out in high\ntemperature radiating Air experimentally. Spherical shock wave focusing was\nachieved with the help of a contoured converging section attached to a shock\ntube. The measured radiation is compared with SPECAIR software to find out that\nthe radiation corresponds to the emission of N2+ Meinal transition. The\ntemperature of the radiating gas was estimated from SPECAIR and was found to be\naround 6000K.",
        "Underexpanded jets are present in various engineering applications; in recent\nyears, they have gained special attention because of the development of\ngaseous-fueled propulsion systems. In these apparatuses, the direct injection\nof fuels such as hydrogen in innovative low-emission engines' chambers induces\nturbulent under-expanded jets. In this study, we performed high-fidelity Large\nEddy Simulations of under-expanded hydrogen jets to investigate these flows and\nprovide valuable insights for developing injectors suitable for hydrogen and,\nmore generally, gaseous-fueled propulsion systems. We initially assessed the\nmethod's accuracy, evaluating the convergence and uncertainty of the numerical\nresults and validating them against experimental particle image velocimetry and\nSchlieren data. The simulated jets, the Mach disc dimensions, and the resulting\nvelocity field align closely with the experimental observations. Then we\nanalyzed the jet structure for pressure ratios of 4 to 25 and examined the\neffects of the geometrical configuration of the nozzle on the characteristics\nof the air-fuel mixture obtained. We compared the jets resulting from a\nround-hole nozzle with annular ones resembling outward-opening injectors.",
        "We introduce a novel toy model for shear flows, exploiting the spatial\nintermittency and the scale separation between large-scale flows and\nsmall-scale structures. The model is highly sparse, focusing exclusively on the\nmost intense structures, which are represented by vortons: dynamically\nregularized quasi-singularities that experience rapid distortion from the\nlarge-scale shear. The vortons, in turn, influence the large-scale flow through\nthe sub-grid stress tensor. Despite its simplicity, the model displays an\ninteresting transition between two distinct regimes: (i) a laminar regime,\nwhere dissipation is entirely attributed to the large-scale flow, and the\nvortons dynamics is essentially diffusive, and (ii) a turbulent regime, in\nwhich most of the dissipation arises from the vortons. These regimes correspond\nto different scalings of dissipation and the Grashof number as functions of the\nReynolds number, with power-law relationships that resemble those observed in\nclassical turbulence.",
        "The wind energy sector is growing rapidly with the installation of wind\nturbines with long, slender blades in a diverse range of locations. To enhance\nthe operational performance under specific wind conditions and to validate the\naerodynamic design of flexible blades, it is crucial to obtain comprehensive\ndata on the aerodynamic behaviour of the blades in the field, such as\ntime-resolved pressure distributions, local inflow conditions and dynamic\nresponses of the blade. However, published field measurements are scarce for\nlarge-scale rotor blades due to the complex and costly installation of the\nrequisite measurement systems. In recent work, we developed a wireless and\nself-sufficient aerodynamic measurement system, named Aerosense, which is less\ncomplex and costly than conventional aerodynamic measurement systems. The\nAerosense system uses Micro-Electro-Mechanical Systems (MEMS) sensors to obtain\nlocal aerodynamic pressures, blade motions, and inflow conditions. In this\npaper, we demonstrate the value of Aerosense in understanding the aerodynamic\nbehaviour of rotor blades, using a 7kW wind turbine operating in the field.\nAfter a thorough calibration and correction process, we demonstrate, for\nexample, that the pressure distribution can vary significantly during one\nrotation of the blade, even under stable wind conditions. These variations are\nfound to be due to the misalignment of the wind direction with the wind\nturbine's rotational axis. We therefore conclude that the Aerosense measurement\nsystem is valuable for understanding the aerodynamic loading on rotor blades as\nwell as the influence of the inflow conditions on wind turbine performance.",
        "The deposition of nanometer-scale particles is of significant interest in\nvarious industrial processes. While these particles offer several advantages,\ntheir deposition can have detrimental effects, such as reducing the heat\ntransfer efficiency in nanofluid-based battery cooling systems. In this study,\nwe investigated particle deposition around different square substrate\nconfigurations as well as experimentally obtained complex porous structure in a\ntwo-dimensional setup. The particles modeled as a concentration field using the\nlattice Boltzmann method, with a given external flow following a parabolic\nprofile. Our results revealed that particle deposition around a substrate\nincreases with higher fluid velocity, greater particle concentration, and\nhigher deposition probability. Additionally, placing multiple number of\nsubstrates in the channel resulted in increased deposition on upstream\nsubstrates compared to downstream ones. As particle deposition around upstream\nsubstrates increases, it eventually obstructs the flow to downstream regions,\nthereby affecting the overall system performance. The insights gained from this\nsimplified model of particle deposition will play a crucial role in advancing\nour understanding of deposition processes in complex systems.",
        "Numerical simulations are a valuable research and layout tool for fluid flow\nproblems, yet repeated evaluations of parametrized problems, necessary to solve\noptimization problems, can be very costly. One option to speed up this process\nis to replace the costly CFD model with a cheaper one. These surrogate models\ncan be either data-driven or they can also rely on reduced basis (RB) methods\nto speed up the calculations. In contrast to data-driven surrogate models, the\nlatter are not based on regression techniques but are still aimed at explicitly\nsolving the conservation equations. Their speed-up comes from a strong\nreduction of the solution space, which results in much smaller algebraic\nsystems that need to be solved. Within this work, an RB model, suited for\nslightly compressible flow, is presented and tested on different flow\nconfigurations. The model is stabilized using a Petrov-Galerkin method with\ntrial and test function spaces of different dimensionality to generate stable\nresults for a wide range of Reynolds numbers. The presented model applies to\ngeometrically and physically parametrized flow problems. Finally, a data-driven\napproach was used to extend it to turbulent flows.",
        "This paper applies Bayesian-optimization-RANS (turbo-RANS) to improve\nReynolds-averaged Navier-Stokes (RANS) turbulence models for a\nconverging-diverging channel, a case with adverse pressure gradients and flow\nseparation. Using Bayesian optimization, the Generalized $k$-$\\omega$ (GEKO)\nmodel was calibrated by tuning $C_\\text{SEP}$ and $C_\\text{NW}$ with sparse\ndirect numerical simulation (DNS) data at $Re = 12,600$. The calibration\nfollowed the Generalized Error Distribution-based Calibration Procedure\n(GEDCP), optimizing coefficients based on pressure recovery ($C_p$) and skin\nfriction ($C_f$). The optimized model was evaluated beyond training data.\nStreamwise velocity ($U$) predictions at $Re = 12,600$ were compared to DNS to\nassess improvements in $C_p$ and $C_f$. To test robustness, comparisons were\nmade against large-eddy simulation (LES) data at $Re = 20,580$ for velocity and\nskin friction. Results show that optimized GEKO (turbo-RANS) improves wall\nquantity predictions, particularly reattachment. Improved velocity profiles at\nboth Reynolds numbers suggest Bayesian-optimized coefficients enhance adverse\npressure gradient modeling. The model retains accuracy across different $Re$,\nshowing turbo-RANS' potential in turbulence model corrections that generalize\nacross flows. While skin friction predictions showed limited improvement due to\nconstraints of two-equation models, this study highlights the role of machine\nlearning-assisted RANS calibration in improving predictive accuracy for complex\nflows. The results suggest optimized coefficients from a single dataset can be\napplied across moderate $Re$ variations, improving turbo-RANS' applicability\nfor turbulence model tuning.",
        "This paper addresses the trade-off between internalisation and\nexternalisation in the management of stochastic trade flows. We consider agents\nwho must absorb flows and manage risk by deciding whether to warehouse it or\nhedge in the market, thereby incurring transaction costs and market impact.\nUnlike market makers, these agents cannot skew their quotes to attract\noffsetting flows and deter risk-increasing ones, leading to a fundamentally\ndifferent problem. Within the Almgren-Chriss framework, we derive\nalmost-closed-form solutions in the case of quadratic execution costs, while\nmore general cases require numerical methods. In particular, we discuss the\nchallenges posed by artificial boundary conditions when using classical\ngrid-based numerical PDE techniques and propose reinforcement learning methods\nas an alternative.",
        "We continue developing the freelance holography program, formulating\ngauge\/gravity correspondence where the gravity side is formulated on a space\nbounded by a generic timelike codimension-one surface inside AdS and arbitrary\nboundary conditions are imposed on the gravity fields on the surface. Our\nanalysis is performed within the Covariant Phase Space Formalism (CPSF). We\ndiscuss how a given boundary condition on the bulk fields on a generic boundary\nevolves as we move the boundary to another boundary inside AdS and work out how\nthis evolution is encoded in deformations of the holographic boundary theory.\nOur analyses here extend the extensively studied T$\\bar{\\text{T}}$-deformation\nby relaxing the boundary conditions at asymptotic AdS or at the cutoff surface\nto be any arbitrary one (besides Dirichlet). We discuss some of the\nimplications of our general freelance holography setting.",
        "Learning to remember over long timescales is fundamentally challenging for\nrecurrent neural networks (RNNs). While much prior work has explored why RNNs\nstruggle to learn long timescales and how to mitigate this, we still lack a\nclear understanding of the dynamics involved when RNNs learn long timescales\nvia gradient descent. Here we build a mathematical theory of the learning\ndynamics of linear RNNs trained to integrate white noise. We show that when the\ninitial recurrent weights are small, the dynamics of learning are described by\na low-dimensional system that tracks a single outlier eigenvalue of the\nrecurrent weights. This reveals the precise manner in which the long timescale\nassociated with white noise integration is learned. We extend our analyses to\nRNNs learning a damped oscillatory filter, and find rich dynamical equations\nfor the evolution of a conjugate pair of outlier eigenvalues. Taken together,\nour analyses build a rich mathematical framework for studying dynamical\nlearning problems salient for both machine learning and neuroscience.",
        "An automated temperature-controlled electrical DC voltage and DC resistance\nmultiple reference standard (MRS) has been developed by Measurements\nInternational (MI) with the scientific support from the Istituto Nazionale di\nRicerca Metrologica (INRIM). The MRS includes a 10 V, a 1 {\\Omega}, and a 10\nk{\\Omega} standards selectable via a switch unit. This setup allows the\nartifact calibration of high-end calibrators and multimeters used in\nlow-frequency electrical measurements. The two resistors are high-stability\nstandards from MI, while the 10 V standard is based on a low-noise circuit\ndeveloped by INRIM in collaboration with MI. A key innovation is the internal\nreal-time clock calendar, which displays the calibration values of the MRS\nstandards and their updated values internally calculated. This ensures reliable\nuse of the MRS standards over extended periods between calibrations,\neffectively minimizing uncertainties due to their drift. The standards are\nhoused in a thermal box, minimizing temperature variations. The MRS standards\nmeet the uncertainty requirements defined by calibrators and multimeters\nmanufacturers for artifact calibration and can also serve as laboratory\nreferences or travelling standards for interlaboratory comparisons (ILCs). MI\nis currently commercializing the MRS.",
        "White dwarfs are often found in close binaries with stellar or even\nsubstellar companions. It is generally thought that these compact binaries form\nvia common envelope evolution, triggered by the progenitor of the white dwarf\nexpanding after it evolved off the main-sequence and engulfing its companion.\nTo date, a handful of white dwarfs in compact binaries with substellar\ncompanions have been found, typically with masses greater than around 50\nM$_\\mathrm{Jup}$. Here we report the discovery of two eclipsing white dwarf\nplus brown dwarf binaries containing very low mass brown dwarfs. ZTF J1828+2308\nconsists of a hot ($15900\\pm75$ K) $0.610\\pm0.004$ M$_{\\odot}$ white dwarf in a\n2.7 hour binary with a $0.0186\\pm0.0008$ M$_{\\odot}$ ($19.5\\pm0.8$\nM$_\\mathrm{Jup}$) brown dwarf. ZTF J1230$-$2655 contains a cool ($10000\\pm110$\nK) $0.65\\pm0.02$ M$_{\\odot}$ white dwarf in a 5.7 hour binary with a companion\nthat has a mass of less than 0.0211 M$_{\\odot}$ (22.1 M$_\\mathrm{Jup}$). While\nthe brown dwarf in ZTF J1828+2308 has a radius consistent with its mass and\nage, ZTF J1230$-$2655 contains a roughly 20 per cent overinflated brown dwarf\nfor its age. We are only able to reconstruct the common envelope phase for\neither system if it occurred after the first thermal pulse, when the white\ndwarf progenitor had already lost a significant fraction of its original mass.\nThis is true even for very high common envelope ejection efficiencies\n($\\alpha_\\mathrm{CE}\\sim 1$), unless both systems have extremely low\nmetallicities. It may be that the lowest mass companions can only survive a\ncommon envelope phase if it occurs at this very late stage.",
        "Despite the extensive use of electrochemiluminescence in sensing\napplications, its potential in lighting and display technology has been\nconstrained by the low luminance and short operational lifetime of\nelectrochemiluminescence devices (ECLDs). Here, we demonstrate a substantial\nenhancement in the luminance, efficiency, and operational longevity of ECLDs by\nintroducing electrochemically induced hyperfluorescence (ECiHF) via\nelectrogeneration of charge-transfer (CT) excimers and subsequent energy\ntransfer to fluorescent acceptors. By assuming a double-decker arrangement of\nthe electron donor and acceptor groups, the molecule TpAT-tFFO supports\nsolution-state thermally activated delayed fluorescence from a CT excimer state\nand efficient energy transfer to the rubrene dye TBRb. Optimized ECLDs based on\nthis material combination achieve an unprecedented luminance of 6,220 cd\/m2 and\ntheir operational lifetime (LT50) at an initial luminance of 100 cd\/m2 exceeds\n20 minutes, more than 10-fold longer than other ECLDs with meaningful\nefficiency or brightness. We identify energy level alignment between the\nexcimer and the emitter as a crucial factor for efficient ECiHF. In mixtures\nwith energy gaps > 0.5 eV, electron transfer results in reduced performance and\nrenders the operation strongly dependent on applied voltage and frequency. By\ncontrast, spectroelectrochemical analysis reveals that devices with favorable\nenergy level alignment operate on a pure excimer mechanism across a wide range\nof frequencies. These findings highlight the innovative potential of ECiHF in\nimproving the performance of ECLD, which can be widely applied in future\ncommercial lighting solutions.",
        "Integrals to calculate generalised magnetic coordinates from an input\nmagnetic flux function asymptotically close to the separatrix are presented,\nand implemented in the GPEC\/DCON code suite. These integrals allow\ncharacterisation of the magnetic equilibrium of a diverted tokamak, in magnetic\ncoordinates, arbitrarily close to the last closed flux surface, avoiding the\nnumerical issues associated with calculating diverging field line integrals\nnear a magnetic x-point. These methods provide an important first step in the\ndevelopment of robust asymptotic equilibrium behaviour for spectral 3D MHD\ncodes at the separatrix.",
        "Star-forming galaxies (SFGs) are considered to be an important component of\nthe diffuse extragalactic gamma-ray background (EGB) radiation observed in 0.1\n-- 820 GeV, but their quantitative contribution has not yet been precisely\ndetermined. In this study, we aim to provide the currently most reliable\nestimate of the contribution of SFGs based on careful calibration with\ngamma-ray luminosities of nearby galaxies and physical quantities (star\nformation rate, stellar mass, and size) of galaxies observed by high-redshift\ngalaxy surveys. Our calculations are based on the latest database of particle\ncollision cross-sections and energy spectra of secondary particles, and take\ninto account not only hadronic but also leptonic processes with various\nradiation fields in a galaxy. We find that SFGs are not the dominant component\nof the unresolved EGB measured by Fermi; the largest contribution is around 50%\n-- 60% in the 1 -- 10 GeV region, and the contribution falls rapidly in lower\nand higher energy ranges. This result appears to contradict a previous study,\nwhich claimed that SFGs are the dominant component of the unresolved EGB, and\nthe origin of the discrepancy is examined. In calculations of cosmic-ray\nproduction, propagation, and interaction in a galaxy, we try models developed\nby two independent groups and find that they have little impact on EGB.",
        "We investigate a difference-of-convex (DC) formulation where the second term\nis allowed to be weakly convex. We examine the precise behavior of a single\niteration of the difference-of-convex algorithm (DCA), providing a tight\ncharacterization of the objective function decrease, distinguishing between six\ndistinct parameter regimes.\n  Our proofs, inspired by the performance estimation framework, are notably\nsimplified compared to related prior research. We subsequently derive sublinear\nconvergence rates for the DCA towards critical points, assuming at least one of\nthe functions is smooth.\n  Additionally, we explore the underexamined equivalence between proximal\ngradient descent (PGD) and DCA iterations, demonstrating how DCA, a\nparameter-free algorithm, without the need for a stepsize, serves as a tool for\nstudying the exact convergence rates of PGD.",
        "Measuring the blood pressure waveform is becoming a more frequently studied\narea. The development of sensor technologies opens many new ways to be able to\nmeasure high-quality signals. The development of such an aim-specific sensor\ncan be time-consuming, expensive, and difficult to test or validate with known\nand consistent waveforms. In this paper, we present an open source blood\npressure waveform simulator with an open source Python validation package to\nreduce development costs for early-stage sensor development and research. The\nsimulator mainly consists of 3D printed parts which technology has become a\nwidely available and cheap solution. The core part of the simulator is a 3D\nprinted cam that can be generated based on real blood pressure waveforms. The\nvalidation framework can create a detailed comparison between the signal\nwaveform used to design the cam and the measured time series from the sensor\nbeing validated. The presented simulator proved to be robust and accurate in\nshort- and long-term use, as it produced the signal waveform consistently and\naccurately. To validate this solution, a 3D force sensor was used, which was\nproven earlier to be able to measure high-quality blood pressure waveforms on\nthe radial artery at the wrist. The results showed high similarity between the\nmeasured and the nominal waveforms, meaning that comparing the normalized\nsignals, the RMSE value ranged from $0.0276 \\pm 0.0047$ to $0.0212 \\pm 0.0023$,\nand the Pearson correlation ranged from $0.9933 \\pm 0.0027$ to $0.9978 \\pm\n0.0005$. Our validation framework is available at\nhttps:\/\/github.com\/repat8\/cam-bpw-sim. Our hardware framework, which allows\nreproduction of the presented solution, is available at\nhttps:\/\/github.com\/repat8\/cam-bpw-sim-hardware. The entire design is an open\nsource project and was developed using free software.",
        "We provide an updated global extraction of the neutron magnetic form factor,\nincluding new extractions from $^3$H-$^3$He comparisons at Jefferson Lab. Our\nnew global fit addresses discrepancies between previous data sets at modest\nmomentum transfer by separating the uncertainties from world data into\nnormalization and uncorrelated uncertainties. We use this updated global fit,\nalong with previous fits for the other form factors, to extract the neutron and\nproton transverse charge and magnetization densities and their uncertainties.",
        "The exponential local-global principle, or Skolem conjecture, says: Suppose\nthat \\(b\\) is a positive integer, and that the sequence \\((u_{n})_{n =\n-\\infty}^{\\infty}\\) is such that every term is in \\(\\mathbb{Z}[1\/b]\\), the\nlinear recurrence \\(u_{n + d} = a_{1}u_{n + d - 1} + \\cdots + a_{d}u_{n}\\)\nholds for all integers \\(n\\), and every root of \\(x^{d} - a_{1}x^{d - 1} -\na_{2}x^{d - 2} - \\cdots - a_{d}\\) is nonzero and simple; then there is no zero\nterm \\(u_{n}\\) if and only if, for some integer \\(m\\) that is larger than \\(1\\)\nand relatively prime to \\(b\\), every term \\(u_{n}\\) is not in\n\\(m\\mathbb{Z}[1\/b]\\).\n  Particular cases of the conjecture are known, but the general conjecture is\nopen. This paper proves some apparently new quadratic and degenerate cubic\ncases of the exponential local-global principle via power residue symbols.\n  This work was presented at the Stellenbosch Number Theory Conference 2025 in\nJanuary 2025 at Stellenbosch University; much of the work was also presented at\nthe 67th Annual Congress of the South African Mathematical Society in December\n2024 at the University of Pretoria.",
        "We calculate the temperature dependence of bottomonium correlators in\n(2+1)-flavor lattice QCD with the aim to constrain in-medium properties of\nbottomonia at high temperature. The lattice calculations are performed using\nHISQ action with physical strange quark mass and light quark masses twenty\ntimes smaller than the strange quark mass at two lattice spacings $a=0.0493$ fm\nand $0.0602$ fm, and temporal extents $N_{\\tau}=16-30$, corresponding to the\ntemperatures $T=133-250$ MeV. We use a tadpole-improved NRQCD action including\nspin-dependent $v^6$ corrections for the heavy quarks and extended meson\noperators in order to be sensitive to in-medium properties of the bottomonium\nstates of interest. We find that within estimated errors the bottomonium masses\ndo not change compared to their vacuum values for all temperatures under our\nconsideration; however, we find different nonzero widths for the various\nbottomonium states.",
        "Convergent Cross Mapping (CCM) is a powerful method for detecting causality\nin coupled nonlinear dynamical systems, providing a model-free approach to\ncapture dynamic causal interactions. Partial Cross Mapping (PCM) was introduced\nas an extension of CCM to address indirect causality in three-variable systems\nby comparing cross-mapping quality between direct cause-effect mapping and\nindirect mapping through an intermediate conditioning variable. However, PCM\nremains limited to univariate delay embeddings in its cross-mapping processes.\nIn this work, we extend PCM to the multivariate setting, introducing multiPCM,\nwhich leverages multivariate embeddings to more effectively distinguish\nindirect causal relationships. We further propose a multivariate cross-mapping\nframework (MXMap) for causal discovery in dynamical systems. This two-phase\nframework combines (1) pairwise CCM tests to establish an initial causal graph\nand (2) multiPCM to refine the graph by pruning indirect causal connections.\nThrough experiments on simulated data and the ERA5 Reanalysis weather dataset,\nwe demonstrate the effectiveness of MXMap. Additionally, MXMap is compared\nagainst several baseline methods, showing advantages in accuracy and causal\ngraph refinement.",
        "This research investigates the adsorption characteristics of carbon nanobelts\n(CNB) and Mobius carbon nanobelts (MCNB) interacting with various greenhouse\ngases, including NH3, CO2, CO, H2S, CH4, CH3OH, NO2, NO, and COCl2. The study\nemploys semi-empirical tight-binding calculations via xTB software,\ncomplemented by topological analysis using MULTIWFN software. Comparative\nanalysis reveals MCNB's superior adsorption properties, particularly for\nspecific gases. Notable adsorption energies for MCNB were measured at -1.595eV,\n-0.669eV, and -0.637eV for NO, COCl2, and NO2, respectively, significantly\nexceeding the corresponding CNB values of -0.636eV, -0.449eV, and -0.438eV. The\ninvestigation of desorption kinetics demonstrates rapid recovery times\n(sub-millisecond) for most gas-nanobelt interactions, with the notable\nexception of the MCNB+NO system, which exhibits persistent bonding. Topological\nanalysis confirms chemisorption mechanisms for NO, COCl2, and NO2 on both\nnanobelt variants, characterized by complex hybridizations of covalent and\nnon-covalent interactions. Molecular dynamics simulations conducted in both\npacked configurations and dry air mixtures demonstrate the nanobelts' effective\ngas-attracting properties, maintaining consistent capture performance across\ndifferent environmental conditions. These findings establish carbon nanobelts,\nparticularly the Mobius configuration, as promising candidates for greenhouse\ngas capture technologies, offering potential applications in environmental\nremediation and climate change mitigation strategies."
      ]
    }
  },
  {
    "id":2412.10196,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Quality assurance procedures for mass spectrometry untargeted metabolomics. a review",
    "start_abstract":"Untargeted metabolomics, as a global approach, has already proven its great potential and capabilities for the investigation of health and disease, as well as the wide applicability for other research areas. Although great progress has been made on the feasibility of metabolomics experiments, there are still some challenges that should be faced and that includes all sources of fluctuations and bias affecting every step involved in multiplatform untargeted metabolomics studies. The identification and reduction of the main sources of unwanted variation regarding the pre-analytical, analytical and post-analytical phase of metabolomics experiments is essential to ensure high data quality. Nowadays, there is still a lack of information regarding harmonized guidelines for quality assurance as those available for targeted analysis. In this review, sources of variations to be considered and minimized along with methodologies and strategies for monitoring and improvement the quality of the results are discussed. The given information is based on evidences from different groups among our own experiences and recommendations for each stage of the metabolomics workflow. The comprehensive overview with tools presented here might serve other researchers interested in monitoring, controlling and improving the reliability of their findings by implementation of good experimental quality practices in the untargeted metabolomics study.",
    "start_categories":[
      "q-bio.QM"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b16"
      ],
      "title":[
        "XGBoost: A Scalable Tree Boosting System"
      ],
      "abstract":[
        "Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "TimesBERT: A BERT-Style Foundation Model for Time Series Understanding",
        "Residual Policy Gradient: A Reward View of KL-regularized Objective",
        "Multi-label feature selection based on binary hashing learning and\n  dynamic graph constraints",
        "Achieving Upper Bound Accuracy of Joint Training in Continual Learning",
        "How Your Location Relates to Health: Variable Importance and\n  Interpretable Machine Learning for Environmental and Sociodemographic Data",
        "Adjusted Count Quantification Learning on Graphs",
        "Exploring Geometric Representational Alignment through Ollivier-Ricci\n  Curvature and Ricci Flow",
        "Learning Efficient Positional Encodings with Graph Neural Networks",
        "Efficient Distributed Optimization under Heavy-Tailed Noise",
        "GeneralizeFormer: Layer-Adaptive Model Generation across Test-Time\n  Distribution Shifts",
        "Mechanistic PDE Networks for Discovery of Governing Equations",
        "Simultaneous Latent State Estimation and Latent Linear Dynamics\n  Discovery from Image Observations",
        "On the Expressivity of Selective State-Space Layers: A Multivariate\n  Polynomial Approach",
        "Target Selection for the Redshift-Limited WAVES-Wide with Machine\n  Learning",
        "Auxiliary Discrminator Sequence Generative Adversarial Networks\n  (ADSeqGAN) for Few Sample Molecule Generation",
        "Competing Effects of Local Solvation Structures on Chemical Shift\n  Changes of Liquid Electrolyte",
        "Diffusion Models for Cayley Graphs",
        "Funzac at CoMeDi Shared Task: Modeling Annotator Disagreement from\n  Word-In-Context Perspectives",
        "A Comprehensive Reanalysis of K2-18 b's JWST NIRISS+NIRSpec Transmission\n  Spectrum",
        "Know You First and Be You Better: Modeling Human-Like User Simulators\n  via Implicit Profiles",
        "Equilibrium Moment Analysis of It\\^o SDEs",
        "NaFM: Pre-training a Foundation Model for Small-Molecule Natural\n  Products",
        "DAST: Difficulty-Adaptive Slow-Thinking for Large Reasoning Models",
        "Hypersurfaces passing through the Galois orbit of a point",
        "Mask-DPO: Generalizable Fine-grained Factuality Alignment of LLMs",
        "Optimal joint reconstruction from CMB observations: application to\n  cosmic birefringence, patchy reionization and CMB lensing",
        "Wireless Network Topology Inference: A Markov Chains Approach",
        "Learning the RoPEs: Better 2D and 3D Position Encodings with STRING"
      ],
      "abstract":[
        "Time series analysis is crucial in diverse scenarios. Beyond forecasting,\nconsiderable real-world tasks are categorized into classification, imputation,\nand anomaly detection, underscoring different capabilities termed time series\nunderstanding in this paper. While GPT-style models have been positioned as\nfoundation models for time series forecasting, the BERT-style architecture,\nwhich has made significant advances in natural language understanding, has not\nbeen fully unlocked for time series understanding, possibly attributed to the\nundesirable dropout of essential elements of BERT. In this paper, inspired by\nthe shared multi-granularity structure between multivariate time series and\nmultisentence documents, we design TimesBERT to learn generic representations\nof time series including temporal patterns and variate-centric characteristics.\nIn addition to a natural adaptation of masked modeling, we propose a parallel\ntask of functional token prediction to embody vital multi-granularity\nstructures. Our model is pre-trained on 260 billion time points across diverse\ndomains. Leveraging multi-granularity representations, TimesBERT achieves\nstate-of-the-art performance across four typical downstream understanding\ntasks, outperforming task-specific models and language pre-trained backbones,\npositioning it as a versatile foundation model for time series understanding.",
        "Reinforcement Learning and Imitation Learning have achieved widespread\nsuccess in many domains but remain constrained during real-world deployment.\nOne of the main issues is the additional requirements that were not considered\nduring training. To address this challenge, policy customization has been\nintroduced, aiming to adapt a prior policy while preserving its inherent\nproperties and meeting new task-specific requirements. A principled approach to\npolicy customization is Residual Q-Learning (RQL), which formulates the problem\nas a Markov Decision Process (MDP) and derives a family of value-based learning\nalgorithms. However, RQL has not yet been applied to policy gradient methods,\nwhich restricts its applicability, especially in tasks where policy gradient\nhas already proven more effective. In this work, we first derive a concise form\nof Soft Policy Gradient as a preliminary. Building on this, we introduce\nResidual Policy Gradient (RPG), which extends RQL to policy gradient methods,\nallowing policy customization in gradient-based RL settings. With the view of\nRPG, we rethink the KL-regularized objective widely used in RL fine-tuning. We\nshow that under certain assumptions, KL-regularized objective leads to a\nmaximum-entropy policy that balances the inherent properties and task-specific\nrequirements on a reward-level. Our experiments in MuJoCo demonstrate the\neffectiveness of Soft Policy Gradient and Residual Policy Gradient.",
        "Multi-label learning poses significant challenges in extracting reliable\nsupervisory signals from the label space. Existing approaches often employ\ncontinuous pseudo-labels to replace binary labels, improving supervisory\ninformation representation. However, these methods can introduce noise from\nirrelevant labels and lead to unreliable graph structures. To overcome these\nlimitations, this study introduces a novel multi-label feature selection method\ncalled Binary Hashing and Dynamic Graph Constraint (BHDG), the first method to\nintegrate binary hashing into multi-label learning. BHDG utilizes\nlow-dimensional binary hashing codes as pseudo-labels to reduce noise and\nimprove representation robustness. A dynamically constrained sample projection\nspace is constructed based on the graph structure of these binary\npseudo-labels, enhancing the reliability of the dynamic graph. To further\nenhance pseudo-label quality, BHDG incorporates label graph constraints and\ninner product minimization within the sample space. Additionally, an\n$l_{2,1}$-norm regularization term is added to the objective function to\nfacilitate the feature selection process. The augmented Lagrangian multiplier\n(ALM) method is employed to optimize binary variables effectively.\nComprehensive experiments on 10 benchmark datasets demonstrate that BHDG\noutperforms ten state-of-the-art methods across six evaluation metrics. BHDG\nachieves the highest overall performance ranking, surpassing the next-best\nmethod by an average of at least 2.7 ranks per metric, underscoring its\neffectiveness and robustness in multi-label feature selection.",
        "Continual learning has been an active research area in machine learning,\nfocusing on incrementally learning a sequence of tasks. A key challenge is\ncatastrophic forgetting (CF), and most research efforts have been directed\ntoward mitigating this issue. However, a significant gap remains between the\naccuracy achieved by state-of-the-art continual learning algorithms and the\nideal or upper-bound accuracy achieved by training all tasks together jointly.\nThis gap has hindered or even prevented the adoption of continual learning in\napplications, as accuracy is often of paramount importance. Recently, another\nchallenge, termed inter-task class separation (ICS), was also identified, which\nspurred a theoretical study into principled approaches for solving continual\nlearning. Further research has shown that by leveraging the theory and the\npower of large foundation models, it is now possible to achieve upper-bound\naccuracy, which has been empirically validated using both text and image\nclassification datasets. Continual learning is now ready for real-life\napplications. This paper surveys the main research leading to this achievement,\njustifies the approach both intuitively and from neuroscience research, and\ndiscusses insights gained.",
        "Health outcomes depend on complex environmental and sociodemographic factors\nwhose effects change over location and time. Only recently has fine-grained\nspatial and temporal data become available to study these effects, namely the\nMEDSAT dataset of English health, environmental, and sociodemographic\ninformation. Leveraging this new resource, we use a variety of variable\nimportance techniques to robustly identify the most informative predictors\nacross multiple health outcomes. We then develop an interpretable machine\nlearning framework based on Generalized Additive Models (GAMs) and Multiscale\nGeographically Weighted Regression (MGWR) to analyze both local and global\nspatial dependencies of each variable on various health outcomes. Our findings\nidentify NO2 as a global predictor for asthma, hypertension, and anxiety,\nalongside other outcome-specific predictors related to occupation, marriage,\nand vegetation. Regional analyses reveal local variations with air pollution\nand solar radiation, with notable shifts during COVID. This comprehensive\napproach provides actionable insights for addressing health disparities, and\nadvocates for the integration of interpretable machine learning in public\nhealth.",
        "Quantification learning is the task of predicting the label distribution of a\nset of instances. We study this problem in the context of graph-structured\ndata, where the instances are vertices. Previously, this problem has only been\naddressed via node clustering methods. In this paper, we extend the popular\nAdjusted Classify & Count (ACC) method to graphs. We show that the prior\nprobability shift assumption upon which ACC relies is often not fulfilled and\npropose two novel graph quantification techniques: Structural importance\nsampling (SIS) makes ACC applicable in graph domains with covariate shift.\nNeighborhood-aware ACC improves quantification in the presence of\nnon-homophilic edges. We show the effectiveness of our techniques on multiple\ngraph quantification tasks.",
        "Representational analysis explores how input data of a neural system are\nencoded in high dimensional spaces of its distributed neural activations, and\nhow we can compare different systems, for instance, artificial neural networks\nand brains, on those grounds. While existing methods offer important insights,\nthey typically do not account for local intrinsic geometrical properties within\nthe high-dimensional representation spaces. To go beyond these limitations, we\nexplore Ollivier-Ricci curvature and Ricci flow as tools to study the alignment\nof representations between humans and artificial neural systems on a geometric\nlevel. As a proof-of-principle study, we compared the representations of face\nstimuli between VGG-Face, a human-aligned version of VGG-Face, and\ncorresponding human similarity judgments from a large online study. Using this\ndiscrete geometric framework, we were able to identify local structural\nsimilarities and differences by examining the distributions of node and edge\ncurvature and higher-level properties by detecting and comparing community\nstructure in the representational graphs.",
        "Positional encodings (PEs) are essential for effective graph representation\nlearning because they provide position awareness in inherently\nposition-agnostic transformer architectures and increase the expressive\ncapacity of Graph Neural Networks (GNNs). However, designing powerful and\nefficient PEs for graphs poses significant challenges due to the absence of\ncanonical node ordering and the scale of the graph. {In this work, we identify\nfour key properties that graph PEs should satisfy}: stability, expressive\npower, scalability, and genericness. We find that existing eigenvector-based PE\nmethods often fall short of jointly satisfying these criteria. To address this\ngap, we introduce PEARL, a novel framework of learnable PEs for graphs. Our\nprimary insight is that message-passing GNNs function as nonlinear mappings of\neigenvectors, enabling the design of GNN architectures for generating powerful\nand efficient PEs. A crucial challenge lies in initializing node attributes in\na manner that is both expressive and permutation equivariant. We tackle this by\ninitializing GNNs with random node inputs or standard basis vectors, thereby\nunlocking the expressive power of message-passing operations, while employing\nstatistical pooling functions to maintain permutation equivariance. Our\nanalysis demonstrates that PEARL approximates equivariant functions of\neigenvectors with linear complexity, while rigorously establishing its\nstability and high expressive power. Experimental evaluations show that PEARL\noutperforms lightweight versions of eigenvector-based PEs and achieves\ncomparable performance to full eigenvector-based PEs, but with one or two\norders of magnitude lower complexity. Our code is available at\nhttps:\/\/github.com\/ehejin\/Pearl-PE.",
        "Distributed optimization has become the default training paradigm in modern\nmachine learning due to the growing scale of models and datasets. To mitigate\ncommunication overhead, local updates are often applied before global\naggregation, resulting in a nested optimization approach with inner and outer\nsteps. However, heavy-tailed stochastic gradient noise remains a significant\nchallenge, particularly in attention-based models, hindering effective\ntraining. In this work, we propose TailOPT, an efficient framework designed to\naddress heavy-tailed noise by leveraging adaptive optimization or clipping\ntechniques. We establish convergence guarantees for the TailOPT framework under\nheavy-tailed noise with potentially unbounded gradient variance and local\nupdates. Among its variants, we highlight a memory and communication efficient\ninstantiation which we call $Bi^2Clip$, which performs coordinate-wise clipping\nat both the inner and outer optimizers, achieving adaptive-like performance\n(e.g., Adam) without the cost of maintaining or transmitting additional\ngradient statistics. Empirically, TailOPT, including $Bi^2Clip$, demonstrates\nsuperior performance on several language tasks and models, outperforming\nstate-of-the-art methods.",
        "We consider the problem of test-time domain generalization, where a model is\ntrained on several source domains and adjusted on target domains never seen\nduring training. Different from the common methods that fine-tune the model or\nadjust the classifier parameters online, we propose to generate multiple layer\nparameters on the fly during inference by a lightweight meta-learned\ntransformer, which we call \\textit{GeneralizeFormer}. The layer-wise parameters\nare generated per target batch without fine-tuning or online adjustment. By\ndoing so, our method is more effective in dynamic scenarios with multiple\ntarget distributions and also avoids forgetting valuable source distribution\ncharacteristics. Moreover, by considering layer-wise gradients, the proposed\nmethod adapts itself to various distribution shifts. To reduce the\ncomputational and time cost, we fix the convolutional parameters while only\ngenerating parameters of the Batch Normalization layers and the linear\nclassifier. Experiments on six widely used domain generalization datasets\ndemonstrate the benefits and abilities of the proposed method to efficiently\nhandle various distribution shifts, generalize in dynamic scenarios, and avoid\nforgetting.",
        "We present Mechanistic PDE Networks -- a model for discovery of governing\npartial differential equations from data. Mechanistic PDE Networks represent\nspatiotemporal data as space-time dependent linear partial differential\nequations in neural network hidden representations. The represented PDEs are\nthen solved and decoded for specific tasks. The learned PDE representations\nnaturally express the spatiotemporal dynamics in data in neural network hidden\nspace, enabling increased power for dynamical modeling. Solving the PDE\nrepresentations in a compute and memory-efficient way, however, is a\nsignificant challenge. We develop a native, GPU-capable, parallel, sparse, and\ndifferentiable multigrid solver specialized for linear partial differential\nequations that acts as a module in Mechanistic PDE Networks. Leveraging the PDE\nsolver, we propose a discovery architecture that can discover nonlinear PDEs in\ncomplex settings while also being robust to noise. We validate PDE discovery on\na number of PDEs, including reaction-diffusion and Navier-Stokes equations.",
        "The problem of state estimation has a long history with many successful\nalgorithms that allow analytical derivation or approximation of posterior\nfiltering distribution given the noisy observations. This report tries to\nconclude previous works to resolve the problem of latent state estimation given\nimage-based observations and also suggests a new solution to this problem.",
        "Recent advances in efficient sequence modeling have introduced selective\nstate-space layers, a key component of the Mamba architecture, which have\ndemonstrated remarkable success in a wide range of NLP and vision tasks. While\nMamba's empirical performance has matched or surpassed SoTA transformers on\nsuch diverse benchmarks, the theoretical foundations underlying its powerful\nrepresentational capabilities remain less explored. In this work, we\ninvestigate the expressivity of selective state-space layers using multivariate\npolynomials, and prove that they surpass linear transformers in expressiveness.\nConsequently, our findings reveal that Mamba offers superior representational\npower over linear attention-based models for long sequences, while not\nsacrificing their generalization. Our theoretical insights are validated by a\ncomprehensive set of empirical experiments on various datasets.",
        "The forthcoming Wide Area Vista Extragalactic Survey (WAVES) on the 4-metre\nMulti-Object Spectroscopic Telescope (4MOST) has a key science goal of probing\nthe halo mass function to lower limits than possible with previous surveys. For\nthat purpose, in its Wide component, galaxies targetted by WAVES will be\nflux-limited to $Z<21.1$ mag and will cover the redshift range of $z<0.2$, at a\nspectroscopic success rate of $\\sim95\\%$. Meeting this completeness\nrequirement, when the redshift is unknown a priori, is a challenge. We solve\nthis problem with supervised machine learning to predict the probability of a\ngalaxy falling within the WAVES-Wide redshift limit, rather than estimate each\nobject's redshift. This is done by training an XGBoost tree-based classifier to\ndecide if a galaxy should be a target or not. Our photometric data come from\n9-band VST+VISTA observations, including KiDS+VIKING surveys. The redshift\nlabels for calibration are derived from an extensive spectroscopic sample\noverlapping with KiDS and ancillary fields. Our current results indicate that\nwith our approach, we should be able to achieve the completeness of $\\sim95\\%$,\nwhich is the WAVES success criterion.",
        "In this work, we introduce Auxiliary Discriminator Sequence Generative\nAdversarial Networks (ADSeqGAN), a novel approach for molecular generation in\nsmall-sample datasets. Traditional generative models often struggle with\nlimited training data, particularly in drug discovery, where molecular datasets\nfor specific therapeutic targets, such as nucleic acids binders and central\nnervous system (CNS) drugs, are scarce. ADSeqGAN addresses this challenge by\nintegrating an auxiliary random forest classifier as an additional\ndiscriminator into the GAN framework, significantly improves molecular\ngeneration quality and class specificity.\n  Our method incorporates pretrained generator and Wasserstein distance to\nenhance training stability and diversity. We evaluate ADSeqGAN on a dataset\ncomprising nucleic acid-targeting and protein-targeting small molecules,\ndemonstrating its superior ability to generate nucleic acid binders compared to\nbaseline models such as SeqGAN, ORGAN, and MolGPT. Through an oversampling\nstrategy, ADSeqGAN also significantly improves CNS drug generation, achieving a\nhigher yield than traditional de novo models. Critical assessments, including\ndocking simulations and molecular property analysis, confirm that\nADSeqGAN-generated molecules exhibit strong binding affinities, enhanced\nchemical diversity, and improved synthetic feasibility.\n  Overall, ADSeqGAN presents a novel framework for generative molecular design\nin data-scarce scenarios, offering potential applications in computational drug\ndiscovery. We have demonstrated the successful applications of ADSeqGAN in\ngenerating synthetic nucleic acid-targeting and CNS drugs in this work.",
        "Understanding the solvation structure of electrolytes is critical for\noptimizing the electrochemical performance of rechargeable batteries, as it\ndirectly influences properties such as ionic conductivity, viscosity, and\nelectrochemical stability. The highly complex structures and strong\ninteractions in high-concentration electrolytes make accurate modeling and\ninterpretation of their ``structure-property\" relationships even more\nchallenging with spectroscopic methods. In this study, we present a machine\nlearning-based approach to predict dynamic $^7$Li NMR chemical shifts in\nLiFSI\/DME electrolyte solutions. Additionally, we provide a comprehensive\nstructural analysis to interpret the observed chemical shift behavior in our\nexperiments, particularly the abrupt changes in $^7$Li chemical shifts at high\nconcentrations. Using advanced modeling techniques, we quantitatively establish\nthe relationship between molecular structure and NMR spectra, offering critical\ninsights into solvation structure assignments. Our findings reveal the\ncoexistence of two competing local solvation structures that shift in dominance\nas electrolyte concentration approaches the concentrated limit, leading to\nanomalous reverse of $^7$Li NMR chemical shift in our experiment. This work\nprovides a detailed molecular-level understanding of the intricate solvation\nstructures probed by NMR spectroscopy, leading the way for enhanced electrolyte\ndesign.",
        "We review the problem of finding paths in Cayley graphs of groups and group\nactions, using the Rubik's cube as an example, and we list several more\nexamples of significant mathematical interest. We then show how to formulate\nthese problems in the framework of diffusion models. The exploration of the\ngraph is carried out by the forward process, while finding the target nodes is\ndone by the inverse backward process. This systematizes the discussion and\nsuggests many generalizations. To improve exploration, we propose a ``reversed\nscore'' ansatz which substantially improves over previous comparable\nalgorithms.",
        "In this work, we evaluate annotator disagreement in Word-in-Context (WiC)\ntasks exploring the relationship between contextual meaning and disagreement as\npart of the CoMeDi shared task competition. While prior studies have modeled\ndisagreement by analyzing annotator attributes with single-sentence inputs,\nthis shared task incorporates WiC to bridge the gap between sentence-level\nsemantic representation and annotator judgment variability. We describe three\ndifferent methods that we developed for the shared task, including a feature\nenrichment approach that combines concatenation, element-wise differences,\nproducts, and cosine similarity, Euclidean and Manhattan distances to extend\ncontextual embedding representations, a transformation by Adapter blocks to\nobtain task-specific representations of contextual embeddings, and classifiers\nof varying complexities, including ensembles. The comparison of our methods\ndemonstrates improved performance for methods that include enriched and\ntask-specfic features. While the performance of our method falls short in\ncomparison to the best system in subtask 1 (OGWiC), it is competitive to the\nofficial evaluation results in subtask 2 (DisWiC).",
        "Sub-Neptunes are the most common type of planet in our galaxy. Interior\nstructure models suggest that the coldest sub-Neptunes could host liquid water\noceans underneath their hydrogen envelopes - sometimes called 'hycean' planets.\nJWST transmission spectra of the $\\sim$ 250 K sub-Neptune K2-18 b were recently\nused to report detections of CH$_4$ and CO$_2$, alongside weaker evidence of\n(CH$_3$)$_2$S (dimethyl sulfide, or DMS). Atmospheric CO$_2$ was interpreted as\nevidence for a liquid water ocean, while DMS was highlighted as a potential\nbiomarker. However, these notable claims were derived using a single data\nreduction and retrieval modeling framework, which did not allow for standard\nrobustness tests. Here we present a comprehensive reanalysis of K2-18 b's JWST\nNIRISS SOSS and NIRSpec G395H transmission spectra, including the first\nanalysis of the second-order NIRISS SOSS data. We incorporate multiple\nwell-tested data reduction pipelines and retrieval codes, spanning 60 different\ndata treatments and over 250 atmospheric retrievals. We confirm the detection\nof CH$_4$ ($\\approx$ 4$\\sigma$), with a volume mixing ratio of log CH$_4$ =\n$-1.15^{+0.40}_{-0.52}$, but we find no statistically significant or reliable\nevidence for CO$_2$ or DMS. Finally, we quantify the observed atmospheric\ncomposition using photochemical-climate and interior models, demonstrating that\nour revised composition of K2-18 b can be explained by an oxygen-poor\nmini-Neptune without requiring a liquid water surface or life.",
        "User simulators are crucial for replicating human interactions with dialogue\nsystems, supporting both collaborative training and automatic evaluation,\nespecially for large language models (LLMs). However, existing simulators often\nrely solely on text utterances, missing implicit user traits such as\npersonality, speaking style, and goals. In contrast, persona-based methods lack\ngeneralizability, as they depend on predefined profiles of famous individuals\nor archetypes. To address these challenges, we propose User Simulator with\nimplicit Profiles (USP), a framework that infers implicit user profiles from\nhuman-machine conversations and uses them to generate more personalized and\nrealistic dialogues. We first develop an LLM-driven extractor with a\ncomprehensive profile schema. Then, we refine the simulation through\nconditional supervised fine-tuning and reinforcement learning with cycle\nconsistency, optimizing it at both the utterance and conversation levels.\nFinally, we adopt a diverse profile sampler to capture the distribution of\nreal-world user profiles. Experimental results demonstrate that USP outperforms\nstrong baselines in terms of authenticity and diversity while achieving\ncomparable performance in consistency. Furthermore, dynamic multi-turn\nevaluations based on USP strongly align with mainstream benchmarks,\ndemonstrating its effectiveness in real-world applications.",
        "Stochastic differential equations have proved to be a valuable governing\nframework for many real-world systems which exhibit ``noise'' or randomness in\ntheir evolution. One quality of interest in such systems is the shape of their\nequilibrium probability distribution, if such a thing exists. In some cases a\nstraightforward integral equation may yield this steady-state distribution, but\nin other cases the equilibrium distribution exists and yet that integral\nequation diverges. Here we establish a new equilibrium-analysis technique based\non the logic of finite-timestep simulation which allows us to glean information\nabout the equilibrium regardless -- in particular, a relationship between the\nraw moments of the equilibrium distribution. We utilize this technique to\nextract information about one such equilibrium resistant to direct definition.",
        "Natural products, as metabolites from microorganisms, animals, or plants,\nexhibit diverse biological activities, making them crucial for drug discovery.\nNowadays, existing deep learning methods for natural products research\nprimarily rely on supervised learning approaches designed for specific\ndownstream tasks. However, such one-model-for-a-task paradigm often lacks\ngeneralizability and leaves significant room for performance improvement.\nAdditionally, existing molecular characterization methods are not well-suited\nfor the unique tasks associated with natural products. To address these\nlimitations, we have pre-trained a foundation model for natural products based\non their unique properties. Our approach employs a novel pretraining strategy\nthat is especially tailored to natural products. By incorporating contrastive\nlearning and masked graph learning objectives, we emphasize evolutional\ninformation from molecular scaffolds while capturing side-chain information.\nOur framework achieves state-of-the-art (SOTA) results in various downstream\ntasks related to natural product mining and drug discovery. We first compare\ntaxonomy classification with synthesized molecule-focused baselines to\ndemonstrate that current models are inadequate for understanding natural\nsynthesis. Furthermore, by diving into a fine-grained analysis at both the gene\nand microbial levels, NaFM demonstrates the ability to capture evolutionary\ninformation. Eventually, our method is experimented with virtual screening,\nillustrating informative natural product representations that can lead to more\neffective identification of potential drug candidates.",
        "Recent advancements in slow-thinking reasoning models have shown exceptional\nperformance in complex reasoning tasks. However, these models often exhibit\noverthinking-generating redundant reasoning steps for simple problems, leading\nto excessive computational resource usage. While current mitigation strategies\nuniformly reduce reasoning tokens, they risk degrading performance on\nchallenging tasks that require extended reasoning. This paper introduces\nDifficulty-Adaptive Slow-Thinking (DAST), a novel framework that enables models\nto autonomously adjust the length of Chain-of-Thought(CoT) based on problem\ndifficulty. We first propose a Token Length Budget (TLB) metric to quantify\ndifficulty, then leveraging length-aware reward shaping and length preference\noptimization to implement DAST. DAST penalizes overlong responses for simple\ntasks while incentivizing sufficient reasoning for complex problems.\nExperiments on diverse datasets and model scales demonstrate that DAST\neffectively mitigates overthinking (reducing token usage by over 30\\% on\naverage) while preserving reasoning accuracy on complex problems.",
        "Asgarli, Ghioca, and Reichstein recently proved that if $K$ is a field with\n$|K|>2$, then for any positive integers $d$ and $n$, and separable field\nextension $L\/K$ with degree $m=\\binom{n+d}{d}$, there exists a point $P\\in\n\\mathbb{P}^n(L)$ which does not lie on any degree $d$ hypersurface defined over\n$K$. They asked whether the result holds when $|K| = 2$. We answer their\nquestion in the affirmative by combining various ideas from arithmetic\ngeometry. More generally, we show that for each positive integer $r$ and\nseparable field extension $L\/K$ with degree $r$, there exists a point $P \\in\n\\mathbb{P}^n(L)$ such that the vector space of degree $d$ forms over $K$ that\nvanish at $P$ has the expected dimension. We also discuss applications to\nlinear systems of hypersurfaces with special properties.",
        "Large language models (LLMs) exhibit hallucinations (i.e., unfaithful or\nnonsensical information) when serving as AI assistants in various domains.\nSince hallucinations always come with truthful content in the LLM responses,\nprevious factuality alignment methods that conduct response-level preference\nlearning inevitably introduced noises during training. Therefore, this paper\nproposes a fine-grained factuality alignment method based on Direct Preference\nOptimization (DPO), called Mask-DPO. Incorporating sentence-level factuality as\nmask signals, Mask-DPO only learns from factually correct sentences in the\npreferred samples and prevents the penalty on factual contents in the not\npreferred samples, which resolves the ambiguity in the preference learning.\nExtensive experimental results demonstrate that Mask-DPO can significantly\nimprove the factuality of LLMs responses to questions from both in-domain and\nout-of-domain datasets, although these questions and their corresponding topics\nare unseen during training. Only trained on the ANAH train set, the score of\nLlama3.1-8B-Instruct on the ANAH test set is improved from 49.19% to 77.53%,\neven surpassing the score of Llama3.1-70B-Instruct (53.44%), while its\nFactScore on the out-of-domain Biography dataset is also improved from 30.29%\nto 39.39%. We further study the generalization property of Mask-DPO using\ndifferent training sample scaling strategies and find that scaling the number\nof topics in the dataset is more effective than the number of questions. We\nprovide a hypothesis of what factual alignment is doing with LLMs, on the\nimplication of this phenomenon, and conduct proof-of-concept experiments to\nverify it. We hope the method and the findings pave the way for future research\non scaling factuality alignment.",
        "Line-of-sight distortions of the cosmic microwave background (CMB), including\ngravitational lensing, cosmic birefringence, and patchy screening, encode\ncrucial cosmological information. While quadratic estimators (QE) have been\nexcellent tools for extracting these signals, they become suboptimal for\ncurrent- and next-generation CMB surveys, failing to maximize signal-to-noise\nand suffering from bias contamination that standard bias-hardening techniques\ncannot mitigate. We present a joint maximum a posteriori framework that\nsimultaneously reconstructs multiple distortion fields while explicitly\naccounting for their mutual contamination. For cosmic birefringence searches,\nour method achieves up to 2.5 improvement in reconstruction noise compared to\nthe QE, while significantly reducing CMB lensing-induced biases up to a factor\nof 5. These gains in lensing biases manifest not only in deep polarization\nsurveys like CMB-S4 and SPT-3G, but also in higher-noise experiments like\nSimons Observatory, where our method reduces lensing-induced biases by a factor\nof two thanks to the power of delensing. Our code provides a first step to\nrobust analyses of CMB secondary anisotropies, their cross-correlations with\nlarge-scale structure, and ultimately enabling more sensitive searches for\nprimordial $B$-modes.",
        "In this work, we address the problem of inferring the topology of a wireless\nnetwork using limited observational data. Specifically, we assume that we can\ndetect when a node is transmitting, but no further information regarding the\ntransmission is available. We propose a novel network estimation procedure\ngrounded in the following abstract problem: estimating the parameters of a\nfinite discrete-time Markov chain by observing, at each time step, which states\nare visited by multiple ``anonymous'' copies of the chain. We develop a\nconsistent estimator that approximates the transition matrix of the chain in\nthe operator norm, with the number of required samples scaling roughly linearly\nwith the size of the state space. Applying this estimation procedure to\nwireless networks, our numerical experiments demonstrate that the proposed\nmethod accurately infers network topology across a wide range of parameters,\nconsistently outperforming transfer entropy, particularly under conditions of\nhigh network congestion.",
        "We introduce STRING: Separable Translationally Invariant Position Encodings.\nSTRING extends Rotary Position Encodings, a recently proposed and widely used\nalgorithm in large language models, via a unifying theoretical framework.\nImportantly, STRING still provides exact translation invariance, including\ntoken coordinates of arbitrary dimensionality, whilst maintaining a low\ncomputational footprint. These properties are especially important in robotics,\nwhere efficient 3D token representation is key. We integrate STRING into Vision\nTransformers with RGB(-D) inputs (color plus optional depth), showing\nsubstantial gains, e.g. in open-vocabulary object detection and for robotics\ncontrollers. We complement our experiments with a rigorous mathematical\nanalysis, proving the universality of our methods."
      ]
    }
  },
  {
    "id":2412.10196,
    "research_type":"applied",
    "start_id":"b16",
    "start_title":"XGBoost: A Scalable Tree Boosting System",
    "start_abstract":"Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Quality assurance procedures for mass spectrometry untargeted metabolomics. a review"
      ],
      "abstract":[
        "Untargeted metabolomics, as a global approach, has already proven its great potential and capabilities for the investigation of health and disease, as well as the wide applicability for other research areas. Although great progress has been made on the feasibility of metabolomics experiments, there are still some challenges that should be faced and that includes all sources of fluctuations and bias affecting every step involved in multiplatform untargeted metabolomics studies. The identification and reduction of the main sources of unwanted variation regarding the pre-analytical, analytical and post-analytical phase of metabolomics experiments is essential to ensure high data quality. Nowadays, there is still a lack of information regarding harmonized guidelines for quality assurance as those available for targeted analysis. In this review, sources of variations to be considered and minimized along with methodologies and strategies for monitoring and improvement the quality of the results are discussed. The given information is based on evidences from different groups among our own experiences and recommendations for each stage of the metabolomics workflow. The comprehensive overview with tools presented here might serve other researchers interested in monitoring, controlling and improving the reliability of their findings by implementation of good experimental quality practices in the untargeted metabolomics study."
      ],
      "categories":[
        "q-bio.QM"
      ]
    },
    "list":{
      "title":[
        "Petri Net Modeling of Root Hair Response to Phosphate Starvation in\n  Arabidopsis Thaliana",
        "Hierarchical Functional Group Ranking via IUPAC Name Analysis for Drug\n  Discovery: A Case Study on TDP1 Inhibitors",
        "Optimal compound downselection to promote diversity and parallel\n  chemistry",
        "Predicting novel pharmacological activities of compounds using PubChem\n  IDs and machine learning (CID-SID ML model)",
        "In silico clinical trials in drug development: a systematic review",
        "Derivation from kinetic theory and 2-D pattern analysis of chemotaxis\n  models for Multiple Sclerosis",
        "Integrating protein sequence embeddings with structure via graph-based\n  deep learning for the prediction of single-residue properties",
        "Data-Driven Modeling of Amyloid-beta Targeted Antibodies for Alzheimer's\n  Disease",
        "Temporal Dynamics of Microbial Communities in Anaerobic Digestion:\n  Influence of Temperature and Feedstock Composition on Reactor Performance and\n  Stability",
        "Efficient Spatial Estimation of Perceptual Thresholds for Retinal\n  Implants via Gaussian Process Regression",
        "Beware of so-called 'good' correlations: a statistical reality check on\n  individual mRNA-protein predictions",
        "Homeostatic Kinematic Growth Model for Arteries -- Residual Stresses and\n  Active Response",
        "A Bayesian Modelling Framework with Model Comparison for Epidemics with\n  Super-Spreading",
        "Stability of 2-class groups in the $\\mathbb{Z}_2$-extension of certain\n  real biquadratic fields",
        "Obstructions for Morin and fold maps: Stiefel-Whitney classes and Euler\n  characteristics of singularity loci",
        "Chiral broadband High Harmonic Generation Source by Vectorial\n  Time-Polarization-Gating",
        "Crystal skeletons: Combinatorics and axioms",
        "Quantum crystal spin Hall effect in two-dimensional altermagnetic\n  systems",
        "Random Variables, Conditional Independence and Categories of Abstract\n  Sample Spaces",
        "First-principles Hubbard parameters with automated and reproducible\n  workflows",
        "Constant-Overhead Fault-Tolerant Bell-Pair Distillation using High-Rate\n  Codes",
        "Bounded Dark Energy",
        "Probing Type II Seesaw Leptogenesis Through Lepton Flavor Violation",
        "Strategic Queues with Priority Classes",
        "Predicting Liquidity-Aware Bond Yields using Causal GANs and Deep\n  Reinforcement Learning with LLM Evaluation",
        "$\\mathrm{G}_2$-structures with torsion and the deformed Shatashvili-Vafa\n  vertex algebra",
        "A new class of non-stationary Gaussian fields with general smoothness on\n  metric graphs",
        "Isolating the hard core of phaseless inference: the Phase selection\n  formulation"
      ],
      "abstract":[
        "Limited availability of inorganic phosphate (Pi) in soil is an important\nconstraint to plant growth. In order to understand better the underlying\nmechanism of plant response to Pi, the response to phosphate starvation in\nArabidopsis thaliana was investigated through use of Petri Nets, a formal\nlanguage suitable for bio-modeling. A. thaliana displays a range of responses\nto deal with Pi starvation, but special attention was paid to root hair\nelongation in this study. A central player in the root hair pathway is the\ntranscription factor ROOT HAIR DEFECTIVE 6-LIKE 4 (RSL4), which has been found\nto be upregulated during the Pi stress. A Petri Net was created which could\nsimulate the gene regulatory networks responsible for the increase in root hair\nlength, as well as the resulting increase in root hair length. Notably,\ndiscrepancies between the model and the literature suggested an important role\nfor RSL2 in regulating RSL4. In the future, the net designed in the current\nstudy could be used as a platform to develop hypotheses about the interaction\nbetween RSL2 and RSL4.",
        "The article proposes a computational approach that can generate a descending\norder of the IUPAC-notated functional groups based on their importance for a\ngiven case study. Thus, a reduced list of functional groups could be obtained\nfrom which drug discovery can be successfully initiated. The approach,\napplicable to any study case with sufficient data, was demonstrated using a\nPubChem bioassay focused on TDP1 inhibitors. The Scikit Learn interpretation of\nthe Random Forest Classifier (RFC) algorithm was employed. The machine learning\n(ML) model RFC obtained 70.9% accuracy, 73.1% precision, 66.1% recall, 69.4% F1\nand 70.8% receiver-operating characteristic (ROC). In addition to the main\nstudy, the CID_SID ML model was developed, which, using only the PubChem\ncompound and substance identifiers (CIDs and SIDs) data, can predict with 85.2%\naccuracy, 94.2% precision, 75% precision, F1 of 83.5% F1 and 85.2% ROC whether\na compound is a TDP1 inhibitor.",
        "Early stage drug discovery and molecular design projects often follow\niterative design-make-test cycles. The selection of which compounds to\nsynthesize from all possible candidate compounds is a complex decision inherent\nto these design cycles that must weigh multiple factors. We build upon the\nalgorithmic downselection framework SPARROW that considers synthetic cost,\nsynthetic feasibility, and compound utility, extending it to address additional\ncritical factors related to the risk of synthesis failure, molecular diversity,\nand parallel chemistry capabilities. These design considerations further align\nalgorithmic compound selection with the true complexity of this decision-making\nprocess, allowing SPARROW to capture a broader set of principles typically\nreliant on expert chemist intuition. The application of these formulations to\nan exemplary case study highlights SPARROW's ability to promote the selection\nof diverse batches of compounds whose syntheses are amenable to parallel\nchemistry.",
        "Significance and Object: The proposed methodology aims to provide time- and\ncost-effective approach for the early stage in drug discovery. The machine\nlearning models developed in this study used only the identification numbers\nprovided by PubChem. Thus, a drug development researcher who has obtained a\nPubChem CID and SID can easily identify new functionality of their compound.\nThe approach was demonstrated, using four bioassay which were on (i) the\nantagonists of human D3 dopamine receptors; (ii) the promoter Rab9 activators;\n(iii) small molecule inhibitors of CHOP to regulate the unfolded protein\nresponse to ER stress; (iv) antagonists of the human M1 muscarinic receptor.\nSolution: The four bioassays used for demonstration of the approach were\nprovided by PubChem. For each bioassay, the generated by PubChem CIDs, SIDs\nwere extracted together with the corresponding activity. The resulting dataset\nwas sifted with the dataset on a water solubility bioassay, remaining only the\ncompounds common for both bioassays. In this way, the inactive compounds were\nreduced. Then, all active compounds were added, and the resulted dataset was\nlater used for machine learning based on scikit learn algorithms. Results: The\naverage values of the ML models` metrics for the four bioassays were: 83.82%\nAccuracy with 5.35 standard deviation; 87.9% Precision with 5.04 standard\ndeviation; 77.1% Recall with 7.65 standard deviation; 82.1% F1 with 6.44\nstandard deviation; 83.4% ROC with 5.09 standard deviation.",
        "In the context of clinical research, computational models have received\nincreasing attention over the past decades. In this systematic review, we aimed\nto provide an overview of the role of so-called in silico clinical trials\n(ISCTs) in medical applications. Exemplary for the broad field of clinical\nmedicine, we focused on in silico (IS) methods applied in drug development,\nsometimes also referred to as model informed drug development (MIDD). We\nsearched PubMed and ClinicalTrials.gov for published articles and registered\nclinical trials related to ISCTs. We identified 202 articles and 48 trials, and\nof these, 76 articles and 19 trials were directly linked to drug development.\nWe extracted information from all 202 articles and 48 clinical trials and\nconducted a more detailed review of the methods used in the 76 articles that\nare connected to drug development. Regarding application, most articles and\ntrials focused on cancer and imaging related research while rare and pediatric\ndiseases were only addressed in 18 and 4 studies, respectively. While some\nmodels were informed combining mechanistic knowledge with clinical or\npreclinical (in-vivo or in-vitro) data, the majority of models were fully\ndata-driven, illustrating that clinical data is a crucial part in the process\nof generating synthetic data in ISCTs. Regarding reproducibility, a more\ndetailed analysis revealed that only 24% (18 out of 76) of the articles\nprovided an open-source implementation of the applied models, and in only 20%\nof the articles the generated synthetic data were publicly available. Despite\nthe widely raised interest, we also found that it is still uncommon for ISCTs\nto be part of a registered clinical trial and their application is restricted\nto specific diseases leaving potential benefits of ISCTs not fully exploited.",
        "In this paper, a class of reaction-diffusion equations for Multiple Sclerosis\nis presented. These models are derived by means of a diffusive limit starting\nfrom a proper kinetic description, taking account of the underlying microscopic\ninteractions among cells. At the macroscopic level, we discuss the necessary\nconditions for Turing instability phenomena and the formation of\ntwo-dimensional patterns, whose shape and stability are investigated by means\nof a weakly nonlinear analysis. Some numerical simulations, confirming and\nextending theoretical results, are proposed for a specific scenario.",
        "Understanding the intertwined contributions of amino acid sequence and\nspatial structure is essential to explain protein behaviour. Here, we introduce\nINFUSSE (Integrated Network Framework Unifying Structure and Sequence\nEmbeddings), a Deep Learning framework that combines sequence embeddings,\ngenerated by a Large Language Model (LLM), with graph-based representations of\nprotein structures, integrated through a diffusive Graph Convolutional Network\n(diff-GCN), to predict single-residue properties within proteins. Our approach\nfollows two steps. First, we fine-tune LLM sequence embeddings obtained from\nbidirectional transformers to make predictions from protein sequence alone.\nSecond, we combine these enriched sequence representations with a geometric\ngraph Laplacian within diff-GCN to refine the initial predictions. This\napproach leads to improved predictions while allowing us to systematically\ndisentangle the contribution of sequence and structure. We illustrate our\nframework by applying it to the prediction of local residue flexibility\n(B-factors) of antibody-antigen complexes, and show that it provides improved\nperformance compared to current Machine Learning (ML) approaches. The addition\nof structural information via geometric graphs is shown to enhance predictions\nespecially for intrinsically disordered regions, protein-protein interaction\nsites, and highly variable amino acid positions.",
        "Alzheimer's disease (AD) is driven by the accumulation of amyloid-beta\n(Abeta) proteins in the brain, leading to memory loss and cognitive decline.\nWhile monoclonal antibodies targeting Abetahave been approved, optimizing their\nuse to maximize benefits while minimizing side effects remains a challenge.\nThis study develops a mathematical model to describe Abeta aggregation,\ncapturing its progression from monomers to toxic oligomers, protofibrils, and\nfibrils using mass-action kinetics and coarse-grained modeling. The model is\ncalibrated with experimental data, incorporating parameter estimation and\nsensitivity analysis to ensure accuracy. An optimal control framework is\nintroduced to determine the best drug dosing strategy that reduces toxic Abeta\naggregates while minimizing adverse effects, such as amyloid-related imaging\nabnormalities (ARIA). Results indicate that Donanemab achieves the greatest\nreduction in fibrils. This work provides a quantitative framework for\noptimizing AD treatment strategies, offering insights into balancing\ntherapeutic efficacy and safety.",
        "Anaerobic digestion (AD) offers a sustainable biotechnology to recover\nresources from carbon-rich wastewater, such as food-processing wastewater.\nDespite crude wastewater characterisation, the impact of detailed chemical\nfingerprinting on AD remains underexplored. This study investigated the\ninfluence of fermentation-wastewater composition and operational parameters on\nAD over time to identify critical factors influencing reactor biodiversity and\nperformance. Eighteen reactors were operated under various operational\nconditions using mycoprotein fermentation wastewater. Detailed chemical\nanalysis fingerprinted the molecules in the fermentation wastewater throughout\nAD including sugars, sugar alcohols and volatile fatty acids (VFAs). Sequencing\nrevealed distinct microbiome profiles linked to temperature and reactor\nconfiguration, with mesophilic conditions supporting a more diverse and densely\nconnected microbiome. Significant elevations in Methanomassiliicoccus were\ncorrelated to high butyric acid concentrations and decreased biogas production,\nfurther elucidating the role of this newly discovered methanogen. Dissimilarity\nanalysis demonstrated the importance of individual molecules on microbiome\ndiversity, highlighting the need for detailed chemical fingerprinting in AD\nstudies of microbial trends. Machine learning (ML) models predicting reactor\nperformance achieved high accuracy based on operational parameters and\nmicrobial taxonomy. Operational parameters had the most substantial influence\non chemical oxygen demand removal, whilst Oscillibacter and two Clostridium sp.\nwere highlighted as key factors in biogas production. By integrating detailed\nchemical and biological fingerprinting with ML models this research presents a\nnovel approach to advance our understanding of AD microbial ecology, offering\ninsights for industrial applications of sustainable waste-to-energy systems.",
        "Retinal prostheses restore vision by electrically stimulating surviving\nneurons, but calibrating perceptual thresholds - the minimum stimulus intensity\nrequired for perception - remains a time-intensive challenge, especially for\nhigh-electrode-count devices. Since neighboring electrodes exhibit spatial\ncorrelations, we propose a Gaussian Process Regression (GPR) framework to\npredict thresholds at unsampled locations while leveraging uncertainty\nestimates to guide adaptive sampling. Using perceptual threshold data from four\nArgus II users, we show that GPR with a Mat\\'ern kernel provides more accurate\nthreshold predictions than a Radial Basis Function (RBF) kernel (p < .001,\nWilcoxon signed-rank test). In addition, spatially optimized sampling yielded\nlower prediction error than uniform random sampling for Participants 1 and 3 (p\n< .05). While adaptive sampling dynamically selects electrodes based on model\nuncertainty, its accuracy gains over spatial sampling were not statistically\nsignificant (p > .05), though it approached significance for Participant 1 (p =\n.074). These findings establish GPR with spatial sampling as a scalable,\nefficient approach to retinal prosthesis calibration, minimizing patient burden\nwhile maintaining predictive accuracy. More broadly, this framework offers a\ngeneralizable solution for adaptive calibration in neuroprosthetic devices with\nspatially structured stimulation thresholds.",
        "Research in the life sciences often employs messenger ribonucleic acids\n(mRNA) quantification as a standalone approach for functional analysis.\nHowever, although the correlation between the measured levels of mRNA and\nproteins is positive, correlation coefficients observed empirically are\nincomplete, necessitating caution in making agnostic inferences. This essay\nprovides a statistical reflection and caveat on the concept of correlation\nstrength in the context of transcriptomics-proteomics studies. It highlights\nthe variability in possible protein levels at given empirical correlation\nvalues, even for precise mRNA amount, and underscores the notable proportion of\nmRNA-protein pairs with abundances at opposite ends of their respective\ndistributions. Cell biologists, data scientists, and biostatisticians should\nrecognise that mRNA-protein correlation alone is insufficient to justify using\na single mRNA quantification to infer the amount or function of its\ncorresponding protein.",
        "A simple kinematic growth model for muscular arteries is presented which\nallows the incorporation of residual stresses such that a homeostatic in-vivo\nstress state under physiological loading is obtained. To this end, new\nevolution equations for growth are proposed, which avoid issues with\ninstability of final growth states known from other kinematric growth models.\nThese evolution equations are connected to a new set of growth driving forces.\nBy introducing a formulation using the principle Cauchy stresses, reasonable in\nvivo stress distributions can be obtained while ensuring realistic geometries\nof arteries after growth. To incorporate realistic Cauchy stresses for muscular\narteries under varying loading conditions, which appear in vivo, e.g., due to\nphysical activity, the growth model is combined with a newly proposed\nstretch-dependent model for smooth muscle activation. To account for the\nchanges of geometry and mechanical behavior during the growth process, an\noptimization procedure is described which leads to a more accurate\nrepresentation of an arterial ring and its mechanical behavior after the\ngrowth-related homogenization of the stresses is reached. Based thereon,\nparameters are adjusted to experimental data of a healthy middle cerebral\nartery of a rat showing that the proposed model accurately describes real\nmaterial behavior. The successful combination of growth and active response\nindicates that the new growth model can be used without problems for modeling\nactive tissues under various conditions.",
        "The transmission dynamics of an epidemic are rarely homogeneous.\nSuper-spreading events and super-spreading individuals are two types of\nheterogeneous transmissibility. Inference of super-spreading is commonly\ncarried out on secondary case data, the expected distribution of which is known\nas the offspring distribution. However, this data is seldom available. Here we\nintroduce a multi-model framework fit to incidence time-series, data that is\nmuch more readily available. The framework consists of five discrete-time,\nstochastic, branching-process models of epidemics spread through a susceptible\npopulation. The framework includes a baseline model of homogeneous\ntransmission, a unimodal and a bimodal model for super-spreading events, as\nwell as a unimodal and a bimodal model for super-spreading individuals.\nBayesian statistics is used to infer model parameters using Markov Chain\nMonte-Carlo. Model comparison is conducted by computing Bayes factors, with\nimportance sampling used to estimate the marginal likelihood of each model.\nThis estimator is selected for its consistency and lower variance compared to\nalternatives. Application to simulated data from each model identifies the\ncorrect model for the majority of simulations and accurately infers the true\nparameters, such as the basic reproduction number. We also apply our methods to\nincidence data from the 2003 SARS outbreak and the Covid-19 pandemic. Model\nselection consistently identifies the same model and mechanism for a given\ndisease, even when using different time series. Our estimates are consistent\nwith previous studies based on secondary case data. Quantifying the\ncontribution of super-spreading to disease transmission has important\nimplications for infectious disease management and control. Our modelling\nframework is disease-agnostic and implemented as an R package, with potential\nto be a valuable tool for public health.",
        "Greenberg's conjecture on the stability of $\\ell$-class groups in the\ncyclotomic $\\mathbb{Z}_{\\ell}$-extension of a real field has been proven for\nvarious infinite families of real quadratic fields for the prime $\\ell=2$. In\nthis work, we consider an infinite family of real biquadratic fields $K$. With\nsome extensive use of elementary group theoretic and class field theoretic\narguments, we investigate the $2$-class groups of the $n$-th layers $K_n$ of\nthe cyclotomic $\\mathbb{Z}_2$-extension of $K$ and verify Greenberg's\nconjecture. We also relate capitulation of ideal classes of certain\nsub-extensions of $K_n$ to the relative sizes of the $2$-class groups.",
        "For a singularity type $\\eta$, let the $\\eta$-avoiding number of an\n$n$-dimensional manifold $M$ be the lowest $k$ for which there is a map\n$M\\to\\mathbb{R}^{n+k}$ without $\\eta$ type singular points. For instance, the\ncase of $\\eta=\\Sigma^1$ is the case of immersions, which has been extensively\nstudied in the case of real projective spaces. In this paper we study the\n$\\eta$-avoiding number for other singularity types. Our results come in two\nlevels: first we give an abstract reasoning that a non-zero cohomology class is\nsupported on the singularity locus $\\eta(f)$, proving that $\\eta(f)$ cannot be\nempty. Second, we interpret this obstruction as a non-zero invariant of the\nsingularity locus $\\eta(f)$ for generic $f$. The main technique that we employ\nis Sullivan's Stiefel-Whitney classes, which are mod 2, real analogues of the\nChern-Schwartz-MacPherson (CSM) classes. We introduce the Segre-Stiefel-Whitney\nclasses of a singularity ${\\rm s}^{\\rm sw}_\\eta$ whose lowest degree term is\nthe mod 2 Thom polynomial of $\\eta$. Using these techniques we compute some\nuniversal formulas for the Euler characteristic of a singularity locus.",
        "Chiral (highly helical) extreme ultraviolet (XUV) sources are pivotal for\ninvestigating chiroptical phenomena on the ultrafast electronic timescale.\nTable-top, coherent High Harmonic Generation (HHG)-based sources are\nparticularly well-suited for these studies. However, chiral materials, such as\norganic chiral molecules and solid-state magnetic materials, exhibit fine\nspectral features which necessitate broadband radiation for their complete\ninterrogation. The generation of radiation that is both broadband and helical\nthrough HHG presents a seemingly paradoxical challenge: while chiral HHG\nemission requires at least two recollisions occurring along different\ndirections in the polarization plane, the Floquet limit might already be\nreached with as few as three recollisions, resulting in a sparse spectrum\ncharacterized by pronounced discrete harmonic peaks. Here we propose a\nstraightforward scheme that enables the interrogation of fine spectral\nfeatures, in principle restricted only by the resolution of the XUV\nspectrometer, with chiral XUV light. Our method is based on using a vectorial\ntwo-color driver with close central-frequencies with slight symmetry breaking.\nIt integrates the time-gating and polarization-gating techniques to generate a\nvectorial driver which induces well-controlled bursts of recollisions,\noccurring along different directions in the polarization plane. The method\nsatisfies the dual requirements of an XUV source which is both broadband and\nhelical. We perform polarization scan and demonstrate that the broadband XUV\nradiation exhibits rapid modulations in its spectral ellipticity, and fast\nalternation in its spectral helicities. The phase of modulations could be\ncontrolled by introducing a slight symmetry breaking. This allows us to control\nand modulate the XUV polarization state, which should enable the detection of\nchiroptical signals with enhanced sensitivity.",
        "Crystal skeletons were introduced by Maas-Gari\\'epy in 2023 by contracting\nquasi-crystal components in a crystal graph. On the representation theoretic\nlevel, crystal skeletons model the expansion of Schur functions into Gessel's\nquasisymmetric functions. Motivated by questions of Schur positivity, we\nprovide a combinatorial description of crystal skeletons, and prove many new\nproperties, including a conjecture by Maas-Gari\\'epy that crystal skeletons\ngeneralize dual equivalence graphs. We then present a new axiomatic approach to\ncrystal skeletons. We give three versions of the axioms based on\n$GL_n$-branching, $S_n$-branching, and local axioms in analogy to the local\nStembridge axioms for crystals based on novel commutation relations.",
        "In the field of condensed matter physics, time-reversal symmetry provides the\nfoundation for a number of interesting quantum phenomena, in particular the\ntopological materials and the quantum spin Hall physics that have been\nextensively studied in recent years. Here, based on the first-principles\nelectronic-structure calculations, symmetry analysis, and model simulations, we\ndemonstrate that time-reversal symmetry is not fundamentally necessary for the\nquantum spin Hall effect. In altermagnetic materials, as an alternative, it can\nalso be protected by crystal symmetry, which can be referred to as the quantum\ncrystal spin Hall effect.",
        "Two high-level \"pictures\" of probability theory have emerged: one that takes\nas central the notion of random variable, and one that focuses on distributions\nand probability channels (Markov kernels). While the channel-based picture has\nbeen successfully axiomatized, and widely generalized, using the notion of\nMarkov category, the categorical semantics of the random variable picture\nremain less clear. Simpson's probability sheaves are a recent approach, in\nwhich probabilistic concepts like random variables are allowed vary over a site\nof sample spaces. Simpson has identified rich structure on these sites, most\nnotably an abstract notion of conditional independence, and given examples\nranging from probability over databases to nominal sets. We aim bring this\ndevelopment together with the generality and abstraction of Markov categories:\nWe show that for any suitable Markov category, a category of sample spaces can\nbe defined which satisfies Simpson's axioms, and that a theory of probability\nsheaves can be developed purely synthetically in this setting. We recover\nSimpson's examples in a uniform fashion from well-known Markov categories, and\nconsider further generalizations.",
        "We introduce an automated, flexible framework (aiida-hubbard) to\nself-consistently calculate Hubbard $U$ and $V$ parameters from\nfirst-principles. By leveraging density-functional perturbation theory, the\ncomputation of the Hubbard parameters is efficiently parallelized using\nmultiple concurrent and inexpensive primitive cell calculations. Furthermore,\nthe intersite $V$ parameters are defined on-the-fly during the iterative\nprocedure to account for atomic relaxations and diverse coordination\nenvironments. We demonstrate the scalability and reliability of the framework\nby computing in high-throughput fashion the self-consistent onsite $U$ and\nintersite $V$ parameters for 115 Li-containing bulk solids. Our analysis of the\nHubbard parameters calculated reveals a significant correlation of the onsite\n$U$ values on the oxidation state and coordination environment of the atom on\nwhich the Hubbard manifold is centered, while intersite $V$ values exhibit a\ngeneral decay with increasing interatomic distance. We find, e.g., that the\nnumerical values of $U$ for Fe and Mn 3d orbitals can vary up to 3 eV and 6 eV,\nrespectively; their distribution is characterized by typical shifts of about\n0.5 eV and 1.0 eV upon change in oxidation state, or local coordination\nenvironment. For the intersite $V$ a narrower spread is found, with values\nranging between 0.2 eV and 1.6 eV when considering transition metal and oxygen\ninteractions. This framework paves the way for the exploration of redox\nmaterials chemistry and high-throughput screening of $d$ and $f$ compounds\nacross diverse research areas, including the discovery and design of novel\nenergy storage materials, as well as other technologically-relevant\napplications.",
        "We present a fault-tolerant Bell-pair distillation scheme achieving constant\noverhead through high-rate quantum low-density parity-check (qLDPC) codes. Our\napproach maintains a constant distillation rate equal to the code rate - as\nhigh as $1\/3$ in our implementations - while requiring no additional overhead\nbeyond the physical qubits of the code. Full circuit-level analysis\ndemonstrates fault-tolerance for input Bell pair infidelities below a threshold\n$\\sim 5\\%$, readily achievable with near-term capabilities. Unlike previous\nproposals, our scheme keeps the output Bell pairs encoded in qLDPC codes at\neach node, eliminating decoding overhead and enabling direct use in distributed\nquantum applications through recent advances in qLDPC computation. These\nresults establish qLDPC-based distillation as a practical route toward\nresource-efficient quantum networks and distributed quantum computing.",
        "Recent cosmological observations suggest that the dark energy equation of\nstate may have changed in the latter stages of cosmic history. We introduce a\nquintessence scenario, termed bounded dark energy, capable of explaining this\nfeature in a technically natural way. Our approach is motivated from a\nbottom-up perspective, based on the concept of mirage cut-off, where we\ndemonstrate the stability of the quintessence potential against large quantum\ncorrections. At the same time, the bounded dark energy framework aligns well\nwith top-down considerations motivated from quantum gravity arguments. We\nemploy both human-driven insights and machine learning techniques to identify\nexplicit realizations of bounded dark energy models. We then perform an\nanalysis based on Markov Chain Monte-Carlo to assess their predictions against\nCMB, galaxy surveys, and supernova data, showing that bounded dark energy\nprovides a good fit to current observations. We also discuss how upcoming\nmeasurements can further test and refine our proposal.",
        "Lepton flavor violation (LFV) offers a powerful probe of physics beyond the\nStandard Model, particularly in models addressing neutrino masses and the\nbaryon asymmetry of the universe. In this study, we investigate LFV processes\nwithin the framework of type II seesaw leptogenesis, where the Standard Model\nis extended by an $SU(2)_L$ triplet Higgs field. We focus on key LFV processes\nincluding $\\mu^+\\to e^+\\gamma$, $\\mu^+ \\to e^+e^-e^+$, and $\\mu \\rightarrow e$\nconversion in nuclei, deriving stringent constraints on the parameter space\nfrom current experimental data. We scan the 3$\\sigma$ range of neutrino\noscillation parameters and identify the most conservative bounds consistent\nwith existing measurements. Our results reveal that the MEG experiment\ncurrently provides the strongest constraints in the normal ordering (NO)\nscenario, while the SINDRUM experiment offers comparable sensitivity in the\ninverted ordering (IO) case. Future experiments, such as MEG II, Mu3e, Mu2e,\nand COMET, are predicted to significantly improve the sensitivity, testing\nlarger regions of the parameter space. This work underscores the crucial role\nof LFV experiments in probing type II seesaw leptogenesis, providing an avenue\nto explore the connections between neutrino mass generation, baryogenesis, and\ninflation at experimentally accessible energy scales.",
        "We consider a strategic M\/M\/1 queueing model under a first-come-first-served\nregime, where customers are split into two classes and class $A$ has priority\nover class $B$. Customers can decide whether to join the queue or balk, and, in\ncase they have joined the queue, whether and when to renege. We study the\nequilibrium strategies and compare the equilibrium outcome and the social\noptimum in the two cases where the social optimum is or is not constrained by\npriority.",
        "Financial bond yield forecasting is challenging due to data scarcity,\nnonlinear macroeconomic dependencies, and evolving market conditions. In this\npaper, we propose a novel framework that leverages Causal Generative\nAdversarial Networks (CausalGANs) and Soft Actor-Critic (SAC) reinforcement\nlearning (RL) to generate high-fidelity synthetic bond yield data for four\nmajor bond categories (AAA, BAA, US10Y, Junk). By incorporating 12 key\nmacroeconomic variables, we ensure statistical fidelity by preserving essential\nmarket properties. To transform this market dependent synthetic data into\nactionable insights, we employ a finetuned Large Language Model (LLM)\nQwen2.5-7B that generates trading signals (BUY\/HOLD\/SELL), risk assessments,\nand volatility projections. We use automated, human and LLM evaluations, all of\nwhich demonstrate that our framework improves forecasting performance over\nexisting methods, with statistical validation via predictive accuracy, MAE\nevaluation(0.103%), profit\/loss evaluation (60% profit rate), LLM evaluation\n(3.37\/5) and expert assessments scoring 4.67 out of 5. The reinforcement\nlearning-enhanced synthetic data generation achieves the least Mean Absolute\nError of 0.103, demonstrating its effectiveness in replicating real-world bond\nmarket dynamics. We not only enhance data-driven trading strategies but also\nprovides a scalable, high-fidelity synthetic financial data pipeline for risk &\nvolatility management and investment decision-making. This work establishes a\nbridge between synthetic data generation, LLM driven financial forecasting, and\nlanguage model evaluation, contributing to AI-driven financial decision-making.",
        "We construct representations of the deformed Shatashvili-Vafa vertex algebra\n$\\mathrm{SV}_a$, with parameter $a \\in \\mathbb{C}$, as recently proposed in the\nphysics literature by Fiset and Gaberdiel. The geometric input for our\nconstruction are integrable $\\mathrm{G}_2$-structures with closed torsion,\nsolving the heterotic $\\mathrm{G}_2$ system with $\\alpha'=0$ on the group\nmanifolds $S^3\\times T^4$ and $S^3\\times S^3\\times S^1$. From considerations in\nstring theory, one expects the chiral algebra of these backgrounds to include\n$\\mathrm{SV}_a$, and we provide a mathematical realization of this expectation\nby obtaining embeddings of $\\mathrm{SV}_a$ in the corresponding superaffine\nvertex algebra and the chiral de Rham complex. In our examples, the parameter\n$a$ is proportional to the scalar torsion class of the $\\mathrm{G}_2$\nstructure, $a \\sim \\tau_0$, as expected from previous work in the\nsemi-classical limit by the second author, jointly with De la Ossa and\nMarchetto.",
        "The increasing availability of network data has driven the development of\nadvanced statistical models specifically designed for metric graphs, where\nGaussian processes play a pivotal role. While models such as Whittle-Mat\\'ern\nfields have been introduced, there remains a lack of practically applicable\noptions that accommodate flexible non-stationary covariance structures or\ngeneral smoothness. To address this gap, we propose a novel class of\ngeneralized Whittle-Mat\\'ern fields, which are rigorously defined on general\ncompact metric graphs and permit both non-stationarity and arbitrary\nsmoothness. We establish new regularity results for these fields, which extend\neven to the standard Whittle-Mat\\'ern case. Furthermore, we introduce a method\nto approximate the covariance operator of these processes by combining the\nfinite element method with a rational approximation of the operator's\nfractional power, enabling computationally efficient Bayesian inference for\nlarge datasets. Theoretical guarantees are provided by deriving explicit\nconvergence rates for the covariance approximation error, and the practical\nutility of our approach is demonstrated through simulation studies and an\napplication to traffic speed data, highlighting the flexibility and\neffectiveness of the proposed model class.",
        "Real-valued Phase retrieval is a non-convex continuous inference problem,\nwhere a high-dimensional signal is to be reconstructed from a dataset of\nsignless linear measurements. Focusing on the noiseless case, we aim to\ndisentangle the two distinct sub-tasks entailed in the Phase retrieval problem:\nthe hard combinatorial problem of retrieving the missing signs of the\nmeasurements, and the nested convex problem of regressing the input-output\nobservations to recover the hidden signal. To this end, we introduce and\nanalytically characterize a two-level formulation of the problem, called\n``Phase selection''. Within the Replica Theory framework, we perform a large\ndeviation analysis to characterize the minimum mean squared error achievable\nwith different guesses for the hidden signs. Moreover, we study the free-energy\nlandscape of the problem when both levels are optimized simultaneously, as a\nfunction of the dataset size. At low temperatures, in proximity to the\nBayes-optimal threshold -- previously derived in the context of Phase retrieval\n-- we detect the coexistence of two free-energy branches, one connected to the\nrandom initialization condition and a second to the signal. We derive the phase\ndiagram for a first-order transition after which the two branches merge.\nInterestingly, introducing an $L_2$ regularization in the regression sub-task\ncan anticipate the transition to lower dataset sizes, at the cost of a bias in\nthe signal reconstructions which can be removed by annealing the regularization\nintensity. Finally, we study the inference performance of three meta-heuristics\nin the context of Phase selection: Simulated Annealing, Approximate Message\nPassing, and Langevin Dynamics on the continuous relaxation of the sign\nvariables. With simultaneous annealing of the temperature and the $L_2$\nregularization, they are shown to approach the Bayes-optimal sample efficiency."
      ]
    }
  },
  {
    "id":2412.00225,
    "research_type":"applied",
    "start_id":"b14",
    "start_title":"The Burgers equation",
    "start_abstract":"The Burgers equation is a simple equation to understand the main properties of the Navier-Stokes equations. In this one-dimensional equation the pressure is neglected but the effects of the nonlinear and viscous terms remain, hence as in the Navier-Stokes equations a Reynolds number can be defined. This number expresses the ratio between the advective and the viscous contribution in a flow. The present book deals with flows at high Reynolds numbers where the nonlinear terms play a fundamental role, and the physics is more complicated than that when the viscous term dominates. The simulation of the flow evolution then necessitates the use of accurate and robust numerical methods. In 3D turbulent flows, where the number of degrees of freedom is greater than in high Re laminar flows, to get solutions it is necessary to introduce some sort of closure to account for the impossibility to resolve the small scales. Before applying any new idea about numerical methods to 3D flows, the good sense suggests to find the simplest equation to test these ideas. This consideration explains why the Burgers equation was often used to check new numerical methods or closure for turbulent flows.",
    "start_categories":[
      "math.AP"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b17"
      ],
      "title":[
        "Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations"
      ],
      "abstract":[
        "We introduce physics-informed neural networks \u2013 neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge\u2013Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction\u2013diffusion systems, and the propagation of nonlinear shallow-water waves."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Behaviour Discovery and Attribution for Explainable Reinforcement\n  Learning",
        "Rule-Based Conflict-Free Decision Framework in Swarm Confrontation",
        "Beyond Text: Implementing Multimodal Large Language Model-Powered\n  Multi-Agent Systems Using a No-Code Platform",
        "Energy-Conscious LLM Decoding: Impact of Text Generation Strategies on\n  GPU Energy Consumption",
        "Towards a Formal Theory of the Need for Competence via Computational\n  Intrinsic Motivation",
        "Visualizing Thought: Conceptual Diagrams Enable Robust Planning in LMMs",
        "Lifelong Learning of Large Language Model based Agents: A Roadmap",
        "Benchmarking Reasoning Robustness in Large Language Models",
        "Driving behavior recognition via self-discovery learning",
        "Generating Counterfactual Explanations Under Temporal Constraints",
        "R-ParVI: Particle-based variational inference through lens of rewards",
        "MiniRAG: Towards Extremely Simple Retrieval-Augmented Generation",
        "Empowering GraphRAG with Knowledge Filtering and Integration",
        "Teach-to-Reason with Scoring: Self-Explainable Rationale-Driven\n  Multi-Trait Essay Scoring",
        "FlashSR: One-step Versatile Audio Super-resolution via Diffusion\n  Distillation",
        "Insights of Transitions to Thermoacoustic Instability in Inverse\n  Diffusion Flame using Multifractal Detrended Fluctuation Analysis",
        "Delta-WKV: A Novel Meta-in-Context Learner for MRI Super-Resolution",
        "AAS2RTO: Automated Alert Streams to Real-Time Observations: Preparing\n  for rapid follow-up of transient objects in the era of LSST",
        "\"Auntie, Please Don't Fall for Those Smooth Talkers\": How Chinese\n  Younger Family Members Safeguard Seniors from Online Fraud",
        "Complex Riemannian spacetime and singularity-free black holes and\n  cosmology",
        "Contrast-Free Myocardial Scar Segmentation in Cine MRI using Motion and\n  Texture Fusion",
        "Proof-Driven Clause Learning in Neural Network Verification",
        "The impact of baryons on the sparsity of simulated galaxy clusters from\n  The Three Hundred Project",
        "Sketch-of-Thought: Efficient LLM Reasoning with Adaptive\n  Cognitive-Inspired Sketching",
        "Efficient Language Modeling for Low-Resource Settings with Hybrid\n  RNN-Transformer Architectures",
        "Semi-supervised Anomaly Detection with Extremely Limited Labels in\n  Dynamic Graphs",
        "Quantifying the degree of hydrodynamic behaviour in heavy-ion collisions",
        "Funnelling super-resolution STED microscopy through multimode fibres"
      ],
      "abstract":[
        "Explaining the decisions made by reinforcement learning (RL) agents is\ncritical for building trust and ensuring reliability in real-world\napplications. Traditional approaches to explainability often rely on saliency\nanalysis, which can be limited in providing actionable insights. Recently,\nthere has been growing interest in attributing RL decisions to specific\ntrajectories within a dataset. However, these methods often generalize\nexplanations to long trajectories, potentially involving multiple distinct\nbehaviors. Often, providing multiple more fine grained explanations would\nimprove clarity. In this work, we propose a framework for behavior discovery\nand action attribution to behaviors in offline RL trajectories. Our method\nidentifies meaningful behavioral segments, enabling more precise and granular\nexplanations associated with high level agent behaviors. This approach is\nadaptable across diverse environments with minimal modifications, offering a\nscalable and versatile solution for behavior discovery and attribution for\nexplainable RL.",
        "Traditional rule-based decision-making methods with interpretable advantage,\nsuch as finite state machine, suffer from the jitter or deadlock(JoD) problems\nin extremely dynamic scenarios. To realize agent swarm confrontation, decision\nconflicts causing many JoD problems are a key issue to be solved. Here, we\npropose a novel decision-making framework that integrates probabilistic finite\nstate machine, deep convolutional networks, and reinforcement learning to\nimplement interpretable intelligence into agents. Our framework overcomes state\nmachine instability and JoD problems, ensuring reliable and adaptable decisions\nin swarm confrontation. The proposed approach demonstrates effective\nperformance via enhanced human-like cooperation and competitive strategies in\nthe rigorous evaluation of real experiments, outperforming other methods.",
        "This study proposes the design and implementation of a multimodal LLM-based\nMulti-Agent System (MAS) leveraging a No-Code platform to address the practical\nconstraints and significant entry barriers associated with AI adoption in\nenterprises. Advanced AI technologies, such as Large Language Models (LLMs),\noften pose challenges due to their technical complexity and high implementation\ncosts, making them difficult for many organizations to adopt. To overcome these\nlimitations, this research develops a No-Code-based Multi-Agent System designed\nto enable users without programming knowledge to easily build and manage AI\nsystems. The study examines various use cases to validate the applicability of\nAI in business processes, including code generation from image-based notes,\nAdvanced RAG-based question-answering systems, text-based image generation, and\nvideo generation using images and prompts. These systems lower the barriers to\nAI adoption, empowering not only professional developers but also general users\nto harness AI for significantly improved productivity and efficiency. By\ndemonstrating the scalability and accessibility of No-Code platforms, this\nstudy advances the democratization of AI technologies within enterprises and\nvalidates the practical applicability of Multi-Agent Systems, ultimately\ncontributing to the widespread adoption of AI across various industries.",
        "Decoding strategies significantly influence the quality and diversity of the\ngenerated texts in large language models (LLMs), yet their impact on\ncomputational resource consumption, particularly GPU energy usage, is\ninsufficiently studied. This paper investigates the relationship between text\ngeneration decoding methods and energy efficiency, focusing on the trade-off\nbetween generation quality and GPU energy consumption across diverse tasks and\ndecoding configurations. By benchmarking multiple strategies across different\ntext generation tasks, such as Translation, Code Summarization, and Math\nProblem Solving, we reveal how selecting appropriate decoding techniques with\ntheir tuned hyperparameters affects text quality and has measurable\nimplications for resource utilization, emphasizing the need for balanced\noptimization. To the best of our knowledge, this study is among the first to\nexplore decoding strategies in LLMs through the lens of energy consumption,\noffering actionable insights for designing resource-aware applications that\nmaintain high-quality text generation.",
        "Computational models offer powerful tools for formalising psychological\ntheories, making them both testable and applicable in digital contexts.\nHowever, they remain little used in the study of motivation within psychology.\nWe focus on the \"need for competence\", postulated as a key basic human need\nwithin Self-Determination Theory (SDT) -- arguably the most influential\npsychological framework for studying intrinsic motivation (IM). The need for\ncompetence is treated as a single construct across SDT texts. Yet, recent\nresearch has identified multiple, ambiguously defined facets of competence in\nSDT. We propose that these inconsistencies may be alleviated by drawing on\ncomputational models from the field of artificial intelligence, specifically\nfrom the domain of reinforcement learning (RL). By aligning the aforementioned\nfacets of competence -- effectance, skill use, task performance, and capacity\ngrowth -- with existing RL formalisms, we provide a foundation for advancing\ncompetence-related theory in SDT and motivational psychology more broadly. The\nformalisms reveal underlying preconditions that SDT fails to make explicit,\ndemonstrating how computational models can improve our understanding of IM.\nAdditionally, our work can support a cycle of theory development by inspiring\nnew computational models formalising aspects of the theory, which can then be\ntested empirically to refine the theory. While our research lays a promising\nfoundation, empirical studies of these models in both humans and machines are\nneeded, inviting collaboration across disciplines.",
        "Human reasoning relies on constructing and manipulating mental\nmodels-simplified internal representations of situations that we use to\nunderstand and solve problems. Conceptual diagrams (for example, sketches drawn\nby humans to aid reasoning) externalize these mental models, abstracting\nirrelevant details to efficiently capture relational and spatial information.\nIn contrast, Large Language Models (LLMs) and Large Multimodal Models (LMMs)\npredominantly reason through textual representations, limiting their\neffectiveness in complex multi-step combinatorial and planning tasks. In this\npaper, we propose a zero-shot fully automatic framework that enables LMMs to\nreason through multiple chains of self-generated intermediate conceptual\ndiagrams, significantly enhancing their combinatorial planning capabilities.\nOur approach does not require any human initialization beyond a natural\nlanguage description of the task. It integrates both textual and diagrammatic\nreasoning within an optimized graph-of-thought inference framework, enhanced by\nbeam search and depth-wise backtracking. Evaluated on multiple challenging PDDL\nplanning domains, our method substantially improves GPT-4o's performance (for\nexample, from 35.5% to 90.2% in Blocksworld). On more difficult planning\ndomains with solution depths up to 40, our approach outperforms even the\no1-preview reasoning model (for example, over 13% improvement in Parking).\nThese results highlight the value of conceptual diagrams as a complementary\nreasoning medium in LMMs.",
        "Lifelong learning, also known as continual or incremental learning, is a\ncrucial component for advancing Artificial General Intelligence (AGI) by\nenabling systems to continuously adapt in dynamic environments. While large\nlanguage models (LLMs) have demonstrated impressive capabilities in natural\nlanguage processing, existing LLM agents are typically designed for static\nsystems and lack the ability to adapt over time in response to new challenges.\nThis survey is the first to systematically summarize the potential techniques\nfor incorporating lifelong learning into LLM-based agents. We categorize the\ncore components of these agents into three modules: the perception module for\nmultimodal input integration, the memory module for storing and retrieving\nevolving knowledge, and the action module for grounded interactions with the\ndynamic environment. We highlight how these pillars collectively enable\ncontinuous adaptation, mitigate catastrophic forgetting, and improve long-term\nperformance. This survey provides a roadmap for researchers and practitioners\nworking to develop lifelong learning capabilities in LLM agents, offering\ninsights into emerging trends, evaluation metrics, and application scenarios.\nRelevant literature and resources are available at \\href{this\nurl}{https:\/\/github.com\/qianlima-lab\/awesome-lifelong-llm-agent}.",
        "Despite the recent success of large language models (LLMs) in reasoning such\nas DeepSeek, we for the first time identify a key dilemma in reasoning\nrobustness and generalization: significant performance degradation on novel or\nincomplete data, suggesting a reliance on memorized patterns rather than\nsystematic reasoning. Our closer examination reveals four key unique\nlimitations underlying this issue:(1) Positional bias--models favor earlier\nqueries in multi-query inputs but answering the wrong one in the latter (e.g.,\nGPT-4o's accuracy drops from 75.8 percent to 72.8 percent); (2) Instruction\nsensitivity--performance declines by 5.0 to 7.5 percent in the Qwen2.5 Series\nand by 5.0 percent in DeepSeek-V3 with auxiliary guidance; (3) Numerical\nfragility--value substitution sharply reduces accuracy (e.g., GPT-4o drops from\n97.5 percent to 82.5 percent, GPT-o1-mini drops from 97.5 percent to 92.5\npercent); and (4) Memory dependence--models resort to guesswork when missing\ncritical data. These findings further highlight the reliance on heuristic\nrecall over rigorous logical inference, demonstrating challenges in reasoning\nrobustness. To comprehensively investigate these robustness challenges, this\npaper introduces a novel benchmark, termed as Math-RoB, that exploits\nhallucinations triggered by missing information to expose reasoning gaps. This\nis achieved by an instruction-based approach to generate diverse datasets that\nclosely resemble training distributions, facilitating a holistic robustness\nassessment and advancing the development of more robust reasoning frameworks.\nBad character(s) in field Abstract.",
        "Autonomous driving systems require a deep understanding of human driving\nbehaviors to achieve higher intelligence and safety.Despite advancements in\ndeep learning, challenges such as long-tail distribution due to scarce samples\nand confusion from similar behaviors hinder effective driving behavior\ndetection.Existing methods often fail to address sample confusion adequately,\nas datasets frequently contain ambiguous samples that obscure unique semantic\ninformation.",
        "Counterfactual explanations are one of the prominent eXplainable Artificial\nIntelligence (XAI) techniques, and suggest changes to input data that could\nalter predictions, leading to more favourable outcomes. Existing counterfactual\nmethods do not readily apply to temporal domains, such as that of process\nmining, where data take the form of traces of activities that must obey to\ntemporal background knowledge expressing which dynamics are possible and which\nnot. Specifically, counterfactuals generated off-the-shelf may violate the\nbackground knowledge, leading to inconsistent explanations. This work tackles\nthis challenge by introducing a novel approach for generating temporally\nconstrained counterfactuals, guaranteed to comply by design with background\nknowledge expressed in Linear Temporal Logic on process traces (LTLp). We do so\nby infusing automata-theoretic techniques for LTLp inside a genetic algorithm\nfor counterfactual generation. The empirical evaluation shows that the\ngenerated counterfactuals are temporally meaningful and more interpretable for\napplications involving temporal dependencies.",
        "A reward-guided, gradient-free ParVI method, \\textit{R-ParVI}, is proposed\nfor sampling partially known densities (e.g. up to a constant). R-ParVI\nformulates the sampling problem as particle flow driven by rewards: particles\nare drawn from a prior distribution, navigate through parameter space with\nmovements determined by a reward mechanism blending assessments from the target\ndensity, with the steady state particle configuration approximating the target\ngeometry. Particle-environment interactions are simulated by stochastic\nperturbations and the reward mechanism, which drive particles towards high\ndensity regions while maintaining diversity (e.g. preventing from collapsing\ninto clusters). R-ParVI offers fast, flexible, scalable and stochastic sampling\nand inference for a class of probabilistic models such as those encountered in\nBayesian inference and generative modelling.",
        "The growing demand for efficient and lightweight Retrieval-Augmented\nGeneration (RAG) systems has highlighted significant challenges when deploying\nSmall Language Models (SLMs) in existing RAG frameworks. Current approaches\nface severe performance degradation due to SLMs' limited semantic understanding\nand text processing capabilities, creating barriers for widespread adoption in\nresource-constrained scenarios. To address these fundamental limitations, we\npresent MiniRAG, a novel RAG system designed for extreme simplicity and\nefficiency. MiniRAG introduces two key technical innovations: (1) a\nsemantic-aware heterogeneous graph indexing mechanism that combines text chunks\nand named entities in a unified structure, reducing reliance on complex\nsemantic understanding, and (2) a lightweight topology-enhanced retrieval\napproach that leverages graph structures for efficient knowledge discovery\nwithout requiring advanced language capabilities. Our extensive experiments\ndemonstrate that MiniRAG achieves comparable performance to LLM-based methods\neven when using SLMs while requiring only 25\\% of the storage space.\nAdditionally, we contribute a comprehensive benchmark dataset for evaluating\nlightweight RAG systems under realistic on-device scenarios with complex\nqueries. We fully open-source our implementation and datasets at:\nhttps:\/\/github.com\/HKUDS\/MiniRAG.",
        "In recent years, large language models (LLMs) have revolutionized the field\nof natural language processing. However, they often suffer from knowledge gaps\nand hallucinations. Graph retrieval-augmented generation (GraphRAG) enhances\nLLM reasoning by integrating structured knowledge from external graphs.\nHowever, we identify two key challenges that plague GraphRAG:(1) Retrieving\nnoisy and irrelevant information can degrade performance and (2)Excessive\nreliance on external knowledge suppresses the model's intrinsic reasoning. To\naddress these issues, we propose GraphRAG-FI (Filtering and Integration),\nconsisting of GraphRAG-Filtering and GraphRAG-Integration. GraphRAG-Filtering\nemploys a two-stage filtering mechanism to refine retrieved information.\nGraphRAG-Integration employs a logits-based selection strategy to balance\nexternal knowledge from GraphRAG with the LLM's intrinsic reasoning,reducing\nover-reliance on retrievals. Experiments on knowledge graph QA tasks\ndemonstrate that GraphRAG-FI significantly improves reasoning performance\nacross multiple backbone models, establishing a more reliable and effective\nGraphRAG framework.",
        "Multi-trait automated essay scoring (AES) systems provide a fine-grained\nevaluation of an essay's diverse aspects. While they excel in scoring, prior\nsystems fail to explain why specific trait scores are assigned. This lack of\ntransparency leaves instructors and learners unconvinced of the AES outputs,\nhindering their practical use. To address this, we propose a self-explainable\nRationale-Driven Multi-trait automated Essay scoring (RaDME) framework. RaDME\nleverages the reasoning capabilities of large language models (LLMs) by\ndistilling them into a smaller yet effective scorer. This more manageable\nstudent model is optimized to sequentially generate a trait score followed by\nthe corresponding rationale, thereby inherently learning to select a more\njustifiable score by considering the subsequent rationale during training. Our\nfindings indicate that while LLMs underperform in direct AES tasks, they excel\nin rationale generation when provided with precise numerical scores. Thus,\nRaDME integrates the superior reasoning capacities of LLMs into the robust\nscoring accuracy of an optimized smaller model. Extensive experiments\ndemonstrate that RaDME achieves both accurate and adequate reasoning while\nsupporting high-quality multi-trait scoring, significantly enhancing the\ntransparency of AES.",
        "Versatile audio super-resolution (SR) is the challenging task of restoring\nhigh-frequency components from low-resolution audio with sampling rates between\n4kHz and 32kHz in various domains such as music, speech, and sound effects.\nPrevious diffusion-based SR methods suffer from slow inference due to the need\nfor a large number of sampling steps. In this paper, we introduce FlashSR, a\nsingle-step diffusion model for versatile audio super-resolution aimed at\nproducing 48kHz audio. FlashSR achieves fast inference by utilizing diffusion\ndistillation with three objectives: distillation loss, adversarial loss, and\ndistribution-matching distillation loss. We further enhance performance by\nproposing the SR Vocoder, which is specifically designed for SR models\noperating on mel-spectrograms. FlashSR demonstrates competitive performance\nwith the current state-of-the-art model in both objective and subjective\nevaluations while being approximately 22 times faster.",
        "The inverse diffusion flame (IDF) can experience thermoacoustic instability\ndue to variations in power input or flow conditions. However, the dynamical\ntransitions in IDF that lead to this instability when altering control\nparameters have not been thoroughly investigated. In this study, we explore the\ncontrol parameters through two different approaches and employ multifractal\ndetrended fluctuation analysis to characterize the transitions observed prior\nto the onset of thermoacoustic instability in the inverse diffusion flame. Our\nfindings reveal a loss of multifractality near the region associated with\nthermoacoustic instability, which suggests a more ordered behavior. We\ndetermine that the singularity exponent, the width of the multifractal\nspectrum, and the Hurst exponent are reliable indicators of thermoacoustic\ninstability and serve as effective classifiers of dynamical states in inverse\ndiffusion flames.",
        "Magnetic Resonance Imaging (MRI) Super-Resolution (SR) addresses the\nchallenges such as long scan times and expensive equipment by enhancing image\nresolution from low-quality inputs acquired in shorter scan times in clinical\nsettings. However, current SR techniques still have problems such as limited\nability to capture both local and global static patterns effectively and\nefficiently. To address these limitations, we propose Delta-WKV, a novel MRI\nsuper-resolution model that combines Meta-in-Context Learning (MiCL) with the\nDelta rule to better recognize both local and global patterns in MRI images.\nThis approach allows Delta-WKV to adjust weights dynamically during inference,\nimproving pattern recognition with fewer parameters and less computational\neffort, without using state-space modeling. Additionally, inspired by\nReceptance Weighted Key Value (RWKV), Delta-WKV uses a quad-directional\nscanning mechanism with time-mixing and channel-mixing structures to capture\nlong-range dependencies while maintaining high-frequency details. Tests on the\nIXI and fastMRI datasets show that Delta-WKV outperforms existing methods,\nimproving PSNR by 0.06 dB and SSIM by 0.001, while reducing training and\ninference times by over 15\\%. These results demonstrate its efficiency and\npotential for clinical use with large datasets and high-resolution imaging.",
        "The upcoming Vera C. Rubin Legacy Survey of Space and Time (LSST) will\ndiscover tens of thousands of astrophysical transients per night, far outpacing\navailable spectroscopic follow-up capabilities. Carefully prioritising\ncandidates for follow-up observations will maximise the scientific return from\nsmall telescopes with a single-object spectrograph. We introduce AAS2RTO, an\nastrophysical transient candidate prioritisation tool written in Python.\nAAS2RTO is flexible in that any number of criteria that consider observed\nproperties of transients can be implemented. The visibility of candidates from\na given observing site is also considered. The prioritised list of candidates\nprovided by AAS2RTO is continually updated when new transient data are made\navailable. Therefore, it can be applied to observing campaigns with a wide\nvariety of scientific motivations. AAS2RTO uses a greedy algorithm to\nprioritise candidates. Candidates are represented by a single numerical value,\nor `score'. Scores are computed by constructing simple numerical factors which\nindividually consider the competing facets of a candidate which make it\nsuitable for follow-up observation. AAS2RTO is currently configured to work\nprimarily with photometric data from the Zwicky Transient Facility (ZTF),\ndistributed by certified LSST community brokers. We provide an example of how\nAAS2RTO can be used by defining a set of criteria to prioritise observations of\ntype Ia supernovae (SNe Ia) close to peak brightness, in preparation for\nobservations with the spectrograph at the Danish-1.54m telescope. Using a\nsample of archival alerts from ZTF, we evaluate the criteria we have designed\nto estimate the number of SNe Ia that we will be able to observe with a 1.5m\ntelescope. Finally, we evaluate the performance of our criteria when applied to\nmock LSST observations of SNe Ia.",
        "Online fraud substantially harms individuals and seniors are\ndisproportionately targeted. While family is crucial for seniors, little\nresearch has empirically examined how they protect seniors against fraud. To\naddress this gap, we employed an inductive thematic analysis of 124 posts and\n16,872 comments on RedNote (Xiaohongshu), exploring the family support\necosystem for senior-targeted online fraud in China. We develop a taxonomy of\nsenior-targeted online fraud from a familial perspective, revealing younger\nmembers often spot frauds hard for seniors to detect, such as unusual charges.\nYounger family members fulfill multiple safeguarding roles, including\npreventative measures, fraud identification, fraud persuasion, loss recovery,\nand education. They also encounter numerous challenges, such as seniors'\nrefusal of help and considerable mental and financial stress. Drawing on these,\nwe develop a conceptual framework to characterize family support in\nsenior-targeted fraud, and outline implications for researchers and\npractitioners to consider the broader stakeholder ecosystem and cultural\naspects.",
        "An approach is presented to address singularities in general relativity using\na complex Riemannian spacetime extension. We demonstrate how this method can be\napplied to both black hole and cosmological singularities, specifically\nfocusing on the Schwarzschild and Kerr black holes and the\nFriedmann-Lema\\^itre-Robertson-Walker (FLRW) Big Bang cosmology. By extending\nthe relevant coordinates into the complex plane and carefully choosing\nintegration contours, we show that it is possible to regularize these\nsingularities, resulting in physically meaningful, singularity-free solutions\nwhen projected back onto real spacetime. The removal of the singularity at the\nBig Bang allows for a bounce cosmology. This approach offers a potential bridge\nbetween classical general relativity and quantum gravity effects, suggesting a\nway to resolve longstanding issues in gravitational physics without requiring a\nfull theory of quantum gravity.",
        "Late gadolinium enhancement MRI (LGE MRI) is the gold standard for the\ndetection of myocardial scars for post myocardial infarction (MI). LGE MRI\nrequires the injection of a contrast agent, which carries potential side\neffects and increases scanning time and patient discomfort. To address these\nissues, we propose a novel framework that combines cardiac motion observed in\ncine MRI with image texture information to segment the myocardium and scar\ntissue in the left ventricle. Cardiac motion tracking can be formulated as a\nfull cardiac image cycle registration problem, which can be solved via deep\nneural networks. Experimental results prove that the proposed method can\nachieve scar segmentation based on non-contrasted cine images with comparable\naccuracy to LGE MRI. This demonstrates its potential as an alternative to\ncontrast-enhanced techniques for scar detection.",
        "The widespread adoption of deep neural networks (DNNs) requires efficient\ntechniques for safety verification. Existing methods struggle to scale to\nreal-world DNNs, and tremendous efforts are being put into improving their\nscalability. In this work, we propose an approach for improving the scalability\nof DNN verifiers using Conflict-Driven Clause Learning (CDCL) -- an approach\nthat has proven highly successful in SAT and SMT solving. We present a novel\nalgorithm for deriving conflict clauses using UNSAT proofs, and propose several\noptimizations for expediting it. Our approach allows a modular integration of\nSAT solvers and DNN verifiers, and we implement it on top of an interface\ndesigned for this purpose. The evaluation of our implementation over several\nbenchmarks suggests a 2X--3X improvement over a similar approach, with specific\ncases outperforming the state of the art.",
        "Measurements of the sparsity of galaxy clusters can be used to probe the\ncosmological information encoded in the host dark matter halo profile, and\ninfer constraints on the cosmological model parameters. Key to the success of\nthese analyses is the control of potential sources of systematic uncertainty.\nAs an example, the presence of baryons can alter the cluster sparsity with\nrespect to predictions from N-body simulations. Similarly, a radial dependent\nmass bias, as in the case of masses inferred under the hydrostatic equilibrium\n(HE) hypothesis, can affect sparsity estimates. We examine the imprint of\nbaryonic processes on the sparsity statistics. Then, we investigate the\nrelation between cluster sparsities and gas mass fraction. Finally, we perform\na study of the impact of HE mass bias on sparsity measurements and the\nimplication on cosmological parameter inference analyses. We use catalogues of\nsimulated galaxy clusters from The Three Hundred project and run a comparative\nanalysis of the sparsity of clusters from N-body\/hydro simulations implementing\ndifferent feedback model scenarios. Sparsities which probe the mass profile\nacross a large radial range are affected by the presence of baryons in a way\nthat is particularly sensitive to astrophysical feedback, whereas those probing\nexclusively external cluster regions are less affected. In the former case, we\nfind the sparsities to be moderately correlated with measurements of the gas\nfraction in the inner cluster regions. We infer constraints on $S_8$ using\nsynthetic average sparsity measurements generated to evaluate the impact of\nbaryons, selection effects and HE bias. In the case of multiple sparsities\nthese lead to highly bias results. Hence, we calibrate linear bias models that\nenable us to correct for these effects and recover unbiased constraints that\nare significantly tighter than those inferred from single sparsity analyses.",
        "Recent advances in large language models have demonstrated remarkable\nreasoning capabilities through Chain of Thought (CoT) prompting, but often at\nthe cost of excessive verbosity in their intermediate outputs, which increases\ncomputational overhead. We introduce Sketch-of-Thought (SoT), a novel prompting\nframework that combines cognitive-inspired reasoning paradigms with linguistic\nconstraints to minimize token usage while preserving reasoning accuracy. SoT is\ndesigned as a flexible framework that can incorporate any custom reasoning\nparadigms based on cognitive science, and we instantiate it with three such\nparadigms - Conceptual Chaining, Chunked Symbolism, and Expert Lexicons - each\ntailored to different reasoning tasks and selected dynamically via a\nlightweight routing model. Through comprehensive evaluation across 15 reasoning\ndatasets with multiple languages and multimodal scenarios, we demonstrate that\nSoT achieves token reductions of 76% with negligible accuracy impact. In\ncertain domains like mathematical and multi-hop reasoning, it even improves\naccuracy while using significantly fewer tokens. Our code is publicly\navailable: https:\/\/www.github.com\/SimonAytes\/SoT.",
        "Transformer-based language models have recently been at the forefront of\nactive research in text generation. However, these models' advances come at the\nprice of prohibitive training costs, with parameter counts in the billions and\ncompute requirements measured in petaflop\/s-decades. In this paper, we\ninvestigate transformer-based architectures for improving model performance in\na low-data regime by selectively replacing attention layers with feed-forward\nand quasi-recurrent neural network layers. We test these architectures on the\nstandard Enwik8 and Wikitext-103 corpora. Our results show that our reduced\narchitectures outperform existing models with a comparable number of\nparameters, and obtain comparable performance to larger models while\nsignificantly reducing the number of parameters.",
        "Semi-supervised graph anomaly detection (GAD) has recently received\nincreasing attention, which aims to distinguish anomalous patterns from graphs\nunder the guidance of a moderate amount of labeled data and a large volume of\nunlabeled data. Although these proposed semi-supervised GAD methods have\nachieved great success, their superior performance will be seriously degraded\nwhen the provided labels are extremely limited due to some unpredictable\nfactors. Besides, the existing methods primarily focus on anomaly detection in\nstatic graphs, and little effort was paid to consider the continuous evolution\ncharacteristic of graphs over time (dynamic graphs). To address these\nchallenges, we propose a novel GAD framework (EL$^{2}$-DGAD) to tackle anomaly\ndetection problem in dynamic graphs with extremely limited labels.\nSpecifically, a transformer-based graph encoder model is designed to more\neffectively preserve evolving graph structures beyond the local neighborhood.\nThen, we incorporate an ego-context hypersphere classification loss to classify\ntemporal interactions according to their structure and temporal neighborhoods\nwhile ensuring the normal samples are mapped compactly against anomalous data.\nFinally, the above loss is further augmented with an ego-context contrasting\nmodule which utilizes unlabeled data to enhance model generalization. Extensive\nexperiments on four datasets and three label rates demonstrate the\neffectiveness of the proposed method in comparison to the existing GAD methods.",
        "Exploiting the first measurements of the same ion species in O+O collisons at\nRHIC and LHC, we propose an experimentally accessible observable to distinguish\nwhether collective behaviour builds up through a hydrodynamic expansion of a\nstrongly interacting QGP or through few rescatterings in a non-equilibrated\ndilute medium. Our procedure allows to disentangle the effects of the initial\nstate geometry and the dynamical response mechanism on the total resulting\nanisotropic flow. We validate the ability of our proposed observable to\ndiscriminate between systems with different interaction rates using results\nfrom event-by-event simulations in kinetic theory in the Relaxation Time\nApproximation (RTA). As a proof of concept, we extract the degree of\nhydrodynamization for Pb+Pb collisions at LHC from experimental data.",
        "Holographic multimode fibre endoscopes have recently shown their ability to\nunveil and monitor deep brain structures with sub-micrometre resolution,\nestablishing themselves as a minimally-invasive technology with promising\napplications in neurobiology. In this approach, holographic control of the\ninput light field entering the multimode fibres is achieved by means of\nwavefront shaping, usually treating the fibre as a complex medium. In contrast\nto other unpredictable and highly scattering complex media, multimode fibres\nfeature symmetries and strong correlations between their input and output\nfields. Both step-index and graded-index multimode fibres offer a specific set\nof such correlations which, when appropriately leveraged, enable generating\nhigh-quality focused pulses with minimal intermodal dispersion. With this, we\nfunnelled pulsed super-resolution STED microscopy with time-gated detection\nthrough a custom multimode fibre probe, combining the correlations of both\nmultimode fibre types. We demonstrate resolution improvements over 3-times\nbeyond the diffraction limit and showcase its applicability in bioimaging. This\nwork provides not only a solution for delivering short pulses through\nstep-index multimode fibre segments but also marks a step towards bringing\nadvanced super-resolution imaging techniques with virtually no depth\nlimitations."
      ]
    }
  },
  {
    "id":2412.00225,
    "research_type":"applied",
    "start_id":"b17",
    "start_title":"Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations",
    "start_abstract":"We introduce physics-informed neural networks \u2013 neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge\u2013Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction\u2013diffusion systems, and the propagation of nonlinear shallow-water waves.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b14"
      ],
      "title":[
        "The Burgers equation"
      ],
      "abstract":[
        "The Burgers equation is a simple equation to understand the main properties of the Navier-Stokes equations. In this one-dimensional equation the pressure is neglected but the effects of the nonlinear and viscous terms remain, hence as in the Navier-Stokes equations a Reynolds number can be defined. This number expresses the ratio between the advective and the viscous contribution in a flow. The present book deals with flows at high Reynolds numbers where the nonlinear terms play a fundamental role, and the physics is more complicated than that when the viscous term dominates. The simulation of the flow evolution then necessitates the use of accurate and robust numerical methods. In 3D turbulent flows, where the number of degrees of freedom is greater than in high Re laminar flows, to get solutions it is necessary to introduce some sort of closure to account for the impossibility to resolve the small scales. Before applying any new idea about numerical methods to 3D flows, the good sense suggests to find the simplest equation to test these ideas. This consideration explains why the Burgers equation was often used to check new numerical methods or closure for turbulent flows."
      ],
      "categories":[
        "math.AP"
      ]
    },
    "list":{
      "title":[
        "Incompressible and vanishing vertical viscosity limit for the\n  compressible Navier-Stokes system with Dirichlet boundary conditions",
        "Regularity properties of certain convolution operators in H\\\"{o}lder\n  spaces",
        "Well-Posedness of Contact Discontinuity Solutions and Vanishing Pressure\n  Limit for the Aw-Rascle Traffic Flow Model",
        "Structural stability of cylindrical supersonic solutions to the steady\n  Euler-Poisson system",
        "Derivation of a Multiscale Ferrofluid Model: Superparamagnetic Behavior\n  due to Fast Spin Flip",
        "Asymptotical Behavior of Global Solutions of the Navier-Stokes-Korteweg\n  Equations with Respect to Capillarity Number at Infinity",
        "Uniqueness of Dirac-harmonic maps from a compact surface with boundary",
        "A semi-adaptive finite difference method for simulating two-sided\n  fractional convection-diffusion quenching problems",
        "Reconstruction of 1-D evolution equations and their initial data from\n  one passive measurement",
        "Spectral multipliers on two-step stratified Lie groups with degenerate\n  group structure",
        "Well-posedness of the nonhomogeneous incompressible\n  Navier-Stokes\/Allen-Cahn system",
        "On isolated singularities of the conformal Gaussian curvature equation\n  and $Q$-curvature equation",
        "The regularity of electronic wave functions in Barron spaces",
        "On the test properties of the Frobenius endomorphism",
        "Twinning in ferromagnetic Heusler Rh2MnSb epitaxial thin films",
        "Cepheids in spectroscopic binary systems -- current status and recent\n  discoveries",
        "Efficient Diffusion Posterior Sampling for Noisy Inverse Problems",
        "Biglobal resolvent analysis of separated flow over a NACA0012 airfoil",
        "Efficient $d$-ary Cuckoo Hashing at High Load Factors by Bubbling Up",
        "Liquidity Competition Between Brokers and an Informed Trader",
        "A plastic damage model with mixed isotropic-kinematic hardening for\n  low-cycle fatigue in 7020 aluminum",
        "A Fast Decoding Algorithm for Generalized Reed-Solomon Codes and\n  Alternant Codes",
        "Dirac-type condition for Hamilton-generated graphs",
        "A free boundary approach to the quasistatic evolution of debonding\n  models",
        "The Lagrangian approach to the compressible primitive equations",
        "Chemistry in the Galactic Center",
        "On complex eigenvalues of a real nonsymmetric matrix",
        "Robust triple-q magnetic order with trainable spin vorticity in\n  Na$_2$Co$_2$TeO$_6$"
      ],
      "abstract":[
        "In this paper, we show the incompressible and vanishing vertical viscosity\nlimits for the strong solutions to the isentropic compressible Navier-Stokes\nsystem with anistropic dissipation, in a domain with Dirichlet boundary\nconditions in the general setting of ill-prepared initial data. We establish\nthe uniform regularity estimates with respect to the Mach number $\\epsilon$ and\nthe vertical viscosity $\\nu$ so that the solution exists on a uniform time\ninterval $[0,T_0]$ independent of these parameters. The key steps toward this\ngoal are the careful construction of the approximate solution in the presence\nof both fast oscillations and two kinds of boundary layers together with the\nstability analysis of the remainder. In the process, it is also shown that the\nsolutions of the compressible systems converge to those of the incompressible\nsystem with only horizontal dissipation, after removing the fast waves whose\nhorizontal derivative is bounded in $L_{T_0}^2L_x^2$ by $\\min\\{1,\n(\\epsilon\/\\nu)^{\\frac14}\\}.$",
        "The aim of this paper is to prove a theorem of C.~Miranda on the H\\\"older\nregularity of convolution operators acting on the boundary of an open set in\nthe limiting case in which the open set is of class $C^{1,1}$ and the densities\nare of class $C^{0,1}$. The convolution operators that we consider are\ngeneralizations of those that are associated to layer potential operators,\nwhich are a useful tool for the analysis of boundary value problems.",
        "This paper investigates the well-posedness of contact discontinuity solutions\nand the vanishing pressure limit for the Aw-Rascle traffic flow model with\ngeneral pressure functions. The well-posedness problem is formulated as a free\nboundary problem, where initial discontinuities propagate along linearly\ndegenerate characteristics. By mollifying initial data in Lagrangian\ncoordinates, the problem is reduced to analyzing the limit of classical\nsolutions. To avoid equation degeneracy due to vacuum states, we establish a\nuniform lower bound for density by categorizing discontinuity types at singular\npoints. Using level sets for velocity derivatives, we derive uniform estimates\nfor density and velocity gradients. Through equivalence of coordinate\ntransformations, the well-posedness of contact discontinuity solutions in\noriginal coordinates is rigorously proven. Results show that compressive\ninitial data induce finite-time singularity formation, while rarefactive\ninitial data ensure global existence. Furthermore, we demonstrate that\nsolutions of the Aw-Rascle model converge to those of the pressureless model\nunder vanishing pressure, with matching convergence rates for velocity and\ncharacteristic triangles. By enhancing regularity in non-discontinuous regions,\nwe prove convergence of the Aw-Rascle model's blow-up time to the pressureless\nmodel's blow-up time. Finally, analogous results are extended to the Chaplygin\ngas pressure case.",
        "This paper concerns the structural stability of smooth cylindrically\nsymmetric supersonic Euler-Poisson flows in nozzles. Both three-dimensional and\naxisymmetric perturbations are considered. On one hand, we establish the\nexistence and uniqueness of three-dimensional smooth supersonic solutions to\nthe potential flow model of the steady Euler-Poisson system. On the other hand,\nthe existence and uniqueness of smooth supersonic flows with nonzero vorticity\nto the steady axisymmetric Euler-Poisson system are proved. The problem is\nreduced to solve a nonlinear boundary value problem for a hyperbolic-elliptic\nmixed system. One of the key ingredients in the analysis of three-dimensional\nsupersonic irrotational flows is the well-posedness theory for a linear second\norder hyperbolic-elliptic coupled system, which is achieved by using the\nmultiplier method and the reflection technique to derive the energy estimates.\nFor smooth axisymmetric supersonic flows with nonzero vorticity, the\ndeformation-curl-Poisson decomposition is utilized to reformulate the steady\naxisymmetric Euler-Poisson system as a deformation-curl-Poisson system together\nwith several transport equations, so that one can design a two-layer iteration\nscheme to establish the nonlinear structural stability of the background\nsupersonic flow within the class of axisymmetric rotational flows.",
        "We consider a microscopic model of $N$ magnetic nanoparticles in a Stokes\nflow. We assume that the temperature is above the critical N\\'eel temperature\nsuch that the particles' magnetizations undergo random flip with rate\n$1\/\\varepsilon$. The microscopic system is the modeled through a piecewise\ndeterministic Markov jump process. We show that for large $N$, small particle\nvolume fraction and small $\\varepsilon$, the system can be effectively\ndescribed by a multiscale model.",
        "Vanishing capillarity in the Navier-Stokes-Korteweg (NSK) equations has been\nwidely investigated, in particular, it is well-known that the NSK equations\nconverge to the Navier-Stokes (NS) equations by vanishing capillarity number.\nTo our best knowledge, this paper first investigates the behavior of large\ncapillary number, denoted by $\\kappa^2$, for the global(-in-time) strong\nsolutions with small initial perturbations of the three-dimensional (3D) NSK\nequations in a slab domain with Navier(-slip) boundary condition. Under the\nwell-prepared initial data, we can construct a family of global strong\nsolutions of the 3D incompressible NSK equations with respect to $\\kappa>0$,\nwhere the solutions converge to a unique solution of 2D incompressible NS-like\nequations as $\\kappa$ goes to infinity.",
        "As a commutative version of the supersymmetric nonlinear sigma model,\nDirac-harmonic maps from Riemann surfaces were introduced fifteen years ago.\nThey are critical points of an unbounded conformally invariant functional\ninvolving two fields, a map from a Riemann surface into a Riemannian manifold\nand a section of a Dirac bundle which is the usual spinor bundle twisted with\nthe pull-back of the tangent bundle of the target by the map. As solutions to a\ncoupled nonlinear elliptic system, the existence and regularity theory of\nDirac-harmonic maps has already received much attention, while the general\nuniqueness theory has not been established yet. For uncoupled Dirac-harmonic\nmaps, the map components are harmonic maps. Since the uniqueness theory of\nharmonic maps from a compact surface with boundary is known, it is sufficient\nto consider the uniqueness of the spinor components, which are solutions to the\ncorresponding boundary value problems for a nonlinear Dirac equation. In\nparticular, when the map components belong to $W^{1,p}$ with $p>2$, the spinor\ncomponents are uniquely determined by boundary values and map components. For\ncoupled Dirac-harmonic maps, the map components are not harmonic maps. So the\nuniqueness problem is more difficult to solve. In this paper, we study the\nuniqueness problem on a compact surface with boundary. More precisely, we prove\nthe energy convexity for weakly Dirac-harmonic maps from the unit disk with\nsmall energy. This yields the first uniqueness result about Dirac-harmonic maps\nfrom a surface conformal to the unit disk with small energy and arbitrary\nboundary values.",
        "This paper investigates quenching solutions of an one-dimensional, two-sided\nRiemann-Liouville fractional order convection-diffusion problem. Fractional\norder spatial derivatives are discretized using weighted averaging\napproximations in conjunction with standard and shifted Gr\\\"{u}nwald formulas.\nThe advective term is handled utilizing a straightforward Euler formula,\nresulting in a semi-discretized system of nonlinear ordinary differential\nequations. The conservativeness of the proposed scheme is rigorously proved and\nvalidated through simulation experiments. The study is further advanced to a\nfully discretized, semi-adaptive finite difference method. Detailed analysis is\nimplemented for the monotonicity, positivity and stability of the scheme.\nInvestigations are carried out to assess the potential impacts of the\nfractional order on quenching location, quenching time, and critical length.\nThe computational results are thoroughly discussed and analyzed, providing a\nmore comprehensive understanding of the quenching phenomena modeled through\ntwo-sided fractional order convection-diffusion problems.",
        "We study formally determined inverse problems with passive measurements for\none dimensional evolution equations where the goal is to simultaneously\ndetermine both the initial data as well as the variable coefficients in such an\nequation from the measurement of its solution at a fixed spatial point for a\ncertain amount of time. This can be considered as a one-dimensional model of\nwidely open inverse problems in photo-acoustic and thermo-acoustic tomography.\nWe provide global uniqueness results for wave and heat equations stated on\nbounded or unbounded spatial intervals. Contrary to all previous related\nresults on the subject, we do not impose any genericity assumptions on the\ncoefficients or initial data. Our proofs are based on creating suitable links\nto the well understood spectral theory for 1D Schr\\\"odinger operators. In\nparticular, in the more challenging case of a bounded spatial domain, our proof\nfor the inverse problem partly relies on the following two key ingredients,\nnamely (i) Paley-Wiener type theorems due to Kahane \\cite{Kahane1957SurLF} and\nRemling \\cite{Remling2002SchrdingerOA} that together provide a quantifiable\nlink between support of a compactly supported function and the upper density of\nits vanishing Schr\\\"odinger spectral modes and (ii) a result of Gesztesy and\nSimon \\cite{Gesztesy1999InverseSA} on partial data inverse spectral problems\nfor reconstructing an unknown potential in a 1D Schr\\\"odinger operator from the\nknowledge of only a fraction of its spectrum.",
        "Let $L$ be a sub-Laplacian on a two-step stratified Lie group $G$ of\ntopological dimension $d$. We prove new $L^p$-spectral multiplier estimates\nunder the sharp regularity condition $s>d\\left|1\/p-1\/2\\right|$ in settings\nwhere the group structure of $G$ is degenerate, extending previously known\nresults for the non-degenerate case. Our results include variants of the free\ntwo-step nilpotent group on three generators and Heisenberg-Reiter groups. The\nproof combines restriction type estimates with a detailed analysis of the\nsub-Riemannian geometry of $G$. A key novelty of our approach is the use of a\nrefined spectral decomposition into caps on the unit sphere in the center of\nthe Lie group.",
        "In this paper, we investigate a system coupled by nonhomogeneous\nincompressible Navier-Stokes equations and Allen-Cahn equations describing a\ndiffuse interface for two-phase flow of viscous fluids with different densities\nin a bounded domain $\\Omega\\subset\\mathbb R^d (d=2, 3)$. The mobility is\nallowed to depend on phase variable but non-degenerate. We first prove the\nexistence of global weak solutions to the initial boundary value problem in 2D\nand 3D cases. Then we obtain the existence of local in time strong solutions in\n3D case as well as the global strong solutions in 2D case. Moreover, by\nimposing smallness conditions on the initial data, the 3D local in time strong\nsolution is extended globally, with an exponential decay rate for\nperturbations. At last, we show the weak-strong uniqueness.",
        "In this paper, we study the isolated singularities of the conformal Gaussian\ncurvature equation \\[\n  -\\Delta u = K(x) e^{u} \\quad ~ in ~ B_{1} \\setminus \\{ 0 \\}, \\] where $B_1\n\\setminus \\{ 0 \\} \\subset \\mathbb{R}^2$ is the punctured unit disc. Under the\nassumption that the Gaussian curvature $K \\in L^\\infty(B_1)$ is nonnegative, we\nestablish the asymptotic behavior of solutions near the singularity. When $K\n\\equiv 1$, a similar result has been obtained by Chou and Wan (Pacific J. Math.\n1994) using the method of complex analysis. Our proof is entirely based on the\nPDE method and applies to the general Gaussian curvature $K(x)$. Furthermore,\nour approach is also available for characterizing isolated singularities of the\nconformal $Q$-curvature equation $(-\\Delta)^{\\frac{n}{2}} u = K(x) e^{u}$ in\nany dimension $n\\geq 3$. This equation arises from the prescribing\n$Q$-curvature problem.",
        "The electronic Schr\\\"odinger equation describes the motion of $N$ electrons\nunder Coulomb interaction forces in a field of clamped nuclei. It is proved\nthat its solutions for eigenvalues below the essential spectrum lie in the\nspectral Barron spaces $\\mathcal{B}^s(\\mathbb{R}^{3N})$ for $s<1$. The example\nof the hydrogen ground state shows that this result cannot be improved.",
        "In this paper, we prove two theorems concerning the test properties of the\nFrobenius endomorphism over commutative Noetherian local rings of prime\ncharacteristic $p$. Our first theorem generalizes a result of Funk-Marley on\nthe vanishing of Ext and Tor modules, while our second theorem generalizes one\nof our previous results on maximal Cohen-Macaulay tensor products. In these\nearlier results, we replace $^{e}R$ with a more general module $^{e}M$, where\n$R$ is a Cohen-Macaulay ring, $M$ is a Cohen-Macaulay $R$-module with full\nsupport, and $^{e}M$ is the module viewed as an $R$-module via the $e$-th\niteration of the Frobenius endomorphism. We also provide examples and present\napplications of our results, yielding new characterizations of the regularity\nof local rings.",
        "Epitaxially grown full Heusler alloy of Rh2MnSb thin films were prepared for\nthe first time using DC magnetron sputtering. The films were deposited on MgO\n[001] substrates with a deposition temperature of 600{\\deg}C, 700{\\deg}C, and\n800{\\deg}C. We report the structural, morphological, optical, magneto-optical,\nand magnetic properties of the films with a 200 nm nominal thickness. The\ngrown-at-600{\\deg}C film was close to stoichiometric and exhibited L21 ordering\ntypical for Heusler alloys. The single-phase Rh2MnSb film had a tetragonal\nstructure with lattice parameters close to the bulk material. X-ray\nphotoelectron spectroscopy revealed the metallic character of the film free\nfrom contamination. The tetragonal films exhibited discernible regular twinning\nwith the majority of twin domains with the c-axis perpendicular to the surface\ndue to a substrate constraint. The twin formation was studied by atomic force\nand transmission electron microscopy and by X-ray diffraction. Magnetic\nmeasurements showed TC of about 220-275 K and saturation magnetization of about\n55 emu\/g, close to the bulk material. Magneto-optical Kerr effect measurements\nof the film prepared at 600 {\\deg}C affirmed paramagnetic behavior at room\ntemperature and suggested the half-metallic behavior. The observed properties\nhighlight the potential for further investigations of Rh2MnSb's thin films,\nfocusing on compositional and structural control.",
        "We present a summary of the current knowledge about Cepheids in binary\nsystems. We focus on the most recent findings and discoveries, such as the\nhighly increasing number of confirmed and candidate spectroscopic binary\nCepheids and the progress in determining their physical parameters. This\nincludes new and newly analyzed binary Cepheids in the Milky Way and Magellanic\nClouds. We will provide an update on the project to increase the number of the\nmost valuable Cepheids in double-lined binary (SB2) systems from six to more\nthan 100. To date, we have confirmed 60 SB2 systems, including detecting a\nsignificant orbital motion for 37. We identified systems with orbital periods\nup to five times shorter than the shortest period reported before and systems\nwith mass ratios significantly different from unity (suggesting past binary\ninteractions, including merger events). Both features are essential to\nunderstanding how multiplicity affects the formation and destruction of Cepheid\nprogenitors and how this influences global Cepheid properties. We will also\npresent nine new systems composed of two Cepheids. Only one such double Cepheid\nsystem was known before.",
        "The pretrained diffusion model as a strong prior has been leveraged to\naddress inverse problems in a zero-shot manner without task-specific\nretraining. Different from the unconditional generation, the measurement-guided\ngeneration requires estimating the expectation of clean image given the current\nimage and the measurement. With the theoretical expectation expression, the\ncrucial task of solving inverse problems is to estimate the noisy likelihood\nfunction at the intermediate image sample. Using the Tweedie's formula and the\nknown noise model, the existing diffusion posterior sampling methods perform\ngradient descent step with backpropagation through the pretrained diffusion\nmodel. To alleviate the costly computation and intensive memory consumption of\nthe backpropagation, we propose an alternative maximum-a-posteriori (MAP)-based\nsurrogate estimator to the expectation. With this approach and further density\napproximation, the MAP estimator for linear inverse problem is the solution to\na traditional regularized optimization, of which the loss comprises of data\nfidelity term and the diffusion model related prior term. Integrating the MAP\nestimator into a general denoising diffusion implicit model (DDIM)-like\nsampler, we achieve the general solving framework for inverse problems. Our\napproach highly resembles the existing $\\Pi$GDM without the manifold projection\noperation of the gradient descent direction. The developed method is also\nextended to nonlinear JPEG decompression. The performance of the proposed\nposterior sampling is validated across a series of inverse problems, where both\nVP and VE SDE-based pretrained diffusion models are taken into consideration.",
        "The effects of Reynolds number across $Re=1000$, $2500$, $5000$, and $10000$\non separated flow over a two-dimensional NACA0012 airfoil at an angle of attack\nof $\\alpha=14^\\circ$ are investigated through the biglobal resolvent analysis.\nWe identify modal structures and energy amplifications over a range of\nfrequency, spanwise wavenumber, and discount parameter, providing insights\nacross various timescales. Using temporal discounting, we find that the shear\nlayer dynamics dominates over short time horizons, while the wake dynamics\nbecomes the primary amplification mechanism over long time horizons. Spanwise\neffects also appear over long time horizon, sustained by low frequencies. At a\nfixed timescale, we investigate the influence of Reynolds number on response\nand forcing mode structures, as well as the energy gain over different\nfrequencies. Across all Reynolds numbers, the response modes shift from\nwake-dominated structures at low frequencies to shear layer-dominated\nstructures at higher frequencies. The frequency at which the dominant mechanism\nchanges is independent of the Reynolds number. The response mode structures\nshow similarities across different Reynolds numbers, with local streamwise\nwavelengths only depending on frequency. Comparisons at a different angle of\nattack ($\\alpha=9^\\circ$) show that the transition from wake to shear layer\ndynamics with increasing frequency only occurs if the unsteady flow is\nthree-dimensional. We also study the dominant frequencies associated with wake\nand shear layer dynamics across the angles of attack and Reynolds numbers, and\npresent the characteristic scaling for each mechanism.",
        "A $d$-ary cuckoo hash table is an open-addressed hash table that stores each\nkey $x$ in one of $d$ random positions $h_1(x), h_2(x), \\ldots, h_d(x)$. In the\noffline setting, where all items are given and keys need only be matched to\nlocations, it is possible to support a load factor of $1 - \\epsilon$ while\nusing $d = \\lceil \\ln \\epsilon^{-1} + o(1) \\rceil$ hashes. The online setting,\nwhere keys are moved as new keys arrive sequentially, has the additional\nchallenge of the time to insert new keys, and it has not been known whether one\ncan use $d = O(\\ln \\epsilon^{-1})$ hashes to support $\\poly(\\epsilon^{-1})$\nexpected-time insertions.\n  In this paper, we introduce bubble-up cuckoo hashing, an implementation of\n$d$-ary cuckoo hashing that achieves all of the following properties\nsimultaneously:\n  (1) uses $d = \\lceil \\ln \\epsilon^{-1} + \\alpha \\rceil$ hash locations per\nitem for an arbitrarily small positive constant $\\alpha$.\n  (2) achieves expected insertion time $O(\\delta^{-1})$ for any insertion\ntaking place at load factor $1 - \\delta \\le 1 - \\epsilon$.\n  (3) achieves expected positive query time $O(1)$, independent of $d$ and\n$\\epsilon$.\n  The first two properties give an essentially optimal value of $d$ without\ncompromising insertion time. The third property is interesting even in the\noffline setting: it says that, even though \\emph{negative} queries must take\ntime $d$, positive queries can actually be implemented in $O(1)$ expected time,\neven when $d$ is large.",
        "We study a multi-agent setting in which brokers transact with an informed\ntrader. Through a sequential Stackelberg-type game, brokers manage trading\ncosts and adverse selection with an informed trader. In particular, supplying\nliquidity to the informed traders allows the brokers to speculate based on the\nflow information. They simultaneously attempt to minimize inventory risk and\ntrading costs with the lit market based on the informed order flow, also known\nas the internalization-externalization strategy. We solve in closed form for\nthe trading strategy that the informed trader uses with each broker and propose\na system of equations which classify the equilibrium strategies of the brokers.\nBy solving these equations numerically we may study the resulting strategies in\nequilibrium. Finally, we formulate a competitive game between brokers in order\nto determine the liquidity prices subject to precommitment supplied to the\ninformed trader and provide a numerical example in which the resulting\nequilibrium is not Pareto efficient.",
        "The paper at hand presents an in-depth investigation into the fatigue\nbehavior of the high-strength aluminum alloy EN AW-7020 T6 using both\nexperimental and numerical approaches. Two types of specimens are investigated:\na dog-bone specimen subjected to cyclic loading in a symmetric\nstrain-controlled regime, and a compact tension specimen subjected to repeated\nloading and unloading, which leads to damage growth from the notch tip.\nExperimental data from these tests are used to identify the different phases of\nfatigue. Subsequently, a plastic-damage model is developed, incorporating J2\nplasticity with Chaboche-type mixed isotropic-kinematic hardening. A detailed\ninvestigation reveals that the Chaboche model must be blended with a suitable\nisotropic hardening and combined with a proper damage growth model to\naccurately describe cyclic fatigue including large plastic strains up to\nfailure. Multiple back-stress components with independent properties are\nsuperimposed, and exponential isotropic hardening with saturation effects is\nintroduced to improve alignment with experimental results. For damage,\ndifferent stress splits are tested, with the deviatoric\/volumetric split\nproving successful in reproducing the desired degradation in peak stress and\nstiffness. A nonlinear activation function is introduced to ensure smooth\ntransitions between tension and compression. Two damage indices, one for the\ndeviatoric part and one for the volumetric part, are defined, each of which is\ngoverned by a distinct trilinear damage growth function. The governing\ndifferential equation of the problem is regularized by higher-order gradient\nterms to address the ill-posedness induced by softening. Finally, the\nplasticity model is calibrated using finite element simulations of the dog-bone\ntest and subsequently applied to the cyclic loading of the compact tension\nspecimen.",
        "In this paper, it is shown that the syndromes of generalized Reed-Solomon\n(GRS) codes and alternant codes can be characterized in terms of inverse fast\nFourier transform, regardless of code definitions. Then a fast decoding\nalgorithm is proposed, which has a computational complexity of $O(n\\log(n-k) +\n(n-k)\\log^2(n-k))$ for all $(n,k)$ GRS codes and $(n,k)$ alternant codes.\nParticularly, this provides a new decoding method for Goppa codes, which is an\nimportant subclass of alternant codes. When decoding the binary Goppa code with\nlength $8192$ and correction capability $128$, the new algorithm is nearly 10\ntimes faster than traditional methods. The decoding algorithm is suitable for\nthe McEliece cryptosystem, which is a candidate for post-quantum cryptography\ntechniques.",
        "The cycle space $\\mathcal{C}(G)$ of a graph $G$ is defined as the linear\nspace spanned by all cycles in $G$. For an integer $k\\ge 3$, let $\\mathcal{C}_k\n(G)$ denote the subspace of $\\mathcal{C}(G)$ generated by the cycles of length\nexactly $k$. A graph $G$ on $n$ vertices is called Hamilton-generated if\n$\\mathcal{C}_n (G) = \\mathcal{C}(G)$, meaning every cycle in $G$ is a symmetric\ndifference of some Hamilton cycles of $G$. %A necessary condition for this\nproperty is that $n$ must be odd. Heinig (European J. Combin., 2014) showed\nthat for any $\\sigma >0$ and sufficiently large odd $n$, every $n$-vertex graph\nwith minimum degree $(1+ \\sigma)n\/2$ is Hamilton-generated. He further posed\nthe question that whether the minimum degree requirement could be lowered to\nthe Dirac threshold $n\/2$. Recent progress by Christoph, Nenadov, and\nPetrova~(arXiv:2402.01447) reduced the minimum degree condition to $n\/2 + C$\nfor some large constant $C$. In this paper, we resolve Heinig's problem\ncompletely by proving that for sufficiently large odd $n$, every\nHamilton-connected graph $G$ on $n$ vertices with minimum degree at least\n$(n-1)\/2$ is Hamilton-generated. Moreover, this result is tight for the minimum\ndegree and the Hamilton-connected condition. The proof relies on the\nparity-switcher technique introduced by Christoph, et al in their recent work,\nas well as a classification lemma that strengthens a previous result by\nKrivelevich, Lee, and Sudakov~(Trans. Amer. Math. Soc., 2014).",
        "The mechanical process of progressively debonding an adhesive membrane from a\nsubstrate is described as a quasistatic variational evolution of sets and\nherein investigated. Existence of energetic solutions, based on global\nminimisers of a suitable functional together with an energy balance, is\nobtained within the natural class of open sets, improving and simplifying\nprevious results known in literature. The proposed approach relies on an\nequivalent reformulation of the model in terms of the celebrated one-phase\nBernoulli free boundary problem. This point of view allows performing the\nMinimizing Movements scheme in spaces of functions instead of the more\ncomplicated framework of sets. Nevertheless, in order to encompass\nirreversibility of the phenomenon, it remains crucial to keep track of the\ndebonded region at each discrete time-step, thus actually resulting in a\ncoupled algorithm.",
        "This article develops the hydrostatic Lagrangian approach to the compressible\nprimitive equations. A fundamental aspect in the analysis is the investigation\nof the compressible hydrostatic Lam\\'{e} and Stokes operators. Local strong\nwell-posedness for large data and global strong well-posedness for small data\nare established under various assumptions on the pressure law, both in the\npresence and absence of gravity.",
        "Gas and dust in the Galactic Center are subjected to energetic processing by\nintense UV radiation fields, widespread shocks, enhanced rates of cosmic-rays\nand X-rays, and strong magnetic fields. The Giant Molecular Clouds in the\nGalactic Center present a rich chemistry in a wide variety of chemical\ncompounds, some of which are prebiotic. We have conducted unbiased,\nultrasensitive and broadband spectral surveys toward the G+0.693-0.027\nmolecular cloud located in the Galactic Center, which have yielded the\ndiscovery of new complex organic molecules proposed as precursors of the\n\"building blocks\" of life. I will review our current understanding of the\nchemistry in Galactic Center molecular clouds, and summarize the recent\ndetections toward G+0.693-0.027 of key precursors of prebiotic chemistry. All\nthis suggests that the ISM is an important source of prebiotic material that\ncould have contributed to the process of the origin of life on Earth and\nelsewhere in the Universe.",
        "We consider real non-symmetric matrices and their factorisation as a product\nof real symmetric matrices. The number of complex eigenvalues of the original\nmatrix reveals restrictions on such factorisations as we shall prove.",
        "Recent studies suggest that the candidate Kitaev magnet Na$_2$Co$_2$TeO$_6$\npossesses novel triple-$\\mathbf{q}$ magnetic order instead of conventional\nsingle-$\\mathbf{q}$ zigzag order. Here we present dedicated experiments in\nsearch for distinct properties expected of the triple-$\\mathbf{q}$ order,\nnamely, insensitivity of the magnetic domains to weak $C_3$ symmetry-breaking\nfields and fictitious magnetic fields generated by the spin vorticity. In\nstructurally pristine single crystals, we show that $C_3$ symmetry-breaking\nin-plane uniaxial strains do not affect the order's magnetic neutron\ndiffraction signals. We further show that $\\mathbf{c}$-axis propagating light\nexhibits large Faraday rotations in the ordered state due to the spin\nvorticity, the sign of which can be trained via the system's ferrimagnetic\nmoment. These results are in favor of the triple-$\\mathbf{q}$ order in\nNa$_2$Co$_2$TeO$_6$ and reveal its unique emerging behavior."
      ]
    }
  },
  {
    "id":2411.05456,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"MRI segmentation of the human brain: challenges, methods, and applications",
    "start_abstract":"Image segmentation is one of the most important tasks in medical image analysis and is often the first and the most critical step in many clinical applications. In brain MRI analysis, image segmentation is commonly used for measuring and visualizing the brain\u2019s anatomical structures, for analyzing brain changes, for delineating pathological regions, and for surgical planning and image-guided interventions. In the last few decades, various segmentation techniques of different accuracy and degree of complexity have been developed and reported in the literature. In this paper we review the most popular methods commonly used for brain MRI segmentation. We highlight differences between them and discuss their capabilities, advantages, and limitations. To address the complexity and challenges of the brain MRI segmentation problem, we first introduce the basic concepts of image segmentation. Then, we explain different MRI preprocessing steps including image registration, bias field correction, and removal of nonbrain tissue. Finally, after reviewing different brain MRI segmentation methods, we discuss the validation problem in brain MRI segmentation.",
    "start_categories":[
      "q-bio.QM"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b11"
      ],
      "title":[
        "U-net: Convolutional networks for biomedical image segmentation"
      ],
      "abstract":[
        "There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "HintsOfTruth: A Multimodal Checkworthiness Detection Dataset with Real\n  and Synthetic Claims",
        "Extending the design space of ontologization practices: Using bCLEARer\n  as an example",
        "Towards AI-assisted Academic Writing",
        "Systematic Abductive Reasoning via Diverse Relation Representations in\n  Vector-symbolic Architecture",
        "SOP-Agent: Empower General Purpose AI Agent with Domain-Specific SOPs",
        "Representation Learning to Advance Multi-institutional Studies with\n  Electronic Health Record Data",
        "ProRCA: A Causal Python Package for Actionable Root Cause Analysis in\n  Real-world Business Scenarios",
        "Unifying and Optimizing Data Values for Selection via\n  Sequential-Decision-Making",
        "RM-PoT: Reformulating Mathematical Problems and Solving via Program of\n  Thoughts",
        "MindMem: Multimodal for Predicting Advertisement Memorability Using LLMs\n  and Deep Learning",
        "An Expectation-Maximization Algorithm-based Autoregressive Model for the\n  Fuzzy Job Shop Scheduling Problem",
        "AgentDAM: Privacy Leakage Evaluation for Autonomous Web Agents",
        "PhysReason: A Comprehensive Benchmark towards Physics-Based Reasoning",
        "Robust Evidence for Declining Disruptiveness: Assessing the Role of\n  Zero-Backward-Citation Works",
        "Coresets for Robust Clustering via Black-box Reductions to Vanilla Case",
        "Thermal Radiation Force and Torque on Moving Nanostructures with\n  Anisotropic Optical Response",
        "Efficient Transformed Gaussian Process State-Space Models for\n  Non-Stationary High-Dimensional Dynamical Systems",
        "Wheel-GINS: A GNSS\/INS Integrated Navigation System with a Wheel-mounted\n  IMU",
        "Electron-Chiral Phonon Coupling, Crystal Angular Momentum, and Phonon\n  Chirality",
        "Exploring the Hidden Reasoning Process of Large Language Models by\n  Misleading Them",
        "Characterizing the Burst Error Correction Ability of Quantum Cyclic\n  Codes",
        "Global Existence and Nonlinear Stability of Finite-Energy Solutions of\n  the Compressible Euler-Riesz Equations with Large Initial Data of Spherical\n  Symmetry",
        "Probing the hollowing transition of a shell-shaped BEC with collective\n  excitation",
        "Gender Dynamics in Software Engineering: Insights from Research on\n  Concurrency Bug Reproduction",
        "CognitiveDrone: A VLA Model and Evaluation Benchmark for Real-Time\n  Cognitive Task Solving and Reasoning in UAVs",
        "On connected subgraph arrangements",
        "Scale-wise Distillation of Diffusion Models",
        "Revealed Social Networks"
      ],
      "abstract":[
        "Misinformation can be countered with fact-checking, but the process is costly\nand slow. Identifying checkworthy claims is the first step, where automation\ncan help scale fact-checkers' efforts. However, detection methods struggle with\ncontent that is 1) multimodal, 2) from diverse domains, and 3) synthetic. We\nintroduce HintsOfTruth, a public dataset for multimodal checkworthiness\ndetection with $27$K real-world and synthetic image\/claim pairs. The mix of\nreal and synthetic data makes this dataset unique and ideal for benchmarking\ndetection methods. We compare fine-tuned and prompted Large Language Models\n(LLMs). We find that well-configured lightweight text-based encoders perform\ncomparably to multimodal models but the first only focus on identifying\nnon-claim-like content. Multimodal LLMs can be more accurate but come at a\nsignificant computational cost, making them impractical for large-scale\napplications. When faced with synthetic data, multimodal models perform more\nrobustly",
        "Our aim in this paper is to outline how the design space for the\nontologization process is richer than current practice would suggest. We point\nout that engineering processes as well as products need to be designed - and\nidentify some components of the design. We investigate the possibility of\ndesigning a range of radically new practices, providing examples of the new\npractices from our work over the last three decades with an outlier\nmethodology, bCLEARer. We also suggest that setting an evolutionary context for\nontologization helps one to better understand the nature of these new practices\nand provides the conceptual scaffolding that shapes fertile processes. Where\nthis evolutionary perspective positions digitalization (the evolutionary\nemergence of computing technologies) as the latest step in a long evolutionary\ntrail of information transitions. This reframes ontologization as a strategic\ntool for leveraging the emerging opportunities offered by digitalization.",
        "We present components of an AI-assisted academic writing system including\ncitation recommendation and introduction writing. The system recommends\ncitations by considering the user's current document context to provide\nrelevant suggestions. It generates introductions in a structured fashion,\nsituating the contributions of the research relative to prior work. We\ndemonstrate the effectiveness of the components through quantitative\nevaluations. Finally, the paper presents qualitative research exploring how\nresearchers incorporate citations into their writing workflows. Our findings\nindicate that there is demand for precise AI-assisted writing systems and\nsimple, effective methods for meeting those needs.",
        "In abstract visual reasoning, monolithic deep learning models suffer from\nlimited interpretability and generalization, while existing neuro-symbolic\napproaches fall short in capturing the diversity and systematicity of\nattributes and relation representations. To address these challenges, we\npropose a Systematic Abductive Reasoning model with diverse relation\nrepresentations (Rel-SAR) in Vector-symbolic Architecture (VSA) to solve\nRaven's Progressive Matrices (RPM). To derive attribute representations with\nsymbolic reasoning potential, we introduce not only various types of atomic\nvectors that represent numeric, periodic and logical semantics, but also the\nstructured high-dimentional representation (SHDR) for the overall Grid\ncomponent. For systematic reasoning, we propose novel numerical and logical\nrelation functions and perform rule abduction and execution in a unified\nframework that integrates these relation representations. Experimental results\ndemonstrate that Rel-SAR achieves significant improvement on RPM tasks and\nexhibits robust out-of-distribution generalization. Rel-SAR leverages the\nsynergy between HD attribute representations and symbolic reasoning to achieve\nsystematic abductive reasoning with both interpretable and computable\nsemantics.",
        "Despite significant advancements in general-purpose AI agents, several\nchallenges still hinder their practical application in real-world scenarios.\nFirst, the limited planning capabilities of Large Language Models (LLM)\nrestrict AI agents from effectively solving complex tasks that require\nlong-horizon planning. Second, general-purpose AI agents struggle to\nefficiently utilize domain-specific knowledge and human expertise. In this\npaper, we introduce the Standard Operational Procedure-guided Agent\n(SOP-agent), a novel framework for constructing domain-specific agents through\npseudocode-style Standard Operational Procedures (SOPs) written in natural\nlanguage. Formally, we represent a SOP as a decision graph, which is traversed\nto guide the agent in completing tasks specified by the SOP. We conduct\nextensive experiments across tasks in multiple domains, including\ndecision-making, search and reasoning, code generation, data cleaning, and\ngrounded customer service. The SOP-agent demonstrates excellent versatility,\nachieving performance superior to general-purpose agent frameworks and\ncomparable to domain-specific agent systems. Additionally, we introduce the\nGrounded Customer Service Benchmark, the first benchmark designed to evaluate\nthe grounded decision-making capabilities of AI agents in customer service\nscenarios based on SOPs.",
        "The adoption of EHRs has expanded opportunities to leverage data-driven\nalgorithms in clinical care and research. A major bottleneck in effectively\nconducting multi-institutional EHR studies is the data heterogeneity across\nsystems with numerous codes that either do not exist or represent different\nclinical concepts across institutions. The need for data privacy further limits\nthe feasibility of including multi-institutional patient-level data required to\nstudy similarities and differences across patient subgroups. To address these\nchallenges, we developed the GAME algorithm. Tested and validated across 7\ninstitutions and 2 languages, GAME integrates data in several levels: (1) at\nthe institutional level with knowledge graphs to establish relationships\nbetween codes and existing knowledge sources, providing the medical context for\nstandard codes and their relationship to each other; (2) between institutions,\nleveraging language models to determine the relationships between\ninstitution-specific codes with established standard codes; and (3) quantifying\nthe strength of the relationships between codes using a graph attention\nnetwork. Jointly trained embeddings are created using transfer and federated\nlearning to preserve data privacy. In this study, we demonstrate the\napplicability of GAME in selecting relevant features as inputs for AI-driven\nalgorithms in a range of conditions, e.g., heart failure, rheumatoid arthritis.\nWe then highlight the application of GAME harmonized multi-institutional EHR\ndata in a study of Alzheimer's disease outcomes and suicide risk among patients\nwith mental health disorders, without sharing patient-level data outside\nindividual institutions.",
        "Root Cause Analysis (RCA) is becoming ever more critical as modern systems\ngrow in complexity, volume of data, and interdependencies. While traditional\nRCA methods frequently rely on correlation-based or rule-based techniques,\nthese approaches can prove inadequate in highly dynamic, multi-layered\nenvironments. In this paper, we present a pathway-tracing package built on the\nDoWhy causal inference library. Our method integrates conditional anomaly\nscoring, noise-based attribution, and depth-first path exploration to reveal\nmulti-hop causal chains. By systematically tracing entire causal pathways from\nan observed anomaly back to the initial triggers, our approach provides a\ncomprehensive, end-to-end RCA solution. Experimental evaluations with synthetic\nanomaly injections demonstrate the package's ability to accurately isolate\ntriggers and rank root causes by their overall significance.",
        "Data selection has emerged as a crucial downstream application of data\nvaluation. While existing data valuation methods have shown promise in\nselection tasks, the theoretical foundations and full potential of using data\nvalues for selection remain largely unexplored. In this work, we first\ndemonstrate that data values applied for selection can be naturally\nreformulated as a sequential-decision-making problem, where the optimal data\nvalue can be derived through dynamic programming. We show this framework\nunifies and reinterprets existing methods like Data Shapley through the lens of\napproximate dynamic programming, specifically as myopic reward function\napproximations to this sequential problem. Furthermore, we analyze how\nsequential data selection optimality is affected when the ground-truth utility\nfunction exhibits monotonic submodularity with curvature. To address the\ncomputational challenges in obtaining optimal data values, we propose an\nefficient approximation scheme using learned bipartite graphs as surrogate\nutility models, ensuring greedy selection is still optimal when the surrogate\nutility is correctly specified and learned. Extensive experiments demonstrate\nthe effectiveness of our approach across diverse datasets.",
        "Recently, substantial advancements have been made in training language models\nto carry out step-by-step reasoning for solving intricate numerical reasoning\ntasks. Beyond the methods used to solve these problems, the structure and\nformulation of the problems themselves also play a crucial role in determining\nthe performance of large language models. We observe that even small changes in\nthe surface form of mathematical problems can have a profound impact on both\nthe answer distribution and solve rate. This highlights the vulnerability of\nLLMs to surface-level variations, revealing its limited robustness when\nreasoning through complex problems. In this paper, we propose RM-PoT, a\nthree-stage framework that integrates problem reformulation (RM), code-aided\nreasoning (PoT), and domain-aware few-shot learning to address these\nlimitations. Our approach first reformulates the input problem into diverse\nsurface forms to reduce structural bias, then retrieves five semantically\naligned examples from a pre-constructed domain-specific question bank to\nprovide contextual guidance, and finally generates executable Python code for\nprecise computation.",
        "In the competitive landscape of advertising, success hinges on effectively\nnavigating and leveraging complex interactions among consumers, advertisers,\nand advertisement platforms. These multifaceted interactions compel advertisers\nto optimize strategies for modeling consumer behavior, enhancing brand recall,\nand tailoring advertisement content. To address these challenges, we present\nMindMem, a multimodal predictive model for advertisement memorability. By\nintegrating textual, visual, and auditory data, MindMem achieves\nstate-of-the-art performance, with a Spearman's correlation coefficient of\n0.631 on the LAMBDA and 0.731 on the Memento10K dataset, consistently\nsurpassing existing methods. Furthermore, our analysis identified key factors\ninfluencing advertisement memorability, such as video pacing, scene complexity,\nand emotional resonance. Expanding on this, we introduced MindMem-ReAd\n(MindMem-Driven Re-generated Advertisement), which employs Large Language\nModel-based simulations to optimize advertisement content and placement,\nresulting in up to a 74.12% improvement in advertisement memorability. Our\nresults highlight the transformative potential of Artificial Intelligence in\nadvertising, offering advertisers a robust tool to drive engagement, enhance\ncompetitiveness, and maximize impact in a rapidly evolving market.",
        "The fuzzy job shop scheduling problem (FJSSP) emerges as an innovative\nextension to the job shop scheduling problem (JSSP), incorporating a layer of\nuncertainty that aligns the problem more closely with the complexities of\nreal-world manufacturing environments. This improvement increases the\ncomputational complexity of deriving the solution while improving its\napplicability. In the domain of deterministic scheduling, neural combinatorial\noptimization (NCO) has recently demonstrated remarkable efficacy. However, its\napplication to the realm of fuzzy scheduling has been relatively unexplored.\nThis paper aims to bridge this gap by investigating the feasibility of\nemploying neural networks to assimilate and process fuzzy information for the\nresolution of FJSSP, thereby leveraging the advancements in NCO to enhance\nfuzzy scheduling methodologies. To achieve this, we approach the FJSSP as a\ngenerative task and introduce an expectation-maximization algorithm-based\nautoregressive model (EMARM) to address it. During training, our model\nalternates between generating scheduling schemes from given instances (E-step)\nand adjusting the autoregressive model weights based on these generated schemes\n(M-step). This novel methodology effectively navigates around the substantial\nhurdle of obtaining ground-truth labels, which is a prevalent issue in NCO\nframeworks. In testing, the experimental results demonstrate the superior\ncapability of EMARM in addressing the FJSSP, showcasing its effectiveness and\npotential for practical applications in fuzzy scheduling.",
        "LLM-powered AI agents are an emerging frontier with tremendous potential to\nincrease human productivity. However, empowering AI agents to take action on\ntheir user's behalf in day-to-day tasks involves giving them access to\npotentially sensitive and private information, which leads to a possible risk\nof inadvertent privacy leakage when the agent malfunctions. In this work, we\npropose one way to address that potential risk, by training AI agents to better\nsatisfy the privacy principle of data minimization. For the purposes of this\nbenchmark, by \"data minimization\" we mean instances where private information\nis shared only when it is necessary to fulfill a specific task-relevant\npurpose. We develop a benchmark called AgentDAM to evaluate how well existing\nand future AI agents can limit processing of potentially private information\nthat we designate \"necessary\" to fulfill the task. Our benchmark simulates\nrealistic web interaction scenarios and is adaptable to all existing web\nnavigation agents. We use AgentDAM to evaluate how well AI agents built on top\nof GPT-4, Llama-3 and Claude can limit processing of potentially private\ninformation when unnecessary, and show that these agents are often prone to\ninadvertent use of unnecessary sensitive information. We finally propose a\nprompting-based approach that reduces this.",
        "Large language models demonstrate remarkable capabilities across various\ndomains, especially mathematics and logic reasoning. However, current\nevaluations overlook physics-based reasoning - a complex task requiring physics\ntheorems and constraints. We present PhysReason, a 1,200-problem benchmark\ncomprising knowledge-based (25%) and reasoning-based (75%) problems, where the\nlatter are divided into three difficulty levels (easy, medium, hard). Notably,\nproblems require an average of 8.1 solution steps, with hard requiring 15.6,\nreflecting the complexity of physics-based reasoning. We propose the Physics\nSolution Auto Scoring Framework, incorporating efficient answer-level and\ncomprehensive step-level evaluations. Top-performing models like Deepseek-R1,\nGemini-2.0-Flash-Thinking, and o3-mini-high achieve less than 60% on\nanswer-level evaluation, with performance dropping from knowledge questions\n(75.11%) to hard problems (31.95%). Through step-level evaluation, we\nidentified four key bottlenecks: Physics Theorem Application, Physics Process\nUnderstanding, Calculation, and Physics Condition Analysis. These findings\nposition PhysReason as a novel and comprehensive benchmark for evaluating\nphysics-based reasoning capabilities in large language models. Our code and\ndata will be published at https:\/dxzxy12138.github.io\/PhysReason.",
        "We respond to Holst et al.'s (HATWG) critique that the observed decline in\nscientific disruptiveness demonstrated in Park et al. (PLF) stems from\nincluding works with zero backward citations (0-bcites). Applying their own\nadvocated dataset, metric, and exclusion criteria, we demonstrate statistically\nand practically significant declines in disruptiveness that equal major\nbenchmark transformations in science. Notably, we show that HATWG's own\nregression model -- designed specifically to address their concerns about\n0-bcite works -- reveals highly significant declines for both papers (p<0.001)\nand patents (p<0.001), a finding they neither acknowledge nor interpret. Their\ncritique is undermined by methodological deficiencies, including reliance on\nvisual inspection without statistical assessment, and severe data quality\nissues in their SciSciNet dataset, which contains nearly three times more\n0-bcite papers than our original data. HATWG's departure from established\nscientometric practices -- notably their inclusion of document types and fields\nknown for poor metadata quality -- invalidates their conclusions. Monte Carlo\nsimulations and additional analyses using multiple disruptiveness measures\nacross datasets further validate the robustness of the declining trend. Our\nfindings collectively demonstrate that the observed decline in disruptiveness\nis not an artifact of 0-bcite works but represents a substantive change in\nscientific and technological innovation patterns.",
        "We devise $\\epsilon$-coresets for robust $(k,z)$-Clustering with $m$ outliers\nthrough black-box reductions to vanilla case. Given an $\\epsilon$-coreset\nconstruction for vanilla clustering with size $N$, we construct coresets of\nsize $N\\cdot \\mathrm{poly}\\log(km\\epsilon^{-1}) +\nO_z\\left(\\min\\{km\\epsilon^{-1}, m\\epsilon^{-2z}\\log^z(km\\epsilon^{-1})\n\\}\\right)$ for various metric spaces, where $O_z$ hides $2^{O(z\\log z)}$\nfactors. This increases the size of the vanilla coreset by a small\nmultiplicative factor of $\\mathrm{poly}\\log(km\\epsilon^{-1})$, and the additive\nterm is up to a $(\\epsilon^{-1}\\log (km))^{O(z)}$ factor to the size of the\noptimal robust coreset. Plugging in vanilla coreset results of [Cohen-Addad et\nal., STOC'21], we obtain the first coresets for $(k,z)$-Clustering with $m$\noutliers with size near-linear in $k$ while previous results have size at least\n$\\Omega(k^2)$ [Huang et al., ICLR'23; Huang et al., SODA'25].\n  Technically, we establish two conditions under which a vanilla coreset is as\nwell a robust coreset. The first condition requires the dataset to satisfy\nspecial structures - it can be broken into \"dense\" parts with bounded diameter.\nWe combine this with a new bounded-diameter decomposition that has only $O_z(km\n\\epsilon^{-1})$ non-dense points to obtain the $O_z(km \\epsilon^{-1})$ additive\nbound. Another condition requires the vanilla coreset to possess an extra\nsize-preserving property. We further give a black-box reduction that turns a\nvanilla coreset to the one satisfying the said size-preserving property,\nleading to the alternative $O_z(m\\epsilon^{-2z}\\log^{z}(km\\epsilon^{-1}))$\nadditive bound.\n  We also implement our reductions in the dynamic streaming setting and obtain\nthe first streaming algorithms for $k$-Median and $k$-Means with $m$ outliers,\nusing space $\\tilde{O}(k+m)\\cdot\\mathrm{poly}(d\\epsilon^{-1}\\log\\Delta)$ for\ninputs on the grid $[\\Delta]^d$.",
        "Nanoscale objects moving relative to a thermal radiation bath experience a\ndrag force due to the imbalance in their interaction with the blue- and\nredshifted components of the electromagnetic field. Here, we show that, in\naddition to this drag force, moving nanostructures with an anisotropic optical\nresponse experience a lateral force and a torque that substantially modify\ntheir trajectory. These phenomena emerge from the additional coupling between\nthe electromagnetic field components polarized parallel and perpendicular to\nthe trajectory, enabled by the anisotropic response of the nanostructure. This\nwork unveils the intricate dynamics of anisotropic nanostructures moving in a\nthermal radiation bath.",
        "Gaussian process state-space models (GPSSMs) have emerged as a powerful\nframework for modeling dynamical systems, offering interpretable uncertainty\nquantification and inherent regularization. However, existing GPSSMs face\nsignificant challenges in handling high-dimensional, non-stationary systems due\nto computational inefficiencies, limited scalability, and restrictive\nstationarity assumptions. In this paper, we propose an efficient transformed\nGaussian process state-space model (ETGPSSM) to address these limitations. Our\napproach leverages a single shared Gaussian process (GP) combined with\nnormalizing flows and Bayesian neural networks, enabling efficient modeling of\ncomplex, high-dimensional state transitions while preserving scalability. To\naddress the lack of closed-form expressions for the implicit process in the\ntransformed GP, we follow its generative process and introduce an efficient\nvariational inference algorithm, aided by the ensemble Kalman filter (EnKF), to\nenable computationally tractable learning and inference. Extensive empirical\nevaluations on synthetic and real-world datasets demonstrate the superior\nperformance of our ETGPSSM in system dynamics learning, high-dimensional state\nestimation, and time-series forecasting, outperforming existing GPSSMs and\nneural network-based methods in both accuracy and computational efficiency.",
        "A long-term accurate and robust localization system is essential for mobile\nrobots to operate efficiently outdoors. Recent studies have shown the\nsignificant advantages of the wheel-mounted inertial measurement unit\n(Wheel-IMU)-based dead reckoning system. However, it still drifts over extended\nperiods because of the absence of external correction signals. To achieve the\ngoal of long-term accurate localization, we propose Wheel-GINS, a Global\nNavigation Satellite System (GNSS)\/inertial navigation system (INS) integrated\nnavigation system using a Wheel-IMU. Wheel-GINS fuses the GNSS position\nmeasurement with the Wheel-IMU via an extended Kalman filter to limit the\nlong-term error drift and provide continuous state estimation when the GNSS\nsignal is blocked. Considering the specificities of the GNSS\/Wheel-IMU\nintegration, we conduct detailed modeling and online estimation of the\nWheel-IMU installation parameters, including the Wheel-IMU leverarm and\nmounting angle and the wheel radius error. Experimental results have shown that\nWheel-GINS outperforms the traditional GNSS\/Odometer\/INS integrated navigation\nsystem during GNSS outages. At the same time, Wheel-GINS can effectively\nestimate the Wheel-IMU installation parameters online and, consequently,\nimprove the localization accuracy and practicality of the system. The source\ncode of our implementation is publicly available\n(https:\/\/github.com\/i2Nav-WHU\/Wheel-GINS).",
        "We explicitly derive the wavefunctions of chiral phonons propagating along\nthe helical axis in chiral crystals and clarify the characteristics of\nelectron-phonon interactions in chiral helical crystals. In particular, we\nelucidate how the conservation of not only the crystal momentum (CM) but also\nthe crystal angular momentum (CAM) manifests in the interaction vertex. This\nformulation provides a microscopic framework for describing physical processes\ninvolving chiral phonons. Furthermore, we construct a phononic analogue of\nZilch, a known measure of chirality carried by light, and discuss its\nrelationship with phonon angular momentum.",
        "Large language models (LLMs) and Vision language models (VLMs) have been able\nto perform various forms of reasoning tasks in a wide range of scenarios, but\nare they truly engaging in task abstraction and rule-based reasoning beyond\nmere memorization and pattern matching? To answer this question, we propose a\nnovel experimental approach, Misleading Fine-Tuning (MisFT), to examine whether\nLLMs\/VLMs perform abstract reasoning by altering their original understanding\nof fundamental rules. In particular, by constructing a dataset with math\nexpressions that contradict correct operation principles, we fine-tune the\nmodel to learn those contradictory rules and assess its generalization ability\non different test domains. Through a series of experiments, we find that\ncurrent LLMs\/VLMs are capable of effectively applying contradictory rules to\nsolve practical math word problems and math expressions represented by images,\nimplying the presence of an internal mechanism that abstracts before reasoning.",
        "Quantum burst error correction codes (QBECCs) are of great importance to deal\nwith the memory effect in quantum channels. As the most important family of\nQBECCs, quantum cyclic codes (QCCs) play a vital role in the correction of\nburst errors. In this work, we characterize the burst error correction ability\nof QCCs constructed from the Calderbank-Shor-Steane (CSS) and the Hermitian\nconstructions. We determine the burst error correction limit of QCCs and\nquantum Reed-Solomon codes with algorithms in polynomial-time complexities. As\na result, lots of QBECCs saturating the quantum Reiger bound are obtained. We\nshow that quantum Reed-Solomon codes have better burst error correction\nabilities than the previous results. At last, we give the quantum\nerror-trapping decoder (QETD) of QCCs for decoding burst errors. The decoder\nruns in linear time and can decode both degenerate and nondegenerate burst\nerrors. What's more, the numerical results show that QETD can decode much more\ndegenerate burst errors than the nondegenerate ones.",
        "The compressible Euler-Riesz equations are fundamental with wide applications\nin astrophysics, plasma physics, and mathematical biology. In this paper, we\nare concerned with the global existence and nonlinear stability of\nfinite-energy solutions of the multidimensional Euler-Riesz equations with\nlarge initial data of spherical symmetry. We consider both attractive and\nrepulsive interactions for a wide range of Riesz and logarithmic potentials for\ndimensions larger than or equal to two. This is achieved by the inviscid limit\nof the solutions of the corresponding Cauchy problem for the\nNavier-Stokes-Riesz equations. The strong convergence of the vanishing\nviscosity solutions is achieved through delicate uniform estimates in $L^p$. It\nis observed that, even if the attractive potential is super-Coulomb, no\nconcentration is formed near the origin in the inviscid limit. Moreover, we\nprove that the nonlinear stability of global finite-energy solutions for the\nEuler-Riesz equations is unconditional under a spherically symmetric\nperturbation around the steady solutions. Unlike the Coulomb case where the\npotential can be represented locally, the singularity and regularity of the\nnonlocal radial Riesz potential near the origin require careful analysis, which\nis a crucial step. Finally, unlike the Coulomb case, a Gr\\\"onwall type estimate\nis required to overcome the difficulty of the appearance of boundary terms in\nthe sub-Coulomb case and the singularity of the super-Coulomb potential.\nFurthermore, we prove the nonlinear stability of global finite-energy solutions\nfor the compressible Euler-Riesz equations around steady states by employing\nconcentration compactness arguments. Steady states properties are obtained by\nvariational arguments connecting to recent advances in aggregation-diffusion\nequations.",
        "We investigate the hollowing transition of a shell-shaped Bose-Einstein\ncondensate using collective excitations. The shell is created using an\nimmiscible dual-species BEC mixture, with its hollowness controlled by tuning\nthe repulsive interspecies interaction via a Feshbach resonance. Our results\nreveal two distinct monopole modes in which the two condensates oscillate\neither in-phase or out-of-phase. The spectrum of the out-of-phase mode exhibits\na non-monotonic dependence on the interspecies interaction, providing a clear\nsignature of the topology change from a filled to a hollow condensate.\nFurthermore, we find that the critical point of the hollowing transition\ndepends strongly on the number ratio of the two species. Our findings provide a\ndetailed understanding of the topology change in shell-shaped quantum gases and\npave the way for future study of quantum many-body phenomena in curved spaces.",
        "Reproducing concurrency bugs is a complex task due to their unpredictable\nbehavior. Researchers, regardless of gender, are contributing to automating\nthis complex task to aid software developers. While some studies have\ninvestigated gender roles in the broader software industry, limited research\nexists on gender representation specifically among researchers working in\nconcurrent bug reproduction. To address this gap, in this paper, we present a\nliterature review to assess the gender ratio in this field. We also explore\npotential variations in technique selection and bug-type focus across genders.\nOur findings indicate that female researchers are underrepresented compared to\ntheir male counterparts in this area, with a current male-to-female author\nratio of 29:6. Through this study, we emphasize the importance of fostering\ngender equity in software engineering research, ensuring a diversity of\nperspectives in the development of automated bug reproduction tools.",
        "This paper introduces CognitiveDrone, a novel Vision-Language-Action (VLA)\nmodel tailored for complex Unmanned Aerial Vehicles (UAVs) tasks that demand\nadvanced cognitive abilities. Trained on a dataset comprising over 8,000\nsimulated flight trajectories across three key categories-Human Recognition,\nSymbol Understanding, and Reasoning-the model generates real-time 4D action\ncommands based on first-person visual inputs and textual instructions. To\nfurther enhance performance in intricate scenarios, we propose\nCognitiveDrone-R1, which integrates an additional Vision-Language Model (VLM)\nreasoning module to simplify task directives prior to high-frequency control.\nExperimental evaluations using our open-source benchmark, CognitiveDroneBench,\nreveal that while a racing-oriented model (RaceVLA) achieves an overall success\nrate of 31.3%, the base CognitiveDrone model reaches 59.6%, and\nCognitiveDrone-R1 attains a success rate of 77.2%. These results demonstrate\nimprovements of up to 30% in critical cognitive tasks, underscoring the\neffectiveness of incorporating advanced reasoning capabilities into UAV control\nsystems. Our contributions include the development of a state-of-the-art VLA\nmodel for UAV control and the introduction of the first dedicated benchmark for\nassessing cognitive tasks in drone operations. The complete repository is\navailable at cognitivedrone.github.io",
        "Recently, Cuntz and K\\\"uhne introduced a particular class of hyperplane\narrangements stemming from a given graph $G$, so called connected subgraph\narrangements $A_G$. In this note we strengthen some of the result from their\nwork and prove new ones for members of this class. For instance, we show that\naspherical members withing this class stem from a rather restricted set of\ngraphs. Specifically, if $A_G$ is an aspherical connected subgraph arrangement,\nthen $A_G$ is free with the unique possible exception when the underlying graph\n$G$ is the complete graph on $4$ nodes.",
        "We present SwD, a scale-wise distillation framework for diffusion models\n(DMs), which effectively employs next-scale prediction ideas for\ndiffusion-based few-step generators. In more detail, SwD is inspired by the\nrecent insights relating diffusion processes to the implicit spectral\nautoregression. We suppose that DMs can initiate generation at lower data\nresolutions and gradually upscale the samples at each denoising step without\nloss in performance while significantly reducing computational costs. SwD\nnaturally integrates this idea into existing diffusion distillation methods\nbased on distribution matching. Also, we enrich the family of distribution\nmatching approaches by introducing a novel patch loss enforcing finer-grained\nsimilarity to the target distribution. When applied to state-of-the-art\ntext-to-image diffusion models, SwD approaches the inference times of two full\nresolution steps and significantly outperforms the counterparts under the same\ncomputation budget, as evidenced by automated metrics and human preference\nstudies.",
        "People are influenced by their peers when making decisions. In this paper, we\nstudy the linear-in-means model which is the standard empirical model of peer\neffects. As data on the underlying social network is often difficult to come\nby, we focus on data that only captures an agent's choices. Under exogenous\nagent participation variation, we study two questions. We first develop a\nrevealed preference style test for the linear-in-means model. We then study the\nidentification properties of the linear-in-means model. With sufficient\nparticipation variation, we show how an analyst is able to recover the\nunderlying network structure and social influence parameters from choice data.\nOur identification result holds when we allow the social network to vary across\ncontexts. To recover predictive power, we consider a refinement which allows us\nto extrapolate the underlying network structure across groups and provide a\ntest of this version of the model."
      ]
    }
  },
  {
    "id":2411.05456,
    "research_type":"applied",
    "start_id":"b11",
    "start_title":"U-net: Convolutional networks for biomedical image segmentation",
    "start_abstract":"There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "MRI segmentation of the human brain: challenges, methods, and applications"
      ],
      "abstract":[
        "Image segmentation is one of the most important tasks in medical image analysis and is often the first and the most critical step in many clinical applications. In brain MRI analysis, image segmentation is commonly used for measuring and visualizing the brain\u2019s anatomical structures, for analyzing brain changes, for delineating pathological regions, and for surgical planning and image-guided interventions. In the last few decades, various segmentation techniques of different accuracy and degree of complexity have been developed and reported in the literature. In this paper we review the most popular methods commonly used for brain MRI segmentation. We highlight differences between them and discuss their capabilities, advantages, and limitations. To address the complexity and challenges of the brain MRI segmentation problem, we first introduce the basic concepts of image segmentation. Then, we explain different MRI preprocessing steps including image registration, bias field correction, and removal of nonbrain tissue. Finally, after reviewing different brain MRI segmentation methods, we discuss the validation problem in brain MRI segmentation."
      ],
      "categories":[
        "q-bio.QM"
      ]
    },
    "list":{
      "title":[
        "Foliar Uptake of Biocides: Statistical Assessment of Compartmental and\n  Diffusion-Based Models",
        "VenusMutHub: A systematic evaluation of protein mutation effect\n  predictors on small-scale experimental data",
        "A technical review of multi-omics data integration methods: from\n  classical statistical to deep generative approaches",
        "Efficient Spatial Estimation of Perceptual Thresholds for Retinal\n  Implants via Gaussian Process Regression",
        "Uncertainty and sensitivity analysis of hair growth duration in human\n  scalp follicles under normal and alopecic conditions",
        "Modeling HIF-ILK Interaction Using Continuous Petri Nets",
        "IA generativa aplicada a la detecci\\'on del c\\'ancer a trav\\'es de\n  Resonancia Magn\\'etica",
        "Genomic and pathological analyses of an asymmetric true hermaphroditism\n  case in a female labrador retriever",
        "A Framework for Building Enviromics Matrices in Mixed Models",
        "A Bayesian Modelling Framework with Model Comparison for Epidemics with\n  Super-Spreading",
        "Purported quantitative support for multiple introductions of SARS-CoV-2\n  into humans is an artefact of an imbalanced hypothesis testing framework",
        "Homeostatic Kinematic Growth Model for Arteries -- Residual Stresses and\n  Active Response",
        "EMICSS: Added-value annotations for EMDB entries",
        "Operator Learning for Reconstructing Flow Fields from Sparse\n  Measurements: an Energy Transformer Approach",
        "Excitability and oscillations of active droplets",
        "Several classes of linear codes with few weights derived from Weil sums",
        "ByteQC: GPU-Accelerated Quantum Chemistry Package for Large-Scale\n  Systems",
        "What exactly has TabPFN learned to do?",
        "Amplification of turbulence through multiple planar shocks",
        "Diophantine approximation and the subspace theorem",
        "Optimal $L^p$-approximation of convex sets by convex subsets",
        "The Hierarchy of Saturating Matching Numbers",
        "Bayesianize Fuzziness in the Statistical Analysis of Fuzzy Data",
        "Escalation dynamics and the severity of wars",
        "Efficient and stable derivative-free Steffensen algorithm for root\n  finding",
        "Analytical Strategies and Winning Conditions for Elliptic-Orbit\n  Target-Attacker-Defender Game",
        "Parameter Estimation of State Space Models Using Particle Importance\n  Sampling",
        "A family of asymptotically bad wild towers of function fields"
      ],
      "abstract":[
        "The global population increase leads to a high food demand, and to reach this\ntarget products such as pesticides are needed to protect the crops. Research is\nfocusing on the development of new products that can be less harmful to the\nenvironment, and mathematical models are tools that can help to understand the\nmechanism of uptake of pesticides and then guide in the product development\nphase. This paper applies a systematic methodology to model the foliar uptake\nof pesticides, to take into account the uncertainties in the experimental data\nand in the model structure. A comparison between different models is conducted,\nfocusing on the identifiability of model parameters through dynamic sensitivity\nprofiles and correlation analysis. Lastly, data augmentation studies are\nconducted to exploit the model for the design of experiments and to provide a\npractical support to future experimental campaigns, paving the way for further\napplication of model-based design of experiments techniques in the context of\nfoliar uptake.",
        "In protein engineering, while computational models are increasingly used to\npredict mutation effects, their evaluations primarily rely on high-throughput\ndeep mutational scanning (DMS) experiments that use surrogate readouts, which\nmay not adequately capture the complex biochemical properties of interest. Many\nproteins and their functions cannot be assessed through high-throughput methods\ndue to technical limitations or the nature of the desired properties, and this\nis particularly true for the real industrial application scenario. Therefore,\nthe desired testing datasets, will be small-size (~10-100) experimental data\nfor each protein, and involve as many proteins as possible and as many\nproperties as possible, which is, however, lacking. Here, we present\nVenusMutHub, a comprehensive benchmark study using 905 small-scale experimental\ndatasets curated from published literature and public databases, spanning 527\nproteins across diverse functional properties including stability, activity,\nbinding affinity, and selectivity. These datasets feature direct biochemical\nmeasurements rather than surrogate readouts, providing a more rigorous\nassessment of model performance in predicting mutations that affect specific\nmolecular functions. We evaluate 23 computational models across various\nmethodological paradigms, such as sequence-based, structure-informed and\nevolutionary approaches. This benchmark provides practical guidance for\nselecting appropriate prediction methods in protein engineering applications\nwhere accurate prediction of specific functional properties is crucial.",
        "The rapid advancement of high-throughput sequencing and other assay\ntechnologies has resulted in the generation of large and complex multi-omics\ndatasets, offering unprecedented opportunities for advancing precision medicine\nstrategies. However, multi-omics data integration presents significant\nchallenges due to the high dimensionality, heterogeneity, experimental gaps,\nand frequency of missing values across data types. Computational methods have\nbeen developed to address these issues, employing statistical and machine\nlearning approaches to uncover complex biological patterns and provide deeper\ninsights into our understanding of disease mechanisms. Here, we comprehensively\nreview state-of-the-art multi-omics data integration methods with a focus on\ndeep generative models, particularly variational autoencoders (VAEs) that have\nbeen widely used for data imputation and augmentation, joint embedding\ncreation, and batch effect correction. We explore the technical aspects of loss\nfunctions and regularisation techniques including adversarial training,\ndisentanglement and contrastive learning. Moreover, we discuss recent\nadvancements in foundation models and the integration of emerging data\nmodalities, while describing the current limitations and outlining future\ndirections for enhancing multi-modal methodologies in biomedical research.",
        "Retinal prostheses restore vision by electrically stimulating surviving\nneurons, but calibrating perceptual thresholds - the minimum stimulus intensity\nrequired for perception - remains a time-intensive challenge, especially for\nhigh-electrode-count devices. Since neighboring electrodes exhibit spatial\ncorrelations, we propose a Gaussian Process Regression (GPR) framework to\npredict thresholds at unsampled locations while leveraging uncertainty\nestimates to guide adaptive sampling. Using perceptual threshold data from four\nArgus II users, we show that GPR with a Mat\\'ern kernel provides more accurate\nthreshold predictions than a Radial Basis Function (RBF) kernel (p < .001,\nWilcoxon signed-rank test). In addition, spatially optimized sampling yielded\nlower prediction error than uniform random sampling for Participants 1 and 3 (p\n< .05). While adaptive sampling dynamically selects electrodes based on model\nuncertainty, its accuracy gains over spatial sampling were not statistically\nsignificant (p > .05), though it approached significance for Participant 1 (p =\n.074). These findings establish GPR with spatial sampling as a scalable,\nefficient approach to retinal prosthesis calibration, minimizing patient burden\nwhile maintaining predictive accuracy. More broadly, this framework offers a\ngeneralizable solution for adaptive calibration in neuroprosthetic devices with\nspatially structured stimulation thresholds.",
        "Hair follicles constantly cycle through phases of growth, regression and\nrest, as matrix keratinocytes (MKs), the cells producing hair fibers,\nproliferate, and then undergo spontaneous apoptosis. Damage to MKs and\nperturbations in their normal dynamics result in a shortened growth phase,\nleading to hair loss. Two common factors causing such disruption are hormonal\nimbalance and attacks by the immune system. Androgenetic alopecia (AGA) is hair\nloss caused by high sensitivity to androgens, and alopecia areata (AA) is hair\nloss caused by an autoimmune reaction against MKs. In this study, we inform a\nmathematical model for the human hair cycle with experimental data for the\nlengths of hair cycle phases available from male control subjects and subjects\nwith AGA. We also, connect a mathematical model for AA with estimates for the\nduration of hair cycle phases obtained from the literature. Subsequently, with\neach model we perform parameter screening, uncertainty quantification and\nglobal sensitivity analysis and compare the results within and between the\ncontrol and AGA subject groups as well as among AA, control and AGA conditions.\nThe findings reveal that in AGA subjects there is greater uncertainty\nassociated with the duration of hair growth than in control subjects and that,\ncompared to control and AGA conditions, in AA it is more certain that longer\nhair growth phase could not be expected. The comparison of results also\nindicates that in AA lower proliferation of MKs and weaker communication of the\ndermal papilla with MKs via signaling molecules could be expected than in\nnormal and AGA conditions, and in AA stronger inhibition of MK proliferation by\nregulatory molecules could be expected than in AGA. Finally, the global\nsensitivity analysis highlights the process of MK apoptosis as highly impactful\nfor the length of hair growth only in the AA case, but not for control and AGA\nconditions.",
        "Oxygen concentration in tumor micro-environment is a well-established signal\nthat can induce aggressive cancer behaviour. In particular, low oxygen levels\n(hypoxia) activate the Hypoxia-Inducible Factor(HIF) pathway which has an array\nof target systems. One of these systems is Integrin-Linked Kinase (ILK)\npathway, which influences key signaling pathways for cell survival,\nproliferation, and migration. Hence, this paper aimed to explore the\ninterconnection between these two pathways. Using the Petri net modeling tool\nSnoopy, an established HIF network model was transformed to be a continuous\nPetri net. Subsequently, the network was expanded to incorporate a feedback\nelement from the ILK pathway to HIF, based on gene expression data. The\nresulting model conserved the oxygen switch response of the original HIF model\nand positively amplified HIF's output. Therefore, this model provides a\nstarting point for establishing a system reflecting crucial effect on\nhypoxia-induced cancer behavior, and could potentially serve as a basis for\nfuture drug development.",
        "Cognitive delegation to artificial intelligence (AI) systems is transforming\nscientific research by enabling the automation of analytical processes and the\ndiscovery of new patterns in large datasets. This study examines the ability of\nAI to complement and expand knowledge in the analysis of breast cancer using\ndynamic contrast-enhanced magnetic resonance imaging (DCE-MRI). Building on a\nprevious study, we assess the extent to which AI can generate novel approaches\nand successfully solve them. For this purpose, AI models, specifically\nChatGPT-4o, were used for data preprocessing, hypothesis generation, and the\napplication of clustering techniques, predictive modeling, and correlation\nnetwork analysis. The results obtained were compared with manually computed\noutcomes, revealing limitations in process transparency and the accuracy of\ncertain calculations. However, as AI reduces errors and improves reasoning\ncapabilities, an important question arises regarding the future of scientific\nresearch: could automation replace the human role in science? This study seeks\nto open the debate on the methodological and ethical implications of a science\ndominated by artificial intelligence.",
        "The two main gonadal development disorders in dogs are true hermaphroditism\nand XX male syndrome. True hermaphroditism can be divided into two\nsubcategories: XX sex reversal and XY sex reversal. XX Sry-negative sex\nreversal is more common, and it is characterized by the presence of both\novarian and testicular tissues in an animal. To date, there are 16 cases of\ntrue hermaphroditism reported in the literature, 15 of which are XX true\nhermaphroditism. Hermaphroditism has not been formally documented in labrador\nretrievers, and no case of asymmetric hermaphroditism has been reported in the\nliterature.",
        "This study introduces a framework for constructing enviromics matrices in\nmixed models to integrate genetic and environmental data to enhance phenotypic\npredictions in plant breeding. Enviromics utilizes diverse data sources, such\nas climate and soil, to characterize genotype-by-environment (GxE)\ninteractions. The approach employs block-diagonal structures in the design\nmatrix to incorporate random effects from genetic and envirotypic covariates\nacross trials. The covariance structure is modeled using the Kronecker product\nof the genetic relationship matrix and an identity matrix representing\nenvirotypic effects, capturing genetic and environmental variability. This dual\nrepresentation enables more accurate crop performance predictions across\nenvironments, improving selection strategies in breeding programs. The\nframework is compatible with existing mixed model software, including rrBLUP\nand BGLR, and can be extended for more complex interactions. By combining\ngenetic relationships and environmental influences, this approach offers a\npowerful tool for advancing GxE studies and accelerating the development of\nimproved crop varieties.",
        "The transmission dynamics of an epidemic are rarely homogeneous.\nSuper-spreading events and super-spreading individuals are two types of\nheterogeneous transmissibility. Inference of super-spreading is commonly\ncarried out on secondary case data, the expected distribution of which is known\nas the offspring distribution. However, this data is seldom available. Here we\nintroduce a multi-model framework fit to incidence time-series, data that is\nmuch more readily available. The framework consists of five discrete-time,\nstochastic, branching-process models of epidemics spread through a susceptible\npopulation. The framework includes a baseline model of homogeneous\ntransmission, a unimodal and a bimodal model for super-spreading events, as\nwell as a unimodal and a bimodal model for super-spreading individuals.\nBayesian statistics is used to infer model parameters using Markov Chain\nMonte-Carlo. Model comparison is conducted by computing Bayes factors, with\nimportance sampling used to estimate the marginal likelihood of each model.\nThis estimator is selected for its consistency and lower variance compared to\nalternatives. Application to simulated data from each model identifies the\ncorrect model for the majority of simulations and accurately infers the true\nparameters, such as the basic reproduction number. We also apply our methods to\nincidence data from the 2003 SARS outbreak and the Covid-19 pandemic. Model\nselection consistently identifies the same model and mechanism for a given\ndisease, even when using different time series. Our estimates are consistent\nwith previous studies based on secondary case data. Quantifying the\ncontribution of super-spreading to disease transmission has important\nimplications for infectious disease management and control. Our modelling\nframework is disease-agnostic and implemented as an R package, with potential\nto be a valuable tool for public health.",
        "A prominent report claimed substantial support for two introductions of\nSARS-CoV-2 into humans using a calculation that combined phylodynamic\ninferences and epidemic models. Inspection of the calculation identifies an\nimbalance in the hypothesis testing framework that confounds this result; the\nsingle-introduction model was tested against more stringent conditions than the\ntwo-introduction model. Here, I show that when the two-introduction model is\ntested against the same conditions, the support disappears.",
        "A simple kinematic growth model for muscular arteries is presented which\nallows the incorporation of residual stresses such that a homeostatic in-vivo\nstress state under physiological loading is obtained. To this end, new\nevolution equations for growth are proposed, which avoid issues with\ninstability of final growth states known from other kinematric growth models.\nThese evolution equations are connected to a new set of growth driving forces.\nBy introducing a formulation using the principle Cauchy stresses, reasonable in\nvivo stress distributions can be obtained while ensuring realistic geometries\nof arteries after growth. To incorporate realistic Cauchy stresses for muscular\narteries under varying loading conditions, which appear in vivo, e.g., due to\nphysical activity, the growth model is combined with a newly proposed\nstretch-dependent model for smooth muscle activation. To account for the\nchanges of geometry and mechanical behavior during the growth process, an\noptimization procedure is described which leads to a more accurate\nrepresentation of an arterial ring and its mechanical behavior after the\ngrowth-related homogenization of the stresses is reached. Based thereon,\nparameters are adjusted to experimental data of a healthy middle cerebral\nartery of a rat showing that the proposed model accurately describes real\nmaterial behavior. The successful combination of growth and active response\nindicates that the new growth model can be used without problems for modeling\nactive tissues under various conditions.",
        "Motivation: The Electron Microscopy Data Bank (EMDB) is a key repository for\n3D electron microscopy (3DEM) data but lacks comprehensive annotations and\nconnections to most of the related biological, functional, and structural data.\nThis limitation arises from the optional nature of such information to reduce\ndepositor burden and the complexity of maintaining up-to-date external\nreferences, often requiring depositor consent. To address these challenges, we\ndeveloped EMICSS (EMDB Integration with Complexes, Structures, and Sequences),\nan independent system that automatically updates cross-references with over 20\nexternal resources, including UniProt, AlphaFold DB, PubMed, Complex Portal and\nGene Ontology. Results: EMICSS (https:\/\/www.ebi.ac.uk\/emdb\/emicss) annotations\nare accessible in multiple formats for every EMDB entry and its linked\nresources, and programmatically via the EMDB Application Programming Interface\n(API). EMICSS plays a crucial role supporting the EMDB website, with\nannotations being used on entry pages, statistics, and in the search system.\nAvailability and implementation: EMICSS is implemented in Python and it is an\nopen-source, distributed under the EMBL-EBI license, with core code available\non GitHub (https:\/\/github.com\/emdb-empiar\/added_annotations).",
        "Machine learning methods have shown great success in various scientific\nareas, including fluid mechanics. However, reconstruction problems, where full\nvelocity fields must be recovered from partial observations, remain\nchallenging. In this paper, we propose a novel operator learning framework for\nsolving reconstruction problems by using the Energy Transformer (ET), an\narchitecture inspired by associative memory models. We formulate reconstruction\nas a mapping from incomplete observed data to full reconstructed fields. The\nmethod is validated on three fluid mechanics examples using diverse types of\ndata: (1) unsteady 2D vortex street in flow past a cylinder using simulation\ndata; (2) high-speed under-expanded impinging supersonic jets impingement using\nSchlieren imaging; and (3) 3D turbulent jet flow using particle tracking. The\nresults demonstrate the ability of ET to accurately reconstruct complex flow\nfields from highly incomplete data (90\\% missing), even for noisy experimental\nmeasurements, with fast training and inference on a single GPU. This work\nprovides a promising new direction for tackling reconstruction problems in\nfluid mechanics and other areas in mechanics, geophysics, weather prediction,\nand beyond.",
        "In living cells, cycles of formation and dissolution of liquid droplets can\nmediate biological functions such as DNA repair. However, the minimal\nphysicochemical prerequisite for such droplet oscillations remains elusive.\nHere, we present a simple model composed of only two independent chemical\ncomponents with their diffusive and chemical fluxes governed by non-equilibrium\nthermodynamics. There is turnover of fuel that maintains a chemical reaction\naway from equilibrium, leading to active droplets. We find that a single active\ndroplet undergoes a pitchfork-bifurcation in the droplet volume upon increasing\nthe fueling strength. Strikingly, the active droplet becomes excitable upon\nadding a further chemical reaction. For sufficient fueling, the system\nundergoes self-sustained oscillations cycling between droplet formation and\ndissolution. The minimal nature of our model suggests self-sustained active\ndroplets as functional modules for de novo life.",
        "Linear codes with few weights have applications in secret sharing,\nauthentication codes, association schemes and strongly regular graphs. In this\npaper, several classes of $t$-weight linear codes over ${\\mathbb F}_{q}$ are\npresented with the defining sets given by the intersection, difference and\nunion of two certain sets, where $t=3,4,5,6$ and $q$ is an odd prime power. By\nusing Weil sums and Gauss sums, the parameters and weight distributions of\nthese codes are determined completely. Moreover, three classes of optimal codes\nmeeting the Griesmer bound are obtained, and computer experiments show that\nmany (almost) optimal codes can be derived from our constructions.",
        "Applying quantum chemistry algorithms to large-scale systems requires\nsubstantial computational resources scaled with the system size and the desired\naccuracy. To address this, ByteQC, a fully-functional and efficient package for\nlarge-scale quantum chemistry simulations, has been open-sourced at\nhttps:\/\/github.com\/bytedance\/byteqc, leveraging recent advances in\ncomputational power and many-body algorithms.\n  Regarding computational power, several standard algorithms are efficiently\nimplemented on modern GPUs, ranging from mean-field calculations (Hartree-Fock\nand density functional theory) to post-Hartree-Fock methods such as\nM{\\o}ller-Plesset perturbation theory, random phase approximation, coupled\ncluster methods, and quantum Monte Carlo methods. For the algorithmic approach,\nwe also employ a quantum embedding method, which significantly expands the\ntractable system size while preserving high accuracy at the gold-standard\nlevel.\n  All these features have been systematically benchmarked. For standalone\nalgorithms, the benchmark results demonstrate up to a 60$\\times$ speedup when\ncompared to 100-core CPUs. Additionally, the tractable system sizes have been\nsignificantly expanded: 1,610 orbitals for coupled cluster with single and\ndouble excitations (1,380 orbitals with perturbative triple excitations),\n11,040 orbitals for M{\\o}ller-Plesset perturbation theory of second order,\n37,120 orbitals for mean-field calculations under open boundary conditions, and\nover 100,000 orbitals for periodic boundary conditions. For the advanced\nquantum embedding feature, two representative examples are demonstrated: the\nwater cluster problem (2,752 orbitals) and a water monomer adsorbed on a boron\nnitride surface (3,929 orbitals), achieving the gold-standard accuracy.",
        "TabPFN [Hollmann et al., 2023], a Transformer model pretrained to perform\nin-context learning on fresh tabular classification problems, was presented at\nthe last ICLR conference. To better understand its behavior, we treat it as a\nblack-box function approximator generator and observe its generated function\napproximations on a varied selection of training datasets. Exploring its\nlearned inductive biases in this manner, we observe behavior that is at turns\neither brilliant or baffling. We conclude this post with thoughts on how these\nresults might inform the development, evaluation, and application of prior-data\nfitted networks (PFNs) in the future.",
        "We study the amplification of isotropic, incompressible turbulence through\nmultiple planar, collisional shocks, using analytical linear theory. There are\ntwo limiting cases we explore. The first assumes shocks occur rapidly in time\nsuch that the turbulence does not evolve between shocks. Whereas the second\ncase allows enough time for turbulence to isotropize between each shock. For\nthe latter case, through a quasi-equation-of-state, we show that the weak\nmulti-shock limit is agnostic to the distinction between thermal and vortical\nturbulent pressures, like an isotropic volumetric compression. When turbulence\ndoes not return to isotropy between shocks, the generated anisotropy -- itself\na function of shock strength -- can feedback on amplification by further\nshocks, altering choices for maximal or minimal amplification. In addition for\nthis case, we find that amplification is sensitive to the shock ordering. We\nmap how choices of shock strength can impact these amplification differences\ndue to ordering, finding, for example, shock pairs which lead to identical mean\npost-shock fields (density, temperature, pressure) but maximally distinct\nturbulent amplification.",
        "Diophantine approximation explores how well irrational numbers can be\napproximated by rationals, with foundational results by Dirichlet, Hurwitz, and\nLiouville culminating in Roth's theorem. Schmidt's subspace theorem extends\nRoth's results to higher dimensions, with profound implications to Diophantine\nequations and transcendence theory. This article provides a self-contained and\naccessible exposition of Roth's theorem and Schlickewei's refinement of the\nsubspace theorem, with an emphasis on proofs. The arguments presented are\nclassical and approachable for readers with a background in algebraic number\ntheory, serving as a streamlined, yet condensed reference for these fundamental\nresults.",
        "Given a convex set $\\Omega$ of $\\mathbb{R}^n$, we consider the shape\noptimization problem of finding a convex subset $\\omega\\subset \\Omega$, of a\ngiven measure, minimizing the $p$-distance functional $$\\mathcal{J}_p(\\omega)\n:= \\left(\\int_{\\mathbb{S}^{n-1}} |h_\\Omega-h_\\omega|^p\nd\\mathcal{H}^{n-1}\\right)^{\\frac{1}{p}},$$ where $1 \\le p <\\infty$ and\n$h_\\omega$ and $h_\\Omega$ are the support functions of $\\omega$ and the fixed\ncontainer $\\Omega$, respectively.\n  We prove the existence of solutions and show that this minimization problem\n$\\Gamma$-converges, when $p$ tends to $+\\infty$, towards the problem of finding\na convex subset $\\omega\\subset \\Omega$, of a given measure, minimizing the\nHausdorff distance to the convex $\\Omega$.\n  In the planar case, we show that the free parts of the boundary of the\noptimal shapes, i.e., those that are in the interior of $\\Omega$, are given by\npolygonal lines.\n  Still in the $2-d$ setting, from a computational perspective, the classical\nmethod based on optimizing Fourier coefficients of support functions is not\nefficient, as it is unable to efficiently capture the presence of segments on\nthe boundary of optimal shapes. We subsequently propose a method combining\nFourier analysis and a recent numerical scheme, allowing to obtain accurate\nresults, as demonstrated through numerical experiments.",
        "In this paper, we study three matching problems all of which came up quite\nrecently in the field of machine teaching. The cost of a matching is defined in\nsuch a way that, for some formal model of teaching, it equals (or bounds) the\nnumber of labeled examples needed to solve a given teaching task. We show how\nthe cost parameters associated with these problems depend on each other and how\nthey are related to other well known combinatorial parameters (like, for\ninstance, the VC-dimension).",
        "Fuzzy data, prevalent in social sciences and other fields, capture\nuncertainties arising from subjective evaluations and measurement imprecision.\nDespite significant advancements in fuzzy statistics, a unified inferential\nregression-based framework remains undeveloped. Hence, we propose a novel\napproach for analyzing bounded fuzzy variables within a regression framework.\nBuilding on the premise that fuzzy data result from a process analogous to\nstatistical coarsening, we introduce a conditional probabilistic approach that\nlinks observed fuzzy statistics (e.g., mode, spread) to the underlying,\nunobserved statistical model, which depends on external covariates. The\ninferential problem is addressed using Approximate Bayesian methods, mainly\nthrough a Gibbs sampler incorporating a quadratic approximation of the\nposterior distribution. Simulation studies and applications involving external\nvalidations are employed to evaluate the effectiveness of the proposed approach\nfor fuzzy data analysis. By reintegrating fuzzy data analysis into a more\ntraditional statistical framework, this work provides a significant step toward\nenhancing the interpretability and applicability of fuzzy statistical methods\nin many applicative contexts.",
        "Although very large wars remain an enduring threat in global politics, we\nlack a clear understanding of how some wars become large and costly, while most\ndo not. There are three possibilities: large conflicts start with and maintain\nintense fighting, they persist over a long duration, or they escalate in\nintensity over time. Using detailed within-conflict data on civil and\ninterstate wars 1946--2008, we show that escalation dynamics -- variations in\nfighting intensity within an armed conflict -- play a fundamental role in\nproducing large conflicts and are a generic feature of both civil and\ninterstate wars. However, civil wars tend to deescalate when they become very\nlarge, limiting their overall severity, while interstate wars exhibit a\npersistent risk of continual escalation. A non-parametric model demonstrates\nthat this distinction in escalation dynamics can explain the differences in the\nhistorical sizes of civil vs. interstate wars, and explain Richardson's Law\ngoverning the frequency and severity of interstate conflicts over the past 200\nyears. Escalation dynamics also drive enormous uncertainty in forecasting the\neventual sizes of both hypothetical and ongoing civil wars, indicating a need\nto better understand the causes of escalation and deescalation within\nconflicts. The close relationship between the size, and hence the cost, of an\narmed conflict and its potential for escalation has broad implications for\ntheories of conflict onset or termination and for risk assessment in\ninternational relations.",
        "We explore a family of numerical methods, based on the Steffensen divided\ndifference iterative algorithm, that do not evaluate the derivative of the\nobjective functions. The family of methods achieves second-order convergence\nwith two function evaluations per iteration with marginal additional\ncomputational cost. An important side benefit of the method is the improvement\nin stability for different initial conditions compared to the vanilla\nSteffensen method. We present numerical results for scalar functions, fields,\nand scalar fields. This family of methods outperforms the Steffensen method\nwith respect to standard quantitative metrics in most cases.",
        "This paper proposes an analytical framework for the orbital\nTarget-Attacker-Defender game with a non-maneuvering target along elliptic\norbits. Focusing on the linear quadratic game, we derive an analytical solution\nto the matrix Riccati equation, which yields analytical Nash-equilibrium\nstrategies for all players. Based on the analytical strategies, we derive the\nanalytical form of the necessary and sufficient winning conditions for the\nattacker. The simulation results show good consistency between the analytical\nand numerical methods, exhibiting 0.004$\\%$ relative error in the cost\nfunction. The analytical method achieves over 99.9$\\%$ reduction in CPU time\ncompared to the conventional numerical method, strengthening the advantage of\ndeveloping the analytical strategies. Furthermore, we verify the proposed\nwinning conditions and investigate the effects of eccentricity on the game\noutcomes. Our analysis reveals that for games with hovering initial states, the\ninitial position of the defender should be constrained inside a mathematically\ndefinable set to ensure that the attacker wins the game. This constrained set\nfurthermore permits geometric interpretation through our proposed framework.\nThis work establishes the analytical framework for orbital\nTarget-Attacker-Defender games, providing fundamental insights into the\nsolution analysis of the game.",
        "State-space models have been used in many applications, including\neconometrics, engineering, medical research, etc. The maximum likelihood\nestimation (MLE) of the static parameter of general state-space models is not\nstraightforward because the likelihood function is intractable. It is popular\nto use the sequential Monte Carlo(SMC) method to perform gradient ascent\noptimisation in either offline or online fashion. One problem with existing\nonline SMC methods for MLE is that the score estimators are inconsistent, i.e.\nthe bias does not vanish with increasing particle size. In this paper, two SMC\nalgorithms are proposed based on an importance sampling weight function to use\neach set of generated particles more efficiently. The first one is an offline\nalgorithm that locally approximates the likelihood function using importance\nsampling, where the locality is adapted by the effective sample size (ESS). The\nsecond one is a semi-online algorithm that has a computational cost linear in\nthe particle size and uses score estimators that are consistent. We study its\nconsistency and asymptotic normality. Their computational superiority is\nillustrated in numerical studies for long time series.",
        "In a previous work general conditions were given to prove the infiniteness of\nthe genus of certain towers of function fields over a perfect field. It was\nshown that many examples where particular cases of those general results. In\nthis paper the genus of a family of wild towers of function fields will be\nconsidered together with a result with less restrictive sufficient conditions\nfor a wild tower to have infinite genus."
      ]
    }
  },
  {
    "id":2411.05213,
    "research_type":"basic",
    "start_id":"b1",
    "start_title":"Microreactors gain wider use as alternative to batch production",
    "start_abstract":"The microreactors are gaining wide use among the pharmaceuticals and chemical companies as an alternative to batch production. They not only offers a flexible approach to continuous processing, but promises to save much of the time and effort consumed while expanding the chemistries at commercial scale. Most of the ten global pharma and chemical companies have acquired the Cytos Lab System,, a microreactor product developed by Cellular Process Chemistry Systems GmbH (CPC). Microreactors are comprised of plates with distinct channels in the submillimeter range, providing high surface-to-volume ratio, ultra fast mixing and high degree of control at all levels of production.",
    "start_categories":[
      "q-bio.OT"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Effect of bioclogging in porous media on complex conductivity signatures"
      ],
      "abstract":[
        "Flow through sand columns inoculated with Pseudomonas aeruginosa were used to investigate the effect of bioclogging on complex conductivity and flow transport properties. Complex (0.1\u20131000 Hz), bulk hydraulic (K), volumetric rate (Q), dispersivity (D), microbial cell concentrations monitored over time. Environmental scanning electron microscope images sands obtained at end experiment. Bioclogging resulting from increases in concentration production exopolymeric substances (EPS) had a large impact imaginary ( \u03c3 \u2033), K, Q, D, porosity (\u03a6). Changes electrical properties developed three stages: an initial stage 1 no significant changes all measured parameters (Days 1\u20138), which we attribute reversible irreversible attachment cells sand. In 2a 9\u201316), caused by growth biomass either as microcolonies filling pore throats and\/or uniform covering surfaces resulted maximum decrease Q but only moderate \u2033. 2b 17\u201324), EPS increase biofilm thickness higher \u2033 compared 2a. 3 25\u201332), reached quasi steady state insignificant are observed parameters. The results this study suggest that can provide complimentary information assessment porous media."
      ],
      "categories":[
        "math.MP"
      ]
    },
    "list":{
      "title":[
        "Reliability of coherent systems whose operating life is defined by the\n  lifetime and power of the components",
        "Semicomplete multipartite weakly distance-regular digraphs",
        "Left invertible quasi-isometric liftings",
        "On the compactness of the support of solitary waves of the complex\n  saturated nonlinear Schr{\\\"o}dinger equation and related problems",
        "Affineness on Noetherian graded rings, algebras and Hopf algebras",
        "On Robust Aggregation for Distributed Data",
        "Constrained polynomial roots and a modulated approach to Schur stability",
        "Biased branching random walks on Bienaym\\'e--Galton--Watson trees",
        "Distal Causal Excursion Effects: Modeling Long-Term Effects of\n  Time-Varying Treatments in Micro-Randomized Trials",
        "Lie Symmetry Analysis, Parametric Reduction and Conservation Laws of\n  (3+1) Dimensional Nonlinear Dispersive Soliton Equation",
        "Crossover from ballistic transport to normal diffusion: a kinetic view",
        "On Traces in Categories of Contractions (Extended Abstract)",
        "Local-global principle for isogenies of elliptic curves over quadratic\n  fields",
        "REALTALK: A 21-Day Real-World Dataset for Long-Term Conversation",
        "AI Load Dynamics--A Power Electronics Perspective",
        "In-sensor 24 classes HAR under 850 Bytes",
        "MapGS: Generalizable Pretraining and Data Augmentation for Online\n  Mapping via Novel View Synthesis",
        "MOVE: A Mixture-of-Vision-Encoders Approach for Domain-Focused\n  Vision-Language Processing",
        "Fine-Tuning Qwen 2.5 3B for Realistic Movie Dialogue Generation",
        "A Text-Based Knowledge-Embedded Soft Sensing Modeling Approach for\n  General Industrial Process Tasks Based on Large Language Model",
        "AdEval: Alignment-based Dynamic Evaluation to Mitigate Data\n  Contamination in Large Language Models",
        "A Novel Control Strategy for Offset Points Tracking in the Context of\n  Agricultural Robotics",
        "EFT & Species Scale: Friends or foes?",
        "ChartCoder: Advancing Multimodal Large Language Model for Chart-to-Code\n  Generation",
        "Intuitionistic modal logics: new and simpler decidability proofs for FIK\n  and LIK",
        "PlotEdit: Natural Language-Driven Accessible Chart Editing in PDFs via\n  Multimodal LLM Agents",
        "Enhancing Disinformation Detection with Explainable AI and Named Entity\n  Replacement",
        "Enhancing Highway Safety: Accident Detection on the A9 Test Stretch\n  Using Roadside Sensors"
      ],
      "abstract":[
        "We consider systems whose lifetime is measured by the time of physical\ndegradation of components, as well as the degree of power each component\ncontributes to the system. The lifetimes of the components of the system are\nrandom variables. The power that each component contributes to the system is\nthe product of a random variable and a time-decreasing stable function. The\noperational reliability of these systems is investigated and shown that it is\ndetermined by the joint lifetime functions of the order statistics and their\nconcomitants. In addition to general formulas, examples are given using some\nknown life distributions, and graphs of the operation life functions are shown.",
        "A digraph is semicomplete multipartite if its underlying graph is a complete\nmultipartite graph. As a special case of semicomplete multipartite digraphs,\nJ{\\o}rgensen et al. \\cite{JG14} initiated the study of doubly regular team\ntournaments. As a natural extension, we introduce doubly regular team\nsemicomplete multipartite digraphs and show that such digraphs fall into three\ntypes. Furthermore, we give a characterization of all semicomplete multipartite\ncommutative weakly distance-regular digraphs.",
        "Quasi-isometric liftings similar to isometries, for the operators similar to\ncontractions in Hilbert spaces, are investigated. The existence of such\nliftings is established, and their applications are explored for specific\noperator classes, including quasicontractions. A particular focus is placed on\noperators that admit left invertible minimal quasi-isometric liftings. These\noperators are characterized within the framework of $A$-contractions, and the\nmatrix structures of their liftings are analyzed, highlighting parallels with\nthe isometric liftings of contractions.",
        "We study the vectorial stationary Schr\\\"odinger equation $-\\Delta\nu+a\\,U+b\\,u=F,$ with a saturated nonlinearity $U=u\/|u|$ and with some complex\ncoefficients $(a,b)\\in\\mathbb{C}^2$. Besides the existence and uniqueness of\nsolutions for the Dirichlet and Neumann problems, we prove the compactness of\nthe support of the solution, under suitable conditions on $(a,b)$ and even when\nthe source in the right hand side $F(x)$ is not vanishing for large values of\n$|x|.$ The proof of the compactness of the support uses a local energy method,\ngiven the impossibility of applying the maximum principle. We also consider the\nassociate Schr\\\"{o}dinger-Poisson system when coupling with a simple magnetic\nfield. Among other consequences, our results give a rigorous proof of the\nexistence of ``solitons with compact support\" claimed, without any proof, by\nseveral previous authors.",
        "In this note, we show that every Noetherian graded ring with an affine degree\nzero part is affine. As a result, a Noetherian graded Hopf algebra whose degree\nzero component is a commutative or a cocommutative Hopf subalgebra is affine.\nMoreover, we show that the braided Hopf algebra of a Noetherian graded Hopf\nalgebra is affine.",
        "When data are stored across multiple locations, directly pooling all the data\ntogether for statistical analysis may be impossible due to communication costs\nand privacy concerns. Distributed computing systems allow the analysis of such\ndata, by getting local servers to separately process their own statistical\nanalyses and using a central processor to aggregate the local statistical\nresults. Naive aggregation of local statistics using simple or weighted\naverages, is vulnerable to contamination within a distributed computing system.\nThis paper develops and investigates a Huber-type aggregation method for\nlocally computed M-estimators to handle contamination in the local estimates.\nOur implementation of this aggregation method requires estimating the\nasymptotic variance-covariance matrix of the M-estimator, which we accomplish\nusing a robust spatial median approach. Theoretically, the Huber-type\naggregation achieves the same convergence rate as if all the data were pooled.\nWe establish its asymptotic normality for making inferences, including\njustifying a two-step approach for detecting contamination in the distributed\ncomputing system. Extensive simulation studies are conducted to validate the\ntheoretical results and the usefulness of our proposed approach is demonstrated\non U.S. airline data.",
        "It is common in stability analysis to linearize a system and investigate the\nspectrum of the Jacobian matrix. This approach faces the challenge of\ndetermining the matrix spectrum when the coefficients depend on parameters or\nwhen the characteristic polynomial is more than quartic. In this paper, we\nreverse the classical process and use the authors' work on global stability to\nfind sufficient conditions on the coefficients that ensure the zeros of the\ncharacteristic polynomial are in the open unit disk. This leads to an algorithm\nthat begins by testing the $\\ell_1$-norm of the polynomial, and if it is not\nless than two, perform an iteration process that can be implemented with\nmoderate effort. We give examples that show the effectiveness of our method\nwhen compared with the Jury's algorithm. Last, we formalize our constructions\nin terms of semialgebraic sets.",
        "We study $\\lambda$-biased branching random walks on\nBienaym\\'e--Galton--Watson trees in discrete time. We consider the maximal\ndisplacement at time $n$, $\\max_{\\vert u \\vert =n} \\vert X(u) \\vert$, and show\nthat it almost surely grows at a deterministic, linear speed. We characterize\nthis speed with the help of the large deviation rate function of the\n$\\lambda$-biased random walk of a single particle. A similar result is given\nfor the minimal displacement at time $n$, $\\min_{\\vert u \\vert =n} \\vert X(u)\n\\vert$.",
        "Micro-randomized trials (MRTs) play a crucial role in optimizing digital\ninterventions. In an MRT, each participant is sequentially randomized among\ntreatment options hundreds of times. While the interventions tested in MRTs\ntarget short-term behavioral responses (proximal outcomes), their ultimate goal\nis to drive long-term behavior change (distal outcomes). However, existing\ncausal inference methods, such as the causal excursion effect, are limited to\nproximal outcomes, making it challenging to quantify the long-term impact of\ninterventions. To address this gap, we introduce the distal causal excursion\neffect (DCEE), a novel estimand that quantifies the long-term effect of\ntime-varying treatments. The DCEE contrasts distal outcomes under two excursion\npolicies while marginalizing over most treatment assignments, enabling a\nparsimonious and interpretable causal model even with a large number of\ndecision points. We propose two estimators for the DCEE -- one with\ncross-fitting and one without -- both robust to misspecification of the outcome\nmodel. We establish their asymptotic properties and validate their performance\nthrough simulations. We apply our method to the HeartSteps MRT to assess the\nimpact of activity prompts on long-term habit formation. Our findings suggest\nthat prompts delivered earlier in the study have a stronger long-term effect\nthan those delivered later, underscoring the importance of intervention timing\nin behavior change. This work provides the critically needed toolkit for\nscientists working on digital interventions to assess long-term causal effects\nusing MRT data.",
        "The core focus of this research work is to obtain invariant solutions and\nconservation laws of the (3+1) dimensional ZK equation which describes the\nphenomenon of wave stability and solitons propagation. ZK equation is\nsignificant in fluid dynamics, nonlinear optics, and acoustics. ZK equation\nplays an important role in understanding the behavior of nonlinear, dispersive\nwaves especially in the presence of the magnetic field. Lie symmetry analysis\nhas been applied to the (3+1)-dimensional ZK equation to derive invariant\nsolutions. These solutions disclose how waves retain their shape as they\ntravel, how they interact in space, and the impact of magnetic fields on wave\npropagation. Using the Lie symmetry, (3+1)-dimensional ZK equation reduces into\nvarious ordinary differential equations. It also concludes with the\nconservation laws and non-linear self adjoint-ness property for the\n(3+1)-dimensional ZK Equation.",
        "The crossover between dispersion patterns has been frequently observed in\nvarious systems. Inspired by the pathway-based kinetic model for E. coli\nchemotaxis that accounts for the intracellular adaptation process and noise, we\npropose a kinetic model that can exhibit a crossover from ballistic transport\nto normal diffusion at the population level. At the particle level, this\nframework aligns with a stochastic individual-based model. Using numerical\nsimulations and rigorous asymptotic analysis, we demonstrate this crossover\nboth analytically and computationally. Notably, under suitable scaling, the\nmodel reveals two distinct limits in which the macroscopic densities exhibit\neither ballistic transport or normal diffusion.",
        "Traced monoidal categories are used to model processes that can feed their\noutputs back to their own inputs, abstracting iteration. The category of finite\ndimensional Hilbert spaces with the direct sum tensor is not traced. But\nsurprisingly, in 2014, Bartha showed that the monoidal subcategory of\nisometries is traced. The same holds for coisometries, unitary maps, and\ncontractions. This suggests the possibility of feeding outputs of quantum\nprocesses back to their own inputs, analogous to iteration. In this paper, we\nshow that Bartha's result is not specifically tied to Hilbert spaces, but works\nin any dagger additive category with Moore-Penrose pseudoinverses (a natural\ndagger-categorical generalization of inverses).",
        "In this paper, we prove that the local-global principle of $11$-isogenies for\nelliptic curves over quadratic fields does not fail. This gives a positive\nanswer to a conjecture by Banwait and Cremona. The proof is based on the\ndetermination of the set of quadratic points on the modular curve\n$X_{D_{10}}(11)$.",
        "Long-term, open-domain dialogue capabilities are essential for chatbots\naiming to recall past interactions and demonstrate emotional intelligence (EI).\nYet, most existing research relies on synthetic, LLM-generated data, leaving\nopen questions about real-world conversational patterns. To address this gap,\nwe introduce REALTALK, a 21-day corpus of authentic messaging app dialogues,\nproviding a direct benchmark against genuine human interactions.\n  We first conduct a dataset analysis, focusing on EI attributes and persona\nconsistency to understand the unique challenges posed by real-world dialogues.\nBy comparing with LLM-generated conversations, we highlight key differences,\nincluding diverse emotional expressions and variations in persona stability\nthat synthetic dialogues often fail to capture.\n  Building on these insights, we introduce two benchmark tasks: (1) persona\nsimulation where a model continues a conversation on behalf of a specific user\ngiven prior dialogue context; and (2) memory probing where a model answers\ntargeted questions requiring long-term memory of past interactions.\n  Our findings reveal that models struggle to simulate a user solely from\ndialogue history, while fine-tuning on specific user chats improves persona\nemulation. Additionally, existing models face significant challenges in\nrecalling and leveraging long-term context within real-world conversations.",
        "As AI-driven computing infrastructures rapidly scale, discussions around data\ncenter design often emphasize energy consumption, water and electricity usage,\nworkload scheduling, and thermal management. However, these perspectives often\noverlook the critical interplay between AI-specific load transients and power\nelectronics. This paper addresses that gap by examining how large-scale AI\nworkloads impose unique demands on power conversion chains and, in turn, how\nthe power electronics themselves shape the dynamic behavior of AI-based\ninfrastructure. We illustrate the fundamental constraints imposed by\nmulti-stage power conversion architectures and highlight the key role of\nfinal-stage modules in defining realistic power slew rates for GPU clusters.\nOur analysis shows that traditional designs, optimized for slower-varying or\nCPU-centric workloads, may not adequately accommodate the rapid load ramps and\ndrops characteristic of AI accelerators. To bridge this gap, we present\ninsights into advanced converter topologies, hierarchical control methods, and\nenergy buffering techniques that collectively enable robust and efficient power\ndelivery. By emphasizing the bidirectional influence between AI workloads and\npower electronics, we hope this work can set a good starting point and offer\npractical design considerations to ensure future exascale-capable data centers\ncan meet the stringent performance, reliability, and scalability requirements\nof next-generation AI deployments.",
        "The year 2023 was a key year for tinyML unleashing a new age of intelligent\nsensors pushing intelligence from the MCU into the source of the data at the\nsensor level, enabling them to perform sophisticated algorithms and machine\nlearning models in real-time. This study presents an innovative approach to\nHuman Activity Recognition (HAR) using Intelligent Sensor Processing Units\n(ISPUs), demonstrating the feasibility of deploying complex machine learning\nmodels directly on ultra-constrained sensor hardware. We developed a 24-class\nHAR model achieving 85\\% accuracy while operating within an 850-byte stack\nmemory limit. The model processes accelerometer and gyroscope data in real\ntime, reducing latency, enhancing data privacy, and consuming only 0.5 mA of\npower. To address memory constraints, we employed incremental class injection\nand feature optimization techniques, enabling scalability without compromising\nperformance. This work underscores the transformative potential of on-sensor\nprocessing for applications in healthcare, predictive maintenance, and smart\nenvironments, while introducing a publicly available, diverse HAR dataset for\nfurther research. Future efforts will explore advanced compression techniques\nand broader IoT integration to push the boundaries of TinyML on constrained\ndevices.",
        "Online mapping reduces the reliance of autonomous vehicles on high-definition\n(HD) maps, significantly enhancing scalability. However, recent advancements\noften overlook cross-sensor configuration generalization, leading to\nperformance degradation when models are deployed on vehicles with different\ncamera intrinsics and extrinsics. With the rapid evolution of novel view\nsynthesis methods, we investigate the extent to which these techniques can be\nleveraged to address the sensor configuration generalization challenge. We\npropose a novel framework leveraging Gaussian splatting to reconstruct scenes\nand render camera images in target sensor configurations. The target config\nsensor data, along with labels mapped to the target config, are used to train\nonline mapping models. Our proposed framework on the nuScenes and Argoverse 2\ndatasets demonstrates a performance improvement of 18% through effective\ndataset augmentation, achieves faster convergence and efficient training, and\nexceeds state-of-the-art performance when using only 25% of the original\ntraining data. This enables data reuse and reduces the need for laborious data\nlabeling. Project page at https:\/\/henryzhangzhy.github.io\/mapgs.",
        "Multimodal language models (MLMs) integrate visual and textual information by\ncoupling a vision encoder with a large language model through the specific\nadapter. While existing approaches commonly rely on a single pre-trained vision\nencoder, there is a great variability of specialized encoders that can boost\nmodel's performance in distinct domains. In this work, we propose MOVE (Mixture\nof Vision Encoders) a simple yet effective approach to leverage multiple\npre-trained encoders for specialized multimodal tasks. MOVE automatically\nroutes inputs to the most appropriate encoder among candidates such as Unichat,\nInternViT, and Texify, thereby enhancing performance across a diverse set of\nbenchmarks, including ChartQA, MMBench, and MMMU. Experimental results\ndemonstrate that MOVE achieves competitive accuracy without incurring the\ncomplexities of image slicing for high-resolution images.",
        "The Qwen 2.5 3B base model was fine-tuned to generate contextually rich and\nengaging movie dialogue, leveraging the Cornell Movie-Dialog Corpus, a curated\ndataset of movie conversations. Due to the limitations in GPU computing and\nVRAM, the training process began with the 0.5B model progressively scaling up\nto the 1.5B and 3B versions as efficiency improvements were implemented. The\nQwen 2.5 series, developed by Alibaba Group, stands at the forefront of small\nopen-source pre-trained models, particularly excelling in creative tasks\ncompared to alternatives like Meta's Llama 3.2 and Google's Gemma. Results\ndemonstrate the ability of small models to produce high-quality, realistic\ndialogue, offering a promising approach for real-time, context-sensitive\nconversation generation.",
        "Data-driven soft sensors (DDSS) have become mainstream methods for predicting\nkey performance indicators in process industries. However, DDSS development\nrequires complex and costly customized designs tailored to various tasks during\nthe modeling process. Moreover, DDSS are constrained to a single structured\ndata modality, limiting their ability to incorporate additional contextual\nknowledge. Furthermore, DDSSs' limited representation learning leads to weak\npredictive performance with scarce data. To address these challenges, we\npropose a general framework named LLM-TKESS (large language model for\ntext-based knowledge-embedded soft sensing), harnessing the powerful general\nproblem-solving capabilities, cross-modal knowledge transfer abilities, and\nfew-shot capabilities of LLM for enhanced soft sensing modeling. Specifically,\nan auxiliary variable series encoder (AVS Encoder) is proposed to unleash LLM's\npotential for capturing temporal relationships within series and spatial\nsemantic relationships among auxiliary variables. Then, we propose a two-stage\nfine-tuning alignment strategy: in the first stage, employing\nparameter-efficient fine-tuning through autoregressive training adjusts LLM to\nrapidly accommodate process variable data, resulting in a soft sensing\nfoundation model (SSFM). Subsequently, by training adapters, we adapt the SSFM\nto various downstream tasks without modifying its architecture. Then, we\npropose two text-based knowledge-embedded soft sensors, integrating new natural\nlanguage modalities to overcome the limitations of pure structured data models.\nFurthermore, benefiting from LLM's pre-existing world knowledge, our model\ndemonstrates outstanding predictive capabilities in small sample conditions.\nUsing the thermal deformation of air preheater rotor as a case study, we\nvalidate through extensive experiments that LLM-TKESS exhibits outstanding\nperformance.",
        "As Large Language Models (LLMs) are pretrained on massive-scale corpora, the\nissue of data contamination has become increasingly severe, leading to\npotential overestimation of model performance during evaluation. To address\nthis, we propose AdEval (Alignment-based Dynamic Evaluation), a dynamic data\nevaluation method aimed at mitigating the impact of data contamination on\nevaluation reliability. Experimental results on multiple datasets demonstrate\nthat AdEval effectively reduces the impact of data contamination on evaluation\noutcomes, enhancing both the fairness and reliability of the evaluation\nprocess.",
        "In this paper, we present a novel method to control a rigidly connected\nlocation on the vehicle, such as a point on the implement in case of\nagricultural tasks. Agricultural robots are transforming modern farming by\nenabling precise and efficient operations, replacing humans in arduous tasks\nwhile reducing the use of chemicals. Traditionnaly, path_following algorithms\nare designed to guide the vehicle's center along a predefined trajetory.\nHowever, since the actual agronomic task is performed by the implement, it is\nessential to control a specific point on the implement itself rather than\nvehicle's center. As such, we present in this paper two approaches for\nachieving the control of an offset point on the robot. The first approach\nadapts existing control laws, initially inteded for rear axle's midpoint, to\nmanage the desired lateral deviation. The second approach employs backstepping\ncontrol techniques to create a control law that directly targets the implement.\nWe conduct real-world experiments, highlighting the limitations of traditional\napproaches for offset points control, and demonstrating the strengths and\nweaknesses of the proposed methods.",
        "Recently the notion that quantum gravity effects could manifest at scales\nmuch lower than the Planck scale has seen an intense Swamplandish revival.\nDozens of works have explored how the so-called species scale -- at which an\neffective description of gravity must break down -- relates to String Theory\nand the Swampland conjectures. In particular, the interplay between this scale\nand the abundant towers of states becoming lighter in asymptotic regions of\nmoduli spaces has proved to be key in understanding the real scale of quantum\ngravity. Nevertheless concerns have been raised regarding the validity of using\ninfinite towers of states when estimating this scale within Effective Field\nTheory and, more precisely, the consistency of cutting the tower part way\nthrough in a framework that relies on a clear separation of scales. In this\nwork we take an EFT point-of-view and provide a detailed perturbative\nderivation of the species scale -- by computing the 1-loop graviton propagator\nin the presence of many fields -- thereby clarifying common sources of\nconfusion in the literature. Not only do we clarify the setup, assumptions and\nregimes of validity of the result, but more importantly apply the same methods\nto an infinite tower of states. We show how each state in the tower contributes\nto the species scale and how the procedure of counting only ''light fields''\ncan be compatible with not cutting the tower, thereby maintaining the harmony\nbetween infinite towers and EFTs even in the context of the species scale.",
        "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in chart understanding tasks. However, interpreting charts with\ntextual descriptions often leads to information loss, as it fails to fully\ncapture the dense information embedded in charts. In contrast, parsing charts\ninto code provides lossless representations that can effectively contain all\ncritical details. Although existing open-source MLLMs have achieved success in\nchart understanding tasks, they still face two major challenges when applied to\nchart-to-code tasks.: (1) Low executability and poor restoration of chart\ndetails in the generated code and (2) Lack of large-scale and diverse training\ndata. To address these challenges, we propose \\textbf{ChartCoder}, the first\ndedicated chart-to-code MLLM, which leverages Code LLMs as the language\nbackbone to enhance the executability of the generated code. Furthermore, we\nintroduce \\textbf{Chart2Code-160k}, the first large-scale and diverse dataset\nfor chart-to-code generation, and propose the \\textbf{Snippet-of-Thought (SoT)}\nmethod, which transforms direct chart-to-code generation data into step-by-step\ngeneration. Experiments demonstrate that ChartCoder, with only 7B parameters,\nsurpasses existing open-source MLLMs on chart-to-code benchmarks, achieving\nsuperior chart restoration and code excitability. Our code will be available at\nhttps:\/\/github.com\/thunlp\/ChartCoder.",
        "In this note, by integrating ideas concerning terminating tableaux-based\nprocedures in modal logics and finite frame property of intuitionistic modal\nlogic IK, we provide new and simpler decidability proofs for FIK and LIK.",
        "Chart visualizations, while essential for data interpretation and\ncommunication, are predominantly accessible only as images in PDFs, lacking\nsource data tables and stylistic information. To enable effective editing of\ncharts in PDFs or digital scans, we present PlotEdit, a novel multi-agent\nframework for natural language-driven end-to-end chart image editing via\nself-reflective LLM agents. PlotEdit orchestrates five LLM agents: (1)\nChart2Table for data table extraction, (2) Chart2Vision for style attribute\nidentification, (3) Chart2Code for retrieving rendering code, (4) Instruction\nDecomposition Agent for parsing user requests into executable steps, and (5)\nMultimodal Editing Agent for implementing nuanced chart component modifications\n- all coordinated through multimodal feedback to maintain visual fidelity.\nPlotEdit outperforms existing baselines on the ChartCraft dataset across style,\nlayout, format, and data-centric edits, enhancing accessibility for visually\nchallenged users and improving novice productivity.",
        "The automatic detection of disinformation presents a significant challenge in\nthe field of natural language processing. This task addresses a multifaceted\nsocietal and communication issue, which needs approaches that extend beyond the\nidentification of general linguistic patterns through data-driven algorithms.\nIn this research work, we hypothesise that text classification methods are not\nable to capture the nuances of disinformation and they often ground their\ndecision in superfluous features. Hence, we apply a post-hoc explainability\nmethod (SHAP, SHapley Additive exPlanations) to identify spurious elements with\nhigh impact on the classification models. Our findings show that\nnon-informative elements (e.g., URLs and emoticons) should be removed and named\nentities (e.g., Rwanda) should be pseudo-anonymized before training to avoid\nmodels' bias and increase their generalization capabilities. We evaluate this\nmethodology with internal dataset and external dataset before and after\napplying extended data preprocessing and named entity replacement. The results\nshow that our proposal enhances on average the performance of a disinformation\nclassification method with external test data in 65.78% without a significant\ndecrease of the internal test performance.",
        "Road traffic injuries are the leading cause of death for people aged 5-29,\nresulting in about 1.19 million deaths each year. To reduce these fatalities,\nit is essential to address human errors like speeding, drunk driving, and\ndistractions. Additionally, faster accident detection and quicker medical\nresponse can help save lives. We propose an accident detection framework that\ncombines a rule-based approach with a learning-based one. We introduce a\ndataset of real-world highway accidents featuring high-speed crash sequences.\nIt includes 294,924 labeled 2D boxes, 93,012 labeled 3D boxes, and track IDs\nacross 48,144 frames captured at 10 Hz using four roadside cameras and LiDAR\nsensors. The dataset covers ten object classes and is released in the OpenLABEL\nformat. Our experiments and analysis demonstrate the reliability of our method."
      ]
    }
  },
  {
    "id":2411.05213,
    "research_type":"basic",
    "start_id":"b8",
    "start_title":"How flocculation can explain coexistence in the chemostat",
    "start_abstract":"We study a chemostat model in which two microbial species grow on single resource. show that coexistence is possible when the would normally win exclusive competition aggregates flocs. Our mathematical analysis exploits fact flocculation fast compared to biological growth, common hypothesis floc models. A numerical shows validity of this approach large parameter range. indicate how our yields mechanistic justification for so-called density-dependent growth.",
    "start_categories":[
      "q-bio.OT"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Effect of bioclogging in porous media on complex conductivity signatures"
      ],
      "abstract":[
        "Flow through sand columns inoculated with Pseudomonas aeruginosa were used to investigate the effect of bioclogging on complex conductivity and flow transport properties. Complex (0.1\u20131000 Hz), bulk hydraulic (K), volumetric rate (Q), dispersivity (D), microbial cell concentrations monitored over time. Environmental scanning electron microscope images sands obtained at end experiment. Bioclogging resulting from increases in concentration production exopolymeric substances (EPS) had a large impact imaginary ( \u03c3 \u2033), K, Q, D, porosity (\u03a6). Changes electrical properties developed three stages: an initial stage 1 no significant changes all measured parameters (Days 1\u20138), which we attribute reversible irreversible attachment cells sand. In 2a 9\u201316), caused by growth biomass either as microcolonies filling pore throats and\/or uniform covering surfaces resulted maximum decrease Q but only moderate \u2033. 2b 17\u201324), EPS increase biofilm thickness higher \u2033 compared 2a. 3 25\u201332), reached quasi steady state insignificant are observed parameters. The results this study suggest that can provide complimentary information assessment porous media."
      ],
      "categories":[
        "math.MP"
      ]
    },
    "list":{
      "title":[
        "Reliability of coherent systems whose operating life is defined by the\n  lifetime and power of the components",
        "Semicomplete multipartite weakly distance-regular digraphs",
        "Left invertible quasi-isometric liftings",
        "On the compactness of the support of solitary waves of the complex\n  saturated nonlinear Schr{\\\"o}dinger equation and related problems",
        "Affineness on Noetherian graded rings, algebras and Hopf algebras",
        "On Robust Aggregation for Distributed Data",
        "Constrained polynomial roots and a modulated approach to Schur stability",
        "Biased branching random walks on Bienaym\\'e--Galton--Watson trees",
        "Distal Causal Excursion Effects: Modeling Long-Term Effects of\n  Time-Varying Treatments in Micro-Randomized Trials",
        "Lie Symmetry Analysis, Parametric Reduction and Conservation Laws of\n  (3+1) Dimensional Nonlinear Dispersive Soliton Equation",
        "Crossover from ballistic transport to normal diffusion: a kinetic view",
        "On Traces in Categories of Contractions (Extended Abstract)",
        "Local-global principle for isogenies of elliptic curves over quadratic\n  fields",
        "REALTALK: A 21-Day Real-World Dataset for Long-Term Conversation",
        "AI Load Dynamics--A Power Electronics Perspective",
        "In-sensor 24 classes HAR under 850 Bytes",
        "MapGS: Generalizable Pretraining and Data Augmentation for Online\n  Mapping via Novel View Synthesis",
        "MOVE: A Mixture-of-Vision-Encoders Approach for Domain-Focused\n  Vision-Language Processing",
        "Fine-Tuning Qwen 2.5 3B for Realistic Movie Dialogue Generation",
        "A Text-Based Knowledge-Embedded Soft Sensing Modeling Approach for\n  General Industrial Process Tasks Based on Large Language Model",
        "AdEval: Alignment-based Dynamic Evaluation to Mitigate Data\n  Contamination in Large Language Models",
        "A Novel Control Strategy for Offset Points Tracking in the Context of\n  Agricultural Robotics",
        "EFT & Species Scale: Friends or foes?",
        "ChartCoder: Advancing Multimodal Large Language Model for Chart-to-Code\n  Generation",
        "Intuitionistic modal logics: new and simpler decidability proofs for FIK\n  and LIK",
        "PlotEdit: Natural Language-Driven Accessible Chart Editing in PDFs via\n  Multimodal LLM Agents",
        "Enhancing Disinformation Detection with Explainable AI and Named Entity\n  Replacement",
        "Enhancing Highway Safety: Accident Detection on the A9 Test Stretch\n  Using Roadside Sensors"
      ],
      "abstract":[
        "We consider systems whose lifetime is measured by the time of physical\ndegradation of components, as well as the degree of power each component\ncontributes to the system. The lifetimes of the components of the system are\nrandom variables. The power that each component contributes to the system is\nthe product of a random variable and a time-decreasing stable function. The\noperational reliability of these systems is investigated and shown that it is\ndetermined by the joint lifetime functions of the order statistics and their\nconcomitants. In addition to general formulas, examples are given using some\nknown life distributions, and graphs of the operation life functions are shown.",
        "A digraph is semicomplete multipartite if its underlying graph is a complete\nmultipartite graph. As a special case of semicomplete multipartite digraphs,\nJ{\\o}rgensen et al. \\cite{JG14} initiated the study of doubly regular team\ntournaments. As a natural extension, we introduce doubly regular team\nsemicomplete multipartite digraphs and show that such digraphs fall into three\ntypes. Furthermore, we give a characterization of all semicomplete multipartite\ncommutative weakly distance-regular digraphs.",
        "Quasi-isometric liftings similar to isometries, for the operators similar to\ncontractions in Hilbert spaces, are investigated. The existence of such\nliftings is established, and their applications are explored for specific\noperator classes, including quasicontractions. A particular focus is placed on\noperators that admit left invertible minimal quasi-isometric liftings. These\noperators are characterized within the framework of $A$-contractions, and the\nmatrix structures of their liftings are analyzed, highlighting parallels with\nthe isometric liftings of contractions.",
        "We study the vectorial stationary Schr\\\"odinger equation $-\\Delta\nu+a\\,U+b\\,u=F,$ with a saturated nonlinearity $U=u\/|u|$ and with some complex\ncoefficients $(a,b)\\in\\mathbb{C}^2$. Besides the existence and uniqueness of\nsolutions for the Dirichlet and Neumann problems, we prove the compactness of\nthe support of the solution, under suitable conditions on $(a,b)$ and even when\nthe source in the right hand side $F(x)$ is not vanishing for large values of\n$|x|.$ The proof of the compactness of the support uses a local energy method,\ngiven the impossibility of applying the maximum principle. We also consider the\nassociate Schr\\\"{o}dinger-Poisson system when coupling with a simple magnetic\nfield. Among other consequences, our results give a rigorous proof of the\nexistence of ``solitons with compact support\" claimed, without any proof, by\nseveral previous authors.",
        "In this note, we show that every Noetherian graded ring with an affine degree\nzero part is affine. As a result, a Noetherian graded Hopf algebra whose degree\nzero component is a commutative or a cocommutative Hopf subalgebra is affine.\nMoreover, we show that the braided Hopf algebra of a Noetherian graded Hopf\nalgebra is affine.",
        "When data are stored across multiple locations, directly pooling all the data\ntogether for statistical analysis may be impossible due to communication costs\nand privacy concerns. Distributed computing systems allow the analysis of such\ndata, by getting local servers to separately process their own statistical\nanalyses and using a central processor to aggregate the local statistical\nresults. Naive aggregation of local statistics using simple or weighted\naverages, is vulnerable to contamination within a distributed computing system.\nThis paper develops and investigates a Huber-type aggregation method for\nlocally computed M-estimators to handle contamination in the local estimates.\nOur implementation of this aggregation method requires estimating the\nasymptotic variance-covariance matrix of the M-estimator, which we accomplish\nusing a robust spatial median approach. Theoretically, the Huber-type\naggregation achieves the same convergence rate as if all the data were pooled.\nWe establish its asymptotic normality for making inferences, including\njustifying a two-step approach for detecting contamination in the distributed\ncomputing system. Extensive simulation studies are conducted to validate the\ntheoretical results and the usefulness of our proposed approach is demonstrated\non U.S. airline data.",
        "It is common in stability analysis to linearize a system and investigate the\nspectrum of the Jacobian matrix. This approach faces the challenge of\ndetermining the matrix spectrum when the coefficients depend on parameters or\nwhen the characteristic polynomial is more than quartic. In this paper, we\nreverse the classical process and use the authors' work on global stability to\nfind sufficient conditions on the coefficients that ensure the zeros of the\ncharacteristic polynomial are in the open unit disk. This leads to an algorithm\nthat begins by testing the $\\ell_1$-norm of the polynomial, and if it is not\nless than two, perform an iteration process that can be implemented with\nmoderate effort. We give examples that show the effectiveness of our method\nwhen compared with the Jury's algorithm. Last, we formalize our constructions\nin terms of semialgebraic sets.",
        "We study $\\lambda$-biased branching random walks on\nBienaym\\'e--Galton--Watson trees in discrete time. We consider the maximal\ndisplacement at time $n$, $\\max_{\\vert u \\vert =n} \\vert X(u) \\vert$, and show\nthat it almost surely grows at a deterministic, linear speed. We characterize\nthis speed with the help of the large deviation rate function of the\n$\\lambda$-biased random walk of a single particle. A similar result is given\nfor the minimal displacement at time $n$, $\\min_{\\vert u \\vert =n} \\vert X(u)\n\\vert$.",
        "Micro-randomized trials (MRTs) play a crucial role in optimizing digital\ninterventions. In an MRT, each participant is sequentially randomized among\ntreatment options hundreds of times. While the interventions tested in MRTs\ntarget short-term behavioral responses (proximal outcomes), their ultimate goal\nis to drive long-term behavior change (distal outcomes). However, existing\ncausal inference methods, such as the causal excursion effect, are limited to\nproximal outcomes, making it challenging to quantify the long-term impact of\ninterventions. To address this gap, we introduce the distal causal excursion\neffect (DCEE), a novel estimand that quantifies the long-term effect of\ntime-varying treatments. The DCEE contrasts distal outcomes under two excursion\npolicies while marginalizing over most treatment assignments, enabling a\nparsimonious and interpretable causal model even with a large number of\ndecision points. We propose two estimators for the DCEE -- one with\ncross-fitting and one without -- both robust to misspecification of the outcome\nmodel. We establish their asymptotic properties and validate their performance\nthrough simulations. We apply our method to the HeartSteps MRT to assess the\nimpact of activity prompts on long-term habit formation. Our findings suggest\nthat prompts delivered earlier in the study have a stronger long-term effect\nthan those delivered later, underscoring the importance of intervention timing\nin behavior change. This work provides the critically needed toolkit for\nscientists working on digital interventions to assess long-term causal effects\nusing MRT data.",
        "The core focus of this research work is to obtain invariant solutions and\nconservation laws of the (3+1) dimensional ZK equation which describes the\nphenomenon of wave stability and solitons propagation. ZK equation is\nsignificant in fluid dynamics, nonlinear optics, and acoustics. ZK equation\nplays an important role in understanding the behavior of nonlinear, dispersive\nwaves especially in the presence of the magnetic field. Lie symmetry analysis\nhas been applied to the (3+1)-dimensional ZK equation to derive invariant\nsolutions. These solutions disclose how waves retain their shape as they\ntravel, how they interact in space, and the impact of magnetic fields on wave\npropagation. Using the Lie symmetry, (3+1)-dimensional ZK equation reduces into\nvarious ordinary differential equations. It also concludes with the\nconservation laws and non-linear self adjoint-ness property for the\n(3+1)-dimensional ZK Equation.",
        "The crossover between dispersion patterns has been frequently observed in\nvarious systems. Inspired by the pathway-based kinetic model for E. coli\nchemotaxis that accounts for the intracellular adaptation process and noise, we\npropose a kinetic model that can exhibit a crossover from ballistic transport\nto normal diffusion at the population level. At the particle level, this\nframework aligns with a stochastic individual-based model. Using numerical\nsimulations and rigorous asymptotic analysis, we demonstrate this crossover\nboth analytically and computationally. Notably, under suitable scaling, the\nmodel reveals two distinct limits in which the macroscopic densities exhibit\neither ballistic transport or normal diffusion.",
        "Traced monoidal categories are used to model processes that can feed their\noutputs back to their own inputs, abstracting iteration. The category of finite\ndimensional Hilbert spaces with the direct sum tensor is not traced. But\nsurprisingly, in 2014, Bartha showed that the monoidal subcategory of\nisometries is traced. The same holds for coisometries, unitary maps, and\ncontractions. This suggests the possibility of feeding outputs of quantum\nprocesses back to their own inputs, analogous to iteration. In this paper, we\nshow that Bartha's result is not specifically tied to Hilbert spaces, but works\nin any dagger additive category with Moore-Penrose pseudoinverses (a natural\ndagger-categorical generalization of inverses).",
        "In this paper, we prove that the local-global principle of $11$-isogenies for\nelliptic curves over quadratic fields does not fail. This gives a positive\nanswer to a conjecture by Banwait and Cremona. The proof is based on the\ndetermination of the set of quadratic points on the modular curve\n$X_{D_{10}}(11)$.",
        "Long-term, open-domain dialogue capabilities are essential for chatbots\naiming to recall past interactions and demonstrate emotional intelligence (EI).\nYet, most existing research relies on synthetic, LLM-generated data, leaving\nopen questions about real-world conversational patterns. To address this gap,\nwe introduce REALTALK, a 21-day corpus of authentic messaging app dialogues,\nproviding a direct benchmark against genuine human interactions.\n  We first conduct a dataset analysis, focusing on EI attributes and persona\nconsistency to understand the unique challenges posed by real-world dialogues.\nBy comparing with LLM-generated conversations, we highlight key differences,\nincluding diverse emotional expressions and variations in persona stability\nthat synthetic dialogues often fail to capture.\n  Building on these insights, we introduce two benchmark tasks: (1) persona\nsimulation where a model continues a conversation on behalf of a specific user\ngiven prior dialogue context; and (2) memory probing where a model answers\ntargeted questions requiring long-term memory of past interactions.\n  Our findings reveal that models struggle to simulate a user solely from\ndialogue history, while fine-tuning on specific user chats improves persona\nemulation. Additionally, existing models face significant challenges in\nrecalling and leveraging long-term context within real-world conversations.",
        "As AI-driven computing infrastructures rapidly scale, discussions around data\ncenter design often emphasize energy consumption, water and electricity usage,\nworkload scheduling, and thermal management. However, these perspectives often\noverlook the critical interplay between AI-specific load transients and power\nelectronics. This paper addresses that gap by examining how large-scale AI\nworkloads impose unique demands on power conversion chains and, in turn, how\nthe power electronics themselves shape the dynamic behavior of AI-based\ninfrastructure. We illustrate the fundamental constraints imposed by\nmulti-stage power conversion architectures and highlight the key role of\nfinal-stage modules in defining realistic power slew rates for GPU clusters.\nOur analysis shows that traditional designs, optimized for slower-varying or\nCPU-centric workloads, may not adequately accommodate the rapid load ramps and\ndrops characteristic of AI accelerators. To bridge this gap, we present\ninsights into advanced converter topologies, hierarchical control methods, and\nenergy buffering techniques that collectively enable robust and efficient power\ndelivery. By emphasizing the bidirectional influence between AI workloads and\npower electronics, we hope this work can set a good starting point and offer\npractical design considerations to ensure future exascale-capable data centers\ncan meet the stringent performance, reliability, and scalability requirements\nof next-generation AI deployments.",
        "The year 2023 was a key year for tinyML unleashing a new age of intelligent\nsensors pushing intelligence from the MCU into the source of the data at the\nsensor level, enabling them to perform sophisticated algorithms and machine\nlearning models in real-time. This study presents an innovative approach to\nHuman Activity Recognition (HAR) using Intelligent Sensor Processing Units\n(ISPUs), demonstrating the feasibility of deploying complex machine learning\nmodels directly on ultra-constrained sensor hardware. We developed a 24-class\nHAR model achieving 85\\% accuracy while operating within an 850-byte stack\nmemory limit. The model processes accelerometer and gyroscope data in real\ntime, reducing latency, enhancing data privacy, and consuming only 0.5 mA of\npower. To address memory constraints, we employed incremental class injection\nand feature optimization techniques, enabling scalability without compromising\nperformance. This work underscores the transformative potential of on-sensor\nprocessing for applications in healthcare, predictive maintenance, and smart\nenvironments, while introducing a publicly available, diverse HAR dataset for\nfurther research. Future efforts will explore advanced compression techniques\nand broader IoT integration to push the boundaries of TinyML on constrained\ndevices.",
        "Online mapping reduces the reliance of autonomous vehicles on high-definition\n(HD) maps, significantly enhancing scalability. However, recent advancements\noften overlook cross-sensor configuration generalization, leading to\nperformance degradation when models are deployed on vehicles with different\ncamera intrinsics and extrinsics. With the rapid evolution of novel view\nsynthesis methods, we investigate the extent to which these techniques can be\nleveraged to address the sensor configuration generalization challenge. We\npropose a novel framework leveraging Gaussian splatting to reconstruct scenes\nand render camera images in target sensor configurations. The target config\nsensor data, along with labels mapped to the target config, are used to train\nonline mapping models. Our proposed framework on the nuScenes and Argoverse 2\ndatasets demonstrates a performance improvement of 18% through effective\ndataset augmentation, achieves faster convergence and efficient training, and\nexceeds state-of-the-art performance when using only 25% of the original\ntraining data. This enables data reuse and reduces the need for laborious data\nlabeling. Project page at https:\/\/henryzhangzhy.github.io\/mapgs.",
        "Multimodal language models (MLMs) integrate visual and textual information by\ncoupling a vision encoder with a large language model through the specific\nadapter. While existing approaches commonly rely on a single pre-trained vision\nencoder, there is a great variability of specialized encoders that can boost\nmodel's performance in distinct domains. In this work, we propose MOVE (Mixture\nof Vision Encoders) a simple yet effective approach to leverage multiple\npre-trained encoders for specialized multimodal tasks. MOVE automatically\nroutes inputs to the most appropriate encoder among candidates such as Unichat,\nInternViT, and Texify, thereby enhancing performance across a diverse set of\nbenchmarks, including ChartQA, MMBench, and MMMU. Experimental results\ndemonstrate that MOVE achieves competitive accuracy without incurring the\ncomplexities of image slicing for high-resolution images.",
        "The Qwen 2.5 3B base model was fine-tuned to generate contextually rich and\nengaging movie dialogue, leveraging the Cornell Movie-Dialog Corpus, a curated\ndataset of movie conversations. Due to the limitations in GPU computing and\nVRAM, the training process began with the 0.5B model progressively scaling up\nto the 1.5B and 3B versions as efficiency improvements were implemented. The\nQwen 2.5 series, developed by Alibaba Group, stands at the forefront of small\nopen-source pre-trained models, particularly excelling in creative tasks\ncompared to alternatives like Meta's Llama 3.2 and Google's Gemma. Results\ndemonstrate the ability of small models to produce high-quality, realistic\ndialogue, offering a promising approach for real-time, context-sensitive\nconversation generation.",
        "Data-driven soft sensors (DDSS) have become mainstream methods for predicting\nkey performance indicators in process industries. However, DDSS development\nrequires complex and costly customized designs tailored to various tasks during\nthe modeling process. Moreover, DDSS are constrained to a single structured\ndata modality, limiting their ability to incorporate additional contextual\nknowledge. Furthermore, DDSSs' limited representation learning leads to weak\npredictive performance with scarce data. To address these challenges, we\npropose a general framework named LLM-TKESS (large language model for\ntext-based knowledge-embedded soft sensing), harnessing the powerful general\nproblem-solving capabilities, cross-modal knowledge transfer abilities, and\nfew-shot capabilities of LLM for enhanced soft sensing modeling. Specifically,\nan auxiliary variable series encoder (AVS Encoder) is proposed to unleash LLM's\npotential for capturing temporal relationships within series and spatial\nsemantic relationships among auxiliary variables. Then, we propose a two-stage\nfine-tuning alignment strategy: in the first stage, employing\nparameter-efficient fine-tuning through autoregressive training adjusts LLM to\nrapidly accommodate process variable data, resulting in a soft sensing\nfoundation model (SSFM). Subsequently, by training adapters, we adapt the SSFM\nto various downstream tasks without modifying its architecture. Then, we\npropose two text-based knowledge-embedded soft sensors, integrating new natural\nlanguage modalities to overcome the limitations of pure structured data models.\nFurthermore, benefiting from LLM's pre-existing world knowledge, our model\ndemonstrates outstanding predictive capabilities in small sample conditions.\nUsing the thermal deformation of air preheater rotor as a case study, we\nvalidate through extensive experiments that LLM-TKESS exhibits outstanding\nperformance.",
        "As Large Language Models (LLMs) are pretrained on massive-scale corpora, the\nissue of data contamination has become increasingly severe, leading to\npotential overestimation of model performance during evaluation. To address\nthis, we propose AdEval (Alignment-based Dynamic Evaluation), a dynamic data\nevaluation method aimed at mitigating the impact of data contamination on\nevaluation reliability. Experimental results on multiple datasets demonstrate\nthat AdEval effectively reduces the impact of data contamination on evaluation\noutcomes, enhancing both the fairness and reliability of the evaluation\nprocess.",
        "In this paper, we present a novel method to control a rigidly connected\nlocation on the vehicle, such as a point on the implement in case of\nagricultural tasks. Agricultural robots are transforming modern farming by\nenabling precise and efficient operations, replacing humans in arduous tasks\nwhile reducing the use of chemicals. Traditionnaly, path_following algorithms\nare designed to guide the vehicle's center along a predefined trajetory.\nHowever, since the actual agronomic task is performed by the implement, it is\nessential to control a specific point on the implement itself rather than\nvehicle's center. As such, we present in this paper two approaches for\nachieving the control of an offset point on the robot. The first approach\nadapts existing control laws, initially inteded for rear axle's midpoint, to\nmanage the desired lateral deviation. The second approach employs backstepping\ncontrol techniques to create a control law that directly targets the implement.\nWe conduct real-world experiments, highlighting the limitations of traditional\napproaches for offset points control, and demonstrating the strengths and\nweaknesses of the proposed methods.",
        "Recently the notion that quantum gravity effects could manifest at scales\nmuch lower than the Planck scale has seen an intense Swamplandish revival.\nDozens of works have explored how the so-called species scale -- at which an\neffective description of gravity must break down -- relates to String Theory\nand the Swampland conjectures. In particular, the interplay between this scale\nand the abundant towers of states becoming lighter in asymptotic regions of\nmoduli spaces has proved to be key in understanding the real scale of quantum\ngravity. Nevertheless concerns have been raised regarding the validity of using\ninfinite towers of states when estimating this scale within Effective Field\nTheory and, more precisely, the consistency of cutting the tower part way\nthrough in a framework that relies on a clear separation of scales. In this\nwork we take an EFT point-of-view and provide a detailed perturbative\nderivation of the species scale -- by computing the 1-loop graviton propagator\nin the presence of many fields -- thereby clarifying common sources of\nconfusion in the literature. Not only do we clarify the setup, assumptions and\nregimes of validity of the result, but more importantly apply the same methods\nto an infinite tower of states. We show how each state in the tower contributes\nto the species scale and how the procedure of counting only ''light fields''\ncan be compatible with not cutting the tower, thereby maintaining the harmony\nbetween infinite towers and EFTs even in the context of the species scale.",
        "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in chart understanding tasks. However, interpreting charts with\ntextual descriptions often leads to information loss, as it fails to fully\ncapture the dense information embedded in charts. In contrast, parsing charts\ninto code provides lossless representations that can effectively contain all\ncritical details. Although existing open-source MLLMs have achieved success in\nchart understanding tasks, they still face two major challenges when applied to\nchart-to-code tasks.: (1) Low executability and poor restoration of chart\ndetails in the generated code and (2) Lack of large-scale and diverse training\ndata. To address these challenges, we propose \\textbf{ChartCoder}, the first\ndedicated chart-to-code MLLM, which leverages Code LLMs as the language\nbackbone to enhance the executability of the generated code. Furthermore, we\nintroduce \\textbf{Chart2Code-160k}, the first large-scale and diverse dataset\nfor chart-to-code generation, and propose the \\textbf{Snippet-of-Thought (SoT)}\nmethod, which transforms direct chart-to-code generation data into step-by-step\ngeneration. Experiments demonstrate that ChartCoder, with only 7B parameters,\nsurpasses existing open-source MLLMs on chart-to-code benchmarks, achieving\nsuperior chart restoration and code excitability. Our code will be available at\nhttps:\/\/github.com\/thunlp\/ChartCoder.",
        "In this note, by integrating ideas concerning terminating tableaux-based\nprocedures in modal logics and finite frame property of intuitionistic modal\nlogic IK, we provide new and simpler decidability proofs for FIK and LIK.",
        "Chart visualizations, while essential for data interpretation and\ncommunication, are predominantly accessible only as images in PDFs, lacking\nsource data tables and stylistic information. To enable effective editing of\ncharts in PDFs or digital scans, we present PlotEdit, a novel multi-agent\nframework for natural language-driven end-to-end chart image editing via\nself-reflective LLM agents. PlotEdit orchestrates five LLM agents: (1)\nChart2Table for data table extraction, (2) Chart2Vision for style attribute\nidentification, (3) Chart2Code for retrieving rendering code, (4) Instruction\nDecomposition Agent for parsing user requests into executable steps, and (5)\nMultimodal Editing Agent for implementing nuanced chart component modifications\n- all coordinated through multimodal feedback to maintain visual fidelity.\nPlotEdit outperforms existing baselines on the ChartCraft dataset across style,\nlayout, format, and data-centric edits, enhancing accessibility for visually\nchallenged users and improving novice productivity.",
        "The automatic detection of disinformation presents a significant challenge in\nthe field of natural language processing. This task addresses a multifaceted\nsocietal and communication issue, which needs approaches that extend beyond the\nidentification of general linguistic patterns through data-driven algorithms.\nIn this research work, we hypothesise that text classification methods are not\nable to capture the nuances of disinformation and they often ground their\ndecision in superfluous features. Hence, we apply a post-hoc explainability\nmethod (SHAP, SHapley Additive exPlanations) to identify spurious elements with\nhigh impact on the classification models. Our findings show that\nnon-informative elements (e.g., URLs and emoticons) should be removed and named\nentities (e.g., Rwanda) should be pseudo-anonymized before training to avoid\nmodels' bias and increase their generalization capabilities. We evaluate this\nmethodology with internal dataset and external dataset before and after\napplying extended data preprocessing and named entity replacement. The results\nshow that our proposal enhances on average the performance of a disinformation\nclassification method with external test data in 65.78% without a significant\ndecrease of the internal test performance.",
        "Road traffic injuries are the leading cause of death for people aged 5-29,\nresulting in about 1.19 million deaths each year. To reduce these fatalities,\nit is essential to address human errors like speeding, drunk driving, and\ndistractions. Additionally, faster accident detection and quicker medical\nresponse can help save lives. We propose an accident detection framework that\ncombines a rule-based approach with a learning-based one. We introduce a\ndataset of real-world highway accidents featuring high-speed crash sequences.\nIt includes 294,924 labeled 2D boxes, 93,012 labeled 3D boxes, and track IDs\nacross 48,144 frames captured at 10 Hz using four roadside cameras and LiDAR\nsensors. The dataset covers ten object classes and is released in the OpenLABEL\nformat. Our experiments and analysis demonstrate the reliability of our method."
      ]
    }
  },
  {
    "id":2411.05213,
    "research_type":"basic",
    "start_id":"b0",
    "start_title":"Effect of bioclogging in porous media on complex conductivity signatures",
    "start_abstract":"Flow through sand columns inoculated with Pseudomonas aeruginosa were used to investigate the effect of bioclogging on complex conductivity and flow transport properties. Complex (0.1\u20131000 Hz), bulk hydraulic (K), volumetric rate (Q), dispersivity (D), microbial cell concentrations monitored over time. Environmental scanning electron microscope images sands obtained at end experiment. Bioclogging resulting from increases in concentration production exopolymeric substances (EPS) had a large impact imaginary ( \u03c3 \u2033), K, Q, D, porosity (\u03a6). Changes electrical properties developed three stages: an initial stage 1 no significant changes all measured parameters (Days 1\u20138), which we attribute reversible irreversible attachment cells sand. In 2a 9\u201316), caused by growth biomass either as microcolonies filling pore throats and\/or uniform covering surfaces resulted maximum decrease Q but only moderate \u2033. 2b 17\u201324), EPS increase biofilm thickness higher \u2033 compared 2a. 3 25\u201332), reached quasi steady state insignificant are observed parameters. The results this study suggest that can provide complimentary information assessment porous media.",
    "start_categories":[
      "math.MP"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b1",
        "b8"
      ],
      "title":[
        "Microreactors gain wider use as alternative to batch production",
        "How flocculation can explain coexistence in the chemostat"
      ],
      "abstract":[
        "The microreactors are gaining wide use among the pharmaceuticals and chemical companies as an alternative to batch production. They not only offers a flexible approach to continuous processing, but promises to save much of the time and effort consumed while expanding the chemistries at commercial scale. Most of the ten global pharma and chemical companies have acquired the Cytos Lab System,, a microreactor product developed by Cellular Process Chemistry Systems GmbH (CPC). Microreactors are comprised of plates with distinct channels in the submillimeter range, providing high surface-to-volume ratio, ultra fast mixing and high degree of control at all levels of production.",
        "We study a chemostat model in which two microbial species grow on single resource. show that coexistence is possible when the would normally win exclusive competition aggregates flocs. Our mathematical analysis exploits fact flocculation fast compared to biological growth, common hypothesis floc models. A numerical shows validity of this approach large parameter range. indicate how our yields mechanistic justification for so-called density-dependent growth."
      ],
      "categories":[
        "q-bio.OT"
      ]
    },
    "list":{
      "title":[
        "Leveraging 13C NMR spectroscopic data derived from SMILES to predict the\n  functionality of small biomolecules by machine learning: a case study on\n  human Dopamine D1 receptor antagonists",
        "Toxicological Evaluation of Phytochemicals and Heavy Metals in Ficus\n  exasperata Vahl (Sandpaper) Leaves obtained in Birnin Kebbi, Nigeria",
        "From FAIR to CURE: Guidelines for Computational Models of Biological\n  Systems",
        "Comprehensive Analysis of Bioactive Peptides from Cuminum cyminum L.\n  Seeds: Sequence Identification and Pharmacological Evaluation",
        "Higher serum 25(OH)D concentration is associated with lower risk of\n  metabolic syndrome among Aboriginal and Torres Strait Islander peoples in\n  Australia",
        "Cytogenetic, Hematobiochemical, and Histopathological Assessment of\n  Albino Rats (Rattus norvegicus) Fed on Gluten Extracts",
        "Co-Translational mRNA Decay in Plants: Recent advances and future\n  directions",
        "Human Genome Book: Words, Sentences and Paragraphs",
        "Data Sharing in the PRIMED Consortium: Design, implementation, and\n  recommendations for future policymaking",
        "Conditional Success of Adaptive Therapy: The Role of Treatment-Holiday\n  Thresholds Revealed by Mathematical Modeling",
        "An Asymptotic Analysis of Bivalent Monoclonal Antibody-Antigen Binding",
        "Toward a General Theory for the Scaling and Universality of Thermal\n  Responses in Biology",
        "Reverse Markov Learning: Multi-Step Generative Models for Complex\n  Distributions",
        "CoNOAir: A Neural Operator for Forecasting Carbon Monoxide Evolution in\n  Cities",
        "Malware Detection based on API calls",
        "The Southern Photometrical Local Universe Survey (S-PLUS): searching for\n  metal-poor dwarf galaxies",
        "Integrating Spatiotemporal Vision Transformer into Digital Twins for\n  High-Resolution Heat Stress Forecasting in Campus Environments",
        "The Regular Ricci-Inverse Cosmology with Multiple Anticurvature Scalars",
        "Compliance while resisting: a shear-thickening fluid controller for\n  physical human-robot interaction",
        "Benchmarking Vision-Language Models on Optical Character Recognition in\n  Dynamic Video Environments",
        "Adaptive Negative Damping Control for User-Dependent Multi-Terrain\n  Walking Assistance with a Hip Exoskeleton",
        "MQG4AI Towards Responsible High-risk AI -- Illustrated for Transparency\n  Focusing on Explainability Techniques",
        "Typographic Attacks in a Multi-Image Setting",
        "DDRM-PR: Fourier Phase Retrieval using Denoising Diffusion Restoration\n  Models",
        "Doping dependence of the magnetic ground state in the frustrated magnets\n  Ba$_2$$M$Te$_{1-x}$W$_{x}$O$_6$ ($M$ = Mn, Co)",
        "Adaptive Entanglement Routing with Deep Q-Networks in Quantum Networks",
        "Radio pulse search from Aql X-1"
      ],
      "abstract":[
        "This study contributes to ongoing research which aims to predict small\nbiomolecule functionality using Carbon-13 Nuclear Magnetic Resonance ($^{13}$C\nNMR) spectrum data and machine learning (ML). The approach was demonstrated\nusing a bioassay on human dopamine D1 receptor antagonists. The Simplified\nMolecular Input Line Entry System (SMILES) notations of compounds in this\nbioassay were extracted and converted into spectroscopic data by software\ndesigned for this purpose. The resulting data was then used for ML with\nscikit-learn algorithms. The ML models were trained by 27,756 samples and\ntested by 5,466. From the estimators K-Nearest neighbor, Decision Tree\nClassifier, Random Forest Classifier, Gradient Boosting Classifier, XGBoost\nClassifier, and Support Vector Classifier, the last performed the best,\nachieving 71.5 % accuracy, 77.4 % precision, 60.6% recall, 68 % F1, 71.5 % ROC,\nand 0.749 cross-validation score with 0.005 standard deviation. The methodology\ncan be applied to predict any functionality of any compound when relevant data\nare available. It was hypothesized also that increasing the number of samples\nwould increase accuracy. In addition to the SMILES $^{13}$C NMR spectrum ML\nmodel, the time- , and cost-efficient CID_SID ML model was developed. This\nmodel allows researchers who have developed a compound and obtained its PubChem\nCID and SID to check whether their compound is also a human dopamine D1\nreceptor antagonist based solely on the PubChem identifiers. The metrics of the\nCID_SID ML model were 80.2% accuracy, 86.3% precision, 70.4% recall, 77.6% F1,\n79.9% ROC, five-fold cross-validation score of 0.8071 with 0.0047 Standard\ndeviation.",
        "Objective: Ficus exasperata Vahl (Sandpaper tree) is extensively used in\nNigeria to treat diseases, but a dearth of documentation about its toxicity\nexists. This information is crucial because pollutants can contaminate\nmedicinal plants. This study determined the heavy metal and phytochemical\ncontent of methanolic leaf extract of F exasperata obtained in Birnin Kebbi,\nNigeria. Material and Methods: The lethality of the plant was also assessed\nusing 70 wild shrimps divided equally into seven groups. Group 1 (negative\ncontrol), groups 2 and 3 (positive controls) were exposed to 500 and 1000ppm of\nformaldehyde, respectively; and groups 4-7 were exposed to 1000, 2000, 4000,\nand 8000ppm of extracts, respectively, for 96 hours. Results: The\nphytochemistry revealed high levels of flavonoids and saponins and moderate\nlevels of tannins and phenols. The heavy metal analysis revealed non-tolerable\nlevels of cadmium, copper, and lead, while zinc was within the tolerable limit.\nThe negative control recorded 10% mortality, 1000 and 2000 ppm (20% each),\n4000ppm (70%), and 8000 ppm (100%). Conclusion: These results inferred safe\ndoses of the plant's extract in low and medium concentrations but toxic and\nfatal at high doses over a period of time. Consumers are advised to seek an\nexpert's guidance before using it.",
        "Guidelines for managing scientific data have been established under the FAIR\nprinciples requiring that data be Findable, Accessible, Interoperable, and\nReusable. In many scientific disciplines, especially computational biology,\nboth data and models are key to progress. For this reason, and recognizing that\nsuch models are a very special type of 'data', we argue that computational\nmodels, especially mechanistic models prevalent in medicine, physiology and\nsystems biology, deserve a complementary set of guidelines. We propose the CURE\nprinciples, emphasizing that models should be Credible, Understandable,\nReproducible, and Extensible. We delve into each principle, discussing\nverification, validation, and uncertainty quantification for model credibility;\nthe clarity of model descriptions and annotations for understandability;\nadherence to standards and open science practices for reproducibility; and the\nuse of open standards and modular code for extensibility and reuse. We outline\nrecommended and baseline requirements for each aspect of CURE, aiming to\nenhance the impact and trustworthiness of computational models, particularly in\nbiomedical applications where credibility is paramount. Our perspective\nunderscores the need for a more disciplined approach to modeling, aligning with\nemerging trends such as Digital Twins and emphasizing the importance of data\nand modeling standards for interoperability and reuse. Finally, we emphasize\nthat given the non-trivial effort required to implement the guidelines, the\ncommunity moves to automate as many of the guidelines as possible.",
        "Cuminum cyminum L. (cumin) is a medicinal and edible plant widely used in\ntraditional Chinese medicine (TCM) for treating various ailments, including\ndiarrhea, abdominal pain, inflammation, asthma, and diabetes. While previous\nresearch has primarily focused on its essential oils, studies on its\nprotein-derived bioactive peptides remain limited. In this study, we employed\nan innovative extraction method to isolate peptides from cumin seeds for the\nfirst time and screened their biological activities, revealing significant\nantimicrobial, antioxidant, and hypoglycemic properties. Guided by bioactivity,\nwe utilized advanced separation and structural identification techniques,\nincluding Matrix-Assisted Laser Desorption\/Ionization Time-of-Flight Mass\nSpectrometry (MALDI-TOF\/TOF MS\/MS), to systematically purify and characterize\ncumin-derived peptides. A total of 479 unique peptide sequences were identified\nusing Mascot software and the SwissProt\/UniProt_Bos databases. Among these, 15\nhighly bioactive peptides were selected for further analysis based on\nbioactivity and toxicity predictions using PeptideRanker and ToxinPred.\nStructural characterization revealed key features, such as {\\alpha}-helices and\n\\b{eta}-sheets, associated with their multifunctional activities. This study\nprovides the first comprehensive analysis of bioactive peptides from Cuminum\ncyminum L. seeds, elucidating their potential as antimicrobial, antioxidant,\nand hypoglycemic agents. These findings not only clarify the pharmacological\nbasis of cumin's traditional uses but also lay a theoretical foundation for the\ndevelopment of novel therapeutic agents from this medicinal plant.",
        "Although previous observational studies have shown associations between serum\n25-hydroxyvitamin D (25(OH)D) concentration and metabolic syndrome, this\nassociation has not yet been investigated among Aboriginal and Torres Strait\nIslander peoples. We aimed to investigate the association between serum 25(OH)D\nconcentration and metabolic syndrome and its risk factors in this population\ngroup. We used cross-sectional data from the 2012-2013 Australian Aboriginal\nand Torres Strait Islander Health Survey. Metabolic syndrome is defined as\nhaving 3 or more risk factors: elevated waist circumference, elevated\ntriglycerides, low high-density lipoprotein (HDL) cholesterol, elevated blood\npressure, or elevated fasting blood glucose. We used binomial logistic\nregression to test associations between serum 25(OH)D concentration and\nmetabolic syndrome, and multiple linear regression to test associations between\nserum 25(OH)D concentration and each risk factor. We included the following\ncovariates: age, sex, smoking status, education level, socio-economic status,\nremoteness of location, season, and body mass index (BMI). After adjusting for\ncovariates, we found that each 10 nmol\/L increase in serum 25(OH)D\nconcentration was statistically significantly associated with a 16% lower risk\nof metabolic syndrome (odds ratio: 0.84, 95% confidence interval: 0.76, 0.92)\nand a 2.1 cm (95% confidence interval: 1.65, 2.57) lower waist circumference\n(BMI was not included in the model for waist circumference). We found small\ninverse associations between serum 25(OH)D concentration and all other risk\nfactors except systolic blood pressure. Given that higher serum 25(OH)D\nconcentration may confer metabolic health benefits, promoting vitamin D\nsufficiency may be beneficial for this population.",
        "Background: Literature shows that most of the information on the toxicity of\ngluten is generated from survey and observational studies, resulting in\ninconsistent outcomes and a decrease in the acceptability of gluten-rich foods.\nTo determine gluten's safety, an in-depth in vitro and in vivo toxicological\nexamination is required. This enables scientists to come up with ameliorative\nstrategies if it turns out to have side effects, and consumers' trust can be\nrestored. Objectives: The objective of this study was to assess the toxicity of\ngluten extracts on albino rats (Rattus norvegicus). Materials and Methods:\nTwenty-four rats were randomly selected and divided into four groups, each\ncomprising six rats. Group 1 (control) rats were fed on pellet feeds and groups\n2, 3, and 4 were fed on daily dosages of 0.5, 1.0, and 1.5 g gluten extracts,\nrespectively. The rats' body weights and reactions were observed for 90 days\nbefore blood samples were collected for hematobiochemical and micronucleus\ntests. Histopathological examinations of the liver and kidneys were also\nperformed. Results: There was no difference (P > 0.05) in body weight, blood\nglucose level, or micronuclei between the control and treated rats. The\nlymphocytes, alkaline phosphatase, alanine transaminase, total protein, and\ncalcium ions of the test rats were all significantly (P < 0.05) altered but\nremained within the normal ranges. Other hematobiochemical parameters,\nincluding packed cell volume, hemoglobin, white and red blood cells, aspartate\ntransaminase, albumin, sodium ions, potassium ions, chloride ions, and urea,\nrevealed no marked changes. The treated rats' livers and kidneys showed no\nhistopathological changes. Conclusion: Gluten had no adverse effects. However,\nit altered hematobiochemical parameters, particularly the lymphocytes, alkaline\nphosphatase, alanine transaminase, total protein, and calcium ions.",
        "Tight regulation of messenger RNA (mRNA) stability is essential to ensure\naccurate gene expression in response to developmental and environmental cues.\nmRNA stability is controlled by mRNA decay pathways, which have traditionally\nbeen proposed to occur independently of translation. However, the recent\ndiscovery of a co-translational mRNA decay pathway (also known as CTRD) reveals\nthat mRNA translation and decay can be coupled. While being translated, a mRNA\ncan be targeted for degradation. This pathway was first described in yeast and\nrapidly identified in several plant species. This review explores recent\nadvances in our understanding of CTRD in plants, emphasizing its regulation and\nits importance for development and stress response. The different metrics used\nto assess CTRD activity are also presented. Furthermore, this review outlines\nfuture directions to explore the importance of mRNA decay in maintaining mRNA\nhomeostasis in plants.",
        "Since the completion of the human genome sequencing project in 2001,\nsignificant progress has been made in areas such as gene regulation editing and\nprotein structure prediction. However, given the vast amount of genomic data,\nthe segments that can be fully annotated and understood remain relatively\nlimited. If we consider the genome as a book, constructing its equivalents of\nwords, sentences, and paragraphs has been a long-standing and popular research\ndirection. Recently, studies on transfer learning in large language models have\nprovided a novel approach to this challenge.Multilingual transfer ability,\nwhich assesses how well models fine-tuned on a source language can be applied\nto other languages, has been extensively studied in multilingual pre-trained\nmodels. Similarly, the transfer of natural language capabilities to \"DNA\nlanguage\" has also been validated. Building upon these findings, we first\ntrained a foundational model capable of transferring linguistic capabilities\nfrom English to DNA sequences. Using this model, we constructed a vocabulary of\nDNA words and mapped DNA words to their English equivalents.Subsequently, we\nfine-tuned this model using English datasets for paragraphing and sentence\nsegmentation to develop models capable of segmenting DNA sequences into\nsentences and paragraphs. Leveraging these models, we processed the GRCh38.p14\nhuman genome by segmenting, tokenizing, and organizing it into a \"book\"\ncomprised of genomic \"words,\" \"sentences,\" and \"paragraphs.\" Additionally,\nbased on the DNA-to-English vocabulary mapping, we created an \"English version\"\nof the genomic book. This study offers a novel perspective for understanding\nthe genome and provides exciting possibilities for developing innovative tools\nfor DNA search, generation, and analysis.",
        "Sharing diverse genomic and other biomedical datasets is critical to advance\nscientific discoveries and their equitable translation to improve human health.\nHowever, data sharing remains challenging in the context of legacy datasets,\nevolving policies, multi-institutional consortium science, and international\nstakeholders. The NIH-funded Polygenic Risk Methods in Diverse Populations\n(PRIMED) Consortium was established to improve the performance of polygenic\nrisk estimates for a broad range of health and disease outcomes with global\nimpacts. Improving polygenic risk score performance across genetically diverse\npopulations requires access to large, diverse cohorts. We report on the design\nand implementation of data sharing policies and procedures developed in PRIMED\nto aggregate and analyze data from multiple, heterogeneous sources while\nadhering to existing data sharing policies for each integrated dataset. We\ndescribe two primary data sharing mechanisms: coordinated dbGaP applications\nand a Consortium Data Sharing Agreement, as well as provide alternatives when\nindividual-level data cannot be shared within the Consortium (e.g., federated\nanalyses). We also describe technical implementation of Consortium data sharing\nin the NHGRI Analysis Visualization and Informatics Lab-space (AnVIL) cloud\nplatform, to share derived individual-level data, genomic summary results, and\nmethods workflows with appropriate permissions. As a Consortium making\nsecondary use of pre-existing data sources, we also discuss challenges and\npropose solutions for release of individual- and summary-level data products to\nthe broader scientific community. We make recommendations for ongoing and\nfuture policymaking with the goal of informing future consortia and other\nresearch activities.",
        "Adaptive therapy (AT) improves cancer treatment by controlling the\ncompetition between sensitive and resistant cells through treatment holidays.\nThis study highlights the critical role of treatment-holiday thresholds in AT\nfor tumors composed of drug-sensitive and resistant cells. Using a\nLotka-Volterra model, the research compares AT with maximum tolerated dose\ntherapy and intermittent therapy, showing that AT's success largely depends on\nthe threshold at which treatment is paused and resumed, as well as on the\ncompetition between sensitive and resistant cells. Three scenarios of\ncomparison between AT and other therapies are identified: uniform-decline,\nconditional-improve, and uniform-improve, illustrating that optimizing the\ntreatment-holiday threshold is crucial for AT effectiveness. Tumor composition,\nincluding initial tumor burden and the proportion of resistant cells,\ninfluences outcomes. Adjusting threshold values enables AT to suppress\nresistant subclones, preserving sensitive cells, ultimately improving\nprogression-free survival. These findings emphasize the importance of\npersonalized treatment strategies potentially enhancing long-term therapeutic\noutcomes.",
        "Ligand-receptor interactions are fundamental to many biological processes.\nFor example in antibody-based immunotherapies, the dynamics of an antibody\nbinding with its target antigen directly influence the potency and efficacy of\nmonoclonal antibody (mAb) therapies. In this paper, we present an asymptotic\nanalysis of an ordinary differential equation (ODE) model of bivalent\nantibody-antigen binding in the context of mAb cancer therapies, highlighting\nthe added complexity associated with bivalency of the antibody. To understand\nwhat drives the complex temporal dynamics of bivalent antibody-antigen binding,\nwe construct asymptotic approximations to the model's solutions at different\ntimescales and antibody concentrations that are in good agreement with\nnumerical simulations of the full model. We show how the dynamics differ\nbetween two scenarios; a region where unbound antigens are abundant, and one\nwhere the number of unbound antigens is small such that the dominant balance\nwithin the model equations changes. Of particular importance to the potency and\nefficacy of mAb treatments are the values of quantities such as antigen\noccupancy and bound antibody number. We use the results of our asymptotic\nanalysis to approximate the long-time values of these quantities that could be\ncombined with experimental data to facilitate parameter estimation.",
        "We developed a theory showing that under appropriate normalizations and\nrescalings, temperature response curves show a remarkably regular behavior and\nfollow a general, universal law. The impressive universality of temperature\nresponse curves remained hidden due to various curve-fitting models not\nwell-grounded in first principles. In addition, this framework has the\npotential to explain the origin of different scaling relationships in thermal\nperformance in biology, from molecules to ecosystems. Here, we summarize the\nbackground, principles and assumptions, predictions, implications, and possible\nextensions of this theory.",
        "Learning complex distributions is a fundamental challenge in contemporary\napplications. Generative models, such as diffusion models, have demonstrated\nremarkable success in overcoming many limitations of traditional statistical\nmethods. Shen and Meinshausen (2024) introduced engression, a generative\napproach based on scoring rules that maps noise (and covariates, if available)\ndirectly to data. While effective, engression struggles with highly complex\ndistributions, such as those encountered in image data. In this work, we extend\nengression to improve its capability in learning complex distributions. We\npropose a framework that defines a general forward process transitioning from\nthe target distribution to a known distribution (e.g., Gaussian) and then\nlearns a reverse Markov process using multiple engression models. This reverse\nprocess reconstructs the target distribution step by step. Our approach\nsupports general forward processes, allows for dimension reduction, and\nnaturally discretizes the generative process. As a special case, when using a\ndiffusion-based forward process, our framework offers a method to discretize\nthe training and inference of diffusion models efficiently. Empirical\nevaluations on simulated and climate data validate our theoretical insights,\ndemonstrating the effectiveness of our approach in capturing complex\ndistributions.",
        "Carbon Monoxide (CO) is a dominant pollutant in urban areas due to the energy\ngeneration from fossil fuels for industry, automobile, and domestic\nrequirements. Forecasting the evolution of CO in real-time can enable the\ndeployment of effective early warning systems and intervention strategies.\nHowever, the computational cost associated with the physics and chemistry-based\nsimulation makes it prohibitive to implement such a model at the city and\ncountry scale. To address this challenge, here, we present a machine learning\nmodel based on neural operator, namely, Complex Neural Operator for Air Quality\n(CoNOAir), that can effectively forecast CO concentrations. We demonstrate this\nby developing a country-level model for short-term (hourly) and long-term\n(72-hour) forecasts of CO concentrations. Our model outperforms\nstate-of-the-art models such as Fourier neural operators (FNO) and provides\nreliable predictions for both short and long-term forecasts. We further analyse\nthe capability of the model to capture extreme events and generate forecasts in\nurban cities in India. Interestingly, we observe that the model predicts the\nnext hour CO concentrations with R2 values greater than 0.95 for all the cities\nconsidered. The deployment of such a model can greatly assist the governing\nbodies to provide early warning, plan intervention strategies, and develop\neffective strategies by considering several what-if scenarios. Altogether, the\npresent approach could provide a fillip to real-time predictions of CO\npollution in urban cities.",
        "Malware attacks pose a significant threat in today's interconnected digital\nlandscape, causing billions of dollars in damages. Detecting and identifying\nfamilies as early as possible provides an edge in protecting against such\nmalware. We explore a lightweight, order-invariant approach to detecting and\nmitigating malware threats: analyzing API calls without regard to their\nsequence. We publish a public dataset of over three hundred thousand samples\nand their function call parameters for this task, annotated with labels\nindicating benign or malicious activity. The complete dataset is above 550GB\nuncompressed in size. We leverage machine learning algorithms, such as random\nforests, and conduct behavioral analysis by examining patterns and anomalies in\nAPI call sequences. By investigating how the function calls occur regardless of\ntheir order, we can identify discriminating features that can help us identify\nmalware early on. The models we've developed are not only effective but also\nefficient. They are lightweight and can run on any machine with minimal\nperformance overhead, while still achieving an impressive F1-Score of over\n85\\%. We also empirically show that we only need a subset of the function call\nsequence, specifically calls to the ntdll.dll library, to identify malware. Our\nresearch demonstrates the efficacy of this approach through empirical\nevaluations, underscoring its accuracy and scalability. The code is open source\nand available at Github along with the dataset on Zenodo.",
        "The metal content of a galaxy's interstellar medium reflects the interplay\nbetween different evolutionary processes such as feedback from massive stars\nand the accretion of gas from the intergalactic medium. Despite the expected\nabundance of low-luminosity galaxies, the low-mass and low-metallicity regime\nremains relatively understudied. Since the properties of their interstellar\nmedium resemble those of early galaxies, identifying such objects in the Local\nUniverse is crucial to understand the early stages of galaxy evolution. We used\nthe DR3 catalog of the Southern Photometric Local Universe Survey (S-PLUS) to\nselect low-metallicity dwarf galaxy candidates based on color selection\ncriteria typical of metal-poor, star-forming, low-mass systems. The final\nsample contains approximately 50 candidates. Spectral energy distribution\nfitting of the 12 S-PLUS bands reveals that $\\sim$ 90\\% of the candidates are\nbest fit by models with very low stellar metallicities. We obtained long-slit\nobservations with the Gemini Multi-Object Spectrograph to follow-up a pilot\nsample and confirm whether these galaxies have low metallicities. We find\noxygen abundances in the range $7.35<$ 12 + log(O\/H) $< 7.93$ (5\\% to 17\\% of\nthe solar value), confirming their metal-poor nature. Most targets are outliers\nin the mass-metallicity relation, i.e. they display a low metal content\nrelative to their observed stellar masses. In some cases, perturbed optical\nmorphologies might give evidence of dwarf-dwarf interactions or mergers. These\nresults suggest that the low oxygen abundances may be associated with an\nexternal event causing the accretion of metal-poor gas, which dilutes the\noxygen abundance in these systems.",
        "Extreme heat events exacerbated by climate change pose significant challenges\nto urban resilience and planning. This study introduces a climate-responsive\ndigital twin framework integrating the Spatiotemporal Vision Transformer\n(ST-ViT) model to enhance heat stress forecasting and decision-making. Using a\nTexas campus as a testbed, we synthesized high-resolution physical model\nsimulations with spatial and meteorological data to develop fine-scale human\nthermal predictions. The ST-ViT-powered digital twin enables efficient,\ndata-driven insights for planners, policymakers, and campus stakeholders,\nsupporting targeted heat mitigation strategies and advancing climate-adaptive\nurban design.",
        "We investigate the modified gravity in which the Lagrangian of gravity is a\nfunction of the trace of the n-th matrix power of Ricci tensor in a\nFriedmann-Lemaitre-Robertson-Walker(FLRW) spacetime. When n is negative, the\ninverse of Ricci tensor, also called the anticurvature tensor, will be\nintroduced. We design a new class of Ricci-inverse theory containing two\nanticurvature scalars and resulting to be free from the singularity problem.",
        "Physical human-robot interaction (pHRI) is widely needed in many fields, such\nas industrial manipulation, home services, and medical rehabilitation, and puts\nhigher demands on the safety of robots. Due to the uncertainty of the working\nenvironment, the pHRI may receive unexpected impact interference, which affects\nthe safety and smoothness of the task execution. The commonly used linear\nadmittance control (L-AC) can cope well with high-frequency small-amplitude\nnoise, but for medium-frequency high-intensity impact, the effect is not as\ngood. Inspired by the solid-liquid phase change nature of shear-thickening\nfluid, we propose a Shear-thickening Fluid Control (SFC) that can achieve both\nan easy human-robot collaboration and resistance to impact interference. The\nSFC's stability, passivity, and phase trajectory are analyzed in detail, the\nfrequency and time domain properties are quantified, and parameter constraints\nin discrete control and coupled stability conditions are provided. We conducted\nsimulations to compare the frequency and time domain characteristics of L-AC,\nnonlinear admittance controller (N-AC), and SFC, and validated their dynamic\nproperties. In real-world experiments, we compared the performance of L-AC,\nN-AC, and SFC in both fixed and mobile manipulators. L-AC exhibits weak\nresistance to impact. N-AC can resist moderate impacts but not high-intensity\nones, and may exhibit self-excited oscillations. In contrast, SFC demonstrated\nsuperior impact resistance and maintained stable collaboration, enhancing\ncomfort in cooperative water delivery tasks. Additionally, a case study was\nconducted in a factory setting, further affirming the SFC's capability in\nfacilitating human-robot collaborative manipulation and underscoring its\npotential in industrial applications.",
        "This paper introduces an open-source benchmark for evaluating Vision-Language\nModels (VLMs) on Optical Character Recognition (OCR) tasks in dynamic video\nenvironments. We present a curated dataset containing 1,477 manually annotated\nframes spanning diverse domains, including code editors, news broadcasts,\nYouTube videos, and advertisements. Three state of the art VLMs - Claude-3,\nGemini-1.5, and GPT-4o are benchmarked against traditional OCR systems such as\nEasyOCR and RapidOCR. Evaluation metrics include Word Error Rate (WER),\nCharacter Error Rate (CER), and Accuracy. Our results highlight the strengths\nand limitations of VLMs in video-based OCR tasks, demonstrating their potential\nto outperform conventional OCR models in many scenarios. However, challenges\nsuch as hallucinations, content security policies, and sensitivity to occluded\nor stylized text remain. The dataset and benchmarking framework are publicly\navailable to foster further research.",
        "Hip exoskeletons are known for their versatility in assisting users across\nvaried scenarios. However, current assistive strategies often lack the\nflexibility to accommodate for individual walking patterns and adapt to diverse\nlocomotion environments. In this work, we present a novel control strategy that\nadapts the mechanical impedance of the human-exoskeleton system. We design the\nhip assistive torques as an adaptive virtual negative damping, which is able to\ninject energy into the system while allowing the users to remain in control and\ncontribute voluntarily to the movements. Experiments with five healthy subjects\ndemonstrate that our controller reduces the metabolic cost of walking compared\nto free walking (average reduction of 7.2%), and it preserves the lower-limbs\nkinematics. Additionally, our method achieves minimal power losses from the\nexoskeleton across the entire gait cycle (less than 2% negative mechanical\npower out of the total power), ensuring synchronized action with the users'\nmovements. Moreover, we use Bayesian Optimization to adapt the assistance\nstrength and allow for seamless adaptation and transitions across multi-terrain\nenvironments. Our strategy achieves efficient power transmission under all\nconditions. Our approach demonstrates an individualized, adaptable, and\nstraightforward controller for hip exoskeletons, advancing the development of\nviable, adaptive, and user-dependent control laws.",
        "As artificial intelligence (AI) systems become increasingly integrated into\ncritical domains, ensuring their responsible design and continuous development\nis imperative. Effective AI quality management (QM) requires tools and\nmethodologies that address the complexities of the AI lifecycle. In this paper,\nwe propose an approach for AI lifecycle planning that bridges the gap between\ngeneric guidelines and use case-specific requirements (MQG4AI). Our work aims\nto contribute to the development of practical tools for implementing\nResponsible AI (RAI) by aligning lifecycle planning with technical, ethical and\nregulatory demands. Central to our approach is the introduction of a flexible\nand customizable Methodology based on Quality Gates, whose building blocks\nincorporate RAI knowledge through information linking along the AI lifecycle in\na continuous manner, addressing AIs evolutionary character. For our present\ncontribution, we put a particular emphasis on the Explanation stage during\nmodel development, and illustrate how to align a guideline to evaluate the\nquality of explanations with MQG4AI, contributing to overall Transparency.",
        "Large Vision-Language Models (LVLMs) are susceptible to typographic attacks,\nwhich are misclassifications caused by an attack text that is added to an\nimage. In this paper, we introduce a multi-image setting for studying\ntypographic attacks, broadening the current emphasis of the literature on\nattacking individual images. Specifically, our focus is on attacking image sets\nwithout repeating the attack query. Such non-repeating attacks are stealthier,\nas they are more likely to evade a gatekeeper than attacks that repeat the same\nattack text. We introduce two attack strategies for the multi-image setting,\nleveraging the difficulty of the target image, the strength of the attack text,\nand text-image similarity. Our text-image similarity approach improves attack\nsuccess rates by 21% over random, non-specific methods on the CLIP model using\nImageNet while maintaining stealth in a multi-image scenario. An additional\nexperiment demonstrates transferability, i.e., text-image similarity calculated\nusing CLIP transfers when attacking InstructBLIP.",
        "Diffusion models have demonstrated their utility as learned priors for\nsolving various inverse problems. However, most existing approaches are limited\nto linear inverse problems. This paper exploits the efficient and unsupervised\nposterior sampling framework of Denoising Diffusion Restoration Models (DDRM)\nfor the solution of nonlinear phase retrieval problem, which requires\nreconstructing an image from its noisy intensity-only measurements such as\nFourier intensity. The approach combines the model-based alternating-projection\nmethods with the DDRM to utilize pretrained unconditional diffusion priors for\nphase retrieval. The performance is demonstrated through both simulations and\nexperimental data. Results demonstrate the potential of this approach for\nimproving the alternating-projection methods as well as its limitations.",
        "Theoretically, the relative change of the Heisenberg-type nearest-neighbor\ncoupling $J_1$ and next-nearest-neighbor coupling $J_2$ in the\nface-centered-cubic lattice can give rise to three main antiferromagnetic\norderings of type-I, type-II, and type-III. However, it is difficult to tune\nthe $J_2\/J_1$ ratio in real materials. Here, we report studies on the influence\nof Te$^{6+}$ and W$^{6+}$ ions replacement to the magnetic interactions and the\nmagnetic ground states in the double-perovskite compounds\nBa$_2$$M$Te$_{1-x}$W$_{x}$O$_6$ ($M$ = Mn, Co). For\nBa$_2$MnTe$_{1-x}$W$_{x}$O$_6$, the W$^{6+}$ doping on Te$^{6+}$ site is\nsuccessful in $0.02 \\leq x \\leq 0.9$ with short-range orders of the type-I\n($0.02 \\leq x \\leq 0.08$) and type-II ($0.1 \\leq x \\leq 0.9$). In\nBa$_2$CoTe$_{1-x}$W${_x}$O$_6$, x-ray diffraction measurements reveal two\ncrystal structures, including the trigonal phase ($0 \\leq x \\leq 0.1$) and the\ncubic phase ($0.5 \\leq x \\leq 1$), between which is a miscibility gap. Two\nmagnetic transitions are identified in the trigonal phase due to two magnetic\nsubsystems, and the type-II magnetic order is observed in the cubic phase.\nMagnetic phase diagrams of Ba$_2M$Te$_{1-x}$W$_{x}$O$_6$ ($M$ = Mn, Co) are\nestablished. Our work shows that the magnetic interactions and ground states of\nBa$_2$$M$Te$_{1-x}$W$_x$O$_6$ can be tuned effectively by the replacement of\nTe$^{6+}$ by W$^{6+}$ ions.",
        "The quantum internet holds transformative potential for global communication\nby harnessing the principles of quantum information processing. Despite\nsignificant advancements in quantum communication technologies, the efficient\ndistribution of critical resources, such as qubits, remains a persistent and\nunresolved challenge. Conventional approaches often fall short of achieving\noptimal resource allocation, underscoring the necessity for more effective\nsolutions. This study proposes a novel reinforcement learning-based adaptive\nentanglement routing framework designed to enable resource allocation tailored\nto the specific demands of quantum applications. The introduced QuDQN model\nutilizes reinforcement learning to optimize the management of quantum networks,\nallocate resources efficiently, and enhance entanglement routing. The model\nintegrates key considerations, including fidelity requirements, network\ntopology, qubit capacity, and request demands.",
        "We present 12 observations of the accreting millisecond X-ray pulsar Aql X-1,\ntaken from August 2022 to October 2023 using the Five-hundred-meter Aperture\nSpherical Radio Telescope at 1250 MHz. These observations covered both the\nquiescence and X-ray outburst states, as determined by analyzing the X-ray data\nfrom the Neutron Star Interior Composition Explorer and the Monitor of All-sky\nX-ray Image. Periodicity and single-pulse searches were conducted for each\nobservation, but no pulsed signals were detected. The obtained upper limit flux\ndensities are in the range of 2.86-5.73 uJy, which provide the lowest limits to\ndate. We discuss several mechanisms that may prevent detection, suggesting that\nAql X-1 may be in the radio-ejection state during quiescence, where the radio\npulsed emissions are absorbed by the matter surrounding the system."
      ]
    }
  },
  {
    "id":2411.07031,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"ChatGPT Hallucinates Non-existent Citations: Evidence from Economics",
    "start_abstract":"In this study, we generate prompts derived from every topic within the Journal of Economic Literature to assess abilities both GPT-3.5 and GPT-4 versions ChatGPT large language model (LLM) write about economic concepts. demonstrates considerable competency in offering general summaries but also cites non-existent references. More than 30% citations provided by version do not exist rate is only slightly reduced for version. Additionally, our findings suggest that reliability decreases as become more specific. We provide quantitative evidence errors output demonstrate importance LLM verification. JEL Codes: B4; O33; I2",
    "start_categories":[
      "q-fin.ST"
    ],
    "start_fields":[
      "Economics and Quantitative Finance"
    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "Accuracy of Chatbots in Citing Journal Articles"
      ],
      "abstract":[
        "This cross-sectional study quantifies the journal article citation error rate of an artificial intelligence chatbot."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Teaching AI to Handle Exceptions: Supervised Fine-Tuning with\n  Human-Aligned Judgment",
        "Comprehensive Metapath-based Heterogeneous Graph Transformer for\n  Gene-Disease Association Prediction",
        "FAIR: Facilitating Artificial Intelligence Resilience in Manufacturing\n  Industrial Internet",
        "Speeding up design and making to reduce time-to-project and\n  time-to-market: an AI-Enhanced approach in engineering education",
        "Leveraging Large Language Models as Knowledge-Driven Agents for Reliable\n  Retrosynthesis Planning",
        "Predicting Team Performance from Communications in Simulated\n  Search-and-Rescue",
        "SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially?",
        "DeclareAligner: A Leap Towards Efficient Optimal Alignments for\n  Declarative Process Model Conformance Checking",
        "Kriging and Gaussian Process Interpolation for Georeferenced Data\n  Augmentation",
        "Advanced Tool Learning and Selection System (ATLASS): A Closed-Loop\n  Framework Using LLM",
        "PhysReason: A Comprehensive Benchmark towards Physics-Based Reasoning",
        "AI Generations: From AI 1.0 to AI 4.0",
        "Towards AI-assisted Academic Writing",
        "Second bounded cohomology of knot quandles",
        "PatentLMM: Large Multimodal Model for Generating Descriptions for Patent\n  Figures",
        "Conditioning on Local Statistics for Scalable Heterogeneous Federated\n  Learning",
        "Vertex-Minimal Triangulation of Complexes with Homology",
        "Epistemic Logic Programs: Non-Ground and Counting Complexity",
        "AudioSpa: Spatializing Sound Events with Text",
        "An upper bound on the size of a code with $s$ distances",
        "MEXA-CTP: Mode Experts Cross-Attention for Clinical Trial Outcome\n  Prediction",
        "Non-homogeneous problem for the fractional wave equation with irregular\n  coefficients and data",
        "QORT-Former: Query-optimized Real-time Transformer for Understanding Two\n  Hands Manipulating Objects",
        "DGNN: A Neural PDE Solver Induced by Discontinuous Galerkin Methods",
        "Blockchain with proof of quantum work",
        "Modularity of preferential attachment graphs",
        "Harmonic Structure of the Brunel spectra",
        "A maximum concurrence criterion to investigate absolutely maximally\n  entangled states"
      ],
      "abstract":[
        "Large language models (LLMs), initially developed for generative AI, are now\nevolving into agentic AI systems, which make decisions in complex, real-world\ncontexts. Unfortunately, while their generative capabilities are\nwell-documented, their decision-making processes remain poorly understood. This\nis particularly evident when models are handling exceptions, a critical and\nchallenging aspect of decision-making made relevant by the inherent\nincompleteness of contracts. Here we demonstrate that LLMs, even ones that\nexcel at reasoning, deviate significantly from human judgments because they\nadhere strictly to policies, even when such adherence is impractical,\nsuboptimal, or even counterproductive. We then evaluate three approaches to\ntuning AI agents to handle exceptions: ethical framework prompting,\nchain-of-thought reasoning, and supervised fine-tuning. We find that while\nethical framework prompting fails and chain-of-thought prompting provides only\nslight improvements, supervised fine-tuning, specifically with human\nexplanations, yields markedly better results. Surprisingly, in our experiments,\nsupervised fine-tuning even enabled models to generalize human-like\ndecision-making to novel scenarios, demonstrating transfer learning of\nhuman-aligned decision-making across contexts. Furthermore, fine-tuning with\nexplanations, not just labels, was critical for alignment, suggesting that\naligning LLMs with human judgment requires explicit training on how decisions\nare made, not just which decisions are made. These findings highlight the need\nto address LLMs' shortcomings in handling exceptions in order to guide the\ndevelopment of agentic AI toward models that can effectively align with human\njudgment and simultaneously adapt to novel contexts.",
        "Discovering gene-disease associations is crucial for understanding disease\nmechanisms, yet identifying these associations remains challenging due to the\ntime and cost of biological experiments. Computational methods are increasingly\nvital for efficient and scalable gene-disease association prediction.\nGraph-based learning models, which leverage node features and network\nrelationships, are commonly employed for biomolecular predictions. However,\nexisting methods often struggle to effectively integrate node features,\nheterogeneous structures, and semantic information. To address these\nchallenges, we propose COmprehensive MEtapath-based heterogeneous graph\nTransformer(COMET) for predicting gene-disease associations. COMET integrates\ndiverse datasets to construct comprehensive heterogeneous networks,\ninitializing node features with BioGPT. We define seven Metapaths and utilize a\ntransformer framework to aggregate Metapath instances, capturing global\ncontexts and long-distance dependencies. Through intra- and inter-metapath\naggregation using attention mechanisms, COMET fuses latent vectors from\nmultiple Metapaths to enhance GDA prediction accuracy. Our method demonstrates\nsuperior robustness compared to state-of-the-art approaches. Ablation studies\nand visualizations validate COMET's effectiveness, providing valuable insights\nfor advancing human health research.",
        "Artificial intelligence (AI) systems have been increasingly adopted in the\nManufacturing Industrial Internet (MII). Investigating and enabling the AI\nresilience is very important to alleviate profound impact of AI system failures\nin manufacturing and Industrial Internet of Things (IIoT) operations, leading\nto critical decision making. However, there is a wide knowledge gap in defining\nthe resilience of AI systems and analyzing potential root causes and\ncorresponding mitigation strategies. In this work, we propose a novel framework\nfor investigating the resilience of AI performance over time under hazard\nfactors in data quality, AI pipelines, and the cyber-physical layer. The\nproposed method can facilitate effective diagnosis and mitigation strategies to\nrecover AI performance based on a multimodal multi-head self latent attention\nmodel. The merits of the proposed method are elaborated using an MII testbed of\nconnected Aerosol Jet Printing (AJP) machines, fog nodes, and Cloud with\ninference tasks via AI pipelines.",
        "This paper explores the integration of AI tools, such as ChatGPT and GitHub\nCopilot, in the Software Architecture for Embedded Systems course. AI-supported\nworkflows enabled students to rapidly prototype complex projects, emphasizing\nreal-world applications like SLAM robotics. Results demon-started enhanced\nproblem-solving, faster development, and more sophisticated outcomes, with AI\naugmenting but not replacing human decision-making.",
        "Identifying reliable synthesis pathways in materials chemistry is a complex\ntask, particularly in polymer science, due to the intricate and often\nnon-unique nomenclature of macromolecules. To address this challenge, we\npropose an agent system that integrates large language models (LLMs) and\nknowledge graphs (KGs). By leveraging LLMs' powerful capabilities for\nextracting and recognizing chemical substance names, and storing the extracted\ndata in a structured knowledge graph, our system fully automates the retrieval\nof relevant literatures, extraction of reaction data, database querying,\nconstruction of retrosynthetic pathway trees, further expansion through the\nretrieval of additional literature and recommendation of optimal reaction\npathways. A novel Multi-branched Reaction Pathway Search (MBRPS) algorithm\nenables the exploration of all pathways, with a particular focus on\nmulti-branched ones, helping LLMs overcome weak reasoning in multi-branched\npaths. This work represents the first attempt to develop a fully automated\nretrosynthesis planning agent tailored specially for macromolecules powered by\nLLMs. Applied to polyimide synthesis, our new approach constructs a\nretrosynthetic pathway tree with hundreds of pathways and recommends optimized\nroutes, including both known and novel pathways, demonstrating its\neffectiveness and potential for broader applications.",
        "Understanding how individual traits influence team performance is valuable,\nbut these traits are not always directly observable. Prior research has\ninferred traits like trust from behavioral data. We analyze conversational data\nto identify team traits and their correlation with teaming outcomes. Using\ntranscripts from a Minecraft-based search-and-rescue experiment, we apply topic\nmodeling and clustering to uncover key interaction patterns. Our findings show\nthat variations in teaming outcomes can be explained through these inferences,\nwith different levels of predictive power derived from individual traits and\nteam dynamics.",
        "Reasoning and strategic behavior in social interactions is a hallmark of\nintelligence. This form of reasoning is significantly more sophisticated than\nisolated planning or reasoning tasks in static settings (e.g., math problem\nsolving). In this paper, we present Strategic Planning, Interaction, and\nNegotiation (SPIN-Bench), a new multi-domain evaluation designed to measure the\nintelligence of strategic planning and social reasoning. While many existing\nbenchmarks focus on narrow planning or single-agent reasoning, SPIN-Bench\ncombines classical PDDL tasks, competitive board games, cooperative card games,\nand multi-agent negotiation scenarios in one unified framework. The framework\nincludes both a benchmark as well as an arena to simulate and evaluate the\nvariety of social settings to test reasoning and strategic behavior of AI\nagents. We formulate the benchmark SPIN-Bench by systematically varying action\nspaces, state complexity, and the number of interacting agents to simulate a\nvariety of social settings where success depends on not only methodical and\nstep-wise decision making, but also conceptual inference of other (adversarial\nor cooperative) participants. Our experiments reveal that while contemporary\nLLMs handle basic fact retrieval and short-range planning reasonably well, they\nencounter significant performance bottlenecks in tasks requiring deep multi-hop\nreasoning over large state spaces and socially adept coordination under\nuncertainty. We envision SPIN-Bench as a catalyst for future research on robust\nmulti-agent planning, social reasoning, and human--AI teaming. Project Website:\nhttps:\/\/spinbench.github.io\/",
        "In many engineering applications, processes must be followed precisely,\nmaking conformance checking between event logs and declarative process models\ncrucial for ensuring adherence to desired behaviors. This is a critical area\nwhere Artificial Intelligence (AI) plays a pivotal role in driving effective\nprocess improvement. However, computing optimal alignments poses significant\ncomputational challenges due to the vast search space inherent in these models.\nConsequently, existing approaches often struggle with scalability and\nefficiency, limiting their applicability in real-world settings. This paper\nintroduces DeclareAligner, a novel algorithm that uses the A* search algorithm,\nan established AI pathfinding technique, to tackle the problem from a fresh\nperspective leveraging the flexibility of declarative models. Key features of\nDeclareAligner include only performing actions that actively contribute to\nfixing constraint violations, utilizing a tailored heuristic to navigate\ntowards optimal solutions, and employing early pruning to eliminate\nunproductive branches, while also streamlining the process through\npreprocessing and consolidating multiple fixes into unified actions. The\nproposed method is evaluated using 8,054 synthetic and real-life alignment\nproblems, demonstrating its ability to efficiently compute optimal alignments\nby significantly outperforming the current state of the art. By enabling\nprocess analysts to more effectively identify and understand conformance\nissues, DeclareAligner has the potential to drive meaningful process\nimprovement and management.",
        "Data augmentation is a crucial step in the development of robust supervised\nlearning models, especially when dealing with limited datasets. This study\nexplores interpolation techniques for the augmentation of geo-referenced data,\nwith the aim of predicting the presence of Commelina benghalensis L. in\nsugarcane plots in La R{\\'e}union. Given the spatial nature of the data and the\nhigh cost of data collection, we evaluated two interpolation approaches:\nGaussian processes (GPs) with different kernels and kriging with various\nvariograms. The objectives of this work are threefold: (i) to identify which\ninterpolation methods offer the best predictive performance for various\nregression algorithms, (ii) to analyze the evolution of performance as a\nfunction of the number of observations added, and (iii) to assess the spatial\nconsistency of augmented datasets. The results show that GP-based methods, in\nparticular with combined kernels (GP-COMB), significantly improve the\nperformance of regression algorithms while requiring less additional data.\nAlthough kriging shows slightly lower performance, it is distinguished by a\nmore homogeneous spatial coverage, a potential advantage in certain contexts.",
        "The combination of LLM agents with external tools enables models to solve\ncomplex tasks beyond their knowledge base. Human-designed tools are inflexible\nand restricted to solutions within the scope of pre-existing tools created by\nexperts. To address this problem, we propose ATLASS, an advanced tool learning\nand selection system designed as a closed-loop framework. It enables the LLM to\nsolve problems by dynamically generating external tools on demand. In this\nframework, agents play a crucial role in orchestrating tool selection,\nexecution, and refinement, ensuring adaptive problem-solving capabilities. The\noperation of ATLASS follows three phases: The first phase, Understanding Tool\nRequirements, involves the Agents determining whether tools are required and\nspecifying their functionality; the second phase, Tool Retrieval\/Generation,\ninvolves the Agents retrieving or generating tools based on their availability;\nand the third phase, Task Solving, involves combining all the component tools\nnecessary to complete the initial task. The Tool Dataset stores the generated\ntools, ensuring reusability and minimizing inference cost. Current LLM-based\ntool generation systems have difficulty creating complex tools that need APIs\nor external packages. In ATLASS, we solve the problem by automatically setting\nup the environment, fetching relevant API documentation online, and using a\nPython interpreter to create a reliable, versatile tool that works in a wider\nrange of situations. OpenAI GPT-4.0 is used as the LLM agent, and safety and\nethical concerns are handled through human feedback before executing generated\ncode. By addressing the limitations of predefined toolsets and enhancing\nadaptability, ATLASS serves as a real-world solution that empowers users with\ndynamically generated tools for complex problem-solving.",
        "Large language models demonstrate remarkable capabilities across various\ndomains, especially mathematics and logic reasoning. However, current\nevaluations overlook physics-based reasoning - a complex task requiring physics\ntheorems and constraints. We present PhysReason, a 1,200-problem benchmark\ncomprising knowledge-based (25%) and reasoning-based (75%) problems, where the\nlatter are divided into three difficulty levels (easy, medium, hard). Notably,\nproblems require an average of 8.1 solution steps, with hard requiring 15.6,\nreflecting the complexity of physics-based reasoning. We propose the Physics\nSolution Auto Scoring Framework, incorporating efficient answer-level and\ncomprehensive step-level evaluations. Top-performing models like Deepseek-R1,\nGemini-2.0-Flash-Thinking, and o3-mini-high achieve less than 60% on\nanswer-level evaluation, with performance dropping from knowledge questions\n(75.11%) to hard problems (31.95%). Through step-level evaluation, we\nidentified four key bottlenecks: Physics Theorem Application, Physics Process\nUnderstanding, Calculation, and Physics Condition Analysis. These findings\nposition PhysReason as a novel and comprehensive benchmark for evaluating\nphysics-based reasoning capabilities in large language models. Our code and\ndata will be published at https:\/dxzxy12138.github.io\/PhysReason.",
        "This paper proposes that Artificial Intelligence (AI) progresses through\nseveral overlapping generations: AI 1.0 (Information AI), AI 2.0 (Agentic AI),\nAI 3.0 (Physical AI), and now a speculative AI 4.0 (Conscious AI). Each of\nthese AI generations is driven by shifting priorities among algorithms,\ncomputing power, and data. AI 1.0 ushered in breakthroughs in pattern\nrecognition and information processing, fueling advances in computer vision,\nnatural language processing, and recommendation systems. AI 2.0 built on these\nfoundations through real-time decision-making in digital environments,\nleveraging reinforcement learning and adaptive planning for agentic AI\napplications. AI 3.0 extended intelligence into physical contexts, integrating\nrobotics, autonomous vehicles, and sensor-fused control systems to act in\nuncertain real-world settings. Building on these developments, AI 4.0 puts\nforward the bold vision of self-directed AI capable of setting its own goals,\norchestrating complex training regimens, and possibly exhibiting elements of\nmachine consciousness. This paper traces the historical foundations of AI\nacross roughly seventy years, mapping how changes in technological bottlenecks\nfrom algorithmic innovation to high-performance computing to specialized data,\nhave spurred each generational leap. It further highlights the ongoing\nsynergies among AI 1.0, 2.0, 3.0, and 4.0, and explores the profound ethical,\nregulatory, and philosophical challenges that arise when artificial systems\napproach (or aspire to) human-like autonomy. Ultimately, understanding these\nevolutions and their interdependencies is pivotal for guiding future research,\ncrafting responsible governance, and ensuring that AI transformative potential\nbenefits society as a whole.",
        "We present components of an AI-assisted academic writing system including\ncitation recommendation and introduction writing. The system recommends\ncitations by considering the user's current document context to provide\nrelevant suggestions. It generates introductions in a structured fashion,\nsituating the contributions of the research relative to prior work. We\ndemonstrate the effectiveness of the components through quantitative\nevaluations. Finally, the paper presents qualitative research exploring how\nresearchers incorporate citations into their writing workflows. Our findings\nindicate that there is demand for precise AI-assisted writing systems and\nsimple, effective methods for meeting those needs.",
        "In this paper, we explore the bounded cohomology of quandles and its\napplications to knot theory. We establish two key results that provide\nsufficient conditions for the infinite dimensionality of the second bounded\ncohomology of quandles. The first condition involves a subspace of homogeneous\ngroup quasimorphisms on the inner automorphism group of the quandle, whereas\nthe second condition concerns the vanishing of the stable commutator length on\na subgroup of this inner automorphism group. As topological applications, we\nshow that the second bounded cohomology of the quandle of any non-split link\nwhose link group is non-solvable as well as the quandle of any split link, is\ninfinite dimensional. From these results, we conclude that the second bounded\ncohomology of the knot quandle detects the unknot. On the algebraic side, we\nprove that the second bounded cohomology of a free product of quandles is\ninfinite dimensional if the inner automorphism group of at least one of the\nfree factors is amenable. This leads to the result that the second bounded\ncohomology of free quandles of rank greater than one, as well as their\ncanonical quotients, is infinite dimensional.",
        "Writing comprehensive and accurate descriptions of technical drawings in\npatent documents is crucial to effective knowledge sharing and enabling the\nreplication and protection of intellectual property. However, automation of\nthis task has been largely overlooked by the research community. To this end,\nwe introduce PatentDesc-355K, a novel large-scale dataset containing ~355K\npatent figures along with their brief and detailed textual descriptions\nextracted from more than 60K US patent documents. In addition, we propose\nPatentLMM - a novel multimodal large language model specifically tailored to\ngenerate high-quality descriptions of patent figures. Our proposed PatentLMM\ncomprises two key components: (i) PatentMME, a specialized multimodal vision\nencoder that captures the unique structural elements of patent figures, and\n(ii) PatentLLaMA, a domain-adapted version of LLaMA fine-tuned on a large\ncollection of patents. Extensive experiments demonstrate that training a vision\nencoder specifically designed for patent figures significantly boosts the\nperformance, generating coherent descriptions compared to fine-tuning\nsimilar-sized off-the-shelf multimodal models. PatentDesc-355K and PatentLMM\npave the way for automating the understanding of patent figures, enabling\nefficient knowledge sharing and faster drafting of patent documents. We make\nthe code and data publicly available.",
        "Federated learning is a distributed machine learning approach where multiple\nclients collaboratively train a model without sharing their local data, which\ncontributes to preserving privacy. A challenge in federated learning is\nmanaging heterogeneous data distributions across clients, which can hinder\nmodel convergence and performance due to the need for the global model to\ngeneralize well across diverse local datasets. We propose to use local\ncharacteristic statistics, by which we mean some statistical properties\ncalculated independently by each client using only their local training\ndataset. These statistics, such as means, covariances, and higher moments, are\nused to capture the characteristics of the local data distribution. They are\nnot shared with other clients or a central node. During training, these local\nstatistics help the model learn how to condition on the local data\ndistribution, and during inference, they guide the client's predictions. Our\nexperiments show that this approach allows for efficient handling of\nheterogeneous data across the federation, has favorable scaling compared to\napproaches that directly try to identify peer nodes that share distribution\ncharacteristics, and maintains privacy as no additional information needs to be\ncommunicated.",
        "For a given pair of numbers $(d,k)$, we establish a lower bound on the number\nof vertices in pure $d$-dimensional simplicial complexes with non-trivial\nhomology in dimension $k$, and prove that this bound is tight. Furthermore, we\nsolve the problem under the additional constraint of strong connectivity with\nrespect to any intermediate dimension.",
        "Answer Set Programming (ASP) is a prominent problem-modeling and solving\nframework, whose solutions are called answer sets. Epistemic logic programs\n(ELP) extend ASP to reason about all or some answer sets. Solutions to an ELP\ncan be seen as consequences over multiple collections of answer sets, known as\nworld views. While the complexity of propositional programs is well studied,\nthe non-ground case remains open. This paper establishes the complexity of\nnon-ground ELPs. We provide a comprehensive picture for well-known program\nfragments, which turns out to be complete for the class NEXPTIME with access to\noracles up to \\Sigma^P_2. In the quantitative setting, we establish complexity\nresults for counting complexity beyond #EXP. To mitigate high complexity, we\nestablish results in case of bounded predicate arity, reaching up to the fourth\nlevel of the polynomial hierarchy. Finally, we provide ETH-tight runtime\nresults for the parameter treewidth, which has applications in quantitative\nreasoning, where we reason on (marginal) probabilities of epistemic literals.",
        "Text-to-audio (TTA) systems have recently demonstrated strong performance in\nsynthesizing monaural audio from text. However, the task of generating binaural\nspatial audio from text, which provides a more immersive auditory experience by\nincorporating the sense of spatiality, have not been explored yet. In this\nwork, we introduce text-guided binaural audio generation. As an early effort,\nwe focus on the scenario where a monaural reference audio is given\nadditionally. The core problem is to associate specific sound events with their\ndirections, thereby creating binaural spatial audio. The challenge lies in the\ncomplexity of textual descriptions and the limited availability of\nsingle-source sound event datasets. To address this, we propose AudioSpa, an\nend-to-end model that applies large language models to process both acoustic\nand textual information. We employ fusion multi-head attention (FMHA) to\nintegrate text tokens, which enhances the generation capability of the\nmultimodal learning. Additionally, we propose a binaural source localization\nmodel to assess the quality of the generated audio. Finally, we design a data\naugmentation strategy to generate diverse datasets, which enables the model to\nspatialize sound events across various spatial positions. Experimental results\ndemonstrate that our model is able to put sounds at the specified locations\naccurately. It achieves competitive performance in both localization accuracy\nand signal distortion. Our demonstrations are available at\nhttps:\/\/linfeng-feng.github.io\/AudioSpa-demo.",
        "Let $C$ be a binary code of length $n$ with distances $0<d_1<\\cdots<d_s\\le\nn$. In this note we prove a general upper bound on the size of $C$ without any\nrestriction on the distances $d_i$. The bound is asymptotically optimal.",
        "Clinical trials are the gold standard for assessing the effectiveness and\nsafety of drugs for treating diseases. Given the vast design space of drug\nmolecules, elevated financial cost, and multi-year timeline of these trials,\nresearch on clinical trial outcome prediction has gained immense traction.\nAccurate predictions must leverage data of diverse modes such as drug\nmolecules, target diseases, and eligibility criteria to infer successes and\nfailures. Previous Deep Learning approaches for this task, such as HINT, often\nrequire wet lab data from synthesized molecules and\/or rely on prior knowledge\nto encode interactions as part of the model architecture. To address these\nlimitations, we propose a light-weight attention-based model, MEXA-CTP, to\nintegrate readily-available multi-modal data and generate effective\nrepresentations via specialized modules dubbed \"mode experts\", while avoiding\nhuman biases in model design. We optimize MEXA-CTP with the Cauchy loss to\ncapture relevant interactions across modes. Our experiments on the Trial\nOutcome Prediction (TOP) benchmark demonstrate that MEXA-CTP improves upon\nexisting approaches by, respectively, up to 11.3% in F1 score, 12.2% in PR-AUC,\nand 2.5% in ROC-AUC, compared to HINT. Ablation studies are provided to\nquantify the effectiveness of each component in our proposed method.",
        "In this paper, we consider the Cauchy problem for a non-homogeneous wave\nequation generated by the fractional Laplacian and involving different kinds of\nlower order terms. We allow the equation coefficients and data to be of\ndistributional type or less regular, having in mind the Dirac delta function\nand its powers, and we prove that the problem is well-posed in the sense of the\nconcept of very weak solutions. Moreover, we prove the uniqueness in an\nappropriate sense and the coherence of the very weak solution concept with\nclassical theory.",
        "Significant advancements have been achieved in the realm of understanding\nposes and interactions of two hands manipulating an object. The emergence of\naugmented reality (AR) and virtual reality (VR) technologies has heightened the\ndemand for real-time performance in these applications. However, current\nstate-of-the-art models often exhibit promising results at the expense of\nsubstantial computational overhead. In this paper, we present a query-optimized\nreal-time Transformer (QORT-Former), the first Transformer-based real-time\nframework for 3D pose estimation of two hands and an object. We first limit the\nnumber of queries and decoders to meet the efficiency requirement. Given\nlimited number of queries and decoders, we propose to optimize queries which\nare taken as input to the Transformer decoder, to secure better accuracy: (1)\nwe propose to divide queries into three types (a left hand query, a right hand\nquery and an object query) and enhance query features (2) by using the contact\ninformation between hands and an object and (3) by using three-step update of\nenhanced image and query features with respect to one another. With proposed\nmethods, we achieved real-time pose estimation performance using just 108\nqueries and 1 decoder (53.5 FPS on an RTX 3090TI GPU). Surpassing\nstate-of-the-art results on the H2O dataset by 17.6% (left hand), 22.8% (right\nhand), and 27.2% (object), as well as on the FPHA dataset by 5.3% (right hand)\nand 10.4% (object), our method excels in accuracy. Additionally, it sets the\nstate-of-the-art in interaction recognition, maintaining real-time efficiency\nwith an off-the-shelf action recognition module.",
        "We propose a general framework for the Discontinuous Galerkin-induced Neural\nNetwork (DGNN), inspired by the Interior Penalty Discontinuous Galerkin Method\n(IPDGM). In this approach, the trial space consists of piecewise neural network\nspace defined over the computational domain, while the test function space is\ncomposed of piecewise polynomials. We demonstrate the advantages of DGNN in\nterms of accuracy and training efficiency across several numerical examples,\nincluding stationary and time-dependent problems. Specifically, DGNN easily\nhandles high perturbations, discontinuous solutions, and complex geometric\ndomains.",
        "We propose a blockchain architecture in which mining requires a quantum\ncomputer. The consensus mechanism is based on proof of quantum work, a\nquantum-enhanced alternative to traditional proof of work that leverages\nquantum supremacy to make mining intractable for classical computers. We have\nrefined the blockchain framework to incorporate the probabilistic nature of\nquantum mechanics, ensuring stability against sampling errors and hardware\ninaccuracies. To validate our approach, we implemented a prototype blockchain\non four D-Wave$^{\\rm TM}$ quantum annealing processors geographically\ndistributed within North America, demonstrating stable operation across\nhundreds of thousands of quantum hashing operations. Our experimental protocol\nfollows the same approach used in the recent demonstration of quantum supremacy\n[1], ensuring that classical computers cannot efficiently perform the same\ncomputation task. By replacing classical machines with quantum systems for\nmining, it is possible to significantly reduce the energy consumption and\nenvironmental impact traditionally associated with blockchain mining. Beyond\nserving as a proof of concept for a meaningful application of quantum\ncomputing, this work highlights the potential for other near-term quantum\ncomputing applications using existing technology.",
        "Modularity is a graph parameter measuring how clearly the set of graph\nvertices may be partitioned into subsets of high edge density. It indicates the\npresence of community structure in the graph. We study its value for a random\npreferential attachment model $G_n^h$ introduced by Barab\\'asi and Albert in\n1999. A graph $G_n^h$ is created from some finite starting graph by adding new\nvertices one by one. A new vertex always connects to $h\\geq1$ already existing\nvertices and those are chosen with probability proportional to their current\ndegrees. We prove that modularity of $G_n^h$ is with high probability upper\nbounded by a function tending to $0$ with $h$ tending to infinity. This\nresolves the conjecture of Prokhorenkova, Pralat and Raigorodskii from 2016. As\na byproduct we obtain novel concentration results for the volume and the edge\ndensity parameters of subsets of $G_n^h$.",
        "Electromagnetic emissions, known as Brunel radiations, are produced in\nplasmas through the coupling between the free electron density and ultrafast\nionizing laser pulses. The radiation spectrum generated in laser-gas\ninteractions is here investigated from a local current model for laser drivers\nwith two frequency components - or \"colors\" - being not necessarily integers of\none another. We provide a general description of this spectrum by deriving\nanalytically the convolution product of the Fourier transforms of the electron\ndensity and of the laser electric field. Our analysis reveals that the only\nknowledge of the optical field extrema in time domain is sufficient to\nreproduce faithfully the numerically-computed Brunel spectrum and justify the\nemergence of various resonance frequencies. The classical combination of two\nlaser harmonics, i.e., a fundamental and its second harmonic, is also\naddressed.",
        "We propose a straightforward method to determine the maximal entanglement of\npure states using the criterion of maximal I-concurrence, a measure of\nentanglement. The square of concurrence for a bipartition $X|X^\\prime$ of a\npure state is defined as $E^2_{X| X ^\\prime}=2[1-tr({\\rho_X}^2)]$. From this,\nwe can infer that the concurrence $E_{X| X ^\\prime}$ reaches its maximum when\n$tr({\\rho_X}^2)$ is minimized. Using this approach, we identify numerous\nAbsolutely Maximally Entangled (AME) pure states that exhibit maximal\nentanglement across all possible bipartitions. Conditions are derived for pure\nstates to achieve maximal mixedness in all bipartitions, revealing that any\npure state with an odd number of subsystem coefficients does not meet the AME\ncriterion. Furthermore, we obtain equal maximal multipartite entangled pure\nstates across all bipartitions using our maximal concurrence criterion."
      ]
    }
  },
  {
    "id":2411.07031,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"Accuracy of Chatbots in Citing Journal Articles",
    "start_abstract":"This cross-sectional study quantifies the journal article citation error rate of an artificial intelligence chatbot.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "ChatGPT Hallucinates Non-existent Citations: Evidence from Economics"
      ],
      "abstract":[
        "In this study, we generate prompts derived from every topic within the Journal of Economic Literature to assess abilities both GPT-3.5 and GPT-4 versions ChatGPT large language model (LLM) write about economic concepts. demonstrates considerable competency in offering general summaries but also cites non-existent references. More than 30% citations provided by version do not exist rate is only slightly reduced for version. Additionally, our findings suggest that reliability decreases as become more specific. We provide quantitative evidence errors output demonstrate importance LLM verification. JEL Codes: B4; O33; I2"
      ],
      "categories":[
        "q-fin.ST"
      ]
    },
    "list":{
      "title":[
        "A mixture transition distribution approach to portfolio optimization",
        "Impermanent loss and Loss-vs-Rebalancing II",
        "Evaluating the resilience of ESG investments in European Markets during\n  turmoil periods",
        "Considerations on the use of financial ratios in the study of family\n  businesses",
        "Shapley-Scarf Markets with Objective Indifferences",
        "A multi-factor model for improved commodity pricing: Calibration and an\n  application to the oil market",
        "Analyzing Communicability and Connectivity in the Indian Stock Market\n  During Crises",
        "Pricing American options under rough volatility using deep-signatures\n  and signature-kernels",
        "Dynamic Factor Correlation Model",
        "Matrix H-theory approach to stock market fluctuations",
        "Will artificial intelligence accelerate or delay the race between\n  nuclear energy technology budgeting and net-zero emissions?",
        "Stochastic Optimal Control of Iron Condor Portfolios for Profitability\n  and Risk Management",
        "Analysis of the Impact of the Union Budget Announcements on the Indian\n  Stock Market: A Fractal Perspective",
        "Existence of optimal controls for stochastic partial differential\n  equations with fully local monotone coefficients",
        "Spin-polarized STM measurement scheme for quantum geometric tensor",
        "Mechanism of tulip flame formation in highly reactive and low reactive\n  gas mixtures",
        "Poincar\\'{e} sphere engineering of dynamical ferroelectric topological\n  solitons",
        "An Optimal Transport approach to arbitrage correction: Application to\n  volatility Stress-Tests",
        "Comment on: \"2005 VL1 is not Venera-2\"",
        "Balancing Flexibility and Interpretability: A Conditional Linear Model\n  Estimation via Random Forest",
        "Numerical simulation of a fine-tunable F\\\"oppl-von K\\'arm\\'an model for\n  foldable and bilayer plates",
        "Bring the noise: exact inference from noisy simulations in collider\n  physics",
        "Quantum Hamiltonian Descent for Non-smooth Optimization",
        "On the spatial distribution of luminous blue variables in the M33 galaxy",
        "The existence of pyramidal Steiner triple systems over abelian groups",
        "Change Point Detection for Random Objects with Possibly Periodic\n  Behavior",
        "Rotating and non-linear magnetic-charged black hole with an anisotropic\n  matter field",
        "SUSY transformation as the coupler of non-interacting systems"
      ],
      "abstract":[
        "Understanding the dependencies among financial assets is critical for\nportfolio optimization. Traditional approaches based on correlation networks\noften fail to capture the nonlinear and directional relationships that exist in\nfinancial markets. In this study, we construct directed and weighted financial\nnetworks using the Mixture Transition Distribution (MTD) model, offering a\nricher representation of asset interdependencies. We apply local assortativity\nmeasures--metrics that evaluate how assets connect based on similarities or\ndifferences--to guide portfolio selection and allocation. Using data from the\nDow Jones 30, Euro Stoxx 50, and FTSE 100 indices constituents, we show that\nportfolios optimized with network-based assortativity measures consistently\noutperform the classical mean-variance framework. Notably, modalities in which\nassets with differing characteristics connect enhance diversification and\nimprove Sharpe ratios. The directed nature of MTD-based networks effectively\ncaptures complex relationships, yielding portfolios with superior risk-adjusted\nreturns. Our findings highlight the utility of network-based methodologies in\nfinancial decision-making, demonstrating their ability to refine portfolio\noptimization strategies. This work thus underscores the potential of leveraging\nadvanced financial networks to achieve enhanced performance, offering valuable\ninsights for practitioners and setting a foundation for future research.",
        "This paper examines the relationship between impermanent loss (IL) and\nloss-versus-rebalancing (LVR) in automated market makers (AMMs). Our main focus\nis on statistical properties, the impact of fees, the role of block times, and,\nrelated to the latter, the continuous time limit. We find there are three\nrelevant regimes: (i) very short times where LVR and IL are identical; (ii)\nintermediate time where LVR and IL show distinct distribution functions but are\nconnected via the central limit theorem exhibiting the same expectation value;\n(iii) long time behavior where both the distribution functions and averages are\ndistinct. Subsequently, we study how fees change this dynamics with a special\nfocus on competing time scales like block times and 'arbitrage times'.",
        "This study investigates the resilience of Environmental, Social, and\nGovernance (ESG) investments during periods of financial instability, comparing\nthem with traditional equity indices across major European markets-Germany,\nFrance, and Italy. Using daily returns from October 2021 to February 2024, the\nanalysis explores the effects of key global disruptions such as the Covid-19\npandemic and the Russia-Ukraine conflict on market performance. A mixture of\ntwo generalised normal distributions (MGND) and EGARCH-in-mean models are used\nto identify periods of market turmoil and assess volatility dynamics. The\nfindings indicate that during crises, ESG investments present higher volatility\nin Germany and Italy than in France. Despite some regional variations, ESG\nportfolios demonstrate greater resilience compared to traditional ones,\noffering potential risk mitigation during market shocks. These results\nunderscore the importance of integrating ESG factors into long-term investment\nstrategies, particularly in the face of unpredictable financial turmoil.",
        "Most empirical works that study the financing decisions of family businesses\nuse financial ratios. These data present asymmetry, non-normality,\nnon-linearity and even dependence on the results of the choice of which\naccounting figure goes to the numerator and denominator of the ratio. This\narticle uses compositional data analysis (CoDa) as well as classical analysis\nstrategies to compare the structure of balance sheet liabilities between family\nand non-family businesses, showing the sensitivity of the results to the\nmethodology used. The results prove the need to use appropriate methodologies\nto advance the academic discipline.",
        "In many object allocation problems, some of the objects may effectively be\nindistinguishable from each other, such as with dorm rooms or school seats. In\nsuch cases, it is reasonable to assume that agents are indifferent between\nidentical copies of the same object. We call this setting ``objective\nindifferences.'' Top trading cycles (TTC) with fixed tie-breaking has been\nsuggested and used in practice to deal with indifferences in object allocation\nproblems. Under general indifferences, TTC with fixed tie-breaking is not\nPareto efficient nor group strategy-proof. Furthermore, it may not select the\ncore, even when it exists. Under objective indifferences, agents are always and\nonly indifferent between copies of the same object. In this setting, TTC with\nfixed tie-breaking maintains Pareto efficiency, group strategy-proofness, and\ncore selection. In fact, we present domain characterization results which\ntogether show that objective indifferences is the most general setting where\nTTC with fixed tie-breaking maintains these important properties.",
        "We present a new model for commodity pricing that enhances accuracy by\nintegrating four distinct risk factors: spot price, stochastic volatility,\nconvenience yield, and stochastic interest rates. While the influence of these\nfour variables on commodity futures prices is well recognized, their combined\neffect has not been addressed in the existing literature. We fill this gap by\nproposing a model that effectively captures key stylized facts including a\ndynamic correlation structure and time-varying risk premiums. Using a Kalman\nfilter-based framework, we achieve simultaneous estimation of parameters while\nfiltering state variables through the joint term structure of futures prices\nand bond yields. We perform an empirical analysis focusing on crude oil\nfutures, where we benchmark our model against established approaches. The\nresults demonstrate that the proposed four-factor model effectively captures\nthe complexities of futures term structures and outperforms existing models.",
        "In financial networks, information does not always follow the shortest path\nbetween two nodes but may also take alternate routes. Communicability, a\nnetwork measure, resolves this complexity and, in diffusion-like processes,\nprovides a reliable measure of the ease with which information flows between\nnodes. As a result, communicability appears to be an important measure for\ndetecting disturbances in connectivity within financial systems, similar to\ninstability caused by periods of high volatility. This study investigates the\nevolution of communicability measures in the stock networks during periods of\ncrises, showing how systemic shocks strengthen the pairwise interdependence\nbetween stocks in the financial market. In this study, the permutation test\nreveals that approximately 83.5 per cent of stock pairs were found to be\nstatistically significant at the significance level of 0.001 and have an\nincrease in the shortest communicability path length during the crisis than the\nnormal days, indicating enhanced interdependence and heightened information\nflow in the market. Furthermore, we show that when employed as features in the\nclassification model, the network shortest path-based measures, along with\ncommunicability measures, are able to accurately classify between the times\nperiods of market stability and volatility. Additionally, our results show that\nthe geometric measures perform better in terms of classification accuracy than\ntopological measures. These findings provide important insights into market\nbehaviour during times of increased volatility and advance our understanding of\nthe financial market crisis.",
        "We extend the signature-based primal and dual solutions to the optimal\nstopping problem recently introduced in [Bayer et al.: Primal and dual optimal\nstopping with signatures, to appear in Finance & Stochastics 2025], by\nintegrating deep-signature and signature-kernel learning methodologies. These\napproaches are designed for non-Markovian frameworks, in particular enabling\nthe pricing of American options under rough volatility. We demonstrate and\ncompare the performance within the popular rough Heston and rough Bergomi\nmodels.",
        "We introduce a new dynamic factor correlation model with a novel\nvariation-free parametrization of factor loadings. The model is applicable to\nhigh dimensions and can accommodate time-varying correlations, heterogeneous\nheavy-tailed distributions, and dependent idiosyncratic shocks, such as those\nobserved in returns on stocks in the same subindustry. We apply the model to a\n\"small universe\" with 12 asset returns and to a \"large universe\" with 323 asset\nreturns. The former facilitates a comprehensive empirical analysis and\ncomparisons and the latter demonstrates the flexibility and scalability of the\nmodel.",
        "We introduce matrix H theory, a framework for analyzing collective behavior\narising from multivariate stochastic processes with hierarchical structure. The\ntheory models the joint distribution of the multiple variables (the measured\nsignal) as a compound of a large-scale multivariate distribution with the\ndistribution of a slowly fluctuating background. The background is\ncharacterized by a hierarchical stochastic evolution of internal degrees of\nfreedom, representing the correlations between stocks at different time scales.\nAs in its univariate version, the matrix H-theory formalism also has two\nuniversality classes: Wishart and inverse Wishart, enabling a concise\ndescription of both the background and the signal probability distributions in\nterms of Meijer G-functions with matrix argument. Empirical analysis of daily\nreturns of stocks within the S&P500 demonstrates the effectiveness of matrix H\ntheory in describing fluctuations in stock markets. These findings contribute\nto a deeper understanding of multivariate hierarchical processes and offer\npotential for developing more informed portfolio strategies in financial\nmarkets.",
        "This study explores the impact of nuclear energy technology budgeting and\nartificial intelligence on carbon dioxide (CO2) emissions in 20 OECD economies.\nUnlike previous research that relied on conventional panel techniques, we\nutilize the Method of Moment Quantile Regression panel data estimation\ntechniques. This approach provides quantile-specific insights while addressing\nissues of endogeneity and heteroscedasticity, resulting in a more nuanced and\nrobust understanding of complex relationships. A novel aspect of this research\nwork is introducing the moderating effect of artificial intelligence on the\nrelationship between nuclear energy and CO2 emissions. The results found that\nthe direct impact of artificial intelligence on CO2 emissions is significant,\nwhile the effect of nuclear energy technology budgeting is not. Additionally,\nartificial intelligence moderates the relationship between nuclear energy\ntechnology budgeting and CO2 emissions, aiding nuclear energy in reducing\ncarbon emissions across OECD countries. Our findings indicate that\ntransitioning to a low-carbon future is achievable by replacing fossil fuel\nenergy sources with increased integration of artificial intelligence to promote\nnuclear energy technologies. This study demonstrates that energy innovations\ncan serve as effective climate-resilience strategies to mitigate the impacts of\nclimate change.",
        "Previous research on option strategies has primarily focused on their\nbehavior near expiration, with limited attention to the transient value process\nof the portfolio. In this paper, we formulate Iron Condor portfolio\noptimization as a stochastic optimal control problem, examining the impact of\nthe control process \\( u(k_i, \\tau) \\) on the portfolio's potential\nprofitability and risk. By assuming the underlying price process as a bounded\nmartingale within $[K_1, K_2]$, we prove that the portfolio with a strike\nstructure of $k_1 < k_2 = K_2 < S_t < k_3 = K_3 < k_4$ has a submartingale\nvalue process, which results in the optimal stopping time aligning with the\nexpiration date $\\tau = T$. Moreover, we construct a data generator based on\nthe Rough Heston model to investigate general scenarios through simulation. The\nresults show that asymmetric, left-biased Iron Condor portfolios with $\\tau =\nT$ are optimal in SPX markets, balancing profitability and risk management.\nDeep out-of-the-money strategies improve profitability and success rates at the\ncost of introducing extreme losses, which can be alleviated by using an optimal\nstopping strategy. Except for the left-biased portfolios $\\tau$ generally falls\nwithin the range of [50\\%,75\\%] of total duration. In addition, we validate\nthese findings through case studies on the actual SPX market, covering bullish,\nsideways, and bearish market conditions.",
        "The stock market closely monitors macroeconomic policy announcements, such as\nannual budget events, due to their substantial influence on various economic\nparticipants. These events tend to impact the stock markets initially before\naffecting the real sector. Our study aims to analyze the effects of the budget\non the Indian stock market, specifically focusing on the announcement for the\nyear 2024. We will compare this with the years 2023, 2022, and 2020, assessing\nits impact on the NIFTY50 index using average abnormal return (AAR) and\ncumulative average abnormal return (CAAR) over a period of -15 and +15 days,\nincluding the budget day. This study utilizes an innovative approach involving\nthe fractal interpolation function, paired with fractal dimensional analysis,\nto study the fluctuations arising from budget announcements. The fractal\nperspective on the data offers an effective framework for understanding complex\nvariations.",
        "This paper deals with a stochastic optimal feedback control problem for the\ncontrolled stochastic partial differential equations. More precisely, we\nestablish the existence of stochastic optimal feedback control for the\ncontrolled stochastic partial differential equations with fully monotone\ncoefficients by a minimizing sequence for the control problem. Using the\nFaedo-Galerkin approximations, the uniform estimates and the tightness in some\nappropriate space for the Faedo-Galerkin approximating solution can be obtain\nto prove the well-posedness of the controlled stochastic partial differential\nequations with fully monotone coefficients. The results obtained in the present\npaper may be applied to various types of controlled stochastic partial\ndifferential equations, such as the controlled stochastic convection diffusion\nequation.",
        "Quantum geometric tensor (QGT) reflects the geometry of the eigenstates of a\nsystem's Hamiltonian. The full characterization of QGT is essential for various\nquantum systems. However, it is challenging to characterize the QGT of the\nsolid-state systems. Here we present a scheme by using spin-polarized STM to\nmeasure QGT of two-dimensional solid-state systems, in which the spin texture\nis extracted from geometric amplitudes of Friedel oscillations induced by the\nintentionally introduced magnetic impurity and then the QGT is derived from the\nmomentum differential of spin texture. The surface states of topological\ninsulator (TISS), as a model spin system, is promising to demonstrate the\nscheme. In a TI slab, the gapped TISS host finite quantum metric and Berry\ncurvature as the symmetric real part and the antisymmetric imaginary part of\nQGT, respectively. Thus, a detailed calculations guide the use of the developed\nscheme to measure the QGT of gapped TISS with or without an external in-plane\nmagnetic field. This study provides a feasible scheme for measuring QGT of\ntwo-dimensional solid-state systems, and hints at the great potential of the\ninformation extraction from the geometric amplitudes of STM and other\nmeasurement.",
        "The early stages of flame dynamics and the development and evolution of tulip\nflames in closed tubes of various aspect ratios and in a semi-open tube are\nstudied by solving the fully compressible reactive Navier-Stokes equations\nusing a high-order numerical method coupled to detailed chemical models in a\nstoichiometric hydrogen\/air and methane\/air mixtures. The use of adaptive mesh\nrefinement provides adequate resolution of the flame reaction zone, pressure\nwaves, and flame-pressure wave interactions. The purpose of this study is to\ngain a deeper insight into the influence of chemical kinetics on the combustion\nregimes leading to the formation of a tulip flame and its subsequent evolution.\nThe simulations highlight the effect of flame thickness, flame velocity, and\nreaction order on the intensity of the rarefaction wave generated by the flame\nduring the deceleration phase, which is the principal physical mechanism of\ntulip flame formation. The obtained results explain most of the experimentally\nobserved features of tulip flame formation, e.g. faster tulip flame formation\nwith deeper tulip shape for faster flames compared to slower flames.",
        "Geometric representation lays the basis for understanding and flexible tuning\nof topological transitions in many physical systems. An example is given by the\nPoincar\\'{e} sphere (PS) that provides an intuitive and continuous\nparameterization of the spin or orbital angular momentum (OAM) light states.\nHere, we apply this geometric construction to understand and continuously\nencode dynamical topologies of ferroelectric solitons driven by OAM-tunable\nlight. We show that: (1) PS engineering enables controlled creation of dynamic\npolar antiskyrmions that are rarely found in ferroelectrics; (2) We link such\ntopological transition to the tuning of the light beam as a ``knob'' from OAM\n(PS pole) to non-OAM (PS equator) modes; (3) Intermediate OAM-state structured\nlight results in new ferroelectric topologies of temporally hybrid\nskyrmion-antiskyrmion states. Our study offers new approaches of robust control\nand flexible tuning of topologies of matter using structured light.",
        "We present a method based on optimal transport to remove arbitrage\nopportunities within a finite set of option prices. The method is notably\nintended for regulatory stress-tests, which impose to apply important local\ndistortions to implied volatility surfaces. The resulting stressed option\nprices are naturally associated to a family of signed marginal measures: we\nformulate the process of removing arbitrage as a projection onto the subset of\nmartingale measures with respect to a Wasserstein metric in the space of signed\nmeasures. We show how this projection problem can be recast as an optimal\ntransport problem; in view of the numerical solution, we apply an entropic\nregularization technique. For the regularized problem, we derive a strong\nduality formula, show convergence results as the regularization parameter\napproaches zero, and formulate a multi-constrained Sinkhorn algorithm, where\neach iteration involves, at worse, finding the root of an explicit scalar\nfunction. The convergence of this algorithm is also established. We compare our\nmethod with the existing approach by [Cohen, Reisinger and Wang, Appl.\\ Math.\\\nFin.\\ 2020] across various scenarios and test cases.",
        "I show that the small differences between the orbital parameters of the dark\ncomet 2005 VL1 and the Venera 2 spacecraft (reported in arXiv:2503.07972) are\nof the magnitude expected from gravitational deflection by a close encounter of\nVenera 2 with Venus.",
        "Traditional parametric econometric models often rely on rigid functional\nforms, while nonparametric techniques, despite their flexibility, frequently\nlack interpretability. This paper proposes a parsimonious alternative by\nmodeling the outcome $Y$ as a linear function of a vector of variables of\ninterest $\\boldsymbol{X}$, conditional on additional covariates\n$\\boldsymbol{Z}$. Specifically, the conditional expectation is expressed as\n$\\mathbb{E}[Y|\\boldsymbol{X},\\boldsymbol{Z}]=\\boldsymbol{X}^{T}\\boldsymbol{\\beta}(\\boldsymbol{Z})$,\nwhere $\\boldsymbol{\\beta}(\\cdot)$ is an unknown Lipschitz-continuous function.\nWe introduce an adaptation of the Random Forest (RF) algorithm to estimate this\nmodel, balancing the flexibility of machine learning methods with the\ninterpretability of traditional linear models. This approach addresses a key\nchallenge in applied econometrics by accommodating heterogeneity in the\nrelationship between covariates and outcomes. Furthermore, the heterogeneous\npartial effects of $\\boldsymbol{X}$ on $Y$ are represented by\n$\\boldsymbol{\\beta}(\\cdot)$ and can be directly estimated using our proposed\nmethod. Our framework effectively unifies established parametric and\nnonparametric models, including varying-coefficient, switching regression, and\nadditive models. We provide theoretical guarantees, such as pointwise and\n$L^p$-norm rates of convergence for the estimator, and establish a pointwise\ncentral limit theorem through subsampling, aiding inference on the function\n$\\boldsymbol\\beta(\\cdot)$. We present Monte Carlo simulation results to assess\nthe finite-sample performance of the method.",
        "A numerical scheme is proposed to identify low energy configurations of a\nF\\\"oppl-von K\\'arm\\'an model for bilayer plates. The dependency of the\ncorresponding elastic energy on the in-plane displacement $u$ and the\nout-of-plane deflection $w$ leads to a practical minimization of the functional\nvia a decoupled gradient flow. In particular, the energies of the resulting\niterates are shown to be monotonically decreasing. The discretization of the\nmodel relies on $P1$ finite elements for the horizontal part $u$ and utilizes\nthe discrete Kirchhoff triangle for the vertical component $w$. The model\nallows for analysing various different problem settings via numerical\nsimulation: (i) stable low-energy configurations are detected dependent on a\nspecified prestrain described by elastic material properties, (ii) curvature\ninversions of spherical and cylindrical configurations are investigated, (iii)\nelastic responses of foldable cardboards for different spontaneous curvatures\nand crease geometries are compared.",
        "We rely on Monte Carlo (MC) simulations to interpret searches for new physics\nat the Large Hadron Collider (LHC) and elsewhere. These simulations result in\nnoisy and approximate estimators of selection efficiencies and likelihoods. In\nthis context we pioneer an exact-approximate computational method -\nexact-approximate Markov Chain Monte Carlo - that returns exact inferences\ndespite noisy simulations. To do so, we introduce an unbiased estimator for a\nPoisson likelihood. We demonstrate the new estimator and new techniques in\nexamples based on a search for neutralinos and charginos at the LHC using a\nsimplified model. We find attractive performance characteristics - exact\ninferences are obtained for a similar computational cost to approximate ones\nfrom existing methods and inferences are robust with respect to the number of\nevents generated per point.",
        "Non-smooth optimization models play a fundamental role in various\ndisciplines, including engineering, science, management, and finance. However,\nclassical algorithms for solving such models often struggle with convergence\nspeed, scalability, and parameter tuning, particularly in high-dimensional and\nnon-convex settings. In this paper, we explore how quantum mechanics can be\nleveraged to overcome these limitations. Specifically, we investigate the\ntheoretical properties of the Quantum Hamiltonian Descent (QHD) algorithm for\nnon-smooth optimization in both continuous and discrete time. First, we propose\ncontinuous-time variants of the general QHD algorithm and establish their\nglobal convergence and convergence rate for non-smooth convex and strongly\nconvex problems through a novel Lyapunov function design. Furthermore, we prove\nthe finite-time global convergence of continuous-time QHD for non-smooth\nnon-convex problems under mild conditions (i.e., locally Lipschitz). In\naddition, we propose discrete-time QHD, a fully digitized implementation of QHD\nvia operator splitting (i.e., product formula). We find that discrete-time QHD\nexhibits similar convergence properties even with large time steps. Finally,\nnumerical experiments validate our theoretical findings and demonstrate the\ncomputational advantages of QHD over classical non-smooth non-convex\noptimization algorithms.",
        "In the current paper, we present a study of the spatial distribution of\nluminous blue variables (LBVs) and various LBV candidates (cLBVs) with respect\nto OB associations in the M33 galaxy. The identification of blue star groups\nwas based on the LGGS data and was carried out by two clustering algorithms\nwith initial parameters determined during simulations of random stellar fields.\nWe have found that the distribution of distances to the nearest OB association\nobtained for the LBV\/cLBV sample is close to that for massive stars with\n$M_{\\rm init}>20\\,M_\\odot$ and Wolf-Rayet stars. This result is in good\nagreement with the standard assumption that luminous blue variables represent\nan intermediate stage in the evolution of the most massive stars. However, some\nobjects from the LBV\/cLBV sample, particularly Fe$\\,$II-emission stars,\ndemonstrated severe isolation compared to other massive stars, which, together\nwith certain features of their spectra, implicitly indicates that the nature of\nthese objects and other LBVs\/cLBVs may differ radically.",
        "A Steiner triple system STS$(v)$ is called $f$-pyramidal if it has an\nautomorphism group fixing $f$ points and acting sharply transitively on the\nremaining ones. In this paper, we focus on the STSs that are $f$-pyramidal over\nsome abelian group. Their existence has been settled only for the smallest\nadmissible values of $f$, that is, $f=0,1,3$.\n  In this paper, we complete this result and determine, for every $f>3$, the\nspectrum of values $(f,v)$ for which there is an $f$-pyramidal STS$(v)$ over an\nabelian group. This result is obtained by constructing difference families\nrelative to a suitable partial spread.",
        "Time-varying random objects have been increasingly encountered in modern data\nanalysis. Moreover, in a substantial number of these applications, periodic\nbehavior of the random objects has been observed. We introduce a new, powerful\nscan statistic and corresponding test for the precise identification and\nlocalization of abrupt changes in the distribution of non-Euclidean random\nobjects with possibly periodic behavior. Our approach is nonparametric and\neffectively captures the entire distribution of these random objects.\nRemarkably, it operates with minimal tuning parameters, requiring only the\nspecification of cut-off intervals near endpoints, where change points are\nassumed not to occur. Our theoretical contributions include deriving the\nasymptotic distribution of the test statistic under the null hypothesis of no\nchange points, establishing the consistency of the test in the presence of\nchange points under contiguous alternatives and providing rigorous guarantees\non the near-optimal consistency in estimating the number and locations of\nchange points, whether dealing with a single change point or multiple ones. We\ndemonstrate that the most competitive method currently in the literature for\nchange point detection in random objects is degraded by periodic behavior, as\nperiodicity leads to blurring of the changes that this procedure aims to\ndiscover. Through comprehensive simulation studies, we demonstrate the superior\npower and accuracy of our approach in both detecting change points and\npinpointing their locations, across scenarios involving both periodic and\nnonperiodic random objects. Our main application is to weighted networks,\nrepresented through graph Laplacians. The proposed method delivers highly\ninterpretable results, as evidenced by the identification of meaningful change\npoints in the New York City Citi Bike sharing system that align with\nsignificant historical events.",
        "We present the solution of a non-linear magnetic-charged black hole with an\nanisotropic matter field and further extend it to obtain the corresponding\nrotating black hole solution using the modified Newman-Janis algorithm. The\nevent horizon and ergosphere of the rotating black hole are studied in terms of\nthe perspective of geometric properties, revealing that the rotating black hole\ncan have up to three horizons. The first law of thermodynamics and the\nsquared-mass formula for the rotating black hole are derived from a\nthermodynamic perspective, based on which we obtain the thermodynamic\nquantities and study the thermodynamic stability of the rotating black hole.\nAdditionally, we calculate the Penrose process for the rotating black hole,\nindicating the influence of various black hole parameters on the maximal\nefficiency of the Penrose process.",
        "Quasi-one-dimensional chains of atoms can be effectively described by\none-dimensional Dirac-type equation. Crystal structure of the chain is\nreflected by pseudo-spin of the quasi-particles. In the article, we present a\nsimple framework where supersymmetric transformation is utilized to generate an\ninteraction between two, initially non-interacting systems described by\npseudo-spin-one Dirac-type equation. In the presented example, the\ntransformation converts two asymptotically non-interacting atomic chains into a\nsaw chain locally. The model possesses a flat band whose energy can be\nfine-tuned deliberately."
      ]
    }
  },
  {
    "id":2411.06184,
    "research_type":"applied",
    "start_id":"b6",
    "start_title":"Multi-Task Bayesian Optimization",
    "start_abstract":"Bayesian optimization has recently been proposed as a framework for automatically tuning the hyperparameters of machine learning models and shown to yield state-of-the-art performance with impressive ease efficiency. In this paper, we explore whether it is possible transfer knowledge gained from previous optimizations new tasks in order find optimal hyperparameter settings more efficiently. Our approach based on extending multi-task Gaussian processes optimization. We show that method significantly speeds up process when compared standard single-task approach. further propose straightforward extension our algorithm jointly minimize average error across multiple demonstrate how can be used greatly speed k-fold cross-validation. Lastly, an adaptation developed acquisition function, entropy search, cost-sensitive, setting. utility function by leveraging small dataset hyper-parameter large dataset. dynamically chooses which query most information per unit cost.",
    "start_categories":[
      "math.OC"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "New Pathologic Classification of Lung Cancer: Relevance for Clinical Practice and Clinical Trials"
      ],
      "abstract":[
        "We summarize significant changes in pathologic classification of lung cancer resulting from the 2011 International Association for Study Lung Cancer\/American Thoracic Society\/European Respiratory Society (IASLC\/ATS\/ERS) adenocarcinoma classification. The was developed by an international core panel experts representing IASLC, ATS, and ERS with oncologists\/pulmonologists, pathologists, radiologists, molecular biologists, thoracic surgeons. Because 70% patients present advanced stages, a new approach to small biopsies cytology specific terminology criteria focused on need distinguishing squamous cell carcinoma testing EGFR mutations ALK rearrangement. Tumors previously classified as non-small-cell carcinoma, not otherwise specified, because lack clear or morphology should be further using limited immunohistochemical workup preserve tissue testing. terms \"bronchioloalveolar carcinoma\" \"mixed subtype adenocarcinoma\" have been discontinued. For resected adenocarcinomas, concepts situ minimally invasive define who, if they undergo complete resection, will 100% disease-free survival. Invasive adenocarcinomas are now predominant pattern after comprehensive histologic subtyping lepidic, acinar, papillary, solid patterns; micropapillary is added poor prognosis. Former mucinous bronchioloalveolar carcinomas called \"invasive adenocarcinoma.\" field rapidly evolving advances occurring frequent basis, particularly arena, this provides much needed standard diagnosis only patient care but also clinical trials TNM"
      ],
      "categories":[
        "q-bio.QM"
      ]
    },
    "list":{
      "title":[
        "A Bayesian Modelling Framework with Model Comparison for Epidemics with\n  Super-Spreading",
        "Beware of so-called 'good' correlations: a statistical reality check on\n  individual mRNA-protein predictions",
        "Leveraging Retrieval-Augmented Generation and Large Language Models to\n  Predict SERCA-Binding Protein Fragments from Cardiac Proteomics Data",
        "Quality of life and perceived care of patients in advanced chronic\n  kidney disease consultations: a cross-sectional descriptive study",
        "Genomic and pathological analyses of an asymmetric true hermaphroditism\n  case in a female labrador retriever",
        "Beyond Cortisol! Physiological Indicators of Welfare for Dogs: Deficits,\n  Misunderstandings and Opportunities",
        "EMICSS: Added-value annotations for EMDB entries",
        "Spatially-Structured Models of Viral Dynamics: A Scoping Review",
        "Towards Precision Oncology: Predicting Mortality and Relapse-Free\n  Survival in Head and Neck Cancer Using Clinical Data",
        "Spatial Analysis of Neuromuscular Junctions Activation in\n  Three-Dimensional Histology-based Muscle Reconstructions",
        "Measuring Fitness and Importance of Species in Food Webs",
        "Collective inference of the truth of propositions from crowd probability\n  judgments",
        "Targeting Neurodegeneration: Three Machine Learning Methods for G9a\n  Inhibitors Discovery Using PubChem and Scikit-learn",
        "Exact Learning of Permutations for Nonzero Binary Inputs with\n  Logarithmic Training Size and Quadratic Ensemble Complexity",
        "Entropy-Guided Attention for Private LLMs",
        "On the Uncertainty of a Simple Estimator for Remote Source Monitoring\n  over ALOHA Channels",
        "Unified Guidance for Geometry-Conditioned Molecular Generation",
        "In Situ Optimization of an Optoelectronic Reservoir Computer with\n  Digital Delayed Feedback",
        "Explaining 3D Computed Tomography Classifiers with Counterfactuals",
        "Detection of Rumors and Their Sources in Social Networks: A\n  Comprehensive Survey",
        "Anisotropic Exchange Spin Model to Investigate the Curie Temperature\n  Dispersion of Finite-Size L10-FePt Magnetic Nanoparticles",
        "PluralLLM: Pluralistic Alignment in LLMs via Federated Learning",
        "Fifteen Years of M31* X-ray Variability and Flares",
        "A Semantic-Loss Function Modeling Framework With Task-Oriented Machine\n  Learning Perspectives",
        "YOLO11-JDE: Fast and Accurate Multi-Object Tracking with Self-Supervised\n  Re-ID",
        "Robust Egoistic Rigid Body Localization",
        "Diminishing Waters: The Great Salt Lake's Desiccation and Its Mental\n  Health Consequences",
        "Code-Verification Techniques for an Arbitrary-Depth Electromagnetic Slot\n  Model"
      ],
      "abstract":[
        "The transmission dynamics of an epidemic are rarely homogeneous.\nSuper-spreading events and super-spreading individuals are two types of\nheterogeneous transmissibility. Inference of super-spreading is commonly\ncarried out on secondary case data, the expected distribution of which is known\nas the offspring distribution. However, this data is seldom available. Here we\nintroduce a multi-model framework fit to incidence time-series, data that is\nmuch more readily available. The framework consists of five discrete-time,\nstochastic, branching-process models of epidemics spread through a susceptible\npopulation. The framework includes a baseline model of homogeneous\ntransmission, a unimodal and a bimodal model for super-spreading events, as\nwell as a unimodal and a bimodal model for super-spreading individuals.\nBayesian statistics is used to infer model parameters using Markov Chain\nMonte-Carlo. Model comparison is conducted by computing Bayes factors, with\nimportance sampling used to estimate the marginal likelihood of each model.\nThis estimator is selected for its consistency and lower variance compared to\nalternatives. Application to simulated data from each model identifies the\ncorrect model for the majority of simulations and accurately infers the true\nparameters, such as the basic reproduction number. We also apply our methods to\nincidence data from the 2003 SARS outbreak and the Covid-19 pandemic. Model\nselection consistently identifies the same model and mechanism for a given\ndisease, even when using different time series. Our estimates are consistent\nwith previous studies based on secondary case data. Quantifying the\ncontribution of super-spreading to disease transmission has important\nimplications for infectious disease management and control. Our modelling\nframework is disease-agnostic and implemented as an R package, with potential\nto be a valuable tool for public health.",
        "Research in the life sciences often employs messenger ribonucleic acids\n(mRNA) quantification as a standalone approach for functional analysis.\nHowever, although the correlation between the measured levels of mRNA and\nproteins is positive, correlation coefficients observed empirically are\nincomplete, necessitating caution in making agnostic inferences. This essay\nprovides a statistical reflection and caveat on the concept of correlation\nstrength in the context of transcriptomics-proteomics studies. It highlights\nthe variability in possible protein levels at given empirical correlation\nvalues, even for precise mRNA amount, and underscores the notable proportion of\nmRNA-protein pairs with abundances at opposite ends of their respective\ndistributions. Cell biologists, data scientists, and biostatisticians should\nrecognise that mRNA-protein correlation alone is insufficient to justify using\na single mRNA quantification to infer the amount or function of its\ncorresponding protein.",
        "Large language models (LLMs) have shown promise in various natural language\nprocessing tasks, including their application to proteomics data to classify\nprotein fragments. In this study, we curated a limited mass spectrometry\ndataset with 1000s of protein fragments, consisting of proteins that appear to\nbe attached to the endoplasmic reticulum in cardiac cells, of which a fraction\nwas cloned and characterized for their impact on SERCA, an ER calcium pump.\nWith this limited dataset, we sought to determine whether LLMs could correctly\npredict whether a new protein fragment could bind SERCA, based only on its\nsequence and a few biophysical characteristics, such as hydrophobicity,\ndetermined from that sequence. To do so, we generated random sequences based on\ncloned fragments, embedded the fragments into a retrieval augmented generation\n(RAG) database to group them by similarity, then fine-tuned large language\nmodel (LLM) prompts to predict whether a novel sequence could bind SERCA. We\nbenchmarked this approach using multiple open-source LLMs, namely the\nMeta\/llama series, and embedding functions commonly available on the\nHuggingface repository. We then assessed the generalizability of this approach\nin classifying novel protein fragments from mass spectrometry that were not\ninitially cloned for functional characterization. By further tuning the prompt\nto account for motifs, such as ER retention sequences, we improved the\nclassification accuracy by and identified several proteins predicted to\nlocalize to the endoplasmic reticulum and bind SERCA, including Ribosomal\nProtein L2 and selenoprotein S. Although our results were based on proteomics\ndata from cardiac cells, our approach demonstrates the potential of LLMs in\nidentifying novel protein interactions and functions with very limited\nproteomic data.",
        "Objetive: In the care of renal patients, prioritising their quality of life\nand nursing care is essential. Research links patients' perceptions of care\nquality to improved outcomes such as safety, clinical efficacy, treatment\nadherence, and preventive practices. This study aimed to evaluate the quality\nof life and care perception in these patients and explore potential\nassociations between these dimensions. Material and methods: A cross-sectional\ndescriptive study was conducted with 43 patients attending an advanced CKD\nclinic. Quality of life was assessed using the KDQOL-36 questionnaire, while\nthe IECPAX questionnaire measured perceived care quality. Sociodemographic and\nclinical data were collected from patient records. Participants completed the\nquestionnaires during routine visits, with scores analysed to identify\nassociations between variables. Results: The study included 60% men (n=28) and\n32% women (n=15), with a mean age of 78 years . Among participants, 45% were\ndiabetic, 79% hypertensive, and 58% took more than five medications daily. Mean\nscores were 78.76 for KDQOL-36 and 5.54 for IECPAX. Significant differences\nwere found in the physical role domain between men and women (p=0.01) and for\nindividuals over 65 years (p=0.04). Higher IECPAX scores were associated with\ntaking more than five medications (p=0.05). However, no correlation was\nobserved between KDQOL-36 and IECPAX scores. Conclusions: The findings suggest\nthat quality of life and perceived care quality are independent in advanced CKD\npatients. While this study provides insights, larger multicentre studies are\nneeded to validate these results. These findings highlight the importance of\naddressing both aspects separately to improve outcomes in this population.",
        "The two main gonadal development disorders in dogs are true hermaphroditism\nand XX male syndrome. True hermaphroditism can be divided into two\nsubcategories: XX sex reversal and XY sex reversal. XX Sry-negative sex\nreversal is more common, and it is characterized by the presence of both\novarian and testicular tissues in an animal. To date, there are 16 cases of\ntrue hermaphroditism reported in the literature, 15 of which are XX true\nhermaphroditism. Hermaphroditism has not been formally documented in labrador\nretrievers, and no case of asymmetric hermaphroditism has been reported in the\nliterature.",
        "This paper aims to initiate new conversations about the use of physiological\nindicators when assessing the welfare of dogs. There are significant concerns\nabout construct validity - whether the measures used accurately reflect\nwelfare. The goal is to provide recommendations for future inquiry and\nencourage debate. We acknowledge that the scientific understanding of animal\nwelfare has evolved and bring attention to the shortcomings of commonly used\nbiomarkers like cortisol. These indicators are frequently used in isolation and\nwith limited salient dog descriptors, so fail to reflect the canine experience\nadequately. Using a systems approach, we explore various physiological systems\nand alternative indicators, such as heart rate variability and oxidative\nstress, to address this limitation. It is essential to consider factors like\nage, body weight, breed, and sex when interpreting these biomarkers correctly,\nand researchers should report on these in their studies. This discussion\nidentifies possible indicators for both positive and negative experiences. In\nconclusion, we advocate for a practical, evidence-based approach to assessing\nindicators of canine welfare, including non-invasive collection methods. We\nacknowledge the complexity of evaluating experiential responses in dogs across\ndifferent situations and the need for continued work to improve practices and\nrefine terminology. This will enhance our ability to accurately understand\nwelfare and improve the wellbeing of dogs, serving to inform standards of\nanimal welfare assessment. We hope this will promote more fundamental research\nin canine physiology to improve construct validity, leading to better\npractices, ultimately improving the lives of dogs.",
        "Motivation: The Electron Microscopy Data Bank (EMDB) is a key repository for\n3D electron microscopy (3DEM) data but lacks comprehensive annotations and\nconnections to most of the related biological, functional, and structural data.\nThis limitation arises from the optional nature of such information to reduce\ndepositor burden and the complexity of maintaining up-to-date external\nreferences, often requiring depositor consent. To address these challenges, we\ndeveloped EMICSS (EMDB Integration with Complexes, Structures, and Sequences),\nan independent system that automatically updates cross-references with over 20\nexternal resources, including UniProt, AlphaFold DB, PubMed, Complex Portal and\nGene Ontology. Results: EMICSS (https:\/\/www.ebi.ac.uk\/emdb\/emicss) annotations\nare accessible in multiple formats for every EMDB entry and its linked\nresources, and programmatically via the EMDB Application Programming Interface\n(API). EMICSS plays a crucial role supporting the EMDB website, with\nannotations being used on entry pages, statistics, and in the search system.\nAvailability and implementation: EMICSS is implemented in Python and it is an\nopen-source, distributed under the EMBL-EBI license, with core code available\non GitHub (https:\/\/github.com\/emdb-empiar\/added_annotations).",
        "There is growing recognition in both the experimental and modelling\nliterature of the importance of spatial structure to the dynamics of viral\ninfections in tissues. Aided by the evolution of computing power and motivated\nby recent biological insights, there has been an explosion of new,\nspatially-explicit models for within-host viral dynamics in recent years. This\ndevelopment has only been accelerated in the wake of the COVID-19 pandemic.\nSpatially-structured models offer improved biological realism and can account\nfor dynamics which cannot be well-described by conventional, mean-field\napproaches. However, despite their growing popularity, spatially-structured\nmodels of viral dynamics are underused in biological applications. One major\nobstacle to the wider application of such models is the huge variety in\napproaches taken, with little consensus as to which features should be included\nand how they should be implemented for a given biological context. Previous\nreviews of the field have focused on specific modelling frameworks or on models\nfor particular viral species. Here, we instead apply a scoping review approach\nto the literature of spatially-structured viral dynamics models as a whole to\nprovide an exhaustive update of the state of the field. Our analysis is\nstructured along two axes, methodology and viral species, in order to examine\nthe breadth of techniques used and the requirements of different biological\napplications. We then discuss the contributions of mathematical and\ncomputational modelling to our understanding of key spatially-structured\naspects of viral dynamics, and suggest key themes for future model development\nto improve robustness and biological utility.",
        "Head and neck squamous cell carcinoma (HNSCC) presents significant challenges\nin clinical oncology due to its heterogeneity and high mortality rates. This\nstudy aims to leverage clinical data and machine learning (ML) principles to\npredict key outcomes for HNSCC patients: mortality, and relapse-free survival.\nUtilizing data sourced from the Cancer Imaging Archive, an extensive pipeline\nwas implemented to ensure robust model training and evaluation. Ensemble and\nindividual classifiers, including XGBoost, Random Forest, and Support Vectors,\nwere employed to develop predictive models. The study identified key clinical\nfeatures influencing HNSCC mortality outcomes and achieved predictive accuracy\nand ROC-AUC values exceeding 90\\% across tasks. Support Vector Machine strongly\nexcelled in relapse-free survival, with an recall value of 0.99 and precision\nof 0.97. Key clinical features including loco-regional control, smoking and\ntreatment type, were identified as critical predictors of patient outcomes.\nThis study underscores the medical impact of using ML-driven insights to refine\nprognostic accuracy and optimize personalized treatment strategies in HNSCC.",
        "Histology has long been a foundational technique for studying anatomical\nstructures through tissue slicing. Advances in computational methods now enable\nthree dimensional (3D) reconstruction of organs from histology images,\nenhancing the analysis of structural and functional features. Here, we present\na novel multimodal computational method to reconstruct rodent muscles in 3D\nusing classical image processing and data analysis techniques, analyze their\nstructural features and correlate them to previously recorded\nelectrophysiological data. The algorithm analyzes spatial distribution patterns\nof features identified through histological staining, normalizing them across\nmultiple samples. Further, the algorithm successfully correlates spatial\npatterns with high density epimysial ElectroMyoGraphy (hdEMG) recordings,\nproviding a multimodal perspective on neuromuscular dynamics, linking spatial\nand electrophysiological information. The code was validated by looking at the\ndistribution of NeuroMuscular Junctions (NMJs) in naive soleus muscles and\ncompared the distributions and patterns observed with ones observed in previous\nliterature. Our results showed consistency with the expected results,\nvalidating our method for features and pattern recognition. The multimodal\naspect was shown in a naive soleus muscle, where a strong correlation was found\nbetween motor unit locations derived via hdEMG, and NMJ locations obtained from\nhistology, highlighting their spatial relationship. This multimodal analysis\ntool integrates 3D structural data with electrophysiological activity, opening\nnew avenues in muscle diagnostics, regenerative medicine, and personalized\ntherapies where spatial insights could one day predict electrophysiological\nbehavior or vice versa.",
        "Ecosystems face intensifying threats from climate change, overexploitation,\nand other human pressures, emphasizing the urgent need to identify keystone\nspecies and vulnerable ones. While established network-based measures often\nrely on a single metric to quantify a species' relevance, they overlook how\norganisms can be both carbon providers and consumers, thus playing a dual role\nin food webs. Here, we introduce a novel approach that assigns each species two\ncomplementary scores -- an importance measure quantifying their centrality as\ncarbon source and a fitness measure capturing their vulnerability. We show that\nspecies with high importance are more likely to trigger co-extinctions upon\nremoval, while high-fitness species typically endure until later stages of\ncollapse, in line with their broader prey ranges. On the other hand, low\nfitness species are the most vulnerable and susceptible to extinctions. Tested\non multiple food webs, our method outperforms traditional degree-based analyses\nand competes effectively with eigenvector-based approaches, while also\nproviding additional insights. Relying solely on interaction data, the approach\nis scalable and avoids reliance on expert-driven classifications, offering a\ncost-effective tool for prioritizing conservation efforts.",
        "Every day, we judge the probability of propositions. When we communicate\ngraded confidence (e.g. \"I am 90% sure\"), we enable others to gauge how much\nweight to attach to our judgment. Ideally, people should share their judgments\nto reach more accurate conclusions collectively. Peer-to-peer tools for\ncollective inference could help debunk disinformation and amplify reliable\ninformation on social networks, improving democratic discourse. However,\nindividuals fall short of the ideal of well-calibrated probability judgments,\nand group dynamics can amplify errors and polarize opinions. Here, we connect\ninsights from cognitive science, structured expert judgment, and crowdsourcing\nto infer the truth of propositions from human probability judgments. In an\nonline experiment, 376 participants judged the probability of each of 1,200\ngeneral-knowledge claims for which we have ground truth (451,200 ratings).\nAggregating binary judgments by majority vote already exhibits the \"wisdom of\nthe crowd\"--the superior accuracy of collective inferences relative to\nindividual inferences. However, using continuous probability ratings and\naccounting for individual accuracy and calibration significantly improves\ncollective inferences. Peer judgment behavior can be modeled probabilistically,\nand individual parameters capturing each peer's accuracy and miscalibration can\nbe inferred jointly with the claim probabilities. This unsupervised approach\ncan be complemented by supervised methods relying on truth labels to learn\nmodels that achieve well-calibrated collective inference. The algorithms we\nintroduce can empower groups of collaborators and online communities to pool\ntheir distributed intelligence and jointly judge the probability of\npropositions with a well-calibrated sense of uncertainty.",
        "In light of the increasing interest in G9a's role in neuroscience, three\nmachine learning (ML) models, that are time efficient and cost effective, were\ndeveloped to support researchers in this area. The models are based on data\nprovided by PubChem and performed by algorithms interpreted by the scikit-learn\nPython-based ML library. The first ML model aimed to predict the efficacy\nmagnitude of active G9a inhibitors. The ML models were trained with 3,112 and\ntested with 778 samples. The Gradient Boosting Regressor perform the best,\nachieving 17.81% means relative error (MRE), 21.48% mean absolute error (MAE),\n27.39% root mean squared error (RMSE) and 0.02 coefficient of determination\n(R2) error. The goal of the second ML model called a CID_SID ML model, utilised\nPubChem identifiers to predict the G9a inhibition probability of a small\nbiomolecule that has been primarily designed for different purposes. The ML\nmodels were trained with 58,552 samples and tested with 14,000. The most\nsuitable classifier for this case study was the Extreme Gradient Boosting\nClassifier, which obtained 78.1% accuracy, 84.3% precision,69.1% recall, 75.9%\nF1-score and 8.1% Receiver-operating characteristic (ROC). The third ML model\nbased on the Random Forest Classifier algorithm led to the generation of a list\nof descending-ordered functional groups based on their importance to the G9a\ninhibition. The model was trained with 19,455 samples and tested with 14,100.\nThe probability of this rank was 70% accuracy.",
        "The ability of an architecture to realize permutations is quite fundamental.\nFor example, Large Language Models need to be able to correctly copy (and\nperhaps rearrange) parts of the input prompt into the output. Classical\nuniversal approximation theorems guarantee the existence of parameter\nconfigurations that solve this task but offer no insights into whether\ngradient-based algorithms can find them. In this paper, we address this gap by\nfocusing on two-layer fully connected feed-forward neural networks and the task\nof learning permutations on nonzero binary inputs. We show that in the infinite\nwidth Neural Tangent Kernel (NTK) regime, an ensemble of such networks\nindependently trained with gradient descent on only the $k$ standard basis\nvectors out of $2^k - 1$ possible inputs successfully learns any fixed\npermutation of length $k$ with arbitrarily high probability. By analyzing the\nexact training dynamics, we prove that the network's output converges to a\nGaussian process whose mean captures the ground truth permutation via\nsign-based features. We then demonstrate how averaging these runs (an\n\"ensemble\" method) and applying a simple rounding step yields an arbitrarily\naccurate prediction on any possible input unseen during training. Notably, the\nnumber of models needed to achieve exact learning with high probability (which\nwe refer to as ensemble complexity) exhibits a linearithmic dependence on the\ninput size $k$ for a single test input and a quadratic dependence when\nconsidering all test inputs simultaneously.",
        "The pervasiveness of proprietary language models has raised critical privacy\nconcerns, necessitating advancements in private inference (PI), where\ncomputations are performed directly on encrypted data without revealing users'\nsensitive information. While PI offers a promising solution, its practical\ndeployment is hindered by substantial communication and latency overheads,\nprimarily stemming from nonlinear operations. To address this, we introduce an\ninformation-theoretic framework to characterize the role of nonlinearities in\ndecoder-only language models, laying a principled foundation for optimizing\ntransformer-architectures tailored to the demands of PI.\n  By leveraging Shannon's entropy as a quantitative measure, we uncover the\npreviously unexplored dual significance of nonlinearities: beyond ensuring\ntraining stability, they are crucial for maintaining attention head diversity.\nSpecifically, we find that their removal triggers two critical failure modes:\n{\\em entropy collapse} in deeper layers that destabilizes training, and {\\em\nentropic overload} in earlier layers that leads to under-utilization of\nMulti-Head Attention's (MHA) representational capacity.\n  We propose an entropy-guided attention mechanism paired with a novel entropy\nregularization technique to mitigate entropic overload. Additionally, we\nexplore PI-friendly alternatives to layer normalization for preventing entropy\ncollapse and stabilizing the training of LLMs with reduced-nonlinearities. Our\nstudy bridges the gap between information theory and architectural design,\nestablishing entropy dynamics as a principled guide for developing efficient PI\narchitectures. The code and implementation are available at\nhttps:\/\/github.com\/Nandan91\/entropy-guided-attention-llm",
        "Efficient remote monitoring of distributed sources is essential for many\nInternet of Things (IoT) applications. This work studies the uncertainty at the\nreceiver when tracking two-state Markov sources over a slotted random access\nchannel without feedback, using the conditional entropy as a performance\nindicator, and considering the last received value as current state estimate.\nWe provide an analytical characterization of the metric, and evaluate three\naccess strategies: (i) maximizing throughput, (ii) transmitting only on state\nchanges, and (iii) minimizing uncertainty through optimized access\nprobabilities. Our results reveal that throughput optimization does not always\nreduce uncertainty. Moreover, while reactive policies are optimal for symmetric\nsources, asymmetric processes benefit from mixed strategies allowing\ntransmissions during state persistence.",
        "Effectively designing molecular geometries is essential to advancing\npharmaceutical innovations, a domain, which has experienced great attention\nthrough the success of generative models and, in particular, diffusion models.\nHowever, current molecular diffusion models are tailored towards a specific\ndownstream task and lack adaptability. We introduce UniGuide, a framework for\ncontrolled geometric guidance of unconditional diffusion models that allows\nflexible conditioning during inference without the requirement of extra\ntraining or networks. We show how applications such as structure-based,\nfragment-based, and ligand-based drug design are formulated in the UniGuide\nframework and demonstrate on-par or superior performance compared to\nspecialised models. Offering a more versatile approach, UniGuide has the\npotential to streamline the development of molecular generative models,\nallowing them to be readily used in diverse application scenarios.",
        "Reservoir computing (RC) is an innovative paradigm in neuromorphic computing\nthat leverages fixed, randomized, internal connections to address the challenge\nof overfitting. RC has shown remarkable effectiveness in signal processing and\npattern recognition tasks, making it well-suited for hardware implementations\nacross various physical substrates, which promise enhanced computation speeds\nand reduced energy consumption. However, achieving optimal performance in RC\nsystems requires effective parameter optimization. Traditionally, this\noptimization has relied on software modeling, limiting the practicality of\nphysical computing approaches. Here, we report an \\emph{in situ} optimization\nmethod for an optoelectronic delay-based RC system with digital delayed\nfeedback. By simultaneously optimizing five parameters, normalized mean squared\nerror (NMSE) of 0.028, 0.561, and 0.271 is achieved in three benchmark tasks:\nwaveform classification, time series prediction, and speech recognition\noutperforming simulation-based optimization (NMSE 0.054, 0.543, and 0.329,\nrespectively) in the two of the three tasks. This method marks a significant\nadvancement in physical computing, facilitating the optimization of RC and\nneuromorphic systems without the need for simulation, thus enhancing their\npractical applicability.",
        "Counterfactual explanations in medical imaging are critical for understanding\nthe predictions made by deep learning models. We extend the Latent Shift\ncounterfactual generation method from 2D applications to 3D computed tomography\n(CT) scans. We address the challenges associated with 3D data, such as limited\ntraining samples and high memory demands, by implementing a slice-based\napproach. This method leverages a 2D encoder trained on CT slices, which are\nsubsequently combined to maintain 3D context. We demonstrate this technique on\ntwo models for clinical phenotype prediction and lung segmentation. Our\napproach is both memory-efficient and effective for generating interpretable\ncounterfactuals in high-resolution 3D medical imaging.",
        "With the recent advancements in social network platform technology, an\noverwhelming amount of information is spreading rapidly. In this situation, it\ncan become increasingly difficult to discern what information is false or true.\nIf false information proliferates significantly, it can lead to undesirable\noutcomes. Hence, when we receive some information, we can pose the following\ntwo questions: $(i)$ Is the information true? $(ii)$ If not, who initially\nspread that information? % The first problem is the rumor detection issue,\nwhile the second is the rumor source detection problem. A rumor-detection\nproblem involves identifying and mitigating false or misleading information\nspread via various communication channels, particularly online platforms and\nsocial media. Rumors can range from harmless ones to deliberately misleading\ncontent aimed at deceiving or manipulating audiences. Detecting misinformation\nis crucial for maintaining the integrity of information ecosystems and\npreventing harmful effects such as the spread of false beliefs, polarization,\nand even societal harm. Therefore, it is very important to quickly distinguish\nsuch misinformation while simultaneously finding its source to block it from\nspreading on the network. However, most of the existing surveys have analyzed\nthese two issues separately. In this work, we first survey the existing\nresearch on the rumor-detection and rumor source detection problems with joint\ndetection approaches, simultaneously. % This survey deals with these two issues\ntogether so that their relationship can be observed and it provides how the two\nproblems are similar and different. The limitations arising from the rumor\ndetection, rumor source detection, and their combination problems are also\nexplained, and some challenges to be addressed in future works are presented.",
        "We developed an anisotropic spin model that accounts for magnetic anisotropy\nand evaluated the Curie temperature (Tc) dispersion due to finite size effects\nin L10-FePt nanoparticles. In heat-assisted magnetic recording (HAMR) media, a\nnext-generation magnetic recording technology, high-density recording is\nachieved by locally heating L10-FePt nanoparticles near their Tc and rapidly\ncooling them. However, variations in Tc caused by differences in particle size\nand shape can compromise recording stability and areal density capacity, making\nthe control of Tc dispersion critical. In this study, we constructed atomistic\nLLG models to explicitly incorporate the spin exchange anisotropy of L10-FePt,\nbased on parameters determined by first-principles calculations. Using this\nmodel, we evaluated the impact of particle size on Tc dispersion. As a result,\n(1) the Tc dispersion critical to the performance of HAMR can be reproduced,\nwhereas it was previously underestimated by isotropic models and (2)\napproximately 70% of the experimentally observed Tc dispersion can be\nattributed to particle size effects. This research highlights the role of\nexchange anisotropy in amplifying finite-size effects and underscores the\nimportance of size control in HAMR media.",
        "Ensuring Large Language Models (LLMs) align with diverse human preferences\nwhile preserving privacy and fairness remains a challenge. Existing methods,\nsuch as Reinforcement Learning from Human Feedback (RLHF), rely on centralized\ndata collection, making them computationally expensive and privacy-invasive. We\nintroduce PluralLLM a federated learning-based approach that enables multiple\nuser groups to collaboratively train a transformer-based preference predictor\nwithout sharing sensitive data, which can also serve as a reward model for\naligning LLMs. Our method leverages Federated Averaging (FedAvg) to aggregate\npreference updates efficiently, achieving 46% faster convergence, a 4%\nimprovement in alignment scores, and nearly the same group fairness measure as\nin centralized training. Evaluated on a Q\/A preference alignment task,\nPluralLLM demonstrates that federated preference learning offers a scalable and\nprivacy-preserving alternative for aligning LLMs with diverse human values.",
        "We append an additional fifteen years (2009-2024) to the Chandra X-ray light\ncurve of M31*, the supermassive black hole at the center of M31, the Andromeda\ngalaxy. Extending and expanding on the work in Li et al. 2011, we show that\nM31* has remained in an elevated X-ray state from 2006 through at least 2016\n(when regular Chandra monitoring ceased) and likely through 2024, with the most\nrecent observations still showing an elevated X-ray flux. We identify one\nmoderate flare in 2013 where the other nuclear X-ray sources are in low-flux\nstates, making that flare a valuable target for followup with multiwavelength\nand multimessenger archival data. We extract a mostly uncontaminated spectrum\nfor M31* from this observation, showing that its X-ray properties are similar\nto those observed at Sgr A* in its quiescent state by Baganoff et al. 2003.\nFurthermore, we find no substantial change in the source's hardness ratio in\nthe 2006 and 2013 flares compared to the post-2006 elevated state, suggesting\nthe these flares are increases in the regular X-ray emission mechanisms instead\nof entirely new emission modes. Our extended light curve for M31* provides\nvaluable context for multimessenger or multiwavelength observations of nearby\nsupermassive black holes.",
        "The integration of machine learning (ML) has significantly enhanced the\ncapabilities of Earth Observation (EO) systems by enabling the extraction of\nactionable insights from complex datasets. However, the performance of\ndata-driven EO applications is heavily influenced by the data collection and\ntransmission processes, where limited satellite bandwidth and latency\nconstraints can hinder the full transmission of original data to the receivers.\nTo address this issue, adopting the concepts of Semantic Communication (SC)\noffers a promising solution by prioritizing the transmission of essential data\nsemantics over raw information. Implementing SC for EO systems requires a\nthorough understanding of the impact of data processing and communication\nchannel conditions on semantic loss at the processing center. This work\nproposes a novel data-fitting framework to empirically model the semantic loss\nusing real-world EO datasets and domain-specific insights. The framework\nquantifies two primary types of semantic loss: (1) source coding loss, assessed\nvia a data quality indicator measuring the impact of processing on raw source\ndata, and (2) transmission loss, evaluated by comparing practical transmission\nperformance against the Shannon limit. Semantic losses are estimated by\nevaluating the accuracy of EO applications using four task-oriented ML models,\nEfficientViT, MobileViT, ResNet50-DINO, and ResNet8-KD, on lossy image datasets\nunder varying channel conditions and compression ratios. These results underpin\na framework for efficient semantic-loss modeling in bandwidth-constrained EO\nscenarios, enabling more reliable and effective operations.",
        "We introduce YOLO11-JDE, a fast and accurate multi-object tracking (MOT)\nsolution that combines real-time object detection with self-supervised\nRe-Identification (Re-ID). By incorporating a dedicated Re-ID branch into\nYOLO11s, our model performs Joint Detection and Embedding (JDE), generating\nappearance features for each detection. The Re-ID branch is trained in a fully\nself-supervised setting while simultaneously training for detection,\neliminating the need for costly identity-labeled datasets. The triplet loss,\nwith hard positive and semi-hard negative mining strategies, is used for\nlearning discriminative embeddings. Data association is enhanced with a custom\ntracking implementation that successfully integrates motion, appearance, and\nlocation cues. YOLO11-JDE achieves competitive results on MOT17 and MOT20\nbenchmarks, surpassing existing JDE methods in terms of FPS and using up to ten\ntimes fewer parameters. Thus, making our method a highly attractive solution\nfor real-world applications.",
        "We consider a robust and self-reliant (or \"egoistic\") variation of the rigid\nbody localization (RBL) problem, in which a primary rigid body seeks to\nestimate the pose (i.e., location and orientation) of another rigid body (or\n\"target\"), relative to its own, without the assistance of external\ninfrastructure, without prior knowledge of the shape of the target, and taking\ninto account the possibility that the available observations are incomplete.\nThree complementary contributions are then offered for such a scenario. The\nfirst is a method to estimate the translation vector between the center point\nof both rigid bodies, which unlike existing techniques does not require that\nboth objects have the same shape or even the same number of landmark points.\nThis technique is shown to significantly outperform the state-of-the-art (SotA)\nunder complete information, but to be sensitive to data erasures, even when\nenhanced by matrix completion methods. The second contribution, designed to\noffer improved performance in the presence of incomplete information, offers a\nrobust alternative to the latter, at the expense of a slight relative loss\nunder complete information. Finally, the third contribution is a scheme for the\nestimation of the rotation matrix describing the relative orientation of the\ntarget rigid body with respect to the primary. Comparisons of the proposed\nschemes and SotA techniques demonstrate the advantage of the contributed\nmethods in terms of root mean square error (RMSE) performance under fully\ncomplete information and incomplete conditions.",
        "This study examines how the desiccation of Utah Great Salt Lake GSL,\nexacerbated by anthropogenic changes, poses significant health risks,\nparticularly communities mental health. Reduced water inflow has exposed the\nlakebed, increasing airborne particulate matter PM2.5 and dust storms, which\nimpact air quality. By integrating diverse datasets spanning from 1980 to\npresent including insitu measurements, satellite imagery, and reanalysis\nproducts this study synthesizes hydrological, atmospheric, and epidemiological\nvariables to comprehensively track the extent of the GSL surface water, local\nair quality fluctuations, and their effects on community mental health. The\nfindings indicate a clear relationship between higher pollution days and more\nsevere depressive symptoms. Specifically, individuals exposed to 22 days with\nPM2.5 levels above the World Health Organizations 24 hour guideline of 15 ug\nper m3 were more likely to experience severe depressive symptoms. Our results\nalso suggest that people experiencing more severe depression not only face a\nhigher number of high pollution days but also encounter such days more\nfrequently. The study highlights the interconnectedness of poor air quality,\nenvironmental degradation and mental health emphasizing the need for more\nsustainable economic growth in the region.",
        "Electromagnetic penetration through openings in an otherwise closed\nelectromagnetic scatterer is an important topic in computational\nelectromagnetics. To efficiently model this phenomenon, aperture or slot models\nare often used in conjunction with surface integral equations to solve\nMaxwell's equations. To establish the credibility of these models, code\nverification is necessary to assess the correctness of the implementation of\nthe underlying numerical methods. However, many characteristics of surface\nintegral equations and slot models render traditional code-verification\napproaches ineffective. In this paper, we present approaches to separately\nmeasure the different sources of numerical error arising from the\nmethod-of-moments implementation of the electric-field integral equation with\nan arbitrary-depth slot model. We demonstrate the effectiveness of these\napproaches for several cases."
      ]
    }
  },
  {
    "id":2411.06184,
    "research_type":"applied",
    "start_id":"b7",
    "start_title":"Taking the Human Out of the Loop: A Review of Bayesian Optimization",
    "start_abstract":"Big Data applications are typically associated with systems involving large numbers of users, massive complex software systems, and large-scale heterogeneous computing storage architectures. The construction such involves many distributed design choices. end products (e.g., recommendation medical analysis tools, real-time game engines, speech recognizers) thus involve tunable configuration parameters. These parameters often specified hard-coded into the by various developers or teams. If optimized jointly, these can result in significant improvements. Bayesian optimization is a powerful tool for joint choices that gaining great popularity recent years. It promises greater automation so as to increase both product quality human productivity. This review paper introduces optimization, highlights some its methodological aspects, showcases wide range applications.",
    "start_categories":[
      "math.OC"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "New Pathologic Classification of Lung Cancer: Relevance for Clinical Practice and Clinical Trials"
      ],
      "abstract":[
        "We summarize significant changes in pathologic classification of lung cancer resulting from the 2011 International Association for Study Lung Cancer\/American Thoracic Society\/European Respiratory Society (IASLC\/ATS\/ERS) adenocarcinoma classification. The was developed by an international core panel experts representing IASLC, ATS, and ERS with oncologists\/pulmonologists, pathologists, radiologists, molecular biologists, thoracic surgeons. Because 70% patients present advanced stages, a new approach to small biopsies cytology specific terminology criteria focused on need distinguishing squamous cell carcinoma testing EGFR mutations ALK rearrangement. Tumors previously classified as non-small-cell carcinoma, not otherwise specified, because lack clear or morphology should be further using limited immunohistochemical workup preserve tissue testing. terms \"bronchioloalveolar carcinoma\" \"mixed subtype adenocarcinoma\" have been discontinued. For resected adenocarcinomas, concepts situ minimally invasive define who, if they undergo complete resection, will 100% disease-free survival. Invasive adenocarcinomas are now predominant pattern after comprehensive histologic subtyping lepidic, acinar, papillary, solid patterns; micropapillary is added poor prognosis. Former mucinous bronchioloalveolar carcinomas called \"invasive adenocarcinoma.\" field rapidly evolving advances occurring frequent basis, particularly arena, this provides much needed standard diagnosis only patient care but also clinical trials TNM"
      ],
      "categories":[
        "q-bio.QM"
      ]
    },
    "list":{
      "title":[
        "A Bayesian Modelling Framework with Model Comparison for Epidemics with\n  Super-Spreading",
        "Beware of so-called 'good' correlations: a statistical reality check on\n  individual mRNA-protein predictions",
        "Leveraging Retrieval-Augmented Generation and Large Language Models to\n  Predict SERCA-Binding Protein Fragments from Cardiac Proteomics Data",
        "Quality of life and perceived care of patients in advanced chronic\n  kidney disease consultations: a cross-sectional descriptive study",
        "Genomic and pathological analyses of an asymmetric true hermaphroditism\n  case in a female labrador retriever",
        "Beyond Cortisol! Physiological Indicators of Welfare for Dogs: Deficits,\n  Misunderstandings and Opportunities",
        "EMICSS: Added-value annotations for EMDB entries",
        "Spatially-Structured Models of Viral Dynamics: A Scoping Review",
        "Towards Precision Oncology: Predicting Mortality and Relapse-Free\n  Survival in Head and Neck Cancer Using Clinical Data",
        "Spatial Analysis of Neuromuscular Junctions Activation in\n  Three-Dimensional Histology-based Muscle Reconstructions",
        "Measuring Fitness and Importance of Species in Food Webs",
        "Collective inference of the truth of propositions from crowd probability\n  judgments",
        "Targeting Neurodegeneration: Three Machine Learning Methods for G9a\n  Inhibitors Discovery Using PubChem and Scikit-learn",
        "Exact Learning of Permutations for Nonzero Binary Inputs with\n  Logarithmic Training Size and Quadratic Ensemble Complexity",
        "Entropy-Guided Attention for Private LLMs",
        "On the Uncertainty of a Simple Estimator for Remote Source Monitoring\n  over ALOHA Channels",
        "Unified Guidance for Geometry-Conditioned Molecular Generation",
        "In Situ Optimization of an Optoelectronic Reservoir Computer with\n  Digital Delayed Feedback",
        "Explaining 3D Computed Tomography Classifiers with Counterfactuals",
        "Detection of Rumors and Their Sources in Social Networks: A\n  Comprehensive Survey",
        "Anisotropic Exchange Spin Model to Investigate the Curie Temperature\n  Dispersion of Finite-Size L10-FePt Magnetic Nanoparticles",
        "PluralLLM: Pluralistic Alignment in LLMs via Federated Learning",
        "Fifteen Years of M31* X-ray Variability and Flares",
        "A Semantic-Loss Function Modeling Framework With Task-Oriented Machine\n  Learning Perspectives",
        "YOLO11-JDE: Fast and Accurate Multi-Object Tracking with Self-Supervised\n  Re-ID",
        "Robust Egoistic Rigid Body Localization",
        "Diminishing Waters: The Great Salt Lake's Desiccation and Its Mental\n  Health Consequences",
        "Code-Verification Techniques for an Arbitrary-Depth Electromagnetic Slot\n  Model"
      ],
      "abstract":[
        "The transmission dynamics of an epidemic are rarely homogeneous.\nSuper-spreading events and super-spreading individuals are two types of\nheterogeneous transmissibility. Inference of super-spreading is commonly\ncarried out on secondary case data, the expected distribution of which is known\nas the offspring distribution. However, this data is seldom available. Here we\nintroduce a multi-model framework fit to incidence time-series, data that is\nmuch more readily available. The framework consists of five discrete-time,\nstochastic, branching-process models of epidemics spread through a susceptible\npopulation. The framework includes a baseline model of homogeneous\ntransmission, a unimodal and a bimodal model for super-spreading events, as\nwell as a unimodal and a bimodal model for super-spreading individuals.\nBayesian statistics is used to infer model parameters using Markov Chain\nMonte-Carlo. Model comparison is conducted by computing Bayes factors, with\nimportance sampling used to estimate the marginal likelihood of each model.\nThis estimator is selected for its consistency and lower variance compared to\nalternatives. Application to simulated data from each model identifies the\ncorrect model for the majority of simulations and accurately infers the true\nparameters, such as the basic reproduction number. We also apply our methods to\nincidence data from the 2003 SARS outbreak and the Covid-19 pandemic. Model\nselection consistently identifies the same model and mechanism for a given\ndisease, even when using different time series. Our estimates are consistent\nwith previous studies based on secondary case data. Quantifying the\ncontribution of super-spreading to disease transmission has important\nimplications for infectious disease management and control. Our modelling\nframework is disease-agnostic and implemented as an R package, with potential\nto be a valuable tool for public health.",
        "Research in the life sciences often employs messenger ribonucleic acids\n(mRNA) quantification as a standalone approach for functional analysis.\nHowever, although the correlation between the measured levels of mRNA and\nproteins is positive, correlation coefficients observed empirically are\nincomplete, necessitating caution in making agnostic inferences. This essay\nprovides a statistical reflection and caveat on the concept of correlation\nstrength in the context of transcriptomics-proteomics studies. It highlights\nthe variability in possible protein levels at given empirical correlation\nvalues, even for precise mRNA amount, and underscores the notable proportion of\nmRNA-protein pairs with abundances at opposite ends of their respective\ndistributions. Cell biologists, data scientists, and biostatisticians should\nrecognise that mRNA-protein correlation alone is insufficient to justify using\na single mRNA quantification to infer the amount or function of its\ncorresponding protein.",
        "Large language models (LLMs) have shown promise in various natural language\nprocessing tasks, including their application to proteomics data to classify\nprotein fragments. In this study, we curated a limited mass spectrometry\ndataset with 1000s of protein fragments, consisting of proteins that appear to\nbe attached to the endoplasmic reticulum in cardiac cells, of which a fraction\nwas cloned and characterized for their impact on SERCA, an ER calcium pump.\nWith this limited dataset, we sought to determine whether LLMs could correctly\npredict whether a new protein fragment could bind SERCA, based only on its\nsequence and a few biophysical characteristics, such as hydrophobicity,\ndetermined from that sequence. To do so, we generated random sequences based on\ncloned fragments, embedded the fragments into a retrieval augmented generation\n(RAG) database to group them by similarity, then fine-tuned large language\nmodel (LLM) prompts to predict whether a novel sequence could bind SERCA. We\nbenchmarked this approach using multiple open-source LLMs, namely the\nMeta\/llama series, and embedding functions commonly available on the\nHuggingface repository. We then assessed the generalizability of this approach\nin classifying novel protein fragments from mass spectrometry that were not\ninitially cloned for functional characterization. By further tuning the prompt\nto account for motifs, such as ER retention sequences, we improved the\nclassification accuracy by and identified several proteins predicted to\nlocalize to the endoplasmic reticulum and bind SERCA, including Ribosomal\nProtein L2 and selenoprotein S. Although our results were based on proteomics\ndata from cardiac cells, our approach demonstrates the potential of LLMs in\nidentifying novel protein interactions and functions with very limited\nproteomic data.",
        "Objetive: In the care of renal patients, prioritising their quality of life\nand nursing care is essential. Research links patients' perceptions of care\nquality to improved outcomes such as safety, clinical efficacy, treatment\nadherence, and preventive practices. This study aimed to evaluate the quality\nof life and care perception in these patients and explore potential\nassociations between these dimensions. Material and methods: A cross-sectional\ndescriptive study was conducted with 43 patients attending an advanced CKD\nclinic. Quality of life was assessed using the KDQOL-36 questionnaire, while\nthe IECPAX questionnaire measured perceived care quality. Sociodemographic and\nclinical data were collected from patient records. Participants completed the\nquestionnaires during routine visits, with scores analysed to identify\nassociations between variables. Results: The study included 60% men (n=28) and\n32% women (n=15), with a mean age of 78 years . Among participants, 45% were\ndiabetic, 79% hypertensive, and 58% took more than five medications daily. Mean\nscores were 78.76 for KDQOL-36 and 5.54 for IECPAX. Significant differences\nwere found in the physical role domain between men and women (p=0.01) and for\nindividuals over 65 years (p=0.04). Higher IECPAX scores were associated with\ntaking more than five medications (p=0.05). However, no correlation was\nobserved between KDQOL-36 and IECPAX scores. Conclusions: The findings suggest\nthat quality of life and perceived care quality are independent in advanced CKD\npatients. While this study provides insights, larger multicentre studies are\nneeded to validate these results. These findings highlight the importance of\naddressing both aspects separately to improve outcomes in this population.",
        "The two main gonadal development disorders in dogs are true hermaphroditism\nand XX male syndrome. True hermaphroditism can be divided into two\nsubcategories: XX sex reversal and XY sex reversal. XX Sry-negative sex\nreversal is more common, and it is characterized by the presence of both\novarian and testicular tissues in an animal. To date, there are 16 cases of\ntrue hermaphroditism reported in the literature, 15 of which are XX true\nhermaphroditism. Hermaphroditism has not been formally documented in labrador\nretrievers, and no case of asymmetric hermaphroditism has been reported in the\nliterature.",
        "This paper aims to initiate new conversations about the use of physiological\nindicators when assessing the welfare of dogs. There are significant concerns\nabout construct validity - whether the measures used accurately reflect\nwelfare. The goal is to provide recommendations for future inquiry and\nencourage debate. We acknowledge that the scientific understanding of animal\nwelfare has evolved and bring attention to the shortcomings of commonly used\nbiomarkers like cortisol. These indicators are frequently used in isolation and\nwith limited salient dog descriptors, so fail to reflect the canine experience\nadequately. Using a systems approach, we explore various physiological systems\nand alternative indicators, such as heart rate variability and oxidative\nstress, to address this limitation. It is essential to consider factors like\nage, body weight, breed, and sex when interpreting these biomarkers correctly,\nand researchers should report on these in their studies. This discussion\nidentifies possible indicators for both positive and negative experiences. In\nconclusion, we advocate for a practical, evidence-based approach to assessing\nindicators of canine welfare, including non-invasive collection methods. We\nacknowledge the complexity of evaluating experiential responses in dogs across\ndifferent situations and the need for continued work to improve practices and\nrefine terminology. This will enhance our ability to accurately understand\nwelfare and improve the wellbeing of dogs, serving to inform standards of\nanimal welfare assessment. We hope this will promote more fundamental research\nin canine physiology to improve construct validity, leading to better\npractices, ultimately improving the lives of dogs.",
        "Motivation: The Electron Microscopy Data Bank (EMDB) is a key repository for\n3D electron microscopy (3DEM) data but lacks comprehensive annotations and\nconnections to most of the related biological, functional, and structural data.\nThis limitation arises from the optional nature of such information to reduce\ndepositor burden and the complexity of maintaining up-to-date external\nreferences, often requiring depositor consent. To address these challenges, we\ndeveloped EMICSS (EMDB Integration with Complexes, Structures, and Sequences),\nan independent system that automatically updates cross-references with over 20\nexternal resources, including UniProt, AlphaFold DB, PubMed, Complex Portal and\nGene Ontology. Results: EMICSS (https:\/\/www.ebi.ac.uk\/emdb\/emicss) annotations\nare accessible in multiple formats for every EMDB entry and its linked\nresources, and programmatically via the EMDB Application Programming Interface\n(API). EMICSS plays a crucial role supporting the EMDB website, with\nannotations being used on entry pages, statistics, and in the search system.\nAvailability and implementation: EMICSS is implemented in Python and it is an\nopen-source, distributed under the EMBL-EBI license, with core code available\non GitHub (https:\/\/github.com\/emdb-empiar\/added_annotations).",
        "There is growing recognition in both the experimental and modelling\nliterature of the importance of spatial structure to the dynamics of viral\ninfections in tissues. Aided by the evolution of computing power and motivated\nby recent biological insights, there has been an explosion of new,\nspatially-explicit models for within-host viral dynamics in recent years. This\ndevelopment has only been accelerated in the wake of the COVID-19 pandemic.\nSpatially-structured models offer improved biological realism and can account\nfor dynamics which cannot be well-described by conventional, mean-field\napproaches. However, despite their growing popularity, spatially-structured\nmodels of viral dynamics are underused in biological applications. One major\nobstacle to the wider application of such models is the huge variety in\napproaches taken, with little consensus as to which features should be included\nand how they should be implemented for a given biological context. Previous\nreviews of the field have focused on specific modelling frameworks or on models\nfor particular viral species. Here, we instead apply a scoping review approach\nto the literature of spatially-structured viral dynamics models as a whole to\nprovide an exhaustive update of the state of the field. Our analysis is\nstructured along two axes, methodology and viral species, in order to examine\nthe breadth of techniques used and the requirements of different biological\napplications. We then discuss the contributions of mathematical and\ncomputational modelling to our understanding of key spatially-structured\naspects of viral dynamics, and suggest key themes for future model development\nto improve robustness and biological utility.",
        "Head and neck squamous cell carcinoma (HNSCC) presents significant challenges\nin clinical oncology due to its heterogeneity and high mortality rates. This\nstudy aims to leverage clinical data and machine learning (ML) principles to\npredict key outcomes for HNSCC patients: mortality, and relapse-free survival.\nUtilizing data sourced from the Cancer Imaging Archive, an extensive pipeline\nwas implemented to ensure robust model training and evaluation. Ensemble and\nindividual classifiers, including XGBoost, Random Forest, and Support Vectors,\nwere employed to develop predictive models. The study identified key clinical\nfeatures influencing HNSCC mortality outcomes and achieved predictive accuracy\nand ROC-AUC values exceeding 90\\% across tasks. Support Vector Machine strongly\nexcelled in relapse-free survival, with an recall value of 0.99 and precision\nof 0.97. Key clinical features including loco-regional control, smoking and\ntreatment type, were identified as critical predictors of patient outcomes.\nThis study underscores the medical impact of using ML-driven insights to refine\nprognostic accuracy and optimize personalized treatment strategies in HNSCC.",
        "Histology has long been a foundational technique for studying anatomical\nstructures through tissue slicing. Advances in computational methods now enable\nthree dimensional (3D) reconstruction of organs from histology images,\nenhancing the analysis of structural and functional features. Here, we present\na novel multimodal computational method to reconstruct rodent muscles in 3D\nusing classical image processing and data analysis techniques, analyze their\nstructural features and correlate them to previously recorded\nelectrophysiological data. The algorithm analyzes spatial distribution patterns\nof features identified through histological staining, normalizing them across\nmultiple samples. Further, the algorithm successfully correlates spatial\npatterns with high density epimysial ElectroMyoGraphy (hdEMG) recordings,\nproviding a multimodal perspective on neuromuscular dynamics, linking spatial\nand electrophysiological information. The code was validated by looking at the\ndistribution of NeuroMuscular Junctions (NMJs) in naive soleus muscles and\ncompared the distributions and patterns observed with ones observed in previous\nliterature. Our results showed consistency with the expected results,\nvalidating our method for features and pattern recognition. The multimodal\naspect was shown in a naive soleus muscle, where a strong correlation was found\nbetween motor unit locations derived via hdEMG, and NMJ locations obtained from\nhistology, highlighting their spatial relationship. This multimodal analysis\ntool integrates 3D structural data with electrophysiological activity, opening\nnew avenues in muscle diagnostics, regenerative medicine, and personalized\ntherapies where spatial insights could one day predict electrophysiological\nbehavior or vice versa.",
        "Ecosystems face intensifying threats from climate change, overexploitation,\nand other human pressures, emphasizing the urgent need to identify keystone\nspecies and vulnerable ones. While established network-based measures often\nrely on a single metric to quantify a species' relevance, they overlook how\norganisms can be both carbon providers and consumers, thus playing a dual role\nin food webs. Here, we introduce a novel approach that assigns each species two\ncomplementary scores -- an importance measure quantifying their centrality as\ncarbon source and a fitness measure capturing their vulnerability. We show that\nspecies with high importance are more likely to trigger co-extinctions upon\nremoval, while high-fitness species typically endure until later stages of\ncollapse, in line with their broader prey ranges. On the other hand, low\nfitness species are the most vulnerable and susceptible to extinctions. Tested\non multiple food webs, our method outperforms traditional degree-based analyses\nand competes effectively with eigenvector-based approaches, while also\nproviding additional insights. Relying solely on interaction data, the approach\nis scalable and avoids reliance on expert-driven classifications, offering a\ncost-effective tool for prioritizing conservation efforts.",
        "Every day, we judge the probability of propositions. When we communicate\ngraded confidence (e.g. \"I am 90% sure\"), we enable others to gauge how much\nweight to attach to our judgment. Ideally, people should share their judgments\nto reach more accurate conclusions collectively. Peer-to-peer tools for\ncollective inference could help debunk disinformation and amplify reliable\ninformation on social networks, improving democratic discourse. However,\nindividuals fall short of the ideal of well-calibrated probability judgments,\nand group dynamics can amplify errors and polarize opinions. Here, we connect\ninsights from cognitive science, structured expert judgment, and crowdsourcing\nto infer the truth of propositions from human probability judgments. In an\nonline experiment, 376 participants judged the probability of each of 1,200\ngeneral-knowledge claims for which we have ground truth (451,200 ratings).\nAggregating binary judgments by majority vote already exhibits the \"wisdom of\nthe crowd\"--the superior accuracy of collective inferences relative to\nindividual inferences. However, using continuous probability ratings and\naccounting for individual accuracy and calibration significantly improves\ncollective inferences. Peer judgment behavior can be modeled probabilistically,\nand individual parameters capturing each peer's accuracy and miscalibration can\nbe inferred jointly with the claim probabilities. This unsupervised approach\ncan be complemented by supervised methods relying on truth labels to learn\nmodels that achieve well-calibrated collective inference. The algorithms we\nintroduce can empower groups of collaborators and online communities to pool\ntheir distributed intelligence and jointly judge the probability of\npropositions with a well-calibrated sense of uncertainty.",
        "In light of the increasing interest in G9a's role in neuroscience, three\nmachine learning (ML) models, that are time efficient and cost effective, were\ndeveloped to support researchers in this area. The models are based on data\nprovided by PubChem and performed by algorithms interpreted by the scikit-learn\nPython-based ML library. The first ML model aimed to predict the efficacy\nmagnitude of active G9a inhibitors. The ML models were trained with 3,112 and\ntested with 778 samples. The Gradient Boosting Regressor perform the best,\nachieving 17.81% means relative error (MRE), 21.48% mean absolute error (MAE),\n27.39% root mean squared error (RMSE) and 0.02 coefficient of determination\n(R2) error. The goal of the second ML model called a CID_SID ML model, utilised\nPubChem identifiers to predict the G9a inhibition probability of a small\nbiomolecule that has been primarily designed for different purposes. The ML\nmodels were trained with 58,552 samples and tested with 14,000. The most\nsuitable classifier for this case study was the Extreme Gradient Boosting\nClassifier, which obtained 78.1% accuracy, 84.3% precision,69.1% recall, 75.9%\nF1-score and 8.1% Receiver-operating characteristic (ROC). The third ML model\nbased on the Random Forest Classifier algorithm led to the generation of a list\nof descending-ordered functional groups based on their importance to the G9a\ninhibition. The model was trained with 19,455 samples and tested with 14,100.\nThe probability of this rank was 70% accuracy.",
        "The ability of an architecture to realize permutations is quite fundamental.\nFor example, Large Language Models need to be able to correctly copy (and\nperhaps rearrange) parts of the input prompt into the output. Classical\nuniversal approximation theorems guarantee the existence of parameter\nconfigurations that solve this task but offer no insights into whether\ngradient-based algorithms can find them. In this paper, we address this gap by\nfocusing on two-layer fully connected feed-forward neural networks and the task\nof learning permutations on nonzero binary inputs. We show that in the infinite\nwidth Neural Tangent Kernel (NTK) regime, an ensemble of such networks\nindependently trained with gradient descent on only the $k$ standard basis\nvectors out of $2^k - 1$ possible inputs successfully learns any fixed\npermutation of length $k$ with arbitrarily high probability. By analyzing the\nexact training dynamics, we prove that the network's output converges to a\nGaussian process whose mean captures the ground truth permutation via\nsign-based features. We then demonstrate how averaging these runs (an\n\"ensemble\" method) and applying a simple rounding step yields an arbitrarily\naccurate prediction on any possible input unseen during training. Notably, the\nnumber of models needed to achieve exact learning with high probability (which\nwe refer to as ensemble complexity) exhibits a linearithmic dependence on the\ninput size $k$ for a single test input and a quadratic dependence when\nconsidering all test inputs simultaneously.",
        "The pervasiveness of proprietary language models has raised critical privacy\nconcerns, necessitating advancements in private inference (PI), where\ncomputations are performed directly on encrypted data without revealing users'\nsensitive information. While PI offers a promising solution, its practical\ndeployment is hindered by substantial communication and latency overheads,\nprimarily stemming from nonlinear operations. To address this, we introduce an\ninformation-theoretic framework to characterize the role of nonlinearities in\ndecoder-only language models, laying a principled foundation for optimizing\ntransformer-architectures tailored to the demands of PI.\n  By leveraging Shannon's entropy as a quantitative measure, we uncover the\npreviously unexplored dual significance of nonlinearities: beyond ensuring\ntraining stability, they are crucial for maintaining attention head diversity.\nSpecifically, we find that their removal triggers two critical failure modes:\n{\\em entropy collapse} in deeper layers that destabilizes training, and {\\em\nentropic overload} in earlier layers that leads to under-utilization of\nMulti-Head Attention's (MHA) representational capacity.\n  We propose an entropy-guided attention mechanism paired with a novel entropy\nregularization technique to mitigate entropic overload. Additionally, we\nexplore PI-friendly alternatives to layer normalization for preventing entropy\ncollapse and stabilizing the training of LLMs with reduced-nonlinearities. Our\nstudy bridges the gap between information theory and architectural design,\nestablishing entropy dynamics as a principled guide for developing efficient PI\narchitectures. The code and implementation are available at\nhttps:\/\/github.com\/Nandan91\/entropy-guided-attention-llm",
        "Efficient remote monitoring of distributed sources is essential for many\nInternet of Things (IoT) applications. This work studies the uncertainty at the\nreceiver when tracking two-state Markov sources over a slotted random access\nchannel without feedback, using the conditional entropy as a performance\nindicator, and considering the last received value as current state estimate.\nWe provide an analytical characterization of the metric, and evaluate three\naccess strategies: (i) maximizing throughput, (ii) transmitting only on state\nchanges, and (iii) minimizing uncertainty through optimized access\nprobabilities. Our results reveal that throughput optimization does not always\nreduce uncertainty. Moreover, while reactive policies are optimal for symmetric\nsources, asymmetric processes benefit from mixed strategies allowing\ntransmissions during state persistence.",
        "Effectively designing molecular geometries is essential to advancing\npharmaceutical innovations, a domain, which has experienced great attention\nthrough the success of generative models and, in particular, diffusion models.\nHowever, current molecular diffusion models are tailored towards a specific\ndownstream task and lack adaptability. We introduce UniGuide, a framework for\ncontrolled geometric guidance of unconditional diffusion models that allows\nflexible conditioning during inference without the requirement of extra\ntraining or networks. We show how applications such as structure-based,\nfragment-based, and ligand-based drug design are formulated in the UniGuide\nframework and demonstrate on-par or superior performance compared to\nspecialised models. Offering a more versatile approach, UniGuide has the\npotential to streamline the development of molecular generative models,\nallowing them to be readily used in diverse application scenarios.",
        "Reservoir computing (RC) is an innovative paradigm in neuromorphic computing\nthat leverages fixed, randomized, internal connections to address the challenge\nof overfitting. RC has shown remarkable effectiveness in signal processing and\npattern recognition tasks, making it well-suited for hardware implementations\nacross various physical substrates, which promise enhanced computation speeds\nand reduced energy consumption. However, achieving optimal performance in RC\nsystems requires effective parameter optimization. Traditionally, this\noptimization has relied on software modeling, limiting the practicality of\nphysical computing approaches. Here, we report an \\emph{in situ} optimization\nmethod for an optoelectronic delay-based RC system with digital delayed\nfeedback. By simultaneously optimizing five parameters, normalized mean squared\nerror (NMSE) of 0.028, 0.561, and 0.271 is achieved in three benchmark tasks:\nwaveform classification, time series prediction, and speech recognition\noutperforming simulation-based optimization (NMSE 0.054, 0.543, and 0.329,\nrespectively) in the two of the three tasks. This method marks a significant\nadvancement in physical computing, facilitating the optimization of RC and\nneuromorphic systems without the need for simulation, thus enhancing their\npractical applicability.",
        "Counterfactual explanations in medical imaging are critical for understanding\nthe predictions made by deep learning models. We extend the Latent Shift\ncounterfactual generation method from 2D applications to 3D computed tomography\n(CT) scans. We address the challenges associated with 3D data, such as limited\ntraining samples and high memory demands, by implementing a slice-based\napproach. This method leverages a 2D encoder trained on CT slices, which are\nsubsequently combined to maintain 3D context. We demonstrate this technique on\ntwo models for clinical phenotype prediction and lung segmentation. Our\napproach is both memory-efficient and effective for generating interpretable\ncounterfactuals in high-resolution 3D medical imaging.",
        "With the recent advancements in social network platform technology, an\noverwhelming amount of information is spreading rapidly. In this situation, it\ncan become increasingly difficult to discern what information is false or true.\nIf false information proliferates significantly, it can lead to undesirable\noutcomes. Hence, when we receive some information, we can pose the following\ntwo questions: $(i)$ Is the information true? $(ii)$ If not, who initially\nspread that information? % The first problem is the rumor detection issue,\nwhile the second is the rumor source detection problem. A rumor-detection\nproblem involves identifying and mitigating false or misleading information\nspread via various communication channels, particularly online platforms and\nsocial media. Rumors can range from harmless ones to deliberately misleading\ncontent aimed at deceiving or manipulating audiences. Detecting misinformation\nis crucial for maintaining the integrity of information ecosystems and\npreventing harmful effects such as the spread of false beliefs, polarization,\nand even societal harm. Therefore, it is very important to quickly distinguish\nsuch misinformation while simultaneously finding its source to block it from\nspreading on the network. However, most of the existing surveys have analyzed\nthese two issues separately. In this work, we first survey the existing\nresearch on the rumor-detection and rumor source detection problems with joint\ndetection approaches, simultaneously. % This survey deals with these two issues\ntogether so that their relationship can be observed and it provides how the two\nproblems are similar and different. The limitations arising from the rumor\ndetection, rumor source detection, and their combination problems are also\nexplained, and some challenges to be addressed in future works are presented.",
        "We developed an anisotropic spin model that accounts for magnetic anisotropy\nand evaluated the Curie temperature (Tc) dispersion due to finite size effects\nin L10-FePt nanoparticles. In heat-assisted magnetic recording (HAMR) media, a\nnext-generation magnetic recording technology, high-density recording is\nachieved by locally heating L10-FePt nanoparticles near their Tc and rapidly\ncooling them. However, variations in Tc caused by differences in particle size\nand shape can compromise recording stability and areal density capacity, making\nthe control of Tc dispersion critical. In this study, we constructed atomistic\nLLG models to explicitly incorporate the spin exchange anisotropy of L10-FePt,\nbased on parameters determined by first-principles calculations. Using this\nmodel, we evaluated the impact of particle size on Tc dispersion. As a result,\n(1) the Tc dispersion critical to the performance of HAMR can be reproduced,\nwhereas it was previously underestimated by isotropic models and (2)\napproximately 70% of the experimentally observed Tc dispersion can be\nattributed to particle size effects. This research highlights the role of\nexchange anisotropy in amplifying finite-size effects and underscores the\nimportance of size control in HAMR media.",
        "Ensuring Large Language Models (LLMs) align with diverse human preferences\nwhile preserving privacy and fairness remains a challenge. Existing methods,\nsuch as Reinforcement Learning from Human Feedback (RLHF), rely on centralized\ndata collection, making them computationally expensive and privacy-invasive. We\nintroduce PluralLLM a federated learning-based approach that enables multiple\nuser groups to collaboratively train a transformer-based preference predictor\nwithout sharing sensitive data, which can also serve as a reward model for\naligning LLMs. Our method leverages Federated Averaging (FedAvg) to aggregate\npreference updates efficiently, achieving 46% faster convergence, a 4%\nimprovement in alignment scores, and nearly the same group fairness measure as\nin centralized training. Evaluated on a Q\/A preference alignment task,\nPluralLLM demonstrates that federated preference learning offers a scalable and\nprivacy-preserving alternative for aligning LLMs with diverse human values.",
        "We append an additional fifteen years (2009-2024) to the Chandra X-ray light\ncurve of M31*, the supermassive black hole at the center of M31, the Andromeda\ngalaxy. Extending and expanding on the work in Li et al. 2011, we show that\nM31* has remained in an elevated X-ray state from 2006 through at least 2016\n(when regular Chandra monitoring ceased) and likely through 2024, with the most\nrecent observations still showing an elevated X-ray flux. We identify one\nmoderate flare in 2013 where the other nuclear X-ray sources are in low-flux\nstates, making that flare a valuable target for followup with multiwavelength\nand multimessenger archival data. We extract a mostly uncontaminated spectrum\nfor M31* from this observation, showing that its X-ray properties are similar\nto those observed at Sgr A* in its quiescent state by Baganoff et al. 2003.\nFurthermore, we find no substantial change in the source's hardness ratio in\nthe 2006 and 2013 flares compared to the post-2006 elevated state, suggesting\nthe these flares are increases in the regular X-ray emission mechanisms instead\nof entirely new emission modes. Our extended light curve for M31* provides\nvaluable context for multimessenger or multiwavelength observations of nearby\nsupermassive black holes.",
        "The integration of machine learning (ML) has significantly enhanced the\ncapabilities of Earth Observation (EO) systems by enabling the extraction of\nactionable insights from complex datasets. However, the performance of\ndata-driven EO applications is heavily influenced by the data collection and\ntransmission processes, where limited satellite bandwidth and latency\nconstraints can hinder the full transmission of original data to the receivers.\nTo address this issue, adopting the concepts of Semantic Communication (SC)\noffers a promising solution by prioritizing the transmission of essential data\nsemantics over raw information. Implementing SC for EO systems requires a\nthorough understanding of the impact of data processing and communication\nchannel conditions on semantic loss at the processing center. This work\nproposes a novel data-fitting framework to empirically model the semantic loss\nusing real-world EO datasets and domain-specific insights. The framework\nquantifies two primary types of semantic loss: (1) source coding loss, assessed\nvia a data quality indicator measuring the impact of processing on raw source\ndata, and (2) transmission loss, evaluated by comparing practical transmission\nperformance against the Shannon limit. Semantic losses are estimated by\nevaluating the accuracy of EO applications using four task-oriented ML models,\nEfficientViT, MobileViT, ResNet50-DINO, and ResNet8-KD, on lossy image datasets\nunder varying channel conditions and compression ratios. These results underpin\na framework for efficient semantic-loss modeling in bandwidth-constrained EO\nscenarios, enabling more reliable and effective operations.",
        "We introduce YOLO11-JDE, a fast and accurate multi-object tracking (MOT)\nsolution that combines real-time object detection with self-supervised\nRe-Identification (Re-ID). By incorporating a dedicated Re-ID branch into\nYOLO11s, our model performs Joint Detection and Embedding (JDE), generating\nappearance features for each detection. The Re-ID branch is trained in a fully\nself-supervised setting while simultaneously training for detection,\neliminating the need for costly identity-labeled datasets. The triplet loss,\nwith hard positive and semi-hard negative mining strategies, is used for\nlearning discriminative embeddings. Data association is enhanced with a custom\ntracking implementation that successfully integrates motion, appearance, and\nlocation cues. YOLO11-JDE achieves competitive results on MOT17 and MOT20\nbenchmarks, surpassing existing JDE methods in terms of FPS and using up to ten\ntimes fewer parameters. Thus, making our method a highly attractive solution\nfor real-world applications.",
        "We consider a robust and self-reliant (or \"egoistic\") variation of the rigid\nbody localization (RBL) problem, in which a primary rigid body seeks to\nestimate the pose (i.e., location and orientation) of another rigid body (or\n\"target\"), relative to its own, without the assistance of external\ninfrastructure, without prior knowledge of the shape of the target, and taking\ninto account the possibility that the available observations are incomplete.\nThree complementary contributions are then offered for such a scenario. The\nfirst is a method to estimate the translation vector between the center point\nof both rigid bodies, which unlike existing techniques does not require that\nboth objects have the same shape or even the same number of landmark points.\nThis technique is shown to significantly outperform the state-of-the-art (SotA)\nunder complete information, but to be sensitive to data erasures, even when\nenhanced by matrix completion methods. The second contribution, designed to\noffer improved performance in the presence of incomplete information, offers a\nrobust alternative to the latter, at the expense of a slight relative loss\nunder complete information. Finally, the third contribution is a scheme for the\nestimation of the rotation matrix describing the relative orientation of the\ntarget rigid body with respect to the primary. Comparisons of the proposed\nschemes and SotA techniques demonstrate the advantage of the contributed\nmethods in terms of root mean square error (RMSE) performance under fully\ncomplete information and incomplete conditions.",
        "This study examines how the desiccation of Utah Great Salt Lake GSL,\nexacerbated by anthropogenic changes, poses significant health risks,\nparticularly communities mental health. Reduced water inflow has exposed the\nlakebed, increasing airborne particulate matter PM2.5 and dust storms, which\nimpact air quality. By integrating diverse datasets spanning from 1980 to\npresent including insitu measurements, satellite imagery, and reanalysis\nproducts this study synthesizes hydrological, atmospheric, and epidemiological\nvariables to comprehensively track the extent of the GSL surface water, local\nair quality fluctuations, and their effects on community mental health. The\nfindings indicate a clear relationship between higher pollution days and more\nsevere depressive symptoms. Specifically, individuals exposed to 22 days with\nPM2.5 levels above the World Health Organizations 24 hour guideline of 15 ug\nper m3 were more likely to experience severe depressive symptoms. Our results\nalso suggest that people experiencing more severe depression not only face a\nhigher number of high pollution days but also encounter such days more\nfrequently. The study highlights the interconnectedness of poor air quality,\nenvironmental degradation and mental health emphasizing the need for more\nsustainable economic growth in the region.",
        "Electromagnetic penetration through openings in an otherwise closed\nelectromagnetic scatterer is an important topic in computational\nelectromagnetics. To efficiently model this phenomenon, aperture or slot models\nare often used in conjunction with surface integral equations to solve\nMaxwell's equations. To establish the credibility of these models, code\nverification is necessary to assess the correctness of the implementation of\nthe underlying numerical methods. However, many characteristics of surface\nintegral equations and slot models render traditional code-verification\napproaches ineffective. In this paper, we present approaches to separately\nmeasure the different sources of numerical error arising from the\nmethod-of-moments implementation of the electric-field integral equation with\nan arbitrary-depth slot model. We demonstrate the effectiveness of these\napproaches for several cases."
      ]
    }
  },
  {
    "id":2411.06184,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"New Pathologic Classification of Lung Cancer: Relevance for Clinical Practice and Clinical Trials",
    "start_abstract":"We summarize significant changes in pathologic classification of lung cancer resulting from the 2011 International Association for Study Lung Cancer\/American Thoracic Society\/European Respiratory Society (IASLC\/ATS\/ERS) adenocarcinoma classification. The was developed by an international core panel experts representing IASLC, ATS, and ERS with oncologists\/pulmonologists, pathologists, radiologists, molecular biologists, thoracic surgeons. Because 70% patients present advanced stages, a new approach to small biopsies cytology specific terminology criteria focused on need distinguishing squamous cell carcinoma testing EGFR mutations ALK rearrangement. Tumors previously classified as non-small-cell carcinoma, not otherwise specified, because lack clear or morphology should be further using limited immunohistochemical workup preserve tissue testing. terms \"bronchioloalveolar carcinoma\" \"mixed subtype adenocarcinoma\" have been discontinued. For resected adenocarcinomas, concepts situ minimally invasive define who, if they undergo complete resection, will 100% disease-free survival. Invasive adenocarcinomas are now predominant pattern after comprehensive histologic subtyping lepidic, acinar, papillary, solid patterns; micropapillary is added poor prognosis. Former mucinous bronchioloalveolar carcinomas called \"invasive adenocarcinoma.\" field rapidly evolving advances occurring frequent basis, particularly arena, this provides much needed standard diagnosis only patient care but also clinical trials TNM",
    "start_categories":[
      "q-bio.QM"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b6",
        "b7"
      ],
      "title":[
        "Multi-Task Bayesian Optimization",
        "Taking the Human Out of the Loop: A Review of Bayesian Optimization"
      ],
      "abstract":[
        "Bayesian optimization has recently been proposed as a framework for automatically tuning the hyperparameters of machine learning models and shown to yield state-of-the-art performance with impressive ease efficiency. In this paper, we explore whether it is possible transfer knowledge gained from previous optimizations new tasks in order find optimal hyperparameter settings more efficiently. Our approach based on extending multi-task Gaussian processes optimization. We show that method significantly speeds up process when compared standard single-task approach. further propose straightforward extension our algorithm jointly minimize average error across multiple demonstrate how can be used greatly speed k-fold cross-validation. Lastly, an adaptation developed acquisition function, entropy search, cost-sensitive, setting. utility function by leveraging small dataset hyper-parameter large dataset. dynamically chooses which query most information per unit cost.",
        "Big Data applications are typically associated with systems involving large numbers of users, massive complex software systems, and large-scale heterogeneous computing storage architectures. The construction such involves many distributed design choices. end products (e.g., recommendation medical analysis tools, real-time game engines, speech recognizers) thus involve tunable configuration parameters. These parameters often specified hard-coded into the by various developers or teams. If optimized jointly, these can result in significant improvements. Bayesian optimization is a powerful tool for joint choices that gaining great popularity recent years. It promises greater automation so as to increase both product quality human productivity. This review paper introduces optimization, highlights some its methodological aspects, showcases wide range applications."
      ],
      "categories":[
        "math.OC"
      ]
    },
    "list":{
      "title":[
        "Decentralized Strategies for Backward Linear-Quadratic Mean Field Games\n  and Teams",
        "Regularity of the Product of Two Relaxed Cutters with Relaxation\n  Parameters Beyond Two",
        "Online Learning-Based Predictive Control for Nonlinear System",
        "Convergence of projected stochastic approximation algorithm",
        "State-Dependent Uncertainty Modeling in Robust Optimal Control Problems\n  through Generalized Semi-Infinite Programming",
        "Entropic optimal transport with congestion aversion Application to\n  relocation of drones",
        "A consensus-based optimization method for nonsmooth nonconvex programs\n  with approximated gradient descent scheme",
        "Rank conditions for exactness of semidefinite relaxations in polynomial\n  optimization",
        "Differentiation of inertial methods for optimizing smooth parametric\n  function",
        "Chance constraints transcription and failure risk estimation for\n  stochastic trajectory optimization",
        "Quantization Of Probability Measures In Maximum~Mean~Discrepancy\n  Distance",
        "Complete systems of inequalities relating the perimeter, the area and\n  the Cheeger constant of planar domains",
        "Human-AI Collaboration in Cloud Security: Cognitive Hierarchy-Driven\n  Deep Reinforcement Learning",
        "Fluctuations of non-local branching Markov processes",
        "Understanding In-Context Machine Translation for Low-Resource Languages:\n  A Case Study on Manchu",
        "Differential virial analysis: a new technique to determine the dynamical\n  state of molecular clouds",
        "Quantum geometry of non-Hermitian systems",
        "Gravity-induced collisions of uncharged cloud droplets in an electric\n  field",
        "Breaking the Clusters: Uniformity-Optimization for Text-Based Sequential\n  Recommendation",
        "Imprints of an early matter-dominated era arising from dark matter\n  dilution mechanism on cosmic string dynamics and gravitational wave\n  signatures",
        "GRAPHGPT-O: Synergistic Multimodal Comprehension and Generation on\n  Graphs",
        "A Simple yet Effective DDG Predictor is An Unsupervised Antibody\n  Optimizer and Explainer",
        "Ways of Seeing, and Selling, AI Art",
        "Project portfolio planning in the pharmaceutical industry -- strategic\n  objectives and quantitative optimization",
        "Transforming Science with Large Language Models: A Survey on AI-assisted\n  Scientific Discovery, Experimentation, Content Generation, and Evaluation",
        "A Possible Four-Month Periodicity in the Activity of FRB 20240209A",
        "Hybrid Near\/Far-Field Frequency-Dependent Beamforming via Joint\n  Phase-Time Arrays"
      ],
      "abstract":[
        "This paper studies a new class of linear-quadratic mean field games and teams\nproblem, where the large-population system satisfies a class of $N$ weakly\ncoupled linear backward stochastic differential equations (BSDEs), and $z_i$ (a\npart of solution of BSDE) enter the state equations and cost functionals. By\nvirtue of stochastic maximum principle and optimal filter technique, we obtain\na Hamiltonian system first, which is a fully coupled forward-backward\nstochastic differential equation (FBSDE). Decoupling the Hamiltonian system, we\nderive a feedback form optimal strategy by introducing Riccati equations,\nstochastic differential equation (SDE) and BSDE. Finally, we provide a\nnumerical example to simulate our results.",
        "We study the product of two relaxed cutters having a common fixed point. We\nassume that one of the relaxation parameters is greater than two so that the\ncorresponding relaxed cutter is no longer quasi-nonexpansive, but rather\ndemicontractive. We show that if both of the operators are (weakly\/linearly)\nregular, then under certain conditions, the resulting product inherits the same\ntype of regularity. We then apply these results to proving convergence in the\nweak, norm and linear sense of algorithms that employ such products.",
        "In this paper, we propose an online learning-based predictive control (LPC)\napproach designed for nonlinear systems that lack explicit system dynamics.\nUnlike traditional model predictive control (MPC) algorithms that rely on known\nsystem models to optimize controller outputs, our proposed algorithm integrates\na reinforcement learning component to learn optimal policies in real time from\nthe offline dataset and real-time data. Additionally, an optimal control\nproblem (OCP)-based optimization framework is incorporated to enhance real-time\ncomputational efficiency while ensuring stability during online operation.\nMoreover, we rigorously establish the super-linear convergence properties of\nthe algorithm. Finally, extensive simulations are performed to evaluate the\nfeasibility and effectiveness of the proposed approach.",
        "We study the Robbins-Monro stochastic approximation algorithm with\nprojections on a hyperrectangle and prove its convergence. This work fills a\ngap in the convergence proof of the classic book by Kushner and Yin. Using the\nODE method, we show that the algorithm converges to stationary points of a\nrelated projected ODE. Our results provide a better theoretical foundation for\nstochastic optimization techniques, including stochastic gradient descent and\nits proximal version. These results extend the algorithm's applicability and\nrelax some assumptions of previous research.",
        "Generalized semi-infinite programs (generalized SIPs) are problems featuring\na finite number of decision variables but an infinite number of constraints.\nThey differ from standard SIPs in that their constraint set itself depends on\nthe choice of the decision variable. Generalized SIPs can be used to model\nrobust optimal control problems where the uncertainty itself is a function of\nthe state or control input, allowing for a less conservative alternative to\nassuming a uniform uncertainty set over the entire decision space. In this\nwork, we demonstrate how any generalized SIP can be converted to an\nexistence-constrained SIP through a reformulation of the constraints and solved\nusing a local reduction approach, which approximates the infinite constraint\nset by a finite number of scenarios. This transformation is then exploited to\nsolve nonlinear robust optimal control problems with state-dependent\nuncertainties. We showcase our proposed approach on a planar quadrotor\nsimulation where it recovers the true generalized SIP solution and outperforms\na SIP-based approach with uniform uncertainty bounds.",
        "We present a mathematical framework for tempo-spatial entropic optimal\ntransport, motivated by the problem of efficiently routing drones back to\nlogistics centers. To address collision risk, we incorporate a convex penalty\nterm into the transport model. We propose the Sinkhorn-Frank-Wolfe algorithm, a\nnumerically efficient method with theoretical convergence guarantees, and\ndemonstrate its effectiveness through experiments on synthetic datasets. Our\napproach provides a foundation for optimizing large-scale autonomous drone\nlogistics while ensuring safe and efficient transportation.",
        "In this paper, we are interested in finding the global minimizer of a\nnonsmooth nonconvex unconstrained optimization problem. By combining the\ndiscrete consensus-based optimization (CBO) algorithm and the gradient descent\nmethod, we develop a novel CBO algorithm with an extra gradient descent scheme\nevaluated by the forward-difference technique on the function values, where\nonly the objective function values are used in the proposed algorithm. First,\nwe prove that the proposed algorithm can exhibit global consensus in an\nexponential rate in two senses and possess a unique global consensus point.\nSecond, we evaluate the error estimate between the objective function value on\nthe global consensus point and its global minimum. In particular, as the\nparameter $\\beta$ tends to $\\infty$, the error converges to zero and the\nconvergence rate is $\\mathcal{O}\\left(\\frac{\\log\\beta}{\\beta}\\right)$. Third,\nunder some suitable assumptions on the objective function, we provide the\nnumber of iterations required for the mean square error in expectation to reach\nthe desired accuracy. It is worth underlining that the theoretical analysis in\nthis paper does not use the mean-field limit. Finally, we illustrate the\nimproved efficiency and promising performance of our novel CBO method through\nsome experiments on several nonconvex benchmark problems and the application to\ntrain deep neural networks.",
        "We consider the Moment-SOS hierarchy in polynomial optimization. We first\nprovide a sufficient condition to solve the truncated K-moment problem\nassociated with a given degree-$2n$ pseudo-moment sequence $\\phi$ n and a\nsemi-algebraic set $K \\subset \\mathbb{R}^d$. Namely, let $2v$ be the maximum\ndegree of the polynomials that describe $K$. If the rank $r$ of its associated\nmoment matrix is less than $nv + 1$, then $\\phi^n$ has an atomic representing\nmeasure supported on at most $r$ points of $K$. When used at step-$n$ of the\nMoment-SOS hierarchy, it provides a sufficient condition to guarantee its\nfinite convergence (i.e., the optimal value of the corresponding degree-n\nsemidefinite relaxation of the hierarchy is the global minimum). For Quadratic\nConstrained Quadratic Problems (QCQPs) one may also recover global minimizers\nfrom the optimal pseudo-moment sequence. Our condition is in the spirit of\nBlekherman's rank condition and while on the one-hand it is more restrictive,\non the other hand it applies to constrained POPs as it provides a localization\non $K$ for the representing measure.",
        "In this paper, we consider the minimization of a $C^2-$smooth and strongly\nconvex objective depending on a given parameter, which is usually found in many\npractical applications. We suppose that we desire to solve the problem with\nsome inertial methods which cover a broader existing well-known inertial\nmethods. Our main goal is to analyze the derivative of this algorithm as an\ninfinite iterative process in the sense of ``automatic'' differentiation. This\nprocedure is very common and has gain more attention recently. From a pure\noptimization perspective and under some mild premises, we show that any\nsequence generated by these inertial methods converge to the unique minimizer\nof the problem, which depends on the parameter. Moreover, we show a local\nlinear convergence rate of the generated sequence. Concerning the\ndifferentiation of the scheme, we prove that the derivative of the sequence\nwith respect to the parameter converges to the derivative of the limit of the\nsequence showing that any sequence is <<derivative stable>>. Finally, we\ninvestigate the rate at which the convergence occurs. We show that, this is\nlocally linear with an error term tending to zero.",
        "Space exploration has advanced significantly, with missions increasingly\nusing complex dynamical systems. Optimal trajectory design is crucial,\ninvolving the minimization of objective functions while ensuring robustness\nagainst measurement and control errors. Recent research has focused on\nstochastic solvers that address uncertainties through chance constraints, which\nare relaxed hard constraints allowing for a given failure risk. This study\nintroduces three novel, general, multidimensional transcription methods for\nchance constraints: the spectral radius, first-order, and d-th order methods.\nAdditionally, we introduce failure risk estimation techniques and a\nconservatism metric to enable comprehensive comparison with existing\napproaches. Applications to aerospace test cases demonstrate the effectiveness\nof the proposed transcriptions, highlighting that state-of-the-art methods\nsignificantly overestimate risk. Notably, the d-th order transcription\ndramatically outperforms the other methods, particularly in high-dimensional\nscenarios. This work shows that spectral radius-based methods are overly\nconservative and computationally intensive, while the first-order and d-th\norder methods offer practical and efficient alternatives.",
        "Accurate approximation of probability measures is essential in numerical\napplications. This paper explores the quantization of probability measures\nusing the maximum mean discrepancy (MMD) distance as a guiding metric. We first\ninvestigate optimal approximations by determining the best weights, followed by\naddressing the problem of optimal facility locations.\n  To facilitate efficient computation, we reformulate the nonlinear objective\nas expectations over a product space, enabling the use of stochastic\napproximation methods. For the Gaussian kernel, we derive closed-form\nexpressions to develop a deterministic optimization approach. By integrating\nstochastic approximation with deterministic techniques, our framework achieves\nprecise and efficient quantization of continuous distributions, with\nsignificant implications for machine learning and signal processing\napplications.",
        "The object of the paper is to find complete systems of inequalities relating\nthe perimeter $P$, the area $|\\cdot|$ and the Cheeger constant $h$ of planar\nsets. To do so, we study the so called Blaschke--Santal\\'o diagram of the\ntriplet $(P,h,|\\cdot|)$ for different classes of domains: simply connected\nsets, convex sets and convex polygons with at most $N$ sides. We completely\ndetermine the diagram in the latter cases except for the class of convex\n$N$-gons when $N\\ge 5$ is odd: therein, we show that the boundary of the\ndiagram is given by the graphs of two continuous and strictly increasing\nfunctions. An explicit formula for the lower one and a numerical method to\nobtain the upper one is provided. At last, some applications of the results are\npresented.",
        "Given the complexity of multi-tenant cloud environments and the need for\nreal-time threat mitigation, Security Operations Centers (SOCs) must integrate\nAI-driven adaptive defenses against Advanced Persistent Threats (APTs).\nHowever, SOC analysts struggle with countering adaptive adversarial tactics,\nnecessitating intelligent decision-support frameworks. To enhance human-AI\ncollaboration in SOCs, we propose a Cognitive Hierarchy Theory-driven Deep\nQ-Network (CHT-DQN) framework that models SOC analysts' decision-making against\nAI-driven APT bots. The SOC analyst (defender) operates at cognitive level-1,\nanticipating attacker strategies, while the APT bot (attacker) follows a\nlevel-0 exploitative policy. By incorporating CHT into DQN, our framework\nenhances SOC defense strategies via Attack Graph (AG)-based reinforcement\nlearning. Simulation experiments across varying AG complexities show that\nCHT-DQN achieves higher data protection and lower action discrepancies compared\nto standard DQN. A theoretical lower bound analysis further validates its\nsuperior Q-value performance. A human-in-the-loop (HITL) evaluation on Amazon\nMechanical Turk (MTurk) reveals that SOC analysts using CHT-DQN-driven\ntransition probabilities align better with adaptive attackers, improving data\nprotection. Additionally, human decision patterns exhibit risk aversion after\nfailure and risk-seeking behavior after success, aligning with Prospect Theory.\nThese findings underscore the potential of integrating cognitive modeling into\ndeep reinforcement learning to enhance SOC operations and develop real-time\nadaptive cloud security mechanisms.",
        "The aim of this paper is to study the fluctuations of a general class of\nsupercritical branching Markov processes with non-local branching mechanisms.\nWe show the existence of three regimes according to the size of the spectral\ngap associated with the expectation semigroup of the branching process and\nestablish functional central limit theorems within each regime.",
        "In-context machine translation (MT) with large language models (LLMs) is a\npromising approach for low-resource MT, as it can readily take advantage of\nlinguistic resources such as grammar books and dictionaries. Such resources are\nusually selectively integrated into the prompt so that LLMs can directly\nperform translation without any specific training, via their in-context\nlearning capability (ICL). However, the relative importance of each type of\nresource e.g., dictionary, grammar book, and retrieved parallel examples, is\nnot entirely clear. To address this gap, this study systematically investigates\nhow each resource and its quality affects the translation performance, with the\nManchu language as our case study. To remove any prior knowledge of Manchu\nencoded in the LLM parameters and single out the effect of ICL, we also\nexperiment with an encrypted version of Manchu texts. Our results indicate that\nhigh-quality dictionaries and good parallel examples are very helpful, while\ngrammars hardly help. In a follow-up study, we showcase a promising application\nof in-context MT: parallel data augmentation as a way to bootstrap the\nconventional MT model. When monolingual data abound, generating synthetic\nparallel data through in-context MT offers a pathway to mitigate data scarcity\nand build effective and efficient low-resource neural MT systems.",
        "Since molecular clouds form stars, at least some parts of them must be in a\nstate of collapse. However, there is a long-standing debate as to whether that\ncollapse is local, involving only a small fraction of the cloud mass, or\nglobal, with most mass in a state of collapse up to the moment when it is\ndispersed by stellar feedback. In principle it is possible to distinguish these\npossibilities from clouds' virial ratios, which should be a factor of two\nlarger for collapse than for equilibrium, but systematic uncertainties have\nthus far prevented such measurements. Here we propose a new analysis method to\novercome this limitation: while the absolute value of a cloud's virial ratio is\ntoo uncertain to distinguish global from local collapse, the differential\nchange in virial ratio as a function of surface density is also diagnostic of\nclouds' dynamical state, and can be measured with far fewer systematic\nuncertainties. We demonstrate the basic principles of the method using simple\nanalytic models of supported and collapsing clouds, validate it from full 3D\nsimulations, and discuss possible challenges in applying the method to real\ndata. We then provide a preliminary application of the technique to recent\nobservations of the molecular clouds in Andromeda, showing that most of them\nare inconsistent with being in a state of global collapse.",
        "The Berry curvature characterizes one aspect of the geometry of quantum\nstates. It materializes, among other consequences, as an anomalous velocity of\nwave packets. In non-Hermitian systems, wave packet dynamics is enriched by\nadditional terms that can be expressed as generalizations of the Berry\nconnection to non-orthogonal eigenstates. Here, we contextualize these\nanomalous non-Hermitian contributions by showing that they directly arise from\nthe geometry of the underlying quantum states as corrections to the distance\nbetween left and perturbed right eigenstates. By calculating the electric\nsusceptibility for a single-band wave packet and comparing it with the wave\npacket's localization, we demonstrate that these terms can, in some\ncircumstances, lead to a violation of fluctuation-dissipation relations in\nnon-Hermitian systems. We discuss experimental signatures in terms of response\nfunctions and transport signatures.",
        "We investigate the collisions of uncharged, conducting droplets settling\nunder gravity in the presence of an external electric field. Previous studies\nhave derived a near-field asymptotic expression for the electric-field-induced\nattraction, suggesting that this force can overcome lubrication resistance and\ndrive surface-to-surface contact between two spherical conductors within a\nfinite time. However, for droplets moving in air, traditional lubrication\ntheory breaks down when the inter-droplet gap approaches the mean free path of\nair molecules. To account for this, we incorporate non-continuum hydrodynamic\neffects to estimate the gravity-driven collision efficiency under\nelectric-field-induced forces. This study examines how an external electric\nfield influences the trajectories of settling droplet pairs of unequal sizes.\nBy analyzing their motion, we compute collision efficiencies and explore their\ndependence on droplet size ratio, electric field strength, the angle between\nthe field and gravity, and key dimensionless parameters governing\nelectric-field-induced and van der Waals forces. Our findings reveal that\nelectric-field-induced forces significantly enhance collision efficiency,\nhighlighting their critical role in droplet coalescence dynamics.",
        "Traditional sequential recommendation (SR) methods heavily rely on explicit\nitem IDs to capture user preferences over time. This reliance introduces\ncritical limitations in cold-start scenarios and domain transfer tasks, where\nunseen items and new contexts often lack established ID mappings. To overcome\nthese limitations, recent studies have shifted towards leveraging text-only\ninformation for recommendation, thereby improving model generalization and\nadaptability across domains. Although promising, text-based SR faces unique\ndifficulties: items' text descriptions often share semantic similarities that\nlead to clustered item representations, compromising their uniformity, a\nproperty essential for promoting diversity and enhancing generalization in\nrecommendation systems. In this paper, we explore a novel framework to improve\nthe uniformity of item representations in text-based SR. Our analysis reveals\nthat items within a sequence exhibit marked semantic similarity, meaning they\nare closer in representation than items overall, and that this effect is more\npronounced for less popular items, which form tighter clusters compared to\ntheir more popular counterparts. Based on these findings, we propose UniT, a\nframework that employs three pairwise item sampling strategies: Unified General\nSampling Strategy, Sequence-Driven Sampling Strategy, and Popularity-Driven\nSampling Strategy. Each strategy applies varying degrees of repulsion to\nselectively adjust the distances between item pairs, thereby refining\nrepresentation uniformity while considering both sequence context and item\npopularity. Extensive experiments on multiple real-world datasets demonstrate\nthat our proposed approach outperforms state-of-the-art models, validating the\neffectiveness of UniT in enhancing both representation uniformity and\nrecommendation accuracy.The source code is available at\nhttps:\/\/github.com\/ccwwhhh\/Model-Rec.",
        "We investigate the influence of an early matter-dominated era in cosmic\nhistory on the dynamics of cosmic strings and the resulting stochastic\ngravitational waves. Specifically, we examine the case where this era\noriginates from the dark matter dilution mechanism within the framework of the\nminimal left-right symmetric model. By numerically solving the Boltzmann\nequations governing the energy densities of the relevant components, we\nmeticulously analyze the modifications to the cosmological scale factor, the\nnumber density of cosmic string loops, and the gravitational wave spectrum. Our\nresults reveal that the early matter-dominated era causes a characteristic\nsuppression in the high-frequency regime of the gravitational wave spectrum,\nproviding distinct and testable signatures for future ground-based\ninterferometer experiments.",
        "The rapid development of Multimodal Large Language Models (MLLMs) has enabled\nthe integration of multiple modalities, including texts and images, within the\nlarge language model (LLM) framework. However, texts and images are usually\ninterconnected, forming a multimodal attributed graph (MMAG). It is\nunderexplored how MLLMs can incorporate the relational information\n(\\textit{i.e.}, graph structure) and semantic information (\\textit{i.e.,} texts\nand images) on such graphs for multimodal comprehension and generation. In this\npaper, we propose GraphGPT-o, which supports omni-multimodal understanding and\ncreation on MMAGs. We first comprehensively study linearization variants to\ntransform semantic and structural information as input for MLLMs. Then, we\npropose a hierarchical aligner that enables deep graph encoding, bridging the\ngap between MMAGs and MLLMs. Finally, we explore the inference choices,\nadapting MLLM to interleaved text and image generation in graph scenarios.\nExtensive experiments on three datasets from different domains demonstrate the\neffectiveness of our proposed method. Datasets and codes will be open-sourced\nupon acceptance.",
        "The proteins that exist today have been optimized over billions of years of\nnatural evolution, during which nature creates random mutations and selects\nthem. The discovery of functionally promising mutations is challenged by the\nlimited evolutionary accessible regions, i.e., only a small region on the\nfitness landscape is beneficial. There have been numerous priors used to\nconstrain protein evolution to regions of landscapes with high-fitness\nvariants, among which the change in binding free energy (DDG) of protein\ncomplexes upon mutations is one of the most commonly used priors. However, the\nhuge mutation space poses two challenges: (1) how to improve the efficiency of\nDDG prediction for fast mutation screening; and (2) how to explain mutation\npreferences and efficiently explore accessible evolutionary regions. To address\nthese challenges, we propose a lightweight DDG predictor (Light-DDG), which\nadopts a structure-aware Transformer as the backbone and enhances it by\nknowledge distilled from existing powerful but computationally heavy DDG\npredictors. Additionally, we augmented, annotated, and released a large-scale\ndataset containing millions of mutation data for pre-training Light-DDG. We\nfind that such a simple yet effective Light-DDG can serve as a good\nunsupervised antibody optimizer and explainer. For the target antibody, we\npropose a novel Mutation Explainer to learn mutation preferences, which\naccounts for the marginal benefit of each mutation per residue. To further\nexplore accessible evolutionary regions, we conduct preference-guided antibody\noptimization and evaluate antibody candidates quickly using Light-DDG to\nidentify desirable mutations.",
        "In early 2025, Augmented Intelligence - Christie's first AI art auction -\ndrew criticism for showcasing a controversial genre. Amid wider legal\nuncertainty, artists voiced concerns over data mining practices, notably with\nrespect to copyright. The backlash could be viewed as a microcosm of AI's\ncontested position in the creative economy. Touching on the auction's\npresentation, reception, and results, this paper explores how, among social\ndissonance, machine learning finds its place in the artworld. Foregrounding\nresponsible innovation, the paper provides a balanced perspective that\nchampions creators' rights and brings nuance to this polarised debate. With a\nfocus on exhibition design, it centres framing, which refers to the way a piece\nis presented to influence consumer perception. Context plays a central role in\nshaping our understanding of how good, valuable, and even ethical an artwork\nis. In this regard, Augmented Intelligence situates AI art within a\nsurprisingly traditional framework, leveraging hallmarks of \"high art\" to\nestablish the genre's cultural credibility. Generative AI has a clear economic\ndimension, converging questions of artistic merit with those of monetary worth.\nScholarship on ways of seeing, or framing, could substantively inform the\ninterpretation and evaluation of creative outputs, including assessments of\ntheir aesthetic and commercial value.",
        "Many pharmaceutical companies face concerns with the maintenance of desired\nrevenue levels. Sales forecasts for the current portfolio of products and\nprojects may indicate a decline in revenue as the marketed products approach\npatent expiry. To counteract the potential downturn in revenue, and to\nestablish revenue growth, an in-flow of new projects into the development\nphases is required. In this article, we devise an approach with which the\nin-flow of new projects could be optimized, while adhering to the objectives\nand constraints set on revenue targets, budget limitations and strategic\nconsiderations on the composition of the company's portfolio.",
        "With the advent of large multimodal language models, science is now at a\nthreshold of an AI-based technological transformation. Recently, a plethora of\nnew AI models and tools has been proposed, promising to empower researchers and\nacademics worldwide to conduct their research more effectively and efficiently.\nThis includes all aspects of the research cycle, especially (1) searching for\nrelevant literature; (2) generating research ideas and conducting\nexperimentation; generating (3) text-based and (4) multimodal content (e.g.,\nscientific figures and diagrams); and (5) AI-based automatic peer review. In\nthis survey, we provide an in-depth overview over these exciting recent\ndevelopments, which promise to fundamentally alter the scientific research\nprocess for good. Our survey covers the five aspects outlined above, indicating\nrelevant datasets, methods and results (including evaluation) as well as\nlimitations and scope for future research. Ethical concerns regarding\nshortcomings of these tools and potential for misuse (fake science, plagiarism,\nharms to research integrity) take a particularly prominent place in our\ndiscussion. We hope that our survey will not only become a reference guide for\nnewcomers to the field but also a catalyst for new AI-based initiatives in the\narea of \"AI4Science\".",
        "Fast Radio Bursts (FRBs) are millisecond-duration radio transients from\ndistant galaxies. While most FRBs are singular events, repeaters emit multiple\nbursts, with only two-FRB 121102 and FRB 180916B-showing periodic activity (160\nand 16 days, respectively). FRB 20240209A, discovered by CHIME-FRB, is\nlocalized to the outskirts of a quiescent elliptical galaxy (z = 0.1384). We\ndiscovered a periodicity of ~ 126 days in the activity of the FRB 20240209A,\npotentially adding to the list of extremely rare periodic repeating FRBs. We\nused auto-correlation and Lomb-Scargle periodogram analyses, validated with\nrandomized control samples, to confirm the periodicity. The FRB's location in\nan old stellar population disfavors young progenitor models, instead pointing\nto scenarios involving globular clusters, late-stage magnetars, or low-mass\nX-ray binaries (LMXBs). Though deep X-ray or polarimetric observations are not\navailable, the localization of the FRB and a possible periodicity points to\nprogenitors likely to be a binary involving a compact object and a stellar\ncompanion or a precessing or rotating old neutron star.",
        "Joint phase-time arrays (JPTA) emerge as a cost-effective and\nenergy-efficient architecture for frequency-dependent beamforming in wideband\ncommunications by utilizing both true-time delay units and phase shifters. This\npaper exploits the potential of JPTA to simultaneously serve multiple users in\nboth near- and far-field regions with a single radio frequency chain. The goal\nis to jointly optimize JPTA-based beamforming and subband allocation to\nmaximize overall system performance. To this end, we formulate a system utility\nmaximization problem, including sum-rate maximization and proportional fairness\nas special cases. We develop a 3-step alternating optimization (AO) algorithm\nand an efficient deep learning (DL) method for this problem. The DL approach\nincludes a 2-layer convolutional neural network, a 3-layer graph attention\nnetwork (GAT), and a normalization module for resource and beamforming\noptimization. The GAT efficiently captures the interactions between resource\nallocation and analog beamformers. Simulation results confirm that JPTA\noutperforms conventional phased arrays (PA) in enhancing user rate and strikes\na good balance between PA and fully-digital approach in energy efficiency.\nEmploying a logarithmic utility function for user rates ensures greater\nfairness than maximizing sum-rates. Furthermore, the DL network achieves\ncomparable performance to the AO approach, while having orders of magnitude\nlower computational complexity."
      ]
    }
  },
  {
    "id":2412.20675,
    "research_type":"applied",
    "start_id":"b13",
    "start_title":"Discriminative feature learning using a multiscale convolutional capsule network from attitude data for fault diagnosis of industrial robots",
    "start_abstract":"Effective fault diagnosis is important to ensure the reliability, safety, and efficiency of industrial robots. This article proposes a simple yet effective data acquisition strategy based on transmission mechanism analysis, using only one attitude sensor mounted on an end effector or an output component to monitor the attitude of all transmission components. Unlike widely used vibration-monitoring signals, attitude signals can provide fault features reflecting spatial relationships. Using one attitude sensor facilitates the data collection, but weakens fault features and introduces strong background noise in attitude signals. To learn discriminative features from the attitude data collected by the attitude sensor, a multiscale convolutional capsule network (MCCN) is proposed. In MCCN, integrating low-level and high-level features in a convolutional neural network (CNN) as multiscale features is conductive to noise reduction and robust feature extraction, and a capsule network (CapsNet) is used to recognize the spatial relationships in attitude data. The extracted multiscale features in CNN and the spatial-relational features in CapsNet are fused for effective fault diagnosis of industrial robots. The performance of MCCN is evaluated by attaching a softmax-based classifier and integrating it into different transfer learning frameworks to diagnose faults in industrial robots under single and variable working conditions, respectively. Fault diagnosis experiments were conducted on a 6-axis series industrial robot and a parallel robot-driven 3D printer. The superiority of the proposed MCCN was demonstrated by comparing its performance with the other feature learning methods.",
    "start_categories":[
      "eess.SP"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b21"
      ],
      "title":[
        "LSTM Based Bearing Fault Diagnosis of Electrical Machines using Motor Current Signal"
      ],
      "abstract":[
        "Rolling element bearings are one of the most critical components rotating machinery, with bearing faults amounting up to 50% in electrical machines. Therefore, fault diagnosis has attracted attention many researchers. Typically, is performed using vibration signals from machine. In addition, by deep learning algorithms on signals, detection accuracy close 100% can be achieved. However, measurement requires an additional sensor, which not present majority Nevertheless, alternative approach, stator current used for diagnosis. paper emphasizes current. The signal processing signature extraction that buried underneath noise signal. uses Paderborn University damaged dataset, contains data healthy, real inner raceway and outer different severity. For redundant frequencies filtered, then filtered eight features extracted time time-frequency domain wavelet packet decomposition (WPD). Then, these well known algorithm Long Short-Term Memory (LSTM), classification made. LSTM mostly speech recognition due its coherence, but this paper, ability also demonstrated 96%, outperforms perform method developed independent speed loading conditions."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "The FFT Strikes Again: An Efficient Alternative to Self-Attention",
        "Whenever, Wherever: Towards Orchestrating Crowd Simulations with\n  Spatio-Temporal Spawn Dynamics",
        "It's My Data Too: Private ML for Datasets with Multi-User Training\n  Examples",
        "Smoothing ADMM for Non-convex and Non-smooth Hierarchical Federated\n  Learning",
        "GPT-HTree: A Decision Tree Framework Integrating Hierarchical Clustering\n  and Large Language Models for Explainable Classification",
        "Al-Khwarizmi: Discovering Physical Laws with Foundation Models",
        "Efficient and Sharp Off-Policy Learning under Unobserved Confounding",
        "Food Delivery Time Prediction in Indian Cities Using Machine Learning\n  Models",
        "LabTOP: A Unified Model for Lab Test Outcome Prediction on Electronic\n  Health Records",
        "Uncovering Utility Functions from Observed Outcomes",
        "Continuous K-Max Bandits",
        "Reinforcement Learning with Segment Feedback",
        "A Structured Reasoning Framework for Unbalanced Data Classification\n  Using Probabilistic Models",
        "Love numbers beyond GR from the modified Teukolsky equation",
        "Formation of Be star decretion discs through boundary layer effects",
        "Action representability in categories of unitary algebras",
        "Impact of the Pandemic on Currency Circulation in Brazil: Projections\n  using the SARIMA Model",
        "A precise asymptotic analysis of learning diffusion models: theory and\n  insights",
        "PINK: physical-informed machine learning for lattice thermal\n  conductivity",
        "Search for doubly charmed dibaryons in baryon-baryon scattering",
        "Nonlinear Nanophotonics for High-Dimensional Quantum States",
        "From discrete-time policies to continuous-time diffusion samplers:\n  Asymptotic equivalences and faster training",
        "Linear-Quadratic Optimal Control for Mean-Field Stochastic Differential\n  Equations in Infinite-Horizon with Regime Switching",
        "Pair correlations of one-dimensional model sets and monstrous\n  covariograms of Rauzy fractals",
        "A population synthesis study of the Gaia 100 pc unresolved white\n  dwarf-main sequence binary population",
        "Max-Min Fairness for IRS-Assisted Secure Two-Way Communications",
        "On Coordinated Drone-Courier Logistics for Intra-city Express Services",
        "Extinction and Extirpation Conditions in Coalescent and Ecotonal\n  Metacommunities"
      ],
      "abstract":[
        "Conventional self-attention mechanisms exhibit quadratic complexity in\nsequence length, making them challenging to scale for long inputs. We present\nFFTNet, an adaptive spectral filtering framework that uses the Fast Fourier\nTransform (FFT) to achieve global token mixing in \\(\\mathcal{O}(n\\log n)\\)\ntime. By mapping inputs into the frequency domain, FFTNet exploits\northogonality and energy preservation-guaranteed by Parseval's theorem-to\nefficiently model long-range dependencies. Our main theoretical contributions\ninclude 1) An adaptive spectral filter that highlights salient frequency\ncomponents, 2) A hybrid scheme combining local windowing with a global FFT\nbranch, 3) Nonlinear feature transformations applied in both the frequency and\ntoken domains. Experiments on Long Range Arena and ImageNet validate our\ntheoretical insights and demonstrate superior performance over fixed\nFourier-based and standard attention models.",
        "Realistic crowd simulations are essential for immersive virtual environments,\nrelying on both individual behaviors (microscopic dynamics) and overall crowd\npatterns (macroscopic characteristics). While recent data-driven methods like\ndeep reinforcement learning improve microscopic realism, they often overlook\ncritical macroscopic features such as crowd density and flow, which are\ngoverned by spatio-temporal spawn dynamics, namely, when and where agents enter\na scene. Traditional methods, like random spawn rates, stochastic processes, or\nfixed schedules, are not guaranteed to capture the underlying complexity or\nlack diversity and realism. To address this issue, we propose a novel approach\ncalled nTPP-GMM that models spatio-temporal spawn dynamics using Neural\nTemporal Point Processes (nTPPs) that are coupled with a spawn-conditional\nGaussian Mixture Model (GMM) for agent spawn and goal positions. We evaluate\nour approach by orchestrating crowd simulations of three diverse real-world\ndatasets with nTPP-GMM. Our experiments demonstrate the orchestration with\nnTPP-GMM leads to realistic simulations that reflect real-world crowd scenarios\nand allow crowd analysis.",
        "We initiate a study of algorithms for model training with user-level\ndifferential privacy (DP), where each example may be attributed to multiple\nusers, which we call the multi-attribution model. We first provide a carefully\nchosen definition of user-level DP under the multi-attribution model. Training\nin the multi-attribution model is facilitated by solving the contribution\nbounding problem, i.e. the problem of selecting a subset of the dataset for\nwhich each user is associated with a limited number of examples. We propose a\ngreedy baseline algorithm for the contribution bounding problem. We then\nempirically study this algorithm for a synthetic logistic regression task and a\ntransformer training task, including studying variants of this baseline\nalgorithm that optimize the subset chosen using different techniques and\ncriteria. We find that the baseline algorithm remains competitive with its\nvariants in most settings, and build a better understanding of the practical\nimportance of a bias-variance tradeoff inherent in solutions to the\ncontribution bounding problem.",
        "This paper presents a hierarchical federated learning (FL) framework that\nextends the alternating direction method of multipliers (ADMM) with smoothing\ntechniques, tailored for non-convex and non-smooth objectives. Unlike\ntraditional hierarchical FL methods, our approach supports asynchronous updates\nand multiple updates per iteration, enhancing adaptability to heterogeneous\ndata and system settings. Additionally, we introduce a flexible mechanism to\nleverage diverse regularization functions at each layer, allowing customization\nto the specific prior information within each cluster and accommodating\n(possibly) non-smooth penalty objectives. Depending on the learning goal, the\nframework supports both consensus and personalization: the total variation norm\ncan be used to enforce consensus across layers, while non-convex penalties such\nas minimax concave penalty (MCP) or smoothly clipped absolute deviation (SCAD)\nenable personalized learning. Experimental results demonstrate the superior\nconvergence rates and accuracy of our method compared to conventional\napproaches, underscoring its robustness and versatility for a wide range of FL\nscenarios.",
        "This paper introduces GPT-HTree, a framework combining hierarchical\nclustering, decision trees, and large language models (LLMs) to address this\nchallenge. By leveraging hierarchical clustering to segment individuals based\non salient features, resampling techniques to balance class distributions, and\ndecision trees to tailor classification paths within each cluster, GPT-HTree\nensures both accuracy and interpretability. LLMs enhance the framework by\ngenerating human-readable cluster descriptions, bridging quantitative analysis\nwith actionable insights.",
        "Inferring physical laws from data is a central challenge in science and\nengineering, including but not limited to healthcare, physical sciences,\nbiosciences, social sciences, sustainability, climate, and robotics. Deep\nnetworks offer high-accuracy results but lack interpretability, prompting\ninterest in models built from simple components. The Sparse Identification of\nNonlinear Dynamics (SINDy) method has become the go-to approach for building\nsuch modular and interpretable models. SINDy leverages sparse regression with\nL1 regularization to identify key terms from a library of candidate functions.\nHowever, SINDy's choice of candidate library and optimization method requires\nsignificant technical expertise, limiting its widespread applicability. This\nwork introduces Al-Khwarizmi, a novel agentic framework for physical law\ndiscovery from data, which integrates foundational models with SINDy.\nLeveraging LLMs, VLMs, and Retrieval-Augmented Generation (RAG), our approach\nautomates physical law discovery, incorporating prior knowledge and iteratively\nrefining candidate solutions via reflection. Al-Khwarizmi operates in two\nsteps: it summarizes system observations-comprising textual descriptions, raw\ndata, and plots-followed by a secondary step that generates candidate feature\nlibraries and optimizer configurations to identify hidden physics laws\ncorrectly. Evaluating our algorithm on over 198 models, we demonstrate\nstate-of-the-art performance compared to alternatives, reaching a 20 percent\nincrease against the best-performing alternative.",
        "We develop a novel method for personalized off-policy learning in scenarios\nwith unobserved confounding. Thereby, we address a key limitation of standard\npolicy learning: standard policy learning assumes unconfoundedness, meaning\nthat no unobserved factors influence both treatment assignment and outcomes.\nHowever, this assumption is often violated, because of which standard policy\nlearning produces biased estimates and thus leads to policies that can be\nharmful. To address this limitation, we employ causal sensitivity analysis and\nderive a statistically efficient estimator for a sharp bound on the value\nfunction under unobserved confounding. Our estimator has three advantages: (1)\nUnlike existing works, our estimator avoids unstable minimax optimization based\non inverse propensity weighted outcomes. (2) Our estimator is statistically\nefficient. (3) We prove that our estimator leads to the optimal\nconfounding-robust policy. Finally, we extend our theory to the related task of\npolicy improvement under unobserved confounding, i.e., when a baseline policy\nsuch as the standard of care is available. We show in experiments with\nsynthetic and real-world data that our method outperforms simple plug-in\napproaches and existing baselines. Our method is highly relevant for\ndecision-making where unobserved confounding can be problematic, such as in\nhealthcare and public policy.",
        "Accurate prediction of food delivery times significantly impacts customer\nsatisfaction, operational efficiency, and profitability in food delivery\nservices. However, existing studies primarily utilize static historical data\nand often overlook dynamic, real-time contextual factors crucial for precise\nprediction, particularly in densely populated Indian cities. This research\naddresses these gaps by integrating real-time contextual variables such as\ntraffic density, weather conditions, local events, and geospatial data\n(restaurant and delivery location coordinates) into predictive models. We\nsystematically compare various machine learning algorithms, including Linear\nRegression, Decision Trees, Bagging, Random Forest, XGBoost, and LightGBM, on a\ncomprehensive food delivery dataset specific to Indian urban contexts. Rigorous\ndata preprocessing and feature selection significantly enhanced model\nperformance. Experimental results demonstrate that the LightGBM model achieves\nsuperior predictive accuracy, with an R2 score of 0.76 and Mean Squared Error\n(MSE) of 20.59, outperforming traditional baseline approaches. Our study thus\nprovides actionable insights for improving logistics strategies in complex\nurban environments. The complete methodology and code are publicly available\nfor reproducibility and further research.",
        "Lab tests are fundamental for diagnosing diseases and monitoring patient\nconditions. However, frequent testing can be burdensome for patients, and test\nresults may not always be immediately available. To address these challenges,\nwe propose LabTOP, a unified model that predicts lab test outcomes by\nleveraging a language modeling approach on EHR data. Unlike conventional\nmethods that estimate only a subset of lab tests or classify discrete value\nranges, LabTOP performs continuous numerical predictions for a diverse range of\nlab items. We evaluate LabTOP on three publicly available EHR datasets and\ndemonstrate that it outperforms existing methods, including traditional machine\nlearning models and state-of-the-art large language models. We also conduct\nextensive ablation studies to confirm the effectiveness of our design choices.\nWe believe that LabTOP will serve as an accurate and generalizable framework\nfor lab test outcome prediction, with potential applications in clinical\ndecision support and early detection of critical conditions.",
        "Determining consumer preferences and utility is a foundational challenge in\neconomics. They are central in determining consumer behaviour through the\nutility-maximising consumer decision-making process. However, preferences and\nutilities are not observable and may not even be known to the individual making\nthe choice; only the outcome is observed in the form of demand. Without the\nability to observe the decision-making mechanism, demand estimation becomes a\nchallenging task and current methods fall short due to lack of scalability or\nability to identify causal effects. Estimating these effects is critical when\nconsidering changes in policy, such as pricing, the impact of taxes and\nsubsidies, and the effect of a tariff. To address the shortcomings of existing\nmethods, we combine revealed preference theory and inverse reinforcement\nlearning to present a novel algorithm, Preference Extraction and Reward\nLearning (PEARL) which, to the best of our knowledge, is the only algorithm\nthat can uncover a representation of the utility function that best\nrationalises observed consumer choice data given a specified functional form.\nWe introduce a flexible utility function, the Input-Concave Neural Network\nwhich captures complex relationships across goods, including cross-price\nelasticities. Results show PEARL outperforms the benchmark on both noise-free\nand noisy synthetic data.",
        "We study the $K$-Max combinatorial multi-armed bandits problem with\ncontinuous outcome distributions and weak value-index feedback: each base arm\nhas an unknown continuous outcome distribution, and in each round the learning\nagent selects $K$ arms, obtains the maximum value sampled from these $K$ arms\nas reward and observes this reward together with the corresponding arm index as\nfeedback. This setting captures critical applications in recommendation\nsystems, distributed computing, server scheduling, etc. The continuous $K$-Max\nbandits introduce unique challenges, including discretization error from\ncontinuous-to-discrete conversion, non-deterministic tie-breaking under limited\nfeedback, and biased estimation due to partial observability. Our key\ncontribution is the computationally efficient algorithm DCK-UCB, which combines\nadaptive discretization with bias-corrected confidence bounds to tackle these\nchallenges. For general continuous distributions, we prove that DCK-UCB\nachieves a $\\widetilde{\\mathcal{O}}(T^{3\/4})$ regret upper bound, establishing\nthe first sublinear regret guarantee for this setting. Furthermore, we identify\nan important special case with exponential distributions under full-bandit\nfeedback. In this case, our proposed algorithm MLE-Exp enables\n$\\widetilde{\\mathcal{O}}(\\sqrt{T})$ regret upper bound through maximal\nlog-likelihood estimation, achieving near-minimax optimality.",
        "Standard reinforcement learning (RL) assumes that an agent can observe a\nreward for each state-action pair. However, in practical applications, it is\noften difficult and costly to collect a reward for each state-action pair.\nWhile there have been several works considering RL with trajectory feedback, it\nis unclear if trajectory feedback is inefficient for learning when trajectories\nare long. In this work, we consider a model named RL with segment feedback,\nwhich offers a general paradigm filling the gap between per-state-action\nfeedback and trajectory feedback. In this model, we consider an episodic Markov\ndecision process (MDP), where each episode is divided into $m$ segments, and\nthe agent observes reward feedback only at the end of each segment. Under this\nmodel, we study two popular feedback settings: binary feedback and sum\nfeedback, where the agent observes a binary outcome and a reward sum according\nto the underlying reward function, respectively. To investigate the impact of\nthe number of segments $m$ on learning performance, we design efficient\nalgorithms and establish regret upper and lower bounds for both feedback\nsettings. Our theoretical and experimental results show that: under binary\nfeedback, increasing the number of segments $m$ decreases the regret at an\nexponential rate; in contrast, surprisingly, under sum feedback, increasing $m$\ndoes not reduce the regret significantly.",
        "This paper studies a Markov network model for unbalanced data, aiming to\nsolve the problems of classification bias and insufficient minority class\nrecognition ability of traditional machine learning models in environments with\nuneven class distribution. By constructing joint probability distribution and\nconditional dependency, the model can achieve global modeling and reasoning\noptimization of sample categories. The study introduced marginal probability\nestimation and weighted loss optimization strategies, combined with\nregularization constraints and structured reasoning methods, effectively\nimproving the generalization ability and robustness of the model. In the\nexperimental stage, a real credit card fraud detection dataset was selected and\ncompared with models such as logistic regression, support vector machine,\nrandom forest and XGBoost. The experimental results show that the Markov\nnetwork performs well in indicators such as weighted accuracy, F1 score, and\nAUC-ROC, significantly outperforming traditional classification models,\ndemonstrating its strong decision-making ability and applicability in\nunbalanced data scenarios. Future research can focus on efficient model\ntraining, structural optimization, and deep learning integration in large-scale\nunbalanced data environments and promote its wide application in practical\napplications such as financial risk control, medical diagnosis, and intelligent\nmonitoring.",
        "We obtain the full set of tidal Love numbers of non-rotating black holes in\nan effective field theory extension of general relativity. We achieve our\nresults using a recently introduced modified Teukolsky equation that describes\nthe perturbations of black holes in this theory. We show how to identify the\nLove numbers and their beta functions in a systematic and gauge invariant way,\napplying analytic continuation on the angular number $\\ell$ when necessary. We\nobserve that there are three types of Love numbers: electric, magnetic, and a\n``mixing'' type, associated to parity-breaking theories, that we identify here\nfor the first time. The modified Teukolsky equation proves to be very useful as\nit allows us to obtain all the different Love numbers in a unified framework.\nWe compare our results with previous literature that utilized the\nRegge-Wheeler-Zerilli equations to compute Love numbers, finding perfect\nagreement. The method introduced here paves the way towards the computation of\nLove numbers of rotating black holes beyond general relativity.",
        "Be stars are rapidly rotating, with angular frequency around $0.7-0.8$ of\ntheir Keplerian break up frequency, as a result of significant accretion during\nthe earlier stellar evolution of a companion star. Material from the equator of\nthe Be star is ejected and forms a decretion disc, although the mechanism for\nthe disc formation has remained elusive. We find one-dimensional steady state\ndecretion disc solutions that smoothly transition from a rapidly rotating star\nthat is in hydrostatic balance. Boundary layer effects in a geometrically thick\ndisc which connects to a rotationally flattened star enable the formation of a\ndecretion disc at stellar spin rates below the break up rate. For a disc with\nan aspect ratio $H\/R\\approx 0.1$ at the inner edge, the torque from the disc on\nthe star slows the stellar spin to the observed range and mass ejection\ncontinues at a rate consistent with observed decretion rates. The critical\nrotation rate, to which the star slows down to, decreases as the disc aspect\nratio increases. More generally, steady state accretion and decretion disc\nsolutions can be found for all stellar spin rates. The outcome for a particular\nsystem depends upon the balance between the decretion rate and any external\ninfall accretion rate.",
        "In a recent article [13], G. Janelidze introduced the concept of ideally\nexact categories as a generalization of semi-abelian categories, aiming to\nincorporate relevant examples of non-pointed categories, such as the categories\n$\\textbf{Ring}$ and $\\textbf{CRing}$ of unitary (commutative) rings. He also\nextended the notion of action representability to this broader framework,\nproving that both $\\textbf{Ring}$ and $\\textbf{CRing}$ are action\nrepresentable.\n  This article investigates the representability of actions of unitary\nnon-associative algebras. After providing a detailed description of the monadic\nadjunction associated with any category of unitary algebra, we use the\nconstruction of the external weak actor [4] in order to prove that the\ncategories of unitary (commutative) associative algebras and that of unitary\nalternative algebras are action representable. The result is then extended for\nunitary (commutative) Poisson algebras, where the explicit construction of the\nuniversal strict general actor is employed.",
        "This study analyzes the impact of the COVID-19 pandemic on currency\ncirculation in Brazil by comparing actual data from 2000 to 2023 with\ncounterfactual projections using the\n\\textbf{SARIMA(3,1,1)(3,1,4)\\textsubscript{12}} model. The model was selected\nbased on an extensive parameter search, balancing accuracy and simplicity, and\nvalidated through the metrics MAPE, RMSE, and AIC. The results indicate a\nsignificant deviation between projected and observed values, with an average\ndifference of R\\$ 47.57 billion (13.95\\%). This suggests that the pandemic,\nemergency policies, and the introduction of \\textit{Pix} had a substantial\nimpact on currency circulation. The robustness of the SARIMA model was\nconfirmed, effectively capturing historical trends and seasonality, though\nfindings emphasize the importance of considering exogenous variables, such as\ninterest rates and macroeconomic policies, in future analyses. Future research\nshould explore multivariate models incorporating economic indicators, long-term\nanalysis of post-pandemic currency circulation trends, and studies on public\ncash-holding behavior. The results reinforce the need for continuous monitoring\nand econometric modeling to support decision-making in uncertain economic\ncontexts.",
        "In this manuscript, we consider the problem of learning a flow or\ndiffusion-based generative model parametrized by a two-layer auto-encoder,\ntrained with online stochastic gradient descent, on a high-dimensional target\ndensity with an underlying low-dimensional manifold structure. We derive a\ntight asymptotic characterization of low-dimensional projections of the\ndistribution of samples generated by the learned model, ascertaining in\nparticular its dependence on the number of training samples. Building on this\nanalysis, we discuss how mode collapse can arise, and lead to model collapse\nwhen the generative model is re-trained on generated synthetic data.",
        "Lattice thermal conductivity ($\\kappa_L$) is crucial for efficient thermal\nmanagement in electronics and energy conversion technologies. Traditional\nmethods for predicting \\k{appa}L are often computationally expensive, limiting\ntheir scalability for large-scale material screening. Empirical models, such as\nthe Slack model, offer faster alternatives but require time-consuming\ncalculations for key parameters such as sound velocity and the Gruneisen\nparameter. This work presents a high-throughput framework, physical-informed\nkappa (PINK), which combines the predictive power of crystal graph\nconvolutional neural networks (CGCNNs) with the physical interpretability of\nthe Slack model to predict \\k{appa}L directly from crystallographic information\nfiles (CIFs). Unlike previous approaches, PINK enables rapid, batch predictions\nby extracting material properties such as bulk and shear modulus from CIFs\nusing a well-trained CGCNN model. These properties are then used to compute the\nnecessary parameters for $\\kappa_L$ calculation through a simplified physical\nformula. PINK was applied to a dataset of 377,221 stable materials, enabling\nthe efficient identification of promising candidates with ultralow $\\kappa_L$\nvalues, such as Ag$_3$Te$_4$W and Ag$_3$Te$_4$Ta. The platform, accessible via\na user-friendly interface, offers an unprecedented combination of speed,\naccuracy, and scalability, significantly accelerating material discovery for\nthermal management and energy conversion applications.",
        "We perform a systematical investigation of the doubly charmed dibaryon system\nwith quantum numbers $IJ=01$, and strangeness numbers $S=0$, $-2$ and $-4$ in\nthe framework of the chiral quark model. Two resonance states with strangeness\nnumbers $S=-2$ is obtained in the $\\Lambda\\Omega_{cc}$ scattering channel,\nwhich are $\\Xi_{cc}^{\\ast}\\Xi$ with resonance mass 5081 MeV and decay width 0.3\nMeV, and the $\\Xi_{c}\\Xi_{c}^{\\ast}$ state with the mass 5213 MeV and decay\nwidth 19.8 MeV, respectively. These two predicted charmed dibaryon candidates\nare worth searching for experimentally. Besides, we would like to emphasize\nthat the multi-channel coupling calculation is important to confirm the\nexistence of multiquark states. The coupling can shift the energy of the\nresonance, give the width to the resonance and even destroy the resonance.\nTherefore, to provide the necessary information for experiments to search for\nexotic hadron states, the coupling calculation between the bound channels and\nopen channels is indispensable.",
        "Quantum nanophotonics merges the precision of nanoscale light manipulation\nwith the capabilities of quantum technologies, offering a pathway for enhanced\nlight-matter interaction and compact realization of quantum devices. Here, we\nshow how a recently-demonstrated nonlinear nanophotonic process can be employed\nto selectively create photonic high-dimensional quantum states (qudits). We\nutilize the nonlinearity on the surface of the nanophotonic device to dress,\nthrough the polarization of the pump field, the near-field modes carrying\nangular momentum and their superpositions. We then use this approach for the\nrealization of a multilevel quantum key distribution protocol, which doubles\nthe key rate compared to standard schemes. This idea is an important step\ntowards experimental realizations of quantum state generation and manipulation\nthrough nonlinearity within nanophotonic platforms, and enables new\ncapabilities for on-chip quantum devices.",
        "We study the problem of training neural stochastic differential equations, or\ndiffusion models, to sample from a Boltzmann distribution without access to\ntarget samples. Existing methods for training such models enforce time-reversal\nof the generative and noising processes, using either differentiable simulation\nor off-policy reinforcement learning (RL). We prove equivalences between\nfamilies of objectives in the limit of infinitesimal discretization steps,\nlinking entropic RL methods (GFlowNets) with continuous-time objects (partial\ndifferential equations and path space measures). We further show that an\nappropriate choice of coarse time discretization during training allows greatly\nimproved sample efficiency and the use of time-local objectives, achieving\ncompetitive performance on standard sampling benchmarks with reduced\ncomputational cost.",
        "This paper is concerned with stochastic linear quadratic (LQ, for short)\noptimal control problems in an infinite horizon with conditional mean-field\nterm in a switching regime environment. The orthogonal decomposition introduced\nin [21] has been adopted. Desired algebraic Riccati equations (AREs, for short)\nand a system of backward stochastic differential equations (BSDEs, for short)\nin infinite time horizon with the coefficients depending on the Markov chain\nhave been derived. The determination of closed-loop optimal strategy follows\nfrom the solvability of ARE and BSDE. Moreover, the solvability of BSDEs leads\nto a characterization of open-loop solvability of the optimal control problem.",
        "The averaged distance structure of one-dimensional regular model sets is\ndetermined via their pair correlation functions. The latter lead to\ncovariograms and cross covariograms of the windows, which give continuous\nfunctions in internal space. While they are simple tent-shaped, piecewise\nlinear functions for intervals, the typical case for inflation systems leads to\nconvolutions of Rauzy fractals, which are difficult to compute. In the presence\nof an inflation structure, an alternative path is possible via the exact\nrenormalisation structures of the pair correlation functions. We introduce this\napproach and derive two concrete examples, which display an unexpectedly\ncomplex and wild behaviour.",
        "Binary stars consisting of a white dwarf and a main sequence star (WDMS) are\nvaluable for studying key astrophysical questions. However, observational\nbiases strongly affect the known population, particularly unresolved systems\nwhere the main sequence star outshines the white dwarf. This work aims to\ncomprehensively simulate the population of unresolved WDMS binaries within 100\npc of the Sun and to compare the outcome with the currently most complete\nvolume-limited sample available from Gaia data. We employ a population\nsynthesis code, MRBIN, extensively developed by our group and based on Monte\nCarlo techniques, which uses a standard binary stellar evolutionary code\nadapted to cover a wide range of stars across all ages, masses, and\nmetallicities. Selection criteria matching those of Gaia observations are\napplied to generate synthetic populations comparable to the observed WDMS\nsample. The synthetic data accurately populate the expected regions in the Gaia\ncolor-magnitude diagram. However, simulations predict a lower number of\nextremely low-mass white dwarfs, suggesting potential issues in observed mass\nderivations. Additionally, our analysis constrains the common envelope\nefficiency to 0.1-0.4, consistent with previous findings, and estimates a total\ncompleteness of about 25% for the observed sample, confirming the strong\nobservational limitations for unresolved WDMS.",
        "This paper investigates an intelligent reflective surface (IRS) assisted\nsecure multi-user two-way communication system. The aim of this paper is to\nenhance the physical layer security by optimizing the minimum secrecy-rate\namong all user-pairs in the presence of a malicious user. The optimization\nproblem is converted into an alternating optimization problem consisting of two\nsub-problems. Transmit power optimization is handled using a fractional\nprogramming method, whereas IRS phase shift optimization is handled with\nsemi-definite programming. The convergence of the proposed algorithm is\ninvestigated numerically. The performance gain in minimum secrecy-rate is\nquantified for four different user configurations in comparison to the baseline\nscheme. Results indicate a 3.6-fold gain in minimum secrecy rate over the\nbaseline scheme when the IRS is positioned near a legitimate user, even when\nthe malicious user is located close to the same legitimate user.",
        "Problem definition: Drones, despite being acknowledged as a transformative\nforce in the city logistics sector, are unable to execute the\n\\textit{last-meter delivery} (unloading goods directly to customers' doorsteps)\ndue to airspace restrictions and safety concerns. To leverage advancements and\novercome the limitations of drones in providing intra-city express services, we\nintroduce a coordinated drone-courier logistics system where drones operate\nwithin a closed network among vertiports, while couriers connect customers to\nthe drone delivery system. This paper aims to shed light on this coordinated\nsystem in terms of system feasibility, network interactivity, and long-term\nsustainability. Methodology\/Results: We develop an integrated optimization\nmodel to optimize the network planning of the coordinated logistics system. The\ninterplay between network planning and tactical operations is mirrored by a\nqueueing network model, resulting in the nonlinear and nonconvex (partially\nconvex and partially concave) feasible region of the optimization model. An\niterative exact algorithm that tightens lower and upper bounds by adaptively\nrefining the linear approximations of nonlinear constraints is developed to\nprovide optimality-guaranteed solutions with finite convergence. The\ncomputational experiments demonstrate the scalability and robustness of our\nalgorithm across various network configurations and scenarios.Managerial\nimplications: The case study, based on a real-world dataset from SF Express, a\nlogistics giant in China, validates that the coordinated logistics system\nefficiently attains cost and time savings by leveraging the effective turnover\nof drones and the coordination between drones and couriers. The optimal network\ndesign features a concentrated structure, streamlining demand consolidation and\nreducing deadhead repositioning.",
        "Here we present extinction, extirpation and coexistence conditions where \/\nwhen two communities combine. We consider one specific model where two\ncommunities coalesce, and another model where the communities coexist side by\nside, blending in a transitionary zone called the ecotone. Specifically, (1) we\nanalytically calculate the shifts in abundances as a function of mixing\nstrength. (2) Obtain a critical value for the mixing strength leading to\nextinction. (3) Derive an inequality condition for full coexistent mixing. (4)\nfind how the individual communities penetrate into one other as a function of\nmixing strength. (5) derive the conditions for one species to cross the ecotone\nand invade an neighboring community and (6) conditions for a native species to\nget extirpated. Lastly, (7) we spatially investigate the species richness\nwithin the ecotone and derive a condition that determines whether the ecotone\nwill have higher or lower richness compared to its surrounding habitats."
      ]
    }
  },
  {
    "id":2412.20675,
    "research_type":"applied",
    "start_id":"b21",
    "start_title":"LSTM Based Bearing Fault Diagnosis of Electrical Machines using Motor Current Signal",
    "start_abstract":"Rolling element bearings are one of the most critical components rotating machinery, with bearing faults amounting up to 50% in electrical machines. Therefore, fault diagnosis has attracted attention many researchers. Typically, is performed using vibration signals from machine. In addition, by deep learning algorithms on signals, detection accuracy close 100% can be achieved. However, measurement requires an additional sensor, which not present majority Nevertheless, alternative approach, stator current used for diagnosis. paper emphasizes current. The signal processing signature extraction that buried underneath noise signal. uses Paderborn University damaged dataset, contains data healthy, real inner raceway and outer different severity. For redundant frequencies filtered, then filtered eight features extracted time time-frequency domain wavelet packet decomposition (WPD). Then, these well known algorithm Long Short-Term Memory (LSTM), classification made. LSTM mostly speech recognition due its coherence, but this paper, ability also demonstrated 96%, outperforms perform method developed independent speed loading conditions.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b13"
      ],
      "title":[
        "Discriminative feature learning using a multiscale convolutional capsule network from attitude data for fault diagnosis of industrial robots"
      ],
      "abstract":[
        "Effective fault diagnosis is important to ensure the reliability, safety, and efficiency of industrial robots. This article proposes a simple yet effective data acquisition strategy based on transmission mechanism analysis, using only one attitude sensor mounted on an end effector or an output component to monitor the attitude of all transmission components. Unlike widely used vibration-monitoring signals, attitude signals can provide fault features reflecting spatial relationships. Using one attitude sensor facilitates the data collection, but weakens fault features and introduces strong background noise in attitude signals. To learn discriminative features from the attitude data collected by the attitude sensor, a multiscale convolutional capsule network (MCCN) is proposed. In MCCN, integrating low-level and high-level features in a convolutional neural network (CNN) as multiscale features is conductive to noise reduction and robust feature extraction, and a capsule network (CapsNet) is used to recognize the spatial relationships in attitude data. The extracted multiscale features in CNN and the spatial-relational features in CapsNet are fused for effective fault diagnosis of industrial robots. The performance of MCCN is evaluated by attaching a softmax-based classifier and integrating it into different transfer learning frameworks to diagnose faults in industrial robots under single and variable working conditions, respectively. Fault diagnosis experiments were conducted on a 6-axis series industrial robot and a parallel robot-driven 3D printer. The superiority of the proposed MCCN was demonstrated by comparing its performance with the other feature learning methods."
      ],
      "categories":[
        "eess.SP"
      ]
    },
    "list":{
      "title":[
        "Age of Information in Multi-Relay Networks with Maximum Age Scheduling",
        "Identification of High Impedance Faults Utilizing Recurrence Plots",
        "Performance Analysis of Spatial and Temporal Learning Networks in the\n  Presence of DVL Noise",
        "RIS-enabled Multi-user M-QAM Uplink NOMA Systems: Design, Analysis, and\n  Optimization",
        "Training Channel Selection for Learning-based 1-bit Precoding in Massive\n  MU-MIMO",
        "Bayesian Beamforming for Integrated Sensing and Communication Systems",
        "Efficient Sampling Allocation Strategies for General Graph-Filter-Based\n  Signal Recovery",
        "RIS-based Physical Layer Security for Integrated Sensing and\n  Communication: A Comprehensive Survey",
        "Joint Bistatic Positioning and Monostatic Sensing: Optimized Beamforming\n  and Performance Tradeoff",
        "Integrated Long-range Sensing and Communications in Multi Target\n  Scenarios using CP-OFDM",
        "Characterisation of exposure to non-ionising electromagnetic fields in\n  the Spanish INMA birth cohort: Study protocol",
        "Maximum-Entropy-Rate Selection of Features for Classifying Changes in\n  Knee and Ankle Dynamics During Running",
        "Bistatic Micro-Doppler Analysis of a Vertical Takeoff and Landing (VTOL)\n  Drone in ICAS Framework",
        "Love numbers beyond GR from the modified Teukolsky equation",
        "Formation of Be star decretion discs through boundary layer effects",
        "Action representability in categories of unitary algebras",
        "Impact of the Pandemic on Currency Circulation in Brazil: Projections\n  using the SARIMA Model",
        "A precise asymptotic analysis of learning diffusion models: theory and\n  insights",
        "PINK: physical-informed machine learning for lattice thermal\n  conductivity",
        "Search for doubly charmed dibaryons in baryon-baryon scattering",
        "Nonlinear Nanophotonics for High-Dimensional Quantum States",
        "From discrete-time policies to continuous-time diffusion samplers:\n  Asymptotic equivalences and faster training",
        "Linear-Quadratic Optimal Control for Mean-Field Stochastic Differential\n  Equations in Infinite-Horizon with Regime Switching",
        "Pair correlations of one-dimensional model sets and monstrous\n  covariograms of Rauzy fractals",
        "A population synthesis study of the Gaia 100 pc unresolved white\n  dwarf-main sequence binary population",
        "Max-Min Fairness for IRS-Assisted Secure Two-Way Communications",
        "On Coordinated Drone-Courier Logistics for Intra-city Express Services",
        "Extinction and Extirpation Conditions in Coalescent and Ecotonal\n  Metacommunities"
      ],
      "abstract":[
        "We propose and evaluate age of information (AoI)-aware multiple access\nmechanisms for the Internet of Things (IoT) in multi-relay two-hop networks.\nThe network considered comprises end devices (EDs) communicating with a set of\nrelays in ALOHA fashion, with new information packets to be potentially\ntransmitted every time slot. The relays, in turn, forward the collected packets\nto an access point (AP), the final destination of the information generated by\nthe EDs. More specifically, in this work we investigate the performance of four\nage-aware algorithms that prioritize older packets to be transmitted, namely\nmax-age matching (MAM), iterative max-age scheduling (IMAS), age-based delayed\nrequest (ABDR), and buffered ABDR (B-ABDR). The former two algorithms are\nadapted into the multi-relay setup from previous research, and achieve\nsatisfactory average AoI and average peak AoI performance, at the expense of a\nsignificant amount of information exchange between the relays and the AP. The\nlatter two algorithms are newly proposed to let relays decide which one(s) will\ntransmit in a given time slot, requiring less signaling than the former\nalgorithms. We provide an analytical formulation for the AoI lower bound\nperformance, compare the performance of all algorithms in this set-up, and show\nthat they approach the lower bound. The latter holds especially true for\nB-ABDR, which approaches the lower bound the most closely, tilting the scale in\nits favor, as it also requires far less signaling than MAM and IMAS.",
        "This paper presents a systematic approach to detecting High Impedance Faults\n(HIFs) in medium voltage distribution networks using recurrence plots and\nmachine learning. We first simulate 1150 internal faults, including 300 HIFs,\n1000 external faults, and 40 normal conditions using the PSCAD\/EMTDC software.\nKey features are extracted from the 3-phase differential currents using wavelet\ncoefficients, which are then converted into recurrence matrices. A multi-stage\nclassification framework is employed, where the first classification stage\nidentifies internal faults, and the second stage distinguishes HIFs from other\ninternal faults. The framework is evaluated using accuracy, precision, recall,\nand F1 score. Tree-based classifiers, particularly Random Forest and Decision\nTree, achieve superior performance, with 99.24% accuracy in the first stage and\n98.26% in the second. The results demonstrate the effectiveness of integrating\nrecurrence analysis with machine learning for fault detection in power\ndistribution networks.",
        "Navigation is a critical aspect of autonomous underwater vehicles (AUVs)\noperating in complex underwater environments. Since global navigation satellite\nsystem (GNSS) signals are unavailable underwater, navigation relies on inertial\nsensing, which tends to accumulate errors over time. To mitigate this, the\nDoppler velocity log (DVL) plays a crucial role in determining navigation\naccuracy. In this paper, we compare two neural network models: an adapted\nversion of BeamsNet, based on a one-dimensional convolutional neural network,\nand a Spectrally Normalized Memory Neural Network (SNMNN). The former focuses\non extracting spatial features, while the latter leverages memory and temporal\nfeatures to provide more accurate velocity estimates while handling biased and\nnoisy DVL data. The proposed approaches were trained and tested on real AUV\ndata collected in the Mediterranean Sea. Both models are evaluated in terms of\naccuracy and estimation certainty and are benchmarked against the least squares\n(LS) method, the current model-based approach. The results show that the neural\nnetwork models achieve over a 50% improvement in RMSE for the estimation of the\nAUV velocity, with a smaller standard deviation.",
        "Non-orthogonal multiple access (NOMA) is widely recognized for enhancing the\nenergy and spectral efficiency through effective radio resource sharing.\nHowever, uplink NOMA systems face greater challenges than their downlink\ncounterparts, as their bit error rate (BER) performance is hindered by an\ninherent error floor due to error propagation caused by imperfect successive\ninterference cancellation (SIC). This paper investigates BER performance\nimprovements enabled by reconfigurable intelligent surfaces (RISs) in\nmulti-user uplink NOMA transmission. Specifically, we propose a novel\nRIS-assisted uplink NOMA design, where the RIS phase shifts are optimized to\nenhance the received signal amplitudes while mitigating the phase rotations\ninduced by the channel. To achieve this, we first develop an accurate channel\nmodel for the effective user channels, which facilitates our BER analysis. We\nthen introduce a channel alignment scheme for a two-user scenario, enabling\nefficient SIC-based detection and deriving closed-form BER expressions. We\nfurther extend the analysis to a generalized setup with an arbitrary number of\nusers and modulation orders for quadrature amplitude modulation signaling.\nUsing the derived BER expressions, we develop an optimized uplink NOMA power\nallocation (PA) scheme that minimizes the average BER while satisfying the user\ntransmit power constraints. It will be shown that the proposed NOMA detection\nscheme, in conjunction with the optimized PA strategy, eliminate SIC error\nfloors at the base station. The theoretical BER expressions are validated using\nsimulations, which confirms the effectiveness of the proposed design in\neliminating BER floors.",
        "Learning-based algorithms have gained great popularity in communications\nsince they often outperform even carefully engineered solutions by learning\nfrom training samples. In this paper, we show that the selection of appropriate\ntraining examples can be important for the performance of such learning-based\nalgorithms. In particular, we consider non-linear 1-bit precoding for massive\nmulti-user MIMO systems using the C2PO algorithm. While previous works have\nalready shown the advantages of learning critical coefficients of this\nalgorithm, we demonstrate that straightforward selection of training samples\nthat follow the channel model distribution does not necessarily lead to the\nbest result. Instead, we provide a strategy to generate training data based on\nthe specific properties of the algorithm, which significantly improves its\nerror floor performance.",
        "The uncertainty of the sensing target brings great challenge to the\nbeamforming design of the integrated sensing and communication (ISAC) system.\nTo address this issue, we model the scattering coefficient and azimuth angle of\nthe target as random variables and introduce a novel metric, expected detection\nprobability (EPd), to quantify the average detection performance from a\nBayesian perspective. Furthermore, we design a Bayesian beamforming scheme to\noptimize the expected detection probability under the limited power budget and\ncommunication performance constraints. A successive convex approximation and\nsemidefinite relaxation-based (SCA-SDR) algorithm is developed for the\ncomplicated non-convex optimization problem corresponding to the beamforming\nscheme. Simulation results show that the proposed scheme outperforms other\nbenchmarks and exhibits robust detection performance when parameters of the\ntarget are unknown and random.",
        "Sensor placement plays a crucial role in graph signal recovery in\nunderdetermined systems. In this paper, we present the graph-filtered\nregularized maximum likelihood (GFR-ML) estimator of graph signals, which\nintegrates general graph filtering with regularization to enhance signal\nrecovery performance under a limited number of sensors. Then, we investigate\ntask-based sampling allocation aimed at minimizing the mean squared error (MSE)\nof the GFR-ML estimator by wisely choosing sensor placement. Since this MSE\ndepends on the unknown graph signals to be estimated, we propose four cost\nfunctions for the optimization of the sampling allocation: the biased\nCram$\\acute{\\text{e}}$r-Rao bound (bCRB), the worst-case MSE (WC-MSE), the\nBayesian MSE (BMSE), and the worst-case BMSE (WC-BMSE), where the last two\nassume a Gaussian prior. We investigate the properties of these cost functions\nand develop two algorithms for their practical implementation: 1) the\nstraightforward greedy algorithm; and 2) the alternating projection gradient\ndescent (PGD) algorithm that reduces the computational complexity. Simulation\nresults on synthetic and real-world datasets of the IEEE 118-bus power system\nand the Minnesota road network demonstrate that the proposed sampling\nallocation methods reduce the MSE by up to $50\\%$ compared to the common\nsampling methods A-design, E-design, and LR-design in the tested scenarios.\nThus, the proposed methods improve the estimation performance and reduce the\nrequired number of measurements in graph signal processing (GSP)-based signal\nrecovery in the case of underdetermined systems.",
        "Integrated Sensing and Communication (ISAC) is a crucial component of future\nwireless networks, enabling seamless integration of Communication and Sensing\n(C\\&S) functionalities. However, ensuring security in ISAC systems remains a\nsignificant challenge, as both C\\&S data are susceptible to adversarial\nthreats. Physical Layer Security (PLS) has emerged as a key framework for\nmitigating these risks at the transmission level. Reconfigurable Intelligent\nSurfaces (RIS) further enhance PLS by dynamically shaping the radio environment\nto improve both secrecy along with C\\&S performance. This survey begins with an\noverview of RIS, PLS, and ISAC fundamentals, establishing a foundation for\nunderstanding their integration. The state-of-the-art RIS-assisted PLS\napproaches in ISAC systems are then categorized into passive RIS and Active RIS\n(ARIS) paradigms. Passive RIS-based techniques focus on optimizing system\nthroughput, covert communication, and Secrecy Rates (SRs), alongside improving\nsensing Signal-to-Noise Ratio (SNR) and Weighted Sum Rate (WSR) under various\nconstraints. ARIS-based strategies extend these capabilities by actively\noptimizing beamforming to enhance secrecy and covert rates while ensuring\nrobust sensing under communication and security constraints. By reviewing both\npassive and ARIS-based security frameworks, this survey highlights the\ntransformative role of RIS in strengthening ISAC security. Furthermore, it\nexplores key optimization methodologies, technical challenges, and future\nresearch directions for integrating RIS with PLS to ensure secure and efficient\nISAC in next-generation 6G wireless networks.",
        "We investigate joint bistatic positioning (BP) and monostatic sensing (MS)\nwithin a multi-input multi-output orthogonal frequency-division system. Based\non the derived Cram\\'er-Rao Bounds (CRBs), we propose novel beamforming\noptimization strategies that enable flexible performance trade-offs between BP\nand MS. Two distinct objectives are considered in this multi-objective\noptimization problem, namely, enabling user equipment to estimate its own\nposition while accounting for unknown clock bias and orientation, and allowing\nthe base station to locate passive targets. We first analyze digital schemes,\nproposing both weighted-sum CRB and weighted-sum mismatch (of beamformers and\ncovariance matrices) minimization approaches. These are examined under\nfull-dimension beamforming (FDB) and low-complexity codebook-based power\nallocation (CPA). To adapt to low-cost hardwares, we develop unit-amplitude\nanalog FDB and CPA schemes based on the weighted-sum mismatch of the covariance\nmatrices paradigm, solved using distinct methods. Numerical results confirm the\neffectiveness of our designs, highlighting the superiority of minimizing the\nweighted-sum mismatch of covariance matrices, and the advantages of mutual\ninformation fusion between BP and MS.",
        "6G communication systems promise to deliver sensing capabilities by utilizing\nthe orthogonal frequency division multiplexing (OFDM) communication signal for\nsensing. However, the cyclic prefix inherent in OFDM systems limits the sensing\nrange, necessitating compensation techniques to detect small, distant targets\nlike drones. In this paper, we show that state-of-the-art coherent compensation\nmethods fail in scenarios involving multiple targets, resulting in an increased\nnoise floor in the radar image. Our contributions include a novel multi target\ncoherent compensation algorithm and a generalized\nsignal-to-interference-and-noise ratio for multiple targets to evaluate the\nperformance. Our algorithm achieves the same detection performance at long\ndistances requiring only 3.6% of the radio resources compared to classical OFDM\nradar processing. This enables resource efficient sensing at long distances in\nmulti target scenarios with legacy communications-only networks.",
        "Analysis of the association between exposure to electromagnetic fields of\nnon-ionising radiation (EMF-NIR) and health in children and adolescents is\nhindered by the limited availability of data, mainly due to the difficulties on\nthe exposure assessment. This study protocol describes the methodologies used\nfor characterising exposure of children to EMF-NIR in the INMA (INfancia y\nMedio Ambiente- Environment and Childhood) Project, a prospective cohort study.\nIndirect (proximity to emission sources, questionnaires on sources use and\ngeospatial propagation models) and direct methods (spot and fixed longer-term\nmeasurements and personal measurements) were conducted in order to assess\nexposure levels of study participants aged between 7 and 18 years old. The\nmethodology used varies depending on the frequency of the EMF-NIR and the\nenvironment (homes, schools and parks). Questionnaires assessed the use of\nsources contributing both to Extremely Low Frequency (ELF) and Radiofrequency\n(RF) exposure levels. Geospatial propagation models (NISMap) are implemented\nand validated for environmental outdoor sources of RFs using spot measurements.\nSpot and fixed longer-term ELF and RF measurements were done in the\nenvironments where children spend most of the time. Moreover, personal\nmeasurements were taken in order to assess individual exposure to RF. The\nexposure data are used to explore their relationships with proximity and\/or use\nof EMF-NIR sources.",
        "This paper investigates deteriorations in knee and ankle dynamics during\nrunning. Changes in lower limb accelerations are analyzed by a wearable\nmusculo-skeletal monitoring system. The system employs a machine learning\ntechnique to classify joint stiffness. A maximum-entropyrate method is\ndeveloped to select the most relevant features. Experimental results\ndemonstrate that distance travelled and energy expended can be estimated from\nobserved changes in knee and ankle motions during 5 km runs.",
        "Integrated Communication and Sensing (ICAS) is a key technology that enables\nsensing functionalities within the next-generation mobile communication (6G).\nJoint design and optimization of both functionalities could allow coexistence,\ntherefore it advances toward joint signal processing and using the same\nhardware platform and common spectrum. Contributing to ICAS sensing, this paper\npresents the measurement and analysis of the micro-Doppler signature of\nVertical Takeoff and Landing (VTOL) drones. Measurement is performed with an\nOFDM-like communication signal and bistatic constellation, which is a typical\ncase in ICAS scenarios. This work shows that micro-Doppler signatures can be\nused to precisely distinguish flight modes, such as take-off, landing,\nhovering, transition, and cruising.",
        "We obtain the full set of tidal Love numbers of non-rotating black holes in\nan effective field theory extension of general relativity. We achieve our\nresults using a recently introduced modified Teukolsky equation that describes\nthe perturbations of black holes in this theory. We show how to identify the\nLove numbers and their beta functions in a systematic and gauge invariant way,\napplying analytic continuation on the angular number $\\ell$ when necessary. We\nobserve that there are three types of Love numbers: electric, magnetic, and a\n``mixing'' type, associated to parity-breaking theories, that we identify here\nfor the first time. The modified Teukolsky equation proves to be very useful as\nit allows us to obtain all the different Love numbers in a unified framework.\nWe compare our results with previous literature that utilized the\nRegge-Wheeler-Zerilli equations to compute Love numbers, finding perfect\nagreement. The method introduced here paves the way towards the computation of\nLove numbers of rotating black holes beyond general relativity.",
        "Be stars are rapidly rotating, with angular frequency around $0.7-0.8$ of\ntheir Keplerian break up frequency, as a result of significant accretion during\nthe earlier stellar evolution of a companion star. Material from the equator of\nthe Be star is ejected and forms a decretion disc, although the mechanism for\nthe disc formation has remained elusive. We find one-dimensional steady state\ndecretion disc solutions that smoothly transition from a rapidly rotating star\nthat is in hydrostatic balance. Boundary layer effects in a geometrically thick\ndisc which connects to a rotationally flattened star enable the formation of a\ndecretion disc at stellar spin rates below the break up rate. For a disc with\nan aspect ratio $H\/R\\approx 0.1$ at the inner edge, the torque from the disc on\nthe star slows the stellar spin to the observed range and mass ejection\ncontinues at a rate consistent with observed decretion rates. The critical\nrotation rate, to which the star slows down to, decreases as the disc aspect\nratio increases. More generally, steady state accretion and decretion disc\nsolutions can be found for all stellar spin rates. The outcome for a particular\nsystem depends upon the balance between the decretion rate and any external\ninfall accretion rate.",
        "In a recent article [13], G. Janelidze introduced the concept of ideally\nexact categories as a generalization of semi-abelian categories, aiming to\nincorporate relevant examples of non-pointed categories, such as the categories\n$\\textbf{Ring}$ and $\\textbf{CRing}$ of unitary (commutative) rings. He also\nextended the notion of action representability to this broader framework,\nproving that both $\\textbf{Ring}$ and $\\textbf{CRing}$ are action\nrepresentable.\n  This article investigates the representability of actions of unitary\nnon-associative algebras. After providing a detailed description of the monadic\nadjunction associated with any category of unitary algebra, we use the\nconstruction of the external weak actor [4] in order to prove that the\ncategories of unitary (commutative) associative algebras and that of unitary\nalternative algebras are action representable. The result is then extended for\nunitary (commutative) Poisson algebras, where the explicit construction of the\nuniversal strict general actor is employed.",
        "This study analyzes the impact of the COVID-19 pandemic on currency\ncirculation in Brazil by comparing actual data from 2000 to 2023 with\ncounterfactual projections using the\n\\textbf{SARIMA(3,1,1)(3,1,4)\\textsubscript{12}} model. The model was selected\nbased on an extensive parameter search, balancing accuracy and simplicity, and\nvalidated through the metrics MAPE, RMSE, and AIC. The results indicate a\nsignificant deviation between projected and observed values, with an average\ndifference of R\\$ 47.57 billion (13.95\\%). This suggests that the pandemic,\nemergency policies, and the introduction of \\textit{Pix} had a substantial\nimpact on currency circulation. The robustness of the SARIMA model was\nconfirmed, effectively capturing historical trends and seasonality, though\nfindings emphasize the importance of considering exogenous variables, such as\ninterest rates and macroeconomic policies, in future analyses. Future research\nshould explore multivariate models incorporating economic indicators, long-term\nanalysis of post-pandemic currency circulation trends, and studies on public\ncash-holding behavior. The results reinforce the need for continuous monitoring\nand econometric modeling to support decision-making in uncertain economic\ncontexts.",
        "In this manuscript, we consider the problem of learning a flow or\ndiffusion-based generative model parametrized by a two-layer auto-encoder,\ntrained with online stochastic gradient descent, on a high-dimensional target\ndensity with an underlying low-dimensional manifold structure. We derive a\ntight asymptotic characterization of low-dimensional projections of the\ndistribution of samples generated by the learned model, ascertaining in\nparticular its dependence on the number of training samples. Building on this\nanalysis, we discuss how mode collapse can arise, and lead to model collapse\nwhen the generative model is re-trained on generated synthetic data.",
        "Lattice thermal conductivity ($\\kappa_L$) is crucial for efficient thermal\nmanagement in electronics and energy conversion technologies. Traditional\nmethods for predicting \\k{appa}L are often computationally expensive, limiting\ntheir scalability for large-scale material screening. Empirical models, such as\nthe Slack model, offer faster alternatives but require time-consuming\ncalculations for key parameters such as sound velocity and the Gruneisen\nparameter. This work presents a high-throughput framework, physical-informed\nkappa (PINK), which combines the predictive power of crystal graph\nconvolutional neural networks (CGCNNs) with the physical interpretability of\nthe Slack model to predict \\k{appa}L directly from crystallographic information\nfiles (CIFs). Unlike previous approaches, PINK enables rapid, batch predictions\nby extracting material properties such as bulk and shear modulus from CIFs\nusing a well-trained CGCNN model. These properties are then used to compute the\nnecessary parameters for $\\kappa_L$ calculation through a simplified physical\nformula. PINK was applied to a dataset of 377,221 stable materials, enabling\nthe efficient identification of promising candidates with ultralow $\\kappa_L$\nvalues, such as Ag$_3$Te$_4$W and Ag$_3$Te$_4$Ta. The platform, accessible via\na user-friendly interface, offers an unprecedented combination of speed,\naccuracy, and scalability, significantly accelerating material discovery for\nthermal management and energy conversion applications.",
        "We perform a systematical investigation of the doubly charmed dibaryon system\nwith quantum numbers $IJ=01$, and strangeness numbers $S=0$, $-2$ and $-4$ in\nthe framework of the chiral quark model. Two resonance states with strangeness\nnumbers $S=-2$ is obtained in the $\\Lambda\\Omega_{cc}$ scattering channel,\nwhich are $\\Xi_{cc}^{\\ast}\\Xi$ with resonance mass 5081 MeV and decay width 0.3\nMeV, and the $\\Xi_{c}\\Xi_{c}^{\\ast}$ state with the mass 5213 MeV and decay\nwidth 19.8 MeV, respectively. These two predicted charmed dibaryon candidates\nare worth searching for experimentally. Besides, we would like to emphasize\nthat the multi-channel coupling calculation is important to confirm the\nexistence of multiquark states. The coupling can shift the energy of the\nresonance, give the width to the resonance and even destroy the resonance.\nTherefore, to provide the necessary information for experiments to search for\nexotic hadron states, the coupling calculation between the bound channels and\nopen channels is indispensable.",
        "Quantum nanophotonics merges the precision of nanoscale light manipulation\nwith the capabilities of quantum technologies, offering a pathway for enhanced\nlight-matter interaction and compact realization of quantum devices. Here, we\nshow how a recently-demonstrated nonlinear nanophotonic process can be employed\nto selectively create photonic high-dimensional quantum states (qudits). We\nutilize the nonlinearity on the surface of the nanophotonic device to dress,\nthrough the polarization of the pump field, the near-field modes carrying\nangular momentum and their superpositions. We then use this approach for the\nrealization of a multilevel quantum key distribution protocol, which doubles\nthe key rate compared to standard schemes. This idea is an important step\ntowards experimental realizations of quantum state generation and manipulation\nthrough nonlinearity within nanophotonic platforms, and enables new\ncapabilities for on-chip quantum devices.",
        "We study the problem of training neural stochastic differential equations, or\ndiffusion models, to sample from a Boltzmann distribution without access to\ntarget samples. Existing methods for training such models enforce time-reversal\nof the generative and noising processes, using either differentiable simulation\nor off-policy reinforcement learning (RL). We prove equivalences between\nfamilies of objectives in the limit of infinitesimal discretization steps,\nlinking entropic RL methods (GFlowNets) with continuous-time objects (partial\ndifferential equations and path space measures). We further show that an\nappropriate choice of coarse time discretization during training allows greatly\nimproved sample efficiency and the use of time-local objectives, achieving\ncompetitive performance on standard sampling benchmarks with reduced\ncomputational cost.",
        "This paper is concerned with stochastic linear quadratic (LQ, for short)\noptimal control problems in an infinite horizon with conditional mean-field\nterm in a switching regime environment. The orthogonal decomposition introduced\nin [21] has been adopted. Desired algebraic Riccati equations (AREs, for short)\nand a system of backward stochastic differential equations (BSDEs, for short)\nin infinite time horizon with the coefficients depending on the Markov chain\nhave been derived. The determination of closed-loop optimal strategy follows\nfrom the solvability of ARE and BSDE. Moreover, the solvability of BSDEs leads\nto a characterization of open-loop solvability of the optimal control problem.",
        "The averaged distance structure of one-dimensional regular model sets is\ndetermined via their pair correlation functions. The latter lead to\ncovariograms and cross covariograms of the windows, which give continuous\nfunctions in internal space. While they are simple tent-shaped, piecewise\nlinear functions for intervals, the typical case for inflation systems leads to\nconvolutions of Rauzy fractals, which are difficult to compute. In the presence\nof an inflation structure, an alternative path is possible via the exact\nrenormalisation structures of the pair correlation functions. We introduce this\napproach and derive two concrete examples, which display an unexpectedly\ncomplex and wild behaviour.",
        "Binary stars consisting of a white dwarf and a main sequence star (WDMS) are\nvaluable for studying key astrophysical questions. However, observational\nbiases strongly affect the known population, particularly unresolved systems\nwhere the main sequence star outshines the white dwarf. This work aims to\ncomprehensively simulate the population of unresolved WDMS binaries within 100\npc of the Sun and to compare the outcome with the currently most complete\nvolume-limited sample available from Gaia data. We employ a population\nsynthesis code, MRBIN, extensively developed by our group and based on Monte\nCarlo techniques, which uses a standard binary stellar evolutionary code\nadapted to cover a wide range of stars across all ages, masses, and\nmetallicities. Selection criteria matching those of Gaia observations are\napplied to generate synthetic populations comparable to the observed WDMS\nsample. The synthetic data accurately populate the expected regions in the Gaia\ncolor-magnitude diagram. However, simulations predict a lower number of\nextremely low-mass white dwarfs, suggesting potential issues in observed mass\nderivations. Additionally, our analysis constrains the common envelope\nefficiency to 0.1-0.4, consistent with previous findings, and estimates a total\ncompleteness of about 25% for the observed sample, confirming the strong\nobservational limitations for unresolved WDMS.",
        "This paper investigates an intelligent reflective surface (IRS) assisted\nsecure multi-user two-way communication system. The aim of this paper is to\nenhance the physical layer security by optimizing the minimum secrecy-rate\namong all user-pairs in the presence of a malicious user. The optimization\nproblem is converted into an alternating optimization problem consisting of two\nsub-problems. Transmit power optimization is handled using a fractional\nprogramming method, whereas IRS phase shift optimization is handled with\nsemi-definite programming. The convergence of the proposed algorithm is\ninvestigated numerically. The performance gain in minimum secrecy-rate is\nquantified for four different user configurations in comparison to the baseline\nscheme. Results indicate a 3.6-fold gain in minimum secrecy rate over the\nbaseline scheme when the IRS is positioned near a legitimate user, even when\nthe malicious user is located close to the same legitimate user.",
        "Problem definition: Drones, despite being acknowledged as a transformative\nforce in the city logistics sector, are unable to execute the\n\\textit{last-meter delivery} (unloading goods directly to customers' doorsteps)\ndue to airspace restrictions and safety concerns. To leverage advancements and\novercome the limitations of drones in providing intra-city express services, we\nintroduce a coordinated drone-courier logistics system where drones operate\nwithin a closed network among vertiports, while couriers connect customers to\nthe drone delivery system. This paper aims to shed light on this coordinated\nsystem in terms of system feasibility, network interactivity, and long-term\nsustainability. Methodology\/Results: We develop an integrated optimization\nmodel to optimize the network planning of the coordinated logistics system. The\ninterplay between network planning and tactical operations is mirrored by a\nqueueing network model, resulting in the nonlinear and nonconvex (partially\nconvex and partially concave) feasible region of the optimization model. An\niterative exact algorithm that tightens lower and upper bounds by adaptively\nrefining the linear approximations of nonlinear constraints is developed to\nprovide optimality-guaranteed solutions with finite convergence. The\ncomputational experiments demonstrate the scalability and robustness of our\nalgorithm across various network configurations and scenarios.Managerial\nimplications: The case study, based on a real-world dataset from SF Express, a\nlogistics giant in China, validates that the coordinated logistics system\nefficiently attains cost and time savings by leveraging the effective turnover\nof drones and the coordination between drones and couriers. The optimal network\ndesign features a concentrated structure, streamlining demand consolidation and\nreducing deadhead repositioning.",
        "Here we present extinction, extirpation and coexistence conditions where \/\nwhen two communities combine. We consider one specific model where two\ncommunities coalesce, and another model where the communities coexist side by\nside, blending in a transitionary zone called the ecotone. Specifically, (1) we\nanalytically calculate the shifts in abundances as a function of mixing\nstrength. (2) Obtain a critical value for the mixing strength leading to\nextinction. (3) Derive an inequality condition for full coexistent mixing. (4)\nfind how the individual communities penetrate into one other as a function of\nmixing strength. (5) derive the conditions for one species to cross the ecotone\nand invade an neighboring community and (6) conditions for a native species to\nget extirpated. Lastly, (7) we spatially investigate the species richness\nwithin the ecotone and derive a condition that determines whether the ecotone\nwill have higher or lower richness compared to its surrounding habitats."
      ]
    }
  },
  {
    "id":2411.10403,
    "research_type":"applied",
    "start_id":"b14",
    "start_title":"Sparse MRI: The application of compressed sensing for rapid MR imaging",
    "start_abstract":"Abstract The sparsity which is implicit in MR images exploited to significantly undersample k \u2010space. Some such as angiograms are already sparse the pixel representation; other, more complicated have a representation some transform domain\u2013for example, terms of spatial finite\u2010differences or their wavelet coefficients. According recently developed mathematical theory compressed\u2010sensing, with can be recovered from randomly undersampled \u2010space data, provided an appropriate nonlinear recovery scheme used. Intuitively, artifacts due random undersampling add noise\u2010like interference. In domain significant coefficients stand out above A thresholding recover coefficients, effectively recovering image itself. this article, practical incoherent schemes and analyzed by means aliasing Incoherence introduced pseudo\u2010random variable\u2010density phase\u2010encodes. reconstruction performed minimizing \u2113 1 norm transformed image, subject data fidelity constraints. Examples demonstrate improved resolution accelerated acquisition for multislice fast spin\u2010echo brain imaging 3D contrast enhanced angiography. Magn Reson Med, 2007. \u00a9 2007 Wiley\u2010Liss, Inc.",
    "start_categories":[
      "cs.LG",
      "stat.CO"
    ],
    "start_fields":[
      "Mathematics and Statistics",
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b13"
      ],
      "title":[
        "Cardiovascular Flow Measurement with Phase-Contrast MR Imaging: Basic Facts and Implementation"
      ],
      "abstract":[
        "Phase-contrast magnetic resonance (MR) imaging is a well-known but undervalued method of obtaining quantitative information on blood flow. Applications this technique in cardiovascular MR are expanding. According to the sequences available, phase-contrast measurement can be performed breath hold or during normal respiration. Prospective as well retrospective gating techniques used. Common errors include mismatched encoding velocity, deviation plane, inadequate temporal resolution, spatial accelerated flow and misregistration, phase offset errors. Flow measurements most precise if plane perpendicular vessel interest set through-plane The sequence should repeated at least once, with high velocity used initially. If peak has estimated, an adapted velocity. overall error comprises prescription that occur image analysis data. With imaging, reduced less than 10%, acceptable level for routine clinical use."
      ],
      "categories":[
        "q-bio.QM"
      ]
    },
    "list":{
      "title":[
        "UNGT: Ultrasound Nasogastric Tube Dataset for Medical Image Analysis",
        "Heuristics based on Adjacency Graph Packing for DCJ Distance Considering\n  Intergenic Regions",
        "Efficient Spatial Estimation of Perceptual Thresholds for Retinal\n  Implants via Gaussian Process Regression",
        "IA generativa aplicada a la detecci\\'on del c\\'ancer a trav\\'es de\n  Resonancia Magn\\'etica",
        "Design of an Automated Ethanol Vapor Generating System for Alcohol Use\n  Disorder(AUD) Animal Studies",
        "A Systematic Computational Framework for Practical Identifiability\n  Analysis in Mathematical Models Arising from Biology",
        "Solar radiation and atmospheric CO$_2$ predict young leaf production in\n  a moist evergreen tropical forest: Insights from 23 years",
        "Reconstructing Noisy Gene Regulation Dynamics Using\n  Extrinsic-Noise-Driven Neural Stochastic Differential Equations",
        "Collective inference of the truth of propositions from crowd probability\n  judgments",
        "Unsupervised detection and fitness estimation of emerging SARS-CoV-2\n  variants. Application to wastewater samples (ANRS0160)",
        "Measuring Fitness and Importance of Species in Food Webs",
        "Leveraging Retrieval-Augmented Generation and Large Language Models to\n  Predict SERCA-Binding Protein Fragments from Cardiac Proteomics Data",
        "Beware of so-called 'good' correlations: a statistical reality check on\n  individual mRNA-protein predictions",
        "Evaluating Answer Reranking Strategies in Time-sensitive Question\n  Answering",
        "On the Potential Galactic Origin of the Ultra-High-Energy Event\n  KM3-230213A",
        "Plantation Monitoring Using Drone Images: A Dataset and Performance\n  Review",
        "ML-Based Optimum Number of CUDA Streams for the GPU Implementation of\n  the Tridiagonal Partition Method",
        "Exact Decoding of Repetition Code under Circuit Level Noise",
        "Audio-to-Image Encoding for Improved Voice Characteristic Detection\n  Using Deep Convolutional Neural Networks",
        "Context-Enhanced Memory-Refined Transformer for Online Action Detection",
        "AI-assisted design of experiments at the frontiers of computation:\n  methods and new perspectives",
        "BioMamba: Leveraging Spectro-Temporal Embedding in Bidirectional Mamba\n  for Enhanced Biosignal Classification",
        "CDI3D: Cross-guided Dense-view Interpolation for 3D Reconstruction",
        "Quantum teleportation between simulated binary black holes",
        "EDGE: Efficient Data Selection for LLM Agents via Guideline\n  Effectiveness",
        "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay\n  Scoring Capabilities of Multimodal Large Language Models",
        "Definable Obstruction Theory",
        "Towards more accurate $B_{(s)}\\rightarrow\\pi(K)$ and\n  $D_{(s)}\\rightarrow\\pi(K)$ form factors"
      ],
      "abstract":[
        "We develop a novel ultrasound nasogastric tube (UNGT) dataset to address the\nlack of public nasogastric tube datasets. The UNGT dataset includes 493 images\ngathered from 110 patients with an average image resolution of approximately\n879 $\\times$ 583. Four structures, encompassing the liver, stomach, tube, and\npancreas are precisely annotated. Besides, we propose a semi-supervised\nadaptive-weighting aggregation medical segmenter to address data limitation and\nimbalance concurrently. The introduced adaptive weighting approach tackles the\nsevere unbalanced challenge by regulating the loss across varying categories as\ntraining proceeds. The presented multiscale attention aggregation block\nbolsters the feature representation by integrating local and global contextual\ninformation. With these, the proposed AAMS can emphasize sparse or small\nstructures and feature enhanced representation ability. We perform extensive\nsegmentation experiments on our UNGT dataset, and the results show that AAMS\noutperforms existing state-of-the-art approaches to varying extents. In\naddition, we conduct comprehensive classification experiments across varying\nstate-of-the-art methods and compare their performance. The dataset and code\nwill be available upon publication at https:\/\/github.com\/NUS-Tim\/UNGT.",
        "In this work, we explore heuristics for the Adjacency Graph Packing problem,\nwhich can be applied to the Double Cut and Join (DCJ) Distance Problem. The DCJ\nis a rearrangement operation and the distance problem considering it is a well\nestablished method for genome comparison. Our heuristics will use the structure\ncalled adjacency graph adapted to include information about intergenic regions,\nmultiple copies of genes in the genomes, and multiple circular or linear\nchromosomes. The only required property from the genomes is that it must be\npossible to turn one into the other with DCJ operations. We propose one greedy\nheuristic and one heuristic based on Genetic Algorithms. Our experimental tests\nin artificial genomes show that the use of heuristics is capable of finding\ngood results that are superior to a simpler random strategy.",
        "Retinal prostheses restore vision by electrically stimulating surviving\nneurons, but calibrating perceptual thresholds - the minimum stimulus intensity\nrequired for perception - remains a time-intensive challenge, especially for\nhigh-electrode-count devices. Since neighboring electrodes exhibit spatial\ncorrelations, we propose a Gaussian Process Regression (GPR) framework to\npredict thresholds at unsampled locations while leveraging uncertainty\nestimates to guide adaptive sampling. Using perceptual threshold data from four\nArgus II users, we show that GPR with a Mat\\'ern kernel provides more accurate\nthreshold predictions than a Radial Basis Function (RBF) kernel (p < .001,\nWilcoxon signed-rank test). In addition, spatially optimized sampling yielded\nlower prediction error than uniform random sampling for Participants 1 and 3 (p\n< .05). While adaptive sampling dynamically selects electrodes based on model\nuncertainty, its accuracy gains over spatial sampling were not statistically\nsignificant (p > .05), though it approached significance for Participant 1 (p =\n.074). These findings establish GPR with spatial sampling as a scalable,\nefficient approach to retinal prosthesis calibration, minimizing patient burden\nwhile maintaining predictive accuracy. More broadly, this framework offers a\ngeneralizable solution for adaptive calibration in neuroprosthetic devices with\nspatially structured stimulation thresholds.",
        "Cognitive delegation to artificial intelligence (AI) systems is transforming\nscientific research by enabling the automation of analytical processes and the\ndiscovery of new patterns in large datasets. This study examines the ability of\nAI to complement and expand knowledge in the analysis of breast cancer using\ndynamic contrast-enhanced magnetic resonance imaging (DCE-MRI). Building on a\nprevious study, we assess the extent to which AI can generate novel approaches\nand successfully solve them. For this purpose, AI models, specifically\nChatGPT-4o, were used for data preprocessing, hypothesis generation, and the\napplication of clustering techniques, predictive modeling, and correlation\nnetwork analysis. The results obtained were compared with manually computed\noutcomes, revealing limitations in process transparency and the accuracy of\ncertain calculations. However, as AI reduces errors and improves reasoning\ncapabilities, an important question arises regarding the future of scientific\nresearch: could automation replace the human role in science? This study seeks\nto open the debate on the methodological and ethical implications of a science\ndominated by artificial intelligence.",
        "Alcohol Use Disorder (AUD) is a prevalent addictive disorder affecting an\nestimated 29.5 million Americans. It is characterized by impaired control over\nalcohol consumption despite negative consequences. The number of diagnostic\ncriteria met by an individual typically determines the severity of AUD.\nResearch into AUD focuses on understanding individual susceptibility\ndifferences and developing preventive strategies. Alcohol vapor inhalation has\nemerged as a promising method for pathophysiological investigations in animals,\nallowing researchers to control the dose and duration of alcohol exposure. This\napproach is crucial for studying the escalation of voluntary alcohol-drinking\nbehavior. Current commercial systems for alcohol vapor generation have\nlimitations, including combustion risks and the need to adjust multiple\nparameters. Other methods, like bubbling or blow-over evaporation, face\nchallenges in maintaining equilibrium and avoiding aerosolization. To address\nthese issues, a new type of ethanol vapor generating system is proposed that\nrelies solely on temperature control, creating a vacuum into which ethanol\nevaporates under thermodynamic control. This approach eliminates the need to\nadjust multiple parameters and offers improved accuracy and precision in vapor\ndose delivery. We validated the system as anticipated, achieving stable ethanol\nvapor after a few priming cycles. Using a 1.2 L cylinder, we obtained\napproximately 3.6 L of saturated vapor\/air mix in 1 minute. Gravimetric results\nshowed that each cycle produced about 100 mg\/L or ~10,000 ppm vapor-to-air\nmixture. The intended use of the ethanol vapor generator is to provide a\nconcentrated ethanol vapor \/ air mixture to be further diluted before\ndelivering to the animals.",
        "Practical identifiability is a critical concern in data-driven modeling of\nmathematical systems. In this paper, we propose a novel framework for practical\nidentifiability analysis to evaluate parameter identifiability in mathematical\nmodels of biological systems. Starting with a rigorous mathematical definition\nof practical identifiability, we demonstrate its equivalence to the\ninvertibility of the Fisher Information Matrix. Our framework establishes the\nrelationship between practical identifiability and coordinate identifiability,\nintroducing a novel metric that simplifies and accelerates the evaluation of\nparameter identifiability compared to the profile likelihood method.\nAdditionally, we introduce new regularization terms to address non-identifiable\nparameters, enabling uncertainty quantification and improving model\nreliability. To guide experimental design, we present an optimal data\ncollection algorithm that ensures all model parameters are practically\nidentifiable. Applications to Hill functions, neural networks, and dynamic\nbiological models demonstrate the feasibility and efficiency of the proposed\ncomputational framework in uncovering critical biological processes and\nidentifying key observable variables.",
        "Climate change impacts ecosystems worldwide, affecting animal behaviour and\nsurvival both directly and indirectly through changes such as the availability\nof food. For animals reliant on leaves as a primary food source, understanding\nhow climate change influences leaf production of trees is crucial, yet this is\nunderstudied, especially in moist evergreen tropical forests. We analyzed a\n23-year dataset of young leaf phenology from a moist tropical forest in Kibale\nNational Park, Uganda, to examine seasonal and long-term patterns of 12 key\ntree species consumed by folivorous primates. We described phenological\npatterns and explored relationships between young leaf production of different\ntree species and climate variables. We also assessed the suitability of the\nEnhanced Vegetation Index (EVI) as a proxy for young leaf production in moist\nevergreen tropical forests. Our results showed that tree species exhibited\ndistinct phenological patterns, with most species producing young leaves during\ntwo seasonal peaks aligned with the rainy seasons. Rainfall, cloud cover, and\nmaximum temperature were the most informative predictors of seasonal variation\nin young leaf production. However, solar radiation and atmospheric CO$_2$ were\nmost informative regarding long-term trends. EVI was strongly correlated with\nyoung leaf production within years but less effective for capturing\ninter-annual trends. These findings highlight the complex relationship between\nclimate and young leaf phenology in moist evergreen tropical forests, and helps\nus understand the changes in food availability for tropical folivores.",
        "Proper regulation of cell signaling and gene expression is crucial for\nmaintaining cellular function, development, and adaptation to environmental\nchanges. Reaction dynamics in cell populations is often noisy because of (i)\ninherent stochasticity of intracellular biochemical reactions (``intrinsic\nnoise'') and (ii) heterogeneity of cellular states across different cells that\nare influenced by external factors (``extrinsic noise''). In this work, we\nintroduce an extrinsic-noise-driven neural stochastic differential equation\n(END-nSDE) framework that utilizes the Wasserstein distance to accurately\nreconstruct SDEs from trajectory data from a heterogeneous population of cells\n(extrinsic noise). We demonstrate the effectiveness of our approach using both\nsimulated and experimental data from three different systems in cell biology:\n(i) circadian rhythms, (ii) RPA-DNA binding dynamics, and (iii) NF$\\kappa$B\nsignaling process. Our END-nSDE reconstruction method can model how cellular\nheterogeneity (extrinsic noise) modulates reaction dynamics in the presence of\nintrinsic noise. It also outperforms existing time-series analysis methods such\nas recurrent neural networks (RNNs) and long short-term memory networks\n(LSTMs). By inferring cellular heterogeneities from data, our END-nSDE\nreconstruction method can reproduce noisy dynamics observed in experiments. In\nsummary, the reconstruction method we propose offers a useful surrogate\nmodeling approach for complex biophysical processes, where high-fidelity\nmechanistic models may be impractical.",
        "Every day, we judge the probability of propositions. When we communicate\ngraded confidence (e.g. \"I am 90% sure\"), we enable others to gauge how much\nweight to attach to our judgment. Ideally, people should share their judgments\nto reach more accurate conclusions collectively. Peer-to-peer tools for\ncollective inference could help debunk disinformation and amplify reliable\ninformation on social networks, improving democratic discourse. However,\nindividuals fall short of the ideal of well-calibrated probability judgments,\nand group dynamics can amplify errors and polarize opinions. Here, we connect\ninsights from cognitive science, structured expert judgment, and crowdsourcing\nto infer the truth of propositions from human probability judgments. In an\nonline experiment, 376 participants judged the probability of each of 1,200\ngeneral-knowledge claims for which we have ground truth (451,200 ratings).\nAggregating binary judgments by majority vote already exhibits the \"wisdom of\nthe crowd\"--the superior accuracy of collective inferences relative to\nindividual inferences. However, using continuous probability ratings and\naccounting for individual accuracy and calibration significantly improves\ncollective inferences. Peer judgment behavior can be modeled probabilistically,\nand individual parameters capturing each peer's accuracy and miscalibration can\nbe inferred jointly with the claim probabilities. This unsupervised approach\ncan be complemented by supervised methods relying on truth labels to learn\nmodels that achieve well-calibrated collective inference. The algorithms we\nintroduce can empower groups of collaborators and online communities to pool\ntheir distributed intelligence and jointly judge the probability of\npropositions with a well-calibrated sense of uncertainty.",
        "Repeated waves of emerging variants during the SARS-CoV-2 pandemics have\nhighlighted the urge of collecting longitudinal genomic data and developing\nstatistical methods based on time series analyses for detecting new threatening\nlineages and estimating their fitness early in time. Most models study the\nevolution of the prevalence of particular lineages over time and require a\nprior classification of sequences into lineages. Such process is prone to\ninduce delays and bias. More recently, few authors studied the evolution of the\nprevalence of mutations over time with alternative clustering approaches,\navoiding specific lineage classification. Most of the aforementioned methods\nare however either non parametric or unsuited to pooled data characterizing,\nfor instance, wastewater samples. In this context, we propose an alternative\nunsupervised method for clustering mutations according to their frequency\ntrajectory over time and estimating group fitness from time series of pooled\nmutation prevalence data. Our model is a mixture of observed count data and\nlatent group assignment and we use the expectation-maximization algorithm for\nmodel selection and parameter estimation. The application of our method to time\nseries of SARS-CoV-2 sequencing data collected from wastewater treatment plants\nin France from October 2020 to April 2021 shows its ability to agnostically\ngroup mutations according to their probability of belonging to B.1.160, Alpha,\nBeta, B.1.177 variants with selection coefficient estimates per group in\ncoherence with the viral dynamics in France reported by Nextstrain. Moreover,\nour method detected the Alpha variant as threatening as early as supervised\nmethods (which track specific mutations over time) with the noticeable\ndifference that, since unsupervised, it does not require any prior information\non the set of mutations.",
        "Ecosystems face intensifying threats from climate change, overexploitation,\nand other human pressures, emphasizing the urgent need to identify keystone\nspecies and vulnerable ones. While established network-based measures often\nrely on a single metric to quantify a species' relevance, they overlook how\norganisms can be both carbon providers and consumers, thus playing a dual role\nin food webs. Here, we introduce a novel approach that assigns each species two\ncomplementary scores -- an importance measure quantifying their centrality as\ncarbon source and a fitness measure capturing their vulnerability. We show that\nspecies with high importance are more likely to trigger co-extinctions upon\nremoval, while high-fitness species typically endure until later stages of\ncollapse, in line with their broader prey ranges. On the other hand, low\nfitness species are the most vulnerable and susceptible to extinctions. Tested\non multiple food webs, our method outperforms traditional degree-based analyses\nand competes effectively with eigenvector-based approaches, while also\nproviding additional insights. Relying solely on interaction data, the approach\nis scalable and avoids reliance on expert-driven classifications, offering a\ncost-effective tool for prioritizing conservation efforts.",
        "Large language models (LLMs) have shown promise in various natural language\nprocessing tasks, including their application to proteomics data to classify\nprotein fragments. In this study, we curated a limited mass spectrometry\ndataset with 1000s of protein fragments, consisting of proteins that appear to\nbe attached to the endoplasmic reticulum in cardiac cells, of which a fraction\nwas cloned and characterized for their impact on SERCA, an ER calcium pump.\nWith this limited dataset, we sought to determine whether LLMs could correctly\npredict whether a new protein fragment could bind SERCA, based only on its\nsequence and a few biophysical characteristics, such as hydrophobicity,\ndetermined from that sequence. To do so, we generated random sequences based on\ncloned fragments, embedded the fragments into a retrieval augmented generation\n(RAG) database to group them by similarity, then fine-tuned large language\nmodel (LLM) prompts to predict whether a novel sequence could bind SERCA. We\nbenchmarked this approach using multiple open-source LLMs, namely the\nMeta\/llama series, and embedding functions commonly available on the\nHuggingface repository. We then assessed the generalizability of this approach\nin classifying novel protein fragments from mass spectrometry that were not\ninitially cloned for functional characterization. By further tuning the prompt\nto account for motifs, such as ER retention sequences, we improved the\nclassification accuracy by and identified several proteins predicted to\nlocalize to the endoplasmic reticulum and bind SERCA, including Ribosomal\nProtein L2 and selenoprotein S. Although our results were based on proteomics\ndata from cardiac cells, our approach demonstrates the potential of LLMs in\nidentifying novel protein interactions and functions with very limited\nproteomic data.",
        "Research in the life sciences often employs messenger ribonucleic acids\n(mRNA) quantification as a standalone approach for functional analysis.\nHowever, although the correlation between the measured levels of mRNA and\nproteins is positive, correlation coefficients observed empirically are\nincomplete, necessitating caution in making agnostic inferences. This essay\nprovides a statistical reflection and caveat on the concept of correlation\nstrength in the context of transcriptomics-proteomics studies. It highlights\nthe variability in possible protein levels at given empirical correlation\nvalues, even for precise mRNA amount, and underscores the notable proportion of\nmRNA-protein pairs with abundances at opposite ends of their respective\ndistributions. Cell biologists, data scientists, and biostatisticians should\nrecognise that mRNA-protein correlation alone is insufficient to justify using\na single mRNA quantification to infer the amount or function of its\ncorresponding protein.",
        "Despite advancements in state-of-the-art models and information retrieval\ntechniques, current systems still struggle to handle temporal information and\nto correctly answer detailed questions about past events. In this paper, we\ninvestigate the impact of temporal characteristics of answers in Question\nAnswering (QA) by exploring several simple answer selection techniques. Our\nfindings emphasize the role of temporal features in selecting the most relevant\nanswers from diachronic document collections and highlight differences between\nexplicit and implicit temporal questions.",
        "The KM3NeT observatory detected the most energetic neutrino candidate ever\nobserved, with an energy between 72 PeV and 2.6 EeV at the 90% confidence\nlevel. The observed neutrino is likely of cosmic origin. In this article, it is\ninvestigated if the neutrino could have been produced within the Milky Way.\nConsidering the low fluxes of the Galactic diffuse emission at these energies,\nthe lack of a nearby potential Galactic particle accelerator in the direction\nof the event and the difficulty to accelerate particles to such high energies\nin Galactic systems, we conclude that if the event is indeed cosmic, it is most\nlikely of extragalactic origin.",
        "Automatic monitoring of tree plantations plays a crucial role in agriculture.\nFlawless monitoring of tree health helps farmers make informed decisions\nregarding their management by taking appropriate action. Use of drone images\nfor automatic plantation monitoring can enhance the accuracy of the monitoring\nprocess, while still being affordable to small farmers in developing countries\nsuch as India. Small, low cost drones equipped with an RGB camera can capture\nhigh-resolution images of agricultural fields, allowing for detailed analysis\nof the well-being of the plantations. Existing methods of automated plantation\nmonitoring are mostly based on satellite images, which are difficult to get for\nthe farmers. We propose an automated system for plantation health monitoring\nusing drone images, which are becoming easier to get for the farmers. We\npropose a dataset of images of trees with three categories: ``Good health\",\n``Stunted\", and ``Dead\". We annotate the dataset using CVAT annotation tool,\nfor use in research purposes. We experiment with different well-known CNN\nmodels to observe their performance on the proposed dataset. The initial low\naccuracy levels show the complexity of the proposed dataset. Further, our study\nrevealed that, depth-wise convolution operation embedded in a deep CNN model,\ncan enhance the performance of the model on drone dataset. Further, we apply\nstate-of-the-art object detection models to identify individual trees to better\nmonitor them automatically.",
        "This paper presents a heuristic for finding the optimum number of CUDA\nstreams by using tools common to the modern AI-oriented approaches and applied\nto the parallel partition algorithm. A time complexity model for the GPU\nrealization of the partition method is built. Further, a refined time\ncomplexity model for the partition algorithm being executed on multiple CUDA\nstreams is formulated. Computational experiments for different SLAE sizes are\nconducted, and the optimum number of CUDA streams for each of them is found\nempirically. Based on the collected data a model for the sum of the times for\nthe non-dominant GPU operations (that take part in the stream overlap) is\nformulated using regression analysis. A fitting non-linear model for the\noverhead time connected with the creation of CUDA streams is created.\nStatistical analysis is done for all the built models. An algorithm for finding\nthe optimum number of CUDA streams is formulated. Using this algorithm,\ntogether with the two models mentioned above, predictions for the optimum\nnumber of CUDA streams are made. Comparing the predicted values with the actual\ndata, the algorithm is deemed to be acceptably good.",
        "Repetition code forms a fundamental basis for quantum error correction\nexperiments. To date, it stands as the sole code that has achieved large\ndistances and extremely low error rates. Its applications span the spectrum of\nevaluating hardware limitations, pinpointing hardware defects, and detecting\nrare events. However, current methods for decoding repetition codes under\ncircuit level noise are suboptimal, leading to inaccurate error correction\nthresholds and introducing additional errors in event detection. In this work,\nwe establish that repetition code under circuit level noise has an exact\nsolution, and we propose an optimal maximum likelihood decoding algorithm\ncalled planar. The algorithm is based on the exact solution of the spin glass\npartition function on planar graphs and has polynomial computational\ncomplexity. Through extensive numerical experiments, we demonstrate that our\nalgorithm uncovers the exact threshold for depolarizing noise and realistic\nsuperconductor SI1000 noise. Furthermore, we apply our method to analyze data\nfrom recent quantum memory experiments conducted by Google Quantum AI,\nrevealing that part of the error floor was attributed to the decoding algorithm\nused by Google. Finally, we implemented the repetition code quantum memory on\nsuperconducting systems with a 72-qubit quantum chip lacking reset gates,\ndemonstrating that even with an unknown error model, the proposed algorithm\nachieves a significantly lower logical error rate than the matching-based\nalgorithm.",
        "This paper introduces a novel audio-to-image encoding framework that\nintegrates multiple dimensions of voice characteristics into a single RGB image\nfor speaker recognition. In this method, the green channel encodes raw audio\ndata, the red channel embeds statistical descriptors of the voice signal\n(including key metrics such as median and mean values for fundamental\nfrequency, spectral centroid, bandwidth, rolloff, zero-crossing rate, MFCCs,\nRMS energy, spectral flatness, spectral contrast, chroma, and harmonic-to-noise\nratio), and the blue channel comprises subframes representing these features in\na spatially organized format. A deep convolutional neural network trained on\nthese composite images achieves 98% accuracy in speaker classification across\ntwo speakers, suggesting that this integrated multi-channel representation can\nprovide a more discriminative input for voice recognition tasks.",
        "Online Action Detection (OAD) detects actions in streaming videos using past\nobservations. State-of-the-art OAD approaches model past observations and their\ninteractions with an anticipated future. The past is encoded using short- and\nlong-term memories to capture immediate and long-range dependencies, while\nanticipation compensates for missing future context. We identify a\ntraining-inference discrepancy in existing OAD methods that hinders learning\neffectiveness. The training uses varying lengths of short-term memory, while\ninference relies on a full-length short-term memory. As a remedy, we propose a\nContext-enhanced Memory-Refined Transformer (CMeRT). CMeRT introduces a\ncontext-enhanced encoder to improve frame representations using additional\nnear-past context. It also features a memory-refined decoder to leverage\nnear-future generation to enhance performance. CMeRT achieves state-of-the-art\nin online detection and anticipation on THUMOS'14, CrossTask, and\nEPIC-Kitchens-100.",
        "Designing the next generation colliders and detectors involves solving\noptimization problems in high-dimensional spaces where the optimal solutions\nmay nest in regions that even a team of expert humans would not explore.\n  Resorting to Artificial Intelligence to assist the experimental design\nintroduces however significant computational challenges in terms of generation\nand processing of the data required to perform such optimizations: from the\nsoftware point of view, differentiable programming makes the exploration of\nsuch spaces with gradient descent feasible; from the hardware point of view,\nthe complexity of the resulting models and their optimization is prohibitive.\nTo scale up to the complexity of the typical HEP collider experiment, a change\nin paradigma is required.\n  In this contribution I will describe the first proofs-of-concept of\ngradient-based optimization of experimental design and implementations in\nneuromorphic hardware architectures, paving the way to more complex challenges.",
        "Biological signals, such as electroencephalograms (EEGs) and\nelectrocardiograms (ECGs), play a pivotal role in numerous clinical practices,\nsuch as diagnosing brain and cardiac arrhythmic diseases. Existing methods for\nbiosignal classification rely on Attention-based frameworks with dense Feed\nForward layers, which lead to inefficient learning, high computational\noverhead, and suboptimal performance. In this work, we introduce BioMamba, a\nSpectro-Temporal Embedding strategy applied to the Bidirectional Mamba\nframework with Sparse Feed Forward layers to enable effective learning of\nbiosignal sequences. By integrating these three key components, BioMamba\neffectively addresses the limitations of existing methods. Extensive\nexperiments demonstrate that BioMamba significantly outperforms\nstate-of-the-art methods with marked improvement in classification performance.\nThe advantages of the proposed BioMamba include (1) Reliability: BioMamba\nconsistently delivers robust results, confirmed across six evaluation metrics.\n(2) Efficiency: We assess both model and training efficiency, the BioMamba\ndemonstrates computational effectiveness by reducing model size and resource\nconsumption compared to existing approaches. (3) Generality: With the capacity\nto effectively classify a diverse set of tasks, BioMamba demonstrates\nadaptability and effectiveness across various domains and applications.",
        "3D object reconstruction from single-view image is a fundamental task in\ncomputer vision with wide-ranging applications. Recent advancements in Large\nReconstruction Models (LRMs) have shown great promise in leveraging multi-view\nimages generated by 2D diffusion models to extract 3D content. However,\nchallenges remain as 2D diffusion models often struggle to produce dense images\nwith strong multi-view consistency, and LRMs tend to amplify these\ninconsistencies during the 3D reconstruction process. Addressing these issues\nis critical for achieving high-quality and efficient 3D reconstruction. In this\npaper, we present CDI3D, a feed-forward framework designed for efficient,\nhigh-quality image-to-3D generation with view interpolation. To tackle the\naforementioned challenges, we propose to integrate 2D diffusion-based view\ninterpolation into the LRM pipeline to enhance the quality and consistency of\nthe generated mesh. Specifically, our approach introduces a Dense View\nInterpolation (DVI) module, which synthesizes interpolated images between main\nviews generated by the 2D diffusion model, effectively densifying the input\nviews with better multi-view consistency. We also design a tilt camera pose\ntrajectory to capture views with different elevations and perspectives.\nSubsequently, we employ a tri-plane-based mesh reconstruction strategy to\nextract robust tokens from these interpolated and original views, enabling the\ngeneration of high-quality 3D meshes with superior texture and geometry.\nExtensive experiments demonstrate that our method significantly outperforms\nprevious state-of-the-art approaches across various benchmarks, producing 3D\ncontent with enhanced texture fidelity and geometric accuracy.",
        "The quantum description of a black hole predicts that quantum information\nhidden behind the event horizon can be teleported outside almost\ninstantaneously. In this work, we demonstrate that a chiral spin-chain model,\nwhich naturally simulates a binary black hole system, can realise this\nteleportation process. Our system captures two essential components of this\nprotocol: Hawking radiation, which generates the necessary entanglement between\nthe black holes, and optimal scrambling, which enables high-fidelity\nteleportation on short timescales. Through numerical simulations, we quantify\nthe key timescales governing the process, including the Page time, radiation\ntime, scrambling time, and butterfly velocity, showing their universal\ndependence on the chiral coupling strength. Our results establish the\nfeasibility of simulating quantum properties of black holes within condensed\nmatter systems, offering an experimentally accessible platform for probing\notherwise inaccessible high-energy phenomena.",
        "Large Language Models (LLMs) have shown remarkable capabilities as AI agents.\nHowever, existing methods for enhancing LLM-agent abilities often lack a focus\non data quality, leading to inefficiencies and suboptimal results in both\nfine-tuning and prompt engineering. To address this issue, we introduce EDGE, a\nnovel approach for identifying informative samples without needing golden\nanswers. We propose the Guideline Effectiveness (GE) metric, which selects\nchallenging samples by measuring the impact of human-provided guidelines in\nmulti-turn interaction tasks. A low GE score indicates that the human expertise\nrequired for a sample is missing from the guideline, making the sample more\ninformative. By selecting samples with low GE scores, we can improve the\nefficiency and outcomes of both prompt engineering and fine-tuning processes\nfor LLMs. Extensive experiments validate the performance of our method. Our\nmethod achieves competitive results on the HotpotQA and WebShop and datasets,\nrequiring 75\\% and 50\\% less data, respectively, while outperforming existing\nmethods. We also provide a fresh perspective on the data quality of LLM-agent\nfine-tuning.",
        "Automated Essay Scoring (AES) plays a crucial role in educational assessment\nby providing scalable and consistent evaluations of writing tasks. However,\ntraditional AES systems face three major challenges: (1) reliance on\nhandcrafted features that limit generalizability, (2) difficulty in capturing\nfine-grained traits like coherence and argumentation, and (3) inability to\nhandle multimodal contexts. In the era of Multimodal Large Language Models\n(MLLMs), we propose EssayJudge, the first multimodal benchmark to evaluate AES\ncapabilities across lexical-, sentence-, and discourse-level traits. By\nleveraging MLLMs' strengths in trait-specific scoring and multimodal context\nunderstanding, EssayJudge aims to offer precise, context-rich evaluations\nwithout manual feature engineering, addressing longstanding AES limitations.\nOur experiments with 18 representative MLLMs reveal gaps in AES performance\ncompared to human evaluation, particularly in discourse-level traits,\nhighlighting the need for further advancements in MLLM-based AES research. Our\ndataset and code will be available upon acceptance.",
        "A series of recent papers by Bergfalk, Lupini and Panagiotopoulus developed\nthe foundations of a field known as `definable algebraic topology,' in which\nclassical cohomological invariants are enriched by viewing them as groups with\na Polish cover. This allows one to apply techniques from descriptive set theory\nto the study of cohomology theories. In this paper, we will establish a\n`definable' version of a classical theorem from obstruction theory, and use\nthis to study the potential complexity of the homotopy relation on the space of\ncontinuous maps $C(X, |K|)$, where $X$ is a locally compact Polish space, and K\nis a locally finite countable simplicial complex. We will also characterize the\nSolecki Groups of the Cech cohomology of X, which are the canonical chain of\nsubgroups with a Polish cover that are least among those of a given complexity.",
        "We present progress on the calculation of scalar, vector, and tensor form\nfactors for the following meson decays: $B\\rightarrow\\pi$, $B_s\\rightarrow K$,\n$D\\rightarrow\\pi$ and $D_s\\rightarrow K$. This calculation uses the MILC HISQ\ngluon field ensembles with HISQ valence quarks. We generate ensembles of\ncorrelator data with varying lattice spacings, some as small as 0.044 fm. Some\nensembles have a strange-to-light quark mass ratio of 5:1 and others use the\nphysical light quark mass. The fully-relativistic, heavy-HISQ approach is used\nfor the heavy quark, with simulation masses ranging from the charm to near the\nbottom. This heavy-HISQ approach provides nearly full coverage of the kinematic\nrange."
      ]
    }
  },
  {
    "id":2411.10403,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"MoDL: Model-Based Deep Learning Architecture for Inverse Problems",
    "start_abstract":"We introduce a model-based image reconstruction framework with convolution neural network (CNN)-based regularization prior. The proposed formulation provides systematic approach for deriving deep architectures inverse problems the arbitrary structure. Since forward model is explicitly accounted for, smaller fewer parameters sufficient to capture information compared direct inversion approaches. Thus, reducing demand training data and time. we rely on end-to-end weight sharing across iterations, CNN weights are customized model, thus offering improved performance over approaches that pre-trained denoisers. Our experiments show decoupling of number iterations from complexity offered by this benefits, including lower data, reduced risk overfitting, implementations significantly memory footprint. propose enforce data-consistency using numerical optimization blocks, such as conjugate gradients algorithm within network. This offers faster convergence per iteration, methods proximal steps consistency. translates performance, primarily when available GPU restricts iterations.",
    "start_categories":[
      "cs.LG",
      "stat.CO"
    ],
    "start_fields":[
      "Mathematics and Statistics",
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b13"
      ],
      "title":[
        "Cardiovascular Flow Measurement with Phase-Contrast MR Imaging: Basic Facts and Implementation"
      ],
      "abstract":[
        "Phase-contrast magnetic resonance (MR) imaging is a well-known but undervalued method of obtaining quantitative information on blood flow. Applications this technique in cardiovascular MR are expanding. According to the sequences available, phase-contrast measurement can be performed breath hold or during normal respiration. Prospective as well retrospective gating techniques used. Common errors include mismatched encoding velocity, deviation plane, inadequate temporal resolution, spatial accelerated flow and misregistration, phase offset errors. Flow measurements most precise if plane perpendicular vessel interest set through-plane The sequence should repeated at least once, with high velocity used initially. If peak has estimated, an adapted velocity. overall error comprises prescription that occur image analysis data. With imaging, reduced less than 10%, acceptable level for routine clinical use."
      ],
      "categories":[
        "q-bio.QM"
      ]
    },
    "list":{
      "title":[
        "UNGT: Ultrasound Nasogastric Tube Dataset for Medical Image Analysis",
        "Heuristics based on Adjacency Graph Packing for DCJ Distance Considering\n  Intergenic Regions",
        "Efficient Spatial Estimation of Perceptual Thresholds for Retinal\n  Implants via Gaussian Process Regression",
        "IA generativa aplicada a la detecci\\'on del c\\'ancer a trav\\'es de\n  Resonancia Magn\\'etica",
        "Design of an Automated Ethanol Vapor Generating System for Alcohol Use\n  Disorder(AUD) Animal Studies",
        "A Systematic Computational Framework for Practical Identifiability\n  Analysis in Mathematical Models Arising from Biology",
        "Solar radiation and atmospheric CO$_2$ predict young leaf production in\n  a moist evergreen tropical forest: Insights from 23 years",
        "Reconstructing Noisy Gene Regulation Dynamics Using\n  Extrinsic-Noise-Driven Neural Stochastic Differential Equations",
        "Collective inference of the truth of propositions from crowd probability\n  judgments",
        "Unsupervised detection and fitness estimation of emerging SARS-CoV-2\n  variants. Application to wastewater samples (ANRS0160)",
        "Measuring Fitness and Importance of Species in Food Webs",
        "Leveraging Retrieval-Augmented Generation and Large Language Models to\n  Predict SERCA-Binding Protein Fragments from Cardiac Proteomics Data",
        "Beware of so-called 'good' correlations: a statistical reality check on\n  individual mRNA-protein predictions",
        "Evaluating Answer Reranking Strategies in Time-sensitive Question\n  Answering",
        "On the Potential Galactic Origin of the Ultra-High-Energy Event\n  KM3-230213A",
        "Plantation Monitoring Using Drone Images: A Dataset and Performance\n  Review",
        "ML-Based Optimum Number of CUDA Streams for the GPU Implementation of\n  the Tridiagonal Partition Method",
        "Exact Decoding of Repetition Code under Circuit Level Noise",
        "Audio-to-Image Encoding for Improved Voice Characteristic Detection\n  Using Deep Convolutional Neural Networks",
        "Context-Enhanced Memory-Refined Transformer for Online Action Detection",
        "AI-assisted design of experiments at the frontiers of computation:\n  methods and new perspectives",
        "BioMamba: Leveraging Spectro-Temporal Embedding in Bidirectional Mamba\n  for Enhanced Biosignal Classification",
        "CDI3D: Cross-guided Dense-view Interpolation for 3D Reconstruction",
        "Quantum teleportation between simulated binary black holes",
        "EDGE: Efficient Data Selection for LLM Agents via Guideline\n  Effectiveness",
        "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay\n  Scoring Capabilities of Multimodal Large Language Models",
        "Definable Obstruction Theory",
        "Towards more accurate $B_{(s)}\\rightarrow\\pi(K)$ and\n  $D_{(s)}\\rightarrow\\pi(K)$ form factors"
      ],
      "abstract":[
        "We develop a novel ultrasound nasogastric tube (UNGT) dataset to address the\nlack of public nasogastric tube datasets. The UNGT dataset includes 493 images\ngathered from 110 patients with an average image resolution of approximately\n879 $\\times$ 583. Four structures, encompassing the liver, stomach, tube, and\npancreas are precisely annotated. Besides, we propose a semi-supervised\nadaptive-weighting aggregation medical segmenter to address data limitation and\nimbalance concurrently. The introduced adaptive weighting approach tackles the\nsevere unbalanced challenge by regulating the loss across varying categories as\ntraining proceeds. The presented multiscale attention aggregation block\nbolsters the feature representation by integrating local and global contextual\ninformation. With these, the proposed AAMS can emphasize sparse or small\nstructures and feature enhanced representation ability. We perform extensive\nsegmentation experiments on our UNGT dataset, and the results show that AAMS\noutperforms existing state-of-the-art approaches to varying extents. In\naddition, we conduct comprehensive classification experiments across varying\nstate-of-the-art methods and compare their performance. The dataset and code\nwill be available upon publication at https:\/\/github.com\/NUS-Tim\/UNGT.",
        "In this work, we explore heuristics for the Adjacency Graph Packing problem,\nwhich can be applied to the Double Cut and Join (DCJ) Distance Problem. The DCJ\nis a rearrangement operation and the distance problem considering it is a well\nestablished method for genome comparison. Our heuristics will use the structure\ncalled adjacency graph adapted to include information about intergenic regions,\nmultiple copies of genes in the genomes, and multiple circular or linear\nchromosomes. The only required property from the genomes is that it must be\npossible to turn one into the other with DCJ operations. We propose one greedy\nheuristic and one heuristic based on Genetic Algorithms. Our experimental tests\nin artificial genomes show that the use of heuristics is capable of finding\ngood results that are superior to a simpler random strategy.",
        "Retinal prostheses restore vision by electrically stimulating surviving\nneurons, but calibrating perceptual thresholds - the minimum stimulus intensity\nrequired for perception - remains a time-intensive challenge, especially for\nhigh-electrode-count devices. Since neighboring electrodes exhibit spatial\ncorrelations, we propose a Gaussian Process Regression (GPR) framework to\npredict thresholds at unsampled locations while leveraging uncertainty\nestimates to guide adaptive sampling. Using perceptual threshold data from four\nArgus II users, we show that GPR with a Mat\\'ern kernel provides more accurate\nthreshold predictions than a Radial Basis Function (RBF) kernel (p < .001,\nWilcoxon signed-rank test). In addition, spatially optimized sampling yielded\nlower prediction error than uniform random sampling for Participants 1 and 3 (p\n< .05). While adaptive sampling dynamically selects electrodes based on model\nuncertainty, its accuracy gains over spatial sampling were not statistically\nsignificant (p > .05), though it approached significance for Participant 1 (p =\n.074). These findings establish GPR with spatial sampling as a scalable,\nefficient approach to retinal prosthesis calibration, minimizing patient burden\nwhile maintaining predictive accuracy. More broadly, this framework offers a\ngeneralizable solution for adaptive calibration in neuroprosthetic devices with\nspatially structured stimulation thresholds.",
        "Cognitive delegation to artificial intelligence (AI) systems is transforming\nscientific research by enabling the automation of analytical processes and the\ndiscovery of new patterns in large datasets. This study examines the ability of\nAI to complement and expand knowledge in the analysis of breast cancer using\ndynamic contrast-enhanced magnetic resonance imaging (DCE-MRI). Building on a\nprevious study, we assess the extent to which AI can generate novel approaches\nand successfully solve them. For this purpose, AI models, specifically\nChatGPT-4o, were used for data preprocessing, hypothesis generation, and the\napplication of clustering techniques, predictive modeling, and correlation\nnetwork analysis. The results obtained were compared with manually computed\noutcomes, revealing limitations in process transparency and the accuracy of\ncertain calculations. However, as AI reduces errors and improves reasoning\ncapabilities, an important question arises regarding the future of scientific\nresearch: could automation replace the human role in science? This study seeks\nto open the debate on the methodological and ethical implications of a science\ndominated by artificial intelligence.",
        "Alcohol Use Disorder (AUD) is a prevalent addictive disorder affecting an\nestimated 29.5 million Americans. It is characterized by impaired control over\nalcohol consumption despite negative consequences. The number of diagnostic\ncriteria met by an individual typically determines the severity of AUD.\nResearch into AUD focuses on understanding individual susceptibility\ndifferences and developing preventive strategies. Alcohol vapor inhalation has\nemerged as a promising method for pathophysiological investigations in animals,\nallowing researchers to control the dose and duration of alcohol exposure. This\napproach is crucial for studying the escalation of voluntary alcohol-drinking\nbehavior. Current commercial systems for alcohol vapor generation have\nlimitations, including combustion risks and the need to adjust multiple\nparameters. Other methods, like bubbling or blow-over evaporation, face\nchallenges in maintaining equilibrium and avoiding aerosolization. To address\nthese issues, a new type of ethanol vapor generating system is proposed that\nrelies solely on temperature control, creating a vacuum into which ethanol\nevaporates under thermodynamic control. This approach eliminates the need to\nadjust multiple parameters and offers improved accuracy and precision in vapor\ndose delivery. We validated the system as anticipated, achieving stable ethanol\nvapor after a few priming cycles. Using a 1.2 L cylinder, we obtained\napproximately 3.6 L of saturated vapor\/air mix in 1 minute. Gravimetric results\nshowed that each cycle produced about 100 mg\/L or ~10,000 ppm vapor-to-air\nmixture. The intended use of the ethanol vapor generator is to provide a\nconcentrated ethanol vapor \/ air mixture to be further diluted before\ndelivering to the animals.",
        "Practical identifiability is a critical concern in data-driven modeling of\nmathematical systems. In this paper, we propose a novel framework for practical\nidentifiability analysis to evaluate parameter identifiability in mathematical\nmodels of biological systems. Starting with a rigorous mathematical definition\nof practical identifiability, we demonstrate its equivalence to the\ninvertibility of the Fisher Information Matrix. Our framework establishes the\nrelationship between practical identifiability and coordinate identifiability,\nintroducing a novel metric that simplifies and accelerates the evaluation of\nparameter identifiability compared to the profile likelihood method.\nAdditionally, we introduce new regularization terms to address non-identifiable\nparameters, enabling uncertainty quantification and improving model\nreliability. To guide experimental design, we present an optimal data\ncollection algorithm that ensures all model parameters are practically\nidentifiable. Applications to Hill functions, neural networks, and dynamic\nbiological models demonstrate the feasibility and efficiency of the proposed\ncomputational framework in uncovering critical biological processes and\nidentifying key observable variables.",
        "Climate change impacts ecosystems worldwide, affecting animal behaviour and\nsurvival both directly and indirectly through changes such as the availability\nof food. For animals reliant on leaves as a primary food source, understanding\nhow climate change influences leaf production of trees is crucial, yet this is\nunderstudied, especially in moist evergreen tropical forests. We analyzed a\n23-year dataset of young leaf phenology from a moist tropical forest in Kibale\nNational Park, Uganda, to examine seasonal and long-term patterns of 12 key\ntree species consumed by folivorous primates. We described phenological\npatterns and explored relationships between young leaf production of different\ntree species and climate variables. We also assessed the suitability of the\nEnhanced Vegetation Index (EVI) as a proxy for young leaf production in moist\nevergreen tropical forests. Our results showed that tree species exhibited\ndistinct phenological patterns, with most species producing young leaves during\ntwo seasonal peaks aligned with the rainy seasons. Rainfall, cloud cover, and\nmaximum temperature were the most informative predictors of seasonal variation\nin young leaf production. However, solar radiation and atmospheric CO$_2$ were\nmost informative regarding long-term trends. EVI was strongly correlated with\nyoung leaf production within years but less effective for capturing\ninter-annual trends. These findings highlight the complex relationship between\nclimate and young leaf phenology in moist evergreen tropical forests, and helps\nus understand the changes in food availability for tropical folivores.",
        "Proper regulation of cell signaling and gene expression is crucial for\nmaintaining cellular function, development, and adaptation to environmental\nchanges. Reaction dynamics in cell populations is often noisy because of (i)\ninherent stochasticity of intracellular biochemical reactions (``intrinsic\nnoise'') and (ii) heterogeneity of cellular states across different cells that\nare influenced by external factors (``extrinsic noise''). In this work, we\nintroduce an extrinsic-noise-driven neural stochastic differential equation\n(END-nSDE) framework that utilizes the Wasserstein distance to accurately\nreconstruct SDEs from trajectory data from a heterogeneous population of cells\n(extrinsic noise). We demonstrate the effectiveness of our approach using both\nsimulated and experimental data from three different systems in cell biology:\n(i) circadian rhythms, (ii) RPA-DNA binding dynamics, and (iii) NF$\\kappa$B\nsignaling process. Our END-nSDE reconstruction method can model how cellular\nheterogeneity (extrinsic noise) modulates reaction dynamics in the presence of\nintrinsic noise. It also outperforms existing time-series analysis methods such\nas recurrent neural networks (RNNs) and long short-term memory networks\n(LSTMs). By inferring cellular heterogeneities from data, our END-nSDE\nreconstruction method can reproduce noisy dynamics observed in experiments. In\nsummary, the reconstruction method we propose offers a useful surrogate\nmodeling approach for complex biophysical processes, where high-fidelity\nmechanistic models may be impractical.",
        "Every day, we judge the probability of propositions. When we communicate\ngraded confidence (e.g. \"I am 90% sure\"), we enable others to gauge how much\nweight to attach to our judgment. Ideally, people should share their judgments\nto reach more accurate conclusions collectively. Peer-to-peer tools for\ncollective inference could help debunk disinformation and amplify reliable\ninformation on social networks, improving democratic discourse. However,\nindividuals fall short of the ideal of well-calibrated probability judgments,\nand group dynamics can amplify errors and polarize opinions. Here, we connect\ninsights from cognitive science, structured expert judgment, and crowdsourcing\nto infer the truth of propositions from human probability judgments. In an\nonline experiment, 376 participants judged the probability of each of 1,200\ngeneral-knowledge claims for which we have ground truth (451,200 ratings).\nAggregating binary judgments by majority vote already exhibits the \"wisdom of\nthe crowd\"--the superior accuracy of collective inferences relative to\nindividual inferences. However, using continuous probability ratings and\naccounting for individual accuracy and calibration significantly improves\ncollective inferences. Peer judgment behavior can be modeled probabilistically,\nand individual parameters capturing each peer's accuracy and miscalibration can\nbe inferred jointly with the claim probabilities. This unsupervised approach\ncan be complemented by supervised methods relying on truth labels to learn\nmodels that achieve well-calibrated collective inference. The algorithms we\nintroduce can empower groups of collaborators and online communities to pool\ntheir distributed intelligence and jointly judge the probability of\npropositions with a well-calibrated sense of uncertainty.",
        "Repeated waves of emerging variants during the SARS-CoV-2 pandemics have\nhighlighted the urge of collecting longitudinal genomic data and developing\nstatistical methods based on time series analyses for detecting new threatening\nlineages and estimating their fitness early in time. Most models study the\nevolution of the prevalence of particular lineages over time and require a\nprior classification of sequences into lineages. Such process is prone to\ninduce delays and bias. More recently, few authors studied the evolution of the\nprevalence of mutations over time with alternative clustering approaches,\navoiding specific lineage classification. Most of the aforementioned methods\nare however either non parametric or unsuited to pooled data characterizing,\nfor instance, wastewater samples. In this context, we propose an alternative\nunsupervised method for clustering mutations according to their frequency\ntrajectory over time and estimating group fitness from time series of pooled\nmutation prevalence data. Our model is a mixture of observed count data and\nlatent group assignment and we use the expectation-maximization algorithm for\nmodel selection and parameter estimation. The application of our method to time\nseries of SARS-CoV-2 sequencing data collected from wastewater treatment plants\nin France from October 2020 to April 2021 shows its ability to agnostically\ngroup mutations according to their probability of belonging to B.1.160, Alpha,\nBeta, B.1.177 variants with selection coefficient estimates per group in\ncoherence with the viral dynamics in France reported by Nextstrain. Moreover,\nour method detected the Alpha variant as threatening as early as supervised\nmethods (which track specific mutations over time) with the noticeable\ndifference that, since unsupervised, it does not require any prior information\non the set of mutations.",
        "Ecosystems face intensifying threats from climate change, overexploitation,\nand other human pressures, emphasizing the urgent need to identify keystone\nspecies and vulnerable ones. While established network-based measures often\nrely on a single metric to quantify a species' relevance, they overlook how\norganisms can be both carbon providers and consumers, thus playing a dual role\nin food webs. Here, we introduce a novel approach that assigns each species two\ncomplementary scores -- an importance measure quantifying their centrality as\ncarbon source and a fitness measure capturing their vulnerability. We show that\nspecies with high importance are more likely to trigger co-extinctions upon\nremoval, while high-fitness species typically endure until later stages of\ncollapse, in line with their broader prey ranges. On the other hand, low\nfitness species are the most vulnerable and susceptible to extinctions. Tested\non multiple food webs, our method outperforms traditional degree-based analyses\nand competes effectively with eigenvector-based approaches, while also\nproviding additional insights. Relying solely on interaction data, the approach\nis scalable and avoids reliance on expert-driven classifications, offering a\ncost-effective tool for prioritizing conservation efforts.",
        "Large language models (LLMs) have shown promise in various natural language\nprocessing tasks, including their application to proteomics data to classify\nprotein fragments. In this study, we curated a limited mass spectrometry\ndataset with 1000s of protein fragments, consisting of proteins that appear to\nbe attached to the endoplasmic reticulum in cardiac cells, of which a fraction\nwas cloned and characterized for their impact on SERCA, an ER calcium pump.\nWith this limited dataset, we sought to determine whether LLMs could correctly\npredict whether a new protein fragment could bind SERCA, based only on its\nsequence and a few biophysical characteristics, such as hydrophobicity,\ndetermined from that sequence. To do so, we generated random sequences based on\ncloned fragments, embedded the fragments into a retrieval augmented generation\n(RAG) database to group them by similarity, then fine-tuned large language\nmodel (LLM) prompts to predict whether a novel sequence could bind SERCA. We\nbenchmarked this approach using multiple open-source LLMs, namely the\nMeta\/llama series, and embedding functions commonly available on the\nHuggingface repository. We then assessed the generalizability of this approach\nin classifying novel protein fragments from mass spectrometry that were not\ninitially cloned for functional characterization. By further tuning the prompt\nto account for motifs, such as ER retention sequences, we improved the\nclassification accuracy by and identified several proteins predicted to\nlocalize to the endoplasmic reticulum and bind SERCA, including Ribosomal\nProtein L2 and selenoprotein S. Although our results were based on proteomics\ndata from cardiac cells, our approach demonstrates the potential of LLMs in\nidentifying novel protein interactions and functions with very limited\nproteomic data.",
        "Research in the life sciences often employs messenger ribonucleic acids\n(mRNA) quantification as a standalone approach for functional analysis.\nHowever, although the correlation between the measured levels of mRNA and\nproteins is positive, correlation coefficients observed empirically are\nincomplete, necessitating caution in making agnostic inferences. This essay\nprovides a statistical reflection and caveat on the concept of correlation\nstrength in the context of transcriptomics-proteomics studies. It highlights\nthe variability in possible protein levels at given empirical correlation\nvalues, even for precise mRNA amount, and underscores the notable proportion of\nmRNA-protein pairs with abundances at opposite ends of their respective\ndistributions. Cell biologists, data scientists, and biostatisticians should\nrecognise that mRNA-protein correlation alone is insufficient to justify using\na single mRNA quantification to infer the amount or function of its\ncorresponding protein.",
        "Despite advancements in state-of-the-art models and information retrieval\ntechniques, current systems still struggle to handle temporal information and\nto correctly answer detailed questions about past events. In this paper, we\ninvestigate the impact of temporal characteristics of answers in Question\nAnswering (QA) by exploring several simple answer selection techniques. Our\nfindings emphasize the role of temporal features in selecting the most relevant\nanswers from diachronic document collections and highlight differences between\nexplicit and implicit temporal questions.",
        "The KM3NeT observatory detected the most energetic neutrino candidate ever\nobserved, with an energy between 72 PeV and 2.6 EeV at the 90% confidence\nlevel. The observed neutrino is likely of cosmic origin. In this article, it is\ninvestigated if the neutrino could have been produced within the Milky Way.\nConsidering the low fluxes of the Galactic diffuse emission at these energies,\nthe lack of a nearby potential Galactic particle accelerator in the direction\nof the event and the difficulty to accelerate particles to such high energies\nin Galactic systems, we conclude that if the event is indeed cosmic, it is most\nlikely of extragalactic origin.",
        "Automatic monitoring of tree plantations plays a crucial role in agriculture.\nFlawless monitoring of tree health helps farmers make informed decisions\nregarding their management by taking appropriate action. Use of drone images\nfor automatic plantation monitoring can enhance the accuracy of the monitoring\nprocess, while still being affordable to small farmers in developing countries\nsuch as India. Small, low cost drones equipped with an RGB camera can capture\nhigh-resolution images of agricultural fields, allowing for detailed analysis\nof the well-being of the plantations. Existing methods of automated plantation\nmonitoring are mostly based on satellite images, which are difficult to get for\nthe farmers. We propose an automated system for plantation health monitoring\nusing drone images, which are becoming easier to get for the farmers. We\npropose a dataset of images of trees with three categories: ``Good health\",\n``Stunted\", and ``Dead\". We annotate the dataset using CVAT annotation tool,\nfor use in research purposes. We experiment with different well-known CNN\nmodels to observe their performance on the proposed dataset. The initial low\naccuracy levels show the complexity of the proposed dataset. Further, our study\nrevealed that, depth-wise convolution operation embedded in a deep CNN model,\ncan enhance the performance of the model on drone dataset. Further, we apply\nstate-of-the-art object detection models to identify individual trees to better\nmonitor them automatically.",
        "This paper presents a heuristic for finding the optimum number of CUDA\nstreams by using tools common to the modern AI-oriented approaches and applied\nto the parallel partition algorithm. A time complexity model for the GPU\nrealization of the partition method is built. Further, a refined time\ncomplexity model for the partition algorithm being executed on multiple CUDA\nstreams is formulated. Computational experiments for different SLAE sizes are\nconducted, and the optimum number of CUDA streams for each of them is found\nempirically. Based on the collected data a model for the sum of the times for\nthe non-dominant GPU operations (that take part in the stream overlap) is\nformulated using regression analysis. A fitting non-linear model for the\noverhead time connected with the creation of CUDA streams is created.\nStatistical analysis is done for all the built models. An algorithm for finding\nthe optimum number of CUDA streams is formulated. Using this algorithm,\ntogether with the two models mentioned above, predictions for the optimum\nnumber of CUDA streams are made. Comparing the predicted values with the actual\ndata, the algorithm is deemed to be acceptably good.",
        "Repetition code forms a fundamental basis for quantum error correction\nexperiments. To date, it stands as the sole code that has achieved large\ndistances and extremely low error rates. Its applications span the spectrum of\nevaluating hardware limitations, pinpointing hardware defects, and detecting\nrare events. However, current methods for decoding repetition codes under\ncircuit level noise are suboptimal, leading to inaccurate error correction\nthresholds and introducing additional errors in event detection. In this work,\nwe establish that repetition code under circuit level noise has an exact\nsolution, and we propose an optimal maximum likelihood decoding algorithm\ncalled planar. The algorithm is based on the exact solution of the spin glass\npartition function on planar graphs and has polynomial computational\ncomplexity. Through extensive numerical experiments, we demonstrate that our\nalgorithm uncovers the exact threshold for depolarizing noise and realistic\nsuperconductor SI1000 noise. Furthermore, we apply our method to analyze data\nfrom recent quantum memory experiments conducted by Google Quantum AI,\nrevealing that part of the error floor was attributed to the decoding algorithm\nused by Google. Finally, we implemented the repetition code quantum memory on\nsuperconducting systems with a 72-qubit quantum chip lacking reset gates,\ndemonstrating that even with an unknown error model, the proposed algorithm\nachieves a significantly lower logical error rate than the matching-based\nalgorithm.",
        "This paper introduces a novel audio-to-image encoding framework that\nintegrates multiple dimensions of voice characteristics into a single RGB image\nfor speaker recognition. In this method, the green channel encodes raw audio\ndata, the red channel embeds statistical descriptors of the voice signal\n(including key metrics such as median and mean values for fundamental\nfrequency, spectral centroid, bandwidth, rolloff, zero-crossing rate, MFCCs,\nRMS energy, spectral flatness, spectral contrast, chroma, and harmonic-to-noise\nratio), and the blue channel comprises subframes representing these features in\na spatially organized format. A deep convolutional neural network trained on\nthese composite images achieves 98% accuracy in speaker classification across\ntwo speakers, suggesting that this integrated multi-channel representation can\nprovide a more discriminative input for voice recognition tasks.",
        "Online Action Detection (OAD) detects actions in streaming videos using past\nobservations. State-of-the-art OAD approaches model past observations and their\ninteractions with an anticipated future. The past is encoded using short- and\nlong-term memories to capture immediate and long-range dependencies, while\nanticipation compensates for missing future context. We identify a\ntraining-inference discrepancy in existing OAD methods that hinders learning\neffectiveness. The training uses varying lengths of short-term memory, while\ninference relies on a full-length short-term memory. As a remedy, we propose a\nContext-enhanced Memory-Refined Transformer (CMeRT). CMeRT introduces a\ncontext-enhanced encoder to improve frame representations using additional\nnear-past context. It also features a memory-refined decoder to leverage\nnear-future generation to enhance performance. CMeRT achieves state-of-the-art\nin online detection and anticipation on THUMOS'14, CrossTask, and\nEPIC-Kitchens-100.",
        "Designing the next generation colliders and detectors involves solving\noptimization problems in high-dimensional spaces where the optimal solutions\nmay nest in regions that even a team of expert humans would not explore.\n  Resorting to Artificial Intelligence to assist the experimental design\nintroduces however significant computational challenges in terms of generation\nand processing of the data required to perform such optimizations: from the\nsoftware point of view, differentiable programming makes the exploration of\nsuch spaces with gradient descent feasible; from the hardware point of view,\nthe complexity of the resulting models and their optimization is prohibitive.\nTo scale up to the complexity of the typical HEP collider experiment, a change\nin paradigma is required.\n  In this contribution I will describe the first proofs-of-concept of\ngradient-based optimization of experimental design and implementations in\nneuromorphic hardware architectures, paving the way to more complex challenges.",
        "Biological signals, such as electroencephalograms (EEGs) and\nelectrocardiograms (ECGs), play a pivotal role in numerous clinical practices,\nsuch as diagnosing brain and cardiac arrhythmic diseases. Existing methods for\nbiosignal classification rely on Attention-based frameworks with dense Feed\nForward layers, which lead to inefficient learning, high computational\noverhead, and suboptimal performance. In this work, we introduce BioMamba, a\nSpectro-Temporal Embedding strategy applied to the Bidirectional Mamba\nframework with Sparse Feed Forward layers to enable effective learning of\nbiosignal sequences. By integrating these three key components, BioMamba\neffectively addresses the limitations of existing methods. Extensive\nexperiments demonstrate that BioMamba significantly outperforms\nstate-of-the-art methods with marked improvement in classification performance.\nThe advantages of the proposed BioMamba include (1) Reliability: BioMamba\nconsistently delivers robust results, confirmed across six evaluation metrics.\n(2) Efficiency: We assess both model and training efficiency, the BioMamba\ndemonstrates computational effectiveness by reducing model size and resource\nconsumption compared to existing approaches. (3) Generality: With the capacity\nto effectively classify a diverse set of tasks, BioMamba demonstrates\nadaptability and effectiveness across various domains and applications.",
        "3D object reconstruction from single-view image is a fundamental task in\ncomputer vision with wide-ranging applications. Recent advancements in Large\nReconstruction Models (LRMs) have shown great promise in leveraging multi-view\nimages generated by 2D diffusion models to extract 3D content. However,\nchallenges remain as 2D diffusion models often struggle to produce dense images\nwith strong multi-view consistency, and LRMs tend to amplify these\ninconsistencies during the 3D reconstruction process. Addressing these issues\nis critical for achieving high-quality and efficient 3D reconstruction. In this\npaper, we present CDI3D, a feed-forward framework designed for efficient,\nhigh-quality image-to-3D generation with view interpolation. To tackle the\naforementioned challenges, we propose to integrate 2D diffusion-based view\ninterpolation into the LRM pipeline to enhance the quality and consistency of\nthe generated mesh. Specifically, our approach introduces a Dense View\nInterpolation (DVI) module, which synthesizes interpolated images between main\nviews generated by the 2D diffusion model, effectively densifying the input\nviews with better multi-view consistency. We also design a tilt camera pose\ntrajectory to capture views with different elevations and perspectives.\nSubsequently, we employ a tri-plane-based mesh reconstruction strategy to\nextract robust tokens from these interpolated and original views, enabling the\ngeneration of high-quality 3D meshes with superior texture and geometry.\nExtensive experiments demonstrate that our method significantly outperforms\nprevious state-of-the-art approaches across various benchmarks, producing 3D\ncontent with enhanced texture fidelity and geometric accuracy.",
        "The quantum description of a black hole predicts that quantum information\nhidden behind the event horizon can be teleported outside almost\ninstantaneously. In this work, we demonstrate that a chiral spin-chain model,\nwhich naturally simulates a binary black hole system, can realise this\nteleportation process. Our system captures two essential components of this\nprotocol: Hawking radiation, which generates the necessary entanglement between\nthe black holes, and optimal scrambling, which enables high-fidelity\nteleportation on short timescales. Through numerical simulations, we quantify\nthe key timescales governing the process, including the Page time, radiation\ntime, scrambling time, and butterfly velocity, showing their universal\ndependence on the chiral coupling strength. Our results establish the\nfeasibility of simulating quantum properties of black holes within condensed\nmatter systems, offering an experimentally accessible platform for probing\notherwise inaccessible high-energy phenomena.",
        "Large Language Models (LLMs) have shown remarkable capabilities as AI agents.\nHowever, existing methods for enhancing LLM-agent abilities often lack a focus\non data quality, leading to inefficiencies and suboptimal results in both\nfine-tuning and prompt engineering. To address this issue, we introduce EDGE, a\nnovel approach for identifying informative samples without needing golden\nanswers. We propose the Guideline Effectiveness (GE) metric, which selects\nchallenging samples by measuring the impact of human-provided guidelines in\nmulti-turn interaction tasks. A low GE score indicates that the human expertise\nrequired for a sample is missing from the guideline, making the sample more\ninformative. By selecting samples with low GE scores, we can improve the\nefficiency and outcomes of both prompt engineering and fine-tuning processes\nfor LLMs. Extensive experiments validate the performance of our method. Our\nmethod achieves competitive results on the HotpotQA and WebShop and datasets,\nrequiring 75\\% and 50\\% less data, respectively, while outperforming existing\nmethods. We also provide a fresh perspective on the data quality of LLM-agent\nfine-tuning.",
        "Automated Essay Scoring (AES) plays a crucial role in educational assessment\nby providing scalable and consistent evaluations of writing tasks. However,\ntraditional AES systems face three major challenges: (1) reliance on\nhandcrafted features that limit generalizability, (2) difficulty in capturing\nfine-grained traits like coherence and argumentation, and (3) inability to\nhandle multimodal contexts. In the era of Multimodal Large Language Models\n(MLLMs), we propose EssayJudge, the first multimodal benchmark to evaluate AES\ncapabilities across lexical-, sentence-, and discourse-level traits. By\nleveraging MLLMs' strengths in trait-specific scoring and multimodal context\nunderstanding, EssayJudge aims to offer precise, context-rich evaluations\nwithout manual feature engineering, addressing longstanding AES limitations.\nOur experiments with 18 representative MLLMs reveal gaps in AES performance\ncompared to human evaluation, particularly in discourse-level traits,\nhighlighting the need for further advancements in MLLM-based AES research. Our\ndataset and code will be available upon acceptance.",
        "A series of recent papers by Bergfalk, Lupini and Panagiotopoulus developed\nthe foundations of a field known as `definable algebraic topology,' in which\nclassical cohomological invariants are enriched by viewing them as groups with\na Polish cover. This allows one to apply techniques from descriptive set theory\nto the study of cohomology theories. In this paper, we will establish a\n`definable' version of a classical theorem from obstruction theory, and use\nthis to study the potential complexity of the homotopy relation on the space of\ncontinuous maps $C(X, |K|)$, where $X$ is a locally compact Polish space, and K\nis a locally finite countable simplicial complex. We will also characterize the\nSolecki Groups of the Cech cohomology of X, which are the canonical chain of\nsubgroups with a Polish cover that are least among those of a given complexity.",
        "We present progress on the calculation of scalar, vector, and tensor form\nfactors for the following meson decays: $B\\rightarrow\\pi$, $B_s\\rightarrow K$,\n$D\\rightarrow\\pi$ and $D_s\\rightarrow K$. This calculation uses the MILC HISQ\ngluon field ensembles with HISQ valence quarks. We generate ensembles of\ncorrelator data with varying lattice spacings, some as small as 0.044 fm. Some\nensembles have a strange-to-light quark mass ratio of 5:1 and others use the\nphysical light quark mass. The fully-relativistic, heavy-HISQ approach is used\nfor the heavy quark, with simulation masses ranging from the charm to near the\nbottom. This heavy-HISQ approach provides nearly full coverage of the kinematic\nrange."
      ]
    }
  },
  {
    "id":2411.10403,
    "research_type":"applied",
    "start_id":"b13",
    "start_title":"Cardiovascular Flow Measurement with Phase-Contrast MR Imaging: Basic Facts and Implementation",
    "start_abstract":"Phase-contrast magnetic resonance (MR) imaging is a well-known but undervalued method of obtaining quantitative information on blood flow. Applications this technique in cardiovascular MR are expanding. According to the sequences available, phase-contrast measurement can be performed breath hold or during normal respiration. Prospective as well retrospective gating techniques used. Common errors include mismatched encoding velocity, deviation plane, inadequate temporal resolution, spatial accelerated flow and misregistration, phase offset errors. Flow measurements most precise if plane perpendicular vessel interest set through-plane The sequence should repeated at least once, with high velocity used initially. If peak has estimated, an adapted velocity. overall error comprises prescription that occur image analysis data. With imaging, reduced less than 10%, acceptable level for routine clinical use.",
    "start_categories":[
      "q-bio.QM"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b14",
        "b1"
      ],
      "title":[
        "Sparse MRI: The application of compressed sensing for rapid MR imaging",
        "MoDL: Model-Based Deep Learning Architecture for Inverse Problems"
      ],
      "abstract":[
        "Abstract The sparsity which is implicit in MR images exploited to significantly undersample k \u2010space. Some such as angiograms are already sparse the pixel representation; other, more complicated have a representation some transform domain\u2013for example, terms of spatial finite\u2010differences or their wavelet coefficients. According recently developed mathematical theory compressed\u2010sensing, with can be recovered from randomly undersampled \u2010space data, provided an appropriate nonlinear recovery scheme used. Intuitively, artifacts due random undersampling add noise\u2010like interference. In domain significant coefficients stand out above A thresholding recover coefficients, effectively recovering image itself. this article, practical incoherent schemes and analyzed by means aliasing Incoherence introduced pseudo\u2010random variable\u2010density phase\u2010encodes. reconstruction performed minimizing \u2113 1 norm transformed image, subject data fidelity constraints. Examples demonstrate improved resolution accelerated acquisition for multislice fast spin\u2010echo brain imaging 3D contrast enhanced angiography. Magn Reson Med, 2007. \u00a9 2007 Wiley\u2010Liss, Inc.",
        "We introduce a model-based image reconstruction framework with convolution neural network (CNN)-based regularization prior. The proposed formulation provides systematic approach for deriving deep architectures inverse problems the arbitrary structure. Since forward model is explicitly accounted for, smaller fewer parameters sufficient to capture information compared direct inversion approaches. Thus, reducing demand training data and time. we rely on end-to-end weight sharing across iterations, CNN weights are customized model, thus offering improved performance over approaches that pre-trained denoisers. Our experiments show decoupling of number iterations from complexity offered by this benefits, including lower data, reduced risk overfitting, implementations significantly memory footprint. propose enforce data-consistency using numerical optimization blocks, such as conjugate gradients algorithm within network. This offers faster convergence per iteration, methods proximal steps consistency. translates performance, primarily when available GPU restricts iterations."
      ],
      "categories":[
        "cs.LG",
        "stat.CO"
      ]
    },
    "list":{
      "title":[
        "Visualizing Machine Learning Models for Enhanced Financial\n  Decision-Making and Risk Management",
        "Geodesic Variational Bayes for Multiway Covariances",
        "Describing Nonstationary Data Streams in Frequency Domain",
        "End-to-End triplet loss based fine-tuning for network embedding in\n  effective PII detection",
        "SMPR: A structure-enhanced multimodal drug-disease prediction model for\n  drug repositioning and cold start",
        "Globality Strikes Back: Rethinking the Global Knowledge of CLIP in\n  Training-Free Open-Vocabulary Semantic Segmentation",
        "TabFSBench: Tabular Benchmark for Feature Shifts in Open Environment",
        "Unveiling the Power of Noise Priors: Enhancing Diffusion Models for\n  Mobile Traffic Prediction",
        "Sparse Autoencoders Can Interpret Randomly Initialized Transformers",
        "HMCGeo: IP Region Prediction Based on Hierarchical Multi-label\n  Classification",
        "LLM360 K2: Building a 65B 360-Open-Source Large Language Model from\n  Scratch",
        "LLM-TabFlow: Synthetic Tabular Data Generation with Inter-column Logical\n  Relationship Preservation",
        "CausalGeD: Blending Causality and Diffusion for Spatial Gene Expression\n  Generation",
        "Pressure-Induced Structural and Dielectric Changes in Liquid Water at\n  Room Temperature",
        "AugGen: Synthetic Augmentation Can Improve Discriminative Models",
        "CAPOS: The bulge Cluster APOGEE Survey VII: First detailed chemical\n  analysis of NGC 6316",
        "Single-Satellite-Based Geolocation of Broadcast GNSS Spoofers from Low\n  Earth Orbit",
        "The Impact of Building-Induced Visibility Restrictions on Intersection\n  Accidents",
        "Scientific literature cited in patents: A Technology Transfer indicator\n  in Portuguese universities",
        "An atomistic approach for modeling of polarizability and Raman\n  scattering of water clusters and liquid water",
        "VEGA: Voids idEntification using Genetic Algorithm",
        "Sobol-CPI: a Doubly Robust Conditional Permutation Importance Statistic",
        "AdaCS: Adaptive Normalization for Enhanced Code-Switching ASR",
        "An Optimal Cascade Feature-Level Spatiotemporal Fusion Strategy for\n  Anomaly Detection in CAN Bus",
        "X-ray Thomson scattering studies on spin-singlet stabilization of highly\n  compressed H-like Be ions heated to two million degrees Kelvin",
        "Joint Optimization of Resource Allocation and Radar Receiver Selection\n  in Integrated Communication-Radar Systems",
        "Personalize Your LLM: Fake it then Align it"
      ],
      "abstract":[
        "This study emphasizes how crucial it is to visualize machine learning models,\nespecially for the banking industry, in order to improve interpretability and\nsupport predictions in high stakes financial settings. Visual tools enable\nperformance improvements and support the creation of innovative financial\nmodels by offering crucial insights into the algorithmic decision-making\nprocesses. Within a financial machine learning framework, the research uses\nvisually guided experiments to make important concepts, such risk assessment\nand portfolio allocation, more understandable. The study also examines\nvariations in trading tactics and how they relate to risk appetite, coming to\nthe conclusion that the frequency of portfolio rebalancing is negatively\ncorrelated with risk tolerance. Finding these ideas is made possible in large\npart by visualization. The study concludes by presenting a novel method of\nlocally stochastic asset weighing, where visualization facilitates data\nextraction and validation. This highlights the usefulness of these methods in\nfurthering the field of financial machine learning research.",
        "This article explores the optimization of variational approximations for\nposterior covariances of Gaussian multiway arrays. To achieve this, we\nestablish a natural differential geometric optimization framework on the space\nusing the pullback of the affine-invariant metric. In the case of a truly\nseparable covariance, we demonstrate a joint approximation in the multiway\nspace outperforms a mean-field approximation in optimization efficiency and\nprovides a superior approximation to an unstructured Inverse-Wishart posterior\nunder the average Mahalanobis distance of the data while maintaining a multiway\ninterpretation. We moreover establish efficient expressions for the Euclidean\nand Riemannian gradients in both cases of the joint and mean-field\napproximation. We end with an analysis of commodity trade data.",
        "Concept drift is among the primary challenges faced by the data stream\nprocessing methods. The drift detection strategies, designed to counteract the\nnegative consequences of such changes, often rely on analyzing the problem\nmetafeatures. This work presents the Frequency Filtering Metadescriptor -- a\ntool for characterizing the data stream that searches for the informative\nfrequency components visible in the sample's feature vector. The frequencies\nare filtered according to their variance across all available data batches. The\npresented solution is capable of generating a metadescription of the data\nstream, separating chunks into groups describing specific concepts on its\nbasis, and visualizing the frequencies in the original spatial domain. The\nexperimental analysis compared the proposed solution with two state-of-the-art\nstrategies and with the PCA baseline in the post-hoc concept identification\ntask. The research is followed by the identification of concepts in the\nreal-world data streams. The generalization in the frequency domain adapted in\nthe proposed solution allows to capture the complex feature dependencies as a\nreduced number of frequency components, while maintaining the semantic meaning\nof data.",
        "There are many approaches in mobile data ecosystem that inspect network\ntraffic generated by applications running on user's device to detect personal\ndata exfiltration from the user's device. State-of-the-art methods rely on\nfeatures extracted from HTTP requests and in this context, machine learning\ninvolves training classifiers on these features and making predictions using\nlabelled packet traces. However, most of these methods include external feature\nselection before model training. Deep learning, on the other hand, typically\ndoes not require such techniques, as it can autonomously learn and identify\npatterns in the data without external feature extraction or selection\nalgorithms. In this article, we propose a novel deep learning based end-to-end\nlearning framework for prediction of exposure of personally identifiable\ninformation (PII) in mobile packets. The framework employs a pre-trained large\nlanguage model (LLM) and an autoencoder to generate embedding of network\npackets and then uses a triplet-loss based fine-tuning method to train the\nmodel, increasing detection effectiveness using two real-world datasets. We\ncompare our proposed detection framework with other state-of-the-art works in\ndetecting PII leaks from user's device.",
        "Repositioning drug-disease relationships has always been a hot field of\nresearch. However, actual cases of biologically validated drug relocation\nremain very limited, and existing models have not yet fully utilized the\nstructural information of the drug. Furthermore, most repositioning models are\nonly used to complete the relationship matrix, and their practicality is poor\nwhen dealing with drug cold start problems. This paper proposes a\nstructure-enhanced multimodal relationship prediction model (SMRP). SMPR is\nbased on the SMILE structure of the drug, using the Mol2VEC method to generate\ndrug embedded representations, and learn disease embedded representations\nthrough heterogeneous network graph neural networks. Ultimately, a drug-disease\nrelationship matrix is constructed. In addition, to reduce the difficulty of\nusers' use, SMPR also provides a cold start interface based on structural\nsimilarity based on reposition results to simply and quickly predict\ndrug-related diseases. The repositioning ability and cold start capability of\nthe model are verified from multiple perspectives. While the AUC and ACUPR\nscores of repositioning reach 99% and 61% respectively, the AUC of cold start\nachieve 80%. In particular, the cold start Recall indicator can reach more than\n70%, which means that SMPR is more sensitive to positive samples. Finally, case\nanalysis is used to verify the practical value of the model and visual analysis\ndirectly demonstrates the improvement of the structure to the model. For quick\nuse, we also provide local deployment of the model and package it into an\nexecutable program.",
        "Recent works modify CLIP to perform open-vocabulary semantic segmentation in\na training-free manner (TF-OVSS). In CLIP, patch-wise image representations\nmainly encode the homogeneous image-level properties and thus are not\ndiscriminative enough, hindering its application to the dense prediction task.\nPrevious works make image features more distinct across patches, through making\neach patch mainly attend to itself or the neighboring patches within a narrow\nlocal window. However, with their modifications, the ability of CLIP to\naggregate global context information, which is known to be useful for\ndistinguishing confusing categories, is largely weakened. In this paper, we\npropose a new method named GCLIP, which mines the beneficial global knowledge\nof CLIP to facilitate the TF-OVSS task. Firstly, we aim to equip the last-block\nattention with image-level properties while not introducing homogeneous\nattention patterns across patches. In GCLIP, we merge the attention from the\nglobal token emerging blocks with the Query-Query attention to realize this\ngoal. Secondly, we aim to make the Value embeddings of the last-block attention\nmodule more distinct and semantically correlated. To realize this, we design a\nnovel channel suppression strategy. As the representation of each patch is\nfinally determined by the attention weights and the Value embeddings, our\nmethod can generate more discriminative patch-level image features while\nabsorbing global context information. Extensive experiments on five standard\nbenchmarks demonstrate that our method consistently outperforms previous\nstate-of-the-arts.",
        "Tabular data is widely utilized in various machine learning tasks. Current\ntabular learning research predominantly focuses on closed environments, while\nin real-world applications, open environments are often encountered, where\ndistribution and feature shifts occur, leading to significant degradation in\nmodel performance. Previous research has primarily concentrated on mitigating\ndistribution shifts, whereas feature shifts, a distinctive and unexplored\nchallenge of tabular data, have garnered limited attention. To this end, this\npaper conducts the first comprehensive study on feature shifts in tabular data\nand introduces the first tabular feature-shift benchmark (TabFSBench).\nTabFSBench evaluates impacts of four distinct feature-shift scenarios on four\ntabular model categories across various datasets and assesses the performance\nof large language models (LLMs) and tabular LLMs in the tabular benchmark for\nthe first time. Our study demonstrates three main observations: (1) most\ntabular models have the limited applicability in feature-shift scenarios; (2)\nthe shifted feature set importance has a linear relationship with model\nperformance degradation; (3) model performance in closed environments\ncorrelates with feature-shift performance. Future research direction is also\nexplored for each observation. TabFSBench is released for public access by\nusing a few lines of Python codes at https:\/\/github.com\/LAMDASZ-ML\/TabFSBench.",
        "Accurate prediction of mobile traffic, \\textit{i.e.,} network traffic from\ncellular base stations, is crucial for optimizing network performance and\nsupporting urban development. However, the non-stationary nature of mobile\ntraffic, driven by human activity and environmental changes, leads to both\nregular patterns and abrupt variations. Diffusion models excel in capturing\nsuch complex temporal dynamics due to their ability to capture the inherent\nuncertainties. Most existing approaches prioritize designing novel denoising\nnetworks but often neglect the critical role of noise itself, potentially\nleading to sub-optimal performance. In this paper, we introduce a novel\nperspective by emphasizing the role of noise in the denoising process. Our\nanalysis reveals that noise fundamentally shapes mobile traffic predictions,\nexhibiting distinct and consistent patterns. We propose NPDiff, a framework\nthat decomposes noise into \\textit{prior} and \\textit{residual} components,\nwith the \\textit{prior} derived from data dynamics, enhancing the model's\nability to capture both regular and abrupt variations. NPDiff can seamlessly\nintegrate with various diffusion-based prediction models, delivering\npredictions that are effective, efficient, and robust. Extensive experiments\ndemonstrate that it achieves superior performance with an improvement over\n30\\%, offering a new perspective on leveraging diffusion models in this domain.",
        "Sparse autoencoders (SAEs) are an increasingly popular technique for\ninterpreting the internal representations of transformers. In this paper, we\napply SAEs to 'interpret' random transformers, i.e., transformers where the\nparameters are sampled IID from a Gaussian rather than trained on text data. We\nfind that random and trained transformers produce similarly interpretable SAE\nlatents, and we confirm this finding quantitatively using an open-source\nauto-interpretability pipeline. Further, we find that SAE quality metrics are\nbroadly similar for random and trained transformers. We find that these results\nhold across model sizes and layers. We discuss a number of number interesting\nquestions that this work raises for the use of SAEs and auto-interpretability\nin the context of mechanistic interpretability.",
        "Fine-grained IP geolocation plays a critical role in applications such as\nlocation-based services and cybersecurity. Most existing fine-grained IP\ngeolocation methods are regression-based; however, due to noise in the input\ndata, these methods typically encounter kilometer-level prediction errors and\nprovide incorrect region information for users. To address this issue, this\npaper proposes a novel hierarchical multi-label classification framework for IP\nregion prediction, named HMCGeo. This framework treats IP geolocation as a\nhierarchical multi-label classification problem and employs residual\nconnection-based feature extraction and attention prediction units to predict\nthe target host region across multiple geographical granularities. Furthermore,\nwe introduce probabilistic classification loss during training, combining it\nwith hierarchical cross-entropy loss to form a composite loss function. This\napproach optimizes predictions by utilizing hierarchical constraints between\nregions at different granularities. IP region prediction experiments on the New\nYork, Los Angeles, and Shanghai datasets demonstrate that HMCGeo achieves\nsuperior performance across all geographical granularities, significantly\noutperforming existing IP geolocation methods.",
        "We detail the training of the LLM360 K2-65B model, scaling up our 360-degree\nOPEN SOURCE approach to the largest and most powerful models under project\nLLM360. While open-source LLMs continue to advance, the answer to \"How are the\nlargest LLMs trained?\" remains unclear within the community. The implementation\ndetails for such high-capacity models are often protected due to business\nconsiderations associated with their high cost. This lack of transparency\nprevents LLM researchers from leveraging valuable insights from prior\nexperience, e.g., \"What are the best practices for addressing loss spikes?\" The\nLLM360 K2 project addresses this gap by providing full transparency and access\nto resources accumulated during the training of LLMs at the largest scale. This\nreport highlights key elements of the K2 project, including our first model, K2\nDIAMOND, a 65 billion-parameter LLM that surpasses LLaMA-65B and rivals\nLLaMA2-70B, while requiring fewer FLOPs and tokens. We detail the\nimplementation steps and present a longitudinal analysis of K2 DIAMOND's\ncapabilities throughout its training process. We also outline ongoing projects\nsuch as TXT360, setting the stage for future models in the series. By offering\npreviously unavailable resources, the K2 project also resonates with the\n360-degree OPEN SOURCE principles of transparency, reproducibility, and\naccessibility, which we believe are vital in the era of resource-intensive AI\nresearch.",
        "Synthetic tabular data have widespread applications in industrial domains\nsuch as healthcare, finance, and supply chains, owing to their potential to\nprotect privacy and mitigate data scarcity. However, generating realistic\nsynthetic tabular data while preserving inter-column logical relationships\nremains a significant challenge for the existing generative models. To address\nthese challenges, we propose LLM-TabFlow, a novel approach that leverages Large\nLanguage Model (LLM) reasoning to capture complex inter-column relationships\nand compress tabular data, while using Score-based Diffusion to model the\ndistribution of the compressed data in latent space. Additionally, we introduce\nan evaluation framework, which is absent in literature, to fairly assess the\nperformance of synthetic tabular data generation methods in real-world\ncontexts. Using this framework, we conduct extensive experiments on two\nreal-world industrial datasets, evaluating LLM-TabFlow against other five\nbaseline methods, including SMOTE (an interpolation-based approach) and other\nstate-of-the-art generative models. Our results show that LLM-TabFlow\noutperforms all baselines, fully preserving inter-column relationships while\nachieving the best balance between data fidelity, utility, and privacy. This\nstudy is the first to explicitly address inter-column relationship preservation\nin synthetic tabular data generation, offering new insights for developing more\nrealistic and reliable tabular data generation methods.",
        "The integration of single-cell RNA sequencing (scRNA-seq) and spatial\ntranscriptomics (ST) data is crucial for understanding gene expression in\nspatial context. Existing methods for such integration have limited\nperformance, with structural similarity often below 60\\%, We attribute this\nlimitation to the failure to consider causal relationships between genes. We\npresent CausalGeD, which combines diffusion and autoregressive processes to\nleverage these relationships. By generalizing the Causal Attention Transformer\nfrom image generation to gene expression data, our model captures regulatory\nmechanisms without predefined relationships. Across 10 tissue datasets,\nCausalGeD outperformed state-of-the-art baselines by 5- 32\\% in key metrics,\nincluding Pearson's correlation and structural similarity, advancing both\ntechnical and biological insights.",
        "Understanding the pressure-dependent dielectric properties of water is\ncrucial for a wide range of scientific and practical applications. In this\nstudy, we employ a deep neural network trained on density functional theory\ndata to investigate the dielectric properties of liquid water at room\ntemperature across a pressure range of 0.1 MPa to 1000 MPa. We observe a\nnonlinear increase in the static dielectric constant $\\epsilon_0$ with\nincreasing pressure, a trend that is qualitatively consistent with experimental\nobservations. This increase in $\\epsilon_0$ is primarily attributed to the\nincrease in water density under compression, which enhances collective dipole\nfluctuations within the hydrogen-bonding network as well as the dielectric\nresponse. Despite the increase in $\\epsilon_0$, our results reveal a decrease\nin the Kirkwood correlation factor $G_K$ with increasing pressure. This\ndecrease in $G_K$ is attributed to pressure-induced structural distortions in\nthe hydrogen-bonding network, which weaken dipolar correlations by disrupting\nthe ideal tetrahedral arrangement of water molecules.",
        "The increasing dependence on large-scale datasets in machine learning\nintroduces significant privacy and ethical challenges. Synthetic data\ngeneration offers a promising solution; however, most current methods rely on\nexternal datasets or pre-trained models, which add complexity and escalate\nresource demands. In this work, we introduce a novel self-contained synthetic\naugmentation technique that strategically samples from a conditional generative\nmodel trained exclusively on the target dataset. This approach eliminates the\nneed for auxiliary data sources. Applied to face recognition datasets, our\nmethod achieves 1--12\\% performance improvements on the IJB-C and IJB-B\nbenchmarks. It outperforms models trained solely on real data and exceeds the\nperformance of state-of-the-art synthetic data generation baselines. Notably,\nthese enhancements often surpass those achieved through architectural\nimprovements, underscoring the significant impact of synthetic augmentation in\ndata-scarce environments. These findings demonstrate that carefully integrated\nsynthetic data not only addresses privacy and resource constraints but also\nsubstantially boosts model performance. Project page\nhttps:\/\/parsa-ra.github.io\/auggen",
        "As part of the bulge Cluster APOgee Survey (CAPOS), high-resolution, high\nSignal-to-Noise Ratio Near-Infrared spectroscopy, we aim to conduct the most\nrobust chemical study to date for NGC 6316, deriving abundances for a number of\nelements with a variety of nucleosynthetic origins, most of which have never\nbeen studied before in this cluster. We use the Brussels Automatic Code for\nCharacterizing High accuracy Spectra (BACCHUS) with atmospheric parameters\nphotometrically obtained in order to determine, for the first time, abundances\nfor C, N, O, Mg, Al, Si, P, K, Ca, Ti, V, Cr, Mn, Fe, Ni and Ce for this\ncluster. We obtained a mean metallicity [Fe\/H] = -0.87 +- 0.02, finding no\nindication of an intrinsic metallicity spread. Our metallicity agrees with the\nmost recent values from other studies, revising earlier values that were ~0.5\ndex metal-richer. With this new value, this cluster, long believed to be a\nmember of the classical metal-rich group of bulge GCs around -0.5, now falls in\nthe dominant bulge globular cluster peak around [Fe\/H] = -1. The cluster\npresents a clear C-N anticorrelation. We also found a [{\\alpha}\/Fe] = 0.3 +-\n0.02. Our abundances show similar behaviour to other in situ globular clusters\nwith comparable metallicity. We obtained E(B-V) = 0.71 and (M-m)_0 = 15.32 +-\n0.05 by isochrone fitting, in good agreement with the recent determinations\nfrom other works. We derive an overall metallicity [M\/H] = -0.6 +- 0.05 by\nisochrone fitting, in agreement with our abundance determination. According to\nthe mean [Mg\/Fe] and [Al\/Fe] abundances from first population stars, NGC 6316\nis an in-situ globular cluster, in accordance with various dynamical\nclassifications.",
        "This paper presents an analysis and experimental demonstration of\nsingle-satellite single-pass geolocation of a terrestrial broadcast Global\nNavigation Satellite System (GNSS) spoofer from Low Earth Orbit (LEO). The\nproliferation of LEO-based GNSS receivers offers the prospect of unprecedented\nspectrum awareness, enabling persistent GNSS interference detection and\ngeolocation. Accurate LEO-based single-receiver emitter geolocation is possible\nwhen a range-rate time history can be extracted for the emitter. This paper\npresents a technique crafted specifically for indiscriminate broadcast-type\nGNSS spoofing signals. Furthermore, it explores how unmodeled oscillator\ninstability and worst-case spoofer-introduced signal variations degrade the\ngeolocation estimate. The proposed geolocation technique is validated by a\ncontrolled experiment, in partnership with Spire Global, in which a LEO-based\nreceiver captures broadcast GNSS spoofing signals transmitted from a known\nground station on a non-GNSS frequency band.",
        "Traffic accidents, especially at intersections, are a major road safety\nconcern. Previous research has extensively studied intersection-related\naccidents, but the effect of building-induced visibility restrictions at\nintersections on accident rates has been under-explored, particularly in urban\ncontexts. Using OpenStreetMap data, the UK's geographic and accident datasets,\nand the UK Traffic Count Dataset, we formulated a novel approach to estimate\naccident risk at intersections. This method factors in the area visible to\ndrivers, accounting for views blocked by buildings - a distinctive aspect in\ntraffic accident analysis. Our findings reveal a notable correlation between\nthe road visible percentage and accident frequency. In the model, the\ncoefficient for \"road visible percentage\" is 1.7450, implying a strong positive\nrelationship. Incorporating this visibility factor enhances the model's\nexplanatory power, with increased R-square values and reduced AIC and BIC,\nindicating a better data fit. This study underscores the essential role of\narchitectural layouts in road safety and suggests that urban planning\nstrategies should consider building-induced visibility restrictions. Such\nconsideration could be an effective approach to mitigate accident rates at\nintersections. This research opens up new avenues for innovative, data-driven\nurban planning and traffic management strategies, highlighting the importance\nof visibility enhancements for safer roads.",
        "The study aims to identify the process of transfer from science to technology\nthat occurs in the main Portuguese public universities. The methodology was\nbased on the analysis of the scientific literature cited in patents. Data was\nobtained from the Lens patent database. 10,514 scientific articles cited in\npatents were retrieved. A descriptive analysis of the data was performed.\nScience maps were created to visualize the main research trends. The results\nshowed a valuable impact of academic research in certain scientific\ndisciplines, such as Chemistry, Biology, Materials Sciences and Medicine. The\nmain research fronts were cancer, nanoparticles, biomaterials, tissue\nengineering or molecular biology. In conclusion, the research produced by\nPortuguese universities has generated relevant knowledge for patented\ninventions and the science-technology flow within specific areas.",
        "In this work, we develop a framework for atomistic modeling of electronic\npolarizability to predict the Raman spectra of hydrogen-bonded clusters and\nliquids from molecular dynamics (MD) simulations. The total polarizability of\nthe system is assumed to arise from contributions of both the monomer unit and\nintermolecular interactions. The generalized bond-polarizability model (GBPM),\ninspired by the classic bond-polarizability model, effectively describes the\nelectronic polarizability of a monomer. To account for the electronic\npolarizability arising from intermolecular interactions, we use a basis set of\nrapidly decaying functions of interatomic distances. We apply this model to\ncalculate the electronic polarizability and Raman spectra of water clusters\n((H2O)r, r = 2, 3, 4, 5, 6) and liquid water. The computational results are\ncompared with the results of quantum-mechanical calculations for clusters and\nto experimental data for the liquid. It is demonstrated that this simple and\nphysically motivated model, which relies on a small number of parameters,\nperforms well for clusters at both low and high temperatures, capturing strong\nanharmonic effects. Moreover, its high transferability suggests its\napplicability to other water clusters. These results suggest that a\nhierarchical approach based on the Jacob's ladder of increasingly sophisticated\nand accurate atomistic polarizability models incorporating additional effects\ncan be used for efficient modeling of Raman spectra from MD simulations of\nclusters, liquids and solids.",
        "Cosmic voids, the nearly empty regions nestled between walls and filaments,\nare recognized for their extensive applications in the field of cosmology and\nastrophysics. However, a consensus on the definition of voids remains elusive,\nas various void-finding methods identify different types of voids, each\ndiffering in shape and density based on the method that were used. In this\npaper, we introduce an innovative void identification method that utilizes\nGenetic Algorithm analysis. VEGA employs the Voronoi tessellation technique and\nthe Convex Hull algorithm to partition the dataset plane into distinct regions\nand calculate the volume of each region. For the first time, VEGA integrates\nGenetic Algorithm analysis with the luminosity density contrast function to\nidentify and locate the possible void region candidates. This method utilizes a\nset of grid points, which enhances the implementation of Voronoi tessellation\nand enables VEGA to more effectively access the dataset space for the\nidentification of void regions candidates, finding the center and the ultimate\nstructure of voids. Finally, we applied the VEGA and Aikio-M\\\"ah\\\"onen (AM)\nmethods to the same test dataset and compared the cosmic voids identified by\nVEGA with those identified by the AM method. This comparison demonstrated that\nthe VEGA void-finding method yields reliable results and can be effectively\napplied to various particle distributions.",
        "Conditional Permutation Importance (CPI) has been recently introduced for\nVariable Importance analysis with good empirical results. In this work, we\nfirst provide theoretical guarantees for CPI. We establish a double robustness\nproperty to detect null covariates, making it a suitable model for variable\nselection. We then present a modified and still computationally efficient\nversion, Sobol-CPI, that aims to estimate a well-known variable importance\nmeasure, the Total Sobol Index (TSI). We prove that it is nonparametrically\nefficient, and we provide a procedure to control the type-I error. Through\nnumerical experiments, we show that Sobol-CPI preserves the double robustness\nproperty in practice.",
        "Intra-sentential code-switching (CS) refers to the alternation between\nlanguages that happens within a single utterance and is a significant challenge\nfor Automatic Speech Recognition (ASR) systems. For example, when a Vietnamese\nspeaker uses foreign proper names or specialized terms within their speech. ASR\nsystems often struggle to accurately transcribe intra-sentential CS due to\ntheir training on monolingual data and the unpredictable nature of CS. This\nissue is even more pronounced for low-resource languages, where limited data\navailability hinders the development of robust models. In this study, we\npropose AdaCS, a normalization model integrates an adaptive bias attention\nmodule (BAM) into encoder-decoder network. This novel approach provides a\nrobust solution to CS ASR in unseen domains, thereby significantly enhancing\nour contribution to the field. By utilizing BAM to both identify and normalize\nCS phrases, AdaCS enhances its adaptive capabilities with a biased list of\nwords provided during inference. Our method demonstrates impressive performance\nand the ability to handle unseen CS phrases across various domains. Experiments\nshow that AdaCS outperforms previous state-of-the-art method on Vietnamese CS\nASR normalization by considerable WER reduction of 56.2% and 36.8% on the two\nproposed test sets.",
        "Autonomous vehicles represent a revolutionary advancement driven by the\nintegration of artificial intelligence within intelligent transportation\nsystems. However, they remain vulnerable due to the absence of robust security\nmechanisms in the Controller Area Network (CAN) bus. In order to mitigate the\nsecurity issue, many machine learning models and strategies have been proposed,\nwhich primarily focus on a subset of dominant patterns of anomalies and lack\nrigorous evaluation in terms of reliability and robustness. Therefore, to\naddress the limitations of previous works and mitigate the security\nvulnerability in CAN bus, the current study develops a model based on the\nintrinsic nature of the problem to cover all dominant patterns of anomalies. To\nachieve this, a cascade feature-level fusion strategy optimized by a\ntwo-parameter genetic algorithm is proposed to combine temporal and spatial\ninformation. Subsequently, the model is evaluated using a paired t-test to\nensure reliability and robustness. Finally, a comprehensive comparative\nanalysis conducted on two widely used datasets advocates that the proposed\nmodel outperforms other models and achieves superior accuracy and F1-score,\ndemonstrating the best performance among all models presented to date.",
        "Experiments at the US National Ignition Facility (NIF) [D\\\"{o}ppner et al.,\nNature {\\bf 618}, 270-275 (2023)] have created highly compressed hot\nhydrogen-like Be plasmas. Published analyses of the the NIF experiment have\nused finite-$T$ multi-atom density-functional theory (DFT) with Molecular\ndynamics (MD), and Path-Integral Monte Carlo (PIMC) simulations. These methods\nare very expensive to implement and often lack physical transparency. Here we\n(i) relate their results to simpler first-principles average-atom results, (ii)\nestablish the feasibility of rapid data analysis, with good accuracy and gain\nin physical transparency, and (iii) show that the NIF experiment reveals\nhigh-$T$ spin-singlet pairing of hydrogen-like Be ions with near neighbours.\nOur analysis predicts such stabilization over a wide range of compressed\ndensities for temperatures close to two million Kelvin. Calculations of\nstructure factors $S(k)$ for electrons or ions, the Raleigh weight and other\nquantities of interest to X-ray Thomson scattering are presented. We find that\nthe NIF data at the scattering wavevector $k_{sc}$ of 7.89 \\AA$^{-1}$ are more\nconsistent with a density of $20\\pm2$ g\/cm$^3$, mean ionization $\\bar{Z}=$3.25,\nat a temperature of $\\simeq$ 1,800,000 K than the 34 g\/cm$^3, \\bar{Z}=3.4$\nproposed by the NIF team. The relevance of ion-electron coupled-modes in\nstudying small $k_{sc}$ data is indicated.",
        "In this paper, we investigate a distributed multi-input multi-output and\northogonal frequency division multiplexing (MIMO-OFDM) dual-function\nradar-communication (DFRC) system, which enables simultaneous communication and\nsensing in different subcarrier sets. To obtain the best tradeoff between\ncommunication and sensing performance, we first derive Cramer-Rao Bound (CRB)\nof targets in the detection area, and then maximize the transmission rate by\njointly optimizing the power\/subcarriers allocation and the selection of radar\nreceivers under the constraints of detection performance and total transmit\npower. To tackle the non-convex mixed integer programming problem, we decompose\nthe original problem into a semidefinite programming (SDP) problem and a convex\nquadratic integer problem and solve them iteratively. The numerical results\ndemonstrate the effectiveness of our proposed algorithm, as well as the\nperformance improvement brought by optimizing radar receivers selection.",
        "Personalizing large language models (LLMs) is essential for delivering\ntailored interactions that improve user experience. Many existing\npersonalization methods require fine-tuning LLMs for each user, rendering them\nprohibitively expensive for widespread adoption. Although retrieval-based\napproaches offer a more compute-efficient alternative, they still depend on\nlarge, high-quality datasets that are not consistently available for all users.\nTo address this challenge, we propose CHAMELEON, a scalable and efficient\npersonalization approach that uses (1) self-generated personal preference data\nand (2) representation editing to enable quick and cost-effective\npersonalization. Our experiments on various tasks, including those from the\nLaMP personalization benchmark, show that CHAMELEON efficiently adapts models\nto personal preferences, improving instruction-tuned models and outperforms two\npersonalization baselines by an average of 40% across two model architectures."
      ]
    }
  },
  {
    "id":2411.15202,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"The Great Barrier Reef: an environmental history",
    "start_abstract":"Reconstructing changes in the Great Barrier Reef 15 3 The natural context of 33 4 spread European settlement coastal Queensland 43 5 beche--de ... mer, pearl shell and trochus fisheries 55 6 Impacts on marine turtles 72 7 dugongs 95 8 whales, sharks fish 9 impacts coral collecting 10 guano rock phosphate mining 11 vi Contents 12 Other reefs 13 Changes island biota 14 Conclusion",
    "start_categories":[
      "q-bio.QM"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b2"
      ],
      "title":[
        "Ilf-lstm: Enhanced loss function in lstm to predict the sea surface temperature"
      ],
      "abstract":[
        "Globe's primary issue is global warming, water temperatures have accompanied it as the sea surface temperature, and it is the primary attribute to balance the energy on the earth's surface. Sea surface temperature prediction is vital to climate forecast. Downwelling currents carry some of this heat to the ocean's bottom layers, which are also heating, covering far behind the increase in sea surface temperature. In deep learning models, the correct loss function will try to reduce the error and converge fast. The proposed improved loss function correctly estimates how close the predictions made by the long short-term memory match the observed values in the training data. This research considers location-specific sea surface temperature predictions using the improved loss function in the long short-term memory neural network at six different locations around India for daily, weekly, and monthly time horizons. Most existing research concentrated on periodic forecasts, but this paper focused on daily, weekly, and monthly predictions. The improved loss function\u2014long short-term memory, achieved 98.7% accuracy, and this improved loss function overcomes the limitations of the existing techniques and reduces the processing time to\u2009~\u20090.35 s. In this research, the sea surface temperature prediction using the improved loss function in the long short-term memory neural network gives better results than the standard prediction models and other existing techniques by considering the long-time dependencies and obtaining features from the spatial data."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "U-Fair: Uncertainty-based Multimodal Multitask Learning for Fairer\n  Depression Detection",
        "MAD-MAX: Modular And Diverse Malicious Attack MiXtures for Automated LLM\n  Red Teaming",
        "Sundial: A Family of Highly Capable Time Series Foundation Models",
        "Minimal Ranks, Maximum Confidence: Parameter-efficient Uncertainty\n  Quantification for LoRA",
        "Evaluating the Systematic Reasoning Abilities of Large Language Models\n  through Graph Coloring",
        "CLDyB: Towards Dynamic Benchmarking for Continual Learning with\n  Pre-trained Models",
        "AdditiveLLM: Large Language Models Predict Defects in Additive\n  Manufacturing",
        "Policy Teaching via Data Poisoning in Learning from Human Preferences",
        "Model-agnostic Coreset Selection via LLM-based Concept Bottlenecks",
        "E2ESlack: An End-to-End Graph-Based Framework for Pre-Routing Slack\n  Prediction",
        "GreenAuto: An Automated Platform for Sustainable AI Model Design on Edge\n  Devices",
        "DPFAGA-Dynamic Power Flow Analysis and Fault Characteristics: A Graph\n  Attention Neural Network",
        "SEMU: Singular Value Decomposition for Efficient Machine Unlearning",
        "Resolution Invariant Autoencoder",
        "Extending Dense Passage Retrieval with Temporal Information",
        "Exploring constraints on the core radius and density jumps inside Earth\n  using atmospheric neutrino oscillations",
        "The Kodaira dimension of Hilbert modular threefolds",
        "Some NP Complete Problems Based on Algebra and Algebraic Geometry",
        "Lagrangian chaos and unique ergodicity for stochastic primitive\n  equations",
        "Simpliciality of vector-valued function spaces",
        "Graphy'our Data: Towards End-to-End Modeling, Exploring and Generating\n  Report from Raw Data",
        "Block Flow: Learning Straight Flow on Data Blocks",
        "LiDAR Remote Sensing Meets Weak Supervision: Concepts, Methods, and\n  Perspectives",
        "Training Consistency Models with Variational Noise Coupling",
        "Formulas as Processes, Deadlock-Freedom as Choreographies (Extended\n  Version)",
        "CoMoGaussian: Continuous Motion-Aware Gaussian Splatting from\n  Motion-Blurred Images",
        "Optimal Control of Fluid Restless Multi-armed Bandits: A Machine\n  Learning Approach",
        "Central-moment-based discrete Boltzmann modeling of compressible flows"
      ],
      "abstract":[
        "Machine learning bias in mental health is becoming an increasingly pertinent\nchallenge. Despite promising efforts indicating that multitask approaches often\nwork better than unitask approaches, there is minimal work investigating the\nimpact of multitask learning on performance and fairness in depression\ndetection nor leveraged it to achieve fairer prediction outcomes. In this work,\nwe undertake a systematic investigation of using a multitask approach to\nimprove performance and fairness for depression detection. We propose a novel\ngender-based task-reweighting method using uncertainty grounded in how the\nPHQ-8 questionnaire is structured. Our results indicate that, although a\nmultitask approach improves performance and fairness compared to a unitask\napproach, the results are not always consistent and we see evidence of negative\ntransfer and a reduction in the Pareto frontier, which is concerning given the\nhigh-stake healthcare setting. Our proposed approach of gender-based\nreweighting with uncertainty improves performance and fairness and alleviates\nboth challenges to a certain extent. Our findings on each PHQ-8 subitem task\ndifficulty are also in agreement with the largest study conducted on the PHQ-8\nsubitem discrimination capacity, thus providing the very first tangible\nevidence linking ML findings with large-scale empirical population studies\nconducted on the PHQ-8.",
        "With LLM usage rapidly increasing, their vulnerability to jailbreaks that\ncreate harmful outputs are a major security risk. As new jailbreaking\nstrategies emerge and models are changed by fine-tuning, continuous testing for\nsecurity vulnerabilities is necessary. Existing Red Teaming methods fall short\nin cost efficiency, attack success rate, attack diversity, or extensibility as\nnew attack types emerge. We address these challenges with Modular And Diverse\nMalicious Attack MiXtures (MAD-MAX) for Automated LLM Red Teaming. MAD-MAX uses\nautomatic assignment of attack strategies into relevant attack clusters,\nchooses the most relevant clusters for a malicious goal, and then combines\nstrategies from the selected clusters to achieve diverse novel attacks with\nhigh attack success rates. MAD-MAX further merges promising attacks together at\neach iteration of Red Teaming to boost performance and introduces a similarity\nfilter to prune out similar attacks for increased cost efficiency. The MAD-MAX\napproach is designed to be easily extensible with newly discovered attack\nstrategies and outperforms the prominent Red Teaming method Tree of Attacks\nwith Pruning (TAP) significantly in terms of Attack Success Rate (ASR) and\nqueries needed to achieve jailbreaks. MAD-MAX jailbreaks 97% of malicious goals\nin our benchmarks on GPT-4o and Gemini-Pro compared to TAP with 66%. MAD-MAX\ndoes so with only 10.9 average queries to the target LLM compared to TAP with\n23.3.\n  WARNING: This paper contains contents which are offensive in nature.",
        "We introduce Sundial, a family of native, flexible, and scalable time series\nfoundation models. To predict the next-patch's distribution, we propose a\nTimeFlow Loss based on flow-matching, which facilitates native pre-training of\nTransformers on time series without discrete tokenization. Conditioned on\narbitrary-length time series, our model is pre-trained without specifying any\nprior distribution and can generate multiple probable predictions, achieving\nflexibility in representation learning beyond using parametric densities.\nTowards time series foundation models, we leverage minimal but crucial\nadaptations of Transformers and curate TimeBench with 1 trillion time points,\ncomprising mostly real-world datasets and synthetic data. By mitigating mode\ncollapse through TimeFlow Loss, we pre-train a family of Sundial models on\nTimeBench, which exhibit unprecedented model capacity and generalization\nperformance on zero-shot forecasting. In addition to presenting good scaling\nbehavior, Sundial achieves new state-of-the-art on both point forecasting and\nprobabilistic forecasting benchmarks. We believe that Sundial's pioneering\ngenerative paradigm will facilitate a wide variety of forecasting scenarios.",
        "Low-Rank Adaptation (LoRA) enables parameter-efficient fine-tuning of large\nlanguage models by decomposing weight updates into low-rank matrices,\nsignificantly reducing storage and computational overhead. While effective,\nstandard LoRA lacks mechanisms for uncertainty quantification, leading to\noverconfident and poorly calibrated models. Bayesian variants of LoRA address\nthis limitation, but at the cost of a significantly increased number of\ntrainable parameters, partially offsetting the original efficiency gains.\nAdditionally, these models are harder to train and may suffer from unstable\nconvergence.\n  In this work, we propose a novel parameter-efficient Bayesian LoRA,\ndemonstrating that effective uncertainty quantification can be achieved in very\nlow-dimensional parameter spaces. The proposed method achieves strong\nperformance with improved calibration and generalization while maintaining\ncomputational efficiency. Our empirical findings show that, with the\nappropriate projection of the weight space: (1) uncertainty can be effectively\nmodeled in a low-dimensional space, and (2) weight covariances exhibit low\nranks.",
        "Contemporary large language models are powerful problem-solving tools, but\nthey exhibit weaknesses in their reasoning abilities which ongoing research\nseeks to mitigate. We investigate graph coloring as a means of evaluating an\nLLM's capacities for systematic step-by-step reasoning and possibility space\nexploration, as well as effects of semantic problem framing. We test Claude 3.5\nSonnet, Llama 3.1 405B, Gemini 1.5 Pro, GPT-4o, o1-mini, and DeepSeek-R1 on a\ndataset of $k$-coloring problems with $2 \\leq k \\leq 4$ and vertex count $4\n\\leq n \\leq 8$, using partial algorithmic solvers to further categorize\nproblems by difficulty. In addition to substantial but varying framing effects,\nwe find that all models except o1-mini and R1 exhibit $>60\\%$ error rates on\ndifficult problem types in all frames ($>15\\%$ for o1-mini and $>10\\%$ for R1),\nand no model achieves perfect accuracy even in the simple domain of 2-coloring\n4-vertex graphs. Our results highlight both the considerable recent progress in\nLLM systematic reasoning and the limits of its reliability, especially in\nrelation to increasing computational costs. We expect that more complex graph\ncoloring problems, and procedural generation of arbitrary-complexity reasoning\nproblems more broadly, offer further untapped potential for LLM benchmarking.",
        "The advent of the foundation model era has sparked significant research\ninterest in leveraging pre-trained representations for continual learning (CL),\nyielding a series of top-performing CL methods on standard evaluation\nbenchmarks. Nonetheless, there are growing concerns regarding potential data\ncontamination during the pre-training stage. Furthermore, standard evaluation\nbenchmarks, which are typically static, fail to capture the complexities of\nreal-world CL scenarios, resulting in saturated performance. To address these\nissues, we describe CL on dynamic benchmarks (CLDyB), a general computational\nframework based on Markov decision processes for evaluating CL methods\nreliably. CLDyB dynamically identifies inherently difficult and\nalgorithm-dependent tasks for the given CL methods, and determines challenging\ntask orders using Monte Carlo tree search. Leveraging CLDyB, we first conduct a\njoint evaluation of multiple state-of-the-art CL methods, leading to a set of\ncommonly challenging and generalizable task sequences where existing CL methods\ntend to perform poorly. We then conduct separate evaluations of individual CL\nmethods using CLDyB, discovering their respective strengths and weaknesses. The\nsource code and generated task sequences are publicly accessible at\nhttps:\/\/github.com\/szc12153\/CLDyB.",
        "In this work we investigate the ability of large language models to predict\nadditive manufacturing defect regimes given a set of process parameter inputs.\nFor this task we utilize a process parameter defect dataset to fine-tune a\ncollection of models, titled AdditiveLLM, for the purpose of predicting\npotential defect regimes including Keyholing, Lack of Fusion, and Balling. We\ncompare different methods of input formatting in order to gauge the model's\nperformance to correctly predict defect regimes on our sparse Baseline dataset\nand our natural language Prompt dataset. The model displays robust predictive\ncapability, achieving an accuracy of 93\\% when asked to provide the defect\nregimes associated with a set of process parameters. The incorporation of\nnatural language input further simplifies the task of process parameters\nselection, enabling users to identify optimal settings specific to their build.",
        "We study data poisoning attacks in learning from human preferences. More\nspecifically, we consider the problem of teaching\/enforcing a target policy\n$\\pi^\\dagger$ by synthesizing preference data. We seek to understand the\nsusceptibility of different preference-based learning paradigms to poisoned\npreference data by analyzing the number of samples required by the attacker to\nenforce $\\pi^\\dagger$. We first propose a general data poisoning formulation in\nlearning from human preferences and then study it for two popular paradigms,\nnamely: (a) reinforcement learning from human feedback (RLHF) that operates by\nlearning a reward model using preferences; (b) direct preference optimization\n(DPO) that directly optimizes policy using preferences. We conduct a\ntheoretical analysis of the effectiveness of data poisoning in a setting where\nthe attacker is allowed to augment a pre-existing dataset and also study its\nspecial case where the attacker can synthesize the entire preference dataset\nfrom scratch. As our main results, we provide lower\/upper bounds on the number\nof samples required to enforce $\\pi^\\dagger$. Finally, we discuss the\nimplications of our results in terms of the susceptibility of these learning\nparadigms under such data poisoning attacks.",
        "Coreset Selection (CS) identifies a subset of training data that achieves\nmodel performance comparable to using the entire dataset. Many state-of-the-art\nCS methods, select coresets using scores whose computation requires training\nthe downstream model on the entire dataset and recording changes in its\nbehavior on samples as it trains (training dynamics). These scores are\ninefficient to compute and hard to interpret as they do not indicate whether a\nsample is difficult to learn in general or only for a specific model. Our work\naddresses these challenges by proposing an interpretable score that gauges a\nsample's difficulty using human-understandable textual attributes (concepts)\nindependent of any downstream model. Specifically, we measure the alignment\nbetween a sample's visual features and concept bottlenecks, derived via large\nlanguage models, by training a linear concept bottleneck layer and compute the\nsample's difficulty score using it. We then use this score and a stratified\nsampling strategy to identify the coreset. Crucially, our score is efficiently\ncomputable without training the downstream model on the full dataset even once,\nleads to high-performing coresets for various downstream models, and is\ncomputable even for an unlabeled dataset. Through experiments on CIFAR-10,\nCIFAR-100, and ImageNet-1K, we show our coresets outperform random subsets,\neven at high pruning rates, and achieve model performance comparable to or\nbetter than coresets found by training dynamics-based methods.",
        "Pre-routing slack prediction remains a critical area of research in\nElectronic Design Automation (EDA). Despite numerous machine learning-based\napproaches targeting this task, there is still a lack of a truly end-to-end\nframework that engineers can use to obtain TNS\/WNS metrics from raw circuit\ndata at the placement stage. Existing works have demonstrated effectiveness in\nArrival Time (AT) prediction but lack a mechanism for Required Arrival Time\n(RAT) prediction, which is essential for slack prediction and obtaining TNS\/WNS\nmetrics. In this work, we propose E2ESlack, an end-to-end graph-based framework\nfor pre-routing slack prediction. The framework includes a TimingParser that\nsupports DEF, SDF and LIB files for feature extraction and graph construction,\nan arrival time prediction model and a fast RAT estimation module. To the best\nof our knowledge, this is the first work capable of predicting path-level\nslacks at the pre-routing stage. We perform extensive experiments and\ndemonstrate that our proposed RAT estimation method outperforms the SOTA\nML-based prediction method and also pre-routing STA tool. Additionally, the\nproposed E2ESlack framework achieves TNS\/WNS values comparable to post-routing\nSTA results while saving up to 23x runtime.",
        "We present GreenAuto, an end-to-end automated platform designed for\nsustainable AI model exploration, generation, deployment, and evaluation.\nGreenAuto employs a Pareto front-based search method within an expanded neural\narchitecture search (NAS) space, guided by gradient descent to optimize model\nexploration. Pre-trained kernel-level energy predictors estimate energy\nconsumption across all models, providing a global view that directs the search\ntoward more sustainable solutions. By automating performance measurements and\niteratively refining the search process, GreenAuto demonstrates the efficient\nidentification of sustainable AI models without the need for human\nintervention.",
        "We propose the joint graph attention neural network (GAT), clustering with\nadaptive neighbors (CAN) and probabilistic graphical model for dynamic power\nflow analysis and fault characteristics. In fact, computational efficiency is\nthe main focus to enhance, whilst we ensure the performance accuracy at the\naccepted level. Note that Machine Learning (ML) based schemes have a\nrequirement of sufficient labeled data during training, which is not easily\nsatisfied in practical applications. Also, there are unknown data due to new\narrived measurements or incompatible smart devices in complex smart grid\nsystems. These problems would be resolved by our proposed GAT based framework,\nwhich models the label dependency between the network data and learns object\nrepresentations such that it could achieve the semi-supervised fault diagnosis.\nTo create the joint label dependency, we develop the graph construction from\nthe raw acquired signals by using CAN. Next, we develop the probabilistic\ngraphical model of Markov random field for graph representation, which supports\nfor the GAT based framework. We then evaluate the proposed framework in the\nuse-case application in smart grid and make a fair comparison to the existing\nmethods.",
        "While the capabilities of generative foundational models have advanced\nrapidly in recent years, methods to prevent harmful and unsafe behaviors remain\nunderdeveloped. Among the pressing challenges in AI safety, machine unlearning\n(MU) has become increasingly critical to meet upcoming safety regulations. Most\nexisting MU approaches focus on altering the most significant parameters of the\nmodel. However, these methods often require fine-tuning substantial portions of\nthe model, resulting in high computational costs and training instabilities,\nwhich are typically mitigated by access to the original training dataset.\n  In this work, we address these limitations by leveraging Singular Value\nDecomposition (SVD) to create a compact, low-dimensional projection that\nenables the selective forgetting of specific data points. We propose Singular\nValue Decomposition for Efficient Machine Unlearning (SEMU), a novel approach\ndesigned to optimize MU in two key aspects. First, SEMU minimizes the number of\nmodel parameters that need to be modified, effectively removing unwanted\nknowledge while making only minimal changes to the model's weights. Second,\nSEMU eliminates the dependency on the original training dataset, preserving the\nmodel's previously acquired knowledge without additional data requirements.\n  Extensive experiments demonstrate that SEMU achieves competitive performance\nwhile significantly improving efficiency in terms of both data usage and the\nnumber of modified parameters.",
        "Deep learning has significantly advanced medical imaging analysis, yet\nvariations in image resolution remain an overlooked challenge. Most methods\naddress this by resampling images, leading to either information loss or\ncomputational inefficiencies. While solutions exist for specific tasks, no\nunified approach has been proposed. We introduce a resolution-invariant\nautoencoder that adapts spatial resizing at each layer in the network via a\nlearned variable resizing process, replacing fixed spatial down\/upsampling at\nthe traditional factor of 2. This ensures a consistent latent space resolution,\nregardless of input or output resolution. Our model enables various downstream\ntasks to be performed on an image latent whilst maintaining performance across\ndifferent resolutions, overcoming the shortfalls of traditional methods. We\ndemonstrate its effectiveness in uncertainty-aware super-resolution,\nclassification, and generative modelling tasks and show how our method\noutperforms conventional baselines with minimal performance loss across\nresolutions.",
        "Temporal awareness is crucial in many information retrieval tasks,\nparticularly in scenarios where the relevance of documents depends on their\nalignment with the query's temporal context. Traditional retrieval methods such\nas BM25 and Dense Passage Retrieval (DPR) excel at capturing lexical and\nsemantic relevance but fall short in addressing time-sensitive queries. To\nbridge this gap, we introduce the temporal retrieval model that integrates\nexplicit temporal signals by incorporating query timestamps and document dates\ninto the representation space. Our approach ensures that retrieved passages are\nnot only topically relevant but also temporally aligned with user intent. We\nevaluate our approach on two large-scale benchmark datasets, ArchivalQA and\nChroniclingAmericaQA, achieving substantial performance gains over standard\nretrieval baselines. In particular, our model improves Top-1 retrieval accuracy\nby 6.63% and NDCG@10 by 3.79% on ArchivalQA, while yielding a 9.56% boost in\nTop-1 retrieval accuracy and 4.68% in NDCG@10 on ChroniclingAmericaQA.\nAdditionally, we introduce a time-sensitive negative sampling strategy, which\nrefines the model's ability to distinguish between temporally relevant and\nirrelevant documents during training. Our findings highlight the importance of\nexplicitly modeling time in retrieval systems and set a new standard for\nhandling temporally grounded queries.",
        "Atmospheric neutrinos, through their weak interactions, can serve as an\nindependent tool for exploring the internal structure of Earth. The information\nobtained would be complementary to that provided by seismic and gravitational\nmeasurements. The Earth matter effects in neutrino oscillations depend upon the\nenergy of neutrinos and the electron density distribution that they encounter\nduring their journey through Earth, and hence, can be used to probe the inner\nstructure of Earth. In this contribution, we demonstrate how well an\natmospheric neutrino experiment, such as an iron calorimeter detector (ICAL),\nwould simultaneously constrain the density jumps inside Earth and determine the\nlocation of the core-mantle boundary. In this work, we employ a five-layered\ndensity model of Earth, where the layer densities and core radius are modified\nto explore the parameter space, ensuring that the mass and moment of inertia of\nEarth remain constant while satisfying the hydrostatic equilibrium condition.\nWe further demonstrate that the charge identification capability of an\nICAL-like detector would play a crucial role in obtaining these correlated\nconstraints.",
        "Following a method introduced by Thomas-Vasquez and developed by Grundman, we\nprove that many Hilbert modular threefolds of arithmetic genus $0$ and $1$ are\nof general type, and that some are of nonnegative Kodaira dimension. The new\ningredient is a detailed study of the geometry and combinatorics of totally\npositive integral elements $x$ of a fractional ideal $I$ in a totally real\nnumber field $K$ with the property that $\\mathop{\\mathrm{tr}} xy <\n\\mathop{\\mathrm{min}} I \\mathop{\\mathrm{tr}} y$ for some $y \\gg 0 \\in K$.",
        "This paper describes several new problems and ideas concerning algebraic\ngeometry and complexity theory. It first uses the idea of coloring graphs with\nelements of finite fields. This procedure then shows that graph coloring\nproblems can be converted into membership problems for a new family of\nalgebraic varieties, coloring varieties, which are closely related to\ndeterminantal varieties. This in turn shows that the problem of NP vs P can be\nconverted into questions of if certain polynomials of large degree over finite\nfields have low multiplicative complexity.",
        "We show that the Lagrangian flow associated with the stochastic 3D primitive\nequations (PEs) with non-degenerate noise is chaotic, i.e., the corresponding\ntop Lyapunov exponent is strictly positive almost surely. This result builds on\nthe landmark work by Bedrossian, Blumenthal, and Punshon-Smith on Lagrangian\nchaos in stochastic fluid mechanics. Our primary contribution is establishing\nan instance where Lagrangian chaos can be proven for a fluid flow with\nsupercritical energy, a key characteristic of 3D fluid dynamics. For the 3D\nPEs, establishing the existence of the top Lyapunov exponent is already a\nchallenging task. We address this difficulty by deriving new estimates for the\ninvariant measures of the 3D PEs, which capture the anisotropic smoothing in\nthe dynamics of the PEs. As a by-product of our results, we also obtain the\nfirst uniqueness result for invariant measures of stochastic PEs.",
        "We investigate integral representation of vector-valued function spaces,\ni.e., of subspaces $H\\subset C(K,E)$, where $K$ is a compact space and $E$ is a\n(real or complex) Banach space. We point out that there are two possible ways\nof generalizing representation theorems known from the scalar case -- either\none may represent (all) functionals from $H^*$ using $E^*$-valued vector\nmeasures on $K$ (as it is done in the literature) or one may represent (some)\noperators from $L(H,E)$ by scalar measures on $K$ using the Bochner integral.\nThese two ways lead to two different notions of simpliciality which we call\n`vector simpliciality' and `weak simpliciality'. It turns out that these two\nnotions are in general incomparable. Moreover, the weak simpliciality is not\naffected by renorming the target space $E$, while vector simpliciality may be\naffected. Further, if $H$ contains constants, vector simpliciality is strictly\nstronger and admits several characterizations (partially analogous to the\ncharacterizations known in the scalar case). We also study orderings of\nmeasures inspired by C.J.K.~Batty which may be (in special cases) used to\ncharacterize $H$-boundary measures. Finally, we give a finer version of\nrepresentation theorem using positive measures on $K\\times B_{E^*}$ and\ncharacterize uniqueness in this case.",
        "Large Language Models (LLMs) have recently demonstrated remarkable\nperformance in tasks such as Retrieval-Augmented Generation (RAG) and\nautonomous AI agent workflows. Yet, when faced with large sets of unstructured\ndocuments requiring progressive exploration, analysis, and synthesis, such as\nconducting literature survey, existing approaches often fall short. We address\nthis challenge -- termed Progressive Document Investigation -- by introducing\nGraphy, an end-to-end platform that automates data modeling, exploration and\nhigh-quality report generation in a user-friendly manner. Graphy comprises an\noffline Scrapper that transforms raw documents into a structured graph of Fact\nand Dimension nodes, and an online Surveyor that enables iterative exploration\nand LLM-driven report generation. We showcase a pre-scrapped graph of over\n50,000 papers -- complete with their references -- demonstrating how Graphy\nfacilitates the literature-survey scenario. The demonstration video can be\nfound at https:\/\/youtu.be\/uM4nzkAdGlM.",
        "Flow-matching models provide a powerful framework for various applications,\noffering efficient sampling and flexible probability path modeling. These\nmodels are characterized by flows with low curvature in learned generative\ntrajectories, which results in reduced truncation error at each sampling step.\nTo further reduce curvature, we propose block matching. This novel approach\nleverages label information to partition the data distribution into blocks and\nmatch them with a prior distribution parameterized using the same label\ninformation, thereby learning straighter flows. We demonstrate that the\nvariance of the prior distribution can control the curvature upper bound of\nforward trajectories in flow-matching models. By designing flexible\nregularization strategies to adjust this variance, we achieve optimal\ngeneration performance, effectively balancing the trade-off between maintaining\ndiversity in generated samples and minimizing numerical solver errors. Our\nresults demonstrate competitive performance with models of the same parameter\nscale.Code is available at \\url{https:\/\/github.com\/wpp13749\/block_flow}.",
        "LiDAR (Light Detection and Ranging) enables rapid and accurate acquisition of\nthree-dimensional spatial data, widely applied in remote sensing areas such as\nsurface mapping, environmental monitoring, urban modeling, and forestry\ninventory. LiDAR remote sensing primarily includes data interpretation and\nLiDAR-based inversion. However, LiDAR interpretation typically relies on dense\nand precise annotations, which are costly and time-consuming. Similarly, LiDAR\ninversion depends on scarce supervisory signals and expensive field surveys for\nannotations. To address this challenge, weakly supervised learning has gained\nsignificant attention in recent years, with many methods emerging to tackle\nLiDAR remote sensing tasks using incomplete, inaccurate, and inexact\nannotations, as well as annotations from other domains. Existing review\narticles treat LiDAR interpretation and inversion as separate tasks. This\nreview, for the first time, adopts a unified weakly supervised learning\nperspective to systematically examine research on both LiDAR interpretation and\ninversion. We summarize the latest advancements, provide a comprehensive review\nof the development and application of weakly supervised techniques in LiDAR\nremote sensing, and discuss potential future research directions in this field.",
        "Consistency Training (CT) has recently emerged as a promising alternative to\ndiffusion models, achieving competitive performance in image generation tasks.\nHowever, non-distillation consistency training often suffers from high variance\nand instability, and analyzing and improving its training dynamics is an active\narea of research. In this work, we propose a novel CT training approach based\non the Flow Matching framework. Our main contribution is a trained\nnoise-coupling scheme inspired by the architecture of Variational Autoencoders\n(VAE). By training a data-dependent noise emission model implemented as an\nencoder architecture, our method can indirectly learn the geometry of the\nnoise-to-data mapping, which is instead fixed by the choice of the forward\nprocess in classical CT. Empirical results across diverse image datasets show\nsignificant generative improvements, with our model outperforming baselines and\nachieving the state-of-the-art (SoTA) non-distillation CT FID on CIFAR-10, and\nattaining FID on par with SoTA on ImageNet at $64 \\times 64$ resolution in\n2-step generation. Our code is available at https:\/\/github.com\/sony\/vct .",
        "We introduce a novel approach to studying properties of processes in the\n{\\pi}-calculus based on a processes-as-formulas interpretation, by establishing\na correspondence between specific sequent calculus derivations and computation\ntrees in the reduction semantics of the recursion-free {\\pi}-calculus. Our\nmethod provides a simple logical characterisation of deadlock-freedom for the\nrecursion- and race-free fragment of the {\\pi}-calculus, supporting key\nfeatures such as cyclic dependencies and an independence of the name\nrestriction and parallel operators. Based on this technique, we establish a\nstrong completeness result for a nontrivial choreographic language: all\ndeadlock-free and race-free finite {\\pi}-calculus processes composed in\nparallel at the top level can be faithfully represented by a choreography. With\nthese results, we show how the paradigm of computation-as-derivation extends\nthe reach of logical methods for the study of concurrency, by bridging\nimportant gaps between logic, the expressiveness of the {\\pi}-calculus, and the\nexpressiveness of choreographic languages.",
        "3D Gaussian Splatting (3DGS) has gained significant attention for their\nhigh-quality novel view rendering, motivating research to address real-world\nchallenges. A critical issue is the camera motion blur caused by movement\nduring exposure, which hinders accurate 3D scene reconstruction. In this study,\nwe propose CoMoGaussian, a Continuous Motion-Aware Gaussian Splatting that\nreconstructs precise 3D scenes from motion-blurred images while maintaining\nreal-time rendering speed. Considering the complex motion patterns inherent in\nreal-world camera movements, we predict continuous camera trajectories using\nneural ordinary differential equations (ODEs). To ensure accurate modeling, we\nemploy rigid body transformations, preserving the shape and size of the object\nbut rely on the discrete integration of sampled frames. To better approximate\nthe continuous nature of motion blur, we introduce a continuous motion\nrefinement (CMR) transformation that refines rigid transformations by\nincorporating additional learnable parameters. By revisiting fundamental camera\ntheory and leveraging advanced neural ODE techniques, we achieve precise\nmodeling of continuous camera trajectories, leading to improved reconstruction\naccuracy. Extensive experiments demonstrate state-of-the-art performance both\nquantitatively and qualitatively on benchmark datasets, which include a wide\nrange of motion blur scenarios, from moderate to extreme blur.",
        "We propose a machine learning approach to the optimal control of fluid\nrestless multi-armed bandits (FRMABs) with state equations that are either\naffine or quadratic in the state variables. By deriving fundamental properties\nof FRMAB problems, we design an efficient machine learning based algorithm.\nUsing this algorithm, we solve multiple instances with varying initial states\nto generate a comprehensive training set. We then learn a state feedback policy\nusing Optimal Classification Trees with hyperplane splits (OCT-H). We test our\napproach on machine maintenance, epidemic control and fisheries control\nproblems. Our method yields high-quality state feedback policies and achieves a\nspeed-up of up to 26 million times compared to a direct numerical algorithm for\nfluid problems.",
        "In this work, a central-moment-based discrete Boltzmann method (CDBM) is\nconstructed for fluid flows with variable specific heat ratios. The central\nkinetic moments are employed to calculate the equilibrium discrete velocity\ndistribution function in the CDBM. In comparison to previous incompressible\ncentral-moment-based lattice Boltzmann method, the CDBM possesses the\ncapability of investigating compressible flows with thermodynamic\nnonequilibrium effects beyond conventional hydrodynamic models. Unlike all\nexisting DBMs which are constructed in raw-moment space, the CDBM stands out by\ndirectly providing the nonequilibrium effects related to the thermal\nfluctuation. The proposed method has been rigorously validated using benchmarks\nof the Sod shock tube, Lax shock tube, shock wave phenomena, two-dimensional\nsound wave, and the Taylor-Green vortex flow. The numerical results exhibit an\nexceptional agreement with theoretical predictions."
      ]
    }
  },
  {
    "id":2411.15202,
    "research_type":"applied",
    "start_id":"b2",
    "start_title":"Ilf-lstm: Enhanced loss function in lstm to predict the sea surface temperature",
    "start_abstract":"Globe's primary issue is global warming, water temperatures have accompanied it as the sea surface temperature, and it is the primary attribute to balance the energy on the earth's surface. Sea surface temperature prediction is vital to climate forecast. Downwelling currents carry some of this heat to the ocean's bottom layers, which are also heating, covering far behind the increase in sea surface temperature. In deep learning models, the correct loss function will try to reduce the error and converge fast. The proposed improved loss function correctly estimates how close the predictions made by the long short-term memory match the observed values in the training data. This research considers location-specific sea surface temperature predictions using the improved loss function in the long short-term memory neural network at six different locations around India for daily, weekly, and monthly time horizons. Most existing research concentrated on periodic forecasts, but this paper focused on daily, weekly, and monthly predictions. The improved loss function\u2014long short-term memory, achieved 98.7% accuracy, and this improved loss function overcomes the limitations of the existing techniques and reduces the processing time to\u2009~\u20090.35 s. In this research, the sea surface temperature prediction using the improved loss function in the long short-term memory neural network gives better results than the standard prediction models and other existing techniques by considering the long-time dependencies and obtaining features from the spatial data.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "The Great Barrier Reef: an environmental history"
      ],
      "abstract":[
        "Reconstructing changes in the Great Barrier Reef 15 3 The natural context of 33 4 spread European settlement coastal Queensland 43 5 beche--de ... mer, pearl shell and trochus fisheries 55 6 Impacts on marine turtles 72 7 dugongs 95 8 whales, sharks fish 9 impacts coral collecting 10 guano rock phosphate mining 11 vi Contents 12 Other reefs 13 Changes island biota 14 Conclusion"
      ],
      "categories":[
        "q-bio.QM"
      ]
    },
    "list":{
      "title":[
        "Efficient Spatial Estimation of Perceptual Thresholds for Retinal\n  Implants via Gaussian Process Regression",
        "Collective inference of the truth of propositions from crowd probability\n  judgments",
        "Homeostatic Kinematic Growth Model for Arteries -- Residual Stresses and\n  Active Response",
        "Beyond Cortisol! Physiological Indicators of Welfare for Dogs: Deficits,\n  Misunderstandings and Opportunities",
        "Quality of life and perceived care of patients in advanced chronic\n  kidney disease consultations: a cross-sectional descriptive study",
        "Uncertainty and sensitivity analysis of hair growth duration in human\n  scalp follicles under normal and alopecic conditions",
        "Foliar Uptake of Biocides: Statistical Assessment of Compartmental and\n  Diffusion-Based Models",
        "Predicting novel pharmacological activities of compounds using PubChem\n  IDs and machine learning (CID-SID ML model)",
        "Petri Net Modeling of Root Hair Response to Phosphate Starvation in\n  Arabidopsis Thaliana",
        "Bowel Incision Closure with a Semi-Automated Robot-Assisted Laser Tissue\n  Soldering System",
        "AI-Driven Hybrid Ecological Model for Predicting Oncolytic Viral Therapy\n  Dynamics",
        "Hierarchical Functional Group Ranking via IUPAC Name Analysis for Drug\n  Discovery: A Case Study on TDP1 Inhibitors",
        "An affordable, wearable, fiber-free pulsed-mode diffuse speckle contrast\n  flowmetry (PM-DSCF) sensor for noninvasive measurements of deep cerebral\n  blood flow",
        "Existence of Viscosity Solutions to Abstract Cauchy Problems via\n  Nonlinear Semigroups",
        "On the Baum-Connes conjecture for $D_{\\infty}$",
        "Abundance of spin liquids in the $S=1$ bilinear-biquadratic model on the\n  pyrochlore lattice, and its application to $\\mathrm{NaCaNi}_2\\mathrm{F}_7$",
        "Coherence DeepClean: Toward autonomous denoising of gravitational-wave\n  detector data",
        "On Stein spaces with finite homotopy rank-sum",
        "Distribution amplitudes of heavy-light pseudo-scalar and vector mesons\n  from Dyson-Schwinger equations framework",
        "Effects of particle angularity on granular self-organization",
        "Random matrices acting on sets: Independent columns",
        "Preons, Braid Topology, and Representations of Fundamental Particles",
        "COMPLETED CYCLES LEAKY HURWITZ NUMBERS",
        "Oriented diameter of the complete tripartite graph (III)",
        "Trends and Reversion in Financial Markets on Time Scales from Minutes to\n  Decades",
        "Thermostats without conjugate points",
        "Rethinking Approximate Gaussian Inference in Classification",
        "Dynamically Learning to Integrate in Recurrent Neural Networks"
      ],
      "abstract":[
        "Retinal prostheses restore vision by electrically stimulating surviving\nneurons, but calibrating perceptual thresholds - the minimum stimulus intensity\nrequired for perception - remains a time-intensive challenge, especially for\nhigh-electrode-count devices. Since neighboring electrodes exhibit spatial\ncorrelations, we propose a Gaussian Process Regression (GPR) framework to\npredict thresholds at unsampled locations while leveraging uncertainty\nestimates to guide adaptive sampling. Using perceptual threshold data from four\nArgus II users, we show that GPR with a Mat\\'ern kernel provides more accurate\nthreshold predictions than a Radial Basis Function (RBF) kernel (p < .001,\nWilcoxon signed-rank test). In addition, spatially optimized sampling yielded\nlower prediction error than uniform random sampling for Participants 1 and 3 (p\n< .05). While adaptive sampling dynamically selects electrodes based on model\nuncertainty, its accuracy gains over spatial sampling were not statistically\nsignificant (p > .05), though it approached significance for Participant 1 (p =\n.074). These findings establish GPR with spatial sampling as a scalable,\nefficient approach to retinal prosthesis calibration, minimizing patient burden\nwhile maintaining predictive accuracy. More broadly, this framework offers a\ngeneralizable solution for adaptive calibration in neuroprosthetic devices with\nspatially structured stimulation thresholds.",
        "Every day, we judge the probability of propositions. When we communicate\ngraded confidence (e.g. \"I am 90% sure\"), we enable others to gauge how much\nweight to attach to our judgment. Ideally, people should share their judgments\nto reach more accurate conclusions collectively. Peer-to-peer tools for\ncollective inference could help debunk disinformation and amplify reliable\ninformation on social networks, improving democratic discourse. However,\nindividuals fall short of the ideal of well-calibrated probability judgments,\nand group dynamics can amplify errors and polarize opinions. Here, we connect\ninsights from cognitive science, structured expert judgment, and crowdsourcing\nto infer the truth of propositions from human probability judgments. In an\nonline experiment, 376 participants judged the probability of each of 1,200\ngeneral-knowledge claims for which we have ground truth (451,200 ratings).\nAggregating binary judgments by majority vote already exhibits the \"wisdom of\nthe crowd\"--the superior accuracy of collective inferences relative to\nindividual inferences. However, using continuous probability ratings and\naccounting for individual accuracy and calibration significantly improves\ncollective inferences. Peer judgment behavior can be modeled probabilistically,\nand individual parameters capturing each peer's accuracy and miscalibration can\nbe inferred jointly with the claim probabilities. This unsupervised approach\ncan be complemented by supervised methods relying on truth labels to learn\nmodels that achieve well-calibrated collective inference. The algorithms we\nintroduce can empower groups of collaborators and online communities to pool\ntheir distributed intelligence and jointly judge the probability of\npropositions with a well-calibrated sense of uncertainty.",
        "A simple kinematic growth model for muscular arteries is presented which\nallows the incorporation of residual stresses such that a homeostatic in-vivo\nstress state under physiological loading is obtained. To this end, new\nevolution equations for growth are proposed, which avoid issues with\ninstability of final growth states known from other kinematric growth models.\nThese evolution equations are connected to a new set of growth driving forces.\nBy introducing a formulation using the principle Cauchy stresses, reasonable in\nvivo stress distributions can be obtained while ensuring realistic geometries\nof arteries after growth. To incorporate realistic Cauchy stresses for muscular\narteries under varying loading conditions, which appear in vivo, e.g., due to\nphysical activity, the growth model is combined with a newly proposed\nstretch-dependent model for smooth muscle activation. To account for the\nchanges of geometry and mechanical behavior during the growth process, an\noptimization procedure is described which leads to a more accurate\nrepresentation of an arterial ring and its mechanical behavior after the\ngrowth-related homogenization of the stresses is reached. Based thereon,\nparameters are adjusted to experimental data of a healthy middle cerebral\nartery of a rat showing that the proposed model accurately describes real\nmaterial behavior. The successful combination of growth and active response\nindicates that the new growth model can be used without problems for modeling\nactive tissues under various conditions.",
        "This paper aims to initiate new conversations about the use of physiological\nindicators when assessing the welfare of dogs. There are significant concerns\nabout construct validity - whether the measures used accurately reflect\nwelfare. The goal is to provide recommendations for future inquiry and\nencourage debate. We acknowledge that the scientific understanding of animal\nwelfare has evolved and bring attention to the shortcomings of commonly used\nbiomarkers like cortisol. These indicators are frequently used in isolation and\nwith limited salient dog descriptors, so fail to reflect the canine experience\nadequately. Using a systems approach, we explore various physiological systems\nand alternative indicators, such as heart rate variability and oxidative\nstress, to address this limitation. It is essential to consider factors like\nage, body weight, breed, and sex when interpreting these biomarkers correctly,\nand researchers should report on these in their studies. This discussion\nidentifies possible indicators for both positive and negative experiences. In\nconclusion, we advocate for a practical, evidence-based approach to assessing\nindicators of canine welfare, including non-invasive collection methods. We\nacknowledge the complexity of evaluating experiential responses in dogs across\ndifferent situations and the need for continued work to improve practices and\nrefine terminology. This will enhance our ability to accurately understand\nwelfare and improve the wellbeing of dogs, serving to inform standards of\nanimal welfare assessment. We hope this will promote more fundamental research\nin canine physiology to improve construct validity, leading to better\npractices, ultimately improving the lives of dogs.",
        "Objetive: In the care of renal patients, prioritising their quality of life\nand nursing care is essential. Research links patients' perceptions of care\nquality to improved outcomes such as safety, clinical efficacy, treatment\nadherence, and preventive practices. This study aimed to evaluate the quality\nof life and care perception in these patients and explore potential\nassociations between these dimensions. Material and methods: A cross-sectional\ndescriptive study was conducted with 43 patients attending an advanced CKD\nclinic. Quality of life was assessed using the KDQOL-36 questionnaire, while\nthe IECPAX questionnaire measured perceived care quality. Sociodemographic and\nclinical data were collected from patient records. Participants completed the\nquestionnaires during routine visits, with scores analysed to identify\nassociations between variables. Results: The study included 60% men (n=28) and\n32% women (n=15), with a mean age of 78 years . Among participants, 45% were\ndiabetic, 79% hypertensive, and 58% took more than five medications daily. Mean\nscores were 78.76 for KDQOL-36 and 5.54 for IECPAX. Significant differences\nwere found in the physical role domain between men and women (p=0.01) and for\nindividuals over 65 years (p=0.04). Higher IECPAX scores were associated with\ntaking more than five medications (p=0.05). However, no correlation was\nobserved between KDQOL-36 and IECPAX scores. Conclusions: The findings suggest\nthat quality of life and perceived care quality are independent in advanced CKD\npatients. While this study provides insights, larger multicentre studies are\nneeded to validate these results. These findings highlight the importance of\naddressing both aspects separately to improve outcomes in this population.",
        "Hair follicles constantly cycle through phases of growth, regression and\nrest, as matrix keratinocytes (MKs), the cells producing hair fibers,\nproliferate, and then undergo spontaneous apoptosis. Damage to MKs and\nperturbations in their normal dynamics result in a shortened growth phase,\nleading to hair loss. Two common factors causing such disruption are hormonal\nimbalance and attacks by the immune system. Androgenetic alopecia (AGA) is hair\nloss caused by high sensitivity to androgens, and alopecia areata (AA) is hair\nloss caused by an autoimmune reaction against MKs. In this study, we inform a\nmathematical model for the human hair cycle with experimental data for the\nlengths of hair cycle phases available from male control subjects and subjects\nwith AGA. We also, connect a mathematical model for AA with estimates for the\nduration of hair cycle phases obtained from the literature. Subsequently, with\neach model we perform parameter screening, uncertainty quantification and\nglobal sensitivity analysis and compare the results within and between the\ncontrol and AGA subject groups as well as among AA, control and AGA conditions.\nThe findings reveal that in AGA subjects there is greater uncertainty\nassociated with the duration of hair growth than in control subjects and that,\ncompared to control and AGA conditions, in AA it is more certain that longer\nhair growth phase could not be expected. The comparison of results also\nindicates that in AA lower proliferation of MKs and weaker communication of the\ndermal papilla with MKs via signaling molecules could be expected than in\nnormal and AGA conditions, and in AA stronger inhibition of MK proliferation by\nregulatory molecules could be expected than in AGA. Finally, the global\nsensitivity analysis highlights the process of MK apoptosis as highly impactful\nfor the length of hair growth only in the AA case, but not for control and AGA\nconditions.",
        "The global population increase leads to a high food demand, and to reach this\ntarget products such as pesticides are needed to protect the crops. Research is\nfocusing on the development of new products that can be less harmful to the\nenvironment, and mathematical models are tools that can help to understand the\nmechanism of uptake of pesticides and then guide in the product development\nphase. This paper applies a systematic methodology to model the foliar uptake\nof pesticides, to take into account the uncertainties in the experimental data\nand in the model structure. A comparison between different models is conducted,\nfocusing on the identifiability of model parameters through dynamic sensitivity\nprofiles and correlation analysis. Lastly, data augmentation studies are\nconducted to exploit the model for the design of experiments and to provide a\npractical support to future experimental campaigns, paving the way for further\napplication of model-based design of experiments techniques in the context of\nfoliar uptake.",
        "Significance and Object: The proposed methodology aims to provide time- and\ncost-effective approach for the early stage in drug discovery. The machine\nlearning models developed in this study used only the identification numbers\nprovided by PubChem. Thus, a drug development researcher who has obtained a\nPubChem CID and SID can easily identify new functionality of their compound.\nThe approach was demonstrated, using four bioassay which were on (i) the\nantagonists of human D3 dopamine receptors; (ii) the promoter Rab9 activators;\n(iii) small molecule inhibitors of CHOP to regulate the unfolded protein\nresponse to ER stress; (iv) antagonists of the human M1 muscarinic receptor.\nSolution: The four bioassays used for demonstration of the approach were\nprovided by PubChem. For each bioassay, the generated by PubChem CIDs, SIDs\nwere extracted together with the corresponding activity. The resulting dataset\nwas sifted with the dataset on a water solubility bioassay, remaining only the\ncompounds common for both bioassays. In this way, the inactive compounds were\nreduced. Then, all active compounds were added, and the resulted dataset was\nlater used for machine learning based on scikit learn algorithms. Results: The\naverage values of the ML models` metrics for the four bioassays were: 83.82%\nAccuracy with 5.35 standard deviation; 87.9% Precision with 5.04 standard\ndeviation; 77.1% Recall with 7.65 standard deviation; 82.1% F1 with 6.44\nstandard deviation; 83.4% ROC with 5.09 standard deviation.",
        "Limited availability of inorganic phosphate (Pi) in soil is an important\nconstraint to plant growth. In order to understand better the underlying\nmechanism of plant response to Pi, the response to phosphate starvation in\nArabidopsis thaliana was investigated through use of Petri Nets, a formal\nlanguage suitable for bio-modeling. A. thaliana displays a range of responses\nto deal with Pi starvation, but special attention was paid to root hair\nelongation in this study. A central player in the root hair pathway is the\ntranscription factor ROOT HAIR DEFECTIVE 6-LIKE 4 (RSL4), which has been found\nto be upregulated during the Pi stress. A Petri Net was created which could\nsimulate the gene regulatory networks responsible for the increase in root hair\nlength, as well as the resulting increase in root hair length. Notably,\ndiscrepancies between the model and the literature suggested an important role\nfor RSL2 in regulating RSL4. In the future, the net designed in the current\nstudy could be used as a platform to develop hypotheses about the interaction\nbetween RSL2 and RSL4.",
        "Traditional methods for closing gastrointestinal (GI) surgery incisions, like\nsuturing and stapling, present significant challenges, including potentially\nlife-threatening leaks. These techniques, especially in robot-assisted\nminimally invasive surgery (RAMIS), require advanced manual skills. While their\nrepetitive and time-consuming nature makes them suitable candidates for\nautomation, the automation process is complicated by the need for extensive\ncontact with the tissue. Addressing this, we demonstrate a semi-autonomous\ncontactless surgical procedure using our novel Robot-assisted Laser Tissue\nSoldering (RLTS) system on a live porcine bowel. Towards this in-vivo\ndemonstration, we optimized soldering protocols and system parameters in\nex-vivo experiments on porcine bowels and a porcine cadaver. To assess the RLTS\nsystem performance, we compared the pressure at which the anastomosis leaked\nbetween our robotic soldering and manual suturing. With the best setup, we\nadvanced to an in-vivo Heineke Mikulicz closure on small bowel incision in live\npigs and evaluated their healing for two weeks. All pigs successfully\ncompleting the procedure (N=5) survived without leaks and the histology\nindicated mucosal regeneration and fibrous tissue adhesion. This marks the\nfirst in-vivo semi-automated contactless incision closure, paving the way for\nautomating GI surgery incision closure which has the potential to become an\nalternative to traditional methods.",
        "Oncolytic viral therapy (OVT) is an emerging precision therapy for aggressive\nand recurrent cancers. However, its clinical efficacy is hindered by the\ncomplexity of tumor-virus-immune interactions and the lack of predictive models\nfor personalized treatment. This study develops a data-driven, AI-powered\ncomputational model combining time-delayed Generalized Lotka-Volterra equations\nwith advanced optimization algorithms, including Genetic Algorithms,\nDifferential Evolution, and Reinforcement Learning, to optimize OVT\noscillations' growth and damping. We hypothesize that the model can provide\naccurate, real-time predictions of OVT responses while identifying key\nbiomarkers to enhance therapeutic efficacy. The model demonstrates strong\npredictive accuracy, achieving mean squared error (MSE) < 0.02 and R-squared >\n0.82. It also identifies experimentally validated biomarkers such as TNF, NFkB,\nCD81, TRAF2, IL18, and BID, among other inflammatory cytokines and\nextracellular matrix reconstruction factors, despite being causally agnostic\nand unaware of specific experimental conditions or therapeutic combinations.\nGene set enrichment analysis confirmed these biosignatures as critical\npredictors of tumor progression and indicated that photodynamic therapy\nactivates immune responses similar to those elicited by combined OVT and immune\ncheckpoint inhibitors. This hybrid model represents a significant step toward\nprecision oncology and computational medicine, enabling longitudinal, adaptive\ntreatment regimens and developing targeted immunotherapies based on molecular\nsignatures, potentially improving patient outcomes.",
        "The article proposes a computational approach that can generate a descending\norder of the IUPAC-notated functional groups based on their importance for a\ngiven case study. Thus, a reduced list of functional groups could be obtained\nfrom which drug discovery can be successfully initiated. The approach,\napplicable to any study case with sufficient data, was demonstrated using a\nPubChem bioassay focused on TDP1 inhibitors. The Scikit Learn interpretation of\nthe Random Forest Classifier (RFC) algorithm was employed. The machine learning\n(ML) model RFC obtained 70.9% accuracy, 73.1% precision, 66.1% recall, 69.4% F1\nand 70.8% receiver-operating characteristic (ROC). In addition to the main\nstudy, the CID_SID ML model was developed, which, using only the PubChem\ncompound and substance identifiers (CIDs and SIDs) data, can predict with 85.2%\naccuracy, 94.2% precision, 75% precision, F1 of 83.5% F1 and 85.2% ROC whether\na compound is a TDP1 inhibitor.",
        "Significance: Measuring cerebral blood flow (CBF) is crucial for diagnosing\nvarious cerebral diseases. An affordable, wearable, and fiber-free\ncontinuous-wave speckle contrast flowmetry (CW-DSCF) technique has been\ndeveloped for continuous monitoring of CBF variations. However, its application\nin adult humans is limited by shallow tissue penetration. Aim: To develop an\ninnovative pulse-mode DSCF (PM-DSCF) system for continuous monitoring of CBF\nvariations in adult humans. Approach: The PM-DSCF utilizes an 808 nm laser\ndiode and a small NanEye camera to capture diffuse laser speckle fluctuations\ncaused by red blood cell movement in the brain (i.e., CBF). Operating in\nshort-pulse mode (duty cycle < 5%), the system maximizes peak pulse light power\nfor deeper tissue penetration, while ensuring that the average power density\nremains within ANSI safety standards for skin exposure. The PM-DSCF was\nevaluated on tissue-simulating phantoms and in adult humans. Results: The\nmaximum effective source-detector distance increased from 15 mm (CW-DSCF) to 35\nmm (PM-DSCF). The PM-DSCF successfully detected CBF variations in adult brains\nduring head-up-tilting experiments, consistent with physiological expectations.\nConclusions: Switching from CW mode to PM mode significantly increases the\nmaximum tissue penetration depth from ~7.5 mm (CW-DSCF) to ~17.5 mm (PM-DSCF),\nenabling successful CBF measurements in adult humans.",
        "In this work, we provide conditions for nonlinear monotone semigroups on\nlocally convex vector lattices to give rise to a generalized notion of\nviscosity solutions to a related nonlinear partial differential equation. The\nsemigroup needs to satisfy a convexity estimate, so called $K$-convexity,\nw.r.t. another family of operators, defined on a potentially larger locally\nconvex vector lattice. We then show that, under mild continuity requirements on\nthe bounding family of operators, the semigroup yields viscosity solutions to\nthe abstract Cauchy problem given in terms of its generator in the larger\nlocally convex vector lattice. We apply our results to drift control problems\nfor infinite-dimensional L\\'evy processes and robust optimal control problems\nfor infinite-dimensional Ornstein-Uhlenbeck processes.",
        "We make an exposition of the proof of the Baum-Connes conjecture for the\ninfinite dihedral group following the ideas of Higson and Kasparov.",
        "Long considered the ''poor cousins'' of spin-1\/2 systems, magnets built of\nspin-1 moments have recently come to fore as a rich source of novel phases of\nmatter. Here we explore the phases which arise in a spin-1 magnet on the\npyrochlore lattice, once biquadratic interactions are taken into account. Using\na combination of variational and Monte Carlo techniques, built around the exact\ntreatment of spin-1 at the level of a single site, we uncover seven distinct\nspin liquid phases. Dynamical calculations for one of these spin liquids are\nshown to be in good agreement with inelastic neutron scattering on the spin-1\npyrochlore $\\mathrm{NaCaNi}_2\\mathrm{F}_7$. These results suggest that the\nrange of spin liquid phases found in spin-1 pyrochlores may be even richer than\nin materials with (pseudo-)spin-1\/2 moments.",
        "Technical and environmental noise in ground-based laser interferometers\ndesigned for gravitational-wave observations like Advanced LIGO, Advanced Virgo\nand KAGRA, can manifest as narrow (<1Hz) or broadband ($10'$s or even $100'$s\nof Hz) spectral lines and features in the instruments' strain amplitude\nspectral density. When the sources of this noise cannot be identified or\nremoved, in cases where there are witness sensors sensitive to this noise\nsource, denoising of the gravitational-wave strain channel can be performed in\nsoftware, enabling recovery of instrument sensitivity over affected frequency\nbands. This noise hunting and removal process can be particularly challenging\ndue to the wealth of auxiliary channels monitoring the interferometry and the\nenvironment and the non-linear couplings that may be present. In this work, we\npresent a comprehensive analysis approach and corresponding cyberinfrastructure\nto promptly identify and remove noise in software using machine learning\ntechniques. The approach builds on earlier work (referred to as DeepClean) in\nusing machine learning methods for linear and non-linear regression of noise.\nWe demonstrate how this procedure can be operated and optimized in a tandem\nfashion close to online data taking; it starts off with a coherence monitoring\nanalysis that first singles out and prioritizes witness channels that can then\nbe used by DeepClean. The resulting denoised strain by DeepClean reflects a\n1.4\\% improvement in the binary neutron star range, which can translate into a\n4.3\\% increase in the sensitive volume. This cyber infrastructure we refer to\nas Coherence DeepClean, or CDC, is a significant step toward autonomous\noperations of noise subtraction for ground-based interferometers.",
        "A topological space (not necessarily simply connected) is said to have finite\nhomotopy rank-sum if the sum of the ranks of all higher homotopy groups (from\nthe second homotopy group onward) is finite. In this article, we consider Stein\nspaces of arbitrary dimension satisfying the above rational homotopy theoretic\nproperty, although most of this article focuses on Stein surfaces only. We\ncharacterize all Stein surfaces satisfying the finite homotopy rank-sum\nproperty. In particular, if such a Stein surface is affine and every element of\nits fundamental group is finite, it is either simply connected or has a\nfundamental group of order $2$. A detailed classification of the smooth complex\naffine surfaces of the non-general type satisfying the finite homotopy rank-sum\nproperty is obtained. It turns out that these affine surfaces are\nEilenberg--MacLane spaces whenever the fundamental group is infinite.",
        "We systematically investigate leading-twist distribution amplitudes of ground\nstate heavy-light pseudo-scalar and vector mesons, the results of $B^*$,\n$B^*_s$, $B_c^*$ mesons are reported for the first time within the\nDyson-Schwinger equations framework. A novel numerical method for calculating\nMellin moments is proposed, which can avoid extrapolation or fitting in\nprevious similar studies. Based on it, we calculate the first eight Mellin\nmoments of mesons and reconstruct their distribution amplitudes. It is found\nthat, in flavor-asymmetric systems, distribution amplitude $\\phi(x)$ is skewed\nto one side, with the position of the maximum $\\sim M^f_E\/(M^f_E+M^g_E)$, where\n$M_E$ is Euclidean constituent quark mass and $f\/g$ denote the flavor of\nheavier\/lighter quark in the meson, respectively. For systems with the same\nvalence quark structure, the first Mellin moments follow the relation $\\langle\n\\xi \\rangle_{0^-} < \\langle \\xi \\rangle^{\\|}_{1^-} < \\langle \\xi\n\\rangle^{\\perp}_{1^-}$, where $\\xi = 2x - 1$ and $x$ is the momentum fraction\ncarried by the heavier quark. Our predictions can be compared with experimental\ndata and further theoretical calculations in the future, and the results of\nlight mesons such as $\\pi$, $K$, $\\rho$ are consistent with recent lattice\ndata.",
        "Recent studies of two-dimensional poly-disperse disc systems revealed a\ncoordinated self-organisation of cell stresses and shapes, with certain\ndistributions collapsing onto a master form for many processes, size\ndistributions, friction coefficients, and cell orders. Here we examine the\neffects of grain angularity on the indicators of self-organisation, using\nsimulations of bi-disperse regular $N$-polygons and varying $N$ systematically.\nWe find that: the strong correlation between local cell stresses and\norientations, as well as the collapses of the conditional distributions of\nscaled cell stress ratios to a master Weibull form for all cell orders $k$, are\nindependent of angularity and friction coefficient. In contrast, increasing\nangularity makes the collapses of the conditional distributions sensitive to\nchanges in the friction coefficient.",
        "We study random matrices with independent subgaussian columns. Assuming each\ncolumn has a fixed Euclidean norm, we establish conditions under which such\nmatrices act as near-isometries when restricted to a given subset of their\ndomain. We show that, with high probability, the maximum distortion caused by\nsuch a matrix is proportional to the Gaussian complexity of the subset, scaled\nby the subgaussian norm of the matrix columns. This linear dependence on the\nsubgaussian norm is a new phenomenon, as random matrices with independent rows\nor independent entries typically exhibit superlinear dependence. As a\nconsequence, normalizing the columns of random sparse matrices leads to\nstronger embedding guarantees.",
        "In particle phenomenology, preon models study compositional rules of standard\nmodel interactions. In spite of empirical success, mathematical underpinnings\nof preon models in terms of group representation theory have not been fully\nworked out. Here, we address this issue while clarifying the relation between\ndifferent preon models. In particular, we focus on two prominent models:\nBilson-Thompson's helon model, and Lambek's 4-vector model. We determine the\nmapping between helon model particle states and representation theory of Lie\nalgebras. Braided ribbon diagrams of the former represent on-shell states of\nspinors of the Lorentz group. Braids correspond to chirality, and twists, to\ncharges. We note that this model captures only the $SU(3)_c\\times U(1)_{em}$\nsector of the standard model. We then map the twists of helon diagrams to the\nweight polytope of $SU(3)_c \\times U(1)_{em}$. The braid structure maps to\nchiral states of fermions. We also show that Lambek's 4-vector can be recovered\nfrom helon diagrams. Alongside, we introduce a new 5-vector representation\nderived from the weight lattice. This representation contains both, the correct\ninteractions found in 4-vectors and the inclusion of chirality found in helons.\nAdditionally, we demonstrate topological analogues of CPT transformations in\nhelon diagrams. Interestingly, the braid diagrams of the helon model are the\nonly ones that are self-consistent with CPT invariance. In contrast to\nfield-theoretic approaches, the compositional character of preon models offers\nan analogous particle-centric perspective on fundamental interactions.",
        "We introduce $(r+1)$-completed cycles $k$-leaky Hurwitz numbers and prove\npiecewise polynomiality as well as establishing their chamber polynomiality\nstructure and their wall crossing formulae. For $k=0$ the results recover\nprevious results of Shadrin-Spitz-Zvonkine. The specialization for $r=1$\nrecovers Hurwitz numbers that are close to the ones studied by\nCavalieri-Markwig-Ranganathan and Cavalieri-Markwig-Schmitt. The ramifications\ndiffer by a lower order torus correction, natural from the Fock space\nperspective, not affecting the genus zero enumeration, nor the enumeration for\nleaky parameter values $k = \\pm 1$ in all genera.",
        "Given a bridgeless graph $G$, let $\\mathbb{D}(G)$ be the set of all strong\norientations of $G$, and define the oriented diameter $f(G)$ of $G$ to be the\nminimum of diameters $diam(D)$ among all the strong orientations $D\\in\n\\mathbb{D}(G)$, i.e., $f(G)=\\min\\{diam(D)\\mid D\\in \\mathbb{D}(G)\\}$. In this\npaper, we determine the oriented diameter of complete tripartite graph\n$K(3,p,q)$ for $p\\geqslant 5$. Combining with the previous results, the\noriented diameter of complete tripartite graph $K(3,p,q)$ are known.",
        "We empirically analyze the reversion of financial market trends with time\nhorizons ranging from minutes to decades. The analysis covers equities,\ninterest rates, currencies and commodities and combines 14 years of futures\ntick data, 30 years of daily futures prices, 330 years of monthly asset prices,\nand yearly financial data since medieval times.\n  Across asset classes, we find that markets are in a trending regime on time\nscales that range from a few hours to a few years, while they are in a\nreversion regime on shorter and longer time scales. In the trending regime,\nweak trends tend to persist, which can be explained by herding behavior of\ninvestors. However, in this regime trends tend to revert before they become\nstrong enough to be statistically significant, which can be interpreted as a\nreturn of asset prices to their intrinsic value. In the reversion regime, we\nfind the opposite pattern: weak trends tend to revert, while those trends that\nbecome statistically significant tend to persist.\n  Our results provide a set of empirical tests of theoretical models of\nfinancial markets. We interpret them in the light of a recently proposed\nlattice gas model, where the lattice represents the social network of traders,\nthe gas molecules represent the shares of financial assets, and efficient\nmarkets correspond to the critical point. If this model is accurate, the\nlattice gas must be near this critical point on time scales from 1 hour to a\nfew days, with a correlation time of a few years.",
        "We generalize Hopf's theorem to thermostats: the total thermostat curvature\nof a thermostat without conjugate points is non-positive, and vanishes only if\nthe thermostat curvature is identically zero. We further show that, if the\nthermostat curvature is zero, then the flow has no conjugate points, and the\nGreen bundles collapse almost everywhere. Given a thermostat without conjugate\npoints, we prove that the Green bundles are transversal everywhere if and only\nif it admits a dominated splitting. Finally, we provide an example showing that\nHopf's rigidity theorem on the 2-torus cannot be extended to thermostats. It is\nalso the first example of a thermostat with a dominated splitting which is not\nAnosov.",
        "In classification tasks, softmax functions are ubiquitously used as output\nactivations to produce predictive probabilities. Such outputs only capture\naleatoric uncertainty. To capture epistemic uncertainty, approximate Gaussian\ninference methods have been proposed, which output Gaussian distributions over\nthe logit space. Predictives are then obtained as the expectations of the\nGaussian distributions pushed forward through the softmax. However, such\nsoftmax Gaussian integrals cannot be solved analytically, and Monte Carlo (MC)\napproximations can be costly and noisy. We propose a simple change in the\nlearning objective which allows the exact computation of predictives and enjoys\nimproved training dynamics, with no runtime or memory overhead. This framework\nis compatible with a family of output activation functions that includes the\nsoftmax, as well as element-wise normCDF and sigmoid. Moreover, it allows for\napproximating the Gaussian pushforwards with Dirichlet distributions by\nanalytic moment matching. We evaluate our approach combined with several\napproximate Gaussian inference methods (Laplace, HET, SNGP) on large- and\nsmall-scale datasets (ImageNet, CIFAR-10), demonstrating improved uncertainty\nquantification capabilities compared to softmax MC sampling. Code is available\nat https:\/\/github.com\/bmucsanyi\/probit.",
        "Learning to remember over long timescales is fundamentally challenging for\nrecurrent neural networks (RNNs). While much prior work has explored why RNNs\nstruggle to learn long timescales and how to mitigate this, we still lack a\nclear understanding of the dynamics involved when RNNs learn long timescales\nvia gradient descent. Here we build a mathematical theory of the learning\ndynamics of linear RNNs trained to integrate white noise. We show that when the\ninitial recurrent weights are small, the dynamics of learning are described by\na low-dimensional system that tracks a single outlier eigenvalue of the\nrecurrent weights. This reveals the precise manner in which the long timescale\nassociated with white noise integration is learned. We extend our analyses to\nRNNs learning a damped oscillatory filter, and find rich dynamical equations\nfor the evolution of a conjugate pair of outlier eigenvalues. Taken together,\nour analyses build a rich mathematical framework for studying dynamical\nlearning problems salient for both machine learning and neuroscience."
      ]
    }
  },
  {
    "id":2411.06741,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"Learning Polynomials with Neural Networks",
    "start_abstract":"We study the effectiveness of learning low degree polynomials using neural networks by gradient descent method. While have been shown to great expressive power, and has widely used in practice for networks, few theoretical guarantees are known such methods. In particular, it is well that can get stuck at local minima, even simple classes target functions. this paper, we present several positive results support networks. focus on twolayer where bottom layer a set non-linear hidden nodes, top node linear function, similar Barron (1993). First show randomly initialized network with sufficiently many units, generic algorithm learns any polynomial, assuming initialize weights randomly. Secondly, if use complex-valued (the function still be real), then under suitable conditions, there no robust minima: always escape minimum performing random perturbation. This property does not hold real-valued weights. Thirdly, discuss whether sparse learned small size dependent sparsity function.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b20"
      ],
      "title":[
        "Physics-informed machine learning"
      ],
      "abstract":[
        "Despite great progress in simulating multiphysics problems using the numerical discretization of partial differential equations (PDEs), one still cannot seamlessly incorporate noisy data into existing algorithms, mesh generation remains complex, and high-dimensional problems governed by parameterized PDEs cannot be tackled. Moreover, solving inverse problems with hidden physics is often prohibitively expensive and requires different formulations and elaborate computer codes. Machine learning has emerged as a promising alternative, but training deep neural networks requires big data, not always available for scientific problems. Instead, such networks can be trained from additional information obtained by enforcing the physical laws (for example, at random points in the continuous space-time domain). Such physics-informed learning integrates (noisy) data and mathematical models, and implements them through neural networks or other kernel-based regression networks. Moreover, it may be possible to design specialized network architectures that automatically satisfy some of the physical invariants for better accuracy, faster training and improved generalization. Here, we review some of the prevailing trends in embedding physics into machine learning, present some of the current capabilities and limitations and discuss diverse applications of physics-informed learning both for forward and inverse problems, including discovering hidden physics and tackling high-dimensional problems."
      ],
      "categories":[
        "physics.chem-ph"
      ]
    },
    "list":{
      "title":[
        "The Conundrum of Diffuse Basis Sets: A Blessing for Accuracy yet a Curse\n  for Sparsity",
        "Equation-of-motion Coupled-cluster singles, doubles and(full) triples\n  for doubly ionized and two-electron-attached states: A Computational\n  implementation",
        "Efficient Machine Learning Approach for Yield Prediction in Chemical\n  Reactions",
        "Photokinetics of Photothermal Reactions",
        "Imaging the Photochemistry of Cyclobutanone using Ultrafast Electron\n  Diffraction: Experimental Results",
        "Stochastic resolution of identity to CC2 for large systems: Oscillator\n  strength and ground state gradient calculations",
        "Monolayer-Defined Flat Colloidal PbSe Quantum Dots in Extreme\n  Confinement",
        "Endofullerenes and Dispersion-Corrected Density Functional\n  Approximations: A Cautionary Tale",
        "MetaWFN: A Platform for Unified Implementation of Many-Electron\n  Wavefunctions",
        "Two-dimensional fluorescence spectroscopy with quantum entangled photons\n  and time- and frequency-resolved two-photon coincidence detection",
        "Complex potential energy surfaces: gradients with projected CAP\n  technique",
        "Investigating ferromagnetic response in monolayer CVD grown MoS$_{2}$\n  flakes using quantum weak measurement",
        "Taming the Virtual Space for Incremental Full Configuration Interaction",
        "Non-Commutative fluid: an alternative source of cosmic acceleration",
        "Gradient estimates for the fractional $p$-Poisson equation",
        "Optimizing compilation of error correction codes for 2xN quantum dot\n  arrays and its NP-hardness",
        "FOSS solution for Molecular Dynamics Simulation Automation and\n  Collaboration with MDSGAT",
        "Leveraging the Bias-Variance Tradeoff in Quantum Chemistry for Accurate\n  Negative Singlet-Triplet Gap Predictions: A Case for Double-Hybrid DFT",
        "Galaxy mass profiles with convolutional neural networks",
        "Auto-Balancer: Harnessing idle network resources for enhanced market\n  stability",
        "On a planetary forcing of global seismicity",
        "Magnetic Interactions in the Polar Ferrimagnet with a Bipartite\n  Structure",
        "Chiral supersolid and dissipative time crystal in Rydberg-dressed\n  Bose-Einstein condensates with Raman-induced spin-orbit coupling",
        "Sub-MHz Radio Background from Ultralight Dark Photon Dark Matter",
        "Kolyvagin's conjecture for modular forms at non-ordinary primes",
        "Global $C^{1,\\alpha}$ regularity for Monge-Amp\\`ere equations on planar\n  convex domains",
        "Investigation of Inverse Velocity Dispersion in a Solar Energetic\n  Particle Event Observed by Solar Orbiter",
        "Averaging over the circles the gaussian free field in the Poincar{\\'e}\n  disk"
      ],
      "abstract":[
        "Diffuse atomic orbital basis sets have proven to be essential to obtain\naccurate interaction energies, especially in regard to non-covalent\ninteractions. However, they also have a detrimental impact on the sparsity of\nthe one-particle density matrix (1-PDM), to a degree stronger than the spatial\nextent of the basis functions alone could explain. This is despite the fact\nthat the matrix elements of the 1-PDM of insulators (systems with significant\nHOMO-LUMO gaps) are expected to decay exponentially with increasing real-space\ndistance from the diagonal and the asymptotic decay rate is expected to have a\nwell-defined basis set limit. The observed low sparsity of the 1-PDM appears to\nbe independent of representation and even persists after projecting the 1-PDM\nonto a real-space grid, leading to the conclusion that this \"curse of sparsity\"\nis solely a basis set artifact, which, counterintuitively, becomes worse for\nlarger basis sets, seemingly contradicting the notion of a well-defined basis\nset limit. We show that this is a consequence of the low locality of the\ncontra-variant basis functions as quantified by the inverse overlap matrix\n$\\mathbf{S}^{-1}$ being significantly less sparse than its covariant dual.\nIntroducing the model system of an infinite non-interacting chain of helium\natoms, we are able to quantify the exponential decay rate to be proportional to\nthe diffuseness as well as local incompleteness of the basis set, meaning small\nand diffuse basis sets are affected the most. Finally, we propose one solution\nto the conundrum in the form of the complementary auxiliary basis set (CABS)\nsingles correction in combination with compact, low l-quantum-number basis\nsets, showing promising results for non-covalent interactions.",
        "We present our computational implementation of the equation-of-motion (EOM)\ncoupled-cluster (CC) singles, doubles, and triples (SDT) method for computing\ndoubly ionized (DIP) and two-electron attached (DEA) states within Q-CHEM.\nThese variants have been implemented within both the (conventional) double\nprecision (DP) and the single precision (SP) algorithms and will be available\nin the upcoming major release of {\\sl Q-CHEM}. We present here the programmable\nexpressions and some pilot application of $CH_2$ for DIP and DEA EOM-CCSDT.",
        "Developing machine learning (ML) models for yield prediction of chemical\nreactions has emerged as an important use case scenario in very recent years.\nIn this space, reaction datasets present a range of challenges mostly stemming\nfrom imbalance and sparsity. Herein, we consider chemical language\nrepresentations for reactions to tap into the potential of natural language\nprocessing models such as the ULMFiT (Universal Language Model Fine Tuning) for\nyield prediction, which is customized to work across such distribution\nsettings. We contribute a new reaction dataset with more than 860 manually\ncurated reactions collected from literature spanning over a decade, belonging\nto a family of catalytic meta-C(sp2)-H bond activation reactions of high\ncontemporary importance. Taking cognizance of the dataset size, skewness toward\nthe higher yields, and the sparse distribution characteristics, we developed a\nnew (i) time- and resource-efficient pre-training strategy for downstream\ntransfer learning, and (ii) the CFR (classification followed by regression)\nmodel that offers state-of-the-art yield predictions, surpassing conventional\ndirect regression (DR) approaches. Instead of the prevailing pre-training\npractice of using a large number of unlabeled molecules (1.4 million) from the\nChEMBL dataset, we first created a pre-training dataset SSP1 (0.11 million), by\nusing a substructure-based mining from the PubChem database, which is found to\nbe equally effective and more time-efficient in offering enhanced performance.\nThe CFR model with the ULMFiT-SSP1 regressor achieved an impressive RMSE of\n8.40 for the CFR-major and 6.48 for the CFR-minor class in yield prediction on\nthe title reaction, with a class boundary of yield at 53 %. Furthermore, the\nCFR model is highly generalizable as evidenced by the significant improvement\nover the previous benchmark reaction datasets.",
        "Photothermal reactions, involving both photochemical and thermal\nreaction-steps, are the most abundant sequences in photochemistry. The\nderivation of their rate-laws is standardized, but the integration of these\nrate-laws has not yet been achieved. Indeed, the field still lacks integrated\nrate-laws for the description of these reactions behavior, and\/or\nidentification of their reaction-order. This made a comprehensive account of\nthe photo-kinetics of photothermal reactions to be a gap in the knowledge. This\ngap is addressed in the present paper by introducing an unprecedented general\nmodel equation capable to mapping out the kinetic traces of such reactions when\nexposed to light or in the dark. The integrated rate-law model equation also\napplies when the reactive medium is exposed to either monochromatic or\npolychromatic light irradiation. The validity of the model equation was\nestablished against simulated data obtained by a fourth-order Runge-Kutta\nmethod. It was then used to describe and quantify several situations of\nphotothermal reactions, such as the effects of initial concentration, spectator\nmolecules, and incident radiation intensity, and the impact of the latter on\nthe photonic yield. The model equation facilitated a general elucidation method\nto determine the intrinsic reaction parameters (quantum yields and\nabsorptivities of the reactive species) for any photothermal mechanism whose\nnumber of species are known. This paper contributes to rationalizing\nphoto-kinetics along the same general guidelines adopted in chemical kinetics.",
        "We investigated the ultrafast structural dynamics of cyclobutanone following\nphotoexcitation at $\\lambda=200$ nm using gas-phase megaelectronvolt ultrafast\nelectron diffraction. Our investigation complements the simulation studies of\nthe same process within this special issue. It provides information about both\nelectronic state population and structural dynamics through well-separable\ninelastic and elastic electron scattering signatures. We observe the\ndepopulation of the photoexcited S$_2$ state of cyclobutanone with n3s Rydberg\ncharacter through its inelastic electron scattering signature with a time\nconstant of $(0.29 \\pm 0.2)$ ps towards the S$_1$ state. The S$_1$ state\npopulation undergoes ring-opening via a Norrish Type-I reaction, likely while\npassing through a conical intersection with S$_0$. The corresponding structural\nchanges can be tracked by elastic electron scattering signatures. These changes\nappear with a delay of $(0.14 \\pm 0.05)$ ps with respect the initial\nphotoexcitation, which is less than the S$_2$ depopulation time constant. This\nbehavior provides evidence for the ballistic nature of the ring-opening once\nthe S$_1$ state is reached. The resulting biradical species react further\nwithin $(1.2 \\pm 0.2)$ ps via two rival fragmentation channels yielding ketene\nand ethylene, or propene and carbon monoxide. Our study showcases both the\nvalue of gas-phase ultrafast diffraction studies as an experimental benchmark\nfor nonadiabatic dynamics simulation methods and the limits in the\ninterpretation of such experimental data without comparison to such\nsimulations.",
        "An implementation of stochastic resolution of identity (sRI) approximation to\nCC2 oscillator strengths as well as ground state analytical gradients is\npresented. The essential 4-index electron repulsion integrals (ERIs) are\ncontracted with a set of stochastic orbitals on the basis of the RI technique\nand the orbital energy differences in the denominators are decoupled with the\nLaplace transform. These lead to a significant scaling reduction from O(N^5) to\nO(N^3) for oscillator strengths and gradients with the size of the basis set,\nN. The gradients need a large number of stochastic orbitals with O(N^3), so we\nprovide an additional O(N^4) version with better accuracy and smaller prefactor\nby adopting sRI partially. Such steep computational acceleration of nearly two\nor one order of magnitude is very attractive for large systems. This work is an\nextension to our previous implementations of sRI-CC2 ground and excited state\nenergies and shows the feasibility of introducing sRI to CC2 properties beyond\nenergies.",
        "Colloidal two-dimensional lead chalcogenide nanocrystals represent an\nintriguing new class of materials that push the boundaries of quantum\nconfinement by combining a crystal thickness down to the monolayer with\nconfinement in the lateral dimension. In particular flat PbSe quantum dots\nexhibit efficient telecommunication band-friendly photoluminescence (1.43 -\n0.83 eV with up to 61% quantum yield) that is highly interesting for\nfiber-optics information processing. By using cryogenic scanning tunneling\nmicroscopy and spectroscopy, we probe distinct single layer-defined PbSe\nquantum dot populations down to a monolayer with in-gap state free quantum\ndot-like density of states, in agreement with theoretical tight binding\ncalculations. Cryogenic ensemble photoluminescence spectra reveal mono-, bi-,\nand trilayer contribution, confirming the structural, electronic and\ntheoretical results. From larger timescale shifts and ratio changes in the\noptical spectra we infer Ostwald ripening in solution and fusing in deposited\nsamples of thinner flat PbSe quantum dots, which can be slowed down by surface\npassivation with PbI2. By uncovering the interplay between thickness, lateral\nsize and density of states, as well as the synthetic conditions and\npost-synthetic handling, our findings enable the target-oriented synthesis of\ntwo-dimensional PbSe quantum dots with precisely tailored optical properties at\ntelecom wavelengths.",
        "A recent study by Panchagnula et al. [J. Chem. Phys. 161, 054308 (2024)]\nillustrated the non-concordance of a variety of electronic structure methods at\ndescribing the symmetric double-well potential expected along the anisotropic\ndirection of the endofullerene Ne@C$_{70}$. In this article we delve deeper\ninto the difficulties of accurately capturing the dispersion interaction within\nthis system, scrutinising a variety of state-of-the-art density-functional\napproximations (DFAs) and dispersion corrections (DCs). We identify rigorous\ncriteria for the double-well potential and compare the shapes, barrier heights,\nand minima positions obtained with the DFAs and DCs to the correlated\nwavefunction data in the previous study, alongside new coupled-cluster\ncalculations. We show that many of the DFAs are extremely sensitive to the\nnumerical integration grid used, and note that the choice of DC is not\nindependent of the DFA. Functionals with many empirical parameters tuned for\nmain-group thermochemistry do not necessarily result in a reasonable PES, while\nimproved performance can be obtained using nearly dispersionless DFAs with very\nfew empirical parameters and allowing the DC to compensate. We pose the\nNe@C$_{70}$ system as a challenge to functional developers and as a diagnostic\nsystem for testing dispersion corrections, and reiterate the need for more\nexperimental data for comparison.",
        "\\texttt{MetaWFN} is a C++ template-based architecture designed for flexible\nand rapid development of wavefunction-based quantum chemical methods. It is\nhighly modular, extendable, and efficient. This is achieved by decoupling the\nthree distinct aspects of quantum chemical methods\n  (i.e., nature of Hamiltonian, structure of wavefunction, and strategy of\nparallelization ), thereby allowing for separate treatment of them through\ntheir internal type-trait and tagging systems furnished by C++ metaprogramming.\nOnce the second-quantized Hamiltonians, whether nonrelativistic (spin-free) or\nrelativistic (spin-dependent), are decomposed into topologically equivalent\ndiagrams for a unified evaluation of the basic coupling coefficients between\n(randomly selected) spin-free or spin-dependent configuration state functions\nor Slater determinants incorporating full molecular symmetry (including single\nor double point group and spin or time reversal symmetry), the many-electron\nwavefunctions, whether built up with scalar or spinor orbitals, can be\nassembled with the same templates. As for parallelization, \\texttt{MetaWFN}\nsupports both OpenMP and MPI, with the majority of the latter being translated\nautomatically from its OpenMP counterparts. The whole structure of\n\\texttt{MetaWFN} is reviewed here, with some showcases for illustrating its\nperformance.",
        "Recent theoretical studies in quantum spectroscopy have emphasized the\npotential of non-classical correlations in entangled photon pairs for\nselectively targeting specific nonlinear optical processes in nonlinear optical\nresponses. However, because of the extremely low intensity of the nonlinear\noptical signal generated by irradiating molecules with entangled photon pairs,\ntime-resolved spectroscopic measurements using entangled photons have yet to be\nexperimentally implemented. In this paper, we theoretically propose a quantum\nspectroscopy measurement employing a time-resolved fluorescence approach that\naligns with the capabilities of current photon detection technologies. The\nproposed quantum spectroscopy affords two remarkable advantages over\nconventional two-dimensional electronic spectroscopy. First, it enables the\nacquisition of two-dimensional spectra without requiring control over multiple\npulsed lasers. Second, it reduces the complexity of the spectra because the\nspectroscopic signal is contingent upon the nonlinear optical process of\nspontaneous emission. These advantages are similar to those achieved in a\nprevious study [Fujihashi et al., J. Chem. Phys. 160, 104201 (2024)]. However,\nour approach achieves sufficient signal intensities that can be readily\ndetected using existing photon detection technologies, thereby rendering it a\npracticable. Our findings will potentially facilitate the first experimental\nreal-time observation of dynamic processes in molecular systems using quantum\nentangled photon pairs.",
        "The complex absorbing potential (CAP) technique is one of the commonly used\nNon-Hermitian quantum mechanics approaches for characterizing electronic\nresonances. CAP combined with various electronic structure methods has shown\npromising results in quantifying the energies and widths of electronic\nresonances in molecular systems. While CAP-based methods can be used to map\ncomplex potential energy surfaces for resonance states, efficient exploration\nof these surfaces, e.g. geometry optimization or dynamical simulations, require\ninformation on the nuclear gradient. Currently, the only nuclear gradients\navailable for CAP-based methods are for Hartree-Fock and Equation-of-Motion\nCoupled-Cluster method with single and double excitations (J. Chem. Phys. 146,\n031101 (2017)). Here we provide a general approach that relies on projected CAP\nformulation and extends gradients and non-adiabatic couplings formulations\ndeveloped for bound-state electronic structure methods to resonances. The\napproach is not limited to a specific electron structure method and is\ngenerally applicable to any electronic structure methods, provided the\ninformation on the gradients and non-adiabatic couplings is available for bound\nstates. Here, we focus on the State-Averaged Complete Active Space\nSelf-Consistent Field (SA-CASSCF) and Multi-Reference Configurational\nInteraction with Single excitation (MR-CIS) as our methods of choice. We\nestablish the accuracy of the developed gradients and report equilibrium\ngeometries for several representative temporary anion species\n($\\mathrm{N_2^-}$, $\\mathrm{H_2CO^-}$, $\\mathrm{H_2CO_2^-}$ and\n$\\mathrm{C_2H_4^-}$).",
        "We synthesize MoS$_{2}$ atomic layer flakes at different growth conditions to\ntailor S-terminated and Mo-terminated edge defect states that are investigated\nfor their ferromagnetic response. We leverage quantum weak measurement\nprinciples to construct a spin Hall effect of light-based magneto-optic Kerr\neffect (SHEL-MOKE) setup to sense the ultra-small magnetic response from the\nsynthesized atomic layers. Our findings demonstrate that Mo-terminated edge\nstates are the primary source of ferromagnetic response from MoS$_{2}$ flakes,\nwhich is consistent with X-ray photoelectron, Raman and photoluminescence\nspectroscopic results. In the process, we demonstrate SHEL-MOKE to be a robust\ntechnique to investigate ultra weak properties in novel atomic-scale materials.",
        "Incremental full configuration interaction (iFCI) closely approximates the\nFCI limit with polynomial cost through a many-body expansion of the correlation\nenergy, providing highly accurate total energies within a given basis set. To\nextend iFCI beyond previous basis set limitations, this work introduces a novel\nnatural orbital screening approach, iNO-FCI. By consideration of the importance\nof virtual orbital selection in the convergence of iFCI, iNO-FCI maximizes the\nconsistency between orbitals selected for each correlated body. iNO-FCI employs\na principle of cancellation of errors and ensures that the same set of virtual\nNOs are used for interdependent terms. This strategy significantly reduces\ncomputational cost without compromises in precision. Computational savings of\nup to 95 percent are demonstrated, allowing access to larger basis sets that\nwere previously computationally prohibitive. iNO-FCI is herein introduced and\nbenchmarked for several difficult test cases involving double-bond\ndissociation, biradical systems, conjugated $\\pi$ systems, and the spin gap of\na Cu-based transition metal complex.",
        "We have developed a Hubble function based on Newtonian Cosmology using\nnon-commutative fluid equations. Our Hubble function contains cosmic fluids\nwith the signature of a new cosmological parameter $\\sigma$, motivated by a\nnon-commutative Poisson bracket structure. Interestingly, this Hubble function\ndoes not include any external fluid content related to dark energy or the\nCosmological constant; the parameter $\\sigma$ acts as the source of accelerated\nexpansion. In this work, we aim to explain the phenomenon of the accelerating\nexpansion of the universe without \"dark energy\". Additionally, we have verified\nthe observational bounds for $\\sigma$ to assess its potential in explaining the\naccelerated expansion.",
        "We consider local weak solutions to the fractional $p$-Poisson equation of\norder $s$, i.e. $\\left( - \\Delta_p\\right)^s u = f$. In the range $p>1$ and\n$s\\in \\big(\\frac{p-1}{p},1\\big)$ we prove Calder\\'on & Zygmund type estimates\nat the gradient level. More precisely, we show for any $q>1$ that\n\\begin{equation*}\n  f\\in L^{\\frac{qp}{p-1}}_{\\rm loc}\n  \\quad\\Longrightarrow\\quad\n  \\nabla u\\in L^{qp}_{\\rm loc}. \\end{equation*} The qualitative result is\naccompanied by a local quantitative estimate.",
        "The ability to physically move qubits within a register allows the design of\nhardware-specific error correction codes which can achieve fault-tolerance\nwhile respecting other constraints. In particular, recent advancements have\ndemonstrated the shuttling of electron and hole spin qubits through a quantum\ndot array with high fidelity. Exploiting this, we design an error correction\narchitecture, consisting merely of two parallel quantum dot arrays, an\nexperimentally validated architecture compatible with classical wiring and\ncontrol constraints. We develop a suite of heuristic methods for compiling any\nstabilizer error-correcting code's syndrome-extraction circuit to run with a\nminimal number of shuttling operations. In simulation, these heuristics show\nthat fault tolerance can be achieved on several contemporary quantum\nerror-correcting codes requiring only modestly-optimistic noise parameters.\nFurthermore, we demonstrate how constant column-weight qLDPC codes can be\ncompiled in a provably minimal number of shuttles that scales constantly with\ncode size using Shor-style syndrome extraction. In addition, we provide a proof\nof the NP hardness of minimizing the number of shuttle operations for codes not\nin that class.",
        "The process of setting up and successfully running Molecular Dynamics\nSimulations (MDS) is outlined to be incredibly labour and computationally\nexpensive with a very high barrier to entry for newcomers wishing to utilise\nthe benefits and insights of MDS. Here, presented, is a unique Free and\nOpen-Source Software (FOSS) solution that aims to not only reduce the barrier\nof entry for new Molecular Dynamics (MD) users, but also significantly reduce\nthe setup time and hardware utilisation overhead for even highly experienced MD\nresearchers. This is accomplished through the creation of the Molecular\nDynamics Simulation Generator and Analysis Tool (MDSGAT) which currently serves\nas a viable alternative to other restrictive or privatised MDS Graphical\nsolutions with a unique design that allows for seamless collaboration and\ndistribution of exact MD simulation setups and initialisation parameters\nthrough a single setup file. This solution is designed from the start with a\nmodular mindset allowing for additional software expansion to incorporate\nnumerous extra MDS packages and analysis methods over time",
        "Molecules that violate Hund's rule -- having first excited singlet state\n(S$_1$) below the triplet state (T$_1$) -- are rare yet promising as efficient\nlight emitters. Their high-throughput identification demands exceptionally\naccurate excited-state modeling to minimize false positives and negatives.\nBenchmarking twelve S$_1$-T$_1$ energy gaps, we find that local variants of\nADC(2) and CC2 deliver excellent accuracy and speed for screening medium-sized\nmolecules. Notably, while double-hybrid DFT approximations (e.g., B2GP-PLYP and\nPBE-QIDH) exhibit high mean errors ($>100$ meV) despite very low standard\ndeviations ($\\approx10$ meV), exploring their parameter space reveals that a\nconfiguration with 75% exchange and 55% correlation reduces the mean error to\nbelow $5$ meV -- albeit with increased variance. Using this low-bias\nparameterization as an internal reference, we correct the systematic error\nwhile maintaining low variance, effectively combining the strengths of both\nlow-bias and low-variance DFT parameterizations to enhance overall accuracy.\nOur findings suggest that low-variance DFT methods -- often overlooked due to\nhigh bias -- can serve as reliable tools for predictive modeling in\nfirst-principles molecular design.",
        "Determining the dynamical mass profiles of dispersion-supported galaxies is\nparticularly challenging due to projection effects and the unknown shape of\ntheir velocity anisotropy profile. Our goal is to develop a machine learning\nalgorithm capable of recovering dynamical mass profiles of dispersion-supported\ngalaxies from line-of-sight stellar data. Traditionally, this task relies on\ntime-consuming methods that require profile parameterization and assume\ndynamical equilibrium and spherical symmetry. We train a convolutional neural\nnetwork model using various sets of cosmological hydrodynamical simulations of\ngalaxies. By extracting projected stellar data from the simulated galaxies and\nfeeding it into the model, we obtain the posterior distribution of the\ndynamical mass profile at ten different radii. Additionally, we evaluate the\nperformance of existing literature mass estimators on our dataset. Our model\nachieves more accurate results than any literature mass estimator while also\nproviding enclosed mass estimates at radii where no previous estimators exist.\nWe confirm that the posterior distributions produced by the model are\nwell-calibrated, ensuring they provide meaningful uncertainties. However,\nissues remain, as the method loses performance when trained on one set of\nsimulations and applied to another, highlighting the importance of improving\nthe generalization of ML methods trained on specific galaxy simulations.",
        "We propose a mechanism embedded into the foundational infrastructure of a\nblockchain network, designed to improve the utility of idle network resources,\nwhilst enhancing market microstructure efficiency during block production by\nleveraging both network-owned and external capital. By systematically seeking\nto use idle network resources for internally capture arbitrageable\ninefficiencies, the mechanism mitigates extractable value leakage, reduces\nexecution frictions, and improves price formation across venues. This framework\noptimises resource allocation by incentivising an ordered set of transactions\nto be identified and automatically executed at the end of each block,\nredirecting any realised arbitrage income - to marketplaces operating on the\nhost blockchain network (and other stakeholders), which may have otherwise been\nextracted as rent by external actors. Crucially, this process operates without\nintroducing additional inventory risk, ensuring that the network remains a\nneutral facilitator of price discovery. While the systematic framework\ngoverning the distribution of these internally captured returns is beyond the\nscope of this work, reinvesting them to support the ecosystem deployed on the\nhost blockchain network is envisioned to endogenously enhance liquidity,\nstrengthen transactional efficiency, and promote the organic adoption of the\nblockchain for end users. This mechanism is designed specifically for Supra's\nblockchain and seeks to maximally utilise its highly efficient automation\nframework to enhance the blockchain network's efficiency.",
        "We have explored the temporal variability of the seismicity at global scale\nover the last 124 years, as well as its potential drivers. To achieve this, we\nconstructed and analyzed an averaged global seismicity curve for earthquakes of\nmagnitude equal or greater than 6 since 1900. Using Singular Spectrum Analysis,\nwe decomposed this curve and compared the extracted pseudo-cycles with two\nglobal geophysical parameters associated with Earth's tides: length-of-day\nvariations and sea-level changes. Our results reveal that these three\ngeophysical phenomena can be be explained with 90% accuracy, as the sum of up\nto seven periodic components, largely aligned with planetary ephemerides: 1\nyear, 3.4 years (Quasi-Biennial Oscillation, QBO), $\\sim$11 years, $\\sim$14\nyears, $\\sim$18.6 years (lunar nodal cycle), $\\sim$33 years, and $\\sim$60\nyears. We discuss these results in the framework of Laplace's theory, with a\nparticular focus on the phase relationships between seismicity, length-of-day\nvariations, and sea-level changes to further elucidate the underlying physical\nmechanisms. Finally,integrating observations from seismogenic regions, we\npropose a trigger mechanism based on solid Earth-hydrosphere interactions,\nemphasizing the key role of water-rock interactions in modulating earthquake\noccurrence.",
        "The polar magnets A$_2$Mo$_3$O$_8$ (A=Fe, Mn, Co, and Ni) feature a bipartite\nstructure, where the magnetic A$^{2+}$ ions occupy two different sites with\noctahedral and tetrahedral oxygen coordinations. This bipartite structure\nprovides a platform for the emergence of nontrivial magnetoelectric (ME)\neffects and intriguing excitation behaviors, and thus creates significant\nresearch interest. In this study, we conduct inelastic neutron scattering\nmeasurements on single crystals of Mn$_2$Mo$_3$O$_8$, an L-type ferrimagnet in\nthe A$_2$Mo$_3$O$_8$ family, to investigate its spin dynamics. The obtained\nmagnetic excitation spectra reveal two distinct magnon dispersions\ncorresponding to the octahedral and tetrahedral spins in Mn$_2$Mo$_3$O$_8$.\nThese magnon bands can be well described by a spin Hamiltonian including\nHeisenberg and single-ion anisotropy terms. Employing our effective spin model,\nwe successfully reproduce the unusual temperature dependence of the L-type\nferrimagnetic susceptibility through self-consistent mean-field theory. This\nresearch reveals the significance of the bipartite structure in determining the\nexcitation properties of the polar magnets $\\rm{A_{2}Mo_{3}O_{8}}$ and provides\nvaluable insights into the spin dynamics of L-type ferrimagnets.",
        "Spin-orbit coupling (SOC) is one of the key factors that affect the chiral\nsymmetry of matter by causing the spatial symmetry breaking of the system. We\nfind that Raman-induced SOC can induce a chiral supersolid phase with a helical\nantiskyrmion lattice in balanced Rydberg-dressed two-component Bose-Einstein\ncondensates (BECs) in a harmonic trap by modulating the Raman coupling\nstrength, strong contrast with the mirror symmetric supersolid phase containing\nskyrmion-antiskyrmion lattice pair for the case of Rashba SOC. Two ground-state\nphase diagrams are presented as a function of the Rydberg interaction strength\nand the SOC strength, as well as that of the Rydberg interaction strength and\nthe Raman coupling strength, respectively. It is shown that the interplay among\nRaman-induced SOC, soft-core long-range Rydberg interactions, and contact\ninteractions favors rich ground-state structures including half-quantum vortex\nphase, stripe supersolid phase, toroidal stripe phase with a central\nAnderson-Toulouse coreless vortex, checkerboard supersolid phase, mirror\nsymmetric supersolid phase, chiral supersolid phase and standing-wave\nsupersolid phase. In addition, the effects of rotation and in-plane quadrupole\nmagnetic field on the ground state of the system are analyzed. In these two\ncases, the chiral supersolid phase is broken and the ground state tends to form\na miscible phase. Furthermore, the stability and superfluid properties of the\ntwo-component BECs with Raman-induced SOC and Rydberg interactions in free\nspace are revealed by solving the Bogoliubov-de Gennes equation. Finally, we\ndemonstrate that when the initial state is a chiral supersolid phase the\nrotating harmonic trapped system sustains dissipative continuous time crystal\nby studying the rotational dynamic behaviors of the system.",
        "Dark photons are a well-motivated candidate for dark matter, but their\ndetection becomes challenging for ultralight masses with both experimental and\nastrophysical probes. In this work, we propose a new approach to explore this\nregime through the dark inverse Compton scattering of ultralight dark photons\nwith cosmic ray electrons and positrons. We show this process generates a\npotentially observable background radiation that is most prominent at\nfrequencies below MHz. We compute this effect using the latest cosmic ray\nmodels and radio absorption maps. Comparing it to observations of the Milky\nWay's radio spectrum from Explorer 43, Radio Astronomy Explorer 2, and the\nParker Solar Probe, we place leading constraints on the kinetic mixing of dark\nphoton dark matter for masses $\\lesssim 2 \\times 10^{-17} \\ \\rm eV$.",
        "In this article we prove a version of Kolyvagin's conjecture for modular\nforms at non-ordinary primes. In particular, we generalize the work of Wang on\na converse to a higher weight Gross-Zagier-Kolyvagin theorem in order to prove\nthe conjecture under the hypothesis that some Selmer group has rank one. The\nmain ingredients that we use in non-ordinary setting are the signed Selmer\ngroups introduced by Lei, Loeffler and Zerbes. We will also use a result of\nWan, i.e., the $p$-part of the Tamagawa number conjecture for non-ordinary\nmodular forms with analytic rank zero. Starting from the rank one case we will\nshow how to prove the full version of the conjecture.",
        "In this paper, we establish the global H\\\"older gradient estimate for\nsolutions to the Dirichlet problem of the Monge-Amp\\`ere equation $\\det D^2u =\nf$ on strictly convex but not uniformly convex domain $\\Omega$.",
        "Inverse velocity dispersion (IVD) events, characterized by higher-energy\nparticles arriving later than lower-energy particles, challenge the classical\nunderstanding of SEP events and are increasingly observed by spacecraft, such\nas Parker Solar Probe (PSP) and Solar Orbiter (SolO). However, the mechanisms\nunderlying IVD events remain poorly understood. This study aims to investigate\nthe physical processes responsible for long-duration IVD events by analyzing\nthe SEP event observed by SolO on 2022 June 7. We explore the role of evolving\nshock connectivity, particle acceleration at interplanetary (IP) shocks, and\ncross-field transport in shaping the observed particle profiles.We utilize data\nfrom Energetic Particle Detector (EPD) suite onboard SolO to analyze the\ncharacteristics of the IVD, and model the event using the Heliospheric\nEnergetic Particle Acceleration and Transport (HEPAT) model. The IVD event\nexhibited a distinct and long-duration IVD signature, across proton energies\nfrom 1 to 20 MeV and lasting for approximately 10 hours. Simulations suggest\nthat evolving shock connectivity and the evolution of shock play a primary role\nin the IVD signature, with SolO transitioning from shock flank to nose over\ntime, resulting in a gradual increase in maximum particle energy along the\nfield line. Furthermore, model results show that limited cross-field diffusion\ncan influence both the nose energy and the duration of the IVD event. This\nstudy demonstrates that long-duration IVD events are primarily driven by\nevolving magnetic connectivity along a non-uniform shock that evolves over\ntime, where the connection moves to more efficient acceleration sites as the\nshock propagates farther from the Sun. Other mechanisms, such as acceleration\ntime at the shock, may also contribute to the observed IVD features.",
        "The gaussian free field on the unit disk $D$ can be seen as a two-dimensional\nversion of the Brownian bridge on the interval [0,1]. It is intrinsically\nassociated with the Sobolev space $H_0^1 (D)$. To define the latter, we can\nchoose any metric conformally equivalent to the Euclidean metric on $D$. This\nnote is an introduction to the gaussian free field on the unit disk whose aim\nis to highlight some of the conveniences offered by hyperbolic geometryon $D$\nto describe the first properties of this probabilistic object."
      ]
    }
  },
  {
    "id":2411.06741,
    "research_type":"applied",
    "start_id":"b20",
    "start_title":"Physics-informed machine learning",
    "start_abstract":"Despite great progress in simulating multiphysics problems using the numerical discretization of partial differential equations (PDEs), one still cannot seamlessly incorporate noisy data into existing algorithms, mesh generation remains complex, and high-dimensional problems governed by parameterized PDEs cannot be tackled. Moreover, solving inverse problems with hidden physics is often prohibitively expensive and requires different formulations and elaborate computer codes. Machine learning has emerged as a promising alternative, but training deep neural networks requires big data, not always available for scientific problems. Instead, such networks can be trained from additional information obtained by enforcing the physical laws (for example, at random points in the continuous space-time domain). Such physics-informed learning integrates (noisy) data and mathematical models, and implements them through neural networks or other kernel-based regression networks. Moreover, it may be possible to design specialized network architectures that automatically satisfy some of the physical invariants for better accuracy, faster training and improved generalization. Here, we review some of the prevailing trends in embedding physics into machine learning, present some of the current capabilities and limitations and discuss diverse applications of physics-informed learning both for forward and inverse problems, including discovering hidden physics and tackling high-dimensional problems.",
    "start_categories":[
      "physics.chem-ph"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Learning Polynomials with Neural Networks"
      ],
      "abstract":[
        "We study the effectiveness of learning low degree polynomials using neural networks by gradient descent method. While have been shown to great expressive power, and has widely used in practice for networks, few theoretical guarantees are known such methods. In particular, it is well that can get stuck at local minima, even simple classes target functions. this paper, we present several positive results support networks. focus on twolayer where bottom layer a set non-linear hidden nodes, top node linear function, similar Barron (1993). First show randomly initialized network with sufficiently many units, generic algorithm learns any polynomial, assuming initialize weights randomly. Secondly, if use complex-valued (the function still be real), then under suitable conditions, there no robust minima: always escape minimum performing random perturbation. This property does not hold real-valued weights. Thirdly, discuss whether sparse learned small size dependent sparsity function."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Memorize or Generalize? Evaluating LLM Code Generation with Evolved\n  Questions",
        "Generative AI in Education: From Foundational Insights to the Socratic\n  Playground for Learning",
        "Graph-Augmented Reasoning: Evolving Step-by-Step Knowledge Graph\n  Retrieval for LLM Reasoning",
        "Make Full Use of Testing Information: An Integrated Accelerated Testing\n  and Evaluation Method for Autonomous Driving Systems",
        "AgentSafe: Safeguarding Large Language Model-based Multi-agent Systems\n  via Hierarchical Data Management",
        "Agent models: Internalizing Chain-of-Action Generation into Reasoning\n  models",
        "Benchmarking Retrieval-Augmented Generation in Multi-Modal Contexts",
        "Parallel Belief Contraction via Order Aggregation",
        "Palatable Conceptions of Disembodied Being: Terra Incognita in the Space\n  of Possible Minds",
        "STGDPM:Vessel Trajectory Prediction with Spatio-Temporal Graph Diffusion\n  Probabilistic Model",
        "MathMistake Checker: A Comprehensive Demonstration for Step-by-Step Math\n  Problem Mistake Finding by Prompt-Guided LLMs",
        "Contextual bandits with entropy-based human feedback",
        "Governing AI Agents",
        "Towards Data-Efficient Pretraining for Atomic Property Prediction",
        "MMXU: A Multi-Modal and Multi-X-ray Understanding Dataset for Disease\n  Progression",
        "Activation Steering in Neural Theorem Provers",
        "Optimal Actuator Attacks on Autonomous Vehicles Using Reinforcement\n  Learning",
        "Spectral Unmixing Comparison with Sparse, Iterative and Mixed Integer\n  Programming Models",
        "Convergence analysis for a variant of manifold proximal point algorithm\n  based on Kurdyka-{\\L}ojasiewicz property",
        "Structure-Aware Correspondence Learning for Relative Pose Estimation",
        "Fixed point results for single and multi-valued three-points\n  contractions",
        "Data-driven Control of T-Product-based Dynamical Systems",
        "Exploring the Effects of Level of Control in the Initialization of\n  Shared Whiteboarding Sessions in Collaborative Augmented Reality",
        "Unveiling Hidden Pivotal Players with GoalNet: A GNN-Based Soccer Player\n  Evaluation System",
        "Incremental Approximate Single-Source Shortest Paths with Predictions",
        "How does the restriction of representations change under translations? A\n  story for the general linear groups and the unitary groups",
        "Understanding SGD with Exponential Moving Average: A Case Study in\n  Linear Regression",
        "Modeling of stochastic processes in $L_p(T)$ using orthogonal\n  polynomials"
      ],
      "abstract":[
        "Large Language Models (LLMs) are known to exhibit a memorization phenomenon\nin code generation: instead of truly understanding the underlying principles of\na programming problem, they tend to memorize the original prompt and its\nsolution together in the training. Consequently, when facing variants of the\noriginal problem, their answers very likely resemble the memorized solutions\nand fail to generalize. In this paper, we investigate this phenomenon by\ndesigning three evolution strategies to create variants: mutation,\nparaphrasing, and code-rewriting. By comparing the performance and AST\nsimilarity of the LLM-generated codes before and after these three evolutions,\nwe develop a memorization score that positively correlates with the level of\nmemorization. As expected, as supervised fine-tuning goes on, the memorization\nscore rises before overfitting, suggesting more severe memorization. We\ndemonstrate that common mitigation approaches, such as prompt translation and\nusing evolved variants as data augmentation in supervised learning and\nreinforcement learning, either compromise the performance or fail to alleviate\nthe memorization issue. Therefore, memorization remains a significant challenge\nin LLM code generation, highlighting the need for a more effective solution.",
        "This paper explores the synergy between human cognition and Large Language\nModels (LLMs), highlighting how generative AI can drive personalized learning\nat scale. We discuss parallels between LLMs and human cognition, emphasizing\nboth the promise and new perspectives on integrating AI systems into education.\nAfter examining challenges in aligning technology with pedagogy, we review\nAutoTutor-one of the earliest Intelligent Tutoring Systems (ITS)-and detail its\nsuccesses, limitations, and unfulfilled aspirations. We then introduce the\nSocratic Playground, a next-generation ITS that uses advanced transformer-based\nmodels to overcome AutoTutor's constraints and provide personalized, adaptive\ntutoring. To illustrate its evolving capabilities, we present a JSON-based\ntutoring prompt that systematically guides learner reflection while tracking\nmisconceptions. Throughout, we underscore the importance of placing pedagogy at\nthe forefront, ensuring that technology's power is harnessed to enhance\nteaching and learning rather than overshadow it.",
        "Recent large language model (LLM) reasoning, despite its success, suffers\nfrom limited domain knowledge, susceptibility to hallucinations, and\nconstrained reasoning depth, particularly in small-scale models deployed in\nresource-constrained environments. This paper presents the first investigation\ninto integrating step-wise knowledge graph retrieval with step-wise reasoning\nto address these challenges, introducing a novel paradigm termed as\ngraph-augmented reasoning. Our goal is to enable frozen, small-scale LLMs to\nretrieve and process relevant mathematical knowledge in a step-wise manner,\nenhancing their problem-solving abilities without additional training. To this\nend, we propose KG-RAR, a framework centered on process-oriented knowledge\ngraph construction, a hierarchical retrieval strategy, and a universal\npost-retrieval processing and reward model (PRP-RM) that refines retrieved\ninformation and evaluates each reasoning step. Experiments on the Math500 and\nGSM8K benchmarks across six models demonstrate that KG-RAR yields encouraging\nresults, achieving a 20.73\\% relative improvement with Llama-3B on Math500.",
        "Testing and evaluation is an important step before the large-scale\napplication of the autonomous driving systems (ADSs). Based on the three level\nof scenario abstraction theory, a testing can be performed within a logical\nscenario, followed by an evaluation stage which is inputted with the testing\nresults of each concrete scenario generated from the logical parameter space.\nDuring the above process, abundant testing information is produced which is\nbeneficial for comprehensive and accurate evaluations. To make full use of\ntesting information, this paper proposes an Integrated accelerated Testing and\nEvaluation Method (ITEM). Based on a Monte Carlo Tree Search (MCTS) paradigm\nand a dual surrogates testing framework proposed in our previous work, this\npaper applies the intermediate information (i.e., the tree structure, including\nthe affiliation of each historical sampled point with the subspaces and the\nparent-child relationship between subspaces) generated during the testing stage\ninto the evaluation stage to achieve accurate hazardous domain identification.\nMoreover, to better serve this purpose, the UCB calculation method is improved\nto allow the search algorithm to focus more on the hazardous domain boundaries.\nFurther, a stopping condition is constructed based on the convergence of the\nsearch algorithm. Ablation and comparative experiments are then conducted to\nverify the effectiveness of the improvements and the superiority of the\nproposed method. The experimental results show that ITEM could well identify\nthe hazardous domains in both low- and high-dimensional cases, regardless of\nthe shape of the hazardous domains, indicating its generality and potential for\nthe safety evaluation of ADSs.",
        "Large Language Model based multi-agent systems are revolutionizing autonomous\ncommunication and collaboration, yet they remain vulnerable to security threats\nlike unauthorized access and data breaches. To address this, we introduce\nAgentSafe, a novel framework that enhances MAS security through hierarchical\ninformation management and memory protection. AgentSafe classifies information\nby security levels, restricting sensitive data access to authorized agents.\nAgentSafe incorporates two components: ThreatSieve, which secures communication\nby verifying information authority and preventing impersonation, and\nHierarCache, an adaptive memory management system that defends against\nunauthorized access and malicious poisoning, representing the first systematic\ndefense for agent memory. Experiments across various LLMs show that AgentSafe\nsignificantly boosts system resilience, achieving defense success rates above\n80% under adversarial conditions. Additionally, AgentSafe demonstrates\nscalability, maintaining robust performance as agent numbers and information\ncomplexity grow. Results underscore effectiveness of AgentSafe in securing MAS\nand its potential for real-world application.",
        "Traditional agentic workflows rely on external prompts to manage interactions\nwith tools and the environment, which limits the autonomy of reasoning models.\nWe position \\emph{Large Agent Models (LAMs)} that internalize the generation of\n\\emph{Chain-of-Action (CoA)}, enabling the model to autonomously decide when\nand how to use external tools. Our proposed AutoCoA framework combines\nsupervised fine-tuning (SFT) and reinforcement learning (RL), allowing the\nmodel to seamlessly switch between reasoning and action while efficiently\nmanaging environment interactions. Main components include step-level action\ntriggering, trajectory-level CoA optimization, and an internal world model to\nreduce real-environment interaction costs. Evaluations on open-domain QA tasks\ndemonstrate that AutoCoA-trained agent models significantly outperform\nReAct-based workflows in task completion, especially in tasks that require\nlong-term reasoning and multi-step actions. Code and dataset are available at\nhttps:\/\/github.com\/ADaM-BJTU\/AutoCoA",
        "This paper introduces Multi-Modal Retrieval-Augmented Generation (M^2RAG), a\nbenchmark designed to evaluate the effectiveness of Multi-modal Large Language\nModels (MLLMs) in leveraging knowledge from multi-modal retrieval documents.\nThe benchmark comprises four tasks: image captioning, multi-modal question\nanswering, multi-modal fact verification, and image reranking. All tasks are\nset in an open-domain setting, requiring RAG models to retrieve query-relevant\ninformation from a multi-modal document collection and use it as input context\nfor RAG modeling. To enhance the context utilization capabilities of MLLMs, we\nalso introduce Multi-Modal Retrieval-Augmented Instruction Tuning (MM-RAIT), an\ninstruction tuning method that optimizes MLLMs within multi-modal contexts. Our\nexperiments show that MM-RAIT improves the performance of RAG systems by\nenabling them to effectively learn from multi-modal contexts. All data and code\nare available at https:\/\/github.com\/NEUIR\/M2RAG.",
        "The standard ``serial'' (aka ``singleton'') model of belief contraction\nmodels the manner in which an agent's corpus of beliefs responds to the removal\nof a single item of information. One salient extension of this model introduces\nthe idea of ``parallel'' (aka ``package'' or ``multiple'') change, in which an\nentire set of items of information are simultaneously removed. Existing\nresearch on the latter has largely focussed on single-step parallel\ncontraction: understanding the behaviour of beliefs after a single parallel\ncontraction. It has also focussed on generalisations to the parallel case of\nserial contraction operations whose characteristic properties are extremely\nweak. Here we consider how to extend serial contraction operations that obey\nstronger properties. Potentially more importantly, we also consider the\niterated case: the behaviour of beliefs after a sequence of parallel\ncontractions. We propose a general method for extending serial iterated belief\nchange operators to handle parallel change based on an n-ary generalisation of\nBooth & Chandler's TeamQueue binary order aggregators.",
        "Is it possible to articulate a conception of consciousness that is compatible\nwith the exotic characteristics of contemporary, disembodied AI systems, and\nthat can stand up to philosophical scrutiny? How would subjective time and\nselfhood show up for an entity that conformed to such a conception? Trying to\nanswer these questions, even metaphorically, stretches the language of\nconsciousness to breaking point. Ultimately, the attempt yields something like\nemptiness, in the Buddhist sense, and helps to undermine our dualistic\ninclinations towards subjectivity and selfhood.",
        "Vessel trajectory prediction is a critical component for ensuring maritime\ntraffic safety and avoiding collisions. Due to the inherent uncertainty in\nvessel behavior, trajectory prediction systems must adopt a multimodal approach\nto accurately model potential future motion states. However, existing vessel\ntrajectory prediction methods lack the ability to comprehensively model\nbehavioral multi-modality. To better capture multimodal behavior in interactive\nscenarios, we propose modeling interactions as dynamic graphs, replacing\ntraditional aggregation-based techniques that rely on vessel states. By\nleveraging the natural multimodal capabilities of diffusion models, we frame\nthe trajectory prediction task as an inverse process of motion uncertainty\ndiffusion, wherein uncertainties across potential navigational areas are\nprogressively eliminated until the desired trajectories is produced. In\nsummary, we pioneer the integration of Spatio-Temporal Graph (STG) with\ndiffusion models in ship trajectory prediction. Extensive experiments on real\nAutomatic Identification System (AIS) data validate the superiority of our\napproach.",
        "We propose a novel system, MathMistake Checker, designed to automate\nstep-by-step mistake finding in mathematical problems with lengthy answers\nthrough a two-stage process. The system aims to simplify grading, increase\nefficiency, and enhance learning experiences from a pedagogical perspective. It\nintegrates advanced technologies, including computer vision and the\nchain-of-thought capabilities of the latest large language models (LLMs). Our\nsystem supports open-ended grading without reference answers and promotes\npersonalized learning by providing targeted feedback. We demonstrate its\neffectiveness across various types of math problems, such as calculation and\nword problems.",
        "In recent years, preference-based human feedback mechanisms have become\nessential for enhancing model performance across diverse applications,\nincluding conversational AI systems such as ChatGPT. However, existing\napproaches often neglect critical aspects, such as model uncertainty and the\nvariability in feedback quality. To address these challenges, we introduce an\nentropy-based human feedback framework for contextual bandits, which\ndynamically balances exploration and exploitation by soliciting expert feedback\nonly when model entropy exceeds a predefined threshold. Our method is\nmodel-agnostic and can be seamlessly integrated with any contextual bandit\nagent employing stochastic policies. Through comprehensive experiments, we show\nthat our approach achieves significant performance improvements while requiring\nminimal human feedback, even under conditions of suboptimal feedback quality.\nThis work not only presents a novel strategy for feedback solicitation but also\nhighlights the robustness and efficacy of incorporating human guidance into\nmachine learning systems. Our code is publicly available:\nhttps:\/\/github.com\/BorealisAI\/CBHF",
        "The field of AI is undergoing a fundamental transition from generative models\nthat can produce synthetic content to artificial agents that can plan and\nexecute complex tasks with only limited human involvement. Companies that\npioneered the development of language models have now built AI agents that can\nindependently navigate the internet, perform a wide range of online tasks, and\nincreasingly serve as AI personal assistants and virtual coworkers. The\nopportunities presented by this new technology are tremendous, as are the\nassociated risks. Fortunately, there exist robust analytic frameworks for\nconfronting many of these challenges, namely, the economic theory of\nprincipal-agent problems and the common law doctrine of agency relationships.\nDrawing on these frameworks, this Article makes three contributions. First, it\nuses agency law and theory to identify and characterize problems arising from\nAI agents, including issues of information asymmetry, discretionary authority,\nand loyalty. Second, it illustrates the limitations of conventional solutions\nto agency problems: incentive design, monitoring, and enforcement might not be\neffective for governing AI agents that make uninterpretable decisions and\noperate at unprecedented speed and scale. Third, the Article explores the\nimplications of agency law and theory for designing and regulating AI agents,\narguing that new technical and legal infrastructure is needed to support\ngovernance principles of inclusivity, visibility, and liability.",
        "This paper challenges the recent paradigm in atomic property prediction that\nlinks progress to growing dataset sizes and computational resources. We show\nthat pretraining on a carefully selected, task-relevant dataset can match or\neven surpass large-scale pretraining, while using as little as 1\/24th of the\ncomputational cost. We introduce the Chemical Similarity Index (CSI), a novel\nmetric inspired by computer vision's Fr\\'echet Inception Distance, for\nmolecular graphs which quantifies the alignment between upstream pretraining\ndatasets and downstream tasks. By selecting the most relevant dataset with\nminimal CSI distance, we show that models pretrained on a smaller, focused\ndataset consistently outperform those pretrained on massive, mixed datasets\nsuch as JMP, even when those larger datasets include the relevant dataset.\nCounterintuitively, we also find that indiscriminately adding more data can\ndegrade model performance when the additional data poorly aligns with the task\nat hand. Our findings highlight that quality often outperforms quantity in\npretraining for atomic property prediction.",
        "Large vision-language models (LVLMs) have shown great promise in medical\napplications, particularly in visual question answering (MedVQA) and diagnosis\nfrom medical images. However, existing datasets and models often fail to\nconsider critical aspects of medical diagnostics, such as the integration of\nhistorical records and the analysis of disease progression over time. In this\npaper, we introduce MMXU (Multimodal and MultiX-ray Understanding), a novel\ndataset for MedVQA that focuses on identifying changes in specific regions\nbetween two patient visits. Unlike previous datasets that primarily address\nsingle-image questions, MMXU enables multi-image questions, incorporating both\ncurrent and historical patient data. We demonstrate the limitations of current\nLVLMs in identifying disease progression on MMXU-\\textit{test}, even those that\nperform well on traditional benchmarks. To address this, we propose a\nMedRecord-Augmented Generation (MAG) approach, incorporating both global and\nregional historical records. Our experiments show that integrating historical\nrecords significantly enhances diagnostic accuracy by at least 20\\%, bridging\nthe gap between current LVLMs and human expert performance. Additionally, we\nfine-tune models with MAG on MMXU-\\textit{dev}, which demonstrates notable\nimprovements. We hope this work could illuminate the avenue of advancing the\nuse of LVLMs in medical diagnostics by emphasizing the importance of historical\ncontext in interpreting medical images. Our dataset is released at\n\\href{https:\/\/github.com\/linjiemu\/MMXU}{https:\/\/github.com\/linjiemu\/MMXU}.",
        "Large Language Models (LLMs) have shown promise in proving formal theorems\nusing proof assistants like Lean. However, current state of the art language\nmodels struggles to predict next step in proofs leading practitioners to use\ndifferent sampling techniques to improve LLMs capabilities. We observe that the\nLLM is capable of predicting the correct tactic; however, it faces challenges\nin ranking it appropriately within the set of candidate tactics, affecting the\noverall selection process. To overcome this hurdle, we use activation steering\nto guide LLMs responses to improve the generations at the time of inference.\nOur results suggest that activation steering offers a promising lightweight\nalternative to specialized fine-tuning for enhancing theorem proving\ncapabilities in LLMs, particularly valuable in resource-constrained\nenvironments.",
        "With the increasing prevalence of autonomous vehicles (AVs), their\nvulnerability to various types of attacks has grown, presenting significant\nsecurity challenges. In this paper, we propose a reinforcement learning\n(RL)-based approach for designing optimal stealthy integrity attacks on AV\nactuators. We also analyze the limitations of state-of-the-art RL-based secure\ncontrollers developed to counter such attacks. Through extensive simulation\nexperiments, we demonstrate the effectiveness and efficiency of our proposed\nmethod.",
        "Hyperspectral unmixing is the analytical process of determining the pure\nmaterials and estimating the proportions of such materials composed within an\nobserved mixed pixel spectrum. We can unmix mixed pixel spectra using linear\nand nonlinear mixture models. Ordinary least squares (OLS) regression serves as\nthe foundation for many linear mixture models employed in Hyperspectral Image\nanalysis. Though variations of OLS are implemented, studies rarely address the\nunderlying assumptions that affect results. This paper provides an in depth\ndiscussion on the assumptions inherently endorsed by the application of OLS\nregression. We also examine variations of OLS models stemming from highly\neffective approaches in spectral unmixing -- sparse regression, iterative\nfeature search strategies and Mathematical programming. These variations are\ncompared to a novel unmixing approach called HySUDeB. We evaluated each\napproach's performance by computing the average error and precision of each\nmodel. Additionally, we provide a taxonomy of the molecular structure of each\nmineral to derive further understanding into the detection of the target\nmaterials.",
        "We incorporate an iteratively reweighted strategy in the manifold proximal\npoint algorithm (ManPPA) in [12] to solve an enhanced sparsity inducing model\nfor identifying sparse yet nonzero vectors in a given subspace. We establish\nthe global convergence of the whole sequence generated by our algorithm by\nassuming the Kurdyka-Lojasiewicz (KL) properties of suitable potential\nfunctions. We also study how the KL exponents of the different potential\nfunctions are related. More importantly, when our enhanced model and algorithm\nreduce, respectively, to the model and ManPPA with constant stepsize considered\nin [12], we show that the sequence generated converges linearly as long as the\noptimal value of the model is positive, and converges finitely when the limit\nof the sequence lies in a set of weak sharp minima. Our results improve [13,\nTheorem 2.4], which asserts local quadratic convergence in the presence of weak\nsharp minima when the constant stepsize is small.",
        "Relative pose estimation provides a promising way for achieving\nobject-agnostic pose estimation. Despite the success of existing 3D\ncorrespondence-based methods, the reliance on explicit feature matching suffers\nfrom small overlaps in visible regions and unreliable feature estimation for\ninvisible regions. Inspired by humans' ability to assemble two object parts\nthat have small or no overlapping regions by considering object structure, we\npropose a novel Structure-Aware Correspondence Learning method for Relative\nPose Estimation, which consists of two key modules. First, a structure-aware\nkeypoint extraction module is designed to locate a set of kepoints that can\nrepresent the structure of objects with different shapes and appearance, under\nthe guidance of a keypoint based image reconstruction loss. Second, a\nstructure-aware correspondence estimation module is designed to model the\nintra-image and inter-image relationships between keypoints to extract\nstructure-aware features for correspondence estimation. By jointly leveraging\nthese two modules, the proposed method can naturally estimate 3D-3D\ncorrespondences for unseen objects without explicit feature matching for\nprecise relative pose estimation. Experimental results on the CO3D, Objaverse\nand LineMOD datasets demonstrate that the proposed method significantly\noutperforms prior methods, i.e., with 5.7{\\deg}reduction in mean angular error\non the CO3D dataset.",
        "In this paper, we are concerned with the study of the existence of fixed\npoints for single and multi-valued three-points contractions. Namely, we first\nintroduce a new class of single-valued mappings defined on a metric space\nequipped with three metrics. A fixed point theorem is established for such\nmappings. The obtained result recovers that established recently by the second\nauthor [J. Fixed Point Theory Appl. 25 (2023) 74] for the class of\nsingle-valued mappings contracting perimeters of triangles. We next extend our\nstudy by introducing the class of multivalued three points contractions. A\nfixed point theorem, which is a multi-valued version of that obtained in the\nabove reference, is established. Some examples showing the validity of our\nobtained results are provided.",
        "Data-driven control is a powerful tool that enables the design and\nimplementation of control strategies directly from data without explicitly\nidentifying the underlying system dynamics. While various data-driven control\ntechniques, such as stabilization, linear quadratic regulation, and model\npredictive control, have been extensively developed, these methods are not\ninherently suited for multi-linear dynamical systems, where the states are\nrepresented as higher-order tensors. In this article, we propose a novel\nframework for data-driven control of T-product-based dynamical systems (TPDSs),\nwhere the system evolution is governed by the T-product between a third-order\ndynamic tensor and a third-order state tensor. In particular, we offer\nnecessary and sufficient conditions to determine the data informativity for\nsystem identification, stabilization by state feedback, and T-product quadratic\nregulation of TPDSs with detailed complexity analyses. Finally, we validate our\nframework through numerical examples.",
        "Augmented Reality (AR) collaboration can benefit from a shared 2D surface,\nsuch as a whiteboard. However, many features of each collaborators physical\nenvironment must be considered in order to determine the best placement and\nshape of the shared surface. We explored the effects of three methods for\nbeginning a collaborative whiteboarding session with varying levels of user\ncontrol: MANUAL, DISCRETE CHOICE, and AUTOMATIC by conducting a simulated AR\nstudy within Virtual Reality (VR). In the MANUAL method, users draw their own\nsurfaces directly in the environment until they agree on the placement; in the\nDISCRETE CHOICE method, the system provides three options for whiteboard size\nand location; and in the AUTOMATIC method, the system automatically creates a\nwhiteboard that fits within each collaborators environment. We evaluate these\nthree conditions in a study in which two collaborators used each method to\nbegin collaboration sessions. After establishing a session, the users worked\ntogether to complete an affinity diagramming task using the shared whiteboard.\nWe found that the majority of participants preferred to have direct control\nduring the initialization of a new collaboration session, despite the\nadditional workload induced by the Manual method.",
        "Soccer analysis tools emphasize metrics such as expected goals, leading to an\noverrepresentation of attacking players' contributions and overlooking players\nwho facilitate ball control and link attacks. Examples include Rodri from\nManchester City and Palhinha who just transferred to Bayern Munich. To address\nthis bias, we aim to identify players with pivotal roles in a soccer team,\nincorporating both spatial and temporal features.\n  In this work, we introduce a GNN-based framework that assigns individual\ncredit for changes in expected threat (xT), thus capturing overlooked yet vital\ncontributions in soccer. Our pipeline encodes both spatial and temporal\nfeatures in event-centric graphs, enabling fair attribution of non-scoring\nactions such as defensive or transitional plays. We incorporate centrality\nmeasures into the learned player embeddings, ensuring that ball-retaining\ndefenders and defensive midfielders receive due recognition for their overall\nimpact. Furthermore, we explore diverse GNN variants-including Graph Attention\nNetworks and Transformer-based models-to handle long-range dependencies and\nevolving match contexts, discussing their relative performance and\ncomputational complexity. Experiments on real match data confirm the robustness\nof our approach in highlighting pivotal roles that traditional attacking\nmetrics typically miss, underscoring the model's utility for more comprehensive\nsoccer analytics.",
        "The algorithms-with-predictions framework has been used extensively to\ndevelop online algorithms with improved beyond-worst-case competitive ratios.\nRecently, there is growing interest in leveraging predictions for designing\ndata structures with improved beyond-worst-case running times. In this paper,\nwe study the fundamental data structure problem of maintaining approximate\nshortest paths in incremental graphs in the algorithms-with-predictions model.\nGiven a sequence $\\sigma$ of edges that are inserted one at a time, the goal is\nto maintain approximate shortest paths from the source to each vertex in the\ngraph at each time step. Before any edges arrive, the data structure is given a\nprediction of the online edge sequence $\\hat{\\sigma}$ which is used to ``warm\nstart'' its state.\n  As our main result, we design a learned algorithm that maintains\n$(1+\\epsilon)$-approximate single-source shortest paths, which runs in\n$\\tilde{O}(m \\eta \\log W\/\\epsilon)$ time, where $W$ is the weight of the\nheaviest edge and $\\eta$ is the prediction error. We show these techniques\nimmediately extend to the all-pairs shortest-path setting as well. Our\nalgorithms are consistent (performing nearly as fast as the offline algorithm)\nwhen predictions are nearly perfect, have a smooth degradation in performance\nwith respect to the prediction error and, in the worst case, match the best\noffline algorithm up to logarithmic factors.\n  As a building block, we study the offline incremental approximate\nsingle-source shortest-paths problem. In this problem, the edge sequence\n$\\sigma$ is known a priori and the goal is to efficiently return the length of\nthe shortest paths in the intermediate graph $G_t$ consisting of the first $t$\nedges, for all $t$. Note that the offline incremental problem is defined in the\nworst-case setting (without predictions) and is of independent interest.",
        "We present a new approach to symmetry breaking for pairs of real forms of\n$(GL(n, \\mathbb{C}), GL(n-1, \\mathbb{C}))$. While translation functors are a\nuseful tool for studying a family of representations of a single reductive\ngroup $G$, when applied to a pair of groups $G \\supset G'$,translation functors\ncan significantly alter the nature of symmetry breaking between the\nrepresentations of $G$ and $G'$, even within the same Weyl chamber of the\ndirect product group $G \\times G'$. We introduce the concept of \\lq\\lq{fences\nfor the interlacing pattern}\\rq\\rq,which provides a refinement of the usual\nnotion of \\lq\\lq{walls for Weyl chambers}\\rq\\rq. We then present a theorem that\nstates that multiplicity is constant unless these \\lq\\lq{fences}\\rq\\rq\\ are\ncrossed. This general theorem is illustrated with examples of both tempered and\nnon-tempered representations. Additionally,we provide a new non-vanishing\ntheorem of period integrals for pairs of reductive symmetric spaces,which is\nfurther strengthened through this approach.",
        "Exponential moving average (EMA) has recently gained significant popularity\nin training modern deep learning models, especially diffusion-based generative\nmodels. However, there have been few theoretical results explaining the\neffectiveness of EMA. In this paper, to better understand EMA, we establish the\nrisk bound of online SGD with EMA for high-dimensional linear regression, one\nof the simplest overparameterized learning tasks that shares similarities with\nneural networks. Our results indicate that (i) the variance error of SGD with\nEMA is always smaller than that of SGD without averaging, and (ii) unlike SGD\nwith iterate averaging from the beginning, the bias error of SGD with EMA\ndecays exponentially in every eigen-subspace of the data covariance matrix.\nAdditionally, we develop proof techniques applicable to the analysis of a broad\nclass of averaging schemes.",
        "In this paper, models that approximate stochastic processes from the space\n$Sub_\\varphi(\\Omega)$ with given reliability and accuracy in $L_p(T)$ are\nconsidered for some specific functions $\\varphi(t)$. For processes that are\ndecomposited in series using orthonormal bases, such models are constructed in\nthe case where elements of such decomposition cannot be found explicitly."
      ]
    }
  },
  {
    "id":2411.18259,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"Physics-informed machine learning",
    "start_abstract":"Despite great progress in simulating multiphysics problems using the numerical discretization of partial differential equations (PDEs), one still cannot seamlessly incorporate noisy data into existing algorithms, mesh generation remains complex, and high-dimensional problems governed by parameterized PDEs cannot be tackled. Moreover, solving inverse problems with hidden physics is often prohibitively expensive and requires different formulations and elaborate computer codes. Machine learning has emerged as a promising alternative, but training deep neural networks requires big data, not always available for scientific problems. Instead, such networks can be trained from additional information obtained by enforcing the physical laws (for example, at random points in the continuous space-time domain). Such physics-informed learning integrates (noisy) data and mathematical models, and implements them through neural networks or other kernel-based regression networks. Moreover, it may be possible to design specialized network architectures that automatically satisfy some of the physical invariants for better accuracy, faster training and improved generalization. Here, we review some of the prevailing trends in embedding physics into machine learning, present some of the current capabilities and limitations and discuss diverse applications of physics-informed learning both for forward and inverse problems, including discovering hidden physics and tackling high-dimensional problems.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b2"
      ],
      "title":[
        "Quantifying the performance of machine learning models in materials discovery"
      ],
      "abstract":[
        "The predictive capabilities of machine learning (ML) models used in materials discovery are typically measured using simple statistics such as the root-mean-square error (RMSE) or coefficient determination ($r^2$) between ML-predicted property values and their known values. A tempting assumption is that with low should be effective at guiding discovery, conversely, high give poor performance. However, we observe no clear connection exists a \"static\" quantity averaged across an entire training set, RMSE, ML model's ability to dynamically guide iterative (and often extrapolative) novel targeted properties. In this work, simulate sequential (SL)-guided process demonstrate decoupling traditional model metrics performance discoveries. We show depends strongly on (1) target range within distribution (e.g., whether 1st 10th decile material desired); (2) incorporation uncertainty estimates SL acquisition function; (3) scientist interested one many targets; (4) how iterations allowed. To overcome limitations static robustly capture performance, recommend Discovery Yield ($DY$), measure high-performing were discovered during SL, Probability ($DP$), likelihood discovering any point process."
      ],
      "categories":[
        "cond-mat.mtrl-sci"
      ]
    },
    "list":{
      "title":[
        "High emissivity surfaces stable at high temperatures",
        "In-operando test of tunable Heusler alloys for thermomagnetic harvesting\n  of low-grade waste heat",
        "Pressure Tuning of Layer-hybridized Excitons in Trilayer WSe2",
        "On the extension of the concept of rheological connections to a finite\n  deformation framework using multiple natural configurations",
        "Antiferromagnetic two-dimensional transition-metal nitride Co$_2$N$_2$\n  layer with high N$\\rm \\acute{\\textbf e}$el temperature and Dirac fermions",
        "Optimizing Lead-Free Chalcogenide Perovskites for High-Efficiency\n  Photovoltaics via Alloying Strategies",
        "Vacancy-induced Modification of Electronic Band Structure of LiBO$_{2}$\n  Material as Cathode Surface Coating of Lithium-ion Batteries",
        "Improvement of Morphology and Electrical Properties of Boron-doped\n  Diamond Films via Seeding with HPHT Nanodiamonds Synthesized from\n  9-Borabicyclononane",
        "A generalized calculation of the rate independent single crystal yield\n  surface",
        "Predicting Mode-I\/II fracture toughness and crack growth in diboride\n  ceramics via machine-learning potentials",
        "Visualizing Nanodomain Superlattices in Halide Perovskites Giving\n  Picosecond Quantum Transients",
        "Interactive Multiscale Modeling to Bridge Atomic Properties and\n  Electrochemical Performance in Li-CO$_2$ Battery Design",
        "Evidence of strong electron correlation effects and magnetic topological\n  excitation in low carbon steel",
        "On the speed of coming down from infinity for (sub)critical branching\n  processes with pairwise interactions",
        "Comment on \"Optimal conversion of Kochen-Specker sets into bipartite\n  perfect quantum strategies\"",
        "A simple extrapolation criterion with an application to wavelet\n  characterization of various function spaces",
        "Predicted Neutrino Signal Features of Core-Collapse Supernovae",
        "A Space Mapping approach for the calibration of financial models with\n  the application to the Heston model",
        "Euclid Quick Data Release (Q1), A first look at the fraction of bars in\n  massive galaxies at $z<1$",
        "H$\\alpha$ Variability of AB Aur b with the Hubble Space Telescope:\n  Probing the Nature of a Protoplanet Candidate with Accretion Light Echoes",
        "Surfaces in 4-manifolds and extendible mapping classes",
        "Discrete Level Set Persistence for Finite Discrete Functions",
        "Monge-Kantorovich quantiles and ranks for image data",
        "Interpreting and Steering Protein Language Models through Sparse\n  Autoencoders",
        "Minimax rates of convergence for the nonparametric estimation of the\n  diffusion coefficient from time-homogeneous SDE paths",
        "Stochastic Equilibrium Raman Spectroscopy (STERS)",
        "The Light Neutralino Dark Matter in the Generalized Minimal Supergravity\n  (GmSUGRA)",
        "Fusion Dynamics of Majorana Zero Modes"
      ],
      "abstract":[
        "Thermal radiative energy transport is essential for high-temperature energy\nharvesting technologies, including thermophotovoltaics (TPVs) and grid-scale\nthermal energy storage. However, the inherently low emissivity of conventional\nhigh-temperature materials constrains radiative energy transfer, thereby\nlimiting both system performance and technoeconomic viability. Here, we\ndemonstrate ultrafast femtosecond laser-material interactions to transform\ndiverse materials into near-blackbody surfaces with broadband spectral\nemissivity above 0.96. This enhancement arises from hierarchically engineered\nlight-trapping microstructures enriched with nanoscale features, effectively\ndecoupling surface optical properties from bulk thermomechanical properties.\nThese laser blackened surfaces (LaBS) exhibit exceptional thermal stability,\nretaining high emissivity for over 100 hours at temperatures exceeding\n1000{\\deg}C, even in oxidizing environments. When applied as TPV thermal\nemitters, Ta LaBS double electrical power output from 2.19 to 4.10 W cm-2 at\n2200{\\deg}C while sustaining TPV conversion efficiencies above 30%. This\nversatile, largely material-independent technique offers a scalable and\neconomically viable pathway to enhance emissivity for advanced thermal energy\napplications.",
        "Thermomagnetic generation stands out as a promising technology for harvesting\nand converting low-grade waste heat below 100 {\\deg}C. Despite the exponential\ngrowth in research on thermomagnetic materials and prototypes over the last\ndecade, there remains, to unlock the full potential of this technology, a\ncritical gap between fundamental research on materials and the design of\nadvanced devices. In this study, we present the in-operando assessment of\nthermomagnetic performance of three representative Ni,Mn-based Heusler alloys\noptimized for harvesting low-grade waste heat below 373 K. These materials were\ntested under operational conditions using a specially designed laboratory-scale\nprototype of a thermomagnetic motor. The mechanical power output of the motor,\noperated with NiMnIn, NiMnSn and NiMnCuGa alloys, was correlated with the\nmagnetic properties of the materials, highlighting the critical role of the\nmagnetic transition temperature and saturation magnetization in determining the\nefficiency of thermomagnetic energy conversion. Austenitic Heusler alloys were\nconfirmed to be promising thermomagnetic materials due to their highly tunable\nCurie temperature and significant magnetization changes in the 300-360 K\ntemperature range. Among the tested materials, the Ni48Mn36In16 alloy\ndemonstrated the highest thermomagnetic performance, surpassing the benchmark\nmaterial Gd in the 320-340 K range. From an experimental perspective, the\ndeveloped prototype of thermomagnetic motor serves as a flexible test-bench for\nevaluating and comparing the thermomagnetic performance of small amounts (less\nthan 0.3 g) of new materials under variable conditions. Additionally, its\nmodular design facilitates testing and optimization of its various components,\nthereby contributing to the advancement of thermomagnetic motor technology.",
        "We demonstrate dynamic pressure tuning (0-6.6 GPa) of layer-hybridized\nexcitons in AB-stacked trilayer WSe$_2$ via diamond-anvil-cell-integrated\nreflectance spectroscopy. Pressure-controlled interlayer coupling manifests in\nenhanced energy-level anti-crossings and oscillator strength redistribution,\nwith Stark shift analysis revealing a characteristic dipole moment reduction of\n11%. Notably, the hybridization strength between the intra- and interlayer\nexcitons triples from $\\sim$10 meV to above $\\sim$30 meV, exhibiting a\nnear-linear scaling of 3.5$\\pm$0.2 meV\/GPa. Spectral density simulations\nresolve four distinct components, i.e., intralayer ground\/excited and\ninterlayer ground\/excited excitons, with their relative weights transitioning\nfrom one component dominant to strongly hybridized at higher pressures. Our\nfindings highlight the potential for controlling excitonic properties and\nengineering novel optoelectronic devices through interlayer compression.",
        "The constitutive behaviors of materials are often modeled using a network of\ndifferent rheological elements. These rheological models are mostly developed\nwithin a one-dimensional small strain framework. One of the key impediments of\nextending these models to a three-dimensional finite deformation setting is to\ndetermine how the different types of connections, i.e., a series and a parallel\nconnection, are incorporated into the material models. The primary objective of\nthis article is to develop an appropriate strategy to address this issue. We\nshow that both the series and the parallel connection between two rheological\nelements can be modeled within a multiple natural configurations framework\nwithout changing or introducing new configurations. The difference in a series\nand a parallel connection is manifested in the ratio of the stress powers\nexpended during the deformations of the associated rheological elements. Finite\ndeformation version of some well-known rheological models have been used to\ndemonstrate the utility of the proposed theory.",
        "Two-dimensional (2D) transition metal nitrides have a wide prospect of\napplications in the fields of physics, chemistry, materials, etc. However, 2D\ntransition metal nitrides with strong magnetism, especially high N$\\rm\n\\acute{e}$el temperature, are very scarce. Based on the first-principles\ncalculations within the framework of density functional theory, we design two\n2D transition-metal nitrides \\textit{M}$_2$N$_2$ (\\textit{M} = Ti, Co), in\nwhich the transition metal atoms and the N atoms form a 2D layer with a\nwrinkled structure. The structural stability is demonstrated by the cohesive\nenergy, formation energy, elastic constants, phonon spectra and molecular\ndynamics simulations. Elastic moduli calculations reveal that the mechanical\nproperties of the two structures are anisotropic. Spin-polarized calculations\nshow that Ti$_2$N$_2$ is a 2D ferromagnetic material while Co$_2$N$_2$ is a 2D\nantiferromagnetic semimetal with a Dirac point at Fermi level. Furthermore, by\nsolveing the Heisenberg model by Monte Carlo method, we discover that the 2D\nCo$_2$N$_2$ layer is a high-temperature antiferromagnetic material and the\nN$\\rm \\acute{e}$el temperature is up to 474 K. Therefore, our findings provide\na rare antiferromagnetic 2D material with both high critical temperature and\nDirac Fermions.",
        "Lead-free chalcogenide perovskites are emerging as game-changers in the race\nfor sustainable, high-performance photovoltaics. These materials offer a\nperfect trifecta: non-toxic elemental composition, exceptional phase stability,\nand outstanding optoelectronic properties. However, unlocking their full\npotential for solar cell applications requires advanced strategies to fine-tune\ntheir electronic and optical behavior. In this study, we take CaHfS$_{3}$-a\npromising but underexplored candidate-and revolutionize its performance by\nintroducing targeted substitutions: Ti at the cation site and Se at the anion\nsite. Using cutting-edge computational techniques, including density functional\ntheory, GW calculations, and the Bethe-Salpeter equation (BSE), we reveal how\nthese substitutions transform the material's properties. Our findings highlight\nthat alloyed compounds such as CaHfS$_{3-x}$Se$_{x}$ and\nCaHf$_{1-y}$Ti$_{y}$X$_{3}$ (X = S, Se) are not only phase-stable but also\nfeature adjustable direct G$_{0}$W$_{0}$@PBE bandgaps (1.29-2.67 eV), reduced\nexciton binding energies, and significantly improved polaron mobility. These\nmodifications enable better light absorption, reduced electron-hole\nrecombination, longer exciton lifetimes, and enhanced quantum yield.\nImpressively, the alloyed perovskites, specifically, for the Ti-rich Se-based\nperovskites, achieve a spectroscopic-limited maximum efficiency of up to\n28.06%, outperforming traditional lead-based halide perovskites. Our results\ndemonstrate that strategic alloying is a powerful tool to supercharge the\noptoelectronic properties of lead-free chalcogenide perovskites, positioning\nthem as strong contenders for next-generation photovoltaic technologies.",
        "LiBO$_{2}$ is an electronic insulator and a promising surface coating for\nstabilizing high-voltage cathodes in lithium-ion batteries. Despite its\npotential, the functional mechanisms of this coating remain unclear,\nparticularly the transport of lithium ions and electrons through LiBO$_{2}$ in\nthe presence of lattice vacancies. This understanding is critical for the\ndesign and development of LiBO$_{2}$-based materials. In our previous work\n[Ziemke $\\textit{et al.}$, J. Mater. Chem. A, 2025, $\\textbf{13}$, 3146-3162],\nwe used density functional theory (DFT) calculations to investigate the impact\nof lattice vacancies on Li-ion transport in both tetragonal (t-LBO) and\nmonoclinic (m-LBO) polymorphs of LiBO$_{2}$, revealing that B vacancies in\neither polymorph enhanced lithium-ion transport. In this study, we expand on\nthese findings by using DFT calculations to examine the effects of lattice\nvacancies on the electronic properties of both t-LBO and m-LBO\npolymorphs,focusing on the electronic band structure. Our analysis shows that B\nvacancies can enhance the electronic insulation of t-LBO while improving the\nionic conduction of m-LBO. The combined results of our previous and current\nworks indicate that B vacancy generation in LiBO$_{2}$ may enable t-LBO to\nfunction as a promising solid electrolyte and enhance the performance of m-LBO\nas a conformal cathode coating in lithium-ion batteries. Overall, generating B\nvacancies, such as through neutron irradiation, would offer a viable strategy\nto improve the functionality of LiBO$_{2}$ as a promising material for energy\nstorage applications.",
        "Boron-doped diamond (BDD) films are becoming increasingly popular as\nelectrode materials due to their broad potential window and stability in harsh\nconditions and environments. Therefore, optimizing the crystal quality and\nminimizing defect density to maximize electronic properties (e.g. conductivity)\nof BDD is of great importance. This study investigates the influence of\ndifferent hydrogenated nanodiamond (H-ND) seeding layers on the growth and\nproperties of BDD films. Three types of seeding H-NDs were examined: detonation\n(H-DND) and top-down high-pressure high-temperature NDs (TD_HPHT H-ND), and\nboron-doped NDs (H-BND) newly synthesized at high-pressure high-temperature\nfrom an organic precursor. Purified and oxidized BND (O-BND) samples yielded\nclear, blue, and stable colloidal dispersions. Subsequent thermal hydrogenation\nreversed their zeta potential from - 32 mV to + 44 mV and promoted the seeding\nof negatively charged surfaces. All three H-ND types formed dense seeding\nlayers on SiO2 and Si\/SiOx substrates, which enabled the growth of BDD films by\nchemical vapor deposition (CVD). Despite variations in initial surface coverage\namong the seeding layers (13-25%), all NDs facilitated the growth of fully\nclosed BDD films approximately 1 {\\mu}m thick. Significant differences in film\nmorphology and electrical properties were observed. H-BND nucleation yielded\nthe BDD films with the largest crystals (up to 1 000 nm) and lowest sheet\nresistance (400 ohm\/sq). This superior performance is attributed to the uniform\nparticle shape and monocrystalline character of H-BND, as corroborated by FTIR,\nTEM, and SAXS measurements. These findings highlight the critical role of\nseeding layer properties in determining consequent diamond film evolution and\nestablish H-BNDs as promising seeding material for the growth of high-quality\nBDD films suitable for electronic and electrochemical applications.",
        "In this paper, we discuss a method to calculate the topology of the rate\nindependent single crystal yield surface for materials with arbitrary slip\nsystems and arbitrary slip strengths. We describe the general problem, as\nmotivated by Schmid's law, and detail the calculation of hyperplanes in\ndeviatoric stress space, $\\mathbb{D}^5$, which describe the criteria for slip\non individual slip systems. We focus on finding the intersection of five\nlinearly independent hyperplanes which represent stresses necessary to satisfy\nthe criteria for general plastic deformation. Finally, we describe a method for\ncalculating the inner convex hull of these intersection points, which describe\nthe vertices of the five dimensional polytope that represents the single\ncrystal yield surface. Our method applies to arbitrary crystal structure,\nallowing for an arbitrary number and type of slip systems and families,\nconsiders plastic anisotropy via inter- and intra-family strength anisotropy,\nand further considers strength anisotropy between slip in the positive and\nnegative direction. We discuss the calculation and possible applications, and\nshare a computational implementation of the calculation of the single crystal\nyield surface.",
        "Fracture toughness and strength are critical for structural ceramics, which\nare prone to brittle failure. However, accurately characterizing these\nproperties is challenging, especially for thin films on substrates. In-situ\nmicroscopy often fails to resolve crack initiation, while fractured samples\nprovide limited insight into fracture modes and crack paths. Here, we employ\nstress intensity factor ($K$) controlled atomistic simulations of fracture to\ncharacterize the crack-initiation properties of hard but brittle diboride\nceramics. Our molecular statics calculations are based on moment-tensor\nmachine-learning interatomic potentials (MLIPs) trained on {\\it{ab initio}}\ninformation collected for a variety of atomistic environments. TMB$_{2}$\n(TM$=$Ti, Zr, Hf) lattice models with six distinct atomically-sharp crack\ngeometries subjected to Mode-I (opening) and\/or Mode-II (sliding) deformation\nserve as platforms to illustrate the capability of the approach. The Mode-I\ninitiation toughness $K_{Ic}$ and fracture strength $\\sigma_{f}$ -- within\nranges 1.8-2.9~MPa$\\cdot\\sqrt{m}$ and 1.6-2.4~GPa -- are extrapolated at the\nmacroscale limit by fitting the results of finite (up to 10$^{6}$ atoms)\ncracked plate models with constitutive scaling relations. Our simulations show\nthat most diboride lattice models fail by extension of the native crack.\nHowever, crack-deflection on the $(1\\overline{1}01)$ plane is observed for the\n$(10\\overline{1}0)[\\overline{1}2\\overline{1}0]$ crystal geometry. As\nexemplified by TiB$_{2}$, varying Mode-I\/II loading ratios have little\ninfluence on crack propagation paths, which overall occurs by decohesion of\nlow-energy fracture planes or combined sliding. Our predictions are supported\nby cube-corner nanoindentation on TiB$_{2}$ samples along the [0001] direction,\nrevealing the same fracture plane as observed in simulations.",
        "The high optoelectronic quality of halide perovskites lends them to be\nutilized in optoelectronic devices and recently in emerging quantum emission\napplications. Advancements in perovskite nanomaterials have led to the\ndiscovery of processes in which luminescence decay times are sub-100\npicoseconds, stimulating the exploration of even faster radiative rates for\nadvanced quantum applications, which have only been prominently realised in\nIII-V materials grown through costly epitaxial growth methods. Here, we\ndiscovered ultrafast quantum transients of time scales ~2 picoseconds at low\ntemperature in bulk formamidinium lead iodide films grown through scalable\nsolution or vapour approaches. Using a multimodal strategy, combining ultrafast\nspectroscopy, optical and electron microscopy, we show that these transients\noriginate from quantum tunnelling in nanodomain superlattices. The outcome of\nthe transient decays, photoluminescence, mirrors the photoabsorption of the\nstates, with an ultra-narrow linewidth at low temperature as low as <2 nm (~4\nmeV). Localized correlation of the emission and structure reveals that the\nnanodomain superlattices are formed by alternating ordered layers of corner\nsharing and face sharing octahedra. This discovery opens new applications\nleveraging intrinsic quantum properties and demonstrates powerful multimodal\napproaches for quantum investigations.",
        "Li-CO$_2$ batteries show promise as energy storage solutions, offering high\ntheoretical energy density and CO$_2$ fixation. Their operation is based on the\nformation and decomposition of Li$_2$CO$_3$\/C during discharge and charge\ncycles, respectively. We used a multiscale modeling framework that integrates\nDensity Functional Theory (DFT), Ab-Initio Molecular Dynamics (AIMD), classical\nMolecular Dynamics (MD), and Finite Element Analysis (FEA) to investigate\natomic and cell-level properties. The considered Li-CO$_2$ battery consists of\na lithium metal anode, an ionic liquid electrolyte, and a carbon cloth porous\ncathode with Sb$_{0.67}$Bi$_{1.33}$Te$_3$ as a catalyst. DFT and AIMD\ndetermined the electrical conductivities of Sb$_{0.67}$Bi$_{1.33}$Te$_3$ and\nLi$_2$CO$_3$ using the Kubo-Greenwood formalism and studied the CO$_2$\nreduction mechanism on the cathode catalyst. MD simulations calculated the\nCO$_2$ diffusion coefficient, Li$^+$ transference number, ionic conductivity,\nand Li$^+$ solvation structure. The FEA model, incorporating results from\natomistic simulations, reproduced experimental voltage-capacity profiles at 1\nmA\/cm$^2$ and revealed spatio-temporal variations in Li$_2$CO$_3$\/C deposition,\nporosity, and CO$_2$ concentration dependence on discharge rates in the\ncathode. Accordingly, Li$_2$CO$_3$ can form large and thin film deposits,\nleading to dispersed and local porosity changes at 0.1 mA\/cm$^2$ and 1\nmA\/cm$^2$, respectively. The capacity decreases exponentially from 81,570 mAh\/g\nat 0.1 mA\/cm$^2$ to 6,200 mAh\/g at 1 mA\/cm$^2$, due to pore clogging from\nexcessive discharge product deposition that limits CO$_2$ transport to the\ncathode interior. Therefore, the performance of Li-CO$_2$ batteries can be\nimproved by enhancing CO$_2$ transport, regulating Li$_2$CO$_3$ deposition, and\noptimizing cathode architecture.",
        "Present study explores how thermal treatments and strain affect the magnetic\nresponse of two plain-carbon steels, with 0.05 wt% and 0.7 wt% carbon.\nElectron-backscattered diffraction and high-frequency magnetic susceptibility\n($\\chi$) measurements in 0.05%C steel reveal that annealing increases $\\chi$ by\nenlarging grains, while quenching reduces it by decreasing grain size. We also\nstudy the 0.7%C steel, to delineate effects of quench-induced strain and\npossible carbon-rich (Fe$_3$C) phase admixture in 0.05%C steel which could\naffect its magnetic response. In 0.7%C steel, uniaxial tensile strain enhances\n$\\chi$ via altered magnetic anisotropy, avoiding the reduction seen in quenched\n0.05%C steel. Micro-magnetic modelling and magnetic force microscopy identify\nmagnetic topological structures (MTS) in 0.05%C steel, especially with\nquenching. Low-temperature transport measurement suggests strong electron\ncorrelations drive Kondo effect in 0.05%C steel and MTS is an emergent feature\nof interplay with local strain. Thus, steel exhibits strong electron\ncorrelations and novel magnetic excitations, making it a promising quantum\nmaterial platform for developing new device applications.",
        "In this paper, we investigate the phenomenon of coming down from infinity for\n(sub)critical cooperative branching processes with pairwise interactions (BPI\nprocesses for short) under appropriate conditions. BPI processes are\ncontinuous-time Markov chains that extend pure branching dynamics by\nincorporating additional mechanisms that allow both competition and cooperation\nevents between pairs of individuals.\n  Specifically, we focus on characterising the speed at which BPI processes\nevolve when starting from a very large initial population in the subcritical\nand critical cooperative regimes. Further, in the subcritical cooperative\nregime, we analyse their second-order fluctuations.",
        "A recent paper of Trandafir and Cabello [Phys. Rev. A, 111, 022408 (2025)]\ncontains a number of errors, inconsistencies, and inefficiencies. They are too\nnumerous to be listed here, so we identify and discuss them in the main body of\nthe comment.",
        "The aim of this paper is to obtain an extrapolation result without using\nconvexification. What is new about this criterion is that the convexification\nof Banach spaces does not come into play. As an application, a characterization\nof ball Banach function spaces in terms of wavelets can be obtained. The result\ncan be formulated so that we can take into account the smoothness property of\nthe function spaces under consideration. The same technique can be used for the\nproof of the vector-valued inequalities for example. Also, the result in the\npresent paper refines a recent result on the extension operator.",
        "In this paper, we examine the neutrino signals from 24 initially\nnon-rotating, three-dimensional core-collapse supernova (CCSN) simulations\ncarried to late times. We find that not only does the neutrino luminosity\nsignal encode information about each stage of the CCSN process, but that the\nmonotonic dependence of the luminosity peak height with compactness enables one\nto infer the progenitor core structure from the neutrino signal. We highlight a\nsystematic relationship between the luminosity peak height with its timing.\nAdditionally, we emphasize that the total energy radiated in neutrinos is\nmonotonic with progenitor compactness, and that the mean neutrino energy\ncontains a unique spiral SASI signature for nonexploding, BH-forming models. We\nalso find that neutrino emissions are not isotropic and that the anisotropy\nincreases roughly with progenitor compactness. To assess the detectability of\nthese neutrino signal features, we provide examples of the event rates for our\nmodels for the JUNO, DUNE, SK, and IceCube detectors using the SNEWPY software,\nand find that many of the trends in the luminosity signal can be detectable\nacross several detectors and oscillation models. Finally, we discuss\ncorrelations between the radiated neutrino energy and the evolution of the\ngravitational-wave f-mode.",
        "We present a novel approach for parameter calibration of the Heston model for\npricing an Asian put option, namely space mapping. Since few parameters of the\nHeston model can be directly extracted from real market data, calibration to\nreal market data is implicit and therefore a challenging task. In addition,\nsome of the parameters in the model are non-linear, which makes it difficult to\nfind the global minimum of the optimization problem within the calibration. Our\napproach is based on the idea of space mapping, exploiting the residuum of a\ncoarse surrogate model that allows optimization and a fine model that needs to\nbe calibrated. In our case, the pricing of an Asian option using the Heston\nmodel SDE is the fine model, and the surrogate is chosen to be the Heston model\nPDE pricing a European option. We formally derive a gradient descent algorithm\nfor the PDE constrained calibration model using well-known techniques from\noptimization with PDEs. Our main goal is to provide evidence that the space\nmapping approach can be useful in financial calibration tasks. Numerical\nresults underline the feasibility of our approach.",
        "Stellar bars are key structures in disc galaxies, driving angular momentum\nredistribution and influencing processes such as bulge growth and star\nformation. Quantifying the bar fraction as a function of redshift and stellar\nmass is therefore important for constraining the physical processes that drive\ndisc formation and evolution across the history of the Universe. Leveraging the\nunprecedented resolution and survey area of the Euclid Q1 data release combined\nwith the Zoobot deep-learning model trained on citizen-science labels, we\nidentify 7711 barred galaxies with $M_* \\gtrsim 10^{10}M_\\odot$ in a\nmagnitude-selected sample $I_E < 20.5$ spanning $63.1 deg^2$. We measure a mean\nbar fraction of $0.2-0.4$, consistent with prior studies. At fixed redshift,\nmassive galaxies exhibit higher bar fractions, while lower-mass systems show a\nsteeper decline with redshift, suggesting earlier disc assembly in massive\ngalaxies. Comparisons with cosmological simulations (e.g., TNG50, Auriga)\nreveal a broadly consistent bar fraction, but highlight overpredictions for\nhigh-mass systems, pointing to potential over-efficiency in central stellar\nmass build-up in simulations. These findings demonstrate Euclid's\ntransformative potential for galaxy morphology studies and underscore the\nimportance of refining theoretical models to better reproduce observed trends.\nFuture work will explore finer mass bins, environmental correlations, and\nadditional morphological indicators.",
        "Giant planets generate accretion luminosity as they form. Much of this energy\nis radiated in strong H$\\alpha$ line emission, which has motivated direct\nimaging surveys at optical wavelengths to search for accreting protoplanets.\nHowever, compact disk structures can mimic accreting planets by scattering\nemission from the host star. This can complicate the interpretation of\nH$\\alpha$ point sources, especially if the host star itself is accreting. We\ndescribe an approach to distinguish accreting protoplanets from scattered-light\ndisk features using \"accretion light echoes.\" This method relies on variable\nH$\\alpha$ emission from a stochastically accreting host star to search for a\ndelayed brightness correlation with a candidate protoplanet. We apply this\nmethod to the candidate protoplanet AB Aur b with a dedicated Hubble Space\nTelescope Wide Field Camera 3 program designed to sequentially sample the host\nstar and the candidate planet in H$\\alpha$ while accounting for the light\ntravel time delay and orbital geometry of the source within the protoplanetary\ndisk. Across five epochs spanning 14 months, AB Aur b is over 20 times more\nvariable than its host star; AB Aur's H$\\alpha$ emission changes by 15% while\nAB Aur b varies by 330%. These brightness changes are not correlated, which\nrules out unobstructed scattered starlight from the host star as the only\nsource of AB Aur b's H$\\alpha$ emission and is consistent with tracing emission\nfrom an independently accreting protoplanet, inner disk shadowing effects, or a\nphysically evolving compact disk structure. More broadly, accretion light\nechoes offer a novel tool to explore the nature of protoplanet candidates with\nwell-timed observations of the host star prior to deep imaging in H$\\alpha$.",
        "We study smooth proper embeddings of compact orientable surfaces in compact\norientable $4$-manifolds and elements in the mapping class group of that\nsurface which are induced by diffeomorphisms of the ambient $4$-manifolds. We\ncall such mapping classes extendible. An embedding for which all mapping\nclasses are extendible is called flexible. We show that for most of the\nsurfaces there exists no flexible embedding in a $4$-manifold with homology\ntype of a $4$-ball or of a $4$-sphere. As an application of our method, we\naddress a question of Etnyre and Lekili and show that there exists no simple\nopen book decomposition of $S^5$ with a spin page where all $3$-dimensional\nopen books admit open book embeddings. We also provide many constructions and\ncriteria for extendible and non-extendible mapping classes, and discuss a\nconnection between extendibility and sliceness of links in a homology $4$-ball\nwith $S^3$ boundary. Finally, we give a new generating set of the group of\nextendible mapping classes for the trivial embedding of a closed genus $g$\nsurface in $S^4$, consisting of $3g$ generators. This improves a previous\nresult of Hirose giving a generating set of size $6g-1$.",
        "We study sublevel set and superlevel set persistent homology on discrete\nfunctions through the perspective of finite ordered sets of both linearly\nordered and cyclically ordered domains. Finite ordered sets also serve as the\ncodomain of our functions making all arguments finite and discrete. We prove\nduality of filtrations of sublevel sets and superlevel sets that undergirths a\nrange of duality results of sublevel set persistent homology without the need\nto invoke complications of continuous functions or classical Morse theory. We\nshow that Morse-like behavior can be achieved for flat extrema without assuming\ngenericity. Additionally, we show that with inversion of order, one can compute\nsublevel set persistence from superlevel set persistence, and vice versa via a\nduality result that does not require the boundary to be treated as a special\ncase. Furthermore, we discuss aspects of barcode construction rules, surgery of\ncircular and linearly ordered sets, as well as surgery on auxiliary structures\nsuch as box snakes, which segment the ordered set by extrema and monotones.",
        "This paper defines quantiles, ranks and statistical depths for image data by\nleveraging ideas from measure transportation. The first step is to embed a\ndistribution of images in a tangent space, with the framework of linear optimal\ntransport. Therein, Monge-Kantorovich quantiles are shown to provide a\nmeaningful ordering of image data, with outward images having unusual shapes.\nNumerical experiments showcase the relevance of the proposed procedure, for\ndescriptive analysis, outlier detection or statistical testing.",
        "The rapid advancements in transformer-based language models have\nrevolutionized natural language processing, yet understanding the internal\nmechanisms of these models remains a significant challenge. This paper explores\nthe application of sparse autoencoders (SAE) to interpret the internal\nrepresentations of protein language models, specifically focusing on the ESM-2\n8M parameter model. By performing a statistical analysis on each latent\ncomponent's relevance to distinct protein annotations, we identify potential\ninterpretations linked to various protein characteristics, including\ntransmembrane regions, binding sites, and specialized motifs.\n  We then leverage these insights to guide sequence generation, shortlisting\nthe relevant latent components that can steer the model towards desired targets\nsuch as zinc finger domains. This work contributes to the emerging field of\nmechanistic interpretability in biological sequence models, offering new\nperspectives on model steering for sequence design.",
        "Consider a diffusion process X, solution of a time-homogeneous stochastic\ndifferential equation. We assume that the diffusion process X is observed at\ndiscrete times, at high frequency, which means that the time step tends toward\nzero. In addition, the drift and diffusion coefficients of the process X are\nassumed to be unknown. In this paper, we study the minimax rates of convergence\nof the nonparametric estimators of the square of the diffusion coefficient. Two\nobservation schemes are considered depending on the estimation interval. The\nsquare of the diffusion coefficient is estimated on the real line from repeated\nobservations of the process X, where the number of diffusion paths tends to\ninfinity. For the case of a compact estimation interval, we study the\nnonparametric estimation of the square of the diffusion coefficient constructed\nfrom a single diffusion path on one side and from repeated observations on the\nother side, where the number of trajectories tends to infinity. In each of\nthese cases, we establish minimax convergence rates of the risk of estimation\nof the diffusion coefficient over a space of Holder functions.",
        "We theoretically propose a new method in cavity- and surface-enhanced Raman\nspectroscopy (SERS) with improved temporal resolution in the measurement of\nstochastic Raman spectral fluctuations. Our approach combines Fourier\nspectroscopy and photon correlation to decouple the integration time from the\ntemporal resolution. Using statistical optics simulations, we establish the\nrelationship between time resolution and Raman signal strength, revealing that\ntypical Raman spectral fluctuations, commensurate with molecular conformational\ndynamics, can theoretically be resolved on micro- to millisecond timescales.\nThe method can further extract average single-molecule dynamics from small\nsub-ensembles, thereby potentially mitigating challenges in achieving strictly\nsingle-molecule isolation on SERS substrates.",
        "We investigate both the $Z$ and $H$ poles solutions for the Higgsino mass\nparameter $\\mu>0$ and $\\mu<0$ for the neutralino dark matter in light of the\nLHC supersymmetry searches and the direct detection dark matter experiments,\nLUX-ZEPLIN (LZ), in the Generalized Minimal Supergravity (GmSUGRA). Our study\nindicates that the latest experimental constraints from the LHC and LZ\nCollaborations exclude the light Higgsinos in the $Z$ and $H$ pole regions for\nthe $\\mu>0$ case. Interestingly, for the $\\mu < 0$ case, a very light Higgsinos\ncan still be consistent with the current constraints from the electroweakino\nsearches and LZ experiment in the $Z$ and $H$ poles. Consequently, the $\\mu <\n0$ case appears more promising and thus requires the dedicated efforts to make\ndefinitive conclusions about their current status from the experimental\nCollaborations. In this framework, our findings indicate a deviation of up to\n$2\\sigma$ from the central value of \\( a_\\mu \\equiv (g-2)_\\mu\/2 \\), resonating\nwith the experimental results reported by CMD and BDM.",
        "Braiding and fusion of Majorana zero modes are key elements of any future\ntopological Majorana-based quantum computer. Here, we investigate the fusion\ndynamics of Majorana zero modes in the spinless Kitaev model, as well as in a\nspinfull model describing magnet-superconductor hybrid structures. We consider\nvarious scenarios allowing us to reproduce the fusion rules of the Ising anyon\nmodel. Particular emphasis is given to the charge of the fermion obtained after\nfusing two Majorana zero modes: as long as it remains on the superconductor,\ncharge quantization is absent. When moving the fermion to a non-superconducting\nregion, such as a quantum dot, nearly-quantized charge can be measured. Our\nfindings confirm for both platforms that fusion dynamics of Majorana zero modes\ncan indeed be used for the readout of Majorana qubits."
      ]
    }
  },
  {
    "id":2411.18259,
    "research_type":"applied",
    "start_id":"b2",
    "start_title":"Quantifying the performance of machine learning models in materials discovery",
    "start_abstract":"The predictive capabilities of machine learning (ML) models used in materials discovery are typically measured using simple statistics such as the root-mean-square error (RMSE) or coefficient determination ($r^2$) between ML-predicted property values and their known values. A tempting assumption is that with low should be effective at guiding discovery, conversely, high give poor performance. However, we observe no clear connection exists a \"static\" quantity averaged across an entire training set, RMSE, ML model's ability to dynamically guide iterative (and often extrapolative) novel targeted properties. In this work, simulate sequential (SL)-guided process demonstrate decoupling traditional model metrics performance discoveries. We show depends strongly on (1) target range within distribution (e.g., whether 1st 10th decile material desired); (2) incorporation uncertainty estimates SL acquisition function; (3) scientist interested one many targets; (4) how iterations allowed. To overcome limitations static robustly capture performance, recommend Discovery Yield ($DY$), measure high-performing were discovered during SL, Probability ($DP$), likelihood discovering any point process.",
    "start_categories":[
      "cond-mat.mtrl-sci"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "Physics-informed machine learning"
      ],
      "abstract":[
        "Despite great progress in simulating multiphysics problems using the numerical discretization of partial differential equations (PDEs), one still cannot seamlessly incorporate noisy data into existing algorithms, mesh generation remains complex, and high-dimensional problems governed by parameterized PDEs cannot be tackled. Moreover, solving inverse problems with hidden physics is often prohibitively expensive and requires different formulations and elaborate computer codes. Machine learning has emerged as a promising alternative, but training deep neural networks requires big data, not always available for scientific problems. Instead, such networks can be trained from additional information obtained by enforcing the physical laws (for example, at random points in the continuous space-time domain). Such physics-informed learning integrates (noisy) data and mathematical models, and implements them through neural networks or other kernel-based regression networks. Moreover, it may be possible to design specialized network architectures that automatically satisfy some of the physical invariants for better accuracy, faster training and improved generalization. Here, we review some of the prevailing trends in embedding physics into machine learning, present some of the current capabilities and limitations and discuss diverse applications of physics-informed learning both for forward and inverse problems, including discovering hidden physics and tackling high-dimensional problems."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Leveraging Large Language Models as Knowledge-Driven Agents for Reliable\n  Retrosynthesis Planning",
        "AI-Enabled Knowledge Sharing for Enhanced Collaboration and\n  Decision-Making in Non-Profit Healthcare Organizations: A Scoping Review\n  Protocol",
        "Agency Is Frame-Dependent",
        "Benchmarking Retrieval-Augmented Generation in Multi-Modal Contexts",
        "Reinforced Large Language Model is a formal theorem prover",
        "From System 1 to System 2: A Survey of Reasoning Large Language Models",
        "Detection of LLM-Paraphrased Code and Identification of the Responsible\n  LLM Using Coding Style Features",
        "A Survey on Mathematical Reasoning and Optimization with Large Language\n  Models",
        "SHACL-SKOS Based Knowledge Representation of Material Safety Data Sheet\n  (SDS) for the Pharmaceutical Industry",
        "Automatic Curriculum Design for Zero-Shot Human-AI Coordination",
        "Artificial Intelligence-Driven Clinical Decision Support Systems",
        "A Law Reasoning Benchmark for LLM with Tree-Organized Structures\n  including Factum Probandum, Evidence and Experiences",
        "RTBAgent: A LLM-based Agent System for Real-Time Bidding",
        "Attention to Trajectory: Trajectory-Aware Open-Vocabulary Tracking",
        "One citation, one vote! A new approach for analysing\n  check-all-that-apply (CATA) data in sensometrics, using L1 norm methods",
        "Split-n-Chain: Privacy-Preserving Multi-Node Split Learning with\n  Blockchain-Based Auditability",
        "Analysis of kinematics of mechanisms containing revolute joints",
        "Controllable Emotion Generation with Emotion Vectors",
        "Automated Refactoring of Non-Idiomatic Python Code: A Differentiated\n  Replication with LLMs",
        "DanmuA11y: Making Time-Synced On-Screen Video Comments (Danmu)\n  Accessible to Blind and Low Vision Users via Multi-Viewer Audio Discussions",
        "Mixing Any Cocktail with Limited Ingredients: On the Structure of Payoff\n  Sets in Multi-Objective MDPs and its Impact on Randomised Strategies",
        "Asymptotic Optimism of Random-Design Linear and Kernel Regression Models",
        "On Nash Equilibria in Play-Once and Terminal Deterministic Graphical\n  Games",
        "Optimal generalisation and learning transition in extensive-width\n  shallow neural networks near interpolation",
        "MAST-Pro: Dynamic Mixture-of-Experts for Adaptive Segmentation of\n  Pan-Tumors with Knowledge-Driven Prompts",
        "VideoAnydoor: High-fidelity Video Object Insertion with Precise Motion\n  Control",
        "A class of anisotropic diffusion-transport equations in non-divergence\n  form",
        "A conjecture on monomial realizations and polyhedral realizations for\n  crystal bases"
      ],
      "abstract":[
        "Identifying reliable synthesis pathways in materials chemistry is a complex\ntask, particularly in polymer science, due to the intricate and often\nnon-unique nomenclature of macromolecules. To address this challenge, we\npropose an agent system that integrates large language models (LLMs) and\nknowledge graphs (KGs). By leveraging LLMs' powerful capabilities for\nextracting and recognizing chemical substance names, and storing the extracted\ndata in a structured knowledge graph, our system fully automates the retrieval\nof relevant literatures, extraction of reaction data, database querying,\nconstruction of retrosynthetic pathway trees, further expansion through the\nretrieval of additional literature and recommendation of optimal reaction\npathways. A novel Multi-branched Reaction Pathway Search (MBRPS) algorithm\nenables the exploration of all pathways, with a particular focus on\nmulti-branched ones, helping LLMs overcome weak reasoning in multi-branched\npaths. This work represents the first attempt to develop a fully automated\nretrosynthesis planning agent tailored specially for macromolecules powered by\nLLMs. Applied to polyimide synthesis, our new approach constructs a\nretrosynthetic pathway tree with hundreds of pathways and recommends optimized\nroutes, including both known and novel pathways, demonstrating its\neffectiveness and potential for broader applications.",
        "This protocol outlines a scoping review designed to systematically map the\nexisting body of evidence on AI-enabled knowledge sharing in resource-limited\nnon-profit healthcare organizations. The review aims to investigate how such\ntechnologies enhance collaboration and decision-making, particularly in the\ncontext of reduced external support following the cessation of USAID\noperations. Guided by three theoretical frameworks namely, the Resource-Based\nView, Dynamic Capabilities Theory, and Absorptive Capacity Theory, this study\nwill explore the dual role of AI as a strategic resource and an enabler of\norganizational learning and agility. The protocol details a rigorous\nmethodological approach based on PRISMA-ScR guidelines, encompassing a\nsystematic search strategy across multiple databases, inclusion and exclusion\ncriteria, and a structured data extraction process. By integrating theoretical\ninsights with empirical evidence, this scoping review seeks to identify\ncritical gaps in the literature and inform the design of effective,\nresource-optimized AI solutions in non-profit healthcare settings.",
        "Agency is a system's capacity to steer outcomes toward a goal, and is a\ncentral topic of study across biology, philosophy, cognitive science, and\nartificial intelligence. Determining if a system exhibits agency is a\nnotoriously difficult question: Dennett (1989), for instance, highlights the\npuzzle of determining which principles can decide whether a rock, a thermostat,\nor a robot each possess agency. We here address this puzzle from the viewpoint\nof reinforcement learning by arguing that agency is fundamentally\nframe-dependent: Any measurement of a system's agency must be made relative to\na reference frame. We support this claim by presenting a philosophical argument\nthat each of the essential properties of agency proposed by Barandiaran et al.\n(2009) and Moreno (2018) are themselves frame-dependent. We conclude that any\nbasic science of agency requires frame-dependence, and discuss the implications\nof this claim for reinforcement learning.",
        "This paper introduces Multi-Modal Retrieval-Augmented Generation (M^2RAG), a\nbenchmark designed to evaluate the effectiveness of Multi-modal Large Language\nModels (MLLMs) in leveraging knowledge from multi-modal retrieval documents.\nThe benchmark comprises four tasks: image captioning, multi-modal question\nanswering, multi-modal fact verification, and image reranking. All tasks are\nset in an open-domain setting, requiring RAG models to retrieve query-relevant\ninformation from a multi-modal document collection and use it as input context\nfor RAG modeling. To enhance the context utilization capabilities of MLLMs, we\nalso introduce Multi-Modal Retrieval-Augmented Instruction Tuning (MM-RAIT), an\ninstruction tuning method that optimizes MLLMs within multi-modal contexts. Our\nexperiments show that MM-RAIT improves the performance of RAG systems by\nenabling them to effectively learn from multi-modal contexts. All data and code\nare available at https:\/\/github.com\/NEUIR\/M2RAG.",
        "To take advantage of Large Language Model in theorem formalization and proof,\nwe propose a reinforcement learning framework to iteratively optimize the\npretrained LLM by rolling out next tactics and comparing them with the expected\nones. The experiment results show that it helps to achieve a higher accuracy\ncompared with directly fine-tuned LLM.",
        "Achieving human-level intelligence requires refining the transition from the\nfast, intuitive System 1 to the slower, more deliberate System 2 reasoning.\nWhile System 1 excels in quick, heuristic decisions, System 2 relies on logical\nreasoning for more accurate judgments and reduced biases. Foundational Large\nLanguage Models (LLMs) excel at fast decision-making but lack the depth for\ncomplex reasoning, as they have not yet fully embraced the step-by-step\nanalysis characteristic of true System 2 thinking. Recently, reasoning LLMs\nlike OpenAI's o1\/o3 and DeepSeek's R1 have demonstrated expert-level\nperformance in fields such as mathematics and coding, closely mimicking the\ndeliberate reasoning of System 2 and showcasing human-like cognitive abilities.\nThis survey begins with a brief overview of the progress in foundational LLMs\nand the early development of System 2 technologies, exploring how their\ncombination has paved the way for reasoning LLMs. Next, we discuss how to\nconstruct reasoning LLMs, analyzing their features, the core methods enabling\nadvanced reasoning, and the evolution of various reasoning LLMs. Additionally,\nwe provide an overview of reasoning benchmarks, offering an in-depth comparison\nof the performance of representative reasoning LLMs. Finally, we explore\npromising directions for advancing reasoning LLMs and maintain a real-time\n\\href{https:\/\/github.com\/zzli2022\/Awesome-Slow-Reason-System}{GitHub\nRepository} to track the latest developments. We hope this survey will serve as\na valuable resource to inspire innovation and drive progress in this rapidly\nevolving field.",
        "Recent progress in large language models (LLMs) for code generation has\nraised serious concerns about intellectual property protection. Malicious users\ncan exploit LLMs to produce paraphrased versions of proprietary code that\nclosely resemble the original. While the potential for LLM-assisted code\nparaphrasing continues to grow, research on detecting it remains limited,\nunderscoring an urgent need for detection system. We respond to this need by\nproposing two tasks. The first task is to detect whether code generated by an\nLLM is a paraphrased version of original human-written code. The second task is\nto identify which LLM is used to paraphrase the original code. For these tasks,\nwe construct a dataset LPcode consisting of pairs of human-written code and\nLLM-paraphrased code using various LLMs.\n  We statistically confirm significant differences in the coding styles of\nhuman-written and LLM-paraphrased code, particularly in terms of naming\nconsistency, code structure, and readability. Based on these findings, we\ndevelop LPcodedec, a detection method that identifies paraphrase relationships\nbetween human-written and LLM-generated code, and discover which LLM is used\nfor the paraphrasing. LPcodedec outperforms the best baselines in two tasks,\nimproving F1 scores by 2.64% and 15.17% while achieving speedups of 1,343x and\n213x, respectively. Our code and data are available at\nhttps:\/\/github.com\/Shinwoo-Park\/detecting_llm_paraphrased_code_via_coding_style_features.",
        "Mathematical reasoning and optimization are fundamental to artificial\nintelligence and computational problem-solving. Recent advancements in Large\nLanguage Models (LLMs) have significantly improved AI-driven mathematical\nreasoning, theorem proving, and optimization techniques. This survey explores\nthe evolution of mathematical problem-solving in AI, from early statistical\nlearning approaches to modern deep learning and transformer-based\nmethodologies. We review the capabilities of pretrained language models and\nLLMs in performing arithmetic operations, complex reasoning, theorem proving,\nand structured symbolic computation. A key focus is on how LLMs integrate with\noptimization and control frameworks, including mixed-integer programming,\nlinear quadratic control, and multi-agent optimization strategies. We examine\nhow LLMs assist in problem formulation, constraint generation, and heuristic\nsearch, bridging theoretical reasoning with practical applications. We also\ndiscuss enhancement techniques such as Chain-of-Thought reasoning, instruction\ntuning, and tool-augmented methods that improve LLM's problem-solving\nperformance. Despite their progress, LLMs face challenges in numerical\nprecision, logical consistency, and proof verification. Emerging trends such as\nhybrid neural-symbolic reasoning, structured prompt engineering, and multi-step\nself-correction aim to overcome these limitations. Future research should focus\non interpretability, integration with domain-specific solvers, and improving\nthe robustness of AI-driven decision-making. This survey offers a comprehensive\nreview of the current landscape and future directions of mathematical reasoning\nand optimization with LLMs, with applications across engineering, finance, and\nscientific research.",
        "We report the development of a knowledge representation and reasoning (KRR)\nsystem built on hybrid SHACL-SKOS ontologies for globally harmonized system\n(GHS) material Safety Data Sheets (SDS) to enhance chemical safety\ncommunication and regulatory compliance. SDS are comprehensive documents\ncontaining safety and handling information for chemical substances. Thus, they\nare an essential part of workplace safety and risk management. However, the\nvast number of Safety Data Sheets from multiple organizations, manufacturers,\nand suppliers that produce and distribute chemicals makes it challenging to\ncentralize and access SDS documents through a single repository. To accomplish\nthe underlying issues of data exchange related to chemical shipping and\nhandling, we construct SDS related controlled vocabulary and conditions\nvalidated by SHACL, and knowledge systems of similar domains linked via SKOS.\nThe resulting hybrid ontologies aim to provide standardized yet adaptable\nrepresentations of SDS information, facilitating better data sharing,\nretrieval, and integration across various platforms. This paper outlines our\nSHACL-SKOS system architectural design and showcases our implementation for an\nindustrial application streamlining the generation of a composite shipping\ncover sheet.",
        "Zero-shot human-AI coordination is the training of an ego-agent to coordinate\nwith humans without using human data. Most studies on zero-shot human-AI\ncoordination have focused on enhancing the ego-agent's coordination ability in\na given environment without considering the issue of generalization to unseen\nenvironments. Real-world applications of zero-shot human-AI coordination should\nconsider unpredictable environmental changes and the varying coordination\nability of co-players depending on the environment. Previously, the multi-agent\nUED (Unsupervised Environment Design) approach has investigated these\nchallenges by jointly considering environmental changes and co-player policy in\ncompetitive two-player AI-AI scenarios. In this paper, our study extends the\nmulti-agent UED approach to a zero-shot human-AI coordination. We propose a\nutility function and co-player sampling for a zero-shot human-AI coordination\nsetting that helps train the ego-agent to coordinate with humans more\neffectively than the previous multi-agent UED approach. The zero-shot human-AI\ncoordination performance was evaluated in the Overcooked-AI environment, using\nhuman proxy agents and real humans. Our method outperforms other baseline\nmodels and achieves a high human-AI coordination performance in unseen\nenvironments.",
        "As artificial intelligence (AI) becomes increasingly embedded in healthcare\ndelivery, this chapter explores the critical aspects of developing reliable and\nethical Clinical Decision Support Systems (CDSS). Beginning with the\nfundamental transition from traditional statistical models to sophisticated\nmachine learning approaches, this work examines rigorous validation strategies\nand performance assessment methods, including the crucial role of model\ncalibration and decision curve analysis. The chapter emphasizes that creating\ntrustworthy AI systems in healthcare requires more than just technical\naccuracy; it demands careful consideration of fairness, explainability, and\nprivacy. The challenge of ensuring equitable healthcare delivery through AI is\nstressed, discussing methods to identify and mitigate bias in clinical\npredictive models. The chapter then delves into explainability as a cornerstone\nof human-centered CDSS. This focus reflects the understanding that healthcare\nprofessionals must not only trust AI recommendations but also comprehend their\nunderlying reasoning. The discussion advances in an analysis of privacy\nvulnerabilities in medical AI systems, from data leakage in deep learning\nmodels to sophisticated attacks against model explanations. The text explores\nprivacy-preservation strategies such as differential privacy and federated\nlearning, while acknowledging the inherent trade-offs between privacy\nprotection and model performance. This progression, from technical validation\nto ethical considerations, reflects the multifaceted challenges of developing\nAI systems that can be seamlessly and reliably integrated into daily clinical\npractice while maintaining the highest standards of patient care and data\nprotection.",
        "While progress has been made in legal applications, law reasoning, crucial\nfor fair adjudication, remains unexplored. We propose a transparent law\nreasoning schema enriched with hierarchical factum probandum, evidence, and\nimplicit experience, enabling public scrutiny and preventing bias. Inspired by\nthis schema, we introduce the challenging task, which takes a textual case\ndescription and outputs a hierarchical structure justifying the final decision.\nWe also create the first crowd-sourced dataset for this task, enabling\ncomprehensive evaluation. Simultaneously, we propose an agent framework that\nemploys a comprehensive suite of legal analysis tools to address the challenge\ntask. This benchmark paves the way for transparent and accountable AI-assisted\nlaw reasoning in the ``Intelligent Court''.",
        "Real-Time Bidding (RTB) enables advertisers to place competitive bids on\nimpression opportunities instantaneously, striving for cost-effectiveness in a\nhighly competitive landscape. Although RTB has widely benefited from the\nutilization of technologies such as deep learning and reinforcement learning,\nthe reliability of related methods often encounters challenges due to the\ndiscrepancies between online and offline environments and the rapid\nfluctuations of online bidding. To handle these challenges, RTBAgent is\nproposed as the first RTB agent system based on large language models (LLMs),\nwhich synchronizes real competitive advertising bidding environments and\nobtains bidding prices through an integrated decision-making process.\nSpecifically, obtaining reasoning ability through LLMs, RTBAgent is further\ntailored to be more professional for RTB via involved auxiliary modules, i.e.,\nclick-through rate estimation model, expert strategy knowledge, and daily\nreflection. In addition, we propose a two-step decision-making process and\nmulti-memory retrieval mechanism, which enables RTBAgent to review historical\ndecisions and transaction records and subsequently make decisions more adaptive\nto market changes in real-time bidding. Empirical testing with real advertising\ndatasets demonstrates that RTBAgent significantly enhances profitability. The\nRTBAgent code will be publicly accessible at:\nhttps:\/\/github.com\/CaiLeng\/RTBAgent.",
        "Open-Vocabulary Multi-Object Tracking (OV-MOT) aims to enable approaches to\ntrack objects without being limited to a predefined set of categories. Current\nOV-MOT methods typically rely primarily on instance-level detection and\nassociation, often overlooking trajectory information that is unique and\nessential for object tracking tasks. Utilizing trajectory information can\nenhance association stability and classification accuracy, especially in cases\nof occlusion and category ambiguity, thereby improving adaptability to novel\nclasses. Thus motivated, in this paper we propose \\textbf{TRACT}, an\nopen-vocabulary tracker that leverages trajectory information to improve both\nobject association and classification in OV-MOT. Specifically, we introduce a\n\\textit{Trajectory Consistency Reinforcement} (\\textbf{TCR}) strategy, that\nbenefits tracking performance by improving target identity and category\nconsistency. In addition, we present \\textbf{TraCLIP}, a plug-and-play\ntrajectory classification module. It integrates \\textit{Trajectory Feature\nAggregation} (\\textbf{TFA}) and \\textit{Trajectory Semantic Enrichment}\n(\\textbf{TSE}) strategies to fully leverage trajectory information from visual\nand language perspectives for enhancing the classification results. Extensive\nexperiments on OV-TAO show that our TRACT significantly improves tracking\nperformance, highlighting trajectory information as a valuable asset for\nOV-MOT. Code will be released.",
        "A unified framework is provided for analysing check-all-that-apply (CATA)\nproduct data following the ``one citation, one vote\" principle. CATA data arise\nfrom studies where A consumers evaluate P products by describing samples by\nchecking all of the T terms that apply. Giving every citation the same weight,\nregardless of the assessor, product, or term, leads to analyses based on the L1\nnorm where the median absolute deviation is the measure of dispersion. Five\npermutation tests are proposed to answer the following questions. Do any\nproducts differ? For which terms do products differ? Within each of the terms,\nwhich products differ? Which product pairs differ? On which terms does each\nproduct pair differ? Additionally, we show how products and terms can be\nclustered following the ``one citation, one vote\" principle and how L1-norm\nprincipal component analysis (L1-norm PCA) can be applied to visualize CATA\nresults in few dimensions. Together, the permutation tests, clustering methods,\nand L1-norm PCA provide a unified approach. The proposed methods are\nillustrated using a data set in which 100 consumers evaluated 11 products using\n34 CATA terms.R code is provided to perform the analyses.",
        "Deep learning, when integrated with a large amount of training data, has the\npotential to outperform machine learning in terms of high accuracy. Recently,\nprivacy-preserving deep learning has drawn significant attention of the\nresearch community. Different privacy notions in deep learning include privacy\nof data provided by data-owners and privacy of parameters and\/or\nhyperparameters of the underlying neural network. Federated learning is a\npopular privacy-preserving execution environment where data-owners participate\nin learning the parameters collectively without leaking their respective data\nto other participants. However, federated learning suffers from certain\nsecurity\/privacy issues. In this paper, we propose Split-n-Chain, a variant of\nsplit learning where the layers of the network are split among several\ndistributed nodes. Split-n-Chain achieves several privacy properties:\ndata-owners need not share their training data with other nodes, and no nodes\nhave access to the parameters and hyperparameters of the neural network (except\nthat of the respective layers they hold). Moreover, Split-n-Chain uses\nblockchain to audit the computation done by different nodes. Our experimental\nresults show that: Split-n-Chain is efficient, in terms of time required to\nexecute different phases, and the training loss trend is similar to that for\nthe same neural network when implemented in a monolithic fashion.",
        "Kinematics of rigid bodies can be analyzed in many different ways. The\nadvantage of using Euler parameters is that the resulting equations are\npolynomials and hence computational algebra, in particular Gr\\\"obner bases, can\nbe used to study them. The disadvantage of the Gr\\\"obner basis methods is that\nthe computational complexity grows quite fast in the worst case in the number\nof variables and the degree of polynomials. In the present article we show how\nto simplify computations when the mechanism contains revolute joints. The idea\nis based on the fact that the ideal representing the constraints of the\nrevolute joint is not prime. Choosing the appropriate prime component reduces\nsignificantly the computational cost. We illustrate the method by applying it\nto the well known Bennett's and Bricard's mechanisms, but it can be applied to\nany mechanism which has revolute joints.",
        "In recent years, technologies based on large-scale language models (LLMs)\nhave made remarkable progress in many fields, especially in customer service,\ncontent creation, and embodied intelligence, showing broad application\npotential. However, The LLM's ability to express emotions with proper tone,\ntiming, and in both direct and indirect forms is still insufficient but\nsignificant. Few works have studied on how to build the controlable emotional\nexpression capability of LLMs. In this work, we propose a method for emotion\nexpression output by LLMs, which is universal, highly flexible, and well\ncontrollable proved with the extensive experiments and verifications. This\nmethod has broad application prospects in fields involving emotions output by\nLLMs, such as intelligent customer service, literary creation, and home\ncompanion robots. The extensive experiments on various LLMs with different\nmodel-scales and architectures prove the versatility and the effectiveness of\nthe proposed method.",
        "In the Python ecosystem, the adoption of idiomatic constructs has been\nfostered because of their expressiveness, increasing productivity and even\nefficiency, despite controversial arguments concerning familiarity or\nunderstandability issues. Recent research contributions have proposed\napproaches -- based on static code analysis and transformation -- to\nautomatically identify and enact refactoring opportunities of non-idiomatic\ncode into idiomatic ones. Given the potential recently offered by Large\nLanguage Models (LLMs) for code-related tasks, in this paper, we present the\nresults of a replication study in which we investigate GPT-4 effectiveness in\nrecommending and suggesting idiomatic refactoring actions. Our results reveal\nthat GPT-4 not only identifies idiomatic constructs effectively but frequently\nexceeds the benchmark in proposing refactoring actions where the existing\nbaseline failed. A manual analysis of a random sample shows the correctness of\nthe obtained recommendations. Our findings underscore the potential of LLMs to\nachieve tasks where, in the past, implementing recommenders based on complex\ncode analyses was required.",
        "By overlaying time-synced user comments on videos, Danmu creates a\nco-watching experience for online viewers. However, its visual-centric design\nposes significant challenges for blind and low vision (BLV) viewers. Our\nformative study identified three primary challenges that hinder BLV viewers'\nengagement with Danmu: the lack of visual context, the speech interference\nbetween comments and videos, and the disorganization of comments. To address\nthese challenges, we present DanmuA11y, a system that makes Danmu accessible by\ntransforming it into multi-viewer audio discussions. DanmuA11y incorporates\nthree core features: (1) Augmenting Danmu with visual context, (2) Seamlessly\nintegrating Danmu into videos, and (3) Presenting Danmu via multi-viewer\ndiscussions. Evaluation with twelve BLV viewers demonstrated that DanmuA11y\nsignificantly improved Danmu comprehension, provided smooth viewing\nexperiences, and fostered social connections among viewers. We further\nhighlight implications for enhancing commentary accessibility in video-based\nsocial media and live-streaming platforms.",
        "We consider multi-dimensional payoff functions in Markov decision processes,\nand ask whether a given expected payoff vector can be achieved or not. In\ngeneral, pure strategies (i.e., not resorting to randomisation) do not suffice\nfor this problem.\n  We study the structure of the set of expected payoff vectors of all\nstrategies given a multi-dimensional payoff function and its consequences\nregarding randomisation requirements for strategies. In particular, we prove\nthat for any payoff for which the expectation is well-defined under all\nstrategies, it is sufficient to mix (i.e., randomly select a pure strategy at\nthe start of a play and committing to it for the rest of the play) finitely\nmany pure strategies to approximate any expected payoff vector up to any\nprecision. Furthermore, for any payoff for which the expected payoff is finite\nunder all strategies, any expected payoff can be obtained exactly by mixing\nfinitely many strategies.",
        "We derived the closed-form asymptotic optimism of linear regression models\nunder random designs, and generalizes it to kernel ridge regression. Using\nscaled asymptotic optimism as a generic predictive model complexity measure, we\nstudied the fundamental different behaviors of linear regression model, tangent\nkernel (NTK) regression model and three-layer fully connected neural networks\n(NN). Our contribution is two-fold: we provided theoretical ground for using\nscaled optimism as a model predictive complexity measure; and we show\nempirically that NN with ReLUs behaves differently from kernel models under\nthis measure. With resampling techniques, we can also compute the optimism for\nregression models with real data.",
        "We consider finite $n$-person deterministic graphical games and study the\nexistence of pure stationary Nash-equilibrium in such games. We assume that all\ninfinite plays are equivalent and form a unique outcome, while each terminal\nposition is a separate outcome. It is known that for $n=2$ such a game always\nhas a Nash equilibrium, while that may not be true for $n > 2$.\n  A game is called {\\em play-once} if each player controls a unique position\nand {\\em terminal} if any terminal outcome is better than the infinite one for\neach player. We prove in this paper that play-once games have Nash equilibria.\n  We also show that terminal games have Nash equilibria if they have at most\nthree terminals.",
        "We consider a teacher-student model of supervised learning with a\nfully-trained 2-layer neural network whose width $k$ and input dimension $d$\nare large and proportional. We compute the Bayes-optimal generalisation error\nof the network for any activation function in the regime where the number of\ntraining data $n$ scales quadratically with the input dimension, i.e., around\nthe interpolation threshold where the number of trainable parameters $kd+k$ and\nof data points $n$ are comparable. Our analysis tackles generic weight\ndistributions. Focusing on binary weights, we uncover a discontinuous phase\ntransition separating a \"universal\" phase from a \"specialisation\" phase. In the\nfirst, the generalisation error is independent of the weight distribution and\ndecays slowly with the sampling rate $n\/d^2$, with the student learning only\nsome non-linear combinations of the teacher weights. In the latter, the error\nis weight distribution-dependent and decays faster due to the alignment of the\nstudent towards the teacher network. We thus unveil the existence of a highly\npredictive solution near interpolation, which is however potentially hard to\nfind.",
        "Accurate tumor segmentation is crucial for cancer diagnosis and treatment.\nWhile foundation models have advanced general-purpose segmentation, existing\nmethods still struggle with: (1) limited incorporation of medical priors, (2)\nimbalance between generic and tumor-specific features, and (3) high\ncomputational costs for clinical adaptation. To address these challenges, we\npropose MAST-Pro (Mixture-of-experts for Adaptive Segmentation of pan-Tumors\nwith knowledge-driven Prompts), a novel framework that integrates dynamic\nMixture-of-Experts (D-MoE) and knowledge-driven prompts for pan-tumor\nsegmentation. Specifically, text and anatomical prompts provide domain-specific\npriors, guiding tumor representation learning, while D-MoE dynamically selects\nexperts to balance generic and tumor-specific feature learning, improving\nsegmentation accuracy across diverse tumor types. To enhance efficiency, we\nemploy Parameter-Efficient Fine-Tuning (PEFT), optimizing MAST-Pro with\nsignificantly reduced computational overhead. Experiments on multi-anatomical\ntumor datasets demonstrate that MAST-Pro outperforms state-of-the-art\napproaches, achieving up to a 5.20% improvement in average DSC while reducing\ntrainable parameters by 91.04%, without compromising accuracy.",
        "Despite significant advancements in video generation, inserting a given\nobject into videos remains a challenging task. The difficulty lies in\npreserving the appearance details of the reference object and accurately\nmodeling coherent motions at the same time. In this paper, we propose\nVideoAnydoor, a zero-shot video object insertion framework with high-fidelity\ndetail preservation and precise motion control. Starting from a text-to-video\nmodel, we utilize an ID extractor to inject the global identity and leverage a\nbox sequence to control the overall motion. To preserve the detailed appearance\nand meanwhile support fine-grained motion control, we design a pixel warper. It\ntakes the reference image with arbitrary key-points and the corresponding\nkey-point trajectories as inputs. It warps the pixel details according to the\ntrajectories and fuses the warped features with the diffusion U-Net, thus\nimproving detail preservation and supporting users in manipulating the motion\ntrajectories. In addition, we propose a training strategy involving both videos\nand static images with a weighted loss to enhance insertion quality.\nVideoAnydoor demonstrates significant superiority over existing methods and\nnaturally supports various downstream applications (e.g., talking head\ngeneration, video virtual try-on, multi-region editing) without task-specific\nfine-tuning.",
        "We generalize Einstein's probabilistic method for the Brownian motion to\nstudy compressible fluids in porous media. The multi-dimensional case is\nconsidered with general probability distribution functions. By relating the\nexpected displacement per unit time with the velocity of the fluid, we derive\nan anisotropic diffusion equation in non-divergence form that contains a\ntransport term. Under the Darcy law assumption, a corresponding nonlinear\npartial differential equations for the density function is obtained. The\nclassical solutions of this equation are studied, and the maximum and strong\nmaximum principles are established. We also obtain exponential decay estimates\nfor the solutions for all time, and particularly, their exponential convergence\nas time tends to infinity. Our analysis uses some transformations of the\nBernstein-Cole--Hopf type which are explicitly constructed even for very\ngeneral equation of state. Moreover, the Lemma of Growth in time is proved and\nutilized in order to achieve the above decaying estimates.",
        "Crystal bases are powerful combinatorial tools in the representation theory\nof quantum groups $U_q(\\mathfrak{g})$ for a symmetrizable Kac-Moody algebras\n$\\mathfrak{g}$. The polyhedral realizations are combinatorial descriptions of\nthe crystal base $B(\\infty)$ for Verma modules in terms of the set of integer\npoints of a polyhedral cone, which equals the string cone when $\\mathfrak{g}$\nis finite dimensional simple. It is a fundamental and natural problem to find\nexplicit forms of the polyhedral cone. The monomial realization expresses\ncrystal bases $B(\\lambda)$ of integrable highest weight representations as\nLaurent monomials with double indexed variables. In this paper, we give a\nconjecture between explicit forms of the polyhedral cones and monomial\nrealizations. We prove the conjecture is true when $\\mathfrak{g}$ is a\nclassical Lie algebra, a rank $2$ Kac-Moody algebra or a classical affine Lie\nalgebra."
      ]
    }
  },
  {
    "id":2411.17617,
    "research_type":"applied",
    "start_id":"b5",
    "start_title":"ALL-Net: Anatomical information lesion-wise loss function integrated into neural network for multiple sclerosis lesion segmentation",
    "start_abstract":"Accurate detection and segmentation of multiple sclerosis (MS) brain lesions on magnetic resonance images are important for disease diagnosis treatment. This is a challenging task as vary greatly in size, shape, location, image contrast. The objective our study was to develop an algorithm based deep convolutional neural network integrated with anatomic information lesion-wise loss function (ALL-Net) fast accurate automated MS lesions. Distance transformation mapping used construct module that encoded lesion-specific anatomical information. To overcome the lesion size imbalance during training improve small lesions, developed which individual were modeled spheres equal size. On ISBI-2015 longitudinal challenge dataset (19 subjects total), ALL-Net achieved overall score 93.32 amongst top performing methods. larger Cornell (176 significantly improved both voxel-wise metrics (Dice improvement 3.9% 35.3% p-values ranging from p < 0.01 0.0001, AUC precision-recall curve 2.1% 29.8%) (lesion-wise F1 12.6% 29.8% all ROC 1.4% 20.0%) compared leading publicly available tools.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b8"
      ],
      "title":[
        "Deep learning-Based 3D inpainting of brain MR images"
      ],
      "abstract":[
        "Abstract The detailed anatomical information of the brain provided by 3D magnetic resonance imaging (MRI) enables various neuroscience research. However, due to long scan time for MR images, 2D images are mainly obtained in clinical environments. purpose this study is generate from a sparsely sampled using an inpainting deep neural network that has U-net-like structure and DenseNet sub-blocks. To train network, not only fidelity loss but also perceptual based on VGG were considered. Various methods used assess overall similarity between inpainted original data. In addition, morphological analyzes performed investigate whether data produced local features similar diagnostic ability was evaluated investigating pattern changes disease groups. Brain anatomy details efficiently recovered proposed network. voxel-based analysis gray matter volume cortical thickness, differences observed small clusters. method will be useful utilizing advanced neuroimaging techniques with MRI"
      ],
      "categories":[
        "q-bio.CB"
      ]
    },
    "list":{
      "title":[
        "Trait-structured chemotaxis: Exploring ligand-receptor dynamics and\n  travelling wave properties in a Keller-Segel model",
        "Estimating invasive rodent abundance using removal data and hierarchical\n  models",
        "Effect of a new type of healthy and live food supplement on osteoporosis\n  blood parameters and induced rheumatoid arthritis in Wistar rats",
        "Intercellular contact is sufficient to drive Fibroblast to Myofibroblast\n  transitions",
        "Tumor-associated CD19$^+$ macrophages induce immunosuppressive\n  microenvironment in hepatocellular carcinoma",
        "Unlocking tropical forest complexity: How tree assemblages in secondary\n  forests boost biodiversity conservation",
        "From random walks to epidemic spreading: Compartment model with\n  mortality for vector transmitted diseases",
        "A spatially resolved and lipid-structured model for macrophage\n  populations in early human atherosclerotic lesions",
        "Co-Translational mRNA Decay in Plants: Recent advances and future\n  directions",
        "Multi-Site rs-fMRI Domain Alignment for Autism Spectrum Disorder\n  Auxiliary Diagnosis Based on Hyperbolic Space",
        "Dynamic Markov Blanket Detection for Macroscopic Physics Discovery",
        "Multicellular self-organization in Escherichia coli",
        "Cytogenetic, Hematobiochemical, and Histopathological Assessment of\n  Albino Rats (Rattus norvegicus) Fed on Gluten Extracts",
        "On the Ising Phase Transition in the Infrared-Divergent Spin Boson Model",
        "Rotational decoherence dynamics in ultracold molecules induced by a\n  tunable spin environment: The Central Rotor Model",
        "Pseudorapidity density distributions of charged particles and transverse\n  momentum spectra of identified particles in pp collisions in PACIAE 4.0 model",
        "Laser-based aberration corrector",
        "Dissociated Neuronal Cultures as Model Systems for Self-Organized\n  Prediction",
        "Geometric origin of supercurrents in Berry phase: Formula for computing\n  currents from wavefunctions with correlation and particle number variation",
        "Center vortices and the $\\mathrm{SU}(3)$ conformal window",
        "High-aspect-ratio silica meta-optics for high-intensity structured light",
        "Reporting on pTP sublimation during evaporation deposition",
        "Observation of a zero-energy excitation mode in the open Dicke model",
        "X-ray Thomson scattering studies on spin-singlet stabilization of highly\n  compressed H-like Be ions heated to two million degrees Kelvin",
        "Comment on: \"2005 VL1 is not Venera-2\"",
        "An Approach to Use Depletion Charges for Modifying Band Profiles for\n  Field-Effect Transistors",
        "On the Gauge Invariance of Secondary Gravitational Waves",
        "A physical model approach to order lot sizing"
      ],
      "abstract":[
        "A novel trait-structured Keller-Segel model that explores the dynamics of a\nmigrating cell population guided by chemotaxis in response to an external\nligand concentration is derived and analysed. Unlike traditional Keller-Segel\nmodels, this framework introduces an explicit representation of ligand-receptor\nbindings on the cell membrane, where the percentage of occupied receptors\nconstitutes the trait that influences cellular phenotype. The model posits that\nthe cell's phenotypic state directly modulates its capacity for chemotaxis and\nproliferation, governed by a trade-off due to a finite energy budget: cells\nhighly proficient in chemotaxis exhibit lower proliferation rates, while more\nproliferative cells show diminished chemotactic abilities. The model is derived\nfrom the principles of a biased random walk, resulting in a system of two\nnon-local partial differential equations, describing the densities of both\ncells and ligands. Using a Hopf-Cole transformation, we derive an equation that\ncharacterises the distribution of cellular traits within travelling wave\nsolutions for the total cell density, allowing us to uncover the monotonicity\nproperties of these waves. Numerical investigations are conducted to examine\nthe model's behaviour across various biological scenarios, providing insights\ninto the complex interplay between chemotaxis, proliferation, and phenotypic\ndiversity in migrating cell populations.",
        "Invasive rodents pose significant ecological, economic, and public health\nchallenges. Robust methods are needed for estimating population abundance to\nguide effective management. Traditional methods such as capture-recapture are\noften impractical for invasive species due to ethical and logistical\nconstraints. Here, I showcase the application of hierarchical multinomial\nN-mixture models for estimating the abundance of invasive rodents using removal\ndata. First, I perform a simulation study which demonstrates minimal bias in\nabundance estimates across a range of sampling scenarios. Second, I analyze\nremoval data for two invasive rodent species, namely coypus (Myocastor coypus)\nin France and muskrats (Ondatra zibethicus) in the Netherlands. Using\nhierarchical multinomial N-mixture models, I examine the effects of temperature\non abundance while accounting for imperfect and time-varying capture\nprobabilities. I also show how to accommodate spatial variability using random\neffects, and quantify uncertainty in parameter estimates. Overall, I hope to\ndemonstrate the flexibility and utility of hierarchical models in invasive\nspecies management.",
        "Summary Osteoporosis is a skeletal disorder, characterized by a decrease in\nbone strength and puts the individual at risk for fracture. On the other hand,\nrheumatoid arthritis is a systemic disease of unknown etiology that causes\ninflammation of the joints of the organs. Purpose Due to the destructive\neffects of these diseases and its increasing prevalence and lack of appropriate\nmedication for treatment, the present study aimed to evaluate the therapeutic\neffect of a new type of healthy and live food supplement on rheumatoid\narthritis and induced osteoporosis in rats. Methods In this research, healthy\nand live food powder were synthesized by a new and green route. This organic\nbiomaterial was named NBS. The NBS food supplement had various vitamins, macro\nand micro molecules, and ingredients. The new healthy and nutritious diet\nshowed that the use of this supplement led to the return of the parameters to\nnormal levels. Results The concentration of 12.5 mg\/ kg showed the least\ntherapeutic effect and 50 mg\/ kg had the highest therapeutic effect for\nosteoporosis. The results of blood parameters involved in inflammation in both\nhealthy and patient groups showed that the use of complete adjuvant induction\ncauses joint inflammation. In the study of the interaction of the\nconcentrations, it was observed that the concentration of 50 mg\/ kg had the\nhighest therapeutic effect against the disease in the studied mice. Conclusion\nThe results showed that the new healthy and viable supplement restores the\nblood osteoporotic and rheumatoid factors of the mice to normal.",
        "Fibroblast cells play a key role in maintaining the extracellular matrix.\nDuring wound healing, fibroblasts differentiate into highly contractile\nmyofibroblasts, which secrete extracellular matrix proteins like collagen to\nfacilitate tissue repair. Under normal conditions, myofibroblasts undergo\nprogrammed cell death after healing to prevent excessive scar formation.\nHowever, in diseases like fibrosis, the myofibroblasts remain active even after\nthe wound is closed, resulting in excessive collagen buildup and a stiff,\nfibrotic matrix. The reasons for the persistence of myofibroblasts in fibrosis\nare not well understood. Here we show the existence of a mechanism where direct\nphysical contact between a fibroblast and a myofibroblast is sufficient for\nfibroblasts to transition into myofibroblasts. We show that\nfibroblast-myofibroblast transition can occur even in the absence of known\nbiochemical cues such as growth factor activation or mechanical cues from a\nstiff, fibrotic matrix. Further, we show that contact-based\nfibroblast-myofibroblast activation can be blocked by G{\\alpha}q\/11\/14\ninhibitor FR9003591, which inhibits the formation of myofibroblasts. These\nfindings offer new insights into the persistence of fibrosis despite\ntherapeutic interventions and suggest a potential strategy to target\nfibroblast-to-myofibroblast transition in fibrosis.",
        "Tumor-associated macrophages are a key component that contributes to the\nimmunosuppressive microenvironment in human cancers. However, therapeutic\ntargeting of macrophages has been a challenge in clinic due to the limited\nunderstanding of their heterogeneous subpopulations and distinct functions.\nHere, we identify a unique and clinically relevant CD19$^+$ subpopulation of\nmacrophages that is enriched in many types of cancer, particularly in\nhepatocellular carcinoma (HCC). The CD19$^+$ macrophages exhibit increased\nlevels of PD-L1 and CD73, enhanced mitochondrial oxidation, and compromised\nphagocytosis, indicating their immunosuppressive functions. Targeting CD19$^+$\nmacrophages with anti-CD19 chimeric antigen receptor T (CAR-T) cells inhibited\nHCC tumor growth. We identify PAX5 as a primary driver of up-regulated\nmitochondrial biogenesis in CD19$^+$ macrophages, which depletes cytoplasmic\nCa$^{2+}$, leading to lysosomal deficiency and consequent accumulation of CD73\nand PD-L1. Inhibiting CD73 or mitochondrial oxidation enhanced the efficacy of\nimmune checkpoint blockade therapy in treating HCC, suggesting great promise\nfor CD19$^+$ macrophage-targeting therapeutics.",
        "Secondary forests now dominate tropical landscapes and play a crucial role in\nachieving COP15 conservation objectives. This study develops a replicable\nnational approach to identifying and characterising forest ecosystems, with a\nfocus on the role of secondary forests. We hypothesised that dominant tree\nspecies in the forest canopy serve as reliable indicators for delineating\nforest ecosystems and untangling biodiversity complexity. Using national\ninventories, we identified in situ clusters through hierarchical clustering\nbased on dominant species abundance dissimilarity, determined using the\nImportance Variable Index. These clusters were characterised by analysing\nspecies assemblages and their interactions. We then applied object-oriented\nRandom Forest modelling, segmenting the national forest cover using NDVI to\nidentify the forest ecosystems derived from in situ clusters. Freely available\nspectral (Sentinel-2) and environmental data were used in the model to\ndelineate and characterise key forest ecosystems. We finished with an\nassessment of distribution of secondary and old-growth forests within\necosystems. In Costa Rica, 495 dominant tree species defined 10 in situ\nclusters, with 7 main clusters successfully modelled. The modelling (F1-score:\n0.73, macro F1-score: 0.58) and species-based characterisation highlighted the\nmain ecological trends of these ecosystems, which are distinguished by specific\nspecies dominance, topography, climate, and vegetation dynamics, aligning with\nlocal forest classifications. The analysis of secondary forest distribution\nprovided an initial assessment of ecosystem vulnerability by evaluating their\nrole in forest maintenance and dynamics. This approach also underscored the\nmajor challenge of in situ data acquisition",
        "We focus on the propagation of vector-transmitted diseases in complex\nnetworks such as Barab\\'asi-Albert (BA) and Watts-Strogatz (WS) types. The\nclass of such diseases includes Malaria, Dengue (vectors are mosquitos),\nPestilence (vectors are fleas), and many others. There is no direct\ntransmission of the disease among individuals. Individuals are mimicked by\nindependent random walkers and the vectors by the nodes of the network. The\nwalkers and nodes can be either susceptible (S) or infected and infectious (I)\nrepresenting their states of health. Walkers in compartment I may die from the\ninfection (entering the dead compartment D) whereas infected nodes never die.\nThis assumption is based on the observation that vectors do not fall ill from\ntheir infection. A susceptible walker can be infected with a certain\nprobability by visiting an infected node, and a susceptible node by visits of\ninfected walkers. The time spans of infection of walkers and nodes as well as\nthe survival time span of infected walkers are assumed to be independent random\nvariables following specific probability density functions (PDFs). We implement\nthis approach into a multiple random walkers model. We establish macroscopic\nstochastic evolution equations for the mean-field compartmental population\nfractions and compare this dynamics with the outcome of the random walk\nsimulations. We obtain explicit expressions for the basic reproduction numbers\n$ R_M , R_0$ with and without mortality, respectively, and prove that $R_M <\nR_0$ . For $R_M , R_0 > 1$ the healthy state is unstable, and the disease is\nstarting to spread in presence of at least one infected walker or node. For\nzero mortality, we obtain in explicit form the stable endemic equilibrium which\nexists for $R_0 > 1$ and which is independent of the initial conditions.",
        "Atherosclerosis is a chronic inflammatory disease of the artery wall. The\nearly stages of atherosclerosis are driven by interactions between lipids and\nmonocyte-derived-macrophages (MDMs). The mechanisms that govern the spatial\ndistribution of lipids and MDMs in the lesion remain poorly understood. In this\npaper, we develop a spatially-resolved and lipid-structured model for early\natherosclerosis. The model development and analysis are guided by images of\nhuman coronary lesions by Nakashima et al. 2007. Consistent with their\nfindings, the model predicts that lipid initially accumulates deep in the\nintima due to a spatially non-uniform LDL retention capacity. The model also\nqualitatively reproduces the global internal maxima in the Nakashima images\nonly when the MDM mobility is sufficiently sensitive to lipid content, and MDM\nlifespan sufficiently insensitive. Introducing lipid content-dependence to MDM\nmobility and mean lifespan produced minimal impact on model behaviour at early\ntimes, but strongly impacted lesion composition at steady state. Increases to\nthe sensitivity of MDM lifespan to lipid content yield lesions with fewer MDMs,\nless total lesion lipid content and reduced mean MDM infiltration depth.\nIncreases to the sensitivity of MDM mobility to lipid content also reduces the\nMDM infiltration depth, but increases the proportion of lipid-laden MDMs. We\nfind that MDM lipid content increases with spatial depth, regardless of blood\nLDL and HDL content. These results shed light on the mechanisms that drive\nspatial variation in the composition of early atherosclerotic lesions, and the\nrole of macrophage lipid content in disease progression.",
        "Tight regulation of messenger RNA (mRNA) stability is essential to ensure\naccurate gene expression in response to developmental and environmental cues.\nmRNA stability is controlled by mRNA decay pathways, which have traditionally\nbeen proposed to occur independently of translation. However, the recent\ndiscovery of a co-translational mRNA decay pathway (also known as CTRD) reveals\nthat mRNA translation and decay can be coupled. While being translated, a mRNA\ncan be targeted for degradation. This pathway was first described in yeast and\nrapidly identified in several plant species. This review explores recent\nadvances in our understanding of CTRD in plants, emphasizing its regulation and\nits importance for development and stress response. The different metrics used\nto assess CTRD activity are also presented. Furthermore, this review outlines\nfuture directions to explore the importance of mRNA decay in maintaining mRNA\nhomeostasis in plants.",
        "In the medical field, most resting-state fMRI (rs-fMRI) data are collected\nfrom multiple hospital sites. Multi-site rs-fMRI data can increase the volume\nof training data, enabling auxiliary diagnostic algorithms for brain diseases\nto learn more accurate and stable models. However, due to the significant\nheterogeneity and domain shift in rs-fMRI data across different sites, the\naccuracy of auxiliary diagnosis remains unsatisfactory. Moreover, there has\nbeen limited exploration of multi-source domain adaptation algorithms, and the\ninterpretability of models is often poor. To address these challenges, we\nproposed a domain-adaptive algorithm based on hyperbolic space embedding.\nHyperbolic space is naturally suited for representing the topology of complex\nnetworks such as brain functional networks. Therefore, we embedded the brain\nfunctional network into hyperbolic space and constructed the corresponding\nhyperbolic space community network to effectively extract brain network\nrepresentations. To address the heterogeneity of data across different sites\nand the issue of domain shift, we introduce a constraint loss function, HMMD\n(Hyperbolic Maximum Mean Discrepancy), to align the marginal distributions in\nthe hyperbolic space. Additionally, we employ class prototype alignment to\nalign the conditional distributions. This significantly improves the quality of\nbrain representations and enhances diagnostic classification accuracy for\nAutism Spectrum Disorder (ASD). Experimental results demonstrated that the\nproposed algorithm is robust to multi-site heterogeneity and shows promising\npotential for brain network mechanism analysis.",
        "The free energy principle (FEP), along with the associated constructs of\nMarkov blankets and ontological potentials, have recently been presented as the\ncore components of a generalized modeling method capable of mathematically\ndescribing arbitrary objects that persist in random dynamical systems; that is,\na mathematical theory of ``every'' ``thing''. Here, we leverage the FEP to\ndevelop a mathematical physics approach to the identification of objects,\nobject types, and the macroscopic, object-type-specific rules that govern their\nbehavior. We take a generative modeling approach and use variational Bayesian\nexpectation maximization to develop a dynamic Markov blanket detection\nalgorithm that is capable of identifying and classifying macroscopic objects,\ngiven partial observation of microscopic dynamics. This unsupervised algorithm\nuses Bayesian attention to explicitly label observable microscopic elements\naccording to their current role in a given system, as either the internal or\nboundary elements of a given macroscopic object; and it identifies macroscopic\nphysical laws that govern how the object interacts with its environment.\nBecause these labels are dynamic or evolve over time, the algorithm is capable\nof identifying complex objects that travel through fixed media or exchange\nmatter with their environment. This approach leads directly to a flexible class\nof structured, unsupervised algorithms that sensibly partition complex\nmany-particle or many-component systems into collections of interacting\nmacroscopic subsystems, namely, ``objects'' or ``things''. We derive a few\nexamples of this kind of macroscopic physics discovery algorithm and\ndemonstrate its utility with simple numerical experiments, in which the\nalgorithm correctly labels the components of Newton's cradle, a burning fuse,\nthe Lorenz attractor, and a simulated cell.",
        "Escherichia coli has long been a trusty companion, maintaining health in our\nguts and advancing biological knowledge in the laboratory. In light of recent\nfindings, we discuss multicellular self-organization in E. coli and develop\ngeneral ideas for multicellularity, including the necessity for multicellular\ndynamics and interpretation by dynamic graphs, applicable to both unicellular\nand multicellular organisms. In this context, we next discuss the documented\nbehaviors of E. coli self-organization (rosette formation, multicellular\nextension, and attached dormancy) and two potential behaviors (internal\ncommunication and mating). Finally, by comparing the dynamic graphs for\ndifferent communities, we develop principles relevant to the theory of\nmulticellularity.",
        "Background: Literature shows that most of the information on the toxicity of\ngluten is generated from survey and observational studies, resulting in\ninconsistent outcomes and a decrease in the acceptability of gluten-rich foods.\nTo determine gluten's safety, an in-depth in vitro and in vivo toxicological\nexamination is required. This enables scientists to come up with ameliorative\nstrategies if it turns out to have side effects, and consumers' trust can be\nrestored. Objectives: The objective of this study was to assess the toxicity of\ngluten extracts on albino rats (Rattus norvegicus). Materials and Methods:\nTwenty-four rats were randomly selected and divided into four groups, each\ncomprising six rats. Group 1 (control) rats were fed on pellet feeds and groups\n2, 3, and 4 were fed on daily dosages of 0.5, 1.0, and 1.5 g gluten extracts,\nrespectively. The rats' body weights and reactions were observed for 90 days\nbefore blood samples were collected for hematobiochemical and micronucleus\ntests. Histopathological examinations of the liver and kidneys were also\nperformed. Results: There was no difference (P > 0.05) in body weight, blood\nglucose level, or micronuclei between the control and treated rats. The\nlymphocytes, alkaline phosphatase, alanine transaminase, total protein, and\ncalcium ions of the test rats were all significantly (P < 0.05) altered but\nremained within the normal ranges. Other hematobiochemical parameters,\nincluding packed cell volume, hemoglobin, white and red blood cells, aspartate\ntransaminase, albumin, sodium ions, potassium ions, chloride ions, and urea,\nrevealed no marked changes. The treated rats' livers and kidneys showed no\nhistopathological changes. Conclusion: Gluten had no adverse effects. However,\nit altered hematobiochemical parameters, particularly the lymphocytes, alkaline\nphosphatase, alanine transaminase, total protein, and calcium ions.",
        "We prove absence of ground states in the infrared-divergent spin boson model\nat large coupling. Our key argument reduces the proof to verifying long range\norder in the dual one-dimensional continuum Ising model, i.e., to showing that\nthe respective two point function is lower bounded by a strictly positive\nconstant. We can then use known results from percolation theory to establish\nlong range order at large coupling. Combined with the known existence of ground\nstates at small coupling, our result proves that the spin boson model undergoes\na phase transition with respect to the coupling strength. We also present an\nexpansion for the vacuum overlap of the spin boson ground state in terms of the\nIsing $n$-point functions, which implies that the phase transition is unique,\ni.e., that there is a critical coupling constant below which a ground state\nexists and above which none can exist.",
        "We show that quantum rotational wavepacket dynamics in molecules can be\ndescribed by a new system-environment model, which consists of a rotational\nsubsystem coupled to a magnetically tunable spin bath formed by the nuclear\nspins within the molecule. The central rotor model shares similarities with the\nparadigmatic central spin model, but features much richer rotational dynamics\nthat is sensitive to the molecule's environment, which can be initiated and\nprobed with short laser pulses used to control molecular orientation and\nalignment. We present numerical simulations of the nuclear-spin-bath-induced\nrotational decoherence dynamics of KRb molecules, which exhibit remarkable\nsensitivity to an external magnetic field. Our results show that ultracold\nmolecular gases provide a natural platform for the experimental realization of\nthe CRM.",
        "The pseudorapidity density distributions of charged particles and the\ntransverse momentum spectra of identified particles in proton-proton (pp)\ncollisions at the center-of-mass energies ranging from $\\sqrt{s}=200$ GeV to 13\nTeV have been systematically studied using the newly released parton and\ncascade model PACIAE 4.0 based on PYTHIA 8.3. The available experimental data\nare well reproduced across all analyzed aspects. This theoretical method can be\neasily extended to anywhere the experimental data for pp collisions are\ncurrently unavailable. Furthermore, since pp collisions serve as the baseline\nfor heavy-ion collisions, our results can provide a valuable resource for both\nexperimentalists and theorists.",
        "Aberration correctors are essential elements for achieving atomic resolution\nin state-of-the-art electron microscopes. Conventional correctors are based on\na series of multipolar electron lenses, but more versatile alternatives are\nintensively sought. Here we suggest spatially tailored intense laser pulses as\none such alternative. Our simulations demonstrate that the free-space\nelectron-photon interaction can be used to compensate for spherical and\nchromatic aberrations of subsequent electron lenses. We show a significant\nimprovement in the simulated electron probe sizes and discuss the prospects of\nutilizing the tailored laser fields as a platform for novel electron optics in\nultrafast electron microscope setups.",
        "Dissociated neuronal cultures provide a simplified yet effective model system\nfor investigating self-organized prediction and information processing in\nneural networks. This review consolidates current research demonstrating that\nthese in vitro networks display fundamental computational capabilities,\nincluding predictive coding, adaptive learning, goal-directed behavior, and\ndeviance detection. We examine how these cultures develop critical dynamics\noptimized for information processing, detail the mechanisms underlying learning\nand memory formation, and explore the relevance of the free energy principle\nwithin these systems. Building on these insights, we discuss how findings from\ndissociated neuronal cultures inform the design of neuromorphic and reservoir\ncomputing architectures, with the potential to enhance energy efficiency and\nadaptive functionality in artificial intelligence. The reduced complexity of\nneuronal cultures allows for precise manipulation and systematic investigation,\nbridging theoretical frameworks with practical implementations in bio-inspired\ncomputing. Finally, we highlight promising future directions, emphasizing\nadvancements in three-dimensional culture techniques, multi-compartment models,\nand brain organoids that deepen our understanding of hierarchical and\npredictive processes in both biological and artificial systems. This review\naims to provide a comprehensive overview of how dissociated neuronal cultures\ncontribute to neuroscience and artificial intelligence, ultimately paving the\nway for biologically inspired computing solutions.",
        "The complexity of itinerant and many-body nature in Bardeen-Cooper-Schrieffer\n(BCS) wavefunctions has traditionally led to the use of coarse-grained order\nparameters for describing currents in superconductors (SC), rather than\ndirectly utilizing wavefunctions. In this work, we introduce a phase-based\nformula that enables the direct computation of currents from microscopic\nwavefunctions, accounting for correlation and particle number variations.\nInterestingly, the formulation draws parallels with insulators, suggesting a\nunified framework for understanding (intra-band) charge transport across two\nextremes of conductivity. A group velocity current\n$J_{band}{\\propto}\\frac{1}{\\hbar}{\\partial}_kE(k)$ is derived from Berry phase,\nindependent of wave package dynamics, robust against correlation. Additionally,\nwe identify a correlation-driven contribution, $J_{corr}$, which reveals that\nthe pairing correlations ${\\langle}c_kc_{-k}{\\rangle}$ among dancing partners\nprovide a current component beyond the velocity operator.",
        "A novel approach for estimating the lower end of the $\\mathrm{SU}(3)$\nconformal window is presented through the study of center vortex geometry and\nits dependence on the number of fermion flavors $N_f$. Values ranging from $N_f\n= 2$--$8$ are utilized to infer an upper limit for vortex behavior in the low\n$N_f$ phase, which may inform the transition to the conformal window. The\nsimulations are performed at a single lattice spacing and pion mass, both fixed\nfor all $N_f$. Visualizations of the center vortex structure in\nthree-dimensional slices of the lattice reveal a growing roughness in the\nvortex matter as a function of $N_f$, embodied by an increase in the density of\nvortex matter in the percolating cluster and a simultaneous reduction in\nsecondary clusters disconnected from the percolating cluster in 3D slices. This\nis quantified by various bulk properties, including the vortex and branching\npoint densities. A correlation of the vortex structure reveals a turning point\nnear $N_f \\simeq 5$ past which a randomness in the vortex field becomes the\ndominant aspect of its evolution with $N_f$. As a byproduct, extrapolations to\nthe vortex content of a uniform-random gauge field provide a critical point at\nwhich there must be a drastic shift in vacuum field structure. A precise\nestimate for the critical value is extracted as $N_f^* = 11.43(16)(17)$, close\nto various other estimates.",
        "Structured light and high-intensity ultrafast lasers are two rapidly\nadvancing frontiers in photonics, yet their intersection remains largely\nunexplored. While ultrafast lasers continue to push the boundaries of peak\nintensities, structured light has enabled unprecedented control over light's\nspatial, temporal, and polarization properties. However, the lack of robust\noptical devices capable of bridging structured light with the high-intensity\ndomain has constrained progress in combining these directions. Here, we\ndemonstrate high-aspect-ratio silica meta-optics, which close this gap by\ncombining silica's extraordinary damage resistance with the advanced phase and\npolarization control offered by metasurfaces. By leveraging anisotropic etching\ntechniques, we fabricate nanopillars exceeding 3 $\\mu$m in height with aspect\nratios up to 14, enabling precise manipulation of complex light fields at\nintensities far beyond the thresholds of conventional metasurfaces. We showcase\ntheir functionality in generating vortex beams and achieving polarization\nmanipulation with large phase retardance at challenging long-visible\nwavelengths. High-aspect-ratio silica meta-optics unlock structured\nlaser-matter interactions in extreme regimes, that will surpass plasma\nionization thresholds and enable applications such as relativistic particle\nacceleration and high-harmonic generation with structured beams, for both\ntabletop ultrafast systems and large-scale laser facilities.",
        "Noble liquid detectors rely on wavelength shifter materials, such as\np-terphenyl (pTP) and Tetraphenyl-butadiene (TPB), which are widely used in\nneutrino and dark matter experiments. Given their importance, a thorough\nunderstanding and characterization of these compounds are essential for\noptimizing experimental techniques and enhancing detector performance. In this\nstudy, we report a novel phenomenon in which commonly used wavelength shifters\nundergo spontaneous sublimation under high vacuum conditions. We quantify the\nsublimation rates of pTP and TPB as a function of pressure and temperature,\nassessing their impact on material growth and physical properties.\nAdditionally, we investigate how variations in film thickness and growth rate\ninfluence the sublimation process. These findings provide critical insights\ninto improving the handling and preparation of wavelength shifters during the\nfabrication of light detectors for these experiments, ensuring their stability\nand performance in low-background photodetection systems.",
        "Approaching phase boundaries in many-body systems can give rise to intriguing\nsignatures in their excitation spectra. Here, we explore the excitation\nspectrum of a Bose-Einstein condensate strongly coupled to an optical cavity\nand pumped by an optical standing wave, which simulates the famous\nDicke-Hepp-Lieb phase transition of the open Dicke model with dissipation\narising due to photon leakage from the cavity. For weak dissipation, the\nexcitation spectrum displays two strongly polaritonic modes. Close to the phase\nboundary, we observe an intriguing regime where the lower-energetic of these\nmodes, instead of showing the expected roton-type mode softening, is found to\napproach and persist at zero energy, well before the critical pump strength for\nthe Dicke-Hepp-Lieb transition boundary is reached. Hence, a peculiar situation\narises, where an excitation is possible at zero energy cost, but nevertheless\nno instability of the system is created.",
        "Experiments at the US National Ignition Facility (NIF) [D\\\"{o}ppner et al.,\nNature {\\bf 618}, 270-275 (2023)] have created highly compressed hot\nhydrogen-like Be plasmas. Published analyses of the the NIF experiment have\nused finite-$T$ multi-atom density-functional theory (DFT) with Molecular\ndynamics (MD), and Path-Integral Monte Carlo (PIMC) simulations. These methods\nare very expensive to implement and often lack physical transparency. Here we\n(i) relate their results to simpler first-principles average-atom results, (ii)\nestablish the feasibility of rapid data analysis, with good accuracy and gain\nin physical transparency, and (iii) show that the NIF experiment reveals\nhigh-$T$ spin-singlet pairing of hydrogen-like Be ions with near neighbours.\nOur analysis predicts such stabilization over a wide range of compressed\ndensities for temperatures close to two million Kelvin. Calculations of\nstructure factors $S(k)$ for electrons or ions, the Raleigh weight and other\nquantities of interest to X-ray Thomson scattering are presented. We find that\nthe NIF data at the scattering wavevector $k_{sc}$ of 7.89 \\AA$^{-1}$ are more\nconsistent with a density of $20\\pm2$ g\/cm$^3$, mean ionization $\\bar{Z}=$3.25,\nat a temperature of $\\simeq$ 1,800,000 K than the 34 g\/cm$^3, \\bar{Z}=3.4$\nproposed by the NIF team. The relevance of ion-electron coupled-modes in\nstudying small $k_{sc}$ data is indicated.",
        "I show that the small differences between the orbital parameters of the dark\ncomet 2005 VL1 and the Venera 2 spacecraft (reported in arXiv:2503.07972) are\nof the magnitude expected from gravitational deflection by a close encounter of\nVenera 2 with Venus.",
        "We present the study of using depletion charges for tailoring lateral band\nprofiles and applying it to the promising gate-all-around field-effect\ntransistors (GAAFET). Specifically, we introduce heavily p-type doped Si next\nto the channel, but outside the channel, of a transistor. They are connected to\nthe heavily n-type doped source and drain for generating the depletion charges.\nThe finite difference method was used for simulations and the results show\nsignificant modifications of the conduction band along the channel. The\ndepletion charges act as built-in electrodes capable of significantly modifying\nthe band profiles of field-effect transistors. Quantum confinement within the\nchannel has been attempted with different approaches, such as additional\nelectrodes and point contacts. The results presented show two aspects of this\napproach, namely, realizing quantum confinement in an all-Si structure and\ntailoring band profiles within channels to modify their transport properties.",
        "Second-order tensor perturbations induced by primordial fluctuations play a\ncrucial role in probing small-scale physics, but gauge dependence of their\nenergy density has remained a fundamental challenge in cosmological\nperturbation theory. We address this issue by introducing a boundary\ncondition-based filtering method that extracts physical radiation through the\nSommerfeld criterion. We demonstrate that after filtering non-physical modes,\nthe energy density of secondary gravitational waves becomes gauge-invariant and\nexhibits physically consistent behavior in the sub-horizon limit. This approach\nprovides a unified framework for both adiabatic and isocurvature perturbations,\nenhancing theoretical predictions and observational signatures of early\nuniverse physics.",
        "The growing need for companies to reduce costs and maximize profits has led\nto an increased focus on logistics activities. Among these, inventory\nmanagement plays a crucial role in minimizing organizational expenses by\noptimizing the storage and transportation of materials. In this context, this\nstudy introduces an optimization model for the lot-sizing problem based on a\nphysical system approach. By establishing that the material supply problem is\nisomorphic to a one-dimensional mechanical system of point particles connected\nby elastic elements, we leverage this analogy to derive cost optimization\nconditions naturally and obtain an exact solution. This approach determines lot\nsizes that minimize the combined ordering and inventory holding costs in a\nsignificantly shorter time, eliminating the need for heuristic methods. The\noptimal lot sizes are defined in terms of the parameter $ \\gamma = 2C_O \/ C_H\n$, which represents the relationship between the ordering cost per order ($ C_O\n$) and the holding cost per period for the material required in one period ($\nC_H $). This parameter fully dictates the system's behavior: when $ \\gamma \\leq\n1 $, the optimal strategy is to place one order per period, whereas for $\n\\gamma > 1 $, the number of orders $ N $ is reduced relative to the planning\nhorizon $ M $, meaning $ N < M $. By formulating the total cost function in\nterms of the intensive variable $ N\/M $, we consolidate the entire optimization\nproblem into a single function of $ \\gamma $. This eliminates the need for\ncomplex algorithms, enabling faster and more precise purchasing decisions. The\nproposed model was validated through a real-world case study and benchmarked\nagainst classical algorithms, demonstrating superior cost optimization and\nreduced execution time. These findings underscore the potential of this\napproach for improving material lot-sizing strategies."
      ]
    }
  },
  {
    "id":2411.17617,
    "research_type":"applied",
    "start_id":"b8",
    "start_title":"Deep learning-Based 3D inpainting of brain MR images",
    "start_abstract":"Abstract The detailed anatomical information of the brain provided by 3D magnetic resonance imaging (MRI) enables various neuroscience research. However, due to long scan time for MR images, 2D images are mainly obtained in clinical environments. purpose this study is generate from a sparsely sampled using an inpainting deep neural network that has U-net-like structure and DenseNet sub-blocks. To train network, not only fidelity loss but also perceptual based on VGG were considered. Various methods used assess overall similarity between inpainted original data. In addition, morphological analyzes performed investigate whether data produced local features similar diagnostic ability was evaluated investigating pattern changes disease groups. Brain anatomy details efficiently recovered proposed network. voxel-based analysis gray matter volume cortical thickness, differences observed small clusters. method will be useful utilizing advanced neuroimaging techniques with MRI",
    "start_categories":[
      "q-bio.CB"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b5"
      ],
      "title":[
        "ALL-Net: Anatomical information lesion-wise loss function integrated into neural network for multiple sclerosis lesion segmentation"
      ],
      "abstract":[
        "Accurate detection and segmentation of multiple sclerosis (MS) brain lesions on magnetic resonance images are important for disease diagnosis treatment. This is a challenging task as vary greatly in size, shape, location, image contrast. The objective our study was to develop an algorithm based deep convolutional neural network integrated with anatomic information lesion-wise loss function (ALL-Net) fast accurate automated MS lesions. Distance transformation mapping used construct module that encoded lesion-specific anatomical information. To overcome the lesion size imbalance during training improve small lesions, developed which individual were modeled spheres equal size. On ISBI-2015 longitudinal challenge dataset (19 subjects total), ALL-Net achieved overall score 93.32 amongst top performing methods. larger Cornell (176 significantly improved both voxel-wise metrics (Dice improvement 3.9% 35.3% p-values ranging from p < 0.01 0.0001, AUC precision-recall curve 2.1% 29.8%) (lesion-wise F1 12.6% 29.8% all ROC 1.4% 20.0%) compared leading publicly available tools."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Generating Symbolic World Models via Test-time Scaling of Large Language\n  Models",
        "Advanced Tool Learning and Selection System (ATLASS): A Closed-Loop\n  Framework Using LLM",
        "Enhancing the Product Quality of the Injection Process Using eXplainable\n  Artificial Intelligence",
        "Equilibrate RLHF: Towards Balancing Helpfulness-Safety Trade-off in\n  Large Language Models",
        "MAPoRL: Multi-Agent Post-Co-Training for Collaborative Large Language\n  Models with Reinforcement Learning",
        "Navigation-GPT: A Robust and Adaptive Framework Utilizing Large Language\n  Models for Navigation Applications",
        "DocPuzzle: A Process-Aware Benchmark for Evaluating Realistic\n  Long-Context Reasoning Capabilities",
        "GKG-LLM: A Unified Framework for Generalized Knowledge Graph\n  Construction",
        "Leveraging Taxonomy and LLMs for Improved Multimodal Hierarchical\n  Classification",
        "The Einstein Test: Towards a Practical Test of a Machine's Ability to\n  Exhibit Superintelligence",
        "Generating Causally Compliant Counterfactual Explanations using ASP",
        "Guidelines for Applying RL and MARL in Cybersecurity Applications",
        "Guided Code Generation with LLMs: A Multi-Agent Framework for Complex\n  Code Tasks",
        "MI-DETR: An Object Detection Model with Multi-time Inquiries Mechanism",
        "Non-linear Partition of Unity method",
        "Linear statistics at the microscopic scale for the 2D Coulomb gas",
        "The role of effective mass and long-range interactions in the band-gap\n  renormalization of photo-excited semiconductors",
        "Machine-learning potentials for structurally and chemically complex MAB\n  phases: strain hardening and ripplocation-mediated plasticity",
        "On Elephant Random Walk with Random Memory",
        "Theoretical Data-Driven MobilePosenet: Lightweight Neural Network for\n  Accurate Calibration-Free 5-DOF Magnet Localization",
        "Security and Quality in LLM-Generated Code: A Multi-Language,\n  Multi-Model Analysis",
        "ProJudge: A Multi-Modal Multi-Discipline Benchmark and\n  Instruction-Tuning Dataset for MLLM-based Process Judges",
        "Analysis of Information Loss on Composition Measurement in Stiff\n  Chemically Reacting Systems",
        "ReVeil: Unconstrained Concealed Backdoor Attack on Deep Neural Networks\n  using Machine Unlearning",
        "Uni-Retrieval: A Multi-Style Retrieval Framework for STEM's Education",
        "Non-ergodic Phase Transition in the Global Hysteresis of the Frustrated\n  Magnet DyRu2Si2",
        "Evaluation for Regression Analyses on Evolving Data Streams",
        "Many-body perturbation theory for moir\\'{e} systems"
      ],
      "abstract":[
        "Solving complex planning problems requires Large Language Models (LLMs) to\nexplicitly model the state transition to avoid rule violations, comply with\nconstraints, and ensure optimality-a task hindered by the inherent ambiguity of\nnatural language. To overcome such ambiguity, Planning Domain Definition\nLanguage (PDDL) is leveraged as a planning abstraction that enables precise and\nformal state descriptions. With PDDL, we can generate a symbolic world model\nwhere classic searching algorithms, such as A*, can be seamlessly applied to\nfind optimal plans. However, directly generating PDDL domains with current LLMs\nremains an open challenge due to the lack of PDDL training data. To address\nthis challenge, we propose to scale up the test-time computation of LLMs to\nenhance their PDDL reasoning capabilities, thereby enabling the generation of\nhigh-quality PDDL domains. Specifically, we introduce a simple yet effective\nalgorithm, which first employs a Best-of-N sampling approach to improve the\nquality of the initial solution and then refines the solution in a fine-grained\nmanner with verbalized machine learning. Our method outperforms o1-mini by a\nconsiderable margin in the generation of PDDL domain, achieving over 50%\nsuccess rate on two tasks (i.e., generating PDDL domains from natural language\ndescription or PDDL problems). This is done without requiring additional\ntraining. By taking advantage of PDDL as state abstraction, our method is able\nto outperform current state-of-the-art methods on almost all competition-level\nplanning tasks.",
        "The combination of LLM agents with external tools enables models to solve\ncomplex tasks beyond their knowledge base. Human-designed tools are inflexible\nand restricted to solutions within the scope of pre-existing tools created by\nexperts. To address this problem, we propose ATLASS, an advanced tool learning\nand selection system designed as a closed-loop framework. It enables the LLM to\nsolve problems by dynamically generating external tools on demand. In this\nframework, agents play a crucial role in orchestrating tool selection,\nexecution, and refinement, ensuring adaptive problem-solving capabilities. The\noperation of ATLASS follows three phases: The first phase, Understanding Tool\nRequirements, involves the Agents determining whether tools are required and\nspecifying their functionality; the second phase, Tool Retrieval\/Generation,\ninvolves the Agents retrieving or generating tools based on their availability;\nand the third phase, Task Solving, involves combining all the component tools\nnecessary to complete the initial task. The Tool Dataset stores the generated\ntools, ensuring reusability and minimizing inference cost. Current LLM-based\ntool generation systems have difficulty creating complex tools that need APIs\nor external packages. In ATLASS, we solve the problem by automatically setting\nup the environment, fetching relevant API documentation online, and using a\nPython interpreter to create a reliable, versatile tool that works in a wider\nrange of situations. OpenAI GPT-4.0 is used as the LLM agent, and safety and\nethical concerns are handled through human feedback before executing generated\ncode. By addressing the limitations of predefined toolsets and enhancing\nadaptability, ATLASS serves as a real-world solution that empowers users with\ndynamically generated tools for complex problem-solving.",
        "The injection molding process is a traditional technique for making products\nin various industries such as electronics and automobiles via solidifying\nliquid resin into certain molds. Although the process is not related to\ncreating the main part of engines or semiconductors, this manufacturing\nmethodology sets the final form of the products. Re-cently, research has\ncontinued to reduce the defect rate of the injection molding process. This\nstudy proposes an optimal injection molding process control system to reduce\nthe defect rate of injection molding products with XAI (eXplainable Artificial\nIntelligence) ap-proaches. Boosting algorithms (XGBoost and LightGBM) are used\nas tree-based classifiers for predicting whether each product is normal or\ndefective. The main features to control the process for improving the product\nare extracted by SHapley Additive exPlanations, while the individual\nconditional expectation analyzes the optimal control range of these extracted\nfeatures. To validate the methodology presented in this work, the actual\ninjection molding AI manufacturing dataset provided by KAMP (Korea AI\nManufacturing Platform) is employed for the case study. The results reveal that\nthe defect rate decreases from 1.00% (Original defect rate) to 0.21% with\nXGBoost and 0.13% with LightGBM, respectively.",
        "Fine-tuning large language models (LLMs) based on human preferences, commonly\nachieved through reinforcement learning from human feedback (RLHF), has been\neffective in improving their performance. However, maintaining LLM safety\nthroughout the fine-tuning process remains a significant challenge, as\nresolving conflicts between safety and helpfulness can be non-trivial.\nTypically, the safety alignment of LLM is trained on data with safety-related\ncategories. However, our experiments find that naively increasing the scale of\nsafety training data usually leads the LLMs to an ``overly safe'' state rather\nthan a ``truly safe'' state, boosting the refusal rate through extensive\nsafety-aligned data without genuinely understanding the requirements for safe\nresponses. Such an approach can inadvertently diminish the models' helpfulness.\nTo understand the phenomenon, we first investigate the role of safety data by\ncategorizing them into three different groups, and observe that each group\nbehaves differently as training data scales up. To boost the balance between\nsafety and helpfulness, we propose an Equilibrate RLHF framework including a\nFine-grained Data-centric (FDC) approach that achieves better safety alignment\neven with fewer training data, and an Adaptive Message-wise Alignment (AMA)\napproach, which selectively highlight the key segments through a gradient\nmasking strategy. Extensive experimental results demonstrate that our approach\nsignificantly enhances the safety alignment of LLMs while balancing safety and\nhelpfulness.",
        "Leveraging multiple large language models (LLMs) to build collaborative\nmulti-agentic workflows has demonstrated significant potential. However, most\nprevious studies focus on prompting the out-of-the-box LLMs, relying on their\ninnate capability for collaboration, which may not improve LLMs' performance as\nshown recently. In this paper, we introduce a new post-training paradigm MAPoRL\n(Multi-Agent Post-co-training for collaborative LLMs with Reinforcement\nLearning), to explicitly elicit the collaborative behaviors and further unleash\nthe power of multi-agentic LLM frameworks. In MAPoRL, multiple LLMs first\ngenerate their own responses independently and engage in a multi-turn\ndiscussion to collaboratively improve the final answer. In the end, a MAPoRL\nverifier evaluates both the answer and the discussion, by assigning a score\nthat verifies the correctness of the answer, while adding incentives to\nencourage corrective and persuasive discussions. The score serves as the\nco-training reward, and is then maximized through multi-agent RL. Unlike\nexisting LLM post-training paradigms, MAPoRL advocates the co-training of\nmultiple LLMs together using RL for better generalization. Accompanied by\nanalytical insights, our experiments demonstrate that training individual LLMs\nalone is insufficient to induce effective collaboration. In contrast,\nmulti-agent co-training can boost the collaboration performance across\nbenchmarks, with generalization to unseen domains.",
        "Existing navigation decision support systems often perform poorly when\nhandling non-predefined navigation scenarios. Leveraging the generalization\ncapabilities of large language model (LLM) in handling unknown scenarios, this\nresearch proposes a dual-core framework for LLM applications to address this\nissue. Firstly, through ReAct-based prompt engineering, a larger LLM core\ndecomposes intricate navigation tasks into manageable sub-tasks, which\nautonomously invoke corresponding external tools to gather relevant\ninformation, using this feedback to mitigate the risk of LLM hallucinations.\nSubsequently, a fine-tuned and compact LLM core, acting like a first-mate is\ndesigned to process such information and unstructured external data, then to\ngenerates context-aware recommendations, ultimately delivering lookout insights\nand navigation hints that adhere to the International Regulations for\nPreventing Collisions at Sea (COLREGs) and other rules. Extensive experiments\ndemonstrate the proposed framework not only excels in traditional ship\ncollision avoidance tasks but also adapts effectively to unstructured,\nnon-predefined, and unpredictable scenarios. A comparative analysis with\nDeepSeek-R1, GPT-4o and other SOTA models highlights the efficacy and\nrationality of the proposed framework. This research bridges the gap between\nconventional navigation systems and LLMs, offering a framework to enhance\nsafety and operational efficiency across diverse navigation applications.",
        "We present DocPuzzle, a rigorously constructed benchmark for evaluating\nlong-context reasoning capabilities in large language models (LLMs). This\nbenchmark comprises 100 expert-level QA problems requiring multi-step reasoning\nover long real-world documents. To ensure the task quality and complexity, we\nimplement a human-AI collaborative annotation-validation pipeline. DocPuzzle\nintroduces an innovative evaluation framework that mitigates guessing bias\nthrough checklist-guided process analysis, establishing new standards for\nassessing reasoning capacities in LLMs. Our evaluation results show that:\n1)Advanced slow-thinking reasoning models like o1-preview(69.7%) and\nDeepSeek-R1(66.3%) significantly outperform best general instruct models like\nClaude 3.5 Sonnet(57.7%); 2)Distilled reasoning models like\nDeepSeek-R1-Distill-Qwen-32B(41.3%) falls far behind the teacher model,\nsuggesting challenges to maintain the generalization of reasoning capabilities\nrelying solely on distillation.",
        "The construction of Generalized Knowledge Graph (GKG), including knowledge\ngraph, event knowledge graph and commonsense knowledge graph, is fundamental\nfor various natural language processing tasks. Current studies typically\nconstruct these types of graph separately, overlooking holistic insights and\npotential unification that could be beneficial in computing resources and usage\nperspectives. However, a key challenge in developing a unified framework for\nGKG is obstacles arising from task-specific differences. In this study, we\npropose a unified framework for constructing generalized knowledge graphs to\naddress this challenge. First, we collect data from 15 sub-tasks in 29 datasets\nacross the three types of graphs, categorizing them into in-sample,\ncounter-task, and out-of-distribution (OOD) data. Then, we propose a\nthree-stage curriculum learning fine-tuning framework, by iteratively injecting\nknowledge from the three types of graphs into the Large Language Models.\nExtensive experiments show that our proposed model improves the construction of\nall three graph types across in-domain, OOD and counter-task data.",
        "Multi-level Hierarchical Classification (MLHC) tackles the challenge of\ncategorizing items within a complex, multi-layered class structure. However,\ntraditional MLHC classifiers often rely on a backbone model with independent\noutput layers, which tend to ignore the hierarchical relationships between\nclasses. This oversight can lead to inconsistent predictions that violate the\nunderlying taxonomy. Leveraging Large Language Models (LLMs), we propose a\nnovel taxonomy-embedded transitional LLM-agnostic framework for multimodality\nclassification. The cornerstone of this advancement is the ability of models to\nenforce consistency across hierarchical levels. Our evaluations on the MEP-3M\ndataset - a multi-modal e-commerce product dataset with various hierarchical\nlevels - demonstrated a significant performance improvement compared to\nconventional LLM structures.",
        "Creative and disruptive insights (CDIs), such as the development of the\ntheory of relativity, have punctuated human history, marking pivotal shifts in\nour intellectual trajectory. Recent advancements in artificial intelligence\n(AI) have sparked debates over whether state of the art models possess the\ncapacity to generate CDIs. We argue that the ability to create CDIs should be\nregarded as a significant feature of machine superintelligence (SI).To this\nend, we propose a practical test to evaluate whether an approach to AI\ntargeting SI can yield novel insights of this kind. We propose the Einstein\ntest: given the data available prior to the emergence of a known CDI, can an AI\nindependently reproduce that insight (or one that is formally equivalent)? By\nachieving such a milestone, a machine can be considered to at least match\nhumanity's past top intellectual achievements, and therefore to have the\npotential to surpass them.",
        "This research is focused on generating achievable counterfactual\nexplanations. Given a negative outcome computed by a machine learning model or\na decision system, the novel CoGS approach generates (i) a counterfactual\nsolution that represents a positive outcome and (ii) a path that will take us\nfrom the negative outcome to the positive one, where each node in the path\nrepresents a change in an attribute (feature) value. CoGS computes paths that\nrespect the causal constraints among features. Thus, the counterfactuals\ncomputed by CoGS are realistic. CoGS utilizes rule-based machine learning\nalgorithms to model causal dependencies between features. The paper discusses\nthe current status of the research and the preliminary results obtained.",
        "Reinforcement Learning (RL) and Multi-Agent Reinforcement Learning (MARL)\nhave emerged as promising methodologies for addressing challenges in automated\ncyber defence (ACD). These techniques offer adaptive decision-making\ncapabilities in high-dimensional, adversarial environments. This report\nprovides a structured set of guidelines for cybersecurity professionals and\nresearchers to assess the suitability of RL and MARL for specific use cases,\nconsidering factors such as explainability, exploration needs, and the\ncomplexity of multi-agent coordination. It also discusses key algorithmic\napproaches, implementation challenges, and real-world constraints, such as data\nscarcity and adversarial interference. The report further outlines open\nresearch questions, including policy optimality, agent cooperation levels, and\nthe integration of MARL systems into operational cybersecurity frameworks. By\nbridging theoretical advancements and practical deployment, these guidelines\naim to enhance the effectiveness of AI-driven cyber defence strategies.",
        "Large Language Models (LLMs) have shown remarkable capabilities in code\ngeneration tasks, yet they face significant limitations in handling complex,\nlong-context programming challenges and demonstrating complex compositional\nreasoning abilities. This paper introduces a novel agentic framework for\n``guided code generation'' that tries to address these limitations through a\ndeliberately structured, fine-grained approach to code generation tasks. Our\nframework leverages LLMs' strengths as fuzzy searchers and approximate\ninformation retrievers while mitigating their weaknesses in long sequential\nreasoning and long-context understanding. Empirical evaluation using OpenAI's\nHumanEval benchmark with Meta's Llama 3.1 8B model (int4 precision)\ndemonstrates a 23.79\\% improvement in solution accuracy compared to direct\none-shot generation. Our results indicate that structured, guided approaches to\ncode generation can significantly enhance the practical utility of LLMs in\nsoftware development while overcoming their inherent limitations in\ncompositional reasoning and context handling.",
        "Based on analyzing the character of cascaded decoder architecture commonly\nadopted in existing DETR-like models, this paper proposes a new decoder\narchitecture. The cascaded decoder architecture constrains object queries to\nupdate in the cascaded direction, only enabling object queries to learn\nrelatively-limited information from image features. However, the challenges for\nobject detection in natural scenes (e.g., extremely-small, heavily-occluded,\nand confusingly mixed with the background) require an object detection model to\nfully utilize image features, which motivates us to propose a new decoder\narchitecture with the parallel Multi-time Inquiries (MI) mechanism. MI enables\nobject queries to learn more comprehensive information, and our MI based model,\nMI-DETR, outperforms all existing DETR-like models on COCO benchmark under\ndifferent backbones and training epochs, achieving +2.3 AP and +0.6 AP\nimprovements compared to the most representative model DINO and SOTA model\nRelation-DETR under ResNet-50 backbone. In addition, a series of diagnostic and\nvisualization experiments demonstrate the effectiveness, rationality, and\ninterpretability of MI.",
        "This paper introduces the Non-linear Partition of Unity Method, a novel\ntechnique integrating Radial Basis Function interpolation and Weighted\nEssentially Non-Oscillatory algorithms. It addresses challenges in\nhigh-accuracy approximations, particularly near discontinuities, by adapting\nweights dynamically. The method is rooted in the Partition of Unity framework,\nenabling efficient decomposition of large datasets into subproblems while\nmaintaining accuracy. Smoothness indicators and compactly supported functions\nensure precision in regions with discontinuities. Error bounds are calculated\nand validate its effectiveness, showing improved interpolation in discontinuous\nand smooth regions. Some numerical experiments are performed to check the\ntheoretical results.",
        "We consider the classical Coulomb gas in two dimensions at the inverse\ntemperature $\\beta=2$, confined within a droplet of radius $R$ by a\nrotationally invariant potential $U(r)$. For $U(r)\\sim r^2$ this describes the\neigenvalues of the complex Ginibre ensemble of random matrices. We study linear\nstatistics of the form ${\\cal L}_N = \\sum_{i=1}^N f(|{\\bf x}_i|)$, where ${\\bf\nx}_i$'s are the positions of the $N$ particles, in the large $N$ limit with\n$R=O(1)$. It is known that for smooth functions $f(r)$ the variance ${\\rm Var}\n\\,{\\cal L}_N= O(1)$, while for an indicator function relevant for the disk\ncounting statistics, all cumulants of ${\\cal L}_N$ of order $q \\geq 2$ behave\nas $\\sim \\sqrt{N}$. In addition, for smooth functions, it was shown that the\ncumulants of ${\\cal L}_N$ of order $q \\geq 3$ scale as $\\sim N^{2-q}$.\nSurprisingly it was found that they depend only on $f'(|\\bf x|)$ and its\nderivatives evaluated exactly at the boundary of the droplet. To understand\nthis property, and interpolate between the two behaviors (smooth versus\nstep-like), we study the microscopic linear statistics given by $f(r) \\to\nf_N(r) = \\phi((r-\\hat r) \\sqrt{N}\/\\xi)$, which probes the fluctuations at the\nscale of the inter-particle distance. We compute the cumulants of ${\\cal L}_N$\nat large $N$ for a fixed $\\phi(u)$ at arbitrary $\\xi$. For large $\\xi$ they\nmatch the predictions for smooth functions which shows that the leading\ncontribution in that case comes from a boundary layer of size $1\/\\sqrt{N}$ near\nthe boundary of the droplet. Finally we show that the full probability\ndistribution of ${\\cal L}_N$ take two distinct large deviation forms, in the\nregime ${\\cal L}_N \\sim \\sqrt{N}$ and ${\\cal L}_N \\sim N$ respectively. We also\ndiscuss applications of our results to fermions in a rotating harmonic trap and\nto the Ginibre symplectic ensemble.",
        "Understanding how to control changes in electronic structure and related\ndynamical renormalizations by external driving fields is the key for\nunderstanding ultrafast spectroscopy and applications in electronics. Here we\nfocus on the band-gap's modulation by external electric fields and uncover the\neffect of band dispersion on the gap renormalization. We employ the Green's\nfunction formalism using the real-time Dyson expansion to account for dynamical\ncorrelations induced by photodoping. The many-body formalism captures the\ndynamics of systems with long-range interactions, carrier mobility, and\nvariable electron and hole effective mass. We also demonstrate that mean-field\nsimulations based on the Hartree-Fock Hamiltonian, which lacks dynamical\ncorrelations, yields a qualitatively incorrect picture of band-gap\nrenormalization. We find the trend that increasing effective mass, thus\ndecreasing mobility, leads to as much as a 6\\% enhancement in band-gap\nrenormalization. Further, the renormalization is strongly dependent on the\ndegree of photodoping. As the screening induced by free electrons and holes\neffectively reduces any long-range and interband interactions for highly\nexcited systems, we show that there is a specific turnover point with minimal\nband-gap. We further demonstrate that the optical gap renormalization follows\nthe same trend though its magnitude is altered by the Moss-Burstein effect.",
        "Though offering unprecedented pathways to molecular dynamics (MD) simulations\nof technologically-relevant materials and conditions, machine-learning\ninteratomic potentials (MLIPs) are typically trained for ``simple'' materials\nand properties with minor size effects. Our study of MAB phases (MABs) -\nalternating transition metal boride (MB) and group A element layers -\nexemplifies that MLIPs for complex materials can be fitted and used in a\nhigh-throughput fashion: for predicting structural and mechanical properties\nacross a large chemical\/phase\/temperature space. Considering group 4-6\ntransition metal based MABs, with A=Al and the 222, 212, and 314 type phases,\nthree MLIPs are trained and tested, including lattice and elastic constants\ncalculations at temperatures $T\\in\\{0,300,1200\\}$ K, extrapolation grade and\nenergy (force, stress) error analysis for $\\approx{3\\cdot10^6}$ ab initio MD\nsnapshots. Subsequently, nanoscale tensile tests serve to quantify upper limits\nof strength and toughness attainable in single-crystal MABs at 300~K as well as\ntheir temperature evolution. In-plane tensile deformation is characterised by\nrelatively high strength, {110}$\\langle001\\rangle$ type slipping, and failure\nby shear banding. The response to [001] loading is softer, triggers work\nhardening, and failure by kinking and layer delamination. Furthermore,\nW$_2$AlB$_2$ able to retard fracture via ripplocations and twinning from 300 up\nto 1200~K.",
        "In this paper, we introduce the elephant random walk (ERW) with memory\nconsisting of randomly selected steps from its history. It is a time-changed\nvariant of the standard elephant random walk with memory consisting of its full\nhistory. At each time point, the time changing component is the composition of\ntwo uniformly distributed independent random variables with support over all\nthe past steps. Several conditional distributional properties including the\nconditional mean increments and conditional displacement of ERW with random\nmemory are obtained. Using these conditional results, we derive the recursive\nand explicit expressions for the mean increments and mean displacement of the\nwalk.",
        "Permanent magnet tracking using the external sensor array is crucial for the\naccurate localization of wireless capsule endoscope robots. Traditional\ntracking algorithms, based on the magnetic dipole model and Levenberg-Marquardt\n(LM) algorithm, face challenges related to computational delays and the need\nfor initial position estimation. More recently proposed neural network-based\napproaches often require extensive hardware calibration and real-world data\ncollection, which are time-consuming and labor-intensive. To address these\nchallenges, we propose MobilePosenet, a lightweight neural network architecture\nthat leverages depthwise separable convolutions to minimize computational cost\nand a channel attention mechanism to enhance localization accuracy. Besides,\nthe inputs to the network integrate the sensors' coordinate information and\nrandom noise, compensating for the discrepancies between the theoretical model\nand the actual magnetic fields and thus allowing MobilePosenet to be trained\nentirely on theoretical data. Experimental evaluations conducted in a \\(90\n\\times 90 \\times 80\\) mm workspace demonstrate that MobilePosenet exhibits\nexcellent 5-DOF localization accuracy ($1.54 \\pm 1.03$ mm and $2.24 \\pm\n1.84^{\\circ}$) and inference speed (0.9 ms) against state-of-the-art methods\ntrained on real-world data. Since network training relies solely on theoretical\ndata, MobilePosenet can eliminate the hardware calibration and real-world data\ncollection process, improving the generalizability of this permanent magnet\nlocalization method and the potential for rapid adoption in different clinical\nsettings.",
        "Artificial Intelligence (AI)-driven code generation tools are increasingly\nused throughout the software development lifecycle to accelerate coding tasks.\nHowever, the security of AI-generated code using Large Language Models (LLMs)\nremains underexplored, with studies revealing various risks and weaknesses.\nThis paper analyzes the security of code generated by LLMs across different\nprogramming languages. We introduce a dataset of 200 tasks grouped into six\ncategories to evaluate the performance of LLMs in generating secure and\nmaintainable code. Our research shows that while LLMs can automate code\ncreation, their security effectiveness varies by language. Many models fail to\nutilize modern security features in recent compiler and toolkit updates, such\nas Java 17. Moreover, outdated methods are still commonly used, particularly in\nC++. This highlights the need for advancing LLMs to enhance security and\nquality while incorporating emerging best practices in programming languages.",
        "As multi-modal large language models (MLLMs) frequently exhibit errors when\nsolving scientific problems, evaluating the validity of their reasoning\nprocesses is critical for ensuring reliability and uncovering fine-grained\nmodel weaknesses. Since human evaluation is laborious and costly, prompting\nMLLMs as automated process judges has become a common practice. However, the\nreliability of these model-based judges remains uncertain. To address this, we\nintroduce ProJudgeBench, the first comprehensive benchmark specifically\ndesigned for evaluating abilities of MLLM-based process judges. ProJudgeBench\ncomprises 2,400 test cases and 50,118 step-level labels, spanning four\nscientific disciplines with diverse difficulty levels and multi-modal content.\nIn ProJudgeBench, each step is meticulously annotated by human experts for\ncorrectness, error type, and explanation, enabling a systematic evaluation of\njudges' capabilities to detect, classify and diagnose errors. Evaluation on\nProJudgeBench reveals a significant performance gap between open-source and\nproprietary models. To bridge this gap, we further propose ProJudge-173k, a\nlarge-scale instruction-tuning dataset, and a Dynamic Dual-Phase fine-tuning\nstrategy that encourages models to explicitly reason through problem-solving\nbefore assessing solutions. Both contributions significantly enhance the\nprocess evaluation capabilities of open-source models. All the resources will\nbe released to foster future research of reliable multi-modal process\nevaluation.",
        "Gas sampling methods have been crucial for the advancement of combustion\nscience, enabling analysis of reaction kinetics and pollutant formation.\nHowever, the measured composition can deviate from the true one because of the\npotential residual reactions in the sampling probes. This study formulates the\ninitial composition estimation in stiff chemically reacting systems as a\nBayesian inference problem, solved using the No-U-Turn Sampler (NUTS).\nInformation loss arises from the restriction of system dynamics by low\ndimensional attracting manifold, where constrained evolution causes initial\nperturbations to decay or vanish in fast eigen-directions in composition space.\nThis study systematically investigates the initial value inference in\ncombustion systems and successfully validates the methodological framework in\nthe Robertson toy system and hydrogen autoignition. Furthermore, a gas sample\ncollected from a one-dimensional hydrogen diffusion flame is analyzed to\ninvestigate the effect of frozen temperature on information loss. The research\nhighlights the importance of species covariance information from observations\nin improving estimation accuracy and identifies how the rank reduction in the\nsensitivity matrix leads to inference failures. Critical failure times for\nspecies inference in the Robertson and hydrogen autoignition systems are\nanalyzed, providing insights into the limits of inference reliability and its\nphysical significance.",
        "Backdoor attacks embed hidden functionalities in deep neural networks (DNN),\ntriggering malicious behavior with specific inputs. Advanced defenses monitor\nanomalous DNN inferences to detect such attacks. However, concealed backdoors\nevade detection by maintaining a low pre-deployment attack success rate (ASR)\nand restoring high ASR post-deployment via machine unlearning. Existing\nconcealed backdoors are often constrained by requiring white-box or black-box\naccess or auxiliary data, limiting their practicality when such access or data\nis unavailable. This paper introduces ReVeil, a concealed backdoor attack\ntargeting the data collection phase of the DNN training pipeline, requiring no\nmodel access or auxiliary data. ReVeil maintains low pre-deployment ASR across\nfour datasets and four trigger patterns, successfully evades three popular\nbackdoor detection methods, and restores high ASR post-deployment through\nmachine unlearning.",
        "In AI-facilitated teaching, leveraging various query styles to interpret\nabstract text descriptions is crucial for ensuring high-quality teaching.\nHowever, current retrieval models primarily focus on natural text-image\nretrieval, making them insufficiently tailored to educational scenarios due to\nthe ambiguities in the retrieval process. In this paper, we propose a diverse\nexpression retrieval task tailored to educational scenarios, supporting\nretrieval based on multiple query styles and expressions. We introduce the STEM\nEducation Retrieval Dataset (SER), which contains over 24,000 query pairs of\ndifferent styles, and the Uni-Retrieval, an efficient and style-diversified\nretrieval vision-language model based on prompt tuning. Uni-Retrieval extracts\nquery style features as prototypes and builds a continuously updated Prompt\nBank containing prompt tokens for diverse queries. This bank can updated during\ntest time to represent domain-specific knowledge for different subject\nretrieval scenarios. Our framework demonstrates scalability and robustness by\ndynamically retrieving prompt tokens based on prototype similarity, effectively\nfacilitating learning for unknown queries. Experimental results indicate that\nUni-Retrieval outperforms existing retrieval models in most retrieval tasks.\nThis advancement provides a scalable and precise solution for diverse\neducational needs.",
        "Some frustrated magnets exhibit a huge hysteresis called \"global hysteresis\n(GH)\", where the magnetic plateaus appearing in the increasing field process\nare skipped in the decreasing field process from the high magnetic field state.\nIn this paper, we focused on the frustrated magnet DyRu2Si2 and measured\nmagnetization relaxations from two plateau states inside the GH loop, the\nphases III and IV, and investigated the phase transitions into them. As a\nresult of the relaxation measurements, no relaxation is observed in the phase\nIII, whereas long-time relaxations of more than 105 sec are observed at the\nphase IV plateau. Moreover, a Mpemba-effect-like relaxation phenomenon where\nthe relaxation from an initial state prepared in the zero-field-cooled\ncondition overtakes that from an initial state prepared in the field-cooled\ncondition is observed. These results indicate that the phase IV is the\nnon-ergodic state with a complex free-energy landscape with multiple local\nminima, while the phase III has a simple free energy structure. Therefore, the\nIII-IV phase transition is considered to be the ergodic to non-ergodic phase\ntransition. Although this type of phase transition typically occurs in random\nglassy systems, the phase IV in DyRu2Si2 has a regular long-range ordered\nmagnetic structure and yet exhibits non-ergodic properties, which is highly\nnontrivial. Our findings open the possibility of observing non-ergodic states\nin frustrated magnets with regular long-range orders.",
        "The paper explores the challenges of regression analysis in evolving data\nstreams, an area that remains relatively underexplored compared to\nclassification. We propose a standardized evaluation process for regression and\nprediction interval tasks in streaming contexts. Additionally, we introduce an\ninnovative drift simulation strategy capable of synthesizing various drift\ntypes, including the less-studied incremental drift. Comprehensive experiments\nwith state-of-the-art methods, conducted under the proposed process, validate\nthe effectiveness and robustness of our approach.",
        "Moir\\'{e} systems such as magic-angle twisted bilayer graphene have attracted\nsignificant attention due to their ability to host correlated phenomena\nincluding superconductivity and strongly correlated insulating states. By\ndefining the single-particle Green's function in the band basis, we\nsystematically develop a many-body perturbation theory framework to address\ncorrelations beyond the usual mean-field Hartree-Fock approaches. As a specific\nexample, we first analyze twisted bilayer graphene within the Hartree-Fock\napproximation. We derive analytical solutions for symmetry-breaking states at\ninteger fillings and the finite-temperature metal-insulator transition that\nclosely match previously known numerical results in the literature. Moving\nbeyond Hartree-Fock, we incorporate self-consistent GW corrections\ndemonstrating that first-order diagrams significantly overestimate the\nfilling-dependent fluctuations in the electronic compressibility. This\nframework provides a comprehensive pathway for exploring strong electronic\ncorrelations in moir\\'{e} systems beyond mean-field, giving new insights into\nthe interplay of symmetry breaking and electron correlations."
      ]
    }
  },
  {
    "id":2411.06513,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"Advances and Open Problems in Federated Learning",
    "start_abstract":"The term Federated Learning was coined as recently 2016 to describe a machine learning setting where multiple entities collaborate in solving problem, under the coordination of central server or service provider. Each client\u2019s raw data is stored locally and not exchanged transferred; instead, focused updates intended for immediate aggregation are used achieve objective. Since then, topic has gathered much interest across many different disciplines realization that these interdisciplinary problems likely requires just but techniques from distributed optimization, cryptography, security, differential privacy, fairness, compressed sensing, systems, information theory, statistics, more. This monograph contributions leading experts disciplines, who latest state-of-the art their perspective. These have been carefully curated into comprehensive treatment enables reader understand work done get pointers effort required solve before can become reality practical systems. Researchers working area systems will find this an enlightening read may inspire them on challenging issues outlined. up speed quickly easily what increasingly important topic: Learning.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "The future of digital health with federated learning"
      ],
      "abstract":[
        "Abstract Data-driven machine learning (ML) has emerged as a promising approach for building accurate and robust statistical models from medical data, which is collected in huge volumes by modern healthcare systems. Existing data not fully exploited ML primarily because it sits silos privacy concerns restrict access to this data. However, without sufficient will be prevented reaching its full potential and, ultimately, making the transition research clinical practice. This paper considers key factors contributing issue, explores how federated (FL) may provide solution future of digital health highlights challenges considerations that need addressed."
      ],
      "categories":[
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "Effect of a new type of healthy and live food supplement on osteoporosis\n  blood parameters and induced rheumatoid arthritis in Wistar rats",
        "Inverse problems with experiment-guided AlphaFold",
        "Integrating anatomy and electrophysiology in the healthy human heart:\n  Insights from biventricular statistical shape analysis using universal\n  coordinates",
        "Photodynamic, UV-curable and fibre-forming polyvinyl alcohol derivative\n  with broad processability and staining-free antibacterial capability",
        "Practical parameter identifiability of respiratory mechanics in the\n  extremely preterm infant",
        "Reducing Size Bias in Sampling for Infectious Disease Spread on Networks",
        "Bridging high resolution sub-cellular imaging with physiologically\n  relevant engineered tissues",
        "Unveiling sex dimorphism in the healthy cardiac anatomy: fundamental\n  differences between male and female heart shapes",
        "A tumor-immune model of chronic myeloid leukemia with optimal\n  immunotherapeutic protocols",
        "Geometric immunosuppression in CAR-T cell treatment: Insights from\n  mathematical modeling",
        "Advanced 3D-Printed Multiphasic Scaffold with Optimal PRP Dosage for\n  Chondrogenesis of BM-MSCs in Osteochondral Tissue Engineering",
        "Tumor microenvironment (Part I): Tissue integrity in a rat model of\n  peripheral neural cancer",
        "Simplified model of immunotherapy for glioblastoma multiforme: cancer\n  stem cells hypothesis perspective",
        "Iterative phase retrieval algorithm for space-variant PSF in optical\n  systems with aberrations",
        "Network-Driven Global Stability Analysis: SVIRS Epidemic Model",
        "VAEs and GANs: Implicitly Approximating Complex Distributions with\n  Simple Base Distributions and Deep Neural Networks -- Principles, Necessity,\n  and Limitations",
        "The Equation of State of QCD up to very high temperatures",
        "Toward Large-Scale Distributed Quantum Long Short-Term Memory with\n  Modular Quantum Computers",
        "Domination Parameters of Graph Covers",
        "$L^2$-estimates on flat vector bundles and Pr\\'ekopa's theorem",
        "Unveiling potential candidates for rare-earth-free permanent magnet and\n  magnetocaloric effect applications: a high throughput screening in Fe-N\n  alloys",
        "QPEs as Lense-Thirring precession of super-Eddington flows",
        "On monotonicity of heat kernels: a new example and counterexamples",
        "3D Surface Reconstruction and Volume Approximation via the meshless\n  methods",
        "SOE's ESG Performance on Financial Flexibility: The Evidence from the\n  Hong Kong Stock Market",
        "Unsupervised optimal deep transfer learning for classification under\n  general conditional shift",
        "Multivariable Behavioral Change Modeling of Epidemics in the Presence of\n  Undetected Infections",
        "SWIFT: Mapping Sub-series with Wavelet Decomposition Improves Time\n  Series Forecasting"
      ],
      "abstract":[
        "Summary Osteoporosis is a skeletal disorder, characterized by a decrease in\nbone strength and puts the individual at risk for fracture. On the other hand,\nrheumatoid arthritis is a systemic disease of unknown etiology that causes\ninflammation of the joints of the organs. Purpose Due to the destructive\neffects of these diseases and its increasing prevalence and lack of appropriate\nmedication for treatment, the present study aimed to evaluate the therapeutic\neffect of a new type of healthy and live food supplement on rheumatoid\narthritis and induced osteoporosis in rats. Methods In this research, healthy\nand live food powder were synthesized by a new and green route. This organic\nbiomaterial was named NBS. The NBS food supplement had various vitamins, macro\nand micro molecules, and ingredients. The new healthy and nutritious diet\nshowed that the use of this supplement led to the return of the parameters to\nnormal levels. Results The concentration of 12.5 mg\/ kg showed the least\ntherapeutic effect and 50 mg\/ kg had the highest therapeutic effect for\nosteoporosis. The results of blood parameters involved in inflammation in both\nhealthy and patient groups showed that the use of complete adjuvant induction\ncauses joint inflammation. In the study of the interaction of the\nconcentrations, it was observed that the concentration of 50 mg\/ kg had the\nhighest therapeutic effect against the disease in the studied mice. Conclusion\nThe results showed that the new healthy and viable supplement restores the\nblood osteoporotic and rheumatoid factors of the mice to normal.",
        "Proteins exist as a dynamic ensemble of multiple conformations, and these\nmotions are often crucial for their functions. However, current structure\nprediction methods predominantly yield a single conformation, overlooking the\nconformational heterogeneity revealed by diverse experimental modalities. Here,\nwe present a framework for building experiment-grounded protein structure\ngenerative models that infer conformational ensembles consistent with measured\nexperimental data. The key idea is to treat state-of-the-art protein structure\npredictors (e.g., AlphaFold3) as sequence-conditioned structural priors, and\ncast ensemble modeling as posterior inference of protein structures given\nexperimental measurements. Through extensive real-data experiments, we\ndemonstrate the generality of our method to incorporate a variety of\nexperimental measurements. In particular, our framework uncovers previously\nunmodeled conformational heterogeneity from crystallographic densities, and\ngenerates high-accuracy NMR ensembles orders of magnitude faster than the\nstatus quo. Notably, we demonstrate that our ensembles outperform AlphaFold3\nand sometimes better fit experimental data than publicly deposited structures\nto the Protein Data Bank (PDB). We believe that this approach will unlock\nbuilding predictive models that fully embrace experimentally observed\nconformational diversity.",
        "A cardiac digital twin is a virtual replica of a patient-specific heart,\nmimicking its anatomy and physiology. A crucial step of building a cardiac\ndigital twin is anatomical twinning, where the computational mesh of the\ndigital twin is tailored to the patient-specific cardiac anatomy. In a number\nof studies, the effect of anatomical variation on clinically relevant\nfunctional measurements like electrocardiograms (ECGs) is investigated, using\ncomputational simulations. While such a simulation environment provides\nresearchers with a carefully controlled ground truth, the impact of anatomical\ndifferences on functional measurements in real-world patients remains\nunderstudied. In this study, we develop a biventricular statistical shape model\nand use it to quantify the effect of biventricular anatomy on ECG-derived and\ndemographic features, providing novel insights for the development of digital\ntwins of cardiac electrophysiology. To this end, a dataset comprising\nhigh-resolution cardiac CT scans from 271 healthy individuals, including\nathletes, is utilized. Furthermore, a novel, universal, ventricular\ncoordinate-based method is developed to establish lightweight shape\ncorrespondence. The performance of the shape model is rigorously established,\nfocusing on its dimensionality reduction capabilities and the training data\nrequirements. Additionally, a comprehensive synthetic cohort is made available,\nfeaturing ready-to-use biventricular meshes with fiber structures and\nanatomical region annotations. These meshes are well-suited for\nelectrophysiological simulations.",
        "Antimicrobial photodynamic therapy (APDT) is a promising antibiotic-free\nstrategy for broad-spectrum infection control in chronic wounds, minimising\nbacterial resistance risks. However, rapid photosensitiser diffusion, tissue\nstaining, side toxicity, and short-lived antimicrobial effects present\nsignificant clinical limitations for integrating APDT into wound dressings. To\naddress these challenges, we present the design of a bespoke polyvinyl alcohol\n(PVA) derivative conjugated with both phenothiazine and methacrylate\nfunctionalities, enabling staining-free antibacterial photodynamic effects,\ncellular tolerability and processability into various wound dressing formats,\nincluding films, textile fibres and nanoscale coatings. Tosylation of PVA is\nleveraged for the covalent coupling of toluidine blue, as confirmed by UV-Vis\nspectroscopy and the minimal release of TB in vitro. UV-induced network\nformation is exploited to accomplish cast films and nanoscale integrated wound\ndressing coatings. UV curing is also successfully coupled with an in-house wet\nspinning process to realise individual, water-insoluble fibres as the building\nblocks of fibrous wound dressings. A fluorometric assay supports the generation\nof reactive oxygen species when the UV-cured samples are exposed to work, but\nnot UV, light, yielding a mean log10 reduction of up to 2.13 in S. aureus, and\nthe complete eradication of P. aeruginosa. Direct and extract cytotoxicity\ntests with UV-cured films and fibres demonstrate the viability of L929\nfibroblasts following 60-min light irradiation and 72-hour cell culture. The\nbespoke molecular architecture, broad processability and cellular tolerability\nof this PVA derivative are highly attractive aiming to integrate durable\nstaining-free photodynamic capability in a wide range of healthcare\ntechnologies, from chronic wound dressings up to minimally invasive localised\ntherapy.",
        "The complexity of mathematical models describing respiratory mechanics has\ngrown in recent years, however, parameter identifiability of such models has\nonly been studied in the last decade in the context of observable data. This\nstudy investigates parameter identifiability of a nonlinear respiratory\nmechanics model tuned to the physiology of an extremely preterm infant, using\nglobal Morris screening, local deterministic sensitivity analysis, and singular\nvalue decomposition-based subset selection. The model predicts airflow and\ndynamic pulmonary volumes and pressures under varying levels of continuous\npositive airway pressure, and a range of parameters characterizing both\nsurfactant-treated and surfactant-deficient lung. Sensitivity analyses\nindicated eleven parameters influence model outputs over the range of\ncontinuous positive airway pressure and lung health scenarios. The model was\nadapted to data from a spontaneously breathing 1 kg infant using gradient-based\noptimization to estimate the parameter subset characterizing the patient's\nstate of health.",
        "Epidemiological models can aid policymakers in reducing disease spread by\npredicting outcomes based on disease dynamics and contact network\ncharacteristics. Calibrating these models requires representative network\nsamples. In this connection, we investigate two sampling algorithms, Random\nWalk (RW), and Metropolis-Hastings Random Walk (MHRW), across three network\ntypes: Erd\\H{o}s-R\\'enyi (ER), Small-world (SW), and Scale-free (SF). Disease\ntransmission is simulated using a susceptible-infected-recovered (SIR)\nframework. Our findings show that RW overestimates infected individuals and\nsecondary infections by $25\\%$ for ER and SW networks due to size bias,\nfavouring highly connected nodes. MHRW, which corrects for size bias, provides\nestimates that are more consistent with the underlying network. Also, both\nmethods yield estimates significantly closer to the underlying network for\ntime-to-infection. However, sampling SF networks exhibits significant\nvariability, for both algorithms. Removing duplicate sampled nodes reduces\nMHRW's accuracy across all network types. We apply both algorithms to a cattle\nmovement network of $46,512$ farms, exhibiting ER, SW, and SF network features.\nRW overestimates infected farms by approximately $100\\%$ and secondary\ninfections by $>900\\%$, reflecting size bias whereas MHRW estimates align\nclosely with the cattle network dynamics. Time-to-infection estimates reveal\nthat RW underestimates by approximately $40\\%$, while MHRW slightly\noverestimates by $10\\%$. Estimates differ greatly when duplicate nodes are\nremoved. These findings underscore choosing algorithms based on network\nstructure and disease severity. RW's conservative estimates suit\nhigh-mortality, fast-spreading diseases, while MHRW provides precise\ninterventions suitable for less severe outbreaks. These insights can guide\npolicymakers in optimizing resource allocation and disease control strategies.",
        "While high-resolution microscopic techniques are crucial for studying\ncellular structures in cell biology, obtaining such images from thick 3D\nengineered tissues remains challenging. In this review, we explore advancements\nin fluorescence microscopy, alongside the use of various fluorescent probes and\nmaterial processing techniques to address these challenges. We navigate through\nthe diverse array of imaging options available in tissue engineering field,\nfrom wide field to super-resolution microscopy, so researchers can make more\ninformed decisions based on the specific tissue and cellular structures of\ninterest. Finally, we provide some recent examples of how traditional\nlimitations on obtaining high-resolution images on sub-cellular architecture\nwithin 3D tissues have been overcome by combining imaging advancements with\ninnovative tissue engineering approaches.",
        "Sex-based differences in cardiovascular disease are well documented, yet the\nprecise nature and extent of these discrepancies in cardiac anatomy remain\nincompletely understood. Traditional scaling models often fail to capture the\ninterplay of age, blood pressure, and body size, prompting a more nuanced\ninvestigation. Here, we employ statistical shape modeling in a healthy subset\n(n=456) of the UK Biobank to explore sex-specific variations in biventricular\nanatomy. We reconstruct 3D meshes and perform multivariate analyses of shape\ncoefficients, controlling for age, blood pressure, and various body size\nmetrics. Our findings reveal that sex alone explains at least 25 percent of\nmorphological variability, with strong discrimination between men and women\n(AUC=0.96-0.71) persisting even after correction for confounders. Notably, the\nmost discriminative modes highlight pronounced differences in cardiac chamber\nvolumes, the anterior-posterior width of the right ventricle, and the relative\npositioning of the cardiac chambers. These results underscore that sex has a\nfundamental influence on cardiac morphology, which may have important clinical\nimplications for differing cardiac structural assessments in men and women.\nFuture work should investigate how these anatomical differences manifest in\nvarious cardiovascular conditions, ultimately paving the way for more precise\nrisk stratification and personalized therapeutic strategies for both men and\nwomen.",
        "The interactions between tumor cells and the immune system play a crucial\nrole in cancer evolution. In this study, we explore how these interactions\ninfluence cancer progression by modeling the relationships among naive T cells,\neffector T cells, and chronic myeloid leukemia cells. We examine the existence\nof equilibria, the asymptotic stability of the positive steady state, and the\nglobal stability of the tumor-free equilibrium. Additionally, we develop a\npartial differential equation to describe the conditions under which the\nconcentration of cancer cells reaches a level that allows for effective control\nof cancer evolution. Finally, we apply our proposed model to investigate\noptimal treatment strategies that aim to minimize both the concentration of\ncancer cells at the end of treatment and the accumulation of tumor burden, as\nwell as the cost associated with treatment during the intervention period. Our\nstudy reveals an optimal therapeutic protocol using optimal control theory. We\nperform numerical simulations to illustrate our theoretical results and to\nexplore the dynamic behavior of the system and optimal therapeutic protocols.\nThe simulations indicate that the optimal treatment strategy can be more\neffective than a constant treatment approach, even when applying the same\ntreatment interval and total drug input.",
        "Chimeric antigen receptor T (CAR-T) cell therapy has emerged as a promising\ntreatment for hematological malignancies, offering a targeted approach to\ncancer treatment. Understanding the complexities of CAR-T cell therapy within\nsolid tumors poses challenges due to the intricate interactions within the\ntumor microenvironment. Mathematical modeling may serve as a valuable tool to\nunravel the dynamics of CAR-T cell therapy and improve its effectiveness in\nsolid tumors. This study aimed to investigate the impact of spatial aspects in\nCAR-T therapy of solid tumors, utilizing cellular automata for modeling\npurposes. Our main objective was to deepen our understanding of treatment\neffects by analyzing scenarios with different spatial distributions and varying\nthe initial quantities of tumor and CAR-T cells. Tumor geometry significantly\ninfluenced treatment efficacy in-silico, with notable differences observed\nbetween tumors with block-like arrangements and those with sparse cell\ndistributions, leading to the concept of immune suppression due to geometrical\neffects. This research delves into the intricate relationship between spatial\ndynamics and the effectiveness of CAR-T therapy in solid tumors, highlighting\nthe relevance of tumor geometry in the outcome of cellular immunotherapy\ntreatments. Our results provide a basis for improving the efficacy of CAR-T\ncell treatments by combining them with other ones reducing the density of\ncompact tumor areas and thus opening access ways for tumor killing T-cells.",
        "In osteochondral tissue engineering (OCTE), simultaneously regenerating\nsubchondral bone and cartilage tissue presents a significant challenge.\nMultiphasic scaffolds were created and manufactured using 3D printing to\naddress this issue. Excellent interfacial mechanical properties and\nbiocompatibility enhance the growth and chondrogenic differentiation of bone\nmarrow mesenchymal stem cells (BM-MSCs). The subchondral bone bottom layer is\nmimicked by incorporating varying concentrations of graphene oxide (GO) (0%,\n1%, and 2% w\/v) into a bioink composed of alginate (Alg) and gelatin (Gel).\nBased on evaluations of mechanical and biocompatibility properties, 1% GO is\nselected for further studies. Subsequently, the GO concentration is kept\nconstant while varying the platelet-rich plasma (PRP) dosage in the multiphasic\nscaffolds. Different PRP dosages (0%, 1%, 2%, and 3% w\/v) are integrated into\nthe Alg-Gel bioink to simulate cartilage tissues. Results indicate that\n3D-printed scaffolds containing 1% or 2% PRP exhibit favorable biomechanical\nproperties, with no significant differences observed. However, BM-MSCs exposed\nto 2% PRP demonstrate enhanced adhesion, growth, and viability. Additionally,\nreal-time PCR and Alcian blue staining confirm increased chondrogenic\nexpression and glycosaminoglycans (GAGs) synthesis. This work highlights the\npromising potential of 3D-printed multiphasic frameworks in the development of\nOCTE.",
        "ICAM-1 (intercellular adhesion molecule 1) and MPZ (myelin protein zero) are\nthought to be a factor in the integrity of nerve tissues. In this report, we\nattempted to trace the expression of ICAM-1, responsible for cell-to-cell\nadhesion, and of MPZ, the main constituent of myelin sheath, in malignant\ntissues of the sciatic nerve (SN) in inbred male Copenhagen rats. AT-1 Cells\n(anaplastic tumor 1) were injected in the perineurial sheath, and tissues of\nthe SNs were collected after 7, 14 and 21 days and compared to a sham-operated\ngroup of rats (n = 6 each). Tissues were sectioned and histologically examined,\nunder light microscope, and stained for measuring the immunoreactivity of\nICAM-1 and MPZ under laser scanning microscope. The cancer model was\nestablished, and the tumor growth was confirmed. ICAM-1 showed severe\ndecreases, proportional to the growing anaplastic cells, as compared to the\nsham group. MPZ revealed, however, a distinct defensive pattern before\nsubstantially decreasing in a comparison with sham. These results support the\nnotion that malignancies damage peripheral nerves and cause severe axonal\ninjury and loss of neuronal integrity, and clearly define the role of ICAM-1\nand MPZ in safeguarding the nerve tissues.",
        "Despite ongoing efforts in cancer research, a fully effective treatment for\nglioblastoma multiforme (GBM) is still unknown. Since adoptive cell transfer\nimmunotherapy is one of the potential cure candidates, efforts have been made\nto assess its effectiveness using mathematical modeling. In this paper, we\nconsider a model of GBM immunotherapy proposed by Abernathy and Burke (2016),\nwhich also takes into account the dynamics of cancer stem cells, i.e., the type\nof cancer cells that are hypothesized to be largely responsible for cancer\nrecurrence. We modify the initial ODE system by applying simplifying\nassumptions and analyze the existence and stability of steady states of the\nobtained simplified model depending on the treatment levels.",
        "Iterative phase retrieval algorithms are widely used in digital optics for\ntheir efficiency and simplicity. Conventionally, these algorithms do not\nconsider aberrations as they assume an ideal, aberration-free optical system.\nHere, we propose modified iterative phase retrieval algorithms that take into\naccount the space-invariant and space-variant point spread function of the\noptical system.",
        "An epidemic Susceptible-Vaccinated-Infected-Removed-Susceptible (SVIRS) model\nis presented on a weighted-undirected network with graph Laplacian diffusion.\nDisease-free equilibrium always exists while the existence and uniqueness of\nendemic equilibrium have been shown. When the basic reproduction number is\nbelow unity, the disease-free equilibrium is asymptotically globally stable.\nThe endemic equilibrium is asymptotically globally stable if the basic\nreproduction number is above unity. Numerical analysis is illustrated with a\nroad graph of the state of Minnesota. The effect of all important model\nparameters has been discussed.",
        "This tutorial focuses on the fundamental architectures of Variational\nAutoencoders (VAE) and Generative Adversarial Networks (GAN), disregarding\ntheir numerous variations, to highlight their core principles. Both VAE and GAN\nutilize simple distributions, such as Gaussians, as a basis and leverage the\npowerful nonlinear transformation capabilities of neural networks to\napproximate arbitrarily complex distributions. The theoretical basis lies in\nthat a linear combination of multiple Gaussians can almost approximate any\nprobability distribution, while neural networks enable further refinement\nthrough nonlinear transformations. Both methods approximate complex data\ndistributions implicitly. This implicit approximation is crucial because\ndirectly modeling high-dimensional distributions explicitly is often\nintractable. However, the choice of a simple latent prior, while\ncomputationally convenient, introduces limitations. In VAEs, the fixed Gaussian\nprior forces the posterior distribution to align with it, potentially leading\nto loss of information and reduced expressiveness. This restriction affects\nboth the interpretability of the model and the quality of generated samples.",
        "We present the non-perturbative computation of the entropy density in QCD for\ntemperatures ranging from 3 GeV up to the electro-weak scale, using $N_f=3$\nflavours of massless O$(a)$-improved Wilson fermions. We adopt a new strategy\ndesigned to be computationally efficient and based on formulating thermal QCD\nin a moving reference frame, where the fields satisfy shifted boundary\nconditions in the temporal direction and periodic boundary conditions along the\nspatial ones. In this setup the entropy density can be computed as the\nderivative of the free-energy density with respect to the shift parameter. For\neach physical temperature, we perform Monte Carlo simulations at four values of\nthe lattice spacing in order to extrapolate the numerical data of the entropy\ndensity to the continuum limit. We achieve a final accuracy of approximatively\n$0.5$-$1.0\\%$ and our results are compared with predictions from\nhigh-temperature perturbation theory.",
        "In this work, we introduce a Distributed Quantum Long Short-Term Memory\n(QLSTM) framework that leverages modular quantum computing to address\nscalability challenges on Noisy Intermediate-Scale Quantum (NISQ) devices. By\nembedding variational quantum circuits into LSTM cells, the QLSTM captures\nlong-range temporal dependencies, while a distributed architecture partitions\nthe underlying Variational Quantum Circuits (VQCs) into smaller, manageable\nsubcircuits that can be executed on a network of quantum processing units. We\nassess the proposed framework using nontrivial benchmark problems such as\ndamped harmonic oscillators and Nonlinear Autoregressive Moving Average\nsequences. Our results demonstrate that the distributed QLSTM achieves stable\nconvergence and improved training dynamics compared to classical approaches.\nThis work underscores the potential of modular, distributed quantum computing\narchitectures for large-scale sequence modelling, providing a foundation for\nthe future integration of hybrid quantum-classical solutions into advanced\nQuantum High-performance computing (HPC) ecosystems.",
        "A graph $G$ is a \\emph{cover} of a graph $F$ if there exists an onto mapping\n$\\pi : V(G) \\to V(F)$, called a (\\emph{covering}) \\emph{projection}, such that\n$\\pi$ maps the neighbours of any vertex $v$ in $G$ bijectively onto the\nneighbours of $\\pi(v)$ in $F$. This paper is the first attempt to study the\nconnection between domination parameters and graph covers. We focus on the\ndomination number, the total domination number, and the connected domination\nnumber. We prove tight upper bounds for the domination parameters of $G$.\nMoreover, we prove lower bounds for the domination parameters of $G$. Finally,\nwe propose a conjecture on the lower bound for the domination number of $G$ and\nprovide evidence to support the conjecture.",
        "In this paper, we will construct H\\\"ormander's $L^2$-estimate of the operator\n$d$ on a flat vector bundle over a $p$-convex Riemannian manifold and discuss\nsome geometric applications of it. In particular, we will generalize the\nclassical Pr\\'ekopa's theorem in convex analysis.",
        "Based on high-throughput density functional theory calculations, we have\nfound 49 ferromag-netic cases in FexN1-x (0<x<1) compounds, focusing especially\non permanent magnet and giant magnetocaloric effect applications. It is found\nthat 15 compounds are potential permanent mag-nets with a magneto-crystalline\nanisotropy energy more than 1 MJ\/m3, filling in the gap of appli-cation\nspectrum between high-performance and widely used permanents. Among the\npotential permanent magnets, Fe2N can be classified as a hard magnet while the\nother 14 compounds can be classified as semi-hard magnets. According to the\ncalculations of magnetic deformation proxy, 40 compounds are identified as\npotential giant magnetocaloric effect candidates. We suspect that Fe-N\ncompounds provide fine opportunities for applications in both rare-earth free\npermanent magnets and magnetocaloric effect.",
        "Quasi-periodic eruptions (QPEs) are a recently identified class of X-ray\ntransient associated with tidal disruption events by supermassive black holes,\nand for which there are multiple possible explanations. In this paper we\npresent a simple model which requires the black hole be spinning, be misaligned\nwith the accretion flow (both conditions of which are almost certainly met) and\nthat the accretion rate is a few times the Eddington limit. We speculate that\nthe resulting Lense-Thirring torques force the disc and entrained outflows to\nprecess, leading to increased X-ray flux when the wind-cone is oriented at\nlower inclinations to the observer. We test the range of parameters for which\nthis model could explain the period and brightness of the QPE events discovered\nthus far, and make qualitative comparisons between the observed X-ray spectra\nand lightcurves to those extracted from GR-RMHD simulations. Overall, we find\nsome areas of promising concordance, and identify challenges related to the\ndetails of current simulations.",
        "We discover a new, non-radial example of a manifold whose heat kernel\ndecreases monotonically along all minimal geodesics. We also classify the flat\ntori with this monotonicity property. Furthermore, we show that for a generic\nmetric on any smooth manifold the monotonicity property fails at large times.\nThis answers a recent question of Alonso-Or\\'an, Chamizo, Mart\\'inez, and Mas.",
        "In this paper, we propose several mathematical models for 3D surface\nreconstruction and volume estimation from a set of scattered cloud data. Three\nmeshless methods including the interpolation-based method by RBF, PDE-based\napproach by Kansa's method and the Method of Fundamental Solutions are employed\nand compared. For the optimal recovery of the surfaces, the selection of free\nparameters in related PDE models are further studied and analyzed. Besides,\nseveral criteria like distance are employed in above methods instead of the\nclassical parameter lambda determination strategy, which leads to a more\nreliable reconstruction performance. Finally, the volume estimation of 3D\nirregular objects is proposed based on the optimal reconstructed geometric\nmodels via proposed meshless methods. Numerous numerical examples are presented\nto demonstrate the effectiveness of the proposed surface reconstruction methods\nand the volume estimation strategy.",
        "As the global economic environment becomes increasingly unstable, enhancing\nfinancial flexibility to cope with risks has become the consensus of many\ncompanies. At the same time, environmental, social, and governance (ESG)\nperformance may be one of the effective ways. We studied the impact of a firm's\nESG performance on its financial flexibility with a sample of companies listed\non the Hong Kong stock market from 2018 to 2022. The empirical results show\nthat good environmental, social and governance performance can significantly\nimprove a firm's financial flexibility. In addition, this paper also finds that\nthe influence of ESG performance on financial flexibility is weak for\nstate-owned enterprises due to the influence of governance structure and market\ncharacteristics. Finally, the further analysis shows that there is a mediating\nrole played by financing constraints in this process. This study can provide\nbackground information for state-owned enterprises' governance, information\ndisclosure, and corporate operations. It also has guiding significance for\nrelevant investors, management and officials.",
        "Classifiers trained solely on labeled source data may yield misleading\nresults when applied to unlabeled target data drawn from a different\ndistribution. Transfer learning can rectify this by transferring knowledge from\nsource to target data, but its effectiveness frequently relies on stringent\nassumptions, such as label shift. In this paper, we introduce a novel General\nConditional Shift (GCS) assumption, which encompasses label shift as a special\nscenario. Under GCS, we demonstrate that both the target distribution and the\nshift function are identifiable. To estimate the conditional probabilities\n${\\bm\\eta}_P$ for source data, we propose leveraging deep neural networks\n(DNNs). Subsequent to transferring the DNN estimator, we estimate the target\nlabel distribution ${\\bm\\pi}_Q$ utilizing a pseudo-maximum likelihood approach.\nUltimately, by incorporating these estimates and circumventing the need to\nestimate the shift function, we construct our proposed Bayes classifier. We\nestablish concentration bounds for our estimators of both ${\\bm\\eta}_P$ and\n${\\bm\\pi}_Q$ in terms of the intrinsic dimension of ${\\bm\\eta}_P$ . Notably,\nour DNN-based classifier achieves the optimal minimax rate, up to a logarithmic\nfactor. A key advantage of our method is its capacity to effectively combat the\ncurse of dimensionality when ${\\bm\\eta}_P$ exhibits a low-dimensional\nstructure. Numerical simulations, along with an analysis of an Alzheimer's\ndisease dataset, underscore its exceptional performance.",
        "Epidemic models are invaluable tools to understand and implement strategies\nto control the spread of infectious diseases, as well as to inform public\nhealth policies and resource allocation. However, current modeling approaches\nhave limitations that reduce their practical utility, such as the exclusion of\nhuman behavioral change in response to the epidemic or ignoring the presence of\nundetected infectious individuals in the population. These limitations became\nparticularly evident during the COVID-19 pandemic, underscoring the need for\nmore accurate and informative models. Motivated by these challenges, we develop\na novel Bayesian epidemic modeling framework to better capture the complexities\nof disease spread by incorporating behavioral responses and undetected\ninfections. In particular, our framework makes three contributions: 1)\nleveraging additional data on hospitalizations and deaths in modeling the\ndisease dynamics, 2) accounting data uncertainty arising from the large\npresence of asymptomatic and undetected infections, and 3) allowing the\npopulation behavioral change to be dynamically influenced by multiple data\nsources (cases and deaths). We thoroughly investigate the properties of the\nproposed model via simulation, and illustrate its utility on COVID-19 data from\nMontreal and Miami.",
        "In recent work on time-series prediction, Transformers and even large\nlanguage models have garnered significant attention due to their strong\ncapabilities in sequence modeling. However, in practical deployments,\ntime-series prediction often requires operation in resource-constrained\nenvironments, such as edge devices, which are unable to handle the\ncomputational overhead of large models. To address such scenarios, some\nlightweight models have been proposed, but they exhibit poor performance on\nnon-stationary sequences. In this paper, we propose $\\textit{SWIFT}$, a\nlightweight model that is not only powerful, but also efficient in deployment\nand inference for Long-term Time Series Forecasting (LTSF). Our model is based\non three key points: (i) Utilizing wavelet transform to perform lossless\ndownsampling of time series. (ii) Achieving cross-band information fusion with\na learnable filter. (iii) Using only one shared linear layer or one shallow MLP\nfor sub-series' mapping. We conduct comprehensive experiments, and the results\nshow that $\\textit{SWIFT}$ achieves state-of-the-art (SOTA) performance on\nmultiple datasets, offering a promising method for edge computing and\ndeployment in this task. Moreover, it is noteworthy that the number of\nparameters in $\\textit{SWIFT-Linear}$ is only 25\\% of what it would be with a\nsingle-layer linear model for time-domain prediction. Our code is available at\nhttps:\/\/github.com\/LancelotXWX\/SWIFT."
      ]
    }
  },
  {
    "id":2411.06513,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"The future of digital health with federated learning",
    "start_abstract":"Abstract Data-driven machine learning (ML) has emerged as a promising approach for building accurate and robust statistical models from medical data, which is collected in huge volumes by modern healthcare systems. Existing data not fully exploited ML primarily because it sits silos privacy concerns restrict access to this data. However, without sufficient will be prevented reaching its full potential and, ultimately, making the transition research clinical practice. This paper considers key factors contributing issue, explores how federated (FL) may provide solution future of digital health highlights challenges considerations that need addressed.",
    "start_categories":[
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Advances and Open Problems in Federated Learning"
      ],
      "abstract":[
        "The term Federated Learning was coined as recently 2016 to describe a machine learning setting where multiple entities collaborate in solving problem, under the coordination of central server or service provider. Each client\u2019s raw data is stored locally and not exchanged transferred; instead, focused updates intended for immediate aggregation are used achieve objective. Since then, topic has gathered much interest across many different disciplines realization that these interdisciplinary problems likely requires just but techniques from distributed optimization, cryptography, security, differential privacy, fairness, compressed sensing, systems, information theory, statistics, more. This monograph contributions leading experts disciplines, who latest state-of-the art their perspective. These have been carefully curated into comprehensive treatment enables reader understand work done get pointers effort required solve before can become reality practical systems. Researchers working area systems will find this an enlightening read may inspire them on challenging issues outlined. up speed quickly easily what increasingly important topic: Learning."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "Credit Risk Identification in Supply Chains Using Generative Adversarial\n  Networks",
        "Sparse Binary Representation Learning for Knowledge Tracing",
        "Safety Representations for Safer Policy Learning",
        "Distributionally Robust Federated Learning: An ADMM Algorithm",
        "Enhancing Pre-Trained Decision Transformers with Prompt-Tuning Bandits",
        "The Role, Trends, and Applications of Machine Learning in Undersea\n  Communication: A Bangladesh Perspective",
        "Unifying Prediction and Explanation in Time-Series Transformers via\n  Shapley-based Pretraining",
        "Task Vector Quantization for Memory-Efficient Model Merging",
        "UPL: Uncertainty-aware Pseudo-labeling for Imbalance Transductive Node\n  Classification",
        "PiKE: Adaptive Data Mixing for Multi-Task Learning Under Low Gradient\n  Conflicts",
        "COSMOS: Continuous Simplicial Neural Networks",
        "Structure based SAT dataset for analysing GNN generalisation",
        "Structural Entropy Guided Unsupervised Graph Out-Of-Distribution\n  Detection",
        "Metering Error Estimation of Fast-Charging Stations Using Charging Data\n  Analytics",
        "Discovering Directly-Follows Graph Model for Acyclic Processes",
        "The Impact of Artificial Intelligence on Emergency Medicine: A Review of\n  Recent Advances",
        "LlaMADRS: Prompting Large Language Models for Interview-Based Depression\n  Assessment",
        "Physical Layer Design for Ambient IoT",
        "Nice and precise $K^*(892) \\to K\\pi$ branching fractions",
        "S$^2$-MAD: Breaking the Token Barrier to Enhance Multi-Agent Debate\n  Efficiency",
        "A plethora of long-range neutrino interactions probed by DUNE and T2HK",
        "SPRI: Aligning Large Language Models with Context-Situated Principles",
        "Discrete Markov Probabilistic Models",
        "Estimation of the generalized Laplace distribution and its projection\n  onto the circle",
        "Efficient Parallel Scheduling for Sparse Triangular Solvers",
        "Electron spin dynamics guide cell motility",
        "Towards More Trustworthy Deep Code Models by Enabling\n  Out-of-Distribution Detection",
        "Real-Time LiDAR Point Cloud Compression and Transmission for\n  Resource-constrained Robots"
      ],
      "abstract":[
        "Credit risk management within supply chains has emerged as a critical\nresearch area due to its significant implications for operational stability and\nfinancial sustainability. The intricate interdependencies among supply chain\nparticipants mean that credit risks can propagate across networks, with impacts\nvarying by industry. This study explores the application of Generative\nAdversarial Networks (GANs) to enhance credit risk identification in supply\nchains. GANs enable the generation of synthetic credit risk scenarios,\naddressing challenges related to data scarcity and imbalanced datasets. By\nleveraging GAN-generated data, the model improves predictive accuracy while\neffectively capturing dynamic and temporal dependencies in supply chain data.\nThe research focuses on three representative industries-manufacturing (steel),\ndistribution (pharmaceuticals), and services (e-commerce) to assess\nindustry-specific credit risk contagion. Experimental results demonstrate that\nthe GAN-based model outperforms traditional methods, including logistic\nregression, decision trees, and neural networks, achieving superior accuracy,\nrecall, and F1 scores. The findings underscore the potential of GANs in\nproactive risk management, offering robust tools for mitigating financial\ndisruptions in supply chains. Future research could expand the model by\nincorporating external market factors and supplier relationships to further\nenhance predictive capabilities. Keywords- Generative Adversarial Networks\n(GANs); Supply Chain Risk; Credit Risk Identification; Machine Learning; Data\nAugmentation",
        "Knowledge tracing (KT) models aim to predict students' future performance\nbased on their historical interactions. Most existing KT models rely\nexclusively on human-defined knowledge concepts (KCs) associated with\nexercises. As a result, the effectiveness of these models is highly dependent\non the quality and completeness of the predefined KCs. Human errors in labeling\nand the cost of covering all potential underlying KCs can limit model\nperformance.\n  In this paper, we propose a KT model, Sparse Binary Representation KT\n(SBRKT), that generates new KC labels, referred to as auxiliary KCs, which can\naugment the predefined KCs to address the limitations of relying solely on\nhuman-defined KCs. These are learned through a binary vector representation,\nwhere each bit indicates the presence (one) or absence (zero) of an auxiliary\nKC. The resulting discrete representation allows these auxiliary KCs to be\nutilized in training any KT model that incorporates KCs. Unlike pre-trained\ndense embeddings, which are limited to models designed to accept such vectors,\nour discrete representations are compatible with both classical models, such as\nBayesian Knowledge Tracing (BKT), and modern deep learning approaches.\n  To generate this discrete representation, SBRKT employs a binarization method\nthat learns a sparse representation, fully trainable via stochastic gradient\ndescent. Additionally, SBRKT incorporates a recurrent neural network (RNN) to\ncapture temporal dynamics and predict future student responses by effectively\ncombining the auxiliary and predefined KCs. Experimental results demonstrate\nthat SBRKT outperforms the tested baselines on several datasets and achieves\ncompetitive performance on others. Furthermore, incorporating the learned\nauxiliary KCs consistently enhances the performance of BKT across all tested\ndatasets.",
        "Reinforcement learning algorithms typically necessitate extensive exploration\nof the state space to find optimal policies. However, in safety-critical\napplications, the risks associated with such exploration can lead to\ncatastrophic consequences. Existing safe exploration methods attempt to\nmitigate this by imposing constraints, which often result in overly\nconservative behaviours and inefficient learning. Heavy penalties for early\nconstraint violations can trap agents in local optima, deterring exploration of\nrisky yet high-reward regions of the state space. To address this, we introduce\na method that explicitly learns state-conditioned safety representations. By\naugmenting the state features with these safety representations, our approach\nnaturally encourages safer exploration without being excessively cautious,\nresulting in more efficient and safer policy learning in safety-critical\nscenarios. Empirical evaluations across diverse environments show that our\nmethod significantly improves task performance while reducing constraint\nviolations during training, underscoring its effectiveness in balancing\nexploration with safety.",
        "Federated learning (FL) aims to train machine learning (ML) models\ncollaboratively using decentralized data, bypassing the need for centralized\ndata aggregation. Standard FL models often assume that all data come from the\nsame unknown distribution. However, in practical situations, decentralized data\nfrequently exhibit heterogeneity. We propose a novel FL model, Distributionally\nRobust Federated Learning (DRFL), that applies distributionally robust\noptimization to overcome the challenges posed by data heterogeneity and\ndistributional ambiguity. We derive a tractable reformulation for DRFL and\ndevelop a novel solution method based on the alternating direction method of\nmultipliers (ADMM) algorithm to solve this problem. Our experimental results\ndemonstrate that DRFL outperforms standard FL models under data heterogeneity\nand ambiguity.",
        "Harnessing large offline datasets is vital for training foundation models\nthat can generalize across diverse tasks. Offline Reinforcement Learning (RL)\noffers a powerful framework for these scenarios, enabling the derivation of\noptimal policies even from suboptimal data. The Prompting Decision Transformer\n(PDT) is an offline RL multi-task model that distinguishes tasks through\nstochastic trajectory prompts, which are task-specific tokens maintained in\ncontext during rollouts. However, PDT samples these tokens uniformly at random\nfrom per-task demonstration datasets, failing to account for differences in\ntoken informativeness and potentially leading to performance degradation. To\naddress this limitation, we introduce a scalable bandit-based prompt-tuning\nmethod that dynamically learns to construct high-performance trajectory\nprompts. Our approach significantly enhances downstream task performance\nwithout modifying the pre-trained Transformer backbone. Empirical results on\nbenchmark tasks and a newly designed multi-task environment demonstrate the\neffectiveness of our method, creating a seamless bridge between general\nmulti-task offline pre-training and task-specific online adaptation.",
        "The rapid evolution of machine learning (ML) has brought about groundbreaking\ndevelopments in numerous industries, not the least of which is in the area of\nundersea communication. This domain is critical for applications like ocean\nexploration, environmental monitoring, resource management, and national\nsecurity. Bangladesh, a maritime nation with abundant resources in the Bay of\nBengal, can harness the immense potential of ML to tackle the unprecedented\nchallenges associated with underwater communication. Beyond that, environmental\nconditions are unique to the region: in addition to signal attenuation,\nmultipath propagation, noise interference, and limited bandwidth. In this\nstudy, we address the necessity to bring ML into communication via undersea; it\ninvestigates the latest technologies under the domain of ML in that respect,\nsuch as deep learning and reinforcement learning, especially concentrating on\nBangladesh scenarios in the sense of implementation. This paper offers a\ncontextualized regional perspective by incorporating region-specific needs,\ncase studies, and recent research to propose a roadmap for deploying ML-driven\nsolutions to improve safety at sea, promote sustainable resource use, and\nenhance disaster response systems. This research ultimately highlights the\npromise of ML-powered solutions for transforming undersea communication,\nleading to more efficient and cost-effective technologies that subsequently\ncontribute to both economic growth and environmental sustainability.",
        "In this paper, we propose ShapTST, a framework that enables time-series\ntransformers to efficiently generate Shapley-value-based explanations alongside\npredictions in a single forward pass. Shapley values are widely used to\nevaluate the contribution of different time-steps and features in a test\nsample, and are commonly generated through repeatedly inferring on each sample\nwith different parts of information removed. Therefore, it requires expensive\ninference-time computations that occur at every request for model explanations.\nIn contrast, our framework unifies the explanation and prediction in training\nthrough a novel Shapley-based pre-training design, which eliminates the\nundesirable test-time computation and replaces it with a single-time\npre-training. Moreover, this specialized pre-training benefits the prediction\nperformance by making the transformer model more effectively weigh different\nfeatures and time-steps in the time-series, particularly improving the\nrobustness against data noise that is common to raw time-series data. We\nexperimentally validated our approach on eight public datasets, where our\ntime-series model achieved competitive results in both classification and\nregression tasks, while providing Shapley-based explanations similar to those\nobtained with post-hoc computation. Our work offers an efficient and\nexplainable solution for time-series analysis tasks in the safety-critical\napplications.",
        "Model merging enables efficient multi-task models by combining task-specific\nfine-tuned checkpoints. However, storing multiple task-specific checkpoints\nrequires significant memory, limiting scalability and restricting model merging\nto larger models and diverse tasks. In this paper, we propose quantizing task\nvectors (i.e., the difference between pre-trained and fine-tuned checkpoints)\ninstead of quantizing fine-tuned checkpoints. We observe that task vectors\nexhibit a narrow weight range, enabling low precision quantization (up to 4\nbit) within existing task vector merging frameworks. To further mitigate\nquantization errors within ultra-low bit precision (e.g., 2 bit), we introduce\nResidual Task Vector Quantization, which decomposes the task vector into a base\nvector and offset component. We allocate bits based on quantization\nsensitivity, ensuring precision while minimizing error within a memory budget.\nExperiments on image classification and dense prediction show our method\nmaintains or improves model merging performance while using only 8% of the\nmemory required for full-precision checkpoints.",
        "Graph-structured datasets often suffer from class imbalance, which\ncomplicates node classification tasks. In this work, we address this issue by\nfirst providing an upper bound on population risk for imbalanced transductive\nnode classification. We then propose a simple and novel algorithm,\nUncertainty-aware Pseudo-labeling (UPL). Our approach leverages pseudo-labels\nassigned to unlabeled nodes to mitigate the adverse effects of imbalance on\nclassification accuracy. Furthermore, the UPL algorithm enhances the accuracy\nof pseudo-labeling by reducing training noise of pseudo-labels through a novel\nuncertainty-aware approach. We comprehensively evaluate the UPL algorithm\nacross various benchmark datasets, demonstrating its superior performance\ncompared to existing state-of-the-art methods.",
        "Modern machine learning models are trained on diverse datasets and tasks to\nimprove generalization. A key challenge in multitask learning is determining\nthe optimal data mixing and sampling strategy across different data sources.\nPrior research in this multi-task learning setting has primarily focused on\nmitigating gradient conflicts between tasks. However, we observe that many\nreal-world multitask learning scenarios-such as multilingual training and\nmulti-domain learning in large foundation models-exhibit predominantly positive\ntask interactions with minimal or no gradient conflict. Building on this\ninsight, we introduce PiKE (Positive gradient interaction-based K-task weights\nEstimator), an adaptive data mixing algorithm that dynamically adjusts task\ncontributions throughout training. PiKE optimizes task sampling to minimize\noverall loss, effectively leveraging positive gradient interactions with almost\nno additional computational overhead. We establish theoretical convergence\nguarantees for PiKE and demonstrate its superiority over static and\nnon-adaptive mixing strategies. Additionally, we extend PiKE to promote fair\nlearning across tasks, ensuring balanced progress and preventing task\nunderrepresentation. Empirical evaluations on large-scale language model\npretraining show that PiKE consistently outperforms existing heuristic and\nstatic mixing strategies, leading to faster convergence and improved downstream\ntask performance.",
        "Simplicial complexes provide a powerful framework for modeling high-order\ninteractions in structured data, making them particularly suitable for\napplications such as trajectory prediction and mesh processing. However,\nexisting simplicial neural networks (SNNs), whether convolutional or\nattention-based, rely primarily on discrete filtering techniques, which can be\nrestrictive. In contrast, partial differential equations (PDEs) on simplicial\ncomplexes offer a principled approach to capture continuous dynamics in such\nstructures. In this work, we introduce COntinuous SiMplicial neural netwOrkS\n(COSMOS), a novel SNN architecture derived from PDEs on simplicial complexes.\nWe provide theoretical and experimental justifications of COSMOS's stability\nunder simplicial perturbations. Furthermore, we investigate the over-smoothing\nphenomenon, a common issue in geometric deep learning, demonstrating that\nCOSMOS offers better control over this effect than discrete SNNs. Our\nexperiments on real-world datasets of ocean trajectory prediction and\nregression on partial deformable shapes demonstrate that COSMOS achieves\ncompetitive performance compared to state-of-the-art SNNs in complex and noisy\nenvironments.",
        "Satisfiability (SAT) solvers based on techniques such as conflict driven\nclause learning (CDCL) have produced excellent performance on both synthetic\nand real world industrial problems. While these CDCL solvers only operate on a\nper-problem basis, graph neural network (GNN) based solvers bring new benefits\nto the field by allowing practitioners to exploit knowledge gained from solved\nproblems to expedite solving of new SAT problems. However, one specific area\nthat is often studied in the context of CDCL solvers, but largely overlooked in\nGNN solvers, is the relationship between graph theoretic measure of structure\nin SAT problems and the generalisation ability of GNN solvers. To bridge the\ngap between structural graph properties (e.g., modularity, self-similarity) and\nthe generalisability (or lack thereof) of GNN based SAT solvers, we present\nStructureSAT: a curated dataset, along with code to further generate novel\nexamples, containing a diverse set of SAT problems from well known problem\ndomains. Furthermore, we utilise a novel splitting method that focuses on\ndeconstructing the families into more detailed hierarchies based on their\nstructural properties. With the new dataset, we aim to help explain problematic\ngeneralisation in existing GNN SAT solvers by exploiting knowledge of\nstructural graph properties. We conclude with multiple future directions that\ncan help researchers in GNN based SAT solving develop more effective and\ngeneralisable SAT solvers.",
        "With the emerging of huge amount of unlabeled data, unsupervised\nout-of-distribution (OOD) detection is vital for ensuring the reliability of\ngraph neural networks (GNNs) by identifying OOD samples from in-distribution\n(ID) ones during testing, where encountering novel or unknown data is\ninevitable. Existing methods often suffer from compromised performance due to\nredundant information in graph structures, which impairs their ability to\neffectively differentiate between ID and OOD data. To address this challenge,\nwe propose SEGO, an unsupervised framework that integrates structural entropy\ninto OOD detection regarding graph classification. Specifically, within the\narchitecture of contrastive learning, SEGO introduces an anchor view in the\nform of coding tree by minimizing structural entropy. The obtained coding tree\neffectively removes redundant information from graphs while preserving\nessential structural information, enabling the capture of distinct graph\npatterns between ID and OOD samples. Furthermore, we present a multi-grained\ncontrastive learning scheme at local, global, and tree levels using triplet\nviews, where coding trees with essential information serve as the anchor view.\nExtensive experiments on real-world datasets validate the effectiveness of\nSEGO, demonstrating superior performance over state-of-the-art baselines in OOD\ndetection. Specifically, our method achieves the best performance on 9 out of\n10 dataset pairs, with an average improvement of 3.7\\% on OOD detection\ndatasets, significantly surpassing the best competitor by 10.8\\% on the\nFreeSolv\/ToxCast dataset pair.",
        "Accurate electric energy metering (EEM) of fast charging stations (FCSs),\nserving as critical infrastructure in the electric vehicle (EV) industry and as\nsignificant carriers of vehicle-to-grid (V2G) technology, is the cornerstone\nfor ensuring fair electric energy transactions. Traditional on-site\nverification methods, constrained by their high costs and low efficiency,\nstruggle to keep pace with the rapid global expansion of FCSs. In response,\nthis paper adopts a data-driven approach and proposes the measuring performance\ncomparison (MPC) method. By utilizing the estimation value of state-of-charge\n(SOC) as a medium, MPC establishes comparison chains of EEM performance of\nmultiple FCSs. Therefore, the estimation of EEM errors for FCSs with high\nefficiency is enabled. Moreover, this paper summarizes the interfering factors\nof estimation results and establishes corresponding error models and\nuncertainty models. Also, a method for discriminating whether there are EEM\nperformance defects in FCSs is proposed. Finally, the feasibility of MPC method\nis validated, with results indicating that for FCSs with an accuracy grade of\n2\\%, the discriminative accuracy exceeds 95\\%. The MPC provides a viable\napproach for the online monitoring of EEM performance for FCSs, laying a\nfoundation for a fair and just electricity trading market.",
        "Process mining is the common name for a range of methods and approaches aimed\nat analysing and improving processes. Specifically, methods that aim to derive\nprocess models from event logs fall under the category of process discovery.\nWithin the range of processes, acyclic processes form a distinct category. In\nsuch processes, previously performed actions are not repeated, forming chains\nof unique actions. However, due to differences in the order of actions,\nexisting process discovery methods can provide models containing cycles even if\na process is acyclic. This paper presents a new process discovery algorithm\nthat allows to discover acyclic DFG models for acyclic processes. A model is\ndiscovered by partitioning an event log into parts that provide acyclic DFG\nmodels and merging them while avoiding the formation of cycles. The resulting\nalgorithm was tested both on real-life and artificial event logs. Absence of\ncycles improves model visual clarity and precision, also allowing to apply\ncycle-sensitive methods or visualisations to the model.",
        "Artificial Intelligence (AI) is revolutionizing emergency medicine by\nenhancing diagnostic processes and improving patient outcomes. This article\nprovides a review of the current applications of AI in emergency imaging\nstudies, focusing on the last five years of advancements. AI technologies,\nparticularly machine learning and deep learning, are pivotal in interpreting\ncomplex imaging data, offering rapid, accurate diagnoses and potentially\nsurpassing traditional diagnostic methods. Studies highlighted within the\narticle demonstrate AI's capabilities in accurately detecting conditions such\nas fractures, pneumothorax, and pulmonary diseases from various imaging\nmodalities including X-rays, CT scans, and MRIs. Furthermore, AI's ability to\npredict clinical outcomes like mechanical ventilation needs illustrates its\npotential in crisis resource optimization. Despite these advancements, the\nintegration of AI into clinical practice presents challenges such as data\nprivacy, algorithmic bias, and the need for extensive validation across diverse\nsettings. This review underscores the transformative potential of AI in\nemergency settings, advocating for a future where AI and clinical expertise\nsynergize to elevate patient care standards.",
        "This study introduces LlaMADRS, a novel framework leveraging open-source\nLarge Language Models (LLMs) to automate depression severity assessment using\nthe Montgomery-Asberg Depression Rating Scale (MADRS). We employ a zero-shot\nprompting strategy with carefully designed cues to guide the model in\ninterpreting and scoring transcribed clinical interviews. Our approach, tested\non 236 real-world interviews from the Context-Adaptive Multimodal Informatics\n(CAMI) dataset, demonstrates strong correlations with clinician assessments.\nThe Qwen 2.5--72b model achieves near-human level agreement across most MADRS\nitems, with Intraclass Correlation Coefficients (ICC) closely approaching those\nbetween human raters. We provide a comprehensive analysis of model performance\nacross different MADRS items, highlighting strengths and current limitations.\nOur findings suggest that LLMs, with appropriate prompting, can serve as\nefficient tools for mental health assessment, potentially increasing\naccessibility in resource-limited settings. However, challenges remain,\nparticularly in assessing symptoms that rely on non-verbal cues, underscoring\nthe need for multimodal approaches in future work.",
        "There is a growing demand for ultra low power and ultra low complexity\ndevices for applications which require maintenance-free and battery-less\noperation. One way to serve such applications is through backscatter devices,\nwhich communicate using energy harvested from ambient sources such as radio\nwaves transmitted by a reader. Traditional backscatter devices, such as RFID,\nare limited by range, interference, low connection density, and security\nissues. To address these problems, the Third Generation Partnership Project\n(3GPP) has started working on Ambient IoT (A-IoT). For the realization of A-IoT\ndevices, various aspects ranging from physical layer design, to the protocol\nstack, to the device architecture should be standardized. In this paper, we\nprovide an overview of the standardization efforts on the physical layer design\nfor A-IoT devices. The various physical channels and signals are discussed,\nfollowed by link level simulations to compare the performance of various\nconfigurations of reader to device and device to reader channels.",
        "Although discovered more than sixty years ago, direct measurement of the\n$K^*(892) \\to K\\pi$ branching fractions is a formidable challenge that has not\nbeen attempted. Typically they are assumed to obey the isospin limit in\nhundreds of particle data measurements. We show that an abundance of recent\namplitude analyses and other data, however, enables recovery of the ratios\n$\\mathcal{B}(K^{*+} \\to K^+ \\pi^0)\/\\mathcal{B}(K^{*+} \\to K_S^0 \\pi^+)$ and\n$4\\mathcal{B}(K^{*0} \\to K_S^0 \\pi^0)\/\\mathcal{B}(K^{*0} \\to K^+ \\pi^-)$ at\n$\\sim 5\\%$ precision.",
        "Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious natural language processing (NLP) scenarios, but they still face\nchallenges when handling complex arithmetic and logical reasoning tasks. While\nChain-Of-Thought (CoT) reasoning, self-consistency (SC) and self-correction\nstrategies have attempted to guide models in sequential, multi-step reasoning,\nMulti-agent Debate (MAD) has emerged as a viable approach for enhancing the\nreasoning capabilities of LLMs. By increasing both the number of agents and the\nfrequency of debates, the performance of LLMs improves significantly. However,\nthis strategy results in a significant increase in token costs, presenting a\nbarrier to scalability. To address this challenge, we introduce a novel\nsparsification strategy designed to reduce token costs within MAD. This\napproach minimizes ineffective exchanges of information and unproductive\ndiscussions among agents, thereby enhancing the overall efficiency of the\ndebate process. We conduct comparative experiments on multiple datasets across\nvarious models, demonstrating that our approach significantly reduces the token\ncosts in MAD to a considerable extent. Specifically, compared to MAD, our\napproach achieves an impressive reduction of up to 94.5\\% in token costs while\nmaintaining performance degradation below 2.0\\%.",
        "The next-generation neutrino oscillation experiments would be sensitive to\nthe new neutrino interactions that would strengthen the search for physics\nbeyond the Standard Model. In this context, we explore the capabilities of the\ntwo leading future long-baseline neutrino oscillation experiments, DUNE and\nT2HK, to search for new flavor-dependent neutrino interactions with electrons,\nprotons, and neutrons that could potentially modify neutrino flavor\ntransitions. We forecast their sensitivities in the context of long-range\nneutrino interactions mediated by a neutral vector boson lighter than\n$10^{-10}$ eV and sourced by the vast amount of nearby and distant matter in\nthe Earth, Moon, Sun, Milky Way, and local Universe. For the first time, we\nexplore a plethora of $U(1)^\\prime$ symmetries inducing the new interactions\nbuilt from the combination of lepton and baryon numbers. We find that in all\ncases, DUNE and T2HK may constrain or discover the existence of new long-range\nneutrino interaction, and in some favorable cases, may identify the new\n$U(1)^\\prime$ symmetry responsible for it. In this short proceeding, we only\nsummarize the prospects of constraining the new interaction in case of all our\ncandidate $U(1)^\\prime$ symmetries, which have been discussed in JHEP 09 (2024)\n055.",
        "Aligning Large Language Models to integrate and reflect human values,\nespecially for tasks that demand intricate human oversight, is arduous since it\nis resource-intensive and time-consuming to depend on human expertise for\ncontext-specific guidance. Prior work has utilized predefined sets of rules or\nprinciples to steer the behavior of models (Bai et al., 2022; Sun et al.,\n2023). However, these principles tend to be generic, making it challenging to\nadapt them to each individual input query or context. In this work, we present\nSituated-PRInciples (SPRI), a framework requiring minimal or no human effort\nthat is designed to automatically generate guiding principles in real-time for\neach input query and utilize them to align each response. We evaluate SPRI on\nthree tasks, and show that 1) SPRI can derive principles in a complex\ndomain-specific task that leads to on-par performance as expert-crafted ones;\n2) SPRI-generated principles lead to instance-specific rubrics that outperform\nprior LLM-as-a-judge frameworks; 3) using SPRI to generate synthetic SFT data\nleads to substantial improvement on truthfulness. We release our code and model\ngenerations at https:\/\/github.com\/honglizhan\/SPRI-public.",
        "This paper introduces the Discrete Markov Probabilistic Model (DMPM), a novel\nalgorithm for discrete data generation. The algorithm operates in the space of\nbits $\\{0,1\\}^d$, where the noising process is a continuous-time Markov chain\nthat can be sampled exactly via a Poissonian clock that flips labels uniformly\nat random. The time-reversal process, like the forward noise process, is a jump\nprocess, with its intensity governed by a discrete analogue of the classical\nscore function. Crucially, this intensity is proven to be the conditional\nexpectation of a function of the forward process, strengthening its theoretical\nalignment with score-based generative models while ensuring robustness and\nefficiency. We further establish convergence bounds for the algorithm under\nminimal assumptions and demonstrate its effectiveness through experiments on\nlow-dimensional Bernoulli-distributed datasets and high-dimensional binary\nMNIST data. The results highlight its strong performance in generating discrete\nstructures. This work bridges theoretical foundations and practical\napplications, advancing the development of effective and theoretically grounded\ndiscrete generative modeling.",
        "The generalized Laplace (GL) distribution, which falls in the larger family\nof generalized hyperbolic distributions, provides a versatile model to deal\nwith a variety of applications thanks to its shape parameters. The elliptically\nsymmetric GL admits a polar representation that can be used to yield a circular\ndistribution, which we call \\emph{projected} GL distribution. The latter does\nnot appear to have been considered yet in practical applications. In this\narticle, we explore an easy-to-implement maximum likelihood estimation strategy\nbased on Gaussian quadrature for the scale-mixture representation of the GL and\nits projection onto the circle. A simulation study is carried out to benchmark\nthe fitting routine against alternative estimation methods to assess its\nfeasibility, while the projected GL model is contrasted with other popular\ncircular distributions.",
        "We develop and analyze new scheduling algorithms for solving sparse\ntriangular linear systems (SpTRSV) in parallel. Our approach, which we call\nbarrier list scheduling, produces highly efficient synchronous schedules for\nthe forward- and backward-substitution algorithm. Compared to state-of-the-art\nbaselines HDagg and SpMP, we achieve a $3.24\\times$ and $1.45\\times$\ngeometric-mean speed-up, respectively. We achieve this by obtaining an up to\n$11\\times$ geometric-mean reduction in the number of synchronization barriers\nover HDagg, whilst maintaining a balanced workload, and by applying a matrix\nreordering step for locality. We show that our improvements are consistent\nacross a variety of input matrices and hardware architectures.",
        "Diverse organisms exploit the geomagnetic field (GMF) for migration.\nMigrating birds employ an intrinsically quantum mechanical mechanism for\ndetecting the geomagnetic field: absorption of a blue photon generates a\nradical pair whose two electrons precess at different rates in the magnetic\nfield, thereby sensitizing cells to the direction of the GMF. In this work,\nusing an in vitro injury model, we discovered a quantum-based mechanism of\ncellular migration. Specifically, we show that migrating cells detect the GMF\nvia an optically activated, electron spin-based mechanism. Cell injury provokes\nacute emission of blue photons, and these photons sensitize muscle progenitor\ncells to the magnetic field. We show that the magnetosensitivity of muscle\nprogenitor cells is (a) activated by blue light, but not by green or red light,\nand (b) disrupted by the application of an oscillatory field at the frequency\ncorresponding to the energy of the electron-spin\/magnetic field interaction. A\ncomprehensive analysis of protein expression reveals that the ability of blue\nphotons to promote cell motility is mediated by activation of calmodulin\ncalcium sensors. Collectively, these data suggest that cells possess a\nlight-dependent magnetic compass driven by electron spin dynamics.",
        "Numerous machine learning (ML) models have been developed, including those\nfor software engineering (SE) tasks, under the assumption that training and\ntesting data come from the same distribution. However, training and testing\ndistributions often differ, as training datasets rarely encompass the entire\ndistribution, while testing distribution tends to shift over time. Hence, when\nconfronted with out-of-distribution (OOD) instances that differ from the\ntraining data, a reliable and trustworthy SE ML model must be capable of\ndetecting them to either abstain from making predictions, or potentially\nforward these OODs to appropriate models handling other categories or tasks.\n  In this paper, we develop two types of SE-specific OOD detection models,\nunsupervised and weakly-supervised OOD detection for code. The unsupervised OOD\ndetection approach is trained solely on in-distribution samples while the\nweakly-supervised approach utilizes a tiny number of OOD samples to further\nenhance the detection performance in various OOD scenarios. Extensive\nexperimental results demonstrate that our proposed methods significantly\noutperform the baselines in detecting OOD samples from four different scenarios\nsimultaneously and also positively impact a main code understanding task.",
        "LiDARs are widely used in autonomous robots due to their ability to provide\naccurate environment structural information. However, the large size of point\nclouds poses challenges in terms of data storage and transmission. In this\npaper, we propose a novel point cloud compression and transmission framework\nfor resource-constrained robotic applications, called RCPCC. We iteratively fit\nthe surface of point clouds with a similar range value and eliminate redundancy\nthrough their spatial relationships. Then, we use Shape-adaptive DCT (SA-DCT)\nto transform the unfit points and reduce the data volume by quantizing the\ntransformed coefficients. We design an adaptive bitrate control strategy based\non QoE as the optimization goal to control the quality of the transmitted point\ncloud. Experiments show that our framework achieves compression rates of\n40$\\times$ to 80$\\times$ while maintaining high accuracy for downstream\napplications. our method significantly outperforms other baselines in terms of\naccuracy when the compression rate exceeds 70$\\times$. Furthermore, in\nsituations of reduced communication bandwidth, our adaptive bitrate control\nstrategy demonstrates significant QoE improvements. The code will be available\nat https:\/\/github.com\/HITSZ-NRSL\/RCPCC.git."
      ]
    }
  },
  {
    "id":2411.16464,
    "research_type":"basic",
    "start_id":"b2",
    "start_title":"Meeting Strangers and Friends of Friends: How Random Are Social Networks?",
    "start_abstract":"We present a dynamic model of network formation where nodes find other with whom to form links in two ways: some are found uniformly at random, while others by searching locally through the current structure (e.g., meeting friends friends). This combination processes results spectrum features exhibited large social networks, including presence more high- and low-degree than when formed independently having low distances between network, high clustering on local level. fit data from six networks impute relative ratio random network-based meetings link formation, which turns out vary dramatically across applications. show that as random\/network-based varies, resulting degree distributions can be ordered sense stochastic dominance, allows us infer how process affects average utility network. (JEL D85, Z13)",
    "start_categories":[
      "q-fin.EC"
    ],
    "start_fields":[
      "Economics and Quantitative Finance"
    ],
    "target_paper":{
      "id":[
        "b6"
      ],
      "title":[
        "An agent-based spatial urban social network generator: A case study of beijing, china"
      ],
      "abstract":[
        "This paper proposes an agent-based spatial social network model, which combines a utility function and heuristic algorithms, to formulate friendships of agents in a given synthetic population comprising individuals and households, as well as their attributes and locations. In order to better and explicitly represent the real social networks, the model attempts to generate both close and somewhat close social networks by linking agents with either close or somewhat close friendships, fitting both distributions of network degree and transitivity, which are two basic characteristics of a network. Here, a utility function, which incorporates the similarity between agents in individual attributes (e.g., sex), as well as the spatial closeness of their residential locations and workplaces, is developed to judge whether a friendship between a pair of agents can be built. Furthermore, the social network model is developed as a key component of an agent-and Geographic Information System (GIS)-based virtual city creator that is a set of synthesis methods used to generate spatially disaggregate urban data. Finally, Beijing, China is used as a case study. Both close and somewhat close social networks are generated with the target and generated distributions well matched, and the generated networks are further analysed from a geographical perspective."
      ],
      "categories":[
        "cs.CE"
      ]
    },
    "list":{
      "title":[
        "Modular Photobioreactor Fa\\c{c}ade Systems for Sustainable Architecture:\n  Design, Fabrication, and Real-Time Monitoring",
        "DCAMamba: Mamba-based Rapid Response DC Arc Fault Detection",
        "Capturing Lifecycle System Degradation in Digital Twin Model Updating",
        "Data-Driven Discovery of Population Balance Equations for the\n  Particulate Sciences",
        "Applying Computational Engineering Modelling to Analyse the Social\n  Impact of Conflict and Violent Events",
        "Competitive algorithms for calculating the ground state properties of\n  Bose-Fermi mixtures",
        "The computation of average kernel with Gauss-Laguerre quadrature for\n  double integrals",
        "Assessment of ChatGPT for Engineering Statics Analysis",
        "How to introduce an initial crack in phase field simulations to\n  accurately predict the linear elastic fracture propagation threshold?",
        "Towards personalised assessment of abdominal aortic aneurysm structural\n  integrity",
        "Isogeometric Analysis for 2D Magnetostatic Computations with Multi-level\n  B\\'{e}zier Extraction for Local Refinement",
        "Multiphysics Continuous Shape Optimization of the TAP Reactor Components",
        "A Coupled PFEM-DEM Model for Fluid-Granular Flows with Free-Surface\n  Dynamics Applied to Landslides",
        "Ferroelectric Properties of van der Waals Chalcogenides: DFT perspective",
        "Vision Transformers on the Edge: A Comprehensive Survey of Model\n  Compression and Acceleration Strategies",
        "Contact process for the spread of knowledge",
        "Site-Decorated Model for Unconventional Frustrated Magnets: Ultranarrow\n  Phase Crossover and Spin Reversal Transition",
        "Path-dependency and emergent computing under vectorial driving",
        "Enhancing the De-identification of Personally Identifiable Information\n  in Educational Data",
        "Multi-compartment diffusion-relaxation MR signal representation in the\n  spherical 3D-SHORE basis",
        "Advancing ATLAS DCS Data Analysis with a Modern Data Platform",
        "Non-linear Partition of Unity method",
        "Network fault costs based on minimum leaf spanning trees",
        "Study of long-term spectral evolution and X-ray and Gamma-ray\n  correlation of blazars seen by HAWC",
        "Optimized Relay Lens Design For High-Resolution Image Transmission In\n  Military Target Detection Systems",
        "Reinforcement Learning in Strategy-Based and Atari Games: A Review of\n  Google DeepMinds Innovations",
        "Detection of Somali-written Fake News and Toxic Messages on the Social\n  Media Using Transformer-based Language Models",
        "Managing target of opportunity (ToO) observations at Observatorio\n  Astrof\\'isico de Javalambre (OAJ)"
      ],
      "abstract":[
        "This paper proposes an innovative solution to the growing issue of greenhouse\ngas emissions: a closed photobioreactor (PBR) fa\\c{c}ade system to mitigate\ngreenhouse gas (GHG) concentrations. With digital fabrication technology, this\nstudy explores the transition from traditional, single function building\nfacades to multifunctional, integrated building systems. It introduces a\nphotobioreactor (PBR) fa\\c{c}ade system to mitigate greenhouse gas (GHG)\nconcentrations while addressing the challenge of large-scale prefabricated\ncomponents transportation. This research introduces a novel approach by\ndesigning the fa\\c{c}ade system as modular, user-friendly and\ntransportation-friendly bricks, enabling the creation of a user-customized and\nself-assembled photobioreactor (PBR) system. The single module in the system is\nproposed to be \"neutralization bricks\", which embedded with algae and equipped\nwith an air circulation system, facilitating the photobioreactor (PBR)'s\nfunctionality. A connection system between modules allows for easy assembly by\nusers, while a limited variety of brick styles ensures modularity in\nmanufacturing without sacrificing customization and diversity. The system is\nalso equipped with an advanced microalgae status detection algorithm, which\nallows users to monitor the condition of the microalgae using monocular camera.\nThis functionality ensures timely alerts and notifications for users to replace\nthe algae, thereby optimizing the operational efficiency and sustainability of\nthe algae cultivation process.",
        "In electrical equipment, even minor contact issues can lead to arc faults.\nTraditional methods often struggle to balance the accuracy and rapid response\nrequired for effective arc fault detection. To address this challenge, we\nintroduce DCAMamba, a novel framework for arc fault detection. Specifically,\nDCAMamba is built upon a state-space model (SSM) and utilizes a hardware-aware\nparallel algorithm, designed in a cyclic mode using the Mamba architecture. To\nmeet the dual demands of high accuracy and fast response in arc fault\ndetection, we have refined the original Mamba model and incorporated a Feature\nAmplification Strategy (FAS), a simple yet effective method that enhances the\nmodel's ability to interpret arc fault data. Experimental results show that\nDCAMamba, with FAS, achieves a 12$\\%$ improvement in accuracy over the original\nMamba, while maintaining an inference time of only 1.87 milliseconds. These\nresults highlight the significant potential of DCAMamba as a future backbone\nfor signal processing. Our code will be made open-source after peer review.",
        "Digital twin (DT) has emerged as a powerful tool to facilitate monitoring,\ncontrol, and other decision-making tasks in real-world engineering systems.\nOnline update methods have been proposed to update DT models. Considering the\ndegradation behavior in the system lifecycle, these methods fail to enable DT\nmodels to predict the system responses affected by the system degradation over\ntime. To alleviate this problem, degradation models of measurable parameters\nhave been integrated into DT construction. However, identifying the degradation\nparameters relies on prior knowledge of the system and expensive experiments.\nTo mitigate those limitations, this paper proposes a lifelong update method for\nDT models to capture the effects of system degradation on system responses\nwithout any prior knowledge and expensive offline experiments on the system.\nThe core idea in the work is to represent the system degradation during the\nlifecycle as the dynamic changes of DT configurations (i.e., model parameters\nwith a fixed model structure) at all degradation stages. During the lifelong\nupdate process, an Autoencoder is adopted to reconstruct the model parameters\nof all hidden layers simultaneously, so that the latent features taking into\naccount the dependencies among hidden layers are obtained for each degradation\nstage. The dynamic behavior of latent features among successive degradation\nstages is then captured by a long short-term memory model, which enables\nprediction of the latent feature at any unseen stage. Based on the predicted\nlatent features, the model configuration at future degradation stage is\nreconstructed to determine the new DT model, which predicts the system\nresponses affected by the degradation at the same stage. The test results on\ntwo engineering datasets demonstrate that the proposed update method could\ncapture effects of system degradation on system responses during the lifecycle.",
        "Understanding the behavior of particles in a dispersed phase system via\npopulation balances holds fundamental importance in studies of particulate\nsciences across various fields. Particle behavior, however, is sophisticated as\na single particle can undergo internal property changes (e.g., size, cell age,\nand energy content) through various mechanisms. When confronted with an unknown\ndistributed particulate system, discovering the underlying population balance\nequation (PBE) entails firstly learning the underlying particulate phenomena\nfollowed by the associated phenomenological laws that govern the kinetics and\nmechanisms of particle transformations in their local conditions. Conventional\ninverse problem approaches reveal the shape of phenomenological functions for\npredetermined forms of PBE (e.g., pure breakage\/aggregation PBE, etc.).\nHowever, these methods can be limited in their ability to uncover the\nmechanisms which govern uncharacterized particulate systems from data.\nLeveraging the increasing abundance of data, we devise a data-driven framework\nbased on sparse regression to learn PBEs as linear combinations of an extensive\npool of candidate terms. Thus, this approach enables effective and accurate\nfunctional identification of PBEs without assuming the structure a priori,\nhence mitigating any potential loss of details, while minimizing model\noverfitting and providing a more interpretable representation of particulate\nsystems. We showcase the proficiency of our approach across a wide spectrum of\nparticulate systems, ranging from simple canonical pure breakage and pure\naggregation systems to complex systems with multiple particulate processes. Our\napproach holds the potential to generalize the discovery of PBEs along with\ntheir phenomenological laws from data, thus facilitating wider adoption of\npopulation balances.",
        "This thesis presents a novel framework for analysing the societal impacts of\narmed conflict by applying principles from engineering and material science.\nBuilding on the idea of a \"social fabric\", it recasts communities as plates\nwith properties, such as resilience and vulnerability, analogous to material\nparameters like thickness or elasticity. Conflict events are treated as\nexternal forces that deform this fabric, revealing how repeated shocks and\nlocal weaknesses can compound over time. Using a custom Python-based Finite\nElement Analysis implementation, the thesis demonstrates how data on\nsocioeconomic indicators (e.g., infrastructure, health, and demographics) and\nconflict incidents can be translated into a single computational model.\nPreliminary tests validate that results align with expected physical\nbehaviours, and a proof-of-concept highlights how this approach can capture\nindirect or spillover effects and illuminate the areas most at risk of\nlong-term harm. By bridging social science insights with computational\nmodelling, this work offers an adaptable frame to inform both academic research\nand on-the-ground policy decisions for communities affected by violence.",
        "In this work we define, analyze, and compare different numerical schemes that\ncan be used to study the ground state properties of Bose-Fermi systems, such as\nmixtures of different atomic species under external forces or self-bound\nquantum droplets. The bosonic atoms are assumed to be condensed and are\ndescribed by the generalized Gross-Pitaevskii equation. The fermionic atoms, on\nthe other hand, are treated individually, and each atom is associated with a\nwave function whose evolution follows the Hartree-Fock equation. We solve such\na formulated set of equations using a variety of methods, including those based\non adiabatic switching of interactions and the imaginary time propagation\ntechnique combined with the Gram-Schmidt orthonormalization or the\ndiagonalization of the Hamiltonian matrix. We show how different algorithms\ncompete at the numerical level by studying the mixture in the range of\nparameters covering the formation of self-bound quantum Bose-Fermi droplets.",
        "The use of average kernel method based on the Laplace transformation can\nsignificantly simplify the procedure for obtaining approximate analytical\nsolution of Smoluchowski equation. However, this method also has its own\nshortcomings, one of which is the higher computational complexity of the binary\nLaplace transformation for a nonlinear collision kernel. In this study, a\nuniversal algorithm based on the Gauss-Laguerre quadrature for treating the\ndouble integral is developed to obtain easily and quickly pre-exponential\nfactor of the average kernel. Furthermore, the corresponding truncation error\nestimate also provided.",
        "Large language models (LLMs) such as OpenAI's ChatGPT hold potential for\nautomating engineering analysis, yet their reliability in solving multi-step\nstatics problems remains uncertain. This study evaluates the performance of\nChatGPT-4o and ChatGPT-o1-preview on foundational statics tasks, from simple\ncalculations of Newton's second law of motion to beam and truss analyses and\ncompares their results to first-year engineering students on a typical statics\nexam. To enhance accuracy, we developed a Custom GPT, embedding refined prompts\ndirectly into its instructions. This optimized model achieved an 82% score,\nsurpassing the 75% student average, demonstrating the impact of tailored\nguidance. Despite these improvements, LLMs continued to exhibit errors in\nnuanced or open-ended problems, such as misidentifying tension and compression\nin truss members. These findings highlight both the promise and current\nlimitations of AI in structural analysis, emphasizing the need for improved\nreasoning, multimodal capabilities, and targeted training data for future\nAI-driven automation in civil and mechanical engineering.",
        "Variational phase field fracture models are now widely used to simulate crack\npropagation in structures. A critical aspect of these simulations is the\ncorrect determination of the propagation threshold of pre-existing cracks, as\nit highly relies on how the initial cracks are implemented. While prior studies\nbriefly discuss initial crack implementation techniques, we present here a\nsystematic investigation. Various techniques to introduce initial cracks in\nphase field fracture simulations are tested, from the crack explicit meshing to\nthe replacement by a fully damaged phase field, including different variants\nfor the boundary conditions. Our focus here is on phase field models aiming to\napproximate, in the $\\Gamma$-convergence limit, Griffith quasi-static\npropagation in the framework of Linear Elastic Fracture Mechanics. Therefore, a\nsharp crack model from classic linear elastic fracture mechanics based on\nGriffith criterion is the reference in this work. To assess the different\ntechniques to introduce initial cracks, we rely on path-following methods to\ncompute the sharp crack and the phase field smeared crack solutions. The\nunderlying idea is that path-following ensures staying at equilibrium at each\ninstant so that any difference between phase field and sharp crack models can\nbe attributed to numerical artifacts. Thus, by comparing the results from both\nmodels, we can provide practical recommendations for reliably incorporating\ninitial cracks in phase field fracture simulations. The comparison shows that\nan improper initial crack implementation often requires the smeared crack to\ntransition to a one-element-wide phase band to adequately represent a\ndisplacement jump along a crack. This transition increases the energy required\nto propagate the crack, leading to a significant overshoot in the\nforce-displacement response. The take-home message is that to predict the\npropagation threshold accurately and avoid artificial toughening; the crack\nmust be initialized either setting the phase field to its damage state over a\none-element-wide band or meshing the crack explicitly as a one-element-wide\nslit and imposing the fully cracked state on the crack surface.",
        "Abdominal aortic aneurysm (AAA) is a life-threatening condition involving the\npermanent dilation of the aorta, often detected incidentally through imaging\nfor some other condition. The standard clinical approach to managing AAA\nfollows a one-size-fits-all model based on aneurysm size and growth rate,\nleading to underestimation or overestimation of rupture risk in individual\npatients. The widely studied stress-based rupture risk estimation using\ncomputational biomechanics requires wall strength information. However,\nnon-invasive methods for local patient-specific wall strength measurement have\nnot yet been developed. Recently, we introduced an image-based approach for\npatient-specific, in vivo, non-invasive AAA kinematic analysis using\ntime-resolved 3D computed tomography angiography (4D-CTA) images to measure\nwall strain throughout the cardiac cycle. In the present study, we integrated\nwall tension computation and strain measurement to develop a novel measure of\nlocal structural integrity of AAA wall - Relative Structural Integrity Index\n(RSII), independent of material properties and thickness of the wall and\nconditions of blood pressure measurement. Our methods provide a visual map of\nAAA wall structural integrity for individual patients using only their medical\nimages and blood pressure data. We applied our methods to twelve patients.\nAdditionally, we compared our measure of structural integrity of aneurysmal and\nnon-aneurysmal aortas. Our results show similar values of the wall structural\nintegrity measure across the patients, indicating the reliability of our\nmethods. In line with experimental observations reported in the literature, our\nanalysis revealed that localized low stiffness areas are primarily found in the\nmost dilated AAA regions. Our results clearly demonstrate that the AAA wall is\nstiffer than the non-aneurysmal aorta.",
        "Local refinement is vital for efficient numerical simulations. In the context\nof Isogeometric Analysis (IGA), hierarchical B-splines have gained prominence.\nThe work applies the methodology of truncated hierarchical B-splines\n(THB-splines) as they keep additional properties. The framework is further\nenriched with B\\'{e}zier extraction, resulting in the multi-level B\\'{e}zier\nextraction method. We apply this discretization method to 2D magnetostatic\nproblems. The implementation is based on an open-source Octave\/MATLAB IGA code\ncalled GeoPDEs, which allows us to compare our routines with globally refined\nspline models as well as locally refined ones where the solver does not rely on\nB\\'{e}zier extraction.",
        "The Transatomic Power (TAP) reactor has an unusual design for a molten salt\nreactor technology, building upon the foundation laid by the Molten Salt\nReactor Experiment (MSRE). This design introduces three key modifications to\nenhance efficiency and compactness: a revised fuel salt composition, an\nalternative moderator material, and moderator pins surrounded by the molten\nsalt fuel. Unlike traditional solid-fueled reactors that rely on excess\npositive reactivity at the beginning of life, the TAP concept employs a dynamic\napproach. The core's design, featuring a cylindrical geometry with square\nassemblies of moderator rods surrounded by flowing fuel salt, provides\nflexibility in adjusting the moderator-to-fuel ratio during operation - using\nmovable moderator rods - further adding criticality control capability in\naddition to the control rods system. Shape optimization of the core can play a\ncrucial role in enhancing performance and efficiency. By applying multiphysics\ncontinuous shape optimization techniques to key components, such as the unit\ncells of the TAP reactor or its moderator assemblies, we can fine-tune the\nreactor's geometry to achieve optimal performance in key physics like\nneutronics and thermal hydraulics. We explore this aspect using the\noptimization module in the Multiphysics Object Oriented Simulation Environment\n(MOOSE) framework which allows for multiphysics continuous shape optimization.\nThe results reported here illustrate the benefits of applying continuous shape\noptimization in the design of nuclear reactor components and can help in\nextending the TAP reactor's performance.",
        "Free surface and granular fluid mechanics problems combine the challenges of\nfluid dynamics with aspects of granular behaviour. This type of problem is\nparticularly relevant in contexts such as the flow of sediments in rivers, the\nmovement of granular soils in reservoirs, or the interactions between a fluid\nand granular materials in industrial processes such as silos. The numerical\nsimulation of these phenomena is challenging because the solution depends not\nonly on the multiple phases that strongly interact with each other, but also on\nthe need to describe the geometric evolution of the different interfaces. This\npaper presents an approach to the simulation of fluid-granular phenomena\ninvolving strongly deforming free surfaces. The Discrete Element Method (DEM)\nis combined with the Particle Finite Element Method (PFEM) and the fluid-grain\ninterface is treated by a two-way coupling between the two phases. The\nfluid-air interface is solved by a free surface model. The geometric and\ntopological variations are therefore naturally provided by the full Lagrangian\ndescription of all phases. The approach is validated on benchmark test cases\nsuch as two-phase dam failures and then applied to a real landslide problem.",
        "Layered materials with non-centrosymmetric stacking order are attracting\nincreasing interest due to the presence of ferroelectric polarization, which is\ndictated by weak interlayer hybridization of atomic orbitals. Here, we use\ndensity functional theory modelling to systematically build a library of van\nder Waals chalcogenides that exhibit substantial ferroelectric polarization.\nFor the most promising materials, we also analyse the pressure dependence of\nthe ferroelectric effect and charge accumulation of photo-induced electrons and\nholes at surfaces and internal twin boundaries in thin films of such materials.",
        "In recent years, vision transformers (ViTs) have emerged as powerful and\npromising techniques for computer vision tasks such as image classification,\nobject detection, and segmentation. Unlike convolutional neural networks\n(CNNs), which rely on hierarchical feature extraction, ViTs treat images as\nsequences of patches and leverage self-attention mechanisms. However, their\nhigh computational complexity and memory demands pose significant challenges\nfor deployment on resource-constrained edge devices. To address these\nlimitations, extensive research has focused on model compression techniques and\nhardware-aware acceleration strategies. Nonetheless, a comprehensive review\nthat systematically categorizes these techniques and their trade-offs in\naccuracy, efficiency, and hardware adaptability for edge deployment remains\nlacking. This survey bridges this gap by providing a structured analysis of\nmodel compression techniques, software tools for inference on edge, and\nhardware acceleration strategies for ViTs. We discuss their impact on accuracy,\nefficiency, and hardware adaptability, highlighting key challenges and emerging\nresearch directions to advance ViT deployment on edge platforms, including\ngraphics processing units (GPUs), tensor processing units (TPUs), and\nfield-programmable gate arrays (FPGAs). The goal is to inspire further research\nwith a contemporary guide on optimizing ViTs for efficient deployment on edge\ndevices.",
        "This paper is concerned with a natural variant of the contact process\nmodeling the spread of knowledge on the integer lattice. Each site is\ncharacterized by its knowledge, measured by a real number ranging from 0 =\nignorant to 1 = omniscient. Neighbors interact at rate $\\lambda$, which results\nin both neighbors attempting to teach each other a fraction $\\mu$ of their\nknowledge, and individuals die at rate one, which results in a new individual\nwith no knowledge. Starting with a single omniscient site, our objective is to\nstudy whether the total amount of knowledge on the lattice converges to zero\n(extinction) or remains bounded away from zero (survival). The process dies out\nwhen $\\lambda \\leq \\lambda_c$ and\/or $\\mu = 0$, where $\\lambda_c$ denotes the\ncritical value of the contact process. In contrast, we prove that, for all\n$\\lambda > \\lambda_c$, there is a unique phase transition in the direction of\n$\\mu$, and for all $\\mu > 0$, there is a unique phase transition in the\ndirection of $\\lambda$. Our proof of survival relies on block constructions\nshowing more generally convergence of the knowledge to infinity, while our\nproof of extinction relies on martingale techniques showing more generally an\nexponential decay of the knowledge.",
        "The site-decorated Ising model is introduced to advance the understanding and\nexperimental realization of the recently discovered one-dimensional\nfinite-temperature ultranarrow phase crossover in an external magnetic field,\nwhile mitigating the geometric complexities of traditional bond-decorated\nmodels. Furthermore, although higher-dimensional Ising models in an external\nfield remain unsolved, an exact solution for a novel spin-reversal transition\n-- driven by an exotic, hidden ``half-ice, half-fire'' state induced by site\ndecoration -- is derived. This transition, triggered by a slight variation in\ntemperature or magnetic field even in the weak-field limit, offers a promising\nroute toward energy-efficient applications such as data storage and processing.\nThe results establish site decoration as a compelling new avenue for materials\nand device design, particularly in systems such as mixed $d$-$f$ compounds,\noptical lattices, and neural networks.",
        "The sequential response of frustrated materials - ranging from crumpled\nsheets and amorphous media to metamaterials - reveals their memory effects and\nemergent computational potential. Despite their spatial extension, most studies\nrely on a single global stimulus, such as compression, effectively reducing the\nproblem to scalar driving. Here, we introduce vectorial driving by applying\nmultiple spatially localized stimuli to explore path-dependent, sequential\nresponses. We uncover a wealth of phenomena absent in scalar driving, including\nnon-Abelian responses, mixed-mode behavior, and chiral loop transients. We find\nthat such path dependencies arise from elementary motifs linked to fold\nsingularities, which connect triplets of states - ancestor, descendant, and\nsibling; and develop a general framework using pt-graphs to describe responses\nunder any vectorial driving protocol. Leveraging binarized vectorial driving,\nwe establish a natural connection to computation, showing that a single sample\ncan encode multiple sequential Boolean circuits, which are selectable by\ndriving strength and reprogrammable via additional inputs. Finally, we\nintroduce graph-based motifs to manage the complexity of high-dimensional\ndriving. Our work paves the way for strategies to explore, harness, and\nunderstand complex materials and memory, while advancing embodied intelligence\nand in-materia computing.",
        "Protecting Personally Identifiable Information (PII), such as names, is a\ncritical requirement in learning technologies to safeguard student and teacher\nprivacy and maintain trust. Accurate PII detection is an essential step toward\nanonymizing sensitive information while preserving the utility of educational\ndata. Motivated by recent advancements in artificial intelligence, our study\ninvestigates the GPT-4o-mini model as a cost-effective and efficient solution\nfor PII detection tasks. We explore both prompting and fine-tuning approaches\nand compare GPT-4o-mini's performance against established frameworks, including\nMicrosoft Presidio and Azure AI Language. Our evaluation on two public\ndatasets, CRAPII and TSCC, demonstrates that the fine-tuned GPT-4o-mini model\nachieves superior performance, with a recall of 0.9589 on CRAPII. Additionally,\nfine-tuned GPT-4o-mini significantly improves precision scores (a threefold\nincrease) while reducing computational costs to nearly one-tenth of those\nassociated with Azure AI Language. Furthermore, our bias analysis reveals that\nthe fine-tuned GPT-4o-mini model consistently delivers accurate results across\ndiverse cultural backgrounds and genders. The generalizability analysis using\nthe TSCC dataset further highlights its robustness, achieving a recall of\n0.9895 with minimal additional training data from TSCC. These results emphasize\nthe potential of fine-tuned GPT-4o-mini as an accurate and cost-effective tool\nfor PII detection in educational data. It offers robust privacy protection\nwhile preserving the data's utility for research and pedagogical analysis. Our\ncode is available on GitHub: https:\/\/github.com\/AnonJD\/PrivacyAI",
        "Modelling the diffusion-relaxation magnetic resonance (MR) signal obtained\nfrom multi-parametric sequences has recently gained immense interest in the\ncommunity due to new techniques significantly reducing data acquisition time. A\npreferred approach for examining the diffusion-relaxation MR data is to follow\nthe continuum modelling principle that employs kernels to represent the tissue\nfeatures, such as the relaxations or diffusion properties. However,\nconstructing reasonable dictionaries with predefined signal components depends\non the sampling density of model parameter space, thus leading to a geometrical\nincrease in the number of atoms per extra tissue parameter considered in the\nmodel. That makes estimating the contributions from each atom in the signal\nchallenging, especially considering diffusion features beyond the\nmono-exponential decay.\n  This paper presents a new Multi-Compartment diffusion-relaxation MR signal\nrepresentation based on the Simple Harmonic Oscillator-based Reconstruction and\nEstimation (MC-SHORE) representation, compatible with scattered acquisitions.\nThe proposed technique imposes sparsity constraint on the solution via the\n$\\ell_1$ norm and enables the estimation of the microstructural measures, such\nas the return-to-the-origin probability, and the orientation distribution\nfunction, depending on the compartments considered in a single voxel. The\nprocedure has been verified with in silico and in vivo data and enabled the\napproximation of the diffusion-relaxation MR signal more accurately than\nsingle-compartment non-Gaussian representations and multi-compartment\nmono-exponential decay techniques, maintaining a low number of atoms in the\ndictionary. Ultimately, the MC-SHORE procedure allows for separating\nintra-\/extra-axonal and free water contributions from the signal, thus reducing\nthe partial volume effect observable in the boundaries of the tissues.",
        "This paper presents a modern and scalable framework for analyzing Detector\nControl System (DCS) data from the ATLAS experiment at CERN. The DCS data,\nstored in an Oracle database via the WinCC OA system, is optimized for\ntransactional operations, posing challenges for large-scale analysis across\nextensive time periods and devices. To address these limitations, we developed\na data pipeline using Apache Spark, CERN's Hadoop service, and the CERN SWAN\nplatform. This framework integrates seamlessly with Python notebooks, providing\nan accessible and efficient environment for data analysis using\nindustry-standard tools. The approach has proven effective in troubleshooting\nData Acquisition (DAQ) links for the ATLAS New Small Wheel (NSW) detector,\ndemonstrating the value of modern data platforms in enabling detector experts\nto quickly identify and resolve critical issues.",
        "This paper introduces the Non-linear Partition of Unity Method, a novel\ntechnique integrating Radial Basis Function interpolation and Weighted\nEssentially Non-Oscillatory algorithms. It addresses challenges in\nhigh-accuracy approximations, particularly near discontinuities, by adapting\nweights dynamically. The method is rooted in the Partition of Unity framework,\nenabling efficient decomposition of large datasets into subproblems while\nmaintaining accuracy. Smoothness indicators and compactly supported functions\nensure precision in regions with discontinuities. Error bounds are calculated\nand validate its effectiveness, showing improved interpolation in discontinuous\nand smooth regions. Some numerical experiments are performed to check the\ntheoretical results.",
        "We study the fault-tolerance of networks from both the structural and\ncomputational point of view using the minimum leaf number of the corresponding\ngraph $G$, i.e. the minimum number of leaves of the spanning trees of $G$, and\nits vertex-deleted subgraphs. We investigate networks that are leaf-guaranteed,\ni.e. which satisfy a certain stability condition with respect to minimum leaf\nnumbers and vertex-deletion. Next to this, our main notion is the so-called\nfault cost, which is based on the number of vertices that have different\ndegrees in minimum leaf spanning trees of the network and its vertex-deleted\nsubgraphs. We characterise networks with vanishing fault cost via\nleaf-guaranteed graphs and describe, for any given network $N$, leaf-guaranteed\nnetworks containing $N$. We determine for all non-negative integers $k \\le 8$\nexcept $1$ the smallest network with fault cost $k$. We also give a detailed\ntreatment of the fault cost $1$ case, prove that there are infinitely many\n$3$-regular networks with fault cost $3$, and show that for any non-negative\ninteger $k$ there exists a network with fault cost exactly $k$.",
        "The HAWC Observatory collected 6 years of extensive data, providing an ideal\nplatform for long-term monitoring of blazars in the Very High Energy (VHE)\nband, without bias towards specific flux states. HAWC continuously monitors\nblazar activity at TeV energies, focusing on sources with a redshift of {z \\lt\n0.3}, based on the Third Fermi-LAT Catalog of High-Energy sources. We\nspecifically focused our analysis on Mrk 421 and Mrk 501, as they are the\nbrightest blazars observed by the HAWC Observatory. With a dataset of 2143\ndays, this work significantly extends the monitoring previously published,\nwhich was based on 511 days of observation. By utilizing HAWC data for the VHE\n{\\gamma}-ray emission in the 300 GeV to 100 TeV energy range, in conjunction\nwith Swift-XRT data for the 0.3 to 10 keV X-ray emission, we aim to explore\npotential correlations between these two bands. For Mrk 501, we found evidence\nof a long-term correlation. Additionally, we identified a period in the light\ncurve where the flux was very low for more than two years. On the other hand,\nour analysis of Mrk 421 measured a strong linear correlation for\nquasi-simultaneous observations collected by HAWC and Swift-XRT. This result is\nconsistent with a linear dependence and a multiple-zone synchrotron\nself-Compton model to explain the X-ray and the {\\gamma}-ray emission. Finally,\nas suggested by previous findings, we confirm a harder-when-brighter behavior\nin the spectral evolution of the flux properties for Mrk 421. These findings\ncontribute to the understanding of blazar emissions and their underlying\nmechanisms.",
        "The design and performance analysis of relay lenses that provide\nhigh-performance image transmission for target acquisition and tracking in\nmilitary optical systems. Relay lenses are critical components for clear and\nlossless image transmission over long distances. In this study, the optical\nperformance of a relay lens system designed and optimized using ZEMAX software\nis investigated in detail. The analysis focuses on important optical properties\nsuch as modulation transfer function (MTF), spot diagrams, Seidel diagram,\nfield curvature and distortion. The results show that the lens has significant\npotential in military applications for target detection and tracking with high\nresolution and low aberration.",
        "Reinforcement Learning (RL) has been widely used in many applications,\nparticularly in gaming, which serves as an excellent training ground for AI\nmodels. Google DeepMind has pioneered innovations in this field, employing\nreinforcement learning algorithms, including model-based, model-free, and deep\nQ-network approaches, to create advanced AI models such as AlphaGo, AlphaGo\nZero, and MuZero. AlphaGo, the initial model, integrates supervised learning\nand reinforcement learning to master the game of Go, surpassing professional\nhuman players. AlphaGo Zero refines this approach by eliminating reliance on\nhuman gameplay data, instead utilizing self-play for enhanced learning\nefficiency. MuZero further extends these advancements by learning the\nunderlying dynamics of game environments without explicit knowledge of the\nrules, achieving adaptability across various games, including complex Atari\ngames. This paper reviews the significance of reinforcement learning\napplications in Atari and strategy-based games, analyzing these three models,\ntheir key innovations, training processes, challenges encountered, and\nimprovements made. Additionally, we discuss advancements in the field of\ngaming, including MiniZero and multi-agent models, highlighting future\ndirections and emerging AI models from Google DeepMind.",
        "The fact that everyone with a social media account can create and share\ncontent, and the increasing public reliance on social media platforms as a news\nand information source bring about significant challenges such as\nmisinformation, fake news, harmful content, etc. Although human content\nmoderation may be useful to an extent and used by these platforms to flag\nposted materials, the use of AI models provides a more sustainable, scalable,\nand effective way to mitigate these harmful contents. However, low-resourced\nlanguages such as the Somali language face limitations in AI automation,\nincluding scarce annotated training datasets and lack of language models\ntailored to their unique linguistic characteristics. This paper presents part\nof our ongoing research work to bridge some of these gaps for the Somali\nlanguage. In particular, we created two human-annotated social-media-sourced\nSomali datasets for two downstream applications, fake news \\& toxicity\nclassification, and developed a transformer-based monolingual Somali language\nmodel (named SomBERTa) -- the first of its kind to the best of our knowledge.\nSomBERTa is then fine-tuned and evaluated on toxic content, fake news and news\ntopic classification datasets. Comparative evaluation analysis of the proposed\nmodel against related multilingual models (e.g., AfriBERTa, AfroXLMR, etc)\ndemonstrated that SomBERTa consistently outperformed these comparators in both\nfake news and toxic content classification tasks while achieving the best\naverage accuracy (87.99%) across all tasks. This research contributes to Somali\nNLP by offering a foundational language model and a replicable framework for\nother low-resource languages, promoting digital and AI inclusivity and\nlinguistic diversity.",
        "The Observatorio Astrof\\'isico de Javalambre (OAJ) is a Spanish astronomical\nICTS (Unique Scientific and Technical Infrastructures) located in the Sierra de\nJavalambre in Teruel (Spain). It has been particularly conceived for carrying\nout large-sky multi-filter surveys. As an ICTS, the OAJ offers Open Time to the\nastronomical community, offering more than 25% through Legacy Surveys, Regular\nPrograms (RP) and Director discretionary time (DDT). Regarding the RP, a new\ncall for proposals is made public each semester accepting only proposals under\nthe modality of Target of Opportunity (ToO).\n  This contribution summarizes how ToOs are managed at OAJ presenting the\ndifferent applications designed and implemented at the observatory to deal with\nthem: the Proposal Preparation portal (to request observing time), the Phase2\nObserving tool and the submitphase2 web service (to trigger the ToOs), the TAC\nTracking portal (for telescope operators to support the observations) and the\nTACData portal (to publish and offer the images and their data products)."
      ]
    }
  },
  {
    "id":2411.16464,
    "research_type":"basic",
    "start_id":"b6",
    "start_title":"An agent-based spatial urban social network generator: A case study of beijing, china",
    "start_abstract":"This paper proposes an agent-based spatial social network model, which combines a utility function and heuristic algorithms, to formulate friendships of agents in a given synthetic population comprising individuals and households, as well as their attributes and locations. In order to better and explicitly represent the real social networks, the model attempts to generate both close and somewhat close social networks by linking agents with either close or somewhat close friendships, fitting both distributions of network degree and transitivity, which are two basic characteristics of a network. Here, a utility function, which incorporates the similarity between agents in individual attributes (e.g., sex), as well as the spatial closeness of their residential locations and workplaces, is developed to judge whether a friendship between a pair of agents can be built. Furthermore, the social network model is developed as a key component of an agent-and Geographic Information System (GIS)-based virtual city creator that is a set of synthesis methods used to generate spatially disaggregate urban data. Finally, Beijing, China is used as a case study. Both close and somewhat close social networks are generated with the target and generated distributions well matched, and the generated networks are further analysed from a geographical perspective.",
    "start_categories":[
      "cs.CE"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b2"
      ],
      "title":[
        "Meeting Strangers and Friends of Friends: How Random Are Social Networks?"
      ],
      "abstract":[
        "We present a dynamic model of network formation where nodes find other with whom to form links in two ways: some are found uniformly at random, while others by searching locally through the current structure (e.g., meeting friends friends). This combination processes results spectrum features exhibited large social networks, including presence more high- and low-degree than when formed independently having low distances between network, high clustering on local level. fit data from six networks impute relative ratio random network-based meetings link formation, which turns out vary dramatically across applications. show that as random\/network-based varies, resulting degree distributions can be ordered sense stochastic dominance, allows us infer how process affects average utility network. (JEL D85, Z13)"
      ],
      "categories":[
        "q-fin.EC"
      ]
    },
    "list":{
      "title":[
        "On the numerical approximation of minimax regret rules via fictitious\n  play",
        "Optimal investment and consumption under $g$- expected utility and\n  general constraints in incomplete market",
        "Nonlinear Forecast Error Variance Decompositions with Hermite\n  Polynomials",
        "Robust distortion risk measures with linear penalty under distribution\n  uncertainty",
        "Revealed Social Networks",
        "Combined climate stress testing of supply-chain networks and the\n  financial system with nation-wide firm-level emission estimates",
        "Locally Robust Policy Learning: Inequality, Inequality of Opportunity\n  and Intergenerational Mobility",
        "Women's Status and Fertility: A Novel Perspective on Low Fertility Issue",
        "A mixture transition distribution approach to portfolio optimization",
        "A Class of Practical and Acceptable Social Welfare Orderings That\n  Satisfy the Principles of Aggregation and Non-Aggregation: Reexamination of\n  the Tyrannies of Aggregation and Non-Aggregation",
        "Techno-Economic Analysis of Hydrogen Production: Costs, Policies, and\n  Scalability in the Transition to Net-Zero",
        "Clearing Sections of Lattice Liability Networks",
        "Robust Quantile Factor Analysis",
        "Reducing Circuit Depth in Quantum State Preparation for Quantum\n  Simulation Using Measurements and Feedforward",
        "Insights from leptohadronic modelling of the brightest blazar flare",
        "Ultra-cold neutrons in qBounce experiments as laboratory for test of\n  chameleon field theories and cosmic acceleration",
        "Twenty years of Ne\\v{s}et\\v{r}il's classification programme of Ramsey\n  classes",
        "The algebraic and geometric classification of Jordan superalgebras",
        "Flipped Rotating Axion Non-minimally Coupled to Gravity: Baryogenesis\n  and Dark Matter",
        "Euclid Quick Data Release (Q1). The Strong Lensing Discovery Engine D --\n  Double-source-plane lens candidates",
        "NICER, NuSTAR and Insight-HXMT views to the newly discovered black hole\n  X-ray binary Swift J1727.8--1613",
        "Can supermassive stars form in protogalaxies due to internal\n  Lyman-Werner feedback?",
        "Experimental Realization of Special-Unitary Operations in Classical\n  Mechanics by Non-Adiabatic Evolutions",
        "Elasticity of a Freely Jointed Chain with Quenched Disorder",
        "Robust Cislunar Low-Thrust Trajectory Optimization under Uncertainties\n  via Sequential Covariance Steering",
        "Human-Agent Interaction in Synthetic Social Networks: A Framework for\n  Studying Online Polarization",
        "Quantile-Based Randomized Kaczmarz for Corrupted Tensor Linear Systems",
        "Free Perpetuities I: Existence, Subordination and Tail Asymptotics"
      ],
      "abstract":[
        "Finding numerical approximations to minimax regret treatment rules is of key\ninterest. To do so when potential outcomes are in {0,1} we discretize the\naction space of nature and apply a variant of Robinson's (1951) algorithm for\niterative solutions for finite two-person zero sum games. Our approach avoids\nthe need to evaluate regret of each treatment rule in each iteration. When\npotential outcomes are in [0,1] we apply the so-called coarsening approach. We\nconsider a policymaker choosing between two treatments after observing data\nwith unequal sample sizes per treatment and the case of testing several\ninnovations against the status quo.",
        "This article studies the problem of utility maximization in an incomplete\nmarket under a class of nonlinear expectations and general constraints on\ntrading strategies. Using a $g$-martingale method, we provide an explicit\nsolution to our optimization problem for different utility functions and\ncharacterize an optimal investment-consumption strategy through the solutions\nto quadratic BSDEs.",
        "A novel approach to Forecast Error Variance Decompositions (FEVD) in\nnonlinear Structural Vector Autoregressive models with Gaussian innovations is\nproposed, called the Hermite FEVD (HFEVD). This method employs a Hermite\npolynomial expansion to approximate the future trajectory of a nonlinear\nprocess. The orthogonality of Hermite polynomials under the Gaussian density\nfacilitates the construction of the decomposition, providing a separation of\nshock effects by time horizon, by components of the structural innovation and\nby degree of nonlinearity. A link between the HFEVD and nonlinear Impulse\nResponse Functions is established and distinguishes between marginal and\ninteraction contributions of shocks. Simulation results from standard nonlinear\nmodels are provided as illustrations and an application to fiscal policy shocks\nis examined.",
        "The paper investigates the robust distortion risk measure with linear penalty\nfunction under distribution uncertainty. The distribution uncertainties are\ncharacterized by predetermined moment conditions or constraints on the\nWasserstein distance. The optimal quantile distribution and the optimal value\nfunction are explicitly characterized. Our results partially extend the results\nof Bernard, Pesenti and Vanduffel (2024) and Li (2018) to robust distortion\nrisk measures with linear penalty. In addition, we also discuss the influence\nof the penalty parameter on the optimal solution.",
        "People are influenced by their peers when making decisions. In this paper, we\nstudy the linear-in-means model which is the standard empirical model of peer\neffects. As data on the underlying social network is often difficult to come\nby, we focus on data that only captures an agent's choices. Under exogenous\nagent participation variation, we study two questions. We first develop a\nrevealed preference style test for the linear-in-means model. We then study the\nidentification properties of the linear-in-means model. With sufficient\nparticipation variation, we show how an analyst is able to recover the\nunderlying network structure and social influence parameters from choice data.\nOur identification result holds when we allow the social network to vary across\ncontexts. To recover predictive power, we consider a refinement which allows us\nto extrapolate the underlying network structure across groups and provide a\ntest of this version of the model.",
        "On the way towards carbon neutrality, climate stress testing provides\nestimates for the physical and transition risks that climate change poses to\nthe economy and the financial system. Missing firm-level CO2 emissions data\nseverely impedes the assessment of transition risks originating from carbon\npricing. Based on the individual emissions of all Hungarian firms (410,523), as\nestimated from their fossil fuel purchases, we conduct a stress test of both\nactual and hypothetical carbon pricing policies. Using a simple 1:1 economic\nABM and introducing the new carbon-to-profit ratio, we identify firms that\nbecome unprofitable and default, and estimate the respective loan write-offs.\nWe find that 45% of all companies are directly exposed to carbon pricing. At a\nprice of 45 EUR\/t, direct economic losses of 1.3% of total sales and bank\nequity losses of 1.2% are expected. Secondary default cascades in supply chain\nnetworks could increase these losses by 300% to 4000%, depending on firms'\nability to substitute essential inputs. To reduce transition risks, firms\nshould reduce their dependence on essential inputs from supply chains with high\nCO2 exposure. We discuss the implications of different policy implementations\non these transition risks.",
        "Policy makers need to decide whether to treat or not to treat heterogeneous\nindividuals. The optimal treatment choice depends on the welfare function that\nthe policy maker has in mind and it is referred to as the policy learning\nproblem. I study a general setting for policy learning with semiparametric\nSocial Welfare Functions (SWFs) that can be estimated by locally\nrobust\/orthogonal moments based on U-statistics. This rich class of SWFs\nsubstantially expands the setting in Athey and Wager (2021) and accommodates a\nwider range of distributional preferences. Three main applications of the\ngeneral theory motivate the paper: (i) Inequality aware SWFs, (ii) Inequality\nof Opportunity aware SWFs and (iii) Intergenerational Mobility SWFs. I use the\nPanel Study of Income Dynamics (PSID) to assess the effect of attending\npreschool on adult earnings and estimate optimal policy rules based on parental\nyears of education and parental income.",
        "This model offers a compelling explanation for the observed decline in\nfertility rates in developed countries, correlating it with the rising economic\nstatus of women, and thereby providing valuable insights for policy-making. In\nsummary, through a game theory model, we identify the rising economic status of\nwomen as the underlying cause of low fertility rates in modern developed\ncountries.",
        "Understanding the dependencies among financial assets is critical for\nportfolio optimization. Traditional approaches based on correlation networks\noften fail to capture the nonlinear and directional relationships that exist in\nfinancial markets. In this study, we construct directed and weighted financial\nnetworks using the Mixture Transition Distribution (MTD) model, offering a\nricher representation of asset interdependencies. We apply local assortativity\nmeasures--metrics that evaluate how assets connect based on similarities or\ndifferences--to guide portfolio selection and allocation. Using data from the\nDow Jones 30, Euro Stoxx 50, and FTSE 100 indices constituents, we show that\nportfolios optimized with network-based assortativity measures consistently\noutperform the classical mean-variance framework. Notably, modalities in which\nassets with differing characteristics connect enhance diversification and\nimprove Sharpe ratios. The directed nature of MTD-based networks effectively\ncaptures complex relationships, yielding portfolios with superior risk-adjusted\nreturns. Our findings highlight the utility of network-based methodologies in\nfinancial decision-making, demonstrating their ability to refine portfolio\noptimization strategies. This work thus underscores the potential of leveraging\nadvanced financial networks to achieve enhanced performance, offering valuable\ninsights for practitioners and setting a foundation for future research.",
        "This paper revisits impossibility results on the tyrannies of aggregation and\nnon-aggregation. I propose two aggregation principles (quantitative aggregation\nand ratio aggregation) and investigate theoretical implications. As a result, I\nshow that quantitative aggregation and minimal non-aggregation are incompatible\nwhile ratio aggregation and minimal non-aggregation are compatible under the\nassumption of standard axioms in social choice theory. Furthermore, this study\nprovides a new characterization of the leximin rule by using replication\ninvariance and the strong version of non-aggregation. Finally, I propose a\nclass of practical and acceptable social welfare orderings that satisfy the\nprinciples of aggregation and non-aggregation, which has various advantages\nover the standard rank-discounted generalized utilitarianism.",
        "This study presents a comprehensive techno-economic analysis of gray, blue,\nand green hydrogen production pathways, evaluating their cost structures,\ninvestment feasibility, infrastructure challenges, and policy-driven cost\nreductions. The findings confirm that gray hydrogen (1.50-2.50\/kg) remains the\nmost cost-effective today but is increasingly constrained by carbon pricing.\nBlue hydrogen (2.00-3.50\/kg) offers a transitional pathway but depends on CCS\ncosts, natural gas price volatility, and regulatory support. Green hydrogen\n(3.50-6.00\/kg) is currently the most expensive but benefits from declining\nrenewable electricity costs, electrolyzer efficiency improvements, and\ngovernment incentives such as the Inflation Reduction Act (IRA), which provides\ntax credits of up to 3.00\/kg. The analysis shows that renewable electricity\ncosts below 20-30\/MWh are essential for green hydrogen to achieve cost parity\nwith fossil-based hydrogen. The DOE's Hydrogen Shot Initiative aims to lower\ngreen hydrogen costs to 1.00\/kg by 2031, emphasizing the need for CAPEX\nreductions, economies of scale, and improved electrolyzer efficiency.\nInfrastructure remains a critical challenge, with pipeline retrofitting\nreducing transport costs by 50-70%, though liquefied hydrogen and chemical\ncarriers remain costly due to energy losses and reconversion expenses.\nInvestment trends indicate a shift toward green hydrogen, with over 250 billion\nprojected by 2035, surpassing blue hydrogen's expected 100 billion. Carbon\npricing above $100\/ton CO2 will likely make gray hydrogen uncompetitive by\n2030, accelerating the shift to low-carbon hydrogen. Hydrogen's long-term\nviability depends on continued cost reductions, policy incentives, and\ninfrastructure expansion, with green hydrogen positioned as a cornerstone of\nthe net-zero energy transition by 2035.",
        "Modern financial networks involve complex obligations that transcend simple\nmonetary debts: multiple currencies, prioritized claims, supply chain\ndependencies, and more. We present a mathematical framework that unifies and\nextends these scenarios by recasting the classical Eisenberg-Noe model of\nfinancial clearing in terms of lattice liability networks. Each node in the\nnetwork carries a complete lattice of possible states, while edges encode\nnominal liabilities. Our framework generalizes the scalar-valued clearing\nvectors of the classical model to lattice-valued clearing sections, preserving\nthe elegant fixed-point structure while dramatically expanding its descriptive\npower. Our main theorem establishes that such networks possess clearing\nsections that themselves form a complete lattice under the product order. This\nstructure theorem enables tractable analysis of equilibria in diverse domains,\nincluding multi-currency financial systems, decentralized finance with\nautomated market makers, supply chains with resource transformation, and\npermission networks with complex authorization structures. We further extend\nour framework to chain-complete lattices for term structure models and\nmultivalued mappings for complex negotiation systems. Our results demonstrate\nhow lattice theory provides a natural language for understanding complex\nnetwork dynamics across multiple domains, creating a unified mathematical\nfoundation for analyzing systemic risk, resource allocation, and network\nstability.",
        "We propose a factor model and an estimator of the factors and loadings that\nare robust to weak factors. The factors can have an arbitrarily weak influence\non the mean or quantile of the outcome variable at most quantile levels; each\nfactor only needs to have a strong impact on the outcome's quantile near one\nunknown quantile level. The estimator for every factor, loading, and common\ncomponent is asymptotically normal at the $\\sqrt{N}$ or $\\sqrt{T}$ rate. It\ndoes not require the knowledge of whether the factors are weak and how weak\nthey are. We also develop a weak-factor-robust estimator of the number of\nfactors and a consistent selectors of factors of any desired strength of\ninfluence on the quantile or mean of the outcome variable. Monte Carlo\nsimulations demonstrate the effectiveness of our methods.",
        "Reducing circuit depth and identifying an optimal trade-off between circuit\ndepth and width is crucial for successful quantum computation. In this context,\nmid-circuit measurement and feedforward have been shown to significantly reduce\nthe depth of quantum circuits, particularly in implementing logical gates. By\nleveraging these techniques, we propose several parallelization strategies that\nreduce quantum circuit depth at the expense of increasing width in preparing\nvarious quantum states relevant to quantum simulation. With measurements and\nfeedforward, we demonstrate that utilizing unary encoding as a bridge between\ntwo quantum states substantially reduces the circuit depth required for\npreparing quantum states, such as sparse quantum states and sums of Slater\ndeterminants within the first quantization framework, while maintaining an\nefficient circuit width. Additionally, we show that a coordinate Bethe ansatz,\ncharacterized by its high degree of freedom in its phase, can be\nprobabilistically prepared in a constant-depth quantum circuit using\nmeasurements and feedforward. We anticipate that our study will contribute to\nthe reduction of circuit depth in initial state preparation, particularly for\nquantum simulation, which is a critical step toward achieving quantum\nadvantage.",
        "The blazar 3C 454.3 experienced a major flare in November 2010 making it the\nbrightest $\\gamma$-ray source in the sky of the Fermi-LAT. We obtain seven\ndaily consecutive spectral-energy distributions (SEDs) of the flare in the\ninfra-red, optical, ultra-violet, X-ray and $\\gamma$-ray bands with publicly\navailable data. We simulate the physical conditions in the blazar and show that\nthe observed SEDs are well reproduced in the framework of a \"standing feature\"\nwhere the position of the emitting region is almost stationary, located beyond\nthe outer radius of the broad-line region and into which fresh blobs of\nrelativistically moving magnetized plasma are continuously injected. Meanwhile,\na model with a single \"moving blob\" does not describe the data well. We obtain\na robust upper limit to the amount of high-energy protons in the jet of 3C\n454.3 from the electromagnetic SED. We construct a neutrino light curve of 3C\n454.3 and estimate the expected neutrino yield at energies $\\geq 100$ TeV for\n3C 454.3 to be up to $6 \\times 10^{-3}$ $\\nu_{\\mu}$ per year. Finally, we\nextrapolate our model findings to the light curves of all Fermi-LAT\nflat-spectrum radio quasars. We find that next-generation neutrino telescopes\nare expected to detect approximately one multimessenger ($\\gamma + \\nu_{\\mu}$)\nflare per year from bright blazars with neutrino peak energy in the hundreds\nTeV -- hundreds PeV energy range and show that the electromagnetic flare peak\ncan precede the neutrino arrival by months to years.",
        "The accelerating expansion of the Universe, attributed to dark energy, has\nspurred interest in theories involving scalar fields such as chameleon field\ntheories. These fields, which couple to matter with density-dependent effective\nmass, offer a promising explanation for cosmic acceleration. Experiments\nleveraging ultra-cold neutrons (UCNs) provide an innovative approach to testing\nthese theories. The existence of a chameleon field, being responsible for the\ncurrent phase of cosmic acceleration, is investigated by analysing a free fall\nof ultra-cold neutrons from the gap between two mirrors after their bouncing\nbetween these two mirrors. We analyse a deformation of the wave functions of\nthe quantum gravitational states of ultra-cold neutrons, induced by a chameleon\nfield, and find a new upper bound $\\beta\\leq6.5\\times10^8$ on the\nchameleon-matter coupling constant $\\beta$ from the unitarity condition. This\nresult refines previous estimates and highlights the potential of ultra-cold\nneutron experiments as laboratories for exploring scalar field theories and\nfundamental physics.",
        "In the 1970s, structural Ramsey theory emerged as a new branch of\ncombinatorics. This development came with the isolation of the concepts of the\n$\\mathbf{A}$-Ramsey property and Ramsey class. Following the influential\nNe\\v{s}et\\v{r}il-R\\\"{o}dl theorem, several Ramsey classes have been identified.\nIn the 1980s Ne\\v{s}et\\v{r}il, inspired by a seminar of Lachlan, discovered a\ncrucial connection between Ramsey classes and Fra\\\"{\\i}ss\\'{e} classes and, in\nhis 1989 paper, connected the classification programme of homogeneous\nstructures to structural Ramsey theory. In 2005, Kechris, Pestov, and\nTodor\\v{c}evi\\'{c} revitalized the field by connecting Ramsey classes to\ntopological dynamics. This breakthrough motivated Ne\\v{s}et\\v{r}il to propose a\nprogram for classifying Ramsey classes. We review the progress made on this\nprogram in the past two decades, list open problems, and discuss recent\nextensions to new areas, namely the extension property for partial\nautomorphisms (EPPA), and big Ramsey structures.",
        "We give the algebraic and geometric classification of complex\nfour-dimensional Jordan superalgebras. In particular, we describe all\nirreducible components in the corresponding varieties.",
        "We demonstrate that the co-genesis of baryon asymmetry and dark matter can be\nachieved through the rotation of an axion-like particle, driven by a flip in\nthe vacuum manifold's direction at the end of inflation. This can occur if the\naxion has a periodic non-minimal coupling to gravity, while preserving the\ndiscrete shift symmetry. In non-oscillating inflation models, after inflation\nthere is typically a period of kination (with $w = 1$). In this case, it is\nshown that the vacuum manifold of the axion is flipped and the axion begins\nrotating in field space, because it can slide across the decreasing potential\nbarrier as in Ricci reheating. Such a rotating axion can generate the baryon\nasymmetry of the Universe through spontaneous baryogenesis, while at later\nepochs it can oscillate as dark matter. The period of kination makes the\nprimordial gravitational waves (GW) generated during inflation sharply\nblue-tilted which constrains the parameter space due to GW overproduction,\nwhile being testable by next generation CMB experiments. As a concrete example,\nwe show that such a cogenesis of baryon asymmetry and dark matter can be\nrealized for the axion as the Majoron in the Type-I seesaw setup, predicting\nmass ranges for the Majoron below sub eVs, with right-handed neutrino mass\nabove $\\mathcal{O}(10^{8})$ GeV. We also show that in order to avoid\nfragmentation of the axion condensate during the rotation, we require the\nnon-minimal coupling \\mbox{$\\xi \\sim (f\/m_P)^2 $} or somewhat larger, where $f$\nis the axion decay constant.",
        "Strong gravitational lensing systems with multiple source planes are powerful\ntools for probing the density profiles and dark matter substructure of the\ngalaxies. The ratio of Einstein radii is related to the dark energy equation of\nstate through the cosmological scaling factor $\\beta$. However, galaxy-scale\ndouble-source-plane lenses (DSPLs) are extremely rare. In this paper, we report\nthe discovery of four new galaxy-scale double-source-plane lens candidates in\nthe Euclid Quick Release 1 (Q1) data. These systems were initially identified\nthrough a combination of machine learning lens-finding models and subsequent\nvisual inspection from citizens and experts. We apply the widely-used {\\tt\nLensPop} lens forecasting model to predict that the full \\Euclid survey will\ndiscover 1700 DSPLs, which scales to $6 \\pm 3$ DSPLs in 63 deg$^2$, the area of\nQ1. The number of discoveries in this work is broadly consistent with this\nforecast. We present lens models for each DSPL and infer their $\\beta$ values.\nOur initial Q1 sample demonstrates the promise of \\Euclid to discover such rare\nobjects.",
        "Swift J1727.8--1613 is a black hole X-ray binary newly discovered in 2023. We\nperform spectral analysis with simultaneous Insight-HXMT, NICER and NuSTAR\nobservations when the source was approaching to the hard intermediate state.\nSuch a joint view reveals an additional hard component apart from the normally\nobserved hard component with reflection in the spectrum, to be distinguished\nfrom the usual black hole X-ray binary systems. By including this extra\ncomponent in the spectrum, we have measured a high spin of\n$0.98^{+0.02}_{-0.07}$ and an inclination of around $40^{+1.2}_{-0.8}$ degrees,\nwhich is consistent with NICER results reported before. However, we find that\nthe additional spectral component can not be exclusively determined due to the\nmodel degeneracy. Accordingly, a possible jet\/corona configuration is adjusted\nto account for the spectral fitting with different model trials. The extra\ncomponent may originate either from a relativistic jet or a jet base\/corona\nunderneath a slow jet.",
        "Population III stars are possible precursors to early massive and\nsupermassive black holes (BHs). The presence of soft UV Lyman Werner (LW)\nbackground radiation can suppress Population III star formation in minihalos\nand allow them to form in pristine atomic cooling halos. In the absence of\nmolecular hydrogen ($\\rm H_2$) cooling, atomic-cooling halos enable rapid\ncollapse with suppressed fragmentation. High background LW fluxes from\npreceding star-formation have been proposed to dissociate $\\rm H_2$. This flux\ncan be supplemented by LW radiation from one or more Population III star(s) in\nthe same halo, reducing the necessary background level. Here we consider\natomic-cooling halos in which multiple protostellar cores form close to one\nanother nearly simultaneously. We assess whether the first star's LW radiation\ncan dissociate nearby $\\rm H_2$, enabling the prompt formation of a second,\nsupermassive star (SMS) from warm, atomically-cooled gas. We use a set of\nhydrodynamical simulations with the code ENZO, with identical LW backgrounds\ncentered on a halo with two adjacent collapsing gas clumps. When an additional\nlarge local LW flux is introduced, we observe immediate reductions in both the\naccretion rates and the stellar masses that form within these clumps. While the\nLW flux reduces the $\\text{H}_2$ fraction and increases the gas temperature,\nthe halo core's potential well is too shallow to promptly heat the gas to\n$\\gtrsim$ 1000 K and increase the accretion rate onto the second protostar. We\nconclude that internal LW feedback inside atomic-cooling halos is unlikely to\nfacilitate the formation of SMSs or massive BH seeds.",
        "Artificial classical wave systems such as wave crystals and metamaterials\nhave demonstrated promising capabilities in simulating a wide range of quantum\nmechanical phenomena. Yet some gaps between quantum and classical worlds are\ngenerally considered fundamental and difficult to bridge. Dynamics obeying\nspecial unitary groups, e.g., electronic spins described by SU(2), color\nsymmetries of fundamental particles described by SU(3), are such examples. In\nthis work, we present the experimental realization of universal SU(2) and SU(3)\ndynamic operations in classical mechanical oscillator systems with temporally\nmodulated coupling terms. Our approach relies on the sequential execution of\nnon-adiabatic holonomic evolutions, which are typically used in constructing\nquantum-logic gates. The method is swift and purely geometric and can be\nextended to realize more sophisticated dynamic operations. Our results open a\nnew way for studying and simulating quantum phenomena in classical systems.",
        "We introduce a simple theoretical model, the Freely Jointed Chain with\nquenched hinges (qFJC), which captures the quenched disorder in the local\nbending stiffness of the polymer. In this article, we analyze the tensile\nelasticity of the qFJC in the Gibbs (fixed-force) ensemble. For finite-size\nsystems, we obtain a recurrence relation of the exact free energy, which allows\nus to calculate the exact force-extension relation numerically for an arbitrary\nsize of the system. In the thermodynamic limit, when $L({\\rm contour\n\\;length})\\gg L_p({\\rm persistence \\;length})$, we obtain a framework to deal\nwith quenched disorder in the polymer configuration. This allows us to obtain\nthe response function for the discrete and continuous qFJC in the thermodynamic\nlimit. It turns out that the extension of the continuous qFJC can be cast in a\nsimple form. Furthermore, we have applied our analysis to rod-coil multiblock\ncopolymers.",
        "Spacecraft operations are influenced by uncertainties such as dynamics\nmodeling, navigation, and maneuver execution errors. Although mission design\nhas traditionally incorporated heuristic safety margins to mitigate the effect\nof uncertainties, particularly before\/after crucial events, it is yet unclear\nwhether this practice will scale in the cislunar region, which features locally\nchaotic nonlinear dynamics and involves frequent lunar flybys. This paper\napplies chance-constrained covariance steering and sequential convex\nprogramming to simultaneously design an optimal trajectory and trajectory\ncorrection policy that can probabilistically guarantee safety constraints under\nthe assumed physical\/navigational error models. The results show that the\nproposed method can effectively control the state uncertainty in a highly\nnonlinear environment and provide a trajectory with better local stability\nproperties than a trajectory designed without considering uncertainties. The\nframework allows faster computation and lossless covariance propagation\ncompared to existing methods, enabling a rapid and accurate comparison of\n$\\Delta V_{99}$ costs for different uncertainty parameters. We demonstrate the\nalgorithm on several transfers in the Earth-Moon Circular Restricted Three Body\nProblem.",
        "Online social networks have dramatically altered the landscape of public\ndiscourse, creating both opportunities for enhanced civic participation and\nrisks of deepening social divisions. Prevalent approaches to studying online\npolarization have been limited by a methodological disconnect: mathematical\nmodels excel at formal analysis but lack linguistic realism, while language\nmodel-based simulations capture natural discourse but often sacrifice\nanalytical precision. This paper introduces an innovative computational\nframework that synthesizes these approaches by embedding formal opinion\ndynamics principles within LLM-based artificial agents, enabling both rigorous\nmathematical analysis and naturalistic social interactions. We validate our\nframework through comprehensive offline testing and experimental evaluation\nwith 122 human participants engaging in a controlled social network\nenvironment. The results demonstrate our ability to systematically investigate\npolarization mechanisms while preserving ecological validity. Our findings\nreveal how polarized environments shape user perceptions and behavior:\nparticipants exposed to polarized discussions showed markedly increased\nsensitivity to emotional content and group affiliations, while perceiving\nreduced uncertainty in the agents' positions. By combining mathematical\nprecision with natural language capabilities, our framework opens new avenues\nfor investigating social media phenomena through controlled experimentation.\nThis methodological advancement allows researchers to bridge the gap between\ntheoretical models and empirical observations, offering unprecedented\nopportunities to study the causal mechanisms underlying online opinion\ndynamics.",
        "The reconstruction of tensor-valued signals from corrupted measurements,\nknown as tensor regression, has become essential in many multi-modal\napplications such as hyperspectral image reconstruction and medical imaging. In\nthis work, we address the tensor linear system problem $\\mathcal{A}\n\\mathcal{X}=\\mathcal{B}$, where $\\mathcal{A}$ is a measurement operator,\n$\\mathcal{X}$ is the unknown tensor-valued signal, and $\\mathcal{B}$ contains\nthe measurements, possibly corrupted by arbitrary errors. Such corruption is\ncommon in large-scale tensor data, where transmission, sensory, or storage\nerrors are rare per instance but likely over the entire dataset and may be\narbitrarily large in magnitude. We extend the Kaczmarz method, a popular\niterative algorithm for solving large linear systems, to develop a Quantile\nTensor Randomized Kaczmarz (QTRK) method robust to large, sparse corruptions in\nthe observations $\\mathcal{B}$. This approach combines the tensor Kaczmarz\nframework with quantile-based statistics, allowing it to mitigate adversarial\ncorruptions and improve convergence reliability. We also propose and discuss\nthe Masked Quantile Randomized Kaczmarz (mQTRK) variant, which selectively\napplies partial updates to handle corruptions further. We present convergence\nguarantees, discuss the advantages and disadvantages of our approaches, and\ndemonstrate the effectiveness of our methods through experiments, including an\napplication for video deblurring.",
        "We study the free analogue of the classical affine fixed-point (or\nperpetuity) equation\n  \\[\n  \\mathbb{X} \\stackrel{d}{=} \\mathbb{A}^{1\/2}\\mathbb{X}\\,\\mathbb{A}^{1\/2} +\n\\mathbb{B},\n  \\] where $\\mathbb{X}$ is assumed to be $*$-free from the pair\n$(\\mathbb{A},\\mathbb{B})$, with $\\mathbb{A}\\ge 0$ and\n$\\mathbb{B}=\\mathbb{B}^*$. Our analysis covers both the subcritical regime,\nwhere $\\tau(\\mathbb{A})<1$, and the critical case $\\tau(\\mathbb{A})=1$, in\nwhich the solution $\\mathbb{X}$ is necessarily unbounded. When\n$\\tau(\\mathbb{A})=1$, we prove that the series defining $\\mathbb{X}$ converges\nbilaterally almost uniformly (and almost uniformly under additional tail\nassumptions), while the perpetuity fails to have higher moments even if all\nmoments of $\\mathbb{A}$ and $\\mathbb{B}$ exist.\n  Our approach relies on a detailed study of the asymptotic behavior of moments\nunder free multiplicative convolution, which reveals a markedly different\nbehavior from the classical setting. By employing subordination techniques for\nnon-commutative random variables, we derive precise asymptotic estimates for\nthe tail of the distributions of $\\mathbb{X}$ in both one-sided and symmetric\ncases. Interestingly, in the critical case, the free perpetuity exhibits a\npower-law tail behavior that mirrors the phenomenon observed in the celebrated\nKesten's theorem."
      ]
    }
  },
  {
    "id":2411.1726,
    "research_type":"applied",
    "start_id":"b9",
    "start_title":"Differentiation of distal ureteral stones and pelvic phleboliths using a convolutional neural network",
    "start_abstract":"Abstract The objectives were to develop and validate a Convolutional Neural Network (CNN) using local features for differentiating distal ureteral stones from pelvic phleboliths, compare the CNN method with semi-quantitative radiologists\u2019 assessments evaluate whether assessment of calcification its surroundings is sufficient discriminating phleboliths in non-contrast-enhanced CT (NECT). We retrospectively included 341 consecutive patients acute renal colic stone on NECT showing either stone, phlebolith or both. A 2.5-dimensional (2.5D-CNN) model was used, where perpendicular axial, coronal sagittal images through each used as input data CNN. trained 384 calcifications, evaluated an unseen dataset 50 phleboliths. compared by seven radiologists who reviewed 5 \u00d7 cm image stack surrounding calcification, cut-off values based attenuation volume calcifications. differentiated sensitivity, specificity accuracy 94%, 90% 92% AUC 0.95. This similar majority vote 93% significantly higher ( p = 0.03) than mean radiologist 86%. 49%. In conclusion, features. However, more are needed reach optimal discrimination.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b27"
      ],
      "title":[
        "Micro-CT data of early physiological cancellous bone formation in the lumbar spine of female C57BL\/6 mice"
      ],
      "abstract":[
        "Micro-CT provides critical data for musculoskeletal research, yielding three-dimensional datasets containing distributions of mineral density. Using high-resolution scans, we quantified changes in the fine architecture of bone in the spine of young mice. This data is made available as a reference to physiological cancellous bone growth. The scans (n\u2009=\u200919) depict the extensive structural changes typical for female C57BL\/6 mice pups, aged 1-, 3-, 7-, 10- and 14-days post-partum, as they attain the\u00a0mature geometry. We reveal the micro-morphology down to individual\u00a0trabeculae in the spine that follow phases of mineral-tissue rearrangement in the growing lumbar vertebra on a micrometer length scale. Phantom data is provided to facilitate mineral density calibration. Conventional histomorphometry matched with our micro-CT data on selected samples confirms the validity and accuracy of our 3D scans. The data may thus serve as a reference for modeling normal bone growth and can be used to benchmark other experiments assessing the effects of biomaterials, tissue growth, healing, and regeneration. Measurement(s) bone growth \u2022 bone mineralization involved in bone maturation Technology Type(s) micro-computed tomography Factor Type(s) age Sample Characteristic - Organism Mus musculus Sample Characteristic - Environment biological_process Machine-accessible metadata file describing the reported data: https:\/\/doi.org\/10.6084\/m9.figshare.14062073"
      ],
      "categories":[
        "physics.med-ph"
      ]
    },
    "list":{
      "title":[
        "Minimizing Human-Induced Variability in Quantitative Angiography for\n  Robust and Explainable AI-Based Occlusion Prediction",
        "An Adaptive Proton FLASH Therapy Using Modularized Pin Ridge Filter",
        "Positronium Lifetime Imaging with the Biograph Vision Quadra using 124I",
        "ISIT-GEN: An in silico imaging trial to assess the inter-scanner\n  generalizability of CTLESS for myocardial perfusion SPECT on defect-detection\n  task",
        "Comprehensive investigation of HYPERSCINT RP-FLASH scintillator for\n  electron FLASH research",
        "Ultrasound imaging of cortical bone: cortex geometry and measurement of\n  porosity based on wave speed for bone remodeling estimation",
        "Electrical and Mechanical Modeling of Uterine Contractions Analysis\n  Using Connectivity Methods and Graph Theory",
        "Pulsed laser diode excitation for transcranial photoacoustic imaging",
        "A Probabilistic Model of Bilateral Lymphatic Spread in Head and Neck\n  Cancer",
        "A coupled planar transmit RF array for ultrahigh field spine MR imaging",
        "3D printed human skull phantoms for transcranial photoacoustic imaging",
        "Evaluation of patient activation and dosimetry after Boron Neutron\n  Capture Therapy",
        "The New CMS Measure of Excessive Radiation Dose or Inadequate CT Image\n  Quality: Methods for Size-Adjusted Dose and Their Variabilities",
        "A variant of \\v{S}emrl's preserver theorem for singular matrices",
        "Optimal domain of Volterra operators in Korenblum spaces",
        "On general versions of the Petty projection inequality",
        "Applying the Liouville-Lanczos Method of Time-Dependent\n  Density-Functional Theory to Warm Dense Matter",
        "Representation Theorems for Convex Expectations and Semigroups on Path\n  Space",
        "Homotopical Entropy",
        "Metastability and Ostwald Step Rule in the Crystallisation of Diamond\n  and Graphite from Molten Carbon",
        "Multiwavelength Variability Analysis of the Blazar PKS 0727-11: A\n  $\\sim$168 Days Quasi-periodic Oscillation in Gamma-ray",
        "Chirality, Nonreciprocity and Symmetries for a Giant Atom",
        "The period-index problem for hyper-K\\\"ahler varieties via\n  hyperholomorphic bundles",
        "Constrained differential operators, Sobolev inequalities, and Riesz\n  potentials",
        "Benchmarking ANN extrapolations of the ground-state energies and radii\n  of Li isotopes",
        "On the Curvature and Topology of Compact Stationary Spacetimes",
        "Global Convergence and Rate Analysis of the Steepest Descent Method for\n  Uncertain Multiobjective Optimization via a Robust Optimization Approach",
        "Topological Invariants in Invasion Percolation"
      ],
      "abstract":[
        "Bias from contrast injection variability is a significant obstacle to\naccurate intracranial aneurysm occlusion prediction using quantitative\nangiography and deep neural networks . This study explores bias removal and\nexplainable AI for outcome prediction. This study used angiograms from 458\npatients with flow diverters treated IAs with six month follow up defining\nocclusion status. We minimized injection variability by deconvolving the parent\nartery input to isolate the impulse response of aneurysms, then reconvolving it\nwith a standardized injection curve. A deep neural network trained on these QA\nderived biomarkers predicted six month occlusion. Local Interpretable Model\nAgnostic Explanations identified the key imaging features influencing the\nmodel, ensuring transparency and clinical relevance.",
        "In this paper, we proposed a method to optimize adaptive proton FLASH therapy\n(ADP FLASH) using modularized pin ridge filters (pRFs) by recycling module pins\nfrom the initial plan while reducing pRF adjustments in adaptive FLASH\nplanning. Initially, single energy (250 MeV) FLASH pRF plans were created using\npencil beam directions (PBDs) from initial IMPT plans on the planning CT (pCT).\nPBDs are classified as new\/changed ($\\Delta$E > > 5 MeV) or unchanged by\ncomparing spot maps for targets between pCT and re-CT. We used an iterative\nleast square regression model to identify recyclable PBDs with minimal relative\nchanges to spot MU weighting. Two PBDs with the least square error were\nretrieved per iteration and added to the background plan, and the remaining\nPBDs were reoptimized for the adaptive plan in subsequent iterations. The\nmethod was validated on three liver SBRT cases (50 Gy in 5 fractions) by\ncomparing various dosimetric parameters across initial pRF plans on pCT, reCT\nand the ADP FLASH pRF plans on reCT. V100 for initial pRF plans on pCT, reCT,\nand ADP FLASH pRF plans for the three cases were as follows: (93.7%, 89.2%,\n91.4%), (93.5%, 60.2%, 91.7%), (97.3%, 69.9%, 98.8%). We observe a decline in\nplan quality when applying the initial pRF to the reCT, whereas the ADP FLASH\npRF approach restores quality comparable to the initial pRF on the pCT. FLASH\neffect of the initial pRF and ADP pRF plans were evaluated with a dose and dose\nrate threshold of 1Gy and 40Gy\/s, respectively, using the FLASH effectiveness\nmodel. The proposed method recycled 91.2%, 71%, and 64.7% of PBDs from initial\npRF plans for the three cases while maintaining all clinical goals and\npreserving FLASH effects across all cases.",
        "Purpose: Measuring the ortho-positronium (oPs) lifetime in human tissue bears\nthe potential of adding clinically relevant information about the tissue\nmicroenvironment to conventional positron emission tomography (PET). Through\nphantom measurements, we investigate the voxel-wise measurement of oPs lifetime\nusing a commercial long-axial field-of-view (LAFOV) PET scanner. Methods: We\nprepared four samples with mixtures of Amberlite XAD4, a porous polymeric\nadsorbent, and water and added between 1.12 MBq and 1.44 MBq of $^{124}$I. The\nsamples were scanned in two different setups: once with a couple of centimeters\nbetween each sample (15 minutes scan time) and once with all samples taped\ntogether (40 minutes scan time). For each scan, we determine the oPs lifetime\nfor the full samples and at the voxel level. The voxel sizes under\nconsideration are $10.0^3$ mm$^3$, $7.1^3$ mm$^3$ and $4.0^3$ mm$^3$. Results:\nAmberlite XAD4 allows the preparation of samples with distinct oPs lifetime.\nUsing a Bayesian fitting procedure, the oPs lifetimes in the whole samples are\n$2.52 \\pm 0.03$ ns, $2.37\\pm 0.03$ ns, $2.27\\pm0.04$ ns and $1.82\\pm 0.02$ ns,\nrespectively. The voxel-wise oPs lifetime fits showed that even with $4.0^3$\nmm$^3$ voxels the samples are clearly distinguishable and a central voxels have\ngood count statistics. However, the situation with the samples close together\nremains challenging with respect to the spatial distinction of regions with\ndifferent oPs lifetimes. Conclusion: Our study shows that positronium lifetime\nimaging on a commercial LAFOV PET\/CT should be feasible under clinical\nconditions using $^{124}$I.",
        "A recently proposed scatter-window and deep learning-based attenuation\ncompensation (AC) method for myocardial perfusion imaging (MPI) by\nsingle-photon emission computed tomography (SPECT), namely CTLESS, demonstrated\npromising performance on the clinical task of myocardial perfusion defect\ndetection with retrospective data acquired on SPECT scanners from a single\nvendor. For clinical translation of CTLESS, it is important to assess the\ngeneralizability of CTLESS across different SPECT scanners. For this purpose,\nwe conducted a virtual imaging trial, titled in silico imaging trial to assess\ngeneralizability (ISIT-GEN). ISIT-GEN assessed the generalizability of CTLESS\non the cardiac perfusion defect detection task across SPECT scanners from three\ndifferent vendors. The performance of CTLESS was compared with a\nstandard-of-care CT-based AC (CTAC) method and a no-attenuation compensation\n(NAC) method using an anthropomorphic model observer. We observed that CTLESS\nhad receiver operating characteristic (ROC) curves and area under the ROC\ncurves similar to those of CTAC. Further, CTLESS was observed to significantly\noutperform the NAC method across three scanners. These results are suggestive\nof the inter-scanner generalizability of CTLESS and motivate further clinical\nevaluations. The study also highlights the value of using in silico imaging\ntrials to assess the generalizability of deep learning-based AC methods\nfeasibly and rigorously.",
        "Accurate, dose-rate-independent, fast-response dosimeters that capture the\nspatiotemporal characteristics of ultra-high dose rate irradiation (>40Gy\/s,\nUHDR) beams are urgently needed to facilitate FLASH research and support the\nclinical translation of FLASH radiotherapy. This study evaluated the\nperformance of HYPERSCINT RP-FLASH scintillator in UHDR electron beam\ndosimetry, with sampling frequency(fs) up to 1 kHz. Four-component calibration\nand 18MeV conventional dose rate irradiation(CONV, ~0.1Gy\/s) were used to\ncalibrate the spectral characteristics of the scintillator and its\nsignal-to-dose conversion. Dosimetric accuracy for CONV and UHDR electron beams\nwas quantified against ion chamber or EBT-XD film measurements. The effects of\nbeam energy, field size, dose per pulse(DPP), pulse repetition frequency(PRF),\nand accumulated dose on the scintillator's response were investigated.\nTime-resolved dose measurements were verified using a PMT-fiber optic detector,\nwhich measures the relative output of electron pulses by scattered radiation.\nThe scintillator system achieved <0.5% accuracy compared to ion chamber\nmeasurements under 0.1-35Gy CONV irradiation at 1Hz and <3% accuracy against\nEBT-XD film in UHDR conditions up to 40Gy at 1 kHz. There was minimal energy\ndependence between 6 and 18MeV, field size dependence for 2x1-25x25cm2, DPP\ndependence for 1-2.3Gy, and PRF dependence for 30-180Hz. The scintillator\nexhibited -2.6%\/kGy signal degradation for 0-2kGy, indicating radiation damage.\nThe time-resolved dose results at 1kHz were verified within 3% accuracy\ncompared to the PMT-fiber optic detector. With proper calibration, the\nscintillator system can serve as an accurate, dose-rate-independent,\nfast-response, millisecond-resolved UHDR electron beam dosimeter, with minimal\ndependence on dose, dose rate, energy, field size, DPP, and PRF within ranges\ncommon to UHDR electron beams.",
        "Intracortical US imaging extends B-mode imaging into bone using a dedicated\nimage reconstruction algorithm that corrects for refraction at the bone-soft\ntissue interfaces. It has shown promising results in a few healthy,\npredominantly young adults, providing anatomical images of the cortex\n(periosteal and endosteal surfaces) along with estimations of US wave speed.\nHowever, its reliability in older or osteoporotic bones remains uncertain. In\nthis study, we critically assessed the performance of intracortical US imaging\nex vivo in bones with various microstructural patterns, including bones\nexhibiting signs of unbalanced intracortical remodeling. We analyzed factors\ninfluencing US image quality, particularly endosteal surface reconstruction, as\nwell as the accuracy of wave speed estimation and its relationship with\nporosity. We imaged 20 regions of interest from the femoral diaphysis of five\nelderly donors using a 2.5 MHz US transducer. The reconstructed US images were\ncompared to site-matched high-resolution micro-CT (HR-muCT) images. In samples\nwith moderate porosity, the endosteal surface was accurately identified, and\nthickness estimates from US and HR-muCT differed by less than 10%. In highly\nremodeled bones with increased porosity, the reconstructed endosteal surface\nappeared less bright and was located above the cortex region containing\nresorption cavities. We observed a decrease in US wave speed with increasing\ncortical porosity suggesting that the method could discriminate between bones\nwith low porosity (less than 5%) and those with moderate to high porosity\n(greater than ~10%). This study paves the way for the application of US imaging\nin diagnosing cortical bone health, particularly for detecting increased\ncortical porosity and reduced cortical thickness.",
        "Premature delivery is a leading cause of fetal death and morbidity, making\nthe prediction and treatment of preterm contractions critical. The\nelectrohysterographic (EHG) signal measures the electrical activity controlling\nuterine contraction. Analyzing EHG features can provide valuable insights for\nlabor detection. In this paper, we propose a framework using simulated EHG\nsignals to identify features sensitive to uterine connectivity. We focus on EHG\nsignal propagation during delivery, recorded by multiple electrodes. Simulated\nEHG signals were generated using electrical diffusion (ED) and\nmechanotransduction (EDM) to identify which connectivity methods and graph\nparameters best represent uterine synchronization. The signals were simulated\nin two scenarios: using only ED by modifying tissue resistance, and using both\nED and EDM by varying mechanotransduction model parameters. A matrix of 16\nsurface electrodes was used for the simulations. Our results show that a\nsimplified electromechanical model can monitor uterine synchronization. Feature\nselection using Fscore on real and simulated EHG signals highlighted that the\nbest features for detecting mechanotransduction shifts were H2 alone or\ncombined with Str, R2(PR), and ICOH(Str). The best features for detecting\nelectrical diffusion shifts were H2, Eff, PR, and BC.",
        "Photoacoustic (PA) imaging of deep tissue tends to employ Q-switched lasers\nwith high pulse energy to generate high optical fluence and therefore high PA\nsignal. Compared to Q-switched lasers, pulsed laser diodes (PLDs) typically\ngenerate low pulse energy. In PA imaging applications with strong acoustic\nattenuation, such as through human skull bone, the broadband PA waves generated\nby nanoseconds laser pulses are significantly reduced in bandwidth during their\npropagation to a detector. As high-frequency PA signal components are not\ntransmitted through skull, we propose to not generate them by increasing\nexcitation pulse duration. Because PLDs are mainly limited in their peak power\noutput, an increase in pulse duration linearly increases pulse energy and\ntherefore PA signal amplitude. Here we show that the optimal pulse duration for\ndeep PA sensing through thick skull bone is far higher than in typical PA\napplications. Counterintuitively, this makes PLD excitation well-suited for\ntranscranial photoacoustics. We show this in PA sensing experiments on ex vivo\nhuman skull bone.",
        "Current guidelines for elective nodal irradiation in oropharyngeal squamous\ncell carcinoma (OPSCC) recommend including large portions of the contralateral\nlymph system in the clinical target volume (CTV-N), even for lateralized tumors\nwith no clinical lymph node involvement in the contralateral neck. This study\nintroduces a probabilistic model of bilateral lymphatic tumor progression in\nOPSCC to estimate personalized risks of occult disease in specific lymph node\nlevels (LNLs) based on clinical involvement, T-stage, and tumor lateralization.\nBuilding on a previously developed hidden Markov model for ipsilateral spread,\nwe extend the approach to the contralateral neck. The model represents LNLs I,\nII, III, IV, V, and VII on both sides of the neck as binary hidden variables\n(healthy\/involved), connected via arcs representing spread probabilities. These\nprobabilities are learned using Markov chain Monte Carlo (MCMC) sampling from a\ndataset of 833 OPSCC patients, enabling the model to reflect the underlying\nlymphatic progression dynamics. The model accurately and precisely describes\nobserved patterns of involvement with a compact set of interpretable\nparameters. Midline extension of the primary tumor is identified as the primary\nrisk factor for contralateral involvement, with advanced T-stage and extensive\nipsilateral involvement further increasing risk. Occult disease in\ncontralateral LNL III is highly unlikely if upstream LNL II is clinically\nnegative, and in contralateral LNL IV, occult disease is exceedingly rare\nwithout LNL III involvement. For lateralized tumors not crossing the midline,\nthe model suggests the contralateral neck may safely be excluded from the\nCTV-N. For tumors extending across the midline but with a clinically negative\ncontralateral neck, the CTV-N could be limited to LNL II, reducing unnecessary\nexposure of normal tissue while maintaining regional tumor control.",
        "Ultrahigh-field MRI, such as those operating at 7 Tesla, enhances diagnostic\ncapabilities but also presents unique challenges, including the need for\nadvanced RF coil designs to achieve an optimal signal-to-noise ratio and\ntransmit efficiency, particularly when imaging large samples. In this work, we\nintroduce the coupled planar array, a novel technique for high-frequency,\nlarge-size RF coil design with enhanced the RF magnetic field (B1) efficiency\nand transmit performance for ultrahigh-field spine imaging applications. This\narray comprises multiple resonators that are electromagnetically coupled to\nfunction as a single multimodal resonator. The field distribution of its\nhighest frequency mode is suitable for spine imaging applications. Based on the\nnumerical modeling and calculation, a prototype of the coupled planar array was\nconstructed and its performance was evaluated through comprehensive numerical\nsimulations, rigorous RF measurements, empirical tests, and a comparison\nagainst a conventional surface coil with the same size and geometry. The\nresults of this study demonstrate that the proposed coupled planar array\nexhibits superior performance compared to conventional surface coils in terms\nof B1 efficiency for both transmit (B1+) and receive (B1-) fields, specific\nabsorption rate (SAR), and the ability to operate at high frequencies. This\nstudy suggests a promising and efficient approach to the design of\nhigh-frequency, large-size RF coils for spine MR imaging at ultrahigh magnetic\nfields.",
        "Photoacoustic (PA) waves are strongly distorted and attenuated in skull bone.\nTo study these effects on PA imaging, we designed and 3D-printed\ntissue-mimicking phantoms of human skull. We present a comparison of results in\nphantom and ex vivo skull.",
        "Boron Neutron Capture Therapy (BNCT) is a form of radiotherapy based on the\nirradiation of the tumour with a low energy neutron beam, after the\nadministration of a selective drug enriched in boron-10. The therapy exploits\nthe high cross section of thermal neutron capture in boron, generating two\nlow-range charged particles. The availability of accelerators able to generate\nhigh-intensity neutron beams via proton nuclear interaction is boosting the\nconstruction of new clinical centres. One of these is under development in\nItaly, using a 5 MeV, 30 mA proton radiofrequency accelerator coupled to a\nberyllium target, funded by the Complementary Plan to the Recovery and\nResilience National Plan, under the project ANTHEM. The present study focuses\non radiation protection aspects of patients undergoing BNCT, specifically on\nthe activation of their organs and tissues. A criterion to establish the\nrelevance of such activation after BNCT has been proposed. Based on the current\nItalian regulatory framework, the level of patient activation following BNCT\ntreatment does not pose a significant radiological concern, even shortly after\nirradiation. Another aspect is the activation of patient's excretions, which\ncan impact on the design of the building and requires a process for the\ndischarge. The described study contributes to the radiation protection study\nfor the ANTHEM BNCT centre in Italy.",
        "The Centers for Medicare & Medicaid Services (CMS) has introduced CMS1074v2,\na quality measure for computed tomography (CT) that assesses radiation dose and\nimage quality across 18 CT exam categories. This measure mandates the\ncalculation of size-adjusted dose (SAD) using patient effective diameter and\npredefined size-adjustment coefficients. However, variability in SAD\ncalculation methods raises concerns about standardization, compliance, and\nclinical applicability. This study evaluates five commonly used methods for\nestimating effective diameter and their impact on SAD determination in thoracic\nand abdominal CT protocols. A retrospective analysis of 719 CT exams was\nperformed, comparing SAD values across different calculation approaches.\nResults indicate significant variability in SAD, with attenuation-based methods\noverestimating SAD in chest exams and projection-based methods exhibiting\ngreater variability in abdominal exams. The findings highlight potential\ninconsistencies in CMS-defined dose thresholds and challenges in applying the\nmeasure across diverse patient populations and institutional imaging practices.\nAddressing these inconsistencies is critical for ensuring accurate dose\nreporting and maintaining diagnostic integrity in CT imaging.",
        "For positive integers $1 \\leq k \\leq n$ let $M_n$ be the algebra of all $n\n\\times n$ complex matrices and $M_n^{\\le k}$ its subset consisting of all\nmatrices of rank at most $k$. We first show that whenever $k>\\frac{n}{2}$, any\ncontinuous spectrum-shrinking map $\\phi : M_n^{\\le k} \\to M_n$ (i.e.\n$\\mathrm{sp}(\\phi(X)) \\subseteq \\mathrm{sp}(X)$ for all $X \\in M_n^{\\le k}$)\neither preserves characteristic polynomials or takes only nilpotent values.\nMoreover, for any $k$ there exists a real analytic embedding of $M_n^{\\le k}$\ninto the space of $n\\times n$ nilpotent matrices for all sufficiently large\n$n$. This phenomenon cannot occur when $\\phi$ is injective and either $k > n -\n\\sqrt{n}$ or the image of $\\phi$ is contained in $M_n^{\\le k}$. We then\nestablish a main result of the paper -- a variant of \\v{S}emrl's preserver\ntheorem for $M_n^{\\le k}$: if $n \\geq 3$, any injective continuous map $\\phi\n:M_n^{\\le k} \\to M_n^{\\le k}$ that preserves commutativity and shrinks spectrum\nis of the form $\\phi(\\cdot)=T(\\cdot)T^{-1}$ or $\\phi(\\cdot)=T(\\cdot)^tT^{-1}$,\nfor some invertible matrix $T\\in M_n$. Moreover, when $k=n-1$, which\ncorresponds to the set of singular $n\\times n$ matrices, this result extends to\nmaps $\\phi$ which take values in $M_n$. Finally, we discuss the\nindispensability of assumptions in our main result.",
        "The aim of this article is to study the largest domain space $[T,X]$,\nwhenever it exists, of a given continuous linear operator $T\\colon X\\to X$,\nwhere $X\\subseteq H(\\mathbb{D})$ is a Banach space of analytic functions on the\nopen unit disc $\\mathbb{D}\\subseteq \\mathbb{C}$. That is, $[T,X]\\subseteq\nH(\\mathbb{D})$ is the \\textit{largest} Banach space of analytic functions\ncontaining $X$ to which $T$ has a continuous, linear, $X$-valued extension\n$T\\colon [T,X]\\to X$. The class of operators considered consists of generalized\nVolterra operators $T$ acting in the Korenblum growth Banach spaces\n$X:=A^{-\\gamma}$, for $\\gamma>0$. Previous studies dealt with the classical\nCes\\`aro operator $T:=C$ acting in the Hardy spaces $H^p$, $1\\leq p<\\infty$,\n\\cite{CR}, \\cite{CR1}, in $A^{-\\gamma}$, \\cite{ABR-R}, and more recently,\ngeneralized Volterra operators $T$ acting in $X:=H^p$, \\cite{BDNS}.",
        "The classical Petty projection inequality is an affine isoperimetric\ninequality which constitutes a cornerstone in the affine geometry of convex\nbodies. By extending the polar projection body to an inter-dimensional\noperator, Petty's inequality was generalized to the so-called $(L_p,Q)$\nsetting, where $Q$ is an $m$-dimensional compact convex set. In this work, we\nfurther extend the $(L_p,Q)$ Petty projection inequality to the broader realm\nof rotationally invariant measures with concavity properties, namely, those\nwith $\\gamma$-concave density (for $\\gamma\\geq-1\/nm$). Moreover, when $p=1$,\nand motivated by a contemporary empirical reinterpretation of Petty's result,\nwe explore empirical analogues of this inequality.",
        "Ab initio modeling of dynamic structure factors (DSF) and related density\nresponse properties in the warm dense matter (WDM) regime is a challenging\ncomputational task. The DSF, convolved with a probing X-ray beam and instrument\nfunction, is measured in X-ray Thomson scattering (XRTS) experiments, which\nallows for the study of electronic structure properties at the microscopic\nlevel. Among the various ab initio methods, linear response time-dependent\ndensity functional theory (LR-TDDFT) is a key framework for simulating the DSF.\nThe standard approach in LR-TDDFT for computing the DSF relies on the orbital\nrepresentation. A significant drawback of this method is the unfavorable\nscaling of the number of required empty bands as the wavenumber increases,\nmaking LR-TDDFT impractical for modeling XRTS measurements over large energy\nscales, such as in backward scattering geometry. We consider and test an\nalternative approach that employs the Liouville-Lanczos (LL) method for\nsimulating the DSF. This approach does not require empty states and allows the\nDSF at large momentum transfer values and over a broad frequency range to be\naccessed. We compare the results obtained from the LL method with those from\nthe standard LR-TDDFT within the projector augmented-wave formalism for\nisochorically heated aluminum and warm dense hydrogen. Additionally, we utilize\nexact path integral Monte Carlo (PIMC) results for the imaginary-time\ndensity-density correlation function (ITCF) of warm dense hydrogen to\nrigorously benchmark the LL approach. We discuss the application of the LL\nmethod for calculating DSFs and ITCFs at different wavenumbers, the effects of\npseudopotentials, and the role of Lorentzian smearing. The successful\nvalidation of the LL method under WDM conditions makes it a valuable addition\nto the ab initio simulation landscape, supporting experimental efforts and\nadvancing WDM theory.",
        "The objective of this paper is to investigate the connection between penalty\nfunctions from stochastic optimal control, convex semigroups from analysis and\nconvex expectations from probability theory. Our main result provides a\none-to-one relation between these objects. As an application, we use the\nrepresentation via penality functions and duality arguments to show that convex\nexpectations are determined by their finite dimensional distributions. To\nillustrate this structural result, we show that Hu and Peng's axiomatic\ndescription of $G$-L\\'evy processes in terms of finite dimensional\ndistributions extends uniquely to the control approach introduced by Neufeld\nand Nutz. Finally, we show that convex expectations with a Markovian structure\nare fully determined by their one-dimensional distributions, which give rise to\na classical semigroup on the state space.",
        "We present a \"homotopification\" of fundamental concepts from information\ntheory. Using homotopy type theory, we define homotopy types that behave\nanalogously to probability spaces, random variables, and the exponentials of\nShannon entropy and relative entropy. The original analytic theories emerge\nthrough homotopy cardinality, which maps homotopy types to real numbers and\ngeneralizes the cardinality of sets.",
        "The crystallisation of carbon from the melt under extreme conditions is\nhighly relevant to earth and planetary science, materials manufacturing, and\nnuclear fusion research. The thermodynamic conditions near the\ngraphite-diamond-liquid (GDL) triple point are especially of interest for\ngeological and technological applications, but high-pressure flash heating\nexperiments aiming to resolve this region of the phase diagram of carbon\nexhibit large discrepancies. Experimental challenges are often related to the\npersistence of metastable crystalline or glassy phases, superheated crystals,\nor supercooled liquids. A deeper understanding of the crystallisation kinetics\nof diamond and graphite is crucial for effectively interpreting the outcomes of\nthese experiments. Here, we reveal the microscopic mechanisms of diamond and\ngraphite nucleation from liquid carbon through molecular simulations with\nfirst-principles machine learning potentials. Our simulations accurately\nreproduce the experimental phase diagram of carbon in the region around the GDL\ntriple point and show that liquid carbon crystallises spontaneously upon\ncooling at constant pressure. Surprisingly, metastable graphite crystallises in\nthe domain of diamond thermodynamic stability at pressures above the triple\npoint. Furthermore, whereas diamond crystallises through a classical nucleation\npathway, graphite follows a two-step process in which low-density fluctuations\nforego ordering. Calculations of the nucleation rates of the two competing\nphases confirm this result and reveal a manifestation of Ostwald's step rule\nwhere the strong metastability of graphite hinders the transformation to the\nstable diamond phase. Our results provide a new key to interpreting melting and\nrecrystallisation experiments and shed light on nucleation kinetics in\npolymorphic materials with deep metastable states.",
        "We performed variability analysis of the multiwavelength light curves for the\nflat-spectrum radio quasar PKS 0727-11. Using the generalized Lomb-Scargle\nperiodogram, we identified a possible quasi-periodic oscillation (QPO) of\n$\\sim$ 168.6 days (persisted for 6 cycles, with a significance of $3.8\\sigma$)\nin the gamma-ray light curve during the flare period (MJD 54687-55738). It is\nthe first time that periodic variations have been detected in this source, and\nfurther supported by other methods: weighted wavelet $z$-transform, phase\ndispersion minimization, REDFIT, autoregressive integrated moving average\nmodel, and structure function analysis. Cross-correlation analysis shows that\nthere is a strong correlation between multi-band light variations, indicating\nthat gamma-ray and radio flares may originate from the same disturbance, and\nthe distance between the emission regions of gamma-ray and radio flares is\ncalculated based on the time lag. We demonstrate that QPO arising from the\nnon-ballistic helical jet motion driven by the orbital motion in a supermassive\nbinary black hole is a plausible physical explanation. In this scenario, the\nestimated mass of the primary black hole is\n$M\\sim3.66\\times10^8-5.79\\times10^{9}M_\\odot$.",
        "Chiral and nonreciprocal quantum devices are crucial for signal routing and\nprocessing in a quantum network. In this work, we study the chirality and\nnonreciprocity of a giant atom coupled to a one-dimensional waveguide. We\nclarify that the chiral emission of the giant atom is not directly related to\nthe time-reversal symmetry breaking but to the mirror-symmetry breaking. We\npropose a passive scheme to realize the chiral emission of a giant atom without\nbreaking time-reversal symmetry by extending the legs of the giant atom. We\nfind the time-reversal symmetry breaking via nonuniform coupling phases is\nartificial and thus cannot result in nonreciprocal single-photon scattering for\nthe giant atom. The nonreciprocity of the giant atom can be obtained by the\nexternal dissipation of the giant atom that truly breaks the time-reversal\nsymmetry. Our work clarifies the roles of symmetries in the chirality and\nnonreciprocity of giant-atom systems and paves the way for the design of\non-chip functional devices with superconducting giant atoms.",
        "We prove new bounds for the period-index problem for hyper-K\\\"ahler varieties\nof $K3^{[n]}$-type using projectively hyperholomorphic bundles constructed by\nMarkman. We show that $\\mathrm{dim}(X)$ is a bound for any $X$ of\n$K3^{[n]}$-type. We also show that the bound can be reduced to\n$\\frac{1}{2}\\mathrm{dim}(X)$, as conjectured by Huybrechts, when the Picard\nrank of $X$ is at least 3.",
        "Inequalities for Riesz potentials are well-known to be equivalent to Sobolev\ninequalities of the same order for domain norms \"far\" from $L^1$, but to be\nweaker otherwise. Recent contributions by Van Schaftingen, by Hernandez,\nRai\\c{t}\\u{a} and Spector, and by Stolyarov proved that this gap can be filled\nin Riesz potential inequalities for vector-valued functions in $L^1$ fulfilling\na co-canceling differential condition. This work demonstrates that such a\nproperty is not just peculiar to the space $L^1$. Indeed, under the same\ndifferential constraint, a Riesz potential inequality is shown to hold for any\ndomain and target rearrangement-invariant norms that render a Sobolev\ninequality of the same order true. This is based on a new interpolation\ninequality, which, via a kind of duality argument, yields a parallel property\nof Sobolev inequalities for any linear homogeneous elliptic canceling\ndifferential operator. Specifically, Sobolev inequalities involving the full\ngradient of a certain order share the same rearrangement-invariant domain and\ntarget spaces as their analogs for any other homogeneous elliptic canceling\ndifferential operator of equal order. As a consequence, Riesz potential\ninequalities under the co-canceling constraint and Sobolev inequalities for\nhomogeneous elliptic canceling differential operators are offered for general\nfamilies of rearrangement-invariant spaces, such as the Orlicz spaces and the\nLorentz-Zygmund spaces. Especially relevant instances of inequalities for\ndomain spaces neighboring $L^1$ are singled out.",
        "We present a comparison of model-space extrapolation methods for No-Core\nShell Model calculations of ground-state energies and root-mean-square radii in\nLi isotopes. In particular, we benchmark the latest machine learning tools\nagainst widely used exponential and infrared extrapolations for energies and\ncrossing point estimates for radii. Our findings demonstrate that machine\nlearning-based approaches provide reliable predictions with robust statistical\nuncertainties for both observables even in small model spaces. These\npredictions are compatible with established exponential and IR extrapolations\nof energies and mark a notable improvement over conventional radius estimates.",
        "Using the result of Petersen $\\&$ Wink '21, we find obstructions to the\ncurvature and topology of compact Lorentzian manifolds admitting a unit-length\ntimelike Killing vector field.",
        "In this article, we extend our previous work (Applicable Analysis, 2024, pp.\n1-25) on the steepest descent method for uncertain multiobjective optimization\nproblems. While that study established local convergence, it did not address\nglobal convergence and the rate of convergence of the steepest descent\nalgorithm. To bridge this gap, we provide rigorous proofs for both global\nconvergence and the linear convergence rate of the steepest descent algorithm.\nGlobal convergence analysis strengthens the theoretical foundation of the\nsteepest descent method for uncertain multiobjective optimization problems,\noffering deeper insights into its efficiency and robustness across a broader\nclass of optimization problems. These findings enhance the method's practical\napplicability and contribute to the advancement of robust optimization\ntechniques.",
        "Based on bond percolation theory, a method is presented here to calculate the\nrelationship between capillary pressure and saturation in porous media from\nfirst principles. The governing equations are formulated on the undirected\ngraph of the pore network. The graph is a simplified mathematical object that\naccounts for the topology of the pore structure. Thus, the calculation is\nextremely computationally efficient since it is mesh-free and voxel-free. Two\ntopological invariants are identified: The bond percolation threshold and the\nresidual saturation. Bond percolation theory is used to obtain a closed-form\npressure-saturation relation in terms of the geometry of the pores (pore throat\ndistribution) and material parameters (contact angle and interfacial tension),\nuniversal exponents, and topological invariants, based on scaling relations."
      ]
    }
  },
  {
    "id":2411.1726,
    "research_type":"applied",
    "start_id":"b27",
    "start_title":"Micro-CT data of early physiological cancellous bone formation in the lumbar spine of female C57BL\/6 mice",
    "start_abstract":"Micro-CT provides critical data for musculoskeletal research, yielding three-dimensional datasets containing distributions of mineral density. Using high-resolution scans, we quantified changes in the fine architecture of bone in the spine of young mice. This data is made available as a reference to physiological cancellous bone growth. The scans (n\u2009=\u200919) depict the extensive structural changes typical for female C57BL\/6 mice pups, aged 1-, 3-, 7-, 10- and 14-days post-partum, as they attain the\u00a0mature geometry. We reveal the micro-morphology down to individual\u00a0trabeculae in the spine that follow phases of mineral-tissue rearrangement in the growing lumbar vertebra on a micrometer length scale. Phantom data is provided to facilitate mineral density calibration. Conventional histomorphometry matched with our micro-CT data on selected samples confirms the validity and accuracy of our 3D scans. The data may thus serve as a reference for modeling normal bone growth and can be used to benchmark other experiments assessing the effects of biomaterials, tissue growth, healing, and regeneration. Measurement(s) bone growth \u2022 bone mineralization involved in bone maturation Technology Type(s) micro-computed tomography Factor Type(s) age Sample Characteristic - Organism Mus musculus Sample Characteristic - Environment biological_process Machine-accessible metadata file describing the reported data: https:\/\/doi.org\/10.6084\/m9.figshare.14062073",
    "start_categories":[
      "physics.med-ph"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b9"
      ],
      "title":[
        "Differentiation of distal ureteral stones and pelvic phleboliths using a convolutional neural network"
      ],
      "abstract":[
        "Abstract The objectives were to develop and validate a Convolutional Neural Network (CNN) using local features for differentiating distal ureteral stones from pelvic phleboliths, compare the CNN method with semi-quantitative radiologists\u2019 assessments evaluate whether assessment of calcification its surroundings is sufficient discriminating phleboliths in non-contrast-enhanced CT (NECT). We retrospectively included 341 consecutive patients acute renal colic stone on NECT showing either stone, phlebolith or both. A 2.5-dimensional (2.5D-CNN) model was used, where perpendicular axial, coronal sagittal images through each used as input data CNN. trained 384 calcifications, evaluated an unseen dataset 50 phleboliths. compared by seven radiologists who reviewed 5 \u00d7 cm image stack surrounding calcification, cut-off values based attenuation volume calcifications. differentiated sensitivity, specificity accuracy 94%, 90% 92% AUC 0.95. This similar majority vote 93% significantly higher ( p = 0.03) than mean radiologist 86%. 49%. In conclusion, features. However, more are needed reach optimal discrimination."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "EdgeRegNet: Edge Feature-based Multimodal Registration Network between\n  Images and LiDAR Point Clouds",
        "Solution for 8th Competition on Affective & Behavior Analysis\n  in-the-wild",
        "EigenActor: Variant Body-Object Interaction Generation Evolved from\n  Invariant Action Basis Reasoning",
        "Unified Uncertainty-Aware Diffusion for Multi-Agent Trajectory Modeling",
        "MMHMER:Multi-viewer and Multi-task for Handwritten Mathematical\n  Expression Recognition",
        "QORT-Former: Query-optimized Real-time Transformer for Understanding Two\n  Hands Manipulating Objects",
        "DEAL: Data-Efficient Adversarial Learning for High-Quality Infrared\n  Imaging",
        "Infinite Mobility: Scalable High-Fidelity Synthesis of Articulated\n  Objects via Procedural Generation",
        "Text-Driven Diffusion Model for Sign Language Production",
        "Tuning-Free Long Video Generation via Global-Local Collaborative\n  Diffusion",
        "CeTAD: Towards Certified Toxicity-Aware Distance in Vision Language\n  Models",
        "SegAnyPET: Universal Promptable Segmentation from Positron Emission\n  Tomography Images",
        "Measuring Anxiety Levels with Head Motion Patterns in Severe Depression\n  Population",
        "Speech Enhancement Using Continuous Embeddings of Neural Audio Codec",
        "Theory-to-Practice Gap for Neural Networks and Neural Operators",
        "Quantitative Derivation of the Two-Component Gross-Pitaevskii Equation\n  with Uniform-in-Time Convergence Rate",
        "Polynomial Time Learning-Augmented Algorithms for NP-hard Permutation\n  Problems",
        "AI-Empowered Catalyst Discovery: A Survey from Classical Machine\n  Learning Approaches to Large Language Models",
        "Characterizing Data Visualization Literacy: a Systematic Literature\n  Review",
        "Geodesic Diffusion Models for Medical Image-to-Image Generation",
        "Langevin Monte-Carlo Provably Learns Depth Two Neural Nets at Any Size\n  and Data",
        "Optimisation of space-time periodic eigenvalues",
        "The least balanced graphs and trees",
        "One Stack, Diverse Vehicles: Checking Safe Portability of Automated\n  Driving Software",
        "Are Large Language Models Good In-context Learners for Financial\n  Sentiment Analysis?",
        "Diffusion on Graph: Augmentation of Graph Structure for Node\n  Classification",
        "Prophet Inequalities for Bandits, Cabinets, and DAGs",
        "Fast Debiasing of the LASSO Estimator"
      ],
      "abstract":[
        "Cross-modal data registration has long been a critical task in computer\nvision, with extensive applications in autonomous driving and robotics.\nAccurate and robust registration methods are essential for aligning data from\ndifferent modalities, forming the foundation for multimodal sensor data fusion\nand enhancing perception systems' accuracy and reliability. The registration\ntask between 2D images captured by cameras and 3D point clouds captured by\nLight Detection and Ranging (LiDAR) sensors is usually treated as a visual pose\nestimation problem. High-dimensional feature similarities from different\nmodalities are leveraged to identify pixel-point correspondences, followed by\npose estimation techniques using least squares methods. However, existing\napproaches often resort to downsampling the original point cloud and image data\ndue to computational constraints, inevitably leading to a loss in precision.\nAdditionally, high-dimensional features extracted using different feature\nextractors from various modalities require specific techniques to mitigate\ncross-modal differences for effective matching. To address these challenges, we\npropose a method that uses edge information from the original point clouds and\nimages for cross-modal registration. We retain crucial information from the\noriginal data by extracting edge points and pixels, enhancing registration\naccuracy while maintaining computational efficiency. The use of edge points and\nedge pixels allows us to introduce an attention-based feature exchange block to\neliminate cross-modal disparities. Furthermore, we incorporate an optimal\nmatching layer to improve correspondence identification. We validate the\naccuracy of our method on the KITTI and nuScenes datasets, demonstrating its\nstate-of-the-art performance.",
        "In this report, we present our solution for the Action Unit (AU) Detection\nChallenge, in 8th Competition on Affective Behavior Analysis in-the-wild. In\norder to achieve robust and accurate classification of facial action unit in\nthe wild environment, we introduce an innovative method that leverages\naudio-visual multimodal data. Our method employs ConvNeXt as the image encoder\nand uses Whisper to extract Mel spectrogram features. For these features, we\nutilize a Transformer encoder-based feature fusion module to integrate the\naffective information embedded in audio and image features. This ensures the\nprovision of rich high-dimensional feature representations for the subsequent\nmultilayer perceptron (MLP) trained on the Aff-Wild2 dataset, enhancing the\naccuracy of AU detection.",
        "This paper explores a cross-modality synthesis task that infers 3D\nhuman-object interactions (HOIs) from a given text-based instruction. Existing\ntext-to-HOI synthesis methods mainly deploy a direct mapping from texts to\nobject-specific 3D body motions, which may encounter a performance bottleneck\nsince the huge cross-modality gap. In this paper, we observe that those HOI\nsamples with the same interaction intention toward different targets, e.g.,\n\"lift a chair\" and \"lift a cup\", always encapsulate similar action-specific\nbody motion patterns while characterizing different object-specific interaction\nstyles. Thus, learning effective action-specific motion priors and\nobject-specific interaction priors is crucial for a text-to-HOI model and\ndominates its performances on text-HOI semantic consistency and body-object\ninteraction realism. In light of this, we propose a novel body pose generation\nstrategy for the text-to-HOI task: infer object-agnostic canonical body action\nfirst and then enrich object-specific interaction styles. Specifically, the\nfirst canonical body action inference stage focuses on learning intra-class\nshareable body motion priors and mapping given text-based semantics to\naction-specific canonical 3D body motions. Then, in the object-specific\ninteraction inference stage, we focus on object affordance learning and enrich\nobject-specific interaction styles on an inferred action-specific body motion\nbasis. Extensive experiments verify that our proposed text-to-HOI synthesis\nsystem significantly outperforms other SOTA methods on three large-scale\ndatasets with better semantic consistency and interaction realism performances.",
        "Multi-agent trajectory modeling has primarily focused on forecasting future\nstates, often overlooking broader tasks like trajectory completion, which are\ncrucial for real-world applications such as correcting tracking data. Existing\nmethods also generally predict agents' states without offering any state-wise\nmeasure of uncertainty. Moreover, popular multi-modal sampling methods lack any\nerror probability estimates for each generated scene under the same prior\nobservations, making it difficult to rank the predictions during inference\ntime. We introduce U2Diff, a \\textbf{unified} diffusion model designed to\nhandle trajectory completion while providing state-wise \\textbf{uncertainty}\nestimates jointly. This uncertainty estimation is achieved by augmenting the\nsimple denoising loss with the negative log-likelihood of the predicted noise\nand propagating latent space uncertainty to the real state space. Additionally,\nwe incorporate a Rank Neural Network in post-processing to enable \\textbf{error\nprobability} estimation for each generated mode, demonstrating a strong\ncorrelation with the error relative to ground truth. Our method outperforms the\nstate-of-the-art solutions in trajectory completion and forecasting across four\nchallenging sports datasets (NBA, Basketball-U, Football-U, Soccer-U),\nhighlighting the effectiveness of uncertainty and error probability estimation.\nVideo at https:\/\/youtu.be\/ngw4D4eJToE",
        "Handwritten Mathematical Expression Recognition (HMER) methods have made\nremarkable progress, with most existing HMER approaches based on either a\nhybrid CNN\/RNN-based with GRU architecture or Transformer architectures. Each\nof these has its strengths and weaknesses. Leveraging different model\nstructures as viewers and effectively integrating their diverse capabilities\npresents an intriguing avenue for exploration. This involves addressing two key\nchallenges: 1) How to fuse these two methods effectively, and 2) How to achieve\nhigher performance under an appropriate level of complexity. This paper\nproposes an efficient CNN-Transformer multi-viewer, multi-task approach to\nenhance the model's recognition performance. Our MMHMER model achieves 63.96%,\n62.51%, and 65.46% ExpRate on CROHME14, CROHME16, and CROHME19, outperforming\nPosformer with an absolute gain of 1.28%, 1.48%, and 0.58%. The main\ncontribution of our approach is that we propose a new multi-view, multi-task\nframework that can effectively integrate the strengths of CNN and Transformer.\nBy leveraging the feature extraction capabilities of CNN and the sequence\nmodeling capabilities of Transformer, our model can better handle the\ncomplexity of handwritten mathematical expressions.",
        "Significant advancements have been achieved in the realm of understanding\nposes and interactions of two hands manipulating an object. The emergence of\naugmented reality (AR) and virtual reality (VR) technologies has heightened the\ndemand for real-time performance in these applications. However, current\nstate-of-the-art models often exhibit promising results at the expense of\nsubstantial computational overhead. In this paper, we present a query-optimized\nreal-time Transformer (QORT-Former), the first Transformer-based real-time\nframework for 3D pose estimation of two hands and an object. We first limit the\nnumber of queries and decoders to meet the efficiency requirement. Given\nlimited number of queries and decoders, we propose to optimize queries which\nare taken as input to the Transformer decoder, to secure better accuracy: (1)\nwe propose to divide queries into three types (a left hand query, a right hand\nquery and an object query) and enhance query features (2) by using the contact\ninformation between hands and an object and (3) by using three-step update of\nenhanced image and query features with respect to one another. With proposed\nmethods, we achieved real-time pose estimation performance using just 108\nqueries and 1 decoder (53.5 FPS on an RTX 3090TI GPU). Surpassing\nstate-of-the-art results on the H2O dataset by 17.6% (left hand), 22.8% (right\nhand), and 27.2% (object), as well as on the FPHA dataset by 5.3% (right hand)\nand 10.4% (object), our method excels in accuracy. Additionally, it sets the\nstate-of-the-art in interaction recognition, maintaining real-time efficiency\nwith an off-the-shelf action recognition module.",
        "Thermal imaging is often compromised by dynamic, complex degradations caused\nby hardware limitations and unpredictable environmental factors. The scarcity\nof high-quality infrared data, coupled with the challenges of dynamic,\nintricate degradations, makes it difficult to recover details using existing\nmethods. In this paper, we introduce thermal degradation simulation integrated\ninto the training process via a mini-max optimization, by modeling these\ndegraded factors as adversarial attacks on thermal images. The simulation is\ndynamic to maximize objective functions, thus capturing a broad spectrum of\ndegraded data distributions. This approach enables training with limited data,\nthereby improving model performance.Additionally, we introduce a\ndual-interaction network that combines the benefits of spiking neural networks\nwith scale transformation to capture degraded features with sharp spike signal\nintensities. This architecture ensures compact model parameters while\npreserving efficient feature representation. Extensive experiments demonstrate\nthat our method not only achieves superior visual quality under diverse single\nand composited degradation, but also delivers a significant reduction in\nprocessing when trained on only fifty clear images, outperforming existing\ntechniques in efficiency and accuracy. The source code will be available at\nhttps:\/\/github.com\/LiuZhu-CV\/DEAL.",
        "Large-scale articulated objects with high quality are desperately needed for\nmultiple tasks related to embodied AI. Most existing methods for creating\narticulated objects are either data-driven or simulation based, which are\nlimited by the scale and quality of the training data or the fidelity and heavy\nlabour of the simulation. In this paper, we propose Infinite Mobility, a novel\nmethod for synthesizing high-fidelity articulated objects through procedural\ngeneration. User study and quantitative evaluation demonstrate that our method\ncan produce results that excel current state-of-the-art methods and are\ncomparable to human-annotated datasets in both physics property and mesh\nquality. Furthermore, we show that our synthetic data can be used as training\ndata for generative models, enabling next-step scaling up. Code is available at\nhttps:\/\/github.com\/Intern-Nexus\/Infinite-Mobility",
        "We introduce the hfut-lmc team's solution to the SLRTP Sign Production\nChallenge. The challenge aims to generate semantically aligned sign language\npose sequences from text inputs. To this end, we propose a Text-driven\nDiffusion Model (TDM) framework. During the training phase, TDM utilizes an\nencoder to encode text sequences and incorporates them into the diffusion model\nas conditional input to generate sign pose sequences. To guarantee the high\nquality and accuracy of the generated pose sequences, we utilize two key loss\nfunctions. The joint loss function L_{joint} is used to precisely measure and\nminimize the differences between the joint positions of the generated pose\nsequences and those of the ground truth. Similarly, the bone orientation loss\nfunction L_{bone} is instrumental in ensuring that the orientation of the bones\nin the generated poses aligns with the actual, correct orientations. In the\ninference stage, the TDM framework takes on a different yet equally important\ntask. It starts with noisy sequences and, under the strict constraints of the\ntext conditions, gradually refines and generates semantically consistent sign\nlanguage pose sequences. Our carefully designed framework performs well on the\nsign language production task, and our solution achieves a BLEU-1 score of\n20.17, placing second in the challenge.",
        "Creating high-fidelity, coherent long videos is a sought-after aspiration.\nWhile recent video diffusion models have shown promising potential, they still\ngrapple with spatiotemporal inconsistencies and high computational resource\ndemands. We propose GLC-Diffusion, a tuning-free method for long video\ngeneration. It models the long video denoising process by establishing\ndenoising trajectories through Global-Local Collaborative Denoising to ensure\noverall content consistency and temporal coherence between frames.\nAdditionally, we introduce a Noise Reinitialization strategy which combines\nlocal noise shuffling with frequency fusion to improve global content\nconsistency and visual diversity. Further, we propose a Video Motion\nConsistency Refinement (VMCR) module that computes the gradient of pixel-wise\nand frequency-wise losses to enhance visual consistency and temporal\nsmoothness. Extensive experiments, including quantitative and qualitative\nevaluations on videos of varying lengths (\\textit{e.g.}, 3\\times and 6\\times\nlonger), demonstrate that our method effectively integrates with existing video\ndiffusion models, producing coherent, high-fidelity long videos superior to\nprevious approaches.",
        "Recent advances in large vision-language models (VLMs) have demonstrated\nremarkable success across a wide range of visual understanding tasks. However,\nthe robustness of these models against jailbreak attacks remains an open\nchallenge. In this work, we propose a universal certified defence framework to\nsafeguard VLMs rigorously against potential visual jailbreak attacks. First, we\nproposed a novel distance metric to quantify semantic discrepancies between\nmalicious and intended responses, capturing subtle differences often overlooked\nby conventional cosine similarity-based measures. Then, we devise a regressed\ncertification approach that employs randomized smoothing to provide formal\nrobustness guarantees against both adversarial and structural perturbations,\neven under black-box settings. Complementing this, our feature-space defence\nintroduces noise distributions (e.g., Gaussian, Laplacian) into the latent\nembeddings to safeguard against both pixel-level and structure-level\nperturbations. Our results highlight the potential of a formally grounded,\nintegrated strategy toward building more resilient and trustworthy VLMs.",
        "Positron Emission Tomography (PET) imaging plays a crucial role in modern\nmedical diagnostics by revealing the metabolic processes within a patient's\nbody, which is essential for quantification of therapy response and monitoring\ntreatment progress. However, the segmentation of PET images presents unique\nchallenges due to their lower contrast and less distinct boundaries compared to\nother structural medical modalities. Recent developments in segmentation\nfoundation models have shown superior versatility across diverse natural image\nsegmentation tasks. Despite the efforts of medical adaptations, these works\nprimarily focus on structural medical images with detailed physiological\nstructural information and exhibit poor generalization ability when adapted to\nmolecular PET imaging. In this paper, we collect and construct PETS-5k, the\nlargest PET segmentation dataset to date, comprising 5,731 three-dimensional\nwhole-body PET images and encompassing over 1.3M 2D images. Based on the\nestablished dataset, we develop SegAnyPET, a modality-specific 3D foundation\nmodel for universal promptable segmentation from PET images. To issue the\nchallenge of discrepant annotation quality of PET images, we adopt a cross\nprompting confident learning (CPCL) strategy with an uncertainty-guided\nself-rectification process to robustly learn segmentation from high-quality\nlabeled data and low-quality noisy labeled data. Experimental results\ndemonstrate that SegAnyPET can correctly segment seen and unseen targets using\nonly one or a few prompt points, outperforming state-of-the-art foundation\nmodels and task-specific fully supervised models with higher accuracy and\nstrong generalization ability for universal segmentation. As the first\nfoundation model for PET images, we believe that SegAnyPET will advance the\napplications to various downstream tasks for molecular imaging.",
        "Depression and anxiety are prevalent mental health disorders that frequently\ncooccur, with anxiety significantly influencing both the manifestation and\ntreatment of depression. An accurate assessment of anxiety levels in\nindividuals with depression is crucial to develop effective and personalized\ntreatment plans. This study proposes a new noninvasive method for quantifying\nanxiety severity by analyzing head movements -- specifically speed,\nacceleration, and angular displacement -- during video-recorded interviews with\npatients suffering from severe depression. Using data from a new CALYPSO\nDepression Dataset, we extracted head motion characteristics and applied\nregression analysis to predict clinically evaluated anxiety levels. Our results\ndemonstrate a high level of precision, achieving a mean absolute error (MAE) of\n0.35 in predicting the severity of psychological anxiety based on head movement\npatterns. This indicates that our approach can enhance the understanding of\nanxiety's role in depression and assist psychiatrists in refining treatment\nstrategies for individuals.",
        "Recent advancements in Neural Audio Codec (NAC) models have inspired their\nuse in various speech processing tasks, including speech enhancement (SE). In\nthis work, we propose a novel, efficient SE approach by leveraging the\npre-quantization output of a pretrained NAC encoder. Unlike prior NAC-based SE\nmethods, which process discrete speech tokens using Language Models (LMs), we\nperform SE within the continuous embedding space of the pretrained NAC, which\nis highly compressed along the time dimension for efficient representation. Our\nlightweight SE model, optimized through an embedding-level loss, delivers\nresults comparable to SE baselines trained on larger datasets, with a\nsignificantly lower real-time factor of 0.005. Additionally, our method\nachieves a low GMAC of 3.94, reducing complexity 18-fold compared to Sepformer\nin a simulated cloud-based audio transmission environment. This work highlights\na new, efficient NAC-based SE solution, particularly suitable for cloud\napplications where NAC is used to compress audio before transmission.\n  Copyright 20XX IEEE. Personal use of this material is permitted. Permission\nfrom IEEE must be obtained for all other uses, in any current or future media,\nincluding reprinting\/republishing this material for advertising or promotional\npurposes, creating new collective works, for resale or redistribution to\nservers or lists, or reuse of any copyrighted component of this work in other\nworks.",
        "This work studies the sampling complexity of learning with ReLU neural\nnetworks and neural operators. For mappings belonging to relevant approximation\nspaces, we derive upper bounds on the best-possible convergence rate of any\nlearning algorithm, with respect to the number of samples. In the\nfinite-dimensional case, these bounds imply a gap between the parametric and\nsampling complexities of learning, known as the \\emph{theory-to-practice gap}.\nIn this work, a unified treatment of the theory-to-practice gap is achieved in\na general $L^p$-setting, while at the same time improving available bounds in\nthe literature. Furthermore, based on these results the theory-to-practice gap\nis extended to the infinite-dimensional setting of operator learning. Our\nresults apply to Deep Operator Networks and integral kernel-based neural\noperators, including the Fourier neural operator. We show that the\nbest-possible convergence rate in a Bochner $L^p$-norm is bounded by\nMonte-Carlo rates of order $1\/p$.",
        "We derive the time-dependent two-component Gross-Pitaevskii equation as an\neffective description of the dynamics of a dilute two-component Bose gas near\nits ground state, which exhibits a two-component mixture Bose-Einstein\ncondensate, in the Gross-Pitaevskii limit regime. Our main result establishes a\nuniform-in-time bound on the convergence rate between the many-body dynamics\nand the effective description, explicitly quantified in terms of the particle\nnumber $N$. This improves upon the works of Michelangeli and Olgliati [73, 85]\nby providing a sharper, $N$-dependent, time-independent convergence rate. Our\napproach also extends the framework of Benedikter, de Oliveira, and Schlein\n[10] to the multi-component Bose gas setting. More specifically, we develop the\nnecessary Bogoliubov theory to analyze the dynamics of multi-component Bose\ngases in the Gross-Pitaevskii regime.",
        "We consider a learning-augmented framework for NP-hard permutation problems.\nThe algorithm has access to predictions telling, given a pair $u,v$ of\nelements, whether $u$ is before $v$ or not in an optimal solution. Building on\nthe work of Braverman and Mossel (SODA 2008), we show that for a class of\noptimization problems including scheduling, network design and other graph\npermutation problems, these predictions allow to solve them in polynomial time\nwith high probability, provided that predictions are true with probability at\nleast $1\/2+\\epsilon$. Moreover, this can be achieved with a parsimonious access\nto the predictions.",
        "Catalysts are essential for accelerating chemical reactions and enhancing\nselectivity, which is crucial for the sustainable production of energy,\nmaterials, and bioactive compounds. Catalyst discovery is fundamental yet\nchallenging in computational chemistry and has garnered significant attention\ndue to the promising performance of advanced Artificial Intelligence (AI)\ntechniques. The development of Large Language Models (LLMs) notably accelerates\nprogress in the discovery of both homogeneous and heterogeneous catalysts,\nwhere their chemical reactions differ significantly in material phases,\ntemperature, dynamics, etc. However, there is currently no comprehensive survey\nthat discusses the progress and latest developments in both areas, particularly\nwith the application of LLM techniques. To address this gap, this paper\npresents a thorough and systematic survey of AI-empowered catalyst discovery,\nemploying a unified and general categorization for homogeneous and\nheterogeneous catalysts. We examine the progress of AI-empowered catalyst\ndiscovery, highlighting their individual advantages and disadvantages, and\ndiscuss the challenges faced in this field. Furthermore, we suggest potential\ndirections for future research from the perspective of computer science. Our\ngoal is to assist researchers in computational chemistry, computer science, and\nrelated fields in easily tracking the latest advancements, providing a clear\noverview and roadmap of this area. We also organize and make accessible\nrelevant resources, including article lists and datasets, in an open repository\nat\nhttps:\/\/github.com\/LuckyGirl-XU\/Awesome-Artificial-Intelligence-Empowered-Catalyst-Discovery.",
        "With the advent of the data era, and of new, more intelligent interfaces for\nsupporting decision making, there is a growing need to define, model and assess\nhuman ability and data visualizations usability for a better encoding and\ndecoding of data patterns. Data Visualization Literacy (DVL) is the ability of\nencoding and decoding data into and from a visual language. Although this\nability and its measurement are crucial for advancing human knowledge and\ndecision capacity, they have seldom been investigated, let alone\nsystematically. To address this gap, this paper presents a systematic\nliterature review comprising 43 reports on DVL, analyzed using the PRISMA\nmethodology. Our results include the identification of the purposes of DVL, its\nsatellite aspects, the models proposed, and the assessments designed to\nevaluate the degree of DVL of people. Eventually, we devise many research\ndirections including, among the most challenging, the definition of a\n(standard) unifying construct of DVL.",
        "Diffusion models transform an unknown data distribution into a Gaussian prior\nby progressively adding noise until the data become indistinguishable from pure\nnoise. This stochastic process traces a path in probability space, evolving\nfrom the original data distribution (considered as a Gaussian with near-zero\nvariance) to an isotropic Gaussian. The denoiser then learns to reverse this\nprocess, generating high-quality samples from random Gaussian noise. However,\nstandard diffusion models, such as the Denoising Diffusion Probabilistic Model\n(DDPM), do not ensure a geodesic (i.e., shortest) path in probability space.\nThis inefficiency necessitates the use of many intermediate time steps, leading\nto high computational costs in training and sampling. To address this\nlimitation, we propose the Geodesic Diffusion Model (GDM), which defines a\ngeodesic path under the Fisher-Rao metric with a variance-exploding noise\nscheduler. This formulation transforms the data distribution into a Gaussian\nprior with minimal energy, significantly improving the efficiency of diffusion\nmodels. We trained GDM by continuously sampling time steps from 0 to 1 and\nusing as few as 15 evenly spaced time steps for model sampling. We evaluated\nGDM on two medical image-to-image generation tasks: CT image denoising and MRI\nimage super-resolution. Experimental results show that GDM achieved\nstate-of-the-art performance while reducing training time by a 50-fold compared\nto DDPM and 10-fold compared to Fast-DDPM, with 66 times faster sampling than\nDDPM and a similar sampling speed to Fast-DDPM. These efficiency gains enable\nrapid model exploration and real-time clinical applications. Our code is\npublicly available at: https:\/\/github.com\/mirthAI\/GDM-VE.",
        "In this work, we will establish that the Langevin Monte-Carlo algorithm can\nlearn depth-2 neural nets of any size and for any data and we give\nnon-asymptotic convergence rates for it. We achieve this via showing that under\nTotal Variation distance and q-Renyi divergence, the iterates of Langevin Monte\nCarlo converge to the Gibbs distribution of Frobenius norm regularized losses\nfor any of these nets, when using smooth activations and in both classification\nand regression settings. Most critically, the amount of regularization needed\nfor our results is independent of the size of the net. This result combines\nseveral recent observations, like our previous papers showing that two-layer\nneural loss functions can always be regularized by a certain constant amount\nsuch that they satisfy the Villani conditions, and thus their Gibbs measures\nsatisfy a Poincare inequality.",
        "The goal of this paper is to provide a qualitative analysis of the\noptimisation of space-time periodic principal eigenvalues. Namely, considering\na fixed time horizon $T$ and the $d$-dimensional torus $\\mathbb{T}^d$, let, for\nany $m\\in L^\\infty((0,T)\\times\\mathbb{T}^d)$, $\\lambda(m)$ be the principal\neigenvalue of the operator $\\partial_t-\\Delta-m$ endowed with (time-space)\nperiodic boundary conditions. The main question we set out to answer is the\nfollowing: how to choose $m$ so as to minimise $\\lambda(m)$? This question\nstems from population dynamics. We prove that in several cases it is always\nbeneficial to rearrange $m$ with respect to time in a symmetric way, which is\nthe first comparison result for the rearrangement in time of parabolic\nequations. Furthermore, we investigate the validity (or lack thereof) of\nTalenti inequalities for the rearrangement in time of parabolic equations. The\nnumerical simulations which illustrate our results were obtained by developing\na framework within which it is possible to optimise criteria with respect to\nfunctions having a prescribed rearrangement (or distribution function).",
        "Given a connected graph, the principal eigenvector of the adjacency matrix\n(often called the Perron vector) can be used to assign positive weights to the\nvertices. A natural way to measure the homogeneousness of this vector is by\nconsidering the ratio of its $\\ell^1$ and $\\ell^2$ norms.\n  It is easy to see that the most balanced graphs in this sense (i.e., the ones\nwith the largest ratio) are the regular graphs. What about the least balanced\ngraphs with the smallest ratio? It was conjectured by R\\\"ucker, R\\\"ucker and\nGutman that, for any given $n \\geq 6$, among $n$-vertex connected graphs the\nsmallest ratio is achieved by the complete graph $K_4$ with a single path\n$P_{n-4}$ attached to one of its vertices. In this paper we confirm this\nconjecture.\n  We also verify the analogous conjecture for trees: for any given $n \\geq 8$,\namong $n$-vertex trees the smallest ratio is achieved by the star graph $S_5$\nwith a path $P_{n-5}$ attached to its central vertex.",
        "Integrating an automated driving software stack into vehicles with variable\nconfiguration is challenging, especially due to different hardware\ncharacteristics. Further, to provide software updates to a vehicle fleet in the\nfield, the functional safety of every affected configuration has to be ensured.\nThese additional demands for dependability and the increasing hardware\ndiversity in automated driving make rigorous automatic analysis essential. This\npaper addresses this challenge by using formal portability checking of adaptive\ncruise controller code for different vehicle configurations. Given a formal\nspecification of the safe behavior, models of target configurations are\nderived, which capture relevant effects of sensors, actuators and computing\nplatforms. A corresponding safe set is obtained and used to check if the\ndesired behavior is achievable on all targets. In a case study, portability\nchecking of a traditional and a neural network controller are performed\nautomatically within minutes for each vehicle hardware configuration. The check\nprovides feedback for necessary adaptations of the controllers, thus, allowing\nrapid integration and testing of software or parameter changes.",
        "Recently, large language models (LLMs) with hundreds of billions of\nparameters have demonstrated the emergent ability, surpassing traditional\nmethods in various domains even without fine-tuning over domain-specific data.\nHowever, when it comes to financial sentiment analysis (FSA)$\\unicode{x2013}$a\nfundamental task in financial AI$\\unicode{x2013}$these models often encounter\nvarious challenges, such as complex financial terminology, subjective human\nemotions, and ambiguous inclination expressions. In this paper, we aim to\nanswer the fundamental question: whether LLMs are good in-context learners for\nFSA? Unveiling this question can yield informative insights on whether LLMs can\nlearn to address the challenges by generalizing in-context demonstrations of\nfinancial document-sentiment pairs to the sentiment analysis of new documents,\ngiven that finetuning these models on finance-specific data is difficult, if\nnot impossible at all. To the best of our knowledge, this is the first paper\nexploring in-context learning for FSA that covers most modern LLMs (recently\nreleased DeepSeek V3 included) and multiple in-context sample selection\nmethods. Comprehensive experiments validate the in-context learning capability\nof LLMs for FSA.",
        "Graph diffusion models have recently been proposed to synthesize entire\ngraphs, such as molecule graphs. Although existing methods have shown great\nperformance in generating entire graphs for graph-level learning tasks, no\ngraph diffusion models have been developed to generate synthetic graph\nstructures, that is, synthetic nodes and associated edges within a given graph,\nfor node-level learning tasks. Inspired by the research in the computer vision\nliterature using synthetic data for enhanced performance, we propose Diffusion\non Graph (DoG), which generates synthetic graph structures to boost the\nperformance of GNNs. The synthetic graph structures generated by DoG are\ncombined with the original graph to form an augmented graph for the training of\nnode-level learning tasks, such as node classification and graph contrastive\nlearning (GCL). To improve the efficiency of the generation process, a Bi-Level\nNeighbor Map Decoder (BLND) is introduced in DoG. To mitigate the adverse\neffect of the noise introduced by the synthetic graph structures, a low-rank\nregularization method is proposed for the training of graph neural networks\n(GNNs) on the augmented graphs. Extensive experiments on various graph datasets\nfor semi-supervised node classification and graph contrastive learning have\nbeen conducted to demonstrate the effectiveness of DoG with low-rank\nregularization. The code of DoG is available at\nhttps:\/\/github.com\/Statistical-Deep-Learning\/DoG.",
        "A decisionmaker faces $n$ alternatives, each of which represents a potential\nreward. After investing costly resources into investigating the alternatives,\nthe decisionmaker may select one, or more generally a feasible subset, and\nobtain the associated reward(s). The objective is to maximize the sum of\nrewards minus total costs invested. We consider this problem under a general\nmodel of an alternative as a \"Markov Search Process,\" a type of undiscounted\nMarkov Decision Process on a finite acyclic graph. Even simple cases generalize\nNP-hard problems such as Pandora's Box with nonobligatory inspection.\n  Despite the apparently adaptive and interactive nature of the problem, we\nprove optimal prophet inequalities for this problem under a variety of\ncombinatorial constraints. That is, we give approximation algorithms that\ninteract with the alternatives sequentially, where each must be fully explored\nand either selected or else discarded before the next arrives. In particular,\nwe obtain a computationally efficient $\\frac{1}{2}-\\epsilon$ prophet inequality\nfor Combinatorial Markov Search subject to any matroid constraint. This result\nimplies incentive-compatible mechanisms with constant Price of Anarchy for\nserving single-parameter agents when the agents strategically conduct\nindependent, costly search processes to discover their values.",
        "In high-dimensional sparse regression, the \\textsc{Lasso} estimator offers\nexcellent theoretical guarantees but is well-known to produce biased estimates.\nTo address this, \\cite{Javanmard2014} introduced a method to ``debias\" the\n\\textsc{Lasso} estimates for a random sub-Gaussian sensing matrix\n$\\boldsymbol{A}$. Their approach relies on computing an ``approximate inverse\"\n$\\boldsymbol{M}$ of the matrix $\\boldsymbol{A}^\\top \\boldsymbol{A}\/n$ by\nsolving a convex optimization problem. This matrix $\\boldsymbol{M}$ plays a\ncritical role in mitigating bias and allowing for construction of confidence\nintervals using the debiased \\textsc{Lasso} estimates. However the computation\nof $\\boldsymbol{M}$ is expensive in practice as it requires iterative\noptimization. In the presented work, we re-parameterize the optimization\nproblem to compute a ``debiasing matrix\" $\\boldsymbol{W} :=\n\\boldsymbol{AM}^{\\top}$ directly, rather than the approximate inverse\n$\\boldsymbol{M}$. This reformulation retains the theoretical guarantees of the\ndebiased \\textsc{Lasso} estimates, as they depend on the \\emph{product}\n$\\boldsymbol{AM}^{\\top}$ rather than on $\\boldsymbol{M}$ alone. Notably, we\nprovide a simple, computationally efficient, closed-form solution for\n$\\boldsymbol{W}$ under similar conditions for the sensing matrix\n$\\boldsymbol{A}$ used in the original debiasing formulation, with an additional\ncondition that the elements of every row of $\\boldsymbol{A}$ have uncorrelated\nentries. Also, the optimization problem based on $\\boldsymbol{W}$ guarantees a\nunique optimal solution, unlike the original formulation based on\n$\\boldsymbol{M}$. We verify our main result with numerical simulations."
      ]
    }
  },
  {
    "id":2412.14846,
    "research_type":"applied",
    "start_id":"b11",
    "start_title":"Evaluation of the Impact of Magnetic Resonance Imaging (MRI) on Gross Tumor Volume (GTV) Definition for Radiation Treatment Planning (RTP) of Inoperable High Grade Gliomas (HGGs)",
    "start_abstract":"Aim and Background . Inoperable high-grade gliomas (HGGs) comprise a specific group of brain tumors portending very poor prognosis. In the absence surgical management, radiation therapy (RT) offers primary local treatment modality for inoperable HGGs. Optimal target definition planning (RTP) HGGs is difficult task given diffusely infiltrative nature disease. this context, detailed multimodality imaging information may add to accuracy in We evaluated impact Magnetic Resonance Imaging (MRI) on Gross Tumor Volume (GTV) RTP study. Materials Methods Twenty-five patients with clinical diagnosis HGG were included GTV was based Computed Tomography- (CT-) simulation images only or both CT-simulation MR images, comparative assessment performed investigate incorporation MRI into Results Median volume acquired by using use CT 65.3 (39.6 - 94.3) cc 76.1 (46.8-108.9) cc, respectively. Incorporation has resulted median increase 12.61% (6%-19%) defined only, which statistically significant (p &lt; 0.05). Conclusion improve have implications dose escalation\/intensification strategies despite need further supporting evidence.",
    "start_categories":[
      "physics.med-ph"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b9"
      ],
      "title":[
        "V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation"
      ],
      "abstract":[
        "Convolutional Neural Networks (CNNs) have been recently employed to solve problems from both the computer vision and medical image analysis fields. Despite their popularity, most approaches are only able process 2D images while data used in clinical practice consists of 3D volumes. In this work we propose an approach segmentation based on a volumetric, fully convolutional, neural network. Our CNN is trained end-to-end MRI volumes depicting prostate, learns predict for whole volume at once. We introduce novel objective function, that optimise during training, Dice coefficient. way can deal with situations where there strong imbalance between number foreground background voxels. To cope limited annotated available augment applying random non-linear transformations histogram matching. show our experimental evaluation achieves good performances challenging test requiring fraction processing time needed by other previous methods."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "STMA: A Spatio-Temporal Memory Agent for Long-Horizon Embodied Task\n  Planning",
        "LLM-driven Effective Knowledge Tracing by Integrating Dual-channel\n  Difficulty",
        "Building Knowledge Graphs Towards a Global Food Systems Datahub",
        "Intention Recognition in Real-Time Interactive Navigation Maps",
        "A Study on Educational Data Analysis and Personalized Feedback Report\n  Generation Based on Tags and ChatGPT",
        "Energy-Conscious LLM Decoding: Impact of Text Generation Strategies on\n  GPU Energy Consumption",
        "FedGAI: Federated Style Learning with Cloud-Edge Collaboration for\n  Generative AI in Fashion Design",
        "MAPoRL: Multi-Agent Post-Co-Training for Collaborative Large Language\n  Models with Reinforcement Learning",
        "Towards more Contextual Agents: An extractor-Generator Optimization\n  Framework",
        "Representation and Interpretation in Artificial and Natural Computing",
        "Assessing instructor-AI cooperation for grading essay-type questions in\n  an introductory sociology course",
        "A Guide to Bayesian Networks Software Packages for Structure and\n  Parameter Learning -- 2025 Edition",
        "R$^2$: A LLM Based Novel-to-Screenplay Generation Framework with Causal\n  Plot Graphs",
        "Bidirectional Prototype-Reward co-Evolution for Test-Time Adaptation of\n  Vision-Language Models",
        "Velocity-free task-space regulator for robot manipulators with external\n  disturbances",
        "Learning from Active Human Involvement through Proxy Value Propagation",
        "Arc2Avatar: Generating Expressive 3D Avatars from a Single Image via ID\n  Guidance",
        "EU-Nets: Enhanced, Explainable and Parsimonious U-Nets",
        "HoneypotNet: Backdoor Attacks Against Model Extraction",
        "Integral gains for non-autonomous Wazewski systems",
        "Multi-agent Auto-Bidding with Latent Graph Diffusion Models",
        "The Large-Scale Structure of Entanglement in Quantum Many-body Systems",
        "Long-Context Inference with Retrieval-Augmented Speculative Decoding",
        "Equivariant localization in Batalin-Vilkovisky formalism",
        "Insights into dendritic growth mechanisms in batteries: A combined\n  machine learning and computational study",
        "General Stability Estimates in NonLocal Traffic Models for Several\n  Populations",
        "A non-D-continuum with weakly infinite-dimensional closed\n  set-aposyndetic Whitney levels",
        "We Can't Understand AI Using our Existing Vocabulary"
      ],
      "abstract":[
        "A key objective of embodied intelligence is enabling agents to perform\nlong-horizon tasks in dynamic environments while maintaining robust\ndecision-making and adaptability. To achieve this goal, we propose the\nSpatio-Temporal Memory Agent (STMA), a novel framework designed to enhance task\nplanning and execution by integrating spatio-temporal memory. STMA is built\nupon three critical components: (1) a spatio-temporal memory module that\ncaptures historical and environmental changes in real time, (2) a dynamic\nknowledge graph that facilitates adaptive spatial reasoning, and (3) a\nplanner-critic mechanism that iteratively refines task strategies. We evaluate\nSTMA in the TextWorld environment on 32 tasks, involving multi-step planning\nand exploration under varying levels of complexity. Experimental results\ndemonstrate that STMA achieves a 31.25% improvement in success rate and a 24.7%\nincrease in average score compared to the state-of-the-art model. The results\nhighlight the effectiveness of spatio-temporal memory in advancing the memory\ncapabilities of embodied agents.",
        "Knowledge Tracing (KT) is a fundamental technology in intelligent tutoring\nsystems used to simulate changes in students' knowledge state during learning,\ntrack personalized knowledge mastery, and predict performance. However, current\nKT models face three major challenges: (1) When encountering new questions,\nmodels face cold-start problems due to sparse interaction records, making\nprecise modeling difficult; (2) Traditional models only use historical\ninteraction records for student personalization modeling, unable to accurately\ntrack individual mastery levels, resulting in unclear personalized modeling;\n(3) The decision-making process is opaque to educators, making it challenging\nfor them to understand model judgments. To address these challenges, we propose\na novel Dual-channel Difficulty-aware Knowledge Tracing (DDKT) framework that\nutilizes Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG)\nfor subjective difficulty assessment, while integrating difficulty bias-aware\nalgorithms and student mastery algorithms for precise difficulty measurement.\nOur framework introduces three key innovations: (1) Difficulty Balance\nPerception Sequence (DBPS) - students' subjective perceptions combined with\nobjective difficulty, measuring gaps between LLM-assessed difficulty,\nmathematical-statistical difficulty, and students' subjective perceived\ndifficulty through attention mechanisms; (2) Difficulty Mastery Ratio (DMR) -\nprecise modeling of student mastery levels through different difficulty zones;\n(3) Knowledge State Update Mechanism - implementing personalized knowledge\nacquisition through gated networks and updating student knowledge state.\nExperimental results on two real datasets show our method consistently\noutperforms nine baseline models, improving AUC metrics by 2% to 10% while\neffectively addressing cold-start problems and enhancing model\ninterpretability.",
        "Sustainable agricultural production aligns with several sustainability goals\nestablished by the United Nations (UN). However, there is a lack of studies\nthat comprehensively examine sustainable agricultural practices across various\nproducts and production methods. Such research could provide valuable insights\ninto the diverse factors influencing the sustainability of specific crops and\nproduce while also identifying practices and conditions that are universally\napplicable to all forms of agricultural production. While this research might\nhelp us better understand sustainability, the community would still need a\nconsistent set of vocabularies. These consistent vocabularies, which represent\nthe underlying datasets, can then be stored in a global food systems datahub.\nThe standardized vocabularies might help encode important information for\nfurther statistical analyses and AI\/ML approaches in the datasets, resulting in\nthe research targeting sustainable agricultural production. A structured method\nof representing information in sustainability, especially for wheat production,\nis currently unavailable. In an attempt to address this gap, we are building a\nset of ontologies and Knowledge Graphs (KGs) that encode knowledge associated\nwith sustainable wheat production using formal logic. The data for this set of\nknowledge graphs are collected from public data sources, experimental results\ncollected at our experiments at Kansas State University, and a Sustainability\nWorkshop that we organized earlier in the year, which helped us collect input\nfrom different stakeholders throughout the value chain of wheat. The modeling\nof the ontology (i.e., the schema) for the Knowledge Graph has been in progress\nwith the help of our domain experts, following a modular structure using KNARM\nmethodology. In this paper, we will present our preliminary results and schemas\nof our Knowledge Graph and ontologies.",
        "In this demonstration, we develop IntentRec4Maps, a system to recognise\nusers' intentions in interactive maps for real-world navigation. IntentRec4Maps\nuses the Google Maps Platform as the real-world interactive map, and a very\neffective approach for recognising users' intentions in real-time. We showcase\nthe recognition process of IntentRec4Maps using two different Path-Planners and\na Large Language Model (LLM).\n  GitHub: https:\/\/github.com\/PeijieZ\/IntentRec4Maps",
        "This study introduces a novel method that employs tag annotation coupled with\nthe ChatGPT language model to analyze student learning behaviors and generate\npersonalized feedback. Central to this approach is the conversion of complex\nstudent data into an extensive set of tags, which are then decoded through\ntailored prompts to deliver constructive feedback that encourages rather than\ndiscourages students. This methodology focuses on accurately feeding student\ndata into large language models and crafting prompts that enhance the\nconstructive nature of feedback. The effectiveness of this approach was\nvalidated through surveys conducted with over 20 mathematics teachers, who\nconfirmed the reliability of the generated reports. This method can be\nseamlessly integrated into intelligent adaptive learning systems or provided as\na tool to significantly reduce the workload of teachers, providing accurate and\ntimely feedback to students. By transforming raw educational data into\ninterpretable tags, this method supports the provision of efficient and timely\npersonalized learning feedback that offers constructive suggestions tailored to\nindividual learner needs.",
        "Decoding strategies significantly influence the quality and diversity of the\ngenerated texts in large language models (LLMs), yet their impact on\ncomputational resource consumption, particularly GPU energy usage, is\ninsufficiently studied. This paper investigates the relationship between text\ngeneration decoding methods and energy efficiency, focusing on the trade-off\nbetween generation quality and GPU energy consumption across diverse tasks and\ndecoding configurations. By benchmarking multiple strategies across different\ntext generation tasks, such as Translation, Code Summarization, and Math\nProblem Solving, we reveal how selecting appropriate decoding techniques with\ntheir tuned hyperparameters affects text quality and has measurable\nimplications for resource utilization, emphasizing the need for balanced\noptimization. To the best of our knowledge, this study is among the first to\nexplore decoding strategies in LLMs through the lens of energy consumption,\noffering actionable insights for designing resource-aware applications that\nmaintain high-quality text generation.",
        "Collaboration can amalgamate diverse ideas, styles, and visual elements,\nfostering creativity and innovation among different designers. In collaborative\ndesign, sketches play a pivotal role as a means of expressing design\ncreativity. However, designers often tend to not openly share these\nmeticulously crafted sketches. This phenomenon of data island in the design\narea hinders its digital transformation under the third wave of AI. In this\npaper, we introduce a Federated Generative Artificial Intelligence Clothing\nsystem, namely FedGAI, employing federated learning to aid in sketch design.\nFedGAI is committed to establishing an ecosystem wherein designers can exchange\nsketch styles among themselves. Through FedGAI, designers can generate sketches\nthat incorporate various designers' styles from their peers, drawing\ninspiration from collaboration without the need for data disclosure or upload.\nExtensive performance evaluations indicate that our FedGAI system can produce\nmulti-styled sketches of comparable quality to human-designed ones while\nsignificantly enhancing efficiency compared to hand-drawn sketches.",
        "Leveraging multiple large language models (LLMs) to build collaborative\nmulti-agentic workflows has demonstrated significant potential. However, most\nprevious studies focus on prompting the out-of-the-box LLMs, relying on their\ninnate capability for collaboration, which may not improve LLMs' performance as\nshown recently. In this paper, we introduce a new post-training paradigm MAPoRL\n(Multi-Agent Post-co-training for collaborative LLMs with Reinforcement\nLearning), to explicitly elicit the collaborative behaviors and further unleash\nthe power of multi-agentic LLM frameworks. In MAPoRL, multiple LLMs first\ngenerate their own responses independently and engage in a multi-turn\ndiscussion to collaboratively improve the final answer. In the end, a MAPoRL\nverifier evaluates both the answer and the discussion, by assigning a score\nthat verifies the correctness of the answer, while adding incentives to\nencourage corrective and persuasive discussions. The score serves as the\nco-training reward, and is then maximized through multi-agent RL. Unlike\nexisting LLM post-training paradigms, MAPoRL advocates the co-training of\nmultiple LLMs together using RL for better generalization. Accompanied by\nanalytical insights, our experiments demonstrate that training individual LLMs\nalone is insufficient to induce effective collaboration. In contrast,\nmulti-agent co-training can boost the collaboration performance across\nbenchmarks, with generalization to unseen domains.",
        "Large Language Model (LLM)-based agents have demonstrated remarkable success\nin solving complex tasks across a wide range of general-purpose applications.\nHowever, their performance often degrades in context-specific scenarios, such\nas specialized industries or research domains, where the absence of\ndomain-relevant knowledge leads to imprecise or suboptimal outcomes. To address\nthis challenge, our work introduces a systematic approach to enhance the\ncontextual adaptability of LLM-based agents by optimizing their underlying\nprompts-critical components that govern agent behavior, roles, and\ninteractions. Manually crafting optimized prompts for context-specific tasks is\nlabor-intensive, error-prone, and lacks scalability. In this work, we introduce\nan Extractor-Generator framework designed to automate the optimization of\ncontextual LLM-based agents. Our method operates through two key stages: (i)\nfeature extraction from a dataset of gold-standard input-output examples, and\n(ii) prompt generation via a high-level optimization strategy that iteratively\nidentifies underperforming cases and applies self-improvement techniques. This\nframework substantially improves prompt adaptability by enabling more precise\ngeneralization across diverse inputs, particularly in context-specific tasks\nwhere maintaining semantic consistency and minimizing error propagation are\ncritical for reliable performance. Although developed with single-stage\nworkflows in mind, the approach naturally extends to multi-stage workflows,\noffering broad applicability across various agent-based systems. Empirical\nevaluations demonstrate that our framework significantly enhances the\nperformance of prompt-optimized agents, providing a structured and efficient\napproach to contextual LLM-based agents.",
        "Artificial computing machinery transforms representations through an\nobjective process, to be interpreted subjectively by humans, so the machine and\nthe interpreter are different entities, but in the putative natural computing\nboth processes are performed by the same agent. The method or process that\ntransforms a representation is called here \\emph{the mode of computing}. The\nmode used by digital computers is the algorithmic one, but there are others,\nsuch as quantum computers and diverse forms of non-conventional computing, and\nthere is an open-ended set of representational formats and modes that could be\nused in artificial and natural computing. A mode based on a notion of computing\ndifferent from Turing's may perform feats beyond what the Turing Machine does\nbut the modes would not be of the same kind and could not be compared. For a\nmode of computing to be more powerful than the algorithmic one, it ought to\ncompute functions lacking an effective algorithm, and Church Thesis would not\nhold. Here, a thought experiment including a computational demon using a\nhypothetical mode for such an effect is presented. If there is natural\ncomputing, there is a mode of natural computing whose properties may be causal\nto the phenomenological experience. Discovering it would come with solving the\nhard problem of consciousness; but if it turns out that such a mode does not\nexist, there is no such thing as natural computing, and the mind is not a\ncomputational process.",
        "This study explores the use of artificial intelligence (AI) as a\ncomplementary tool for grading essay-type questions in higher education,\nfocusing on its consistency with human grading and potential to reduce biases.\nUsing 70 handwritten exams from an introductory sociology course, we evaluated\ngenerative pre-trained transformers (GPT) models' performance in transcribing\nand scoring students' responses. GPT models were tested under various settings\nfor both transcription and grading tasks. Results show high similarity between\nhuman and GPT transcriptions, with GPT-4o-mini outperforming GPT-4o in\naccuracy. For grading, GPT demonstrated strong correlations with the human\ngrader scores, especially when template answers were provided. However,\ndiscrepancies remained, highlighting GPT's role as a \"second grader\" to flag\ninconsistencies for assessment reviewing rather than fully replace human\nevaluation. This study contributes to the growing literature on AI in\neducation, demonstrating its potential to enhance fairness and efficiency in\ngrading essay-type questions.",
        "A representation of the cause-effect mechanism is needed to enable artificial\nintelligence to represent how the world works. Bayesian Networks (BNs) have\nproven to be an effective and versatile tool for this task. BNs require\nconstructing a structure of dependencies among variables and learning the\nparameters that govern these relationships. These tasks, referred to as\nstructural learning and parameter learning, are actively investigated by the\nresearch community, with several algorithms proposed and no single method\nhaving established itself as standard. A wide range of software, tools, and\npackages have been developed for BNs analysis and made available to academic\nresearchers and industry practitioners. As a consequence of having no\none-size-fits-all solution, moving the first practical steps and getting\noriented into this field is proving to be challenging to outsiders and\nbeginners. In this paper, we review the most relevant tools and software for\nBNs structural and parameter learning to date, providing our subjective\nrecommendations directed to an audience of beginners. In addition, we provide\nan extensive easy-to-consult overview table summarizing all software packages\nand their main features. By improving the reader understanding of which\navailable software might best suit their needs, we improve accessibility to the\nfield and make it easier for beginners to take their first step into it.",
        "Automatically adapting novels into screenplays is important for the TV, film,\nor opera industries to promote products with low costs. The strong performances\nof large language models (LLMs) in long-text generation call us to propose a\nLLM based framework Reader-Rewriter (R$^2$) for this task. However, there are\ntwo fundamental challenges here. First, the LLM hallucinations may cause\ninconsistent plot extraction and screenplay generation. Second, the\ncausality-embedded plot lines should be effectively extracted for coherent\nrewriting. Therefore, two corresponding tactics are proposed: 1) A\nhallucination-aware refinement method (HAR) to iteratively discover and\neliminate the affections of hallucinations; and 2) a causal plot-graph\nconstruction method (CPC) based on a greedy cycle-breaking algorithm to\nefficiently construct plot lines with event causalities. Recruiting those\nefficient techniques, R$^2$ utilizes two modules to mimic the human screenplay\nrewriting process: The Reader module adopts a sliding window and CPC to build\nthe causal plot graphs, while the Rewriter module generates first the scene\noutlines based on the graphs and then the screenplays. HAR is integrated into\nboth modules for accurate inferences of LLMs. Experimental results demonstrate\nthe superiority of R$^2$, which substantially outperforms three existing\napproaches (51.3%, 22.6%, and 57.1% absolute increases) in pairwise comparison\nat the overall win rate for GPT-4o.",
        "Test-time adaptation (TTA) is crucial in maintaining Vision-Language Models\n(VLMs) performance when facing real-world distribution shifts, particularly\nwhen the source data or target labels are inaccessible. Existing TTA methods\nrely on CLIP's output probability distribution for feature evaluation, which\ncan introduce biases under domain shifts. This misalignment may cause features\nto be misclassified due to text priors or incorrect textual associations. To\naddress these limitations, we propose Bidirectional Prototype-Reward\nco-Evolution (BPRE), a novel TTA framework for VLMs that integrates feature\nquality assessment with prototype evolution through a synergistic feedback\nloop. BPRE first employs a Multi-Dimensional Quality-Aware Reward Module to\nevaluate feature quality and guide prototype refinement precisely. The\ncontinuous refinement of prototype quality through Prototype-Reward Interactive\nEvolution will subsequently enhance the computation of more robust\nMulti-Dimensional Quality-Aware Reward Scores. Through the bidirectional\ninteraction, the precision of rewards and the evolution of prototypes mutually\nreinforce each other, forming a self-evolving cycle. Extensive experiments are\nconducted across 15 diverse recognition datasets encompassing natural\ndistribution shifts and cross-dataset generalization scenarios. Results\ndemonstrate that BPRE consistently achieves superior average performance\ncompared to state-of-the-art methods across different model architectures, such\nas ResNet-50 and ViT-B\/16. By emphasizing comprehensive feature evaluation and\nbidirectional knowledge refinement, BPRE advances VLM generalization\ncapabilities, offering a new perspective on TTA.",
        "This paper addresses the problem of task-space robust regulation of robot\nmanipulators subject to external disturbances. A velocity-free control law is\nproposed by combining the internal model principle and the passivity-based\noutput-feedback control approach. The developed output-feedback controller\nensures not only asymptotic convergence of the regulation error but also\nsuppression of unwanted external step\/sinusoidal disturbances. The potential of\nthe proposed method lies in its simplicity, intuitively appealing, and simple\ngain selection criteria for synthesis of multi-joint robot manipulator control\nsystems.",
        "Learning from active human involvement enables the human subject to actively\nintervene and demonstrate to the AI agent during training. The interaction and\ncorrective feedback from human brings safety and AI alignment to the learning\nprocess. In this work, we propose a new reward-free active human involvement\nmethod called Proxy Value Propagation for policy optimization. Our key insight\nis that a proxy value function can be designed to express human intents,\nwherein state-action pairs in the human demonstration are labeled with high\nvalues, while those agents' actions that are intervened receive low values.\nThrough the TD-learning framework, labeled values of demonstrated state-action\npairs are further propagated to other unlabeled data generated from agents'\nexploration. The proxy value function thus induces a policy that faithfully\nemulates human behaviors. Human-in-the-loop experiments show the generality and\nefficiency of our method. With minimal modification to existing reinforcement\nlearning algorithms, our method can learn to solve continuous and discrete\ncontrol tasks with various human control devices, including the challenging\ntask of driving in Grand Theft Auto V. Demo video and code are available at:\nhttps:\/\/metadriverse.github.io\/pvp",
        "Inspired by the effectiveness of 3D Gaussian Splatting (3DGS) in\nreconstructing detailed 3D scenes within multi-view setups and the emergence of\nlarge 2D human foundation models, we introduce Arc2Avatar, the first SDS-based\nmethod utilizing a human face foundation model as guidance with just a single\nimage as input. To achieve that, we extend such a model for diverse-view human\nhead generation by fine-tuning on synthetic data and modifying its\nconditioning. Our avatars maintain a dense correspondence with a human face\nmesh template, allowing blendshape-based expression generation. This is\nachieved through a modified 3DGS approach, connectivity regularizers, and a\nstrategic initialization tailored for our task. Additionally, we propose an\noptional efficient SDS-based correction step to refine the blendshape\nexpressions, enhancing realism and diversity. Experiments demonstrate that\nArc2Avatar achieves state-of-the-art realism and identity preservation,\neffectively addressing color issues by allowing the use of very low guidance,\nenabled by our strong identity prior and initialization strategy, without\ncompromising detail. Please visit https:\/\/arc2avatar.github.io for more\nresources.",
        "In this study, we propose MHEX+, a framework adaptable to any U-Net\narchitecture. Built upon MHEX+, we introduce novel U-Net variants, EU-Nets,\nwhich enhance explainability and uncertainty estimation, addressing the\nlimitations of traditional U-Net models while improving performance and\nstability. A key innovation is the Equivalent Convolutional Kernel, which\nunifies consecutive convolutional layers, boosting interpretability. For\nuncertainty estimation, we propose the collaboration gradient approach,\nmeasuring gradient consistency across decoder layers. Notably, EU-Nets achieve\nan average accuracy improvement of 1.389\\% and a variance reduction of 0.83\\%\nacross all networks and datasets in our experiments, requiring fewer than 0.1M\nparameters.",
        "Model extraction attacks are one type of inference-time attacks that\napproximate the functionality and performance of a black-box victim model by\nlaunching a certain number of queries to the model and then leveraging the\nmodel's predictions to train a substitute model. These attacks pose severe\nsecurity threats to production models and MLaaS platforms and could cause\nsignificant monetary losses to the model owners. A body of work has proposed to\ndefend machine learning models against model extraction attacks, including both\nactive defense methods that modify the model's outputs or increase the query\noverhead to avoid extraction and passive defense methods that detect malicious\nqueries or leverage watermarks to perform post-verification. In this work, we\nintroduce a new defense paradigm called attack as defense which modifies the\nmodel's output to be poisonous such that any malicious users that attempt to\nuse the output to train a substitute model will be poisoned. To this end, we\npropose a novel lightweight backdoor attack method dubbed HoneypotNet that\nreplaces the classification layer of the victim model with a honeypot layer and\nthen fine-tunes the honeypot layer with a shadow model (to simulate model\nextraction) via bi-level optimization to modify its output to be poisonous\nwhile remaining the original performance. We empirically demonstrate on four\ncommonly used benchmark datasets that HoneypotNet can inject backdoors into\nsubstitute models with a high success rate. The injected backdoor not only\nfacilitates ownership verification but also disrupts the functionality of\nsubstitute models, serving as a significant deterrent to model extraction\nattacks.",
        "In this work we consider linear non-autonomous systems of Wazewski type on\nHilbert spaces and provide a new approach to study their stability properties\nby means of a decomposition into subsystems and conditions implied on the\ninterconnection properties. These conditions are of the small-gain type but the\nappoach is based on a conceptually new notion which we call integral gain. This\nnotion is introduced for the first time in this paper. We compare our approach\nwith known results from the literature and demonstrate advantages of our\nresults.",
        "This paper proposes a diffusion-based auto-bidding framework that leverages\ngraph representations to model large-scale auction environments. In such\nsettings, agents must dynamically optimize bidding strategies under constraints\ndefined by key performance indicator (KPI) metrics, all while operating in\ncompetitive environments characterized by uncertain, sparse, and stochastic\nvariables. To address these challenges, we introduce a novel approach combining\nlearnable graph-based embeddings with a planning-based latent diffusion model\n(LDM). By capturing patterns and nuances underlying the interdependence of\nimpression opportunities and the multi-agent dynamics of the auction\nenvironment, the graph representation enable expressive computations regarding\nauto-bidding outcomes. With reward alignment techniques, the LDM's posterior is\nfine-tuned to generate auto-bidding trajectories that maximize KPI metrics\nwhile satisfying constraint thresholds. Empirical evaluations on both\nreal-world and synthetic auction environments demonstrate significant\nimprovements in auto-bidding performance across multiple common KPI metrics, as\nwell as accuracy in forecasting auction outcomes.",
        "We show that the thermodynamic limit of a many-body system can reveal\nentanglement properties that are hard to detect in finite-size systems --\nsimilar to how phase transitions only sharply emerge in the thermodynamic\nlimit. The resulting operational entanglement properties are in one-to-one\ncorrespondence with abstract properties of the local observable algebras that\nemerge in the thermodynamic limit. These properties are insensitive to finite\nperturbations and hence describe the \\emph{large-scale structure of\nentanglement} of many-body systems. We formulate and discuss the emerging\nstructures and open questions, both for gapped and gapless many-body systems.\nIn particular, we show that every gapped phase of matter, even the trivial one,\nin $D\\geq 2$ dimensions contains models with the strongest possible bipartite\nlarge-scale entanglement. Conversely, we conjecture the existence of\ntopological phases of matter, where all representatives have the strongest form\nof entanglement.",
        "The emergence of long-context large language models (LLMs) offers a promising\nalternative to traditional retrieval-augmented generation (RAG) for processing\nextensive documents. However, the computational overhead of long-context\ninference, particularly in managing key-value (KV) caches, presents significant\nefficiency challenges. While Speculative Decoding (SD) traditionally\naccelerates inference using smaller draft models, its effectiveness diminishes\nsubstantially in long-context scenarios due to memory-bound KV cache\noperations. We present Retrieval-Augmented Speculative Decoding (RAPID), which\nleverages RAG for both accelerating and enhancing generation quality in\nlong-context inference. RAPID introduces the RAG drafter-a draft LLM operating\non shortened retrieval contexts-to speculate on the generation of long-context\ntarget LLMs. Our approach enables a new paradigm where same-scale or even\nlarger LLMs can serve as RAG drafters while maintaining computational\nefficiency. To fully leverage the potentially superior capabilities from\nstronger RAG drafters, we develop an inference-time knowledge transfer dynamic\nthat enriches the target distribution by RAG. Extensive experiments on the\nLLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID effectively integrates\nthe strengths of both approaches, achieving significant performance\nimprovements (e.g., from 39.33 to 42.83 on InfiniteBench for LLaMA-3.1-8B) with\nmore than 2x speedups. Our analyses reveal that RAPID achieves robust\nacceleration beyond 32K context length and demonstrates superior generation\nquality in real-world applications.",
        "We derive equivariant localization formulas of Atiyah--Bott and cohomological\nfield theory types in the Batalin-Vilkovisky formalism and discuss their\napplications in Poisson geometry and quantum field theory.",
        "In recent years, researchers have increasingly sought batteries as an\nefficient and cost-effective solution for energy storage and supply, owing to\ntheir high energy density, low cost, and environmental resilience. However, the\nissue of dendrite growth has emerged as a significant obstacle in battery\ndevelopment. Excessive dendrite growth during charging and discharging\nprocesses can lead to battery short-circuiting, degradation of electrochemical\nperformance, reduced cycle life, and abnormal exothermic events. Consequently,\nunderstanding the dendrite growth process has become a key challenge for\nresearchers. In this study, we investigated dendrite growth mechanisms in\nbatteries using a combined machine learning approach, specifically a\ntwo-dimensional artificial convolutional neural network (CNN) model, along with\ncomputational methods. We developed two distinct computer models to predict\ndendrite growth in batteries. The CNN-1 model employs standard convolutional\nneural network techniques for dendritic growth prediction, while CNN-2\nintegrates additional physical parameters to enhance model robustness. Our\nresults demonstrate that CNN-2 significantly enhances prediction accuracy,\noffering deeper insights into the impact of physical factors on dendritic\ngrowth. This improved model effectively captures the dynamic nature of dendrite\nformation, exhibiting high accuracy and sensitivity. These findings contribute\nto the advancement of safer and more reliable energy storage systems.",
        "We prove global existence, uniqueness and $\\L1$ stability of solutions to\ngeneral systems of nonlocal conservation laws modeling multiclass vehicular\ntraffic. Each class follows its own speed law and has specific effects on the\nother classes' speeds. Moreover, general explicit dependencies of the speed\nlaws on space and time are allowed. Solutions are proved to depend continuously\n-- in suitable norms -- on all terms appearing in the equations, as well as on\nthe initial data. Numerical simulations show the relevance and the effects of\nthe nonlocal terms.",
        "In this paper, we introduce the new class of continua; weakly\ninfinite-dimensional closed set-aposyndetic continua. With this notion, we show\nthat there exists a non-D-continuum such that each positive Whitney level of\nthe hyperspace of the continuum is a weakly infinite-dimensional closed\nset-aposyndetic continuum. This result strengthens those of van Douwen and\nGoodykoontz [2], Illanes [7], and the main result of Illanes et al. [9].",
        "This position paper argues that, in order to understand AI, we cannot rely on\nour existing vocabulary of human words. Instead, we should strive to develop\nneologisms: new words that represent precise human concepts that we want to\nteach machines, or machine concepts that we need to learn. We start from the\npremise that humans and machines have differing concepts. This means\ninterpretability can be framed as a communication problem: humans must be able\nto reference and control machine concepts, and communicate human concepts to\nmachines. Creating a shared human-machine language through developing\nneologisms, we believe, could solve this communication problem. Successful\nneologisms achieve a useful amount of abstraction: not too detailed, so they're\nreusable in many contexts, and not too high-level, so they convey precise\ninformation. As a proof of concept, we demonstrate how a \"length neologism\"\nenables controlling LLM response length, while a \"diversity neologism\" allows\nsampling more variable responses. Taken together, we argue that we cannot\nunderstand AI using our existing vocabulary, and expanding it through\nneologisms creates opportunities for both controlling and understanding\nmachines better."
      ]
    }
  },
  {
    "id":2412.14846,
    "research_type":"applied",
    "start_id":"b9",
    "start_title":"V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation",
    "start_abstract":"Convolutional Neural Networks (CNNs) have been recently employed to solve problems from both the computer vision and medical image analysis fields. Despite their popularity, most approaches are only able process 2D images while data used in clinical practice consists of 3D volumes. In this work we propose an approach segmentation based on a volumetric, fully convolutional, neural network. Our CNN is trained end-to-end MRI volumes depicting prostate, learns predict for whole volume at once. We introduce novel objective function, that optimise during training, Dice coefficient. way can deal with situations where there strong imbalance between number foreground background voxels. To cope limited annotated available augment applying random non-linear transformations histogram matching. show our experimental evaluation achieves good performances challenging test requiring fraction processing time needed by other previous methods.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b11"
      ],
      "title":[
        "Evaluation of the Impact of Magnetic Resonance Imaging (MRI) on Gross Tumor Volume (GTV) Definition for Radiation Treatment Planning (RTP) of Inoperable High Grade Gliomas (HGGs)"
      ],
      "abstract":[
        "Aim and Background . Inoperable high-grade gliomas (HGGs) comprise a specific group of brain tumors portending very poor prognosis. In the absence surgical management, radiation therapy (RT) offers primary local treatment modality for inoperable HGGs. Optimal target definition planning (RTP) HGGs is difficult task given diffusely infiltrative nature disease. this context, detailed multimodality imaging information may add to accuracy in We evaluated impact Magnetic Resonance Imaging (MRI) on Gross Tumor Volume (GTV) RTP study. Materials Methods Twenty-five patients with clinical diagnosis HGG were included GTV was based Computed Tomography- (CT-) simulation images only or both CT-simulation MR images, comparative assessment performed investigate incorporation MRI into Results Median volume acquired by using use CT 65.3 (39.6 - 94.3) cc 76.1 (46.8-108.9) cc, respectively. Incorporation has resulted median increase 12.61% (6%-19%) defined only, which statistically significant (p &lt; 0.05). Conclusion improve have implications dose escalation\/intensification strategies despite need further supporting evidence."
      ],
      "categories":[
        "physics.med-ph"
      ]
    },
    "list":{
      "title":[
        "Assessing ultrasonic and optical flow velocimetry in a millifluidic\n  device using oil-in-water emulsions as blood mimicking fluid",
        "Myocardial T1 mapping at 5T using multi-inversion recovery real-time\n  spoiled GRE",
        "Non-Invasive Temporal Interference Electrical Stimulation for Spinal\n  Cord Injury Rehabilitation: A Simulation Study",
        "Fabrication of Fibers with Complex Features Using Thermal Drawing of\n  3D-Printed Preforms",
        "Ultrafast Proton Delivery with Pin Ridge Filters (pRFs): A Novel\n  Approach for Motion Management in Proton Therapy",
        "Numerical Analysis of Antenna Parameter Influence on Brightness\n  Temperature in Medical Microwave Radiometers",
        "Analysis of the sensitivity of tumor control probability in molecular\n  radiotherapy to uncertainties in the dose rate curves",
        "Compound Mask for Divergent Wave Imaging in Medical Ultrasound",
        "Neutron relative effectiveness factors in Boron Neutron Capture Therapy:\n  estimation of their values from the secondary charged particles and\n  evaluation of weighted kerma factors for a standard tissue",
        "Developing an Agent-Based Mathematical Model for Simulating\n  Post-Irradiation Cellular Response: A Crucial Component of a Digital Twin\n  Framework for Personalized Radiation Treatment",
        "A self-contact electromechanical framework for intestinal motility",
        "Advancing Tumor Budding Detection with Fourier Ptychography Microscopy",
        "A Conditional Point Cloud Diffusion Model for Deformable Liver Motion\n  Tracking Via a Single Arbitrarily-Angled X-ray Projection",
        "Efficient sampling approaches based on generalized Golub-Kahan methods\n  for large-scale hierarchical Bayesian inverse problems",
        "Optimal Insurance under Endogenous Default and Background Risk",
        "Optimizing confidence in negative-partial-transpose-based entanglement\n  criteria",
        "On almost Gallai colourings in complete graphs",
        "Stabilization of an unstable reaction-diffusion PDE with input delay\n  despite state and input quantization",
        "Rigidity in a Fixed Number Field and a Directional $p$-Adic Littlewood\n  Conjecture for Algebraic Vectors",
        "Enhancing finite-difference based derivative-free optimization methods\n  with machine learning",
        "New Representations of Catalan's Constant, Apery's Constant and the\n  Euler Numbers Obtained from the Half Hyperbolic Secant Distribution",
        "The COSMOS-Web deep galaxy group catalog up to $z=3.7$",
        "Counting principal ideals of small norm in the simplest cubic fields",
        "Explaining the Unexplainable: A Systematic Review of Explainable AI in\n  Finance",
        "A new transcendence measure for the values of the exponential function\n  at algebraic arguments",
        "Sensitivity analysis of path-dependent options in an incomplete market\n  with pathwise functional Ito calculus",
        "Scattering resonances and pairing in a Rabi-coupled Fermi gas",
        "Feature Learning beyond the Lazy-Rich Dichotomy: Insights from\n  Representational Geometry"
      ],
      "abstract":[
        "Blood-mimicking fluids (BMFs) play a critical role in ultrasonic imaging and\nDoppler flow studies by replicating the physical and acoustic properties of\nblood. This study introduces a novel soybean oil-in-water emulsion as a BMF\nwith particle size and deformability akin to red blood cells. Using a\nmillifluidic device, we cross-validated flow profiles through both Doppler\nvelocimetry and optical particle tracking, demonstrating compatibility with\ntheoretical Poiseuille flow models. The millifluidic chip, fabricated via\nstereolithography, provided an optimized platform for dual optical and\nultrasonic assessments. Results showed strong agreement between the two methods\nacross a range of flow rates, affirming the suitability of the emulsion for\nvelocimetry applications. Furthermore, the acoustic properties of soybean oil\ndroplets support their potential as an echogenic and stable alternative to\nconventional BMFs.",
        "Purpose: To develop an accurate myocardial T1 mapping technique at 5T using\nLook-Locker-based multiple inversion-recovery with the real-time spoiled\ngradient echo (GRE) acquisition. Methods: The proposed T1 mapping technique\n(mIR-rt) samples the recovery of inverted magnetization using the real-time GRE\nand the images captured during diastole are selected for T1 fitting.\nMultiple-inversion recoveries are employed to increase the sample size for\naccurate fitting. Furthermore, the inversion pulse (IR) was tailored for\ncardiac imaging at 5T, optimized to maximize the inversion efficiency over\nspecified ranges of B1 and off-resonance. The T1 mapping method was validated\nusing Bloch simulation, phantom studies, and in 16 healthy volunteers at 5T.\nResults: The optimized IR pulse based on the tangent\/hyperbolic tangent pulse\nwas found to outperform the conventional hyperbolic secant IR pulse within a\nlimited peak amplitude of 10.6 {\\mu}T at the 5T scanner. This optimized IR\npulse achieves an average inversion factor of 0.9014 within a B0 range of\n+\/-250Hz and a B1 range of -50% to 20%. In both simulation and phantom studies,\nthe T1 values measured by mIR-rt closely approximate the reference T1 values,\nwith errors less than 3%, while the conventional MOLLI sequence underestimates\nT1 values. The myocardial T1 values at 5T are 1553 +\/- 52 ms, 1531 +\/- 53 ms,\nand 1526 +\/- 60 ms (mean +\/- standard deviation) at the apex, middle, and base,\nrespectively. Conclusion: The proposed method is feasible for myocardial T1\nmapping at 5T and provides better accuracy than the conventional MOLLI\nsequence.\n  Keywords: Myocardial T1 mapping, 5T, Look-Locker",
        "Background: Spinal cord injury (SCI) rehabilitation remains a major clinical\nchallenge, with limited treatment options for functional recovery. Temporal\ninterference (TI) electrical stimulation has emerged as a promising\nnon-invasive neuromodulation technique capable of delivering deep and targeted\nstimulation. However, the application of TI stimulation in SCI rehabilitation\nremains largely unexplored. Methods: This study aims to investigate the\nfeasibility of applying non-invasive TI electrical stimulation for SCI\nrehabilitation. Through computational modeling, we analyzed the electric field\ndistribution characteristics in the spinal cord under different TI stimulation\nconfigurations. Based on these findings, we propose a clinically applicable TI\nstimulation protocol for SCI rehabilitation. Results: The results demonstrate\nthat TI stimulation can effectively deliver focused electric fields to targeted\nspinal cord segments while maintaining non-invasiveness. The electric field\nintensity varied depending on individual anatomical differences, highlighting\nthe need for personalized stimulation parameters. The proposed protocol\nprovides a practical framework for applying TI stimulation in SCI\nrehabilitation and offers a non-invasive alternative to traditional spinal cord\nstimulation techniques. Conclusions: This study establishes the feasibility of\nusing non-invasive TI stimulation for SCI rehabilitation. The proposed\nstimulation protocol enables precise and targeted spinal cord modulation.\nHowever, further research is needed to refine personalized stimulation\nparameters and validate the clinical efficacy of this approach.",
        "High-aspect-ratio polymer materials are widely utilized in applications\nranging from everyday materials such as clothing to specialized equipment in\nindustrial and medical fields. Traditional fabrication methods, such as\nextrusion and molding, face challenges in integrating diverse materials and\nachieving complex geometries. Additionally, these methods are limited in their\nability to provide low-cost and rapid prototyping, which are critical for\nresearch and development processes. In this work, we investigated the use of\ncommercially available 3D printers to fabricate fiber preforms, which were\nsubsequently thermally drawn into fibers. By optimizing 3D printing parameters,\nwe achieved the fabrication of fibers with diameters as small as 200 um having\ncomplex shapes, with features down to a few microns. We demonstrated the\nversatility of this method by fabricating fibers from diverse set of materials,\nsuch as fibers with different stiffnesses and fibers with magnetic\ncharacteristics, which are beneficial for developing tendon-driven and\nmagnetically actuated robotic fibers. In addition, by designing novel preform\ngeometries, we produced tapered fibers and fibers with interlocking mechanisms,\nalso tailored for use in medical steerable catheter applications. These\nadvancements highlight the scalability and versatility of this approach,\noffering a robust platform for producing high-precision polymer fibers for\ndiverse applications.",
        "Active breath-hold techniques effectively mitigate respiratory motion but\npose challenges for patients who are ineligible for the procedure. Conventional\ntreatment planning relies on multiple energy layers, extending delivery time\ndue to slow layer switching. We propose to use pin ridge filters (pRFs),\ninitially developed for FLASH radiotherapy, to construct a single energy beam\nplan and minimize dose delivery time. The conventional ITV--based\nfree--breathing treatment plan served as the reference. A GTV--based IMPT--DS\nplan with a downstream energy modulation strategy was developed based on a new\nbeam model that was commissioned using the maximum energy of the IMPT plan.\nConsequently, a nested iterative pencil beam direction (PBD) spot reduction\nprocess removed low--weighted spots along each PBD, generating pRFs with\ncoarser resolution. Finally, the IMPT--DS plan was then converted into an\nIMPT--pRF plan, using a monoenergetic beam with optimized spot positions and\nweights. This approach was validated on lung and liver SBRT cases (10 Gy RBE x\n5). For the lung case, the mean lung--GTV dose decreased from 10.3 Gy to 6.9\nGy, with delivery time reduced from 188.79 to 36.16 seconds. The largest time\nreduction was at 150{\\deg}, from 47.4 to 3.99 seconds. For the liver case, the\nmean liver--GTV dose decreased from 5.7 Gy to 3.8 Gy, with delivery time\nreduced from 111.13 to 30.54 seconds. The largest time reduction was at\n180{\\deg}, from 38.57 to 3.94 seconds. This method significantly reduces dose\ndelivery time and organ at risk dose. Further analysis is needed to validate\nits clinical feasibility.",
        "This article presents a study on the influence of antenna parameters in\nmedical microwave radiometers on brightness temperature. A series of\ncomputational experiments was conducted to analyse the dependence of brightness\ntemperature on antenna characteristics. Various antenna parameters and their\neffect on the distribution of electromagnetic fields in biological tissues were\nexamined. It was demonstrated that considering the antenna mismatch parameter\nis crucial when modelling the brightness temperature of biological tissues,\ncontributing about 2 percent to its formation. The depth range of brightness\ntemperature measurement was determined. The dependence of brightness\ntemperature on the antenna diameter and frequency was established. The findings\nof this study can be applied to improve medical microwave radiometers and\nenhance their efficiency in the early diagnosis of various diseases.",
        "In this work, we have investigated the sensitivity of the effectiveness (TCP)\nof molecular radiotherapy (MRT) treatment to uncertainties of the dose rate\ncurves that may appear when reconstructing those curves.\n  We generated different dose rate curves from experimental data, imposing the\nconstraint of equal dose for each of them. Then, we computed TCPs and looked\nfor correlations between metrics measuring the differences between the dose\nrate curves and differences in TCP. Finally, according to these results, we\nestimated the range of tolerable uncertainties in the dose rate curves. The\nstudy was performed for different radiopharmaceuticals and different\nradiosensitive parameters that can affect the dose rate response\n($\\alpha\/\\beta$, sub-lethal repair rate).\n  The best correlation between differences in the dose rate curves and TCP was\nfound for a metric that computes averaged linear differences between the\ncurves. With this metric, we quantified differences in dose rate curves that\nwould lead to differences in TCP of 0.02, a parameter denoted $m_{1,\\: 0.02}$\nthat is a surrogate of the dependence of the TCP on the dose rate profile. The\nresults showed that the sensitivity of the TCP to dose rate variations\ndecreases (i.e. larger values of $m_{1,\\: 0.02}$) with increasing\n$\\alpha\/\\beta$ and sub-lethal damage repair rate of the tumor cells, and\nincreasing biological half-life of the dose rate curves.\n  The radiobiological effect of a MRT treatment on a tumor depends on the\nabsorbed dose and the dose rate profile. Ideally, both magnitudes should be\nmeasured with accuracy in order to progress towards the optimization of\nindividualized MRT treatments. Our study suggests that this would be more\nimportant for tumors with low $\\alpha\/\\beta$ and moderately slow sub-lethal\ndamage repair treated with fast-decaying radiopharmaceuticals.",
        "Divergent wave imaging with coherent compounding allows obtaining broad field\nof view and higher frame rate with respect to line by line insonification.\nHowever, the spatial and contrast resolution crucially depends on the weights\napplied in the compound phase, whose optimization is often cumbersome and based\non trial and error. This study addresses these limitations by introducing a\nclosed-form approach that maps the transmit apodization weights used in\nsynthetic aperture imaging into the compound mask applied to divergent wave\nimaging. The approach draws inspiration from a successful technique developed\nfor plane wave imaging, leveraging synthetic aperture imaging as a reference\ndue to its superior image quality. It works for both linear and convex\ngeometries and arbitrary spatial arrangements of virtual sources generating\ndivergent waves. The approach has been validated through simulated data using\nboth linear and convex probes, demonstrating that the Full Width at Half\nMaximum (FWHM) in Divergent Wave Linear Array (DWLA) increased by 7.5% at 20 mm\nand 9% at 30 mm compared to Synthetic Aperture Linear Array (SALA). For\nDivergent Wave Convex Array (DWCA), the increase was 1.64% at 20 mm and 26.56%\nat 30 mm compared to Synthetic Aperture Convex Array (SACA), witnessing the\nmethod's effectiveness.",
        "The average Relative Biological effectiveness (RBE) factors for neutron\nirradiation in the context of a BNCT treatment are studied. This research\nconsiders the various interactions and secondary particles of each process and\nestimates the RBE based on the damage induced in tissues by all of these\nparticles. A novel concept of estimating the biological dose by means of\nweighted kerma factors is introduced. These weighted kerma factors include the\nRBE of each energy deposition based on an RBE-LET relationship for secondary\ncharged particles and can be directly incorporated in weighted dose\ncalculations from Monte Carlo simulations. Furthermore, the dependence of the\nneutron weighting factor on neutron energy for standard soft tissue is\ndiscussed.",
        "In this study, we present the Physical-Bio Translator, an agent-based\nsimulation model designed to simulate cellular responses following irradiation.\nThis simulation framework is based on a novel cell-state transition model that\naccurately reflects the characteristics of irradiated cells. To validate the\nPhysical-Bio Translator, we performed simulations of cell phase evolution, cell\nphenotype evolution, and cell survival. The results indicate that the\nPhysical-Bio Translator effectively replicates experimental cell irradiation\noutcomes, suggesting that digital cell irradiation experiments can be conducted\nvia computer simulation, offering a more sophisticated model for radiation\nbiology. This work lays the foundation for developing a robust and versatile\ndigital twin at multicellular or tissue scales, aiming to comprehensively study\nand predict patient responses to radiation therapy.",
        "This study introduces an advanced multiphysics and multiscale modeling\napproach to investigate intestinal motility. We propose a generalized\nelectromechanical framework that incorporates contact mechanics, enabling the\ndevelopment of a unique and innovative model for intestinal motility. The\ntheoretical framework includes an electromechanical model coupling a\nmicrostructural material model, which describes the intestinal structure, with\nan electrophysiological model that captures the propagation of slow waves.\nAdditionally, it integrates a self-contact detection algorithm based on a\nnearest-neighbour search and the penalty method, along with boundary conditions\nthat account for the influence of surrounding organs. A staggered finite\nelement scheme implemented in FEniCS is employed to solve the governing\nequations using the finite element method. The model is applied to study cases\nof moderate and severe strangulation hernia, as well as intestinal adhesion\nsyndrome. The results demonstrate that low peristalsis takes place in the\npre-strangulation zone. At the same time, very high pressure is recorded in the\nstrangulation zone, and peristaltic contractions persisted in the healthy\nregion. For adhesions, the results indicate a complete absence of peristalsis\nin the adherent region. The model successfully reproduces both qualitatively\nand quantitatively propagative contractions in complex scenarios, such as pre-\nand post-surgical conditions, thereby highlighting its potential to provide\nvaluable insights for clinical applications.",
        "Background: Tumour budding is an independent predictor of metastasis and\nprognosis in colorectal cancer and is a vital part of the pathology\nspecification report. In a conventional pathological section observation\nprocess, pathologists have to repeatedly switch from 10x objective to 20x\nobjective several times to localize and image the target region. Besides the\nswitching operations, repeated manual or electro-mechanical focusing is also\nvery time-consuming, affecting the total time for pathological diagnosis. In\naddition, It is usually necessary to remove the manually marked symbols on the\nstained pathology slides used for classification and management before\nobservation. Methods: In this paper, we utilize Fourier ptychographic\nmicroscopy (FPM) in the pathological diagnosis process to realize large\nspace-bandwidth product imaging, quantitative phase imaging, and digital\nrefocusing in the observation process without any mechanical operations, which\ncan therefore simplify the above-mentioned cumbersome diagnostic processes. We\nfirst verify the effectiveness and efficiency of the proposed method with\nseveral typical pathological sections. Then, instead of manually erasing, we\nalso prove that FP framework can digitally remove the artificial markers with\nits digital refocusing ability. Results: At last, we demonstrated pathologists\ncan achieve 100% diagnostic accuracy with FPM imaging results. Conclusions: The\nproposed method can greatly simplify the process of pathological diagnosis, and\nthe related addon hardware system does not require expensive components, which\nmakes it have great potential for promotion in the field of pathological\ndiagnosis.",
        "Deformable liver motion tracking using a single X-ray projection enables\nreal-time motion monitoring and treatment intervention. We introduce a\nconditional point cloud diffusion model-based framework for accurate and robust\nliver motion tracking from arbitrarily angled single X-ray projections\n(PCD-Liver), which estimates volumetric liver motion by solving deformable\nvector fields (DVFs) of a prior liver surface point cloud based on a single\nX-ray image. The model is patient-specific and consists of two main components:\na rigid alignment model to estimate the liver's overall shifts and a\nconditional point cloud diffusion model that further corrects for liver surface\ndeformations. Conditioned on motion-encoded features extracted from a single\nX-ray projection via a geometry-informed feature pooling layer, the diffusion\nmodel iteratively solves detailed liver surface DVFs in a projection\nangle-agnostic manner. The liver surface motion estimated by PCD-Liver serves\nas a boundary condition for a U-Net-based biomechanical model to infer internal\nliver motion and localize liver tumors. A dataset of ten liver cancer patients\nwas used for evaluation. The accuracy of liver point cloud motion estimation\nwas assessed using root mean square error (RMSE) and 95th-percentile Hausdorff\ndistance (HD95), while liver tumor localization error was quantified using\ncenter-of-mass error (COME). The mean (standard deviation) RMSE, HD95, and COME\nof the prior liver or tumor before motion estimation were 8.86(1.51) mm,\n10.88(2.56) mm, and 9.41(3.08) mm, respectively. After PCD-Liver motion\nestimation, the corresponding values improved to 3.59(0.28) mm, 4.29(0.62) mm,\nand 3.45(0.96) mm. Under highly noisy conditions, PCD-Liver maintained stable\nperformance. This study presents an accurate and robust framework for\ndeformable liver motion estimation and tumor localization in image-guided\nradiotherapy.",
        "Uncertainty quantification for large-scale inverse problems remains a\nchallenging task. For linear inverse problems with additive Gaussian noise and\nGaussian priors, the posterior is Gaussian but sampling can be challenging,\nespecially for problems with a very large number of unknown parameters (e.g.,\ndynamic inverse problems) and for problems where computation of the square root\nand inverse of the prior covariance matrix are not feasible. Moreover, for\nhierarchical problems where several hyperparameters that define the prior and\nthe noise model must be estimated from the data, the posterior distribution may\nno longer be Gaussian, even if the forward operator is linear. Performing\nlarge-scale uncertainty quantification for these hierarchical settings requires\nnew computational techniques. In this work, we consider a hierarchical Bayesian\nframework where both the noise and prior variance are modeled as\nhyperparameters. Our approach uses Metropolis-Hastings independence sampling\nwithin Gibbs where the proposal distribution is based on generalized\nGolub-Kahan based methods. We consider two proposal samplers, one that uses a\nlow rank approximation to the conditional covariance matrix and another that\nuses a preconditioned Lanczos method. Numerical examples from seismic imaging,\ndynamic photoacoustic tomography, and atmospheric inverse modeling demonstrate\nthe effectiveness of the described approaches.",
        "This paper studies an optimal insurance problem for a utility-maximizing\nbuyer of insurance, subject to the seller's endogenous default and background\nrisk. An endogenous default occurs when the buyer's contractual indemnity\nexceeds the seller's available reserve, which is random due to the background\nrisk. We obtain an analytical solution to the optimal contract for two types of\ncontracts, differentiated by whether their indemnity functions depend on the\nseller's background risk. The results shed light on the joint effect of the\nseller's default and background risk on the buyer's insurance demand.",
        "A key requirement of any separable quantum state is that its density matrix\nhas a positive partial transpose. For continuous bipartite quantum states,\nviolation of this condition may be tested via the hierarchy of\nnegative-partial-transpose (NPT) based entanglement criteria introduced by\nShchukin and Vogel [Phys. Rev. Lett. 95, 230502 (2005)]. However, a procedure\nfor selecting the optimal NPT-based criterion is currently lacking. Here, we\ndevelop a framework to select the optimal criterion by determining the level of\nconfidence of criteria within the Shchukin and Vogel hierarchy for finite\nmeasurement number, environmental noise, and the optimal allocation of\nmeasurement resources. To demonstrate the utility of our approach, we apply our\nstatistical framework to prominent example Gaussian and non-Gaussian states,\nincluding the two-mode squeezed vacuum state, the quanta-subtracted two-mode\nsqueezed vacuum state, and the two-mode Schr\\\"odinger-cat state. Beyond\nbipartite inseparability tests, our framework can be applied to any Hermitian\nmatrix constructed of observable moments and thus can be utilized for a wide\nvariety of other nonclassicality criteria and multi-mode entanglement tests.",
        "For $t \\in \\mathbb{N}$, we say that a colouring of $E(K_n)$ is\n$\\textit{almost}$ $t$-$\\textit{Gallai}$ if no two rainbow $t$-cliques share an\nedge. Motivated by a lemma of Berkowitz on bounding the modulus of the\ncharacteristic function of clique counts in random graphs, we study the maximum\nnumber $\\tau_t(n)$ of rainbow $t$-cliques in an almost $t$-Gallai colouring of\n$E(K_n)$. For every $t \\ge 4$, we show that $n^{2-o(1)} \\leq \\tau_t(n) =\no(n^2)$. For $t=3$, surprisingly, the behaviour is substantially different. Our\nmain result establishes that $$\\left ( \\frac{1}{2}-o(1) \\right ) n\\log n \\le\n\\tau_3(n) = O\\big (n^{\\sqrt{2}}\\log n \\big ),$$ which gives the first\nnon-trivial improvements over the simple lower and upper bounds. Our proof\ncombines various applications of the probabilistic method and a generalisation\nof the edge-isoperimetric inequality for the hypercube.",
        "We solve the global asymptotic stability problem of an unstable\nreaction-diffusion Partial Differential Equation (PDE) subject to input delay\nand state quantization developing a switched predictor-feedback law. To deal\nwith the input delay, we reformulate the problem as an actuated transport PDE\ncoupled with the original reaction-diffusion PDE. Then, we design a quantized\npredictor-based feedback mechanism that employs a dynamic switching strategy to\nadjust the quantization range and error over time. The stability of the\nclosed-loop system is proven properly combining backstepping with a small-gain\napproach and input-to-state stability techniques, for deriving estimates on\nsolutions, despite the quantization effect and the system's instability. We\nalso extend this result to the input quantization case.",
        "Let $X_n$ be the space of unimodular lattices in $\\RR^n$ and let $A$ be the\nfull diagonal group in $\\on{SL}_n(\\RR)$. It is known that compact $A$-orbits\noriginate from moduls in totally real degree $n$ number fields. Our first\nresult shows that for a natural family of compact orbits $(Ax_k)_k$ all\noriginating from a fixed number field $K$, every weak limit of the Haar\nmeasures on those orbits $m_{Ax_k}$ must contain the Haar measure $m_{X_n}$ as\nan ergodic component. This result generalizes certain aspects of the work by\nAka and Shapira in \\cite{Shapira-Aka} to arbitrary dimensions, as well as\nelements from Shapira-Zheng in \\cite{shapira2021translates}.\n  For every vector $\\overline \\alpha\\in \\RR^n$ and for every rational\napproximation $(\\overline p,q)\\in \\RR^n\\times\\RR$ we can associate the\ndisplacement vector $q\\alpha-\\overline p$. We focus on algebraic vectors,\nnamely $\\overline \\alpha=(\\alpha_1,\\dots,\\alpha_n)$ such that $1, \\alpha_1,\n\\dots, \\alpha_n$ span a rank $n$ number field. For these vectors, we\ninvestigate the size of their displacements as well as the distribution of\ntheir directions. We establish that algebraic vector $\\overline \\alpha$ satisfy\nthe $p$-adic Littlewood Conjecture. Namely, we prove that \\begin{equation}\n  \\liminf_{k \\to \\infty} \\left( k \\|k\\|_p \\right)^{1\/n} \\| k (\\alpha_1, \\dots,\n\\alpha_n) \\|_\\infty = 0. \\end{equation} Additionally, we classify all limiting\ndistributions, with a special weighting, of the sequence of directions of the\ndefects in the $\\varepsilon$-approximations of $(\\alpha_1, \\dots, \\alpha_n)$.\nEach such limiting measure is expressed as the pushforward of an algebraic\nmeasure on $X_n$ to the sphere.\n  Our proof relies on estimates of the asymptotic orders of units in fixed\nnumber fields modulo families of natural numbers and on rigidity results from\n\\cite{ELMV1}.",
        "Derivative-Free Optimization (DFO) involves methods that rely solely on\nevaluations of the objective function. One of the earliest strategies for\ndesigning DFO methods is to adapt first-order methods by replacing gradients\nwith finite-difference approximations. The execution of such methods generates\na rich dataset about the objective function, including iterate points, function\nvalues, approximate gradients, and successful step sizes. In this work, we\npropose a simple auxiliary procedure to leverage this dataset and enhance the\nperformance of finite-difference-based DFO methods. Specifically, our procedure\ntrains a surrogate model using the available data and applies the gradient\nmethod with Armijo line search to the surrogate until it fails to ensure\nsufficient decrease in the true objective function, in which case we revert to\nthe original algorithm and improve our surrogate based on the new available\ninformation. As a proof of concept, we integrate this procedure with the\nderivative-free method proposed in (Optim. Lett. 18: 195--213, 2024). Numerical\nresults demonstrate significant performance improvements, particularly when the\napproximate gradients are also used to train the surrogates.",
        "New expressions and bounds for Catalan's and Apery's constants, derived from\nthe half hyperbolic secant distribution, are presented. These constants are\nobtained by using expressions for the Lorenz curve, the Gini and Theil indices,\nconvolutions and a mixture of distributions, among other approaches. The new\nexpressions are presented both in terms of integral (simple and double)\nrepresentation and also as an interesting series representation. Some of these\nfeatures are well known, while others are new. In addition, some integral\nrepresentations of Euler's numbers are obtained.",
        "Galaxy groups with $M_{tot} \\lesssim 10^{14}$ $M_\\odot$ and up to a few tens\nof members are the most common galaxy environment, marking the transition\nbetween field and massive clusters. Identifying groups plays a crucial role in\nunderstanding structure formation and galaxy evolution. Modern deep surveys\nallow us to build well-characterized samples of groups up to the regime where\nstructures were taking shape. We aimed to build the largest deep catalog of\ngalaxy groups to date over the COSMOS-Web field effective area of 0.45 deg$^2$,\nleveraging the deep high quality data of the new COSMOS-Web photometric catalog\nresulted from the James Webb Space Telescope observations of the COSMOS-Web\nfield. We performed the group search with the AMICO algorithm, a linear matched\nfilter based on an analytical model for the group signal. AMICO has already\nbeen tested in wide and deep field surveys, including COSMOS data up to $z=2$.\nIn this work, we tested the algorithm performances at even higher redshift and\nsearched for protocluster cores at $z>2$. We compiled a list of known\nprotoclusters in COSMOS at $2 \\leq z \\leq 3.7$, matched them with our\ndetections and studied the clustering of the detected cores. We estimated\npurity and completeness of our sample by creating data-driven mocks with the\nSinFoniA code and linked signal-to-noise to purity. We detected 1678 groups in\nthe COSMOS-Web field up to $z=3.7$, including lists of members extending nearly\ntwo magnitudes deeper than the previous AMICO-COSMOS catalog. 756 groups were\ndetected with purity of 80\\%. More than 500 groups have their redshift\nconfirmed by assigning spectroscopic counterparts. This group catalog offers a\nunique opportunity to explore galaxy evolution in different environments\nspanning $\\sim$12 Gyr and to study groups, from the least rich population to\nthe formation of the most massive clusters.",
        "We estimate the number of principal ideals $ I $ of norm $ \\mathrm{N}(I) \\leq\nx $ in the family of the simplest cubic fields. The advantage of our result is\nthat it provides the correct order of magnitude for arbitrary $ x \\geq 1 $,\neven when $ x $ is significantly smaller than the discriminant. In particular,\nit shows that there exist surprisingly many principal ideals of small norm.",
        "Practitioners and researchers trying to strike a balance between accuracy and\ntransparency center Explainable Artificial Intelligence (XAI) at the junction\nof finance. This paper offers a thorough overview of the changing scene of XAI\napplications in finance together with domain-specific implementations,\nmethodological developments, and trend mapping of research. Using bibliometric\nand content analysis, we find topic clusters, significant research, and most\noften used explainability strategies used in financial industries. Our results\nshow a substantial dependence on post-hoc interpretability techniques;\nattention mechanisms, feature importance analysis and SHAP are the most often\nused techniques among them. This review stresses the need of multidisciplinary\napproaches combining financial knowledge with improved explainability paradigms\nand exposes important shortcomings in present XAI systems.",
        "Let $P\\in \\mathbb Z[X]\\setminus\\{0\\}$ be of degree $\\delta\\ge 1$ and usual\nheight $H\\ge 1$, and let $\\alpha\\in \\overline{\\mathbb Q}^*$ be of degree $d\\ge\n2$. Mahler proved in 1931 the following transcendence measure for $e^\\alpha$:\nfor any $\\varepsilon\\&gt;0$, there exists $c\\&gt;0$ such that $\\vert\nP(e^\\alpha)\\vert\\&gt;c\/H^{\\mu(d,\\delta)+\\varepsilon}$ where the exponent\n$\\mu(d,\\delta)=(4d^2-2d)\\delta+2d-1$. Zheng obtained a better result in 1991\nwith $\\mu(d,\\delta)=(4d^2-2d)\\delta-1$. In this paper, we provide a new\nexplicit exponent $\\mu(d,\\delta)$ which improves on Zheng's transcendence\nmeasure for all $\\delta\\ge 2$ and all $d\\ge 2$. When $\\delta=1$, we recover his\nbound for all $d\\ge 2$, which had in fact already been obtained by Kappe in\n1966. Our improvement rests upon the optimization of an accessory parameter in\nSiegel's classical determinant method applied to Hermite-Pad{\\'e} approximants\nto powers of the exponential function.",
        "Functional It^o calculus is based on an extension of the classical It^o\ncalculus to functionals depending on the entire past evolution of the\nunderlying paths and not only on its current value. The calculus builds on\nFollmer's deterministic proof of the It^o formula, see [3], and a notion of\npathwise functional derivatives introduced by [5]. There are no smoothness\nassumptions required on the functionals, however, they are required to possess\ncertain directional derivatives which may be computed pathwise, see [6, 9, 8].\nUsing functional It^o calculus and the notion of quadratic variation, we derive\nthe functional It^o formula along with the Feynman-Kac formula for functional\nprocesses. Furthermore, we express the Greeks for path-dependent options as\nexpectations, which can be efficiently computed numerically using Monte Carlo\nsimulations. We illustrate these results by applying the formulae to digital\noptions within the Black-Scholes model framework.",
        "We investigate the possibility of using a Rabi drive to tune the interactions\nin an atomic Fermi gas. Specifically, we consider the scenario where two\nfermion species (spins) are Rabi coupled and interacting with a third uncoupled\nspecies. Using an exact calculation within a minimal low-energy model, we\nderive analytical expressions for the effective scattering length and effective\nrange that characterize the collisions between a Rabi-dressed atom and an atom\nfrom the third species. In particular, we find that new scattering resonances\nemerge in the Rabi-coupled system, which we demonstrate are linked to the\nexistence of hybrid two-body bound states. Furthermore, we show via a\ngeneralized Thouless criterion that the scattering properties have a direct\nimpact on the superfluid transitions in the Rabi-coupled Fermi gas. The\npresence of Rabi-induced resonances thus has implications for the investigation\nof many-body physics with driven atomic gases.",
        "The ability to integrate task-relevant information into neural\nrepresentations is a fundamental aspect of both biological and artificial\nintelligence. To enable theoretical analysis, recent work has examined whether\na network learns task-relevant features (rich learning) or resembles a random\nfeature model (or a kernel machine, i.e., lazy learning). However, this simple\nlazy-versus-rich dichotomy overlooks the possibility of various subtypes of\nfeature learning that emerge from different architectures, learning rules, and\ndata properties. Furthermore, most existing approaches emphasize weight\nmatrices or neural tangent kernels, limiting their applicability to\nneuroscience because they do not explicitly characterize representations.\n  In this work, we introduce an analysis framework based on representational\ngeometry to study feature learning. Instead of analyzing what are the learned\nfeatures, we focus on characterizing how task-relevant representational\nmanifolds evolve during the learning process. In both theory and experiment, we\nfind that when a network learns features useful for solving a task, the\ntask-relevant manifolds become increasingly untangled. Moreover, by tracking\nchanges in the underlying manifold geometry, we uncover distinct learning\nstages throughout training, as well as different learning strategies associated\nwith training hyperparameters, uncovering subtypes of feature learning beyond\nthe lazy-versus-rich dichotomy. Applying our method to neuroscience and machine\nlearning, we gain geometric insights into the structural inductive biases of\nneural circuits solving cognitive tasks and the mechanisms underlying\nout-of-distribution generalization in image classification. Our framework\nprovides a novel geometric perspective for understanding and quantifying\nfeature learning in both artificial and biological neural networks."
      ]
    }
  },
  {
    "id":2412.16995,
    "research_type":"applied",
    "start_id":"b12",
    "start_title":"Multi-objective performance optimization & thermodynamic analysis of solar powered supercritical co2 power cycles using machine learning methods & genetic algorithm",
    "start_abstract":"The present study is focused on multi-objective performance optimization & thermodynamic analysis from the perspectives of energy and exergy for Recompression, Partial Cooling & Main Compression Intercooling supercritical CO2 (sCO2) Brayton cycles for concentrated solar power (CSP) applications using machine learning algorithms. The novelty of this work lies in the integration of artificial neural networks (ANN) and genetic algorithms (GA) for optimizing the performance of advanced sCO2 power cycles considering climatic variation, which has significant implications for both the scientific community and engineering applications in the renewable energy sector. The methodology employed includes thermodynamic analysis based on energy, exergy & environmental factors including system performance optimization. The system is modelled for net power production of 15 MW thermal output utilizing equations for the energy and exergy balance for each component. Subsequently, thermodynamic model extracted dataset used for prediction & evaluation of Random Forest, XGBoost, KNN, AdaBoost, ANN and LightGBM algorithm. Finally, considering climate conditions, multi-objective optimization is carried out for the CSP integrated sCO2 Power cycle for optimal power output, exergy destruction, thermal and exergetic efficiency. Genetic algorithm and TOPSIS (technique for order of preference by similarity to ideal solution), multi-objective decision-making tool, were used to determine the optimum operating conditions. The major findings of this work reveal significant improvements in the performance of the advanced sCO2 cycle by 1.68 % and 7.87 % compared to conventional recompression and partial cooling cycle, respectively. This research could advance renewable energy technologies, particularly concentrated solar power, by improving power cycle designs to increase system efficiency and economic feasibility. Optimized advanced supercritical CO2 power cycles in concentrated solar power plants might increase renewable energy use and energy generation infrastructure, potentially opening new research avenues.",
    "start_categories":[
      "math.OC"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b15"
      ],
      "title":[
        "A method for real-time optimal heliostat aiming strategy generation via deep learning"
      ],
      "abstract":[
        "Optimal aiming strategies are essential for efficient solar power tower technology operation. However, the high calculation complexity makes it difficult for existing optimization methods to solve the optimization problem in real-time directly. This work proposes a real-time optimal heliostat aiming strategy generation method via deep learning. First, a two-stage learning scheme where the neural network models are trained by genetic algorithm (GA) benchmark solutions to produce an optimal aiming strategy is presented. Then, an end-to-end model without needing GA solutions for training is developed and discussed. Furthermore, a robust end-to-end training method using randomly sampled flux maps is also proposed. The proposed models demonstrated comparable performance as GA with two orders of magnitude less computation time through case studies. Among the proposed models, the end-to-end model shows significantly better generalization ability than the pure data-driven two-stage model on the test set. A robust end-to-end model with data enhancement has better robustness on unseen flux maps."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Attention Pruning: Automated Fairness Repair of Language Models via\n  Surrogate Simulated Annealing",
        "LLM Reasoner and Automated Planner: A new NPC approach",
        "CollabLLM: From Passive Responders to Active Collaborators",
        "DualOpt: A Dual Divide-and-Optimize Algorithm for the Large-scale\n  Traveling Salesman Problem",
        "A Frontier AI Risk Management Framework: Bridging the Gap Between\n  Current AI Practices and Established Risk Management",
        "Advice for Diabetes Self-Management by ChatGPT Models: Challenges and\n  Recommendations",
        "A Driver Advisory System Based on Large Language Model for High-speed\n  Train",
        "Leveraging Large Language Models to Develop Heuristics for Emerging\n  Optimization Problems",
        "Training-Free Safe Denoisers for Safe Use of Diffusion Models",
        "AI Will Always Love You: Studying Implicit Biases in Romantic AI\n  Companions",
        "Human-Alignment Influences the Utility of AI-assisted Decision Making",
        "AI-Driven Decision Support in Oncology: Evaluating Data Readiness for\n  Skin Cancer Treatment",
        "Correctness Learning: Deductive Verification Guided Learning for\n  Human-AI Collaboration",
        "Uncovering the Iceberg in the Sea: Fundamentals of Pulse Shaping and\n  Modulation Design for Random ISAC Signals",
        "Efficient Image Restoration via Latent Consistency Flow Matching",
        "Quantum Computer Controlled by Superconducting Digital Electronics at\n  Millikelvin Temperature",
        "ESPARGOS: An Ultra Low-Cost, Realtime-Capable Multi-Antenna WiFi Channel\n  Sounder",
        "Safety Verification of Nonlinear Stochastic Systems via Probabilistic\n  Tube",
        "Graph Neural Network Flavor Tagger and measurement of\n  $\\mathrm{sin}2\\beta$ at Belle II",
        "Remining Hard Negatives for Generative Pseudo Labeled Domain Adaptation",
        "Channel Resolvability Using Multiplicative Weight Update Algorithm",
        "DEFEND: A Large-scale 1M Dataset and Foundation Model for Tobacco\n  Addiction Prevention",
        "Which Code Statements Implement Privacy Behaviors in Android\n  Applications?",
        "Do Existing Testing Tools Really Uncover Gender Bias in Text-to-Image\n  Models?",
        "A Survey on the Optimization of Large Language Model-based Agents",
        "Algorithmic Clustering based on String Compression to Extract P300\n  Structure in EEG Signals",
        "Deep Reinforcement Learning based Triggering Function for Early\n  Classifiers of Time Series",
        "Decentralized Online Ensembles of Gaussian Processes for Multi-Agent\n  Systems"
      ],
      "abstract":[
        "This paper explores pruning attention heads as a post-processing bias\nmitigation method for large language models (LLMs). Modern AI systems such as\nLLMs are expanding into sensitive social contexts where fairness concerns\nbecome especially crucial. Since LLMs develop decision-making patterns by\ntraining on massive datasets of human-generated content, they naturally encode\nand perpetuate societal biases. While modifying training datasets and\nalgorithms is expensive and requires significant resources; post-processing\ntechniques-such as selectively deactivating neurons and attention heads in\npre-trained LLMs-can provide feasible and effective approaches to improve\nfairness. However, identifying the optimal subset of parameters to prune\npresents a combinatorial challenge within LLMs' immense parameter space,\nrequiring solutions that efficiently balance competing objectives across the\nfrontiers of model fairness and utility.\n  To address the computational challenges, we explore a search-based program\nrepair approach via randomized simulated annealing. Given the prohibitive\nevaluation costs in billion-parameter LLMs, we develop surrogate deep neural\nnetworks that efficiently model the relationship between attention head states\n(active\/inactive) and their corresponding fairness\/utility metrics. This allows\nus to perform optimization over the surrogate models and efficiently identify\noptimal subsets of attention heads for selective pruning rather than directly\nsearching through the LLM parameter space. This paper introduces Attention\nPruning, a fairness-aware surrogate simulated annealing approach to prune\nattention heads in LLMs that disproportionately contribute to bias while\nminimally impacting overall model utility. Our experiments show that Attention\nPruning achieves up to $40\\%$ reduction in gender bias and outperforms the\nstate-of-the-art bias mitigation strategies.",
        "In domains requiring intelligent agents to emulate plausible human-like\nbehaviour, such as formative simulations, traditional techniques like behaviour\ntrees encounter significant challenges. Large Language Models (LLMs), despite\nnot always yielding optimal solutions, usually offer plausible and human-like\nresponses to a given problem. In this paper, we exploit this capability and\npropose a novel architecture that integrates an LLM for decision-making with a\nclassical automated planner that can generate sound plans for that decision.\nThe combination aims to equip an agent with the ability to make decisions in\nvarious situations, even if they were not anticipated during the design phase.",
        "Large Language Models are typically trained with next-turn rewards, limiting\ntheir ability to optimize for long-term interaction. As a result, they often\nrespond passively to ambiguous or open-ended user requests, failing to help\nusers reach their ultimate intents and leading to inefficient conversations. To\naddress these limitations, we introduce CollabLLM, a novel and general training\nframework that enhances multiturn human-LLM collaboration. Its key innovation\nis a collaborative simulation that estimates the long-term contribution of\nresponses using Multiturn-aware Rewards. By reinforcement fine-tuning these\nrewards, CollabLLM goes beyond responding to user requests, and actively\nuncovers user intent and offers insightful suggestions-a key step towards more\nhuman-centered AI. We also devise a multiturn interaction benchmark with three\nchallenging tasks such as document creation. CollabLLM significantly\noutperforms our baselines with averages of 18.5% higher task performance and\n46.3% improved interactivity by LLM judges. Finally, we conduct a large user\nstudy with 201 judges, where CollabLLM increases user satisfaction by 17.6% and\nreduces user spent time by 10.4%.",
        "This paper proposes a dual divide-and-optimize algorithm (DualOpt) for\nsolving the large-scale traveling salesman problem (TSP). DualOpt combines two\ncomplementary strategies to improve both solution quality and computational\nefficiency. The first strategy is a grid-based divide-and-conquer procedure\nthat partitions the TSP into smaller sub-problems, solving them in parallel and\niteratively refining the solution by merging nodes and partial routes. The\nprocess continues until only one grid remains, yielding a high-quality initial\nsolution. The second strategy involves a path-based divide-and-optimize\nprocedure that further optimizes the solution by dividing it into sub-paths,\noptimizing each using a neural solver, and merging them back to progressively\nimprove the overall solution. Extensive experiments conducted on two groups of\nTSP benchmark instances, including randomly generated instances with up to\n100,000 nodes and real-world datasets from TSPLIB, demonstrate the\neffectiveness of DualOpt. The proposed DualOpt achieves highly competitive\nresults compared to 10 state-of-the-art algorithms in the literature. In\nparticular, DualOpt achieves an improvement gap up to 1.40% for the largest\ninstance TSP100K with a remarkable 104x speed-up over the leading heuristic\nsolver LKH3. Additionally, DualOpt demonstrates strong generalization on TSPLIB\nbenchmarks, confirming its capability to tackle diverse real-world TSP\napplications.",
        "The recent development of powerful AI systems has highlighted the need for\nrobust risk management frameworks in the AI industry. Although companies have\nbegun to implement safety frameworks, current approaches often lack the\nsystematic rigor found in other high-risk industries. This paper presents a\ncomprehensive risk management framework for the development of frontier AI that\nbridges this gap by integrating established risk management principles with\nemerging AI-specific practices. The framework consists of four key components:\n(1) risk identification (through literature review, open-ended red-teaming, and\nrisk modeling), (2) risk analysis and evaluation using quantitative metrics and\nclearly defined thresholds, (3) risk treatment through mitigation measures such\nas containment, deployment controls, and assurance processes, and (4) risk\ngovernance establishing clear organizational structures and accountability.\nDrawing from best practices in mature industries such as aviation or nuclear\npower, while accounting for AI's unique challenges, this framework provides AI\ndevelopers with actionable guidelines for implementing robust risk management.\nThe paper details how each component should be implemented throughout the\nlife-cycle of the AI system - from planning through deployment - and emphasizes\nthe importance and feasibility of conducting risk management work prior to the\nfinal training run to minimize the burden associated with it.",
        "Given their ability for advanced reasoning, extensive contextual\nunderstanding, and robust question-answering abilities, large language models\nhave become prominent in healthcare management research. Despite adeptly\nhandling a broad spectrum of healthcare inquiries, these models face\nsignificant challenges in delivering accurate and practical advice for chronic\nconditions such as diabetes. We evaluate the responses of ChatGPT versions 3.5\nand 4 to diabetes patient queries, assessing their depth of medical knowledge\nand their capacity to deliver personalized, context-specific advice for\ndiabetes self-management. Our findings reveal discrepancies in accuracy and\nembedded biases, emphasizing the models' limitations in providing tailored\nadvice unless activated by sophisticated prompting techniques. Additionally, we\nobserve that both models often provide advice without seeking necessary\nclarification, a practice that can result in potentially dangerous advice. This\nunderscores the limited practical effectiveness of these models without human\noversight in clinical settings. To address these issues, we propose a\ncommonsense evaluation layer for prompt evaluation and incorporating\ndisease-specific external memory using an advanced Retrieval Augmented\nGeneration technique. This approach aims to improve information quality and\nreduce misinformation risks, contributing to more reliable AI applications in\nhealthcare settings. Our findings seek to influence the future direction of AI\nin healthcare, enhancing both the scope and quality of its integration.",
        "With the rapid development of China high-speed railway, drivers face\nincreasingly significant technical challenges during operations, such as fault\nhandling. Currently, drivers depend on the onboard mechanic when facing\ntechnical issues, for instance, traction loss or sensor faults. This dependency\ncan hinder effective operation, even lead to accidents, while waiting for\nfaults to be addressed. To enhance the accuracy and explainability of actions\nduring fault handling, an Intelligent Driver Advisory System (IDAS) framework\nbased on a large language model (LLM) named IDAS-LLM, is introduced. Initially,\ndomain-fine-tuning of the LLM is performed using a constructed railway\nknowledge question-and-answer dataset to improve answer accuracy in\nrailway-related questions. Subsequently, integration of the Retrieval-augmented\nGeneration (RAG) architecture is pursued for system design to enhance the\nexplainability of generated responses. Comparative experiments are conducted\nusing the constructed railway driving knowledge assessment dataset. Results\nindicate that domain-fine-tuned LLMs show an improvement in answer accuracy by\nan average of 10%, outperforming some current mainstream LLMs. Additionally,\nthe inclusion of the RAG framework increases the average recall rate of\nquestion-and-answer sessions by about 4%. Finally, the fault handling\ncapability of IDAS-LLM is demonstrated through simulations of real operational\nscenarios, proving that the proposed framework has practical application\nprospects.",
        "Combinatorial optimization problems often rely on heuristic algorithms to\ngenerate efficient solutions. However, the manual design of heuristics is\nresource-intensive and constrained by the designer's expertise. Recent advances\nin artificial intelligence, particularly large language models (LLMs), have\ndemonstrated the potential to automate heuristic generation through\nevolutionary frameworks. Recent works focus only on well-known combinatorial\noptimization problems like the traveling salesman problem and online bin\npacking problem when designing constructive heuristics. This study investigates\nwhether LLMs can effectively generate heuristics for niche, not yet broadly\nresearched optimization problems, using the unit-load pre-marshalling problem\nas an example case. We propose the Contextual Evolution of Heuristics (CEoH)\nframework, an extension of the Evolution of Heuristics (EoH) framework, which\nincorporates problem-specific descriptions to enhance in-context learning\nduring heuristic generation. Through computational experiments, we evaluate\nCEoH and EoH and compare the results. Results indicate that CEoH enables\nsmaller LLMs to generate high-quality heuristics more consistently and even\noutperform larger models. Larger models demonstrate robust performance with or\nwithout contextualized prompts. The generated heuristics exhibit scalability to\ndiverse instance configurations.",
        "There is growing concern over the safety of powerful diffusion models (DMs),\nas they are often misused to produce inappropriate, not-safe-for-work (NSFW)\ncontent or generate copyrighted material or data of individuals who wish to be\nforgotten. Many existing methods tackle these issues by heavily relying on\ntext-based negative prompts or extensively retraining DMs to eliminate certain\nfeatures or samples. In this paper, we take a radically different approach,\ndirectly modifying the sampling trajectory by leveraging a negation set (e.g.,\nunsafe images, copyrighted data, or datapoints needed to be excluded) to avoid\nspecific regions of data distribution, without needing to retrain or fine-tune\nDMs. We formally derive the relationship between the expected denoised samples\nthat are safe and those that are not safe, leading to our $\\textit{safe}$\ndenoiser which ensures its final samples are away from the area to be negated.\nInspired by the derivation, we develop a practical algorithm that successfully\nproduces high-quality samples while avoiding negation areas of the data\ndistribution in text-conditional, class-conditional, and unconditional image\ngeneration scenarios. These results hint at the great potential of our\ntraining-free safe denoiser for using DMs more safely.",
        "While existing studies have recognised explicit biases in generative models,\nincluding occupational gender biases, the nuances of gender stereotypes and\nexpectations of relationships between users and AI companions remain\nunderexplored. In the meantime, AI companions have become increasingly popular\nas friends or gendered romantic partners to their users. This study bridges the\ngap by devising three experiments tailored for romantic, gender-assigned AI\ncompanions and their users, effectively evaluating implicit biases across\nvarious-sized LLMs. Each experiment looks at a different dimension: implicit\nassociations, emotion responses, and sycophancy. This study aims to measure and\ncompare biases manifested in different companion systems by quantitatively\nanalysing persona-assigned model responses to a baseline through newly devised\nmetrics. The results are noteworthy: they show that assigning gendered,\nrelationship personas to Large Language Models significantly alters the\nresponses of these models, and in certain situations in a biased, stereotypical\nway.",
        "Whenever an AI model is used to predict a relevant (binary) outcome in\nAI-assisted decision making, it is widely agreed that, together with each\nprediction, the model should provide an AI confidence value. However, it has\nbeen unclear why decision makers have often difficulties to develop a good\nsense on when to trust a prediction using AI confidence values. Very recently,\nCorvelo Benz and Gomez Rodriguez have argued that, for rational decision\nmakers, the utility of AI-assisted decision making is inherently bounded by the\ndegree of alignment between the AI confidence values and the decision maker's\nconfidence on their own predictions. In this work, we empirically investigate\nto what extent the degree of alignment actually influences the utility of\nAI-assisted decision making. To this end, we design and run a large-scale human\nsubject study (n=703) where participants solve a simple decision making task -\nan online card game - assisted by an AI model with a steerable degree of\nalignment. Our results show a positive association between the degree of\nalignment and the utility of AI-assisted decision making. In addition, our\nresults also show that post-processing the AI confidence values to achieve\nmulticalibration with respect to the participants' confidence on their own\npredictions increases both the degree of alignment and the utility of\nAI-assisted decision making.",
        "This research focuses on evaluating and enhancing data readiness for the\ndevelopment of an Artificial Intelligence (AI)-based Clinical Decision Support\nSystem (CDSS) in the context of skin cancer treatment. The study, conducted at\nthe Skin Tumor Center of the University Hospital M\\\"unster, delves into the\nessential role of data quality, availability, and extractability in\nimplementing effective AI applications in oncology. By employing a multifaceted\nmethodology, including literature review, data readiness assessment, and expert\nworkshops, the study addresses the challenges of integrating AI into clinical\ndecision-making. The research identifies crucial data points for skin cancer\ntreatment decisions, evaluates their presence and quality in various\ninformation systems, and highlights the difficulties in extracting information\nfrom unstructured data. The findings underline the significance of\nhigh-quality, accessible data for the success of AI-driven CDSS in medical\nsettings, particularly in the complex field of oncology.",
        "Despite significant progress in AI and decision-making technologies in\nsafety-critical fields, challenges remain in verifying the correctness of\ndecision output schemes and verification-result driven design. We propose\ncorrectness learning (CL) to enhance human-AI collaboration integrating\ndeductive verification methods and insights from historical high-quality\nschemes. The typical pattern hidden in historical high-quality schemes, such as\nchange of task priorities in shared resources, provides critical guidance for\nintelligent agents in learning and decision-making. By utilizing deductive\nverification methods, we proposed patten-driven correctness learning (PDCL),\nformally modeling and reasoning the adaptive behaviors-or 'correctness\npattern'-of system agents based on historical high-quality schemes, capturing\nthe logical relationships embedded within these schemes. Using this logical\ninformation as guidance, we establish a correctness judgment and feedback\nmechanism to steer the intelligent decision model toward the 'correctness\npattern' reflected in historical high-quality schemes. Extensive experiments\nacross multiple working conditions and core parameters validate the framework's\ncomponents and demonstrate its effectiveness in improving decision-making and\nresource optimization.",
        "Integrated Sensing and Communications (ISAC) is expected to play a pivotal\nrole in future 6G networks. To maximize time-frequency resource utilization, 6G\nISAC systems must exploit data payload signals, that are inherently random, for\nboth communication and sensing tasks. This paper provides a comprehensive\nanalysis of the sensing performance of such communication-centric ISAC signals,\nwith a focus on modulation and pulse shaping design to reshape the statistical\nproperties of their auto-correlation functions (ACFs), thereby improving the\ntarget ranging performance. We derive a closed-form expression for the\nexpectation of the squared ACF of random ISAC signals, considering arbitrary\nmodulation bases and constellation mappings within the Nyquist pulse shaping\nframework. The structure is metaphorically described as an ``iceberg hidden in\nthe sea\", where the ``iceberg'' represents the squared mean of the ACF of\nrandom ISAC signals, that is determined by the pulse shaping filter, and the\n``sea level'' characterizes the corresponding variance, caused by the\nrandomness of the data payload. Our analysis shows that, for QAM\/PSK\nconstellations with Nyquist pulse shaping, Orthogonal Frequency Division\nMultiplexing (OFDM) achieves the lowest ranging sidelobe level across all lags.\nBuilding on these insights, we propose a novel Nyquist pulse shaping design to\nenhance the sensing performance of random ISAC signals. Numerical results\nvalidate our theoretical findings, showing that the proposed pulse shaping\nsignificantly reduces ranging sidelobes compared to conventional root-raised\ncosine (RRC) pulse shaping, thereby improving the ranging performance.",
        "Recent advances in generative image restoration (IR) have demonstrated\nimpressive results. However, these methods are hindered by their substantial\nsize and computational demands, rendering them unsuitable for deployment on\nedge devices. This work introduces ELIR, an Efficient Latent Image Restoration\nmethod. ELIR operates in latent space by first predicting the latent\nrepresentation of the minimum mean square error (MMSE) estimator and then\ntransporting this estimate to high-quality images using a latent consistency\nflow-based model. Consequently, ELIR is more than 4x faster compared to the\nstate-of-the-art diffusion and flow-based approaches. Moreover, ELIR is also\nmore than 4x smaller, making it well-suited for deployment on\nresource-constrained edge devices. Comprehensive evaluations of various image\nrestoration tasks show that ELIR achieves competitive results, effectively\nbalancing distortion and perceptual quality metrics while offering improved\nefficiency in terms of memory and computation.",
        "Current superconducting quantum computing platforms face significant scaling\nchallenges, as individual signal lines are required for control of each qubit.\nThis wiring overhead is a result of the low level of integration between\ncontrol electronics at room temperature and qubits operating at millikelvin\ntemperatures, which raise serious doubts among technologists about whether\nutility-scale quantum computers can be built. A promising alternative is to\nutilize cryogenic, superconducting digital control electronics that coexist\nwith qubits. Here, we report the first multi-qubit system integrating this\ntechnology. The system utilizes digital demultiplexing, breaking the linear\nscaling of control lines to number of qubits. We also demonstrate single-qubit\nfidelities above 99%, and up to 99.9%. This work is a critical step forward in\nrealizing highly scalable chip-based quantum computers.",
        "Multi-antenna channel sounding is a technique for measuring the propagation\ncharacteristics of electromagnetic waves that is commonly employed for\nparameterizing channel models. Channel sounders are usually custom-built from\nmany Software Defined Radio receivers, making them expensive to procure and\ndifficult to operate, which constrains the set of users to a few specialized\nscientific institutions and industrial research laboratories. Recent\ndevelopments in Joint Communications and Sensing (JCaS) extend the possible\nuses of channel data to applications like human activity recognition, human\npresence detection, user localization and wireless Channel Charting, all of\nwhich are of great interest to security researchers, experts in industrial\nautomation and others. However, due to a lack of affordable, easy-to-use and\ncommercially available multi-antenna channel sounders, those scientific\ncommunities can be hindered by their lack of access to wireless channel\nmeasurements. To lower the barrier to entry for channel sounding, we develop an\nultra low-cost measurement hardware platform based on mass-produced WiFi chips,\nwhich is easily affordable to research groups and even hobbyists.",
        "We address the problem of safety verification for nonlinear stochastic\nsystems, specifically the task of certifying that system trajectories remain\nwithin a safe set with high probability. To tackle this challenge, we adopt a\nset-erosion strategy, which decouples the effects of stochastic disturbances\nfrom deterministic dynamics. This approach converts the stochastic safety\nverification problem on a safe set into a deterministic safety verification\nproblem on an eroded subset of the safe set. The success of this strategy\nhinges on the depth of erosion, which is determined by a probabilistic tube\nthat bounds the deviation of stochastic trajectories from their corresponding\ndeterministic trajectories. Our main contribution is the establishment of a\ntight bound for the probabilistic tube of nonlinear stochastic systems. To\nobtain a probabilistic bound for stochastic trajectories, we adopt a\nmartingale-based approach. The core innovation lies in the design of a novel\nenergy function associated with the averaged moment generating function, which\nforms an affine martingale, a generalization of the traditional c-martingale.\nUsing this energy function, we derive a precise bound for the probabilistic\ntube. Furthermore, we enhance this bound by incorporating the union-bound\ninequality for strictly contractive dynamics. By integrating the derived\nprobabilistic tubes into the set-erosion strategy, we demonstrate that the\nsafety verification problem for nonlinear stochastic systems can be reduced to\na deterministic safety verification problem. Our theoretical results are\nvalidated through applications in reachability-based safety verification and\nsafe controller synthesis, accompanied by several numerical examples that\nillustrate their effectiveness.",
        "We present GFlaT, a new algorithm that uses a graph-neural-network to\ndetermine the flavor of neutral B mesons produced in $\\mathrm{\\Upsilon(4S)}$\ndecays. We evaluate its performance using $B$ decays to flavor-specific\nhadronic final states reconstructed in a $362$ $\\mathrm{fb}^{-1}$ sample of\nelectron-positron collisions recorded at the $\\mathrm{\\Upsilon(4S)}$ resonance\nwith the Belle II detector at the SuperKEKB collider. We achieve an effective\ntagging efficiency of $(37.40 \\pm 0.43 \\pm 0.36) \\%$, where the first\nuncertainty is statistical and the second systematic, which is $18\\%$ better\nthan the previous Belle II algorithm. Demonstrating the algorithm, we use $B^0\n\\to J\/\\psi K_\\mathrm{S}^0$ decays to measure the direct and mixing-induced CP\nviolation parameters, $C = (-0.035 \\pm 0.026 \\pm 0.013)$ and $S = (0.724 \\pm\n0.035 \\pm 0.014)$, from which we obtain $\\beta = (23.2 \\pm 1.5 \\pm\n0.6)^{\\circ}$.",
        "Dense retrievers have demonstrated significant potential for neural\ninformation retrieval; however, they exhibit a lack of robustness to domain\nshifts, thereby limiting their efficacy in zero-shot settings across diverse\ndomains. A state-of-the-art domain adaptation technique is Generative Pseudo\nLabeling (GPL). GPL uses synthetic query generation and initially mined hard\nnegatives to distill knowledge from cross-encoder to dense retrievers in the\ntarget domain. In this paper, we analyze the documents retrieved by the\ndomain-adapted model and discover that these are more relevant to the target\nqueries than those of the non-domain-adapted model. We then propose refreshing\nthe hard-negative index during the knowledge distillation phase to mine better\nhard negatives. Our remining R-GPL approach boosts ranking performance in 13\/14\nBEIR datasets and 9\/12 LoTTe datasets. Our contributions are (i) analyzing hard\nnegatives returned by domain-adapted and non-domain-adapted models and (ii)\napplying the GPL training with and without hard-negative re-mining in LoTTE and\nBEIR datasets.",
        "We study the channel resolvability problem, which is used to prove strong\nconverse of identification via channel. Channel resolvability has been solved\nby only random coding in the literature. We prove channel resolvability using\nthe multiplicative weight update algorithm. This is the first approach to\nchannel resolvability using non-random coding.",
        "While tobacco advertising innovates at unprecedented speed, traditional\nsurveillance methods remain frozen in time, especially in the context of social\nmedia. The lack of large-scale, comprehensive datasets and sophisticated\nmonitoring systems has created a widening gap between industry advancement and\npublic health oversight. This paper addresses this critical challenge by\nintroducing Tobacco-1M, a comprehensive dataset of one million tobacco product\nimages with hierarchical labels spanning 75 product categories, and DEFEND, a\nnovel foundation model for tobacco product understanding. Our approach\nintegrates a Feature Enhancement Module for rich multimodal representation\nlearning, a Local-Global Visual Coherence mechanism for detailed feature\ndiscrimination, and an Enhanced Image-Text Alignment strategy for precise\nproduct characterization. Experimental results demonstrate DEFEND's superior\nperformance, achieving 83.1% accuracy in product classification and 73.8% in\nvisual question-answering tasks, outperforming existing methods by significant\nmargins. Moreover, the model exhibits robust zero-shot learning capabilities\nwith 45.6% accuracy on novel product categories. This work provides regulatory\nbodies and public health researchers with powerful tools for monitoring\nemerging tobacco products and marketing strategies, potentially revolutionizing\napproaches to tobacco control and public health surveillance.",
        "A \"privacy behavior\" in software is an action where the software uses\npersonal information for a service or a feature, such as a website using\nlocation to provide content relevant to a user. Programmers are required by\nregulations or application stores to provide privacy notices and labels\ndescribing these privacy behaviors. Although many tools and research prototypes\nhave been developed to help programmers generate these notices by analyzing the\nsource code, these approaches are often fairly coarse-grained (i.e., at the\nlevel of whole methods or files, rather than at the statement level). But this\nis not necessarily how privacy behaviors exist in code. Privacy behaviors are\nembedded in specific statements in code. Current literature does not examine\nwhat statements programmers see as most important, how consistent these views\nare, or how to detect them. In this paper, we conduct an empirical study to\nexamine which statements programmers view as most-related to privacy behaviors.\nWe find that expression statements that make function calls are most associated\nwith privacy behaviors, while the type of privacy label has little effect on\nthe attributes of the selected statements. We then propose an approach to\nautomatically detect these privacy-relevant statements by fine-tuning three\nlarge language models with the data from the study. We observe that the\nagreement between our approach and participants is comparable to or higher than\nan agreement between two participants. Our study and detection approach can\nhelp programmers understand which statements in code affect privacy in mobile\napplications.",
        "Text-to-Image (T2I) models have recently gained significant attention due to\ntheir ability to generate high-quality images and are consequently used in a\nwide range of applications. However, there are concerns about the gender bias\nof these models. Previous studies have shown that T2I models can perpetuate or\neven amplify gender stereotypes when provided with neutral text prompts.\nResearchers have proposed automated gender bias uncovering detectors for T2I\nmodels, but a crucial gap exists: no existing work comprehensively compares the\nvarious detectors and understands how the gender bias detected by them deviates\nfrom the actual situation. This study addresses this gap by validating previous\ngender bias detectors using a manually labeled dataset and comparing how the\nbias identified by various detectors deviates from the actual bias in T2I\nmodels, as verified by manual confirmation. We create a dataset consisting of\n6,000 images generated from three cutting-edge T2I models: Stable Diffusion XL,\nStable Diffusion 3, and Dreamlike Photoreal 2.0. During the human-labeling\nprocess, we find that all three T2I models generate a portion (12.48% on\naverage) of low-quality images (e.g., generate images with no face present),\nwhere human annotators cannot determine the gender of the person. Our analysis\nreveals that all three T2I models show a preference for generating male images,\nwith SDXL being the most biased. Additionally, images generated using prompts\ncontaining professional descriptions (e.g., lawyer or doctor) show the most\nbias. We evaluate seven gender bias detectors and find that none fully capture\nthe actual level of bias in T2I models, with some detectors overestimating bias\nby up to 26.95%. We further investigate the causes of inaccurate estimations,\nhighlighting the limitations of detectors in dealing with low-quality images.\nBased on our findings, we propose an enhanced detector...",
        "With the rapid development of Large Language Models (LLMs), LLM-based agents\nhave been widely adopted in various fields, becoming essential for autonomous\ndecision-making and interactive tasks. However, current work typically relies\non prompt design or fine-tuning strategies applied to vanilla LLMs, which often\nleads to limited effectiveness or suboptimal performance in complex\nagent-related environments. Although LLM optimization techniques can improve\nmodel performance across many general tasks, they lack specialized optimization\ntowards critical agent functionalities such as long-term planning, dynamic\nenvironmental interaction, and complex decision-making. Although numerous\nrecent studies have explored various strategies to optimize LLM-based agents\nfor complex agent tasks, a systematic review summarizing and comparing these\nmethods from a holistic perspective is still lacking. In this survey, we\nprovide a comprehensive review of LLM-based agent optimization approaches,\ncategorizing them into parameter-driven and parameter-free methods. We first\nfocus on parameter-driven optimization, covering fine-tuning-based\noptimization, reinforcement learning-based optimization, and hybrid strategies,\nanalyzing key aspects such as trajectory data construction, fine-tuning\ntechniques, reward function design, and optimization algorithms. Additionally,\nwe briefly discuss parameter-free strategies that optimize agent behavior\nthrough prompt engineering and external knowledge retrieval. Finally, we\nsummarize the datasets and benchmarks used for evaluation and tuning, review\nkey applications of LLM-based agents, and discuss major challenges and\npromising future directions. Our repository for related references is available\nat https:\/\/github.com\/YoungDubbyDu\/LLM-Agent-Optimization.",
        "P300 is an Event-Related Potential widely used in Brain-Computer Interfaces,\nbut its detection is challenging due to inter-subject and temporal variability.\nThis work introduces a clustering methodology based on Normalized Compression\nDistance (NCD) to extract the P300 structure, ensuring robustness against\nvariability. We propose a novel signal-to-ASCII transformation to generate\ncompression-friendly objects, which are then clustered using a hierarchical\ntree-based method and a multidimensional projection approach. Experimental\nresults on two datasets demonstrate the method's ability to reveal relevant\nP300 structures, showing clustering performance comparable to state-of-the-art\napproaches. Furthermore, analysis at the electrode level suggests that the\nmethod could assist in electrode selection for P300 detection. This\ncompression-driven clustering methodology offers a complementary tool for EEG\nanalysis and P300 identification.",
        "Early Classification of Time Series (ECTS) has been recognized as an\nimportant problem in many areas where decisions have to be taken as soon as\npossible, before the full data availability, while time pressure increases.\nNumerous ECTS approaches have been proposed, based on different triggering\nfunctions, each taking into account various pieces of information related to\nthe incoming time series and\/or the output of a classifier. Although their\nperformances have been empirically compared in the literature, no studies have\nbeen carried out on the optimality of these triggering functions that involve\n``man-tailored'' decision rules. Based on the same information, could there be\nbetter triggering functions? This paper presents one way to investigate this\nquestion by showing first how to translate ECTS problems into Reinforcement\nLearning (RL) ones, where the very same information is used in the state space.\nA thorough comparison of the performance obtained by ``handmade'' approaches\nand their ``RL-based'' counterparts has been carried out. A second question\ninvestigated in this paper is whether a different combination of information,\ndefining the state space in RL systems, can achieve even better performance.\nExperiments show that the system we describe, called \\textsc{Alert},\nsignificantly outperforms its state-of-the-art competitors on a large number of\ndatasets.",
        "Flexible and scalable decentralized learning solutions are fundamentally\nimportant in the application of multi-agent systems. While several recent\napproaches introduce (ensembles of) kernel machines in the distributed setting,\nBayesian solutions are much more limited. We introduce a fully decentralized,\nasymptotically exact solution to computing the random feature approximation of\nGaussian processes. We further address the choice of hyperparameters by\nintroducing an ensembling scheme for Bayesian multiple kernel learning based on\nonline Bayesian model averaging. The resulting algorithm is tested against\nBayesian and frequentist methods on simulated and real-world datasets."
      ]
    }
  },
  {
    "id":2412.16995,
    "research_type":"applied",
    "start_id":"b15",
    "start_title":"A method for real-time optimal heliostat aiming strategy generation via deep learning",
    "start_abstract":"Optimal aiming strategies are essential for efficient solar power tower technology operation. However, the high calculation complexity makes it difficult for existing optimization methods to solve the optimization problem in real-time directly. This work proposes a real-time optimal heliostat aiming strategy generation method via deep learning. First, a two-stage learning scheme where the neural network models are trained by genetic algorithm (GA) benchmark solutions to produce an optimal aiming strategy is presented. Then, an end-to-end model without needing GA solutions for training is developed and discussed. Furthermore, a robust end-to-end training method using randomly sampled flux maps is also proposed. The proposed models demonstrated comparable performance as GA with two orders of magnitude less computation time through case studies. Among the proposed models, the end-to-end model shows significantly better generalization ability than the pure data-driven two-stage model on the test set. A robust end-to-end model with data enhancement has better robustness on unseen flux maps.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b12"
      ],
      "title":[
        "Multi-objective performance optimization & thermodynamic analysis of solar powered supercritical co2 power cycles using machine learning methods & genetic algorithm"
      ],
      "abstract":[
        "The present study is focused on multi-objective performance optimization & thermodynamic analysis from the perspectives of energy and exergy for Recompression, Partial Cooling & Main Compression Intercooling supercritical CO2 (sCO2) Brayton cycles for concentrated solar power (CSP) applications using machine learning algorithms. The novelty of this work lies in the integration of artificial neural networks (ANN) and genetic algorithms (GA) for optimizing the performance of advanced sCO2 power cycles considering climatic variation, which has significant implications for both the scientific community and engineering applications in the renewable energy sector. The methodology employed includes thermodynamic analysis based on energy, exergy & environmental factors including system performance optimization. The system is modelled for net power production of 15 MW thermal output utilizing equations for the energy and exergy balance for each component. Subsequently, thermodynamic model extracted dataset used for prediction & evaluation of Random Forest, XGBoost, KNN, AdaBoost, ANN and LightGBM algorithm. Finally, considering climate conditions, multi-objective optimization is carried out for the CSP integrated sCO2 Power cycle for optimal power output, exergy destruction, thermal and exergetic efficiency. Genetic algorithm and TOPSIS (technique for order of preference by similarity to ideal solution), multi-objective decision-making tool, were used to determine the optimum operating conditions. The major findings of this work reveal significant improvements in the performance of the advanced sCO2 cycle by 1.68 % and 7.87 % compared to conventional recompression and partial cooling cycle, respectively. This research could advance renewable energy technologies, particularly concentrated solar power, by improving power cycle designs to increase system efficiency and economic feasibility. Optimized advanced supercritical CO2 power cycles in concentrated solar power plants might increase renewable energy use and energy generation infrastructure, potentially opening new research avenues."
      ],
      "categories":[
        "math.OC"
      ]
    },
    "list":{
      "title":[
        "A global approach for generalized semi-infinte programs with polyhedral\n  parameter sets",
        "Pareto sensitivity, most-changing sub-fronts, and knee solutions",
        "Extension of Controllability Score to Infinite-Dimensional Systems",
        "La M\\'ethode du Gradient Proxim\\'e",
        "Inertial Bregman Proximal Gradient under Partial Smoothness",
        "Price and Assortment Optimization under the Multinomial Logit Model with\n  Opaque Products",
        "A Stochastic Linear-Quadratic Leader-Follower Differential Game with\n  Elephant Memory",
        "Distributionally Fair Peer-to-Peer Electricity Trading",
        "Set-valued evenly convex functions: characterizations and c-conjugacy",
        "On the acceleration of gradient methods: the triangle steepest descent\n  method",
        "A Linear Complexity Algorithm for Optimal Transport Problem with\n  Log-type Cost",
        "Regularity of the Product of Two Relaxed Cutters with Relaxation\n  Parameters Beyond Two",
        "Scalable Second-Order Optimization Algorithms for Minimizing Low-rank\n  Functions",
        "Tukey Depth Mechanisms for Practical Private Mean Estimation",
        "A hybrid pressure formulation of the face-centred finite volume method\n  for viscous laminar incompressible flows",
        "Chemically-Accurate Prediction of the Ionisation Potential of Helium\n  Using a Quantum Processor",
        "Testing the QCD formation time with reconstructed parton splittings",
        "Cauchy Random Features for Operator Learning in Sobolev Space",
        "Grid-based exoplanet atmospheric mass loss predictions through neural\n  network",
        "Hyper-neutron stars from an ab initio calculation",
        "The Method of ${\\cal M}_{n}$-Extension: The KdV Equation",
        "Stronger Constraints on Primordial Black Holes as Dark Matter Derived\n  from the Thermal Evolution of the Intergalactic Medium over the Last Twelve\n  Billion Years",
        "Impulsive mixing of stellar populations in dwarf spheroidal galaxies",
        "A Flux-Tunable cavity for Dark matter detection",
        "Apparent teleportation of indistinguishable particles",
        "Long Lived Quasinormal Modes of Regular and Extreme Black Holes",
        "Positive Feedback: How a Synergy Between the Streaming Instability and\n  Dust Coagulation Forms Planetesimals",
        "Signs of Non-Monotonic Finite-Volume Corrections to $g_A$"
      ],
      "abstract":[
        "This paper studies generalized semi-infinite programs (GSIPs) defined with\npolyhedral parameter sets. Assume these GSIPs are given by polynomials. We\npropose a new approach to solve them as a disjunctive program. This approach is\nbased on the Kurash-Kuhn-Tucker (KKT) conditions of the robust constraint and a\ntechnique called partial Lagrange multiplier expressions. We summarize a\nsemidefinite algorithm and study its convergence properties. Numerical\nexperiments are given to show the efficiency of our method. In addition, we\nchecked its performance in gemstone cutting and robust control applications.",
        "When dealing with a multi-objective optimization problem, obtaining a\ncomprehensive representation of the Pareto front can be computationally\nexpensive. Furthermore, identifying the most representative Pareto solutions\ncan be difficult and sometimes ambiguous. A popular selection are the so-called\nPareto knee solutions, where a small improvement in any objective leads to a\nlarge deterioration in at least one other objective. In this paper, using\nPareto sensitivity, we show how to compute Pareto knee solutions according to\ntheir verbal definition of least maximal change. We refer to the resulting\napproach as the sensitivity knee (snee) approach, and we apply it to\nunconstrained and constrained problems. Pareto sensitivity can also be used to\ncompute the most-changing Pareto sub-fronts around a Pareto solution, where the\npoints are distributed along directions of maximum change, which could be of\ninterest in a decision-making process if one is willing to explore solutions\naround a current one. Our approach is still restricted to scalarized methods,\nin particular to the weighted-sum or epsilon-constrained methods, and require\nthe computation or approximations of first- and second-order derivatives. We\ninclude numerical results from synthetic problems that illustrate the benefits\nof our approach.",
        "Centrality analysis in dynamical network systems is essential for\nunderstanding system behavior. In finite-dimensional settings, controllability\nscores -- namely, the Volumetric Controllability Score (VCS) and the Average\nEnergy Controllability Score (AECS) -- are defined as the unique solutions of\nspecific optimization problems. In this work, we extend these concepts to\ninfinite-dimensional systems by formulating analogous optimization problems.\nMoreover, we prove that these optimization problems have optimal solutions\nunder weak assumptions, and that both VCS and AECS remain unique in the\ninfinite-dimensional context under appropriate assumptions. The uniqueness of\nthe controllability scores is essential to use them as a centrality measure,\nsince it not only reflects the importance of each state in the dynamical\nnetwork but also provides a consistent basis for interpretation and comparison\nacross different researchers. Finally, we illustrate the behavior of VCS and\nAECS with a numerical experiment based on the heat equation.",
        "English version of abstract for \"The Proximal Gradient Method\": The proximal\ngradient method is a splitting algorithm for the minimization of the sum of two\nconvex functions, one of which is smooth. It has applications in areas such as\nmechanics, inverse problems, machine learning, image reconstruction,\nvariational inequalities, statistics, operations research, and optimal\ntransportation. Its formalism encompasses a wide variety of numerical methods\nin optimization such as gradient descent, projected gradient, iterative\nthresholding, alternating projections, the constrained Landweber method, as\nwell as various algorithms in statistics and sparse data analysis. This paper\naims at providing an account of the main properties of the proximal gradient\nmethod and to discuss some of its applications. -- -- -- -- -- -\n  R\\'esum\\'e : La m\\'ethode du gradient proxim\\'e est un algorithme\nd'\\'eclatement pour la minimisation de la somme de deux fonctions convexes,\ndont l'une est lisse. Elle trouve des applications dans des domaines tels que\nla m\\'ecanique, le traitement du signal, les probl\\`emes inverses,\nl'apprentissage automatique, la reconstruction d'images, les in\\'equations\nvariationnelles, les statistiques, la recherche op\\'erationnelle et le\ntransport optimal. Son formalisme englobe une grande vari\\'et\\'e de m\\'ethodes\nnum\\'eriques en optimisation, telles que la descente de gradient, le gradient\nprojet\\'e, la m\\'ethode de seuillage it\\'eratif, la m\\'ethode des projections\naltern\\'ees, la m\\'ethode de Landweber contrainte, ainsi que divers algorithmes\nen statistique et en analyse parcimonieuse de donn\\'ees. Cet article vise \\`a\ndonner un aper\\c{c}u des principales propri\\'et\\'es de la m\\'ethode du gradient\nproxim\\'e et d'aborder certaines de ses applications.",
        "This work considers an Inertial version of Bregman Proximal Gradient\nalgorithm (IBPG) for minimizing the sum of two single-valued functions in\nfinite dimension. We suppose that one of the functions is proper, closed, and\nconvex but non-necessarily smooth whilst the second is a smooth enough function\nbut not necessarily convex. For the latter, we ask the smooth adaptable\nproperty (smad) with respect to some kernel or entropy which allows to remove\nthe very popular global Lipschitz continuity requirement on the gradient of the\nsmooth part. We consider the IBPG under the framework of the triangle scaling\nproperty (TSP) which is a geometrical property for which one can provably\nensure acceleration for a certain subset of kernel\/entropy functions in the\nconvex setting. Based on this property, we provide global convergence\nguarantees when the entropy is strongly convex under the framework of the\nKurdyka-\\L{}ojasiewicz (KL) property. Turning to the local convergence\nproperties, we show that when the nonsmooth part is partly smooth relative to a\nsmooth submanifold, IBPG has a finite activity identification property before\nentering a local linear convergence regime for which we establish a sharp\nestimate of the convergence rate. We report numerical simulations to illustrate\nour theoretical results on low complexity regularized phase retrieval.",
        "An opaque product is a product for which only partial information is\ndisclosed to the buyer at the time of purchase. Opaque products are common in\nsectors such as travel and online retail, where the car type or product color\nis hidden in the opaque product. Opaque products enable sellers to target\ncustomers who prefer a price discount in exchange for being flexible about the\nproduct they receive. In this paper, we integrate opaque products and\ntraditional products together into the multinomial logit (MNL) choice model and\nstudy the associated price and assortment optimization problems. For the price\noptimization problem, we surprisingly show that uniform pricing is optimal\nwhich implies it has the same optimal pricing solution and value as the\ntraditional MNL model. While adding an opaque product priced at the\nrevenue-maximizing price may enhance revenue given arbitrary traditional\nproduct prices, this advantage disappears when all prices are optimized\njointly. For the assortment optimization problem, we show that the\nrevenue-maximizing assortment is nested-by-valuation for uniformly priced\nproducts. For non-uniformly priced cases, we propose a natural\nnested-by-revenue-and-valuation heuristic that performs extremely well in an\nextensive numerical study.",
        "This paper is concerned with a stochastic linear-quadratic leader-follower\ndifferential game with elephant memory. The model is general in that the state\nequation for both the leader and the follower includes the elephant memory of\nthe state and the control, which are part of the diffusion term. Under certain\nassumptions, the state feedback representation of the open-loop Stackelberg\nstrategy is derived by introducing two Riccati equations and a special\nmatrix-valued equation. Finally, theoretical results are illustrated by means\nof an example concerning a dynamic advertising problem with elephant memory.",
        "Peer-to-peer energy trading platforms enable direct electricity exchanges\nbetween peers who belong to the same energy community. In a semi-decentralized\nsystem, a community manager adheres to grid restrictions while optimizing\nsocial welfare. However, with no further supervision, some peers can be\ndiscriminated against from participating in the electricity trades. To solve\nthis issue, this paper proposes an optimization-based mechanism to enable\ndistributionally fair peer-to-peer electricity trading. For the implementation\nof our mechanism, peers are grouped by energy poverty level. The proposed model\naims to redistribute the electricity trades to minimize the maximum Wasserstein\ndistance among the transaction distributions linked to the groups while\nlimiting the sacrifice level with a predefined parameter. We demonstrate the\neffectiveness of our proposal using the IEEE 33-bus distribution grid,\nsimulating an energy community with 1600 peers. Results indicate that up to\n70.1% of unfairness can be eliminated by using our proposed model, even\nachieving a full elimination when including a non-profit community photovoltaic\nplant.",
        "In this work we deal with set-valued functions with values in the power set\nof a separated locally convex space where a nontrivial pointed convex cone\ninduces a partial order relation. A set-valued function is evenly convex if its\nepigraph is an evenly convex set, i.e., it is the intersection of an arbitrary\nfamily of open half-spaces. In this paper we characterize evenly convex\nset-valued functions as the pointwise supremum of its set-valued e-affine\nminorants. Moreover, a suitable conjugation pattern will be developed for these\nfunctions, as well as the counterpart of the biconjugation Fenchel-Moreau\ntheorem.",
        "The gradient type of methods has been a competitive choice in solving large\nscale problems arising from various applications such as machine learning.\nHowever, there is still space to accelerate the gradient methods. To this end,\nin this paper, we pay attention to the cyclic steepest descent method (CSD),\nand prove that the CSD method has a gradient subsequence that is\nR-superlinearly convergent for the 2-dimensional strictly convex quadratic\ncase. Moreover, we propose a new gradient method called triangle steepest\ndescent method (TSD) which has a parameter $j$ to control the number of cycles.\nThis method is motivated by utilizing a geometric property of the steepest\ndescent method (SD) method to get around the zigzag behavior. We show that the\nTSD method is at least R-linearly convergent for strictly convex quadratic\nproblems. The advantage of the TSD method is that it is not sensitive to the\ncondition number of a strictly convex quadratic problem. For example, it\nperforms better than other competitive gradient methods when the condition\nnumber reaches 1e20 or 1e100 for some strictly convex quadratic problems.\nExtensive numerical results verify the efficiency of the TSD method compared to\nother types of gradient methods.",
        "In [Q. Liao et al., Commun. Math. Sci., 20(2022)], a linear-time Sinkhorn\nalgorithm is developed based on dynamic programming, which significantly\nreduces the computational complexity involved in solving optimal transport\nproblems. However, this algorithm is specifically designed for the\nWasserstein-1 metric. We are curious whether the preceding dynamic programming\nframework can be extended to tackle optimal transport problems with different\ntransport costs. Notably, two special kinds of optimal transport problems, the\nSinkhorn ranking and the far-field reflector and refractor problems, are\nclosely associated with the log-type transport costs. Interestingly, by\nemploying series rearrangement and dynamic programming techniques, it is\nfeasible to perform the matrix-vector multiplication within the Sinkhorn\niteration in linear time for this type of cost. This paper provides a detailed\nexposition of its implementation and applications, with numerical simulations\ndemonstrating the effectiveness and efficiency of our methods.",
        "We study the product of two relaxed cutters having a common fixed point. We\nassume that one of the relaxation parameters is greater than two so that the\ncorresponding relaxed cutter is no longer quasi-nonexpansive, but rather\ndemicontractive. We show that if both of the operators are (weakly\/linearly)\nregular, then under certain conditions, the resulting product inherits the same\ntype of regularity. We then apply these results to proving convergence in the\nweak, norm and linear sense of algorithms that employ such products.",
        "We present a random-subspace variant of cubic regularization algorithm that\nchooses the size of the subspace adaptively, based on the rank of the projected\nsecond derivative matrix. Iteratively, our variant only requires access to\n(small-dimensional) projections of first- and second-order problem derivatives\nand calculates a reduced step inexpensively. The ensuing method maintains the\noptimal global rate of convergence of (full-dimensional) cubic regularization,\nwhile showing improved scalability both theoretically and numerically,\nparticularly when applied to low-rank functions. When applied to the latter,\nour algorithm naturally adapts the subspace size to the true rank of the\nfunction, without knowing it a priori.",
        "Mean estimation is a fundamental task in statistics and a focus within\ndifferentially private statistical estimation. While univariate methods based\non the Gaussian mechanism are widely used in practice, more advanced techniques\nsuch as the exponential mechanism over quantiles offer robustness and improved\nperformance, especially for small sample sizes. Tukey depth mechanisms carry\nthese advantages to multivariate data, providing similar strong theoretical\nguarantees. However, practical implementations fall behind these theoretical\ndevelopments.\n  In this work, we take the first step to bridge this gap by implementing the\n(Restricted) Tukey Depth Mechanism, a theoretically optimal mean estimator for\nmultivariate Gaussian distributions, yielding improved practical methods for\nprivate mean estimation. Our implementations enable the use of these mechanisms\nfor small sample sizes or low-dimensional data. Additionally, we implement\nvariants of these mechanisms that use approximate versions of Tukey depth,\ntrading off accuracy for faster computation. We demonstrate their efficiency in\npractice, showing that they are viable options for modest dimensions. Given\ntheir strong accuracy and robustness guarantees, we contend that they are\ncompetitive approaches for mean estimation in this regime. We explore future\ndirections for improving the computational efficiency of these algorithms by\nleveraging fast polytope volume approximation techniques, paving the way for\nmore accurate private mean estimation in higher dimensions.",
        "This work presents a hybrid pressure face-centred finite volume (FCFV) solver\nto simulate steady-state incompressible Navier-Stokes flows. The method\nleverages the robustness, in the incompressible limit, of the hybridisable\ndiscontinuous Galerkin paradigm for compressible and weakly compressible flows\nto derive the formulation of a novel, low-order face-based discretisation. The\nincompressibility constraint is enforced in a weak sense, by introducing an\ninter-cell mass flux defined in terms of a new, hybrid variable, representing\nthe pressure at the cell faces. This results in a new hybridisation strategy\nwhere cell variables (velocity, pressure and deviatoric strain rate tensor) are\nexpressed as a function of velocity and pressure at the barycentre of the cell\nfaces. The hybrid pressure formulation provides first-order convergence of all\nvariables, including the stress, independently of cell type, stretching and\ndistortion. Numerical benchmarks of Navier-Stokes flows at low and moderate\nReynolds numbers, in two and three dimensions, are presented to evaluate\naccuracy and robustness of the method. In particular, the hybrid pressure\nformulation outperforms the FCFV method when convective effects are relevant,\nachieving accurate predictions on significantly coarser meshes.",
        "Quantum computers have the potential to revolutionise our understanding of\nthe microscopic behaviour of materials and chemical processes by enabling\nhigh-accuracy electronic structure calculations to scale more efficiently than\nis possible using classical computers. Current quantum computing hardware\ndevices suffer from the dual challenges of noise and cost, which raises the\nquestion of what practical value these devices might offer before full fault\ntolerance is achieved and economies of scale enable cheaper access. Here we\nexamine the practical value of noisy quantum computers as tools for\nhigh-accuracy electronic structure, by using a Quantinuum ion-trap quantum\ncomputer to predict the ionisation potential of helium. By combining a series\nof techniques suited for use with current hardware including qubit-efficient\nencoding coupled with chemical insight, low-cost variational optimisation with\nhardware-adapted quantum circuits, and moments-based corrections, we obtain an\nionisation potential of 24.5536 (+0.0011, -0.0005) eV, which agrees with the\nexperimentally measured value to within true chemical accuracy, and with high\nstatistical confidence. The methods employed here can be generalised to predict\nother properties and expand our understanding of the value that might be\nprovided by near-term quantum computers.",
        "In high-energy elementary collisions the space-time ordering of parton\nbranching processes is not accessible experimentally. In contrast, in heavy-ion\ncollisions, parton showers interact with a spatially extended dense medium.\nThis sets a reference length scale with respect to which the space-time\nordering may be analysed. Here, we explore the possibility of identifying\nexperimental signatures of the QCD formation time, $\\tau_f$, on the level of a\nsingle parton splitting. Since heavy flavour offers an additional handle on\ntracing the propagation of individual quarks through the medium, we focus on\nthe $g\\to c\\bar{c}$ splitting. Combining adapted versions of the\nCambridge-Aachen and FlavourCone jet finding algorithms with grooming\ntechniques, we show how the kinematics of such splittings can be reconstructed\nwith high fidelity using either final state partons or hadrons, and how the\nformation time distribution of parton splittings can be constructed therefrom.\nMedium modification leads to a characteristic modification of this $\\tau_f$\ndistribution. This effect can be used to construct experimentally-accessible\nratios of $\\tau_f$ distributions, in which the sensitivity of the medium\nmodification to the QCD formation time becomes measurable.",
        "Operator learning is the approximation of operators between infinite\ndimensional Banach spaces using machine learning approaches. While most\nprogress in this area has been driven by variants of deep neural networks such\nas the Deep Operator Network and Fourier Neural Operator, the theoretical\nguarantees are often in the form of a universal approximation property.\nHowever, the existence theorems do not guarantee that an accurate operator\nnetwork is obtainable in practice. Motivated by the recent kernel-based\noperator learning framework, we propose a random feature operator learning\nmethod with theoretical guarantees and error bounds. The random feature method\ncan be viewed as a randomized approximation of a kernel method, which\nsignificantly reduces the computation requirements for training. We provide a\ngeneralization error analysis for our proposed random feature operator learning\nmethod along with comprehensive numerical results. Compared to kernel-based\nmethod and neural network methods, the proposed method can obtain similar or\nbetter test errors across benchmarks examples with significantly reduced\ntraining times. An additional advantages it that our implementation is simple\nand does require costly computational resources, such as GPU.",
        "The fast and accurate estimation of planetary mass-loss rates is critical for\nplanet population and evolution modelling. We use machine learning (ML) for\nfast interpolation across an existing large grid of hydrodynamic upper\natmosphere models, providing mass-loss rates for any planet inside the grid\nboundaries with superior accuracy compared to previously published\ninterpolation schemes. We consider an already available grid comprising about\n11000 hydrodynamic upper atmosphere models for training and generate an\nadditional grid of about 250 models for testing purposes. We develop the ML\ninterpolation scheme (dubbed \"atmospheric Mass Loss INquiry frameworK\"; MLink)\nusing a Dense Neural Network, further comparing the results with what was\nobtained employing classical approaches (e.g. linear interpolation and radial\nbasis function-based regression). Finally, we study the impact of the different\ninterpolation schemes on the evolution of a small sample of carefully selected\nsynthetic planets. MLink provides high-quality interpolation across the entire\nparameter space by significantly reducing both the number of points with large\ninterpolation errors and the maximum interpolation error compared to previously\navailable schemes. For most cases, evolutionary tracks computed employing MLink\nand classical schemes lead to comparable planetary parameters at\nGyr-timescales. However, particularly for planets close to the top edge of the\nradius gap, the difference between the predicted planetary radii at a given age\nof tracks obtained employing MLink and classical interpolation schemes can\nexceed the typical observational uncertainties. Machine learning can be\nsuccessfully used to estimate atmospheric mass-loss rates from model grids\npaving the way to explore future larger and more complex grids of models\ncomputed accounting for more physical processes.",
        "The equation of state (EoS) of neutron matter plays a decisive role to\nunderstand the neutron star properties and the gravitational waves from neutron\nstar mergers. At sufficient densities, the appearance of hyperons generally\nsoftens the EoS, leading to a reduction in the maximum mass of neutron stars\nwell below the observed values of about 2 solar masses. Even though repulsive\nthree-body forces are known to solve this so-called ``hyperon puzzle'', so far\nperforming \\textit{ab initio} calculations with a substantial number of\nhyperons for neutron star properties has remained elusive. Starting from the\nnewly developed auxiliary field quantum Monte Carlo algorithm to simulate\nhyper-neutron matter (HNM) without any sign oscillations, we derive three\ndistinct EoSs by employing the state-of-the-art Nuclear Lattice Effective Field\nTheory. We include $N\\Lambda$, $\\Lambda\\Lambda$ two-body forces, $NN\\Lambda$,\nand $N\\Lambda\\Lambda$ three-body forces. Consequently, we determine essential\nastrophysical quantities such as the neutron star mass, radius, tidal\ndeformability, and the universal $I$-Love-$Q$ relation. The maximum mass,\nradius and tidal deformability of a $1.4M_\\odot$ neutron star are predicted to\nbe $2.17(1)(1)~M_\\odot$, $R_{1.4M\\odot}=13.10(1)(7)~$km, and\n$\\Lambda_{1.4M_\\odot}=597(5)(18)$, respectively, based on our most realistic\nEoS. These predictions are in good agreement with the latest astrophysical\nconstraints derived from observations of massive neutron stars, gravitational\nwaves, and joint mass-radius measurements. Also, for the first time in\n\\textit{ab initio} calculations, we investigate both non-rotating and rotating\nneutron star configurations. The results indicate that the impact of rotational\ndynamics on the maximum mass is small, regardless of whether hyperons are\npresent in the EoS or not.",
        "In this work we generalize ${\\cal M}_{2}$-extension that has been introduced\nrecently. For illustration we use the KdV equation. We present five different\n${\\cal M}_{3}$-extensions of the KdV equation and their recursion operators. We\ngive a compact form of ${\\cal M}_{n}$-extension of the KdV equation and\nrecursion operator of the coupled KdV system. The method of ${\\cal\nM}_{n}$-extension can be applied to any integrable scalar equation to obtain\nintegrable multi-field system of equations. We also present unshifted and\nshifted nonlocal reductions of an example of ${\\cal M}_{3}$-extension of KdV.",
        "Primordial black holes (PBHs) have been explored as potential dark matter\ncandidates, with various astrophysical observations placing upper limits on the\nfraction $f_\\mathrm{PBH}$ of dark matter in the form of PBHs. However, a\nlargely underutilized probe of PBH abundance is the temperature of the\nintergalactic medium (IGM), inferred from the thermal broadening of absorption\nlines in the Lyman-$\\alpha$ forest of quasar spectra. PBHs inject energy into\nthe IGM via Hawking radiation, altering its thermal evolution. In this work, we\nconstrain this energy injection by self-consistently modeling its interplay\nwith the cosmological ultraviolet background from galaxies and supermassive\nblack holes. Leveraging IGM temperature measurements spanning the past twelve\nbillion years ($z \\sim 0$ to $6$), we derive one of the most stringent\nconstraints on PBH-induced heating from light PBHs within the mass range\n$10^{15}\\unicode{x2013}10^{17}$ g. Specifically, for $M_\\mathrm{PBH} = 10^{16}$\ng, we find $f_\\mathrm{PBH} < 5 \\times 10^{-5}$ at 95% confidence, with the\nbound scaling approximately as $M_\\mathrm{PBH}^{4}$ at other masses. Our\ninclusion of helium reionization and low-redshift temperature measurements\nstrengthens previous IGM-based PBH constraints by an order of magnitude or\nmore. Compared to other existing limits, our result is among the strongest,\nsecond only to the constraints from the 511 keV line from the Galactic Centre,\nbut with distinct systematics. More broadly, this study highlights the IGM\nthermal history as a powerful and independent probe of beyond-standard-model\nphysics.",
        "We study the response of mono-energetic stellar populations with initially\nisotropic kinematics to impulsive and adiabatic changes to an underlying dark\nmatter potential. Half-light radii expand and velocity dispersions decrease as\nenclosed dark matter is removed. The details of this expansion and cooling\ndepend on the time scale on which the underlying potential changes. In the\nadiabatic regime, the product of half-light radius and average velocity\ndispersion is conserved. We show that the stellar populations maintain\ncentrally isotropic kinematics throughout their adiabatic evolution, and their\ndensities can be approximated by a family of analytical radial profiles.\nMetallicity gradients within the galaxy flatten as dark matter is slowly\nremoved. In the case of strong impulsive perturbations, stellar populations\ndevelop power-law-like density tails with radially biased kinematics. We show\nthat the distribution of stellar binding energies within the dark matter halo\nsubstantially widens after an impulsive perturbation, no matter the sign of the\nperturbation. This allows initially energetically separated stellar populations\nto mix, to the extent that previously chemo-dynamically distinct populations\nmay masquerade as a single population with large metallicity and energy spread.\nFinally, we show that in response to an impulsive perturbation, stellar\npopulations that are deeply embedded in cored dark matter halos undergo a\nseries of damped oscillations before reaching a virialised equilibrium state,\ndriven by inefficient phase mixing in the harmonic potentials of cored halos.\nThis slow return to equilibrium adds substantial systematic uncertainty to\ndynamical masses estimated from Jeans modeling or the virial theorem.",
        "Developing a dark matter detector with wide mass tunability is an immensely\ndesirable property, yet it is challenging due to maintaining strong\nsensitivity. Resonant cavities for dark matter detection have traditionally\nemployed mechanical tuning, moving parts around to change electromagnetic\nboundary conditions. However, these cavities have proven challenging to operate\nin sub-Kelvin cryogenic environments due to differential thermal contraction,\nlow heat capacities, and low thermal conductivities. Instead, we develop an\nelectronically tunable cavity architecture by coupling a superconducting 3D\nmicrowave cavity with a DC flux tunable SQUID. With a flux delivery system\nengineered to maintain high coherence in the cavity, we perform a hidden-photon\ndark matter search below the quantum-limited threshold. A microwave photon\ncounting technique is employed through repeated quantum non-demolition\nmeasurements using a transmon qubit. With this device, we perform a\nhidden-photon search with a dark count rate of around 64 counts\/s and constrain\nthe kinetic mixing angle to ${\\varepsilon}< 4\\times 10^{-13}$ in a tunable band\nfrom 5.672 GHz to 5.694 GHz. By coupling multimode tunable cavities to the\ntransmon, wider hidden-photon searching ranges are possible.",
        "Teleportation, introduced in science fiction literature, is an instantaneous\nchange of the position of a microscopic object. Two teleportation-like\nphenomena were predicted by quantum mechanics: quantum teleportation and,\nrecently, quantum particle teleportation. The former is investigated\nexperimentally and has applications in quantum communication and computing.\n  Here, we introduced the third teleportation-like phenomenon - an apparent\nteleportation. It seems to be a natural consequence of elementary particles and\nantiparticles of the Standard Model being indistinguishable. We give an example\nof a process leading to the apparent teleportation within a toy model of\nboson-like particles. It utilizes the local transport of particles and\nantiparticles and the local creation and annihilation of particle-antiparticle\npairs. Furthermore, we suggest a method to observe the apparent teleportation\nin nucleus-nucleus collisions at properly selected collision energy. The method\nrequires the measurement of correlations between momenta of charm and anticharm\nhadrons in collisions with a single $c\\bar{c}$ pair being produced. The\nultimate prediction following the apparent teleportation hypothesis is the\nuncorrelated emission of charm and anticharm hadrons. It can be tested by\ncontemporary experiments.\n  Observing the apparent teleportation would uncover the basic transport\nproperties of indistinguishable particles. In particular, the apparent\nteleportation may explain the rapid thermalisation of the system created in\ncollisions of two atomic nuclei. Theoretical and experimental efforts are\nneeded to observe the apparent teleportation processes and study their\nproperties.",
        "Recently, black hole models in a nonlinear modification of the Maxwell\nelectrodynamics were suggested, possessing simultaneously properties of an\nextreme charge and regularity (Bronnikov K. A., Phys. Rev. D, 110 (2024)\n024021). We study quasinormal modes of a massive scalar field around such black\nholes and show that they are characterized by a comparatively small damping\nrate, indicating the possible existence of arbitrarily long-lived quasinormal\nmodes, called quasi-resonances.",
        "One of the most important open questions in planet formation is how dust\ngrains in a protoplanetary disk manage to overcome growth barriers and form the\n$\\sim$100km planet building blocks that we call planetesimals. There appears to\nbe a gap between the largest grains that can be produce by coagulation, and the\nsmallest grains that are needed for the streaming instability (SI) to form\nplanetesimals. Here we explore a novel hypothesis: That dust coagulation and\nthe SI work in tandem. That they form a feedback loop where each one boosts the\naction of the other to bridge the gap between dust grains and planetesimals. We\ndevelop a semi-analytical model of dust concentration due to the SI, and an\nanalytic model of how the SI affects the fragmentation and radial drift\nbarriers. We then combine those to model our proposed feedback loop. In the\nfragmentation-limited regime, we find a powerful synergy between the SI and\ndust growth that drastically increases both grain sizes and densities. We find\nthat a midplane dust-to-gas ratio of $\\epsilon \\ge 0.3$ is a sufficient\ncondition for the feedback loop to reach the planetesimal-forming region for\nturbulence values $10^{-4} \\le \\alpha \\le 10^{-3}$ and grain sizes $0.01 \\le\n{\\rm St} \\le 0.1$. In contrast, the drift-limited regime only shows grain\ngrowth, without significant dust accumulation. Planet formation in the\ndrift-limited portion of the disk may require other processes (particle traps)\nto halt radial drift.",
        "We study finite-volume (FV) corrections to determinations of $g_A$ via\nlattice quantum chromodynamics (QCD) using analytic results and numerical\nanalysis. We observe that $SU(2)$ Heavy Baryon Chiral Perturbation Theory does\nnot provide an unambiguous prediction for the sign of the FV correction, which\nis not surprising when one also considers large-$N_c$ constraints on the axial\ncouplings. We further show that non-monotonic FV corrections are naturally\nallowed when one considers either including explicit $\\Delta$-resonance degrees\nof freedom or one works to higher orders in the chiral expansion. We\ninvestigate the potential impact of these FV corrections with a precision study\nof $g_A$ using models of FV corrections that are monotonic and non-monotonic.\nUsing lattice QCD data that is approximately at the 1% level of precision, we\ndo not see significant evidence of non-monotonic corrections. Looking forward\nto the next phase of lattice QCD calculations, we estimate that calculations\nthat are between the 0.1%-1%-level of precision may be sensitive to these FV\nartifacts. Finally, we present an update of the CalLat prediction of $g_A$ in\nthe isospin limit with sub-percent precision, $g_A^{\\rm QCD} = 1.2674(96)$."
      ]
    }
  },
  {
    "id":2411.07018,
    "research_type":"applied",
    "start_id":"b12",
    "start_title":"Field Emission in Superconducting Accelerators: Instrumented Measurements for Its Understanding and Mitigation",
    "start_abstract":"Several new accelerator projects are adopting superconducting RF (SRF) technology. When accelerating SRF cavities maintain high RF gradients, field emission, the emission of electrons from cavity walls, can occur and may impact operational cavity gradient, radiological environment via activated components, and reliability. In this talk, we will discuss instrumented measurements of field emission from the two 1.1 GeV superconducting continuous wave (CW) linacs in CEBAF. The goal is to improve the understanding of field emission sources originating from cryomodule production, installation and operation. Such basic knowledge is needed in guiding field emission control, mitigation, and reduction toward high gradient and reliable operation of superconducting accelerators.",
    "start_categories":[
      "physics.acc-ph"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b18"
      ],
      "title":[
        "Accelerating cavity fault prediction using deep learning at Jefferson laboratory"
      ],
      "abstract":[
        "Abstract Accelerating cavities are an integral part of the continuous electron beam accelerator facility (CEBAF) at Jefferson Laboratory. When any over 400 in CEBAF experiences a fault, it disrupts delivery to experimental user halls. In this study, we propose use deep learning model predict slowly developing cavity faults. By utilizing pre-fault signals, train long short-term memory-convolutional neural network binary classifier distinguish between radio-frequency (RF) signals during normal operation and RF indicative impending We optimize by adjusting fault confidence threshold implementing multiple consecutive window criterion identify events, ensuring low false positive rate. Results obtained from analysis real dataset collected accelerating simulating deployed scenario demonstrate model\u2019s ability with 99.99% accuracy correctly 80% Notably, these achievements were achieved context highly imbalanced dataset, predictions made several hundred milliseconds before onset fault. Anticipating faults enables preemptive measures improve operational efficiency preventing or mitigating their occurrence."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "An Efficient Diffusion-based Non-Autoregressive Solver for Traveling\n  Salesman Problem",
        "A HEART for the environment: Transformer-Based Spatiotemporal Modeling\n  for Air Quality Prediction",
        "Near-optimal Regret Using Policy Optimization in Online MDPs with\n  Aggregate Bandit Feedback",
        "Primal-Dual Sample Complexity Bounds for Constrained Markov Decision\n  Processes with Multiple Constraints",
        "Filter, Obstruct and Dilute: Defending Against Backdoor Attacks on\n  Semi-Supervised Learning",
        "An Efficient Real Time DDoS Detection Model Using Machine Learning\n  Algorithms",
        "Exploring Representation-Aligned Latent Space for Better Generation",
        "ARMAX identification of low rank graphical models",
        "ATM-Net: Adaptive Termination and Multi-Precision Neural Networks for\n  Energy-Harvested Edge Intelligence",
        "Iterative Multi-Agent Reinforcement Learning: A Novel Approach Toward\n  Real-World Multi-Echelon Inventory Optimization",
        "On Rollouts in Model-Based Reinforcement Learning",
        "Training LLMs with MXFP4",
        "Contextual Linear Bandits with Delay as Payoff",
        "DRAMA: Diverse Augmentation from Large Language Models to Smaller Dense\n  Retrievers",
        "LexPro-1.0 Technical Report",
        "Observer-Based Data-Driven Consensus Control for Nonlinear Multi-Agent\n  Systems against DoS and FDI attacks",
        "Automated Extraction of Spatio-Semantic Graphs for Identifying Cognitive\n  Impairment",
        "Generative AI & Changing Work: Systematic Review of Practitioner-led\n  Work Transformations through the Lens of Job Crafting",
        "Learning Code-Edit Embedding to Model Student Debugging Behavior",
        "Monochromatic graph decompositions and monochromatic piercing inspired\n  by anti-Ramsey colorings",
        "Select2Drive: Pragmatic Communications for Real-Time Collaborative\n  Autonomous Driving",
        "Determination of the density in the linear elastic wave equation",
        "Semicustom Frontend VLSI Design and Analysis of a 32-bit Brent-Kung\n  Adder in Cadence Suite",
        "DiffCLIP: Differential Attention Meets CLIP",
        "Splitting algorithms for paraxial and It\\^o-Schr\\\"odinger models of wave\n  propagation in random media",
        "Numerical homological regularities over positively graded algebras",
        "Provable Benefits of Task-Specific Prompts for In-context Learning",
        "Chain-of-Tools: Utilizing Massive Unseen Tools in the CoT Reasoning of\n  Frozen Language Models"
      ],
      "abstract":[
        "Recent advances in neural models have shown considerable promise in solving\nTraveling Salesman Problems (TSPs) without relying on much hand-crafted\nengineering. However, while non-autoregressive (NAR) approaches benefit from\nfaster inference through parallelism, they typically deliver solutions of\ninferior quality compared to autoregressive ones. To enhance the solution\nquality while maintaining fast inference, we propose DEITSP, a diffusion model\nwith efficient iterations tailored for TSP that operates in a NAR manner.\nFirstly, we introduce a one-step diffusion model that integrates the controlled\ndiscrete noise addition process with self-consistency enhancement, enabling\noptimal solution prediction through simultaneous denoising of multiple\nsolutions. Secondly, we design a dual-modality graph transformer to bolster the\nextraction and fusion of features from node and edge modalities, while further\naccelerating the inference with fewer layers. Thirdly, we develop an efficient\niterative strategy that alternates between adding and removing noise to improve\nexploration compared to previous diffusion methods. Additionally, we devise a\nscheduling framework to progressively refine the solution space by adjusting\nnoise levels, facilitating a smooth search for optimal solutions. Extensive\nexperiments on real-world and large-scale TSP instances demonstrate that DEITSP\nperforms favorably against existing neural approaches in terms of solution\nquality, inference latency, and generalization ability. Our code is available\nat $\\href{https:\/\/github.com\/DEITSP\/DEITSP}{https:\/\/github.com\/DEITSP\/DEITSP}$.",
        "Accurate and reliable air pollution forecasting is crucial for effective\nenvironmental management and policy-making. llull-environment is a\nsophisticated and scalable forecasting system for air pollution, inspired by\nprevious models currently operational in Madrid and Valladolid (Spain). It\ncontains (among other key components) an encoder-decoder convolutional neural\nnetwork to forecast mean pollution levels for four key pollutants (NO$_2$,\nO$_3$, PM$_{10}$, PM$_{2.5}$) using historical data, external forecasts, and\nother contextual features. This paper investigates the augmentation of this\nneural network with an attention mechanism to improve predictive accuracy. The\nproposed attention mechanism pre-processes tensors containing the input\nfeatures before passing them to the existing mean forecasting model. The\nresulting model is a combination of several architectures and ideas and can be\ndescribed as a \"Hybrid Enhanced Autoregressive Transformer\", or HEART. The\neffectiveness of the approach is evaluated by comparing the mean square error\n(MSE) across different attention layouts against the system without such a\nmechanism. We observe a significant reduction in MSE of up to 22%, with an\naverage of 7.5% across tested cities and pollutants. The performance of a given\nattention mechanism turns out to depend on the pollutant, highlighting the\ndifferences in their creation and dissipation processes. Our findings are not\nrestricted to optimizing air quality prediction models, but are applicable\ngenerally to (fixed length) time series forecasting.",
        "We study online finite-horizon Markov Decision Processes with adversarially\nchanging loss and aggregate bandit feedback (a.k.a full-bandit). Under this\ntype of feedback, the agent observes only the total loss incurred over the\nentire trajectory, rather than the individual losses at each intermediate step\nwithin the trajectory. We introduce the first Policy Optimization algorithms\nfor this setting. In the known-dynamics case, we achieve the first\n\\textit{optimal} regret bound of $\\tilde \\Theta(H^2\\sqrt{SAK})$, where $K$ is\nthe number of episodes, $H$ is the episode horizon, $S$ is the number of\nstates, and $A$ is the number of actions. In the unknown dynamics case we\nestablish regret bound of $\\tilde O(H^3 S \\sqrt{AK})$, significantly improving\nthe best known result by a factor of $H^2 S^5 A^2$.",
        "This paper addresses the challenge of solving Constrained Markov Decision\nProcesses (CMDPs) with $d > 1$ constraints when the transition dynamics are\nunknown, but samples can be drawn from a generative model. We propose a\nmodel-based algorithm for infinite horizon CMDPs with multiple constraints in\nthe tabular setting, aiming to derive and prove sample complexity bounds for\nlearning near-optimal policies. Our approach tackles both the relaxed and\nstrict feasibility settings, where relaxed feasibility allows some constraint\nviolations, and strict feasibility requires adherence to all constraints. The\nmain contributions include the development of the algorithm and the derivation\nof sample complexity bounds for both settings. For the relaxed feasibility\nsetting we show that our algorithm requires $\\tilde{\\mathcal{O}} \\left( \\frac{d\n|\\mathcal{S}| |\\mathcal{A}| \\log(1\/\\delta)}{(1-\\gamma)^3\\epsilon^2} \\right)$\nsamples to return $\\epsilon$-optimal policy, while in the strict feasibility\nsetting it requires $\\tilde{\\mathcal{O}} \\left( \\frac{d^3 |\\mathcal{S}|\n|\\mathcal{A}| \\log(1\/\\delta)}{(1-\\gamma)^5\\epsilon^2{\\zeta_{\\mathbf{c}}^*}^2}\n\\right)$ samples.",
        "Recent studies have verified that semi-supervised learning (SSL) is\nvulnerable to data poisoning backdoor attacks. Even a tiny fraction of\ncontaminated training data is sufficient for adversaries to manipulate up to\n90\\% of the test outputs in existing SSL methods. Given the emerging threat of\nbackdoor attacks designed for SSL, this work aims to protect SSL against such\nrisks, marking it as one of the few known efforts in this area. Specifically,\nwe begin by identifying that the spurious correlations between the backdoor\ntriggers and the target class implanted by adversaries are the primary cause of\nmanipulated model predictions during the test phase. To disrupt these\ncorrelations, we utilize three key techniques: Gaussian Filter, complementary\nlearning and trigger mix-up, which collectively filter, obstruct and dilute the\ninfluence of backdoor attacks in both data pre-processing and feature learning.\nExperimental results demonstrate that our proposed method, Backdoor Invalidator\n(BI), significantly reduces the average attack success rate from 84.7\\% to\n1.8\\% across different state-of-the-art backdoor attacks. It is also worth\nmentioning that BI does not sacrifice accuracy on clean data and is supported\nby a theoretical guarantee of its generalization capability.",
        "Distributed Denial of Service attacks have become a significant threat to\nindustries and governments leading to substantial financial losses. With the\ngrowing reliance on internet services, DDoS attacks can disrupt services by\noverwhelming servers with false traffic causing downtime and data breaches.\nAlthough various detection techniques exist, selecting an effective method\nremains challenging due to trade-offs between time efficiency and accuracy.\nThis research focuses on developing an efficient real-time DDoS detection\nsystem using machine learning algorithms leveraging the UNB CICDDoS2019 dataset\nincluding various traffic features. The study aims to classify DDoS and\nnon-DDoS traffic through various ML classifiers including Logistic Regression,\nK-Nearest Neighbors, Random Forest, Support Vector Machine, Naive Bayes. The\ndataset is preprocessed through data cleaning, standardization and feature\nselection techniques using Principal Component Analysis. The research explores\nthe performance of these algorithms in terms of precision, recall and F1-score\nas well as time complexity to create a reliable system capable of real-time\ndetection and mitigation of DDoS attacks. The findings indicate that RF,\nAdaBoost and XGBoost outperform other algorithms in accuracy and efficiency,\nmaking them ideal candidates for real-time applications.",
        "Generative models serve as powerful tools for modeling the real world, with\nmainstream diffusion models, particularly those based on the latent diffusion\nmodel paradigm, achieving remarkable progress across various tasks, such as\nimage and video synthesis. Latent diffusion models are typically trained using\nVariational Autoencoders (VAEs), interacting with VAE latents rather than the\nreal samples. While this generative paradigm speeds up training and inference,\nthe quality of the generated outputs is limited by the latents' quality.\nTraditional VAE latents are often seen as spatial compression in pixel space\nand lack explicit semantic representations, which are essential for modeling\nthe real world. In this paper, we introduce ReaLS (Representation-Aligned\nLatent Space), which integrates semantic priors to improve generation\nperformance. Extensive experiments show that fundamental DiT and SiT trained on\nReaLS can achieve a 15% improvement in FID metric. Furthermore, the enhanced\nsemantic latent space enables more perceptual downstream tasks, such as\nsegmentation and depth estimation.",
        "In large-scale systems, complex internal relationships are often present.\nSuch interconnected systems can be effectively described by low rank stochastic\nprocesses. When identifying a predictive model of low rank processes from\nsampling data, the rank-deficient property of spectral densities is often\nobscured by the inevitable measurement noise in practice. However, existing low\nrank identification approaches often did not take noise into explicit\nconsideration, leading to non-negligible inaccuracies even under weak noise. In\nthis paper, we address the identification issue of low rank processes under\nmeasurement noise. We find that the noisy measurement model admits a sparse\nplus low rank structure in latent-variable graphical models. Specifically, we\nfirst decompose the problem into a maximum entropy covariance extension\nproblem, and a low rank graphical estimation problem based on an autoregressive\nmoving-average with exogenous input (ARMAX) model. To identify the ARMAX low\nrank graphical models, we propose an estimation approach based on maximum\nlikelihood. The identifiability and consistency of this approach are proven\nunder certain conditions. Simulation results confirm the reliable performance\nof the entire algorithm in both the parameter estimation and noisy data\nfiltering.",
        "ATM-Net is a novel neural network architecture tailored for energy-harvested\nIoT devices, integrating adaptive termination points with multi-precision\ncomputing. It dynamically adjusts computational precision (32\/8\/4-bit) and\nnetwork depth based on energy availability via early exit points. An\nenergy-aware task scheduler optimizes the energy-accuracy trade-off.\nExperiments on CIFAR-10, PlantVillage, and TissueMNIST show ATM-Net achieves up\nto 96.93% accuracy while reducing power consumption by 87.5% with Q4\nquantization compared to 32-bit operations. The power-delay product improves\nfrom 13.6J to 0.141J for DenseNet-121 and from 10.3J to 0.106J for ResNet-18,\ndemonstrating its suitability for energy-harvesting systems.",
        "Multi-echelon inventory optimization (MEIO) is critical for effective supply\nchain management, but its inherent complexity can pose significant challenges.\nHeuristics are commonly used to address this complexity, yet they often face\nlimitations in scope and scalability. Recent research has found deep\nreinforcement learning (DRL) to be a promising alternative to traditional\nheuristics, offering greater versatility by utilizing dynamic decision-making\ncapabilities. However, since DRL is known to struggle with the curse of\ndimensionality, its relevance to complex real-life supply chain scenarios is\nstill to be determined. This thesis investigates DRL's applicability to MEIO\nproblems of increasing complexity. A state-of-the-art DRL model was replicated,\nenhanced, and tested across 13 supply chain scenarios, combining diverse\nnetwork structures and parameters. To address DRL's challenges with\ndimensionality, additional models leveraging graph neural networks (GNNs) and\nmulti-agent reinforcement learning (MARL) were developed, culminating in the\nnovel iterative multi-agent reinforcement learning (IMARL) approach. IMARL\ndemonstrated superior scalability, effectiveness, and reliability in optimizing\ninventory policies, consistently outperforming benchmarks. These findings\nconfirm the potential of DRL, particularly IMARL, to address real-world supply\nchain challenges and call for additional research to further expand its\napplicability.",
        "Model-based reinforcement learning (MBRL) seeks to enhance data efficiency by\nlearning a model of the environment and generating synthetic rollouts from it.\nHowever, accumulated model errors during these rollouts can distort the data\ndistribution, negatively impacting policy learning and hindering long-term\nplanning. Thus, the accumulation of model errors is a key bottleneck in current\nMBRL methods. We propose Infoprop, a model-based rollout mechanism that\nseparates aleatoric from epistemic model uncertainty and reduces the influence\nof the latter on the data distribution. Further, Infoprop keeps track of\naccumulated model errors along a model rollout and provides termination\ncriteria to limit data corruption. We demonstrate the capabilities of Infoprop\nin the Infoprop-Dyna algorithm, reporting state-of-the-art performance in\nDyna-style MBRL on common MuJoCo benchmark tasks while substantially increasing\nrollout length and data quality.",
        "Low precision (LP) datatypes such as MXFP4 can accelerate matrix\nmultiplications (GEMMs) and reduce training costs. However, directly using\nMXFP4 instead of BF16 during training significantly degrades model quality. In\nthis work, we present the first near-lossless training recipe that uses MXFP4\nGEMMs, which are $2\\times$ faster than FP8 on supported hardware. Our key\ninsight is to compute unbiased gradient estimates with stochastic rounding\n(SR), resulting in more accurate model updates. However, directly applying SR\nto MXFP4 can result in high variance from block-level outliers, harming\nconvergence. To overcome this, we use the random Hadamard tranform to\ntheoretically bound the variance of SR. We train GPT models up to 6.7B\nparameters and find that our method induces minimal degradation over\nmixed-precision BF16 training. Our recipe computes $>1\/2$ the training FLOPs in\nMXFP4, enabling an estimated speedup of $>1.3\\times$ over FP8 and $>1.7\\times$\nover BF16 during backpropagation.",
        "A recent work by Schlisselberg et al. (2024) studies a delay-as-payoff model\nfor stochastic multi-armed bandits, where the payoff (either loss or reward) is\ndelayed for a period that is proportional to the payoff itself. While this\ncaptures many real-world applications, the simple multi-armed bandit setting\nlimits the practicality of their results. In this paper, we address this\nlimitation by studying the delay-as-payoff model for contextual linear bandits.\nSpecifically, we start from the case with a fixed action set and propose an\nefficient algorithm whose regret overhead compared to the standard no-delay\ncase is at most $D\\Delta_{\\max}\\log T$, where $T$ is the total horizon, $D$ is\nthe maximum delay, and $\\Delta_{\\max}$ is the maximum suboptimality gap. When\npayoff is loss, we also show further improvement of the bound, demonstrating a\nseparation between reward and loss similar to Schlisselberg et al. (2024).\nContrary to standard linear bandit algorithms that construct least squares\nestimator and confidence ellipsoid, the main novelty of our algorithm is to\napply a phased arm elimination procedure by only picking actions in a\nvolumetric spanner of the action set, which addresses challenges arising from\nboth payoff-dependent delays and large action sets. We further extend our\nresults to the case with varying action sets by adopting the reduction from\nHanna et al. (2023). Finally, we implement our algorithm and showcase its\neffectiveness and superior performance in experiments.",
        "Large language models (LLMs) have demonstrated strong effectiveness and\nrobustness while fine-tuned as dense retrievers. However, their large parameter\nsize brings significant inference time computational challenges, including high\nencoding costs for large-scale corpora and increased query latency, limiting\ntheir practical deployment. While smaller retrievers offer better efficiency,\nthey often fail to generalize effectively with limited supervised fine-tuning\ndata. In this work, we introduce DRAMA, a training framework that leverages\nLLMs to train smaller generalizable dense retrievers. In particular, we adopt\npruned LLMs as the backbone and train on diverse LLM-augmented data in a\nsingle-stage contrastive learning setup. Experiments show that DRAMA offers\nbetter multilingual and long-context capabilities than traditional\nencoder-based retrievers, and achieves strong performance across multiple tasks\nand languages. These highlight the potential of connecting the training of\nsmaller retrievers with the growing advancements in LLMs, bridging the gap\nbetween efficiency and generalization.",
        "In this report, we introduce our first-generation reasoning model,\nLexPro-1.0, a large language model designed for the highly specialized Chinese\nlegal domain, offering comprehensive capabilities to meet diverse realistic\nneeds. Existing legal LLMs face two primary challenges. Firstly, their design\nand evaluation are predominantly driven by computer science perspectives,\nleading to insufficient incorporation of legal expertise and logic, which is\ncrucial for high-precision legal applications, such as handling complex\nprosecutorial tasks. Secondly, these models often underperform due to a lack of\ncomprehensive training data from the legal domain, limiting their ability to\neffectively address real-world legal scenarios. To address this, we first\ncompile millions of legal documents covering over 20 types of crimes from 31\nprovinces in China for model training. From the extensive dataset, we further\nselect high-quality for supervised fine-tuning, ensuring enhanced relevance and\nprecision. The model further undergoes large-scale reinforcement learning\nwithout additional supervision, emphasizing the enhancement of its reasoning\ncapabilities and explainability. To validate its effectiveness in complex legal\napplications, we also conduct human evaluations with legal experts. We develop\nfine-tuned models based on DeepSeek-R1-Distilled versions, available in three\ndense configurations: 14B, 32B, and 70B.",
        "Existing data-driven control methods generally do not address False Data\nInjection (FDI) and Denial-of-Service (DoS) attacks simultaneously. This letter\nintroduces a distributed data-driven attack-resilient consensus problem under\nboth FDI and DoS attacks and proposes a data-driven consensus control\nframework, consisting of a group of comprehensive attack-resilient observers.\nThe proposed group of observers is designed to estimate FDI attacks, external\ndisturbances, and lumped disturbances, combined with a DoS attack compensation\nmechanism. A rigorous stability analysis of the approach is provided to ensure\nthe boundedness of the distributed neighborhood estimation consensus error. The\neffectiveness of the approach is validated through numerical examples involving\nboth leaderless consensus and leader-follower consensus, demonstrating\nsignificantly improved resilient performance compared to existing data-driven\ncontrol approaches.",
        "Existing methods for analyzing linguistic content from picture descriptions\nfor assessment of cognitive-linguistic impairment often overlook the\nparticipant's visual narrative path, which typically requires eye tracking to\nassess. Spatio-semantic graphs are a useful tool for analyzing this narrative\npath from transcripts alone, however they are limited by the need for manual\ntagging of content information units (CIUs). In this paper, we propose an\nautomated approach for estimation of spatio-semantic graphs (via automated\nextraction of CIUs) from the Cookie Theft picture commonly used in\ncognitive-linguistic analyses. The method enables the automatic\ncharacterization of the visual semantic path during picture description.\nExperiments demonstrate that the automatic spatio-semantic graphs effectively\ndifferentiate between cognitively impaired and unimpaired speakers. Statistical\nanalyses reveal that the features derived by the automated method produce\ncomparable results to the manual method, with even greater group differences\nbetween clinical groups of interest. These results highlight the potential of\nthe automated approach for extracting spatio-semantic features in developing\nclinical speech models for cognitive impairment assessment.",
        "Widespread integration of Generative AI tools is transforming white-collar\nwork, reshaping how workers define their roles, manage their tasks, and\ncollaborate with peers. This has created a need to develop an overarching\nunderstanding of common worker-driven patterns around these transformations. To\nfill this gap, we conducted a systematic literature review of 23 studies from\nthe ACM Digital Library that focused on workers' lived-experiences and\npractitioners with GenAI. Our findings reveal that while many professionals\nhave delegated routine tasks to GenAI to focus on core responsibilities, they\nhave also taken on new forms of AI managerial labor to monitor and refine GenAI\noutputs. Additionally, practitioners have restructured collaborations,\nsometimes bypassing traditional peer and subordinate interactions in favor of\nGenAI assistance. These shifts have fragmented cohesive tasks into piecework\ncreating tensions around role boundaries and professional identity. Our\nanalysis suggests that current frameworks, like job crafting, need to evolve to\naddress the complexities of GenAI-driven transformations.",
        "Providing effective feedback for programming assignments in computer science\neducation can be challenging: students solve problems by iteratively submitting\ncode, executing it, and using limited feedback from the compiler or the\nauto-grader to debug. Analyzing student debugging behavior in this process may\nreveal important insights into their knowledge and inform better personalized\nsupport tools. In this work, we propose an encoder-decoder-based model that\nlearns meaningful code-edit embeddings between consecutive student code\nsubmissions, to capture their debugging behavior. Our model leverages\ninformation on whether a student code submission passes each test case to\nfine-tune large language models (LLMs) to learn code editing representations.\nIt enables personalized next-step code suggestions that maintain the student's\ncoding style while improving test case correctness. Our model also enables us\nto analyze student code-editing patterns to uncover common student errors and\ndebugging behaviors, using clustering techniques. Experimental results on a\nreal-world student code submission dataset demonstrate that our model excels at\ncode reconstruction and personalized code suggestion while revealing\ninteresting patterns in student debugging behavior.",
        "Anti-Ramsey theory was initiated in 1975 by Erd\\H{o}s, Simonovits and S\\'os,\ninspiring hundreds of publications since then. The present work is the third\nand last piece of our trilogy in which we introduce a far-reaching\ngeneralization via the following two functions for any graph $G$ and family\n${\\cal F}$ of graphs:\n  If $K_2 \\in {\\cal F}$, let $f(n,G|{\\cal F})$ be the smallest integer $k$ such\nthat every edge coloring of $K_n$ with at least $k$ colors forces a copy of $G$\nin which all color classes are members of ${\\cal F}$.\n  If $K_2 \\notin {\\cal F}$, let $g(n,G|{\\cal F})$ be the largest integer $k$\nfor which there exists an edge coloring of $K_n$ using exactly $k$ colors, such\nthat every copy of $G$ contains an induced color class which is a member of\n${\\cal F}$.\n  We develop methods suitable for deriving asymptotically tight results for the\n$f$-function and the $g$-function for many combinations of $G$ and ${\\cal F}$.\n  The preceding parts of the trilogy are arXiv: 2405.19812 and 2408.04257,\npublished in Discrete Applied Math. Vol. 363 and Mathematics Vol. 12:23,\nrespectively.",
        "Vehicle-to-Everything communications-assisted Autonomous Driving (V2X-AD) has\nwitnessed remarkable advancements in recent years, with pragmatic\ncommunications (PragComm) emerging as a promising paradigm for real-time\ncollaboration among vehicles and other agents.Simultaneously, extensive\nresearch has explored the interplay between collaborative perception and\ndecision-making in end-to-end driving frameworks.In this work, we revisit the\ncollaborative driving problem and propose the Select2Drive framework to\noptimize the utilization of limited computational and communication\nresources.Particularly, to mitigate cumulative latency in perception and\ndecision-making, Select2Drive introduces Distributed Predictive Perception\n(DPP) by formulating an active prediction paradigm and simplifies\nhigh-dimensional semantic feature prediction into computation cost-efficient,\nmotion-aware reconstruction. Given the \"less is more\" principle that a\nbroadened perceptual horizon possibly confuses the decision module rather than\ncontributing to it, Select2Drive utilizes Area-of-Importance-based PragComm\n(APC) to prioritize the communications of critical regions, thus boosting both\ncommunication efficiency and decision-making efficacy. Empirical evaluations on\nthe V2Xverse dataset and CARLA driving simulator demonstrate that Select2Drive\nachieves a 11.31% (resp. 7.69%) improvement in offline perception tasks under\nlimited bandwidth (resp. pose error conditions). Moreover, it delivers at most\n14.68% and 31.76% enhancement in closed-loop driving scores and route\ncompletion rates, particularly in scenarios characterized by dense traffic and\nhigh-speed dynamics.",
        "We study the inverse boundary value problem for the linear elastic wave\nequation in three-dimensional isotropic medium. We show that both the Lam\\'e\nparameters and the density can be uniquely recovered from the boundary\nmeasurements under the strictly convex foliation condition.",
        "Adders are fundamental components in digital circuits, playing a crucial role\nin arithmetic operations within computing systems and many other applications.\nThis paper focuses on the design and simulation of a 32-bit Brent-Kung parallel\nprefix adder, which is recognized for its efficient carry propagation and\nlogarithmic delay characteristics. The Brent-Kung architecture balances\ncomputational speed and hardware complexity, making it suitable for high-speed\ndigital applications. The design is implemented using Verilog HDL and simulated\nusing Cadence Design Suite tools, including NCLaunch and Genus, to evaluate its\nperformance in terms of scalability, speed, and functional working. Comparative\nanalysis with traditional adder architectures highlights the advantages of the\nBrent-Kung adder for modern digital systems.",
        "We propose DiffCLIP, a novel vision-language model that extends the\ndifferential attention mechanism to CLIP architectures. Differential attention\nwas originally developed for large language models to amplify relevant context\nwhile canceling out noisy information. In this work, we integrate this\nmechanism into CLIP's dual encoder (image and text) framework. With minimal\nadditional parameters, DiffCLIP achieves superior performance on image-text\nunderstanding tasks. Across zero-shot classification, retrieval, and robustness\nbenchmarks, DiffCLIP consistently outperforms baseline CLIP models. Notably,\nthese gains come with negligible computational overhead, demonstrating that\ndifferential attention can significantly enhance multi-modal representations\nwithout sacrificing efficiency. Code can be found at\nhttps:\/\/github.com\/hammoudhasan\/DiffCLIP.",
        "This paper introduces a full discretization procedure to solve wave beam\npropagation in random media modeled by a paraxial wave equation or an\nIt\\^o-Schr\\\"odinger stochastic partial differential equation. This method bears\nsimilarities with the phase screen method used routinely to solve such\nproblems. The main axis of propagation is discretized by a centered splitting\nscheme with step $\\Delta z$ while the transverse variables are treated by a\nspectral method after appropriate spatial truncation. The originality of our\napproach is its theoretical validity even when the typical wavelength $\\theta$\nof the propagating signal satisfies $\\theta\\ll\\Delta z$. More precisely, we\nobtain a convergence of order $\\Delta z$ in mean-square sense while the errors\non statistical moments are of order $(\\Delta z)^2$ as expected for standard\ncentered splitting schemes. This is a surprising result as splitting schemes\ntypically do not converge when $\\Delta z$ is not the smallest scale of the\nproblem. The analysis is based on equations satisfied by statistical moments in\nthe It\\^o-Schr\\\"odinger case and on integral (Duhamel) expansions for the\nparaxial model. Several numerical simulations illustrate and confirm the\ntheoretical findings.",
        "We study numerical regularities for complexes over noncommutative noetherian\nlocally finite $\\mathbb{N}$-graded algebras $A$ such as CM (cm)-regularity, Tor\n(tor)-regularity (Ext (ext)-regularity) and Ex (ex)-regularity, which are the\nsupremum or infimum degrees of some associated canonical complexes. We show\nthat for any right bounded complex $X$ with finitely generated cohomologies,\nthe supremum degree of $R\\underline{\\text{Hom}}_A(X, A_0)$ coincides with the\nopposite of the infimum degree of $X$ if $A_0$ is semisimple. If $A$ has a\nbalanced dualizing complex and $A_0$ is semisimple, we prove that the\nCM-regularity of $X$ coincides with the supremum degree of\n$R\\underline{\\text{Hom}}_A(A_0,X)$ for any left bounded complex $X$ with\nfinitely generated cohomologies.\n  Several inequalities concerning the numerical regularities and the supremum\nor infimum degree of derived Hom or derived tensor complexes are given for\nnoncommutative noetherian locally finite $\\mathbb{N}$-graded algebras. Some of\nthese are generalizations of J\\o rgensen's results on the inequalities between\nthe CM-regularity and Tor-regularity, some are new even in the connected graded\ncase. Conditions are given under which the inequalities become equalities by\nestablishing two technical lemmas.\n  Following Kirkman, Won and Zhang, we also use the numerical AS-regularity\n(resp. little AS-regularity) to study Artin-Schelter regular property\n(finite-dimensional property) for noetherian $\\mathbb{N}$-graded algebras. We\nprove that the numerical AS-regularity of $A$ is zero if and only if that $A$\nis an $\\mathbb{N}$-graded AS-regular algebra under some mild conditions, which\ngeneralizes a result of Dong-Wu and a result of Kirkman-Won-Zhang. If $A$ has a\nbalanced dualizing complex and $A_0$ is semisimple, we prove that the little\nAS-regularity of $A$ is zero if and only if $A$ is finite-dimensional.",
        "The in-context learning capabilities of modern language models have motivated\na deeper mathematical understanding of sequence models. A line of recent work\nhas shown that linear attention models can emulate projected gradient descent\niterations to implicitly learn the task vector from the data provided in the\ncontext window. In this work, we consider a novel setting where the global task\ndistribution can be partitioned into a union of conditional task distributions.\nWe then examine the use of task-specific prompts and prediction heads for\nlearning the prior information associated with the conditional task\ndistribution using a one-layer attention model. Our results on loss landscape\nshow that task-specific prompts facilitate a covariance-mean decoupling where\nprompt-tuning explains the conditional mean of the distribution whereas the\nvariance is learned\/explained through in-context learning. Incorporating\ntask-specific head further aids this process by entirely decoupling estimation\nof mean and variance components. This covariance-mean perspective similarly\nexplains how jointly training prompt and attention weights can provably help\nover fine-tuning after pretraining.",
        "Tool learning can further broaden the usage scenarios of large language\nmodels (LLMs). However most of the existing methods either need to finetune\nthat the model can only use tools seen in the training data, or add tool\ndemonstrations into the prompt with lower efficiency. In this paper, we present\na new Tool Learning method Chain-of-Tools. It makes full use of the powerful\nsemantic representation capability of frozen LLMs to finish tool calling in CoT\nreasoning with a huge and flexible tool pool which may contain unseen tools.\nEspecially, to validate the effectiveness of our approach in the massive unseen\ntool scenario, we construct a new dataset SimpleToolQuestions. We conduct\nexperiments on two numerical reasoning benchmarks (GSM8K-XL and FuncQA) and two\nknowledge-based question answering benchmarks (KAMEL and SimpleToolQuestions).\nExperimental results show that our approach performs better than the baseline.\nWe also identify dimensions of the model output that are critical in tool\nselection, enhancing the model interpretability. Our code and data are\navailable at: https:\/\/github.com\/fairyshine\/Chain-of-Tools ."
      ]
    }
  },
  {
    "id":2411.07018,
    "research_type":"applied",
    "start_id":"b18",
    "start_title":"Accelerating cavity fault prediction using deep learning at Jefferson laboratory",
    "start_abstract":"Abstract Accelerating cavities are an integral part of the continuous electron beam accelerator facility (CEBAF) at Jefferson Laboratory. When any over 400 in CEBAF experiences a fault, it disrupts delivery to experimental user halls. In this study, we propose use deep learning model predict slowly developing cavity faults. By utilizing pre-fault signals, train long short-term memory-convolutional neural network binary classifier distinguish between radio-frequency (RF) signals during normal operation and RF indicative impending We optimize by adjusting fault confidence threshold implementing multiple consecutive window criterion identify events, ensuring low false positive rate. Results obtained from analysis real dataset collected accelerating simulating deployed scenario demonstrate model\u2019s ability with 99.99% accuracy correctly 80% Notably, these achievements were achieved context highly imbalanced dataset, predictions made several hundred milliseconds before onset fault. Anticipating faults enables preemptive measures improve operational efficiency preventing or mitigating their occurrence.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b12"
      ],
      "title":[
        "Field Emission in Superconducting Accelerators: Instrumented Measurements for Its Understanding and Mitigation"
      ],
      "abstract":[
        "Several new accelerator projects are adopting superconducting RF (SRF) technology. When accelerating SRF cavities maintain high RF gradients, field emission, the emission of electrons from cavity walls, can occur and may impact operational cavity gradient, radiological environment via activated components, and reliability. In this talk, we will discuss instrumented measurements of field emission from the two 1.1 GeV superconducting continuous wave (CW) linacs in CEBAF. The goal is to improve the understanding of field emission sources originating from cryomodule production, installation and operation. Such basic knowledge is needed in guiding field emission control, mitigation, and reduction toward high gradient and reliable operation of superconducting accelerators."
      ],
      "categories":[
        "physics.acc-ph"
      ]
    },
    "list":{
      "title":[
        "New Generation Compact Linear Accelerator for Low-Current, Low-Energy\n  Multiple Applications",
        "Measurement of the mean excitation energy of liquid argon",
        "Using Convolutional Neural Networks to Accelerate 3D Coherent\n  Synchrotron Radiation Computations",
        "Polarized electron bunch refresh rates in an electron storage ring",
        "Concept of an Infrared FEL for the Chemical Dynamics Research Laboratory\n  at PETRA IV",
        "Stability Enhancement of a Self-Amplified Spontaneous Emission\n  Free-electron Laser with Bunching Containment",
        "On the influence of electron velocity spread on Compton FEL operation",
        "FCC-ee positron source from conventional to crystal-based",
        "Incoherent horizontal emittance growth due to the interplay of beam-beam\n  and longitudinal wakefield in crab-waist colliders",
        "Effects of Curved Superconducting Magnets on Beam Stability in a Compact\n  Ion Therapy Synchrotron",
        "Recent progress in high-temperature superconducting undulators",
        "Triple Evaporation of Bialkali Antimonide Photocathodes and\n  Photoemission Characterization at the PhoTEx Experiment",
        "Cyclotron Maser Cooling towards Coherent Particle Beams",
        "An exposition on the supersimplicity of certain expansions of the\n  additive group of the integers",
        "Evaluating the resilience of ESG investments in European Markets during\n  turmoil periods",
        "A new local time-decoupled squared Wasserstein-2 method for training\n  stochastic neural networks to reconstruct uncertain parameters in dynamical\n  systems",
        "A Comprehensive Framework for Statistical Inference in Measurement\n  System Assessment Studies",
        "A Packaging Method for ALPIDE Integration Enabling Flexible and\n  Low-Material-Budget Designs",
        "FairUDT: Fairness-aware Uplift Decision Trees",
        "Generalization error bound for denoising score matching under relaxed\n  manifold assumption",
        "Kauffman bracket skein module of the $(3,3,3,3)$-pretzel link exterior",
        "Breakdown of broken-symmetry approach to exchange interaction",
        "The $D_{s1}(2460) \\to D_s \\pi^+ \\pi^- $ decay from a $D_{s1}$ molecular\n  perspective",
        "Gravitational waves and primordial black holes from the T-model\n  inflation with Gauss-Bonnet correction",
        "Inverse Gaussian Distribution, Introduction and\n  Applications:Comprehensive Analysis of Power Plant Performance: A Study of\n  Combined Cycle and Nuclear Power Plant",
        "Personalized Convolutional Dictionary Learning of Physiological Time\n  Series",
        "Liquidity Competition Between Brokers and an Informed Trader",
        "A Comparison of Strategies to Embed Physics-Informed Neural Networks in\n  Nonlinear Model Predictive Control Formulations Solved via Direct\n  Transcription"
      ],
      "abstract":[
        "A new compact linear proton accelerator project (named LINAC 7) for multiple\nlow-current applications, designed and built in-house at the Beam Laboratory of\nthe University of the Basque Country (UPV\/EHU) is described. The project\ncombines the University, a research technology center and a private company\nwith the aim of designing and building a compact, low-current proton\naccelerator capable of accelerating particles up to 7 MeV. In this paper, we\npresent an overview of the accelerator design, summarize the progress and\ntesting of the components that have been built, and describe the components\nthat are being designed that will allow us to achieve the final desired energy\nof 7 MeV.",
        "The mean excitation energy (I-value) of liquid argon is a critical input for\nenergy estimation in neutrino oscillation experiments. It is measured to be\n$(205\\pm4)$\\,eV using the range of 402.2\\,MeV protons from the Fermilab Linac.\nThis compares to the author's recent evaluation of $(197\\pm 7)$\\,eV based on a\ncombination of an oscillator strength distribution analysis, gaseous argon\nrange measurements, sparse stopping power data on solid argon, and an\nextrapolation of data on the effect of phase from other substances. Using all\nsources of information, we recommend a value of $(203.0\\pm3.2)$\\,eV for liquid\nargon, which is significantly higher than 188\\,eV, from ICRU-37's gaseous argon\nevaluation, commonly used in Monte Carlo codes such as \\textsc{Geant4}.",
        "Calculating the effects of Coherent Synchrotron Radiation (CSR) is one of the\nmost computationally expensive tasks in accelerator physics. Here, we use\nconvolutional neural networks (CNN's), along with a latent conditional\ndiffusion (LCD) model, trained on physics-based simulations to speed up\ncalculations. Specifically, we produce the 3D CSR wakefields generated by\nelectron bunches in circular orbit in the steady-state condition. Two datasets\nare used for training and testing the models: wakefields generated by\nthree-dimensional Gaussian electron distributions and wakefields from a sum of\nup to 25 three-dimensional Gaussian distributions. The CNN's are able to\naccurately produce the 3D wakefields $\\sim 250-1000$ times faster than the\nnumerical calculations, while the LCD has a gain of a factor of $\\sim 34$. We\nalso test the extrapolation and out-of-distribution generalization ability of\nthe models. They generalize well on distributions with larger spreads than what\nthey were trained on, but struggle with smaller spreads.",
        "When polarized electron bunches are injected and circulated in a high-energy\nstorage ring, the polarization of the bunches relaxes to the asymptotic value\nof the radiative polarization, caused by the synchrotron radiation. Hence the\nbunches must be refreshed periodically, to maintain a predetermined\ntime-averaged value of the bunch polarization. In general, the refresh rates of\nthe \"up\" and \"down\" polarization bunches are different. We suggest an\nalternative policy. We point out that the total bunch refresh rate is almost\nindependent of the asymptotic level of the radiative polarization. We also note\nthat the true goal of so-called \"spin matching\" is to maximize the buildup time\nconstant (not the asymptotic level) of the radiative polarization. We suggest a\nscheme to equalize the refresh rates of the \"up\" and \"down\" polarization\nbunches, which may be (i) helpful for accelerator operations, and also (ii)\nreduce systematic errors in HEP experiments.",
        "We describe an infrared free electron laser (FEL), proposed as a part of a\nuser facility that also incorporates synchrotron-radiation beamlines for the\nPETRA IV. The FEL itself addresses the needs of the chemical sciences community\nfor a high-brightness, tunable source covering a broad region of the infrared\nspectrum - from 5 to 100 mkm. The user facility will allow, for the first time,\nthe integrated and simultaneous use of dedicated infrared FEL and\nsynchrotron-radiation beamlines for pump-probe experiments that will focus on\ngaining a rigorous molecular-level understanding of combustion and other\nenergetic molecular processes. These (pump-probe) requirements dictate the use\nof storage ring RF structures and cw operation. The technical approach adopted\nin FEL design uses an old PETRA III RF system and accelerating cavities. The\nprimary motivation for adopting this approach was to minimize facility costs.",
        "The self-amplified spontaneous emission (SASE) mechanism, the fundamental\noperating principle of numerous free-electron laser (FEL) facilities, is driven\nby electron beam shot noise and leads to significant fluctuations in the output\npulse energy. This study presents a robust method for improving pulse energy\nstability by incorporating a dispersion element that introduces longitudinal\ndispersion into the electron beam during the exponential growth phase of the\nSASE process. At this phase, the density modulation of the electron beam,\ncharacterized by the bunching factor, undergoes large fluctuations, resulting\nin substantial variations in the emitted radiation power. The introduction of\nlongitudinal dispersion allows for controlled manipulation of the bunching\ndistribution, suppressing fluctuations and enhancing pulse energy stability.\nThe stabilization mechanism is explained in this paper, and its impact on the\nradiation properties is analyzed for both the standard SASE scheme and advanced\nlasing setups, such as a two-stage lasing process for two-color pulse\ngeneration, with the initial stage operating in SASE mode.",
        "For the field amplitude, a nonlinear integro-differential equation is derived\nthat describes the operation of a Compton FEL in the presence of electron\nvelocity spread typical for modern facilities. Numerical solutions of the\nequation are in good agreement with particle simulations for the bunching\nfactor less than 0.6, reproduce the frequency detuning spectrum near its\nmaximum, and describe the amplification process up to saturation.",
        "The high-luminosity requirement in future lepton colliders imposes a need for\na high-intensity positron source. In the conventional scheme, positron beams\nare obtained by the conversion of bremsstrahlung photons into electron-positron\npairs through the interaction between a high-energy electron beam and a high-Z\namorphous target. One method to enhance the number of produced positrons is by\nboosting the incident electron beam power. However, the maximum heat load and\nthermo-mechanical stresses bearable by the target severely limit the beam power\nof the incident electrons. To overcome these limitations, an innovative\napproach using lattice coherent effects in oriented crystals appears promising.\nThis approach uses a single thick crystal that serves as a radiator and a\nconverter. In this paper, we investigate the application of this scheme as an\nalternative to the conventional positron source at the Future Circular Collider\n(FCC-ee). Simulations were carried out from the positron production stage to\nthe entrance of the damping ring to estimate the accepted positron yield. The\nresults demonstrate the advantages of the crystal-based positron source: it\nrequires thinner targets than the conventional scheme, resulting in a 14%\nreduction in the deposited power while achieving a 10% increase in accepted\npositron yield.",
        "In this paper, we investigate quadrupolar sychrobetatron resonances caused by\nbeam-beam collisions and their interplay with longitudinal wakefields in the\ncontext of crab-waist colliders. We present a comprehensive theoretical review\nof the established theory of sychrobetatron resonances and extend the formalism\nto explore horizontal sychrobetatron resonances specific to crab-waist\ncolliders. As a case study, we examine incoherent horizontal emittance growth\nat the SuperKEKB and demonstrate through simulations that the interplay between\nbeam-beam and longitudinal wakefields leads to a horizontal blowup of the bunch\nsize and that the study of the dynamics can be reduced to the\nhorizontal-longitudinal plane, independent of the motion in the vertical\ndimension. We present extensive simulation results using the codes BBWS,\nPyHEADTAIL and Xsuite, connect our analytical findings with these findings, and\npropose strategies to mitigate horizontal blowup.",
        "Superconducting, curved magnets can reduce accelerator footprints by\nproducing strong fields (>3T) and reducing the total number of magnets through\ntheir capability for combined-function multipolar fields, making them an\nattractive choice for applications such as heavy ion therapy. There exists the\nproblem that the effect of strongly curved harmonics and fringe fields on\ncompact accelerator beam dynamics is not well represented: existing approaches\nuse integrated cylindrical multipoles to describe and model the fields for beam\ndynamics studies, which are invalid in curved coordinate systems and assume\nindividual errors cancel out over the full machine. In the modelling of these\nmachines, the effect of strongly curved harmonics and fringe fields on compact\naccelerator beam dynamics needs to properly included. An alternative approach\nmust be introduced for capturing off-axis fields in a strongly curved magnet,\nwhich may affect long-term beam stability in a compact accelerator. In this\narticle, we investigate the impacts of deploying a curved canted-cosine-theta\n(CCT) superconducting magnet in a compact medical synchrotron for the first\ntime. We develop a method to analyse and characterise the 3D curved fields of\nan electromagnetic model of a CCT developed for the main bending magnets of a\n27m circumference carbon ion therapy synchrotron, designed within the Heavy Ion\nTherapy Research Integration Plus European project, and the CERN Next Ion\nMedical Machine Study (NIMMS). The fields are modelled in the compact\nsynchrotron in MAD-X\/PTC to study their effects on beam dynamics and long-term\nbeam stability. The insights gained through the methods presented allow for the\noptimisation of both magnet and synchrotron designs, with the potential to\nimpact the operational performance of future ion therapy facilities.",
        "Considerable effort has been devoted to the development of superconducting\nundulators (SCUs) intended for particle accelerator-based light sources,\nincluding synchrotrons and free electron laser (FEL) facilities. Recently, a\nhigh-temperature superconducting (HTS) undulator prototype, consisting of\nstaggered-array Re-Ba-Cu-O bulks, achieved an on-axis sinusoidal magnetic field\nprofile with a peak amplitude B$_0$ of 2.1 T and a period length of 10 mm,\nresulting in a deflection parameter K = 1.96. Such a short period HTS undulator\nnot only enables the generation of higher-energy photons, but also supports the\nconstruction of economically feasible and compact FELs with shorter linear\naccelerators (LINACs). This article provides a comprehensive review of recent\nadvances in the staggered-array bulk HTS undulator as well as other types of\nHTS undulators. Furthermore, it offers insights into the development of\nengineering HTS undulator prototypes designed for deployment in synchrotron and\nfree electron laser (FEL) facilities. We conclude by discussing opportunities\nfor and the challenges facing the use of HTS undulators in practical\napplications.",
        "The development of high-performance photocathodes is essential for generating\nhigh-brightness electron beams required by existing and future accelerators.\nThis work introduces a state-of-the-art triple evaporation growth system\ndesigned for bialkali antimonide photocathodes. By enabling the simultaneous\ndeposition of all three materials, this system significantly enhances vacuum\nstability and the reproducibility of photocathode fabrication. Complementing\nthis, the novel characterization system PhoTEx allows spatially and spectrally\nresolved measurements of key photocathode parameters, such as quantum\nefficiency (QE), mean transverse energy (MTE), reflectance and lifetime.\nCrucially, all measurements are performed within a single compact setup,\nwithout moving the sample, preserving ultra-high vacuum conditions. The\nspectral resolved measurement of the reflectance allows the investigation of\nthe color. Photocathode colorimetry may provide valuable insights into material\nhomogeneity and aging. A Na-K-Sb photocathode was grown using the triple\nevaporation method, achieving an initial QE of $5.5\\,\\%$ at $520\\,$nm. The\nphotocathode was characterized at PhoTEx over two months, demonstrating\nconsistent MTE measurements and a dataset with spectral response, reflectance\nand colorimetry data. Together, the triple evaporation growth system and PhoTEx\nmark a significant advancement in optimizing photocathodes with exceptional\nperformance, paving the way for brighter and more stable electron sources for\nnext-generation accelerator facilities.",
        "This article presents a new particle beam cooling scheme, namely cyclotron\nmaser cooling (CMC). Relativistic gyrating particles, forced by a solenoidal\nmagnetic field over some length of their trajectory, move in a helical path and\nundergo emission or absorption of radiations stimulated by a resonance RF\nfield. Theoretical and experimental investigations on electron beams indicate\nthat when the action of the RF field exceeds a critical value the beam jumps\nabruptly to a coherent radiative system undergoing CMC in which most electrons\nare accumulated to a discrete energy with the same gyration phase. The\nmechanism of CMC was proved to be an elementary cooling process that is common\nto dissipative systems consisting of a driving field and oscillators with a\nstable energy. It leads to the generation of a coherent beam of particles that\nprovides means to control miscellaneous particle induced reactions. For\nexample, CMC electrons would generate coherent X-ray and gamma-ray photons\nthrough coherent inverse Compton scattering of laser radiation.",
        "In this short note, we present a self-contained exposition of the\nsupersimplicity of certain expansions of the additive group of the integers,\nsuch as adding a generic predicate (due to Chatzidakis and Pillay), a predicate\nfor the square-free integers (due to Bhardwaj and Tran) or a predicate for the\nprime integers (due to Kaplan and Shelah, assuming Dickson's conjecture).",
        "This study investigates the resilience of Environmental, Social, and\nGovernance (ESG) investments during periods of financial instability, comparing\nthem with traditional equity indices across major European markets-Germany,\nFrance, and Italy. Using daily returns from October 2021 to February 2024, the\nanalysis explores the effects of key global disruptions such as the Covid-19\npandemic and the Russia-Ukraine conflict on market performance. A mixture of\ntwo generalised normal distributions (MGND) and EGARCH-in-mean models are used\nto identify periods of market turmoil and assess volatility dynamics. The\nfindings indicate that during crises, ESG investments present higher volatility\nin Germany and Italy than in France. Despite some regional variations, ESG\nportfolios demonstrate greater resilience compared to traditional ones,\noffering potential risk mitigation during market shocks. These results\nunderscore the importance of integrating ESG factors into long-term investment\nstrategies, particularly in the face of unpredictable financial turmoil.",
        "In this work, we propose and analyze a new local time-decoupled squared\nWasserstein-2 method for reconstructing the distribution of unknown parameters\nin dynamical systems. Specifically, we show that a stochastic neural network\nmodel, which can be effectively trained by minimizing our proposed local\ntime-decoupled squared Wasserstein-2 loss function, is an effective model for\napproximating the distribution of uncertain model parameters in dynamical\nsystems. Through several numerical examples, we showcase the effectiveness of\nour proposed method in reconstructing the distribution of parameters in\ndifferent dynamical systems.",
        "Measurement system analysis aims to quantify the variability in data\nattributable to the measurement system and evaluate its contribution to overall\ndata variability. This paper conducts a rigorous theoretical investigation of\nthe statistical methods used in such analyses, focusing on variance components\nand other critical parameters. While established techniques exist for\nsingle-variable cases, a systematic theoretical exploration of their properties\nhas been largely overlooked. This study addresses this gap by examining\nestimators for variance components and other key parameters in measurement\nsystem assessment, analyzing their statistical properties, and providing new\ninsights into their reliability, performance, and applicability.",
        "This work presents a novel solution for the packaging of ALPIDE chips that\nfacilitates non-planar assembly with a minimal material budget. This solution\nrepresents a technological advancement based on methodologies developed for the\nALICE ITS1 and the STAR tracker two decades ago. The core of this approach\ninvolves the use of flexible cables composed of aluminum and polyimide, with\nthicknesses on the order of tens of micrometers. These cables are connected to\nthe sensors using single-point Tape Automated Bonding (spTAB), which replaces\nthe traditional wire bonding technique that is suboptimal for curved\nintegrations. The spTAB bonding is achieved by creating openings in the\npolyimide layer, allowing aluminum wires to remain free-standing, which are\nthen connected to the sensor using pressure and ultrasonic energy. Extending\nthis concept, we have applied this approach to entire printed circuit boards\n(PCBs), resulting in a fully flexible packaging solution maintaining an\nultra-low material budget. This work introduces a prototype utilizing this\nmethod to bond an ALPIDE chip, proposing it as a viable option for future\ndesigns necessitating flexible packaging for both the chip and associated\nelectronics. The overall workflow, comprising microfabrication and assembly, is\ncarried out at the Fondazione Bruno Kessler and INFN TIFPA laboratories and\nwill be detailed to elucidate our procedures and demonstrate the applicability\nof our solution in future experimental setups. The proposed packaging features\na flexible PCB constructed from three stacked layers, each containing 20 $\\mu$m\nthick aluminum features and a 25 $\\mu$m thick polyimide substrate. These layers\ninclude a ground layer, a signal layer (encompassing both digital and analog\nsignals), and a local bonding layer (which substitutes wire bonding).",
        "Training data used for developing machine learning classifiers can exhibit\nbiases against specific protected attributes. Such biases typically originate\nfrom historical discrimination or certain underlying patterns that\ndisproportionately under-represent minority groups, such as those identified by\ntheir gender, religion, or race. In this paper, we propose a novel approach,\nFairUDT, a fairness-aware Uplift-based Decision Tree for discrimination\nidentification. FairUDT demonstrates how the integration of uplift modeling\nwith decision trees can be adapted to include fair splitting criteria.\nAdditionally, we introduce a modified leaf relabeling approach for removing\ndiscrimination. We divide our dataset into favored and deprived groups based on\na binary sensitive attribute, with the favored dataset serving as the treatment\ngroup and the deprived dataset as the control group. By applying FairUDT and\nour leaf relabeling approach to preprocess three benchmark datasets, we achieve\nan acceptable accuracy-discrimination tradeoff. We also show that FairUDT is\ninherently interpretable and can be utilized in discrimination detection tasks.\nThe code for this project is available https:\/\/github.com\/ara-25\/FairUDT",
        "We examine theoretical properties of the denoising score matching estimate.\nWe model the density of observations with a nonparametric Gaussian mixture. We\nsignificantly relax the standard manifold assumption allowing the samples step\naway from the manifold. At the same time, we are still able to leverage a nice\ndistribution structure. We derive non-asymptotic bounds on the approximation\nand generalization errors of the denoising score matching estimate. The rates\nof convergence are determined by the intrinsic dimension. Furthermore, our\nbounds remain valid even if we allow the ambient dimension grow polynomially\nwith the sample size.",
        "We show that the Kauffman bracket skein module of the $(3,3,3,3)$-pretzel\nlink exterior over $\\mathbb{Q}(q^{\\frac{1}{2}})$ is not finitely generated as a\nmodule over $\\mathbb{Q}(q^{\\frac{1}{2}})[t_1,t_2]$, where $t_1,t_2$ are the\nmeridians of two components. This disproves a finiteness conjecture proposed in\n2021.",
        "Broken-symmetry (BS) approaches are widely employed to evaluate Heisenberg\nexchange parameters, primarily in combination with DFT calculations. For many\nmagnetic materials, BS-DFT calculations give reasonable estimations of exchange\nparameters although systematic failures have also been reported. While the\nlatter were attributed to deficiencies of approximate exchange-correlation\nfunctional, we prove here by treating a simple model system that the\nbroken-symmetry methodology has serious problems. Detailed analysis clarifies\nthe intrinsic issue with the broken-symmetry treatment of low-spin states. It\nshows, in particular, that the error in the BS calculation of exchange\nparameter scales with the degree of covalency between the magnetic and the\nbridging orbitals. As a possible tool to overcome this intrinsic drawback of\nsingle-determinant BS approaches, we propose their extension to a minimal\nmulticonfigurational version.",
        "We conduct a theoretical study of the $ D_{s1}(2460) \\to D_s \\pi^+ \\pi^- $\ndecay from the perspective that the $D_{s1}$ is a molecular state, built mostly\nfrom the $D^* K$ and $D_s^* \\eta$ components. The $D^*$ and $D_s^*$ mesons are\nallowed to decay into two pseudoscalars, with one of them merging with the\nother pseudoscalar that forms the $D_{s1}$ state, ultimately leading to the\n$\\pi^+ \\pi^- D_s$ final state. This results in a triangle diagram mechanism\nwhere all theoretical ingredients are well-known, leading to a free parameter\nframework. We evaluate the mass distributions of particle pairs and find good\nagreement with the experimental distributions of a recent LHCb experiment,\nproviding strong support to the molecular picture of the $D_{s1}(2460)$ state.\nWe also discuss the role played by the scalar mesons $f_0(500)$ and $f_0(980)$,\nat odds with the interpretation of the experimental analysis.",
        "Recently, the worldwide Pulsar Timing Array (PTA) collaborations detected a\nstochastic gravitational wave(GW) background in the nanohertz range, which may\noriginate from the early universe's inflationary phase. So in this work, we\ninvestigated induce GWs in the T-model inflation with Gauss-Bonnet coupling.\nConsider the scenario of traversing a domain wall in moduli space, we take the\ncoupling coefficient to be an approximately step function. Within suitable\nparameter regions, the model exhibits de Sitter fixed points, which allows\ninflation to undergo an ultra-slow-roll phase, which causes the power spectrum\nto exhibit a peak. Such a peak can induce nanohertz GWs, which provids an\nexplanation for the PTA observational data. Furthermore, we consider the case\nof multiple domain wall crossings, and adopting a double-step coupling\nfunction. In this case, the resulting GW spectrum has two peaks with\nfrequencies around \\(10^{-8} \\,\\text{Hz}\\) and \\(10^{-2}\\,\\text{Hz}\\),\nrespectively. Which can be observed by the PTA and the space GW detectors\nsimultaneously.Additionally, the reentry of the power spectrum peaks into the\nhorizon leads to the collapse into primordial black holes (PBHs). We calculate\nthe abundance of PBHs and found that the masses is in the range of \\(10^{-14}\n\\sim 10^{-13} M_\\odot\\) and around \\(10^{-2} M_\\odot\\) , which constitute\nsignificant components of the current dark matter.",
        "This paper presents a comprehensive analysis of power plant performance using\nthe inverse Gaussian (IG) distribution framework. We combine theoretical\nfoundations with practical applications, focusing on both combined cycle and\nnuclear power plant contexts. The study demonstrates the advantages of the IG\ndistribution in modeling right-skewed industrial data, particularly in power\ngeneration. Using the UCI Combined Cycle Power Plant Dataset, we establishthe\nsuperiority of IG-based models over traditional approaches through rigorous\nstatistical testing and model validation. The methodology developed here\nextends naturally to nuclear power plant applications, where similar\nstatistical patterns emerge in operational data. Our findings suggest that\nIG-based models provide more accurate predictions and better capture the\nunderlying physical processes in power generation systems.",
        "Human physiological signals tend to exhibit both global and local structures:\nthe former are shared across a population, while the latter reflect\ninter-individual variability. For instance, kinetic measurements of the gait\ncycle during locomotion present common characteristics, although idiosyncrasies\nmay be observed due to biomechanical disposition or pathology. To better\nrepresent datasets with local-global structure, this work extends Convolutional\nDictionary Learning (CDL), a popular method for learning interpretable\nrepresentations, or dictionaries, of time-series data. In particular, we\npropose Personalized CDL (PerCDL), in which a local dictionary models local\ninformation as a personalized spatiotemporal transformation of a global\ndictionary. The transformation is learnable and can combine operations such as\ntime warping and rotation. Formal computational and statistical guarantees for\nPerCDL are provided and its effectiveness on synthetic and real human\nlocomotion data is demonstrated.",
        "We study a multi-agent setting in which brokers transact with an informed\ntrader. Through a sequential Stackelberg-type game, brokers manage trading\ncosts and adverse selection with an informed trader. In particular, supplying\nliquidity to the informed traders allows the brokers to speculate based on the\nflow information. They simultaneously attempt to minimize inventory risk and\ntrading costs with the lit market based on the informed order flow, also known\nas the internalization-externalization strategy. We solve in closed form for\nthe trading strategy that the informed trader uses with each broker and propose\na system of equations which classify the equilibrium strategies of the brokers.\nBy solving these equations numerically we may study the resulting strategies in\nequilibrium. Finally, we formulate a competitive game between brokers in order\nto determine the liquidity prices subject to precommitment supplied to the\ninformed trader and provide a numerical example in which the resulting\nequilibrium is not Pareto efficient.",
        "This study aims to benchmark candidate strategies for embedding neural\nnetwork (NN) surrogates in nonlinear model predictive control (NMPC)\nformulations that are subject to systems described with partial differential\nequations and that are solved via direct transcription (i.e., simultaneous\nmethods). This study focuses on the use of physics-informed NNs and\nphysics-informed convolutional NNs as the internal (surrogate) models within\nthe NMPC formulation. One strategy embeds NN models as explicit algebraic\nconstraints, leveraging the automatic differentiation (AD) of an algebraic\nmodelling language (AML) to evaluate the derivatives. Alternatively, the solver\ncan be provided with derivatives computed external to the AML via the AD\nroutines of the machine learning environment the NN is trained in. The three\nnumerical experiments considered in this work reveal that replacing mechanistic\nmodels with NN surrogates may not always offer computational advantages when\nsmooth activation functions are used in conjunction with a local nonlinear\nsolver (e.g., Ipopt), even with highly nonlinear systems. Moreover, in this\ncontext, the external function evaluation of the NN surrogates often\noutperforms the embedding strategies that rely on explicit algebraic\nconstraints, likely due to the difficulty in initializing the auxiliary\nvariables and constraints introduced by explicit algebraic reformulations."
      ]
    }
  },
  {
    "id":2411.19844,
    "research_type":"applied",
    "start_id":"b2",
    "start_title":"A New Kind of Science",
    "start_abstract":"3R3. A New Kind of Science. - S Wolfram (Wolfram Res Inc, 100 Trade Center Dr, Champaign IL 61820-7237). Media, Champaign, IL. 2002. 1197 pp. ISBN 1-57955-008-8. $44.95. Reviewed by M Gad-el-Hak (Eng Build, Rm 303, Virginia Commonwealth Univ, 601 W Main St, PO Box 843015, Richmond, VA 23284-3015). Reviewing Science is like stepping in a minefield. The danger lies going against the deluge praise, proving relevance to this audience, and arguing proposed new science that allegedly set replace science, as we know it. Those issues will be addressed turn, but first brief background. Stephen considered many have been child prodigy: journal paper particle physics at age 15; stint Oxford; PhD from Caltech 20; youngest recipient MacArthur Prize; faculty positions Caltech, Princeton, Illinois; significant contributions cellular automata complexity theory; developer popular software Mathematica; successful entrepreneur, becoming multi-millionaire 30. Running his company via e-mail videoconference, spent last 10 years virtual seclusion, relentlessly, tirelessly, secretly, nocturnally working on an idea possessed him: generating simple computations, algorithms only few lines. book, targeting both scientists non-scientists, partially about using rules generate complex patterns. In task, author has succeeded beyond reproach not showing can done brilliantly beautifully, also explaining it lucidly enough for all understand, appreciate, savor. opinion several reviewers, including one, aspect book tour de force clarity, elegance, simplicity. problem huge leap takes since nature computer-generated patterns look or behave similarly natural man-made things around us\u2014a snow flake, turbulent flow, lung, mollusk shell, traffic jam, outbreak starfish coral reef, entire universe\u2014therefore must way works. Nature runs its course same computer program. That essence science: yield secrets universe, solve our long-standing problems, provide theory everything. More flight fancy later. Deluge: was widely anticipated before actual publication. Published May 14, 2002, quickly became Amazon.com bestseller promptly reviewed scientific press. Heavyweights former included York Times, Chicago Tribune, Newsweek, Time, Daily Telegraph, Le Monde, Frankfurter Allgemeine Zeitung, Economist. Except last, press went gaga over touting author's claim stand existing head. Economist (p 79, June 1, 2002) more subdued even provocatively titling review \"The Emperor's Theory.\" press, reviews were somewhat less glorious skeptical. Physics Today 55, July 2002), Leo Kadanoff's once pointed, subtle polite, concluding he cannot support view any \"new kind science\" displayed Wolfram's book. Newsweek 59, 27, quoted famed physicist Freeman Dyson: \"There's tradition approaching senility come up with grand, improbable theories. unusual he's doing 40s.\" Kadanoff Dyson express minority opinion, however, majority reviewers being excited reason every human mystery currently depressed stock market, free will, quantum field theory, entropy. For present reviewer, lurks high particularly so months behind who already anointed Isaac Newton 21st century. Relevance: As aims replacing readers Applied Mechanics Reviews stake matter. Mechanics\u2014classical most part occasionally quantum\u2014is underlying branch upon which almost applied mechanics based. mathematics here often form partial differential equations, where space time are indefinitely divisible continuum. example, most, all, fluid flows described well-known, well-posed Navier\u2013Stokes equations. those first-principles equations solved agreement experiment reproach. It problem, such frustrated scores him. search simpler alternative is, therefore, quite alluring. mechanics, when they solved, powerful predictive tool explain mechanical world us well help design machines. When analytical solutions unattainable, discretized brute numerical integration used. But possible some situations, example realistic high-Reynolds-number other multi-scale problems required computational memory speed overwhelm today's supercomputers. impenetrable certain degree empiricism introduced relatively faster computations then proceed. Heuristic turbulence modeling compromise. Despite limitations, traditional works exceedingly well, mechanicians happily practice their craft. Readers should, care passionately if laws supplanted science. Argument: Cellular late 1940s John von Neumann Stanislaw Ulam, although claims independently discovered three decades discrete dynamical systems whose behavior completely specified terms repetitive local relation. continuum represented uniform one-, two-, three-dimensional grid, each cell containing single bit data, 0 red, white, blue, etc, bits states. advances steps. state cell, location, computed step algorithm priori defined close neighbors. Simple programs could, fact, result researched one-dimensional arranged line. data updated based value two nearest cells. methodically studied identified total 256 different rules. Space\u2013time diagrams generated show four distinct patterns: dull uniformity; periodic time-dependence; fractal behavior; truly non-repetitive says broken than 300 fix \"errors\" Darwin, Newton, great ones corrected all. proposes radical notion development world, uncover fundamental universe. pattern-generating capabilities supplant difficult-to-solve yet-to-be-found just because resemble does mean work way. Furthermore, believed represent reality used make predictions agree observations. This Galileo's paradigm underpinning modern explanatory power authority stem ability verifiable predictions; otherwise mere post-hoc speculation. exactly what is. games speculation possibly compete horsepower F=ma E=mc2.Wolfram's boasting, throughout 1200 pages, minimum excessive. He writes, \"I vastly I ever thought possible, fact now touches area besides.\" writes ideas originating him, credits belong elsewhere. Alan Turing conceptualized simplest universal computer, machine. Thinking universe vast digital brainchild Edward Fredkin. use machine environment physical detailed Tommaso Toffoli Norman Margolus. Other Per Bak, Charles Bennett, Hans Meinhardt percolate properly credited. Writing person, relegating notes 350 pages grudgingly dismissively mentioning names, restricting list references own publications, dispel important shortcoming. took approach bypassing peer process. self-published acting author, editor, publisher. opening paragraph mostly favorable Time's (May 20, worth reflecting on: \"Cranks occupational hazard scientist eventually faces. Fortunately, these characters usually easy spot. If someone grand overturns centuries knowledge\u2014especially spans unrelated fields biology economics\u2014the odds good she crank. publishes standard journals general readers, watch out. And issued rather conventional publisher, case pretty much airtight.\" extravagant cold fusion\u2014a` la Stanley Pons Martin Fleischman\u2014and deserve proportionally vigilant scrutiny. validated nor subjected process rest mortals expected do. contrast old anti-Newtonian model predict anything. emperor no clothes. offense play brick build edifice call Bottom Line: fun reading pictures, bad recommendation. inspiration, read Newton's Principia Mathematica, Latin. solving Newtonian framework still best bet, one's better books mechanics.",
    "start_categories":[
      "cs.FL"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "The structure of musical harmony as an ordered phase of sound: A statistical mechanics approach to music theory"
      ],
      "abstract":[
        "Music, while allowing nearly unlimited creative expression, almost always conforms to a set of rigid rules at fundamental level. The description and study these rules, the ordered structures that arise from them, is basis field music theory. Here, I present theoretical formalism aims explain why basic patterns emerge in music, using same statistical mechanics framework describes emergent order across phase transitions physical systems. first apply mean approximation demonstrate occur this model disordered sound discrete sets pitches, including 12-fold octave division used Western music. Beyond model, use numerical simulation uncover musical harmony. These results provide new lens through which view discover ideas explore."
      ],
      "categories":[
        "cs.NA"
      ]
    },
    "list":{
      "title":[
        "Global Semantic-Guided Sub-image Feature Weight Allocation in\n  High-Resolution Large Vision-Language Models",
        "Mobile Manipulation Instruction Generation from Multiple Images with\n  Automatic Metric Enhancement",
        "Indexing Join Inputs for Fast Queries and Maintenance",
        "Message Replication for Improving Reliability of LR-FHSS\n  Direct-to-Satellite IoT",
        "AE-NeRF: Augmenting Event-Based Neural Radiance Fields for Non-ideal\n  Conditions and Larger Scene",
        "DNRSelect: Active Best View Selection for Deferred Neural Rendering",
        "How is Google using AI for internal code migrations?",
        "Poisoned-MRAG: Knowledge Poisoning Attacks to Multimodal Retrieval\n  Augmented Generation",
        "PatchPilot: A Stable and Cost-Efficient Agentic Patching Framework",
        "Addressing Domain Shift via Imbalance-Aware Domain Adaptation in Embryo\n  Development Assessment",
        "Riddle Me This! Stealthy Membership Inference for Retrieval-Augmented\n  Generation",
        "Diffusion based Text-to-Music Generation with Global and Local Text\n  based Conditioning",
        "Rethinking Epistemic and Aleatoric Uncertainty for Active Open-Set\n  Annotation: An Energy-Based Approach",
        "A \"cubist\" decomposition of the Handel-Mosher axis bundle and the\n  conjugacy problem for $\\mathrm{Out}(F_r)$",
        "Design of Cavity Backed Slotted Antenna using Machine Learning\n  Regression Model",
        "A numerical scheme for a multi-scale model of thrombus in arteries",
        "A Novel P-bit-based Probabilistic Computing Approach for Solving the 3-D\n  Protein Folding Problem",
        "Discovering Dataset Nature through Algorithmic Clustering based on\n  String Compression",
        "Modeling Driver Behavior in Speed Advisory Systems: Koopman-based\n  Approach with Online Update",
        "The effect of thermal misbalance on magnetohydrodynamic modes in coronal\n  magnetic cylinders",
        "The steady inviscid compressible self-similar flows and the stability\n  analysis",
        "Tactical Asset Allocation with Macroeconomic Regime Detection",
        "Clustering by Nonparametric Smoothing",
        "Vacuum permittivity and gravitational refractive index revisited",
        "Deceptive Sequential Decision-Making via Regularized Policy Optimization",
        "Feedback control solves pseudoconvex optimal tracking problems in\n  nonlinear dynamical systems",
        "Axion Emission from Proton Cooper Pairs in Neutron Stars",
        "Some characterizations of weak left braces"
      ],
      "abstract":[
        "As the demand for high-resolution image processing in Large Vision-Language\nModels (LVLMs) grows, sub-image partitioning has become a popular approach for\nmitigating visual information loss associated with fixed-resolution processing.\nHowever, existing partitioning methods uniformly process sub-images, resulting\nin suboptimal image understanding. In this work, we reveal that the sub-images\nwith higher semantic relevance to the entire image encapsulate richer visual\ninformation for preserving the model's visual understanding ability. Therefore,\nwe propose the Global Semantic-guided Weight Allocator (GSWA) module, which\ndynamically allocates weights to sub-images based on their relative information\ndensity, emulating human visual attention mechanisms. This approach enables the\nmodel to focus on more informative regions, overcoming the limitations of\nuniform treatment. We integrate GSWA into the InternVL2-2B framework to create\nSleighVL, a lightweight yet high-performing model. Extensive experiments\ndemonstrate that SleighVL outperforms models with comparable parameters and\nremains competitive with larger models. Our work provides a promising direction\nfor more efficient and contextually aware high-resolution image processing in\nLVLMs, advancing multimodal system development.",
        "We consider the problem of generating free-form mobile manipulation\ninstructions based on a target object image and receptacle image. Conventional\nimage captioning models are not able to generate appropriate instructions\nbecause their architectures are typically optimized for single-image. In this\nstudy, we propose a model that handles both the target object and receptacle to\ngenerate free-form instruction sentences for mobile manipulation tasks.\nMoreover, we introduce a novel training method that effectively incorporates\nthe scores from both learning-based and n-gram based automatic evaluation\nmetrics as rewards. This method enables the model to learn the co-occurrence\nrelationships between words and appropriate paraphrases. Results demonstrate\nthat our proposed method outperforms baseline methods including representative\nmultimodal large language models on standard automatic evaluation metrics.\nMoreover, physical experiments reveal that using our method to augment data on\nlanguage instructions improves the performance of an existing multimodal\nlanguage understanding model for mobile manipulation.",
        "In database systems, joins are often expensive despite many years of research\nproducing numerous join algorithms. Precomputed and materialized join views\ndeliver the best query performance, whereas traditional indexes, used as\npre-sorted inputs for merge joins, permit very efficient maintenance. Neither\ntraditional indexes nor materialized join views require blocking phases, in\ncontrast to query-time sorting and transient indexes, e.g., hash tables in hash\njoins, that impose high memory requirements and possibly spill to temporary\nstorage.\n  Here, we introduce a hybrid of traditional indexing and materialized join\nviews. The *merged index* can be implemented with traditional b-trees, permits\nhigh-bandwidth maintenance using log-structured merge-forests, supports all\njoin types (inner joins, all outer joins, all semi joins), and enables\nnon-blocking query processing. Experiments across a wide range of scenarios\nconfirm its query performance comparable to materialized join views and\nmaintenance efficiency comparable to traditional indexes.",
        "Long-range frequency-hopping spread spectrum (LR-FHSS) promises to enhance\nnetwork capacity by integrating frequency hopping into existing Long Range Wide\nArea Networks (LoRaWANs). Due to its simplicity and scalability, LR-FHSS has\ngenerated significant interest as a potential candidate for direct-to-satellite\nIoT (D2S-IoT) applications. This paper explores methods to improve the\nreliability of data transfer on the uplink (i.e., from terrestrial IoT nodes to\nsatellite) of LR-FHSS D2S-IoT networks.\n  Because D2S-IoT networks are expected to support large numbers of potentially\nuncoordinated IoT devices per satellite,\nacknowledgment-cum-retransmission-aided reliability mechanisms are not suitable\ndue to their lack of scalability. We therefore leverage message-replication,\nwherein every application-layer message is transmitted multiple times to\nimprove the probability of reception without the use of receiver\nacknowledgments. We propose two message-replication schemes. One scheme is\nbased on conventional replication, where multiple replicas of a message are\ntransmitted, each as a separate link-layer frame. In the other scheme, multiple\ncopies of a message is included in the payload of a single link-layer frame. We\nshow that both techniques improve LR-FHSS reliability. Which method is more\nsuitable depends on the network's traffic characteristics. We provide\nguidelines to choose the optimal method.",
        "Compared to frame-based methods, computational neuromorphic imaging using\nevent cameras offers significant advantages, such as minimal motion blur,\nenhanced temporal resolution, and high dynamic range. The multi-view\nconsistency of Neural Radiance Fields combined with the unique benefits of\nevent cameras, has spurred recent research into reconstructing NeRF from data\ncaptured by moving event cameras. While showing impressive performance,\nexisting methods rely on ideal conditions with the availability of uniform and\nhigh-quality event sequences and accurate camera poses, and mainly focus on the\nobject level reconstruction, thus limiting their practical applications. In\nthis work, we propose AE-NeRF to address the challenges of learning event-based\nNeRF from non-ideal conditions, including non-uniform event sequences, noisy\nposes, and various scales of scenes. Our method exploits the density of event\nstreams and jointly learn a pose correction module with an event-based NeRF\n(e-NeRF) framework for robust 3D reconstruction from inaccurate camera poses.\nTo generalize to larger scenes, we propose hierarchical event distillation with\na proposal e-NeRF network and a vanilla e-NeRF network to resample and refine\nthe reconstruction process. We further propose an event reconstruction loss and\na temporal loss to improve the view consistency of the reconstructed scene. We\nestablished a comprehensive benchmark that includes large-scale scenes to\nsimulate practical non-ideal conditions, incorporating both synthetic and\nchallenging real-world event datasets. The experimental results show that our\nmethod achieves a new state-of-the-art in event-based 3D reconstruction.",
        "Deferred neural rendering (DNR) is an emerging computer graphics pipeline\ndesigned for high-fidelity rendering and robotic perception. However, DNR\nheavily relies on datasets composed of numerous ray-traced images and demands\nsubstantial computational resources. It remains under-explored how to reduce\nthe reliance on high-quality ray-traced images while maintaining the rendering\nfidelity. In this paper, we propose DNRSelect, which integrates a reinforcement\nlearning-based view selector and a 3D texture aggregator for deferred neural\nrendering. We first propose a novel view selector for deferred neural rendering\nbased on reinforcement learning, which is trained on easily obtained rasterized\nimages to identify the optimal views. By acquiring only a few ray-traced images\nfor these selected views, the selector enables DNR to achieve high-quality\nrendering. To further enhance spatial awareness and geometric consistency in\nDNR, we introduce a 3D texture aggregator that fuses pyramid features from\ndepth maps and normal maps with UV maps. Given that acquiring ray-traced images\nis more time-consuming than generating rasterized images, DNRSelect minimizes\nthe need for ray-traced data by using only a few selected views while still\nachieving high-fidelity rendering results. We conduct detailed experiments and\nablation studies on the NeRF-Synthetic dataset to demonstrate the effectiveness\nof DNRSelect. The code will be released.",
        "In recent years, there has been a tremendous interest in using generative AI,\nand particularly large language models (LLMs) in software engineering; indeed\nthere are now several commercially available tools, and many large companies\nalso have created proprietary ML-based tools for their own software engineers.\nWhile the use of ML for common tasks such as code completion is available in\ncommodity tools, there is a growing interest in application of LLMs for more\nbespoke purposes. One such purpose is code migration.\n  This article is an experience report on using LLMs for code migrations at\nGoogle. It is not a research study, in the sense that we do not carry out\ncomparisons against other approaches or evaluate research questions\/hypotheses.\nRather, we share our experiences in applying LLM-based code migration in an\nenterprise context across a range of migration cases, in the hope that other\nindustry practitioners will find our insights useful. Many of these learnings\napply to any application of ML in software engineering. We see evidence that\nthe use of LLMs can reduce the time needed for migrations significantly, and\ncan reduce barriers to get started and complete migration programs.",
        "Multimodal retrieval-augmented generation (RAG) enhances the visual reasoning\ncapability of vision-language models (VLMs) by dynamically accessing\ninformation from external knowledge bases. In this work, we introduce\n\\textit{Poisoned-MRAG}, the first knowledge poisoning attack on multimodal RAG\nsystems. Poisoned-MRAG injects a few carefully crafted image-text pairs into\nthe multimodal knowledge database, manipulating VLMs to generate the\nattacker-desired response to a target query. Specifically, we formalize the\nattack as an optimization problem and propose two cross-modal attack\nstrategies, dirty-label and clean-label, tailored to the attacker's knowledge\nand goals. Our extensive experiments across multiple knowledge databases and\nVLMs show that Poisoned-MRAG outperforms existing methods, achieving up to 98\\%\nattack success rate with just five malicious image-text pairs injected into the\nInfoSeek database (481,782 pairs). Additionally, We evaluate 4 different\ndefense strategies, including paraphrasing, duplicate removal, structure-driven\nmitigation, and purification, demonstrating their limited effectiveness and\ntrade-offs against Poisoned-MRAG. Our results highlight the effectiveness and\nscalability of Poisoned-MRAG, underscoring its potential as a significant\nthreat to multimodal RAG systems.",
        "Recent research builds various patching agents that combine large language\nmodels (LLMs) with non-ML tools and achieve promising results on the\nstate-of-the-art (SOTA) software patching benchmark, SWE-Bench. Based on how to\ndetermine the patching workflows, existing patching agents can be categorized\nas agent-based planning methods, which rely on LLMs for planning, and\nhuman-based planning methods, which follow a pre-defined workflow. At a high\nlevel, agent-based planning methods achieve high patching performance but with\na high cost and limited stability. Human-based planning methods, on the other\nhand, are more stable and efficient but have key workflow limitations that\ncompromise their patching performance. In this paper, we propose PatchPilot, an\nagentic patcher that strikes a balance between patching efficacy, stability,\nand cost-efficiency. PatchPilot proposes a novel human-based planning workflow\nwith five components: reproduction, localization, generation, validation, and\nrefinement (where refinement is unique to PatchPilot). We introduce novel and\ncustomized designs to each component to optimize their effectiveness and\nefficiency. Through extensive experiments on the SWE-Bench benchmarks,\nPatchPilot shows a superior performance than existing open-source methods while\nmaintaining low cost (less than 1$ per instance) and ensuring higher stability.\nWe also conduct a detailed ablation study to validate the key designs in each\ncomponent.",
        "Deep learning models in medical imaging face dual challenges: domain shift,\nwhere models perform poorly when deployed in settings different from their\ntraining environment, and class imbalance, where certain disease conditions are\nnaturally underrepresented. We present Imbalance-Aware Domain Adaptation\n(IADA), a novel framework that simultaneously tackles both challenges through\nthree key components: (1) adaptive feature learning with class-specific\nattention mechanisms, (2) balanced domain alignment with dynamic weighting, and\n(3) adaptive threshold optimization. Our theoretical analysis establishes\nconvergence guarantees and complexity bounds. Through extensive experiments on\nembryo development assessment across four imaging modalities, IADA demonstrates\nsignificant improvements over existing methods, achieving up to 25.19\\% higher\naccuracy while maintaining balanced performance across classes. In challenging\nscenarios with low-quality imaging systems, IADA shows robust generalization\nwith AUC improvements of up to 12.56\\%. These results demonstrate IADA's\npotential for developing reliable and equitable medical imaging systems for\ndiverse clinical settings. The code is made public available at\n\\url{https:\/\/github.com\/yinghemedical\/imbalance-aware_domain_adaptation}",
        "Retrieval-Augmented Generation (RAG) enables Large Language Models (LLMs) to\ngenerate grounded responses by leveraging external knowledge databases without\naltering model parameters. Although the absence of weight tuning prevents\nleakage via model parameters, it introduces the risk of inference adversaries\nexploiting retrieved documents in the model's context. Existing methods for\nmembership inference and data extraction often rely on jailbreaking or\ncarefully crafted unnatural queries, which can be easily detected or thwarted\nwith query rewriting techniques common in RAG systems. In this work, we present\nInterrogation Attack (IA), a membership inference technique targeting documents\nin the RAG datastore. By crafting natural-text queries that are answerable only\nwith the target document's presence, our approach demonstrates successful\ninference with just 30 queries while remaining stealthy; straightforward\ndetectors identify adversarial prompts from existing methods up to ~76x more\nfrequently than those generated by our attack. We observe a 2x improvement in\nTPR@1%FPR over prior inference attacks across diverse RAG configurations, all\nwhile costing less than $0.02 per document inference.",
        "Diffusion based Text-To-Music (TTM) models generate music corresponding to\ntext descriptions. Typically UNet based diffusion models condition on text\nembeddings generated from a pre-trained large language model or from a\ncross-modality audio-language representation model. This work proposes a\ndiffusion based TTM, in which the UNet is conditioned on both (i) a uni-modal\nlanguage model (e.g., T5) via cross-attention and (ii) a cross-modal\naudio-language representation model (e.g., CLAP) via Feature-wise Linear\nModulation (FiLM). The diffusion model is trained to exploit both a local text\nrepresentation from the T5 and a global representation from the CLAP.\nFurthermore, we propose modifications that extract both global and local\nrepresentations from the T5 through pooling mechanisms that we call mean\npooling and self-attention pooling. This approach mitigates the need for an\nadditional encoder (e.g., CLAP) to extract a global representation, thereby\nreducing the number of model parameters. Our results show that incorporating\nthe CLAP global embeddings to the T5 local embeddings enhances text adherence\n(KL=1.47) compared to a baseline model solely relying on the T5 local\nembeddings (KL=1.54). Alternatively, extracting global text embeddings directly\nfrom the T5 local embeddings through the proposed mean pooling approach yields\nsuperior generation quality (FAD=1.89) while exhibiting marginally inferior\ntext adherence (KL=1.51) against the model conditioned on both CLAP and T5 text\nembeddings (FAD=1.94 and KL=1.47). Our proposed solution is not only efficient\nbut also compact in terms of the number of parameters required.",
        "Active learning (AL), which iteratively queries the most informative examples\nfrom a large pool of unlabeled candidates for model training, faces significant\nchallenges in the presence of open-set classes. Existing methods either\nprioritize query examples likely to belong to known classes, indicating low\nepistemic uncertainty (EU), or focus on querying those with highly uncertain\npredictions, reflecting high aleatoric uncertainty (AU). However, they both\nyield suboptimal performance, as low EU corresponds to limited useful\ninformation, and closed-set AU metrics for unknown class examples are less\nmeaningful. In this paper, we propose an Energy-based Active Open-set\nAnnotation (EAOA) framework, which effectively integrates EU and AU to achieve\nsuperior performance. EAOA features a $(C+1)$-class detector and a target\nclassifier, incorporating an energy-based EU measure and a margin-based energy\nloss designed for the detector, alongside an energy-based AU measure for the\ntarget classifier. Another crucial component is the target-driven adaptive\nsampling strategy. It first forms a smaller candidate set with low EU scores to\nensure closed-set properties, making AU metrics meaningful. Subsequently,\nexamples with high AU scores are queried to form the final query set, with the\ncandidate set size adjusted adaptively. Extensive experiments show that EAOA\nachieves state-of-the-art performance while maintaining high query precision\nand low training overhead. The code is available at\nhttps:\/\/github.com\/chenchenzong\/EAOA.",
        "We show that the axis bundle of a nongeometric fully irreducible outer\nautomorphism admits a canonical \"cubist\" decomposition into branched cubes that\nfit together with special combinatorics. From this structure, we locate a\ncanonical finite collection of periodic fold lines in each axis bundle. This\ngives a solution to the conjugacy problem in $\\mathrm{Out}(F_r)$ for fully\nirreducible outer automorphisms. This can be considered as an analogue of\nresults of Hamenst\\\"adt and Agol from the surface setting, which state that the\nset of trivalent train tracks carrying the unstable lamination of a\npseudo-Anosov map can be given the structure of a CAT(0) cube complex, and that\nthere is a canonical periodic fold line in this cube complex.",
        "In this paper, a regression-based machine learning model is used for the\ndesign of cavity backed slotted antenna. This type of antenna is commonly used\nin military and aviation communication systems. Initial reflection coefficient\ndata of cavity backed slotted antenna is generated using electromagnetic\nsolver. These reflection coefficient data is then used as input for training\nregression-based machine learning model. The model is trained to predict the\ndimensions of cavity backed slotted antenna based on the input reflection\ncoefficient for a wide frequency band varying from 1 GHz to 8 GHz. This\napproach allows for rapid prediction of optimal antenna configurations,\nreducing the need for repeated physical testing and manual adjustments, may\nlead to significant amount of design and development cost saving. The proposed\nmodel also demonstrates its versatility in predicting multi frequency resonance\nacross 1 GHz to 8 GHz. Also, the proposed approach demonstrates the potential\nfor leveraging machine learning in advanced antenna design, enhancing\nefficiency and accuracy in practical applications such as radar, military\nidentification systems and secure communication networks.",
        "In this article, the time-discretization of the fluid structure interaction\nmodel in the three-dimensional boundary domain is taken into account, which\nexplains the mechanical interaction between the blood flow and the Hookean\nelasticity. The interface between the two phases is given by a soft transition\nlayer and spreads to the finite thickness. On the implicit Euler scheme for\nthis discretization, We derive a variety of priori estimates and then use the\nFaedo-Galerkin method to prove the local well-poseedness results.",
        "In the post-Moore era, the need for efficient solutions to non-deterministic\npolynomial-time (NP) problems is becoming more pressing. In this context, the\nIsing model implemented by the probabilistic computing systems with\nprobabilistic bits (p-bits) has attracted attention due to the widespread\navailability of p-bits and support for large-scale simulations. This study\nmarks the first work to apply probabilistic computing to tackle protein\nfolding, a significant NP-complete problem challenge in biology. We represent\nproteins as sequences of hydrophobic (H) and polar (P) beads within a\nthree-dimensional (3-D) grid and introduce a novel many-body interaction-based\nencoding method to map the problem onto an Ising model. Our simulations show\nthat this approach significantly simplifies the energy landscape for short\npeptide sequences of six amino acids, halving the number of energy levels.\nFurthermore, the proposed mapping method achieves approximately 100 times\nacceleration for sequences consisting of ten amino acids in identifying the\ncorrect folding configuration. We predicted the optimal folding configuration\nfor a peptide sequence of 36 amino acids by identifying the ground state. These\nfindings highlight the unique potential of the proposed encoding method for\nsolving protein folding and, importantly, provide new tools for solving similar\nNP-complete problems in biology by probabilistic computing approach.",
        "Text datasets can be represented using models that do not preserve text\nstructure, or using models that preserve text structure. Our hypothesis is that\ndepending on the dataset nature, there can be advantages using a model that\npreserves text structure over one that does not, and viceversa. The key is to\ndetermine the best way of representing a particular dataset, based on the\ndataset itself. In this work, we propose to investigate this problem by\ncombining text distortion and algorithmic clustering based on string\ncompression. Specifically, a distortion technique previously developed by the\nauthors is applied to destroy text structure progressively. Following this, a\nclustering algorithm based on string compression is used to analyze the effects\nof the distortion on the information contained in the texts. Several\nexperiments are carried out on text datasets and artificially-generated\ndatasets. The results show that in strongly structural datasets the clustering\nresults worsen as text structure is progressively destroyed. Besides, they show\nthat using a compressor which enables the choice of the size of the\nleft-context symbols helps to determine the nature of the datasets. Finally,\nthe results are contrasted with a method based on multidimensional projections\nand analogous conclusions are obtained.",
        "Accurate driver behavior modeling is essential for improving the interaction\nand cooperation of the human driver with the driver assistance system. This\npaper presents a novel approach for modeling the response of human drivers to\nvisual cues provided by a speed advisory system using a Koopman-based method\nwith online updates. The proposed method utilizes the Koopman operator to\ntransform the nonlinear dynamics of driver-speed advisory system interactions\ninto a linear framework, allowing for efficient real-time prediction. An online\nupdate mechanism based on Recursive Least Squares (RLS) is integrated into the\nKoopman-based model to ensure continuous adaptation to changes in driver\nbehavior over time. The model is validated using data collected from a\nhuman-in-the-loop driving simulator, capturing diverse driver-specific\ntrajectories. The results demonstrate that the offline learned Koopman-based\nmodel can closely predict driver behavior and its accuracy is further enhanced\nthrough an online update mechanism with the RLS method.",
        "This study investigates the dispersion of magnetohydrodynamic waves\ninfluenced by thermal misbalance in a cylindrical configuration with a finite\naxial magnetic field within solar coronal plasmas. Specifically, it examines\nhow thermal misbalance, characterized by two distinct timescales directly\nlinked to the cooling and heating functions, influences the dispersion\nrelation. This investigation is a key approach for understanding non-adiabatic\neffects on the behaviour of these waves. Our findings reveal that the effect of\nthermal misbalance on fast sausage and kink modes, consistent with previous\nstudies on slabs, is small but slightly more pronounced than previously\nthought. The impact is smaller at long-wavelength limits but increases at\nshorter wavelengths, leading to higher damping rates. This minor effect on fast\nmodes occurs despite the complex interaction of thermal misbalance terms within\nthe dispersion relation, even at low-frequency limits defined by the\ncharacteristic timescales. Additionally, a very small amplification is\nobserved, indicating a suppressed damping state for the long-wavelength\nfundamental fast kink mode. In contrast, slow magnetoacoustic modes are\nsignificantly affected by thermal misbalance, with the cusp frequency shifting\nslightly to lower values, which is significant for smaller longitudinal\nwavenumbers. This thermal misbalance likely accounts for the substantial\nattenuation observed in the propagation of slow magnetoacoustic waves within\nthe solar atmosphere. The long-wavelength limit leads to an analytical\nexpression that accurately describes the frequency shifts in slow modes due to\nmisbalance, closely aligning with both numerical and observational results.",
        "We investigate the steady inviscid compressible self-similar flows which\ndepends only on the polar angle in spherical coordinates. It is shown that\nbesides the purely supersonic and subsonic self-similar flows, there exists\npurely sonic flows, Beltrami flows with a nonconstant proportionnality factor\nand smooth transonic self-similar flows with large vorticity. For a constant\nsupersonic incoming flow past an infinitely long circular cone, a conic shock\nattached to the tip of the cone will form, provided the opening angle of the\ncone is less than a critical value. We introduce the shock polar for the radial\nand polar components of the velocity and show that there exists a monotonicity\nrelation between the shock angle and the radial velocity, which seems to be new\nand not been observed before. If a supersonic incoming flow is self-similar\nwith nonzero azimuthal velocity, a conic shock also form attached to the tip of\nthe cone. The state at the downstream may change smoothly from supersonic to\nsubsonic, thus the shock can be supersonic-supersonic, supersonic-subsonic and\neven supersonic-sonic where the shock front and the sonic front coincide. We\nfurther investigate the structural stability of smooth self-similar\nirrotational transonic flows and analyze the corresponding linear mixed type\nsecond order equation of Tricomi type. By exploring some key properties of the\nself-similar solutions, we find a multiplier and identify a class of admissible\nboundary conditions for the linearized mixed type second-order equation. We\nalso prove the existence and uniqueness of a class of smooth transonic flows\nwith nonzero vorticity which depends only on the polar and azimuthal angles in\nspherical coordinates.",
        "This paper extends the tactical asset allocation literature by incorporating\nregime modeling using techniques from machine learning. We propose a novel\nmodel that classifies current regimes, forecasts the distribution of future\nregimes, and integrates these forecasts with the historical performance of\nindividual assets to optimize portfolio allocations. Utilizing a macroeconomic\ndata set from the FRED-MD database, our approach employs a modified k-means\nalgorithm to ensure consistent regime classification over time. We then\nleverage these regime predictions to estimate expected returns and\nvolatilities, which are subsequently mapped into portfolio allocations using\nvarious sizing schemes. Our method outperforms traditional benchmarks such as\nequal-weight, buy-and-hold, and random regime models. Additionally, we are the\nfirst to apply a regime detection model from a large macroeconomic dataset to\ntactical asset allocation, demonstrating significant improvements in portfolio\nperformance. Our work presents several key contributions, including a novel\ndata-driven regime detection algorithm tailored for uncertainty in forecasted\nregimes and applying the FRED-MD data set for tactical asset allocation.",
        "A novel formulation of the clustering problem is introduced in which the task\nis expressed as an estimation problem, where the object to be estimated is a\nfunction which maps a point to its distribution of cluster membership. Unlike\nexisting approaches which implicitly estimate such a function, like Gaussian\nMixture Models (GMMs), the proposed approach bypasses any explicit modelling\nassumptions and exploits the flexible estimation potential of nonparametric\nsmoothing. An intuitive approach for selecting the tuning parameters governing\nestimation is provided, which allows the proposed method to automatically\ndetermine both an appropriate level of flexibility and also the number of\nclusters to extract from a given data set. Experiments on a large collection of\npublicly available data sets are used to document the strong performance of the\nproposed approach, in comparison with relevant benchmarks from the literature.\nR code to implement the proposed approach is available from\nhttps:\/\/github.com\/DavidHofmeyr\/ CNS",
        "The present paper reanalyzes the problem of the refractive properties of the\nphysical vacuum and their modification under the action of the gravitational\nfield and the electromagnetic field. This problem was studied in our previous\nworks and in the subsequent works of the researchers: Leuchs, Urban, Mainland\nand their collaborators. By modeling the physical vacuum as a\nparticle-antiparticle system, we can deduce with a certain approximation, in a\nsemiclassical theory, the properties of the free vacuum and the vacuum modified\nby the interaction with a gravitational field and an electromagnetic field.\nMore precise calculation of permittivities of free vacuum and near a particle\ncan lead to a non-point model of the particle. This modeling can follow both\nthe quantum and the general relativistic path as well as the phenomenological\npath, the results complementing each other.",
        "Autonomous systems are increasingly expected to operate in the presence of\nadversaries, though an adversary may infer sensitive information simply by\nobserving a system, without even needing to interact with it. Therefore, in\nthis work we present a deceptive decision-making framework that not only\nconceals sensitive information, but in fact actively misleads adversaries about\nit. We model autonomous systems as Markov decision processes, and we consider\nadversaries that attempt to infer their reward functions using inverse\nreinforcement learning. To counter such efforts, we present two regularization\nstrategies for policy synthesis problems that actively deceive an adversary\nabout a system's underlying rewards. The first form of deception is\n``diversionary'', and it leads an adversary to draw any false conclusion about\nwhat the system's reward function is. The second form of deception is\n``targeted'', and it leads an adversary to draw a specific false conclusion\nabout what the system's reward function is. We then show how each form of\ndeception can be implemented in policy optimization problems, and we\nanalytically bound the loss in total accumulated reward that is induced by\ndeception. Next, we evaluate these developments in a multi-agent sequential\ndecision-making problem with one real agent and multiple decoys. We show that\ndiversionary deception can cause the adversary to believe that the most\nimportant agent is the least important, while attaining a total accumulated\nreward that is $98.83\\%$ of its optimal, non-deceptive value. Similarly, we\nshow that targeted deception can make any decoy appear to be the most important\nagent, while still attaining a total accumulated reward that is $99.25\\%$ of\nits optimal, non-deceptive value.",
        "Achieving optimality in controlling physical systems is a profound challenge\nacross diverse scientific and engineering fields, spanning neuromechanics,\nbiochemistry, autonomous systems, economics, and beyond. Traditional solutions,\nrelying on time-consuming offline iterative algorithms, often yield limited\ninsights into fundamental natural processes. In this work, we introduce a\nnovel, causally deterministic approach, presenting the closed-form optimal\ntracking controller (OTC) that inherently solves pseudoconvex optimization\nproblems in various fields. Through rigorous analysis and comprehensive\nnumerical examples, we demonstrate OTC's capability of achieving both high\naccuracy and rapid response, even when facing high-dimensional and\nhigh-dynamical real-world problems. Notably, our OTC outperforms\nstate-of-the-art methods by, e.g., solving a 1304-dimensional neuromechanics\nproblem 1311 times faster or with 113 times higher accuracy. Most importantly,\nOTC embodies a causally deterministic system interpretation of optimality\nprinciples, providing a new and fundamental perspective of optimization in\nnatural and artificial processes. We anticipate our work to be an important\nstep towards establishing a general causally deterministic optimization theory\nfor a broader spectrum of system and problem classes, promising advances in\nunderstanding optimality principles in complex systems.",
        "We investigate axion emission from singlet proton Cooper pairs in neutron\nstars, a process that dominates axion emission in young neutron stars in the\nKSVZ model. By re-deriving its emissivity, we confirm consistency with most\nexisting literature, except for a recent study that exhibits a different\ndependence on the effective mass. This discrepancy results in more than an\norder-of-magnitude deviation in emissivity, significantly impacting constraints\non the KSVZ axion from the cooling observations of the Cassiopeia A neutron\nstar. Furthermore, we examine uncertainties arising from neutron-star equations\nof state and their role in the discrepancy, finding that the large deviation\npersists regardless of the choice of equations of state.",
        "As generalizations of skew left braces, weak left braces were introduced\nrecently by Catino, Mazzotta, Miccoli and Stefanelli to study ceratin special\ndegenerate set-theoretical solutions of the Yang-Baxter equation. In this note,\nas analogues of the notions of regular subgroups of holomorph of groups, Gamma\nfunctions on groups and affine and semi-affine structures on groups, we propose\nthe notions of good inverse subsemigroups and Gamma functions associated to\nClifford semigroups and affine structures on inverse semigroups, respectively,\nby which weak left braces are characterized. Moreover, symmetric,\n$\\lambda$-homomorphic and $\\lambda$-anti-homomorphic weak left braces are\nintroduced and the algebraic structures of these weak left braces are given."
      ]
    }
  },
  {
    "id":2411.19844,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"The structure of musical harmony as an ordered phase of sound: A statistical mechanics approach to music theory",
    "start_abstract":"Music, while allowing nearly unlimited creative expression, almost always conforms to a set of rigid rules at fundamental level. The description and study these rules, the ordered structures that arise from them, is basis field music theory. Here, I present theoretical formalism aims explain why basic patterns emerge in music, using same statistical mechanics framework describes emergent order across phase transitions physical systems. first apply mean approximation demonstrate occur this model disordered sound discrete sets pitches, including 12-fold octave division used Western music. Beyond model, use numerical simulation uncover musical harmony. These results provide new lens through which view discover ideas explore.",
    "start_categories":[
      "cs.NA"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b2"
      ],
      "title":[
        "A New Kind of Science"
      ],
      "abstract":[
        "3R3. A New Kind of Science. - S Wolfram (Wolfram Res Inc, 100 Trade Center Dr, Champaign IL 61820-7237). Media, Champaign, IL. 2002. 1197 pp. ISBN 1-57955-008-8. $44.95. Reviewed by M Gad-el-Hak (Eng Build, Rm 303, Virginia Commonwealth Univ, 601 W Main St, PO Box 843015, Richmond, VA 23284-3015). Reviewing Science is like stepping in a minefield. The danger lies going against the deluge praise, proving relevance to this audience, and arguing proposed new science that allegedly set replace science, as we know it. Those issues will be addressed turn, but first brief background. Stephen considered many have been child prodigy: journal paper particle physics at age 15; stint Oxford; PhD from Caltech 20; youngest recipient MacArthur Prize; faculty positions Caltech, Princeton, Illinois; significant contributions cellular automata complexity theory; developer popular software Mathematica; successful entrepreneur, becoming multi-millionaire 30. Running his company via e-mail videoconference, spent last 10 years virtual seclusion, relentlessly, tirelessly, secretly, nocturnally working on an idea possessed him: generating simple computations, algorithms only few lines. book, targeting both scientists non-scientists, partially about using rules generate complex patterns. In task, author has succeeded beyond reproach not showing can done brilliantly beautifully, also explaining it lucidly enough for all understand, appreciate, savor. opinion several reviewers, including one, aspect book tour de force clarity, elegance, simplicity. problem huge leap takes since nature computer-generated patterns look or behave similarly natural man-made things around us\u2014a snow flake, turbulent flow, lung, mollusk shell, traffic jam, outbreak starfish coral reef, entire universe\u2014therefore must way works. Nature runs its course same computer program. That essence science: yield secrets universe, solve our long-standing problems, provide theory everything. More flight fancy later. Deluge: was widely anticipated before actual publication. Published May 14, 2002, quickly became Amazon.com bestseller promptly reviewed scientific press. Heavyweights former included York Times, Chicago Tribune, Newsweek, Time, Daily Telegraph, Le Monde, Frankfurter Allgemeine Zeitung, Economist. Except last, press went gaga over touting author's claim stand existing head. Economist (p 79, June 1, 2002) more subdued even provocatively titling review \"The Emperor's Theory.\" press, reviews were somewhat less glorious skeptical. Physics Today 55, July 2002), Leo Kadanoff's once pointed, subtle polite, concluding he cannot support view any \"new kind science\" displayed Wolfram's book. Newsweek 59, 27, quoted famed physicist Freeman Dyson: \"There's tradition approaching senility come up with grand, improbable theories. unusual he's doing 40s.\" Kadanoff Dyson express minority opinion, however, majority reviewers being excited reason every human mystery currently depressed stock market, free will, quantum field theory, entropy. For present reviewer, lurks high particularly so months behind who already anointed Isaac Newton 21st century. Relevance: As aims replacing readers Applied Mechanics Reviews stake matter. Mechanics\u2014classical most part occasionally quantum\u2014is underlying branch upon which almost applied mechanics based. mathematics here often form partial differential equations, where space time are indefinitely divisible continuum. example, most, all, fluid flows described well-known, well-posed Navier\u2013Stokes equations. those first-principles equations solved agreement experiment reproach. It problem, such frustrated scores him. search simpler alternative is, therefore, quite alluring. mechanics, when they solved, powerful predictive tool explain mechanical world us well help design machines. When analytical solutions unattainable, discretized brute numerical integration used. But possible some situations, example realistic high-Reynolds-number other multi-scale problems required computational memory speed overwhelm today's supercomputers. impenetrable certain degree empiricism introduced relatively faster computations then proceed. Heuristic turbulence modeling compromise. Despite limitations, traditional works exceedingly well, mechanicians happily practice their craft. Readers should, care passionately if laws supplanted science. Argument: Cellular late 1940s John von Neumann Stanislaw Ulam, although claims independently discovered three decades discrete dynamical systems whose behavior completely specified terms repetitive local relation. continuum represented uniform one-, two-, three-dimensional grid, each cell containing single bit data, 0 red, white, blue, etc, bits states. advances steps. state cell, location, computed step algorithm priori defined close neighbors. Simple programs could, fact, result researched one-dimensional arranged line. data updated based value two nearest cells. methodically studied identified total 256 different rules. Space\u2013time diagrams generated show four distinct patterns: dull uniformity; periodic time-dependence; fractal behavior; truly non-repetitive says broken than 300 fix \"errors\" Darwin, Newton, great ones corrected all. proposes radical notion development world, uncover fundamental universe. pattern-generating capabilities supplant difficult-to-solve yet-to-be-found just because resemble does mean work way. Furthermore, believed represent reality used make predictions agree observations. This Galileo's paradigm underpinning modern explanatory power authority stem ability verifiable predictions; otherwise mere post-hoc speculation. exactly what is. games speculation possibly compete horsepower F=ma E=mc2.Wolfram's boasting, throughout 1200 pages, minimum excessive. He writes, \"I vastly I ever thought possible, fact now touches area besides.\" writes ideas originating him, credits belong elsewhere. Alan Turing conceptualized simplest universal computer, machine. Thinking universe vast digital brainchild Edward Fredkin. use machine environment physical detailed Tommaso Toffoli Norman Margolus. Other Per Bak, Charles Bennett, Hans Meinhardt percolate properly credited. Writing person, relegating notes 350 pages grudgingly dismissively mentioning names, restricting list references own publications, dispel important shortcoming. took approach bypassing peer process. self-published acting author, editor, publisher. opening paragraph mostly favorable Time's (May 20, worth reflecting on: \"Cranks occupational hazard scientist eventually faces. Fortunately, these characters usually easy spot. If someone grand overturns centuries knowledge\u2014especially spans unrelated fields biology economics\u2014the odds good she crank. publishes standard journals general readers, watch out. And issued rather conventional publisher, case pretty much airtight.\" extravagant cold fusion\u2014a` la Stanley Pons Martin Fleischman\u2014and deserve proportionally vigilant scrutiny. validated nor subjected process rest mortals expected do. contrast old anti-Newtonian model predict anything. emperor no clothes. offense play brick build edifice call Bottom Line: fun reading pictures, bad recommendation. inspiration, read Newton's Principia Mathematica, Latin. solving Newtonian framework still best bet, one's better books mechanics."
      ],
      "categories":[
        "cs.FL"
      ]
    },
    "list":{
      "title":[
        "On Constructing Finite Automata by Relational Programming",
        "Counting Abstraction for the Verification of Structured Parameterized\n  Networks",
        "An Automata-Based Method to Formalize Psychological Theories -- The Case\n  Study of Lazarus and Folkman's Stress Theory",
        "Omega-Regular Robustness",
        "The rIC3 Hardware Model Checker",
        "Active Learning Techniques for Pomset Recognizers",
        "Soundness of reset workflow nets",
        "Behaviorally Correct Learning from Informants",
        "Fined-Grained Complexity of Ambiguity Problems on Automata and Directed\n  Graphs",
        "Asynchronism in Cellular Automata",
        "Learning Automata with Name Allocation",
        "Lexicographic transductions of finite words",
        "Complexity of the Uniform Membership Problem for Hyperedge Replacement\n  Grammars",
        "A \"cubist\" decomposition of the Handel-Mosher axis bundle and the\n  conjugacy problem for $\\mathrm{Out}(F_r)$",
        "Design of Cavity Backed Slotted Antenna using Machine Learning\n  Regression Model",
        "A numerical scheme for a multi-scale model of thrombus in arteries",
        "A Novel P-bit-based Probabilistic Computing Approach for Solving the 3-D\n  Protein Folding Problem",
        "Discovering Dataset Nature through Algorithmic Clustering based on\n  String Compression",
        "Modeling Driver Behavior in Speed Advisory Systems: Koopman-based\n  Approach with Online Update",
        "The effect of thermal misbalance on magnetohydrodynamic modes in coronal\n  magnetic cylinders",
        "The steady inviscid compressible self-similar flows and the stability\n  analysis",
        "Tactical Asset Allocation with Macroeconomic Regime Detection",
        "Clustering by Nonparametric Smoothing",
        "Vacuum permittivity and gravitational refractive index revisited",
        "Deceptive Sequential Decision-Making via Regularized Policy Optimization",
        "Feedback control solves pseudoconvex optimal tracking problems in\n  nonlinear dynamical systems",
        "Axion Emission from Proton Cooper Pairs in Neutron Stars",
        "Some characterizations of weak left braces"
      ],
      "abstract":[
        "We consider ways to construct a transducer for a given set of input word to\noutput symbol pairs. This is motivated by the need for representing game\nplaying programs in a low-level mathematical format that can be analyzed by\nalgebraic tools. This is different from the classical applications of finite\nstate automata, thus the usual optimization techniques are not directly\napplicable. Therefore, we use relational programming tools to find minimal\ntransducers realizing a given set of input-output pairs.",
        "We consider the verification of parameterized networks of replicated\n  processes whose architecture is described by hyperedge-replacement\n  graph grammars. Due to the undecidability of verification problems\n  such as reachability or coverability of a given configuration, in\n  which we count the number of replicas in each local state, we\n  develop two orthogonal verification techniques. We present a\n  counting abstraction able to produce, from a graph grammar\n  describing a parameterized system, a finite set of Petri nets that\n  over-approximate the behaviors of the original system. The counting\n  abstraction is implemented in a prototype tool, evalutated on a\n  non-trivial set of test cases. Moreover, we identify a decidable\n  fragment, for which the coverability problem is in 2EXPTIME\n  and PSPACE-hard.",
        "Formal models are important for theory-building, enhancing the precision of\npredictions and promoting collaboration. Researchers have argued that there is\na lack of formal models in psychology. We present an automata-based method to\nformalize psychological theories, i.e. to transform verbal theories into formal\nmodels. This approach leverages the tools of theoretical computer science for\nformal theory development, for verification, comparison, collaboration, and\nmodularity. We exemplify our method on Lazarus and Folkman's theory of stress,\nshowcasing a step-by-step modeling of the theory.",
        "Roughly speaking, a system is said to be robust if it can resist disturbances\nand still function correctly. For instance, if the requirement is that the\ntemperature remains in an allowed range $[l,h]$, then a system that remains in\na range $[l',h']\\subset[l,h]$ is more robust than one that reaches $l$ and $h$\nfrom time to time. In this example the initial specification is quantitative in\nnature, this is not the case in $\\omega$-regular properties. Still, it seems\nthere is a natural robustness preference relation induced by an\n$\\omega$-regular property. E.g. for a property requiring that every request is\neventually granted, one would say that a system where requests are granted two\nticks after they are issued is more robust than one in which requests are\nanswered ninety ticks after they are issued. In this work we manage to distill\na robustness preference relation that is induced by a given $\\omega$-regular\nlanguage. The relation is a semantic notion (agnostic to the given\nrepresentation of $L$) that satisfies some natural criteria.",
        "In this paper, we present rIC3, an efficient bit-level hardware model checker\nprimarily based on the IC3 algorithm. It boasts a highly efficient\nimplementation and integrates several recently proposed optimizations, such as\nthe specifically optimized SAT solver, dynamically adjustment of generalization\nstrategies, and the use of predicates with internal signals, among others. As a\nfirst-time participant in the Hardware Model Checking Competition, rIC3 was\nindependently evaluated as the best-performing tool, not only in the bit-level\ntrack but also in the word-level bit-vector track through bit-blasting. Our\nexperiments further demonstrate significant advancements in both efficiency and\nscalability. rIC3 can also serve as a backend for verifying industrial RTL\ndesigns using SymbiYosys. Additionally, the source code of rIC3 is highly\nmodular, with the IC3 algorithm module being particularly concise, making it an\nacademic platform that is easy to modify and extend.",
        "Pomsets are a promising formalism for concurrent programs based on partially\nordered sets. Among this class, series-parallel pomsets admit a convenient\nlinear representation and can be recognized by simple algebraic structures\nknown as pomset recognizers. Active learning consists in inferring a formal\nmodel of a recognizable language by asking membership and equivalence queries\nto a minimally adequate teacher (MAT). We improve existing learning algorithms\nfor pomset recognizers by 1. introducing a new counter-example analysis\nprocedure that is in the best case scenario exponentially more efficient than\nexisting methods 2. adapting the state-of-the-art $L^{\\lambda}$ algorithm to\nminimize the impact of exceedingly verbose counter-examples and remove\nredundant queries 3. designing a suitable finite test suite that ensures\ngeneral equivalence between two pomset recognizers by extending the well-known\nW-method.",
        "Workflow nets are a well-established variant of Petri nets for the modeling\nof process activities such as business processes. The standard correctness\nnotion of workflow nets is soundness, which comes in several variants. Their\ndecidability was shown decades ago, but their complexity was only identified\nrecently. In this work, we are primarily interested in two popular variants:\n$1$-soundness and generalised soundness.\n  Workflow nets have been extended with resets to model workflows that can,\ne.g., cancel actions. It has been known for a while that, for this extension,\nall variants of soundness, except possibly generalised soundness, are\nundecidable.\n  We complete the picture by showing that generalised soundness is also\nundecidable for reset workflow nets. We then blur this undecidability landscape\nby identifying a property, coined ``$1$-in-between soundness'', which lies\nbetween $1$-soundness and generalised soundness. It reveals an unusual\nnon-monotonic complexity behaviour: a decidable soundness property is in\nbetween two undecidable ones. This can be valuable in the algorithmic analysis\nof reset workflow nets, as our procedure yields an output of the form\n``$1$-sound'' or ``not generalised sound'' which is always correct.",
        "In inductive inference, we investigate the learnability of classes of formal\nlanguages. We are interested in what classes of languages are learnable in\ncertain learning settings. A class of languages is learnable, if there is a\nlearner that can identify all of its languages and satisfies the constraints of\nthe learning setting. To identify a language, a learner is presented with\ninformation about this very language. When learning from informants, this\ninformation consists of examples for numbers that are, and numbers that are not\nincluded in the target language. As more and more examples are presented, the\nlearner outputs a hypothesis sequence. To satisfy behaviorally correct\nidentification, this hypothesis sequence must eventually only list correct\nlabels for the target language. In this thesis, we compare the effects of a\nnumber of semantic learning restrictions on the learning capabilities for\nbehaviorally correct learning from informants.",
        "Two fundamental classes of finite automata are deterministic and\nnondeterministic ones (DFAs and NFAs). Natural intermediate classes arise from\nbounds on an NFA's allowed ambiguity, i.e. number of accepting runs per word:\nunambiguous, finitely ambiguous, and polynomially ambiguous finite automata. It\nis known that deciding whether a given NFA is unambiguous and whether it is\npolynomially ambiguous is possible in quadratic time, and deciding finite\nambiguity is possible in cubic time. We provide matching lower bounds showing\nthese running times to be optimal, assuming popular fine-grained complexity\nhypotheses.\n  We improve the upper bounds for unary automata, which are essentially\ndirected graphs with a source and a target. In this view, unambiguity asks\nwhether all walks from the source to the target have different lengths. The\nrunning time analysis of our algorithm reduces to bounding the entry-wise\n1-norm of a GCD matrix, yielding a near-linear upper bound. For finite and\npolynomial ambiguity, we provide simple linear-time algorithms in the unary\ncase.\n  Finally, we study the twins property for weighted automata over the tropical\nsemiring, which characterises the determinisability of unambiguous weighted\nautomata. It occurs naturally in our context as deciding the twins property is\nan intermediate step in determinisability algorithms for weighted automata with\nbounded ambiguity. We show that Allauzen and Mohri's quadratic-time algorithm\nchecking the twins property is optimal up to the same fine-grained hypotheses\nas for unambiguity. For unary automata, we show that the problem can be\nrephrased to whether all cycles in a weighted directed graph have the same\naverage weight and give a linear-time algorithm.",
        "This study introduces Skewed Fully Asynchronous Cellular Automata (SACA), a\nnovel update scheme in cellular automata that updates the states of only two\nconsecutive and adjacent cells, such as ci and ci+1, simultaneously at each\ntime step. The behavior and dynamics of elementary cellular automata (ECA)\nunder this scheme are analyzed and compared with those of synchronous and fully\nasynchronous update methods. The comparative analysis highlights a range of\nphenomena, including transitions in ECAs from convergent or non-reversible\ndynamics to reversible, divergent behavior. The divisibility of lattice size by\n2 or 4 is shown to have significant effects on the system dynamics, linked to\nthe presence or absence of atomicity. The study also explores the convergence\nof ECAs to all-zero or all-one point attractors under SACA, providing\ntheoretical insights that align with experimental findings.\n  Additionally, the research investigates the application of fully asynchronous\ncellular automata in solving clustering problems. Clustering is defined as\ngrouping objects with similar properties. The proposed method employs\nreversible asynchronous cellular automata to merge clusters iteratively based\non their closeness, continuing until the desired number of clusters is\nachieved. This approach leverages a small set of rules, leading to faster\nconvergence and efficiency in clustering tasks. The findings underscore the\npotential of asynchronous cellular automata as a versatile and effective\nframework for studying complex system dynamics and solving practical problems\nsuch as clustering.",
        "Automata over infinite alphabets have emerged as a convenient computational\nmodel for processing structures involving data, such as nonces in cryptographic\nprotocols or data values in XML documents. We introduce active learning methods\nfor bar automata, a species of automata that process finite data words\nrepresented as bar strings, which are words with explicit name binding letters.\nBar automata have pleasant algorithmic properties. We develop a framework in\nwhich every learning algorithm for standard deterministic or nondeterministic\nfinite automata over finite alphabets can be used to learn bar automata, with a\nquery complexity determined by that of the chosen learner. The technical key to\nour approach is the algorithmic handling of $\\alpha$-equivalence of bar\nstrings, which allows to bridge the gap between finite and infinite alphabets.\nThe principles underlying our framework are generic and also apply to bar\nB\\\"uchi automata and bar tree automata, leading to the first active learning\nmethods for data languages of infinite words and finite trees.",
        "Regular transductions over finite words have linear input-to-output growth.\nThis class of transductions enjoys many characterizations. Recently, regular\ntransductions have been extended by Boja\\'nczyk to polyregular transductions,\nwhich have polynomial growth, and are characterized by pebble transducers and\nMSO interpretations. Another class of interest is that of transductions defined\nby streaming string transducers or marble transducers, which have exponential\ngrowth and are incomparable with polyregular transductions.\n  In this paper, we consider MSO set interpretations (MSOSI) over finite words\nwhich were introduced by Colcombet and Loeding. MSOSI are a natural candidate\nfor the class of \"regular transductions with exponential growth\", and are\nrather well-behaved. However MSOSI lack, for now, two desirable properties that\nregular and polyregular transductions have. The first property is being\ndescribed by an automaton model, which is closely related to the second\nproperty of regularity preserving meaning preserving regular languages under\ninverse image. We first show that if MSOSI are (effectively) regularity\npreserving then any automatic $\\omega$-word has a decidable MSO theory, an\nalmost 20 years old conjecture of B\\'ar\\'any.\n  Our main contribution is the introduction of a class of transductions of\nexponential growth, which we call lexicographic transductions. We provide three\ndifferent presentations for this class: 1) as the closure of simple\ntransductions (recognizable transductions) under a single operator called\nmaplex; 2) as a syntactic fragment of MSOSI (but the regular languages are\ngiven by automata instead of formulas); 3) we give an automaton based model\ncalled nested marble transducers, which generalize both marble transducers and\npebble transducers. We show that this class enjoys many nice properties\nincluding being regularity preserving.",
        "We investigate complexity of the uniform membership problem for hyperedge\nreplacement grammars in comparison with other mildly context-sensitive grammar\nformalisms. It turns out that the complexity of the problem considered depends\nheavily on how one defines a hypergraph. There are two commonly used\ndefinitions in the field which differ in whether repetitions of attachment\nnodes of a hyperedge are allowed in a hypergraph or not. We show that, if\nrepetitions are allowed, then the problem under consideration is\nEXPTIME-complete even for string-generating hyperedge replacement grammars\nwhile it is NP-complete if repetitions are disallowed. We also prove that\nchecking whether a hyperedge replacement grammar is string-generating is\nEXPTIME-complete.",
        "We show that the axis bundle of a nongeometric fully irreducible outer\nautomorphism admits a canonical \"cubist\" decomposition into branched cubes that\nfit together with special combinatorics. From this structure, we locate a\ncanonical finite collection of periodic fold lines in each axis bundle. This\ngives a solution to the conjugacy problem in $\\mathrm{Out}(F_r)$ for fully\nirreducible outer automorphisms. This can be considered as an analogue of\nresults of Hamenst\\\"adt and Agol from the surface setting, which state that the\nset of trivalent train tracks carrying the unstable lamination of a\npseudo-Anosov map can be given the structure of a CAT(0) cube complex, and that\nthere is a canonical periodic fold line in this cube complex.",
        "In this paper, a regression-based machine learning model is used for the\ndesign of cavity backed slotted antenna. This type of antenna is commonly used\nin military and aviation communication systems. Initial reflection coefficient\ndata of cavity backed slotted antenna is generated using electromagnetic\nsolver. These reflection coefficient data is then used as input for training\nregression-based machine learning model. The model is trained to predict the\ndimensions of cavity backed slotted antenna based on the input reflection\ncoefficient for a wide frequency band varying from 1 GHz to 8 GHz. This\napproach allows for rapid prediction of optimal antenna configurations,\nreducing the need for repeated physical testing and manual adjustments, may\nlead to significant amount of design and development cost saving. The proposed\nmodel also demonstrates its versatility in predicting multi frequency resonance\nacross 1 GHz to 8 GHz. Also, the proposed approach demonstrates the potential\nfor leveraging machine learning in advanced antenna design, enhancing\nefficiency and accuracy in practical applications such as radar, military\nidentification systems and secure communication networks.",
        "In this article, the time-discretization of the fluid structure interaction\nmodel in the three-dimensional boundary domain is taken into account, which\nexplains the mechanical interaction between the blood flow and the Hookean\nelasticity. The interface between the two phases is given by a soft transition\nlayer and spreads to the finite thickness. On the implicit Euler scheme for\nthis discretization, We derive a variety of priori estimates and then use the\nFaedo-Galerkin method to prove the local well-poseedness results.",
        "In the post-Moore era, the need for efficient solutions to non-deterministic\npolynomial-time (NP) problems is becoming more pressing. In this context, the\nIsing model implemented by the probabilistic computing systems with\nprobabilistic bits (p-bits) has attracted attention due to the widespread\navailability of p-bits and support for large-scale simulations. This study\nmarks the first work to apply probabilistic computing to tackle protein\nfolding, a significant NP-complete problem challenge in biology. We represent\nproteins as sequences of hydrophobic (H) and polar (P) beads within a\nthree-dimensional (3-D) grid and introduce a novel many-body interaction-based\nencoding method to map the problem onto an Ising model. Our simulations show\nthat this approach significantly simplifies the energy landscape for short\npeptide sequences of six amino acids, halving the number of energy levels.\nFurthermore, the proposed mapping method achieves approximately 100 times\nacceleration for sequences consisting of ten amino acids in identifying the\ncorrect folding configuration. We predicted the optimal folding configuration\nfor a peptide sequence of 36 amino acids by identifying the ground state. These\nfindings highlight the unique potential of the proposed encoding method for\nsolving protein folding and, importantly, provide new tools for solving similar\nNP-complete problems in biology by probabilistic computing approach.",
        "Text datasets can be represented using models that do not preserve text\nstructure, or using models that preserve text structure. Our hypothesis is that\ndepending on the dataset nature, there can be advantages using a model that\npreserves text structure over one that does not, and viceversa. The key is to\ndetermine the best way of representing a particular dataset, based on the\ndataset itself. In this work, we propose to investigate this problem by\ncombining text distortion and algorithmic clustering based on string\ncompression. Specifically, a distortion technique previously developed by the\nauthors is applied to destroy text structure progressively. Following this, a\nclustering algorithm based on string compression is used to analyze the effects\nof the distortion on the information contained in the texts. Several\nexperiments are carried out on text datasets and artificially-generated\ndatasets. The results show that in strongly structural datasets the clustering\nresults worsen as text structure is progressively destroyed. Besides, they show\nthat using a compressor which enables the choice of the size of the\nleft-context symbols helps to determine the nature of the datasets. Finally,\nthe results are contrasted with a method based on multidimensional projections\nand analogous conclusions are obtained.",
        "Accurate driver behavior modeling is essential for improving the interaction\nand cooperation of the human driver with the driver assistance system. This\npaper presents a novel approach for modeling the response of human drivers to\nvisual cues provided by a speed advisory system using a Koopman-based method\nwith online updates. The proposed method utilizes the Koopman operator to\ntransform the nonlinear dynamics of driver-speed advisory system interactions\ninto a linear framework, allowing for efficient real-time prediction. An online\nupdate mechanism based on Recursive Least Squares (RLS) is integrated into the\nKoopman-based model to ensure continuous adaptation to changes in driver\nbehavior over time. The model is validated using data collected from a\nhuman-in-the-loop driving simulator, capturing diverse driver-specific\ntrajectories. The results demonstrate that the offline learned Koopman-based\nmodel can closely predict driver behavior and its accuracy is further enhanced\nthrough an online update mechanism with the RLS method.",
        "This study investigates the dispersion of magnetohydrodynamic waves\ninfluenced by thermal misbalance in a cylindrical configuration with a finite\naxial magnetic field within solar coronal plasmas. Specifically, it examines\nhow thermal misbalance, characterized by two distinct timescales directly\nlinked to the cooling and heating functions, influences the dispersion\nrelation. This investigation is a key approach for understanding non-adiabatic\neffects on the behaviour of these waves. Our findings reveal that the effect of\nthermal misbalance on fast sausage and kink modes, consistent with previous\nstudies on slabs, is small but slightly more pronounced than previously\nthought. The impact is smaller at long-wavelength limits but increases at\nshorter wavelengths, leading to higher damping rates. This minor effect on fast\nmodes occurs despite the complex interaction of thermal misbalance terms within\nthe dispersion relation, even at low-frequency limits defined by the\ncharacteristic timescales. Additionally, a very small amplification is\nobserved, indicating a suppressed damping state for the long-wavelength\nfundamental fast kink mode. In contrast, slow magnetoacoustic modes are\nsignificantly affected by thermal misbalance, with the cusp frequency shifting\nslightly to lower values, which is significant for smaller longitudinal\nwavenumbers. This thermal misbalance likely accounts for the substantial\nattenuation observed in the propagation of slow magnetoacoustic waves within\nthe solar atmosphere. The long-wavelength limit leads to an analytical\nexpression that accurately describes the frequency shifts in slow modes due to\nmisbalance, closely aligning with both numerical and observational results.",
        "We investigate the steady inviscid compressible self-similar flows which\ndepends only on the polar angle in spherical coordinates. It is shown that\nbesides the purely supersonic and subsonic self-similar flows, there exists\npurely sonic flows, Beltrami flows with a nonconstant proportionnality factor\nand smooth transonic self-similar flows with large vorticity. For a constant\nsupersonic incoming flow past an infinitely long circular cone, a conic shock\nattached to the tip of the cone will form, provided the opening angle of the\ncone is less than a critical value. We introduce the shock polar for the radial\nand polar components of the velocity and show that there exists a monotonicity\nrelation between the shock angle and the radial velocity, which seems to be new\nand not been observed before. If a supersonic incoming flow is self-similar\nwith nonzero azimuthal velocity, a conic shock also form attached to the tip of\nthe cone. The state at the downstream may change smoothly from supersonic to\nsubsonic, thus the shock can be supersonic-supersonic, supersonic-subsonic and\neven supersonic-sonic where the shock front and the sonic front coincide. We\nfurther investigate the structural stability of smooth self-similar\nirrotational transonic flows and analyze the corresponding linear mixed type\nsecond order equation of Tricomi type. By exploring some key properties of the\nself-similar solutions, we find a multiplier and identify a class of admissible\nboundary conditions for the linearized mixed type second-order equation. We\nalso prove the existence and uniqueness of a class of smooth transonic flows\nwith nonzero vorticity which depends only on the polar and azimuthal angles in\nspherical coordinates.",
        "This paper extends the tactical asset allocation literature by incorporating\nregime modeling using techniques from machine learning. We propose a novel\nmodel that classifies current regimes, forecasts the distribution of future\nregimes, and integrates these forecasts with the historical performance of\nindividual assets to optimize portfolio allocations. Utilizing a macroeconomic\ndata set from the FRED-MD database, our approach employs a modified k-means\nalgorithm to ensure consistent regime classification over time. We then\nleverage these regime predictions to estimate expected returns and\nvolatilities, which are subsequently mapped into portfolio allocations using\nvarious sizing schemes. Our method outperforms traditional benchmarks such as\nequal-weight, buy-and-hold, and random regime models. Additionally, we are the\nfirst to apply a regime detection model from a large macroeconomic dataset to\ntactical asset allocation, demonstrating significant improvements in portfolio\nperformance. Our work presents several key contributions, including a novel\ndata-driven regime detection algorithm tailored for uncertainty in forecasted\nregimes and applying the FRED-MD data set for tactical asset allocation.",
        "A novel formulation of the clustering problem is introduced in which the task\nis expressed as an estimation problem, where the object to be estimated is a\nfunction which maps a point to its distribution of cluster membership. Unlike\nexisting approaches which implicitly estimate such a function, like Gaussian\nMixture Models (GMMs), the proposed approach bypasses any explicit modelling\nassumptions and exploits the flexible estimation potential of nonparametric\nsmoothing. An intuitive approach for selecting the tuning parameters governing\nestimation is provided, which allows the proposed method to automatically\ndetermine both an appropriate level of flexibility and also the number of\nclusters to extract from a given data set. Experiments on a large collection of\npublicly available data sets are used to document the strong performance of the\nproposed approach, in comparison with relevant benchmarks from the literature.\nR code to implement the proposed approach is available from\nhttps:\/\/github.com\/DavidHofmeyr\/ CNS",
        "The present paper reanalyzes the problem of the refractive properties of the\nphysical vacuum and their modification under the action of the gravitational\nfield and the electromagnetic field. This problem was studied in our previous\nworks and in the subsequent works of the researchers: Leuchs, Urban, Mainland\nand their collaborators. By modeling the physical vacuum as a\nparticle-antiparticle system, we can deduce with a certain approximation, in a\nsemiclassical theory, the properties of the free vacuum and the vacuum modified\nby the interaction with a gravitational field and an electromagnetic field.\nMore precise calculation of permittivities of free vacuum and near a particle\ncan lead to a non-point model of the particle. This modeling can follow both\nthe quantum and the general relativistic path as well as the phenomenological\npath, the results complementing each other.",
        "Autonomous systems are increasingly expected to operate in the presence of\nadversaries, though an adversary may infer sensitive information simply by\nobserving a system, without even needing to interact with it. Therefore, in\nthis work we present a deceptive decision-making framework that not only\nconceals sensitive information, but in fact actively misleads adversaries about\nit. We model autonomous systems as Markov decision processes, and we consider\nadversaries that attempt to infer their reward functions using inverse\nreinforcement learning. To counter such efforts, we present two regularization\nstrategies for policy synthesis problems that actively deceive an adversary\nabout a system's underlying rewards. The first form of deception is\n``diversionary'', and it leads an adversary to draw any false conclusion about\nwhat the system's reward function is. The second form of deception is\n``targeted'', and it leads an adversary to draw a specific false conclusion\nabout what the system's reward function is. We then show how each form of\ndeception can be implemented in policy optimization problems, and we\nanalytically bound the loss in total accumulated reward that is induced by\ndeception. Next, we evaluate these developments in a multi-agent sequential\ndecision-making problem with one real agent and multiple decoys. We show that\ndiversionary deception can cause the adversary to believe that the most\nimportant agent is the least important, while attaining a total accumulated\nreward that is $98.83\\%$ of its optimal, non-deceptive value. Similarly, we\nshow that targeted deception can make any decoy appear to be the most important\nagent, while still attaining a total accumulated reward that is $99.25\\%$ of\nits optimal, non-deceptive value.",
        "Achieving optimality in controlling physical systems is a profound challenge\nacross diverse scientific and engineering fields, spanning neuromechanics,\nbiochemistry, autonomous systems, economics, and beyond. Traditional solutions,\nrelying on time-consuming offline iterative algorithms, often yield limited\ninsights into fundamental natural processes. In this work, we introduce a\nnovel, causally deterministic approach, presenting the closed-form optimal\ntracking controller (OTC) that inherently solves pseudoconvex optimization\nproblems in various fields. Through rigorous analysis and comprehensive\nnumerical examples, we demonstrate OTC's capability of achieving both high\naccuracy and rapid response, even when facing high-dimensional and\nhigh-dynamical real-world problems. Notably, our OTC outperforms\nstate-of-the-art methods by, e.g., solving a 1304-dimensional neuromechanics\nproblem 1311 times faster or with 113 times higher accuracy. Most importantly,\nOTC embodies a causally deterministic system interpretation of optimality\nprinciples, providing a new and fundamental perspective of optimization in\nnatural and artificial processes. We anticipate our work to be an important\nstep towards establishing a general causally deterministic optimization theory\nfor a broader spectrum of system and problem classes, promising advances in\nunderstanding optimality principles in complex systems.",
        "We investigate axion emission from singlet proton Cooper pairs in neutron\nstars, a process that dominates axion emission in young neutron stars in the\nKSVZ model. By re-deriving its emissivity, we confirm consistency with most\nexisting literature, except for a recent study that exhibits a different\ndependence on the effective mass. This discrepancy results in more than an\norder-of-magnitude deviation in emissivity, significantly impacting constraints\non the KSVZ axion from the cooling observations of the Cassiopeia A neutron\nstar. Furthermore, we examine uncertainties arising from neutron-star equations\nof state and their role in the discrepancy, finding that the large deviation\npersists regardless of the choice of equations of state.",
        "As generalizations of skew left braces, weak left braces were introduced\nrecently by Catino, Mazzotta, Miccoli and Stefanelli to study ceratin special\ndegenerate set-theoretical solutions of the Yang-Baxter equation. In this note,\nas analogues of the notions of regular subgroups of holomorph of groups, Gamma\nfunctions on groups and affine and semi-affine structures on groups, we propose\nthe notions of good inverse subsemigroups and Gamma functions associated to\nClifford semigroups and affine structures on inverse semigroups, respectively,\nby which weak left braces are characterized. Moreover, symmetric,\n$\\lambda$-homomorphic and $\\lambda$-anti-homomorphic weak left braces are\nintroduced and the algebraic structures of these weak left braces are given."
      ]
    }
  },
  {
    "id":2411.07453,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Review of Research on Condition Assessment of Nuclear Power Plant Equipment Based on Data-Driven",
    "start_abstract":"The condition assessment of the entire life cycle of nuclear power equipment has a significant impact on improving the safety and economy of nuclear power plants. In the past, operation and maintenance of systems, equipment, and structures of domestic nuclear power plants, mostly relied on the alarm mechanism of equipments, the simple threshold judgments of parameters, or the empirical judgments of engineers. With the implementation of online monitoring system in nuclear power plants, a large number of equipment operation data have been accumulated, and the use of data-driven technology to assess the health of equipment has become the focus of attention in the industry. In this paper, the current situation of the online monitoring system of nuclear power equipment was introduced and the common malfunction of nuclear power equipment was analyzed. The condition assessment of nuclear power equipment were categorized into three major problems (i.e., anomaly detection, life prediction, and fault diagnosis), the situation of research and application were summarized respectively, and the application potential of deep learning technology in this field was emphasized. Based on this, the challenges and possible solutions to the condition assessment of nuclear power plant equipment were further analyzed.",
    "start_categories":[
      "nucl-th"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"
      ],
      "abstract":[
        "Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth\/width\/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. \nTo go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.4% top-1 \/ 97.1% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at this https URL."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "TRKM: Twin Restricted Kernel Machines for Classification and Regression",
        "Categorical Schr\\\"odinger Bridge Matching",
        "Picard-KKT-hPINN: Enforcing Nonlinear Enthalpy Balances for Physically\n  Consistent Neural Networks",
        "MUSS: Multilevel Subset Selection for Relevance and Diversity",
        "ElasticZO: A Memory-Efficient On-Device Learning with Combined Zeroth-\n  and First-Order Optimization",
        "Temperature-Annealed Boltzmann Generators",
        "The Tabular Foundation Model TabPFN Outperforms Specialized Time Series\n  Forecasting Models Based on Simple Features",
        "D3: Diversity, Difficulty, and Dependability-Aware Data Selection for\n  Sample-Efficient LLM Instruction Tuning",
        "Value-Based Deep RL Scales Predictably",
        "FedEM: A Privacy-Preserving Framework for Concurrent Utility\n  Preservation in Federated Learning",
        "Technical Report: Aggregation on Learnable Manifolds for Asynchronous\n  Federated Optimization",
        "Network-Wide Traffic Flow Estimation Across Multiple Cities with Global\n  Open Multi-Source Data: A Large-Scale Case Study in Europe and North America",
        "Scalable Trajectory-User Linking with Dual-Stream Representation\n  Networks",
        "Reward Models Identify Consistency, Not Causality",
        "On the Data-Driven Modeling of Price-Responsive Flexible Loads:\n  Formulation and Algorithm",
        "Using Large Language Models for Solving Thermodynamic Problems",
        "Advancing Precision Oncology Through Modeling of Longitudinal and\n  Multimodal Data",
        "Spherical Dense Text-to-Image Synthesis",
        "Expressive Music Data Processing and Generation",
        "Show Me Your Code! Kill Code Poisoning: A Lightweight Method Based on\n  Code Naturalness",
        "Language-agnostic, automated assessment of listeners' speech recall\n  using large language models",
        "Beyond Pairwise: Global Zero-shot Temporal Graph Generation",
        "Temporal Context Awareness: A Defense Framework Against Multi-turn\n  Manipulation Attacks on Large Language Models",
        "Dimension of diagonal self-affine measures with exponentially separated\n  projections",
        "Global linearization without hyperbolicity",
        "TRCE: Towards Reliable Malicious Concept Erasure in Text-to-Image\n  Diffusion Models",
        "Design of Bayesian Clinical Trials with Clustered Data and Multiple\n  Endpoints",
        "Asynchronous Cooperative Multi-Agent Reinforcement Learning with Limited\n  Communication"
      ],
      "abstract":[
        "Restricted kernel machines (RKMs) have considerably improved generalization\nin machine learning. Recent advancements explored various techniques within the\nRKM framework, integrating kernel functions with least squares support vector\nmachines (LSSVM) to mirror the energy function of restricted Boltzmann machines\n(RBM), leading to enhanced performance. However, RKMs may face challenges in\ngeneralization when dealing with unevenly distributed or complexly clustered\ndata. Additionally, as the dataset size increases, the computational burden of\nmanaging high-dimensional feature spaces can become substantial, potentially\nhindering performance in large-scale datasets. To address these challenges, we\npropose twin restricted kernel machine (TRKM). TRKM combines the benefits of\ntwin models with the robustness of the RKM framework to enhance classification\nand regression tasks. By leveraging the Fenchel-Young inequality, we introduce\na novel conjugate feature duality, allowing the formulation of classification\nand regression problems in terms of dual variables. This duality provides an\nupper bound to the objective function of the TRKM problem, resulting in a new\nmethodology under the RKM framework. The model uses an energy function similar\nto that of RBM, incorporating both visible and hidden variables corresponding\nto both classes. Additionally, the kernel trick is employed to map data into a\nhigh-dimensional feature space, where the model identifies an optimal\nseparating hyperplane using a regularized least squares approach. Experiments\non UCI and KEEL datasets confirm TRKM's superiority over baselines, showcasing\nits robustness and efficiency in handling complex data. Furthermore, We\nimplemented the TRKM model on the brain age dataset, demonstrating its efficacy\nin predicting brain age.",
        "The Schr\\\"odinger Bridge (SB) is a powerful framework for solving generative\nmodeling tasks such as unpaired domain translation. Most SB-related research\nfocuses on continuous data space $\\mathbb{R}^{D}$ and leaves open theoretical\nand algorithmic questions about applying SB methods to discrete data, e.g, on\nfinite spaces $\\mathbb{S}^{D}$. Notable examples of such sets $\\mathbb{S}$ are\ncodebooks of vector-quantized (VQ) representations of modern autoencoders,\ntokens in texts, categories of atoms in molecules, etc. In this paper, we\nprovide a theoretical and algorithmic foundation for solving SB in discrete\nspaces using the recently introduced Iterative Markovian Fitting (IMF)\nprocedure. Specifically, we theoretically justify the convergence of\ndiscrete-time IMF (D-IMF) to SB in discrete spaces. This enables us to develop\na practical computational algorithm for SB which we call Categorical\nSchr\\\"odinger Bridge Matching (CSBM). We show the performance of CSBM via a\nseries of experiments with synthetic data and VQ representations of images.",
        "Neural networks are widely used as surrogate models but they do not guarantee\nphysically consistent predictions thereby preventing adoption in various\napplications. We propose a method that can enforce NNs to satisfy physical laws\nthat are nonlinear in nature such as enthalpy balances. Our approach, inspired\nby Picard successive approximations method, aims to enforce multiplicatively\nseparable constraints by sequentially freezing and projecting a set of the\nparticipating variables. We demonstrate our PicardKKThPINN for surrogate\nmodeling of a catalytic packed bed reactor for methanol synthesis. Our results\nshow that the method efficiently enforces nonlinear enthalpy and linear atomic\nbalances at machine-level precision. Additionally, we show that enforcing\nconservation laws can improve accuracy in data-scarce conditions compared to\nvanilla multilayer perceptron.",
        "The problem of relevant and diverse subset selection has a wide range of\napplications, including recommender systems and retrieval-augmented generation\n(RAG). For example, in recommender systems, one is interested in selecting\nrelevant items, while providing a diversified recommendation. Constrained\nsubset selection problem is NP-hard, and popular approaches such as Maximum\nMarginal Relevance (MMR) are based on greedy selection. Many real-world\napplications involve large data, but the original MMR work did not consider\ndistributed selection. This limitation was later addressed by a method called\nDGDS which allows for a distributed setting using random data partitioning.\nHere, we exploit structure in the data to further improve both scalability and\nperformance on the target application. We propose MUSS, a novel method that\nuses a multilevel approach to relevant and diverse selection. We provide a\nrigorous theoretical analysis and show that our method achieves a constant\nfactor approximation of the optimal objective. In a recommender system\napplication, our method can achieve the same level of performance as baselines,\nbut 4.5 to 20 times faster. Our method is also capable of outperforming\nbaselines by up to 6 percent points of RAG-based question answering accuracy.",
        "Zeroth-order (ZO) optimization is being recognized as a simple yet powerful\nalternative to standard backpropagation (BP)-based training. Notably, ZO\noptimization allows for training with only forward passes and (almost) the same\nmemory as inference, making it well-suited for edge devices with limited\ncomputing and memory resources. In this paper, we propose ZO-based on-device\nlearning (ODL) methods for full-precision and 8-bit quantized deep neural\nnetworks (DNNs), namely ElasticZO and ElasticZO-INT8. ElasticZO lies in the\nmiddle between pure ZO- and pure BP-based approaches, and is based on the idea\nto employ BP for the last few layers and ZO for the remaining layers.\nElasticZO-INT8 achieves integer arithmetic-only ZO-based training for the first\ntime, by incorporating a novel method for computing quantized ZO gradients from\ninteger cross-entropy loss values. Experimental results on the classification\ndatasets show that ElasticZO effectively addresses the slow convergence of\nvanilla ZO and shrinks the accuracy gap to BP-based training. Compared to\nvanilla ZO, ElasticZO achieves 5.2-9.5% higher accuracy with only 0.072-1.7%\nmemory overhead, and can handle fine-tuning tasks as well as full training.\nElasticZO-INT8 further reduces the memory usage and training time by 1.46-1.60x\nand 1.38-1.42x without compromising the accuracy. These results demonstrate a\nbetter tradeoff between accuracy and training cost compared to pure ZO- and\nBP-based approaches, and also highlight the potential of ZO optimization in\non-device learning.",
        "Efficient sampling of unnormalized probability densities such as the\nBoltzmann distribution of molecular systems is a longstanding challenge. Next\nto conventional approaches like molecular dynamics or Markov chain Monte Carlo,\nvariational approaches, such as training normalizing flows with the reverse\nKullback-Leibler divergence, have been introduced. However, such methods are\nprone to mode collapse and often do not learn to sample the full\nconfigurational space. Here, we present temperature-annealed Boltzmann\ngenerators (TA-BG) to address this challenge. First, we demonstrate that\ntraining a normalizing flow with the reverse Kullback-Leibler divergence at\nhigh temperatures is possible without mode collapse. Furthermore, we introduce\na reweighting-based training objective to anneal the distribution to lower\ntarget temperatures. We apply this methodology to three molecular systems of\nincreasing complexity and, compared to the baseline, achieve better results in\nalmost all metrics while requiring up to three times fewer target energy\nevaluations. For the largest system, our approach is the only method that\naccurately resolves the metastable states of the system.",
        "Foundation models have become popular in forecasting due to their ability to\nmake accurate predictions, even with minimal fine-tuning on specific datasets.\nIn this paper, we demonstrate how the newly released regression variant of\nTabPFN, a general tabular foundation model, can be applied to time series\nforecasting. We propose a straightforward approach, TabPFN-TS, which pairs\nTabPFN with simple feature engineering to achieve strong forecasting\nperformance. Despite its simplicity and with only 11M parameters, TabPFN-TS\noutperforms Chronos-Mini, a model of similar size, and matches or even slightly\noutperforms Chronos-Large, which has 65-fold more parameters. A key strength of\nour method lies in its reliance solely on artificial data during pre-training,\navoiding the need for large training datasets and eliminating the risk of\nbenchmark contamination.",
        "Recent advancements in instruction tuning for large language models (LLMs)\nsuggest that a small, high-quality dataset can significantly equip LLMs with\ninstruction-following capabilities, outperforming large datasets often burdened\nby quality and redundancy issues. However, the challenge lies in automatically\nidentifying valuable subsets from large datasets to boost both the\neffectiveness and efficiency of instruction tuning. In this paper, we first\nestablish data selection criteria based on three distinct aspects of data\nvalue: diversity, difficulty, and dependability, and then propose the D3 method\ncomprising two key steps of scoring and selection. Specifically, in the scoring\nstep, we define the diversity function to measure sample distinctiveness and\nintroduce the uncertainty-based prediction difficulty to evaluate sample\ndifficulty by mitigating the interference of context-oriented generation\ndiversity. Additionally, we integrate an external LLM for dependability\nassessment. In the selection step, we formulate the D3 weighted coreset\nobjective, which jointly optimizes three aspects of data value to solve for the\nmost valuable subset. The two steps of D3 can iterate multiple rounds,\nincorporating feedback to refine the selection focus adaptively. Experiments on\nthree datasets demonstrate the effectiveness of D3 in endowing LLMs with\ncompetitive or even superior instruction-following capabilities using less than\n10% of the entire dataset.",
        "Scaling data and compute is critical to the success of machine learning.\nHowever, scaling demands predictability: we want methods to not only perform\nwell with more compute or data, but also have their performance be predictable\nfrom small-scale runs, without running the large-scale experiment. In this\npaper, we show that value-based off-policy RL methods are predictable despite\ncommunity lore regarding their pathological behavior. First, we show that data\nand compute requirements to attain a given performance level lie on a Pareto\nfrontier, controlled by the updates-to-data (UTD) ratio. By estimating this\nfrontier, we can predict this data requirement when given more compute, and\nthis compute requirement when given more data. Second, we determine the optimal\nallocation of a total resource budget across data and compute for a given\nperformance and use it to determine hyperparameters that maximize performance\nfor a given budget. Third, this scaling behavior is enabled by first estimating\npredictable relationships between hyperparameters, which is used to manage\neffects of overfitting and plasticity loss unique to RL. We validate our\napproach using three algorithms: SAC, BRO, and PQL on DeepMind Control, OpenAI\ngym, and IsaacGym, when extrapolating to higher levels of data, compute,\nbudget, or performance.",
        "Federated Learning (FL) enables collaborative training of models across\ndistributed clients without sharing local data, addressing privacy concerns in\ndecentralized systems. However, the gradient-sharing process exposes private\ndata to potential leakage, compromising FL's privacy guarantees in real-world\napplications. To address this issue, we propose Federated Error Minimization\n(FedEM), a novel algorithm that incorporates controlled perturbations through\nadaptive noise injection. This mechanism effectively mitigates gradient leakage\nattacks while maintaining model performance. Experimental results on benchmark\ndatasets demonstrate that FedEM significantly reduces privacy risks and\npreserves model accuracy, achieving a robust balance between privacy protection\nand utility preservation.",
        "In Federated Learning (FL), a primary challenge to the server-side\naggregation of client models is device heterogeneity in both loss landscape\ngeometry and computational capacity. This issue can be particularly pronounced\nin clinical contexts where variations in data distribution (aggravated by class\nimbalance), infrastructure requirements, and sample sizes are common. We\npropose AsyncManifold, a novel asynchronous FL framework to address these\nissues by taking advantage of underlying solution space geometry at each of the\nlocal training, delay-correction, and aggregation stages. Our proposal is\naccompanied by a convergence proof in a general form and, motivated through\nexploratory studies of local behaviour, a proof-of-concept algorithm which\nperforms aggregation along non-linear mode connections and hence avoids\nbarriers to convergence that techniques based on linear interpolation will\nencounter.",
        "Network-wide traffic flow, which captures dynamic traffic volume on each link\nof a general network, is fundamental to smart mobility applications. However,\nthe observed traffic flow from sensors is usually limited across the entire\nnetwork due to the associated high installation and maintenance costs. To\naddress this issue, existing research uses various supplementary data sources\nto compensate for insufficient sensor coverage and estimate the unobserved\ntraffic flow. Although these studies have shown promising results, the\ninconsistent availability and quality of supplementary data across cities make\ntheir methods typically face a trade-off challenge between accuracy and\ngenerality. In this research, we first time advocate using the Global Open\nMulti-Source (GOMS) data within an advanced deep learning framework to break\nthe trade-off. The GOMS data primarily encompass geographical and demographic\ninformation, including road topology, building footprints, and population\ndensity, which can be consistently collected across cities. More importantly,\nthese GOMS data are either causes or consequences of transportation activities,\nthereby creating opportunities for accurate network-wide flow estimation.\nFurthermore, we use map images to represent GOMS data, instead of traditional\ntabular formats, to capture richer and more comprehensive geographical and\ndemographic information. To address multi-source data fusion, we develop an\nattention-based graph neural network that effectively extracts and synthesizes\ninformation from GOMS maps while simultaneously capturing spatiotemporal\ntraffic dynamics from observed traffic data. A large-scale case study across 15\ncities in Europe and North America was conducted. The results demonstrate\nstable and satisfactory estimation accuracy across these cities, which suggests\nthat the trade-off challenge can be successfully addressed using our approach.",
        "Trajectory-user linking (TUL) aims to match anonymous trajectories to the\nmost likely users who generated them, offering benefits for a wide range of\nreal-world spatio-temporal applications. However, existing TUL methods are\nlimited by high model complexity and poor learning of the effective\nrepresentations of trajectories, rendering them ineffective in handling\nlarge-scale user trajectory data. In this work, we propose a novel\n$\\underline{Scal}$abl$\\underline{e}$ Trajectory-User Linking with dual-stream\nrepresentation networks for large-scale $\\underline{TUL}$ problem, named\nScaleTUL. Specifically, ScaleTUL generates two views using temporal and spatial\naugmentations to exploit supervised contrastive learning framework to\neffectively capture the irregularities of trajectories. In each view, a\ndual-stream trajectory encoder, consisting of a long-term encoder and a\nshort-term encoder, is designed to learn unified trajectory representations\nthat fuse different temporal-spatial dependencies. Then, a TUL layer is used to\nassociate the trajectories with the corresponding users in the representation\nspace using a two-stage training model. Experimental results on check-in\nmobility datasets from three real-world cities and the nationwide U.S.\ndemonstrate the superiority of ScaleTUL over state-of-the-art baselines for\nlarge-scale TUL tasks.",
        "Reward models (RMs) play a crucial role in aligning large language models\n(LLMs) with human preferences and enhancing reasoning quality. Traditionally,\nRMs are trained to rank candidate outputs based on their correctness and\ncoherence. However, in this work, we present several surprising findings that\nchallenge common assumptions about RM behavior. Our analysis reveals that\nstate-of-the-art reward models prioritize structural consistency over causal\ncorrectness. Specifically, removing the problem statement has minimal impact on\nreward scores, whereas altering numerical values or disrupting the reasoning\nflow significantly affects RM outputs. Furthermore, RMs exhibit a strong\ndependence on complete reasoning trajectories truncated or incomplete steps\nlead to significant variations in reward assignments, indicating that RMs\nprimarily rely on learned reasoning patterns rather than explicit problem\ncomprehension. These findings hold across multiple architectures, datasets, and\ntasks, leading to three key insights: (1) RMs primarily assess coherence rather\nthan true reasoning quality; (2) The role of explicit problem comprehension in\nreward assignment is overstated; (3) Current RMs may be more effective at\nranking responses than verifying logical validity. Our results suggest a\nfundamental limitation in existing reward modeling approaches, emphasizing the\nneed for a shift toward causality-aware reward models that go beyond\nconsistency-driven evaluation.",
        "The flexible loads in power systems, such as interruptible and transferable\nloads, are critical flexibility resources for mitigating power imbalances.\nDespite their potential, accurate modeling of these loads is a challenging work\nand has not received enough attention, limiting their integration into\noperational frameworks. To bridge this gap, this paper develops a data-driven\nidentification theory and algorithm for price-responsive flexible loads\n(PRFLs). First, we introduce PRFL models that capture both static and dynamic\ndecision mechanisms governing their response to electricity price variations.\nSecond, We develop a data-driven identification framework that explicitly\nincorporates forecast and measurement errors. Particularly, we give a\ntheoretical analysis to quantify the statistical impact of such noise on\nparameter estimation. Third, leveraging the bilevel structure of the\nidentification problem, we propose a Bayesian optimization-based algorithm that\nfeatures the scalability to large sample sizes and the ability to offer\nposterior differentiability certificates as byproducts. Numerical tests\ndemonstrate the effectiveness and superiority of the proposed approach.",
        "Large Language Models (LLMs) have made significant progress in reasoning,\ndemonstrating their capability to generate human-like responses. This study\nanalyzes the problem-solving capabilities of LLMs in the domain of\nthermodynamics. A benchmark of 22 thermodynamic problems to evaluate LLMs is\npresented that contains both simple and advanced problems. Five different LLMs\nare assessed: GPT-3.5, GPT-4, and GPT-4o from OpenAI, Llama 3.1 from Meta, and\nle Chat from MistralAI. The answers of these LLMs were evaluated by trained\nhuman experts, following a methodology akin to the grading of academic exam\nresponses. The scores and the consistency of the answers are discussed,\ntogether with the analytical skills of the LLMs. Both strengths and weaknesses\nof the LLMs become evident. They generally yield good results for the simple\nproblems, but also limitations become clear: The LLMs do not provide consistent\nresults, they often fail to fully comprehend the context and make wrong\nassumptions. Given the complexity and domain-specific nature of the problems,\nthe statistical language modeling approach of the LLMs struggles with the\naccurate interpretation and the required reasoning. The present results\nhighlight the need for more systematic integration of thermodynamic knowledge\nwith LLMs, for example, by using knowledge-based methods.",
        "Cancer evolves continuously over time through a complex interplay of genetic,\nepigenetic, microenvironmental, and phenotypic changes. This dynamic behavior\ndrives uncontrolled cell growth, metastasis, immune evasion, and therapy\nresistance, posing challenges for effective monitoring and treatment. However,\ntoday's data-driven research in oncology has primarily focused on\ncross-sectional analysis using data from a single modality, limiting the\nability to fully characterize and interpret the disease's dynamic\nheterogeneity. Advances in multiscale data collection and computational methods\nnow enable the discovery of longitudinal multimodal biomarkers for precision\noncology. Longitudinal data reveal patterns of disease progression and\ntreatment response that are not evident from single-timepoint data, enabling\ntimely abnormality detection and dynamic treatment adaptation. Multimodal data\nintegration offers complementary information from diverse sources for more\nprecise risk assessment and targeting of cancer therapy. In this review, we\nsurvey methods of longitudinal and multimodal modeling, highlighting their\nsynergy in providing multifaceted insights for personalized care tailored to\nthe unique characteristics of a patient's cancer. We summarize the current\nchallenges and future directions of longitudinal multimodal analysis in\nadvancing precision oncology.",
        "Recent advancements in text-to-image (T2I) have improved synthesis results,\nbut challenges remain in layout control and generating omnidirectional\npanoramic images. Dense T2I (DT2I) and spherical T2I (ST2I) models address\nthese issues, but so far no unified approach exists. Trivial approaches, like\nprompting a DT2I model to generate panoramas can not generate proper spherical\ndistortions and seamless transitions at the borders. Our work shows that\nspherical dense text-to-image (SDT2I) can be achieved by integrating\ntraining-free DT2I approaches into finetuned panorama models. Specifically, we\npropose MultiStitchDiffusion (MSTD) and MultiPanFusion (MPF) by integrating\nMultiDiffusion into StitchDiffusion and PanFusion, respectively. Since no\nbenchmark for SDT2I exists, we further construct Dense-Synthetic-View\n(DSynView), a new synthetic dataset containing spherical layouts to evaluate\nour models. Our results show that MSTD outperforms MPF across image quality as\nwell as prompt- and layout adherence. MultiPanFusion generates more diverse\nimages but struggles to synthesize flawless foreground objects. We propose\nbootstrap-coupling and turning off equirectangular perspective-projection\nattention in the foreground as an improvement of MPF. Link to code\nhttps:\/\/github.com\/sdt2i\/spherical-dense-text-to-image",
        "Musical expressivity and coherence are indispensable in music composition and\nperformance, while often neglected in modern AI generative models. In this\nwork, we introduce a listening-based data-processing technique that captures\nthe expressivity in musical performance. This technique derived from Weber's\nlaw reflects the human perceptual truth of listening and preserves musical\nsubtlety and expressivity in the training input. To facilitate musical\ncoherence, we model the output interdependencies among multiple arguments in\nthe music data such as pitch, duration, velocity, etc. in the neural networks\nbased on the probabilistic chain rule. In practice, we decompose the\nmulti-output sequential model into single-output submodels and condition\npreviously sampled outputs on the subsequent submodels to induce conditional\ndistributions. Finally, to select eligible sequences from all generations, a\ntentative measure based on the output entropy was proposed. The entropy\nsequence is set as a criterion to select predictable and stable generations,\nwhich is further studied under the context of informational aesthetic measures\nto quantify musical pleasure and information gain along the music tendency.",
        "Neural code models (NCMs) have demonstrated extraordinary capabilities in\ncode intelligence tasks. Meanwhile, the security of NCMs and NCMs-based systems\nhas garnered increasing attention. In particular, NCMs are often trained on\nlarge-scale data from potentially untrustworthy sources, providing attackers\nwith the opportunity to manipulate them by inserting crafted samples into the\ndata. This type of attack is called a code poisoning attack (also known as a\nbackdoor attack). It allows attackers to implant backdoors in NCMs and thus\ncontrol model behavior, which poses a significant security threat. However,\nthere is still a lack of effective techniques for detecting various complex\ncode poisoning attacks.\n  In this paper, we propose an innovative and lightweight technique for code\npoisoning detection named KillBadCode. KillBadCode is designed based on our\ninsight that code poisoning disrupts the naturalness of code. Specifically,\nKillBadCode first builds a code language model (CodeLM) on a lightweight\n$n$-gram language model. Then, given poisoned data, KillBadCode utilizes CodeLM\nto identify those tokens in (poisoned) code snippets that will make the code\nsnippets more natural after being deleted as trigger tokens. Considering that\nthe removal of some normal tokens in a single sample might also enhance code\nnaturalness, leading to a high false positive rate (FPR), we aggregate the\ncumulative improvement of each token across all samples. Finally, KillBadCode\npurifies the poisoned data by removing all poisoned samples containing the\nidentified trigger tokens. The experimental results on two code poisoning\nattacks and four code intelligence tasks demonstrate that KillBadCode\nsignificantly outperforms four baselines. More importantly, KillBadCode is very\nefficient, with a minimum time consumption of only 5 minutes, and is 25 times\nfaster than the best baseline on average.",
        "Speech-comprehension difficulties are common among older people. Standard\nspeech tests do not fully capture such difficulties because the tests poorly\nresemble the context-rich, story-like nature of ongoing conversation and are\ntypically available only in a country's dominant\/official language (e.g.,\nEnglish), leading to inaccurate scores for native speakers of other languages.\nAssessments for naturalistic, story speech in multiple languages require\naccurate, time-efficient scoring. The current research leverages modern large\nlanguage models (LLMs) in native English speakers and native speakers of 10\nother languages to automate the generation of high-quality, spoken stories and\nscoring of speech recall in different languages. Participants listened to and\nfreely recalled short stories (in quiet\/clear and in babble noise) in their\nnative language. LLM text-embeddings and LLM prompt engineering with semantic\nsimilarity analyses to score speech recall revealed sensitivity to known\neffects of temporal order, primacy\/recency, and background noise, and high\nsimilarity of recall scores across languages. The work overcomes limitations\nassociated with simple speech materials and testing of closed native-speaker\ngroups because recall data of varying length and details can be mapped across\nlanguages with high accuracy. The full automation of speech generation and\nrecall scoring provides an important step towards comprehension assessments of\nnaturalistic speech with clinical applicability.",
        "Temporal relation extraction (TRE) is a fundamental task in natural language\nprocessing (NLP) that involves identifying the temporal relationships between\nevents in a document. Despite the advances in large language models (LLMs),\ntheir application to TRE remains limited. Most existing approaches rely on\npairwise classification, in which event pairs are considered individually,\nleading to computational inefficiency and a lack of global consistency in the\nresulting temporal graph. In this work, we propose a novel zero-shot method for\nTRE that generates a document's complete temporal graph at once, then applies\ntransitive constraints optimization to refine predictions and enforce temporal\nconsistency across relations. Additionally, we introduce OmniTemp, a new\ndataset with complete annotations for all pairs of targeted events within a\ndocument. Through experiments and analyses, we demonstrate that our method\nsignificantly outperforms existing zero-shot approaches while achieving\ncompetitive performance with supervised models.",
        "Large Language Models (LLMs) are increasingly vulnerable to sophisticated\nmulti-turn manipulation attacks, where adversaries strategically build context\nthrough seemingly benign conversational turns to circumvent safety measures and\nelicit harmful or unauthorized responses. These attacks exploit the temporal\nnature of dialogue to evade single-turn detection methods, representing a\ncritical security vulnerability with significant implications for real-world\ndeployments.\n  This paper introduces the Temporal Context Awareness (TCA) framework, a novel\ndefense mechanism designed to address this challenge by continuously analyzing\nsemantic drift, cross-turn intention consistency and evolving conversational\npatterns. The TCA framework integrates dynamic context embedding analysis,\ncross-turn consistency verification, and progressive risk scoring to detect and\nmitigate manipulation attempts effectively. Preliminary evaluations on\nsimulated adversarial scenarios demonstrate the framework's potential to\nidentify subtle manipulation patterns often missed by traditional detection\ntechniques, offering a much-needed layer of security for conversational AI\nsystems. In addition to outlining the design of TCA , we analyze diverse attack\nvectors and their progression across multi-turn conversation, providing\nvaluable insights into adversarial tactics and their impact on LLM\nvulnerabilities. Our findings underscore the pressing need for robust,\ncontext-aware defenses in conversational AI systems and highlight TCA framework\nas a promising direction for securing LLMs while preserving their utility in\nlegitimate applications. We make our implementation available to support\nfurther research in this emerging area of AI security.",
        "Let $ \\mu $ be a self-affine measure associated with a diagonal affine\niterated function system (IFS) $ \\Phi = \\{ (x_{1}, \\ldots, x_{d}) \\mapsto (\nr_{i, 1}x_{1} + t_{i,1}, \\ldots, r_{i,d}x_{d} + t_{i,d}) \\}_{i\\in\\Lambda} $ on\n$ \\mathbb{R}^{d} $ and a probability vector $ p = (p_{i})_{i\\in\\Lambda}$. For $\n1 \\leq j \\leq d $, denote the $ j $-th the Lyapunov exponent by $ \\chi_{j} :=\n\\sum_{i\\in\\Lambda} - p_{i} \\log | r_{i,j} |$, and define the IFS induced by $\n\\Phi $ on the $j$-th coordinate as $ \\Phi_{j} := \\{ x \\mapsto r_{i,j}x +\nt_{i,j}\\}_{i\\in\\Lambda}$. We prove that if $ \\chi_{j_{1}} \\neq \\chi_{j_{2}} $\nfor $ 1 \\leq j_{1} < j_{2} \\leq d $, and $ \\Phi_{j}$ is exponentially separated\nfor $ 1 \\leq j \\leq d $, then the dimension of $ \\mu $ is the minimum of $ d $\nand its Lyapunov dimension. This confirms a conjecture of Rapaport by removing\nthe additional assumption that the linear parts of the maps in $ \\Phi $ are\ncontained in a 1-dimensional subgroup. One of the main ingredients of the proof\ninvolves disintegrating $ \\mu $ into random measures with convolution\nstructure. In the course of the proof, we establish new results on dimension\nand entropy increase for these random measures.",
        "We give a proof of an extension of the Hartman-Grobman theorem to\nnonhyperbolic but asymptotically stable equilibria of vector fields. Moreover,\nthe linearizing topological conjugacy is (i) defined on the entire basin of\nattraction if the vector field is complete, and (ii) a $C^{k\\geq 1}$\ndiffeomorphism on the complement of the equilibrium if the vector field is\n$C^k$ and the underlying space is not $5$-dimensional. We also show that the\n$C^k$ statement in the $5$-dimensional case is equivalent to the\n$4$-dimensional smooth Poincar\\'{e} conjecture.",
        "Recent advances in text-to-image diffusion models enable photorealistic image\ngeneration, but they also risk producing malicious content, such as NSFW\nimages. To mitigate risk, concept erasure methods are studied to facilitate the\nmodel to unlearn specific concepts. However, current studies struggle to fully\nerase malicious concepts implicitly embedded in prompts (e.g., metaphorical\nexpressions or adversarial prompts) while preserving the model's normal\ngeneration capability. To address this challenge, our study proposes TRCE,\nusing a two-stage concept erasure strategy to achieve an effective trade-off\nbetween reliable erasure and knowledge preservation. Firstly, TRCE starts by\nerasing the malicious semantics implicitly embedded in textual prompts. By\nidentifying a critical mapping objective(i.e., the [EoT] embedding), we\noptimize the cross-attention layers to map malicious prompts to contextually\nsimilar prompts but with safe concepts. This step prevents the model from being\noverly influenced by malicious semantics during the denoising process.\nFollowing this, considering the deterministic properties of the sampling\ntrajectory of the diffusion model, TRCE further steers the early denoising\nprediction toward the safe direction and away from the unsafe one through\ncontrastive learning, thus further avoiding the generation of malicious\ncontent. Finally, we conduct comprehensive evaluations of TRCE on multiple\nmalicious concept erasure benchmarks, and the results demonstrate its\neffectiveness in erasing malicious concepts while better preserving the model's\noriginal generation ability. The code is available at:\nhttp:\/\/github.com\/ddgoodgood\/TRCE. CAUTION: This paper includes model-generated\ncontent that may contain offensive material.",
        "In the design of clinical trials, it is essential to assess the design\noperating characteristics (i.e., the probabilities of making correct\ndecisions). Common practice for the evaluation of operating characteristics in\nBayesian clinical trials relies on estimating the sampling distribution of\nposterior summaries via Monte Carlo simulation. It is computationally intensive\nto repeat this estimation process for each design configuration considered,\nparticularly for clustered data that are analyzed using complex,\nhigh-dimensional models. In this paper, we propose an efficient method to\nassess operating characteristics and determine sample sizes for Bayesian trials\nwith clustered data and multiple endpoints. We prove theoretical results that\nenable posterior probabilities to be modelled as a function of the sample size.\nUsing these functions, we assess operating characteristics at a range of sample\nsizes given simulations conducted at only two sample sizes. These theoretical\nresults are also leveraged to quantify the impact of simulation variability on\nour sample size recommendations. The applicability of our methodology is\nillustrated using a current clinical trial with clustered data.",
        "We consider the problem setting in which multiple autonomous agents must\ncooperatively navigate and perform tasks in an unknown,\ncommunication-constrained environment. Traditional multi-agent reinforcement\nlearning (MARL) approaches assume synchronous communications and perform poorly\nin such environments. We propose AsynCoMARL, an asynchronous MARL approach that\nuses graph transformers to learn communication protocols from dynamic graphs.\nAsynCoMARL can accommodate infrequent and asynchronous communications between\nagents, with edges of the graph only forming when agents communicate with each\nother. We show that AsynCoMARL achieves similar success and collision rates as\nleading baselines, despite 26\\% fewer messages being passed between agents."
      ]
    }
  },
  {
    "id":2411.07453,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
    "start_abstract":"Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth\/width\/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. \nTo go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.4% top-1 \/ 97.1% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at this https URL.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Review of Research on Condition Assessment of Nuclear Power Plant Equipment Based on Data-Driven"
      ],
      "abstract":[
        "The condition assessment of the entire life cycle of nuclear power equipment has a significant impact on improving the safety and economy of nuclear power plants. In the past, operation and maintenance of systems, equipment, and structures of domestic nuclear power plants, mostly relied on the alarm mechanism of equipments, the simple threshold judgments of parameters, or the empirical judgments of engineers. With the implementation of online monitoring system in nuclear power plants, a large number of equipment operation data have been accumulated, and the use of data-driven technology to assess the health of equipment has become the focus of attention in the industry. In this paper, the current situation of the online monitoring system of nuclear power equipment was introduced and the common malfunction of nuclear power equipment was analyzed. The condition assessment of nuclear power equipment were categorized into three major problems (i.e., anomaly detection, life prediction, and fault diagnosis), the situation of research and application were summarized respectively, and the application potential of deep learning technology in this field was emphasized. Based on this, the challenges and possible solutions to the condition assessment of nuclear power plant equipment were further analyzed."
      ],
      "categories":[
        "nucl-th"
      ]
    },
    "list":{
      "title":[
        "Systematic calculation on alpha decay and cluster radioactivity of\n  superheavy nuclei",
        "Description of nucleon transfer reactions at intermediate energies\n  within the impulse picture",
        "Nuclear Schiff Moments and CP Violation",
        "Chiral symmetry and peripheral neutron-$\\alpha$ scattering",
        "New Skyrme parametrizations to describe finite nuclei and neutron star\n  matter with realistic effective masses. II. Adjusting the spin-dependent\n  terms",
        "$\\alpha+\\alpha+{}^{3}$He cluster structure in ${}^{11}$C",
        "Patterns of spin and pseudo-spin symmetries in nuclear relativistic\n  mean-field approaches",
        "Electromagnetic form factors of ${}^6$Li, ${}^7$Li, and ${}^7$Be in\n  cluster effective field theory",
        "Towards shell model interactions with credible uncertainties",
        "$\\Lambda$ and $\\Sigma$ potentials in neutron stars, hypernuclei, and\n  heavy-ion collisions",
        "$A=2,3$ nuclear contact coefficients in the Generalized Contact\n  Formalism",
        "$\\gamma W$-exchange contributions in neutron $\\beta$ decay in the\n  forward-angle limit",
        "Electric dipole excitations near the neutron separation energies in\n  $^{96}$Mo",
        "Nano-topographical changes in latent fingerprint due to degradation over\n  time studied by Atomic force microscopy -- option to set a timeline?",
        "Nonlinear Temperature Sensitivity of Residential Electricity Demand:\n  Evidence from a Distributional Regression Approach",
        "Conformal Prediction and Human Decision Making",
        "On monotone alternating inverse monoids",
        "Oblique rotational axis detection using elliptical optical vortex based\n  on rotational Doppler effect",
        "Sublinear Variational Optimization of Gaussian Mixture Models with\n  Millions to Billions of Parameters",
        "On the asymptotic validity of confidence sets for linear functionals of\n  solutions to integral equations",
        "Revisiting Frank-Wolfe for Structured Nonconvex Optimization",
        "Cayley unitary elements in group algebras under oriented involutions",
        "Tractable General Equilibrium",
        "Mesoscopic Collective Dynamics in Liquids and the Dual Model",
        "Comprehensive landscape and simple rules for transition-metal Heusler\n  semiconductors",
        "Efficient Truncations of SU($N_c$) Lattice Gauge Theory for Quantum\n  Simulation",
        "Attractors for Singular-Degenerate Porous Medium Type Equations Arising\n  in Models for Biofilm Growth",
        "Topology-preserving discretization for the magneto-frictional equations\n  arising in the Parker conjecture"
      ],
      "abstract":[
        "In the Coulomb and Proximity Potential Model (CPPM) framework, we have\ninvestigated the cluster radioactivity and alpha decay half-lives of superheavy\nnuclei. We study 22 different versions of proximity potential forms that have\nbeen proposed to describe proton radioactivity, two-proton radioactivity,\nheavy-ion radioactivity, quasi-elastic scattering, fusion reactions, and other\napplications. The half-lives of cluster radioactivity and alpha decay of 41\natomic nuclei ranging from 221Fr to 244Cm were calculated, and the results\nindicate that the refined nuclear potential named BW91 is the most suitable\nproximity potential form for the cluster radioactivity and alpha decay of\nsuperheavy nuclei since the root-mean-square (RMS) deviation between the\nexperimental data and the relevant theoretical calculation results is the\nsmallest ({\\sigma}= 0.841). By using CPPM, we predicted the half-lives of 20\npotential cluster radioactivity and alpha decay candidates. These cluster\nradioactivities and alpha decays are energetically allowed or observable but\nnot yet quantified in NUBASE2020.",
        "Background:\n  At intermediate energies, transfer reactions hardly occur because the\nmomentum-matching condition is difficult to satisfy. In the standard\ndistorted-wave Born approximation, a particle to be transferred must have a\nmomentum being similar to the momentum transfer.\n  Purpose:\n  We propose a new reaction framework based on the distorted-wave impulse\napproximation for transfer reactions at intermediate energies, aiming to ease\nthe momentum-matching condition.\n  Methods:\n  The $(p,d)$ reaction is described as a $p$-$d$ elastic scattering for\nbackward-angle scattering in the target nucleus and the proton that formed a\n$pn$ pair is left and bound in the residual nucleus in the final channel. The\nmomentum transfer is shared by the deuteron in the target nucleus and the\nproton in the residual nucleus.\n  Results:\n  The new framework is applied to the $^{16}$O($p,d$)$^{15}$O reaction at\n200~MeV and compared with experimental data. The angular distribution is\nsatisfactorily well described, whereas an anomalously large scaling factor is\nneeded to reproduce the absolute value. The transition matrix is analyzed in\ndetail and the mechanism of the momentum sharing is clarified.\n  Conclusions:\n  The new reaction framework for transfer reactions at intermediate energies\nseems to be promising for describing the reaction mechanism but fails to\nexplain the absolute value of the cross section. The use of the $p$-$d$\ntransition amplitude instead of its cross section will be necessary to draw a\nconclusion on the applicability of the present framework.",
        "This paper reviews the calculation of nuclear Schiff moments, which one must\nknow in order to interpret experiments that search for time-reversal-violating\nelectric dipole moments in certain atoms and molecules. After briefly reviewing\nthe connection between dipole moments and CP violation in and beyond the\nStandard Model of particle physics, Schiff's theorem, which concerns the\nscreening of nuclear electric dipole moments by electrons, Schiff moments, and\nexperiments to measure dipole moments in atoms and molecules, the paper\nexamines attempts to compute Schiff moments in nuclei such as $^{199}$Hg and\noctupole-deformed isotopes such as $^{225}$Ra, which are particularly useful in\nexperiments. It then turns to ab initio nuclear-structure theory, describing\nways in which both the In-Medium Similarity Renormalization Group and\ncoupled-cluster theory can be used to compute important Schiff moments more\naccurately than the less controlled methods that have been applied so far.",
        "We propose and demonstrate that peripheral neutron-$\\alpha$ scattering at low\nenergies can serve as a sensitive and clean probe of the long-range\nthree-nucleon forces. To this aim, we perform {\\it ab initio} quantum Monte\nCarlo calculations using two- and three-nucleon interactions derived in chiral\neffective field theory up to third expansion order. We show that the\nlongest-range three-nucleon force stemming from the two-pion exchange plays a\ncrucial role in the proper description of the neutron-$\\alpha$ $D$-wave phase\nshifts. Our study reveals the predictive power of chiral symmetry in the\nfew-body sector and opens a new direction for probing and constraining\nthree-nucleon forces.",
        "Many common Skyrme functionals present ferromagnetic instabilities or\nunrealistic density dependence of the spin-1 Landau parameters. To solve these\nproblems, we consider the Skyrme interaction as a density-functional rather\nthan a density-dependent two-body force. This allows us to adjust the\nspin-dependent terms of the new extended Skyrme functionals of our previous\npaper [M. Duan and M. Urban, Phys. Rev. C 110, 065806 (2024)] independently\nwithout altering the properties of spin saturated matter. The parameters of the\nspin-dependent terms are determined by fitting the Landau parameters $G_0$ and\n$G_0^\\prime$ in neutron matter and symmetric nuclear matter and the\neffective-mass splitting of up and down particles in spin polarized matter to\nthe results of microscopic calculations. Using the new parametrizations, called\nSky3s and Sky4s, the spin-related properties of nuclear matter are in good\nagreement with the microscopic results. As an application, we compute response\nfunctions and neutrino scattering rates of neutron-star matter with the new\nfunctionals having realistic effective masses and Landau parameters.",
        "We study the $\\alpha + \\alpha + {}^{3}$He cluster structure of ${}^{11}$C\nwithin the microscopic cluster model. The calculations essentially reproduce\nthe energy spectra for both negative and positive parity states, particularly\nthe $3\/2_3^-$ state near the $\\alpha+\\alpha$+${}^{3}$He threshold. We also\ncalculate the isoscalar monopole, electric quadrupole transition strengths, and\nroot-mean-square radii for the low-lying states. These results suggest that the\n$3\/2_3^-$, $1\/2_2^-$, and $5\/2_3^-$ states have a well-developed $\\alpha +\n\\alpha$ + ${}^{3}$He cluster structure. The analysis of the generator\ncoordinate method wave functions indicates the dilute gaslike nature for the\n$3\/2_3^-$, $1\/2_2^-$, and $5\/2_3^-$ states, suggesting that they could be\ncandidates for the Hoyle-analog state. Furthermore, it is found that the\n$5\/2_2^+$ and $5\/2_3^+$ states may possess a linear chain structure.",
        "The behavior of spin doublets is known to play a major role in nuclear\nstructure and shell effects. Pseudo-spin doublets are also known to impact the\nsingle-particle spectrum. The covariant framework, having these two effects\nencoded in its approach, is an excellent tool to understand the main mechanism\ndriving theses spin and pseudo-spin symmetries and their breaking. A\nperturbative expansion of the degeneracy raising related to spin and\npseudo-spin effects is proposed, up to second order. It allows to understand\nthe main behavior of spin and pseudo-spin energy doublets, such as their A\ndependence, as well as their common footing and differences. In the case of the\nspin symmetry, only the lower component of the Dirac bi-spinor is involved,\nwhereas in the case of the pseudo-spin one, both the upper and lower components\nare involved. Their interplay with the covariant potentials is also analyzed.",
        "Effective field theory (EFT) provides a powerful model-independent\ntheoretical framework for illuminating complicated interactions across a wide\nrange of physics areas and subfields. In this work, we consider the low-energy\ndeuteron-Helium-4, triton-Helium-4, and helion-Helium-4 systems at low energies\nin cluster EFT. In particular, we focus on the deuteron + Helium-4 cluster\nconfiguration of the Lithium-6 nucleus, the triton + Helium-4 cluster\nconfiguration of the Lithium-7 nucleus, and the Helium-3-Helium-4 configuration\nof the Beryllium-7 nucleus, respectively. We illustrate how to directly extract\nthe asymptotic normalization coefficient and several observables using\nexperimental measurement of the electromagnetic form factors of these nuclei.",
        "Background: The nuclear shell model offers realistic predictions of nuclear\nstructure starting from (quasi-) proton and neutron degrees of freedom, but\nrelies on coupling constants (interaction matrix elements) that must be fit to\nexperiment. To extend the shell model's applicability across the nuclear chart,\nand specifically toward the driplines, we must first be able to efficiently\ntest new interaction matrix elements and assign credible uncertainties.\n  Purpose: We develop and test a framework to efficiently fit new shell model\ninteractions and obtain credible uncertainties. We further demonstrate its use\nby validating the uncertainty estimates of the known \\textit{sd}-shell\neffective interactions.\n  Methods: We use eigenvector continuation to emulate solutions to the exact\nshell model. First, we use the emulator to replicate earlier results using a\nwell-known linear-combination chi-squared minimization algorithm. Then, we\nemploy a modern Markov Chain Monte Carlo method to test for nonlinearities in\nthe observable posterior distributions, which previous sensitivity analyses\nprecluded.\n  Results: The emulator reproduces the USDB interaction within a small margin\nof error, allowing for the quantification of the matrix element uncertainty.\nHowever, we find that to obtain credible predictive intervals the model defect\nof the shell model itself, rather than experimental or emulator\nuncertainty\/error, must be taken into account.\n  Conclusions: Eigenvector continuation can be used to accelerate fitting shell\nmodel interactions. We confirm that the linear approximation used to develop\ninteractions in the past is indeed sufficient. However, we find that typical\nassumptions about the likelihood function must be modified in order to obtain a\ncredible uncertainty-quantified interaction.",
        "With an appropriate $YNN$ force, the $\\Lambda$ single-particle potential\n($\\Lambda$ potential) can be made strongly repulsive at high density, and one\ncan solve the hyperon puzzle of neutron stars. We investigate the consistency\nof such a $\\Lambda$ potential, evaluated recently from $YN$ and $YNN$ forces\nbased on chiral effective field theory, with hypernuclear data and heavy-ion\ncollision data. It is found that model calculations with such a $\\Lambda$\npotential can reproduce the data of the $\\Lambda$ hypernuclear spectroscopy and\nthe $\\Lambda$ directed flow in heavy-ion collisions. Also, we evaluate the\n$\\Sigma$ potential, which can be calculated by using the same hyperon forces as\nfor the $\\Lambda$ potential. Specifically, we show that the low-energy\nconstants characterizing the strength of the $YNN$ force can be chosen to\nsuppress the appearance of the $\\Lambda$'s in neutron stars while at the same\ntime the empirical value of the $\\Sigma$ potential is reproduced.",
        "This work focuses on extracting nuclear contact coefficients for \\( A = 2 \\),\n\\( A = 3 \\) and \\( A = 4 \\) nuclei within the Generalized Contact Formalism\nframework. We investigate the universality of these coefficients across\ndifferent nuclear systems and interaction models, using both local (in \\( r\n\\)-space) and non-local (in \\( k \\)-space) chiral potentials. The\nHyperspherical Harmonics method is employed to calculate the nuclear wave\nfunctions from which we obtain the two-body momentum distributions and the\ntwo-body density functions, which are essential for extracting the contact\ncoefficients. The adopted method is a rigorous ab-initio approach that can be\napplied to virtually any potential.\n  We present ratios of contact coefficients across various spin and isospin\nchannels, highlighting their independence from the used nuclear potential. This\nstudy extends previous work where only local interaction models were employed.\nFurthermore, we verify whether the contact coefficient ratio between different\nnuclei remains consistent even when non-local potentials are considered.\n  Future work will extend this analysis to heavier nuclei, such as \\( A = 4 \\)\nand \\( A = 6 \\) nuclei.",
        "In this work, the contributions from $\\gamma W$-exchange in neutron $\\beta$\ndecay are estimated at the amplitude level. Using a general form for the\nelectromagnetic (EM) form factors (FFs) of the proton, the EM FFs of the\nneutron, and the weak FFs of the $Wnp$ interaction as inputs, we present\nanalytical expressions for the inner part of the $\\gamma W$-exchange amplitude\nunder the forward angle limit. The differences and relations between our method\nand those used in the references are discussed. To compare our numerical\nresults with those provided in the references, we consider three types of FFs\nas examples. The numerical results show that when the favored EM FFs are used,\nour result for the contribution from the Fermi part is consistent with those\nreported in the references, while our results for the Gamow-Teller parts are\nabout 7\\% and 13\\% larger than those reported in Ref.~\\cite{Hayen-2021}.",
        "Electric dipole strength near the neutron separation energy significantly\nimpacts nuclear structure properties and astrophysical scenarios. These\nexcitations are complex in nature and may involve the so-called pygmy dipole\nresonance (PDR). Transition densities play a crucial role in understanding the\nnature of nuclear excited states, including collective excitations, as well as\nin constructing transition potentials in DWBA or coupled-channels equations. In\nthis work, we focus on electric dipole excitations in spherical molybdenum\nisotopes, particularly $^{96}$Mo, employing fully consistent\nHartree-Fock-Bogoliubov (HFB) and Quasiparticle Random Phase Approximation\n(QRPA) methods. We analyze the dipole strength near the neutron separation\nenergy, which represents the threshold for neutron capture processes, and\nexamine the isospin characteristics of PDR states through transition density\ncalculations. Examination of proton and neutron transition densities reveals\ndistinctive features of each dipole state, indicating their isoscalar and\nisovector nature. We observe that the primary component in the enhanced\nlow-energy region exhibits isovector character. The PDR displays a mixture of\nisoscalar and isovector nature, distinguishing it from the isovector giant\ndipole resonance (IVGDR). These findings lay the groundwork for future\ninvestigations into the role of transition densities in reaction models and for\ntheir application to inelastic scattering calculations.",
        "Latent fingerprints, if present, are crucial in identifying the suspect who\nwas at the crime scene. If there are many latent fingerprints or the suspect is\nfrom the same household, crime investigators may have difficulty identifying\nwhose latent fingerprints are time-related to the crime. Here, we report\nchanges in the nanoscale topography of latent fingerprints, which may serve as\na timeline and could help estimate when the latent fingerprint was imprinted.\nOn the latent fingerprint of an adolescent, we observed a change in\nnano-topography over time, specifically the formation of nano-chain structures\nin space between the imprinted papillary ridges. We consequently compared this\nobservation with the decomposition of the latent fingerprints of a child and\nadult. We observed a significant difference in the time change in\nnano-topography of latent fingerprints of a child, adolescent, and young adult.\nThe nano-topographical changes of latent fingerprints were studied by atomic\nforce microscopy over 70 days. In the case of child's and adolescent's latent\nfingerprints, the first nano-chains were observed already 24 hours after\nimprinting of the latent fingerprint, and the number of nano-chains increased\nsteadily up to 21 days, then we observed that another organic material covered\nthe nano-chains, and they started slowly deteriorating; nevertheless, the\nnano-chains were still present on the 70th day.",
        "We estimate the temperature sensitivity of residential electricity demand\nduring extreme temperature events using the distribution-to-scalar regression\nmodel. Rather than relying on simple averages or individual quantile statistics\nof raw temperature data, we construct distributional summaries, such as\nprobability density, hazard rate, and quantile functions, to retain a more\ncomprehensive representation of temperature variation. This approach not only\nutilizes richer information from the underlying temperature distribution but\nalso enables the examination of extreme temperature effects that conventional\nmodels fail to capture. Additionally, recognizing that distribution functions\nare typically estimated from limited discrete observations and may be subject\nto measurement errors, our econometric framework explicitly addresses this\nissue. Empirical findings from the hazard-to-demand model indicate that\nresidential electricity demand exhibits a stronger nonlinear response to cold\nwaves than to heat waves, while heat wave shocks demonstrate a more pronounced\nincremental effect. Moreover, the temperature quantile-to-demand model produces\nlargely insignificant demand response estimates, attributed to the offsetting\ninfluence of two counteracting forces.",
        "Methods to quantify uncertainty in predictions from arbitrary models are in\ndemand in high-stakes domains like medicine and finance. Conformal prediction\nhas emerged as a popular method for producing a set of predictions with\nspecified average coverage, in place of a single prediction and confidence\nvalue. However, the value of conformal prediction sets to assist human\ndecisions remains elusive due to the murky relationship between coverage\nguarantees and decision makers' goals and strategies. How should we think about\nconformal prediction sets as a form of decision support? We outline a decision\ntheoretic framework for evaluating predictive uncertainty as informative\nsignals, then contrast what can be said within this framework about idealized\nuse of calibrated probabilities versus conformal prediction sets. Informed by\nprior empirical results and theories of human decisions under uncertainty, we\nformalize a set of possible strategies by which a decision maker might use a\nprediction set. We identify ways in which conformal prediction sets and posthoc\npredictive uncertainty quantification more broadly are in tension with common\ngoals and needs in human-AI decision making. We give recommendations for future\nresearch in predictive uncertainty quantification to support human decision\nmakers.",
        "In this paper, we consider the inverse submonoids $AM_n$ of monotone\ntransformations and $AO_n$ of order-preserving transformations of the\nalternating inverse monoid $AI_n$ on a chain with $n$ elements. We compute the\ncardinalities, describe the Green's structures and the congruences, and\ncalculate the ranks of these two submonoids of $AI_n$.",
        "The rotational Doppler effect (RDE) of structured light carrying orbital\nangular momentum (OAM) has attracted widespread attention for applications in\noptical sensors and OAM spectrum detection. These studies, however, based on\nRDE, are mostly focused on the motion parameters of rotating objects; other\nequally important attitude characteristics, e.g., the tilt angle of the axis of\nrotation, have rarely been considered. We observed an interesting phenomenon in\nthe experiments: the rotational Doppler spectral distribution varies with the\nellipticity of the elliptical optical vortex (EOV) and the tilt angle between\nthe rotational axis and optical axis, which inspired us to wonder if it is\npossible to detect oblique rotational axis or compensate the rotational Doppler\nbroadening effect induced by oblique incidence by utilizing the EOV. Here, we\nreveal the RDE quantitative relationship with tilt angle and ellipticity for\nthe first time and report a novel approach for tilt angle measurement. By\nemploying a series of EOV with periodically varying ellipticity to illuminate a\nrotating object and analyze the time-frequency spectral distribution of\nscattered light associated with ellipticity and tilt angle, the tilt angle can\nbe acquired accurately based on the specific relationship between the tilt\nangle and ellipticity of the EOV. Furthermore, the spectrum broadening effect\narising from oblique incidence in the actual scenario may be addressed through\nour scheme. The method may find applications in industrial manufacturing and\ntarget attitude measurement, and our results provide new insights for obtaining\nmore information about objects.",
        "Gaussian Mixture Models (GMMs) range among the most frequently used machine\nlearning models. However, training large, general GMMs becomes computationally\nprohibitive for datasets with many data points $N$ of high-dimensionality $D$.\nFor GMMs with arbitrary covariances, we here derive a highly efficient\nvariational approximation, which is integrated with mixtures of factor\nanalyzers (MFAs). For GMMs with $C$ components, our proposed algorithm\nsignificantly reduces runtime complexity per iteration from\n$\\mathcal{O}(NCD^2)$ to a complexity scaling linearly with $D$ and remaining\nconstant w.r.t. $C$. Numerical validation of this theoretical complexity\nreduction then shows the following: the distance evaluations required for the\nentire GMM optimization process scale sublinearly with $NC$. On large-scale\nbenchmarks, this sublinearity results in speed-ups of an order-of-magnitude\ncompared to the state-of-the-art. As a proof of concept, we train GMMs with\nover 10 billion parameters on about 100 million images, and observe training\ntimes of approximately nine hours on a single state-of-the-art CPU.",
        "This paper examines the construction of confidence sets for parameters\ndefined as a linear functional of the solution to an integral equation\ninvolving conditional expectations. We show that any confidence set uniformly\nvalid over a broad class of probability laws, allowing the integral equation to\nbe arbitrarily ill-posed must have, with high probability under some laws, a\ndiameter at least as large as the diameter of the parameter's range over the\nmodel. Additionally, we establish that uniformly consistent estimators of the\nparameter do not exist. We show that, consistent with the weak instruments\nliterature, Wald confidence intervals are not uniformly valid. Furthermore, we\nargue that inverting the score test, a successful approach in that literature,\ndoes not extend to the broader class of parameters considered here. We present\na method for constructing uniformly valid confidence sets in the special case\nwhere all variables are binary and discuss its limitations. Finally, we\nemphasize that developing uniformly valid confidence sets for the general class\nof parameters considered in this paper remains an open problem.",
        "We introduce a new projection-free (Frank-Wolfe) method for optimizing\nstructured nonconvex functions that are expressed as a difference of two convex\nfunctions. This problem class subsumes smooth nonconvex minimization,\npositioning our method as a promising alternative to the classical Frank-Wolfe\nalgorithm. DC decompositions are not unique; by carefully selecting a\ndecomposition, we can better exploit the problem structure, improve\ncomputational efficiency, and adapt to the underlying problem geometry to find\nbetter local solutions. We prove that the proposed method achieves a\nfirst-order stationary point in $O(1\/\\epsilon^2)$ iterations, matching the\ncomplexity of the standard Frank-Wolfe algorithm for smooth nonconvex\nminimization in general. Specific decompositions can, for instance, yield a\ngradient-efficient variant that requires only $O(1\/\\epsilon)$ calls to the\ngradient oracle. Finally, we present numerical experiments demonstrating the\neffectiveness of the proposed method compared to the standard Frank-Wolfe\nalgorithm.",
        "Let $\\mathbf{F}$ be a real extension of $\\mathbb{Q}$, $G$ a finite group and\n$\\mathbf{F}G$ its group algebra. Given both a group homomorphism\n$\\sigma:G\\rightarrow \\{\\pm1\\}$ (called an orientation) and a group involution\n$^\\ast:G \\rightarrow G$ such that $gg^\\ast\\in N=ker(\\sigma)$, an oriented group\ninvolution $\\circledast$ of $\\mathbf{F}G$ is defined by $\\alpha=\\sum_{g\\in\nG}\\alpha_{g}g \\mapsto \\alpha^\\circledast=\\sum_{g\\in\nG}\\alpha_{g}\\sigma(g)g^{\\ast}$. In this paper, in case the involution on $G$ is\nthe classical one, $x\\mapsto x^{-1}$, $\\beta=x+x^{-1}$ is a skew-symmetric\nelement in $\\mathbf{F}G$ such that $1+\\beta$ is invertible, for $x\\in G$ with\n$\\sigma(x)=-1$, we consider Cayley unitary elements built out of $\\beta$. We\nprove that the coefficients of $(1+\\beta)^{-1}$ involve an interesting sequence\nwhich is a Fibonacci-like sequence.",
        "We study Walrasian economies (or general equilibrium models) and their\nsolution concept, the Walrasian equilibrium. A key challenge in this domain is\nidentifying price-adjustment processes that converge to equilibrium. One such\nprocess, t\\^atonnement, is an auction-like algorithm first proposed in 1874 by\nL\\'eon Walras. While continuous-time variants of t\\^atonnement are known to\nconverge to equilibrium in economies satisfying the Weak Axiom of Revealed\nPreferences (WARP), the process fails to converge in a pathological Walrasian\neconomy known as the Scarf economy. To address these issues, we analyze\nWalrasian economies using variational inequalities (VIs), an optimization\nframework. We introduce the class of mirror extragradient algorithms, which,\nunder suitable Lipschitz-continuity-like assumptions, converge to a solution of\nany VI satisfying the Minty condition in polynomial time. We show that the set\nof Walrasian equilibria of any balanced economy-which includes among others\nArrow-Debreu economies-corresponds to the solution set of an associated VI that\nsatisfies the Minty condition but is generally discontinuous. Applying the\nmirror extragradient algorithm to this VI we obtain a class of\nt\\^atonnement-like processes, which we call the mirror extrat\\^atonnement\nprocess. While our VI formulation is generally discontinuous, it is\nLipschitz-continuous in variationally stable Walrasian economies with bounded\nelasticity-including those satisfying WARP and the Scarf economy-thus\nestablishing the polynomial-time convergence of mirror extrat\\^atonnement in\nthese economies. We validate our approach through experiments on large\nArrow-Debreu economies with Cobb-Douglas, Leontief, and CES consumers, as well\nas the Scarf economy, demonstrating fast convergence in all cases without\nfailure.",
        "A microscopic vision is presented of a Dual Model of Liquids from a solid\npicture. Among the novelties of this model is that it provides quantitative\nexpressions of various extensive thermophysical properties. The introduction of\nthe statistical number of excited degrees of freedom (DoF) allows bypassing the\nproblem of other dual models which are sometimes unable to correctly reproduce\nthe expressions for those thermophysical quantities showing deviations due to\nthe activation or deactivation of internal DoF. The interpretation of the\nrelaxation times is given, their Order of Magnitude calculated and the way in\nwhich these times are involved in the different phases of the collective\ndynamics of liquids is discussed. A comparison is provided with results\nobtained in the frame of another phononic model of liquids, as well as with the\npredictions for the viscoelastic transition regions and with systems exhibiting\nkgap. In the last part of the paper, theoretical insights and experiments are\nsuggested as potential directions for future research and development.",
        "Heusler alloys, renowned for their multifunctionality and capacity for vast\nelemental customization, are primarily classified into half-Heusler (XYZ) and\nfull-Heusler (X2YZ) structural types. Typically, the 18-electron half-Heusler\nand the 24-electron full-Heusler alloys are recognized as semiconductors,\nfollowing the Slater-Pauling rule. Semiconductors are desired for many\napplications, but they represent a minor portion compared to the predominantly\nmetallic and half-metallic members of the Heusler family. To broaden the scope\nof Heusler semiconductors, advancements have been made in developing variants\nsuch as double-half Heuslers XX'Y2Z2 and quaternary full Heuslers XX'YZ, which\nincorporate four constituent elements. Recently, vacancy-filling\noff-stoichiometric Heuslers of ternary X1+bYZ (0 <= b <= 1) and quaternary\nXaX'bYZ (1 <= a + b <= 2) have emerged as a more versatile strategy. However,\nthe flexibility associated with off-stoichiometry inevitably leads to\ncomplications, including issues with fractional filling ratios and complex site\noccupations. This work presents a comprehensive landscape of\ntransition-metal-containing Heusler semiconductors, focusing on the\noff-stoichiometric Heuslers but seamlessly encompassing the\ninteger-stoichiometric systems. The structural and electronic properties can be\ntheoretically understood through a few simple rules. Many systems have been\nexperimentally validated, showcasing their potential for applications such as\nthermoelectric converters.",
        "Quantum simulations of lattice gauge theories offer the potential to directly\nstudy the non-perturbative dynamics of quantum chromodynamics, but naive\nanalyses suggest that they require large computational resources. Large $N_c$\nexpansions are performed to order 1\/$N_c$ to simplify the Hamiltonian of pure\nSU($N_c$) lattice gauge theories. A reformulation of the electric basis is\nintroduced with a truncation strategy based on the construction of local Krylov\nsubspaces with plaquette operators. Numerical simulations show that these\ntruncated Hamiltonians are consistent with traditional lattice calculations at\nrelatively small couplings. It is shown that the computational resources\nrequired for quantum simulation of time evolution generated by these\nHamiltonians is 17-19 orders of magnitude smaller than previous approaches.",
        "We investigate the long-time behaviour of solutions of a class of\nsingular-degenerate porous medium type equations in bounded Lipschitz domains\nwith mixed Dirichlet-Neumann boundary conditions. The existence of global\nattractors is shown under very general assumptions. Assuming, in addition, that\nsolutions are globally H\\\"older continuous and the reaction terms satisfy a\nsuitable sign condition in the vicinity of the degeneracy, we also prove the\nexistence of an exponential attractor, which, in turn, yields the finite\nfractal dimension of the global attractor. Moreover, we extend the results for\nscalar equations to systems where the degenerate equation is coupled to a\nsemilinear reaction-diffusion equation. The study of such systems is motivated\nby models for biofilm growth.",
        "The Parker conjecture, which explores whether magnetic fields in perfectly\nconducting plasmas can develop tangential discontinuities during magnetic\nrelaxation, remains an open question in astrophysics. Helicity conservation\nprovides a topological barrier during relaxation, preventing topologically\nnontrivial initial data relaxing to trivial solutions; preserving this\nmechanism discretely over long time periods is therefore crucial for numerical\nsimulation. This work presents an energy- and helicity-preserving finite\nelement discretization for the magneto-frictional system, for investigating the\nParker conjecture. The algorithm preserves a discrete version of the\ntopological barrier and a discrete Arnold inequality. We also discuss\nextensions to domains with nontrivial topology."
      ]
    }
  },
  {
    "id":2411.17971,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"Multiscale modeling and simulation of brain blood flow",
    "start_abstract":"The aim of this work is to present an overview recent advances in multi-scale modeling brain blood flow. In particular, we some approaches that enable the silico study and multi-physics phenomena cerebral vasculature. We discuss formulation continuum atomistic approaches, a consistent framework for their concurrent coupling, list challenges one needs overcome achieving seamless scalable integration heterogeneous numerical solvers. effectiveness proposed demonstrated realistic case involving thrombus formation process taking place on wall patient-specific aneurysm. This highlights ability algorithms resolve important biophysical processes span several spatial temporal scales, potentially yielding new insight into key aspects flow health disease. Finally, open questions emerging topics future research.",
    "start_categories":[
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b5"
      ],
      "title":[
        "Learning Reduced-Order Models for Cardiovascular Simulations with Graph Neural Networks"
      ],
      "abstract":[
        "Reduced-order models based on physics are a popular choice in cardiovascular modeling due to their efficiency, but they may experience loss in accuracy when working with anatomies that contain numerous junctions or pathological conditions. We develop one-dimensional reduced-order models that simulate blood flow dynamics using a graph neural network trained on three-dimensional hemodynamic simulation data. Given the initial condition of the system, the network iteratively predicts the pressure and flow rate at the vessel centerline nodes. Our numerical results demonstrate the accuracy and generalizability of our method in physiological geometries comprising a variety of anatomies and boundary conditions. Our findings demonstrate that our approach can achieve errors below 3% for pressure and flow rate, provided there is adequate training data. As a result, our method exhibits superior performance compared to physics-based one-dimensional models while maintaining high efficiency at inference time."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "FOLDER: Accelerating Multi-modal Large Language Models with Enhanced\n  Performance",
        "A Physics-Informed Blur Learning Framework for Imaging Systems",
        "Syllables to Scenes: Literary-Guided Free-Viewpoint 3D Scene Synthesis\n  from Japanese Haiku",
        "From Poses to Identity: Training-Free Person Re-Identification via\n  Feature Centralization",
        "Fine-Grained Image-Text Correspondence with Cost Aggregation for\n  Open-Vocabulary Part Segmentation",
        "Color Matching Using Hypernetwork-Based Kolmogorov-Arnold Networks",
        "SFDLA: Source-Free Document Layout Analysis",
        "CineBrain: A Large-Scale Multi-Modal Brain Dataset During Naturalistic\n  Audiovisual Narrative Processing",
        "AdaFV: Rethinking of Visual-Language alignment for VLM acceleration",
        "Towards Interactive Deepfake Analysis",
        "EHNet: An Efficient Hybrid Network for Crowd Counting and Localization",
        "A Label-Free High-Precision Residual Moveout Picking Method for Travel\n  Time Tomography based on Deep Learning",
        "USegMix: Unsupervised Segment Mix for Efficient Data Augmentation in\n  Pathology Images",
        "Behaviour of Newton Polygon over polynomial composition",
        "Paying Attention to Facts: Quantifying the Knowledge Capacity of\n  Attention Layers",
        "Optimizing Fire Safety: Reducing False Alarms Using Advanced Machine\n  Learning Techniques",
        "$\\ell_{p}$ has nontrivial Euclidean distortion growth when $2<p<4$",
        "Avoiding spurious sharpness minimization broadens applicability of SAM",
        "Disparities in LLM Reasoning Accuracy and Explanations: A Case Study on\n  African American English",
        "MuJoCo Playground",
        "Adaptive Mesh Refinement for Variational Inequalities",
        "Efficient and Universal Neural-Network Decoder for Stabilizer-Based\n  Quantum Error Correction",
        "A View of the Certainty-Equivalence Method for PAC RL as an Application\n  of the Trajectory Tree Method",
        "The BAD Paradox: A Critical Assessment of the Belin\/Ambr\\'osio Deviation\n  Model",
        "Agentic AI Systems Applied to tasks in Financial Services: Modeling and\n  model risk management crews",
        "AoI-Sensitive Data Forwarding with Distributed Beamforming in\n  UAV-Assisted IoT",
        "Balanced Rate-Distortion Optimization in Learned Image Compression",
        "GEMA-Score: Granular Explainable Multi-Agent Score for Radiology Report\n  Evaluation"
      ],
      "abstract":[
        "Recently, Multi-modal Large Language Models (MLLMs) have shown remarkable\neffectiveness for multi-modal tasks due to their abilities to generate and\nunderstand cross-modal data. However, processing long sequences of visual\ntokens extracted from visual backbones poses a challenge for deployment in\nreal-time applications. To address this issue, we introduce FOLDER, a simple\nyet effective plug-and-play module designed to reduce the length of the visual\ntoken sequence, mitigating both computational and memory demands during\ntraining and inference. Through a comprehensive analysis of the token reduction\nprocess, we analyze the information loss introduced by different reduction\nstrategies and develop FOLDER to preserve key information while removing visual\nredundancy. We showcase the effectiveness of FOLDER by integrating it into the\nvisual backbone of several MLLMs, significantly accelerating the inference\nphase. Furthermore, we evaluate its utility as a training accelerator or even\nperformance booster for MLLMs. In both contexts, FOLDER achieves comparable or\neven better performance than the original models, while dramatically reducing\ncomplexity by removing up to 70% of visual tokens.",
        "Accurate blur estimation is essential for high-performance imaging across\nvarious applications. Blur is typically represented by the point spread\nfunction (PSF). In this paper, we propose a physics-informed PSF learning\nframework for imaging systems, consisting of a simple calibration followed by a\nlearning process. Our framework could achieve both high accuracy and universal\napplicability. Inspired by the Seidel PSF model for representing spatially\nvarying PSF, we identify its limitations in optimization and introduce a novel\nwavefront-based PSF model accompanied by an optimization strategy, both\nreducing optimization complexity and improving estimation accuracy. Moreover,\nour wavefront-based PSF model is independent of lens parameters, eliminate the\nneed for prior knowledge of the lens. To validate our approach, we compare it\nwith recent PSF estimation methods (Degradation Transfer and Fast Two-step)\nthrough a deblurring task, where all the estimated PSFs are used to train\nstate-of-the-art deblurring algorithms. Our approach demonstrates improvements\nin image quality in simulation and also showcases noticeable visual quality\nimprovements on real captured images.",
        "In the era of the metaverse, where immersive technologies redefine human\nexperiences, translating abstract literary concepts into navigable 3D\nenvironments presents a fundamental challenge in preserving semantic and\nemotional fidelity. This research introduces HaikuVerse, a novel framework for\ntransforming poetic abstraction into spatial representation, with Japanese\nHaiku serving as an ideal test case due to its sophisticated encapsulation of\nprofound emotions and imagery within minimal text. While existing text-to-3D\nmethods struggle with nuanced interpretations, we present a literary-guided\napproach that synergizes traditional poetry analysis with advanced generative\ntechnologies. Our framework centers on two key innovations: (1) Hierarchical\nLiterary-Criticism Theory Grounded Parsing (H-LCTGP), which captures both\nexplicit imagery and implicit emotional resonance through structured semantic\ndecomposition, and (2) Progressive Dimensional Synthesis (PDS), a multi-stage\npipeline that systematically transforms poetic elements into coherent 3D scenes\nthrough sequential diffusion processes, geometric optimization, and real-time\nenhancement. Extensive experiments demonstrate that HaikuVerse significantly\noutperforms conventional text-to-3D approaches in both literary fidelity and\nvisual quality, establishing a new paradigm for preserving cultural heritage in\nimmersive digital spaces. Project website at:\nhttps:\/\/syllables-to-scenes.github.io\/",
        "Person re-identification (ReID) aims to extract accurate identity\nrepresentation features. However, during feature extraction, individual samples\nare inevitably affected by noise (background, occlusions, and model\nlimitations). Considering that features from the same identity follow a normal\ndistribution around identity centers after training, we propose a Training-Free\nFeature Centralization ReID framework (Pose2ID) by aggregating the same\nidentity features to reduce individual noise and enhance the stability of\nidentity representation, which preserves the feature's original distribution\nfor following strategies such as re-ranking. Specifically, to obtain samples of\nthe same identity, we introduce two components: Identity-Guided Pedestrian\nGeneration: by leveraging identity features to guide the generation process, we\nobtain high-quality images with diverse poses, ensuring identity consistency\neven in complex scenarios such as infrared, and occlusion. Neighbor Feature\nCentralization: it explores each sample's potential positive samples from its\nneighborhood. Experiments demonstrate that our generative model exhibits strong\ngeneralization capabilities and maintains high identity consistency. With the\nFeature Centralization framework, we achieve impressive performance even with\nan ImageNet pre-trained model without ReID training, reaching mAP\/Rank-1 of\n52.81\/78.92 on Market1501. Moreover, our method sets new state-of-the-art\nresults across standard, cross-modality, and occluded ReID tasks, showcasing\nstrong adaptability.",
        "Open-Vocabulary Part Segmentation (OVPS) is an emerging field for recognizing\nfine-grained parts in unseen categories. We identify two primary challenges in\nOVPS: (1) the difficulty in aligning part-level image-text correspondence, and\n(2) the lack of structural understanding in segmenting object parts. To address\nthese issues, we propose PartCATSeg, a novel framework that integrates\nobject-aware part-level cost aggregation, compositional loss, and structural\nguidance from DINO. Our approach employs a disentangled cost aggregation\nstrategy that handles object and part-level costs separately, enhancing the\nprecision of part-level segmentation. We also introduce a compositional loss to\nbetter capture part-object relationships, compensating for the limited part\nannotations. Additionally, structural guidance from DINO features improves\nboundary delineation and inter-part understanding. Extensive experiments on\nPascal-Part-116, ADE20K-Part-234, and PartImageNet datasets demonstrate that\nour method significantly outperforms state-of-the-art approaches, setting a new\nbaseline for robust generalization to unseen part categories.",
        "We present cmKAN, a versatile framework for color matching. Given an input\nimage with colors from a source color distribution, our method effectively and\naccurately maps these colors to match a target color distribution in both\nsupervised and unsupervised settings. Our framework leverages the spline\ncapabilities of Kolmogorov-Arnold Networks (KANs) to model the color matching\nbetween source and target distributions. Specifically, we developed a\nhypernetwork that generates spatially varying weight maps to control the\nnonlinear splines of a KAN, enabling accurate color matching. As part of this\nwork, we introduce a first large-scale dataset of paired images captured by two\ndistinct cameras and evaluate the efficacy of our and existing methods in\nmatching colors. We evaluated our approach across various color-matching tasks,\nincluding: (1) raw-to-raw mapping, where the source color distribution is in\none camera's raw color space and the target in another camera's raw space; (2)\nraw-to-sRGB mapping, where the source color distribution is in a camera's raw\nspace and the target is in the display sRGB space, emulating the color\nrendering of a camera ISP; and (3) sRGB-to-sRGB mapping, where the goal is to\ntransfer colors from a source sRGB space (e.g., produced by a source camera\nISP) to a target sRGB space (e.g., from a different camera ISP). The results\nshow that our method outperforms existing approaches by 37.3% on average for\nsupervised and unsupervised cases while remaining lightweight compared to other\nmethods. The codes, dataset, and pre-trained models are available at:\nhttps:\/\/github.com\/gosha20777\/cmKAN",
        "Document Layout Analysis (DLA) is a fundamental task in document\nunderstanding. However, existing DLA and adaptation methods often require\naccess to large-scale source data and target labels. This requirements severely\nlimiting their real-world applicability, particularly in privacy-sensitive and\nresource-constrained domains, such as financial statements, medical records,\nand proprietary business documents. According to our observation, directly\ntransferring source-domain fine-tuned models on target domains often results in\na significant performance drop (Avg. -32.64%). In this work, we introduce\nSource-Free Document Layout Analysis (SFDLA), aiming for adapting a pre-trained\nsource DLA models to an unlabeled target domain, without access to any source\ndata. To address this challenge, we establish the first SFDLA benchmark,\ncovering three major DLA datasets for geometric- and content-aware adaptation.\nFurthermore, we propose Document Layout Analysis Adapter (DLAdapter), a novel\nframework that is designed to improve source-free adaptation across document\ndomains. Our method achieves a +4.21% improvement over the source-only baseline\nand a +2.26% gain over existing source-free methods from PubLayNet to\nDocLayNet. We believe this work will inspire the DLA community to further\ninvestigate source-free document understanding. To support future research of\nthe community, the benchmark, models, and code will be publicly available at\nhttps:\/\/github.com\/s3setewe\/sfdla-DLAdapter.",
        "In this paper, we introduce CineBrain, the first large-scale dataset\nfeaturing simultaneous EEG and fMRI recordings during dynamic audiovisual\nstimulation. Recognizing the complementary strengths of EEG's high temporal\nresolution and fMRI's deep-brain spatial coverage, CineBrain provides\napproximately six hours of narrative-driven content from the popular television\nseries The Big Bang Theory for each of six participants. Building upon this\nunique dataset, we propose CineSync, an innovative multimodal decoding\nframework integrates a Multi-Modal Fusion Encoder with a diffusion-based Neural\nLatent Decoder. Our approach effectively fuses EEG and fMRI signals,\nsignificantly improving the reconstruction quality of complex audiovisual\nstimuli. To facilitate rigorous evaluation, we introduce Cine-Benchmark, a\ncomprehensive evaluation protocol that assesses reconstructions across semantic\nand perceptual dimensions. Experimental results demonstrate that CineSync\nachieves state-of-the-art video reconstruction performance and highlight our\ninitial success in combining fMRI and EEG for reconstructing both video and\naudio stimuli. Project Page: https:\/\/jianxgao.github.io\/CineBrain.",
        "The success of VLMs often relies on the dynamic high-resolution schema that\nadaptively augments the input images to multiple crops, so that the details of\nthe images can be retained. However, such approaches result in a large number\nof redundant visual tokens, thus significantly reducing the efficiency of the\nVLMs. To improve the VLMs' efficiency without introducing extra training costs,\nmany research works are proposed to reduce the visual tokens by filtering the\nuninformative visual tokens or aggregating their information. Some approaches\npropose to reduce the visual tokens according to the self-attention of VLMs,\nwhich are biased, to result in inaccurate responses. The token reduction\napproaches solely rely on visual cues are text-agnostic, and fail to focus on\nthe areas that are most relevant to the question, especially when the queried\nobjects are non-salient to the image. In this work, we first conduct\nexperiments to show that the original text embeddings are aligned with the\nvisual tokens, without bias on the tailed visual tokens. We then propose a\nself-adaptive cross-modality attention mixture mechanism that dynamically\nleverages the effectiveness of visual saliency and text-to-image similarity in\nthe pre-LLM layers to select the visual tokens that are informative. Extensive\nexperiments demonstrate that the proposed approach achieves state-of-the-art\ntraining-free VLM acceleration performance, especially when the reduction rate\nis sufficiently large.",
        "Existing deepfake analysis methods are primarily based on discriminative\nmodels, which significantly limit their application scenarios. This paper aims\nto explore interactive deepfake analysis by performing instruction tuning on\nmulti-modal large language models (MLLMs). This will face challenges such as\nthe lack of datasets and benchmarks, and low training efficiency. To address\nthese issues, we introduce (1) a GPT-assisted data construction process\nresulting in an instruction-following dataset called DFA-Instruct, (2) a\nbenchmark named DFA-Bench, designed to comprehensively evaluate the\ncapabilities of MLLMs in deepfake detection, deepfake classification, and\nartifact description, and (3) construct an interactive deepfake analysis system\ncalled DFA-GPT, as a strong baseline for the community, with the Low-Rank\nAdaptation (LoRA) module. The dataset and code will be made available at\nhttps:\/\/github.com\/lxq1000\/DFA-Instruct to facilitate further research.",
        "In recent years, crowd counting and localization have become crucial\ntechniques in computer vision, with applications spanning various domains. The\npresence of multi-scale crowd distributions within a single image remains a\nfundamental challenge in crowd counting tasks. To address these challenges, we\nintroduce the Efficient Hybrid Network (EHNet), a novel framework for efficient\ncrowd counting and localization. By reformulating crowd counting into a point\nregression framework, EHNet leverages the Spatial-Position Attention Module\n(SPAM) to capture comprehensive spatial contexts and long-range dependencies.\nAdditionally, we develop an Adaptive Feature Aggregation Module (AFAM) to\neffectively fuse and harmonize multi-scale feature representations. Building\nupon these, we introduce the Multi-Scale Attentive Decoder (MSAD). Experimental\nresults on four benchmark datasets demonstrate that EHNet achieves competitive\nperformance with reduced computational overhead, outperforming existing methods\non ShanghaiTech Part \\_A, ShanghaiTech Part \\_B, UCF-CC-50, and UCF-QNRF. Our\ncode is in https:\/\/anonymous.4open.science\/r\/EHNet.",
        "Residual moveout (RMO) provides critical information for travel time\ntomography. The current industry-standard method for fitting RMO involves\nscanning high-order polynomial equations. However, this analytical approach\ndoes not accurately capture local saltation, leading to low iteration\nefficiency in tomographic inversion. Supervised learning-based image\nsegmentation methods for picking can effectively capture local variations;\nhowever, they encounter challenges such as a scarcity of reliable training\nsamples and the high complexity of post-processing. To address these issues,\nthis study proposes a deep learning-based cascade picking method. It\ndistinguishes accurate and robust RMOs using a segmentation network and a\npost-processing technique based on trend regression. Additionally, a data\nsynthesis method is introduced, enabling the segmentation network to be trained\non synthetic datasets for effective picking in field data. Furthermore, a set\nof metrics is proposed to quantify the quality of automatically picked RMOs.\nExperimental results based on both model and real data demonstrate that,\ncompared to semblance-based methods, our approach achieves greater picking\ndensity and accuracy.",
        "In computational pathology, researchers often face challenges due to the\nscarcity of labeled pathology datasets. Data augmentation emerges as a crucial\ntechnique to mitigate this limitation. In this study, we introduce an efficient\ndata augmentation method for pathology images, called USegMix. Given a set of\npathology images, the proposed method generates a new, synthetic image in two\nphases. In the first phase, USegMix constructs a pool of tissue segments in an\nautomated and unsupervised manner using superpixels and the Segment Anything\nModel (SAM). In the second phase, USegMix selects a candidate segment in a\ntarget image, replaces it with a similar segment from the segment pool, and\nblends them by using a pre-trained diffusion model. In this way, USegMix can\ngenerate diverse and realistic pathology images. We rigorously evaluate the\neffectiveness of USegMix on two pathology image datasets of colorectal and\nprostate cancers. The results demonstrate improvements in cancer classification\nperformance, underscoring the substantial potential of USegMix for pathology\nimage analysis.",
        "In this paper, we study the structure of Newton polygons for compositions of\npolynomials over the rationals. We establish sufficient conditions under which\nthe successive vertices of the Newton polygon of the composition $ g(f^n(x)) $\nwith respect to a prime $ p $ can be explicitly described in terms of the\nNewton polygon of the polynomial $ g(x) $. Our results provide deeper insights\ninto how the Newton polygon of a polynomial evolves under iteration and\ncomposition, with applications to the study of dynamical irreducibility,\neventual stability, non-monogenity of tower of number fields, etc.",
        "In this paper, we investigate the ability of single-layer attention-only\ntransformers (i.e. attention layers) to memorize facts contained in databases\nfrom a linear-algebraic perspective. We associate with each database a\n3-tensor, propose the rank of this tensor as a measure of the size of the\ndatabase, and provide bounds on the rank in terms of properties of the\ndatabase. We also define a 3-tensor corresponding to an attention layer, and\nempirically demonstrate the relationship between its rank and database rank on\na dataset of toy models and random databases. By highlighting the roles played\nby the value-output and query-key weights, and the effects of argmax and\nsoftmax on rank, our results shed light on the `additive motif' of factual\nrecall in transformers, while also suggesting a way of increasing layer\ncapacity without increasing the number of parameters.",
        "Fire safety practices are important to reduce the extent of destruction\ncaused by fire. While smoke alarms help save lives, firefighters struggle with\nthe increasing number of false alarms. This paper presents a precise and\nefficient Weighted ensemble model for decreasing false alarms. It estimates the\ndensity, computes weights according to the high and low-density regions,\nforwards the high region weights to KNN and low region weights to XGBoost and\ncombines the predictions. The proposed model is effective at reducing response\ntime, increasing fire safety, and minimizing the damage that fires cause. A\nspecifically designed dataset for smoke detection is utilized to test the\nproposed model. In addition, a variety of ML models, such as Logistic\nRegression (LR), Decision Tree (DT), Random Forest (RF), Nai:ve Bayes (NB),\nK-Nearest Neighbour (KNN), Support Vector Machine (SVM), Extreme Gradient\nBoosting (XGBoost), Adaptive Boosting (ADAB), have also been utilized. To\nmaximize the use of the smoke detection dataset, all the algorithms utilize the\nSMOTE re-sampling technique. After evaluating the assessment criteria, this\npaper presents a concise summary of the comprehensive findings obtained by\ncomparing the outcomes of all models.",
        "We prove that if $2 < p < 4$, then every $n$-point subset of $\\ell_{p}$\nembeds into a Hilbert space with distortion $o(\\log n)$, i.e., with distortion\nthat is asymptotically smaller than what Bourgain's embedding theorem provides\nfor arbitrary $n$-point metric spaces. This has been previously unknown for any\n$2<p<\\infty$. We also prove that for every $2<p<\\infty$ the largest possible\nseparation modulus of an $n$-point subset of $\\ell_{p}$ is bounded from above\nand from below by positive constant multiples of $\\sqrt{\\log n}$ which may\ndepend only on $p$.",
        "Curvature regularization techniques like Sharpness Aware Minimization (SAM)\nhave shown great promise in improving generalization on vision tasks. However,\nwe find that SAM performs poorly in domains like natural language processing\n(NLP), often degrading performance -- even with twice the compute budget. We\ninvestigate the discrepancy across domains and find that in the NLP setting,\nSAM is dominated by regularization of the logit statistics -- instead of\nimproving the geometry of the function itself. We use this observation to\ndevelop an alternative algorithm we call Functional-SAM, which regularizes\ncurvature only through modification of the statistics of the overall function\nimplemented by the neural network, and avoids spurious minimization through\nlogit manipulation. Furthermore, we argue that preconditioning the SAM\nperturbation also prevents spurious minimization, and when combined with\nFunctional-SAM, it gives further improvements. Our proposed algorithms show\nimproved performance over AdamW and SAM baselines when trained for an equal\nnumber of steps, in both fixed-length and Chinchilla-style training settings,\nat various model scales (including billion-parameter scale). On the whole, our\nwork highlights the importance of more precise characterizations of sharpness\nin broadening the applicability of curvature regularization to large language\nmodels (LLMs).",
        "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nreasoning tasks, leading to their widespread deployment. However, recent\nstudies have highlighted concerning biases in these models, particularly in\ntheir handling of dialectal variations like African American English (AAE). In\nthis work, we systematically investigate dialectal disparities in LLM reasoning\ntasks. We develop an experimental framework comparing LLM performance given\nStandard American English (SAE) and AAE prompts, combining LLM-based dialect\nconversion with established linguistic analyses. We find that LLMs consistently\nproduce less accurate responses and simpler reasoning chains and explanations\nfor AAE inputs compared to equivalent SAE questions, with disparities most\npronounced in social science and humanities domains. These findings highlight\nsystematic differences in how LLMs process and reason about different language\nvarieties, raising important questions about the development and deployment of\nthese systems in our multilingual and multidialectal world. Our code repository\nis publicly available at https:\/\/github.com\/Runtaozhou\/dialect_bias_eval.",
        "We introduce MuJoCo Playground, a fully open-source framework for robot\nlearning built with MJX, with the express goal of streamlining simulation,\ntraining, and sim-to-real transfer onto robots. With a simple \"pip install\nplayground\", researchers can train policies in minutes on a single GPU.\nPlayground supports diverse robotic platforms, including quadrupeds, humanoids,\ndexterous hands, and robotic arms, enabling zero-shot sim-to-real transfer from\nboth state and pixel inputs. This is achieved through an integrated stack\ncomprising a physics engine, batch renderer, and training environments. Along\nwith video results, the entire framework is freely available at\nplayground.mujoco.org",
        "Variational inequalities play a pivotal role in a wide array of scientific\nand engineering applications. This project presents two techniques for adaptive\nmesh refinement (AMR) in the context of variational inequalities, with a\nspecific focus on the classical obstacle problem.\n  We propose two distinct AMR strategies: Variable Coefficient Elliptic\nSmoothing (VCES) and Unstructured Dilation Operator (UDO). VCES uses a nodal\nactive set indicator function as the initial iterate to a time-dependent heat\nequation problem. Solving a single step of this problem has the effect of\nsmoothing the indicator about the free boundary. We threshold this smoothed\nindicator function to identify elements near the free boundary. Key parameters\nsuch as timestep and threshold values significantly influence the efficacy of\nthis method.\n  The second strategy, UDO, focuses on the discrete identification of elements\nadjacent to the free boundary, employing a graph-based approach to mark\nneighboring elements for refinement. This technique resembles the dilation\nmorphological operation in image processing, but tailored for unstructured\nmeshes.\n  We also examine the theory of variational inequalities, the convergence\nbehavior of finite element solutions, and implementation in the Firedrake\nfinite element library. Convergence analysis reveals that accurate free\nboundary estimation is pivotal for solver performance. Numerical experiments\ndemonstrate the effectiveness of the proposed methods in dynamically enhancing\nmesh resolution around free boundaries, thereby improving the convergence rates\nand computational efficiency of variational inequality solvers. Our approach\nintegrates seamlessly with existing Firedrake numerical solvers, and it is\npromising for solving more complex free boundary problems.",
        "Quantum error correction is crucial for large-scale quantum computing, but\nthe absence of efficient decoders for new codes like quantum low-density\nparity-check (QLDPC) codes has hindered progress. Here we introduce a universal\ndecoder based on linear attention sequence modeling and graph neural network\nthat operates directly on any stabilizer code's graph structure. Our numerical\nexperiments demonstrate that this decoder outperforms specialized algorithms in\nboth accuracy and speed across diverse stabilizer codes, including surface\ncodes, color codes, and QLDPC codes. The decoder maintains linear time scaling\nwith syndrome measurements and requires no structural modifications between\ndifferent codes. For the Bivariate Bicycle code with distance 12, our approach\nachieves a 39.4% lower logical error rate than previous best decoders while\nrequiring only ~1% of the decoding time. These results provide a practical,\nuniversal solution for quantum error correction, eliminating the need for\ncode-specific decoders.",
        "Reinforcement learning (RL) enables an agent interacting with an unknown MDP\n$M$ to optimise its behaviour by observing transitions sampled from $M$. A\nnatural entity that emerges in the agent's reasoning is $\\widehat{M}$, the\nmaximum likelihood estimate of $M$ based on the observed transitions. The\nwell-known \\textit{certainty-equivalence} method (CEM) dictates that the agent\nupdate its behaviour to $\\widehat{\\pi}$, which is an optimal policy for\n$\\widehat{M}$. Not only is CEM intuitive, it has been shown to enjoy\nminimax-optimal sample complexity in some regions of the parameter space for\nPAC RL with a generative model~\\citep{Agarwal2020GenModel}.\n  A seemingly unrelated algorithm is the ``trajectory tree method''\n(TTM)~\\citep{Kearns+MN:1999}, originally developed for efficient decision-time\nplanning in large POMDPs. This paper presents a theoretical investigation that\nstems from the surprising finding that CEM may indeed be viewed as an\napplication of TTM. The qualitative benefits of this view are (1) new and\nsimple proofs of sample complexity upper bounds for CEM, in fact under a (2)\nweaker assumption on the rewards than is prevalent in the current literature.\nOur analysis applies to both non-stationary and stationary MDPs.\nQuantitatively, we obtain (3) improvements in the sample-complexity upper\nbounds for CEM both for non-stationary and stationary MDPs, in the regime that\nthe ``mistake probability'' $\\delta$ is small. Additionally, we show (4) a\nlower bound on the sample complexity for finite-horizon MDPs, which establishes\nthe minimax-optimality of our upper bound for non-stationary MDPs in the\nsmall-$\\delta$ regime.",
        "The Belin\/Ambr\\'osio Deviation (BAD) model is a widely used diagnostic tool\nfor detecting keratoconus and corneal ectasia. The input to the model is a set\nof z-score normalized $D$ indices that represent physical characteristics of\nthe cornea. Paradoxically, the output of the model, Total Deviation Value\n($D_{\\text{final}}$), is reported in standard deviations from the mean, but\n$D_{\\text{final}}$ does not behave like a z-score normalized value. Although\nthresholds like $D_{\\text{final}} \\ge 1.6$ for \"suspicious\" and\n$D_{\\text{final}} \\ge 3.0$ for \"abnormal\" are commonly cited, there is little\nexplanation on how to interpret values outside of those thresholds or to\nunderstand how they relate to physical characteristics of the cornea. This\nstudy explores the reasons for $D_{\\text{final}}$'s apparent inconsistency\nthrough a meta-analysis of published data and a more detailed statistical\nanalysis of over 1,600 Pentacam exams. The results reveal that systematic bias\nin the BAD regression model, multicollinearity among predictors, and\ninconsistencies in normative datasets contribute to the non-zero mean of\n$D_{\\text{final}}$, complicating its clinical interpretation. These findings\nhighlight critical limitations in the model's design and underscore the need\nfor recalibration to enhance its transparency and diagnostic reliability.",
        "The advent of large language models has ushered in a new era of agentic\nsystems, where artificial intelligence programs exhibit remarkable autonomous\ndecision-making capabilities across diverse domains. This paper explores\nagentic system workflows in the financial services industry. In particular, we\nbuild agentic crews that can effectively collaborate to perform complex\nmodeling and model risk management (MRM) tasks. The modeling crew consists of a\nmanager and multiple agents who perform specific tasks such as exploratory data\nanalysis, feature engineering, model selection, hyperparameter tuning, model\ntraining, model evaluation, and writing documentation. The MRM crew consists of\na manager along with specialized agents who perform tasks such as checking\ncompliance of modeling documentation, model replication, conceptual soundness,\nanalysis of outcomes, and writing documentation. We demonstrate the\neffectiveness and robustness of modeling and MRM crews by presenting a series\nof numerical examples applied to credit card fraud detection, credit card\napproval, and portfolio credit risk modeling datasets.",
        "This paper proposes a UAV-assisted forwarding system based on distributed\nbeamforming to enhance age of information (AoI) in Internet of Things (IoT).\nSpecifically, UAVs collect and relay data between sensor nodes (SNs) and the\nremote base station (BS). However, flight delays increase the AoI and degrade\nthe network performance. To mitigate this, we adopt distributed beamforming to\nextend the communication range, reduce the flight frequency and ensure the\ncontinuous data relay and efficient energy utilization. Then, we formulate an\noptimization problem to minimize AoI and UAV energy consumption, by jointly\noptimizing the UAV trajectories and communication schedules. The problem is\nnon-convex and with high dynamic, and thus we propose a deep reinforcement\nlearning (DRL)-based algorithm to solve the problem, thereby enhancing the\nstability and accelerate convergence speed. Simulation results show that the\nproposed algorithm effectively addresses the problem and outperforms other\nbenchmark algorithms.",
        "Learned image compression (LIC) using deep learning architectures has seen\nsignificant advancements, yet standard rate-distortion (R-D) optimization often\nencounters imbalanced updates due to diverse gradients of the rate and\ndistortion objectives. This imbalance can lead to suboptimal optimization,\nwhere one objective dominates, thereby reducing overall compression efficiency.\nTo address this challenge, we reformulate R-D optimization as a multi-objective\noptimization (MOO) problem and introduce two balanced R-D optimization\nstrategies that adaptively adjust gradient updates to achieve more equitable\nimprovements in both rate and distortion. The first proposed strategy utilizes\na coarse-to-fine gradient descent approach along standard R-D optimization\ntrajectories, making it particularly suitable for training LIC models from\nscratch. The second proposed strategy analytically addresses the reformulated\noptimization as a quadratic programming problem with an equality constraint,\nwhich is ideal for fine-tuning existing models. Experimental results\ndemonstrate that both proposed methods enhance the R-D performance of LIC\nmodels, achieving around a 2\\% BD-Rate reduction with acceptable additional\ntraining cost, leading to a more balanced and efficient optimization process.\nCode will be available at https:\/\/gitlab.com\/viper-purdue\/Balanced-RD.",
        "Automatic medical report generation supports clinical diagnosis, reduces the\nworkload of radiologists, and holds the promise of improving diagnosis\nconsistency. However, existing evaluation metrics primarily assess the accuracy\nof key medical information coverage in generated reports compared to\nhuman-written reports, while overlooking crucial details such as the location\nand certainty of reported abnormalities. These limitations hinder the\ncomprehensive assessment of the reliability of generated reports and pose risks\nin their selection for clinical use. Therefore, we propose a Granular\nExplainable Multi-Agent Score (GEMA-Score) in this paper, which conducts both\nobjective quantification and subjective evaluation through a large language\nmodel-based multi-agent workflow. Our GEMA-Score parses structured reports and\nemploys NER-F1 calculations through interactive exchanges of information among\nagents to assess disease diagnosis, location, severity, and uncertainty.\nAdditionally, an LLM-based scoring agent evaluates completeness, readability,\nand clinical terminology while providing explanatory feedback. Extensive\nexperiments validate that GEMA-Score achieves the highest correlation with\nhuman expert evaluations on a public dataset, demonstrating its effectiveness\nin clinical scoring (Kendall coefficient = 0.70 for Rexval dataset and Kendall\ncoefficient = 0.54 for RadEvalX dataset). The anonymous project demo is\navailable at: https:\/\/github.com\/Zhenxuan-Zhang\/GEMA_score."
      ]
    }
  },
  {
    "id":2411.17971,
    "research_type":"applied",
    "start_id":"b5",
    "start_title":"Learning Reduced-Order Models for Cardiovascular Simulations with Graph Neural Networks",
    "start_abstract":"Reduced-order models based on physics are a popular choice in cardiovascular modeling due to their efficiency, but they may experience loss in accuracy when working with anatomies that contain numerous junctions or pathological conditions. We develop one-dimensional reduced-order models that simulate blood flow dynamics using a graph neural network trained on three-dimensional hemodynamic simulation data. Given the initial condition of the system, the network iteratively predicts the pressure and flow rate at the vessel centerline nodes. Our numerical results demonstrate the accuracy and generalizability of our method in physiological geometries comprising a variety of anatomies and boundary conditions. Our findings demonstrate that our approach can achieve errors below 3% for pressure and flow rate, provided there is adequate training data. As a result, our method exhibits superior performance compared to physics-based one-dimensional models while maintaining high efficiency at inference time.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Multiscale modeling and simulation of brain blood flow"
      ],
      "abstract":[
        "The aim of this work is to present an overview recent advances in multi-scale modeling brain blood flow. In particular, we some approaches that enable the silico study and multi-physics phenomena cerebral vasculature. We discuss formulation continuum atomistic approaches, a consistent framework for their concurrent coupling, list challenges one needs overcome achieving seamless scalable integration heterogeneous numerical solvers. effectiveness proposed demonstrated realistic case involving thrombus formation process taking place on wall patient-specific aneurysm. This highlights ability algorithms resolve important biophysical processes span several spatial temporal scales, potentially yielding new insight into key aspects flow health disease. Finally, open questions emerging topics future research."
      ],
      "categories":[
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "Geometric immunosuppression in CAR-T cell treatment: Insights from\n  mathematical modeling",
        "A tumor-immune model of chronic myeloid leukemia with optimal\n  immunotherapeutic protocols",
        "Unveiling sex dimorphism in the healthy cardiac anatomy: fundamental\n  differences between male and female heart shapes",
        "Tumor microenvironment (Part I): Tissue integrity in a rat model of\n  peripheral neural cancer",
        "Photodynamic, UV-curable and fibre-forming polyvinyl alcohol derivative\n  with broad processability and staining-free antibacterial capability",
        "Artificial Neural Networks for Magnetoencephalography: A review of an\n  emerging field",
        "Simplified model of immunotherapy for glioblastoma multiforme: cancer\n  stem cells hypothesis perspective",
        "Effect of a new type of healthy and live food supplement on osteoporosis\n  blood parameters and induced rheumatoid arthritis in Wistar rats",
        "Bridging high resolution sub-cellular imaging with physiologically\n  relevant engineered tissues",
        "Causal Spike Timing Dependent Plasticity Prevents Assembly Fusion in\n  Recurrent Networks",
        "Practical parameter identifiability of respiratory mechanics in the\n  extremely preterm infant",
        "Advanced 3D-Printed Multiphasic Scaffold with Optimal PRP Dosage for\n  Chondrogenesis of BM-MSCs in Osteochondral Tissue Engineering",
        "Integrating anatomy and electrophysiology in the healthy human heart:\n  Insights from biventricular statistical shape analysis using universal\n  coordinates",
        "Fundamental Trade-off Between Computation and Communication in Private\n  Coded Distributed Computing",
        "Reinforcement-Learning Portfolio Allocation with Dynamic Embedding of\n  Market Information",
        "Adaptive Moment Estimation Optimization Algorithm Using Projection\n  Gradient for Deep Learning",
        "Completely Integrable Foliations: Singular Locus, Invariant Curves and\n  Topological Counterparts",
        "Enhanced Derivative-Free Optimization Using Adaptive Correlation-Induced\n  Finite Difference Estimators",
        "Efficient Framework for Solving Plasma Waves with Arbitrary\n  Distributions",
        "Scalable skewed Bayesian inference for latent Gaussian models",
        "Signal-to-noise ratio aware minimax analysis of sparse linear regression",
        "Unveiling the Dynamics and Genesis of Small-scale Fine Structure Loops\n  in the Lower Solar Atmosphere",
        "Data mining the functional architecture of the brain's circuitry",
        "LuxNAS: A Coherent Photonic Neural Network Powered by Neural\n  Architecture Search",
        "Single spin asymmetry in forward $pA$ collisions from Pomeron-Odderon\n  interference",
        "Keldysh field theory approach to direct electric and thermoelectric\n  currents in quantum dots coupled to superconducting leads",
        "Quantum-enhanced quickest change detection of transmission loss",
        "Alpha-element abundance patterns in star-forming regions of the local\n  Universe"
      ],
      "abstract":[
        "Chimeric antigen receptor T (CAR-T) cell therapy has emerged as a promising\ntreatment for hematological malignancies, offering a targeted approach to\ncancer treatment. Understanding the complexities of CAR-T cell therapy within\nsolid tumors poses challenges due to the intricate interactions within the\ntumor microenvironment. Mathematical modeling may serve as a valuable tool to\nunravel the dynamics of CAR-T cell therapy and improve its effectiveness in\nsolid tumors. This study aimed to investigate the impact of spatial aspects in\nCAR-T therapy of solid tumors, utilizing cellular automata for modeling\npurposes. Our main objective was to deepen our understanding of treatment\neffects by analyzing scenarios with different spatial distributions and varying\nthe initial quantities of tumor and CAR-T cells. Tumor geometry significantly\ninfluenced treatment efficacy in-silico, with notable differences observed\nbetween tumors with block-like arrangements and those with sparse cell\ndistributions, leading to the concept of immune suppression due to geometrical\neffects. This research delves into the intricate relationship between spatial\ndynamics and the effectiveness of CAR-T therapy in solid tumors, highlighting\nthe relevance of tumor geometry in the outcome of cellular immunotherapy\ntreatments. Our results provide a basis for improving the efficacy of CAR-T\ncell treatments by combining them with other ones reducing the density of\ncompact tumor areas and thus opening access ways for tumor killing T-cells.",
        "The interactions between tumor cells and the immune system play a crucial\nrole in cancer evolution. In this study, we explore how these interactions\ninfluence cancer progression by modeling the relationships among naive T cells,\neffector T cells, and chronic myeloid leukemia cells. We examine the existence\nof equilibria, the asymptotic stability of the positive steady state, and the\nglobal stability of the tumor-free equilibrium. Additionally, we develop a\npartial differential equation to describe the conditions under which the\nconcentration of cancer cells reaches a level that allows for effective control\nof cancer evolution. Finally, we apply our proposed model to investigate\noptimal treatment strategies that aim to minimize both the concentration of\ncancer cells at the end of treatment and the accumulation of tumor burden, as\nwell as the cost associated with treatment during the intervention period. Our\nstudy reveals an optimal therapeutic protocol using optimal control theory. We\nperform numerical simulations to illustrate our theoretical results and to\nexplore the dynamic behavior of the system and optimal therapeutic protocols.\nThe simulations indicate that the optimal treatment strategy can be more\neffective than a constant treatment approach, even when applying the same\ntreatment interval and total drug input.",
        "Sex-based differences in cardiovascular disease are well documented, yet the\nprecise nature and extent of these discrepancies in cardiac anatomy remain\nincompletely understood. Traditional scaling models often fail to capture the\ninterplay of age, blood pressure, and body size, prompting a more nuanced\ninvestigation. Here, we employ statistical shape modeling in a healthy subset\n(n=456) of the UK Biobank to explore sex-specific variations in biventricular\nanatomy. We reconstruct 3D meshes and perform multivariate analyses of shape\ncoefficients, controlling for age, blood pressure, and various body size\nmetrics. Our findings reveal that sex alone explains at least 25 percent of\nmorphological variability, with strong discrimination between men and women\n(AUC=0.96-0.71) persisting even after correction for confounders. Notably, the\nmost discriminative modes highlight pronounced differences in cardiac chamber\nvolumes, the anterior-posterior width of the right ventricle, and the relative\npositioning of the cardiac chambers. These results underscore that sex has a\nfundamental influence on cardiac morphology, which may have important clinical\nimplications for differing cardiac structural assessments in men and women.\nFuture work should investigate how these anatomical differences manifest in\nvarious cardiovascular conditions, ultimately paving the way for more precise\nrisk stratification and personalized therapeutic strategies for both men and\nwomen.",
        "ICAM-1 (intercellular adhesion molecule 1) and MPZ (myelin protein zero) are\nthought to be a factor in the integrity of nerve tissues. In this report, we\nattempted to trace the expression of ICAM-1, responsible for cell-to-cell\nadhesion, and of MPZ, the main constituent of myelin sheath, in malignant\ntissues of the sciatic nerve (SN) in inbred male Copenhagen rats. AT-1 Cells\n(anaplastic tumor 1) were injected in the perineurial sheath, and tissues of\nthe SNs were collected after 7, 14 and 21 days and compared to a sham-operated\ngroup of rats (n = 6 each). Tissues were sectioned and histologically examined,\nunder light microscope, and stained for measuring the immunoreactivity of\nICAM-1 and MPZ under laser scanning microscope. The cancer model was\nestablished, and the tumor growth was confirmed. ICAM-1 showed severe\ndecreases, proportional to the growing anaplastic cells, as compared to the\nsham group. MPZ revealed, however, a distinct defensive pattern before\nsubstantially decreasing in a comparison with sham. These results support the\nnotion that malignancies damage peripheral nerves and cause severe axonal\ninjury and loss of neuronal integrity, and clearly define the role of ICAM-1\nand MPZ in safeguarding the nerve tissues.",
        "Antimicrobial photodynamic therapy (APDT) is a promising antibiotic-free\nstrategy for broad-spectrum infection control in chronic wounds, minimising\nbacterial resistance risks. However, rapid photosensitiser diffusion, tissue\nstaining, side toxicity, and short-lived antimicrobial effects present\nsignificant clinical limitations for integrating APDT into wound dressings. To\naddress these challenges, we present the design of a bespoke polyvinyl alcohol\n(PVA) derivative conjugated with both phenothiazine and methacrylate\nfunctionalities, enabling staining-free antibacterial photodynamic effects,\ncellular tolerability and processability into various wound dressing formats,\nincluding films, textile fibres and nanoscale coatings. Tosylation of PVA is\nleveraged for the covalent coupling of toluidine blue, as confirmed by UV-Vis\nspectroscopy and the minimal release of TB in vitro. UV-induced network\nformation is exploited to accomplish cast films and nanoscale integrated wound\ndressing coatings. UV curing is also successfully coupled with an in-house wet\nspinning process to realise individual, water-insoluble fibres as the building\nblocks of fibrous wound dressings. A fluorometric assay supports the generation\nof reactive oxygen species when the UV-cured samples are exposed to work, but\nnot UV, light, yielding a mean log10 reduction of up to 2.13 in S. aureus, and\nthe complete eradication of P. aeruginosa. Direct and extract cytotoxicity\ntests with UV-cured films and fibres demonstrate the viability of L929\nfibroblasts following 60-min light irradiation and 72-hour cell culture. The\nbespoke molecular architecture, broad processability and cellular tolerability\nof this PVA derivative are highly attractive aiming to integrate durable\nstaining-free photodynamic capability in a wide range of healthcare\ntechnologies, from chronic wound dressings up to minimally invasive localised\ntherapy.",
        "Magnetoencephalography (MEG) is a cutting-edge neuroimaging technique that\nmeasures the intricate brain dynamics underlying cognitive processes with an\nunparalleled combination of high temporal and spatial precision. MEG data\nanalytics has always relied on advanced signal processing and mathematical and\nstatistical tools for various tasks ranging from data cleaning to probing the\nsignals' rich dynamics and estimating the neural sources underlying the\nsurface-level recordings. Like in most domains, the surge in Artificial\nIntelligence (AI) has led to the increased use of Machine Learning (ML) methods\nfor MEG data classification. More recently, an emerging trend in this field is\nusing Artificial Neural Networks (ANNs) to address many MEG-related tasks. This\nreview provides a comprehensive overview of how ANNs are being used with MEG\ndata from three vantage points: First, we review work that employs ANNs for MEG\nsignal classification, i.e., for brain decoding. Second, we report on work that\nhas used ANNs as putative models of information processing in the human brain.\nFinally, we examine studies that use ANNs as techniques to tackle\nmethodological questions in MEG, including artifact correction and source\nestimation. Furthermore, we assess the current strengths and limitations of\nusing ANNs with MEG and discuss future challenges and opportunities in this\nfield. Finally, by establishing a detailed portrait of the field and providing\npractical recommendations for the future, this review seeks to provide a\nhelpful reference for both seasoned MEG researchers and newcomers to the field\nwho are interested in using ANNs to enhance the exploration of the complex\ndynamics of the human brain with MEG.",
        "Despite ongoing efforts in cancer research, a fully effective treatment for\nglioblastoma multiforme (GBM) is still unknown. Since adoptive cell transfer\nimmunotherapy is one of the potential cure candidates, efforts have been made\nto assess its effectiveness using mathematical modeling. In this paper, we\nconsider a model of GBM immunotherapy proposed by Abernathy and Burke (2016),\nwhich also takes into account the dynamics of cancer stem cells, i.e., the type\nof cancer cells that are hypothesized to be largely responsible for cancer\nrecurrence. We modify the initial ODE system by applying simplifying\nassumptions and analyze the existence and stability of steady states of the\nobtained simplified model depending on the treatment levels.",
        "Summary Osteoporosis is a skeletal disorder, characterized by a decrease in\nbone strength and puts the individual at risk for fracture. On the other hand,\nrheumatoid arthritis is a systemic disease of unknown etiology that causes\ninflammation of the joints of the organs. Purpose Due to the destructive\neffects of these diseases and its increasing prevalence and lack of appropriate\nmedication for treatment, the present study aimed to evaluate the therapeutic\neffect of a new type of healthy and live food supplement on rheumatoid\narthritis and induced osteoporosis in rats. Methods In this research, healthy\nand live food powder were synthesized by a new and green route. This organic\nbiomaterial was named NBS. The NBS food supplement had various vitamins, macro\nand micro molecules, and ingredients. The new healthy and nutritious diet\nshowed that the use of this supplement led to the return of the parameters to\nnormal levels. Results The concentration of 12.5 mg\/ kg showed the least\ntherapeutic effect and 50 mg\/ kg had the highest therapeutic effect for\nosteoporosis. The results of blood parameters involved in inflammation in both\nhealthy and patient groups showed that the use of complete adjuvant induction\ncauses joint inflammation. In the study of the interaction of the\nconcentrations, it was observed that the concentration of 50 mg\/ kg had the\nhighest therapeutic effect against the disease in the studied mice. Conclusion\nThe results showed that the new healthy and viable supplement restores the\nblood osteoporotic and rheumatoid factors of the mice to normal.",
        "While high-resolution microscopic techniques are crucial for studying\ncellular structures in cell biology, obtaining such images from thick 3D\nengineered tissues remains challenging. In this review, we explore advancements\nin fluorescence microscopy, alongside the use of various fluorescent probes and\nmaterial processing techniques to address these challenges. We navigate through\nthe diverse array of imaging options available in tissue engineering field,\nfrom wide field to super-resolution microscopy, so researchers can make more\ninformed decisions based on the specific tissue and cellular structures of\ninterest. Finally, we provide some recent examples of how traditional\nlimitations on obtaining high-resolution images on sub-cellular architecture\nwithin 3D tissues have been overcome by combining imaging advancements with\ninnovative tissue engineering approaches.",
        "The organization of neurons into functionally related assemblies is a\nfundamental feature of cortical networks, yet our understanding of how these\nassemblies maintain distinct identities while sharing members remains limited.\nHere we analyze how spike-timing-dependent plasticity (STDP) shapes the\nformation and stability of overlapping neuronal assemblies in recurrently\ncoupled networks of spiking neuron models. Using numerical simulations and an\nassociated mean-field theory, we demonstrate that the temporal structure of the\nSTDP rule, specifically its degree of causality, critically determines whether\nassemblies that share neurons maintain segregation or merge together after\ntraining is completed. We find that causal STDP rules, where\npotentiation\/depression occurs strictly when presynaptic spikes precede\/proceed\npostsynaptic spikes, allow assemblies to remain distinct even with substantial\noverlap in membership. This stability arises because causal STDP effectively\ncancels the symmetric correlations introduced by common inputs from shared\nneurons. In contrast, acausal STDP rules lead to assembly fusion when overlap\nexceeds a critical threshold, due to unchecked growth of common input\ncorrelations. Our results provide theoretical insight into how\nspike-timing-dependent learning rules can support distributed representation\nwhere individual neurons participate in multiple assemblies while maintaining\nfunctional specificity.",
        "The complexity of mathematical models describing respiratory mechanics has\ngrown in recent years, however, parameter identifiability of such models has\nonly been studied in the last decade in the context of observable data. This\nstudy investigates parameter identifiability of a nonlinear respiratory\nmechanics model tuned to the physiology of an extremely preterm infant, using\nglobal Morris screening, local deterministic sensitivity analysis, and singular\nvalue decomposition-based subset selection. The model predicts airflow and\ndynamic pulmonary volumes and pressures under varying levels of continuous\npositive airway pressure, and a range of parameters characterizing both\nsurfactant-treated and surfactant-deficient lung. Sensitivity analyses\nindicated eleven parameters influence model outputs over the range of\ncontinuous positive airway pressure and lung health scenarios. The model was\nadapted to data from a spontaneously breathing 1 kg infant using gradient-based\noptimization to estimate the parameter subset characterizing the patient's\nstate of health.",
        "In osteochondral tissue engineering (OCTE), simultaneously regenerating\nsubchondral bone and cartilage tissue presents a significant challenge.\nMultiphasic scaffolds were created and manufactured using 3D printing to\naddress this issue. Excellent interfacial mechanical properties and\nbiocompatibility enhance the growth and chondrogenic differentiation of bone\nmarrow mesenchymal stem cells (BM-MSCs). The subchondral bone bottom layer is\nmimicked by incorporating varying concentrations of graphene oxide (GO) (0%,\n1%, and 2% w\/v) into a bioink composed of alginate (Alg) and gelatin (Gel).\nBased on evaluations of mechanical and biocompatibility properties, 1% GO is\nselected for further studies. Subsequently, the GO concentration is kept\nconstant while varying the platelet-rich plasma (PRP) dosage in the multiphasic\nscaffolds. Different PRP dosages (0%, 1%, 2%, and 3% w\/v) are integrated into\nthe Alg-Gel bioink to simulate cartilage tissues. Results indicate that\n3D-printed scaffolds containing 1% or 2% PRP exhibit favorable biomechanical\nproperties, with no significant differences observed. However, BM-MSCs exposed\nto 2% PRP demonstrate enhanced adhesion, growth, and viability. Additionally,\nreal-time PCR and Alcian blue staining confirm increased chondrogenic\nexpression and glycosaminoglycans (GAGs) synthesis. This work highlights the\npromising potential of 3D-printed multiphasic frameworks in the development of\nOCTE.",
        "A cardiac digital twin is a virtual replica of a patient-specific heart,\nmimicking its anatomy and physiology. A crucial step of building a cardiac\ndigital twin is anatomical twinning, where the computational mesh of the\ndigital twin is tailored to the patient-specific cardiac anatomy. In a number\nof studies, the effect of anatomical variation on clinically relevant\nfunctional measurements like electrocardiograms (ECGs) is investigated, using\ncomputational simulations. While such a simulation environment provides\nresearchers with a carefully controlled ground truth, the impact of anatomical\ndifferences on functional measurements in real-world patients remains\nunderstudied. In this study, we develop a biventricular statistical shape model\nand use it to quantify the effect of biventricular anatomy on ECG-derived and\ndemographic features, providing novel insights for the development of digital\ntwins of cardiac electrophysiology. To this end, a dataset comprising\nhigh-resolution cardiac CT scans from 271 healthy individuals, including\nathletes, is utilized. Furthermore, a novel, universal, ventricular\ncoordinate-based method is developed to establish lightweight shape\ncorrespondence. The performance of the shape model is rigorously established,\nfocusing on its dimensionality reduction capabilities and the training data\nrequirements. Additionally, a comprehensive synthetic cohort is made available,\nfeaturing ready-to-use biventricular meshes with fiber structures and\nanatomical region annotations. These meshes are well-suited for\nelectrophysiological simulations.",
        "Distributed computing enables scalable machine learning by distributing tasks\nacross multiple nodes, but ensuring privacy in such systems remains a\nchallenge. This paper introduces a private coded distributed computing model\nthat integrates privacy constraints to keep task assignments hidden. By\nleveraging placement delivery arrays (PDAs), we design an extended PDA\nframework to characterize achievable computation and communication loads under\nprivacy constraints. By constructing two classes of extended PDAs, we explore\nthe trade-offs between computation and communication, showing that although\nprivacy increases communication overhead, it can be significantly alleviated\nthrough optimized PDA-based coded strategies.",
        "We develop a portfolio allocation framework that leverages deep learning\ntechniques to address challenges arising from high-dimensional, non-stationary,\nand low-signal-to-noise market information. Our approach includes a dynamic\nembedding method that reduces the non-stationary, high-dimensional state space\ninto a lower-dimensional representation. We design a reinforcement learning\n(RL) framework that integrates generative autoencoders and online meta-learning\nto dynamically embed market information, enabling the RL agent to focus on the\nmost impactful parts of the state space for portfolio allocation decisions.\nEmpirical analysis based on the top 500 U.S. stocks demonstrates that our\nframework outperforms common portfolio benchmarks and the predict-then-optimize\n(PTO) approach using machine learning, particularly during periods of market\nstress. Traditional factor models do not fully explain this superior\nperformance. The framework's ability to time volatility reduces its market\nexposure during turbulent times. Ablation studies confirm the robustness of\nthis performance across various reinforcement learning algorithms.\nAdditionally, the embedding and meta-learning techniques effectively manage the\ncomplexities of high-dimensional, noisy, and non-stationary financial data,\nenhancing both portfolio performance and risk management.",
        "Training deep neural networks is challenging. To accelerate training and\nenhance performance, we propose PadamP, a novel optimization algorithm. PadamP\nis derived by applying the adaptive estimation of the p-th power of the\nsecond-order moments under scale invariance, enhancing projection adaptability\nby modifying the projection discrimination condition. It is integrated into\nAdam-type algorithms, accelerating training, boosting performance, and\nimproving generalization in deep learning. Combining projected gradient\nbenefits with adaptive moment estimation, PadamP tackles unconstrained\nnon-convex problems. Convergence for the non-convex case is analyzed, focusing\non the decoupling of first-order moment estimation coefficients and\nsecond-order moment estimation coefficients. Unlike prior work relying on , our\nproof generalizes the convergence theorem, enhancing practicality. Experiments\nusing VGG-16 and ResNet-18 on CIFAR-10 and CIFAR-100 show PadamP's\neffectiveness, with notable performance on CIFAR-10\/100, especially for VGG-16.\nThe results demonstrate that PadamP outperforms existing algorithms in terms of\nconvergence speed and generalization ability, making it a valuable addition to\nthe field of deep learning optimization.",
        "We study codimension $q \\geq 2$ holomorphic foliations defined in a\nneighborhood of a point $P$ of a complex manifold that are completely\nintegrable, i.e. with $q$ independent meromorphic first integrals. We show that\neither $P$ is a regular point, a non-isolated singularity or there are\ninfinitely many invariant analytic varieties through $P$ of the same dimension\nas the foliation, the so called separatrices. Moreover, we see that this\nphenomenon is of topological nature.\n  Indeed, we introduce topological counterparts of completely integrable local\nholomorphic foliations and tools, specially the concept of total holonomy\ngroup, to build holomorphic first integrals if they have isolated separatrices.\nAs a result, we provide a topological characterization of completely integrable\nnon-degenerated elementary isolated singularities of vector fields with an\nisolated separatrix.",
        "Gradient-based methods are well-suited for derivative-free optimization\n(DFO), where finite-difference (FD) estimates are commonly used as gradient\nsurrogates. Traditional stochastic approximation methods, such as\nKiefer-Wolfowitz (KW) and simultaneous perturbation stochastic approximation\n(SPSA), typically utilize only two samples per iteration, resulting in\nimprecise gradient estimates and necessitating diminishing step sizes for\nconvergence. In this paper, we first explore an efficient FD estimate, referred\nto as correlation-induced FD estimate, which is a batch-based estimate. Then,\nwe propose an adaptive sampling strategy that dynamically determines the batch\nsize at each iteration. By combining these two components, we develop an\nalgorithm designed to enhance DFO in terms of both gradient estimation\nefficiency and sample efficiency. Furthermore, we establish the consistency of\nour proposed algorithm and demonstrate that, despite using a batch of samples\nper iteration, it achieves the same convergence rate as the KW and SPSA\nmethods. Additionally, we propose a novel stochastic line search technique to\nadaptively tune the step size in practice. Finally, comprehensive numerical\nexperiments confirm the superior empirical performance of the proposed\nalgorithm.",
        "Plasma, which constitutes 99\\% of the visible matter in the universe, is\ncharacterized by a wide range of waves and instabilities that play a pivotal\nrole in space physics, astrophysics, laser-plasma interactions, fusion\nresearch, and laboratory experiments. The linear physics of these phenomena is\ndescribed by kinetic dispersion relations (KDR). However, solving KDRs for\narbitrary velocity distributions remains a significant challenge, particularly\nfor non-Maxwellian distributions frequently observed in various plasma\nenvironments. This work introduces a novel, efficient, and unified numerical\nframework to address this challenge. The proposed method rapidly and accurately\nyields all significant solutions of KDRs for nearly arbitrary velocity\ndistributions, supporting both unstable and damped modes across all frequencies\nand wavevectors. The approach expands plasma species' velocity distribution\nfunctions using a series of carefully chosen orthogonal basis functions and\nemploys a highly accurate rational approximation to transform the problem into\nan equivalent matrix eigenvalue problem, eliminating the need for initial\nguesses. The efficiency and versatility of this framework are demonstrated,\nenabling simplified studies of plasma waves with arbitrary distributions. This\nadvancement paves the way for uncovering new physics in natural plasma\nenvironments, such as spacecraft observations in space plasmas, and\napplications like wave heating in fusion research.",
        "Approximate Bayesian inference for the class of latent Gaussian models can be\nachieved efficiently with integrated nested Laplace approximations (INLA).\nBased on recent reformulations in the INLA methodology, we propose a further\nextension that is necessary in some cases like heavy-tailed likelihoods or\nbinary regression with imbalanced data. This extension formulates a skewed\nversion of the Laplace method such that some marginals are skewed and some are\nkept Gaussian while the dependence is maintained with the Gaussian copula from\nthe Laplace method. Our approach is formulated to be scalable in model and data\nsize, using a variational inferential framework enveloped in INLA. We\nillustrate the necessity and performance using simulated cases, as well as a\ncase study of a rare disease where class imbalance is naturally present.",
        "We consider parameter estimation under sparse linear regression -- an\nextensively studied problem in high-dimensional statistics and compressed\nsensing. While the minimax framework has been one of the most fundamental\napproaches for studying statistical optimality in this problem, we identify two\nimportant issues that the existing minimax analyses face: (i) The\nsignal-to-noise ratio appears to have no effect on the minimax optimality,\nwhile it shows a major impact in numerical simulations. (ii) Estimators such as\nbest subset selection and Lasso are shown to be minimax optimal, yet they\nexhibit significantly different performances in simulations. In this paper, we\ntackle the two issues by employing a minimax framework that accounts for\nvariations in the signal-to-noise ratio (SNR), termed the SNR-aware minimax\nframework. We adopt a delicate higher-order asymptotic analysis technique to\nobtain the SNR-aware minimax risk. Our theoretical findings determine three\ndistinct SNR regimes: low-SNR, medium-SNR, and high-SNR, wherein minimax\noptimal estimators exhibit markedly different behaviors. The new theory not\nonly offers much better elaborations for empirical results, but also brings new\ninsights to the estimation of sparse signals in noisy data.",
        "Recent high-resolution solar observations have unveiled the presence of\nsmall-scale loop-like structures in the lower solar atmosphere, often referred\nto as unresolved fine structures, low-lying loops, and miniature hot loops.\nThese structures undergo rapid changes within minutes, and their formation\nmechanism has remained elusive. In this study, we conducted a comprehensive\nanalysis of two small loops utilizing data from the Interface Region Imaging\nSpectrograph (IRIS), the Goode Solar Telescope (GST) at Big Bear Solar\nObservatory, and the Atmospheric Imaging Assembly (AIA) and the Helioseismic\nMagnetic Imager (HMI) onboard the Solar Dynamics Observatory (SDO), aiming to\nelucidate the underlying process behind their formation. The GST observations\nrevealed that these loops, with lengths of $\\sim$3.5 Mm and heights of $\\sim$1\nMm, manifest as bright emission structures in H$\\alpha$ wing images,\nparticularly prominent in the red wing. IRIS observations showcased these loops\nin 1330 angstrom slit-jaw images, with TR and chromospheric line spectra\nexhibiting significant enhancement and broadening above the loops, indicative\nof plasmoid-mediated reconnection during their formation. Additionally, we\nobserved upward-erupting jets above these loops across various passbands.\nFurthermore, differential emission measurement analysis reveals an enhanced\nemission measure at the location of these loops, suggesting the presence of\nplasma exceeding 1 MK. Based on our observations, we propose that these loops\nand associated jets align with the minifilament eruption model. Our findings\nsuggest a unified mechanism governing the formation of small-scale loops and\njets akin to larger-scale X-ray jets.",
        "The brain is a highly complex organ consisting of a myriad of subsystems that\nflexibly interact and adapt over time and context to enable perception,\ncognition, and behavior. Understanding the multi-scale nature of the brain,\ni.e., how circuit- and moleclular-level interactions build up the fundamental\ncomponents of brain function, holds incredible potential for developing\ninterventions for neurodegenerative and psychiatric diseases, as well as open\nnew understanding into our very nature. Historically technological limitations\nhave forced systems neuroscience to be local in anatomy (localized, small\nneural populations in single brain areas), in behavior (studying single tasks),\nin time (focusing on specific stages of learning or development), and in\nmodality (focusing on imaging single biological quantities). New developments\nin neural recording technology and behavioral monitoring now provide the data\nneeded to break free of local neuroscience to global neuroscience: i.e.,\nunderstanding how the brain's many subsystem interact, adapt, and change across\nthe multitude of behaviors animals and humans must perform to thrive.\nSpecifically, while we have much knowledge of the anatomical architecture of\nthe brain (i.e., the hardware), we finally are approaching the data needed to\nfind the functional architecture and discover the fundamental properties of the\nsoftware that runs on the hardware. We must take this opportunity to bridge\nbetween the vast amounts of data to discover this functional architecture which\nwill face numerous challenges from low-level data alignment up to high level\nquestions of interpretable mathematical models of behavior that can synthesize\nthe myriad of datasets together.",
        "We demonstrate a novel coherent photonic neural network using tunable\nphase-change-material-based couplers and neural architecture search. Compared\nto the MZI-based Clements network, our results indicate 85% reduction in the\nnetwork footprint while maintaining the accuracy.",
        "Working in the hybrid framework of the high energy $pA$ collisions we\nidentify a new contribution to transverse single spin asymmetry (SSA). The\nphase necessary for the SSA is provided by the Pomeron-Odderon interference in\nthe dense nuclear target. The complete formula for the $pA \\to h X$ polarized\ncross section also contains the transversity distribution for the polarized\nprojectile as well as the real part of the twist-3 fragmentation function. We\nnumerically estimate the asymmetry $A_N$ and its nuclear dependence. Based on a\nmodel computation we find that $A_N$ can be a percent level in the forward and\nlow-$P_{h\\perp}$ region. For large nuclei we find significant suppression, with\n$A_N \\propto A^{-7\/6}$ parametrically. As a notable feature we find a node of\n$A_N$ as a function of the $P_{h\\perp}$ around the values of the initial\nsaturation scale that could be used to test this mechanism experimentally.",
        "We study the transport properties of a quantum dot contacted to two\nsuperconducting reservoirs by means of the Keldysh field theory approach. We\ndetermine the direct current occurring at equilibrium and the electric and\nthermoelectric currents triggered when the system is driven out of equilibrium\nby a voltage or a temperature bias, also for a normal-quantum\ndot-superconductor junction. In particular, we derive and present for the first\ntime the explicit expression of the thermoelectric current in a\nsuperconductor-quantum dot-superconductor junction for any values of the\ntemperature difference between the superconducting leads. We show that in the\nlinear response regime, in addition to the Josephson current, a weakly\nphase-dependent thermoelectric contribution occurs, providing that\nelectron-hole symmetry is broken. Far from linearity, instead, other\ncontributions arise which lead to thermoelectric effects, dominant at weak\ncoupling, also in the presence of particle-hole symmetry.",
        "A sudden increase of loss in an optical communications channel can be caused\nby a malicious wiretapper, or for a benign reason such as inclement weather in\na free-space channel or an unintentional bend in an optical fiber. We show that\nadding a small amount of squeezing to bright phase-modulated coherent-state\npulses can dramatically increase the homodyne detection receiver's sensitivity\nto change detection in channel loss, without affecting the communications rate.\nWe further show that augmenting blocks of $n$ pulses of a coherent-state\ncodeword with weak continuous-variable entanglement generated by splitting\nsqueezed vacuum pulses in a temporal $n$-mode equal splitter progressively\nenhances this change-detection sensitivity as $n$ increases; the aforesaid\nsqueezed-light augmentation being the $n=1$ special case. For $n$ high enough,\nan arbitrarily small amount of quantum-augmented photons per pulse diminishes\nthe change-detection latency by the inverse of the pre-detection channel loss.\nThis superadditivity-like phenomenon in the entanglement-augmented relative\nentropy rate, which quantifies the latency of change-point detection, may find\nother uses. We discuss the quantum limit of quickest change detection and a\nreceiver that achieves it, tradeoffs between continuous and discrete-variable\nquantum augmentation, and the broad problem of joint classical-and-quantum\ncommunications and channel-change-detection that our study opens up.",
        "(Abridged) We reassess the alpha-element abundance ratios (Ne\/O, S\/O, Ar\/O)\nwith respect to metallicity in ~1000 spectra of Galactic and extragalactic HII\nregions and star-forming galaxies (SFGs) of the local Universe. Using the DEep\nSpectra of Ionised REgions Database (DESIRED) Extended project (DESIRED-E),\nwhich includes spectra with direct electron temperature determinations, we\nhomogeneously derive physical conditions and chemical abundances for all\nobjects. Various ionisation correction factor (ICF) schemes are analyzed for\nNe, S, and Ar to identify the most reliable abundance estimates. Our findings\nindicate that the ICF scheme by Izotov et al. (2006) better reproduces the\nNe\/O, S\/O, and Ar\/O trends. Ne\/O ratios in HII regions display large dispersion\nand no clear dependence on O\/H, suggesting that current ICF(Ne) schemes fail\nfor these objects. However, SFGs show consistent linear relations with slightly\npositive slopes for log(Ne\/O) vs. 12+log(O\/H) or 12+log(Ne\/H), likely\ninfluenced by metallicity-dependent O dust depletion and ICF effects. The\nlog(S\/O) vs. 12+log(O\/H) distribution is largely constant, especially for HII\nregions or combined samples (SFGs + HII regions). Conversely, log(S\/O) vs.\n12+log(S\/H) shows a tight linear fit with a positive slope, flattening at\n12+log(S\/H) < 6.0, suggesting S contributions from SNe Ia. For log(Ar\/O) vs.\n12+log(O\/H), similar trends emerge for HII regions and SFGs, independent of\nionisation degree or ICF(Ar). A slight log(Ar\/O) decrease with increasing\n12+log(O\/H) contrasts with log(Ar\/O) vs. 12+log(Ar\/H), which shows a small\npositive slope, indicating a possible minor Ar contribution from SNe Ia."
      ]
    }
  }
]