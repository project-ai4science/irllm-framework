[
  {
    "id":2411.00749,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"TransMIL: Transformer based Correlated Multiple Instance Learning for Whole Slide Image Classification",
    "start_abstract":"Multiple instance learning (MIL) is a powerful tool to solve the weakly supervised classification in whole slide image (WSI) based pathology diagnosis. However, current MIL methods are usually on independent and identical distribution hypothesis, thus neglect correlation among different instances. To address this problem, we proposed new framework, called correlated MIL, provided proof for convergence. Based devised Transformer (TransMIL), which explored both morphological spatial information. The TransMIL can effectively deal with unbalanced\/balanced binary\/multiple great visualization interpretability. We conducted various experiments three computational problems achieved better performance faster convergence compared state-of-the-art methods. test AUC binary tumor be up 93.09% over CAMELYON16 dataset. And cancer subtypes 96.03% 98.82% TCGA-NSCLC dataset TCGA-RCC dataset, respectively. Implementation available at: https:\/\/github.com\/szc19990412\/TransMIL.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "A 2021 update on cancer image analytics with deep learning"
      ],
      "abstract":[
        "Deep learning (DL)-based interpretation of medical images has reached a critical juncture of expanding outside research projects into translational ones, and is ready to make its way to the clinics. Advances over the last decade in data availability, DL techniques, as well as computing capabilities have accelerated this journey. Through this journey, today we have a better understanding of the challenges to and pitfalls of wider adoption of DL into clinical care, which, according to us, should and will drive the advances in this field in the next few years. The most important among these challenges are the lack of an appropriately digitized environment within healthcare institutions, the lack of adequate open and representative datasets on which DL algorithms can be trained and tested, and the lack of robustness of widely used DL training algorithms to certain pervasive pathological characteristics of medical images and repositories. In this review, we provide an overview of the role of imaging in oncology, the different techniques that are shaping the way DL algorithms are being made ready for clinical use, and also the problems that DL techniques still need to address before DL can find a home in clinics. Finally, we also provide a summary of how DL can potentially drive the adoption of digital pathology, vendor neutral archives, and picture archival and communication systems. We caution that the respective researchers may find the coverage of their own fields to be at a high-level. This is so by design as this format is meant to only introduce those looking in from outside of deep learning and medical research, respectively, to gain an appreciation for the main concerns and limitations of these two fields instead of telling them something new about their own."
      ],
      "categories":[
        "q-bio.CB"
      ]
    },
    "list":{
      "title":[
        "Conditional Success of Adaptive Therapy: The Role of Treatment-Holiday\n  Thresholds Revealed by Mathematical Modeling",
        "Language-specific Tonal Features Drive Speaker-Listener Neural\n  Synchronization",
        "Tumor-associated CD19$^+$ macrophages induce immunosuppressive\n  microenvironment in hepatocellular carcinoma",
        "Application of Single-cell Deep Learning in Elucidating the Mapping\n  Relationship Between Visceral and Body Surface Inflammatory Patterns",
        "A Bayesian Modelling Framework with Model Comparison for Epidemics with\n  Super-Spreading",
        "Trait-structured chemotaxis: Exploring ligand-receptor dynamics and\n  travelling wave properties in a Keller-Segel model",
        "A network-driven framework for enhancing gene-disease association\n  studies in coronary artery disease",
        "Multicellular self-organization in Escherichia coli",
        "Artificial Intelligence Approaches for Anti-Addiction Drug Discovery",
        "Intercellular contact is sufficient to drive Fibroblast to Myofibroblast\n  transitions",
        "Heuristics based on Adjacency Graph Packing for DCJ Distance Considering\n  Intergenic Regions",
        "Derivation from kinetic theory and 2-D pattern analysis of chemotaxis\n  models for Multiple Sclerosis",
        "A spatially resolved and lipid-structured model for macrophage\n  populations in early human atherosclerotic lesions",
        "Weight Distribution of the Weighted Coordinates Poset Block Space and\n  Singleton Bound",
        "Feedback-augmented Non-homogeneous Hidden Markov Models for Longitudinal\n  Causal Inference",
        "Nonlocal characteristics of two-qubit gates and their argand diagrams",
        "Stability of the Euclidean 3-ball under L2-curvature pinching",
        "Observational and Theoretical Constraints on First-Order Phase\n  Transitions in Neutron Stars",
        "Breather interactions in the integrable discrete Manakov system and\n  trigonometric Yang-Baxter maps",
        "Structure evolution with cosmic backgrounds from radio to far infrared",
        "A Family of Semi-norms in $C^*$-algebras",
        "An Iterative Block Matrix Inversion (IBMI) Algorithm for Symmetric\n  Positive Definite Matrices with Applications to Covariance Matrices",
        "Mean-Field Analysis of Latent Variable Process Models on Dynamically\n  Evolving Graphs with Feedback Effects",
        "Dirichlet problem for diffusions with jumps",
        "Remote preparation of motional Schr\\\"{o}dinger cat states via\n  dissipatively-driven non-Gaussian mechanical entanglement",
        "Frequency-resolved time lags due to X-ray disk reprocessing in AGN",
        "The rise of the galactic empire: luminosity functions at $z\\sim17$ and\n  $z\\sim25$ estimated with the MIDIS$+$NGDEEP ultra-deep JWST\/NIRCam dataset",
        "Subdomains of Post-COVID-Syndrome (PCS) -- A Population-Based Study"
      ],
      "abstract":[
        "Adaptive therapy (AT) improves cancer treatment by controlling the\ncompetition between sensitive and resistant cells through treatment holidays.\nThis study highlights the critical role of treatment-holiday thresholds in AT\nfor tumors composed of drug-sensitive and resistant cells. Using a\nLotka-Volterra model, the research compares AT with maximum tolerated dose\ntherapy and intermittent therapy, showing that AT's success largely depends on\nthe threshold at which treatment is paused and resumed, as well as on the\ncompetition between sensitive and resistant cells. Three scenarios of\ncomparison between AT and other therapies are identified: uniform-decline,\nconditional-improve, and uniform-improve, illustrating that optimizing the\ntreatment-holiday threshold is crucial for AT effectiveness. Tumor composition,\nincluding initial tumor burden and the proportion of resistant cells,\ninfluences outcomes. Adjusting threshold values enables AT to suppress\nresistant subclones, preserving sensitive cells, ultimately improving\nprogression-free survival. These findings emphasize the importance of\npersonalized treatment strategies potentially enhancing long-term therapeutic\noutcomes.",
        "Verbal communication transmits information across diverse linguistic levels,\nwith neural synchronization (NS) between speakers and listeners emerging as a\nputative mechanism underlying successful exchange. However, the specific speech\nfeatures driving this synchronization and how language-specific versus\nuniversal characteristics facilitate information transfer remain poorly\nunderstood. We developed a novel content-based interbrain encoding model to\ndisentangle the contributions of acoustic and linguistic features to\nspeaker-listener NS during Mandarin storytelling and listening, as measured via\nmagnetoencephalography (MEG). Results revealed robust NS throughout\nfrontotemporal-parietal networks with systematic time lags between speech\nproduction and perception. Crucially, suprasegmental lexical tone features\n(tone categories, pitch height, and pitch contour), essential for lexical\nmeaning in Mandarin, contributed more significantly to NS than either acoustic\nelements or universal segmental units (consonants and vowels). These tonal\nfeatures generated distinctive spatiotemporal NS patterns, creating\nlanguage-specific neural \"communication channels\" that facilitated efficient\nrepresentation sharing between interlocutors. Furthermore, the strength and\npatterns of NS driven by these language-specific features predicted\ncommunication success. These findings demonstrate the neural mechanisms\nunderlying shared representations during verbal exchange and highlight how\nlanguage-specific features can shape neural coupling to optimize information\ntransfer during human communication.",
        "Tumor-associated macrophages are a key component that contributes to the\nimmunosuppressive microenvironment in human cancers. However, therapeutic\ntargeting of macrophages has been a challenge in clinic due to the limited\nunderstanding of their heterogeneous subpopulations and distinct functions.\nHere, we identify a unique and clinically relevant CD19$^+$ subpopulation of\nmacrophages that is enriched in many types of cancer, particularly in\nhepatocellular carcinoma (HCC). The CD19$^+$ macrophages exhibit increased\nlevels of PD-L1 and CD73, enhanced mitochondrial oxidation, and compromised\nphagocytosis, indicating their immunosuppressive functions. Targeting CD19$^+$\nmacrophages with anti-CD19 chimeric antigen receptor T (CAR-T) cells inhibited\nHCC tumor growth. We identify PAX5 as a primary driver of up-regulated\nmitochondrial biogenesis in CD19$^+$ macrophages, which depletes cytoplasmic\nCa$^{2+}$, leading to lysosomal deficiency and consequent accumulation of CD73\nand PD-L1. Inhibiting CD73 or mitochondrial oxidation enhanced the efficacy of\nimmune checkpoint blockade therapy in treating HCC, suggesting great promise\nfor CD19$^+$ macrophage-targeting therapeutics.",
        "As a system of integrated homeostasis, life is susceptible to disruptions by\nvisceral inflammation, which can disturb internal environment equilibrium. The\nrole of body-spread subcutaneous fascia (scFascia) in this process is poorly\nunderstood. In the rat model of Salmonella-induced dysentery, scRNA-seq of\nscFascia and deep-learning analysis revealed Warburg-like metabolic\nreprogramming in macrophages (MPs) with reduced citrate cycle activity.\nCd34+\/Pdgfra+ telocytes (CPTCs) regulated MPs differentiation and proliferation\nvia Wnt\/Fgf signal, suggesting a pathological crosstalk pattern in the\nscFascia, herein termed the fascia-visceral inflammatory crosstalk pattern\n(FVICP). PySCENIC analysis indicated increased activity transcription factors\nFosl1, Nfkb2, and Atf4, modulated by CPTCs signaling to MPs, downregulating\naerobic respiration and upregulating cell cycle, DNA replication, and\ntranscription. This study highlights scFascia's role in immunomodulation and\nmetabolic reprogramming during visceral inflammation, underscoring its function\nin systemic homeostasis.",
        "The transmission dynamics of an epidemic are rarely homogeneous.\nSuper-spreading events and super-spreading individuals are two types of\nheterogeneous transmissibility. Inference of super-spreading is commonly\ncarried out on secondary case data, the expected distribution of which is known\nas the offspring distribution. However, this data is seldom available. Here we\nintroduce a multi-model framework fit to incidence time-series, data that is\nmuch more readily available. The framework consists of five discrete-time,\nstochastic, branching-process models of epidemics spread through a susceptible\npopulation. The framework includes a baseline model of homogeneous\ntransmission, a unimodal and a bimodal model for super-spreading events, as\nwell as a unimodal and a bimodal model for super-spreading individuals.\nBayesian statistics is used to infer model parameters using Markov Chain\nMonte-Carlo. Model comparison is conducted by computing Bayes factors, with\nimportance sampling used to estimate the marginal likelihood of each model.\nThis estimator is selected for its consistency and lower variance compared to\nalternatives. Application to simulated data from each model identifies the\ncorrect model for the majority of simulations and accurately infers the true\nparameters, such as the basic reproduction number. We also apply our methods to\nincidence data from the 2003 SARS outbreak and the Covid-19 pandemic. Model\nselection consistently identifies the same model and mechanism for a given\ndisease, even when using different time series. Our estimates are consistent\nwith previous studies based on secondary case data. Quantifying the\ncontribution of super-spreading to disease transmission has important\nimplications for infectious disease management and control. Our modelling\nframework is disease-agnostic and implemented as an R package, with potential\nto be a valuable tool for public health.",
        "A novel trait-structured Keller-Segel model that explores the dynamics of a\nmigrating cell population guided by chemotaxis in response to an external\nligand concentration is derived and analysed. Unlike traditional Keller-Segel\nmodels, this framework introduces an explicit representation of ligand-receptor\nbindings on the cell membrane, where the percentage of occupied receptors\nconstitutes the trait that influences cellular phenotype. The model posits that\nthe cell's phenotypic state directly modulates its capacity for chemotaxis and\nproliferation, governed by a trade-off due to a finite energy budget: cells\nhighly proficient in chemotaxis exhibit lower proliferation rates, while more\nproliferative cells show diminished chemotactic abilities. The model is derived\nfrom the principles of a biased random walk, resulting in a system of two\nnon-local partial differential equations, describing the densities of both\ncells and ligands. Using a Hopf-Cole transformation, we derive an equation that\ncharacterises the distribution of cellular traits within travelling wave\nsolutions for the total cell density, allowing us to uncover the monotonicity\nproperties of these waves. Numerical investigations are conducted to examine\nthe model's behaviour across various biological scenarios, providing insights\ninto the complex interplay between chemotaxis, proliferation, and phenotypic\ndiversity in migrating cell populations.",
        "Over the last decade, genome-wide association studies (GWAS) have\nsuccessfully identified numerous genetic variants associated with complex\ndiseases. These associations have the potential to reveal the molecular\nmechanisms underlying complex diseases and lead to the identification of novel\ndrug targets. Despite these advancements, the biological pathways and\nmechanisms linking genetic variants to complex diseases are still not fully\nunderstood. Most trait-associated variants reside in non-coding regions and are\npresumed to influence phenotypes through regulatory effects on gene expression.\nYet, it is often unclear which genes they regulate and in which cell types this\nregulation occurs. Transcriptome-wide association studies (TWAS) aim to bridge\nthis gap by detecting trait-associated tissue gene expression regulated by GWAS\nvariants. However, traditional TWAS approaches frequently overlook the critical\ncontributions of trans-regulatory effects and fail to integrate comprehensive\nregulatory networks. Here, we present a novel framework that leverages\ntissue-specific gene regulatory networks (GRNs) to integrate cis- and\ntrans-genetic regulatory effects into the TWAS framework for complex diseases.\nWe validate our approach using coronary artery disease (CAD), utilizing data\nfrom the STARNET project, which provides multi-tissue gene expression and\ngenetic data from around 600 living patients with cardiovascular disease.\nPreliminary results demonstrate the potential of our GRN-driven framework to\nuncover more genes and pathways that may underlie CAD. This framework extends\ntraditional TWAS methodologies by utilizing tissue-specific regulatory insights\nand advancing the understanding of complex disease genetic architecture.",
        "Escherichia coli has long been a trusty companion, maintaining health in our\nguts and advancing biological knowledge in the laboratory. In light of recent\nfindings, we discuss multicellular self-organization in E. coli and develop\ngeneral ideas for multicellularity, including the necessity for multicellular\ndynamics and interpretation by dynamic graphs, applicable to both unicellular\nand multicellular organisms. In this context, we next discuss the documented\nbehaviors of E. coli self-organization (rosette formation, multicellular\nextension, and attached dormancy) and two potential behaviors (internal\ncommunication and mating). Finally, by comparing the dynamic graphs for\ndifferent communities, we develop principles relevant to the theory of\nmulticellularity.",
        "Drug addiction is a complex and pervasive global challenge that continues to\npose significant public health concerns. Traditional approaches to\nanti-addiction drug discovery have struggled to deliver effective therapeutics,\nfacing high attrition rates, long development timelines, and inefficiencies in\nprocessing large-scale data. Artificial intelligence (AI) has emerged as a\ntransformative solution to address these issues. Using advanced algorithms, AI\nis revolutionizing drug discovery by enhancing the speed and precision of key\nprocesses. This review explores the transformative role of AI in the pipeline\nfor anti-addiction drug discovery, including data collection, target\nidentification, and compound optimization. By highlighting the potential of AI\nto overcome traditional barriers, this review systematically examines how AI\naddresses critical gaps in anti-addiction research, emphasizing its potential\nto revolutionize drug discovery and development, overcome challenges, and\nadvance more effective therapeutic strategies.",
        "Fibroblast cells play a key role in maintaining the extracellular matrix.\nDuring wound healing, fibroblasts differentiate into highly contractile\nmyofibroblasts, which secrete extracellular matrix proteins like collagen to\nfacilitate tissue repair. Under normal conditions, myofibroblasts undergo\nprogrammed cell death after healing to prevent excessive scar formation.\nHowever, in diseases like fibrosis, the myofibroblasts remain active even after\nthe wound is closed, resulting in excessive collagen buildup and a stiff,\nfibrotic matrix. The reasons for the persistence of myofibroblasts in fibrosis\nare not well understood. Here we show the existence of a mechanism where direct\nphysical contact between a fibroblast and a myofibroblast is sufficient for\nfibroblasts to transition into myofibroblasts. We show that\nfibroblast-myofibroblast transition can occur even in the absence of known\nbiochemical cues such as growth factor activation or mechanical cues from a\nstiff, fibrotic matrix. Further, we show that contact-based\nfibroblast-myofibroblast activation can be blocked by G{\\alpha}q\/11\/14\ninhibitor FR9003591, which inhibits the formation of myofibroblasts. These\nfindings offer new insights into the persistence of fibrosis despite\ntherapeutic interventions and suggest a potential strategy to target\nfibroblast-to-myofibroblast transition in fibrosis.",
        "In this work, we explore heuristics for the Adjacency Graph Packing problem,\nwhich can be applied to the Double Cut and Join (DCJ) Distance Problem. The DCJ\nis a rearrangement operation and the distance problem considering it is a well\nestablished method for genome comparison. Our heuristics will use the structure\ncalled adjacency graph adapted to include information about intergenic regions,\nmultiple copies of genes in the genomes, and multiple circular or linear\nchromosomes. The only required property from the genomes is that it must be\npossible to turn one into the other with DCJ operations. We propose one greedy\nheuristic and one heuristic based on Genetic Algorithms. Our experimental tests\nin artificial genomes show that the use of heuristics is capable of finding\ngood results that are superior to a simpler random strategy.",
        "In this paper, a class of reaction-diffusion equations for Multiple Sclerosis\nis presented. These models are derived by means of a diffusive limit starting\nfrom a proper kinetic description, taking account of the underlying microscopic\ninteractions among cells. At the macroscopic level, we discuss the necessary\nconditions for Turing instability phenomena and the formation of\ntwo-dimensional patterns, whose shape and stability are investigated by means\nof a weakly nonlinear analysis. Some numerical simulations, confirming and\nextending theoretical results, are proposed for a specific scenario.",
        "Atherosclerosis is a chronic inflammatory disease of the artery wall. The\nearly stages of atherosclerosis are driven by interactions between lipids and\nmonocyte-derived-macrophages (MDMs). The mechanisms that govern the spatial\ndistribution of lipids and MDMs in the lesion remain poorly understood. In this\npaper, we develop a spatially-resolved and lipid-structured model for early\natherosclerosis. The model development and analysis are guided by images of\nhuman coronary lesions by Nakashima et al. 2007. Consistent with their\nfindings, the model predicts that lipid initially accumulates deep in the\nintima due to a spatially non-uniform LDL retention capacity. The model also\nqualitatively reproduces the global internal maxima in the Nakashima images\nonly when the MDM mobility is sufficiently sensitive to lipid content, and MDM\nlifespan sufficiently insensitive. Introducing lipid content-dependence to MDM\nmobility and mean lifespan produced minimal impact on model behaviour at early\ntimes, but strongly impacted lesion composition at steady state. Increases to\nthe sensitivity of MDM lifespan to lipid content yield lesions with fewer MDMs,\nless total lesion lipid content and reduced mean MDM infiltration depth.\nIncreases to the sensitivity of MDM mobility to lipid content also reduces the\nMDM infiltration depth, but increases the proportion of lipid-laden MDMs. We\nfind that MDM lipid content increases with spatial depth, regardless of blood\nLDL and HDL content. These results shed light on the mechanisms that drive\nspatial variation in the composition of early atherosclerotic lesions, and the\nrole of macrophage lipid content in disease progression.",
        "In this paper, we determine the complete weight distribution of the space $\n\\mathbb{F}_q^N $ endowed by the weighted coordinates poset block metric\n($(P,w,\\pi)$-metric), also known as the $(P,w,\\pi)$-space, thereby obtaining it\nfor $(P,w)$-space, $(P,\\pi)$-space, $\\pi$-space, and $P$-space as special\ncases. Further, when $P$ is a chain, the resulting space is called as\nNiederreiter-Rosenbloom-Tsfasman (NRT) weighted block space and when $P$ is\nhierarchical, the resulting space is called as weighted coordinates\nhierarchical poset block space. The complete weight distribution of both the\nspaces are deduced from the main result. Moreover, we define an $I$-ball for an\nideal $I$ in $P$ and study the characteristics of it in $(P,w,\\pi)$-space.\n  We investigate the relationship between the $I$-perfect codes and $t$-perfect\ncodes in $(P,w,\\pi)$-space. Given an ideal $I$, we investigate how the maximum\ndistance separability (MDS) is related with $I$-perfect codes and $t$-perfect\ncodes in $(P,w,\\pi)$-space. Duality theorem is derived for an MDS\n$(P,w,\\pi)$-code when all the blocks are of same length. Finally, the\ndistribution of codewords among $r$-balls is analyzed in the case of chain\nposet, when all the blocks are of same length.",
        "Hidden Markov models are widely used for modeling sequential data but\ntypically have limited applicability in observational causal inference due to\ntheir strong conditional independence assumptions. I introduce\nfeedback-augmented non-homogeneous hidden Markov model (FAN-HMM), which\nincorporate time-varying covariates and feedback mechanisms from past\nobservations to latent states and future responses. Integrating these models\nwith the structural causal model framework allows flexible causal inference in\nlongitudinal data with time-varying unobserved heterogeneity and multiple\ncausal pathways. I show how, in a common case of categorical response\nvariables, long-term causal effects can be estimated efficiently without the\nneed for simulating counterfactual trajectories. Using simulation experiments,\nI study the performance of FAN-HMM under the common misspecification of the\nnumber of latent states, and finally apply the proposed approach to estimate\nthe effect of the 2013 parental leave reform on fathers' paternal leave uptake\nin Finnish workplaces.",
        "In this paper, we show the usefulness of the chords present in the argand\ndiagram of squared eigenvalues of nonlocal part of two-qubit gates to study\ntheir nonlocal characteristics. We discuss the criteria for perfect entanglers\nto transform a pair of orthonormal product states into a pair of orthonormal\nmaximally entangled states. Perfect entanglers with a chord passing through\norigin can do such a transformation. In the Weyl chamber, we identify the\nregions of perfect entanglers with at least one chord passing through origin.\nWe also provide the conditions for a perfect entangler without any chord\npassing through origin to transform a pair of orthonormal product states into\northonormal maximally entangled states. Finally, we show that similar to\nentangling power, gate typicality can also be described using the chords\npresent in the argand diagram. For each chord describing the entangling power,\nthere exists a chord describing the gate typicality. We show the geometrical\nrelation between the two sets of chords.",
        "In this article, we consider compact Riemannian 3-manifolds with boundary. We\nprove that if the $L^2$-norm of the curvature is small and if the\n$H^{1\/2}$-norm of the difference of the fundamental forms of the boundary is\nsmall, then the manifold is diffeomorphic to the Euclidean ball. Moreover, we\nobtain that the manifold and the ball are metrically close (uniformly and in\n$H^2$-norm), with a quantitative, optimal bound. The required smallness\nassumption only depends on the volumes of the manifold and its boundary and on\na trace and Sobolev constant of the manifold. The proof only relies on\nelementary computations based on the Bochner formula for harmonic functions and\ntensors, and on the 2-spheres effective uniformisation result of\nKlainerman-Szeftel.",
        "Understanding the equation of state (EOS) of neutron stars (NSs) is a\nfundamental challenge in astrophysics and nuclear physics. A first-order phase\ntransition (FOPT) at high densities could lead to the formation of a quark\ncore, significantly affecting NS properties. This review explores observational\nand theoretical constraints on such transitions using multi-messenger\nastrophysics. X-ray observations, including mass-radius measurements from NICER\nand spectral features like quasi-periodic oscillations (QPOs) and cyclotron\nresonance scattering features (CRSFs), provide indirect evidence of EOS\nmodifications. Gravitational wave detections, particularly from binary NS\nmergers such as GW170817, constrain tidal deformability and post-merger\noscillations, which may carry signatures of phase transitions. Pulsar timing\noffers additional constraints through measurements of mass, spin evolution, and\nglitches, with millisecond pulsars exceeding twice the solar mass posing\nchallenges to purely hadronic EOSs. Theoretical models and numerical\nsimulations predict that an FOPT could impact gravitational wave signals,\ntwin-star configurations, and NS cooling. Future advancements, including\nnext-generation gravitational wave detectors, high-precision X-ray telescopes,\nand improved theoretical modeling, will enhance our ability to probe phase\ntransitions in NSs. A combination of these approaches will provide crucial\ninsights into the existence and properties of deconfined quark matter in NS\ninteriors.",
        "The goal of this work is to obtain a complete characterization of soliton and\nbreather interactions in the integrable discrete Manakov (IDM) system, a vector\ngeneralization of the Ablowitz-Ladik model. The IDM system, which in the\ncontinuous limit reduces to the Manakov system (i.e., a 2-component vector\nnonlinear Schrodinger equation), was shown to admit a variety of discrete\nvector soliton solutions: fundamental solitons, fundamental breathers, and\ncomposite breathers. While the interaction of fundamental solitons was studied\nearly on, no results are presently available for other types of\nsoliton-breather and breather-breather interactions. Our study reveals that\nupon interacting with a fundamental breather, a fundamental soliton becomes a\nfundamental breather. Conversely, the interaction of two fundamental breathers\ngenerically yields two fundamental breathers with polarization shifts, but may\nalso result in a fundamental soliton and a fundamental breather. Composite\nbreathers interact trivially both with each other and with a fundamental\nsoliton or breather. Explicit formulas for the scattering coefficients that\ncharacterize fundamental and composite breathers are given. This allows us to\ninterpret the interactions in terms of a refactorization problem and derive the\nassociated Yang-Baxter maps describing the effect of interactions on the\npolarizations. These give the first examples of parametric Yang-Baxter maps of\ntrigonometric type.",
        "Cosmic background radiation, both diffuse and discrete in nature, produced at\ndifferent cosmic epochs before and after recombination, provides key\ninformation on the evolution of cosmic structures. We discuss the main classes\nof sources that contribute to the extragalactic background light from radio to\nsub-millimetre wavelenghs and the currently open question on the level of the\ncosmic radio background spectrum. The redshifted 21cm line signal from\ncosmological neutral Hydrogen during the primeval phases of cosmic structures\nas a probe of the cosmological reionisation process is presented, along with\nthe route for confident detection of this signal. We then describe the basic\nformalism and the feasibility to study via a differential approach, based\nmainly on dipole analysis, the tiny imprints in the CB spectrum expected from a\nvariety of cosmological and astrophysical processes at work during the early\nphases of cosmic perturbation and structure evolution. Finally, we discuss the\nidentification of high-redshift sub-millimetre lensed galaxies with extreme\nmagnifications in the Planck maps and their use for the comprehension of\nfundamental processes in early galaxy formation and evolution.",
        "We introduce a new family of non-negative real-valued functions on a\n$C^*$-algebra $\\mathcal{A}$, i.e., for $0\\leq \\mu \\leq 1,$\n$$\\|a\\|_{\\sigma_{\\mu}}= \\text{sup}\\left\\lbrace \\sqrt{|f(a)|^2 \\sigma_{\\mu}\nf(a^*a)}: f\\in \\mathcal{A}', \\, f(1)=\\|f\\|=1 \\right\\rbrace, \\quad $$ where\n$a\\in \\mathcal{A}$ and $\\sigma_{\\mu}$ is an interpolation path of the symmetric\nmean $\\sigma$. These functions are semi-norms as they satisfy the norm axioms,\nexcept for the triangle inequality. Special cases satisfying triangle\ninequality, and a complete equality characterization is also discussed. Various\nbounds and relationships will be established for this new family, with a\nconnection to the existing literature in the algebra of all bounded linear\noperators on a Hilbert space.",
        "Obtaining the inverse of a large symmetric positive definite matrix\n$\\mathcal{A}\\in\\mathbb{R}^{p\\times p}$ is a continual challenge across many\nmathematical disciplines. The computational complexity associated with direct\nmethods can be prohibitively expensive, making it infeasible to compute the\ninverse. In this paper, we present a novel iterative algorithm (IBMI), which is\ndesigned to approximate the inverse of a large, dense, symmetric positive\ndefinite matrix. The matrix is first partitioned into blocks, and an iterative\nprocess using block matrix inversion is repeated until the matrix approximation\nreaches a satisfactory level of accuracy. We demonstrate that the two-block,\nnon-overlapping approach converges for any positive definite matrix, while\nnumerical results provide strong evidence that the multi-block, overlapping\napproach also converges for such matrices.",
        "In this paper, we study the asymptotic behavior of a class of dynamic\nco-evolving latent space networks. The model we study is subject to\nbi-directional feedback effects, meaning that at any given time, the latent\nprocess depends on its own value and the graph structure at the previous time\nstep, and the graph structure at the current time depends on the value of the\nlatent processes at the current time but also on the graph structure at the\nprevious time instance (sometimes called a persistence effect). We construct\nthe mean-field limit of this model, which we use to characterize the limiting\nbehavior of a random sample taken from the latent space network in the limit as\nthe number of nodes in the network diverges. From this limiting model, we can\nderive the limiting behavior of the empirical measure of the latent process and\nestablish the related graphon limit of the latent particle network process. We\nalso provide a description of the rich conditional probabilistic structure of\nthe limiting model. The inherent dependence structure complicates the\nmathematical analysis significantly. In the process of proving our main\nresults, we derive a general conditional propagation of chaos result, which is\nof independent interest. In addition, our novel approach of studying the\nlimiting behavior of random samples proves to be a very useful methodology for\nfully grasping the asymptotic behavior of co-evolving particle systems.\nNumerical results are included to illustrate the theoretical findings.",
        "In this paper, we study Dirichlet problem for non-local operator on bounded\ndomains in ${\\mathbb R}^d$\n  $$\n  {\\cal L}u = {\\rm div}(A(x) \\nabla (x)) + b(x) \\cdot \\nabla u(x)\n  + \\int_{{\\mathbb R}^d} (u(y)-u(x) ) J(x, dy) , $$ where\n$A(x)=(a_{ij}(x))_{1\\leq i,j\\leq d}$ is a measurable $d\\times d$ matrix-valued\nfunction on ${\\mathbb R}^d$ that is uniformly elliptic and bounded, $b$ is an\n${\\mathbb R}^d$-valued function so that $|b|^2$ is in some Kato class ${\\mathbb\nK}_d$, for each $x\\in {\\mathbb R}^d$, $J(x, dy)$ is a finite measure on\n${\\mathbb R}^d$ so that $x\\mapsto J(x, {\\mathbb R}^d)$ is in the Kato class\n${\\mathbb K}_d$. We show there is a unique Feller process $X$ having strong\nFeller property associated with ${\\cal L}$, which can be obtained from the\ndiffusion process having generator $ {\\rm div}(A(x) \\nabla (x)) + b(x) \\cdot\n\\nabla u(x) $ through redistribution. We further show that for any bounded\nconnected open subset $D\\subset{\\mathbb R}^d$ that is regular with respect to\nthe Laplace operator $\\Delta$ and for any bounded continuous function $\\varphi\n$ on $D^c$, the Dirichlet problem ${\\cal L} u=0$ in $D$ with $u=\\varphi$ on\n$D^c$ has a unique bounded continuous weak solution on ${\\mathbb R}^d$. This\nunique weak solution can be represented in terms of the Feller process\nassociated with ${\\cal L}$.",
        "In this paper, we propose a driven-dissipative scheme for generating\nnon-Gaussian mechanical entangled states and remotely preparing mechanical\nSchr\\\"{o}dinger cat states via the entanglement. The system under study\nconsists of a cavity optomechanical setup with two frequency-mismatched\nmechanical oscillators coupled to a cavity field driven by a bichromatic pump.\nWe show that under proper conditions, an effective Hamiltonian for\nnondegenerate parametric downconversion involving the two mechanical\noscillators and the cavity field can be engineered. We demonstrate analytically\nand numerically that the cavity dissipation drives the mechanical oscillators\ninto a steady-state pair-coherent state. The no-Gaussianity and nonclassical\nproperties, including Winger negativity, entanglement and quantum steering, of\nthe achieved non-Gaussian mechanical state are investigated in detail. We\nfurther show that homodyne detection on one mechanical oscillator enables the\nremote generation of Schr\\\"{o}dinger cat states in the other oscillator through\nthe non-Gaussian mechanical entanglement. As we show, this detection can be\nimplemented by transferring the mechanical state to the output field of an\nauxiliary probe cavity coupled to the target oscillator, followed by homodyne\ndetection on the output field. We also discuss the robustness of the mechanical\nentangled states and cat states against thermal fluctuations. Our findings\nestablish a feasible approach for the dissipative and remote preparation of\nmechanical nonclassical states.",
        "Over the last years, a number of broadband reverberation mapping campaigns\nhave been conducted to explore the short-term UV and optical variability of\nnearby AGN. Despite the extensive data collected, the origin of the observed\nvariability is still debated in the literature. Frequency-resolved time lags\noffer a promising approach to distinguish between different scenarios, as they\nprobe variability on different time scales. In this study, we present the\nexpected frequency-resolved lags resulting from X-ray reprocessing in the\naccretion disk. The predicted lags are found to feature a general shape that\nresembles that of observational measurements, while exhibiting strong\ndependence on various physical parameters. Additionally, we compare our model\npredictions to observational data for the case of NGC 5548, concluding that the\nX-ray illumination of the disk can effectively account for the observed\nfrequency-resolved lags and power spectra in a self-consistent way. To date,\nX-ray disk reprocessing is the only physical model that has successfully\nreproduced the observed multi-wavelength variability, in both amplitude and\ntime delays, across a range of temporal frequencies.",
        "We present a sample of six F200W and three F277W dropout sources identified\nas $16<z<25$ galaxy candidates based on the deepest JWST\/NIRCam data to date,\nprovided by the MIRI Deep Imaging Survey (MIDIS) and the Next Generation Deep\nExtragalactic Exploratory Public survey (NGDEEP), reaching 5$\\sigma$ depths of\n$\\sim31.5$ mag (AB) at $\\geq2$ $\\mu$m. We estimate ultraviolet (UV) luminosity\nfunctions and densities at $z\\sim17$ and $z\\sim25$. We find that the number\ndensity of galaxies with absolute magnitudes $-19<M_\\mathrm{UV}<-18$ (AB) at\n$z\\sim17$ ($z\\sim25$) is a factor of 4 (25) smaller than at $z\\sim12$; a\nsimilar evolution is observed for the luminosity density. Compared to\nstate-of-the-art galaxy simulations, we find the need for an enhanced UV-photon\nproduction at $z=17-25$ in $\\mathrm{M}_\\mathrm{DM}=10^{8.5-9.5}$ M$_\\odot$ dark\nmatter halos, maybe provided by an increase in the star formation efficiency at\nearly times and\/or by intense bursts fed by very low metallicity or primordial\ngas. There are few robust theoretical predictions for the evolution of galaxies\nabove $z\\sim20$ in the literature, however, the continuing rapid drop in the\nhalo mass function suggests more rapid evolution than we observe if photon\nproduction efficiencies remained constant. Our $z>16$ galaxy candidates present\nmass-weighted ages around 30 Myr, and attenuations $\\mathrm{A(V)}<0.1$ mag.\nTheir average stellar mass is\n$\\mathrm{M}_\\bigstar\\sim10^{7}\\,\\mathrm{M}_\\odot$, implying a star formation\nefficiency (stellar-to-baryon mass fraction) around 10%. We find three galaxies\nwith very blue UV spectral slopes ($\\beta\\sim-3$) compatible with low\nmetallicity or Pop III and young ($\\lesssim10$ Myr) stars and\/or high escape\nfractions of ionizing photons, the rest presenting slopes $\\beta\\sim-2.5$\nsimilar to $z=10-12$ samples.",
        "Post-COVID Syndrome (PCS), encompassing the multifaceted sequelae of\nCOVID-19, can be severity-graded using a score comprising 12 different\nlong-term symptom complexes. Acute COVID-19 severity and individual resilience\nwere previously identified as key predictors of this score. This study\nvalidated these predictors and examined their relationship to PCS symptom\ncomplexes, using an expanded dataset (n=3,372) from the COVIDOM cohort study.\nClassification and Regression Tree (CART) analysis resolved the detailed\nrelationship between the predictors and the constituting symptom complexes of\nthe PCS score. Among newly recruited COVIDOM participants (n=1,930), the PCS\nscore was again found to be associated with both its putative predictors. Of\nthe score-constituting symptom complexes, neurological symptoms, sleep\ndisturbance, and fatigue were predicted by individual resilience, whereas acute\ndisease severity predicted exercise intolerance, chemosensory deficits, joint\nor muscle pain, signs of infection, and fatigue. These associations inspired\nthe definition of two novel PCS scores that included the above-mentioned\nsubsets of symptom complexes only. Both novel scores were inversely correlated\nwith quality of life, measured by the EQ-5D-5L index. The newly defined scores\nmay enhance the assessment of PCS severity, both in a research context and to\ndelineate distinct PCS subdomains with different therapeutic and interventional\nneeds in clinical practise."
      ]
    }
  },
  {
    "id":2411.00758,
    "research_type":"basic",
    "start_id":"b6",
    "start_title":"Introduction to Nonimaging Optics, second edition",
    "start_abstract":"Introduction to Nonimaging Optics covers the theoretical foundations and design methods of nonimaging optics, as well as key concepts from related fields. This fully updated, revised, and expanded Second Edition: \u2022 Features a new and intuitive introduction with a basic description of the advantages of nonimaging optics \u2022 Adds new chapters on wavefronts for a prescribed output (irradiance or intensity), infinitesimal \u00e9tendue optics (generalization of the aplanatic optics), and K\u00f6hler optics and color mixing \u2022 Incorporates new material on the simultaneous multiple surface (SMS) design method in 3-D, integral invariants, and \u00e9tendue 2-D \u2022 Contains 21 chapters, 24 fully worked and several other examples, and 1,000+ illustrations, including photos of real devices \u2022 Addresses applications ranging from solar energy concentration to illumination engineering Introduction to Nonimaging Optics, Second Edition invites newcomers to explore the growing field of nonimaging optics, while providing seasoned veterans with an extensive reference book.",
    "start_categories":[
      "physics.optics"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b18"
      ],
      "title":[
        "Inverse methods for illumination optics"
      ],
      "abstract":[
        "\u2022 A submitted manuscript is the version of the article upon submission and before peer-review. There can be important differences between the submitted version and the official published version of record. People interested in the research are advised to contact the author for the final version of the publication, or visit the DOI to the publisher's website. \u2022 The final author version and the galley proof are versions of the publication after peer review. \u2022 The final published version features the final layout of the paper including the volume, issue and page numbers."
      ],
      "categories":[
        "math.MP"
      ]
    },
    "list":{
      "title":[
        "On the local Maxwellians solving the Boltzmann equation with boundary\n  condition",
        "Ill-posedness of the pure-noise Dean-Kawasaki equation",
        "Langevin Bi-fidelity Importance Sampling for Failure Probability\n  Estimation",
        "Exponential stability for an infinite memory wave equation with\n  frictional damping and logarithmic nonlinear terms",
        "A Multi-Objective Portfolio of Portfolios Problem with Qualitative\n  Performance Assessments",
        "Statistical inference for Levy-driven graph supOU processes: From short-\n  to long-memory in high-dimensional time series",
        "Nonradial stability of self-similar blowup to Keller-Segel equation in\n  three dimensions",
        "Local intersection cohomology of varieties of complexes",
        "A lower bound on the Ulrich complexity of hypersurfaces",
        "Association measures for two-way contingency tables based on\n  multi-categorical proportional reduction in error",
        "Stein's unbiased risk estimate and Hyv\\\"arinen's score matching",
        "Stochastic very weak solution to parabolic equations with singular\n  coefficients",
        "On Bezdek's conjecture for high-dimensional convex bodies with an\n  aligned center of symmetry",
        "Increasing the Energy-Efficiency of Wearables Using Low-Precision Posit\n  Arithmetic with PHEE",
        "Coherent Local Explanations for Mathematical Optimization",
        "Transformers with Joint Tokens and Local-Global Attention for Efficient\n  Human Pose Estimation",
        "Imperfect Knowledge Management (IKM) in GEFRED (GENeralized model for\n  Fuzzy RElational Databases)",
        "An Interpretable Neural Control Network with Adaptable Online Learning\n  for Sample Efficient Robot Locomotion Learning",
        "We Need to Effectively Integrate Computing Skills Across Discipline\n  Curricula",
        "Sweeping Orders for Simplicial Complex Reconstruction",
        "Rapid and Inexpensive Inertia Tensor Estimation from a Single Object\n  Throw",
        "Fractional Correspondence Framework in Detection Transformer",
        "Modelling Capillary Rise with a Slip Boundary Condition: Well-posedness\n  and Long-time Dynamics of Solutions to Washburn's Equation",
        "A comprehensive study of bound-states for the nonlinear Schr\\\"odinger\n  equation on single-knot metric graphs",
        "A precise asymptotic analysis of learning diffusion models: theory and\n  insights",
        "BoKDiff: Best-of-K Diffusion Alignment for Target-Specific 3D Molecule\n  Generation",
        "TMI-CLNet: Triple-Modal Interaction Network for Chronic Liver Disease\n  Prognosis From Imaging, Clinical, and Radiomic Data Fusion",
        "Constructing self-similar subsets within the fractal support of Lacunary\n  Wavelet Series for their multifractal analysis"
      ],
      "abstract":[
        "We derive the expressions of the local Maxwellians that solve the Boltzmann\nequation in the interior of an open domain. We determine which of these local\nMaxwellians satisfy the Boltzmann equation in a regular domain with boundary,\nwithout assuming the boundedness of the domain. We investigate separately, on\nthe one hand, the case of the bounce-back boundary condition in any dimension,\nand on the other hand the case of the specular reflection boundary condition,\nin dimension $d = 2$ and $d = 3$. In the case of the bounce-back boundary\ncondition, we prove that the only local Maxwellians solving the Boltzmann\nequation with boundary condition are the global Maxwellians. In the case of the\nspecular reflection, we provide a complete classification of the domains for\nwhich only the global Maxwellians solve the Boltzmann equation with boundary\ncondition, and we describe all the local Maxwellians that solve the equation\nfor the domains presenting symmetries.",
        "We prove that the Dean-Kawasaki-type stochastic partial differential equation\n$$\\partial \\rho= \\nabla\\cdot (\\sqrt{\\rho\\,}\\, \\xi) + \\nabla\\cdot \\left(\\rho\\,\nH(\\rho)\\right)$$ with vector-valued space-time white noise $\\xi$, does not\nadmit solutions for any initial measure and any vector-valued bounded\nmeasurable function $H$ on the space of measures. This applies in particular to\nthe pure-noise Dean-Kawasaki equation ($H\\equiv 0$). The result is sharp, in\nthe sense that solutions are known to exist for some unbounded $H$.",
        "Estimating failure probability is one of the key tasks in the field of\nuncertainty quantification. In this domain, importance sampling has proven to\nbe an effective estimation strategy; however, its efficiency heavily depends on\nthe choice of the biasing distribution. An improperly selected biasing\ndistribution can significantly increase estimation error. One way to solve this\nproblem is to leverage a less expensive, lower-fidelity surrogate. Building on\nthe accessibility to such a model and its derivative on the random uncertain\ninputs, we introduce an importance-sampling-based estimator, termed the\nLangevin bi-fidelity importance sampling (L-BF-IS), which uses\nscore-function-based sampling algorithms to generate new samples and\nsubstantially reduces the mean square error (MSE) of failure probability\nestimation. The proposed method demonstrates lower estimation error, especially\nin high-dimensional ($\\geq 100$) input spaces and when limited high-fidelity\nevaluations are available. The L-BF-IS estimator's effectiveness is validated\nthrough experiments with two synthetic functions and two real-world\napplications governed by partial differential equations. These real-world\napplications involve a composite beam, which is represented using a simplified\nEuler-Bernoulli equation as a low-fidelity surrogate, and a steady-state\nstochastic heat equation, for which a pre-trained neural operator serves as the\nlow-fidelity surrogate.",
        "This article is concerned with the energy decay of an infinite memory wave\nequation with a logarithmic nonlinear term and a frictional damping term. The\nproblem is formulated in a bounded domain in $\\mathbb R^d$ ($d\\ge3$) with a\nsmooth boundary, on which we prescribe a mixed boundary condition of the\nDirichlet and the acoustic types. We establish an exponential decay result for\nthe energy with a general material density $\\rho(x)$ under certain assumptions\non the involved coefficients. The proof is based on a contradiction argument,\nthe multiplier method and some microlocal analysis techniques. In addition, if\n$\\rho(x)$ takes a special form, our result even holds without the damping\neffect, that is, the infinite memory effect alone is strong enough to guarantee\nthe exponential stability of the system.",
        "We present a multi-objective portfolio decision model that involves selecting\nboth a portfolio of projects and a set of elements to allocate to each project.\nOur model includes a defined set of objectives to optimize, with projects\ncontributing to these objectives in various ways. The elements included in the\nportfolios are assessed based on both qualitative and quantitative criteria.\nProjects can only be selected for the portfolio if they meet specific\nrequirements defined by threshold values on the criteria. The model is\nadaptable to include temporal considerations and stochastic, making it suitable\nfor a wide range of real-life applications. To manage the decision-making\nprocess, we employ an interactive multi-objective method that integrates the\nselection of both portfolios and elements. After making initial selections, we\nask the decision-maker to evaluate the portfolios, from which we derive a\nseries of rules to be incorporated into the multiobjective model until the\ndecision-maker is satisfied. We illustrate the functionality of our model\nthrough an illustrative case study.",
        "This article introduces Levy-driven graph supOU processes, offering a\nparsimonious parametrisation for high-dimensional time-series, where\ndependencies between the individual components are governed via a graph\nstructure. Specifically, we propose a model specification that allows for a\nsmooth transition between short- and long-memory settings while accommodating a\nwide range of marginal distributions.\n  We further develop an inference procedure based on the generalised method of\nmoments, establish its asymptotic properties and demonstrate its strong finite\nsample performance through a simulation study.\n  Finally, we illustrate the practical relevance of our new model and\nestimation method in an empirical study of wind capacity factors in an European\nelectricity network context.",
        "In three dimensions, the parabolic-elliptic Keller-Segel system exhibits a\nrich variety of singularity formations. Notably, it admits an explicit\nself-similar blow-up solution whose radial stability, conjectured more than two\ndecades ago in [Brenner-Constantin-Kadanoff-Schenkel-Venkataramani, 1999], was\nrecently confirmed by [Glogi\\'c-Sch\\\"orkhuber, 2024]. This paper aims to extend\nthe radial stability to the nonradial setting, building on the\nfinite-codimensional stability analysis in our previous work [Li-Zhou, 2024].\nThe main input is the mode stability of the linearized operator, whose nonlocal\nnature presents challenges for the spectral analysis. Besides a quantitative\nperturbative analysis for the high spherical classes, we adapted in the first\nspherical class the wave operator method of [Li-Wei-Zhang, 2020] for the fluid\nstability to localize the operator and remove the known unstable mode\nsimultaneously. Our method provides localization beyond the partial mass\nvariable and is independent of the explicit formula of the profile, so it\npotentially sheds light on other linear nonlocal problems.",
        "We compute the local intersection cohomology of the irreducible components of\nvarieties of complexes, by using Lusztig's geometric approach to quantum groups\nand explicit constructions of elements of Lusztig's canonical bases.",
        "We give a lower bound on the Ulrich complexity of hypersurfaces of dimension\n$n \\ge 6$.",
        "In two-way contingency tables under an asymmetric situation, where the row\nand column variables are defined as explanatory and response variables\nrespectively, quantifying the extent to which the explanatory variable\ncontributes to predicting the response variable is important. One\nquantification method is the association measure, which indicates the degree of\nassociation in a range from $0$ to $1$. Among various measures, those based on\nproportional reduction in error (PRE) are particularly notable for their\nsimplicity and intuitive interpretation. These measures, including\nGoodman-Kruskal's lambda proposed in 1954, are widely implemented in\nstatistical software such as R and SAS and remain extensively used. However, a\nknown limitation of PRE measures is their potential to return a value of $0$\ndespite no independence. This issue arises because the measures are constructed\nbased solely on the maximum joint and marginal probabilities, failing to\nutilize the information available in the contingency table fully. To address\nthis problem, we propose new association measures designed for the proportional\nreduction in error with multiple categories. The properties of the proposed\nmeasure are examined and their utility is demonstrated through simulations and\nreal data analyses. The results suggest their potential as practical tools in\napplied statistics.",
        "We study two G-modeling strategies for estimating the signal distribution\n(the empirical Bayesian's prior) from observations corrupted with normal noise.\nFirst, we choose the signal distribution by minimizing Stein's unbiased risk\nestimate (SURE) of the implied Eddington\/Tweedie Bayes denoiser, an approach\nmotivated by optimal empirical Bayesian shrinkage estimation of the signals.\nSecond, we select the signal distribution by minimizing Hyv\\\"arinen's score\nmatching objective for the implied score (derivative of log-marginal density),\ntargeting minimal Fisher divergence between estimated and true marginal\ndensities. While these strategies appear distinct, they are known to be\nmathematically equivalent. We provide a unified analysis of SURE and score\nmatching under both well-specified signal distribution classes and\nmisspecification. In the classical well-specified setting with homoscedastic\nnoise and compactly supported signal distribution, we establish nearly\nparametric rates of convergence of the empirical Bayes regret and the Fisher\ndivergence. In a commonly studied misspecified model, we establish fast rates\nof convergence to the oracle denoiser and corresponding oracle inequalities.\nOur empirical results demonstrate competitiveness with nonparametric maximum\nlikelihood in well-specified settings, while showing superior performance under\nmisspecification, particularly in settings involving heteroscedasticity and\nside information.",
        "A class of stochastic parabolic equations with singular potentials is\nanalysed in the chaos expansion setting where the Wick product is used to give\nsense to the product of generalized stochastic processes. For the analysis of\nsuch equations we combine the chaos expansion method from the white noise\nanalysis and the concept of very weak solutions from the theory of partial\ndifferential equations. The stochastic very weak solution to the stochastic\nparabolic evolution problem is defined and its existence and uniqueness are\nshown. For regular enough potentials and data we prove consistency of\nstochastic very weak solution with a stochastic weak solution. We give an\nexample to illustrate the method and possible applications.",
        "In 1999, K. Bezdek posed a conjecture stating that among all convex bodies in\n$\\mathbb R^3$, ellipsoids and bodies of revolution are characterized by the\nfact that all their planar sections have an axis of reflection. We prove\nBezdek's conjecture in arbitrary dimension $n\\geq 3$, assuming only that\nsections passing through a fixed point have an axis of reflection, provided\nthat the complementary invariant subspaces are all parallel to a fixed\nhyperplane. The result is proven in both orthogonal and affine settings.",
        "Wearable biomedical devices are increasingly being used for continuous\npatient health monitoring, enabling real-time insights and extended data\ncollection without the need for prolonged hospital stays. These devices must be\nenergy efficient to minimize battery size, improve comfort, and reduce\nrecharging intervals. This paper investigates the use of specialized\nlow-precision arithmetic formats to enhance the energy efficiency of biomedical\nwearables. Specifically, we explore posit arithmetic, a floating-point-like\nrepresentation, in two key applications: cough detection for chronic cough\nmonitoring and R peak detection in ECG analysis. Simulations reveal that 16-bit\nposits can replace 32-bit IEEE 754 floating point numbers with minimal accuracy\nloss in cough detection. For R peak detection, posit arithmetic achieves\nsatisfactory accuracy with as few as 10 or 8 bits, compared to the 16-bit\nrequirement for floating-point formats. To further this exploration, we\nintroduce PHEE, a modular and extensible architecture that integrates the\nCoprosit posit coprocessor within a RISC-V-based system. Using the X-HEEP\nframework, PHEE seamlessly incorporates posit arithmetic, demonstrating reduced\nhardware area and power consumption compared to a floating-point counterpart\nsystem. Post-synthesis results targeting 16nm TSMC technology show that the\nposit hardware targeting these biomedical applications can be 38% smaller and\nconsume up to 54% less energy at the functional unit level, with no performance\ncompromise. These findings establish the potential of low-precision posit\narithmetic to significantly improve the energy efficiency of wearable\nbiomedical devices.",
        "The surge of explainable artificial intelligence methods seeks to enhance\ntransparency and explainability in machine learning models. At the same time,\nthere is a growing demand for explaining decisions taken through complex\nalgorithms used in mathematical optimization. However, current explanation\nmethods do not take into account the structure of the underlying optimization\nproblem, leading to unreliable outcomes. In response to this need, we introduce\nCoherent Local Explanations for Mathematical Optimization (CLEMO). CLEMO\nprovides explanations for multiple components of optimization models, the\nobjective value and decision variables, which are coherent with the underlying\nmodel structure. Our sampling-based procedure can provide explanations for the\nbehavior of exact and heuristic solution algorithms. The effectiveness of CLEMO\nis illustrated by experiments for the shortest path problem, the knapsack\nproblem, and the vehicle routing problem.",
        "Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) have led\nto significant progress in 2D body pose estimation. However, achieving a good\nbalance between accuracy, efficiency, and robustness remains a challenge. For\ninstance, CNNs are computationally efficient but struggle with long-range\ndependencies, while ViTs excel in capturing such dependencies but suffer from\nquadratic computational complexity. This paper proposes two ViT-based models\nfor accurate, efficient, and robust 2D pose estimation. The first one,\nEViTPose, operates in a computationally efficient manner without sacrificing\naccuracy by utilizing learnable joint tokens to select and process a subset of\nthe most important body patches, enabling us to control the trade-off between\naccuracy and efficiency by changing the number of patches to be processed. The\nsecond one, UniTransPose, while not allowing for the same level of direct\ncontrol over the trade-off, efficiently handles multiple scales by combining\n(1) an efficient multi-scale transformer encoder that uses both local and\nglobal attention with (2) an efficient sub-pixel CNN decoder for better speed\nand accuracy. Moreover, by incorporating all joints from different benchmarks\ninto a unified skeletal representation, we train robust methods that learn from\nmultiple datasets simultaneously and perform well across a range of scenarios\n-- including pose variations, lighting conditions, and occlusions. Experiments\non six benchmarks demonstrate that the proposed methods significantly\noutperform state-of-the-art methods while improving computational efficiency.\nEViTPose exhibits a significant decrease in computational complexity (30% to\n44% less in GFLOPs) with a minimal drop of accuracy (0% to 3.5% less), and\nUniTransPose achieves accuracy improvements ranging from 0.9% to 43.8% across\nthese benchmarks.",
        "Imperfect Knowledge Management (IKM) aids in managing imprecise, uncertain,\nor incomplete aspects of meaning. IKM acknowledges that an enterprise's\nknowledge is often imperfect, characterized by varying degrees of imprecision,\nuncertainty, or incompleteness. In this context, knowledge is viewed as an\nobject described by attributes and values. Our focus is on the domain of\ncompetencies (know how) in the production of coated cardboard, particularly the\nprocess of converting finished products from the manufactured cardboard. This\nprocess involves both classic and fuzzy attributes that are used to assess the\nquality of the cardboard. This article introduces a set of protocols designed\nto model a fuzzy metaknowledge base using GEFRED (GENeralized model for Fuzzy\nRElational Databases) in an Oracle 8i relational database system.",
        "Robot locomotion learning using reinforcement learning suffers from training\nsample inefficiency and exhibits the non-understandable\/black-box nature. Thus,\nthis work presents a novel SME-AGOL to address such problems. Firstly,\nSequential Motion Executor (SME) is a three-layer interpretable neural network,\nwhere the first produces the sequentially propagating hidden states, the second\nconstructs the corresponding triangular bases with minor non-neighbor\ninterference, and the third maps the bases to the motor commands. Secondly, the\nAdaptable Gradient-weighting Online Learning (AGOL) algorithm prioritizes the\nupdate of the parameters with high relevance score, allowing the learning to\nfocus more on the highly relevant ones. Thus, these two components lead to an\nanalyzable framework, where each sequential hidden state\/basis represents the\nlearned key poses\/robot configuration. Compared to state-of-the-art methods,\nthe SME-AGOL requires 40% fewer samples and receives 150% higher final\nreward\/locomotion performance on a simulated hexapod robot, while taking merely\n10 minutes of learning time from scratch on a physical hexapod robot. Taken\ntogether, this work not only proposes the SME-AGOL for sample efficient and\nunderstandable locomotion learning but also emphasizes the potential\nexploitation of interpretability for improving sample efficiency and learning\nperformance.",
        "Computing is increasingly central to innovation across a wide range of\ndisciplinary and interdisciplinary problem domains. Students across\nnoncomputing disciplines need to apply sophisticated computational skills and\nmethods to fields as diverse as biology, linguistics, and art. Furthermore,\ncomputing plays a critical role in \"momentous geopolitical events\", such as\nelections in several countries including the US, and is changing how people\n\"work, collaborate, communicate, shop, eat, travel, get news and entertainment,\nand quite simply live\". Traditional computing courses, however, fail to equip\nnon-computing discipline students with the necessary computing skills - if they\ncan even get into classes packed with CS majors. A pressing question facing\nacademics today is: How do we effectively integrate computing skills that are\nuseful for the discipline into discipline curricula?\n  We advocate an approach where courses in discipline X include the computing\nrelevant to the learning outcomes of that course, as used by practitioners in\nX. We refer to the computing skills relevant to a course in discipline X as an\n\"ounce of computing skills\", to highlight our belief regarding the amount of\ncomputing to be integrated in that course. In this article, we outline our\ninsights regarding the development of an ounce of computing skills for a\ndiscipline course, and the evaluation of the developed ounce. The key takeaways\nare that the goal has to be to advance students in their disciplines, and only\nthe disciplinary experts can tell us how computing is used in that discipline.\nComputer scientists know how to teach computing, but the classes can't be about\nCS values. The disciplinary values are paramount.",
        "Simplicial complexes arising from real-world settings may not be directly\nobservable. Hence, for an unknown simplicial complex in Euclidean space, we\nwant to efficiently reconstruct it by querying local structure. In particular,\nwe are interested in queries for the indegree of a simplex $\\sigma$ in some\ndirection: the number of cofacets of $\\sigma$ contained in some halfspace\n\"below\" $\\sigma$. Fasy et al. proposed a method that, given the vertex set of a\nsimplicial complex, uses indegree queries to reconstruct the set of edges. In\nparticular, they use a sweep algorithm through the vertex set, identifying\nedges adjacent to and above each vertex in the sweeping order. The algorithm\nrelies on a natural but crucial property of the sweeping order: at a given\nvertex $v$, all edges adjacent to $v$ contained in the halfspace below $v$ have\nanother endpoint that appeared earlier in the order.\n  The edge reconstruction algorithm does not immediately extend to\nhigher-dimensional simplex reconstruction. In particular, it is not possible to\nsweep through a set of $i$-simplices in a fixed direction and maintain that all\n$(i+1)$-cofacets of a given simplex $\\sigma$ that come below $\\sigma$ are\nknown. We circumvent this by defining a sweeping order on a set of\n$i$-simplices, that additionally pairs each $i$-simplex $\\sigma$ with a\ndirection perpendicular to $\\sigma$. Analogous to Fasy et al., our order has\nthe crucial property that, at any $i$-simplex $\\sigma$ paired with direction\n$s$, each $(i+1)$-dimensional coface of $\\sigma$ that lies in the halfspace\nbelow $\\sigma$ with respect to the direction $s$ has an $i$-dimensional face\nthat appeared earlier in the order. We show how to compute such an order and\nuse it to extend the edge reconstruction algorithm of Fasy et al. to simplicial\ncomplex reconstruction. Our algorithm can reconstruct arbitrary embedded\nsimplicial complexes.",
        "The inertia tensor is an important parameter in many engineering fields, but\nmeasuring it can be cumbersome and involve multiple experiments or accurate and\nexpensive equipment. We propose a method to measure the moment of inertia\ntensor of a rigid body from a single spinning throw, by attaching a small and\ninexpensive stand-alone measurement device consisting of a gyroscope,\naccelerometer and a reaction wheel. The method includes a compensation for the\nincrease of moment of inertia due to adding the measurement device to the body,\nand additionally obtains the location of the centre of gravity of the body as\nan intermediate result. Experiments performed with known rigid bodies show that\nthe mean accuracy is around 2\\%.",
        "The Detection Transformer (DETR), by incorporating the Hungarian algorithm,\nhas significantly simplified the matching process in object detection tasks.\nThis algorithm facilitates optimal one-to-one matching of predicted bounding\nboxes to ground-truth annotations during training. While effective, this strict\nmatching process does not inherently account for the varying densities and\ndistributions of objects, leading to suboptimal correspondences such as failing\nto handle multiple detections of the same object or missing small objects. To\naddress this, we propose the Regularized Transport Plan (RTP). RTP introduces a\nflexible matching strategy that captures the cost of aligning predictions with\nground truths to find the most accurate correspondences between these sets. By\nutilizing the differentiable Sinkhorn algorithm, RTP allows for soft,\nfractional matching rather than strict one-to-one assignments. This approach\nenhances the model's capability to manage varying object densities and\ndistributions effectively. Our extensive evaluations on the MS-COCO and VOC\nbenchmarks demonstrate the effectiveness of our approach. RTP-DETR, surpassing\nthe performance of the Deform-DETR and the recently introduced DINO-DETR,\nachieving absolute gains in mAP of +3.8% and +1.7%, respectively.",
        "The aim of this paper is to extend Washburn's capillary rise equation by\nincorporating a slip condition at the pipe wall. The governing equation is\nderived using fundamental principles from continuum mechanics. A new scaling is\nintroduced, allowing for a systematic analysis of different flow regimes. We\nprove the global-in-time existence and uniqueness of a bounded positive\nsolution to Washburn's equation that includes the slip parameter, as well as\nthe continuous dependence of the solution in the maximum norm on the initial\ndata. Thus, the initial-value problem for Washburn's equation is shown to be\nwell-posed in the sense of Hadamard. Additionally, we show that the unique\nequilibrium solution may be reached either monotonically or in an oscillatory\nfashion, similarly to the no-slip case. Finally, we determine the basin of\nattraction for the system, ensuring that the equilibrium state will be reached\nfrom the initial data we impose. These results hold for any positive value of\nthe nondimensional slip parameter in the model, and for all values of the ratio\n$h_0\/h_e$ in the range $[0,3\/2]$, where $h_0$ is the initial height of the\nfluid column and $h_e$ is its equilibrium height.",
        "We study the existence and qualitative properties of action ground-states\n(that is, bound-states with minimal action) {of the nonlinear Schr\\\"odinger\nequation} over single-knot metric graphs -- which are made of half-lines, loops\nand pendants, all connected at a single vertex. First, we prove existence of\naction ground-state for generic single-knot graphs, even in the absence of an\nassociated variational problem. Second, for regular single-knot graphs of\nlength $\\ell$, we perform a complete analysis of positive monotone\nbound-states. Furthermore, we characterize all positive bound-states when\n$\\ell$ is small and prove some symmetry-breaking results for large $\\ell$.\nFinally, we apply the results to some particular graphs to illustrate the\ncomplex relation between action ground-states and the topological {and metric}\nfeatures of the underlying metric graph.\n  The proofs are nonvariational, using a careful phase-plane analysis, the\nstudy of sections of period functions, asymptotic estimates and blowup\narguments. We show, in particular, how nonvariational techniques are\ncomplementary to variational ones in order to deeply understand bound-states of\nthe nonlinear Schr\\\"odinger equation on metric graphs.",
        "In this manuscript, we consider the problem of learning a flow or\ndiffusion-based generative model parametrized by a two-layer auto-encoder,\ntrained with online stochastic gradient descent, on a high-dimensional target\ndensity with an underlying low-dimensional manifold structure. We derive a\ntight asymptotic characterization of low-dimensional projections of the\ndistribution of samples generated by the learned model, ascertaining in\nparticular its dependence on the number of training samples. Building on this\nanalysis, we discuss how mode collapse can arise, and lead to model collapse\nwhen the generative model is re-trained on generated synthetic data.",
        "Structure-based drug design (SBDD) leverages the 3D structure of biomolecular\ntargets to guide the creation of new therapeutic agents. Recent advances in\ngenerative models, including diffusion models and geometric deep learning, have\ndemonstrated promise in optimizing ligand generation. However, the scarcity of\nhigh-quality protein-ligand complex data and the inherent challenges in\naligning generated ligands with target proteins limit the effectiveness of\nthese methods. We propose BoKDiff, a novel framework that enhances ligand\ngeneration by combining multi-objective optimization and Best-of-K alignment\nmethodologies. Built upon the DecompDiff model, BoKDiff generates diverse\ncandidates and ranks them using a weighted evaluation of molecular properties\nsuch as QED, SA, and docking scores. To address alignment challenges, we\nintroduce a method that relocates the center of mass of generated ligands to\ntheir docking poses, enabling accurate sub-component extraction. Additionally,\nwe integrate a Best-of-N (BoN) sampling approach, which selects the optimal\nligand from multiple generated candidates without requiring fine-tuning. BoN\nachieves exceptional results, with QED values exceeding 0.6, SA scores above\n0.75, and a success rate surpassing 35%, demonstrating its efficiency and\npracticality. BoKDiff achieves state-of-the-art results on the CrossDocked2020\ndataset, including a -8.58 average Vina docking score and a 26% success rate in\nmolecule generation. This study is the first to apply Best-of-K alignment and\nBest-of-N sampling to SBDD, highlighting their potential to bridge generative\nmodeling with practical drug discovery requirements. The code is provided at\nhttps:\/\/github.com\/khodabandeh-ali\/BoKDiff.git.",
        "Chronic liver disease represents a significant health challenge worldwide and\naccurate prognostic evaluations are essential for personalized treatment plans.\nRecent evidence suggests that integrating multimodal data, such as computed\ntomography imaging, radiomic features, and clinical information, can provide\nmore comprehensive prognostic information. However, modalities have an inherent\nheterogeneity, and incorporating additional modalities may exacerbate the\nchallenges of heterogeneous data fusion. Moreover, existing multimodal fusion\nmethods often struggle to adapt to richer medical modalities, making it\ndifficult to capture inter-modal relationships. To overcome these limitations,\nWe present the Triple-Modal Interaction Chronic Liver Network (TMI-CLNet).\nSpecifically, we develop an Intra-Modality Aggregation module and a\nTriple-Modal Cross-Attention Fusion module, which are designed to eliminate\nintra-modality redundancy and extract cross-modal information, respectively.\nFurthermore, we design a Triple-Modal Feature Fusion loss function to align\nfeature representations across modalities. Extensive experiments on the liver\nprognosis dataset demonstrate that our approach significantly outperforms\nexisting state-of-the-art unimodal models and other multi-modal techniques. Our\ncode is available at https:\/\/github.com\/Mysterwll\/liver.git.",
        "Given a fractal $\\mathcal{I}$ whose Hausdorff dimension matches with the\nupper-box dimension, we propose a new method which consists in selecting inside\n$\\mathcal{I}$ some subsets (called quasi-Cantor sets) of almost same dimension\nand with controled properties of self-similarties at prescribed scales. It\nallows us to estimate below the Hausdorff dimension $\\mathcal{I}$ intersected\nto limsup sets of contracted balls selected according a Bernoulli law, in\ncontexts where classical Mass Transference Principles cannot be applied. We\napply this result to the computation of the increasing multifractal spectrum of\nlacunary wavelet series supported on $\\mathcal{I}$."
      ]
    }
  },
  {
    "id":2411.00758,
    "research_type":"basic",
    "start_id":"b18",
    "start_title":"Inverse methods for illumination optics",
    "start_abstract":"\u2022 A submitted manuscript is the version of the article upon submission and before peer-review. There can be important differences between the submitted version and the official published version of record. People interested in the research are advised to contact the author for the final version of the publication, or visit the DOI to the publisher's website. \u2022 The final author version and the galley proof are versions of the publication after peer review. \u2022 The final published version features the final layout of the paper including the volume, issue and page numbers.",
    "start_categories":[
      "math.MP"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b6"
      ],
      "title":[
        "Introduction to Nonimaging Optics, second edition"
      ],
      "abstract":[
        "Introduction to Nonimaging Optics covers the theoretical foundations and design methods of nonimaging optics, as well as key concepts from related fields. This fully updated, revised, and expanded Second Edition: \u2022 Features a new and intuitive introduction with a basic description of the advantages of nonimaging optics \u2022 Adds new chapters on wavefronts for a prescribed output (irradiance or intensity), infinitesimal \u00e9tendue optics (generalization of the aplanatic optics), and K\u00f6hler optics and color mixing \u2022 Incorporates new material on the simultaneous multiple surface (SMS) design method in 3-D, integral invariants, and \u00e9tendue 2-D \u2022 Contains 21 chapters, 24 fully worked and several other examples, and 1,000+ illustrations, including photos of real devices \u2022 Addresses applications ranging from solar energy concentration to illumination engineering Introduction to Nonimaging Optics, Second Edition invites newcomers to explore the growing field of nonimaging optics, while providing seasoned veterans with an extensive reference book."
      ],
      "categories":[
        "physics.optics"
      ]
    },
    "list":{
      "title":[
        "Electrical Generation of Colour Centres in Hexagonal Boron Nitride",
        "Modeling Nonlinear Optics with the Transfer Matrix Method",
        "Efficient and accurate analysis of oscillation dynamics for dissipative\n  cavity solitons based on the artificial neural network",
        "Engineering 2D Van der Waals Electrode via MBE Grown Weyl Semimetal\n  1T-WTe2 for Enhanced Photodetection in InSe",
        "Pseudospin Transverse Localization of Light in an Optical Disordered\n  Spin-Glass Phase",
        "Skyrmion Generation through the Chirality Interplay of Light and\n  Magnetism",
        "Optical force and torque on a spinning dielectric sphere",
        "Direct Observation of Strongly Tilted Dirac Points at General Positions\n  in the Reciprocal Space",
        "Singleshot Multispectral Imaging via a Chromatic Metalens Array",
        "Dual-Frequency Comb in Fiber Fabry-Perot Resonator",
        "Towards Efficient PCSEL Design: A Data-Driven Approach for Design\n  Insights",
        "High Contrast Nulling in Photonic Meshes Through Architectural\n  Redundancy",
        "Resolution enhancement in quantitative phase microscopy: a review",
        "Opinion Dynamics with Multiple Adversaries",
        "Text to Band Gap: Pre-trained Language Models as Encoders for\n  Semiconductor Band Gap Prediction",
        "Dynamical analysis of an HIV infection model including quiescent cells\n  and immune response",
        "GAIA: A Global, Multi-modal, Multi-scale Vision-Language Dataset for\n  Remote Sensing Image Analysis",
        "Controlling spin currents with magnon interference in a canted\n  antiferromagnet",
        "Pre-trained Models Succeed in Medical Imaging with Representation\n  Similarity Degradation",
        "Mass and Metal Flows in Isolated IllustrisTNG Halos",
        "Resonance nuclear excitation of the $^{229}$Th nucleus via electronic\n  bridge process in Th~II",
        "Adaptive Prompt: Unlocking the Power of Visual Prompt Tuning",
        "SN1987A bounds on neutrino quantum decoherence",
        "Adaptive Contextual Caching for Mobile Edge Large Language Model Service",
        "Cryogenic operation of silicon photomultiplier arrays",
        "LexGenie: Automated Generation of Structured Reports for European Court\n  of Human Rights Case Law",
        "Low-Rank Adapters Meet Neural Architecture Search for LLM Compression",
        "Strong Solutions and Quantization-Based Numerical Schemes for a Class of\n  Non-Markovian Volatility Models"
      ],
      "abstract":[
        "Defects in wide band gap crystals have emerged as a promising platform for\nhosting colour centres that enable quantum photonic applications. Among these,\nhexagonal boron nitride (hBN), a van der Waals material, stands out for its\nability to be integrated into heterostructures, enabling unconventional charge\ninjection mechanisms that bypass the need for p-n junctions. This advancement\nallows for the electrical excitation of hBN colour centres deep inside the\nlarge hBN bandgap, which has seen rapid progress in recent developments. Here,\nwe fabricate hBN electroluminescence (EL) devices that generate narrowband\ncolour centres suitable for electrical excitation. The colour centres are\nlocalised to tunnelling current hotspots within the hBN flake, which are\ndesigned during device fabrication. We outline the optimal conditions for\ndevice operation and colour centre stability, focusing on minimising background\nemission and ensuring prolonged operation. Our findings follow up on the\nexisting literature and mark a step forward towards the integration of hBN\nbased colour centres into quantum photonic technologies.",
        "The Transfer Matrix Method (TMM) is a widely used technique for modeling\nlinear propagation of electromagnetic waves through stratified layered media.\nHowever, since its extension to inhomogeneous and nonlinear systems is not\nstraightforward, much more computationally demanding methods such as\nFinite-difference time-domain (FDTD) or Method of lines (MoL) are typically\nused. In this work, we extend the TMM framework to incorporate the effects of\nnonlinearity. We consider the case when strong coupling between excitons\n(electron-hole pairs) and photons leads to the formation of exciton-polaritons.\nThis extension is crucial for accurately simulating the behavior of light in\npolariton microcavities, where nonlinearities arising from exciton-exciton\ninteractions play a key role. We perform efficient simulations of light\ntransmission and reflection in a multidimensional system using the plane wave\nbasis. Additionally, we compare our extended TMM approach with the\nstate-of-the-art admittance transfer method, and highlight the computational\nadvantage of extended TMM for large-scale systems. The extended TMM not only\nprovides a robust and computationally efficient numerical framework, but also\npaves the way for the development of future low-power nonlinear optical\ndevices, polariton-based photonic circuits, and quantum photonic technologies.",
        "As a conventional means to analyze the system mechanism based on partial\ndifferential equations (PDE) or nonlinear dynamics, iterative algorithms are\ncomputationally intensive. In this framework, the details of oscillating\ndynamics of cavity solitons are beyond the reach of traditional mathematical\nanalysis. In this work, we demonstrate that this long-standing challenge could\nbe tackled down with the Long Short-Term Memory (LSTM) neural network. We\npropose the incorporating parameter-fed ports, which are capable of recognizing\nperiod-doubling bifurcations of respiratory solitons and quickly predicting\nnonlinear dynamics of solitons with arbitrary parameter combinations and\narbitrary time series lengths. The model predictions capture oscillatory\nfeatures with a small Root Mean Square Errors (RMSE) = 0.01676 and an absolute\nerror that barely grows with the length of the prediction time. Lugiato-Lefever\nequation (LLE) based parameter space boundaries for typical oscillatory\npatterns are plotted at about 120 times the speed relative to the split-step\nFourier method (SSFM) and higher resolution.",
        "Achieving low contact resistance in advanced electronic devices remains a\nsignificant challenge. As the demand for faster and more energy-efficient\ndevices grows, 2D contact engineering emerges as a promising solution for\nnext-generation electronics. Beyond graphene, 1T-WTe2 has gained attention due\nto its outstanding electrical transport properties, quantum phenomena, and Weyl\nsemimetallic characteristics. We demonstrate the direct wafer-scale growth of\n1T-WTe2 via molecular beam epitaxy (MBE) and use it as a 2D contact for layered\nmaterials like InSe, which exhibits broad photoresponsivity. The performance of\nthis 2D electrode in InSe-based photodetectors is compared with conventional\nmetal electrodes. Under near-infrared (NIR) to deep ultraviolet (DUV)\nillumination, the InSe\/1T-WTe2 configuration shows a broad photoresponsivity\nrange from 0.14 to 217.58 A\/W, with fast rise\/fall times of 42\/126 ms in the\nvisible region. In contrast, the InSe\/Ti-Au configuration exhibits a peak\nphotoresponsivity of 3.64 A\/W in the DUV range, with an overall lower\nresponsivity spanning from 0.000865 A\/W to 3.64 A\/W under NIR and DUV\nillumination, respectively. Additionally, in the visible regime, it exhibits\nslower rise and fall times of 150 ms and 144 ms, respectively, compared to\nInSe\/1T-WTe2. These findings indicate that MBE-grown 1T-WTe2 serves as an\neffective 2D electrode, delivering higher photoresponsivity and faster\nphotodetection compared to traditional metal contacts. This approach offers a\nsimplified, high-performance alternative for layered material-based devices,\neliminating the need for complex heterostructure configurations.",
        "Localization phenomena during transport are typically driven by disordered\nscalar potentials. Here, we predict a universal pseudospin localization\nphenomenon induced by a disordered vectorial potential and demonstrate it\nexperimentally in an optical analogue of a classical disordered spin-glass\nmagnetic phase. In our system, a transverse disorder in the second-order\nnonlinear coupling of a nonlinear photonic crystal causes the idler-signal\nlight beam, representing the pseudospin current, to become localized in the\ntransverse plane. This effect depends strongly on the nonlinear coupling\nstrength, controlled by the optical pump power, revealing its inherently\nnonlinear and non-perturbative nature. Furthermore, this phenomenon is marked\nby decaying Rabi oscillations between the idler and signal fields, linked to\nthe disorder properties, suggesting an accompanied longitudinal decoherence\neffect. Our findings offer deep insights into spin transport in disordered\nmagnetic textures and open avenues for exploring complex magnetic phases and\nphase transitions using nonlinear optics.",
        "Light beams, with their rich degrees of freedom, including polarization and\nphase, along with their flexible tunability, have emerged as an ideal tool for\ngenerating magnetic topological textures.However, how to precisely control the\nlight beams to generate a specific number of magnetic topological textures on\ndemand remains a critical scientific issue that needs to be resolved. Based on\nthe numerical simulation of the Landau-Lifshitz-Gilbert equation, we propose\nthat circularly polarized Laguerre-Gaussian beams can induce chiral magnetic\nfields through the interaction of the chirality of these beams'angular momenta.\nBy utilizing these chiral magnetic fields, skyrmions or skyrmionium can be\ninduced in chiral magnets. Moreover, the vectorial magnetic fields can be\nmanipulated by adjusting the angular momenta and light intensity, thereby\ngenerating target chiral patterns and strengths, which allows for precise\ncontrol over the type and number of these topological magnetic textures. This\nfinding not only reveals the underlying physical mechanisms of the interaction\nbetween light and magnetic systems but also provides a feasible solution for\nthe on-demand generation and encoding of skyrmions.",
        "Optical force can enable precise manipulations of small particles for various\napplications. It is well known that an isotropic lossless dielectric sphere is\nonly subject to forward optical force under the illumination of an\nelectromagnetic plane wave. By using rigorous full-wave simulations, we show\nthat such a sphere can experience a lateral optical force and an optical torque\nbesides the conventional longitudinal force, if it spins with a constant\nangular velocity. The emergence of the unusual optical force and torque is\nattributed to the breaking of mirror and cylindrical symmetries by the spinning\nmotion. Using the multipole expansion in source representation, we illustrate\nhow the spinning-induced effective bi-anisotropy generates the lateral force\nand torque on the sphere through the interference of electric and magnetic\nmultipoles. We also uncover the effect of Sagnac frequency splitting on the\noptical force and torque. The results contribute to the understanding of the\noptical force and torque in moving media and can be applied to realize\nunconventional optical manipulations of small particles.",
        "Type-II Dirac points (DPs), which occur at the intersection of strongly\ntilted and touching energy bands, exhibit many intriguing physical phenomena\nfundamentally different from the non-tilted type-I counterparts. Over the past\ndecade, their discovery has spurred extensive research into electronic systems\nand other Bloch-wave systems, such as photonic and phononic crystals. However,\ncurrent studies typically focus on type-II DPs along high-symmetry directions\nin the first Brillouin zone (FBZ) under mirror symmetry conditions, which are\nhighly restrictive and limit further investigations and applications. To\novercome the stringent constraint, here we identify and demonstrate the\nemergence of type-II DPs at general positions inside the FBZ without requiring\nthe mirror symmetry. The type-II DPs, being accidental degeneracies, are\nexperimentally realized on a metacrystal slab with H-shaped metallic patterns.\nOur findings indicate that even in the absence of mirror symmetry, type-II DPs\ncan emerge at designated locations inside the FBZ by simply rotating the\nH-shaped patterns and adjusting geometrical and physical parameters.\nFurthermore, based on the rotated type-II DPs, off-axis conical diffractions\nhave been both realized and experimentally observed. Meanwhile, we discovered\nthat during the rotation process, the type-II DPs transform into off-axis\ntype-I DPs, but still strongly tilted, resulting in the emergence of negative\nrefractions. Hence, the generic method we propose for inducing type-II or\nstrongly tilted type-I DPs without the high-symmetry limitations opens\npotential avenues for related research. For example, the observed off-axis\nconical diffraction and negative refraction could inspire future development\nand applications in photonics and other Bloch-wave systems.",
        "Real time, singleshot multispectral imaging systems are crucial for\nenvironment monitoring and biomedical imaging. Most singleshot multispectral\nimagers rely on complex computational backends, which precludes real time\noperations. In this work, we leverage the spectral selectivity afforded by\nengineered photonic materials to perform bulk of the multispectral data\nextraction in the optical domain, thereby circumventing the need for heavy\nbackend computation. We use our imager to extract multispectral data for two\nreal world objects at 8 predefined spectral channels in the 400 to 900 nm\nwavelength range. For both objects, an RGB image constructed using extracted\nmultispectral data shows good agreement with an image taken using a phone\ncamera, thereby validating our imaging approach. We believe that the proposed\nsystem can provide new avenues for the development of highly compact and low\nlatency multispectral imaging technologies.",
        "This paper presents a novel approach to dual-frequency comb generation\nutilizing a single fiber Fabry-Perot resonator, advancing the implementation of\nthese sources in fiber-based systems. Dual-comb applications such as\nspectroscopy, ranging, and imaging, known for their high-resolution and rapid\ndata acquisition capabilities, benefit significantly from the stability and\ncoherence of optical frequency comb sources. Our method leverages the\nbirefringent property of the resonator induced by the optical fiber to generate\ntwo orthogonally polarized optical frequency combs in a monolitic resonator.\nThis approach allows for the generation of two different frequency combs with\nslightly different repetition rates, exhibiting excellent mutual coherence,\nmaking it highly relevant for dual-comb applications. The 40 nm bandwidth\ngenerated combs are induced by switching-waves in a normal dispersion fiber\nFabry-Perot resonator. These comb types have the advantage of being easily\ngenerated by a pulse pumping scheme, which is employed in this study. Finally,\nthe potential of the source is demonstrated by a proof-of-concept spectroscopy\nmeasurement.",
        "We present a data-driven design approach for photonic crystals to achieve\nhigh efficiency in photonic crystal surface-emitting lasers (PCSELs). By\ndiscretizing the photonic crystal structure into a grid, we enable the\ngeneration of arbitrary lattice designs. Multiple fully connected layers\ncombined with a position embedding module extract essential features from the\nphotonic crystal designs, while coupled-wave theory (CWT) is used to evaluate\nthe efficiency (based on the ratio of surface-emitting to edge-emitting\nresonant) and quality factor Q. We introduce the Neural Networks (NNs) model to\nevaluate the structures, and to find a better performance design according to\nthe evaluation result. The model achieves high prediction accuracy, with\nPearson correlation coefficients of 0.780 for SEE and 0.887 for the\nlog-transformed Q. Additionally, we perform Shapley value analysis to identify\nthe most important Fourier coefficients, providing insights into the factors\nthat impact the performance of PCSEL designs. Our work speeds up the design\nprocess and offers valuable guidance for optimizing high-performance PCSELs,\nsupporting the development of fully photonic design automation (PDA).",
        "We demonstrate a silicon photonic architecture comprised of Double\nMach-Zehnder Interferometers (DMZIs) designed for high-contrast photonic\napplications. This configuration significantly enhances the achievable\nextinction ratio of photonic integrated circuits (PICs), reaching levels\nexceeding 80 dB. By leveraging the tunable properties of DMZIs and implementing\na systematic configuration algorithm, the proposed mesh effectively compensates\nfor fabrication imperfections and mitigates non-idealities such as back\nreflections. Experimental validation on a silicon-on-insulator platform\ndemonstrates the potential of this approach for applications requiring high\ncontrast nulling such as astronomical sensing.",
        "Quantitative phase microscopy (QPM), a technique combining phase imaging and\nmicroscopy, enables visualization of the 3D topography in reflective samples,\nas well as the inner structure or refractive index distribution of transparent\nand translucent samples. Similar to other imaging modalities, QPM is\nconstrained by the conflict between numerical aperture (NA) and field of view\n(FOV): an imaging system with a low NA has to be employed to maintain a large\nFOV. This fact severely limits the resolution in QPM up to being the\nillumination wavelength. Consequently, finer structures of samples cannot be\nresolved by using modest NA objectives in QPM. Aimed to that, many approaches,\nsuch as oblique illumination, structured illumination, and speckle illumination\n(just to cite a few), have been proposed to improve the spatial resolution (or\nthe space bandwidth product) in phase microscopy by restricting other degrees\nof freedom (mostly time). This paper aims to provide an up to date review on\nthe resolution enhancement approaches in QPM, discussing the pros and cons of\neach technique as well as the confusion on resolution definition claims on QPM\nand other coherent microscopy methods. Through this survey, we will review the\nmost appealing and useful techniques for superresolution in coherent\nmicroscopy, working with and without lenses and with special attention to QPM.",
        "Opinion dynamics model how the publicly expressed opinions of users in a\nsocial network coevolve according to their neighbors as well as their own\nintrinsic opinion. Motivated by the real-world manipulation of social networks\nduring the 2016 US elections and the 2019 Hong Kong protests, a growing body of\nwork models the effects of a strategic actor who interferes with the network to\ninduce disagreement or polarization. We lift the assumption of a single\nstrategic actor by introducing a model in which any subset of network users can\nmanipulate network outcomes. They do so by acting according to a fictitious\nintrinsic opinion. Strategic actors can have conflicting goals, and push\ncompeting narratives. We characterize the Nash Equilibrium of the resulting\nmeta-game played by the strategic actors. Experiments on real-world social\nnetwork datasets from Twitter, Reddit, and Political Blogs show that strategic\nagents can significantly increase polarization and disagreement, as well as\nincrease the \"cost\" of the equilibrium. To this end, we give worst-case upper\nbounds on the Price of Misreporting (analogous to the Price of Anarchy).\nFinally, we give efficient learning algorithms for the platform to (i) detect\nwhether strategic manipulation has occurred, and (ii) learn who the strategic\nactors are. Our algorithms are accurate on the same real-world datasets,\nsuggesting how platforms can take steps to mitigate the effects of strategic\nbehavior.",
        "In this study, we explore the use of a transformer-based language model as an\nencoder to predict the band gaps of semiconductor materials directly from their\ntext descriptions. Quantum chemistry simulations, including Density Functional\nTheory (DFT), are computationally intensive and time-consuming, which limits\ntheir practicality for high-throughput material screening, particularly for\ncomplex systems. Shallow machine learning (ML) models, while effective, often\nrequire extensive data preprocessing to convert non-numerical material\nproperties into numerical inputs. In contrast, our approach leverages textual\ndata directly, bypassing the need for complex feature engineering. We generate\nmaterial descriptions in two formats: formatted strings combining features and\nnatural language text generated using the ChatGPT API. We demonstrate that the\nRoBERTa model, pre-trained on natural language processing tasks, performs\neffectively as an encoder for prediction tasks. With minimal fine-tuning, it\nachieves a mean absolute error (MAE) of approximately 0.33 eV, performing\nbetter than shallow machine learning models such as Support Vector Regression,\nRandom Forest, and XGBoost. Even when only the linear regression head is\ntrained while keeping the RoBERTa encoder layers frozen, the accuracy remains\nnearly identical to that of the fully trained model. This demonstrates that the\npre-trained RoBERTa encoder is highly adaptable for processing domain-specific\ntext related to material properties, such as the band gap, significantly\nreducing the need for extensive retraining. This study highlights the potential\nof transformer-based language models to serve as efficient and versatile\nencoders for semiconductor materials property prediction tasks.",
        "This research gives a thorough examination of an HIV infection model that\nincludes quiescent cells and immune response dynamics in the host. The model,\nrepresented by a system of ordinary differential equations, captures the\ncomplex interaction between the host's immune response and viral infection. The\nstudy focuses on the model's fundamental aspects, such as equilibrium analysis,\ncomputing the basic reproduction number $\\mathcal{R}_0$, stability analysis,\nbifurcation phenomena, numerical simulations, and sensitivity analysis.\n  The analysis reveals both an infection equilibrium, which indicates the\npersistence of the illness, and an infection-free equilibrium, which represents\ndisease control possibilities. Applying matrix-theoretical approaches,\nstability analysis proved that the infection-free equilibrium is both locally\nand globally stable for $\\mathcal{R}_0 < 1$. For the situation of\n$\\mathcal{R}_0 > 1$, the infection equilibrium is locally asymptotically stable\nvia the Routh--Hurwitz criterion. We also studied the uniform persistence of\nthe infection, demonstrating that the infection remains present above a\npositive threshold under certain conditions. The study also found a\ntranscritical forward-type bifurcation at $\\mathcal{R}_0 = 1$, indicating a\ncritical threshold that affects the system's behavior. The model's temporal\ndynamics are studied using numerical simulations, and sensitivity analysis\nidentifies the most significant variables by assessing the effects of parameter\nchanges on system behavior.",
        "The continuous operation of Earth-orbiting satellites generates vast and\never-growing archives of Remote Sensing (RS) images. Natural language presents\nan intuitive interface for accessing, querying, and interpreting the data from\nsuch archives. However, existing Vision-Language Models (VLMs) are\npredominantly trained on web-scraped, noisy image-text data, exhibiting limited\nexposure to the specialized domain of RS. This deficiency results in poor\nperformance on RS-specific tasks, as commonly used datasets often lack\ndetailed, scientifically accurate textual descriptions and instead emphasize\nsolely on attributes like date and location. To bridge this critical gap, we\nintroduce GAIA, a novel dataset designed for multi-scale, multi-sensor, and\nmulti-modal RS image analysis. GAIA comprises of 205,150 meticulously curated\nRS image-text pairs, representing a diverse range of RS modalities associated\nto different spatial resolutions. Unlike existing vision-language datasets in\nRS, GAIA specifically focuses on capturing a diverse range of RS applications,\nproviding unique information about environmental changes, natural disasters,\nand various other dynamic phenomena. The dataset provides a spatially and\ntemporally balanced distribution, spanning across the globe, covering the last\n25 years with a balanced temporal distribution of observations. GAIA's\nconstruction involved a two-stage process: (1) targeted web-scraping of images\nand accompanying text from reputable RS-related sources, and (2) generation of\nfive high-quality, scientifically grounded synthetic captions for each image\nusing carefully crafted prompts that leverage the advanced vision-language\ncapabilities of GPT-4o. Our extensive experiments, including fine-tuning of\nCLIP and BLIP2 models, demonstrate that GAIA significantly improves performance\non RS image classification, cross-modal retrieval and image captioning tasks.",
        "Controlling spin current lies at the heart of spintronics and its\napplications. The sign of spin currents is monotonous in ferromagnets once the\ncurrent direction is determined. Spin currents in antiferromagnets can possess\nopposite polarization, but requires enormous magnetic fields to lift the\ndegeneracy. Controlling spin currents with different polarization is urgently\ndemanded but remains hitherto elusive. Here, we demonstrate the control of spin\ncurrents at room temperature by magnon interference in a canted\nantiferromagnet, hematite recently also classified as an altermagnet.\nMagneto-optical characterization by Brillouin light scattering revealed that\nthe spatial periodicity of the beating patterns was tunable via the microwave\nfrequency. The inverse spin-Hall voltage changed sign as the frequency was\nscanned, i.e., a frequency-controlled switching of polarization in pure spin\ncurrents was obtained. Our work marks the use of antiferromagnetic magnon\ninterference to control spin currents, which substantially extends the horizon\nfor the emerging field of coherent antiferromagnetic spintronics.",
        "This paper investigates the critical problem of representation similarity\nevolution during cross-domain transfer learning, with particular focus on\nunderstanding why pre-trained models maintain effectiveness when adapted to\nmedical imaging tasks despite significant domain gaps. The study establishes a\nrigorous problem definition centered on quantifying and analyzing\nrepresentation similarity trajectories throughout the fine-tuning process,\nwhile carefully delineating the scope to encompass both medical image analysis\nand broader cross-domain adaptation scenarios. Our empirical findings reveal\nthree critical discoveries: the potential existence of high-performance models\nthat preserve both task accuracy and representation similarity to their\npre-trained origins; a robust linear correlation between layer-wise similarity\nmetrics and representation quality indicators; and distinct adaptation patterns\nthat differentiate supervised versus self-supervised pre-training paradigms.\nThe proposed similarity space framework not only provides mechanistic insights\ninto knowledge transfer dynamics but also raises fundamental questions about\noptimal utilization of pre-trained models. These results advance our\nunderstanding of neural network adaptation processes while offering practical\nimplications for transfer learning strategies that extend beyond medical\nimaging applications. The code will be available once accepted.",
        "The cicumgalactic medium (CGM) is a reservoir of metals and star-forming\nfuel. Most baryons in the universe are in the circumgalactic medium (CGM) or\nintergalactic medium (IGM). The baryon cycle -- how mass and metals reach the\nCGM from the inner regions of the galaxy and how gas from the CGM replenishes\nstar-forming activity in the inner regions -- is an essential question in\ngalaxy evolution. In this paper, we study the flow of mass and metals in a\nstacked sample of 2770 isolated halos from the IllustrisTNG cosmological\nhydrodynamic simulation. The mean gas flow as a function of radius and angle is\nsimilar across a large galactic mass range when accounting for different\nfeedback modes. Although both star formation and black holes cause powerful\noutflows, the flows from star formation are more angularly restricted. Black\nhole feedback dominates massflow throughout the halo, while star-formation\nfeedback mainly affects the inner region. When scaling by virial radius\n($R_v$), large dynamical changes occur at $0.2R_v$ for most halos, suggesting a\ncharacteristic size for the inner galaxy. Despite radio mode feedback from\nblack holes being the primary quenching mechanism in IllustrisTNG, a small\npopulation of high mass radio mode disks are able to form stars.",
        "The 8.4 eV transition in the $^{229}$Th nucleus is the basis for a\nhigh-precision nuclear clock with exceptional sensitivity to new physics\neffects. We have identified several cases in the Th$^+$ ion where electronic\nexcitations closely resonate with the nuclear excitation, with the smallest\nenergy difference being $\\Delta = -0.09$ cm$^{-1}$. We investigate the\nelectronic bridge process, in which nuclear excitation is induced via\nelectronic transitions, and demonstrate that a proper selection of laser\nfrequencies can lead to a dramatic enhancement of this effect. Additionally, we\nshow that the interaction with electrons significantly shortens the lifetime of\nthe nuclear excited state.",
        "Visual Prompt Tuning (VPT) has recently emerged as a powerful method for\nadapting pre-trained vision models to downstream tasks. By introducing\nlearnable prompt tokens as task-specific instructions, VPT effectively guides\npre-trained transformer models with minimal overhead. Despite its empirical\nsuccess, a comprehensive theoretical understanding of VPT remains an active\narea of research. Building on recent insights into the connection between\nmixture of experts and prompt-based approaches, we identify a key limitation in\nVPT: the restricted functional expressiveness in prompt formulation. To address\nthis limitation, we propose Visual Adaptive Prompt Tuning (VAPT), a new\ngeneration of prompts that redefines prompts as adaptive functions of the\ninput. Our theoretical analysis shows that this simple yet intuitive approach\nachieves optimal sample efficiency. Empirical results on VTAB-1K and FGVC\nfurther demonstrate VAPT's effectiveness, with performance gains of 7.34% and\n1.04% over fully fine-tuning baselines, respectively. Notably, VAPT also\nsurpasses VPT by a substantial margin while using fewer parameters. These\nresults highlight both the effectiveness and efficiency of our method and pave\nthe way for future research to explore the potential of adaptive prompts.",
        "We obtain stringent bounds on neutrino quantum decoherence from the analysis\nof SN1987A data. We show that for the decoherence model considered here, which\nallows for neutrino-loss along the trajectory, the bounds are many orders of\nmagnitude stronger than the ones that can be obtained from the analysis of data\nfrom reactor neutrino oscillation experiments or neutrino telescopes.",
        "Mobile edge Large Language Model (LLM) deployments face inherent constraints,\nsuch as limited computational resources and network bandwidth. Although\nRetrieval-Augmented Generation (RAG) mitigates some challenges by integrating\nexternal knowledge bases, inefficient cache management can still result in high\nretrieval latency and frequent cache updates. To address these issues, we\npropose an Adaptive Contextual Caching (ACC) framework that anticipates user\nneeds by proactively caching semantically relevant data for mobile-edge LLMs.\nACC utilizes a deep reinforcement learning (DRL) module to refine cache\nreplacement policies, balancing user context, document similarity, and the\noverhead associated with cache misses. Experimental results demonstrate that\nACC increases cache hit rates to over 80\\% after only 11 training episodes,\noutperforming FIFO, LRU, and semantic-only caching while reducing retrieval\nlatency by up to 40\\%. In particular, ACC also reduces local caching overhead\n(i.e., the cost of updating the cache when a miss occurs) by as much as 55\\%,\nenabling scalable, low-latency LLM services in resource-constrained edge\nenvironments.",
        "The LHCb experiment at CERN has been upgraded for the Run 3 operation of the\nLarge Hadron Collider (LHC). A new concept of tracking detector based on\nScintillating Fibres (SciFi) read out with multichannel silicon\nphotomultipliers (SiPMs) was installed during its upgrade. One of the main\nchallenges the SciFi tracker will face during the Run 4 operation of the LHC is\nthe higher radiation environment due to fast neutrons, where the SiPMs are\nlocated. To cope with the increase in radiation, cryogenic cooling with liquid\nnitrogen is being investigated as a possible solution to mitigate the\nperformance degradation of the SiPMs induced by radiation damage. Thus, a\ndetailed performance study of different layouts of SiPM arrays produced by\nFondazione Bruno Kessler (FBK) and Hamamatsu Photonics K.K. is being carried\nout. These SiPMs have been designed to operate at cryogenic temperatures.\nSeveral SiPMs have been tested in a dedicated cryogenic setup down to 100 K.\nKey performance parameters such as breakdown voltage, dark count rate, photon\ndetection efficiency, gain and direct cross-talk are characterized as a\nfunction of the temperature. The main results of this study are going to be\npresented here.",
        "Analyzing large volumes of case law to uncover evolving legal principles,\nacross multiple cases, on a given topic is a demanding task for legal\nprofessionals. Structured topical reports provide an effective solution by\nsummarizing key issues, principles, and judgments, enabling comprehensive legal\nanalysis on a particular topic. While prior works have advanced query-based\nindividual case summarization, none have extended to automatically generating\nmulti-case structured reports. To address this, we introduce LexGenie, an\nautomated LLM-based pipeline designed to create structured reports using the\nentire body of case law on user-specified topics within the European Court of\nHuman Rights jurisdiction. LexGenie retrieves, clusters, and organizes relevant\npassages by topic to generate a structured outline and cohesive content for\neach section. Expert evaluation confirms LexGenie's utility in producing\nstructured reports that enhance efficient, scalable legal analysis.",
        "The rapid expansion of Large Language Models (LLMs) has posed significant\nchallenges regarding the computational resources required for fine-tuning and\ndeployment. Recent advancements in low-rank adapters have demonstrated their\nefficacy in parameter-efficient fine-tuning (PEFT) of these models. This\nretrospective paper comprehensively discusses innovative approaches that\nsynergize low-rank representations with Neural Architecture Search (NAS)\ntechniques, particularly weight-sharing super-networks. Robust solutions for\ncompressing and fine-tuning large pre-trained models are developed by\nintegrating these methodologies. Our analysis highlights the potential of these\ncombined strategies to democratize the use of LLMs, making them more accessible\nfor deployment in resource-constrained environments. The resulting models\nexhibit reduced memory footprints and faster inference times, paving the way\nfor more practical and scalable applications of LLMs. Models and code are\navailable at\nhttps:\/\/github.com\/IntelLabs\/Hardware-Aware-Automated-Machine-Learning.",
        "We investigate a class of non-Markovian processes that hold particular\nrelevance in the realm of mathematical finance. This family encompasses\npath-dependent volatility models, including those pioneered by [Platen and\nRendek, 2018] and, more recently, by [Guyon and Lekeufack, 2023], as well as an\nextension of the framework proposed by [Blanc et al., 2017]. Our study unfolds\nin two principal phases. In the first phase, we introduce a functional\nquantization scheme based on an extended version of the Lamperti transformation\nthat we propose to handle the presence of a memory term incorporated into the\ndiffusion coefficient. For scenarios involving a Brownian integral in the\ndiffusion term, we propose alternative numerical schemes that leverage the\npower of marginal recursive quantization. In the second phase, we study the\nproblem of existence and uniqueness of a strong solution for the SDEs related\nto the examples that motivate our study, in order to provide a theoretical\nbasis to correctly apply the proposed numerical schemes."
      ]
    }
  },
  {
    "id":2411.01291,
    "research_type":"applied",
    "start_id":"b15",
    "start_title":"CMRxRecon2024: A Multi-Modality, Multi-View K-Space Dataset Boosting\n  Universal Machine Learning for Accelerated Cardiac MRI",
    "start_abstract":"Cardiac magnetic resonance imaging (MRI) has emerged as a clinically gold-standard technique for diagnosing cardiac diseases, thanks to its ability provide diverse information with multiple modalities and anatomical views. Accelerated MRI is highly expected achieve time-efficient patient-friendly imaging, then advanced image reconstruction approaches are required recover high-quality, interpretable images from undersampled measurements. However, the lack of publicly available k-space dataset in terms both quantity diversity severely hindered substantial technological progress, particularly data-driven artificial intelligence. Here, we standardized, diverse, high-quality CMRxRecon2024 facilitate technical development, fair evaluation, clinical transfer approaches, towards promoting universal frameworks that enable fast robust reconstructions across different protocols practice. To best our knowledge, largest most dataset. It acquired 330 healthy volunteers, covering commonly used modalities, views, acquisition trajectories workflows. Besides, an open platform tutorials, benchmarks, data processing tools provided usage, method performance evaluation.",
    "start_categories":[
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b19"
      ],
      "title":[
        "vSHARP: variable Splitting Half-quadratic ADMM algorithm for Reconstruction of inverse-Problems"
      ],
      "abstract":[
        "Medical Imaging (MI) tasks, such as accelerated parallel Magnetic Resonance Imaging (MRI), often involve reconstructing an image from noisy or incomplete measurements. This amounts to solving ill-posed inverse problems, where a satisfactory closed-form analytical solution is not available. Traditional methods such as Compressed Sensing (CS) in MRI reconstruction can be time-consuming or prone to obtaining low-fidelity images. Recently, a plethora of Deep Learning (DL) approaches have demonstrated superior performance in inverse-problem solving, surpassing conventional methods. In this study, we propose vSHARP (variable Splitting Half-quadratic ADMM algorithm for Reconstruction of inverse Problems), a novel DL-based method for solving ill-posed inverse problems arising in MI. vSHARP utilizes the Half-Quadratic Variable Splitting method and employs the Alternating Direction Method of Multipliers (ADMM) to unroll the optimization process. For data consistency, vSHARP unrolls a differentiable gradient descent process in the image domain, while a DL-based denoiser, such as a U-Net architecture, is applied to enhance image quality. vSHARP also employs a dilated-convolution DL-based model to predict the Lagrange multipliers for the ADMM initialization. We evaluate vSHARP on tasks of accelerated parallel MRI Reconstruction using two distinct datasets and on accelerated parallel dynamic MRI Reconstruction using another dataset. Our comparative analysis with state-of-the-art methods demonstrates the superior performance of vSHARP in these applications."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "A Scalable Approach to Probabilistic Neuro-Symbolic Verification",
        "Parallelized Planning-Acting for Efficient LLM-based Multi-Agent Systems",
        "Statistical Scenario Modelling and Lookalike Distributions for\n  Multi-Variate AI Risk",
        "OvercookedV2: Rethinking Overcooked for Zero-Shot Coordination",
        "ARES: Auxiliary Range Expansion for Outlier Synthesis",
        "Agent models: Internalizing Chain-of-Action Generation into Reasoning\n  models",
        "Intrinsic Bias is Predicted by Pretraining Data and Correlates with\n  Downstream Performance in Vision-Language Encoders",
        "MIH-TCCT: Mitigating Inconsistent Hallucinations in LLMs via\n  Event-Driven Text-Code Cyclic Training",
        "Can Reasoning Models Reason about Hardware? An Agentic HLS Perspective",
        "Spatiotemporal Prediction of Secondary Crashes by Rebalancing Dynamic\n  and Static Data with Generative Adversarial Networks",
        "SPPD: Self-training with Process Preference Learning Using Dynamic Value\n  Margin",
        "Ensemble based approach to quantifying uncertainty of LLM based\n  classifications",
        "CoPEFT: Fast Adaptation Framework for Multi-Agent Collaborative\n  Perception with Parameter-Efficient Fine-Tuning",
        "Developing and Evaluating an AI-Assisted Prediction Model for Unplanned\n  Intensive Care Admissions following Elective Neurosurgery using Natural\n  Language Processing within an Electronic Healthcare Record System",
        "A Vehicle-Infrastructure Multi-layer Cooperative Decision-making\n  Framework",
        "Three-body structures of low-lying nuclear states of $^8$Li",
        "Nonequilibrium Continuous Transition in a Fast Rotating Turbulence",
        "Training Allostery-Inspired Mechanical Response in Disordered Elastic\n  Networks",
        "Spin glass behavior in amorphous CrSiTe3 alloy",
        "Representative dietary behavior patterns and associations with\n  cardiometabolic outcomes in Puerto Rico using a Bayesian latent class\n  analysis for non-probability samples",
        "On the coefficients of Tutte polynomials with one variable at 1",
        "Enhancing, Refining, and Fusing: Towards Robust Multi-Scale and Dense\n  Ship Detection",
        "Fluctuations in the email size modeled by a log-normal-like distribution",
        "Heterogeneity Matters even More in Distributed Learning: Study from\n  Generalization Perspective",
        "From Dense to Dynamic: Token-Difficulty Driven MoEfication of\n  Pre-Trained LLMs",
        "Differentiable Information Enhanced Model-Based Reinforcement Learning",
        "Underwater Soft Fin Flapping Motion with Deep Neural Network Based\n  Surrogate Model",
        "Automatic detection of single-electron regime of quantum dots and\n  definition of virtual gates using U-Net and clustering"
      ],
      "abstract":[
        "Neuro-Symbolic Artificial Intelligence (NeSy AI) has emerged as a promising\ndirection for integrating neural learning with symbolic reasoning. In the\nprobabilistic variant of such systems, a neural network first extracts a set of\nsymbols from sub-symbolic input, which are then used by a symbolic component to\nreason in a probabilistic manner towards answering a query. In this work, we\naddress the problem of formally verifying the robustness of such NeSy\nprobabilistic reasoning systems, therefore paving the way for their safe\ndeployment in critical domains. We analyze the complexity of solving this\nproblem exactly, and show that it is $\\mathrm{NP}^{\\# \\mathrm{P}}$-hard. To\novercome this issue, we propose the first approach for approximate,\nrelaxation-based verification of probabilistic NeSy systems. We demonstrate\nexperimentally that the proposed method scales exponentially better than\nsolver-based solutions and apply our technique to a real-world autonomous\ndriving dataset, where we verify a safety property under large input\ndimensionalities and network sizes.",
        "Recent advancements in Large Language Model(LLM)-based Multi-Agent\nSystems(MAS) have demonstrated remarkable potential for tackling complex\ndecision-making tasks. However, existing frameworks inevitably rely on\nserialized execution paradigms, where agents must complete sequential LLM\nplanning before taking action. This fundamental constraint severely limits\nreal-time responsiveness and adaptation, which is crucial in dynamic\nenvironments with ever-changing scenarios. In this paper, we propose a novel\nparallelized planning-acting framework for LLM-based MAS, featuring a\ndual-thread architecture with interruptible execution to enable concurrent\nplanning and acting. Specifically, our framework comprises two core threads:(1)\na planning thread driven by a centralized memory system, maintaining\nsynchronization of environmental states and agent communication to support\ndynamic decision-making; and (2) an acting thread equipped with a comprehensive\nskill library, enabling automated task execution through recursive\ndecomposition. Extensive experiments on challenging Minecraft demonstrate the\neffectiveness of the proposed framework.",
        "Evaluating AI safety requires statistically rigorous methods and risk metrics\nfor understanding how the use of AI affects aggregated risk. However, much AI\nsafety literature focuses upon risks arising from AI models in isolation,\nlacking consideration of how modular use of AI affects risk distribution of\nworkflow components or overall risk metrics. There is also a lack of\nstatistical grounding enabling sensitisation of risk models in the presence of\nabsence of AI to estimate causal contributions of AI. This is in part due to\nthe dearth of AI impact data upon which to fit distributions. In this work, we\naddress these gaps in two ways. First, we demonstrate how scenario modelling\n(grounded in established statistical techniques such as Markov chains, copulas\nand Monte Carlo simulation) can be used to model AI risk holistically. Second,\nwe show how lookalike distributions from phenomena analogous to AI can be used\nto estimate AI impacts in the absence of directly observable data. We\ndemonstrate the utility of our methods for benchmarking cumulative AI risk via\nrisk analysis of a logistic scenario simulations.",
        "AI agents hold the potential to transform everyday life by helping humans\nachieve their goals. To do this successfully, agents need to be able to\ncoordinate with novel partners without prior interaction, a setting known as\nzero-shot coordination (ZSC). Overcooked has become one of the most popular\nbenchmarks for evaluating coordination capabilities of AI agents and learning\nalgorithms. In this work, we investigate the origins of ZSC challenges in\nOvercooked. We introduce a state augmentation mechanism which mixes states that\nmight be encountered when paired with unknown partners into the training\ndistribution, reducing the out-of-distribution challenge associated with ZSC.\nWe show that independently trained agents under this algorithm coordinate\nsuccessfully in Overcooked. Our results suggest that ZSC failure can largely be\nattributed to poor state coverage under self-play rather than more\nsophisticated coordination challenges. The Overcooked environment is therefore\nnot suitable as a ZSC benchmark. To address these shortcomings, we introduce\nOvercookedV2, a new version of the benchmark, which includes asymmetric\ninformation and stochasticity, facilitating the creation of interesting ZSC\nscenarios. To validate OvercookedV2, we conduct experiments demonstrating that\nmere exhaustive state coverage is insufficient to coordinate well. Finally, we\nuse OvercookedV2 to build a new range of coordination challenges, including\nones that require test time protocol formation, and we demonstrate the need for\nnew coordination algorithms that can adapt online. We hope that OvercookedV2\nwill help benchmark the next generation of ZSC algorithms and advance\ncollaboration between AI agents and humans.",
        "Recent successes of artificial intelligence and deep learning often depend on\nthe well-collected training dataset which is assumed to have an identical\ndistribution with the test dataset. However, this assumption, which is called\nclosed-set learning, is hard to meet in realistic scenarios for deploying deep\nlearning models. As one of the solutions to mitigate this assumption, research\non out-of-distribution (OOD) detection has been actively explored in various\ndomains. In OOD detection, we assume that we are given the data of a new class\nthat was not seen in the training phase, i.e., outlier, at the evaluation\nphase. The ultimate goal of OOD detection is to detect and classify such unseen\noutlier data as a novel \"unknown\" class. Among various research branches for\nOOD detection, generating a virtual outlier during the training phase has been\nproposed. However, conventional generation-based methodologies utilize\nin-distribution training dataset to imitate outlier instances, which limits the\nquality of the synthesized virtual outlier instance itself. In this paper, we\npropose a novel methodology for OOD detection named Auxiliary Range Expansion\nfor Outlier Synthesis, or ARES. ARES models the region for generating\nout-of-distribution instances by escaping from the given in-distribution\nregion; instead of remaining near the boundary of in-distribution region.\nVarious stages consists ARES to ultimately generate valuable OOD-like virtual\ninstances. The energy score-based discriminator is then trained to effectively\nseparate in-distribution data and outlier data. Quantitative experiments on\nbroad settings show the improvement of performance by our method, and\nqualitative results provide logical explanations of the mechanism behind it.",
        "Traditional agentic workflows rely on external prompts to manage interactions\nwith tools and the environment, which limits the autonomy of reasoning models.\nWe position \\emph{Large Agent Models (LAMs)} that internalize the generation of\n\\emph{Chain-of-Action (CoA)}, enabling the model to autonomously decide when\nand how to use external tools. Our proposed AutoCoA framework combines\nsupervised fine-tuning (SFT) and reinforcement learning (RL), allowing the\nmodel to seamlessly switch between reasoning and action while efficiently\nmanaging environment interactions. Main components include step-level action\ntriggering, trajectory-level CoA optimization, and an internal world model to\nreduce real-environment interaction costs. Evaluations on open-domain QA tasks\ndemonstrate that AutoCoA-trained agent models significantly outperform\nReAct-based workflows in task completion, especially in tasks that require\nlong-term reasoning and multi-step actions. Code and dataset are available at\nhttps:\/\/github.com\/ADaM-BJTU\/AutoCoA",
        "While recent work has found that vision-language models trained under the\nContrastive Language Image Pre-training (CLIP) framework contain intrinsic\nsocial biases, the extent to which different upstream pre-training features of\nthe framework relate to these biases, and hence how intrinsic bias and\ndownstream performance are connected has been unclear. In this work, we present\nthe largest comprehensive analysis to-date of how the upstream pre-training\nfactors and downstream performance of CLIP models relate to their intrinsic\nbiases. Studying 131 unique CLIP models, trained on 26 datasets, using 55\narchitectures, and in a variety of sizes, we evaluate bias in each model using\n26 well-established unimodal and cross-modal principled Embedding Association\nTests. We find that the choice of pre-training dataset is the most significant\nupstream predictor of bias, whereas architectural variations have minimal\nimpact. Additionally, datasets curated using sophisticated filtering techniques\naimed at enhancing downstream model performance tend to be associated with\nhigher levels of intrinsic bias. Finally, we observe that intrinsic bias is\noften significantly correlated with downstream performance ($0.3 \\leq r \\leq\n0.8$), suggesting that models optimized for performance inadvertently learn to\namplify representational biases. Comparisons between unimodal and cross-modal\nassociation tests reveal that social group bias depends heavily on the\nmodality. Our findings imply that more sophisticated strategies are needed to\naddress intrinsic model bias for vision-language models across the entire model\ndevelopment pipeline.",
        "Recent methodologies utilizing synthetic datasets have aimed to address\ninconsistent hallucinations in large language models (LLMs); however,these\napproaches are primarily tailored to specific tasks, limiting their\ngeneralizability. Inspired by the strong performance of code-trained models in\nlogic-intensive domains, we propose a novel framework that leverages\nevent-based text to generate corresponding code and employs cyclic training to\ntransfer the logical consistency of code to natural language effectively. Our\nmethod significantly reduces inconsistent hallucinations across three leading\nLLMs and two categories of natural language tasks while maintaining overall\nperformance. This framework effectively alleviates hallucinations without\nnecessitating adaptation to downstream tasks, demonstrating generality and\nproviding new perspectives to tackle the challenge of inconsistent\nhallucinations.",
        "Recent Large Language Models (LLMs) such as OpenAI o3-mini and DeepSeek-R1\nuse enhanced reasoning through Chain-of-Thought (CoT). Their potential in\nhardware design, which relies on expert-driven iterative optimization, remains\nunexplored. This paper investigates whether reasoning LLMs can address\nchallenges in High-Level Synthesis (HLS) design space exploration and\noptimization. During HLS, engineers manually define pragmas\/directives to\nbalance performance and resource constraints. We propose an LLM-based\noptimization agentic framework that automatically restructures code, inserts\npragmas, and identifies optimal design points via feedback from HLs tools and\naccess to integer-linear programming (ILP) solvers. Experiments compare\nreasoning models against conventional LLMs on benchmarks using success rate,\nefficiency, and design quality (area\/latency) metrics, and provide the\nfirst-ever glimpse into the CoTs produced by a powerful open-source reasoning\nmodel like DeepSeek-R1.",
        "Data imbalance is a common issue in analyzing and predicting sudden traffic\nevents. Secondary crashes constitute only a small proportion of all crashes.\nThese secondary crashes, triggered by primary crashes, significantly exacerbate\ntraffic congestion and increase the severity of incidents. However, the severe\nimbalance of secondary crash data poses significant challenges for prediction\nmodels, affecting their generalization ability and prediction accuracy.\nExisting methods fail to fully address the complexity of traffic crash data,\nparticularly the coexistence of dynamic and static features, and often struggle\nto effectively handle data samples of varying lengths. Furthermore, most\ncurrent studies predict the occurrence probability and spatiotemporal\ndistribution of secondary crashes separately, lacking an integrated solution.\nTo address these challenges, this study proposes a hybrid model named\nVarFusiGAN-Transformer, aimed at improving the fidelity of secondary crash data\ngeneration and jointly predicting the occurrence and spatiotemporal\ndistribution of secondary crashes. The VarFusiGAN-Transformer model employs\nLong Short-Term Memory (LSTM) networks to enhance the generation of\nmultivariate long-time series data, incorporating a static data generator and\nan auxiliary discriminator to model the joint distribution of dynamic and\nstatic features. In addition, the model's prediction module achieves\nsimultaneous prediction of both the occurrence and spatiotemporal distribution\nof secondary crashes. Compared to existing methods, the proposed model\ndemonstrates superior performance in generating high-fidelity data and\nimproving prediction accuracy.",
        "Recently, enhancing the numerical and logical reasoning capability of Large\nLanguage Models (LLMs) has emerged as a research hotspot. Existing methods face\nseveral limitations: inference-phase techniques (e.g., Chain of Thoughts) rely\non prompt selection and the pretrained knowledge; sentence-level Supervised\nFine-Tuning (SFT) and Direct Preference Optimization (DPO) struggle with\nstep-wise mathematical correctness and depend on stronger models distillation\nor human annotations; while Reinforcement Learning (RL) approaches incur high\nGPU memory costs and unstable training. To address these, we propose\n\\textbf{S}elf-training framework integrating \\textbf{P}rocess\n\\textbf{P}reference learning using \\textbf{D}ynamic value margin (SPPD). SPPD\nleverages a process-based Markov Decision Process (MDP) and Bellman optimality\nequation to derive \\textbf{dynamic value margin} on step-level preference\noptimization, which employs tree-based self-sampling on model responses\n\\textbf{without any distillation} from other models. Furthermore, we\ntheoretically prove that SPPD is \\textbf{equivalent to on-policy policy\ngradient methods} under reward constraints. Experiments on 7B-scale models\ndemonstrate superior performance across in-domain and out-domain mathematical\nbenchmarks. We open-source our code at\n\\href{https:\/\/anonymous.4open.science\/r\/SSDPO-D-DCDD}{https:\/\/anonymous.4open.science\/r\/SPPD-DCDD}.",
        "The output of Large Language Models (LLMs) are a function of the internal\nmodel's parameters and the input provided into the context window. The\nhypothesis presented here is that under a greedy sampling strategy the variance\nin the LLM's output is a function of the conceptual certainty embedded in the\nmodel's parametric knowledge, as well as the lexical variance in the input.\nFinetuning the model results in reducing the sensitivity of the model output to\nthe lexical input variations. This is then applied to a classification problem\nand a probabilistic method is proposed for estimating the certainties of the\npredicted classes.",
        "Multi-agent collaborative perception is expected to significantly improve\nperception performance by overcoming the limitations of single-agent perception\nthrough exchanging complementary information. However, training a robust\ncollaborative perception model requires collecting sufficient training data\nthat covers all possible collaboration scenarios, which is impractical due to\nintolerable deployment costs. Hence, the trained model is not robust against\nnew traffic scenarios with inconsistent data distribution and fundamentally\nrestricts its real-world applicability. Further, existing methods, such as\ndomain adaptation, have mitigated this issue by exposing the deployment data\nduring the training stage but incur a high training cost, which is infeasible\nfor resource-constrained agents. In this paper, we propose a\nParameter-Efficient Fine-Tuning-based lightweight framework, CoPEFT, for fast\nadapting a trained collaborative perception model to new deployment\nenvironments under low-cost conditions. CoPEFT develops a Collaboration Adapter\nand Agent Prompt to perform macro-level and micro-level adaptations separately.\nSpecifically, the Collaboration Adapter utilizes the inherent knowledge from\ntraining data and limited deployment data to adapt the feature map to new data\ndistribution. The Agent Prompt further enhances the Collaboration Adapter by\ninserting fine-grained contextual information about the environment. Extensive\nexperiments demonstrate that our CoPEFT surpasses existing methods with less\nthan 1\\% trainable parameters, proving the effectiveness and efficiency of our\nproposed method.",
        "Introduction: Timely care in a specialised neuro-intensive therapy unit (ITU)\nreduces mortality and hospital stays, with planned admissions being safer than\nunplanned ones. However, post-operative care decisions remain subjective. This\nstudy used artificial intelligence (AI), specifically natural language\nprocessing (NLP) to analyse electronic health records (EHRs) and predict ITU\nadmissions for elective surgery patients. Methods: This study analysed the EHRs\nof elective neurosurgery patients from University College London Hospital\n(UCLH) using NLP. Patients were categorised into planned high dependency unit\n(HDU) or ITU admission; unplanned HDU or ITU admission; or ward \/ overnight\nrecovery (ONR). The Medical Concept Annotation Tool (MedCAT) was used to\nidentify SNOMED-CT concepts within the clinical notes. We then explored the\nutility of these identified concepts for a range of AI algorithms trained to\npredict ITU admission. Results: The CogStack-MedCAT NLP model, initially\ntrained on hospital-wide EHRs, underwent two refinements: first with data from\npatients with Normal Pressure Hydrocephalus (NPH) and then with data from\nVestibular Schwannoma (VS) patients, achieving a concept detection F1-score of\n0.93. This refined model was then used to extract concepts from EHR notes of\n2,268 eligible neurosurgical patients. We integrated the extracted concepts\ninto AI models, including a decision tree model and a neural time-series model.\nUsing the simpler decision tree model, we achieved a recall of 0.87 (CI 0.82 -\n0.91) for ITU admissions, reducing the proportion of unplanned ITU cases missed\nby human experts from 36% to 4%. Conclusion: The NLP model, refined for\naccuracy, has proven its efficiency in extracting relevant concepts, providing\na reliable basis for predictive AI models to use in clinically valid\napplications.",
        "Autonomous driving has entered the testing phase, but due to the limited\ndecision-making capabilities of individual vehicle algorithms, safety and\nefficiency issues have become more apparent in complex scenarios. With the\nadvancement of connected communication technologies, autonomous vehicles\nequipped with connectivity can leverage vehicle-to-vehicle (V2V) and\nvehicle-to-infrastructure (V2I) communications, offering a potential solution\nto the decision-making challenges from individual vehicle's perspective. We\npropose a multi-level vehicle-infrastructure cooperative decision-making\nframework for complex conflict scenarios at unsignalized intersections. First,\nbased on vehicle states, we define a method for quantifying vehicle impacts and\ntheir propagation relationships, using accumulated impact to group vehicles\nthrough motif-based graph clustering. Next, within and between vehicle groups,\na pass order negotiation process based on Large Language Models (LLM) is\nemployed to determine the vehicle passage order, resulting in planned vehicle\nactions. Simulation results from ablation experiments show that our approach\nreduces negotiation complexity and ensures safer, more efficient vehicle\npassage at intersections, aligning with natural decision-making logic.",
        "The four nucleons in $^8$Li outside the $\\alpha$-particle ($\\alpha=^4$He) can\nbe divided into pairs of one neutron ($n$) and 3 nucleons in the triton\n($t=^3$H), or 2 in the deuteron ($d=^2$H) and two neutrons in a dineutron\n($^2n$). The corresponding three-body structures, $\\alpha$+$t$+$n$ or\n$\\alpha$+$d$+$^2n$, are suggested to describe the bulk part of the low-energy\n($<10$~MeV) states of $^8$Li. Several breakup thresholds influence the\nstructures and possible decays. We calculate the three-body structures of the\nvarious $J^{\\pi}$ states, where different clustering appear, e.g. $^7$Li*+$n$,\n$^6$Li*$+^2n$, $^6$He*$+d$. The experimental $^8$Li spectrum can be reproduced\nwith fine tuning by a three-body potential parameter. Three unobserved $0^+$\nand an excited 2$^+$ states are found. All states appear as bound states or\nresonances. The lowest or highest energies have cluster structures,\n$\\alpha$+$t$+$n$ or $\\alpha$+$d$+$^2n$, respectively. We give calculated energy\nand width (if possible), geometry, and partial wave decomposition for all\nstates.",
        "We study the saturation of three-dimensional unstable perturbations on a fast\nrotating turbulent flow using direct numerical simulations (DNSs). Under the\neffect of Kolmogorov forcing, a transition between states dominated by coherent\ntwo-dimensional modes to states with three-dimensional variations\n(quasi-two-dimensional) is observed as we change the global rotation rate. We\nfind this akin to a critical phenomenon, wherein the order parameter scales\nwith the distance to the critical point raised to an exponent. The exponent\nitself deviates from the predicted mean field value. Also, the nature of the\nfluctuations of the order parameter near the critical point indicate the\npresence of on-off intermittency. The critical rotation rate at which the\ntransition occurs exhibits a linear scaling behaviour with the forcing wave\nnumber. A reduced model based on linear stability analysis is used to find the\nlinear threshold estimates; we find these to be in good agreement with the 3D\nnonlinear DNS results.",
        "Disordered elastic networks are a model material system in which it is\npossible to achieve tunable and trainable functions. This work investigates the\nmodification of local mechanical properties in disordered networks inspired by\nallosteric interactions in proteins: applying strain locally to a set of source\nnodes triggers a strain response at a distant set of target nodes. This is\ndemonstrated first by using directed aging to modify the existing mechanical\ncoupling between pairs of distant source and target nodes, and later as a means\nfor inducing coupling between formerly isolated source-target pairs. The\nexperimental results are compared with those predicted by simulations.",
        "Owing to the intrinsically high crystallization temperatures, layered\nphase-change materials, such as CrGeTe3 and InGeTe3, are attracting attention\nfor embedded memory applications, In addition to the electrical contrast, a\nmajor change in magnetic properties is observed in CrGeTe3 upon switching from\nthe crystalline to the amorphous state. In this work, we report a combined ab\ninitio modeling and magnetic characterization study on the isostructural\nsilicon parent compound of CrGeTe3, namely, CrSiTe3. Amorphous CrSiTe3 has\nsimilar structural properties to amorphous CrGeTe3; however, it shows a smaller\nenergy difference between the ferromagnetic configuration and the random\nmagnetic configuration, indicating a high probability of spin glass formation.\nIndeed, direct-current and alternating-current magnetic measurements show that\nthe coercive force of amorphous CrSiTe3 is higher than that of amorphous\nCrGeTe3. Therefore, the pinning effect of spins is enhanced in amorphous\nCrSiTe3, leading to a more robust spin glass state with a higher freezing\ntemperature. The large magnetic contrast between the amorphous and crystalline\nphase could make CrSiTe3 a potential candidate for phase-change magnetic\nswitching applications.",
        "There is limited understanding of how dietary behaviors cluster together and\ninfluence cardiometabolic health at a population level in Puerto Rico. Data\navailability is scarce, particularly outside of urban areas, and is often\nlimited to non-probability sample (NPS) data where sample inclusion mechanisms\nare unknown. In order to generalize results to the broader Puerto Rican\npopulation, adjustments are necessary to account for selection bias but are\ndifficult to implement for NPS data. Although Bayesian latent class models\nenable summaries of dietary behavior variables through underlying patterns,\nthey have not yet been adapted to the NPS setting. We propose a novel Weighted\nOverfitted Latent Class Analysis for Non-probability samples (WOLCAN). WOLCAN\nutilizes a quasi-randomization framework to (1) model pseudo-weights for an NPS\nusing Bayesian additive regression trees (BART) and a reference probability\nsample, and (2) integrate the pseudo-weights within a weighted\npseudo-likelihood approach for Bayesian latent class analysis, while\npropagating pseudo-weight uncertainty into parameter estimation. A stacked\nsample approach is used to allow shared individuals between the NPS and the\nreference sample. We evaluate model performance through simulations and apply\nWOLCAN to data from the Puerto Rico Observational Study of Psychosocial,\nEnvironmental, and Chronic Disease Trends (PROSPECT). We identify dietary\nbehavior patterns for adults in Puerto Rico aged 30 to 75 and examine their\nassociations with type 2 diabetes, hypertension, and hypercholesterolemia. Our\nfindings suggest that an out-of-home eating pattern is associated with a higher\nlikelihood of these cardiometabolic outcomes compared to a nutrition-sensitive\npattern. WOLCAN effectively reveals generalizable dietary behavior patterns and\ndemonstrates relevant applications in studying diet-disease relationships.",
        "Denote the Tutte polynomial of a graph $G$ and a matroid $M$ by $T_G(x,y)$\nand $T_M(x,y)$ respectively. $T_G(x,1)$ and $T_G(1,y)$ were generalized to\nhypergraphs and further extended to integer polymatroids by K\\'{a}lm\\'{a}n\n\\cite{Kalman} in 2013, called interior and exterior polynomials respectively.\nLet $G$ be a $(k+1)$-edge connected graph of order $n$ and size $m$, and let\n$g=m-n+1$. Guan et al. (2023) \\cite{Guan} obtained the coefficients of\n$T_G(1,y)$: \\[[y^j]T_G(1,y)=\\binom{m-j-1}{n-2} \\text{ for } g-k\\leq j\\leq g,\\]\nwhich was deduced from coefficients of the exterior polynomial of polymatroids.\nRecently, Chen and Guo (2025) \\cite{Chen} further obtained\n\\[[y^j]T_G(1,y)=\\binom{m-j-1}{n-2}-\\sum_{i=k+1}^{g-j}\\binom{m-j-i-1}{n-2}|\\mathcal{EC}_i(G)|\\]\nfor $g-3(k+1)\/2< j\\leq g$, where $\\mathcal{EC}_i(G)$ denotes the set of all\nminimal edge cuts with $i$ edges. In this paper, for any matroid $M=(X,rk)$ we\nfirst obtain\n\\[[y^j]T_M(1,y)=\\sum_{t=j}^{|X|-r}(-1)^{t-j}\\binom{t}{j}\\sigma_{r+t}(M),\\]\nwhere $\\sigma_{r+t}(M)$ denotes the number of spanning sets with $r+t$ elements\nin $M$ and $r=rk(M)$. Moveover, the expression of $[x^i]T_M(x,1)$ is obtained\nimmediately from the duality of the Tutte polynomial. As applications of our\nresults, we generalize the two aforementioned results on graphs to the setting\nof matroids. This not only resolves two open problems posed by Chen and Guo in\n\\cite{Chen} but also provides a purely combinatorial proof that is\nsignificantly simpler than their original proofs.",
        "Synthetic aperture radar (SAR) imaging, celebrated for its high resolution,\nall-weather capability, and day-night operability, is indispensable for\nmaritime applications. However, ship detection in SAR imagery faces significant\nchallenges, including complex backgrounds, densely arranged targets, and large\nscale variations. To address these issues, we propose a novel framework,\nCenter-Aware SAR Ship Detector (CASS-Det), designed for robust multi-scale and\ndensely packed ship detection. CASS-Det integrates three key innovations: (1) a\ncenter enhancement module (CEM) that employs rotational convolution to\nemphasize ship centers, improving localization while suppressing background\ninterference; (2) a neighbor attention module (NAM) that leverages cross-layer\ndependencies to refine ship boundaries in densely populated scenes; and (3) a\ncross-connected feature pyramid network (CC-FPN) that enhances multi-scale\nfeature fusion by integrating shallow and deep features. Extensive experiments\non the SSDD, HRSID, and LS-SSDD-v1.0 datasets demonstrate the state-of-the-art\nperformance of CASS-Det, excelling at detecting multi-scale and densely\narranged ships.",
        "A previously established frequency distribution model combining a log-normal\ndistribution with a logarithmic equation describes fluctuations in the email\nsize during send requests. Although the frequency distribution fit was\nconsidered satisfactory, the underlying mechanism driving this distribution\nremains inadequately explained. To address this gap, this study introduced a\nnovel email-send model that characterizes the sending process as an exponential\nfunction modulated by noise from a normal distribution. This model is\nconsistent with both the observed frequency distribution and the previously\nproposed frequency distribution model.",
        "In this paper, we investigate the effect of data heterogeneity across clients\non the performance of distributed learning systems, i.e., one-round Federated\nLearning, as measured by the associated generalization error. Specifically,\n\\(K\\) clients have each \\(n\\) training samples generated independently\naccording to a possibly different data distribution and their individually\nchosen models are aggregated by a central server. We study the effect of the\ndiscrepancy between the clients' data distributions on the generalization error\nof the aggregated model. First, we establish in-expectation and tail upper\nbounds on the generalization error in terms of the distributions. In part, the\nbounds extend the popular Conditional Mutual Information (CMI) bound which was\ndeveloped for the centralized learning setting, i.e., \\(K=1\\), to the\ndistributed learning setting with arbitrary number of clients $K \\geq 1$. Then,\nwe use a connection with information theoretic rate-distortion theory to derive\npossibly tighter \\textit{lossy} versions of these bounds. Next, we apply our\nlossy bounds to study the effect of data heterogeneity across clients on the\ngeneralization error for distributed classification problem in which each\nclient uses Support Vector Machines (D-SVM). In this case, we establish\nexplicit generalization error bounds which depend explicitly on the data\nheterogeneity degree. It is shown that the bound gets smaller as the degree of\ndata heterogeneity across clients gets higher, thereby suggesting that D-SVM\ngeneralizes better when the dissimilarity between the clients' training samples\nis bigger. This finding, which goes beyond D-SVM, is validated experimentally\nthrough a number of experiments.",
        "Training large language models (LLMs) for different inference constraints is\ncomputationally expensive, limiting control over efficiency-accuracy\ntrade-offs. Moreover, once trained, these models typically process tokens\nuniformly, regardless of their complexity, leading to static and inflexible\nbehavior. In this paper, we introduce a post-training optimization framework,\nDynaMoE, that adapts a pre-trained dense LLM to a token-difficulty-driven\nMixture-of-Experts model with minimal fine-tuning cost. This adaptation makes\nthe model dynamic, with sensitivity control to customize the balance between\nefficiency and accuracy. DynaMoE features a token-difficulty-aware router that\npredicts the difficulty of tokens and directs them to the appropriate\nsub-networks or experts, enabling larger experts to handle more complex tokens\nand smaller experts to process simpler ones. Our experiments demonstrate that\nDynaMoE can generate a range of adaptive model variants of the existing trained\nLLM with a single fine-tuning step, utilizing only $10B$ tokens, a minimal cost\ncompared to the base model's training. Each variant offers distinct trade-offs\nbetween accuracy and performance. Compared to the baseline post-training\noptimization framework, Flextron, our method achieves similar aggregated\naccuracy across downstream tasks, despite using only $\\frac{1}{9}\\text{th}$ of\ntheir fine-tuning cost.",
        "Differentiable environments have heralded new possibilities for learning\ncontrol policies by offering rich differentiable information that facilitates\ngradient-based methods. In comparison to prevailing model-free reinforcement\nlearning approaches, model-based reinforcement learning (MBRL) methods exhibit\nthe potential to effectively harness the power of differentiable information\nfor recovering the underlying physical dynamics. However, this presents two\nprimary challenges: effectively utilizing differentiable information to 1)\nconstruct models with more accurate dynamic prediction and 2) enhance the\nstability of policy training. In this paper, we propose a Differentiable\nInformation Enhanced MBRL method, MB-MIX, to address both challenges. Firstly,\nwe adopt a Sobolev model training approach that penalizes incorrect model\ngradient outputs, enhancing prediction accuracy and yielding more precise\nmodels that faithfully capture system dynamics. Secondly, we introduce mixing\nlengths of truncated learning windows to reduce the variance in policy gradient\nestimation, resulting in improved stability during policy learning. To validate\nthe effectiveness of our approach in differentiable environments, we provide\ntheoretical analysis and empirical results. Notably, our approach outperforms\nprevious model-based and model-free methods, in multiple challenging tasks\ninvolving controllable rigid robots such as humanoid robots' motion control and\ndeformable object manipulation.",
        "This study presents a novel framework for precise force control of\nfin-actuated underwater robots by integrating a deep neural network (DNN)-based\nsurrogate model with reinforcement learning (RL). To address the complex\ninteractions with the underwater environment and the high experimental costs, a\nDNN surrogate model acts as a simulator for enabling efficient training for the\nRL agent. Additionally, grid-switching control is applied to select optimized\nmodels for specific force reference ranges, improving control accuracy and\nstability. Experimental results show that the RL agent, trained in the\nsurrogate simulation, generates complex thrust motions and achieves precise\ncontrol of a real soft fin actuator. This approach provides an efficient\ncontrol solution for fin-actuated robots in challenging underwater\nenvironments.",
        "To realize practical quantum computers, a large number of quantum bits\n(qubits) will be required. Semiconductor spin qubits offer advantages such as\nhigh scalability and compatibility with existing semiconductor technologies.\nHowever, as the number of qubits increases, manual qubit tuning becomes\ninfeasible, motivating automated tuning approaches. In this study, we use\nU-Net, a neural network method for object detection, to identify charge\ntransition lines in experimental charge stability diagrams. The extracted\ncharge transition lines are analyzed using the Hough transform to determine\ntheir positions and angles. Based on this analysis, we obtain the\ntransformation matrix to virtual gates. Furthermore, we identify the\nsingle-electron regime by clustering the Hough transform outputs. We also show\nthe single-electron regime within the virtual gate space. These sequential\nprocesses are performed automatically. This approach will advance automated\ncontrol technologies for large-scale quantum devices."
      ]
    }
  },
  {
    "id":2411.01291,
    "research_type":"applied",
    "start_id":"b19",
    "start_title":"vSHARP: variable Splitting Half-quadratic ADMM algorithm for Reconstruction of inverse-Problems",
    "start_abstract":"Medical Imaging (MI) tasks, such as accelerated parallel Magnetic Resonance Imaging (MRI), often involve reconstructing an image from noisy or incomplete measurements. This amounts to solving ill-posed inverse problems, where a satisfactory closed-form analytical solution is not available. Traditional methods such as Compressed Sensing (CS) in MRI reconstruction can be time-consuming or prone to obtaining low-fidelity images. Recently, a plethora of Deep Learning (DL) approaches have demonstrated superior performance in inverse-problem solving, surpassing conventional methods. In this study, we propose vSHARP (variable Splitting Half-quadratic ADMM algorithm for Reconstruction of inverse Problems), a novel DL-based method for solving ill-posed inverse problems arising in MI. vSHARP utilizes the Half-Quadratic Variable Splitting method and employs the Alternating Direction Method of Multipliers (ADMM) to unroll the optimization process. For data consistency, vSHARP unrolls a differentiable gradient descent process in the image domain, while a DL-based denoiser, such as a U-Net architecture, is applied to enhance image quality. vSHARP also employs a dilated-convolution DL-based model to predict the Lagrange multipliers for the ADMM initialization. We evaluate vSHARP on tasks of accelerated parallel MRI Reconstruction using two distinct datasets and on accelerated parallel dynamic MRI Reconstruction using another dataset. Our comparative analysis with state-of-the-art methods demonstrates the superior performance of vSHARP in these applications.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b15"
      ],
      "title":[
        "CMRxRecon2024: A Multi-Modality, Multi-View K-Space Dataset Boosting\n  Universal Machine Learning for Accelerated Cardiac MRI"
      ],
      "abstract":[
        "Cardiac magnetic resonance imaging (MRI) has emerged as a clinically gold-standard technique for diagnosing cardiac diseases, thanks to its ability provide diverse information with multiple modalities and anatomical views. Accelerated MRI is highly expected achieve time-efficient patient-friendly imaging, then advanced image reconstruction approaches are required recover high-quality, interpretable images from undersampled measurements. However, the lack of publicly available k-space dataset in terms both quantity diversity severely hindered substantial technological progress, particularly data-driven artificial intelligence. Here, we standardized, diverse, high-quality CMRxRecon2024 facilitate technical development, fair evaluation, clinical transfer approaches, towards promoting universal frameworks that enable fast robust reconstructions across different protocols practice. To best our knowledge, largest most dataset. It acquired 330 healthy volunteers, covering commonly used modalities, views, acquisition trajectories workflows. Besides, an open platform tutorials, benchmarks, data processing tools provided usage, method performance evaluation."
      ],
      "categories":[
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "Photodynamic, UV-curable and fibre-forming polyvinyl alcohol derivative\n  with broad processability and staining-free antibacterial capability",
        "Simplified model of immunotherapy for glioblastoma multiforme: cancer\n  stem cells hypothesis perspective",
        "Unveiling sex dimorphism in the healthy cardiac anatomy: fundamental\n  differences between male and female heart shapes",
        "Integrating anatomy and electrophysiology in the healthy human heart:\n  Insights from biventricular statistical shape analysis using universal\n  coordinates",
        "Bridging high resolution sub-cellular imaging with physiologically\n  relevant engineered tissues",
        "Learnable Group Transform: Enhancing Genotype-to-Phenotype Prediction\n  for Rice Breeding with Small, Structured Datasets",
        "An Efficient Algorithm for Determining the Equivalence of Zero-one\n  Reaction Networks",
        "Advanced 3D-Printed Multiphasic Scaffold with Optimal PRP Dosage for\n  Chondrogenesis of BM-MSCs in Osteochondral Tissue Engineering",
        "Geometric immunosuppression in CAR-T cell treatment: Insights from\n  mathematical modeling",
        "Tumor microenvironment (Part I): Tissue integrity in a rat model of\n  peripheral neural cancer",
        "A tumor-immune model of chronic myeloid leukemia with optimal\n  immunotherapeutic protocols",
        "Effect of a new type of healthy and live food supplement on osteoporosis\n  blood parameters and induced rheumatoid arthritis in Wistar rats",
        "Practical parameter identifiability of respiratory mechanics in the\n  extremely preterm infant",
        "$\\tt GrayHawk$: A public code for calculating the Gray Body Factors of\n  massless fields around spherically symmetric Black Holes",
        "MinGRU-Based Encoder for Turbo Autoencoder Frameworks",
        "Type semigroups for twisted groupoids and a dichotomy for groupoid\n  C*-algebras",
        "Algebraic surfaces as Hadamard products of curves",
        "$C^{1}$-Stable-Manifolds for Periodic Heteroclinic Chains in Bianchi IX:\n  Symbolic Computations and Statistical Properties",
        "Slow Light Waveguides based on Bound States in the Continuum",
        "High-Accuracy Physical Property Prediction for Organics via Molecular\n  Representation Learning: Bridging Data to Discovery",
        "Identical Suppression of Spin and Charge Density Wave Transitions in\n  La$_4$Ni$_3$O$_{10}$ by Pressure",
        "A Relaxed Wasserstein Distance Formulation for Mixtures of Radially\n  Contoured Distributions",
        "Weight Distribution of the Weighted Coordinates Poset Block Space and\n  Singleton Bound",
        "Cold dark gas in Cygnus X: The first large-scale mapping of\n  low-frequency carbon recombination lines",
        "Properties of Turnpike Functions for Discounted Finite MDPs",
        "Raman Forbidden Layer-Breathing Modes in Layered Semiconductor Materials\n  Activated by Phonon and Optical Cavity Effects",
        "Anomalous temperature-dependent magnetization in the nearly collinear\n  antiferromagnet Y$_2$Co$_3$",
        "The nebular spectra of SN 2023ixf: A lower mass, partially stripped\n  progenitor may be the result of binary interaction"
      ],
      "abstract":[
        "Antimicrobial photodynamic therapy (APDT) is a promising antibiotic-free\nstrategy for broad-spectrum infection control in chronic wounds, minimising\nbacterial resistance risks. However, rapid photosensitiser diffusion, tissue\nstaining, side toxicity, and short-lived antimicrobial effects present\nsignificant clinical limitations for integrating APDT into wound dressings. To\naddress these challenges, we present the design of a bespoke polyvinyl alcohol\n(PVA) derivative conjugated with both phenothiazine and methacrylate\nfunctionalities, enabling staining-free antibacterial photodynamic effects,\ncellular tolerability and processability into various wound dressing formats,\nincluding films, textile fibres and nanoscale coatings. Tosylation of PVA is\nleveraged for the covalent coupling of toluidine blue, as confirmed by UV-Vis\nspectroscopy and the minimal release of TB in vitro. UV-induced network\nformation is exploited to accomplish cast films and nanoscale integrated wound\ndressing coatings. UV curing is also successfully coupled with an in-house wet\nspinning process to realise individual, water-insoluble fibres as the building\nblocks of fibrous wound dressings. A fluorometric assay supports the generation\nof reactive oxygen species when the UV-cured samples are exposed to work, but\nnot UV, light, yielding a mean log10 reduction of up to 2.13 in S. aureus, and\nthe complete eradication of P. aeruginosa. Direct and extract cytotoxicity\ntests with UV-cured films and fibres demonstrate the viability of L929\nfibroblasts following 60-min light irradiation and 72-hour cell culture. The\nbespoke molecular architecture, broad processability and cellular tolerability\nof this PVA derivative are highly attractive aiming to integrate durable\nstaining-free photodynamic capability in a wide range of healthcare\ntechnologies, from chronic wound dressings up to minimally invasive localised\ntherapy.",
        "Despite ongoing efforts in cancer research, a fully effective treatment for\nglioblastoma multiforme (GBM) is still unknown. Since adoptive cell transfer\nimmunotherapy is one of the potential cure candidates, efforts have been made\nto assess its effectiveness using mathematical modeling. In this paper, we\nconsider a model of GBM immunotherapy proposed by Abernathy and Burke (2016),\nwhich also takes into account the dynamics of cancer stem cells, i.e., the type\nof cancer cells that are hypothesized to be largely responsible for cancer\nrecurrence. We modify the initial ODE system by applying simplifying\nassumptions and analyze the existence and stability of steady states of the\nobtained simplified model depending on the treatment levels.",
        "Sex-based differences in cardiovascular disease are well documented, yet the\nprecise nature and extent of these discrepancies in cardiac anatomy remain\nincompletely understood. Traditional scaling models often fail to capture the\ninterplay of age, blood pressure, and body size, prompting a more nuanced\ninvestigation. Here, we employ statistical shape modeling in a healthy subset\n(n=456) of the UK Biobank to explore sex-specific variations in biventricular\nanatomy. We reconstruct 3D meshes and perform multivariate analyses of shape\ncoefficients, controlling for age, blood pressure, and various body size\nmetrics. Our findings reveal that sex alone explains at least 25 percent of\nmorphological variability, with strong discrimination between men and women\n(AUC=0.96-0.71) persisting even after correction for confounders. Notably, the\nmost discriminative modes highlight pronounced differences in cardiac chamber\nvolumes, the anterior-posterior width of the right ventricle, and the relative\npositioning of the cardiac chambers. These results underscore that sex has a\nfundamental influence on cardiac morphology, which may have important clinical\nimplications for differing cardiac structural assessments in men and women.\nFuture work should investigate how these anatomical differences manifest in\nvarious cardiovascular conditions, ultimately paving the way for more precise\nrisk stratification and personalized therapeutic strategies for both men and\nwomen.",
        "A cardiac digital twin is a virtual replica of a patient-specific heart,\nmimicking its anatomy and physiology. A crucial step of building a cardiac\ndigital twin is anatomical twinning, where the computational mesh of the\ndigital twin is tailored to the patient-specific cardiac anatomy. In a number\nof studies, the effect of anatomical variation on clinically relevant\nfunctional measurements like electrocardiograms (ECGs) is investigated, using\ncomputational simulations. While such a simulation environment provides\nresearchers with a carefully controlled ground truth, the impact of anatomical\ndifferences on functional measurements in real-world patients remains\nunderstudied. In this study, we develop a biventricular statistical shape model\nand use it to quantify the effect of biventricular anatomy on ECG-derived and\ndemographic features, providing novel insights for the development of digital\ntwins of cardiac electrophysiology. To this end, a dataset comprising\nhigh-resolution cardiac CT scans from 271 healthy individuals, including\nathletes, is utilized. Furthermore, a novel, universal, ventricular\ncoordinate-based method is developed to establish lightweight shape\ncorrespondence. The performance of the shape model is rigorously established,\nfocusing on its dimensionality reduction capabilities and the training data\nrequirements. Additionally, a comprehensive synthetic cohort is made available,\nfeaturing ready-to-use biventricular meshes with fiber structures and\nanatomical region annotations. These meshes are well-suited for\nelectrophysiological simulations.",
        "While high-resolution microscopic techniques are crucial for studying\ncellular structures in cell biology, obtaining such images from thick 3D\nengineered tissues remains challenging. In this review, we explore advancements\nin fluorescence microscopy, alongside the use of various fluorescent probes and\nmaterial processing techniques to address these challenges. We navigate through\nthe diverse array of imaging options available in tissue engineering field,\nfrom wide field to super-resolution microscopy, so researchers can make more\ninformed decisions based on the specific tissue and cellular structures of\ninterest. Finally, we provide some recent examples of how traditional\nlimitations on obtaining high-resolution images on sub-cellular architecture\nwithin 3D tissues have been overcome by combining imaging advancements with\ninnovative tissue engineering approaches.",
        "Genotype-to-Phenotype (G2P) prediction plays a pivotal role in crop breeding,\nenabling the identification of superior genotypes based on genomic data. Rice\n(Oryza sativa), one of the most important staple crops, faces challenges in\nimproving yield and resilience due to the complex genetic architecture of\nagronomic traits and the limited sample size in breeding datasets. Current G2P\nprediction methods, such as GWAS and linear models, often fail to capture\ncomplex non-linear relationships between genotypes and phenotypes, leading to\nsuboptimal prediction accuracy. Additionally, population stratification and\noverfitting are significant obstacles when models are applied to small datasets\nwith diverse genetic backgrounds. This study introduces the Learnable Group\nTransform (LGT) method, which aims to overcome these challenges by combining\nthe advantages of traditional linear models with advanced machine learning\ntechniques. LGT utilizes a group-based transformation of genotype data to\ncapture spatial relationships and genetic structures across diverse rice\npopulations, offering flexibility to generalize even with limited data. Through\nextensive experiments on the Rice529 dataset, a panel of 529 rice accessions,\nLGT demonstrated substantial improvements in prediction accuracy for multiple\nagronomic traits, including yield and plant height, compared to\nstate-of-the-art baselines such as linear models and recent deep learning\napproaches. Notably, LGT achieved an R^2 improvement of up to 15\\% for yield\nprediction, significantly reducing error and demonstrating its ability to\nextract meaningful signals from high-dimensional, noisy genomic data. These\nresults highlight the potential of LGT as a powerful tool for genomic\nprediction in rice breeding, offering a promising solution for accelerating the\nidentification of high-yielding and resilient rice varieties.",
        "Zero-one reaction networks play a crucial role in cell signaling. Determining\nthe equivalence of reaction networks is a fundamental computational problem in\nthe field of chemical reaction networks. In this work, we develop an efficient\nmethod for determining the equivalence of zero-one networks. The efficiency\ncomes from several criteria for determining the equivalence of the steady-state\nideals arising from zero-one networks, which helps for cutting down the\nexpenses on computing Grobner bases. Experiments show that our method can\nsuccessfully classify over three million networks according to their\nequivalence in a reasonable time.",
        "In osteochondral tissue engineering (OCTE), simultaneously regenerating\nsubchondral bone and cartilage tissue presents a significant challenge.\nMultiphasic scaffolds were created and manufactured using 3D printing to\naddress this issue. Excellent interfacial mechanical properties and\nbiocompatibility enhance the growth and chondrogenic differentiation of bone\nmarrow mesenchymal stem cells (BM-MSCs). The subchondral bone bottom layer is\nmimicked by incorporating varying concentrations of graphene oxide (GO) (0%,\n1%, and 2% w\/v) into a bioink composed of alginate (Alg) and gelatin (Gel).\nBased on evaluations of mechanical and biocompatibility properties, 1% GO is\nselected for further studies. Subsequently, the GO concentration is kept\nconstant while varying the platelet-rich plasma (PRP) dosage in the multiphasic\nscaffolds. Different PRP dosages (0%, 1%, 2%, and 3% w\/v) are integrated into\nthe Alg-Gel bioink to simulate cartilage tissues. Results indicate that\n3D-printed scaffolds containing 1% or 2% PRP exhibit favorable biomechanical\nproperties, with no significant differences observed. However, BM-MSCs exposed\nto 2% PRP demonstrate enhanced adhesion, growth, and viability. Additionally,\nreal-time PCR and Alcian blue staining confirm increased chondrogenic\nexpression and glycosaminoglycans (GAGs) synthesis. This work highlights the\npromising potential of 3D-printed multiphasic frameworks in the development of\nOCTE.",
        "Chimeric antigen receptor T (CAR-T) cell therapy has emerged as a promising\ntreatment for hematological malignancies, offering a targeted approach to\ncancer treatment. Understanding the complexities of CAR-T cell therapy within\nsolid tumors poses challenges due to the intricate interactions within the\ntumor microenvironment. Mathematical modeling may serve as a valuable tool to\nunravel the dynamics of CAR-T cell therapy and improve its effectiveness in\nsolid tumors. This study aimed to investigate the impact of spatial aspects in\nCAR-T therapy of solid tumors, utilizing cellular automata for modeling\npurposes. Our main objective was to deepen our understanding of treatment\neffects by analyzing scenarios with different spatial distributions and varying\nthe initial quantities of tumor and CAR-T cells. Tumor geometry significantly\ninfluenced treatment efficacy in-silico, with notable differences observed\nbetween tumors with block-like arrangements and those with sparse cell\ndistributions, leading to the concept of immune suppression due to geometrical\neffects. This research delves into the intricate relationship between spatial\ndynamics and the effectiveness of CAR-T therapy in solid tumors, highlighting\nthe relevance of tumor geometry in the outcome of cellular immunotherapy\ntreatments. Our results provide a basis for improving the efficacy of CAR-T\ncell treatments by combining them with other ones reducing the density of\ncompact tumor areas and thus opening access ways for tumor killing T-cells.",
        "ICAM-1 (intercellular adhesion molecule 1) and MPZ (myelin protein zero) are\nthought to be a factor in the integrity of nerve tissues. In this report, we\nattempted to trace the expression of ICAM-1, responsible for cell-to-cell\nadhesion, and of MPZ, the main constituent of myelin sheath, in malignant\ntissues of the sciatic nerve (SN) in inbred male Copenhagen rats. AT-1 Cells\n(anaplastic tumor 1) were injected in the perineurial sheath, and tissues of\nthe SNs were collected after 7, 14 and 21 days and compared to a sham-operated\ngroup of rats (n = 6 each). Tissues were sectioned and histologically examined,\nunder light microscope, and stained for measuring the immunoreactivity of\nICAM-1 and MPZ under laser scanning microscope. The cancer model was\nestablished, and the tumor growth was confirmed. ICAM-1 showed severe\ndecreases, proportional to the growing anaplastic cells, as compared to the\nsham group. MPZ revealed, however, a distinct defensive pattern before\nsubstantially decreasing in a comparison with sham. These results support the\nnotion that malignancies damage peripheral nerves and cause severe axonal\ninjury and loss of neuronal integrity, and clearly define the role of ICAM-1\nand MPZ in safeguarding the nerve tissues.",
        "The interactions between tumor cells and the immune system play a crucial\nrole in cancer evolution. In this study, we explore how these interactions\ninfluence cancer progression by modeling the relationships among naive T cells,\neffector T cells, and chronic myeloid leukemia cells. We examine the existence\nof equilibria, the asymptotic stability of the positive steady state, and the\nglobal stability of the tumor-free equilibrium. Additionally, we develop a\npartial differential equation to describe the conditions under which the\nconcentration of cancer cells reaches a level that allows for effective control\nof cancer evolution. Finally, we apply our proposed model to investigate\noptimal treatment strategies that aim to minimize both the concentration of\ncancer cells at the end of treatment and the accumulation of tumor burden, as\nwell as the cost associated with treatment during the intervention period. Our\nstudy reveals an optimal therapeutic protocol using optimal control theory. We\nperform numerical simulations to illustrate our theoretical results and to\nexplore the dynamic behavior of the system and optimal therapeutic protocols.\nThe simulations indicate that the optimal treatment strategy can be more\neffective than a constant treatment approach, even when applying the same\ntreatment interval and total drug input.",
        "Summary Osteoporosis is a skeletal disorder, characterized by a decrease in\nbone strength and puts the individual at risk for fracture. On the other hand,\nrheumatoid arthritis is a systemic disease of unknown etiology that causes\ninflammation of the joints of the organs. Purpose Due to the destructive\neffects of these diseases and its increasing prevalence and lack of appropriate\nmedication for treatment, the present study aimed to evaluate the therapeutic\neffect of a new type of healthy and live food supplement on rheumatoid\narthritis and induced osteoporosis in rats. Methods In this research, healthy\nand live food powder were synthesized by a new and green route. This organic\nbiomaterial was named NBS. The NBS food supplement had various vitamins, macro\nand micro molecules, and ingredients. The new healthy and nutritious diet\nshowed that the use of this supplement led to the return of the parameters to\nnormal levels. Results The concentration of 12.5 mg\/ kg showed the least\ntherapeutic effect and 50 mg\/ kg had the highest therapeutic effect for\nosteoporosis. The results of blood parameters involved in inflammation in both\nhealthy and patient groups showed that the use of complete adjuvant induction\ncauses joint inflammation. In the study of the interaction of the\nconcentrations, it was observed that the concentration of 50 mg\/ kg had the\nhighest therapeutic effect against the disease in the studied mice. Conclusion\nThe results showed that the new healthy and viable supplement restores the\nblood osteoporotic and rheumatoid factors of the mice to normal.",
        "The complexity of mathematical models describing respiratory mechanics has\ngrown in recent years, however, parameter identifiability of such models has\nonly been studied in the last decade in the context of observable data. This\nstudy investigates parameter identifiability of a nonlinear respiratory\nmechanics model tuned to the physiology of an extremely preterm infant, using\nglobal Morris screening, local deterministic sensitivity analysis, and singular\nvalue decomposition-based subset selection. The model predicts airflow and\ndynamic pulmonary volumes and pressures under varying levels of continuous\npositive airway pressure, and a range of parameters characterizing both\nsurfactant-treated and surfactant-deficient lung. Sensitivity analyses\nindicated eleven parameters influence model outputs over the range of\ncontinuous positive airway pressure and lung health scenarios. The model was\nadapted to data from a spontaneously breathing 1 kg infant using gradient-based\noptimization to estimate the parameter subset characterizing the patient's\nstate of health.",
        "We introduce and describe $\\tt GrayHawk$, a publicly available\nMathematica-based tool designed for the efficient computation of gray-body\nfactors for spherically symmetric and asymptotically flat black holes. This\nprogram provides users with a rapid and reliable means to compute gray-body\nfactors for massless fields with spin \\(s = 0, 1\/2, 1, 2\\) in modes specified\nby the angular quantum number \\(l\\), given a black hole metric and the\nassociated parameter values. $\\tt GrayHawk$ is preloaded with seven different\nblack hole metrics, offering immediate applicability to a variety of\ntheoretical models. Additionally, its modular structure allows users to extend\nits functionality easily by incorporating alternative metrics or\nconfigurations. This versatility makes $\\tt GrayHawk$ a powerful and adaptable\nresource for researchers studying black hole physics and Hawking radiation. The\ncodes described in this work are publicly available at\nhttps:\/\/github.com\/marcocalza89\/GrayHawk.",
        "Early neural channel coding approaches leveraged dense neural networks with\none-hot encodings to design adaptive encoder-decoder pairs, improving block\nerror rate (BLER) and automating the design process. However, these methods\nstruggled with scalability as the size of message sets and block lengths\nincreased. TurboAE addressed this challenge by focusing on bit-sequence inputs\nrather than symbol-level representations, transforming the scalability issue\nassociated with large message sets into a sequence modeling problem. While\nrecurrent neural networks (RNNs) were a natural fit for sequence processing,\ntheir reliance on sequential computations made them computationally expensive\nand inefficient for long sequences. As a result, TurboAE adopted convolutional\nnetwork blocks, which were faster to train and more scalable, but lacked the\nsequential modeling advantages of RNNs. Recent advances in efficient RNN\narchitectures, such as minGRU and minLSTM, and structured state space models\n(SSMs) like S4 and S6, overcome these limitations by significantly reducing\nmemory and computational overhead. These models enable scalable sequence\nprocessing, making RNNs competitive for long-sequence tasks. In this work, we\nrevisit RNNs for Turbo autoencoders by integrating the lightweight minGRU model\nwith a Mamba block from SSMs into a parallel Turbo autoencoder framework. Our\nresults demonstrate that this hybrid design matches the performance of\nconvolutional network-based Turbo autoencoder approaches for short sequences\nwhile significantly improving scalability and training efficiency for long\nblock lengths. This highlights the potential of efficient RNNs in advancing\nneural channel coding for long-sequence scenarios.",
        "We develop a theory of type semigroups for arbitrary twisted, not necessarily\nHausdorff \\'etale groupoids. The type semigroup is a dynamical version of the\nCuntz semigroup. We relate it to traces, ideals, pure infiniteness, and stable\nfiniteness of the reduced and essential C*-algebras. If the reduced C*-algebra\nof a twisted groupoid is simple and the type semigroup satisfies a weak version\nof almost unperforation, then the C*-algebra is either stably finite or purely\ninfinite. We apply our theory to Cartan inclusions. We calculate the type\nsemigroup for the possibly non-Hausdorff groupoids associated to self-similar\ngroup actions on graphs and deduce a dichotomy for the resulting Exel-Pardo\nalgebras.",
        "We study projective surfaces in $\\mathbb{P}^3$ which can be written as\nHadamard product of two curves. We show that quadratic surfaces which are\nHadamard product of two lines are smooth and tangent to all coordinate planes,\nand such tangency points uniquely identify the quadric. The variety of such\nquadratic surfaces corresponds to the Zariski closure of the space of symmetric\nmatrices whose inverse has null diagonal. For higher-degree surfaces which are\nHadamard product of a line and a curve we show that the intersection with the\ncoordinate planes is always non-transversal.",
        "In this paper we study oscillatory Bianchi models of class A and are able to\nshow that for admissible periodic heteroclinic chains in Bianchi IX there\nexisist $C^{1}$- stable - manifolds of orbits that follow these chains towards\nthe big bang. A detailed study of Takens Linearization Theorem and the\nNon-Resonance-Conditions leads us to this new result in Bianchi class A. More\nprecisely, we can show that there are no heteroclinic chains in Bianchi IX with\nconstant continued fraction development that allow Takens-Linearization at all\nof their base points. Geometrically speaking, this excludes \"symmetric\"\nheteroclinic chains with the same number of \"bounces\" near all of the 3 Taub\nPoints - the result shows that we have to require some \"asymmetry\" in the\nbounces in order to allow for Takens Linearization, e.g. by considering\nadmissible 2-periodic continued fraction developments. We conclude by\ndiscussing the statistical properties of those solutions, including their\ntopological and measure-theoretic genericity.",
        "The concept of bound states in the continuum (BIC) has been advancing light\nconfinement technology in leaky environments. In this letter, we propose and\nnumerically demonstrate a slow light waveguide based on a BIC mode. We\nconsidered a waveguide with a polymer core loaded on a plane slab, which\nsupports a leaky guided mode coupled to the radiation continuum in the slab. We\nfound that periodic modulation of the polymer core along the propagation\ndirection can result in a high group index mode with a low propagation loss due\nto BIC confinement. The introduction of one-dimensional photonic crystals into\nthe BIC waveguides will largely expand its functionality and applications in\nintegrated photonics.",
        "The ongoing energy crisis has underscored the urgent need for\nenergy-efficient materials with high energy utilization efficiency, prompting a\nsurge in research into organic compounds due to their environmental\ncompatibility, cost-effective processing, and versatile modifiability. To\naddress the high experimental costs and time-consuming nature of traditional\ntrial-and-error methods in the discovery of highly functional organic\ncompounds, we apply the 3D transformer-based molecular representation learning\nalgorithm to construct a pre-trained model using 60 million semi-empirically\noptimized structures of small organic molecules, namely, Org-Mol, which is then\nfine-tuned with public experimental data to obtain prediction models for\nvarious physical properties. Despite the pre-training process relying solely on\nsingle molecular coordinates, the fine-tuned models achieves high accuracy\n(with $R^2$ values for the test set exceeding 0.95). These fine-tuned models\nare applied in a high-throughput screening process to identify novel immersion\ncoolants among millions of automatically constructed ester molecules, resulting\nin the experimental validation of two promising candidates. This work not only\ndemonstrates the potential of Org-Mol in predicting bulk properties for organic\ncompounds but also paves the way for the rational and efficient development of\nideal candidates for energy-saving materials.",
        "Understanding the interplay between magnetism and superconductivity in\nnickelate systems is a key focus of condensed matter research. Microscopic\ninsights into magnetism, which emerges near superconductivity, require a\nsynergistic approach that combines complementary techniques with controlled\nparameter tuning. In this paper, we present a systematic investigation of the\nthree-layer Ruddlesden-Popper (RP) nickelate La$_4$Ni$_3$O$_{10}$ using\nmuon-spin rotation\/relaxation ($\\mu$SR), neutron powder diffraction (NPD),\nresistivity, and specific heat measurements. At ambient pressure, two\nincommensurate spin density wave (SDW) transitions were identified at $T_{\\rm\nSDW} \\simeq 132$ K and $T^\\ast \\simeq 90$ K. NPD experiments revealed that the\nmagnetic wave vector $(0, 0.574, 0)$ remains unchanged below 130 K, indicating\nthat the transition at $T^\\ast$ corresponds to a reorientation of the Ni\nmagnetic moments within a similar magnetic structure. Comparison of the\nobserved internal magnetic fields with dipole-field calculations reveals a\nmagnetic structure consistent with an antiferromagnetically coupled SDW on the\nouter two Ni layers, with smaller moments on the inner Ni layer. The internal\nfields at muon stopping sites appeared abruptly at $T_{\\rm SDW}$, suggesting a\nfirst-order-like nature of the SDW transition, which is closely linked to the\ncharge density wave (CDW) order occurring at the same temperature ($T_{\\rm SDW}\n= T_{\\rm CDW}$). Under applied pressure, all transition temperatures, including\n$T_{\\rm SDW}$, $T^\\ast$, and $T_{\\rm CDW}$, were suppressed at a nearly uniform\nrate of $\\simeq -13$ K\/GPa. This behavior contrasts with the double-layer RP\nnickelate La$_3$Ni$_2$O$_7$, where pressure enhances the separation of the\ndensity wave transitions.",
        "Recently, a Wasserstein-type distance for Gaussian mixture models has been\nproposed. However, that framework can only be generalized to identifiable\nmixtures of general elliptically contoured distributions whose components come\nfrom the same family and satisfy marginal consistency. In this paper, we\npropose a simple relaxed Wasserstein distance for identifiable mixtures of\nradially contoured distributions whose components can come from different\nfamilies. We show some properties of this distance and that its definition does\nnot require marginal consistency. We apply this distance in color transfer\ntasks and compare its performance with the Wasserstein-type distance for\nGaussian mixture models in an experiment. The error of our method is more\nstable and the color distribution of our output image is more desirable.",
        "In this paper, we determine the complete weight distribution of the space $\n\\mathbb{F}_q^N $ endowed by the weighted coordinates poset block metric\n($(P,w,\\pi)$-metric), also known as the $(P,w,\\pi)$-space, thereby obtaining it\nfor $(P,w)$-space, $(P,\\pi)$-space, $\\pi$-space, and $P$-space as special\ncases. Further, when $P$ is a chain, the resulting space is called as\nNiederreiter-Rosenbloom-Tsfasman (NRT) weighted block space and when $P$ is\nhierarchical, the resulting space is called as weighted coordinates\nhierarchical poset block space. The complete weight distribution of both the\nspaces are deduced from the main result. Moreover, we define an $I$-ball for an\nideal $I$ in $P$ and study the characteristics of it in $(P,w,\\pi)$-space.\n  We investigate the relationship between the $I$-perfect codes and $t$-perfect\ncodes in $(P,w,\\pi)$-space. Given an ideal $I$, we investigate how the maximum\ndistance separability (MDS) is related with $I$-perfect codes and $t$-perfect\ncodes in $(P,w,\\pi)$-space. Duality theorem is derived for an MDS\n$(P,w,\\pi)$-code when all the blocks are of same length. Finally, the\ndistribution of codewords among $r$-balls is analyzed in the case of chain\nposet, when all the blocks are of same length.",
        "Understanding the transition from atomic gas to molecular gas is critical to\nexplain the formation and evolution of molecular clouds. However, the gas\nphases involved, cold HI and CO-dark molecular gas, are challenging to directly\nobserve and physically characterize. We observed the Cygnus X star-forming\ncomplex in carbon radio recombination lines (CRRLs) at 274--399 MHz with the\nGreen Bank Telescope at 21 pc (48') resolution. Of the 30 deg^2 surveyed, we\ndetect line-synthesized C273alpha emission from 24 deg^2 and produce the first\nlarge-area maps of low-frequency CRRLs. The morphology of the C273alpha\nemission reveals arcs, ridges, and extended possibly sheet-like gas which are\noften on the outskirts of CO emission and likely transitioning from HI-to-H_2.\nThe typical angular separation of C273alpha and 13CO emission is 12 pc, and we\nestimate C273alpha gas densities of n_H ~ 40 - 400 cm^3. The C273alpha line\nprofiles are Gaussian and likely turbulent broadened, spanning a large range of\nFWHM from 2 to 20 km\/s with a median of 10.6 km\/s. Mach numbers fall within\n10--30. The turbulent timescale is relatively short, 2.6 Myr, and we deduce\nthat the turbulent pressure likely dominates the evolution of the C273alpha\ngas. Velocity offsets between C273alpha and 13CO components are apparent\nthroughout the region and have a typical value of 2.9 km\/s. Two regimes have\nemerged from the data: one regime in which C273alpha and 13CO are strongly\nrelated (at N_H ~ 4 x 10^21 cm^-2), and a second, in which C273alpha emits\nindependently of the 13CO intensity. In the former regime, C273alpha may arise\nfrom the the envelopes of massive clouds (filaments), and in the latter,\nC273alpha emits from cold clumps in a more-diffuse mix of HI and H_2 gas.",
        "This paper studies discounted Markov Decision Processes (MDPs) with finite\nsets of states and actions. Value iteration is one of the major methods for\nfinding optimal policies. For each discount factor, starting from a finite\nnumber of iterations, which is called the turnpike integer, value iteration\nalgorithms always generate decision rules, which are deterministic optimal\npolicies for the infinite-horizon problems. This fact justifies the rolling\nhorizon approach for computing infinite-horizon optimal policies by conducting\na finite number of value iterations. This paper describes properties of\nturnpike integers and provides their upper bounds.",
        "We report Raman forbidden layer-breathing modes (LBMs) in layered\nsemiconductor materials (LSMs). The intensity distribution of all observed LBMs\ndepends on layer number, incident light wavelength and refractive index\nmismatch between LSM and underlying substrate. These results are understood by\na Raman scattering theory via the proposed spatial interference model, where\nthe naturally occurring optical and phonon cavities in LSMs enable spatially\ncoherent photon-phonon coupling mediated by the corresponding one-dimensional\nperiodic electronic states. Our work reveals the spatial coherence of photon\nand phonon fields on the phonon excitation via photon\/phonon cavity\nengineering.",
        "Y$_2$Co$_3$ is a newly discovered antiferromagnetic (AFM) compound with\ndistorted kagome layers. Previous investigations via bulk magnetization\nmeasurements suggested a complex noncollinear magnetic behavior, with magnetic\nmoments primarily anti-aligned along the $b$ axis and some canting towards the\n$ac$ plane. In this study, we report the magnetic structure of Y$_2$Co$_3$ to\nbe an A-type AFM structure with ferromagnetic (FM) interactions within the\ndistorted kagome plane and an interplane antiferromagnetic interaction, as\ndetermined by single-crystal neutron diffraction. The magnetic moments align\nalong the $b$ axis, with minimal canting towards the $c$ axis, at odds with the\nprevious interpretation of bulk magnetization measurements. The magnetic\nmoments on the two distinct Co sites are [0, -0.68(3), 0] $\\mu_B$ and [0,\n1.25(4), 0.07(1)] $\\mu_B$. We attribute the previously reported \"noncollinear\"\nbehavior to the considerable temperature dependence of itinerant AFM exchange\ninteractions, induced by thermal contraction along the $b$ axis. Additionally,\nour examination of lattice constants through pressure studies reveals\ncompensating effects on FM and AFM interactions, resulting in negligible\npressure dependence of $T_\\textrm{N}$.",
        "SN 2023ixf is one of the brightest Core Collapse Supernovae of the 21st\ncentury and offers a rare opportunity to investigate the late stage of a\nSupernova through nebular phase spectroscopy. We present four nebular phase\nspectra from day +291 to +413 after explosion. This is supplemented with high\ncadence early phase spectroscopic observations and photometry covering the\nfirst 500 days to investigate explosion parameters. The narrow and blue-shifted\nnebular Oxygen emission lines are used to infer an ejected Oxygen mass of\n$<0.65M_\\odot$, consistent with models of a relatively low mass\n($M_{ZAMS}<15M_\\odot$) progenitor. An energy of 0.3 to $1.4 \\times10^{51}$ erg\nand a light curve powered by an initial $^{56}$Ni mass of $0.049 \\pm 0.005\nM_\\odot$ appear consistent with a relatively standard Type II explosion, while\nan incomplete $\\gamma$-ray trapping (with timescale of $240\\pm4$ days) suggests\na lower ejecta mass. Assuming a typical explosion, the broad Hydrogen and\nCalcium profiles suggest a common origin within a lower mass, partially\nstripped envelope. Hydrogen emission broadens with time, indicating\ncontribution from an additional power source at an extended distance; while the\nemergence of high velocity ($\\sim$6,000 km s$^{-1}$) Hydrogen emission features\n(beginning around day +200) may be explained by Shock Interaction with a dense\nHydrogen-rich region located at $\\sim1.5 \\times 10^{16}$cm. Such envelope mass\nloss for a low mass progenitor may be explained through theoretical models of\nBinary interaction."
      ]
    }
  },
  {
    "id":2411.01758,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Multi-site quality and variability analysis of 3D FDG PET segmentations based on phantom and clinical image data",
    "start_abstract":"Purpose: Radiomics utilizes a large number of image-derived features for quantifying tumor characteristics that can in turn be correlated with response and prognosis. Unfortunately, extraction and analysis of such image-based features is subject to measurement variability and bias. The challenge for radiomics is particularly acute in Positron Emission Tomography (PET) where limited resolution, a high noise component related to the limited stochastic nature of the raw data, and the wide variety of reconstruction options confound quantitative feature metrics. Extracted feature quality is also affected by tumor segmentation methods used to define regions over which to calculate features, making it challenging to produce consistent radiomics analysis results across multiple institutions that use different segmentation algorithms in their PET image analysis. Understanding each element contributing to these inconsistencies in quantitative image feature and metric generation is paramount for ultimate utilization of these methods in multi-institutional trials and clinical oncology decision making. Methods: To assess segmentation quality and consistency at the multi-institutional level, we conducted a study of seven institutional members of the National Cancer Institute Quantitative Imaging Network. For the study, members were asked to segment a common set of phantom PET scans acquired over a range of imaging conditions as well as a second set of head and neck cancer (HNC) PET scans. Segmentations were generated at each institution using their preferred approach. In addition, participants were asked to repeat segmentations with a time interval between initial and repeat segmentation. This procedure resulted in overall 806 phantom insert and 641 lesion segmentations. Subsequently, the volume was computed from the segmentations and compared to the corresponding reference volume by means of statistical analysis. Results: On the two test sets (phantom and HNC PET scans), the performance of the seven segmentation approaches was as follows. On the phantom test set, the mean relative volume errors ranged from 29.9 to 87.8% of the ground truth reference volumes, and the repeat difference for each institution ranged between -36.4 to 39.9%. On the HNC test set, the mean relative volume error ranged between -50.5 to 701.5%, and the repeat difference for each institution ranged between -37.7 to 31.5%. In addition, performance measures per phantom insert\/lesion size categories are given in the paper. On phantom data, regression analysis resulted in coefficient of variation (CV) components of 42.5% for scanners, 26.8% for institutional approaches, 21.1% for repeated segmentations, 14.3% for relative contrasts, 5.3% for count statistics (acquisition times), and 0.0% for repeated scans. Analysis showed that the CV components for approaches and repeated segmentations were significantly larger on the HNC test set with increases by 112.7% and 102.4%, respectively. Conclusion: Analysis results underline the importance of PET scanner reconstruction harmonization and imaging protocol standardization for quantification of lesion volumes. In addition, to enable a distributed multi-site analysis of FDG PET images, harmonization of analysis approaches and operator training in combination with highly automated segmentation methods seems to be advisable. Future work will focus on quantifying the impact of segmentation variation on radiomics system performance.",
    "start_categories":[
      "q-bio.CB"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b2"
      ],
      "title":[
        "A review on segmentation of positron emission tomography images"
      ],
      "abstract":[
        "Positron Emission Tomography (PET), a non-invasive functional imaging method at the molecular level, images the distribution of biologically targeted radiotracers with high sensitivity. PET imaging provides detailed quantitative information about many diseases and is often used to evaluate inflammation, infection, and cancer by detecting emitted photons from a radiotracer localized to abnormal cells. In order to differentiate abnormal tissue from surrounding areas in PET images, image segmentation methods play a vital role; therefore, accurate image segmentation is often necessary for proper disease detection, diagnosis, treatment planning, and follow-ups. In this review paper, we present state-of-the-art PET image segmentation methods, as well as the recent advances in image segmentation techniques. In order to make this manuscript self-contained, we also briefly explain the fundamentals of PET imaging, the challenges of diagnostic PET image analysis, and the effects of these challenges on the segmentation results."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Application of Artificial Intelligence (AI) in Civil Engineering",
        "MiniRAG: Towards Extremely Simple Retrieval-Augmented Generation",
        "Reproducibility Study of Cooperation, Competition, and Maliciousness:\n  LLM-Stakeholders Interactive Negotiation",
        "AoI-Sensitive Data Forwarding with Distributed Beamforming in\n  UAV-Assisted IoT",
        "Interpretable Machine Learning for Oral Lesion Diagnosis through\n  Prototypical Instances Identification",
        "L2R: Learning to Reduce Search Space for Generalizable Neural Routing\n  Solver",
        "From Text to Visuals: Using LLMs to Generate Math Diagrams with Vector\n  Graphics",
        "Talking like Piping and Instrumentation Diagrams (P&IDs)",
        "Imitation Learning of Correlated Policies in Stackelberg Games",
        "Why Do Multi-Agent LLM Systems Fail?",
        "Artificial Intelligence-Driven Clinical Decision Support Systems",
        "GDiffRetro: Retrosynthesis Prediction with Dual Graph Enhanced Molecular\n  Representation and Diffusion Generation",
        "SPRIG: Stackelberg Perception-Reinforcement Learning with Internal Game\n  Dynamics",
        "CoddLLM: Empowering Large Language Models for Data Analytics",
        "Unlocking the Potential of Unlabeled Data in Semi-Supervised Domain\n  Generalization",
        "Towards a Study of Low Energy Antiproton Annihilations on Nuclei",
        "Knowledge Phenomenology Research of Future Industrial Iconic Product\n  Innovation",
        "The simplest solutions of cold plasma equations: change in properties\n  from a hydrodynamic to a kinetic model",
        "Position: Stop Acting Like Language Model Agents Are Normal Agents",
        "Optimizing Energy Efficiency in Subthreshold RISC-V Cores",
        "Non-archimedean integration on totally disconnected spaces",
        "Cohomology of classifying spaces of rank 3 Kac-Moody groups",
        "Optimization Methods for Joint Eigendecomposition",
        "Graph factors and powers of Hamilton cycles in the budget-constrained\n  random graph process",
        "Teaching Dense Retrieval Models to Specialize with Listwise Distillation\n  and LLM Data Augmentation",
        "Learning to be Smooth: An End-to-End Differentiable Particle Smoother",
        "Advanced Zero-Shot Text-to-Speech for Background Removal and\n  Preservation with Controllable Masked Speech Prediction",
        "A diagrammatic approach to the Rasmussen invariant via tangles and\n  cobordisms"
      ],
      "abstract":[
        "Hard computing generally deals with precise data, which provides ideal\nsolutions to problems. However, in the civil engineering field, amongst other\ndisciplines, that is not always the case as real-world systems are continuously\nchanging. Here lies the need to explore soft computing methods and artificial\nintelligence to solve civil engineering shortcomings. The integration of\nadvanced computational models, including Artificial Neural Networks (ANNs),\nFuzzy Logic, Genetic Algorithms (GAs), and Probabilistic Reasoning, has\nrevolutionized the domain of civil engineering. These models have significantly\nadvanced diverse sub-fields by offering innovative solutions and improved\nanalysis capabilities. Sub-fields such as: slope stability analysis, bearing\ncapacity, water quality and treatment, transportation systems, air quality,\nstructural materials, etc. ANNs predict non-linearities and provide accurate\nestimates. Fuzzy logic uses an efficient decision-making process to provide a\nmore precise assessment of systems. Lastly, while GAs optimizes models (based\non evolutionary processes) for better outcomes, probabilistic reasoning lowers\ntheir statistical uncertainties.",
        "The growing demand for efficient and lightweight Retrieval-Augmented\nGeneration (RAG) systems has highlighted significant challenges when deploying\nSmall Language Models (SLMs) in existing RAG frameworks. Current approaches\nface severe performance degradation due to SLMs' limited semantic understanding\nand text processing capabilities, creating barriers for widespread adoption in\nresource-constrained scenarios. To address these fundamental limitations, we\npresent MiniRAG, a novel RAG system designed for extreme simplicity and\nefficiency. MiniRAG introduces two key technical innovations: (1) a\nsemantic-aware heterogeneous graph indexing mechanism that combines text chunks\nand named entities in a unified structure, reducing reliance on complex\nsemantic understanding, and (2) a lightweight topology-enhanced retrieval\napproach that leverages graph structures for efficient knowledge discovery\nwithout requiring advanced language capabilities. Our extensive experiments\ndemonstrate that MiniRAG achieves comparable performance to LLM-based methods\neven when using SLMs while requiring only 25\\% of the storage space.\nAdditionally, we contribute a comprehensive benchmark dataset for evaluating\nlightweight RAG systems under realistic on-device scenarios with complex\nqueries. We fully open-source our implementation and datasets at:\nhttps:\/\/github.com\/HKUDS\/MiniRAG.",
        "This paper presents a reproducibility study and extension of \"Cooperation,\nCompetition, and Maliciousness: LLM-Stakeholders Interactive Negotiation.\" We\nvalidate the original findings using a range of open-weight models (1.5B-70B\nparameters) and GPT-4o Mini while introducing several novel contributions. We\nanalyze the Pareto front of the games, propose a communication-free baseline to\ntest whether successful negotiations are possible without agent interaction,\nevaluate recent small language models' performance, analyze structural\ninformation leakage in model responses, and implement an inequality metric to\nassess negotiation fairness. Our results demonstrate that smaller models (<10B\nparameters) struggle with format adherence and coherent responses, but larger\nopen-weight models can approach proprietary model performance. Additionally, in\nmany scenarios, single-agent approaches can achieve comparable results to\nmulti-agent negotiations, challenging assumptions about the necessity of agent\ncommunication to perform well on the benchmark. This work also provides\ninsights into the accessibility, fairness, environmental impact, and privacy\nconsiderations of LLM-based negotiation systems.",
        "This paper proposes a UAV-assisted forwarding system based on distributed\nbeamforming to enhance age of information (AoI) in Internet of Things (IoT).\nSpecifically, UAVs collect and relay data between sensor nodes (SNs) and the\nremote base station (BS). However, flight delays increase the AoI and degrade\nthe network performance. To mitigate this, we adopt distributed beamforming to\nextend the communication range, reduce the flight frequency and ensure the\ncontinuous data relay and efficient energy utilization. Then, we formulate an\noptimization problem to minimize AoI and UAV energy consumption, by jointly\noptimizing the UAV trajectories and communication schedules. The problem is\nnon-convex and with high dynamic, and thus we propose a deep reinforcement\nlearning (DRL)-based algorithm to solve the problem, thereby enhancing the\nstability and accelerate convergence speed. Simulation results show that the\nproposed algorithm effectively addresses the problem and outperforms other\nbenchmark algorithms.",
        "Decision-making processes in healthcare can be highly complex and\nchallenging. Machine Learning tools offer significant potential to assist in\nthese processes. However, many current methodologies rely on complex models\nthat are not easily interpretable by experts. This underscores the need to\ndevelop interpretable models that can provide meaningful support in clinical\ndecision-making. When approaching such tasks, humans typically compare the\nsituation at hand to a few key examples and representative cases imprinted in\ntheir memory. Using an approach which selects such exemplary cases and grounds\nits predictions on them could contribute to obtaining high-performing\ninterpretable solutions to such problems. To this end, we evaluate PivotTree,\nan interpretable prototype selection model, on an oral lesion detection\nproblem, specifically trying to detect the presence of neoplastic, aphthous and\ntraumatic ulcerated lesions from oral cavity images. We demonstrate the\nefficacy of using such method in terms of performance and offer a qualitative\nand quantitative comparison between exemplary cases and ground-truth prototypes\nselected by experts.",
        "Constructive neural combinatorial optimization (NCO) has attracted growing\nresearch attention due to its ability to solve complex routing problems without\nrelying on handcrafted rules. However, existing NCO methods face significant\nchallenges in generalizing to large-scale problems due to high computational\ncomplexity and inefficient capture of structural patterns. To address this\nissue, we propose a novel learning-based search space reduction method that\nadaptively selects a small set of promising candidate nodes at each step of the\nconstructive NCO process. Unlike traditional methods that rely on fixed\nheuristics, our selection model dynamically prioritizes nodes based on learned\npatterns, significantly reducing the search space while maintaining solution\nquality. Experimental results demonstrate that our method, trained solely on\n100-node instances from uniform distribution, generalizes remarkably well to\nlarge-scale Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing\nProblem (CVRP) instances with up to 1 million nodes from the uniform\ndistribution and over 80K nodes from other distributions.",
        "Advances in large language models (LLMs) offer new possibilities for\nenhancing math education by automating support for both teachers and students.\nWhile prior work has focused on generating math problems and high-quality\ndistractors, the role of visualization in math learning remains under-explored.\nDiagrams are essential for mathematical thinking and problem-solving, yet\nmanually creating them is time-consuming and requires domain-specific\nexpertise, limiting scalability. Recent research on using LLMs to generate\nScalable Vector Graphics (SVG) presents a promising approach to automating\ndiagram creation. Unlike pixel-based images, SVGs represent geometric figures\nusing XML, allowing seamless scaling and adaptability. Educational platforms\nsuch as Khan Academy and IXL already use SVGs to display math problems and\nhints. In this paper, we explore the use of LLMs to generate math-related\ndiagrams that accompany textual hints via intermediate SVG representations. We\naddress three research questions: (1) how to automatically generate math\ndiagrams in problem-solving hints and evaluate their quality, (2) whether SVG\nis an effective intermediate representation for math diagrams, and (3) what\nprompting strategies and formats are required for LLMs to generate accurate\nSVG-based diagrams. Our contributions include defining the task of\nautomatically generating SVG-based diagrams for math hints, developing an LLM\nprompting-based pipeline, and identifying key strategies for improving diagram\ngeneration. Additionally, we introduce a Visual Question Answering-based\nevaluation setup and conduct ablation studies to assess different pipeline\nvariations. By automating the math diagram creation, we aim to provide students\nand teachers with accurate, conceptually relevant visual aids that enhance\nproblem-solving and learning experiences.",
        "We propose a methodology that allows communication with Piping and\nInstrumentation Diagrams (P&IDs) using natural language. In particular, we\nrepresent P&IDs through the DEXPI data model as labeled property graphs and\nintegrate them with Large Language Models (LLMs). The approach consists of\nthree main parts: 1) P&IDs are cast into a graph representation from the DEXPI\nformat using our pyDEXPI Python package. 2) A tool for generating P&ID\nknowledge graphs from pyDEXPI. 3) Integration of the P&ID knowledge graph to\nLLMs using graph-based retrieval augmented generation (graph-RAG). This\napproach allows users to communicate with P&IDs using natural language. It\nextends LLM's ability to retrieve contextual data from P&IDs and mitigate\nhallucinations. Leveraging the LLM's large corpus, the model is also able to\ninterpret process information in PIDs, which could help engineers in their\ndaily tasks. In the future, this work will also open up opportunities in the\ncontext of other generative Artificial Intelligence (genAI) solutions on P&IDs,\nand AI-assisted HAZOP studies.",
        "Stackelberg games, widely applied in domains like economics and security,\ninvolve asymmetric interactions where a leader's strategy drives follower\nresponses. Accurately modeling these dynamics allows domain experts to optimize\nstrategies in interactive scenarios, such as turn-based sports like badminton.\nIn multi-agent systems, agent behaviors are interdependent, and traditional\nMulti-Agent Imitation Learning (MAIL) methods often fail to capture these\ncomplex interactions. Correlated policies, which account for opponents'\nstrategies, are essential for accurately modeling such dynamics. However, even\nmethods designed for learning correlated policies, like CoDAIL, struggle in\nStackelberg games due to their asymmetric decision-making, where leaders and\nfollowers cannot simultaneously account for each other's actions, often leading\nto non-correlated policies. Furthermore, existing MAIL methods that match\noccupancy measures or use adversarial techniques like GAIL or Inverse RL face\nscalability challenges, particularly in high-dimensional environments, and\nsuffer from unstable training. To address these challenges, we propose a\ncorrelated policy occupancy measure specifically designed for Stackelberg games\nand introduce the Latent Stackelberg Differential Network (LSDN) to match it.\nLSDN models two-agent interactions as shared latent state trajectories and uses\nmulti-output Geometric Brownian Motion (MO-GBM) to effectively capture joint\npolicies. By leveraging MO-GBM, LSDN disentangles environmental influences from\nagent-driven transitions in latent space, enabling the simultaneous learning of\ninterdependent policies. This design eliminates the need for adversarial\ntraining and simplifies the learning process. Extensive experiments on\nIterative Matrix Games and multi-agent particle environments demonstrate that\nLSDN can better reproduce complex interaction dynamics than existing MAIL\nmethods.",
        "Despite growing enthusiasm for Multi-Agent Systems (MAS), where multiple LLM\nagents collaborate to accomplish tasks, their performance gains across popular\nbenchmarks remain minimal compared to single-agent frameworks. This gap\nhighlights the need to analyze the challenges hindering MAS effectiveness.\n  In this paper, we present the first comprehensive study of MAS challenges. We\nanalyze five popular MAS frameworks across over 150 tasks, involving six expert\nhuman annotators. We identify 14 unique failure modes and propose a\ncomprehensive taxonomy applicable to various MAS frameworks. This taxonomy\nemerges iteratively from agreements among three expert annotators per study,\nachieving a Cohen's Kappa score of 0.88. These fine-grained failure modes are\norganized into 3 categories, (i) specification and system design failures, (ii)\ninter-agent misalignment, and (iii) task verification and termination. To\nsupport scalable evaluation, we integrate MASFT with LLM-as-a-Judge. We also\nexplore if identified failures could be easily prevented by proposing two\ninterventions: improved specification of agent roles and enhanced orchestration\nstrategies. Our findings reveal that identified failures require more complex\nsolutions, highlighting a clear roadmap for future research. We open-source our\ndataset and LLM annotator.",
        "As artificial intelligence (AI) becomes increasingly embedded in healthcare\ndelivery, this chapter explores the critical aspects of developing reliable and\nethical Clinical Decision Support Systems (CDSS). Beginning with the\nfundamental transition from traditional statistical models to sophisticated\nmachine learning approaches, this work examines rigorous validation strategies\nand performance assessment methods, including the crucial role of model\ncalibration and decision curve analysis. The chapter emphasizes that creating\ntrustworthy AI systems in healthcare requires more than just technical\naccuracy; it demands careful consideration of fairness, explainability, and\nprivacy. The challenge of ensuring equitable healthcare delivery through AI is\nstressed, discussing methods to identify and mitigate bias in clinical\npredictive models. The chapter then delves into explainability as a cornerstone\nof human-centered CDSS. This focus reflects the understanding that healthcare\nprofessionals must not only trust AI recommendations but also comprehend their\nunderlying reasoning. The discussion advances in an analysis of privacy\nvulnerabilities in medical AI systems, from data leakage in deep learning\nmodels to sophisticated attacks against model explanations. The text explores\nprivacy-preservation strategies such as differential privacy and federated\nlearning, while acknowledging the inherent trade-offs between privacy\nprotection and model performance. This progression, from technical validation\nto ethical considerations, reflects the multifaceted challenges of developing\nAI systems that can be seamlessly and reliably integrated into daily clinical\npractice while maintaining the highest standards of patient care and data\nprotection.",
        "Retrosynthesis prediction focuses on identifying reactants capable of\nsynthesizing a target product. Typically, the retrosynthesis prediction\ninvolves two phases: Reaction Center Identification and Reactant Generation.\nHowever, we argue that most existing methods suffer from two limitations in the\ntwo phases: (i) Existing models do not adequately capture the ``face''\ninformation in molecular graphs for the reaction center identification. (ii)\nCurrent approaches for the reactant generation predominantly use sequence\ngeneration in a 2D space, which lacks versatility in generating reasonable\ndistributions for completed reactive groups and overlooks molecules' inherent\n3D properties. To overcome the above limitations, we propose GDiffRetro. For\nthe reaction center identification, GDiffRetro uniquely integrates the original\ngraph with its corresponding dual graph to represent molecular structures,\nwhich helps guide the model to focus more on the faces in the graph. For the\nreactant generation, GDiffRetro employs a conditional diffusion model in 3D to\nfurther transform the obtained synthon into a complete reactant. Our\nexperimental findings reveal that GDiffRetro outperforms state-of-the-art\nsemi-template models across various evaluative metrics.",
        "Deep reinforcement learning agents often face challenges to effectively\ncoordinate perception and decision-making components, particularly in\nenvironments with high-dimensional sensory inputs where feature relevance\nvaries. This work introduces SPRIG (Stackelberg Perception-Reinforcement\nlearning with Internal Game dynamics), a framework that models the internal\nperception-policy interaction within a single agent as a cooperative\nStackelberg game. In SPRIG, the perception module acts as a leader,\nstrategically processing raw sensory states, while the policy module follows,\nmaking decisions based on extracted features. SPRIG provides theoretical\nguarantees through a modified Bellman operator while preserving the benefits of\nmodern policy optimization. Experimental results on the Atari BeamRider\nenvironment demonstrate SPRIG's effectiveness, achieving around 30% higher\nreturns than standard PPO through its game-theoretical balance of feature\nextraction and decision-making.",
        "Large Language Models (LLMs) have the potential to revolutionize data\nanalytics by simplifying tasks such as data discovery and SQL query synthesis\nthrough natural language interactions. This work serves as a pivotal first step\ntoward the development of foundation models explicitly designed for data\nanalytics applications. To propel this vision forward, we unveil a new data\nrecipe for post-training LLMs, enhancing their comprehension of data management\nand empowering them to tackle complex real-world analytics tasks. Specifically,\nour innovative approach includes a scalable synthetic data generation method\nthat enables the creation of a broad spectrum of topics centered on data\nrepresentation and manipulation. Furthermore, we introduce two new tasks that\nseamlessly bridge tables and text. We show that such tasks can enhance models'\nunderstanding of schema creation and the nuanced translation between natural\nlanguage and tabular data. Leveraging this data recipe, we post-train a new\nfoundation model, named CoddLLM, based on Mistral-NeMo-12B. To assess the\nlanguage understanding and reasoning capabilities of LLMs in the realm of data\nanalytics, we contribute AnalyticsMMLU, a benchmark containing thousands of\nmultiple-choice questions on databases, data analysis, and machine learning.\nOur focus on data discovery, has resulted in the contribution of three\ncomprehensive benchmarks that address both database and data lake scenarios.\nCoddLLM not only excels in performance but also sets a new standard, achieving\nthe highest average accuracy across eight datasets. It outperforms\nGPT-3.5-Turbo on AnalyticsMMLU, exceeding GPT-4o by 12.1% in table selection\nand showing an average improvement of 24.9% in Text-to-SQL compared to the base\nmodel.",
        "We address the problem of semi-supervised domain generalization (SSDG), where\nthe distributions of train and test data differ, and only a small amount of\nlabeled data along with a larger amount of unlabeled data are available during\ntraining. Existing SSDG methods that leverage only the unlabeled samples for\nwhich the model's predictions are highly confident (confident-unlabeled\nsamples), limit the full utilization of the available unlabeled data. To the\nbest of our knowledge, we are the first to explore a method for incorporating\nthe unconfident-unlabeled samples that were previously disregarded in SSDG\nsetting. To this end, we propose UPCSC to utilize these unconfident-unlabeled\nsamples in SSDG that consists of two modules: 1) Unlabeled Proxy-based\nContrastive learning (UPC) module, treating unconfident-unlabeled samples as\nadditional negative pairs and 2) Surrogate Class learning (SC) module,\ngenerating positive pairs for unconfident-unlabeled samples using their\nconfusing class set. These modules are plug-and-play and do not require any\ndomain labels, which can be easily integrated into existing approaches.\nExperiments on four widely used SSDG benchmarks demonstrate that our approach\nconsistently improves performance when attached to baselines and outperforms\ncompeting plug-and-play methods. We also analyze the role of our method in\nSSDG, showing that it enhances class-level discriminability and mitigates\ndomain gaps. The code is available at https:\/\/github.com\/dongkwani\/UPCSC.",
        "A study of antiproton annihilations at rest on thin solid targets is underway\nat the ASACUSA facility, which now features a dedicated beam line for slow\nextraction at 250 eV. The experiment will employ new technologies, such as the\nTimepix4 ASICs coupled to silicon sensors, to measure the total multiplicity,\nenergy, and angular distribution of various prongs produced in thin solid\ntargets. A detection system consisting of seven Timepix4, covering most of the\nsolid angle, is being constructed. A 3D annihilation vertex reconstruction\nalgorithm from particle tracks in the single-plane detectors has been developed\nusing Monte Carlo simulations. The measurements will enable a study of\npbar-nucleus interactions, their dependence on nucleus mass and branching\nratios. The results will be used to assess and potentially improve various\nsimulation models.",
        "Iconic products, as innovative carriers supporting the development of future\nindustries, are key breakthrough points for driving the transformation of new\nquality productive forces. This article is grounded in the philosophy of\ntechnology and examines the evolution of human civilization to accurately\nidentify the patterns of product innovation. By integrating theories from\nsystems science, it analyzes the intrinsic logical differences between\ntraditional products and iconic products. The study finds that iconic products\nare based on a comprehensive knowledge system that integrates explicit and\ntacit knowledge, enabling them to adapt to complex dynamic environments.\nTherefore, based on the method of phenomenological essence reduction and the\nprocess of specialized knowledge acquisition, this study establishes the first\nprinciple of knowledge phenomenology: \"knowledge generation-moving from the\ntacit to the explicit-moving from the explicit to the tacit-fusion of the\nexplicit and tacit.\" Grounded in knowledge phenomenology, it reconstructs the\nproduct design evolution process and establishes a forward innovative design\nframework for iconic products, consisting of \"design problem space-explicit\nknowledge space-tacit knowledge space-innovative solution space.\" Furthermore,\nbased on FBS design theory, it develops a disruptive technology innovation\nforecasting framework of \"technology problem space-knowledge base\nprediction-application scenario prediction-coupled technology prediction,\"\nwhich collectively advances the innovation systems engineering of iconic\nproducts. In light of the analysis of the global future industrial competitive\nlandscape, it proposes a strategy for enhancing embodied intelligence in iconic\nproducts.",
        "We consider the transition from the kinetic model of Landau cold plasma to\nthe hydrodynamic one by constructing a \"multi-speed\" moment chain in the case\nof one spatial variable. Closing this chain at the first step leads to the\nstandard hydrodynamic system of cold plasma. The change in the properties of\nthe solution when closing the chain at the second step is discussed using the\nexample of two classes of solutions - affine in space and traveling waves, and\nit is shown that their properties change significantly compared to the\nhydrodynamic model.",
        "Language Model Agents (LMAs) are increasingly treated as capable of\nautonomously navigating interactions with humans and tools. Their design and\ndeployment tends to presume they are normal agents capable of sustaining\ncoherent goals, adapting across contexts and acting with a measure of\nintentionality. These assumptions are critical to prospective use cases in\nindustrial, social and governmental settings. But LMAs are not normal agents.\nThey inherit the structural problems of the large language models (LLMs) around\nwhich they are built: hallucinations, jailbreaking, misalignment and\nunpredictability. In this Position paper we argue LMAs should not be treated as\nnormal agents, because doing so leads to problems that undermine their utility\nand trustworthiness. We enumerate pathologies of agency intrinsic to LMAs.\nDespite scaffolding such as external memory and tools, they remain\nontologically stateless, stochastic, semantically sensitive, and linguistically\nintermediated. These pathologies destabilise the ontological properties of LMAs\nincluding identifiability, continuity, persistence and and consistency,\nproblematising their claim to agency. In response, we argue LMA ontological\nproperties should be measured before, during and after deployment so that the\nnegative effects of pathologies can be mitigated.",
        "Our goal in this paper is to understand how to maximize energy efficiency\nwhen designing standard-ISA processor cores for subthreshold operation. We\nhence develop a custom subthreshold library and use it to synthesize the\nopen-source RISC-V cores SERV, QERV, PicoRV32, Ibex, Rocket, and two variants\nof Vex, targeting a supply voltage of 300 mV in a commercial 130 nm process.\nSERV, QERV, and PicoRV32 are multi-cycle architectures, while Ibex, Vex, and\nRocket are pipelined architectures.\n  We find that SERV, QERV, PicoRV32, and Vex are Pareto optimal in one or more\nof performance, power, and area. The 2-stage Vex (Vex-2) is the most energy\nefficient core overall, mainly because it uses fewer cycles per instruction\nthan multi-cycle SERV, QERV, and PicoRV32 while retaining similar power\nconsumption. Pipelining increases core area, and we observe that for\nsubthreshold operation, the longer wires of pipelined designs require adding\nbuffers to maintain a cycle time that is low enough to achieve high energy\nefficiency. These buffers limit the performance gains achievable by deeper\npipelining because they result in cycle time no longer scaling proportionally\nwith pipeline stages. The added buffers and the additional area required for\npipelining logic however increase power consumption, and Vex-2 therefore\nprovides similar performance and lower power consumption than the 5-stage cores\nVex-5 and Rocket. A key contribution of this paper is therefore to demonstrate\nthat limited-depth pipelined RISC-V designs hit the sweet spot in balancing\nperformance and power consumption when optimizing for energy efficiency in\nsubthreshold operation.",
        "We work in the category $\\mathcal{CLM}^u_k$ of [5] of separated complete\nbounded $k$-linearly topologized modules over a complete linearly topologized\nring $k$ and discuss duality on certain exact subcategories. We study\ntopological and uniform structures on locally compact paracompact\n$0$-dimensional topological spaces $X$, named $td$-spaces in [11] and [17], and\nthe corresponding algebras $\\mathscr{C}_?(X,k)$ of continuous $k$-valued\nfunctions, with a choice of support and uniformity conditions. We apply the\nprevious duality theory to define and study the dual coalgebras\n$\\mathscr{D}_?(X,k)$ of $k$-valued measures on $X$. We then complete the\npicture by providing a direct definition of the various types of measures. In\nthe case of $X$ a commutative $td$-group $G$ the integration pairing provides\nperfect dualities of Hopf $k$-algebras between $$\\mathscr{C}_{\\rm unif}(G,k)\n\\longrightarrow \\mathscr{C}(G,k) \\;\\;\\;\\mbox{and}\\;\\;\\; \\mathscr{D}_{\\rm\nacs}(G,k) \\longrightarrow \\mathscr{D}_{\\rm unif}(G,k) \\;.$$ We conclude the\npaper with the remarkable example of $G= \\mathbb{G}_a(\\mathbb{Q}_p)$ and $k =\n\\mathbb{Z}_p$, leading to the basic Fontaine ring $${\\bf A}_{\\rm inf} = {\\rm W}\n\\left(\\widehat{\\mathbb{F}_p[[t^{1\/p^\\infty}]]}\\right) = \\mathscr{D}_{\\rm\nunif}(\\mathbb{Q}_p,\\mathbb{Z}_p) \\;.$$ We discuss Fourier duality between ${\\bf\nA}_{\\rm inf}$ and $\\mathscr{C}_{\\rm unif}(\\mathbb{Q}_p,\\mathbb{Z}_p)$ and\nexhibit a remarkable Fr\\'echet basis of $\\mathscr{C}_{\\rm\nunif}(\\mathbb{Q}_p,\\mathbb{Z}_p)$ related to the classical binomial\ncoefficients.",
        "We represent the rational and mod $p$ cohomology groups of classifying spaces\nof rank 3 Kac-Moody groups by a direct sum of the invariants of Weyl groups and\ntheir quotients. As an application, the authors conclude that there is a\n$p$-torsion for each prime $p$ in the integral cohomology groups of classifying\nspaces of rank 3 Kac-Moody groups. We also determine the ring structure of the\nrational cohomology with one exception case.",
        "Joint diagonalization, the process of finding a shared set of approximate\neigenvectors for a collection of matrices, arises in diverse applications such\nas multidimensional harmonic analysis or quantum information theory. This task\nis typically framed as an optimization problem: minimizing a non-convex\nfunction that quantifies off-diagonal matrix elements across possible bases. In\nthis work, we introduce a suite of efficient algorithms designed to locate\nlocal minimizers of this functional. Our methods leverage the Hessian's\nstructure to bypass direct computation of second-order derivatives, evaluating\nit as either an operator or bilinear form - a strategy that remains\ncomputationally feasible even for large-scale applications. Additionally, we\ndemonstrate that this Hessian-based information enables precise estimation of\nparameters, such as step-size, in first-order optimization techniques like\nGradient Descent and Conjugate Gradient, and the design of second-order methods\nsuch as (Quasi-)Newton. The resulting algorithms for joint diagonalization\noutperform existing techniques, and we provide comprehensive numerical evidence\nof their superior performance.",
        "We consider the following budget-constrained random graph process introduced\nby Frieze, Krivelevich and Michaeli. A player, called Builder, is presented\nwith $t$ distinct edges of $K_n$ one by one, chosen uniformly at random.\nBuilder may purchase at most $b$ of these edges, and must (irrevocably) decide\nwhether to purchase each edge as soon as it is offered. Builder's goal is to\nconstruct a graph which satisfies a certain property; we investigate the\nproperties of containing different $F$-factors or powers of Hamilton cycles.\n  We obtain general lower bounds on the budget $b$, as a function of $t$,\nrequired for Builder to obtain partial $F$-factors, for arbitrary $F$. These\nimply lower bounds for many distinct spanning structures, such as powers of\nHamilton cycles. Notably, our results show that, if $t$ is close to the hitting\ntime for a partial $F$-factor, then the budget $b$ cannot be substantially\nlower than $t$. These results give negative answers to questions of Frieze,\nKrivelevich and Michaeli.\n  Conversely, we also exhibit a simple strategy for constructing (partial)\n$F$-factors, in particular showing that our general lower bound is tight up to\nconstant factors. The ideas from this strategy can be exploited for other\nproperties. As an example, we obtain an essentially optimal strategy for powers\nof Hamilton cycles. In order to formally prove that this strategy succeeds, we\ndevelop novel tools for analysing multi-stage strategies, which may be of\ngeneral interest for studying other properties.",
        "While the current state-of-the-art dense retrieval models exhibit strong\nout-of-domain generalization, they might fail to capture nuanced\ndomain-specific knowledge. In principle, fine-tuning these models for\nspecialized retrieval tasks should yield higher effectiveness than relying on a\none-size-fits-all model, but in practice, results can disappoint. We show that\nstandard fine-tuning methods using an InfoNCE loss can unexpectedly degrade\neffectiveness rather than improve it, even for domain-specific scenarios. This\nholds true even when applying widely adopted techniques such as hard-negative\nmining and negative de-noising. To address this, we explore a training strategy\nthat uses listwise distillation from a teacher cross-encoder, leveraging rich\nrelevance signals to fine-tune the retriever. We further explore synthetic\nquery generation using large language models. Through listwise distillation and\ntraining with a diverse set of queries ranging from natural user searches and\nfactual claims to keyword-based queries, we achieve consistent effectiveness\ngains across multiple datasets. Our results also reveal that synthetic queries\ncan rival human-written queries in training utility. However, we also identify\nlimitations, particularly in the effectiveness of cross-encoder teachers as a\nbottleneck. We release our code and scripts to encourage further research.",
        "For challenging state estimation problems arising in domains like vision and\nrobotics, particle-based representations attractively enable temporal reasoning\nabout multiple posterior modes. Particle smoothers offer the potential for more\naccurate offline data analysis by propagating information both forward and\nbackward in time, but have classically required human-engineered dynamics and\nobservation models. Extending recent advances in discriminative training of\nparticle filters, we develop a framework for low-variance propagation of\ngradients across long time sequences when training particle smoothers. Our\n\"two-filter'' smoother integrates particle streams that are propagated forward\nand backward in time, while incorporating stratification and importance weights\nin the resampling step to provide low-variance gradient estimates for neural\nnetwork dynamics and observation models. The resulting mixture density particle\nsmoother is substantially more accurate than state-of-the-art particle filters,\nas well as search-based baselines, for city-scale global vehicle localization\nfrom real-world videos and maps.",
        "The acoustic background plays a crucial role in natural conversation. It\nprovides context and helps listeners understand the environment, but a strong\nbackground makes it difficult for listeners to understand spoken words. The\nappropriate handling of these backgrounds is situation-dependent: Although it\nmay be necessary to remove background to ensure speech clarity, preserving the\nbackground is sometimes crucial to maintaining the contextual integrity of the\nspeech. Despite recent advancements in zero-shot Text-to-Speech technologies,\ncurrent systems often struggle with speech prompts containing backgrounds. To\naddress these challenges, we propose a Controllable Masked Speech Prediction\nstrategy coupled with a dual-speaker encoder, utilizing a task-related control\nsignal to guide the prediction of dual background removal and preservation\ntargets. Experimental results demonstrate that our approach enables precise\ncontrol over the removal or preservation of background across various acoustic\nconditions and exhibits strong generalization capabilities in unseen scenarios.",
        "We introduce a diagrammatic approach to Rasmussen's $s$-invariant via tangles\nand cobordisms, combining Bar-Natan's formulation of Khovanov homology for\ntangles and cobordisms with the characterization of $s$ via the divisibility of\nthe Lee class, as developed in the author's previous works. This framework\nenables a \"divide-and-conquer\" method for computing $s$ from a tangle\ndecomposition of a given knot diagram, making it suitable for both\npen-and-paper calculations and algorithmic implementations. As an application,\nwe determine the $s$-invariants of pretzel knots of the form $P(p_1, -p_2,\n\\ldots, -p_l)$, where $l \\geq 3$ is odd, all $p_i$ are positive and odd, and\n$p_1 < \\min\\{p_2, \\ldots, p_l\\}$."
      ]
    }
  },
  {
    "id":2411.01758,
    "research_type":"applied",
    "start_id":"b2",
    "start_title":"A review on segmentation of positron emission tomography images",
    "start_abstract":"Positron Emission Tomography (PET), a non-invasive functional imaging method at the molecular level, images the distribution of biologically targeted radiotracers with high sensitivity. PET imaging provides detailed quantitative information about many diseases and is often used to evaluate inflammation, infection, and cancer by detecting emitted photons from a radiotracer localized to abnormal cells. In order to differentiate abnormal tissue from surrounding areas in PET images, image segmentation methods play a vital role; therefore, accurate image segmentation is often necessary for proper disease detection, diagnosis, treatment planning, and follow-ups. In this review paper, we present state-of-the-art PET image segmentation methods, as well as the recent advances in image segmentation techniques. In order to make this manuscript self-contained, we also briefly explain the fundamentals of PET imaging, the challenges of diagnostic PET image analysis, and the effects of these challenges on the segmentation results.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Multi-site quality and variability analysis of 3D FDG PET segmentations based on phantom and clinical image data"
      ],
      "abstract":[
        "Purpose: Radiomics utilizes a large number of image-derived features for quantifying tumor characteristics that can in turn be correlated with response and prognosis. Unfortunately, extraction and analysis of such image-based features is subject to measurement variability and bias. The challenge for radiomics is particularly acute in Positron Emission Tomography (PET) where limited resolution, a high noise component related to the limited stochastic nature of the raw data, and the wide variety of reconstruction options confound quantitative feature metrics. Extracted feature quality is also affected by tumor segmentation methods used to define regions over which to calculate features, making it challenging to produce consistent radiomics analysis results across multiple institutions that use different segmentation algorithms in their PET image analysis. Understanding each element contributing to these inconsistencies in quantitative image feature and metric generation is paramount for ultimate utilization of these methods in multi-institutional trials and clinical oncology decision making. Methods: To assess segmentation quality and consistency at the multi-institutional level, we conducted a study of seven institutional members of the National Cancer Institute Quantitative Imaging Network. For the study, members were asked to segment a common set of phantom PET scans acquired over a range of imaging conditions as well as a second set of head and neck cancer (HNC) PET scans. Segmentations were generated at each institution using their preferred approach. In addition, participants were asked to repeat segmentations with a time interval between initial and repeat segmentation. This procedure resulted in overall 806 phantom insert and 641 lesion segmentations. Subsequently, the volume was computed from the segmentations and compared to the corresponding reference volume by means of statistical analysis. Results: On the two test sets (phantom and HNC PET scans), the performance of the seven segmentation approaches was as follows. On the phantom test set, the mean relative volume errors ranged from 29.9 to 87.8% of the ground truth reference volumes, and the repeat difference for each institution ranged between -36.4 to 39.9%. On the HNC test set, the mean relative volume error ranged between -50.5 to 701.5%, and the repeat difference for each institution ranged between -37.7 to 31.5%. In addition, performance measures per phantom insert\/lesion size categories are given in the paper. On phantom data, regression analysis resulted in coefficient of variation (CV) components of 42.5% for scanners, 26.8% for institutional approaches, 21.1% for repeated segmentations, 14.3% for relative contrasts, 5.3% for count statistics (acquisition times), and 0.0% for repeated scans. Analysis showed that the CV components for approaches and repeated segmentations were significantly larger on the HNC test set with increases by 112.7% and 102.4%, respectively. Conclusion: Analysis results underline the importance of PET scanner reconstruction harmonization and imaging protocol standardization for quantification of lesion volumes. In addition, to enable a distributed multi-site analysis of FDG PET images, harmonization of analysis approaches and operator training in combination with highly automated segmentation methods seems to be advisable. Future work will focus on quantifying the impact of segmentation variation on radiomics system performance."
      ],
      "categories":[
        "q-bio.CB"
      ]
    },
    "list":{
      "title":[
        "Collective inference of the truth of propositions from crowd probability\n  judgments",
        "Functional Correspondences in the Human and Marmoset Visual Cortex\n  During Movie Watching: Insights from Correlation, Redundancy, and Synergy",
        "Neural Constraints on Cognitive Experience and Mental Health",
        "Trait-structured chemotaxis: Exploring ligand-receptor dynamics and\n  travelling wave properties in a Keller-Segel model",
        "Tumor-associated CD19$^+$ macrophages induce immunosuppressive\n  microenvironment in hepatocellular carcinoma",
        "A spatially resolved and lipid-structured model for macrophage\n  populations in early human atherosclerotic lesions",
        "Effect of a new type of healthy and live food supplement on osteoporosis\n  blood parameters and induced rheumatoid arthritis in Wistar rats",
        "Structural determinants of soft memory in recurrent biological networks",
        "Neuronal Correlates of Semantic Event Classes during Presentation of\n  Complex Naturalistic Stimuli: Anatomical Patterns, Context-Sensitivity, and\n  Potential Impact on shared Human-Robot Ontologies",
        "Multicellular self-organization in Escherichia coli",
        "Automating Care by Self-maintainability for Full Laboratory Automation",
        "Intercellular contact is sufficient to drive Fibroblast to Myofibroblast\n  transitions",
        "The Nature of Organization in Living Systems",
        "Operational Feasibility Analysis of a Cryogenic Active Intake Device for\n  Atmosphere-Breathing Electric Propulsion",
        "Exponentially Better Bounds for Quantum Optimization via Dynamical\n  Simulation",
        "Scalar Field Kantowski--Sachs Solutions in Teleparallel $F(T)$ Gravity",
        "In the graphical Sierpinski gasket, the reverse Riesz transform is\n  unbounded on $L^p$, $p\\in (1,2)$",
        "On the origin of radio polarization in pulsar polar caps",
        "Heavy Axions Can Disrupt $\\gamma$-ray Bursts",
        "TOPCAT\/STILTS Integration",
        "Estimating Task-based Performance Bounds for Accelerated MRI Image\n  Reconstruction Methods by Use of Learned-Ideal Observers",
        "JT Gravity in de Sitter Space and Its Extensions",
        "Generalised Process Theories",
        "Observer-Based Output-Feedback Backstepping Stabilization of Continua of\n  Hyperbolic PDEs and Application to Large-Scale $n+m$ Coupled Hyperbolic PDEs",
        "Deep inference of simulated strong lenses in ground-based surveys",
        "A simple recursive representation of the Faulhaber series",
        "Fragmentation measurements with the FOOT experiment",
        "Galactic structure dependence of cloud-cloud collisions driven star\n  formation in the barred galaxy NGC 3627"
      ],
      "abstract":[
        "Every day, we judge the probability of propositions. When we communicate\ngraded confidence (e.g. \"I am 90% sure\"), we enable others to gauge how much\nweight to attach to our judgment. Ideally, people should share their judgments\nto reach more accurate conclusions collectively. Peer-to-peer tools for\ncollective inference could help debunk disinformation and amplify reliable\ninformation on social networks, improving democratic discourse. However,\nindividuals fall short of the ideal of well-calibrated probability judgments,\nand group dynamics can amplify errors and polarize opinions. Here, we connect\ninsights from cognitive science, structured expert judgment, and crowdsourcing\nto infer the truth of propositions from human probability judgments. In an\nonline experiment, 376 participants judged the probability of each of 1,200\ngeneral-knowledge claims for which we have ground truth (451,200 ratings).\nAggregating binary judgments by majority vote already exhibits the \"wisdom of\nthe crowd\"--the superior accuracy of collective inferences relative to\nindividual inferences. However, using continuous probability ratings and\naccounting for individual accuracy and calibration significantly improves\ncollective inferences. Peer judgment behavior can be modeled probabilistically,\nand individual parameters capturing each peer's accuracy and miscalibration can\nbe inferred jointly with the claim probabilities. This unsupervised approach\ncan be complemented by supervised methods relying on truth labels to learn\nmodels that achieve well-calibrated collective inference. The algorithms we\nintroduce can empower groups of collaborators and online communities to pool\ntheir distributed intelligence and jointly judge the probability of\npropositions with a well-calibrated sense of uncertainty.",
        "The world of beauty is deeply connected to the visual cortex, as perception\noften begins with vision in both humans and marmosets. Quantifying functional\ncorrespondences in the visual cortex across species can help us understand how\ninformation is processed in the primate visual cortex, while also providing\ndeeper insights into human visual cortex functions through the study of\nmarmosets. In this study, we measured pairwise and beyond pairwise correlation,\nredundancy, and synergy in movie-driven fMRI data across species. Our first key\nfinding was that humans and marmosets exhibited significant overlaps in\nfunctional synergy. Second, we observed that the strongest functional\ncorrespondences between the human peri-entorhinal and entorhinal cortex (PeEc)\nand the occipitotemporal higher-level visual regions in the marmoset during\nmovie watching reflected a functional synergistic relationship. These regions\nare known to correspond to face-selective areas in both species. Third,\nredundancy measures maintained stable high-order hubs, indicating a steady core\nof shared information processing, while synergy measures revealed a dynamic\nshift from low- to high-level visual regions as interaction increased,\nreflecting adaptive integration. This highlights distinct patterns of\ninformation processing across the visual hierarchy. Ultimately, our results\nreveal the marmoset as a compelling model for investigating visual perception,\ndistinguished by its remarkable functional parallels to the human visual\ncortex.",
        "Understanding how neural dynamics shape cognitive experiences remains a\ncentral challenge in neuroscience and psychiatry. Here, we present a novel\nframework leveraging state-to-output controllability from dynamical systems\ntheory to model the interplay between cognitive perturbations, neural activity,\nand subjective experience. We demonstrate that large-scale fMRI signals are\nconstrained to low-dimensional manifolds, where affective and cognitive states\nare naturally organized. Furthermore, we provide a theoretically robust method\nto estimate the controllability Gramian from steady-state neural responses,\noffering a direct measure of the energy required to steer cognitive outcomes.\nIn five healthy participants viewing 2,185 emotionally evocative short videos,\nour analyses reveal a strong alignment between neural activations and affective\nratings, with an average correlation of $r \\approx 0.7$. In a clinical cohort\nof 255 patients with major depressive disorder, biweekly Hamilton Rating Scale\ntrajectories over 11 weeks significantly mapped onto these manifolds,\nexplaining approximately 20% more variance than chance ($p < 10^{-10}$,\nnumerically better than chance in 93% reaching statistical significance in\none-third of subjects). Our work bridges dynamical systems theory and clinical\nneuroscience, providing a principled approach to optimize mental health\ntreatments by targeting the most efficient neural pathways for cognitive\nchange.",
        "A novel trait-structured Keller-Segel model that explores the dynamics of a\nmigrating cell population guided by chemotaxis in response to an external\nligand concentration is derived and analysed. Unlike traditional Keller-Segel\nmodels, this framework introduces an explicit representation of ligand-receptor\nbindings on the cell membrane, where the percentage of occupied receptors\nconstitutes the trait that influences cellular phenotype. The model posits that\nthe cell's phenotypic state directly modulates its capacity for chemotaxis and\nproliferation, governed by a trade-off due to a finite energy budget: cells\nhighly proficient in chemotaxis exhibit lower proliferation rates, while more\nproliferative cells show diminished chemotactic abilities. The model is derived\nfrom the principles of a biased random walk, resulting in a system of two\nnon-local partial differential equations, describing the densities of both\ncells and ligands. Using a Hopf-Cole transformation, we derive an equation that\ncharacterises the distribution of cellular traits within travelling wave\nsolutions for the total cell density, allowing us to uncover the monotonicity\nproperties of these waves. Numerical investigations are conducted to examine\nthe model's behaviour across various biological scenarios, providing insights\ninto the complex interplay between chemotaxis, proliferation, and phenotypic\ndiversity in migrating cell populations.",
        "Tumor-associated macrophages are a key component that contributes to the\nimmunosuppressive microenvironment in human cancers. However, therapeutic\ntargeting of macrophages has been a challenge in clinic due to the limited\nunderstanding of their heterogeneous subpopulations and distinct functions.\nHere, we identify a unique and clinically relevant CD19$^+$ subpopulation of\nmacrophages that is enriched in many types of cancer, particularly in\nhepatocellular carcinoma (HCC). The CD19$^+$ macrophages exhibit increased\nlevels of PD-L1 and CD73, enhanced mitochondrial oxidation, and compromised\nphagocytosis, indicating their immunosuppressive functions. Targeting CD19$^+$\nmacrophages with anti-CD19 chimeric antigen receptor T (CAR-T) cells inhibited\nHCC tumor growth. We identify PAX5 as a primary driver of up-regulated\nmitochondrial biogenesis in CD19$^+$ macrophages, which depletes cytoplasmic\nCa$^{2+}$, leading to lysosomal deficiency and consequent accumulation of CD73\nand PD-L1. Inhibiting CD73 or mitochondrial oxidation enhanced the efficacy of\nimmune checkpoint blockade therapy in treating HCC, suggesting great promise\nfor CD19$^+$ macrophage-targeting therapeutics.",
        "Atherosclerosis is a chronic inflammatory disease of the artery wall. The\nearly stages of atherosclerosis are driven by interactions between lipids and\nmonocyte-derived-macrophages (MDMs). The mechanisms that govern the spatial\ndistribution of lipids and MDMs in the lesion remain poorly understood. In this\npaper, we develop a spatially-resolved and lipid-structured model for early\natherosclerosis. The model development and analysis are guided by images of\nhuman coronary lesions by Nakashima et al. 2007. Consistent with their\nfindings, the model predicts that lipid initially accumulates deep in the\nintima due to a spatially non-uniform LDL retention capacity. The model also\nqualitatively reproduces the global internal maxima in the Nakashima images\nonly when the MDM mobility is sufficiently sensitive to lipid content, and MDM\nlifespan sufficiently insensitive. Introducing lipid content-dependence to MDM\nmobility and mean lifespan produced minimal impact on model behaviour at early\ntimes, but strongly impacted lesion composition at steady state. Increases to\nthe sensitivity of MDM lifespan to lipid content yield lesions with fewer MDMs,\nless total lesion lipid content and reduced mean MDM infiltration depth.\nIncreases to the sensitivity of MDM mobility to lipid content also reduces the\nMDM infiltration depth, but increases the proportion of lipid-laden MDMs. We\nfind that MDM lipid content increases with spatial depth, regardless of blood\nLDL and HDL content. These results shed light on the mechanisms that drive\nspatial variation in the composition of early atherosclerotic lesions, and the\nrole of macrophage lipid content in disease progression.",
        "Summary Osteoporosis is a skeletal disorder, characterized by a decrease in\nbone strength and puts the individual at risk for fracture. On the other hand,\nrheumatoid arthritis is a systemic disease of unknown etiology that causes\ninflammation of the joints of the organs. Purpose Due to the destructive\neffects of these diseases and its increasing prevalence and lack of appropriate\nmedication for treatment, the present study aimed to evaluate the therapeutic\neffect of a new type of healthy and live food supplement on rheumatoid\narthritis and induced osteoporosis in rats. Methods In this research, healthy\nand live food powder were synthesized by a new and green route. This organic\nbiomaterial was named NBS. The NBS food supplement had various vitamins, macro\nand micro molecules, and ingredients. The new healthy and nutritious diet\nshowed that the use of this supplement led to the return of the parameters to\nnormal levels. Results The concentration of 12.5 mg\/ kg showed the least\ntherapeutic effect and 50 mg\/ kg had the highest therapeutic effect for\nosteoporosis. The results of blood parameters involved in inflammation in both\nhealthy and patient groups showed that the use of complete adjuvant induction\ncauses joint inflammation. In the study of the interaction of the\nconcentrations, it was observed that the concentration of 50 mg\/ kg had the\nhighest therapeutic effect against the disease in the studied mice. Conclusion\nThe results showed that the new healthy and viable supplement restores the\nblood osteoporotic and rheumatoid factors of the mice to normal.",
        "Recurrent neural networks are frequently studied in terms of their\ninformation-processing capabilities. The structural properties of these\nnetworks are seldom considered, beyond those emerging from the connectivity\ntuning necessary for network training. However, real biological networks have\nnon-contingent architectures that have been shaped by evolution over eons,\nconstrained partly by information-processing criteria, but more generally by\nfitness maximization requirements. Here we examine the topological properties\nof existing biological networks, focusing in particular on gene regulatory\nnetworks in bacteria. We identify structural features, both local and global,\nthat dictate the ability of recurrent networks to store information on the fly\nand process complex time-dependent inputs.",
        "The present study forms part of a research project that aims to develop\ncognition-enabled robotic agents with environmental interaction capabilities\nclose to human proficiency. This approach is based on human-derived neuronal\ndata in combination with a shared ontology to enable robots to learn from human\nexperiences. To gain further insight into the relation between human neuronal\nactivity patterns and ontological classes, we introduced General Linear Model\n(GLM) analyses on fMRI data of participants who were presented with complex\nnaturalistic video stimuli comparable to the robot tasks. We modeled four event\nclasses (pick, place, fetch and deliver) attached to different environmental\nand object-related context and employed a Representational Similarity Analysis\n(RSA) on associated brain activity patterns as a starting point for an\nautomatic hierarchical clustering. Based on the default values for the\nHemodynamic Response Function (HRF), the activity patterns were reliably\ngrouped according to their parent classes of object interaction and navigation.\nAlthough fetch and deliver events were also distinguished by neuronal patterns,\npick and place events demonstrated higher ambiguity with respect to neuronal\nactivation patterns. Introducing a shorter HRF time-to-peak leads to a more\nreliable grouping of all four semantic classes, despite contextual factors.\nThese data might give novel insights into the neuronal representation of\ncomplex stimuli and may enable further research in ontology validation in\ncognition-enabled robotics.",
        "Escherichia coli has long been a trusty companion, maintaining health in our\nguts and advancing biological knowledge in the laboratory. In light of recent\nfindings, we discuss multicellular self-organization in E. coli and develop\ngeneral ideas for multicellularity, including the necessity for multicellular\ndynamics and interpretation by dynamic graphs, applicable to both unicellular\nand multicellular organisms. In this context, we next discuss the documented\nbehaviors of E. coli self-organization (rosette formation, multicellular\nextension, and attached dormancy) and two potential behaviors (internal\ncommunication and mating). Finally, by comparing the dynamic graphs for\ndifferent communities, we develop principles relevant to the theory of\nmulticellularity.",
        "The automation of experiments in life sciences and chemistry has\nsignificantly advanced with the development of various instruments and AI\ntechnologies. However, achieving full laboratory automation, where experiments\nconceived by scientists are seamlessly executed in automated laboratories,\nremains a challenge. We identify the lack of automation in planning and\noperational tasks--critical human-managed processes collectively termed\n\"care\"--as a major barrier. Automating care is the key enabler for full\nlaboratory automation. To address this, we propose the concept of\nself-maintainability (SeM): the ability of a laboratory system to autonomously\nadapt to internal and external disturbances, maintaining operational readiness\nakin to living cells. A SeM-enabled laboratory features autonomous recognition\nof its state, dynamic resource and information management, and adaptive\nresponses to unexpected conditions. This shifts the planning and execution of\nexperimental workflows, including scheduling and reagent allocation, from\nhumans to the system. We present a conceptual framework for implementing\nSeM-enabled laboratories, comprising three modules--Requirement manager,\nLabware manager, and Device manager--and a Central manager. SeM not only\nenables scientists to execute envisioned experiments seamlessly but also\nprovides developers with a design concept that drives the technological\ninnovations needed for full automation.",
        "Fibroblast cells play a key role in maintaining the extracellular matrix.\nDuring wound healing, fibroblasts differentiate into highly contractile\nmyofibroblasts, which secrete extracellular matrix proteins like collagen to\nfacilitate tissue repair. Under normal conditions, myofibroblasts undergo\nprogrammed cell death after healing to prevent excessive scar formation.\nHowever, in diseases like fibrosis, the myofibroblasts remain active even after\nthe wound is closed, resulting in excessive collagen buildup and a stiff,\nfibrotic matrix. The reasons for the persistence of myofibroblasts in fibrosis\nare not well understood. Here we show the existence of a mechanism where direct\nphysical contact between a fibroblast and a myofibroblast is sufficient for\nfibroblasts to transition into myofibroblasts. We show that\nfibroblast-myofibroblast transition can occur even in the absence of known\nbiochemical cues such as growth factor activation or mechanical cues from a\nstiff, fibrotic matrix. Further, we show that contact-based\nfibroblast-myofibroblast activation can be blocked by G{\\alpha}q\/11\/14\ninhibitor FR9003591, which inhibits the formation of myofibroblasts. These\nfindings offer new insights into the persistence of fibrosis despite\ntherapeutic interventions and suggest a potential strategy to target\nfibroblast-to-myofibroblast transition in fibrosis.",
        "Living systems are thermodynamically open but closed in their organization.\nIn other words, even though their material components turn over constantly, a\nmaterial-independent property persists, which we call organization. Moreover,\norganization comes from within organisms themselves, which requires us to\nexplain how this self-organization is established and maintained. In this paper\nwe propose a mathematical and conceptual framework to understand the kinds of\norganized systems that living systems are, aiming to explain how\nself-organization emerges from more basic elemental processes. Additionally, we\nmap our own notions to existing traditions in theoretical biology and\nphilosophy, aiming to bring the main formal ideas into conceptual congruence.",
        "Atmosphere-breathing electric propulsion (ABEP) systems are emerging for\norbit maintenance in very-low-Earth orbit (VLEO) by capturing atmospheric\npropellant \\textit{in situ} using an intake device. A previous study proposed\nthe cryocondensation-regeneration active intake device (CRAID) to significantly\nenhance intake performance. This study investigates the operational feasibility\nof CRAID. A conceptual prototype model (CPM) is presented to verify its\nfeasibility, and numerical analyses demonstrate the practical operational\nsequences, required cryocooler capacity, intake performance, and flight\nenvelope. The numerical analyses employ the direct simulation Monte Carlo\n(DSMC) method with a phase change model and a 0D analytical model for RF ion\nthrusters. A significant improvement in intake performance is estimated based\non the practical sequences, with compression performance at least 1000 times\nhigher than that of prevalent intake devices. The capability for consistent\npropellant supply is observed regardless of atmospheric conditions. A model\nsatellite incorporating CPM confirms that CRAID enables complete drag\ncompensation at altitudes above 190 km without limiting the upper boundary of\nthe flight envelope.",
        "We provide several quantum algorithms for continuous optimization that do not\nrequire any gradient estimation. Instead, we encode the optimization problem\ninto the dynamics of a physical system and coherently simulate the time\nevolution. This allows us, in certain cases, to obtain exponentially better\nquery upper bounds relative to the best known upper bounds for gradient-based\noptimization schemes which utilize quantum computers only for the evaluation of\ngradients. Our first two algorithms can find local optima of a differentiable\nfunction $f: \\mathbb{R}^N \\rightarrow \\mathbb{R}$ by simulating either\nclassical or quantum dynamics with friction via a time-dependent Hamiltonian.\nWe show that these methods require $O(N\\kappa^2\/h_x^2\\epsilon)$ queries to a\nphase oracle to find an $\\epsilon$-approximate local optimum of a locally\nquadratic objective function, where $\\kappa$ is the condition number of the\nHessian matrix and $h_x$ is the discretization spacing. In contrast, we show\nthat gradient-based methods require $O(N(1\/\\epsilon)^{\\kappa \\log(3)\/4})$\nqueries. Our third algorithm can find the global optimum of $f$ by preparing a\nclassical low-temperature thermal state via simulation of the classical\nLiouvillian operator associated with the Nos\\'e Hamiltonian. We use results\nfrom the quantum thermodynamics literature to bound the thermalization time for\nthe discrete system. Additionally, we analyze barren plateau effects that\ncommonly plague quantum optimization algorithms and observe that our approach\nis vastly less sensitive to this problem than standard gradient-based\noptimization. Our results suggests that these dynamical optimization approaches\nmay be far more scalable for future quantum machine learning, optimization and\nvariational experiments than was widely believed.",
        "In this paper, we investigate time-dependent Kantowski--Sachs spherically\nsymmetric teleparallel $F(T)$ gravity with a scalar field source. We begin by\nsetting the exact field equations to be solved and solve conservation laws for\npossible scalar field potential, $V\\left(\\phi\\right)$, solutions. Then, we find\nnew non-trivial teleparallel $F(T)$ solutions by using power-law and\nexponential ansatz for each potential case arising from conservation laws, such\nas linear, quadratic, or logarithmic, to name a few. We find a general formula\nallowing us to compute all possible new teleparallel $F(T)$ solutions\napplicable for any scalar field potential and ansatz. Then, we apply this\nformula and find a large number of exact and approximate new teleparallel\n$F(T)$ solutions for several types of cases. Some new $F(T)$ solution classes\nmay be relevant for future cosmological applications, especially concerning\ndark matter, dark energy quintessence, phantom energy leading to the Big Rip\nevent, and quintom models of physical processes.",
        "In this article, we proved that the reverse Riesz transform on the graphical\nSierpinski gasket is unbounded on $L^p$ for $p\\in (1,2)$. Together with\nprevious results, it shows that the Riesz transform on the graphical Sierpinski\ngasket is bounded on $L^p$ if and only if $p\\in (1,2]$ and the reverse Riesz\ntransform is bounded on $L^p$ if and only if $p\\in [2,\\infty)$.\n  Moreover, our method is quite flexible - but requires explicit computations -\nand hints to the fact that the reverse Riesz transforms is never bounded on\n$L^p$, $p\\in (1,2)$, on graphs with slow diffusions.",
        "A knowledge of polarization properties of coherent radio waves escaping\npulsar polar caps is crucial for calculating radiative transfer through the\nmagnetosphere and for obtaining specific predictions of observable radio\nproperties. We describe the pair cascades in the pulsar polar cap, and for the\nfirst time, determine the Stokes parameters of the escaping radio waves from\nfirst-principle kinetic simulations for a pulsar with an inclination angle of\nthe magnetic axis 60{\\deg}.\n  Our model provides a quantitative and qualitative explanation of the observed\npulsar radio powers and spectra, the pulse profiles and polarization curves,\ntheir temporal variability, the strong Stokes L and weak Stokes V polarization\ncomponents, as well as the fact that linear polarization decreases with\nfrequency and the non-existence of a radius to frequency relationship. We find\nthat the radio emission from the polar cap can produce a diverse range of\nobserved pulsar properties, including single or double peaked profiles. Most of\nthe Stokes V curves from our simulations appear to be antisymmetric, but\nsymmetric curves are also present at some viewing angles. Although the PA swing\nof the radiation from the polar cap can be fitted by the rotating vector model\n(RVM) for most viewing angles, the angles obtained from the RVM do not\ncorrespond to the angular distance of the observer from the magnetic axis.\nInstead, the PA is directly related to the plasma flows in the polar cap and\nnot to the dipole geometry of the magnetic field. The observed range of other\npolarization features, in addition to our results, can be explained by\npropagation effects which are not part of the simulation.\n  Our simulations demonstrate that pair discharges determine the majority of\nits typically observed properties. The usage of RVM for estimations of the\nmagnetic field geometry from observations needs to be reevaluated.",
        "Axion-like particles (ALPs) can be produced in the hot dense plasma of\nfireballs that develop in the initial stage of $\\gamma$-ray burst (GRB)\noutflows. They can transport an enormous amount of energy away from the jet by\npropagating out of the fireball. The photons produced by the eventual decay of\nsuch ALPs do not reach a sufficient density to re-thermalize through pair\nproduction, preventing fireball re-emergence. Thus, the production of heavy\nALPs disrupts the fireball and dims GRBs, allowing bright GRB observations to\nstrongly constrain the existence of heavy ALPs. By adding ALP interactions to\nexisting models of GRB fireballs, we set competitive bounds on the ALP-photon\ncoupling down to $g_{a \\gamma \\gamma} \\sim 4 \\times\n10^{-12}~{\\mathrm{GeV}^{-1}}$ for ALPs in the mass range of 200 MeV - 5 GeV.",
        "TOPCAT and STILTS are related packages for desktop analysis of tabular data,\npresenting GUI and command-line interfaces respectively to much of the same\nfunctionality. This paper presents features in TOPCAT that facilitate use of\nSTILTS.",
        "Medical imaging systems are commonly assessed and optimized by the use of\nobjective measures of image quality (IQ). The performance of the ideal observer\n(IO) acting on imaging measurements has long been advocated as a\nfigure-of-merit to guide the optimization of imaging systems. For computed\nimaging systems, the performance of the IO acting on imaging measurements also\nsets an upper bound on task-performance that no image reconstruction method can\ntranscend. As such, estimation of IO performance can provide valuable guidance\nwhen designing under-sampled data-acquisition techniques by enabling the\nidentification of designs that will not permit the reconstruction of\ndiagnostically inappropriate images for a specified task - no matter how\nadvanced the reconstruction method is or how plausible the reconstructed images\nappear. The need for such analysis is urgent because of the substantial\nincrease of medical device submissions on deep learning-based image\nreconstruction methods and the fact that they may produce clean images\ndisguising the potential loss of diagnostic information when data is\naggressively under-sampled. Recently, convolutional neural network (CNN)\napproximated IOs (CNN-IOs) was investigated for estimating the performance of\ndata space IOs to establish task-based performance bounds for image\nreconstruction, under an X-ray computed tomographic (CT) context. In this work,\nthe application of such data space CNN-IO analysis to multi-coil magnetic\nresonance imaging (MRI) systems has been explored. This study utilized stylized\nmulti-coil sensitivity encoding (SENSE) MRI systems and deep-generated\nstochastic brain models to demonstrate the approach. Signal-known-statistically\nand background-known-statistically (SKS\/BKS) binary signal detection tasks were\nselected to study the impact of different acceleration factors on the data\nspace IO performance.",
        "We discuss and extend some aspects pertaining to the canonical quantisation\nof JT gravity in de Sitter space, including the problem of time and the\nconstruction of a Hilbert space. We then extend this discussion to other two\ndimensional models obtained by changing the dilaton potential and show that the\ncanonical quantisation procedure can be carried out for a large class of such\nmodels. Some discussion leading towards a path integral understanding for\nstates, other than the Hartle Hawking state, is also included here, along with\ncomments pertaining to Holography and the entropy of de Sitter space.",
        "Process theories provide a powerful framework for describing compositional\nstructures across diverse fields, from quantum mechanics to computational\nlinguistics. Traditionally, they have been formalized using symmetric monoidal\ncategories (SMCs). However, various generalizations, including time-neutral,\nhigher-order, and enriched process theories, do not naturally conform to this\nstructure. In this work, we propose an alternative formalization using operad\nalgebras, motivated by recent results connecting SMCs to operadic structures,\nwhich captures a broader class of process theories. By leveraging the\nstring-diagrammatic language, we provide an accessible yet rigorous formulation\nthat unifies and extends traditional process-theoretic approaches. Our operadic\nframework not only recovers standard process theories as a special case but\nalso enables new insights into quantum foundations and compositional\nstructures. This work paves the way for further investigations into the\nalgebraic and operational properties of generalised process theories within an\noperadic setting.",
        "We develop a non-collocated, observer-based output-feedback law for a class\nof continua of linear hyperbolic PDE systems, which are viewed as the continuum\nversion of $n+m$, general heterodirectional hyperbolic systems as $n\\to\\infty$.\nThe design relies on the introduction of a novel, continuum PDE backstepping\ntransformation, which enables the construction of a Lyapunov functional for the\nestimation error system. Stability under the observer-based output-feedback law\nis established by using the Lyapunov functional construction for the estimation\nerror system and proving well-posedness of the complete closed-loop system,\nwhich allows utilization of the separation principle.\n  Motivated by the fact that the continuum-based designs may provide\ncomputationally tractable control laws for large-scale, $n+m$ systems, we then\nutilize the control\/observer kernels and the observer constructed for the\ncontinuum system to introduce an output-feedback control design for the\noriginal $n+m$ system. We establish exponential stability of the resulting\nclosed-loop system, which consists of a mixed $n+m$-continuum PDE system\n(comprising the plant-observer dynamics), introducing a virtual continuum\nsystem with resets, which enables utilization of the continuum approximation\nproperty of the solutions of the $n+m$ system by its continuum counterpart (for\nlarge $n$). We illustrate the potential computational complexity\/flexibility\nbenefits of our approach via a numerical example of stabilization of a\nlarge-scale $n+m$ system, for which we employ the continuum observer-based\ncontroller, while the continuum-based stabilizing control\/observer kernels can\nbe computed in closed form.",
        "The large number of strong lenses discoverable in future astronomical surveys\nwill likely enhance the value of strong gravitational lensing as a cosmic probe\nof dark energy and dark matter. However, leveraging the increased statistical\npower of such large samples will require further development of automated lens\nmodeling techniques. We show that deep learning and simulation-based inference\n(SBI) methods produce informative and reliable estimates of parameter\nposteriors for strong lensing systems in ground-based surveys. We present the\nexamination and comparison of two approaches to lens parameter estimation for\nstrong galaxy-galaxy lenses -- Neural Posterior Estimation (NPE) and Bayesian\nNeural Networks (BNNs). We perform inference on 1-, 5-, and 12-parameter lens\nmodels for ground-based imaging data that mimics the Dark Energy Survey (DES).\nWe find that NPE outperforms BNNs, producing posterior distributions that are\nmore accurate, precise, and well-calibrated for most parameters. For the\n12-parameter NPE model, the calibration is consistently within $<$10\\% of\noptimal calibration for all parameters, while the BNN is rarely within 20\\% of\noptimal calibration for any of the parameters. Similarly, residuals for most of\nthe parameters are smaller (by up to an order of magnitude) with the NPE model\nthan the BNN model. This work takes important steps in the systematic\ncomparison of methods for different levels of model complexity.",
        "We present a simple elementary recursive representation of the so called\nFaulhaber series $\\sum_{k=1}^n k^N$ for integer $n$ and $N$, without reference\nto Bernoulli numbers or polynomials.",
        "Particle Therapy (PT) has emerged as a powerful tool in cancer treatment,\nleveraging the unique dose distribution of charged particles to deliver high\nradiation levels to the tumor while minimizing damage to surrounding healthy\ntissue. Despite its advantages, further improvements in Treatment Planning\nSystems (TPS) are needed to address uncertainties related to fragmentation\nprocess, which can affect both dose deposition and effectiveness. These\nfragmentation effects also play a critical role in Radiation Protection in\nSpace, where astronauts are exposed to high level of radiation, necessitating\nprecise models for shielding optimization. The FOOT (FragmentatiOn Of Target)\nexperiment addresses these challenges by measuring fragmentation cross-section\nwith high precision, providing essential data for improving TPS for PT and\nspace radiation protection strategies. This thesis contributes to the FOOT\nexperiment in two key areas. First, it focuses on the performances of the\nvertex detector, which is responsible for reconstructing particle tracks and\nfragmentation vertexes with high spatial resolution. The study evaluates the\ndetector's reconstruction algorithm and its efficiency to detect particles.\nSecond the thesis present a preliminary calculation of fragmentation cross\nsection, incorporating the vertex detector for the first time in these\nmeasurements.",
        "While cloud-cloud collisions (CCCs) have been proposed as a mechanism for\ntriggering massive star formation, it is suggested that higher collision\nvelocities ($v_{\\rm col}$) and lower GMC mass ($M_{\\rm GMC}$) or\/and density\n($\\Sigma_{\\rm GMC}$) tend to suppress star formation. In this study, we choose\nthe nearby barred galaxy NGC 3627 to examine the SFR and SFE of a colliding GMC\n($m^\\star_{\\rm CCC}$ and $\\epsilon_{\\rm CCC}$) and explore the connections\nbetween $m^\\star_{\\rm CCC}$ and $\\epsilon_{\\rm CCC}$, $M_{\\rm\nGMC}$($\\Sigma_{\\rm GMC}$) and $v_{\\rm col}$, and galactic structures (disk,\nbar, and bar-end). Using ALMA CO(2--1) data (60~pc resolution), we estimated\n$v_{\\rm col}$ within 500~pc apertures, based on line-of-sight GMC velocities,\nassuming random motion in a two-dimensional plane. We extracted apertures where\nat least 0.1 collisions occur per 1 Myr, identifying them as regions dominated\nby CCC-driven star formation, and then calculated $m^\\star_{\\rm CCC}$ and\n$\\epsilon_{\\rm CCC}$ using attenuation-corrected H$\\alpha$ data from VLT MUSE.\nWe found that both $m^\\star_{\\rm CCC}$ and $\\epsilon_{\\rm CCC}$ are lower in\nthe bar (median values: $10^{3.84}~M_\\odot$ and $0.18~\\%$), and higher in the\nbar-end ($10^{4.89}~M_\\odot$ and $1.10~\\%$) compared to the disk\n($10^{4.28}~M_\\odot$ and $0.75~\\%$). Furthermore, we found that structural\ndifferences within the parameter space of $v_{\\rm col}$ and $M_{\\rm\nGMC}$($\\Sigma_{\\rm GMC}$), with higher $M_{\\rm GMC}$($\\Sigma_{\\rm GMC}$) in the\nbar-end and higher $v_{\\rm col}$ in the bar compared to the disk, lead to\nhigher star formation activity in the bar-end and lower activity in the bar.\nOur results support the scenario that variations in CCC properties across\ndifferent galactic structures can explain the observed differences in SFE on a\nkpc scale within a disk galaxy."
      ]
    }
  },
  {
    "id":2411.03389,
    "research_type":"applied",
    "start_id":"b9",
    "start_title":"Attention Is All You Need",
    "start_abstract":"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. best performing also connect the encoder and decoder through attention mechanism. We propose a new simple network architecture, Transformer, solely mechanisms, dispensing with recurrence convolutions entirely. Experiments two machine translation tasks show these to be superior quality while being more parallelizable requiring significantly less time train. Our model achieves 28.4 BLEU WMT 2014 English-to-German task, improving over existing results, including ensembles by 2 BLEU. On English-to-French our establishes single-model state-of-the-art score of 41.8 after training for 3.5 days eight GPUs, small fraction costs from literature. that Transformer generalizes well other applying it successfully English constituency parsing both large limited data.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "Continuous-energy Monte Carlo neutron transport on GPUs in the Shift code"
      ],
      "abstract":[
        "A continuous-energy Monte Carlo neutron transport solver executing on GPUs has been developed within the Shift code. Several algorithmic approaches are considered, including both history-based and event-based implementations. Unlike in previous work involving multigroup Monte Carlo transport, it is demonstrated that event-based algorithms significantly outperform a history-based approach for continuous-energy transport as a result of increased device occupancy and reduced thread divergence. Numerical results are presented for detailed full-core models of a small modular reactor (SMR), including a model containing depleted fuel materials. These results demonstrate the substantial gains in performance that are possible with the latest-generation of GPUs. On the depleted SMR core configuration, an NVIDIA P100 GPU with 56 streaming multiprocessors provides performance equivalent to 90 CPU cores, and the latest V100 GPU with 80 multiprocessors offers the performance of more than 150 CPU cores."
      ],
      "categories":[
        "astro-ph.HE"
      ]
    },
    "list":{
      "title":[
        "Carpet-3 detection of a photon-like air shower with estimated primary\n  energy above 100 TeV in a spatial and temporal coincidence with GRB 221009A",
        "Observation of discontinuities in the periodic modulation of PSR\n  B1828-11",
        "Forecasting of the time-dependent fluxes of antiprotons in the AMS-02\n  era",
        "Peculiar radio-bright behaviour of the Galactic black hole transient 4U\n  1543-47 in the 2021-2023 outburst",
        "Blazars Jets and prospects for TeV-PeV neutrinos & gamma-rays through\n  cosmic-ray interactions",
        "Nonextensive entropic behavior observed in Quasar 3C 273",
        "A second-scale periodicity in an active repeating fast radio burst\n  source",
        "A Possible Four-Month Periodicity in the Activity of FRB 20240209A",
        "Spectroscopy of Supernova Remnants and Candidates in M31",
        "Comparison of Equations of State for Neutron Stars with First-Order\n  Phase Transitions: A Qualitative Study",
        "Evolution of LISA Observables for Binary Black Holes Lensed by an SMBH",
        "Discovery of the variable optical counterpart of the redback pulsar PSR\n  J2055+1545",
        "Photon-ALP beam propagation from Mrk 501",
        "Investigating Evolving Wormholes in $f(R,T)$ Gravity",
        "Bounded-Confidence Models of Multi-Dimensional Opinions with\n  Topic-Weighted Discordance",
        "Separation of the initial conditions in the inverse problem for 1D\n  non-linear tsunami wave run-up theory",
        "Energy Dispersion, Superconductivity and Magnetic Fluctuations in\n  Stacked Altermagnetism Materials",
        "SpecPT (Spectroscopy Pre-trained Transformer) Model for Extragalactic\n  Spectroscopy: I. Architecture and Automated Redshift Measurement",
        "Time-resolved second-order autocorrelation function of parametric\n  downconversion",
        "FinRLlama: A Solution to LLM-Engineered Signals Challenge at FinRL\n  Contest 2024",
        "Different physical and numerical sources of scatter in the\n  $M_{\\star}$-$M_{\\mathrm{BH}}$ relation and their connection to galaxy\n  evolution",
        "Membrane Charge Effects on Solute Transport in Polyamide Membranes",
        "Cycle Patterns and Mean Payoff Games",
        "Machine Learning-Driven Analytical Models for Threshold Displacement\n  Energy Prediction in Materials",
        "Non-Variational Quantum Random Access Optimization with Alternating\n  Operator Ansatz",
        "Rhizaform algebras",
        "Conical Targets for Enhanced High-Current Positron Sources",
        "Electroweak diboson production in association with a high-mass dijet\n  system in semileptonic final states from $pp$ collisions at $\\sqrt{s} = 13$\n  TeV with the ATLAS detector"
      ],
      "abstract":[
        "The brightest cosmic gamma-ray burst (GRB) ever detected, GRB 221009A, was\naccompanied by photons of very high energies. These gamma rays may be used to\ntest both the astrophysical models of the burst and our understanding of\nlong-distance propagation of energetic photons, including potential new-physics\neffects. Here we present the observation of a photon-like air shower with the\nestimated primary energy of $300^{+43}_{-38}$ TeV, coincident (with the chance\nprobability of $\\sim 9\\cdot 10^{-3}$) with the GRB in its arrival direction and\ntime. Making use of the upgraded Carpet-3 muon detector and new machine\nlearning analysis, we estimate the probability that the primary was hadronic as\n$\\sim 3 \\cdot 10^{-4}$. This is the highest-energy event ever associated with\nany GRB.",
        "PSR B1828-11 is a radio pulsar that undergoes periodic modulations (~500\ndays) of its spin-down rate and beam width, providing a valuable opportunity to\nunderstand the rotational dynamics of neutron stars. The periodic modulations\nhave previously been attributed to planetary companion(s), precession, or\nmagnetospheric effects and have several interesting features: they persist over\n10 cycles, there are at least two harmonically related components, and the\nperiod is decreasing at a rate of about 5 days per cycle. PSR B1828-11 also\nexperienced a glitch, a sudden increase in its rotation frequency, at 55 040.9\nModified Julian Day(MJD). By studying the interaction of the periodic\nmodulations with the glitch, we seek to find evidence to distinguish\nexplanations of the periodic modulation. Using a phenomenological model, we\nanalyse a recently published open data set from Jodrell Bank Observatory,\nproviding the longest and highest resolution measurements of the pulsar's\nspin-down rate data. Our phenomenological model consists of step changes in the\namplitude, modulation frequency, and phase of the long-term periodic modulation\nand the usual spin-down glitch behaviour. We find clear evidence with a\n(natural-log) Bayes factor of 1486 to support that not only is there a change\nto these three separate parameters but that the shifts occur before the glitch.\nFinally, we also present model-independent evidence which demonstrates visually\nhow and when the modulation period and amplitude change. Discontinuities in the\nmodulation period are difficult to explain if a planetary companion sources the\nperiodic modulations, but we conclude with a discussion on the insights into\nprecession and magnetospheric switching.",
        "The spectra of galactic cosmic rays (GCRs) contain crucial information about\ntheir origin and propagation through the interstellar medium. When GCRs reach\nEarth, they are significantly influenced by the solar wind and the heliospheric\nmagnetic field, a phenomenon known as solar modulation. This effect introduces\ntime-dependent variations in GCR fluxes. The AMS-02 experiment has released\ntime-dependent flux data for protons, electrons, and positrons, revealing clear\ncorrelations with solar modulation. Studies suggest that cosmic rays with the\nsame charge, such as protons and helium nuclei, exhibit similar\/same solar\nmodulation parameters. In this work, we derive the LIS for protons and\npositrons under the assumption of a common solar modulation potential, using\ndata from Voyager 1 and a 7-year average from AMS-02. Similarly, the LIS for\nantiprotons and electrons is derived by assuming they are governed by a\nseparate solar modulation potential. We demonstrate that the time-dependent\nfluxes of positrons and protons can be accurately modeled using the same set of\nsolar modulation parameters within a modified force-field approximation\nframework. Based on this, we predict the time-dependent fluxes of antiprotons\nusing the corresponding electron flux data.",
        "Correlated behaviours between the radio emission and the X-ray emission in\nGalactic black hole X-ray binaries (BH XRBs) in the X-ray hard state are\ncrucial to the understanding of disc-jet coupling of accreting black holes. The\nBH transient 4U 1543-47 went into outburst in 2021 following ~19 years of\nquiescence. We followed it up with ~weekly cadence with MeerKAT for about one\nyear and a half until it faded into quiescence. Multi-epoch quasi-simultaneous\nMeerKAT and X-ray observations allowed us to trace the compact jet emission and\nits X-ray emission. In its hard spectral state across three orders of magnitude\nof X-ray luminosities above ~10$^{34}$ ergs\/s, we found the correlation between\nradio and X-ray emission had a power-law index of 0.82$\\pm$0.09, steeper than\nthe canonical value of ~0.6 for BH XRBs. In addition, the radio vs. X-ray\ncorrelation shows a large range of the power-law normalization, with the\nmaximum significantly larger than that obtained for most BH XRBs, indicating it\ncan be particularly radio-bright and variable in the X-ray binary sample. The\nradio emission is unlikely diluted by discrete jet components. The observed\npeculiar radio-bright and variable behaviours provide the evidence for the\nrelativistic effects of a variable Lorentz factor in the range between 1 and ~2\nof the compact jet.",
        "This study explores the origins of cosmic rays and their secondary\nmessengers, focusing on the potential role of four BL Lacs W Comae, 1ES\n1959+650, PKS 2005-489, and PKS 2155-304 as potential sources of astrophysical\nneutrinos and gamma rays. We analyzed a single-zone model to understand the\ninteractions between high-energy protons and ambient photons within blazar\njets, leading to neutrino production observables and gamma-ray emission. This\nmodeling contextualizes the emissions within multiwavelength observations and\nevaluates the capabilities of the next-generation Cherenkov Telescope Array\nObservatory (CTAO) in detecting these emissions. Our estimations suggest that\nthese sources could be effective emitters of CRs, highlighting the need for\nfuture multimessenger observations to further investigate and constrain this\nclass of sources.",
        "We investigate the flux intensities spanning from radio waves to X-rays\nacross 39 light curves of Quasar 3C 273, utilizing publicly available data\ncollected by the Integral Science Data Centre (ISDC) database. Our results\nsuggest that Quasar 3C 273 exhibits nonextensive behavior. Furthermore, we\ncalculate the $q$ entropic indices for these light curves using the\n$q$-Gaussian distribution with a predominant observation of cases where $q>1$.\nBased on this index, we estimate the non-extensive entropy ($S_{q}$) and\nexplore its correlation with the energy (in eV). In this context, we identify\ntwo jump-like increases in entropy, particularly evident in the infrared (IR)\nand X-ray wavebands. The peak in the far-IR band, around 0.34 eV, results from\nsynchrotron flares evolving from higher to lower energies and thermal radiation\nemitted by hot dust near the sublimation radius. However, the second entropic\npeak in the hard X-ray range lacks statistical robustness due to limited data\nor large measurement uncertainties.",
        "Fast radio bursts (FRBs) are fierce radio flashes from the deep sky. Abundant\nobservations have indicated that highly magnetized neutron stars might be\ninvolved in these energetic bursts, but the underlying trigger mechanism is\nstill enigmatic. Especially, the widely expected periodicity connected to the\nspin of the central engine has never been discovered, which leads to further\ndebates on the nature of FRBs. Here we report the first discovery of a $\\sim\n1.7$ s period in the repeating source of FRB 20201124A. This is an active\nrepeater, from which more than 2800 FRBs have been observed on a total of 49\ndays. The phase-folding method is adopted to analyze the bursts on each day\nseparately. While no periodical signal is found in all other datasets, a clear\nperiodicity does appear on two specific days, i.e. a period of $1.706015(2)$ s\non MJD 59310, and a slightly larger period of $1.707972(1)$ s on MJD 59347. A\nperiod derivative of $6.14\\times10^{-10}$ s s$^{-1}$ can be derived from these\ntwo periods, which further implies a surface magnetic field strength of\n$1.04\\times10^{15}$ G and a spin-down age of $44$ years for the central engine.\nIt is thus concluded that FRB 20201124A should be associated with a young\nmagnetar.",
        "Fast Radio Bursts (FRBs) are millisecond-duration radio transients from\ndistant galaxies. While most FRBs are singular events, repeaters emit multiple\nbursts, with only two-FRB 121102 and FRB 180916B-showing periodic activity (160\nand 16 days, respectively). FRB 20240209A, discovered by CHIME-FRB, is\nlocalized to the outskirts of a quiescent elliptical galaxy (z = 0.1384). We\ndiscovered a periodicity of ~ 126 days in the activity of the FRB 20240209A,\npotentially adding to the list of extremely rare periodic repeating FRBs. We\nused auto-correlation and Lomb-Scargle periodogram analyses, validated with\nrandomized control samples, to confirm the periodicity. The FRB's location in\nan old stellar population disfavors young progenitor models, instead pointing\nto scenarios involving globular clusters, late-stage magnetars, or low-mass\nX-ray binaries (LMXBs). Though deep X-ray or polarimetric observations are not\navailable, the localization of the FRB and a possible periodicity points to\nprogenitors likely to be a binary involving a compact object and a stellar\ncompanion or a precessing or rotating old neutron star.",
        "With a star formation rate of order 0.4 M$_\\odot $ yr$^{-1}$, M31 should have\nsignificant population of supernova remnants (SNRs), and, in fact, 156 SNR and\nSNR candidates have been suggested by Lee et al. (2014) by searching for\nnebulae with elevated [SII]\/H${\\alpha}$ ratios in narrow band images. Here we\nuse a combination of low and high resolution optical spectroscopy obtained with\nHectospec on the MMT to characterize 152 of these nebulae. Of these candidates,\nwe find 93 nebulae that have [SII]\/H${\\alpha}$ ratios that exceed 0.4, the\ntraditional ratio used to separate SNRs from HII regions, strongly suggesting\nthat at least these objects are SNRs. Our high resolution spectroscopy reveals\n108 nebulae that have velocity widths in H${\\alpha} $ (full-width at 20% peak\nflux) that exceed 50 km s$^{-1}$, significantly larger than found in HII\nregions. There are 72 objects that satisfy both tests. Here we discuss the\nspectroscopic characteristics of all of the objects in our sample, and the\nlikelihood that other objects in the sample of Lee et al. are also SNRs, and we\nbriefly consider confirmation by X-ray, radio and UV observations. We also\ndiscuss several new candidates that have been identified serendipitously in the\ncourse of examining a large amount of archival Hectospec data.",
        "The equation of state is fundamental in describing matter under the extreme\nconditions characteristic of neutron stars and is central to advancing our\nunderstanding of dense matter physics. A critical challenge, however, lies in\naccurately modelling first-order phase transitions while ensuring thermodynamic\nconsistency and aligning with astrophysical observations. This study explores\ntwo frameworks for constructing EoSs with first-order phase transitions: the\npolytropic interpolation method and the randomized speed-of-sound interpolation\napproach. It is found that the mass-radius relation and pressure vs. energy\ndensity relation are blind towards the thermodynamic consistency check. The\npolytropic interpolation method can exhibit discontinuities in the chemical\npotential for first-order phase transition, raising concerns regarding\npotential causality violations and thermodynamic inconsistencies. In contrast,\nthe speed of sound interpolation approach ensures continuity in the chemical\npotential, offering a more thermodynamically consistent and reliable framework.\nMoreover, the sound speed method effectively captures the softer segment of the\nmass-radius spectrum, a capability not achieved by the consistent\npiecewise-polytropic approach due to its monotonic stiffness constraints. The\nspeed of sound definition involving number density and chemical potential\nreveals the thermodynamic inconsistency, making it a more consistent and robust\ndefinition. These findings underscore the importance of thermodynamic\nconsistency in EoS construction and highlight the advantages of the randomized\nspeed-of-sound method for modelling phase transitions in dense matter.",
        "Binary black holes (BBH) are expected to form and merge in active galactic\nnuclei (AGN), deep in the potential well of a supermassive black hole (SMBH),\nfrom populations that exist in a nuclear star cluster (NSC). Here we\ninvestigate the gravitational wave (GW) signature of a BBH lensed by a nearby\nSMBH. For a fiducial GW150914-like BBH orbiting close to a $10^{8}M_{\\odot}$\nSMBH located at $z=0.1$, the lensed GW signal varies in a predictable manner in\nand out of the LISA detectability band and across frequencies. The occurrence\nof such signatures has the potential to confound LISA global fit models if they\nare not modelled. Detection of these sources provide an independent measure of\nAGN inclination angles, along with detecting warping of the inner disk, and\nmeasuring the SMBH spin.",
        "We present the discovery of the variable optical counterpart to PSR\nJ2055+1545, a redback millisecond pulsar, and the first radial velocity curve\nof its companion star. The multi-band optical light curves of this system show\na $0.4$$-$$0.6 \\ \\mathrm{mag}$ amplitude modulation with a single peak per\norbit and variable colours, suggesting that the companion is mildly irradiated\nby the pulsar wind. We find that the flux maximum is asymmetric and occurs at\norbital phase $\\simeq0.4$, anticipating the superior conjunction of the\ncompanion (where the optical emission of irradiated redback companions is\ntypically brightest). We ascribe this asymmetry, well fit with a hot spot in\nour light curve modelling, to irradiation from the intrabinary shock between\npulsar and companion winds. The optical spectra obtained with the \\textit{Gran\nTelescopio Canarias} reveal a G-dwarf companion star with temperatures of $5749\n\\pm 34 \\ \\mathrm{K}$ and $6106 \\pm 35 \\ \\mathrm{K}$ at its inferior and\nsuperior orbital conjunctions, respectively, and a radial velocity\nsemi-amplitude of $385 \\pm 3 \\ \\mathrm{km}\\ \\mathrm{s}^{-1}$. Our best-fit\nmodel yields a neutron star mass of $1.7^{+0.4}_{-0.1} \\ \\mathrm{M_{sun}}$ and\na companion mass of $0.29^{+0.07}_{-0.01} \\ \\mathrm{M_{sun}}$. Based on the\nclose similarity between the optical light curve of PSR~J2055$+$1545 and those\nobserved from PSR J1023+0038 and PSR J1227-4853 during their rotation-powered\nstates, we suggest this system may develop an accretion disc in the future and\nmanifest as a transitional millisecond pulsar.",
        "The very high energy (VHE, E $>$ $100 \\mathrm~{GeV}$) $\\gamma$-ray\nobservations offer a possibility of indirectly detecting the presence of\naxion-like particles (ALPs). The paper focuses on detecting photon-ALP\noscillations on $\\gamma$-ray spectra from distant sources in astrophysical\nmagnetic fields. Strong evidence indicates that: (1) the photon-ALP\noscillations can effectively decrease the photon absorption at energies of\nseveral tens of TeV -- caused by the extragalactic background light (EBL) -- to\na level able to explain better the observational data; (2) the impact of\nmagnetic-field models in photon-ALP beams crossing several magnetized media is\nsignificant. We revisit the expected signature for the photon-ALP oscillation\neffects on $\\gamma-\\gamma $ absorption in the TeV spectra of Mrk 501. The\nresult issues that the photon-ALP beam propagation with mass\n$\\mathrm{m_a}\\sim10^{-10} eV$ and two-photon coupling constant\n$\\begin{aligned}g_{a\\gamma}\\sim0.417\\times10^{-11}GeV^{-1}\\end{aligned}$\ncrossing reasonable magnetic field scenarios considered here can roughly\nreproduce the observed TeV $\\gamma$-ray spectra for Mrk 501.",
        "The present work examines whether evolving wormhole solution is possible or\nnot in $f(R,T)$ modified gravity theory. In the background of inhomogeneous\nFLRW type wormhole configuration the field equations are investigated for\ndifferent choices of scale factors and shape functions. For the power law and\nexponential choice of the scale factor from cosmological context and decoupled\npower law of $f(R,T)$ in each variable, wormhole configuration has been\nexamined for two viable choices of shape function. Energy conditions are\nexamined graphically for a range of values of the parameters involved. Finally,\nthe possibility of emergent scenario at early cosmic evolution has been\nexamined.",
        "People's opinions on a wide range of topics often evolve over time through\ntheir interactions with others. Models of opinion dynamics primarily focus on\none-dimensional opinions which represent opinions on one topic. However,\nopinions on various topics are rarely isolated; instead, they can be\ninterdependent and exhibit correlations. In a bounded-confidence model (BCM) of\nopinion dynamics, agents influence each other's opinions only if their opinions\nare sufficiently similar. We extend classical agent-based BCMs -- namely, the\nHegeselmann--Krause BCM, which has synchronous interactions, and the\nDeffuant--Weisbuch BCM, which has asynchronous interactions -- to a\nmultidimensional setting, in which the opinions are multidimensional vectors\nrepresenting opinions of different topics and opinions on different topics are\ninterdependent. To measure opinion differences between agents, we introduce\ntopic-weighted discordance functions that account for opinion differences in\nall topics. We use the regions of receptiveness to characterize the\nsteady-state opinion clusters and provide an analytical approach to compute\nthese regions. In addition, we numerically simulate our models on various\nnetworks with initial opinions drawn from a variety of distributions. When\ninitial opinions are correlated across different topics, our topic-weighted\nBCMs yield significantly different results in both transient and steady states\ncompared to baseline models, where the dynamics of each opinion topic are\nindependent.",
        "We investigate the inverse tsunami wave problem within the framework of the\n1D nonlinear shallow water equations (SWE). Specifically, we focus on\ndetermining the initial displacement $\\eta_0(x)$ and velocity $u_0(x)$ of the\nwave, given the known motion of the shoreline $R(t)$ (the wet\/dry free\nboundary). We demonstrate that for power-shaped inclined bathymetries, this\nproblem admits a complete solution for any $\\eta_0$ and $u_0$, provided the\nwave does not break. In particular, we show that the knowledge of $R(t)$\nenables the unique recovery of both $\\eta_0(x$) and $u_0(x)$ in terms of the\nAbel transform.\n  It is important to note that, in contrast to the direct problem (also known\nas the tsunami wave run-up problem), where $R(t)$ can be computed exactly only\nfor $u_0(x)=0$, our algorithm can recover $\\eta_0$ and $u_0$ exactly for any\nnon-zero $u_0$. This highlights an interesting asymmetry between the direct and\ninverse problems. Our results extend the work presented in\n\\cite{Rybkin23,Rybkin24}, where the inverse problem was solved for $u_0(x)=0$.\nAs in previous work, our approach utilizes the Carrier-Greenspan\ntransformation, which linearizes the SWE for inclined bathymetries. Extensive\nnumerical experiments confirm the efficiency of our algorithms.",
        "Recently, altermagnetism (AM) has emerged as a new category of magnetism,\nalongside conventional antiferromagnetism (AFM) and ferromagnetism (FM). In an\nAM, superconductivity (SC) is faced with a dilemma that the spin-polarized\nbands, induced by the broken time reversal (T ) symmetry, dominantly supports\nspin-triplet pairing. In contrast, AM spin fluctuations routinely facilitate\nspin-singlet pairing as in AFM. Consequently, unconventional SC is either\nabsent or weak in AM materials. Here, we propose that stacking 2D AM materials\ncould resolve this dilemma. Stacked 2D materials have yielded a variety of new\nelectronic properties by altering the symmetries inherent in the monolayer. In\na 2D anisotropic Hubbard model, we investigate the general energy dispersions\nof both single-layer and stacked AM materials. We demonstrate that AM sheet\nstacking can alter the original symmetries, consequently affecting the energy\ndispersion. The interlayer magnetic coupling enhances the low q magnetic\nfluctuations. T symmetry is restored in the AA stacking with an\nantiferromagnetic interlayer coupling, and then both the energy dispersion and\npairing interaction are in favor of spin-singlet SC. The ferromagnetic\ninterlayer coupling in the AB stacking not only recovers T symmetry but also\nsupports spin-triplet pairing. It is further anticipated that twisted bilayer\nAM sheets could exhibit additional novel electronic properties, including\ntopology, flat bands, and collective excitations. Our work illustrates that\nstacking sheets of AM materials could open up a unique research domain in\nexploring novel quantum phenomena and offer a fertile ground for potential\nelectronic applications.",
        "We introduce the Spectroscopy Pre-trained Transformer (SpecPT), a\ntransformer-based model designed to analyze spectroscopic data, with\napplications in spectrum reconstruction and redshift measurement. Using the\nEarly Data Release (EDR) of the DESI survey, we evaluate SpecPT's performance\non two distinct datasets: the Bright Galaxy Survey (BGS) and Emission Line\nGalaxy (ELG) samples. SpecPT successfully reconstructs spectra, accurately\ncapturing emission lines, absorption features, and continuum shapes while\neffectively reducing noise. For redshift prediction, SpecPT achieves\ncompetitive accuracy, with Normalized Median Absolute Deviation (NMAD) values\nof 0.0006 and 0.0008, and catastrophic outlier fractions of 0.20% and 0.80% for\nBGS and ELG, respectively. Notably, SpecPT performs consistently well across\nthe full redshift range ($0 < z < 1.6$), demonstrating its versatility and\nrobustness. By leveraging its learned latent representations, SpecPT lays the\ngroundwork for a foundational spectroscopic model, with potential applications\nin outlier detection, interstellar medium (ISM) property estimation, and\ntransfer learning to other datasets. This work represents a first step in\nbuilding a generalized framework for spectroscopic analysis, capable of scaling\nto the full DESI dataset and beyond.",
        "We study a possibility of measuring the time-resolved second-order\nautocorrelation function of one of two beams generated in type-II parametric\ndownconversion by means of temporal magnification of this beam, bringing its\ncorrelation time from the picosecond to the nanosecond scale, which can be\nresolved by modern photodetectors. We show that such a measurement enables one\nto infer directly the degree of global coherence of that beam, which is linked\nby a simple relation to the number of modes characterizing the entanglement\nbetween the two generated beams. We illustrate the proposed method by an\nexample of photon pairs generated in a periodically poled KTP crystal with a\nsymmetric group velocity matching for various durations of the pump pulse,\nresulting in different numbers of modes. Our theoretical model also shows that\nthe magnified double-heralded autocorrelation function of one beam exhibits a\nlocal maximum around zero delay time, corresponding to photon bunching at a\nshort time scale.",
        "In response to Task II of the FinRL Challenge at ACM ICAIF 2024, this study\nproposes a novel prompt framework for fine-tuning large language models (LLM)\nwith Reinforcement Learning from Market Feedback (RLMF). Our framework\nincorporates market-specific features and short-term price dynamics to generate\nmore precise trading signals. Traditional LLMs, while competent in sentiment\nanalysis, lack contextual alignment for financial market applications. To\nbridge this gap, we fine-tune the LLaMA-3.2-3B-Instruct model using a custom\nRLMF prompt design that integrates historical market data and reward-based\nfeedback. Our evaluation shows that this RLMF-tuned framework outperforms\nbaseline methods in signal consistency and achieving tighter trading outcomes;\nawarded as winner of Task II. You can find the code for this project on GitHub.",
        "Observations have established that the masses of supermassive black holes\n(SMBHs) correlate tightly with the stellar masses of their host galaxies,\nalbeit with substantial scatter. The size of this scatter as a function of\ngalaxy mass and redshift contains valuable information about the origin of\nSMBHs and the physical nature of their co-evolution with galaxies. In this\nwork, we highlight this connection by studying the scatter in the\n$M_{\\mathrm{BH}} - M_{\\star}$ relation for massive galaxies in the Illustris,\nIllustrisTNG (TNG), and EAGLE cosmological simulations. We find that the\nscatter in TNG is significantly lower than in Illustris and EAGLE, reflecting\ntheir different BH feedback models. By performing various numerical\nexperiments, we quantify different contributions to the scatter in the\nsimulations, and also identify a suitably defined intrinsic scatter. The\nintrinsic scatter in Illustris and EAGLE is $\\sim0.3$ dex at $z=0$, and is\ndominated by variations from BH accretion, whereas the smaller scatter of TNG\nis rather dominated by hierarchical merging, suggesting that the massive\ngalaxies in TNG are more tightly quenched. Variations in the BH seed mass can\ncontribute to the scatter of the $M_{\\rm BH}-M_{\\star}$ relation as well, but\nwhether this still plays a role at $z=0$ depends on the feedback model.\nSimulations with disabled AGN feedback produce much higher scatter for low-mass\ngalaxies than seen in our cosmological simulations, demonstrating the crucial\ninfluence of feedback for determining the co-evolution of SMBHs and their host\ngalaxies in this regime. In contrast, an important factor in reducing the\nscatter for massive galaxies is hierarchical merging of mostly quenched\nsystems. Based on our results, we expect that the scatter in the\n$M_{\\mathrm{BH}} - M_{\\star}$ relation at high redshift could be particularly\npowerful in providing clues to the origin of SMBHs.",
        "Polyamide membranes, such as nanofiltration (NF) and reverse osmosis (RO)\nmembranes, are widely used for water desalination and purification. However,\nthe mechanisms of solute transport and solute rejection due to charge\ninteractions remain unclear at the molecular level. Here we use molecular\ndynamics (MD) simulations to examine the transport of single-solute feeds\nthrough charged nanofiltration membranes with different membrane charge\nconcentrations of COO$^{\\text{-}}$ and NH$_2\\!^+$ corresponding to different pH\nlevels. Results show that Na$^+$ and Cl$^{\\text{-}}$ solute ions are better\nrejected when the membrane has a higher concentration of negatively charged\ngroups, corresponding to a higher pH, whereas CaCl$_2$ is well-rejected at all\npH levels studied. These results are consistent with experimental findings\nwhich are performed at the same pH conditions as simulation setup. Moreover,\nsolute transport behavior depends on the membrane functional group\ndistribution. When COO$^{\\text{-}}$ functional groups are concentrated at\nmembrane feed surface, ion permeation into the membrane is reduced.\nCounter-ions tend to associate with charged functional groups while co-ions\nseem to pass by the charged groups more easily. In addition, steric effects\nplay a role when ions of opposite charge cluster in pores of the membrane. This\nstudy reveals solute transport and rejection mechanisms related to membrane\ncharge and provides insights into how membranes might be designed to achieve\nspecific desired solute rejection.",
        "We introduce the concept of a \\emph{cycle pattern} for directed graphs as\nfunctions from the set of cycles to the set $\\{-,0,+\\}$. The key example for\nsuch a pattern is derived from a weight function, giving rise to the sign of\nthe total weight of the edges for each cycle. Hence, cycle patterns describe a\nfundamental structure of a weighted digraph, and they arise naturally in games\non graphs, in particular parity games, mean payoff games, and energy games.\n  Our contribution is threefold: we analyze the structure and derive hardness\nresults for the realization of cycle patterns by weight functions. Then we use\nthem to show hardness of solving games given the limited information of a cycle\npattern. Finally, we identify a novel geometric hardness measure for solving\nmean payoff games (MPG) using the framework of linear decision trees, and use\ncycle patterns to derive lower bounds with respect to this measure, for large\nclasses of algorithms for MPGs.",
        "Understanding the behavior of materials under irradiation is crucial for the\ndesign and safety of nuclear reactors, spacecraft, and other radiation\nenvironments. The threshold displacement energy (Ed) is a critical parameter\nfor understanding radiation damage in materials, yet its determination often\nrelies on costly experiments or simulations. This work leverages the machine\nlearning-based Sure Independence Screening and Sparsifying Operator (SISSO)\nmethod to derive accurate, analytical models for predicting Ed using\nfundamental material properties. The models outperform traditional approaches\nfor monoatomic materials, capturing key trends with high accuracy. While\npredictions for polyatomic materials highlight challenges due to dataset\ncomplexity, they reveal opportunities for improvement with expanded data. This\nstudy identifies cohesive energy and melting temperature as key factors\ninfluencing Ed, offering a robust framework for efficient, data-driven\npredictions of radiation damage in diverse materials.",
        "Solving hard optimization problems is one of the most promising application\ndomains for quantum computers due to the ubiquity of such problems in industry\nand the availability of broadly applicable quantum speedups. However, the\nability of near-term quantum computers to tackle industrial-scale optimization\nproblems is limited by their size and the overheads of quantum error\ncorrection. Quantum Random Access Optimization (QRAO) has been proposed to\nreduce the space requirements of quantum optimization. However, to date QRAO\nhas only been implemented using variational algorithms, which suffer from the\nneed to train instance-specific variational parameters, making them difficult\nto scale. We propose and benchmark a non-variational approach to QRAO based on\nthe Quantum Alternating Operator Ansatz (QAOA) for the MaxCut problem. We show\nthat instance-independent ``fixed'' parameters achieve good performance,\nremoving the need for variational parameter optimization. Additionally, we\nevaluate different design choices, such as various mixers and initial states,\nas well as QAOA operator implementations when customizing for QRAO, and\nidentify a strategy that performs well in practice. Our results pave the way\nfor the practical execution of QRAO on early fault-tolerant quantum computers.",
        "Any anti-associative algebra gives rise to a Jacobi-Jordan algebra by [x, y]\n= xy + yx. This article aims to introduce the concept of \"rhizaform algebras\",\nwhich offer an approach to addressing anti-associativity. These algebras are\ndefined by two operations whose sum is anti-associative, with the left and\nright multiplication operators forming bimodules of the sum of anti-associative\nalgebras. This characterization parallels that of dendriform algebras, where\nthe sum of operations preserves associativity. Additionally, the notions of\nO-operators and Rota-Baxter operators on anti-associative algebras are\npresented as tools to interpret rhizaform algebras. Notably, anti-associative\nalgebras with nondegenerate Connes cocycles admit compatible rhizaform algebra\nstructures.",
        "Previous pair-production-driven positron source designs have assumed that the\ntransverse dimension of the target is significantly greater than the secondary\nbeam it generates. This paper explores the use of targets with different\ntransverse profiles with the aim of enhancing positron production. The starting\npoint of this research is the concept of wire targets, proposed by M. James et\nal. in 1991 for the former SLC positron source. Building on this foundation,\nthis study takes this concept a step further by introducing conical-shaped\ntargets, which can substantially improve the yield by reducing the reabsorption\nof positrons by the target--an issue that is worsened by the high-field\nsolenoid lenses commonly used for positron capture. Using Geant4 simulations,\nwe propose new conical targets adapted for the parameters of the future\ncollider FCC-ee and its positron source test facility P-cubed (PSI Positron\nProduction experiment) at the Paul Scherrer Institute. We find that conical\ntargets can nearly double the positron production at the target and enhance the\nbaseline positron yield of FCC-ee by around 60%. Additionally, we present the\nthermo-mechanical studies for the conical targets based on the FCC-ee primary\nbeam power requirements and outline the mechanical implementation for a future\nproof-of-principle demonstration at the P-cubed facility.",
        "This paper reports the observation of electroweak diboson ($WW\/WZ\/ZZ$)\nproduction in association with a high-mass dijet system, in which final states\nwith one boson decaying leptonically and the other boson decaying hadronically\nare studied. The hadronically decaying $W\/Z$ boson is reconstructed as either\ntwo small-radius jets or one large-radius jet with jet substructure\nrequirements. The data analyzed correspond to an integrated luminosity of 140\nfb$^{-1}$ of proton-proton collisions at a center-of-mass energy of\n$\\sqrt{s}=13$ TeV collected with the ATLAS detector during the 2015-2018 data\ntaking at the Large Hadron Collider. The electroweak production of $WW\/WZ\/ZZ$\nin association with two jets is observed in a phase space dominated by\nvector-boson scattering with a significance of $7.4\\sigma$ (expected\n$6.1\\sigma$) and the signal strength is determined to be\n$1.28^{+0.23}_{-0.21}$. The corresponding production cross section in a\nfiducial phase space is measured in addition. The signal strengths of both\nelectroweak and QCD associated diboson productions are furthermore measured in\na two-dimensional fit, the result of which agrees with the Standard Model\nprediction. The data are interpreted in the context of a dimension-8 effective\nfield theory to probe anomalous quartic gauge couplings resulting in the first\nset of exclusion limits on the Wilson coefficients in the semileptonic channel\nreported by the ATLAS Collaboration. The observed limits for the S02, T0 and M0\noperators are $(-3.96 < f_{S02} \/ \\Lambda^4 < 3.96)$ TeV$^{-4}$, $(-0.25 <\nf_{T0} \/ \\Lambda^4 < 0.22)$ TeV$^{-4}$, $(-1.26 < f_{M0} \/ \\Lambda^4 < 1.25)$\nTeV$^{-4}$."
      ]
    }
  },
  {
    "id":2411.03389,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"Continuous-energy Monte Carlo neutron transport on GPUs in the Shift code",
    "start_abstract":"A continuous-energy Monte Carlo neutron transport solver executing on GPUs has been developed within the Shift code. Several algorithmic approaches are considered, including both history-based and event-based implementations. Unlike in previous work involving multigroup Monte Carlo transport, it is demonstrated that event-based algorithms significantly outperform a history-based approach for continuous-energy transport as a result of increased device occupancy and reduced thread divergence. Numerical results are presented for detailed full-core models of a small modular reactor (SMR), including a model containing depleted fuel materials. These results demonstrate the substantial gains in performance that are possible with the latest-generation of GPUs. On the depleted SMR core configuration, an NVIDIA P100 GPU with 56 streaming multiprocessors provides performance equivalent to 90 CPU cores, and the latest V100 GPU with 80 multiprocessors offers the performance of more than 150 CPU cores.",
    "start_categories":[
      "astro-ph.HE"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b9"
      ],
      "title":[
        "Attention Is All You Need"
      ],
      "abstract":[
        "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. best performing also connect the encoder and decoder through attention mechanism. We propose a new simple network architecture, Transformer, solely mechanisms, dispensing with recurrence convolutions entirely. Experiments two machine translation tasks show these to be superior quality while being more parallelizable requiring significantly less time train. Our model achieves 28.4 BLEU WMT 2014 English-to-German task, improving over existing results, including ensembles by 2 BLEU. On English-to-French our establishes single-model state-of-the-art score of 41.8 after training for 3.5 days eight GPUs, small fraction costs from literature. that Transformer generalizes well other applying it successfully English constituency parsing both large limited data."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Principles for Responsible AI Consciousness Research",
        "Agentic Mixture-of-Workflows for Multi-Modal Chemical Search",
        "Benchmarking Retrieval-Augmented Generation in Multi-Modal Contexts",
        "Patterns Over Principles: The Fragility of Inductive Reasoning in LLMs\n  under Noisy Observations",
        "L2R: Learning to Reduce Search Space for Generalizable Neural Routing\n  Solver",
        "A consensus set for the aggregation of partial rankings: the case of the\n  Optimal Set of Bucket Orders Problem",
        "Cost-Saving LLM Cascades with Early Abstention",
        "Agentic AI Needs a Systems Theory",
        "Towards A Litmus Test for Common Sense",
        "Hierarchical Expert Prompt for Large-Language-Model: An Approach Defeat\n  Elite AI in TextStarCraft II for the First Time",
        "KU AIGEN ICL EDI@BC8 Track 3: Advancing Phenotype Named Entity\n  Recognition and Normalization for Dysmorphology Physical Examination Reports",
        "Theorem Prover as a Judge for Synthetic Data Generation",
        "NS-Gym: Open-Source Simulation Environments and Benchmarks for\n  Non-Stationary Markov Decision Processes",
        "Types of elements in non-commutative Poisson algebras and Dixmier\n  Conjecture",
        "Deep Learning and Foundation Models for Weather Prediction: A Survey",
        "CacheMamba: Popularity Prediction for Mobile Edge Caching Networks via\n  Selective State Spaces",
        "Solving the Catastrophic Forgetting Problem in Generalized Category\n  Discovery",
        "Motion planning for highly-dynamic unconditioned reflexes based on\n  chained Signed Distance Functions",
        "Diffusion-Based Imitation Learning for Social Pose Generation",
        "Hochschild cohomology and extensions of triangulated categories",
        "Provably-Stable Neural Network-Based Control of Nonlinear Systems",
        "Towards a Digital Twin Modeling Method for Container Terminal Port",
        "Multiport Support for Vortex OpenGPU Memory Hierarchy",
        "Anomize: Better Open Vocabulary Video Anomaly Detection",
        "Piano Transcription by Hierarchical Language Modeling with Pretrained\n  Roll-based Encoders",
        "On the limit of random hives with GUE boundary conditions",
        "Transformer Semantic Genetic Programming for Symbolic Regression",
        "Mutation Testing via Iterative Large Language Model-Driven Scientific\n  Debugging"
      ],
      "abstract":[
        "Recent research suggests that it may be possible to build conscious AI\nsystems now or in the near future. Conscious AI systems would arguably deserve\nmoral consideration, and it may be the case that large numbers of conscious\nsystems could be created and caused to suffer. Furthermore, AI systems or\nAI-generated characters may increasingly give the impression of being\nconscious, leading to debate about their moral status. Organisations involved\nin AI research must establish principles and policies to guide research and\ndeployment choices and public communication concerning consciousness. Even if\nan organisation chooses not to study AI consciousness as such, it will still\nneed policies in place, as those developing advanced AI systems risk\ninadvertently creating conscious entities. Responsible research and deployment\npractices are essential to address this possibility. We propose five principles\nfor responsible research and argue that research organisations should make\nvoluntary, public commitments to principles on these lines. Our principles\nconcern research objectives and procedures, knowledge sharing and public\ncommunications.",
        "The vast and complex materials design space demands innovative strategies to\nintegrate multidisciplinary scientific knowledge and optimize materials\ndiscovery. While large language models (LLMs) have demonstrated promising\nreasoning and automation capabilities across various domains, their application\nin materials science remains limited due to a lack of benchmarking standards\nand practical implementation frameworks. To address these challenges, we\nintroduce Mixture-of-Workflows for Self-Corrective Retrieval-Augmented\nGeneration (CRAG-MoW) - a novel paradigm that orchestrates multiple agentic\nworkflows employing distinct CRAG strategies using open-source LLMs. Unlike\nprior approaches, CRAG-MoW synthesizes diverse outputs through an orchestration\nagent, enabling direct evaluation of multiple LLMs across the same problem\ndomain. We benchmark CRAG-MoWs across small molecules, polymers, and chemical\nreactions, as well as multi-modal nuclear magnetic resonance (NMR) spectral\nretrieval. Our results demonstrate that CRAG-MoWs achieve performance\ncomparable to GPT-4o while being preferred more frequently in comparative\nevaluations, highlighting the advantage of structured retrieval and multi-agent\nsynthesis. By revealing performance variations across data types, CRAG-MoW\nprovides a scalable, interpretable, and benchmark-driven approach to optimizing\nAI architectures for materials discovery. These insights are pivotal in\naddressing fundamental gaps in benchmarking LLMs and autonomous AI agents for\nscientific applications.",
        "This paper introduces Multi-Modal Retrieval-Augmented Generation (M^2RAG), a\nbenchmark designed to evaluate the effectiveness of Multi-modal Large Language\nModels (MLLMs) in leveraging knowledge from multi-modal retrieval documents.\nThe benchmark comprises four tasks: image captioning, multi-modal question\nanswering, multi-modal fact verification, and image reranking. All tasks are\nset in an open-domain setting, requiring RAG models to retrieve query-relevant\ninformation from a multi-modal document collection and use it as input context\nfor RAG modeling. To enhance the context utilization capabilities of MLLMs, we\nalso introduce Multi-Modal Retrieval-Augmented Instruction Tuning (MM-RAIT), an\ninstruction tuning method that optimizes MLLMs within multi-modal contexts. Our\nexperiments show that MM-RAIT improves the performance of RAG systems by\nenabling them to effectively learn from multi-modal contexts. All data and code\nare available at https:\/\/github.com\/NEUIR\/M2RAG.",
        "Inductive reasoning, a cornerstone of human cognition, enables generalization\nfrom limited data but hasn't yet been fully achieved by large language models\n(LLMs). While modern LLMs excel at reasoning tasks, their ability to maintain\nstable and consistent rule abstraction under imperfect observations remains\nunderexplored. To fill this gap, in this work, we introduce Robust Rule\nInduction, a task that evaluates LLMs' capability in inferring rules from data\nthat are fused with noisy examples. To address this task, we further propose\nSample-steered Rule Refinement (SRR), a method enhancing reasoning stability\nvia observation diversification and execution-guided feedback. Experiments\nacross arithmetic, cryptography, and list functions reveal: (1) SRR outperforms\nother methods with minimal performance degradation under noise; (2) Despite\nslight accuracy variation, LLMs exhibit instability under noise (e.g., 0%\naccuracy change with only 70% consistent score); (3) Counterfactual task gaps\nhighlight LLMs' reliance on memorized patterns over genuine abstraction. Our\nfindings challenge LLMs' reasoning robustness, revealing susceptibility to\nhypothesis drift and pattern overfitting, while providing empirical evidence\ncritical for developing human-like inductive systems. Code and data are\navailable at\n\\href{https:\/\/github.com\/lcy2723\/Robust-Rule-Induction}{https:\/\/github.com\/lcy2723\/Robust-Rule-Induction}.",
        "Constructive neural combinatorial optimization (NCO) has attracted growing\nresearch attention due to its ability to solve complex routing problems without\nrelying on handcrafted rules. However, existing NCO methods face significant\nchallenges in generalizing to large-scale problems due to high computational\ncomplexity and inefficient capture of structural patterns. To address this\nissue, we propose a novel learning-based search space reduction method that\nadaptively selects a small set of promising candidate nodes at each step of the\nconstructive NCO process. Unlike traditional methods that rely on fixed\nheuristics, our selection model dynamically prioritizes nodes based on learned\npatterns, significantly reducing the search space while maintaining solution\nquality. Experimental results demonstrate that our method, trained solely on\n100-node instances from uniform distribution, generalizes remarkably well to\nlarge-scale Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing\nProblem (CVRP) instances with up to 1 million nodes from the uniform\ndistribution and over 80K nodes from other distributions.",
        "In rank aggregation problems (RAP), the solution is usually a consensus\nranking that generalizes a set of input orderings. There are different variants\nthat differ not only in terms of the type of rankings that are used as input\nand output, but also in terms of the objective function employed to evaluate\nthe quality of the desired output ranking. In contrast, in some machine\nlearning tasks (e.g. subgroup discovery) or multimodal optimization tasks,\nattention is devoted to obtaining several models\/results to account for the\ndiversity in the input data or across the search landscape. Thus, in this paper\nwe propose to provide, as the solution to an RAP, a set of rankings to better\nexplain the preferences expressed in the input orderings. We exemplify our\nproposal through the Optimal Bucket Order Problem (OBOP), an RAP which consists\nin finding a single consensus ranking (with ties) that generalizes a set of\ninput rankings codified as a precedence matrix. To address this, we introduce\nthe Optimal Set of Bucket Orders Problem (OSBOP), a generalization of the OBOP\nthat aims to produce not a single ranking as output but a set of consensus\nrankings. Experimental results are presented to illustrate this proposal,\nshowing how, by providing a set of consensus rankings, the fitness of the\nsolution significantly improves with respect to the one of the original OBOP,\nwithout losing comprehensibility.",
        "LLM cascades are based on the idea that processing all queries with the\nlargest and most expensive LLMs is inefficient. Instead, cascades deploy small\nLLMs to answer the majority of queries, limiting the use of large and expensive\nLLMs to only the most difficult queries. This approach can significantly reduce\ncosts without impacting performance. However, risk-sensitive domains such as\nfinance or medicine place an additional premium on avoiding model errors.\nRecognizing that even the most expensive models may make mistakes, applications\nin these domains benefit from allowing LLM systems to completely abstain from\nanswering a query when the chance of making a mistake is significant. However,\ngiving a cascade the ability to abstain poses an immediate design question for\nLLM cascades: should abstention only be allowed at the final model or also at\nearlier models? Since the error patterns of small and large models are\ncorrelated, the latter strategy may further reduce inference costs by letting\ninexpensive models anticipate abstention decisions by expensive models, thereby\nobviating the need to run the expensive models. We investigate the benefits of\n\"early abstention\" in LLM cascades and find that it reduces the overall test\nloss by 2.2% on average across six benchmarks (GSM8K, MedMCQA, MMLU, TriviaQA,\nTruthfulQA, and XSum). These gains result from a more effective use of\nabstention, which trades a 4.1% average increase in the overall abstention rate\nfor a 13.0% reduction in cost and a 5.0% reduction in error rate. Our findings\ndemonstrate that it is possible to leverage correlations between the error\npatterns of different language models to drive performance improvements for LLM\nsystems with abstention.",
        "The endowment of AI with reasoning capabilities and some degree of agency is\nwidely viewed as a path toward more capable and generalizable systems. Our\nposition is that the current development of agentic AI requires a more\nholistic, systems-theoretic perspective in order to fully understand their\ncapabilities and mitigate any emergent risks. The primary motivation for our\nposition is that AI development is currently overly focused on individual model\ncapabilities, often ignoring broader emergent behavior, leading to a\nsignificant underestimation in the true capabilities and associated risks of\nagentic AI. We describe some fundamental mechanisms by which advanced\ncapabilities can emerge from (comparably simpler) agents simply due to their\ninteraction with the environment and other agents. Informed by an extensive\namount of existing literature from various fields, we outline mechanisms for\nenhanced agent cognition, emergent causal reasoning ability, and metacognitive\nawareness. We conclude by presenting some key open challenges and guidance for\nthe development of agentic AI. We emphasize that a systems-level perspective is\nessential for better understanding, and purposefully shaping, agentic AI\nsystems.",
        "This paper is the second in a planned series aimed at envisioning a path to\nsafe and beneficial artificial intelligence. Building on the conceptual\ninsights of \"Common Sense Is All You Need,\" we propose a more formal litmus\ntest for common sense, adopting an axiomatic approach that combines minimal\nprior knowledge (MPK) constraints with diagonal or Godel-style arguments to\ncreate tasks beyond the agent's known concept set. We discuss how this approach\napplies to the Abstraction and Reasoning Corpus (ARC), acknowledging\ntraining\/test data constraints, physical or virtual embodiment, and large\nlanguage models (LLMs). We also integrate observations regarding emergent\ndeceptive hallucinations, in which more capable AI systems may intentionally\nfabricate plausible yet misleading outputs to disguise knowledge gaps. The\noverarching theme is that scaling AI without ensuring common sense risks\nintensifying such deceptive tendencies, thereby undermining safety and trust.\nAligning with the broader goal of developing beneficial AI without causing\nharm, our axiomatic litmus test not only diagnoses whether an AI can handle\ntruly novel concepts but also provides a stepping stone toward an ethical,\nreliable foundation for future safe, beneficial, and aligned artificial\nintelligence.",
        "Since the emergence of the Large Language Model (LLM), LLM has been widely\nused in fields such as writing, translating, and searching. However, there is\nstill great potential for LLM-based methods in handling complex tasks such as\ndecision-making in the StarCraft II environment. To address problems such as\nlack of relevant knowledge and poor control over subtasks of varying\nimportance, we propose a Hierarchical Expert Prompt (HEP) for LLM. Our method\nimproves the understanding of game situations through expert-level tactical\nknowledge, improving the processing quality of tasks of varying importance\nthrough a hierarchical framework. Our approach defeated the highest level\n(Elite) standard built-in agent in TextStarCraft II for the first time and\nconsistently outperformed the baseline method in other difficulties. Our\nexperiments suggest that the proposed method is a practical solution for\ntackling complex decision-making challenges. The replay video can be viewed on\nhttps:\/\/www.bilibili.com\/video\/BV1uz42187EF and https:\/\/youtu.be\/dO3PshWLV5M,\nand our codes have been open-sourced on\nhttps:\/\/github.com\/luchang1113\/HEP-LLM-play-StarCraftII.",
        "The objective of BioCreative8 Track 3 is to extract phenotypic key medical\nfindings embedded within EHR texts and subsequently normalize these findings to\ntheir Human Phenotype Ontology (HPO) terms. However, the presence of diverse\nsurface forms in phenotypic findings makes it challenging to accurately\nnormalize them to the correct HPO terms. To address this challenge, we explored\nvarious models for named entity recognition and implemented data augmentation\ntechniques such as synonym marginalization to enhance the normalization step.\nOur pipeline resulted in an exact extraction and normalization F1 score 2.6\\%\nhigher than the mean score of all submissions received in response to the\nchallenge. Furthermore, in terms of the normalization F1 score, our approach\nsurpassed the average performance by 1.9\\%. These findings contribute to the\nadvancement of automated medical data extraction and normalization techniques,\nshowcasing potential pathways for future research and application in the\nbiomedical domain.",
        "The demand for synthetic data in mathematical reasoning has increased due to\nits potential to enhance the mathematical capabilities of large language models\n(LLMs). However, ensuring the validity of intermediate reasoning steps remains\na significant challenge, affecting data quality. While formal verification via\ntheorem provers effectively validates LLM reasoning, the autoformalisation of\nmathematical proofs remains error-prone. In response, we introduce iterative\nautoformalisation, an approach that iteratively refines theorem prover\nformalisation to mitigate errors, thereby increasing the execution rate on the\nLean prover from 60% to 87%. Building upon that, we introduce Theorem Prover as\na Judge (TP-as-a-Judge), a method that employs theorem prover formalisation to\nrigorously assess LLM intermediate reasoning, effectively integrating\nautoformalisation with synthetic data generation. Finally, we present\nReinforcement Learning from Theorem Prover Feedback (RLTPF), a framework that\nreplaces human annotation with theorem prover feedback in Reinforcement\nLearning from Human Feedback (RLHF). Across multiple LLMs, applying\nTP-as-a-Judge and RLTPF improves benchmarks with only 3,508 samples, achieving\n5.56% accuracy gain on Mistral-7B for MultiArith, 6.00% on Llama-2-7B for\nSVAMP, and 3.55% on Llama-3.1-8B for AQUA.",
        "In many real-world applications, agents must make sequential decisions in\nenvironments where conditions are subject to change due to various exogenous\nfactors. These non-stationary environments pose significant challenges to\ntraditional decision-making models, which typically assume stationary dynamics.\nNon-stationary Markov decision processes (NS-MDPs) offer a framework to model\nand solve decision problems under such changing conditions. However, the lack\nof standardized benchmarks and simulation tools has hindered systematic\nevaluation and advance in this field. We present NS-Gym, the first simulation\ntoolkit designed explicitly for NS-MDPs, integrated within the popular\nGymnasium framework. In NS-Gym, we segregate the evolution of the environmental\nparameters that characterize non-stationarity from the agent's decision-making\nmodule, allowing for modular and flexible adaptations to dynamic environments.\nWe review prior work in this domain and present a toolkit encapsulating key\nproblem characteristics and types in NS-MDPs. This toolkit is the first effort\nto develop a set of standardized interfaces and benchmark problems to enable\nconsistent and reproducible evaluation of algorithms under non-stationary\nconditions. We also benchmark six algorithmic approaches from prior work on\nNS-MDPs using NS-Gym. Our vision is that NS-Gym will enable researchers to\nassess the adaptability and robustness of their decision-making algorithms to\nnon-stationary conditions.",
        "Non-commutative Poisson algebras are the algebras having an associative\nalgebra structure and a Lie algebra structure together with the Leibniz law.\nLet $P$ be a non-commutative Poisson algebra over some algebraically closed\nfield of characteristic zero. For any $z\\in P$, there exist four subalgebras of\n$P$ associated with the inner derivation $ad_z$ on $P$. Based on the\nrelationships between these four subalgebras, elements of $P$ can be divided\ninto eight types. We will mainly focus on two types of non-commutative Poisson\nalgebras: the usual Poisson algebras and the associative algebras with the\ncommutator as the Poisson bracket. The following problems are studied for such\nnon-commutative Poisson algebras: how the type of an element changes under\nhomomorphisms between non-commutative Poisson algebras, how the type of an\nelement changes after localization, and what the type of the elements of the\nform $z_1 \\otimes z_2$ and $z_1 \\otimes 1 + 1 \\otimes z_2$ is in the tensor\nproduct of non-commutative Poisson algebras $P_1\\otimes P_2$. As an application\nof above results, one knows that Dixmier Conjecture for $A_1$ holds under\ncertain conditions. Some properties of the Weyl algebras are also obtained,\nsuch as the commutativity of certain subalgebras.",
        "Physics-based numerical models have been the bedrock of atmospheric sciences\nfor decades, offering robust solutions but often at the cost of significant\ncomputational resources. Deep learning (DL) models have emerged as powerful\ntools in meteorology, capable of analyzing complex weather and climate data by\nlearning intricate dependencies and providing rapid predictions once trained.\nWhile these models demonstrate promising performance in weather prediction,\noften surpassing traditional physics-based methods, they still face critical\nchallenges. This paper presents a comprehensive survey of recent deep learning\nand foundation models for weather prediction. We propose a taxonomy to classify\nexisting models based on their training paradigms: deterministic predictive\nlearning, probabilistic generative learning, and pre-training and fine-tuning.\nFor each paradigm, we delve into the underlying model architectures, address\nmajor challenges, offer key insights, and propose targeted directions for\nfuture research. Furthermore, we explore real-world applications of these\nmethods and provide a curated summary of open-source code repositories and\nwidely used datasets, aiming to bridge research advancements with practical\nimplementations while fostering open and trustworthy scientific practices in\nadopting cutting-edge artificial intelligence for weather prediction. The\nrelated sources are available at https:\/\/github.com\/JimengShi\/\nDL-Foundation-Models-Weather.",
        "Mobile Edge Caching (MEC) plays a pivotal role in mitigating latency in\ndata-intensive services by dynamically caching frequently requested content on\nedge servers. This capability is critical for applications such as Augmented\nReality (AR), Virtual Reality (VR), and Autonomous Vehicles (AV), where\nefficient content caching and accurate popularity prediction are essential for\noptimizing performance. In this paper, we explore the problem of popularity\nprediction in MEC by utilizing historical time-series request data of intended\nfiles, formulating this problem as a ranking task. To this aim, we propose\nCacheMamba model by employing Mamba, a state-space model (SSM)-based\narchitecture, to identify the top-K files with the highest likelihood of being\nrequested. We then benchmark the proposed model against a Transformer-based\napproach, demonstrating its superior performance in terms of cache-hit rate,\nMean Average Precision (MAP), Normalized Discounted Cumulative Gain (NDCG), and\nFloating-Point Operations Per Second (FLOPS), particularly when dealing with\nlonger sequences.",
        "Generalized Category Discovery (GCD) aims to identify a mix of known and\nnovel categories within unlabeled data sets, providing a more realistic setting\nfor image recognition. Essentially, GCD needs to remember existing patterns\nthoroughly to recognize novel categories. Recent state-of-the-art method SimGCD\ntransfers the knowledge from known-class data to the learning of novel classes\nthrough debiased learning. However, some patterns are catastrophically forgot\nduring adaptation and thus lead to poor performance in novel categories\nclassification. To address this issue, we propose a novel learning approach,\nLegoGCD, which is seamlessly integrated into previous methods to enhance the\ndiscrimination of novel classes while maintaining performance on previously\nencountered known classes. Specifically, we design two types of techniques\ntermed as Local Entropy Regularization (LER) and Dual-views Kullback Leibler\ndivergence constraint (DKL). The LER optimizes the distribution of potential\nknown class samples in unlabeled data, thus ensuring the preservation of\nknowledge related to known categories while learning novel classes. Meanwhile,\nDKL introduces Kullback Leibler divergence to encourage the model to produce a\nsimilar prediction distribution of two view samples from the same image. In\nthis way, it successfully avoids mismatched prediction and generates more\nreliable potential known class samples simultaneously. Extensive experiments\nvalidate that the proposed LegoGCD effectively addresses the known category\nforgetting issue across all datasets, eg, delivering a 7.74% and 2.51% accuracy\nboost on known and novel classes in CUB, respectively. Our code is available\nat: https:\/\/github.com\/Cliffia123\/LegoGCD.",
        "The unconditioned reflex (e.g., protective reflex), which is the innate\nreaction of the organism and usually performed through the spinal cord rather\nthan the brain, can enable organisms to escape harms from environments. In this\npaper, we propose an online, highly-dynamic motion planning algorithm to endow\nmanipulators the highly-dynamic unconditioned reflexes to humans and\/or\nenvironments. Our method is based on a chained version of Signed Distance\nFunctions (SDFs), which can be pre-computed and stored. Our proposed algorithm\nis divided into two stages. In the offline stage, we create 3 groups of local\nSDFs to store the geometric information of the manipulator and its working\nenvironment. In the online stage, the pre-computed local SDFs are chained\ntogether according the configuration of the manipulator, to provide global\ngeometric information about the environment. While the point clouds of the\ndynamic objects serve as query points to look up these local SDFs for quickly\ngenerating escape velocity. Then we propose a modified geometric Jacobian\nmatrix and use the Jacobian-pseudo-inverse method to generate real-time reflex\nbehaviors to avoid the static and dynamic obstacles in the environment. The\nbenefits of our method are validated in both static and dynamic scenarios. In\nthe static scenario, our method identifies the path solutions with lower time\nconsumption and shorter trajectory length compared to existing solutions. In\nthe dynamic scenario, our method can reliably pursue the dynamic target point,\navoid dynamic obstacles, and react to these obstacles within 1ms, which\nsurpasses the unconditioned reflex reaction time of humans.",
        "Intelligent agents, such as robots and virtual agents, must understand the\ndynamics of complex social interactions to interact with humans. Effectively\nrepresenting social dynamics is challenging because we require multi-modal,\nsynchronized observations to understand a scene. We explore how using a single\nmodality, the pose behavior, of multiple individuals in a social interaction\ncan be used to generate nonverbal social cues for the facilitator of that\ninteraction. The facilitator acts to make a social interaction proceed smoothly\nand is an essential role for intelligent agents to replicate in human-robot\ninteractions. In this paper, we adapt an existing diffusion behavior cloning\nmodel to learn and replicate facilitator behaviors. Furthermore, we evaluate\ntwo representations of pose observations from a scene, one representation has\npre-processing applied and one does not. The purpose of this paper is to\nintroduce a new use for diffusion behavior cloning for pose generation in\nsocial interactions. The second is to understand the relationship between\nperformance and computational load for generating social pose behavior using\ntwo different techniques for collecting scene observations. As such, we are\nessentially testing the effectiveness of two different types of conditioning\nfor a diffusion model. We then evaluate the resulting generated behavior from\neach technique using quantitative measures such as mean per-joint position\nerror (MPJPE), training time, and inference time. Additionally, we plot\ntraining and inference time against MPJPE to examine the trade-offs between\nefficiency and performance. Our results suggest that the further pre-processed\ndata can successfully condition diffusion models to generate realistic social\nbehavior, with reasonable trade-offs in accuracy and processing time.",
        "We define a notion of categorical first order deformations for (enhanced)\ntriangulated categories. For a category $\\mathcal{T}$, we show that there is a\nbijection between $\\operatorname{HH}^2(\\mathcal{T})$ and the set of categorical\ndeformations of $\\mathcal{T}$. We show that in the case of curved deformations\nof dg algebras considered in arXiv:2406.04945, the $1$-derived category of the\ndeformation (introduced in arXiv:24020.8660) is a categorical deformation of\nthe derived category of the base; the Hochschild class identified by this\ndeformation is shown to restrict to the class defining the deformation of the\nalgebra. As an application, we give a conceptual proof of the fact that (for a\nsmooth base) the filtered derived category of a dg deformation yields a\ncategorical resolution of the classical derived category.",
        "In recent years, Neural Networks (NNs) have been employed to control\nnonlinear systems due to their potential capability in dealing with situations\nthat might be difficult for conventional nonlinear control schemes. However, to\nthe best of our knowledge, the current literature on NN-based control lacks\ntheoretical guarantees for stability and tracking performance. This precludes\nthe application of NN-based control schemes to systems where stringent\nstability and performance guarantees are required. To address this gap, this\npaper proposes a systematic and comprehensive methodology to design\nprovably-stable NN-based control schemes for affine nonlinear systems. Rigorous\nanalysis is provided to show that the proposed approach guarantees stability of\nthe closed-loop system with the NN in the loop. Also, it is shown that the\nresulting NN-based control scheme ensures that system states asymptotically\nconverge to a neighborhood around the desired equilibrium point, with a tunable\nproximity threshold. The proposed methodology is validated and evaluated via\nsimulation studies on an inverted pendulum and experimental studies on a Parrot\nBebop 2 drone.",
        "This paper introduces a novel strategy aimed at enhancing productivity and\nminimizing non-productive movements within container terminals, specifically\nfocusing on container yards. It advocates for the implementation of a digital\ntwin-based methodology to streamline the operations of stacking cranes (SCs)\nresponsible for container handling. The proposed approach entails the creation\nof a virtual container yard that mirrors the physical yard within a digital\ntwin system, facilitating real-time observation and validation. In addition,\nthis article demonstrates the effectiveness of using a digital twin to reduce\nunproductive movements and improve productivity through simulation. It defines\nvarious operational strategies and takes into account different yard contexts,\nproviding a comprehensive understanding of optimisation possibilities. By\nexploiting the capabilities of the digital twin, managers and operators are\nprovided with crucial information on operational dynamics, enabling them to\nidentify areas for improvement. This visualisation helps decision-makers to\nmake informed choices about their stacking strategies, thereby improving the\nefficiency of overall container terminal operations. Overall, this paper\npresent a digital twin solution in container terminal operations, offering a\npowerful tool for optimising productivity and minimising inefficiencies.",
        "Modern day applications have grown in size and require more computational\npower. The rise of machine learning and AI increased the need for parallel\ncomputation, which has increased the need for GPGPUs. With the increasing\ndemand for computational power, GPGPUs' SIMT architecture has solved this with\nan increase in the number of threads and the number of cores in a GPU,\nincreasing the throughput of these processors to match the demand of the\napplications. However, this created a larger demand for the memory, making the\nmemory bandwidth a bottleneck. The introduction of High-Bandwidth Memory (HBM)\nwith its increased number of memory ports offers a potential solution for the\nGPU to exploit its memory parallelism to increase the memory bandwidth.\nHowever, effectively leveraging HBM's memory parallelism to maximize bandwidth\npresents a unique and complex challenge for GPU architectures on how to\ndistribute those ports among the streaming multiprocessors in the GPGPU. In\nthis work, we extend the Vortex OpenGPU microarchitecture to incorporate a\nmultiport memory hierarchy, spanning from the L1 cache to the last-level cache\n(LLC). In addition, we propose various arbitration strategies to optimize\nmemory transfers across the cache hierarchy. The results have shown that an\nincrease in memory ports increases IPC, achieving an average speedup of 2.34x\nwith 8 memory ports in the tested configuration while showing relatively small\narea overhead.",
        "Open Vocabulary Video Anomaly Detection (OVVAD) seeks to detect and classify\nboth base and novel anomalies. However, existing methods face two specific\nchallenges related to novel anomalies. The first challenge is detection\nambiguity, where the model struggles to assign accurate anomaly scores to\nunfamiliar anomalies. The second challenge is categorization confusion, where\nnovel anomalies are often misclassified as visually similar base instances. To\naddress these challenges, we explore supplementary information from multiple\nsources to mitigate detection ambiguity by leveraging multiple levels of visual\ndata alongside matching textual information. Furthermore, we propose\nincorporating label relations to guide the encoding of new labels, thereby\nimproving alignment between novel videos and their corresponding labels, which\nhelps reduce categorization confusion. The resulting Anomize framework\neffectively tackles these issues, achieving superior performance on UCF-Crime\nand XD-Violence datasets, demonstrating its effectiveness in OVVAD.",
        "Automatic Music Transcription (AMT), aiming to get musical notes from raw\naudio, typically uses frame-level systems with piano-roll outputs or language\nmodel (LM)-based systems with note-level predictions. However, frame-level\nsystems require manual thresholding, while the LM-based systems struggle with\nlong sequences. In this paper, we propose a hybrid method combining pre-trained\nroll-based encoders with an LM decoder to leverage the strengths of both\nmethods. Besides, our approach employs a hierarchical prediction strategy,\nfirst predicting onset and pitch, then velocity, and finally offset. The\nhierarchical prediction strategy reduces computational costs by breaking down\nlong sequences into different hierarchies. Evaluated on two benchmark\nroll-based encoders, our method outperforms traditional piano-roll outputs 0.01\nand 0.022 in onset-offset-velocity F1 score, demonstrating its potential as a\nperformance-enhancing plug-in for arbitrary roll-based music transcription\nencoder.",
        "We show that hives chosen at random with independent GUE boundary conditions\non two sides, weighted by a Vandermonde factor depending on the third side\n(which is necessary in the context of the randomized Horn problem), when\nnormalized so that the eigenvalues at the edge are asymptotically constant,\nconverge in probability to a continuum hive as $n \\rightarrow \\infty.$ It had\npreviously been shown in joint work with Sheffield and Tao \\cite{NST} that the\nvariance of these scaled random hives tends to $0$ and consequently, from\ncompactness, that they converge in probability subsequentially. In the present\npaper, building on \\cite{NST}, we prove convergence in probability to a single\ncontinuum hive, without having to pass to a subsequence. We moreover show that\nthe value at a given point $v$ of this continuum hive equals the supremum of a\ncertain functional acting on asymptotic height functions of lozenge tilings.",
        "In standard genetic programming (stdGP), solutions are varied by modifying\ntheir syntax, with uncertain effects on their semantics. Geometric-semantic\ngenetic programming (GSGP), a popular variant of GP, effectively searches the\nsemantic solution space using variation operations based on linear\ncombinations, although it results in significantly larger solutions. This paper\npresents Transformer Semantic Genetic Programming (TSGP), a novel and flexible\nsemantic approach that uses a generative transformer model as search operator.\nThe transformer is trained on synthetic test problems and learns semantic\nsimilarities between solutions. Once the model is trained, it can be used to\ncreate offspring solutions with high semantic similarity also for unseen and\nunknown problems. Experiments on several symbolic regression problems show that\nTSGP generates solutions with comparable or even significantly better\nprediction quality than stdGP, SLIM_GSGP, DSR, and DAE-GP. Like SLIM_GSGP, TSGP\nis able to create new solutions that are semantically similar without creating\nsolutions of large size. An analysis of the search dynamic reveals that the\nsolutions generated by TSGP are semantically more similar than the solutions\ngenerated by the benchmark approaches allowing a better exploration of the\nsemantic solution space.",
        "Large Language Models (LLMs) can generate plausible test code. Intuitively\nthey generate this by imitating tests seen in their training data, rather than\nreasoning about execution semantics. However, such reasoning is important when\napplying mutation testing, where individual tests need to demonstrate\ndifferences in program behavior between a program and specific artificial\ndefects (mutants). In this paper, we evaluate whether Scientific Debugging,\nwhich has been shown to help LLMs when debugging, can also help them to\ngenerate tests for mutants. In the resulting approach, LLMs form hypotheses\nabout how to kill specific mutants, and then iteratively generate and refine\ntests until they succeed, all with detailed explanations for each step. We\ncompare this method to three baselines: (1) directly asking the LLM to generate\ntests, (2) repeatedly querying the LLM when tests fail, and (3) search-based\ntest generation with Pynguin. Our experiments evaluate these methods based on\nseveral factors, including mutation score, code coverage, success rate, and the\nability to identify equivalent mutants. The results demonstrate that LLMs,\nalthough requiring higher computation cost, consistently outperform Pynguin in\ngenerating tests with better fault detection and coverage. Importantly, we\nobserve that the iterative refinement of test cases is important for achieving\nhigh-quality test suites."
      ]
    }
  },
  {
    "id":2411.04682,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Distributed Sky Imaging Radiometry and Tomography",
    "start_abstract":"The composition of the atmosphere is significant to our ecosystem. Accordingly, there a need sense distributions atmospheric scatterers such as aerosols and cloud droplets. There growing interest in recovering these scattering fields three-dimensions (3D). Even so, current observations usually use expensive unscalable equipment. Moreover, analysis retrieves partial information (e.g., cloud-base altitudes, water droplet size at tops) based on simplified 1D models. To advance retrievals, we develop new computational imaging approach for sensing analyzing atmosphere, volumetrically. Our comprises ground-based network cameras. We deployed it conjunction with additional remote equipment, including Raman lidar sunphotometer, which provide initialization algorithms ground truth. camera scalable, low cost, enables 3D high spatial temporal resolution. describe how system calibrated absolute radiometric readouts light field. Consequently, recover volumetric field scatterers, using tomography. tomography process adapted relative prior art, run large-scale domains being in-situ within scatterer fields. empirically demonstrate feasibility clouds, data.",
    "start_categories":[
      "astro-ph.EP"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b8"
      ],
      "title":[
        "Variable Imaging Projection Cloud Scattering Tomography"
      ],
      "abstract":[
        "Scattering-based computed tomography (CT) recovers a heterogeneous volumetric scattering medium using images taken from multiple directions. It is nonlinear problem. Prior art mainly approached it by explicit physics-based optimization of image-fitting, being slow and difficult to scale. Scale particularly important when the objects constitute large cloud fields, where recovery for climate studies. Besides speed, imaging need be flexible, efficiently handle variable viewing geometries resolutions. These can caused perturbation in camera poses or fusion data different types observational sensors. There fast projection clouds (VIP-CT). We develop learning-based solution, deep-neural network (DNN) which trains on labeled dataset. The DNN parameters are oblivious domain scale, hence work with arbitrarily domains. VIP-CT offers much better quality than state art. inference speed flexibility make effectively real-time context spaceborne observations. paper first demonstrate CT real empirical directly DNN. may offer model solution problems other scientific Our code available online."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "Perception-as-Control: Fine-grained Controllable Image Animation with\n  3D-aware Motion Representation",
        "A Light Perspective for 3D Object Detection",
        "Surgical Scene Understanding in the Era of Foundation AI Models: A\n  Comprehensive Review",
        "PolarFree: Polarization-based Reflection-free Imaging",
        "Shazam: Unifying Multiple Foundation Models for Advanced Computational\n  Pathology",
        "Emergence of Painting Ability via Recognition-Driven Evolution",
        "RGB-Only Gaussian Splatting SLAM for Unbounded Outdoor Scenes",
        "Feature-based One-For-All: A Universal Framework for Heterogeneous\n  Knowledge Distillation",
        "myEye2Wheeler: A Two-Wheeler Indian Driver Real-World Eye-Tracking\n  Dataset",
        "D2GV: Deformable 2D Gaussian Splatting for Video Representation in\n  400FPS",
        "High Resolution Tree Height Mapping of the Amazon Forest using Planet\n  NICFI Images and LiDAR-Informed U-Net Model",
        "ActiveInitSplat: How Active Image Selection Helps Gaussian Splatting",
        "Hybrid State-Space and GRU-based Graph Tokenization Mamba for\n  Hyperspectral Image Classification",
        "Codes with symmetric distances",
        "RobotIQ: Empowering Mobile Robots with Human-Level Planning for\n  Real-World Execution",
        "UniDemoir\\'e: Towards Universal Image Demoir\\'eing with Data Generation\n  and Synthesis",
        "How Vital is the Jurisprudential Relevance: Law Article Intervened Legal\n  Case Retrieval and Matching",
        "GASP: Unifying Geometric and Semantic Self-Supervised Pre-training for\n  Autonomous Driving",
        "Enhanced Derivative-Free Optimization Using Adaptive Correlation-Induced\n  Finite Difference Estimators",
        "Advanced 3D-Printed Multiphasic Scaffold with Optimal PRP Dosage for\n  Chondrogenesis of BM-MSCs in Osteochondral Tissue Engineering",
        "Recovering Partially Corrupted Major Objects through Tri-modality Based\n  Image Completion",
        "Designing VR Simulation System for Clinical Communication Training with\n  LLMs-Based Embodied Conversational Agents",
        "On the asymptotic validity of confidence sets for linear functionals of\n  solutions to integral equations",
        "On the approaching geodesics property",
        "Multilevel Generative Samplers for Investigating Critical Phenomena",
        "Empowering LLMs with Logical Reasoning: A Comprehensive Survey",
        "Multi-View Depth Consistent Image Generation Using Generative AI Models:\n  Application on Architectural Design of University Buildings",
        "Universal Chern classes on the moduli of bundles"
      ],
      "abstract":[
        "Motion-controllable image animation is a fundamental task with a wide range\nof potential applications. Recent works have made progress in controlling\ncamera or object motion via various motion representations, while they still\nstruggle to support collaborative camera and object motion control with\nadaptive control granularity. To this end, we introduce 3D-aware motion\nrepresentation and propose an image animation framework, called\nPerception-as-Control, to achieve fine-grained collaborative motion control.\nSpecifically, we construct 3D-aware motion representation from a reference\nimage, manipulate it based on interpreted user instructions, and perceive it\nfrom different viewpoints. In this way, camera and object motions are\ntransformed into intuitive and consistent visual changes. Then, our framework\nleverages the perception results as motion control signals, enabling it to\nsupport various motion-related video synthesis tasks in a unified and flexible\nway. Experiments demonstrate the superiority of the proposed approach. For more\ndetails and qualitative results, please refer to our anonymous project webpage:\nhttps:\/\/chen-yingjie.github.io\/projects\/Perception-as-Control.",
        "Comprehending the environment and accurately detecting objects in 3D space\nare essential for advancing autonomous vehicle technologies. Integrating Camera\nand LIDAR data has emerged as an effective approach for achieving high accuracy\nin 3D Object Detection models. However, existing methodologies often rely on\nheavy, traditional backbones that are computationally demanding. This paper\nintroduces a novel approach that incorporates cutting-edge Deep Learning\ntechniques into the feature extraction process, aiming to create more efficient\nmodels without compromising performance. Our model, NextBEV, surpasses\nestablished feature extractors like ResNet50 and MobileNetV2. On the KITTI 3D\nMonocular detection benchmark, NextBEV achieves an accuracy improvement of\n2.39%, having less than 10% of the MobileNetV3 parameters. Moreover, we propose\nchanges in LIDAR backbones that decreased the original inference time to 10 ms.\nAdditionally, by fusing these lightweight proposals, we have enhanced the\naccuracy of the VoxelNet-based model by 2.93% and improved the F1-score of the\nPointPillar-based model by approximately 20%. Therefore, this work contributes\nto establishing lightweight and powerful models for individual or fusion\ntechniques, making them more suitable for onboard implementations.",
        "Recent advancements in machine learning (ML) and deep learning (DL),\nparticularly through the introduction of foundational models (FMs), have\nsignificantly enhanced surgical scene understanding within minimally invasive\nsurgery (MIS). This paper surveys the integration of state-of-the-art ML and DL\ntechnologies, including Convolutional Neural Networks (CNNs), Vision\nTransformers (ViTs), and foundational models like the Segment Anything Model\n(SAM), into surgical workflows. These technologies improve segmentation\naccuracy, instrument tracking, and phase recognition in surgical endoscopic\nvideo analysis. The paper explores the challenges these technologies face, such\nas data variability and computational demands, and discusses ethical\nconsiderations and integration hurdles in clinical settings. Highlighting the\nroles of FMs, we bridge the technological capabilities with clinical needs and\noutline future research directions to enhance the adaptability, efficiency, and\nethical alignment of AI applications in surgery. Our findings suggest that\nsubstantial progress has been made; however, more focused efforts are required\nto achieve seamless integration of these technologies into clinical workflows,\nensuring they complement surgical practice by enhancing precision, reducing\nrisks, and optimizing patient outcomes.",
        "Reflection removal is challenging due to complex light interactions, where\nreflections obscure important details and hinder scene understanding.\nPolarization naturally provides a powerful cue to distinguish between reflected\nand transmitted light, enabling more accurate reflection removal. However,\nexisting methods often rely on small-scale or synthetic datasets, which fail to\ncapture the diversity and complexity of real-world scenarios. To this end, we\nconstruct a large-scale dataset, PolaRGB, for Polarization-based reflection\nremoval of RGB images, which enables us to train models that generalize\neffectively across a wide range of real-world scenarios. The PolaRGB dataset\ncontains 6,500 well-aligned mixed-transmission image pairs, 8x larger than\nexisting polarization datasets, and is the first to include both RGB and\npolarization images captured across diverse indoor and outdoor environments\nwith varying lighting conditions. Besides, to fully exploit the potential of\npolarization cues for reflection removal, we introduce PolarFree, which\nleverages diffusion process to generate reflection-free cues for accurate\nreflection removal. Extensive experiments show that PolarFree significantly\nenhances image clarity in challenging reflective scenarios, setting a new\nbenchmark for polarized imaging and reflection removal. Code and dataset are\navailable at https:\/\/github.com\/mdyao\/PolarFree.",
        "Foundation Models (FMs) in computational pathology (CPath) have significantly\nadvanced the extraction of meaningful features from histopathology image\ndatasets, achieving strong performance across various clinical tasks. Despite\ntheir impressive performance, these models often exhibit variability when\napplied to different tasks, prompting the need for a unified framework capable\nof consistently excelling across various applications. In this work, we propose\nShazam, a novel framework designed to efficiently combine multiple CPath\nmodels. Unlike previous approaches that train a fixed-parameter FM, Shazam\ndynamically extracts and refines information from diverse FMs for each specific\ntask. To ensure that each FM contributes effectively without dominance, a novel\ndistillation strategy is applied, guiding the student model with features from\nall teacher models, which enhances its generalization ability. Experimental\nresults on two pathology patch classification datasets demonstrate that Shazam\noutperforms existing CPath models and other fusion methods. Its lightweight,\nflexible design makes it a promising solution for improving CPath analysis in\nreal-world settings. Code will be available at\nhttps:\/\/github.com\/Tuner12\/Shazam.",
        "From Paleolithic cave paintings to Impressionism, human painting has evolved\nto depict increasingly complex and detailed scenes, conveying more nuanced\nmessages. This paper attempts to emerge this artistic capability by simulating\nthe evolutionary pressures that enhance visual communication efficiency.\nSpecifically, we present a model with a stroke branch and a palette branch that\ntogether simulate human-like painting. The palette branch learns a limited\ncolour palette, while the stroke branch parameterises each stroke using\nB\\'ezier curves to render an image, subsequently evaluated by a high-level\nrecognition module. We quantify the efficiency of visual communication by\nmeasuring the recognition accuracy achieved with machine vision. The model then\noptimises the control points and colour choices for each stroke to maximise\nrecognition accuracy with minimal strokes and colours. Experimental results\nshow that our model achieves superior performance in high-level recognition\ntasks, delivering artistic expression and aesthetic appeal, especially in\nabstract sketches. Additionally, our approach shows promise as an efficient\nbit-level image compression technique, outperforming traditional methods.",
        "3D Gaussian Splatting (3DGS) has become a popular solution in SLAM, as it can\nproduce high-fidelity novel views. However, previous GS-based methods primarily\ntarget indoor scenes and rely on RGB-D sensors or pre-trained depth estimation\nmodels, hence underperforming in outdoor scenarios. To address this issue, we\npropose a RGB-only gaussian splatting SLAM method for unbounded outdoor\nscenes--OpenGS-SLAM. Technically, we first employ a pointmap regression network\nto generate consistent pointmaps between frames for pose estimation. Compared\nto commonly used depth maps, pointmaps include spatial relationships and scene\ngeometry across multiple views, enabling robust camera pose estimation. Then,\nwe propose integrating the estimated camera poses with 3DGS rendering as an\nend-to-end differentiable pipeline. Our method achieves simultaneous\noptimization of camera poses and 3DGS scene parameters, significantly enhancing\nsystem tracking accuracy. Specifically, we also design an adaptive scale mapper\nfor the pointmap regression network, which provides more accurate pointmap\nmapping to the 3DGS map representation. Our experiments on the Waymo dataset\ndemonstrate that OpenGS-SLAM reduces tracking error to 9.8\\% of previous 3DGS\nmethods, and achieves state-of-the-art results in novel view synthesis. Project\nPage: https:\/\/3dagentworld.github.io\/opengs-slam\/",
        "Knowledge distillation (KD) involves transferring knowledge from a\npre-trained heavy teacher model to a lighter student model, thereby reducing\nthe inference cost while maintaining comparable effectiveness. Prior KD\ntechniques typically assume homogeneity between the teacher and student models.\nHowever, as technology advances, a wide variety of architectures have emerged,\nranging from initial Convolutional Neural Networks (CNNs) to Vision\nTransformers (ViTs), and Multi-Level Perceptrons (MLPs). Consequently,\ndeveloping a universal KD framework compatible with any architecture has become\nan important research topic. In this paper, we introduce a feature-based\none-for-all (FOFA) KD framework to enable feature distillation across diverse\narchitecture. Our framework comprises two key components. First, we design\nprompt tuning blocks that incorporate student feedback, allowing teacher\nfeatures to adapt to the student model's learning process. Second, we propose\nregion-aware attention to mitigate the view mismatch problem between\nheterogeneous architecture. By leveraging these two modules, effective\ndistillation of intermediate features can be achieved across heterogeneous\narchitectures. Extensive experiments on CIFAR, ImageNet, and COCO demonstrate\nthe superiority of the proposed method.",
        "This paper presents the myEye2Wheeler dataset, a unique resource of\nreal-world gaze behaviour of two-wheeler drivers navigating complex Indian\ntraffic. Most datasets are from four-wheeler drivers on well-planned roads and\nhomogeneous traffic. Our dataset offers a critical lens into the unique visual\nattention patterns and insights into the decision-making of Indian two-wheeler\ndrivers. The analysis demonstrates that existing saliency models, like\nTASED-Net, perform less effectively on the myEye-2Wheeler dataset compared to\nwhen applied on the European 4-wheeler eye tracking datasets (DR(Eye)VE),\nhighlighting the need for models specifically tailored to the traffic\nconditions. By introducing the dataset, we not only fill a significant gap in\ntwo-wheeler driver behaviour research in India but also emphasise the critical\nneed for developing context-specific saliency models. The larger aim is to\nimprove road safety for two-wheeler users and lane-planning to support a\ncost-effective mode of transport.",
        "Implicit Neural Representations (INRs) have emerged as a powerful approach\nfor video representation, offering versatility across tasks such as compression\nand inpainting. However, their implicit formulation limits both\ninterpretability and efficacy, undermining their practicality as a\ncomprehensive solution. We propose a novel video representation based on\ndeformable 2D Gaussian splatting, dubbed D2GV, which aims to achieve three key\nobjectives: 1) improved efficiency while delivering superior quality; 2)\nenhanced scalability and interpretability; and 3) increased friendliness for\ndownstream tasks. Specifically, we initially divide the video sequence into\nfixed-length Groups of Pictures (GoP) to allow parallel training and linear\nscalability with video length. For each GoP, D2GV represents video frames by\napplying differentiable rasterization to 2D Gaussians, which are deformed from\na canonical space into their corresponding timestamps. Notably, leveraging\nefficient CUDA-based rasterization, D2GV converges fast and decodes at speeds\nexceeding 400 FPS, while delivering quality that matches or surpasses\nstate-of-the-art INRs. Moreover, we incorporate a learnable pruning and\nquantization strategy to streamline D2GV into a more compact representation. We\ndemonstrate D2GV's versatility in tasks including video interpolation,\ninpainting and denoising, underscoring its potential as a promising solution\nfor video representation. Code is available at:\nhttps:\/\/github.com\/Evan-sudo\/D2GV.",
        "Tree canopy height is one of the most important indicators of forest biomass,\nproductivity, and ecosystem structure, but it is challenging to measure\naccurately from the ground and from space. Here, we used a U-Net model adapted\nfor regression to map the mean tree canopy height in the Amazon forest from\nPlanet NICFI images at ~4.78 m spatial resolution for the period 2020-2024. The\nU-Net model was trained using canopy height models computed from aerial LiDAR\ndata as a reference, along with their corresponding Planet NICFI images.\nPredictions of tree heights on the validation sample exhibited a mean error of\n3.68 m and showed relatively low systematic bias across the entire range of\ntree heights present in the Amazon forest. Our model successfully estimated\ncanopy heights up to 40-50 m without much saturation, outperforming existing\ncanopy height products from global models in this region. We determined that\nthe Amazon forest has an average canopy height of ~22 m. Events such as logging\nor deforestation could be detected from changes in tree height, and encouraging\nresults were obtained to monitor the height of regenerating forests. These\nfindings demonstrate the potential for large-scale mapping and monitoring of\ntree height for old and regenerating Amazon forests using Planet NICFI imagery.",
        "Gaussian splatting (GS) along with its extensions and variants provides\noutstanding performance in real-time scene rendering while meeting reduced\nstorage demands and computational efficiency. While the selection of 2D images\ncapturing the scene of interest is crucial for the proper initialization and\ntraining of GS, hence markedly affecting the rendering performance, prior works\nrely on passively and typically densely selected 2D images. In contrast, this\npaper proposes `ActiveInitSplat', a novel framework for active selection of\ntraining images for proper initialization and training of GS. ActiveInitSplat\nrelies on density and occupancy criteria of the resultant 3D scene\nrepresentation from the selected 2D images, to ensure that the latter are\ncaptured from diverse viewpoints leading to better scene coverage and that the\ninitialized Gaussian functions are well aligned with the actual 3D structure.\nNumerical tests on well-known simulated and real environments demonstrate the\nmerits of ActiveInitSplat resulting in significant GS rendering performance\nimprovement over passive GS baselines, in the widely adopted LPIPS, SSIM, and\nPSNR metrics.",
        "Hyperspectral image (HSI) classification plays a pivotal role in domains such\nas environmental monitoring, agriculture, and urban planning. However, it faces\nsignificant challenges due to the high-dimensional nature of the data and the\ncomplex spectral-spatial relationships inherent in HSI. Traditional methods,\nincluding conventional machine learning and convolutional neural networks\n(CNNs), often struggle to effectively capture these intricate spectral-spatial\nfeatures and global contextual information. Transformer-based models, while\npowerful in capturing long-range dependencies, often demand substantial\ncomputational resources, posing challenges in scenarios where labeled datasets\nare limited, as is commonly seen in HSI applications. To overcome these\nchallenges, this work proposes GraphMamba, a hybrid model that combines\nspectral-spatial token generation, graph-based token prioritization, and\ncross-attention mechanisms. The model introduces a novel hybridization of\nstate-space modeling and Gated Recurrent Units (GRU), capturing both linear and\nnonlinear spatial-spectral dynamics. GraphMamba enhances the ability to model\ncomplex spatial-spectral relationships while maintaining scalability and\ncomputational efficiency across diverse HSI datasets. Through comprehensive\nexperiments, we demonstrate that GraphMamba outperforms existing\nstate-of-the-art models, offering a scalable and robust solution for complex\nHSI classification tasks.",
        "For a code $C$ in a space with maximal distance $n$, we say that $C$ has\nsymmetric distances if its distance set $S(C)$ is symmetric with respect to $n\n\/ 2$. In this paper, we prove that if $C$ is a binary code with length $2n$,\nconstant weight $n$ and symmetric distances, then \\[\n  |C| \\leq \\binom{2 n - 1}{|S(C)|}. \\] This result can be interpreted using the\nlanguage of Johnson association schemes. More generally, we give a framework to\nstudy codes with symmetric distances in Q-bipartite Q-polynomial association\nschemes, and provide upper bounds for such codes. Moreover, we use number\ntheoretic techniques to determine when the equality holds.",
        "This paper introduces RobotIQ, a framework that empowers mobile robots with\nhuman-level planning capabilities, enabling seamless communication via natural\nlanguage instructions through any Large Language Model. The proposed framework\nis designed in the ROS architecture and aims to bridge the gap between humans\nand robots, enabling robots to comprehend and execute user-expressed text or\nvoice commands. Our research encompasses a wide spectrum of robotic tasks,\nranging from fundamental logical, mathematical, and learning reasoning for\ntransferring knowledge in domains like navigation, manipulation, and object\nlocalization, enabling the application of learned behaviors from simulated\nenvironments to real-world operations. All encapsulated within a modular\ncrafted robot library suite of API-wise control functions, RobotIQ offers a\nfully functional AI-ROS-based toolset that allows researchers to design and\ndevelop their own robotic actions tailored to specific applications and robot\nconfigurations. The effectiveness of the proposed system was tested and\nvalidated both in simulated and real-world experiments focusing on a home\nservice scenario that included an assistive application designed for elderly\npeople. RobotIQ with an open-source, easy-to-use, and adaptable robotic library\nsuite for any robot can be found at https:\/\/github.com\/emmarapt\/RobotIQ.",
        "Image demoir\\'eing poses one of the most formidable challenges in image\nrestoration, primarily due to the unpredictable and anisotropic nature of\nmoir\\'e patterns. Limited by the quantity and diversity of training data,\ncurrent methods tend to overfit to a single moir\\'e domain, resulting in\nperformance degradation for new domains and restricting their robustness in\nreal-world applications. In this paper, we propose a universal image\ndemoir\\'eing solution, UniDemoir\\'e, which has superior generalization\ncapability. Notably, we propose innovative and effective data generation and\nsynthesis methods that can automatically provide vast high-quality moir\\'e\nimages to train a universal demoir\\'eing model. Our extensive experiments\ndemonstrate the cutting-edge performance and broad potential of our approach\nfor generalized image demoir\\'eing.",
        "Legal case retrieval (LCR) aims to automatically scour for comparable legal\ncases based on a given query, which is crucial for offering relevant precedents\nto support the judgment in intelligent legal systems. Due to similar goals, it\nis often associated with a similar case matching (LCM) task. To address them, a\ndaunting challenge is assessing the uniquely defined legal-rational similarity\nwithin the judicial domain, which distinctly deviates from the semantic\nsimilarities in general text retrieval. Past works either tagged\ndomain-specific factors or incorporated reference laws to capture\nlegal-rational information. However, their heavy reliance on expert or\nunrealistic assumptions restricts their practical applicability in real-world\nscenarios. In this paper, we propose an end-to-end model named LCM-LAI to solve\nthe above challenges. Through meticulous theoretical analysis, LCM-LAI employs\na dependent multi-task learning framework to capture legal-rational information\nwithin legal cases by a law article prediction (LAP) sub-task, without any\nadditional assumptions in inference. Besides, LCM-LAI proposes an article-aware\nattention mechanism to evaluate the legal-rational similarity between\nacross-case sentences based on law distribution, which is more effective than\nconventional semantic similarity. Weperform a series of exhaustive experiments\nincluding two different tasks involving four real-world datasets. Results\ndemonstrate that LCM-LAI achieves state-of-the-art performance.",
        "Self-supervised pre-training based on next-token prediction has enabled large\nlanguage models to capture the underlying structure of text, and has led to\nunprecedented performance on a large array of tasks when applied at scale.\nSimilarly, autonomous driving generates vast amounts of spatiotemporal data,\nalluding to the possibility of harnessing scale to learn the underlying\ngeometric and semantic structure of the environment and its evolution over\ntime. In this direction, we propose a geometric and semantic self-supervised\npre-training method, GASP, that learns a unified representation by predicting,\nat any queried future point in spacetime, (1) general occupancy, capturing the\nevolving structure of the 3D scene; (2) ego occupancy, modeling the ego vehicle\npath through the environment; and (3) distilled high-level features from a\nvision foundation model. By modeling geometric and semantic 4D occupancy fields\ninstead of raw sensor measurements, the model learns a structured,\ngeneralizable representation of the environment and its evolution through time.\nWe validate GASP on multiple autonomous driving benchmarks, demonstrating\nsignificant improvements in semantic occupancy forecasting, online mapping, and\nego trajectory prediction. Our results demonstrate that continuous 4D geometric\nand semantic occupancy prediction provides a scalable and effective\npre-training paradigm for autonomous driving. For code and additional\nvisualizations, see \\href{https:\/\/research.zenseact.com\/publications\/gasp\/.",
        "Gradient-based methods are well-suited for derivative-free optimization\n(DFO), where finite-difference (FD) estimates are commonly used as gradient\nsurrogates. Traditional stochastic approximation methods, such as\nKiefer-Wolfowitz (KW) and simultaneous perturbation stochastic approximation\n(SPSA), typically utilize only two samples per iteration, resulting in\nimprecise gradient estimates and necessitating diminishing step sizes for\nconvergence. In this paper, we first explore an efficient FD estimate, referred\nto as correlation-induced FD estimate, which is a batch-based estimate. Then,\nwe propose an adaptive sampling strategy that dynamically determines the batch\nsize at each iteration. By combining these two components, we develop an\nalgorithm designed to enhance DFO in terms of both gradient estimation\nefficiency and sample efficiency. Furthermore, we establish the consistency of\nour proposed algorithm and demonstrate that, despite using a batch of samples\nper iteration, it achieves the same convergence rate as the KW and SPSA\nmethods. Additionally, we propose a novel stochastic line search technique to\nadaptively tune the step size in practice. Finally, comprehensive numerical\nexperiments confirm the superior empirical performance of the proposed\nalgorithm.",
        "In osteochondral tissue engineering (OCTE), simultaneously regenerating\nsubchondral bone and cartilage tissue presents a significant challenge.\nMultiphasic scaffolds were created and manufactured using 3D printing to\naddress this issue. Excellent interfacial mechanical properties and\nbiocompatibility enhance the growth and chondrogenic differentiation of bone\nmarrow mesenchymal stem cells (BM-MSCs). The subchondral bone bottom layer is\nmimicked by incorporating varying concentrations of graphene oxide (GO) (0%,\n1%, and 2% w\/v) into a bioink composed of alginate (Alg) and gelatin (Gel).\nBased on evaluations of mechanical and biocompatibility properties, 1% GO is\nselected for further studies. Subsequently, the GO concentration is kept\nconstant while varying the platelet-rich plasma (PRP) dosage in the multiphasic\nscaffolds. Different PRP dosages (0%, 1%, 2%, and 3% w\/v) are integrated into\nthe Alg-Gel bioink to simulate cartilage tissues. Results indicate that\n3D-printed scaffolds containing 1% or 2% PRP exhibit favorable biomechanical\nproperties, with no significant differences observed. However, BM-MSCs exposed\nto 2% PRP demonstrate enhanced adhesion, growth, and viability. Additionally,\nreal-time PCR and Alcian blue staining confirm increased chondrogenic\nexpression and glycosaminoglycans (GAGs) synthesis. This work highlights the\npromising potential of 3D-printed multiphasic frameworks in the development of\nOCTE.",
        "Diffusion models have become widely adopted in image completion tasks, with\ntext prompts commonly employed to ensure semantic coherence by providing\nhigh-level guidance. However, a persistent challenge arises when an object is\npartially obscured in the damaged region, yet its remaining parts are still\nvisible in the background. While text prompts offer semantic direction, they\noften fail to precisely recover fine-grained structural details, such as the\nobject's overall posture, ensuring alignment with the visible object\ninformation in the background. This limitation stems from the inability of text\nprompts to provide pixel-level specificity. To address this, we propose\nsupplementing text-based guidance with a novel visual aid: a casual sketch,\nwhich can be roughly drawn by anyone based on visible object parts. This sketch\nsupplies critical structural cues, enabling the generative model to produce an\nobject structure that seamlessly integrates with the existing background. We\nintroduce the Visual Sketch Self-Aware (VSSA) model, which integrates the\ncasual sketch into each iterative step of the diffusion process, offering\ndistinct advantages for partially corrupted scenarios. By blending\nsketch-derived features with those of the corrupted image, and leveraging text\nprompt guidance, the VSSA assists the diffusion model in generating images that\npreserve both the intended object semantics and structural consistency across\nthe restored objects and original regions. To support this research, we created\ntwo datasets, CUB-sketch and MSCOCO-sketch, each combining images, sketches,\nand text. Extensive qualitative and quantitative experiments demonstrate that\nour approach outperforms several state-of-the-art methods.",
        "VR simulation in Health Professions (HP) education demonstrates huge\npotential, but fixed learning content with little customization limits its\napplication beyond lab environments. To address these limitations in the\ncontext of VR for patient communication training, we conducted a user-centered\nstudy involving semi-structured interviews with advanced HP students to\nunderstand their challenges in clinical communication training and perceptions\nof VR-based solutions. From this, we derived design insights emphasizing the\nimportance of realistic scenarios, simple interactions, and unpredictable\ndialogues. Building on these insights, we developed the Virtual AI Patient\nSimulator (VAPS), a novel VR system powered by Large Language Models (LLMs) and\nEmbodied Conversational Agents (ECAs), supporting dynamic and customizable\npatient interactions for immersive learning. We also provided an example of how\nclinical professors could use user-friendly design forms to create personalized\nscenarios that align with course objectives in VAPS and discuss future\nimplications of integrating AI-driven technologies into VR education.",
        "This paper examines the construction of confidence sets for parameters\ndefined as a linear functional of the solution to an integral equation\ninvolving conditional expectations. We show that any confidence set uniformly\nvalid over a broad class of probability laws, allowing the integral equation to\nbe arbitrarily ill-posed must have, with high probability under some laws, a\ndiameter at least as large as the diameter of the parameter's range over the\nmodel. Additionally, we establish that uniformly consistent estimators of the\nparameter do not exist. We show that, consistent with the weak instruments\nliterature, Wald confidence intervals are not uniformly valid. Furthermore, we\nargue that inverting the score test, a successful approach in that literature,\ndoes not extend to the broader class of parameters considered here. We present\na method for constructing uniformly valid confidence sets in the special case\nwhere all variables are binary and discuss its limitations. Finally, we\nemphasize that developing uniformly valid confidence sets for the general class\nof parameters considered in this paper remains an open problem.",
        "We survey some recent results and open questions on the approaching geodesics\nproperty and its application to the study of the Gromov and horofunction\ncompactifications of a proper geodesic Gromov metric space. We obtain results\non the dynamics of isometries and we exhibit an example of a Gromov hyperbolic\ndomain of $\\mathbb{C}$ which does not satisfy the approaching geodesic\nproperty.",
        "Investigating critical phenomena or phase transitions is of high interest in\nphysics and chemistry, for which Monte Carlo (MC) simulations, a crucial tool\nfor numerically analyzing macroscopic properties of given systems, are often\nhindered by an emerging divergence of correlation length -- known as scale\ninvariance at criticality (SIC) in the renormalization group theory. SIC causes\nthe system to behave the same at any length scale, from which many existing\nsampling methods suffer: long-range correlations cause critical slowing down in\nMarkov chain Monte Carlo (MCMC), and require intractably large receptive fields\nfor generative samplers. In this paper, we propose a Renormalization-informed\nGenerative Critical Sampler (RiGCS) -- a novel sampler specialized for\nnear-critical systems, where SIC is leveraged as an advantage rather than a\nnuisance. Specifically, RiGCS builds on MultiLevel Monte Carlo (MLMC) with Heat\nBath (HB) algorithms, which perform ancestral sampling from low-resolution to\nhigh-resolution lattice configurations with site-wise-independent conditional\nHB sampling. Although MLMC-HB is highly efficient under exact SIC, it suffers\nfrom a low acceptance rate under slight SIC violation. Notably, SIC violation\nalways occurs in finite-size systems, and may induce long-range and\nhigher-order interactions in the renormalized distributions, which are not\nconsidered by independent HB samplers. RiGCS enhances MLMC-HB by replacing a\npart of the conditional HB sampler with generative models that capture those\nresidual interactions and improve the sampling efficiency. Our experiments show\nthat the effective sample size of RiGCS is a few orders of magnitude higher\nthan state-of-the-art generative model baselines in sampling configurations for\n128x128 two-dimensional Ising systems.",
        "Large language models (LLMs) have achieved remarkable successes on various\nnatural language tasks. However, recent studies have found that there are still\nsignificant challenges to the logical reasoning abilities of LLMs. This paper\nsummarizes and categorizes the main challenges into two aspects: (1) Logical\nquestion answering, LLMs often fail to generate the correct answer within\ncomplex logical problem which requires sophisticated deductive, inductive or\nabductive reasoning given a collection of premises and constrains. (2) Logical\nconsistency, LLMs are prone to producing responses contradicting themselves\nacross different questions. For example, a state-of-the-art Macaw\nquestion-answering LLM answers Yes to both questions Is a magpie a bird? and\nDoes a bird have wings? but answers No to Does a magpie have wings?. To\nfacilitate this research direction, we comprehensively investigate the most\ncutting-edge methods and propose detailed taxonomies of these methods.\nSpecifically, to accurately answer complex logic questions, previous methods\ncan be categorized based on reliance on external solvers, prompts, pretraining,\nand fine-tuning. To avoid logical contradictions, we discuss concepts and\nsolutions of various logical consistencies, including implication, negation,\ntransitivity, factuality consistency, and their composites. In addition, we\nreview commonly used benchmark datasets and evaluation metrics, and discuss\npromising research directions, such as extensions to modal logic to account for\nuncertainty, and efficient algorithms satisfying multiple logical consistencies\nsimultaneously.",
        "In the early stages of architectural design, shoebox models are typically\nused as a simplified representation of building structures but require\nextensive operations to transform them into detailed designs. Generative\nartificial intelligence (AI) provides a promising solution to automate this\ntransformation, but ensuring multi-view consistency remains a significant\nchallenge. To solve this issue, we propose a novel three-stage consistent image\ngeneration framework using generative AI models to generate architectural\ndesigns from shoebox model representations. The proposed method enhances\nstate-of-the-art image generation diffusion models to generate multi-view\nconsistent architectural images. We employ ControlNet as the backbone and\noptimize it to accommodate multi-view inputs of architectural shoebox models\ncaptured from predefined perspectives. To ensure stylistic and structural\nconsistency across multi-view images, we propose an image space loss module\nthat incorporates style loss, structural loss and angle alignment loss. We then\nuse depth estimation method to extract depth maps from the generated multi-view\nimages. Finally, we use the paired data of the architectural images and depth\nmaps as inputs to improve the multi-view consistency via the depth-aware 3D\nattention module. Experimental results demonstrate that the proposed framework\ncan generate multi-view architectural images with consistent style and\nstructural coherence from shoebox model inputs.",
        "The goal of this paper is to construct universal cohomology classes on the\nmoduli space of stable bundles over a curve when it is not a fine moduli space,\ni.e. when the rank and degree are not coprime. More precisely, we show that\ncertain Chern classes of the universal bundle on the product of the curve with\nthe moduli stack of bundles lift to the product of the curve with the moduli\nspace of stable bundles."
      ]
    }
  },
  {
    "id":2411.04682,
    "research_type":"applied",
    "start_id":"b8",
    "start_title":"Variable Imaging Projection Cloud Scattering Tomography",
    "start_abstract":"Scattering-based computed tomography (CT) recovers a heterogeneous volumetric scattering medium using images taken from multiple directions. It is nonlinear problem. Prior art mainly approached it by explicit physics-based optimization of image-fitting, being slow and difficult to scale. Scale particularly important when the objects constitute large cloud fields, where recovery for climate studies. Besides speed, imaging need be flexible, efficiently handle variable viewing geometries resolutions. These can caused perturbation in camera poses or fusion data different types observational sensors. There fast projection clouds (VIP-CT). We develop learning-based solution, deep-neural network (DNN) which trains on labeled dataset. The DNN parameters are oblivious domain scale, hence work with arbitrarily domains. VIP-CT offers much better quality than state art. inference speed flexibility make effectively real-time context spaceborne observations. paper first demonstrate CT real empirical directly DNN. may offer model solution problems other scientific Our code available online.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Distributed Sky Imaging Radiometry and Tomography"
      ],
      "abstract":[
        "The composition of the atmosphere is significant to our ecosystem. Accordingly, there a need sense distributions atmospheric scatterers such as aerosols and cloud droplets. There growing interest in recovering these scattering fields three-dimensions (3D). Even so, current observations usually use expensive unscalable equipment. Moreover, analysis retrieves partial information (e.g., cloud-base altitudes, water droplet size at tops) based on simplified 1D models. To advance retrievals, we develop new computational imaging approach for sensing analyzing atmosphere, volumetrically. Our comprises ground-based network cameras. We deployed it conjunction with additional remote equipment, including Raman lidar sunphotometer, which provide initialization algorithms ground truth. camera scalable, low cost, enables 3D high spatial temporal resolution. describe how system calibrated absolute radiometric readouts light field. Consequently, recover volumetric field scatterers, using tomography. tomography process adapted relative prior art, run large-scale domains being in-situ within scatterer fields. empirically demonstrate feasibility clouds, data."
      ],
      "categories":[
        "astro-ph.EP"
      ]
    },
    "list":{
      "title":[
        "Prediction and observation of a stellar occultation by Haumea's\n  satellite Namaka",
        "Polar circumtriple planets and disks around misaligned hierarchical\n  triple stars",
        "Smuggling unnoticed: Towards a 2D view of water and dust delivery to the\n  inner regions of protoplanetary discs",
        "The Interplay between Dust Dynamics and Turbulence Induced by the\n  Vertical Shear Instability",
        "Three-dimensional transport of solids in a protoplanetary disk\n  containing a growing giant planet",
        "On the road to the radius valley: distinguishing between gas dwarfs and\n  water worlds with young transiting exoplanets",
        "The coexistence of the streaming instability and the vertical shear\n  instability in protoplanetary disks: Scale-dependence of dust diffusion",
        "An Energy-Angular Momentum Phase Function for Rubble Pile Asteroids",
        "Activity of comet 7P\/Pons-Winnecke during the 2021 apparition",
        "Revisiting the multi-planetary system of the nearby star HD 20794:\n  Confirmation of a low-mass planet in the habitable zone of a nearby G-dwarf",
        "Precise Parameters from Bayesian SED Fitting Indicate Thermally-Driven\n  Mass Loss Likely Driver of Radius Valley",
        "TOI-6324b: An Earth-Mass Ultra-Short-Period Planet Transiting a Nearby M\n  Dwarf",
        "Stellar occultation observations of (38628) Huya and its satellite: a\n  detailed look into the system",
        "Signless Laplacian State Transfer on Vertex Complemented Coronae",
        "Testing and Combining Transient Spectral Classification Tools on\n  4MOST-like Blended Spectra",
        "Highly Entangled Magnetodielectric and Magnetostriction effects, and\n  Spin-Phonon coupling in the Antiferromagnetic Ni$_2$ScSbO$_6$",
        "Sky localization of gravitational waves from eccentric binaries",
        "Mean-Field Analysis of Latent Variable Process Models on Dynamically\n  Evolving Graphs with Feedback Effects",
        "J-braid groups are torus necklace groups",
        "PoSSUM: A Protocol for Surveying Social-media Users with Multimodal LLMs",
        "Revealing higher-order neural representations with generative artificial\n  intelligence",
        "Are compact open-charm tetraquarks consistent with recent lattice\n  results?",
        "Partial Condition Numbers for Double Saddle Point Problems",
        "ZAPBench: A Benchmark for Whole-Brain Activity Prediction in Zebrafish",
        "Regularity for free boundary surfaces minimizing degenerate area\n  functionals",
        "A parameterization method for quasi-periodic systems with noise:\n  computation of random invariant tori",
        "Really perverse periodic solutions of the planar N-body problem",
        "Topological superconductivity in hourglass Dirac chain metals (Ti,\n  Hf)IrGe"
      ],
      "abstract":[
        "Stellar occultations are an ideal way to characterize the physical and\norbital properties of trans-Neptunian binary systems. In this research note, we\ndetail the prediction and observation of a stellar occultation observed with\nNASA's IRTF on March 16$^{\\mathrm{th}}$, 2025 (UT), with drop-outs from both\nthe dwarf planet Haumea and its smaller satellite Namaka. This occultation\nplaces a lower limit of 83 $\\pm$ 2 km on Namaka's diameter. We also discuss the\npossibility that this detection could help to constrain the orbit of Namaka,\nmeasure Haumea's gravitational harmonics, and provide a path to measuring the\ninternal structure of Haumea.",
        "Observations of hierarchical triple star systems show that misalignments are\ncommon both between the angular momentum vector of the inner binary and the\nouter companion orbit, and between the outer binary orbit and a circumtriple\ngas disk. With analytic methods and n-body simulations we explore the dynamics\nof circumtriple orbits around a misaligned hierarchical triple star.\nCircumtriple test particle orbits nodally precess either about the outer binary\nangular momentum vector (circulating orbits) or about a stationary inclination\nthat depends upon the binary properties (librating orbits). For a coplanar (or\nretrograde coplanar) triple star, the apsidal precession rate is maximal and\nthe critical orbital radius outside of which all orbits are circulating is\nminimal. Polar alignment of a circumtriple gas disk requires nodal libration\nand therefore it can be more likely if there is a large misalignment between\nthe inner and outer binary orbits. There are two values of the mutual\nmisalignment, i_c and 180-i_c, for which the apsidal precession rate of the\ntriple star is zero and polar alignment is possible at all orbital radii. For a\ncircular inner binary orbit i_c=55, and it changes with eccentricity of the\ninner binary while being insensitive to other triple star parameters.",
        "Infrared spectroscopy, e.g., with JWST, provides a glimpse into the chemical\ninventory of the innermost region of protoplanetary discs, where terrestrial\nplanets eventually form. The chemical make-up of regions inside snowlines is\nconnected to the material drifting from the outer regions, which can be modeled\nwith dust evolution models. However, infrared observations are limited by the\nhigh dust extinction in the inner disc, and only probes the abundances of\ngaseous species in the disc surface layers. As a result, the bulk mass of\ndelivered volatiles is not directly relatable to what is measured through\ninfrared spectra. In this paper, we investigate how the delivery of dust and\nice after prolonged pebble drift affects the observable reservoir of water\nvapor in the inner disc. We develop a 1+1D approach based on dust evolution\nmodels to determine the delivery and distribution of vapor compared to the\nheight of the $\\tau = 1$ surface in the dust continuum. We find that the\nobservable column density of water vapor at wavelengths probed by JWST spans\nmany orders of magnitude over time, exhibiting different radial profiles\ndepending on dust properties, drift rate, and local processing. In the presence\nof a traffic-jam effect inside the snowline, the observable vapor reservoir\nappears constant in time despite the ongoing delivery by pebble drift, such\nthat water is effectively smuggled unnoticed. Differences in measured column\ndensities then originate not only from variations in bulk vapor content, but\nalso from differences in the properties and distribution of dust particles.",
        "The interaction between gas and dust in protoplanetary disks (PPDs) plays a\ncrucial role in setting the stage of planet formation. In particular, the\nstreaming instability (SI) is well recognized as the mechanism for planetesimal\nformation out of this interaction. The outer region of PPDs is likely subject\nto the vertical shear instability (VSI), representing a major source of disk\nturbulence characterized by vertical corrugation that leads to strong dust\nstirring. In the meantime, the VSI turbulence in 3D generates vortices through\nthe Rossby wave instability (RWI), which can trap dust and thereby promote dust\nconcentration. In this study, we use the multifluid dust module in Athena++ to\nconduct 2D axisymmetric global simulations of PPDs with mesh refinement and 3D\nglobal simulations with modest resolution. In 2D, the VSI corrugation mode is\nweakened by dust back-reaction, while the SI can still survive regardless of\ninitial conditions. Dust clumping occurs and is seeded by VSI-induced zonal\nflows. In 3D, dust can settle even more with increased dusty buoyancy,\nsuppressing the VSI corrugation mode. Meanwhile, dust back-reaction enhances\ndust concentration in RWI vortices, though higher resolution is needed to\nassess dust clumping.",
        "We present the results of combined hydrodynamic and particle tracking\npost-processing modeling to study the transport of small dust in a\nprotoplanetary disk containing an embedded embryo in 3D. We use a suite of\nFARGO3D hydrodynamic simulations of disks containing a planetary embryo varying\nin mass up to 300 $M_\\oplus$ on a fixed orbit in both high and low viscosity\ndisks. We then simulate solid particles through the disk as a post-processing\nstep using a Monte Carlo integration, allowing us to track the trajectories of\nindividual particles as they travel throughout the disk. We find that gas\nadvection onto the planet can carry small, well-coupled solids across the gap\nopened in the disk by the embedded planet for planetary masses above the pebble\nisolation mass. This mixing between the inner and outer disk can occur in both\ndirections, with solids in the inner disk mixing to the outer disk as well.\nAdditionally, in low viscosity disks, multiple pile-ups in the outer disk may\npreserve isotopic heterogeneities, possibly providing an outermost tertiary\nisotopic reservoir. Throughout Jupiter's growth, the extent of mixing between\nisotopic reservoirs varied depending on dust size, gas turbulence, and the\nJovian embryo mass.",
        "The detection of young transiting exoplanets represents a new frontier in our\nunderstanding of planet formation and evolution. For the population of observed\nclose-in sub-Neptunes, two proposed formation pathways can reproduce their\nobserved masses and radii at $\\sim$Gyr ages: the \"gas dwarf\" hypothesis and the\n\"water world\" hypothesis. We show that a sub-Neptune's size at early ages\n$\\lesssim 100$ Myrs is strongly dependent on the bulk mean molecular weight\nwithin its envelope. As a result, gas dwarfs and water worlds should diverge in\nsize at early ages since the mean molecular weight of gas dwarf envelopes is\npredicted to be smaller than that of water worlds. We construct population\nmodels under both scenarios that reproduce Kepler demographics in the age range\n$\\sim1-10$ Gyrs. We find tentative evidence that the gas dwarf model is more\nconsistent with the small population of young exoplanets $< 40$ Myrs from TESS.\nWe show that planet radius is relatively insensitive to planet mass for young,\npuffy sub-Neptunes, meaning that well-characterised masses are not necessarily\nrequired to exploit the effects of mean molecular weight at the population\nlevel. We confirm the predicted difference in planet size between the models is\nalso true under mixed-envelope scenarios, in which envelopes consist of\nmixtures of hydrogen and steam. We highlight that transit surveys of young\nexoplanets should target the youngest observable stellar clusters to exploit\nthe effects of mean molecular weight.",
        "The vertical shear instability and the streaming instability are two robust\nsources of turbulence in protoplanetary disks. The former has been found to\ninduce anisotropic turbulence that is stronger in the vertical than in the\nradial dimension and to be overall stronger compared to the largely isotropic\nturbulence caused by the streaming instability. In this study, we shed light on\nthe dust diffusion by the vertical shear instability and the streaming\ninstability separately and together, and in particular on the direction- and\nscale-dependence of the diffusion. To this end, we employ two-dimensional\nglobal models of the two instabilities either in isolation or in combination.\nThe vertical shear instability in isolation diffuses dust more strongly in the\nvertical direction than the streaming instability in isolation, resulting in a\nwave-shaped dust layer in our two-dimensional simulations. Compared with this\nlarge-scale diffusion, though, our study highlights that the vertical shear\ninstability causes substantially weaker or even negligible small-scale\ndiffusion. We validate this result using previously published three-dimensional\nsimulations. In particular when simulating centimetre-sized dust, the\nundulating dust layer becomes internally razor-thin. In contrast, the diffusion\nowing to the streaming instability exhibits only a marginal scale-dependence,\nwith the dust layer possessing a Gaussian shape. In models including both\ninstabilities, the undulating mid-plane layer is broadened to a width set by\nthe intrinsic diffusion level caused by the streaming instability.",
        "This work analyzes the energetics of asteroid rubble piles in order to\nunderstand what asteroid morphologies should naturally arise from their\nformation and evolution process. In doing this, a phase diagram is developed\nthat maps out the range of final minimum energy states that a collapsing\ngravitational aggregate can achieve as a function of total angular momentum and\nmass distribution. This is developed assuming properties associated with rubble\npile asteroids, and can provide insight into the formation and subsequent\nevolution of contact binaries and orbital binaries in the solar system as an\noutcome of catastrophic disruptions. The system angular momentum is used as an\nindependent parameter, combined with resulting minimum energy configurations as\na simple function of mass morphology of the final system. The configuration of\nsystems with an energy boosted above the minimum energy state are also\nconsidered. This paper considers an ideal case, but outlines general results\nthat can be continued for more precise models of distributed granular media\nmodeled using continuum models or using discrete element models.",
        "Comet 7P\/Pons-Winnecke was observed from the Calar Alto Observatory (Spain)\nfor four months during the 2021 inbound apparition. Broad-band visible images\nwere taken between 1.71 and 1.25 AU pre-perihelion, while long-slit\nspectrophotometric data were taken at $\\sim$ 1.25 AU pre-perihelion. This\ndataset has been complemented with three $r$-Sloan images observed from Zwicky\nTransient Facility (ZTF) to model the physical properties and loss rate of the\ndust with a forward Monte Carlo dust tail code. The model fits the observed\nisophotes well for most observations. The peak dust production rate was\nmeasured at 83 kg s$^{-1}$, 15 days after perihelion. The particle terminal\nspeed ranges from 3 m s$^{-1}$ for 0.1 m particles to 23 m s$^{-1}$ for 5\n$\\mu$m particles. Regarding the gas production from spectra, CN and C$_2$ show\nasymmetric emission between the sunward and antisunward directions beyond the\ndata uncertainties and error propagation, while a clear asymmetry for C$_3$\ncannot be definitively claimed. Average production rates for CN, C$_2$, and\nC$_3$ near 2021 perihelion are 1.15 $\\times 10^{24}$, 2.32$\\times 10^{24}$, and\n1.69$\\times 10^{23}$ s$^{-1}$, respectively. The dust-to-gas mass ratio value\nis estimated to be around 2, suggesting a dust-rich composition. Based on the\ngas composition and the $Af\\rho$ value, we classify 7P\/Pons-Winnecke as having\na typical composition for Jupiter Family comets, with some C$_3$ depletion.\nGiven the limited previous knowledge, our work contributes to expanding the\nunderstanding of the activity and characteristics of 7P\/Pons-Winnecke.",
        "Close-by Earth analogs and super-Earths are of primary importance because\nthey will be preferential targets for the next generation of direct imaging\ninstruments. Bright and close-by G-to-M type stars are preferential targets in\nradial velocity surveys to find Earth analogs. We present an analysis of the RV\ndata of the star HD 20794, a target whose planetary system has been extensively\ndebated in the literature. The broad time span of the observations makes it\npossible to find planets with signal semi-amplitudes below 1 m\/s in the\nhabitable zone. We monitored the system with ESPRESSO. We joined ESPRESSO data\nwith the HARPS data, including archival data and new measurements from a recent\nprogram. We applied the post-processing pipeline YARARA to HARPS data to\ncorrect systematics, improve the quality of RV measurements, and mitigate the\nimpact of stellar activity. Results. We confirm the presence of three planets,\nwith periods of 18.3142 +\/- 0.0022 d, 89.68 +\/- 0.10 d, and 647.6 +\/- 2.6 d,\nalong with masses of 2.15 +\/- 0.17 MEarth, 2.98 +\/- 0.29 MEarth, and 5.82 +\/-\n0.57 MEarth respectively. For the outer planet, we find an eccentricity of 0.45\n+\/- 0.10, whereas the inner planets are compatible with circular orbits. The\nlatter is likely to be a rocky planet in the habitable zone of HD 20794. From\nthe analysis of activity indicators, we find evidence of a magnetic cycle with\na period around 3000 d, along with evidence pointing to a rotation period\naround 39 d. We have determined the presence of a system of three planets\norbiting the solar-type star HD 20794. This star is bright (V=4.34 mag) and\nclose (d = 6.04 pc), and HD 20794 d resides in the stellar habitable zone,\nmaking this system a high-priority target for future atmospheric\ncharacterization with direct imaging facilities.",
        "Several planet formation models have been proposed to explain the gap in the\npopulation of planets between $1.8$ $R_\\oplus$ to $2.0$ $R_\\oplus$ known as the\nRadius Valley. To apply these models to confirmed exoplanets, accurate and\nprecise host star and planet parameters are required to ensure the observed\nmeasurements correctly match model predictions. Previous studies have\nemphasized the need for a larger, more precise sample to further confirm\ndominant formation processes. By enhancing standard SED (Spectral Energy\nDistribution) fitting using Bayesian methods we derived highly accurate and\nprecise host star and planet parameters. Specifically, we achieved median\nfractional uncertainties for stellar and planet radii of 2.4% and 3.4%,\nrespectively. We then produced the largest, most precise sample to date of 1923\nplanets when compared to previous studies. This full sample, as well as a\nsampled filtered for host stellar masses between $0.8$ and $1.2$ $M_\\odot$, are\nthen used to derive the slope and position of the radius valley as a function\nof orbital period, insolation flux and stellar mass to compare them to\npredictive models and previous observational results. Our results are\nconsistent with thermally-driven mass loss with a planet radius vs. orbital\nperiod slope of $-0.142$ $\\pm0.006$ for the full sample leaning toward\ncore-powered mass loss. The planet radius vs. insolation flux slope of $0.136$\n$\\pm0.014$ for the filtered sample leaned toward photoevaporation. Also, the\nslope as a function of stellar mass for both samples appear more consistent\nwith thermally driven processes when compared to models and previous studies.",
        "We report the confirmation of TOI-6324 b, an Earth-sized (1.059 $\\pm$ 0.041\nR$_\\oplus$) ultra-short-period (USP) planet orbiting a nearby ($\\sim$20 pc) M\ndwarf. Using the newly commissioned Keck Planet Finder (KPF) spectrograph, we\nhave measured the mass of TOI-6324 b 1.17 $\\pm$ 0.22 M$_\\oplus$. Because of its\nextremely short orbit of just $\\sim$6.7 hours, TOI-6324 b is intensely\nirradiated by its M dwarf host, and is expected to be stripped of any thick,\nH\/He envelope. We were able to constrain its interior composition and found an\niron core mass fraction (CMF = 27$\\pm$37%) consistent with that of Earth\n($\\sim$33%) and other confirmed USPs. TOI-6324 b is the closest to Earth-sized\nUSP confirmed to date. TOI-6324 b is a promising target for JWST phase curve\nand secondary eclipse observations (Emission Spectroscopy Metric = 25) which\nmay reveal its surface mineralogy, day-night temperature contrast, and possible\ntidal deformation. From 7 sectors of TESS data, we report a tentative detection\nof the optical phase curve variation with an amplitude of 42$\\pm$28 ppm.",
        "The physical and orbital parameters of Trans-Neptunian Objects (TNOs) provide\nvaluable information about the Solar System's formation and evolution. In\nparticular, the characterization of binaries provides insights into the\nformation mechanisms that may be playing a role at such large distances from\nthe Sun. Studies show two distinct populations, and (38628) Huya occupies an\nintermediate position between the unequal-size binaries and those with\ncomponents of roughly equal sizes. In this work, we predicted and observed\nthree stellar occultation events by Huya. Huya and its satellite - S\/2012\n(38628) 1 - were detected during occultations in March 2021 and again in June\n2023. Additionally, an attempt to detect Huya in February 2023 resulted in an\nadditional single-chord detection of the secondary. A spherical body with a\nminimum diameter of D = 165 km can explain the three single-chord observations\nand provide a lower limit for the satellite size. The astrometry of Huya's\nsystem, as derived from the occultations and supplemented by observations from\nthe Hubble Space Telescope and Keck Observatory, provided constraints on the\nsatellite orbit and the mass of the system. Therefore, assuming the secondary\nis in an equatorial orbit around the primary, the limb fitting was constrained\nby the satellite orbit position angle. The system density, calculated by\nsumming the most precise measurement of Huya's volume to the spherical\nsatellite average volume, is $\\rho_{1}$ = 1073 $\\pm$ 66 kg m$^{-3}$. The\ndensity that the object would have assuming a Maclaurin equilibrium shape with\na rotational period of 6.725 $\\pm$ 0.01 hours is $\\rho_{2}$ = 768 $\\pm$ 42 kg\nm$^{-3}$. This difference rules out the Maclaurin equilibrium assumption for\nthe main body shape.",
        "Given a graph $G$ with vertex set $V(G)=\\{v_1,v_2,\\ldots,v_{n_1}\\}$ and a\ngraph $H$ of order $n_2$, the vertex complemented corona, denoted by\n$G\\tilde{\\circ}{H}$, is the graph produced by copying $H$ $n_1$ times, with the\n$i$-th copy of $H$ corresponding to the vertex $v_i$, and then adding edges\nbetween any vertex in $V(G)\\setminus\\{v_{i}\\}$ and any vertex of the $i$-th\ncopy of $H$. The present article deals with quantum state transfer of vertex\ncomplemented coronae concerning signless Laplacian matrix. Our research\ninvestigates conditions in which signless Laplacian perfect state transfer\nexists or not on vertex complemented coronae. Additionally, we also provide\nsome mild conditions for the class of graphs under consideration that allow\nsignless Laplacian pretty good state transfer.",
        "With the 4-meter Multi-Object Spectroscopic Telescope (4MOST) expected to\nprovide an influx of transient spectra when it begins observations in early\n2026 we consider the potential for real-time classification of these spectra.\nWe investigate three extant spectroscopic transient classifiers: the Deep\nAutomated Supernova and Host classifier (DASH), Next Generation SuperFit (NGSF)\nand SuperNova IDentification (SNID), with a focus on comparing the efficiency\nand purity of the transient samples they produce. We discuss our method for\nsimulating realistic, 4MOST-like, host-galaxy contaminated spectra and\ndetermining quality cuts for each classifier used to ensure pure SN Ia samples\nwhile maintaining efficient classification in other transient classes. We\ninvestigate the classifiers individually and in combinations. We find that a\ncombination of DASH and NGSF can produce a SN Ia sample with a purity of 99.9%\nwhile successfully classifying 70% of SNe Ia. However, it struggles to classify\nnon-SN Ia transients. We investigate photometric cuts to transient magnitude\nand transient flux fraction, finding that both can be used to improve transient\nclassification efficiencies by 7--25% depending on the transient subclass.\nFinally, we present an example classification plan for live classification and\nthe predicted purities and efficiencies across five transient classes: Ia, Ibc,\nII, superluminous and non-supernova transients.",
        "Magnetic systems with noncentrosymmetric crystal structures are renowned for\ntheir complex magnetic ordering and diverse and fascinating physical\nproperties. In this report, we provide a comprehensive study of the chiral\nmagnetic system Ni$_2$ScSbO$_6$, which exhibits a robust incommensurate\nlong-range antiferromagnetic spin ordering at a temperature of $T_N = 62$~K, as\nrevealed by bulk magnetization, specific heat, and neutron diffraction studies.\nThis magnetic ordering triggers a series of intriguing phenomena, including\nprominent magnetodielectric coupling manifested by a dielectric peak at $T_N$,\nsignificant spin-phonon coupling resulting in strong phonon renormalization\ncharacterized by anomalous softening of various Raman modes, and a remarkable\nvolume magnetostriction effect probed by high-resolution synchrotron X-ray\ndiffraction. These phenomena are intricately interlinked, positioning the\npresent system as a rare and interesting material.",
        "We demonstrate that the orbital eccentricity in compact binary mergers can be\nused to improve their sky localization using gravitational wave observations.\nExisting algorithms that conduct the localizations are not optimized for\neccentric sources. We use a semi-Bayesian technique to carry out localizations\nof simulated sources recovered using a matched-filter search. Through these\nsimulations, we find that if a non-negligible eccentricity is obtained during\nthe detection, an eccentricity-optimized algorithm can significantly improve\nthe localization areas compared to the existing methods. We also lay out the\nfoundation for an eccentric early-warning system using the matched-filter\nsearch. The potential impact on the early-warning localization is investigated.\nWe indicate a few possible cases of improvements while accounting for\neccentricity toward any detectable eccentric neutron star binaries in the\nforthcoming observing scenarios of ground-based detectors. Improved\nlocalizations can be useful in effectually utilizing the capabilities of the\nfollow-up facilities.",
        "In this paper, we study the asymptotic behavior of a class of dynamic\nco-evolving latent space networks. The model we study is subject to\nbi-directional feedback effects, meaning that at any given time, the latent\nprocess depends on its own value and the graph structure at the previous time\nstep, and the graph structure at the current time depends on the value of the\nlatent processes at the current time but also on the graph structure at the\nprevious time instance (sometimes called a persistence effect). We construct\nthe mean-field limit of this model, which we use to characterize the limiting\nbehavior of a random sample taken from the latent space network in the limit as\nthe number of nodes in the network diverges. From this limiting model, we can\nderive the limiting behavior of the empirical measure of the latent process and\nestablish the related graphon limit of the latent particle network process. We\nalso provide a description of the rich conditional probabilistic structure of\nthe limiting model. The inherent dependence structure complicates the\nmathematical analysis significantly. In the process of proving our main\nresults, we derive a general conditional propagation of chaos result, which is\nof independent interest. In addition, our novel approach of studying the\nlimiting behavior of random samples proves to be a very useful methodology for\nfully grasping the asymptotic behavior of co-evolving particle systems.\nNumerical results are included to illustrate the theoretical findings.",
        "We construct a family of links we call torus necklaces for which the link\ngroups are precisely the braid groups of generalised $J$-reflection groups.\nMoreover, this correspondence exhibits the meridians of the aforementioned link\ngroups as braid reflections. In particular, this construction generalises to\nall irreducible rank two complex reflection groups a well-known correspondence\nbetween some rank two complex braid groups and some torus knot groups. In\naddition, as abstract groups, we show that the family of link groups associated\nto Seifert links coincides with the family of circular groups. This shows that\nevery time a link group has a non-trivial center, it is a Garside group.",
        "This paper introduces PoSSUM, an open-source protocol for unobtrusive polling\nof social-media users via multimodal Large Language Models (LLMs). PoSSUM\nleverages users' real-time posts, images, and other digital traces to create\nsilicon samples that capture information not present in the LLM's training\ndata. To obtain representative estimates, PoSSUM employs Multilevel Regression\nand Post-Stratification (MrP) with structured priors to counteract the\nobservable selection biases of social-media platforms. The protocol is\nvalidated during the 2024 U.S. Presidential Election, for which five PoSSUM\npolls were conducted and published on GitHub and X. In the final poll, fielded\nOctober 17-26 with a synthetic sample of 1,054 X users, PoSSUM accurately\npredicted the outcomes in 50 of 51 states and assigned the Republican candidate\na win probability of 0.65. Notably, it also exhibited lower state-level bias\nthan most established pollsters. These results demonstrate PoSSUM's potential\nas a fully automated, unobtrusive alternative to traditional survey methods.",
        "Studies often aim to reveal how neural representations encode aspects of an\nobserver's environment, such as its contents or structure. These are\n``first-order\" representations (FORs), because they're ``about\" the external\nworld. A less-common target is ``higher-order\" representations (HORs), which\nare ``about\" FORs -- their contents, stability, or uncertainty. HORs of\nuncertainty appear critically involved in adaptive behaviors including learning\nunder uncertainty, influencing learning rates and internal model updating based\non environmental feedback. However, HORs about uncertainty are unlikely to be\ndirect ``read-outs\" of FOR characteristics, instead reflecting estimation\nprocesses which may be lossy, bias-prone, or distortive and which may also\nincorporate estimates of distributions of uncertainty the observer is likely to\nexperience. While some research has targeted neural representations of\n``instantaneously\" estimated uncertainty, how the brain represents\n\\textit{distributions} of expected uncertainty remains largely unexplored.\nHere, we propose a novel reinforcement learning (RL) based generative\nartificial intelligence (genAI) approach to explore neural representations of\nuncertainty distributions. We use existing functional magnetic resonance\nimaging data, where humans learned to `de-noise' their brain states to achieve\ntarget neural patterns, to train denoising diffusion genAI models with RL\nalgorithms to learn noise distributions similar to how humans might learn to do\nthe same. We then explore these models' learned noise-distribution HORs\ncompared to control models trained with traditional backpropagation. Results\nreveal model-dependent differences in noise distribution representations --\nwith the RL-based model offering much higher explanatory power for human\nbehavior -- offering an exciting path towards using genAI to explore neural\nnoise-distribution HORs.",
        "We argue that the hypothesis that positive-parity charm meson resonances\nexhibit a compact tetraquark structure has some clear tension with recent\nlattice results for the $S$-wave $\\pi D$ system for an SU(3) flavor symmetric\nsetting. In particular, we show that such a diquark--anti-diquark tetraquark\nscenario would call for the presence of a state in the flavor\n$[{\\mathbf{\\overline{15}}}]$ representation, not seen in the lattice analysis.\nMoreover, we show that analogous lattice data in the axial-vector channel are\neven more sensitive to the internal structure of these very interesting states.",
        "This paper presents a unified framework for investigating the partial\ncondition number (CN) of the solution of double saddle point problems (DSPPs)\nand provides closed-form expressions for it. This unified framework encompasses\nthe well-known partial normwise CN (NCN), partial mixed CN (MCN) and partial\ncomponentwise CN (CCN) as special cases. Furthermore, we derive sharp upper\nbounds for the partial NCN, MCN and CCN, which are computationally efficient\nand free of expensive Kronecker products. By applying perturbations that\npreserve the structure of the block matrices of the DSPPs, we analyze the\nstructured partial NCN, MCN and CCN when the block matrices exhibit linear\nstructures. By leveraging the relationship between DSPP and equality\nconstrained indefinite least squares (EILS) problems, we recover the partial\nCNs for the EILS problem. Numerical results confirm the sharpness of the\nderived upper bounds and demonstrate their effectiveness in estimating the\npartial CNs.",
        "Data-driven benchmarks have led to significant progress in key scientific\nmodeling domains including weather and structural biology. Here, we introduce\nthe Zebrafish Activity Prediction Benchmark (ZAPBench) to measure progress on\nthe problem of predicting cellular-resolution neural activity throughout an\nentire vertebrate brain. The benchmark is based on a novel dataset containing\n4d light-sheet microscopy recordings of over 70,000 neurons in a larval\nzebrafish brain, along with motion stabilized and voxel-level cell\nsegmentations of these data that facilitate development of a variety of\nforecasting methods. Initial results from a selection of time series and\nvolumetric video modeling approaches achieve better performance than naive\nbaseline methods, but also show room for further improvement. The specific\nbrain used in the activity recording is also undergoing synaptic-level\nanatomical mapping, which will enable future integration of detailed structural\ninformation into forecasting methods.",
        "We establish an epsilon-regularity theorem at points in the free boundary of\nalmost-minimizers of the energy\n$\\mathrm{Per}_{w}(E)=\\int_{\\partial^*E}w\\,\\mathrm{d} {\\mathscr{H}}^{n-1}$,\nwhere $w$ is a weight asymptotic to $d(\\cdot,\\mathbb{R}^n\\setminus\\Omega)^a$\nnear $\\partial\\Omega$ and $a>0$.\n  This implies that the boundaries of almost-minimizers are\n$C^{1,\\gamma_0}$-surfaces that touch $\\partial \\Omega$ orthogonally, up to a\nSingular Set $\\mathrm{Sing}(\\partial E)$ whose Hausdorff dimension satisfies\nthe bound\n  $d_{\\mathscr{H}}(\\mathrm{Sing}(\\partial E)) \\leq n +a -(5+\\sqrt{8})$.",
        "This work is devoted to studying normally hyperbolic invariant manifolds\n(NHIMs) for a class of quasi-periodically forced systems subject to additional\nstochastic noise. These systems can be understood as skew-product systems. The\nexistence of NHIMs is established by developing a parameterization method in\nrandom settings and applying the Implicit Function Theorem in appropriate\nBanach spaces. Based on this, we propose a numerical algorithm to compute the\nstatistics of NHIMs and Lyapunov exponents.",
        "Examples are given of solutions of the planar N-body problem which remain the\nsame for at least two systems of masses with the same sum and same center of\nmass. The least value of N achieved up to now with this property is 474, a\nnumber which had been announced in the first author's thesis.",
        "Realizing topological superconductivity in stoichiometric materials is a key\nchallenge in condensed matter physics. Here, we report the discovery of ternary\ngermanide superconductors, $M$IrGe ($M$ = Ti, Hf), as prime candidates for\ntopological superconductivity, predicted to exhibit nonsymmorphic\nsymmetry-protected hourglass Dirac chains. Using comprehensive thermodynamic\nand muon-spin rotation\/relaxation ($\\mu$SR) measurements, we establish these\nmaterials as conventional bulk type-II superconductors with transition\ntemperatures of 2.24(5) K for TiIrGe and 5.64(4) K for HfIrGe, featuring a full\ngap and preserved time-reversal symmetry. First-principles calculations reveal\nstriking topological features in $M$IrGe, including hourglass-shaped bulk\ndispersions and a Dirac chain -- a ring of fourfold-degenerate Dirac points\nprotected by nonsymmorphic symmetry. Each Dirac point corresponds to the neck\nof the hourglass dispersion, while the Dirac chain gives rise to drumhead-like\nsurface states near the Fermi level. Additionally, nontrivial $\\mathbb{Z}_2$\ntopology leads to isolated Dirac surface states with helical spin textures that\ndisperse across the Fermi level, forming an ideal platform for\nproximity-induced topological superconductivity. The coexistence of\nconventional bulk superconductivity, symmetry-protected hourglass topology, and\nhelical spin-textured surface states establishes $M$IrGe as a rare and robust\nplatform to realize topological superconductivity, opening new avenues for\nnext-generation quantum technologies."
      ]
    }
  },
  {
    "id":2411.03156,
    "research_type":"applied",
    "start_id":"b7",
    "start_title":"Physics guided deep learning for generative design of crystal materials with symmetry constraints",
    "start_abstract":"Abstract Discovering new materials is a challenging task in science crucial to the progress of human society. Conventional approaches based on experiments and simulations are labor-intensive or costly with success heavily depending experts\u2019 heuristic knowledge. Here, we propose deep learning Physics Guided Crystal Generative Model (PGCGM) for efficient crystal material design high structural diversity symmetry. Our model increases generation validity by more than 700% compared FTCP, one latest structure generators 45% our previous CubicGAN model. Density Functional Theory (DFT) calculations used validate generated structures 1869 out 2000 successfully optimized deposited into Carolina Materials Database www.carolinamatdb.org , which 39.6% have negative formation energy 5.3% energy-above-hull less 0.25 eV\/atom, indicating their thermodynamic stability potential synthesizability.",
    "start_categories":[
      "physics.comp-ph"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b6"
      ],
      "title":[
        "High\u2010Throughput Discovery of Novel Cubic Crystal Materials Using Deep Generative Neural Networks"
      ],
      "abstract":[
        "Abstract High\u2010throughput screening has become one of the major strategies for discovery novel functional materials. However, its effectiveness is severely limited by lack sufficient and diverse materials in current repositories such as open quantum database (OQMD). Recent progress deep learning have enabled generative that learn implicit chemical rules creating hypothetical with new compositions structures. models difficulty generating structurally diverse, chemically valid, stable Here we propose CubicGAN, a adversarial network (GAN) based neural model large scale design cubic When trained on 375 749 ternary from OQMD database, authors show able to not only rediscover most currently known but also generate structure prototypes. A total 506 been verified phonon dispersion calculation. Considering importance wide applications solar panels, GAN provides promising approach significantly expand existing repositories, enabling via screening. The crystal structures discovered are freely accessible at www.carolinamatdb.org ."
      ],
      "categories":[
        "cs.NE"
      ]
    },
    "list":{
      "title":[
        "Cancermorphic Computing Toward Multilevel Machine Intelligence",
        "Range and Angle Estimation with Spiking Neural Resonators for FMCW Radar",
        "Hybrid Firefly Algorithm and Sperm Swarm Optimization Algorithm using\n  Newton-Raphson Method (HFASSON) and its application in CR-VANET",
        "A Runtime Analysis of the Multi-Valued Compact Genetic Algorithm on\n  Generalized LeadingOnes",
        "A Hybrid Tabu Scatter Search Algorithm for Simulation-Based Optimization\n  of Multi-Objective Runway Operations Scheduling",
        "Quantum Simplicial Neural Networks",
        "A Digital Machine Learning Algorithm Simulating Spiking Neural Network\n  CoLaNET",
        "Adding numbers with spiking neural circuits on neuromorphic hardware",
        "An Enhancement of Cuckoo Search Algorithm for Optimal Earthquake\n  Evacuation Space Allocation in Intramuros, Manila City",
        "Evolving Form and Function: Dual-Objective Optimization in Neural\n  Symbolic Regression Networks",
        "Brain in the Dark: Design Principles for Neuromimetic Inference under\n  the Free Energy Principle",
        "A RankNet-Inspired Surrogate-Assisted Hybrid Metaheuristic for Expensive\n  Coverage Optimization",
        "Warm Starting of CMA-ES for Contextual Optimization Problems",
        "Sovereign Debt Default and Climate Risk",
        "GFlowVLM: Enhancing Multi-step Reasoning in Vision-Language Models with\n  Generative Flow Networks",
        "Mathematical modelling and homogenization of thin fiber-reinforced\n  hydrogels",
        "Training Dynamics of In-Context Learning in Linear Attention",
        "A new fuzzy fractional differential variational inequality with\n  Mittag-Leffler kernel of order $q \\in (1,2]$",
        "Surface Diagrams for Frobenius Algebras and Frobenius-Schur Indicators\n  in Grothendieck-Verdier Categories",
        "Technical Note: Targeted Maximum Likelihood Estimator for an ATE\n  Standardized for New Target Population",
        "Temporal-Guided Spiking Neural Networks for Event-Based Human Action\n  Recognition",
        "SoK: A Review of Cross-Chain Bridge Hacks in 2023",
        "O-RIS-ing: Evaluating RIS-Assisted NextG Open RAN",
        "Algebraization of rigid analytic varieties and formal schemes via\n  perfect complexes",
        "Decision from Suboptimal Classifiers: Excess Risk Pre- and\n  Post-Calibration",
        "Enhancing Large Language Model Efficiencyvia Symbolic Compression: A\n  Formal Approach Towards Interpretability",
        "Improved quasi-invariance result for the periodic Benjamin-Ono-BBM\n  equation",
        "The dynamics of meaning through time: Assessment of Large Language\n  Models"
      ],
      "abstract":[
        "Despite their potential to address crucial bottlenecks in computing\narchitectures and contribute to the pool of biological inspiration for\nengineering, pathological biological mechanisms remain absent from\ncomputational theory. We hereby introduce the concept of cancer-inspired\ncomputing as a paradigm drawing from the adaptive, resilient, and evolutionary\nstrategies of cancer, for designing computational systems capable of thriving\nin dynamic, adversarial or resource-constrained environments. Unlike known\nbioinspired approaches (e.g., evolutionary and neuromorphic architectures),\ncancer-inspired computing looks at emulating the uniqueness of cancer cells\nsurvival tactics, such as somatic mutation, metastasis, angiogenesis and immune\nevasion, as parallels to desirable features in computing architectures, for\nexample decentralized propagation and resource optimization, to impact areas\nlike fault tolerance and cybersecurity. While the chaotic growth of cancer is\ncurrently viewed as uncontrollable in biology, randomness-based algorithms are\nalready being successfully demonstrated in enhancing the capabilities of other\ncomputing architectures, for example chaos computing integration. This vision\nfocuses on the concepts of multilevel intelligence and context-driven mutation,\nand their potential to simultaneously overcome plasticity-limited neuromorphic\napproaches and the randomness of chaotic approaches. The introduction of this\nconcept aims to generate interdisciplinary discussion to explore the potential\nof cancer-inspired mechanisms toward powerful and resilient artificial systems.",
        "Automotive radar systems face the challenge of managing high sampling rates\nand large data bandwidth while complying with stringent real-time and energy\nefficiency requirements. The growing complexity of autonomous vehicles further\nintensifies these requirements. Neuromorphic computing offers promising\nsolutions because of its inherent energy efficiency and parallel processing\ncapacity. This research presents a novel spiking neuron model for signal\nprocessing of frequency-modulated continuous wave (FMCW) radars that\noutperforms the state-of-the-art spectrum analysis algorithms in latency and\ndata bandwidth. These spiking neural resonators are based on the\nresonate-and-fire neuron model and optimized to dynamically process raw radar\ndata while simultaneously emitting an output in the form of spikes. We designed\nthe first neuromorphic neural network consisting of these spiking neural\nresonators that estimates range and angle from FMCW radar data. We evaluated\nthe range-angle maps on simulated datasets covering multiple scenarios and\ncompared the results with a state-of-the-art pipeline for radar processing. The\nproposed neuron model significantly reduces the processing latency compared to\ntraditional frequency analysis algorithms, such as the Fourier transformation\n(FT), which needs to sample and store entire data frames before processing. The\nevaluations demonstrate that these spiking neural resonators achieve\nstate-of-the-art detection accuracy while emitting spikes simultaneously to\nprocessing and transmitting only 0.02 % of the data compared to a float-32 FT.\nThe results showcase the potential for neuromorphic signal processing for FMCW\nradar systems and pave the way for designing neuromorphic radar sensors.",
        "This paper proposes a new hybrid algorithm, combining FA, SSO, and the N-R\nmethod to accelerate convergence towards global optima, named the Hybrid\nFirefly Algorithm and Sperm Swarm Optimization with Newton-Raphson (HFASSON).\nThe performance of HFASSON is evaluated using 23 benchmark functions from the\nCEC 2017 suite, tested in 30, 50, and 100 dimensions. A statistical comparison\nis performed to assess the effectiveness of HFASSON against FA, SSO, HFASSO,\nand five hybrid algorithms: Water Cycle Moth Flame Optimization (WCMFO), Hybrid\nParticle Swarm Optimization and Genetic Algorithm (HPSOGA), Hybrid Sperm Swarm\nOptimization and Gravitational Search Algorithm (HSSOGSA), Grey Wolf and Cuckoo\nSearch Algorithm (GWOCS), and Hybrid Firefly Genetic Algorithm (FAGA). Results\nfrom the Friedman rank test show the superior performance of HFASSON.\nAdditionally, HFASSON is applied to Cognitive Radio Vehicular Ad-hoc Networks\n(CR-VANET), outperforming basic CR-VANET in spectrum utilization. These\nfindings demonstrate HFASSON's efficiency in wireless network applications.",
        "In the literature on runtime analyses of estimation of distribution\nalgorithms (EDAs), researchers have recently explored univariate EDAs for\nmulti-valued decision variables. Particularly, Jedidia et al. gave the first\nruntime analysis of the multi-valued UMDA on the r-valued LeadingOnes\n(r-LeadingOnes) functions and Adak et al. gave the first runtime analysis of\nthe multi-valued cGA (r-cGA) on the r-valued OneMax function. We utilize their\nframework to conduct an analysis of the multi-valued cGA on the r-valued\nLeadingOnes function. Even for the binary case, a runtime analysis of the\nclassical cGA on LeadingOnes was not yet available. In this work, we show that\nthe runtime of the r-cGA on r-LeadingOnes is O(n^2r^2 log^3 n log^2 r) with\nhigh probability.",
        "This dissertation addresses the growing challenge of air traffic flow\nmanagement by proposing a simulation-based optimization (SbO) approach for\nmulti-objective runway operations scheduling. The goal is to optimize airport\ncapacity utilization while minimizing delays, fuel consumption, and\nenvironmental impacts. Given the NP-Hard complexity of the problem, traditional\nanalytical methods often rely on oversimplifications and fail to account for\nreal-world uncertainties, limiting their practical applicability. The proposed\nSbO framework integrates a discrete-event simulation model to handle stochastic\nconditions and a hybrid Tabu-Scatter Search algorithm to identify\nPareto-optimal solutions, explicitly incorporating uncertainty and fairness\namong aircraft as key objectives. Computational experiments using real-world\ndata from a major U.S. airport demonstrate the approach's effectiveness and\ntractability, outperforming traditional methods such as First-Come-First-Served\n(FCFS) and deterministic approaches while maintaining schedule fairness. The\nalgorithm's ability to generate trade-off solutions between competing\nobjectives makes it a promising decision support tool for air traffic\ncontrollers managing complex runway operations.",
        "Graph Neural Networks (GNNs) excel at learning from graph-structured data but\nare limited to modeling pairwise interactions, insufficient for capturing\nhigher-order relationships present in many real-world systems. Topological Deep\nLearning (TDL) has allowed for systematic modeling of hierarchical higher-order\ninteractions by relying on combinatorial topological spaces such as simplicial\ncomplexes. In parallel, Quantum Neural Networks (QNNs) have been introduced to\nleverage quantum mechanics for enhanced computational and learning power. In\nthis work, we present the first Quantum Topological Deep Learning Model:\nQuantum Simplicial Networks (QSNs), being QNNs operating on simplicial\ncomplexes. QSNs are a stack of Quantum Simplicial Layers, which are inspired by\nthe Ising model to encode higher-order structures into quantum states.\nExperiments on synthetic classification tasks show that QSNs can outperform\nclassical simplicial TDL models in accuracy and efficiency, demonstrating the\npotential of combining quantum computing with TDL for processing data on\ncombinatorial topological spaces.",
        "During last several years, our research team worked on development of a\nspiking neural network (SNN) architecture, which could be used in the wide\nrange of supervised learning classification tasks. It should work under the\ncondition, that all participating signals (the classified object description,\ncorrect class label and SNN decision) should have spiking nature. As a result,\nthe CoLaNET (columnar layered network) SNN architecture was invented. The\ndistinctive feature of this architecture is a combination of prototypical\nnetwork structures corresponding to different classes and significantly\ndistinctive instances of one class (=columns) and functionally differing\npopulations of neurons inside columns (=layers). The other distinctive feature\nis a novel combination of anti-Hebbian and dopamine-modulated plasticity. While\nCoLaNET is relatively simple, it includes several hyperparameters. Their choice\nfor particular classification tasks is not trivial. Besides that, specific\nfeatures of the data classified (e.g. classification of separate pictures like\nin MNIST dataset vs. classifying objects in a continuous video stream) require\ncertain modifications of CoLaNET structure. To solve these problems, the deep\nmathematical exploration of CoLaNET should be carried out. However, SNNs, being\nstochastic discrete systems, are usually very hard for exact mathematical\nanalysis. To make it easier, I developed a continuous numeric (non-spiking)\nmachine learning algorithm which approximates CoLaNET behavior with\nsatisfactory accuracy. It is described in the paper. At present, it is being\nstudied by exact analytic methods. We hope that the results of this study could\nbe applied to direct calculation of CoLaNET hyperparameters and optimization of\nits structure.",
        "Progress in neuromorphic computing requires efficient implementation of\nstandard computational problems, like adding numbers. Here we implement one\nsequential and two parallel binary adders in the Lava software framework, and\ndeploy them to the neuromorphic chip Loihi 2. We describe the time complexity,\nneuron and synaptic resources, as well as constraints on the bit width of the\nnumbers that can be added with the current implementations. Further, we measure\nthe time required for the addition operation on-chip. Importantly, we encounter\ntrade-offs in terms of time complexity and required chip resources for the\nthree considered adders. While sequential adders have linear time complexity\n$\\bf\\mathcal{O}(n)$ and require a linearly increasing number of neurons and\nsynapses with number of bits $n$, the parallel adders have constant time\ncomplexity $\\bf\\mathcal{O}(1)$ and also require a linearly increasing number of\nneurons, but nonlinearly increasing synaptic resources (scaling with $\\bf n^2$\nor $\\bf n \\sqrt{n}$). This trade-off between compute time and chip resources\nmay inform decisions in application development, and the implementations we\nprovide may serve as a building block for further progress towards efficient\nneuromorphic algorithms.",
        "The Cuckoo Search Algorithm (CSA), while effective in solving complex\noptimization problems, faces limitations in random population initialization\nand reliance on fixed parameters. Random initialization of the population often\nresults in clustered solutions, resulting in uneven exploration of the search\nspace and hindering effective global optimization. Furthermore, the use of\nfixed values for discovery rate and step size creates a trade-off between\nsolution accuracy and convergence speed. To address these limitations, an\nEnhanced Cuckoo Search Algorithm (ECSA) is proposed. This algorithm utilizes\nthe Sobol Sequence to generate a more uniformly distributed initial population\nand incorporates Cosine Annealing with Warm Restarts to dynamically adjust the\nparameters. The performance of the algorithms was evaluated on 13 benchmark\nfunctions (7 unimodal, 6 multimodal). Statistical analyses were conducted to\ndetermine the significance and consistency of the results. The ECSA outperforms\nthe CSA in 11 out of 13 benchmark functions with a mean fitness improvement of\n30% across all functions, achieving 35% for unimodal functions and 24% for\nmultimodal functions. The enhanced algorithm demonstrated increased convergence\nefficiency, indicating its superiority to the CSA in solving a variety of\noptimization problems. The ECSA is subsequently applied to optimize earthquake\nevacuation space allocation in Intramuros, Manila.",
        "Data increasingly abounds, but distilling their underlying relationships down\nto something interpretable remains challenging. One approach is genetic\nprogramming, which `symbolically regresses' a data set down into an equation.\n  However, symbolic regression (SR) faces the issue of requiring training from\nscratch for each new dataset. To generalize across all datasets, deep learning\ntechniques have been applied to SR.\n  These networks, however, are only able to be trained using a symbolic\nobjective: NN-generated and target equations are symbolically compared. But\nthis does not consider the predictive power of these equations, which could be\nmeasured by a behavioral objective that compares the generated equation's\npredictions to actual data.\n  Here we introduce a method that combines gradient descent and evolutionary\ncomputation to yield neural networks that minimize the symbolic and behavioral\nerrors of the equations they generate from data.\n  As a result, these evolved networks are shown to generate more symbolically\nand behaviorally accurate equations than those generated by networks trained by\nstate-of-the-art gradient based neural symbolic regression methods.\n  We hope this method suggests that evolutionary algorithms, combined with\ngradient descent, can improve SR results by yielding equations with more\naccurate form and function.",
        "Deep learning has revolutionised artificial intelligence (AI) by enabling\nautomatic feature extraction and function approximation from raw data. However,\nit faces challenges such as a lack of out-of-distribution generalisation,\ncatastrophic forgetting and poor interpretability. In contrast, biological\nneural networks, such as those in the human brain, do not suffer from these\nissues, inspiring AI researchers to explore neuromimetic deep learning, which\naims to replicate brain mechanisms within AI models. A foundational theory for\nthis approach is the Free Energy Principle (FEP), which despite its potential,\nis often considered too complex to understand and implement in AI as it\nrequires an interdisciplinary understanding across a variety of fields. This\npaper seeks to demystify the FEP and provide a comprehensive framework for\ndesigning neuromimetic models with human-like perception capabilities. We\npresent a roadmap for implementing these models and a Pytorch code repository\nfor applying FEP in a predictive coding network.",
        "Coverage optimization generally involves deploying a set of facilities to\nbest satisfy the demands of specified points, with broad applications in fields\nsuch as location science and sensor networks. Recent applications reveal that\nthe subset site selection coupled with continuous angular parameter\noptimization can be formulated as Mixed-Variable Optimization Problems (MVOPs).\nMeanwhile, high-fidelity discretization and visibility analysis significantly\nincrease computational cost and complexity, evolving the MVOP into an Expensive\nMixed-Variable Optimization Problem (EMVOP). While canonical Evolutionary\nAlgorithms have yielded promising results, their reliance on numerous fitness\nevaluations is too costly for our problem. Furthermore, most surrogate-assisted\nmethods face limitations due to their reliance on regression-based models. To\naddress these issues, we propose the RankNet-Inspired Surrogate-assisted Hybrid\nMetaheuristic (RI-SHM), an extension of our previous work. RI-SHM integrates\nthree key components: (1) a RankNet-based pairwise global surrogate that\ninnovatively predicts rankings between pairs of individuals, bypassing the\nchallenges of fitness estimation in discontinuous solution space; (2) a\nsurrogate-assisted local Estimation of Distribution Algorithm that enhances\nlocal exploitation and helps escape from local optima; and (3) a fitness\ndiversity-driven switching strategy that dynamically balances exploration and\nexploitation. Experiments demonstrate that our algorithm can effectively handle\nlarge-scale coverage optimization tasks of up to 300 dimensions and more than\n1,800 targets within desirable runtime. Compared to state-of-the-art algorithms\nfor EMVOPs, RI-SHM consistently outperforms them by up to 56.5$\\%$ across all\ntested instances.",
        "Several practical applications of evolutionary computation possess objective\nfunctions that receive the design variables and externally given parameters.\nSuch problems are termed contextual optimization problems. These problems\nrequire finding the optimal solutions corresponding to the given context\nvectors. Existing contextual optimization methods train a policy model to\npredict the optimal solution from context vectors. However, the performance of\nsuch models is limited by their representation ability. By contrast, warm\nstarting methods have been used to initialize evolutionary algorithms on a\ngiven problem using the optimization results on similar problems. Because warm\nstarting methods do not consider the context vectors, their performances can be\nimproved on contextual optimization problems. Herein, we propose a covariance\nmatrix adaptation evolution strategy with contextual warm starting (CMA-ES-CWS)\nto efficiently optimize the contextual optimization problem with a given\ncontext vector. The CMA-ES-CWS utilizes the optimization results of past\ncontext vectors to train the multivariate Gaussian process regression.\nSubsequently, the CMA-ES-CWS performs warm starting for a given context vector\nby initializing the search distribution using posterior distribution of the\nGaussian process regression. The results of the numerical simulation suggest\nthat CMA-ES-CWS outperforms the existing contextual optimization and warm\nstarting methods.",
        "We explore the interplay between sovereign debt default\/renegotiation and\nenvironmental factors (e.g., pollution from land use, natural resource\nexploitation). Pollution contributes to the likelihood of natural disasters and\ninfluences economic growth rates. The country can default on its debt at any\ntime while also deciding whether to invest in pollution abatement. The\nframework provides insights into the credit spreads of sovereign bonds and\nexplains the observed relationship between bond spread and a country's climate\nvulnerability. Through calibration for developing and low-income countries, we\ndemonstrate that there is limited incentive for these countries to address\nclimate risk, and the sensitivity of bond spreads to climate vulnerability\nremains modest. Climate risk does not play a relevant role on the decision to\ndefault on sovereign debt. Financial support for climate abatement expenditures\ncan effectively foster climate adaptation actions, instead renegotiation\nconditional upon pollution abatement does not produce any effect.",
        "Vision-Language Models (VLMs) have recently shown promising advancements in\nsequential decision-making tasks through task-specific fine-tuning. However,\ncommon fine-tuning methods, such as Supervised Fine-Tuning (SFT) and\nReinforcement Learning (RL) techniques like Proximal Policy Optimization (PPO),\npresent notable limitations: SFT assumes Independent and Identically\nDistributed (IID) data, while PPO focuses on maximizing cumulative rewards.\nThese limitations often restrict solution diversity and hinder generalization\nin multi-step reasoning tasks. To address these challenges, we introduce a\nnovel framework, GFlowVLM, a framework that fine-tune VLMs using Generative\nFlow Networks (GFlowNets) to promote generation of diverse solutions for\ncomplex reasoning tasks. GFlowVLM models the environment as a non-Markovian\ndecision process, allowing it to capture long-term dependencies essential for\nreal-world applications. It takes observations and task descriptions as inputs\nto prompt chain-of-thought (CoT) reasoning which subsequently guides action\nselection. We use task based rewards to fine-tune VLM with GFlowNets. This\napproach enables VLMs to outperform prior fine-tuning methods, including SFT\nand RL. Empirical results demonstrate the effectiveness of GFlowVLM on complex\ntasks such as card games (NumberLine, BlackJack) and embodied planning tasks\n(ALFWorld), showing enhanced training efficiency, solution diversity, and\nstronger generalization capabilities across both in-distribution and\nout-of-distribution scenarios.",
        "This work considers simultaneous homogenization dimension reduction of a\nporoelastic model for thin fiber-reinforced hydrogels. The analysed medium is\ndefined as a two-component system consisting of a continuous fiber framework\nwith hydrogel inclusions arranged periodically throughout. The fibers are\nassumed to operate under quasi-stationary linear elasticity, whereas the\nhydrogel's hydromechanical behavior is represented using Biot's linear\nporoelasticity model. The asymptotic limit of the coupled system is established\nwhen the periodicity and thickness parameters are of the same order and tend to\nzero simultaneously, utilizing the re-scaling unfolding operator. It is\ndemonstrated that the limit displacement exhibits Kirchhoff-Love-type behavior\nthrough Griso's decomposition of plate displacements. Towards the end, a unique\nsolution for the macroscopic problem has been demonstrated.",
        "While attention-based models have demonstrated the remarkable ability of\nin-context learning, the theoretical understanding of how these models acquired\nthis ability through gradient descent training is still preliminary. Towards\nanswering this question, we study the gradient descent dynamics of multi-head\nlinear self-attention trained for in-context linear regression. We examine two\nparametrizations of linear self-attention: one with the key and query weights\nmerged as a single matrix (common in theoretical studies), and one with\nseparate key and query matrices (closer to practical settings). For the merged\nparametrization, we show the training dynamics has two fixed points and the\nloss trajectory exhibits a single, abrupt drop. We derive an analytical\ntime-course solution for a certain class of datasets and initialization. For\nthe separate parametrization, we show the training dynamics has exponentially\nmany fixed points and the loss exhibits saddle-to-saddle dynamics, which we\nreduce to scalar ordinary differential equations. During training, the model\nimplements principal component regression in context with the number of\nprincipal components increasing over training time. Overall, we characterize\nhow in-context learning abilities evolve during gradient descent training of\nlinear attention, revealing dynamics of abrupt acquisition versus progressive\nimprovements in models with different parametrizations.",
        "This paper considers a new fuzzy fractional differential variational\ninequality with Mittag-Leffler kernel of order $q \\in (1,2]$ comprising a fuzzy\nfractional differential inclusion with Mittag-Leffler kernel of order $q \\in\n(1,2]$ and a variational inequality in Euclidean spaces. The existence of\nsolutions for such a novel system is obtained under some mild conditions.",
        "Grothendieck-Verdier categories (also known as $\\ast$-autonomous categories)\ngeneralize rigid monoidal categories, with notable representation-theoretic\nexamples including categories of bimodules, modules over Hopf algebroids, and\nmodules over vertex operator algebras.\n  In this paper, we develop a surface-diagrammatic calculus for\nGrothendieck-Verdier categories, extending the string-diagrammatic calculus of\nJoyal and Street for rigid monoidal categories into a third dimension. This\nextension naturally arises from the non-invertibility of coherence data in\nGrothendieck-Verdier categories.\n  We show that key properties of Frobenius algebras in rigid monoidal\ncategories carry over to the Grothendieck-Verdier setting. Moreover, we\nintroduce higher Frobenius-Schur indicators for suitably finite $k$-linear\npivotal Grothendieck-Verdier categories and prove their invariance under\npivotal Frobenius linearly distributive equivalences.\n  The proofs are carried out using the surface-diagrammatic calculus. To\nfacilitate the verification of some of our results, we provide auxiliary files\nfor the graphical proof assistant homotopy.io.",
        "In this technical note we present a targeted maximum likelihood estimator\n(TMLE) for a previously studied target parameter that aims to transport an\naverage treatment effect (ATE) on a clinical outcome in a source population to\nwhat the ATE would have been in another target population. It is assumed that\none only observes baseline covariates in the target population, while we assume\nthat one can learn the conditional treatment effect on the outcome of interest\nin the source population. We also allow that one might observe only a subset of\nthe covariates in the target population while all covariates are measured in\nthe source population. We consider the case that the outcome is a clinical\noutcome at some future time point that is subject to missingness, or that our\noutcome of interest is a time to event that is subject to right-censoring. We\nderive the canonical gradients and present the corresponding TMLEs for these\ntwo cases.",
        "This paper explores the promising interplay between spiking neural networks\n(SNNs) and event-based cameras for privacy-preserving human action recognition\n(HAR). The unique feature of event cameras in capturing only the outlines of\nmotion, combined with SNNs' proficiency in processing spatiotemporal data\nthrough spikes, establishes a highly synergistic compatibility for event-based\nHAR. Previous studies, however, have been limited by SNNs' ability to process\nlong-term temporal information, essential for precise HAR. In this paper, we\nintroduce two novel frameworks to address this: temporal segment-based SNN\n(\\textit{TS-SNN}) and 3D convolutional SNN (\\textit{3D-SNN}). The\n\\textit{TS-SNN} extracts long-term temporal information by dividing actions\ninto shorter segments, while the \\textit{3D-SNN} replaces 2D spatial elements\nwith 3D components to facilitate the transmission of temporal information. To\npromote further research in event-based HAR, we create a dataset,\n\\textit{FallingDetection-CeleX}, collected using the high-resolution CeleX-V\nevent camera $(1280 \\times 800)$, comprising 7 distinct actions. Extensive\nexperimental results show that our proposed frameworks surpass state-of-the-art\nSNN methods on our newly collected dataset and three other neuromorphic\ndatasets, showcasing their effectiveness in handling long-range temporal\ninformation for event-based HAR.",
        "Blockchain technology has revolutionized industries by enabling secure and\ndecentralized transactions. However, the isolated nature of blockchain\necosystems hinders the seamless transfer of digital assets across different\nchains. Cross-chain bridges have emerged as vital web3 infrastructure to\naddress this challenge by facilitating interoperability between distinct\nblockchains. Cross-chain bridges remain vulnerable to various attacks despite\nsophisticated designs and security measures. The industry has experienced a\nsurge in bridge attacks, resulting in significant financial losses. The largest\nhack impacted Axie Infinity Ronin Bridge, with a loss of almost \\$600 million\nUSD. This paper analyzes recent cross-chain bridge hacks in 2022 and 2023 and\nexamines the exploited vulnerabilities. By understanding the attack nature and\nunderlying weaknesses, the paper aims to enhance bridge security and propose\npotential countermeasures. The findings contribute to developing industry-wide\nstandards for bridge security and operational resilience. Addressing the\nvulnerabilities and weaknesses exploited in recent cross-chain bridge hacks\nfosters trust and confidence in cross-chain interoperability.",
        "Reconfigurable Intelligent Surfaces (RISs) pose as a transformative\ntechnology to revolutionize the cellular architecture of Next Generation\n(NextG) Radio Access Networks (RANs). Previous studies have demonstrated the\ncapabilities of RISs in optimizing wireless propagation, achieving high\nspectral efficiency, and improving resource utilization. At the same time, the\ntransition to softwarized, disaggregated, and virtualized architectures, such\nas those being standardized by the O-RAN ALLIANCE, enables the vision of a\nreconfigurable Open RAN. In this work, we aim to integrate these technologies\nby studying how different resource allocation policies enhance the performance\nof RIS-assisted Open RANs. We perform a comparative analysis among various\nnetwork configurations and show how proper network optimization can enhance the\nperformance across the Enhanced Mobile Broadband (eMBB) and Ultra Reliable and\nLow Latency Communications (URLLC) network slices, achieving up to ~34%\nthroughput improvement. Furthermore, leveraging the capabilities of OpenRAN\nGym, we deploy an xApp on Colosseum, the world's largest wireless system\nemulator with hardware-in-the-loop, to control the Base Station (BS)'s\nscheduling policy. Experimental results demonstrate that RIS-assisted\ntopologies achieve high resource efficiency and low latency, regardless of the\nBS's scheduling policy.",
        "In this paper, we extend a theorem of To\\\"en and Vaqui\\'e to the\nnon-Archimedean and formal settings. More precisely, we prove that a smooth and\nproper rigid analytic variety is algebraizable if and only if its category of\nperfect complexes is smooth and proper. As a corollary, we deduce an analogous\nstatement for formal schemes and demonstrate that, in general, the bounded\nderived category of coherent sheaves on a formal scheme is not smooth.",
        "Probabilistic classifiers are central for making informed decisions under\nuncertainty. Based on the maximum expected utility principle, optimal decision\nrules can be derived using the posterior class probabilities and\nmisclassification costs. Yet, in practice only learned approximations of the\noracle posterior probabilities are available. In this work, we quantify the\nexcess risk (a.k.a. regret) incurred using approximate posterior probabilities\nin batch binary decision-making. We provide analytical expressions for\nmiscalibration-induced regret ($R^{\\mathrm{CL}}$), as well as tight and\ninformative upper and lower bounds on the regret of calibrated classifiers\n($R^{\\mathrm{GL}}$). These expressions allow us to identify regimes where\nrecalibration alone addresses most of the regret, and regimes where the regret\nis dominated by the grouping loss, which calls for post-training beyond\nrecalibration. Crucially, both $R^{\\mathrm{CL}}$ and $R^{\\mathrm{GL}}$ can be\nestimated in practice using a calibration curve and a recent grouping loss\nestimator. On NLP experiments, we show that these quantities identify when the\nexpected gain of more advanced post-training is worth the operational cost.\nFinally, we highlight the potential of multicalibration approaches as efficient\nalternatives to costlier fine-tuning approaches.",
        "Large language models (LLMs) face significant token efficiency bottlenecks in\ncode generation and logical reasoning tasks, a challenge that directly impacts\ninference cost and model interpretability. This paper proposes a formal\nframework based on symbolic compression,integrating combinatory logic,\ninformation-theoretic optimal encoding, and context-aware inference techniques\nto achieve a step-change improvement in token efficiency while preserving\nsemantic integrity. We establish a mathematical framework within a functional\nprogramming paradigm, derive the quantitative relationship between symbolic\ndensity and model interpretability, and propose a differentiable compression\nfactor metric to evaluate encoding efficiency. Furthermore, we leverage\nparameter-efficient fine-tuning (PEFT) techniques to achieve a low-cost\napplication of the GAEL language. Experimental results show that this method\nachieves a 78.3% token compression rate in code generation tasks while\nimproving logical traceability by 62% through structural explicitness. This\nresearch provides new theoretical tools for efficient inference in LLMs and\nopens a symbolic path for modelinterpretability research.",
        "We extend recent results of Genovese-Luca-Tzvetkov (2022) regarding the\nquasi-invariance of Gaussian measures under the flow of the periodic\nBenjamin-Ono-BBM (BO-BBM) equation to the full range where BO-BBM is globally\nwell-posed. The main difficulty is due to the critical nature of the dispersion\nwhich we overcome by combining the approach of Coe-Tolomeo (2024) with an\niteration argument due to Forlano-Tolomeo (2024) to obtain long-time higher\nintegrability bounds on the transported density.",
        "Understanding how large language models (LLMs) grasp the historical context\nof concepts and their semantic evolution is essential in advancing artificial\nintelligence and linguistic studies. This study aims to evaluate the\ncapabilities of various LLMs in capturing temporal dynamics of meaning,\nspecifically how they interpret terms across different time periods. We analyze\na diverse set of terms from multiple domains, using tailored prompts and\nmeasuring responses through both objective metrics (e.g., perplexity and word\ncount) and subjective human expert evaluations. Our comparative analysis\nincludes prominent models like ChatGPT, GPT-4, Claude, Bard, Gemini, and Llama.\nFindings reveal marked differences in each model's handling of historical\ncontext and semantic shifts, highlighting both strengths and limitations in\ntemporal semantic understanding. These insights offer a foundation for refining\nLLMs to better address the evolving nature of language, with implications for\nhistorical text analysis, AI design, and applications in digital humanities."
      ]
    }
  },
  {
    "id":2411.03156,
    "research_type":"applied",
    "start_id":"b6",
    "start_title":"High\u2010Throughput Discovery of Novel Cubic Crystal Materials Using Deep Generative Neural Networks",
    "start_abstract":"Abstract High\u2010throughput screening has become one of the major strategies for discovery novel functional materials. However, its effectiveness is severely limited by lack sufficient and diverse materials in current repositories such as open quantum database (OQMD). Recent progress deep learning have enabled generative that learn implicit chemical rules creating hypothetical with new compositions structures. models difficulty generating structurally diverse, chemically valid, stable Here we propose CubicGAN, a adversarial network (GAN) based neural model large scale design cubic When trained on 375 749 ternary from OQMD database, authors show able to not only rediscover most currently known but also generate structure prototypes. A total 506 been verified phonon dispersion calculation. Considering importance wide applications solar panels, GAN provides promising approach significantly expand existing repositories, enabling via screening. The crystal structures discovered are freely accessible at www.carolinamatdb.org .",
    "start_categories":[
      "cs.NE"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b7"
      ],
      "title":[
        "Physics guided deep learning for generative design of crystal materials with symmetry constraints"
      ],
      "abstract":[
        "Abstract Discovering new materials is a challenging task in science crucial to the progress of human society. Conventional approaches based on experiments and simulations are labor-intensive or costly with success heavily depending experts\u2019 heuristic knowledge. Here, we propose deep learning Physics Guided Crystal Generative Model (PGCGM) for efficient crystal material design high structural diversity symmetry. Our model increases generation validity by more than 700% compared FTCP, one latest structure generators 45% our previous CubicGAN model. Density Functional Theory (DFT) calculations used validate generated structures 1869 out 2000 successfully optimized deposited into Carolina Materials Database www.carolinamatdb.org , which 39.6% have negative formation energy 5.3% energy-above-hull less 0.25 eV\/atom, indicating their thermodynamic stability potential synthesizability."
      ],
      "categories":[
        "physics.comp-ph"
      ]
    },
    "list":{
      "title":[
        "CardSharp: A python library for generating MCNP6 input decks",
        "Exact Constraint of Density Functional Approximations at the\n  Semiclassical Limit",
        "Magnetic skyrmions embedded in a vortex",
        "Global physics-informed neural networks (GPINNs): from local point-wise\n  constraint to global nodal association",
        "Many-Body Coarse-Grained Molecular Dynamics with the Atomic Cluster\n  Expansion",
        "Monotone conservative strategies in data assimilation",
        "Application of the Pathline Method to the Aircraft Reactor Experiment",
        "High-Order Modulation Large MIMO Detector Based on Physics-Inspired\n  Methods",
        "An a-posteriori analysis of co-kurtosis PCA based dimensionality\n  reduction using a neural ODE solver",
        "A Batch Power Iteration Approach for the Iterative Quasi-Monte Carlo\n  Method Using a Randomized-Halton Sequence",
        "Micromagnetic Simulation and Optimization of Spin-Wave Transducers",
        "Discovering dense hydrogen solid at 1200K with deep variational free\n  energy approach",
        "DiffChip: Thermally Aware Chip Placement with Automatic Differentiation",
        "Inductive methods for counting number fields",
        "Strengthening the No-Go Theorem for QRNGs",
        "Social Influence Distorts Ratings in Online Interfaces",
        "Bounds for quasimodes with polynomially narrow bandwidth on surfaces of\n  revolution",
        "Semiclassical scar on tori in high dimension",
        "Decision from Suboptimal Classifiers: Excess Risk Pre- and\n  Post-Calibration",
        "Some limit theorems for locally stationary Hawkes processes",
        "Energy burdens of carbon lock-in in household heating transitions",
        "Perturbations of a minimal surface with triple junctions in\n  $\\mathbb{R}^2 \\times \\mathbb{S}^1$",
        "Transferable Foundation Models for Geometric Tasks on Point Cloud\n  Representations: Geometric Neural Operators",
        "Theoretical and Experimental Investigations of High-Performance\n  Sr2CoNbO6-delta Double Perovskite for IT-SOFC Cathode Applications",
        "Exploring quasar evolution with proximate molecular absorbers: Insights\n  from the kinematics of highly ionized nitrogen",
        "Any function I can actually write down is measurable, right?",
        "SyNPar: Synthetic Null Data Parallelism for High-Power False Discovery\n  Rate Control in High-Dimensional Variable Selection",
        "Strategizing with AI: Insights from a Beauty Contest Experiment"
      ],
      "abstract":[
        "A python library for the creation of MCNP6 input decks is described. The\nlibrary supports geometry generation with automatic assignment of surface\/facet\nnumbers, cell numbers, transform numbers and material numbers along with MCNP\nUniverses and FILL feature. Rectangular and Hexagonal Lattices are also\nsupported. A large material library is included. Support for a good selection\nof common sources and tallies is also provided. Cards or features which are\ncurrently not supported in the library can also be inserted as raw strings into\nthe output stream. Combining Python features like descriptively named\nvariables, functions and for loops with library functions provides an intuitive\nand parametric way to create, modify and maintain complicated geometries and\nsimulation models. The generated card deck also has human readable comments\nwhich makes it easy to read and relate back to the python source. Some support\nfor running MCNP, reading tallies and plotting is also provided.",
        "We introduce the semiclassical limit to electronic systems by taking the\nlimit $\\hbar\\rightarrow 0$ in the solution of Schr\\\"odinger equations. We show\nthat this limit is closely related to one type of strong correlation that is\nparticularly challenging from conventional multi-configurational perspective\nbut can be readily described through semiclassical analysis. Furthermore, by\nstudying the performance of density functional approximations (DFAs) in the\nsemiclassical limit, we find that mainstream DFAs have erroneous divergent\nenergy behaviors as $\\hbar \\rightarrow 0$, violating the exact constraint of\nfinite energy. Importantly, by making connection of the significantly\nunderestimated DFA energies of many strongly correlated transition-metal\ndiatomic molecules to their rather small estimated $\\hbar_{\\text{eff}}$, we\ndemonstrate the usefulness of our semiclassical analysis and its promise for\ninspiring better DFAs.",
        "Magnetic vortices and skyrmions represent two fundamental classes of\ntopological spin textures in ferromagnetic systems, distinguished by their\nunique stabilization mechanisms and degrees of freedom. Vortices, characterized\nby circular in-plane magnetization (chirality) and out-of-plane core\npolarization, naturally arise in confined geometries due to the interplay\nbetween exchange and dipolar interactions. In contrast, skyrmions typically\nrequire the Dzyaloshinskii-Moriya interaction for stabilization and exhibit\nfixed chirality-polarity relationships. Through micromagnetic simulations, we\nreveal that these seemingly distinct topological states can coexist, forming a\nnovel composite state termed the \\textit{n}-skyrmion vortex, which represents a\nskyrmion-embedded vortex state. These composite states possess quantized\ntopological charges $Q$ that follow the relation $Q_{\\text{total}} =\nQ_{\\text{vortex}} + nQ_{\\text{skyrmion}}$, where $n$ denotes the number of\nembedded skyrmions. Similar to vortices, these states exhibit independent\nchirality and polarity and are energetically degenerate.",
        "Recently, physics-informed neural networks (PINNs) and their variants have\ngained significant popularity as a scientific computing method for solving\npartial differential equations (PDEs), whereas accuracy is still its main\nshortcoming. Despite numerous development efforts, there is no literature\ndemonstrating that these methods surpass classic numerical algorithms in\nsolving the forward issue. In this paper, by analyzing the disparities between\nPINNs and traditional numerical methods based on mesh discretization, we\ninvestigate the underlying causes for the in adequate precision of PINNs and\nintroduce a novel approach named global physics-informed neural networks\n(GPINNs). Inspired by the crucial concept of global nodal association in\nconventional numerical algorithms, GPINNs leverages the prior field\ndistribution information from pre-trained PINNs to estimate the association\nweights between arbitrary nodes in space. GPINNs can not only be regarded as a\nmeshless approach but also be demonstrated, both theoretically and in practical\ncircumstances, to have the ability of second-order convergence when trained\nwith equidistant nodes. Overall, GPINNs may be seen as an ideal approach to\ninheriting the merits of scientific machine learning (SciML) and conventional\nnumerical computing, which also represent the first SciML algorithm to surpass\nstandard numerical methods in terms of accuracy.",
        "Molecular dynamics (MD) simulations provide detailed insight into\natomic-scale mechanisms but are inherently restricted to small spatio-temporal\nscales. Coarse-grained molecular dynamics (CGMD) techniques allow simulations\nof much larger systems over extended timescales. In theory, these techniques\ncan be quantitatively accurate, but common practice is to only target\nqualitatively correct behaviour of coarse-grained models. Recent advances in\napplying machine learning methodology in this setting are now being applied to\ncreate also quantitatively accurate CGMD models. We demonstrate how the Atomic\nCluster Expansion parameterization (Drautz, 2019) can be used in this task to\nconstruct highly efficient, interpretable and accurate CGMD models. We focus in\nparticular on exploring the role of many-body effects.",
        "This paper studies whether numerically preserving monotonic properties can\noffer modelling advantages in data assimilation, particularly when the signal\nor data is a realization of a stochastic partial differential equation (SPDE)\nor partial differential equation (PDE) with a monotonic property. We\ninvestigate the combination of stochastic Strong Stability Preserving (SSP)\ntime-stepping, nonlinear solving strategies and data assimilation. Experimental\nresults indicate that a particle filter whose ensemble members are solved\nmonotonically can increase forecast skill when the reference data (not\nnecessarily observations) also has a monotone property. Additionally, more\nadvanced techniques used to avoid the degeneracy of the filter\n(tempering-jittering) are shown to be compatible with a conservative monotone\napproach.",
        "In this work, a new numerical method for the transport of Delayed Neutron\nPrecursors (DNPs) is applied to the Aircraft Reactor Experiment (ARE). The\npathline method is based on the Method of Characteristics (MOC) and leverages\nthe pathlines of the liquid nuclear fuel to derive an integral form of the DNPs\nbalance equation. The method has previously been tested on the CNRS benchmark\nand in a simplified 2D geometry where turbulent diffusivity was significant\ncompared to advection. Here, the pathline method is applied to a real-world\nMolten Salt Reactor (MSR), the ARE. DNPs transport is implemented in the\nframework of the coupling between neutron transport solver\nAPOLLO3\\textregistered{} and computational fluid dynamics code TrioCFD, both\ndeveloped at the French Atomic and Energy Commission (CEA). The DNPs\nconcentration obtained with the pathline method were compared with those\npreviously computed by TrioCFD, highlighting the importance of recirculation of\nfission products. The L-7 experiment was also replicated to demonstrate the\nmethod's capability.",
        "Applying quantum annealing or current quantum-\/physics-inspired algorithms\nfor MIMO detection always abandon the direct gray-coded bit-to-symbol mapping\nin order to obtain Ising form, leading to inconsistency errors. This often\nresults in slow convergence rates and error floor, particularly with high-order\nmodulations. We propose HOPbit, a novel MIMO detector designed to address this\nissue by transforming the MIMO detection problem into a higher-order\nunconstrained binary optimization (HUBO) problem while maintaining gray-coded\nbit-to-symbol mapping. The method then employs the simulated probabilistic bits\n(p-bits) algorithm to directly solve HUBO without degradation. This innovative\nstrategy enables HOPbit to achieve rapid convergence and attain near-optimal\nmaximum-likelihood performance in most scenarios, even those involving\nhigh-order modulations. The experiments show that HOPbit surpasses ParaMax by\nseveral orders of magnitude in terms of bit error rate (BER) in the context of\n12-user massive and large MIMO systems even with computing resources. In\naddition, HOPbit achieves lower BER rates compared to other traditional\ndetectors.",
        "A low-dimensional representation of thermochemical scalars based on\ncokurtosis principal component analysis (CoK-PCA) has been shown to effectively\ncapture stiff chemical dynamics in reacting flows relative to the widely used\nprincipal component analysis (PCA). The effectiveness of the reduced manifold\nwas evaluated in a priori analyses using both linear and nonlinear\nreconstructions of thermochemical scalars from aggressively truncated principal\ncomponents (PCs). In this study, we demonstrate the efficacy of a CoK-PCA-based\nreduced manifold using a posteriori analysis. Simulations of spontaneous\nignition in a homogeneous reactor that pose a challenge in accurately capturing\nthe ignition delay time as well as the scalar profiles within the reaction zone\nare considered. The governing ordinary differential equations (ODEs) in the PC\nspace were evolved from the initial conditions using two ODE solvers. First, a\nstandard ODE solver that uses a pre-trained artificial neural network (ANN) to\nestimate the source terms and integrates the solution in time. Second, a neural\nODE solver that incorporates the time integration of PCs into the ANN training.\nThe time-evolved profiles of the PCs and reconstructed thermochemical scalars\ndemonstrate the robustness of the CoK-PCA-based low-dimensional manifold in\naccurately capturing the ignition process. Furthermore, we observed that the\nneural ODE solver minimized propagation errors across time steps and provided\nmore accurate results than the standard ODE solver. The results of this study\ndemonstrate the potential of CoK-PCA-based manifolds to be implemented in\nmassively parallel reacting flow solvers.",
        "The Iterative Quasi-Monte Carlo (iQMC) method is a recently developed hybrid\nmethod for neutron transport simulations. iQMC replaces standard quadrature\ntechniques used in deterministic linear solvers with Quasi-Monte Carlo\nsimulation for accurate and efficient solutions to the neutron transport\nequation. Previous iQMC studies utilized a fixed-seed approach wherein\nparticles were reset to the same initial position and direction of travel at\nthe start of every transport sweep. While the QMC samples offered greatly\nimproved uniformity compared to pseudo-random samples, the fixed-seed approach\nmeant that some regions of the problem were under-sampled and resulted in\nerrors similar to ray effects observed in discrete ordinates methods.\n  This work explores using randomized-Quasi Monte Carlo techniques (RQMC) to\ngenerate unique sets of QMC samples for each transport sweep and gain a\nmuch-improved sampling of the phase space. The use of RQMC introduces some\nstochastic noise to iQMC's iterative process, which was previously absent. To\ncompensate, we adopt a ``batch'' approach similar to typical Monte Carlo\nk-eigenvalue problems, where the iQMC source is converged over\n$N_\\text{inactive}$ batches, then results from $N_\\text{active}$ batches are\nrecorded and used to calculate the average and standard deviation of the\nsolution.\n  The RQMC batch method was implemented in the Monte Carlo Dynamic Code (MC\/DC)\nand is shown to be a large improvement over the fixed-seed method. The batch\nmethod was able to provide iteratively stable and more accurate solutions with\nnearly two orders of magnitude reduction in the number of particle histories\nper batch. Notably, despite introducing some stochastic noise to the solution,\nthe RQMC batch approach converges both the k-effective and mean scalar flux\nerror at the theoretical QMC convergence rate of $O(N^{-1})$.",
        "The increasing demand for higher data volume and faster transmission in\nmodern wireless telecommunication systems has elevated requirements for 5G\nhigh-band RF hardware. Spin-Wave technology offers a promising solution, but\nits adoption is hindered by significant insertion loss stemming from the low\nefficiency of magnonic transducers. This work introduces a micromagnetic\nsimulation method for directly computing the spin-wave resistance, the real\npart of spin-wave impedance, which is crucial for optimizing magnonic\ntransducers. By integrating into finite-difference micromagnetic simulations,\nthis approach extends analytical models to arbitrary transducer geometries. We\ndemonstrate its effectiveness through parameter studies on transducer design\nand waveguide properties, identifying key strategies to enhance the overall\ntransducer efficiency. Our studies show that by varying single parameters of\nthe transducer geometry or the YIG thickness, the spin-wave efficiency, the\nparameter describing the efficiency of the transfer of electromagnetic energy\nto the spin wave, can reach values up to 0.75. The developed numerical model\nallows further fine-tuning of the transducers to achieve even higher\nefficiencies.",
        "We perform deep variational free energy calculations to investigate the dense\nhydrogen system at 1200 K and high pressures. In this computational framework,\nneural networks are used to model the free energy through the proton Boltzmann\ndistribution and the electron wavefunction. By directly minimizing the free\nenergy, our results reveal the emergence of a crystalline order associated with\nthe center of mass of hydrogen molecules at approximately 180 GPa. This\ntransition from atomic liquid to a molecular solid is marked by discontinuities\nin both the pressure and thermal entropy. Additionally, we discuss the broader\nimplications and limitations of these findings in the context of recent studies\nof dense hydrogen under similar conditions.",
        "Chiplets are modular integrated circuits that can be combined to form a\nlarger system, offering flexibility and performance enhancements. However,\ntheir dense packing often leads to significant thermal management challenges,\nrequiring careful floorplanning to ensure efficient heat distribution. To\naddress thermal considerations, layout optimization algorithms concurrently\nminimize the total wirelength and the maximum temperature. However, these\nefforts employ gradient-free approaches, such as simulated annealing, which\nsuffer from poor scaling and slow convergence. In this paper, we propose\nDiffChip, a chiplet placement algorithm based on automatic differentiation\n(AD). The proposed framework relies on a differentiable thermal solver that\ncomputes the sensitivity of the temperature map with respect to the positions\nof the chiplets. Regularization strategies for peak temperature, heat sources,\nand material properties enable end-to-end differentiability, allowing for\ngradient-based optimization. We apply DiffChip to optimize a layout where the\ntotal wirelength is minimized while keeping the maximum temperature below a\ndesired threshold. By leveraging AD and physics-aware optimization, our\napproach accelerates the design process of microelectronic systems, exceeding\ntraditional trial-and-error and gradient-free methods.",
        "We give a new method for counting extensions of a number field asymptotically\nby discriminant, which we employ to prove many new cases of Malle's Conjecture\nand counterexamples to Malle's Conjecture. We consider families of extensions\nwhose Galois closure is a fixed permutation group $G$. Our method relies on\nhaving asymptotic counts for $T$-extensions for some normal subgroup $T$ of\n$G$, uniform bounds for the number of such $T$-extensions, and possibly weak\nbounds on the asymptotic number of $G\/T$-extensions. However, we do not require\nthat most $T$-extensions of a $G\/T$-extension are $G$-extensions. Our new\nresults use $T$ either abelian or $S_3^m$, though our framework is general.",
        "Quantum random numbers are essential for security against quantum algorithms.\nRandomness as a beacon is a service being provided for companies and\ngovernments to upgrade their security standards from RSA to PQC - QKD or\nPQC-RSA protocols. Both security mechanisms assume trust in the service\nprovider unless one aims for device-independent protocols. How does an entity\nensure that the beacon service has a quantum signature other than relying on\nfaith? Specifically, given a bit-stream, can a user verify a quantum signature\nin it? Researchers claim this is indecipherable and have stated a no-go theorem\nfor post-processed bit-streams. In this article, we corroborate the results of\nthe no-go theorem while discussing its nuances using two different random\nnumber generators and four test methods. These include the NIST statistical\ntest suite and machine learning algorithms that strengthen the theorem. This\nwork is relevant for companies and governments using QRNG OpenAPI to enhance\nsecurity against quantum threats.",
        "Theoretical work on sequential choice and large-scale experiments in online\nranking and voting systems has demonstrated that social influence can have a\ndrastic impact on social and technological systems. Yet, the effect of social\ninfluence on online rating systems remains understudied and the few existing\ncontributions suggest that online ratings would self-correct given enough\nusers. Here, we propose a new framework for studying the effect of social\ninfluence on online ratings. We start from the assumption that people are\ninfluenced linearly by the observed average rating, but postulate that their\npropensity to be influenced varies. When the weight people assign to the\nobserved average depends only on their own latent rating, the resulting system\nis linear, but the long-term rating may substantially deviate from the true\nmean rating. When the weight people put on the observed average depends on both\ntheir own latent rating and the observed average rating, the resulting system\nis non-linear, and may support multiple equilibria, suggesting that ratings\nmight be path-dependent and deviations dramatic. Our results highlight\npotential limitations in crowdsourced information aggregation and can inform\nthe design of more robust online rating systems.",
        "Given a compact surface of revolution with Laplace-beltrami operator\n$\\Delta$, we consider the spectral projector $P_{\\lambda,\\delta}$ on a\npolynomially narrow frequency interval $[\\lambda-\\delta,\\lambda + \\delta]$,\nwhich is associated to the self-adjoint operator $\\sqrt{-\\Delta}$. For a large\nclass of surfaces of revolution, and after excluding small disks around the\npoles, we prove that the $L^2 \\to L^{\\infty}$ norm of $P_{\\lambda,\\delta}$ is\nof order $\\lambda^{\\frac{1}{2}} \\delta^{\\frac{1}{2}}$ up to $\\delta \\geq\n\\lambda^{-\\frac{1}{32}}$. We adapt the microlocal approach introduced by Sogge\nfor the case $\\delta = 1$, by using the Quantum Completely Integrable structure\nof surfaces of revolution introduced by Colin de Verdi\\`ere. This reduces the\nanalysis to a number of estimates of explicit oscillatory integrals, for which\nwe introduce new quantitative tools.",
        "We show that the eigenfunctions of the self-adjoint elliptic $h-$differential\noperator $P_{h}(t)$ exhibits semiclassical scar phenomena on the\n$d-$dimensional torus, under the $\\sigma$-Bruno-R\\\"{u}ssmann condition, instead\nof the Diophantine one. Its equivalence is described as: for almost all\nperturbed Hamiltonian's KAM Lagrangian tori $\\Lambda_{\\omega}$, there exists a\nsemiclassical measure with positive mass on $\\Lambda_{\\omega}$. The premise is\nthat we can obatain a family of quasimodes for the $h-$differential operator\n$P_{h}(t)$ in the semiclassical limit as $h\\rightarrow0$, under the\n$\\sigma$-Bruno-R\\\"{u}ssmann condition.",
        "Probabilistic classifiers are central for making informed decisions under\nuncertainty. Based on the maximum expected utility principle, optimal decision\nrules can be derived using the posterior class probabilities and\nmisclassification costs. Yet, in practice only learned approximations of the\noracle posterior probabilities are available. In this work, we quantify the\nexcess risk (a.k.a. regret) incurred using approximate posterior probabilities\nin batch binary decision-making. We provide analytical expressions for\nmiscalibration-induced regret ($R^{\\mathrm{CL}}$), as well as tight and\ninformative upper and lower bounds on the regret of calibrated classifiers\n($R^{\\mathrm{GL}}$). These expressions allow us to identify regimes where\nrecalibration alone addresses most of the regret, and regimes where the regret\nis dominated by the grouping loss, which calls for post-training beyond\nrecalibration. Crucially, both $R^{\\mathrm{CL}}$ and $R^{\\mathrm{GL}}$ can be\nestimated in practice using a calibration curve and a recent grouping loss\nestimator. On NLP experiments, we show that these quantities identify when the\nexpected gain of more advanced post-training is worth the operational cost.\nFinally, we highlight the potential of multicalibration approaches as efficient\nalternatives to costlier fine-tuning approaches.",
        "We prove a law of large numbers and functional central limit theorem for a\nclass of multivariate Hawkes processes with time-dependent reproduction rate.\nWe address the difficulties induced by the use of non-convolutive Volterra\nprocesses by recombining classical martingale methods introduced in Bacry et\nal. [3] with novel ideas proposed by Kwan et al. [19]. The asymptotic theory we\nobtain yields useful applications in financial statistics. As an illustration,\nwe derive closed-form expressions for price distortions under liquidity\nconstraints.",
        "Heating electrification presents opportunities and challenges for energy\naffordability. Without careful planning and policy, the costs of natural gas\nservice will be borne by a shrinking customer base, driving up expenses for\nthose who are left behind. This affordability issue is worsened by new fossil\nfuel investments, which risk locking communities into carbon-intensive\ninfrastructure. Here, we introduce a framework to quantify the distributional\neffects of natural gas phasedown on energy affordability, integrating detailed\nhousehold data with utility financial and planning documents. Applying our\nframework first to Massachusetts and then nationwide, we show that vulnerable\ncommunities face disproportionate affordability risks in building energy\ntransitions. Households that do not electrify may bear up to 50% higher energy\ncosts over the next decade. Targeted electrification may help to alleviate\nimmediate energy burdens, but household heating transitions will ultimately\nrequire coordinated, neighborhood-scale strategies that consider the high fixed\ncosts of legacy infrastructure.",
        "We construct stationary perturbations of a specific minimal surface with a\ncircle of triple junctions in $\\mathbb{R}^2 \\times \\mathbb{S}^1$, that satisfy\ngiven boundary data.",
        "We introduce methods for obtaining pretrained Geometric Neural Operators\n(GNPs) that can serve as basal foundation models for use in obtaining geometric\nfeatures. These can be used within data processing pipelines for machine\nlearning tasks and numerical methods. We show how our GNPs can be trained to\nlearn robust latent representations for the differential geometry of\npoint-clouds to provide estimates of metric, curvature, and other shape-related\nfeatures. We demonstrate how our pre-trained GNPs can be used (i) to estimate\nthe geometric properties of surfaces of arbitrary shape and topologies with\nrobustness in the presence of noise, (ii) to approximate solutions of geometric\npartial differential equations (PDEs) on manifolds, and (iii) to solve\nequations for shape deformations such as curvature driven flows. We also\nrelease a package of the codes and weights for using our pre-trained GNPs for\nprocessing point cloud representations. This allows for incorporating our\npre-trained GNPs as components for reuse within existing and new data\nprocessing pipelines. The GNPs also can be used as part of numerical solvers\ninvolving geometry or as part of methods for performing inference and other\ngeometric tasks.",
        "Enhancing the transport of oxygen anions in the cathode while maintaining\nsurface stability is essential for improving the performance of\nintermediate-temperature solid oxide fuel cells (IT-SOFCs). This study\ninvestigates a novel cathode material candidate, Sr2CoNbO6-delta (SCNO), using\ndensity functional theory, molecular dynamics, and experimental\ncharacterization. The redox active Co cation at B-site and less reducible Nb\ncation at the B'-site together enhance both surface stability and\nelectrocatalytic performance. SCNO is observed to have a higher concentration\nof oxygen vacancies and increased oxygen diffusivity on the surface. The\nsurface stability of SCNO is further improved when simulated under compressive\nstrain due to the GDC electrolyte substrate. These findings offer new insights\ninto controlling Sr segregation in SCNO, contributing to a better understanding\nof its enhanced oxygen reduction reaction (ORR) activity and high surface\nstability. Subsequently, SCNO was synthesized to evaluate its potential as a\ncathode material in SOFCs. To assess its performance, symmetric cells with\nuniform dense thin films of varying thicknesses (40 and 80 nm) were fabricated\nusing the pulsed laser deposition technique. Electrochemical impedance\nspectroscopy and distributed relaxation time analysis indicate that bulk oxygen\nion diffusion is a limiting factor for the ORR in SCNO. The polarization\nresistance for the 40 and 80 nm dense thin film symmetric cells ranged between\n0.329 - 0.241 ohm cm2 and 1.095 - 0.438 ohm cm2, respectively, within the\ntemperature range of 773 - 973 K in an air atmosphere. The full cell\nconfiguration of NiO-GDC|GDC|SCNO demonstrated a significantly high peak power\ndensity of 0.633 W\/cm2 at 973 K. This theory-guided design and experimental\nstudy suggest that SCNO is a promising candidate for IT-SOFC cathode materials.",
        "We investigate the presence and kinematics of NV absorption proximate to high\nredshift quasars selected upon the presence of strong $H_{2}$ and HI absorption\nat the quasar redshift. Our spectroscopic observations with X-shooter at the\nVLT reveal a 70% detection rate of NV (9 of 13 quasars with 2.5 < z < 3.3),\nremarkably higher than the 10% detection rate in intervening DLA systems and\nthe 30% rate observed within a few thousand km\/s of the source in the general\nquasar population. While many NV components lie within the velocity range of\nthe neutral gas, the kinematic profiles of high-ionization species appear\ndecoupled from those of low-ionization species, with the former extending over\nmuch larger velocity ranges, particularly towards bluer velocities. We also\nobserve significant variations in the NV\/SiIV, which we attribute to varying\nionization conditions, with a velocity-dependent trend: blueshifted NV\ncomponents systematically exhibit higher ionization parameters compared to\nthose near the quasar's systemic redshift. Furthermore, the most redshifted\nsystems relative to the quasar show no evidence of NV absorption. The results\nsuggest that proximate $H_{2}$ absorption systems select critical stages of\nquasar evolution, during which the quasar remains embedded in a rich molecular\nenvironment. Redshifted systems trace infalling gas, potentially associated\nwith mergers, preceding the onset of outflows. Such outflows may reach or even\ncarry out neutral and molecular gas.This latter stage would correspond to\nproximate $H_{2}$ systems located around or blueshifted relative to the\nquasar's systemic z. Finally, the only case in our sample featuring highly\nblueshifted neutral gas shows no evidence of an association with the quasar.Our\nfindings highlight the need to account for the ionization state when defining a\nvelocity threshold to distinguish quasar-associated systems from intervening.",
        "In this expository paper aimed at a general mathematical audience, we discuss\nhow to combine certain classic theorems of set-theoretic inner model theory and\neffective descriptive set theory with work on Hilbert's tenth problem and\nuniversal Diophantine equations to produce the following surprising result:\nThere is a specific polynomial $p(x,y,z,n,k_1,\\dots,k_{70})$ of degree $7$ with\ninteger coefficients such that it is independent of $\\mathsf{ZFC}$ (and much\nstronger theories) whether the function $$f(x) = \\inf_{y \\in \\mathbb{R}}\\sup_{z\n\\in \\mathbb{R}}\\inf_{n \\in \\mathbb{N}}\\sup_{\\bar{k} \\in\n\\mathbb{N}^{70}}p(x,y,z,n,\\bar{k})$$ is Lebesgue measurable. We also give\nsimilarly defined $g(x,y)$ with the property that the statement \"$x \\mapsto\ng(x,r)$ is measurable for every $r \\in \\mathbb{R}$\" has large cardinal\nconsistency strength (and in particular implies the consistency of\n$\\mathsf{ZFC}$) and $h(m,x,y,z)$ such that $h(1,x,y,z),\\dots,h(16,x,y,z)$ can\nconsistently be the indicator functions of a Banach$\\unicode{x2013}$Tarski\nparadoxical decomposition of the sphere.\n  Finally, we discuss some situations in which measurability of analogously\ndefined functions can be concluded by inspection, which touches on\nmodel-theoretic o-minimality and the fact that sufficiently strong large\ncardinal hypotheses (such as Vop\\v{e}nka's principle and much weaker\nassumptions) imply that all 'reasonably definable' functions (including the\nabove $f(x)$, $g(x,y)$, and $h(m,x,y,z)$) are universally measurable.",
        "Balancing false discovery rate (FDR) and statistical power to ensure reliable\ndiscoveries is a key challenge in high-dimensional variable selection. Although\nseveral FDR control methods have been proposed, most involve perturbing the\noriginal data, either by concatenating knockoff variables or splitting the data\ninto two halves, both of which can lead to a loss of power. In this paper, we\nintroduce a novel approach called Synthetic Null Parallelism (SyNPar), which\ncontrols the FDR in high-dimensional variable selection while preserving the\noriginal data. SyNPar generates synthetic null data from a model fitted to the\noriginal data and modified to reflect the null hypothesis. It then applies the\nsame estimation procedure in parallel to both the original and synthetic null\ndata to estimate coefficients that indicate feature importance. By comparing\nthe coefficients estimated from the null data with those from the original\ndata, SyNPar effectively identifies false positives, functioning as a numerical\nanalog of a likelihood ratio test. We provide theoretical guarantees for FDR\ncontrol at any desired level while ensuring that the power approaches one with\nhigh probability asymptotically. SyNPar is straightforward to implement and can\nbe applied to a wide range of statistical models, including high-dimensional\nlinear regression, generalized linear models, Cox models, and Gaussian\ngraphical models. Through extensive simulations and real data applications, we\ndemonstrate that SyNPar outperforms state-of-the-art methods, including\nknockoffs and data-splitting methods, in terms of FDR control, power, and\ncomputational efficiency.",
        "A Keynesian beauty contest is a wide class of games of guessing the most\npopular strategy among other players. In particular, guessing a fraction of a\nmean of numbers chosen by all players is a classic behavioral experiment\ndesigned to test iterative reasoning patterns among various groups of people.\nThe previous literature reveals that the level of sophistication of the\nopponents is an important factor affecting the outcome of the game. Smarter\ndecision makers choose strategies that are closer to theoretical Nash\nequilibrium and demonstrate faster convergence to equilibrium in iterated\ncontests with information revelation. We replicate a series of classic\nexperiments by running virtual experiments with modern large language models\n(LLMs) who play against various groups of virtual players. We test how advanced\nthe LLMs' behavior is compared to the behavior of human players. We show that\nLLMs typically take into account the opponents' level of sophistication and\nadapt by changing the strategy. In various settings, most LLMs (with the\nexception of Llama) are more sophisticated and play lower numbers compared to\nhuman players. Our results suggest that LLMs (except Llama) are rather\nsuccessful in identifying the underlying strategic environment and adopting the\nstrategies to the changing set of parameters of the game in the same way that\nhuman players do. All LLMs still fail to play dominant strategies in a\ntwo-player game. Our results contribute to the discussion on the accuracy of\nmodeling human economic agents by artificial intelligence."
      ]
    }
  },
  {
    "id":2411.05055,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems",
    "start_abstract":"TensorFlow is an interface for expressing machine learning algorithms, and implementation executing such algorithms. A computation expressed using can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices as phones tablets up to large-scale distributed systems hundreds machines thousands computational GPU cards. The system flexible used express including training inference algorithms deep neural network models, it has been conducting research deploying into production across more than dozen areas computer science other fields, speech recognition, vision, robotics, information retrieval, natural language processing, geographic extraction, drug discovery. This paper describes the that we have built at Google. API reference were released open-source package under Apache 2.0 license in November, 2015 are available www.tensorflow.org.",
    "start_categories":[
      "cs.CL"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "Predicting Functional Effect of Human Missense Mutations Using PolyPhen\u20102"
      ],
      "abstract":[
        "Abstract PolyPhen\u20102 (Polymorphism Phenotyping v2), available as software and via a Web server, predicts the possible impact of amino acid substitutions on stability function human proteins using structural comparative evolutionary considerations. It performs functional annotation single\u2010nucleotide polymorphisms (SNPs), maps coding SNPs to gene transcripts, extracts protein sequence annotations attributes, builds conservation profiles. then estimates probability missense mutation being damaging based combination all these properties. features include high\u2010quality multiple alignment pipeline prediction method employing machine\u2010learning classification. The also integrates UCSC Genome Browser's genome MultiZ alignments vertebrate genomes with genome. is capable analyzing large volumes data produced by next\u2010generation sequencing projects, thanks built\u2010in support for high\u2010performance computing environments like Grid Engine Platform LSF. Curr. Protoc. Hum. Genet . 76:7.20.1\u20107.20.41. \u00a9 2013 John Wiley &amp; Sons, Inc."
      ],
      "categories":[
        "q-bio.GN"
      ]
    },
    "list":{
      "title":[
        "GiantHunter: Accurate detection of giant virus in metagenomic data using\n  reinforcement-learning and Monte Carlo tree search",
        "Genomic Analysis of Date Palm Fruit Size Traits and Identification of\n  Candidate Genes through GWAS",
        "Find Central Dogma Again: Leveraging Multilingual Transfer in Large\n  Language Models",
        "TopoLa: A Universal Framework to Enhance Cell Representations for\n  Single-cell and Spatial Omics through Topology-encoded Latent Hyperbolic\n  Geometry",
        "Eukaryotes evade information storage-replication rate trade-off with\n  endosymbiont assistance leading to larger genomes",
        "Transcriptome signature for the identification of bevacizumab responders\n  in ovarian cancer",
        "Genotype-to-Phenotype Prediction in Rice with High-Dimensional Nonlinear\n  Features",
        "CoverM: Read alignment statistics for metagenomics",
        "Deep Learning-based Feature Discovery for Decoding Phenotypic Plasticity\n  in Pediatric High-Grade Gliomas Single-Cell Transcriptomics",
        "Causes of evolutionary divergence in prostate cancer",
        "Origin of $\\alpha$-satellite repeat arrays from mitochondrial molecular\n  fossils -- sequential insertion, expansion, and evolution in the nuclear\n  genome",
        "Learnable Group Transform: Enhancing Genotype-to-Phenotype Prediction\n  for Rice Breeding with Small, Structured Datasets",
        "Curated loci prime editing (cliPE) for accessible multiplexed assays of\n  variant effect (MAVEs)",
        "Non-uniqueness of normalized NLS ground states on bounded domains with\n  homogeneous Neumann boundary conditions",
        "Drinfeld modules with maximal Galois action",
        "Toughness of double network hydrogels: the role of reduced stress\n  propagation",
        "Traffic noise assessment in urban Bulgaria using explainable machine\n  learning",
        "Quadratic BSDEs with Singular Generators and Unbounded Terminal\n  Conditions: Theory and Applications",
        "Improved Online Confidence Bounds for Multinomial Logistic Bandits",
        "The three-dimensional impulse-response model: Modeling the training\n  process in accordance with energy system-specific adaptation",
        "Polarization-controlled strong light-matter interaction with templated\n  molecular aggregates",
        "Joint Power Allocation and Phase Shift Design for Stacked Intelligent\n  Metasurfaces-aided Cell-Free Massive MIMO Systems with MARL",
        "An exact closed walks series formula for the complexity of regular\n  graphs and some related bounds",
        "Transformer-Enhanced Variational Autoencoder for Crystal Structure\n  Prediction",
        "Experimental Realization of Special-Unitary Operations in Classical\n  Mechanics by Non-Adiabatic Evolutions",
        "Allostatic Control of Persistent States in Spiking Neural Networks for\n  perception and computation",
        "Multi-messenger detection of black hole binaries in dark matter spikes",
        "Steady compressible Navier-Stokes-Fourier system with general\n  temperature dependent viscosities I: density estimates based on Bogovskii\n  operator"
      ],
      "abstract":[
        "Motivation: Nucleocytoplasmic large DNA viruses (NCLDVs) are notable for\ntheir large genomes and extensive gene repertoires, which contribute to their\nwidespread environmental presence and critical roles in processes such as host\nmetabolic reprogramming and nutrient cycling. Metagenomic sequencing has\nemerged as a powerful tool for uncovering novel NCLDVs in environmental\nsamples. However, identifying NCLDV sequences in metagenomic data remains\nchallenging due to their high genomic diversity, limited reference genomes, and\nshared regions with other microbes. Existing alignment-based and machine\nlearning methods struggle with achieving optimal trade-offs between sensitivity\nand precision. Results: In this work, we present GiantHunter, a reinforcement\nlearning-based tool for identifying NCLDVs from metagenomic data. By employing\na Monte Carlo tree search strategy, GiantHunter dynamically selects\nrepresentative non-NCLDV sequences as the negative training data, enabling the\nmodel to establish a robust decision boundary. Benchmarking on rigorously\ndesigned experiments shows that GiantHunter achieves high precision while\nmaintaining competitive sensitivity, improving the F1-score by 10% and reducing\ncomputational cost by 90% compared to the second-best method. To demonstrate\nits real-world utility, we applied GiantHunter to 60 metagenomic datasets\ncollected from six cities along the Yangtze River, located both upstream and\ndownstream of the Three Gorges Dam. The results reveal significant differences\nin NCLDV diversity correlated with proximity to the dam, likely influenced by\nreduced flow velocity caused by the dam. These findings highlight the potential\nof GiantSeeker to advance our understanding of NCLDVs and their ecological\nroles in diverse environments.",
        "The commercial value of economically significant fruits, including date palm\nfruit (dates), is influenced by various factors, such as biochemical\ncomposition and morphological features like size, shape, and visual appearance,\nwhich are key determinants of their quality and market value. Dates are\ntypically consumed at the dry stage (Tamar), during which they exhibit a wide\nrange of physical characteristics, such as color, length, weight, and skin\nappearance. Understanding the genetic basis of these traits is crucial for\nimproving crop quality and breeding new cultivars. In this study, we integrated\na genome dataset from highly diverse date cultivars with phenotypes of dry\nfruit such as length, width, area, and weight, identifying multiple significant\ngenetic loci (SNPs) associated with these traits. We also identified candidate\ngenes located near the associated SNPs that are involved in biological\nprocesses such as cell differentiation, proliferation, growth, and the\nregulation of signalling pathways for growth regulators like auxin and abscisic\nacid, as observed in other plants. Gene expression analysis reveals that many\nof these genes are highly expressed in the early stage of fruit development\nwhen the fruit attains its maximum size and weight. These findings will enhance\nour understanding of genetic determinants of fruit size particularly at the\ncommercially important Tamar stage.",
        "In recent years, large language models (LLMs) have achieved state-of-the-art\nresults in various biological sequence analysis tasks, such as sequence\nclassification, structure prediction, and function prediction. Similar to\nadvancements in AI for other scientific fields, deeper research into biological\nLLMs has begun to focus on using these models to rediscover important existing\nbiological laws or uncover entirely new patterns in biological sequences. This\nstudy leverages GPT-like LLMs to utilize language transfer capabilities to\nrediscover the genetic code rules of the central dogma. In our experimental\ndesign, we transformed the central dogma into a binary classification problem\nof aligning DNA sequences with protein sequences, where positive examples are\nmatching DNA and protein sequences, and negative examples are non-matching\npairs. We first trained a GPT-2 model from scratch using a dataset comprising\nprotein sequences, DNA sequences, and sequences from languages such as English\nand Chinese. Subsequently, we fine-tuned the model using the natural language\nsentences similarity judgment dataset from PAWS-X. When tested on a dataset for\nDNA and protein sequence alignment judgment, the fine-tuned model achieved a\nclassification accuracy of 81%. The study also analyzed factors contributing to\nthis zero-shot capability, including model training stability and types of\ntraining data. This research demonstrates that LLMs can, through the transfer\nof natural language capabilities and solely relying on the analysis of\nsequences themselves, rediscover the central dogma without prior knowledge of\nit. This study bridges natural language and genetic language, opening a new\ndoor for AI-driven biological research.",
        "Recent advances in cellular research demonstrate that scRNA-seq characterizes\ncellular heterogeneity, while spatial transcriptomics reveals the spatial\ndistribution of gene expression. Cell representation is the fundamental issue\nin the two fields. Here, we propose Topology-encoded Latent Hyperbolic Geometry\n(TopoLa), a computational framework enhancing cell representations by capturing\nfine-grained intercellular topological relationships. The framework introduces\na new metric, TopoLa distance (TLd), which quantifies the geometric distance\nbetween cells within latent hyperbolic space, capturing the network's\ntopological structure more effectively. With this framework, the cell\nrepresentation can be enhanced considerably by performing convolution on its\nneighboring cells. Performance evaluation across seven biological tasks,\nincluding scRNA-seq data clustering and spatial transcriptomics domain\nidentification, shows that TopoLa significantly improves the performance of\nseveral state-of-the-art models. These results underscore the generalizability\nand robustness of TopoLa, establishing it as a valuable tool for advancing both\nbiological discovery and computational methodologies.",
        "Genome length varies widely among organisms, from compact genomes of\nprokaryotes to vast and complex genomes of eukaryotes. In this study, we\ntheoretically identify the evolutionary pressures that may have driven this\ndivergence in genome length. We use a parameter-free model to study genome\nlength evolution under selection pressure to minimize replication time and\nmaximize information storage capacity. We show that prokaryotes tend to reduce\ngenome length, constrained by a single replication origin, while eukaryotes\nexpand their genomes by incorporating multiple replication origins. We propose\na connection between genome length and cellular energetics, suggesting that\nendosymbiotic organelles, mitochondria and chloroplasts, evolutionarily\nregulate the number of replication origins, thereby influencing genome length\nin eukaryotes. We show that the above two selection pressures also lead to\nstrict equalization of the number of purines and their corresponding\nbase-pairing pyrimidines within a single DNA strand, known as Chagraff's second\nparity rule, a hitherto unexplained observation in genomes of nearly all known\nspecies. This arises from the symmetrization of replichore length, another\nobservation that has been shown to hold across species, which our model\nreproduces. The model also reproduces other experimentally observed phenomena,\nsuch as a general preference for deletions over insertions, and elongation and\nhigh variance of genome lengths under reduced selection pressure for\nreplication rate, termed the C-value paradox. We highlight the possibility of\nregulation of the firing of latent replication origins in response to cues from\nthe extracellular environment leading to the regulation of cell cycle rates in\nmulticellular eukaryotes.",
        "The standard of care for ovarian cancer comprises cytoreductive surgery,\nfollowed by adjuvant platinum-based chemotherapy plus taxane therapy and\nmaintenance therapy with the antiangiogenic compound bevacizumab and\/or a PARP\ninhibitor. Nevertheless, there is currently no clear clinical indication for\nthe use of bevacizumab, highlighting the urgent need for biomarkers to assess\nthe response to bevacizumab. In the present study, based on a novel RNA-seq\ndataset (n=181) and a previously published microarray-based dataset (n=377), we\nhave identified an expression signature potentially associated with benefit\nfrom bevacizumab addition and assumed to reflect cancer stemness acquisition\ndriven by activation of CTCFL. Patients with this signature demonstrated\nimproved overall survival when bevacizumab was added to standard chemotherapy\nin both novel (HR=0.41(0.23-0.74), adj.p-value=7.70e-03) and previously\npublished cohorts (HR=0.51(0.34-0.75), adj.p-value=3.25e-03), while no\nsignificant differences in survival explained by treatment were observed in\npatients negative for this signature. In addition to the CTCFL signature, we\nfound several other reproducible expression signatures which may also represent\nbiomarker candidates not related to established molecular subtypes of ovarian\ncancer and require further validation studies based on additional RNA-seq data.",
        "Genotype-to-Phenotype prediction can promote advances in modern genomic\nresearch and crop improvement, guiding precision breeding and genomic\nselection. However, high-dimensional nonlinear features often hinder the\naccuracy of genotype-to-phenotype prediction by increasing computational\ncomplexity. The challenge also limits the predictive accuracy of traditional\napproaches. Therefore, effective solutions are needed to improve the accuracy\nof genotype-to-phenotype prediction. In our paper, we propose MLFformer.\nMLFformer is a Transformer-based architecture that incorporates the Fast\nAttention mechanism and a multilayer perceptron module to handle\nhigh-dimensional nonlinear features. In MLFformer, the Fast Attention mechanism\nis utilized to handle computational complexity and enhance processing\nefficiency. In addition, the MLP structure further captures high-dimensional\nnonlinear features. Through experiments, the results show that MLFformer\nreduces the average MAPE by 7.73% compared to the vanilla Transformer. In\nunivariate and multivariate prediction scenarios, MLFformer achieves the best\npredictive performance among all compared models.",
        "Genome-centric analysis of metagenomic samples is a powerful method for\nunderstanding the function of microbial communities. Calculating read coverage\nis a central part of analysis, enabling differential coverage binning for\nrecovery of genomes and estimation of microbial community composition. Coverage\nis determined by processing read alignments to reference sequences of either\ncontigs or genomes. Per-reference coverage is typically calculated in an ad-hoc\nmanner, with each software package providing its own implementation and\nspecific definition of coverage. Here we present a unified software package\nCoverM which calculates several coverage statistics for contigs and genomes in\nan ergonomic and flexible manner. It uses 'Mosdepth arrays' for computational\nefficiency and avoids unnecessary I\/O overhead by calculating coverage\nstatistics from streamed read alignment results. CoverM is free software\navailable at https:\/\/github.com\/wwood\/coverm. CoverM is implemented in Rust,\nwith Python (https:\/\/github.com\/apcamargo\/pycoverm) and Julia\n(https:\/\/github.com\/JuliaBinaryWrappers\/CoverM_jll.jl) interfaces.",
        "By use of complex network dynamics and graph-based machine learning, we\nidentified critical determinants of lineage-specific plasticity across the\nsingle-cell transcriptomics of pediatric high-grade glioma (pHGGs) subtypes:\nIDHWT glioblastoma and K27M-mutant glioma. Our study identified network\ninteractions regulating glioma morphogenesis via the tumor-immune\nmicroenvironment, including neurodevelopmental programs, calcium dynamics, iron\nmetabolism, metabolic reprogramming, and feedback loops between MAPK\/ERK and\nWNT signaling. These relationships highlight the emergence of a hybrid spectrum\nof cellular states navigating a disrupted neuro-differentiation hierarchy. We\nidentified transition genes such as DKK3, NOTCH2, GATAD1, GFAP, and SEZ6L in\nIDHWT glioblastoma, and H3F3A, ANXA6, HES6\/7, SIRT2, FXYD6, PTPRZ1, MEIS1,\nCXXC5, and NDUFAB1 in K27M subtypes. We also identified MTRNR2L1, GAPDH, IGF2,\nFKBP variants, and FXYD7 as transition genes that influence cell fate\ndecision-making across both subsystems. Our findings suggest pHGGs are\ndevelopmentally trapped in states exhibiting maladaptive behaviors, and hybrid\ncellular identities. In effect, tumor heterogeneity (metastability) and\nplasticity emerge as stress-response patterns to immune-inflammatory\nmicroenvironments and oxidative stress. Furthermore, we show that pHGGs are\nsteered by developmental trajectories from radial glia predominantly favoring\nneocortical cell fates, in telencephalon and prefrontal cortex (PFC)\ndifferentiation. By addressing underlying patterning processes and plasticity\nnetworks as therapeutic vulnerabilities, our findings provide precision\nmedicine strategies aimed at modulating glioma cell fates and overcoming\ntherapeutic resistance. We suggest transition therapy toward neuronal-like\nlineage differentiation as a potential therapy to help stabilize pHGG\nplasticity and aggressivity.",
        "Cancer progression involves the sequential accumulation of genetic\nalterations that cumulatively shape the tumour phenotype. In prostate cancer,\ntumours can follow divergent evolutionary trajectories that lead to distinct\nsubtypes, but the causes of this divergence remain unclear. While causal\ninference could elucidate the factors involved, conventional methods are\nunsuitable due to the possibility of unobserved confounders and ambiguity in\nthe direction of causality. Here, we propose a method that circumvents these\nissues and apply it to genomic data from 829 prostate cancer patients. We\nidentify several genetic alterations that drive divergence as well as others\nthat prevent this transition, locking tumours into one trajectory. Further\nanalysis reveals that these genetic alterations may cause each other, implying\na positive-feedback loop that accelerates divergence. Our findings provide\ninsights into how cancer subtypes emerge and offer a foundation for genomic\nsurveillance strategies aimed at monitoring the progression of prostate cancer.",
        "Alpha satellite DNA is large tandem arrays of 150-400 bp units, and its\norigin remains an evolutionary mystery. In this research, we identified 1,545\nalpha-satellite-like (SatL) repeat units in the nuclear genome of jewel wasp\nNasonia vitripennis. Among them, thirty-nine copies of SatL were organized in\ntwo palindromic arrays in mitochondria, resulting in a 50% increase in the\ngenome size. Strikingly, genomic neighborhood analyses of 1,516 nuclear SatL\nrepeats revealed that they are located in NuMT (nuclear mitochondrial DNA)\nregions, and SatL phylogeny matched perfectly with mitochondrial genes and NuMT\npseudogenes. These results support that SatL arrays originated from ten\nindependent mitochondria insertion events into the nuclear genome within the\nlast 500,000 years, after divergence from its sister species N. giraulti.\nDramatic repeat GC-percent elevation (from 33.9% to 50.4%) is a hallmark of\nrapid SatL sequence evolution in mitochondria due to GC-biased gene conversion\nfacilitated by the palindromic sequence pairing of the two mitochondrial SatL\narrays. The nuclear SatL repeat arrays underwent substantial copy number\nexpansion, from 12-15 (SatL1) to over 400 copies (SatL4). The oldest SatL4B\narray consists of four types of repeat units derived from deletions in the\nAT-rich region of ancestral repeats, and complex high-order structures have\nevolved through duplications. We also discovered similar repeat insertions into\nthe nuclear genome of Muscidifurax, suggesting this mechanism can be common in\ninsects. This is the first report of the mitochondrial origin of nuclear\nsatellite sequences, and our findings shed new light on the origin and\nevolution of satellite DNA.",
        "Genotype-to-Phenotype (G2P) prediction plays a pivotal role in crop breeding,\nenabling the identification of superior genotypes based on genomic data. Rice\n(Oryza sativa), one of the most important staple crops, faces challenges in\nimproving yield and resilience due to the complex genetic architecture of\nagronomic traits and the limited sample size in breeding datasets. Current G2P\nprediction methods, such as GWAS and linear models, often fail to capture\ncomplex non-linear relationships between genotypes and phenotypes, leading to\nsuboptimal prediction accuracy. Additionally, population stratification and\noverfitting are significant obstacles when models are applied to small datasets\nwith diverse genetic backgrounds. This study introduces the Learnable Group\nTransform (LGT) method, which aims to overcome these challenges by combining\nthe advantages of traditional linear models with advanced machine learning\ntechniques. LGT utilizes a group-based transformation of genotype data to\ncapture spatial relationships and genetic structures across diverse rice\npopulations, offering flexibility to generalize even with limited data. Through\nextensive experiments on the Rice529 dataset, a panel of 529 rice accessions,\nLGT demonstrated substantial improvements in prediction accuracy for multiple\nagronomic traits, including yield and plant height, compared to\nstate-of-the-art baselines such as linear models and recent deep learning\napproaches. Notably, LGT achieved an R^2 improvement of up to 15\\% for yield\nprediction, significantly reducing error and demonstrating its ability to\nextract meaningful signals from high-dimensional, noisy genomic data. These\nresults highlight the potential of LGT as a powerful tool for genomic\nprediction in rice breeding, offering a promising solution for accelerating the\nidentification of high-yielding and resilient rice varieties.",
        "Multiplexed assays of variant effect (MAVEs) perform simultaneous\ncharacterization of many variants. Prime editing has been recently adopted for\nintroducing many variants in their native genomic contexts. However, robust\nprotocols and standards are limited, preventing widespread uptake. Herein, we\ndescribe curated loci prime editing (cliPE) which is an accessible, low-cost\nexperimental pipeline to perform MAVEs using prime editing of a target gene, as\nwell as a companion Shiny app (pegRNA Designer) to rapidly and easily design\nuser-specific MAVE libraries.",
        "We provide a general non-uniqueness result for normalized ground states of\nnonlinear Schr\\\"odinger equations with pure power nonlinearity on bounded\ndomains with homogeneous Neumann boundary conditions, defined as global\nminimizers of the associated energy functional among functions with prescribed\nmass. Precisely, for nonlinearity powers slightly smaller than the\n$L^2$-critical exponent, we prove that there always exists at least one value\nof the mass for which normalized ground states are not unique.",
        "With a fixed prime power $q>1$, define the ring of polynomials\n$A=\\mathbb{F}_q[t]$ and its fraction field $F=\\mathbb{F}_q(t)$. For each pair\n$a=(a_1,a_2) \\in A^2$ with $a_2$ nonzero, let $\\phi(a)\\colon A\\to F\\{\\tau\\}$ be\nthe Drinfeld $A$-module of rank $2$ satisfying $t\\mapsto t+a_1\\tau+a_2\\tau^2$.\nThe Galois action on the torsion of $\\phi(a)$ gives rise to a Galois\nrepresentation $\\rho_{\\phi(a)}\\colon\n\\operatorname{Gal}(F^{\\operatorname{sep}}\/F)\\to\n\\operatorname{GL}_2(\\widehat{A})$, where $\\widehat{A}$ is the profinite\ncompletion of $A$. We show that the image of $\\rho_{\\phi(a)}$ is large for\nrandom $a$. More precisely, for all $a\\in A^2$ away from a set of density $0$,\nwe prove that the index\n$[\\operatorname{GL}_2(\\widehat{A}):\\rho_{\\phi(a)}(\\operatorname{Gal}(F^{\\operatorname{sep}}\/F))]$\ndivides $q-1$ when $q>2$ and divides $4$ when $q=2$. We also show that the\nrepresentation $\\rho_{\\phi(a)}$ is surjective for a positive density set of\n$a\\in A^2$.",
        "Double network hydrogels show remarkable mechanical performance, combining\nhigh strength and fracture toughness with sufficient stiffness to bear load,\ndespite containing only a low density of cross-linked polymer molecules in\nwater. We introduce a simple mesoscale model of a double network material,\ndetailed enough to resolve the salient microphysics of local plastic bond\nbreakage, yet simple enough to address macroscopic cracking. Load sharing\nbetween the networks results in a delocalisation of stress such that the double\nnetwork inherits both the stiffness of its stiff-and-brittle sacrificial\nnetwork and the ductility of its soft-and-ductile matrix network. The\nunderlying mechanism is a reduction in the Eshelby stress propagator between\nsacrificial bonds, inhibiting the tendency for the plastic failure of one\nsacrificial bond to propagate stress to neighbouring sacrificial bonds and\ncause a follow-on cascade of breakages. The mechanism of brittle macroscopic\ncracking is thereby suppressed, giving instead ductile deformation via\ndiffusely distributed microcracking.",
        "Fine-grained noise maps are vital for epidemiological studies on traffic\nnoise. However, detailed information on traffic noise is often limited,\nespecially in Eastern Europe. Rigid linear noise land-use regressions are\ntypically employed to estimate noise levels; however, machine learning likely\noffers more accurate noise predictions. We innovated by comparing the\npredictive accuracies of supervised machine learning models to estimate traffic\nnoise levels across the five largest Bulgarian cities. In situ A-weighted\nequivalent continuous sound levels were obtained from 232 fixed-site monitors\nacross these cities. We included transport- and land-use-related predictors\nusing 50-1,000 m buffers. Extreme gradient boosting (XGB) had the highest\nten-fold cross-validated fit (R2=0.680) and the lowest root mean square error\n(RMSE=4.739), insignificantly besting the random forest-based model (R2=0.667,\nRMSE=4.895). Support vector regression (R2=0.633, RMSE=5.358), elastic net\n(R2=0.568, RMSE=5.625), and linear regression (R2=0.548, RMSE=5.569) performed\nsignificantly worse. Shapley values for the XGB showed that the length of major\nroads within 100 m buffers, footways within 50 m buffers, residential roads\nwithin 50 m buffers, and the number of buildings within 50 m buffers were\nimportant non-linear predictors. Our spatially resolved noise maps revealed\nstriking geographic noise variations and that, on average, 96.8% of the urban\npopulation experiences harmful noise levels.",
        "We investigate a class of quadratic backward stochastic differential\nequations (BSDEs) with generators singular in $ y $. First, we establish the\nexistence of solutions and a comparison theorem, thereby extending results in\nthe literature. Additionally, we analyze the stability property and the\nFeynman-Kac formula, and prove the uniqueness of viscosity solutions for the\ncorresponding singular semilinear partial differential equations (PDEs).\nFinally, we demonstrate applications in the context of robust control linked to\nstochastic differential utility and certainty equivalent based on\n$g$-expectation. In these applications, the coefficient of the quadratic term\nin the generator captures the level of ambiguity aversion and the coefficient\nof absolute risk aversion, respectively.",
        "In this paper, we propose an improved online confidence bound for multinomial\nlogistic (MNL) models and apply this result to MNL bandits, achieving\nvariance-dependent optimal regret. Recently, Lee & Oh (2024) established an\nonline confidence bound for MNL models and achieved nearly minimax-optimal\nregret in MNL bandits. However, their results still depend on the\nnorm-boundedness of the unknown parameter $B$ and the maximum size of possible\noutcomes $K$. To address this, we first derive an online confidence bound of\n$O\\left(\\sqrt{d \\log t} + B \\right)$, which is a significant improvement over\nthe previous bound of $O (B \\sqrt{d} \\log t \\log K )$ (Lee & Oh, 2024). This is\nmainly achieved by establishing tighter self-concordant properties of the MNL\nloss and introducing a novel intermediary term to bound the estimation error.\nUsing this new online confidence bound, we propose a constant-time algorithm,\nOFU-MNL++, which achieves a variance-dependent regret bound of $O \\Big( d \\log\nT \\sqrt{ \\sum_{t=1}^T \\sigma_t^2 } \\Big) $ for sufficiently large $T$, where\n$\\sigma_t^2$ denotes the variance of the rewards at round $t$, $d$ is the\ndimension of the contexts, and $T$ is the total number of rounds. Furthermore,\nwe introduce a Maximum Likelihood Estimation (MLE)-based algorithm,\nOFU-MN$^2$L, which achieves an anytime poly(B)-free regret of $O \\Big( d \\log\n(BT) \\sqrt{ \\sum_{t=1}^T \\sigma_t^2 } \\Big) $.",
        "Athletic training is characterized by physiological systems responding to\nrepeated exercise-induced stress, resulting in gradual alterations in the\nfunctional properties of these systems. The adaptive response leading to\nimproved performance follows a remarkably predictable pattern that may be\ndescribed by a systems model provided that training load can be accurately\nquantified and that the constants defining the training-performance\nrelationship are known. While various impulse-response models have been\nproposed, they are inherently limited in reducing training stress (the impulse)\ninto a single metric, assuming that the adaptive responses are independent of\nthe type of training performed. This is despite ample evidence of markedly\ndiverse acute and chronic responses to exercise of different intensities and\ndurations. Herein, we propose an alternative, three-dimensional\nimpulse-response model that uses three training load metrics as inputs and\nthree performance metrics as outputs. These metrics, represented by a\nthree-parameter critical power model, reflect the stress imposed on each of the\nthree energy systems: the alactic (phosphocreatine\/immediate) system; the\nlactic (glycolytic) system; and the aerobic (oxidative) system. The purpose of\nthis article is to outline the scientific rationale and the practical\nimplementation of the three-dimensional impulse-response model.",
        "We demonstrate strong light-matter interaction for a layer of templated\nmerocyanine molecules in a planar microcavity. Using a single layer of graphene\nnanoribbons as a templating layer, we obtain an aligned layer of aggregated\nmolecules. The molecular layer displays anisotropic optical properties\nresembling those of a biaxial crystal. The anisotropic excitonic component in\nthe cavity results in strongly polarization-dependent light-matter interaction\nand in increased Rabi-energies. The increased light-matter interaction is\npossibly due to reduced molecular disorder in the templated molecular layer.\nThis conclusion is supported by an analysis based on a multi-oscillator model.\nWe further use photoluminescence microspectroscopy to demonstrate that the\nlight-matter coupling is spatially homogeneous. Our study introduces molecular\ntemplating to strong light-matter studies. The reduced disorder of the system\nas a consequence of templating is highly beneficial for engineering\nlight-matter interaction.",
        "Cell-free (CF) massive multiple-input multiple-output (mMIMO) systems offer\nhigh spectral efficiency (SE) through multiple distributed access points (APs).\nHowever, the large number of antennas increases power consumption. We propose\nincorporating stacked intelligent metasurfaces (SIM) into CF mMIMO systems as a\ncost-effective, energy-efficient solution. This paper focuses on optimizing the\njoint power allocation of APs and the phase shift of SIMs to maximize the sum\nSE. To address this complex problem, we introduce a fully distributed\nmulti-agent reinforcement learning (MARL) algorithm. Our novel algorithm, the\nnoisy value method with a recurrent policy in multi-agent policy optimization\n(NVR-MAPPO), enhances performance by encouraging diverse exploration under\ncentralized training and decentralized execution. Simulations demonstrate that\nNVR-MAPPO significantly improves sum SE and robustness across various\nscenarios.",
        "The complexity of a graph is the number of its labeled spanning trees. In\nthis work complexity is studied in settings that admit regular graphs. An exact\nformula is established linking complexity of the complement of a regular graph\nto numbers of closed walks in the graph by way of an infinite alternating\nseries. Some consequences of this result yield infinite classes of lower and\nupper bounds on the complexity of such graphs. Applications of these\nmathematical results to biological problems on neuronal activity are described.",
        "Crystal structure forms the foundation for understanding the physical and\nchemical properties of materials. Generative models have emerged as a new\nparadigm in crystal structure prediction(CSP), however, accurately capturing\nkey characteristics of crystal structures, such as periodicity and symmetry,\nremains a significant challenge. In this paper, we propose a\nTransformer-Enhanced Variational Autoencoder for Crystal Structure Prediction\n(TransVAE-CSP), who learns the characteristic distribution space of stable\nmaterials, enabling both the reconstruction and generation of crystal\nstructures. TransVAE-CSP integrates adaptive distance expansion with\nirreducible representation to effectively capture the periodicity and symmetry\nof crystal structures, and the encoder is a transformer network based on an\nequivariant dot product attention mechanism. Experimental results on the\ncarbon_24, perov_5, and mp_20 datasets demonstrate that TransVAE-CSP\noutperforms existing methods in structure reconstruction and generation tasks\nunder various modeling metrics, offering a powerful tool for crystal structure\ndesign and optimization.",
        "Artificial classical wave systems such as wave crystals and metamaterials\nhave demonstrated promising capabilities in simulating a wide range of quantum\nmechanical phenomena. Yet some gaps between quantum and classical worlds are\ngenerally considered fundamental and difficult to bridge. Dynamics obeying\nspecial unitary groups, e.g., electronic spins described by SU(2), color\nsymmetries of fundamental particles described by SU(3), are such examples. In\nthis work, we present the experimental realization of universal SU(2) and SU(3)\ndynamic operations in classical mechanical oscillator systems with temporally\nmodulated coupling terms. Our approach relies on the sequential execution of\nnon-adiabatic holonomic evolutions, which are typically used in constructing\nquantum-logic gates. The method is swift and purely geometric and can be\nextended to realize more sophisticated dynamic operations. Our results open a\nnew way for studying and simulating quantum phenomena in classical systems.",
        "We introduce a novel model for updating perceptual beliefs about the\nenvironment by extending the concept of Allostasis to the control of internal\nrepresentations. Allostasis is a fundamental regulatory mechanism observed in\nanimal physiology that orchestrates responses to maintain a dynamic equilibrium\nin bodily needs and internal states. In this paper, we focus on an application\nin numerical cognition, where a bump of activity in an attractor network is\nused as a spatial numerical representation. While existing neural networks can\nmaintain persistent states, to date, there is no unified framework for\ndynamically controlling spatial changes in neuronal activity in response to\nenvironmental changes. To address this, we couple a well known allostatic\nmicrocircuit, the Hammel model, with a ring attractor, resulting in a Spiking\nNeural Network architecture that can modulate the location of the bump as a\nfunction of some reference input. This localized activity in turn is used as a\nperceptual belief in a simulated subitization task a quick enumeration process\nwithout counting. We provide a general procedure to fine-tune the model and\ndemonstrate the successful control of the bump location. We also study the\nresponse time in the model with respect to changes in parameters and compare it\nwith biological data. Finally, we analyze the dynamics of the network to\nunderstand the selectivity and specificity of different neurons to distinct\ncategories present in the input. The results of this paper, particularly the\nmechanism for moving persistent states, are not limited to numerical cognition\nbut can be applied to a wide range of tasks involving similar representations.",
        "We investigate the inspiral of a high mass-ratio black hole binary located in\nthe nucleus of a galaxy, where the primary central black hole is surrounded by\na dense dark matter spike formed through accretion during the black hole growth\nphase. Within this spike, dark matter undergoes strong self-annihilation,\nproducing a compact source of $\\gamma$-ray radiation that is highly sensitive\nto spike density, while the binary emits gravitational waves at frequencies\ndetectable by LISA. As the inspiralling binary interacts with the surrounding\ndark matter particles, it alters the density of the spike, thereby influencing\nthe $\\gamma$-ray flux from dark matter annihilation. We demonstrate that the\nspike self-annihilation luminosity decreases by $10\\%$ to $90\\%$ of its initial\nvalue, depending on the initial density profile and binary mass ratio, as the\nbinary sweeps through the LISA band. This presents a new opportunity to\nindirectly probe dark matter through multi-messenger observations of galactic\nnuclei.",
        "The aim of this paper is to reconsider the existence theory for steady\ncompressible Navier--Stokes--Fourier system assuming more general condition of\nthe dependence of the viscosities on the temperature in the form\n$\\mu(\\vartheta)$, $\\xi(\\vartheta) \\sim (1+\\vartheta)^\\alpha$ for $0\\leq \\alpha\n\\leq 1$. This extends the known theory for $\\alpha=1$ from and improves\nsignificantly the results for $\\alpha =0$. This paper is the first of a series\nof two papers dealing with this problem and is connected with the\nBogovskii-type estimates of the sequence of densities. This leads, among\nothers, to the limitation $\\gamma >\\frac 32$ for the pressure law\n$p(\\varrho,\\vartheta) \\sim \\varrho^\\gamma + \\varrho\\vartheta$. The paper\nconsiders both the heat-flux (Robin) and Dirichlet boundary conditions for the\ntemperature as well as both the homogeneous Dirichlet and zero inflow\/outflow\nNavier boundary conditions for the velocity. Further extension for $\\gamma >1$\nonly is based on different type of pressure estimates and will be the content\nof the subsequent paper."
      ]
    }
  },
  {
    "id":2411.05055,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"Predicting Functional Effect of Human Missense Mutations Using PolyPhen\u20102",
    "start_abstract":"Abstract PolyPhen\u20102 (Polymorphism Phenotyping v2), available as software and via a Web server, predicts the possible impact of amino acid substitutions on stability function human proteins using structural comparative evolutionary considerations. It performs functional annotation single\u2010nucleotide polymorphisms (SNPs), maps coding SNPs to gene transcripts, extracts protein sequence annotations attributes, builds conservation profiles. then estimates probability missense mutation being damaging based combination all these properties. features include high\u2010quality multiple alignment pipeline prediction method employing machine\u2010learning classification. The also integrates UCSC Genome Browser's genome MultiZ alignments vertebrate genomes with genome. is capable analyzing large volumes data produced by next\u2010generation sequencing projects, thanks built\u2010in support for high\u2010performance computing environments like Grid Engine Platform LSF. Curr. Protoc. Hum. Genet . 76:7.20.1\u20107.20.41. \u00a9 2013 John Wiley &amp; Sons, Inc.",
    "start_categories":[
      "q-bio.GN"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems"
      ],
      "abstract":[
        "TensorFlow is an interface for expressing machine learning algorithms, and implementation executing such algorithms. A computation expressed using can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices as phones tablets up to large-scale distributed systems hundreds machines thousands computational GPU cards. The system flexible used express including training inference algorithms deep neural network models, it has been conducting research deploying into production across more than dozen areas computer science other fields, speech recognition, vision, robotics, information retrieval, natural language processing, geographic extraction, drug discovery. This paper describes the that we have built at Google. API reference were released open-source package under Apache 2.0 license in November, 2015 are available www.tensorflow.org."
      ],
      "categories":[
        "cs.CL"
      ]
    },
    "list":{
      "title":[
        "Analysis of Indic Language Capabilities in LLMs",
        "Explicit vs. Implicit: Investigating Social Bias in Large Language\n  Models through Self-Reflection",
        "Accelerating Antibiotic Discovery with Large Language Models and\n  Knowledge Graphs",
        "Chain of Strategy Optimization Makes Large Language Models Better\n  Emotional Supporter",
        "DUAL: Diversity and Uncertainty Active Learning for Text Summarization",
        "LexGenie: Automated Generation of Structured Reports for European Court\n  of Human Rights Case Law",
        "Learning to Search Effective Example Sequences for In-Context Learning",
        "Information Types in Product Reviews",
        "FRAG: A Flexible Modular Framework for Retrieval-Augmented Generation\n  based on Knowledge Graphs",
        "Challenges in Expanding Portuguese Resources: A View from Open\n  Information Extraction",
        "Elevating Legal LLM Responses: Harnessing Trainable Logical Structures\n  and Semantic Knowledge with Legal Reasoning",
        "Adaptive Prompting: Ad-hoc Prompt Composition for Social Bias Detection",
        "Query-Aware Learnable Graph Pooling Tokens as Prompt for Large Language\n  Models",
        "A novel Facial Recognition technique with Focusing on Masked Faces",
        "Exact Maximin Share Fairness via Adjusted Supply",
        "Network-assisted collective operations for efficient distributed quantum\n  computing",
        "A Moving Mesh Isogeometric Method Based on Harmonic Maps",
        "Quasi-two-dimensional magnetism and antiferromagnetic ground state in\n  Li$_2$FeSiO$_4$",
        "Channel deformations during elastocapillary spreading of gaseous\n  embolisms in biomimetic leaves",
        "Pretraining Generative Flow Networks with Inexpensive Rewards for\n  Molecular Graph Generation",
        "Measuring Star Formation Rates in the Milky Way from Hi-GAL 70 $\\mu$m\n  Observations",
        "Non-negative tensor factorization-based dependence map analysis for\n  local damage detection in presence of non-Gaussian noise",
        "Low-energy insulating reconstructions of Si(111)-7x7 surface with and\n  without stacking fault discovered by graph theory",
        "Cultivating Precision: Comparative Analysis of Sensor-Based Yogurt\n  Fermentation Monitoring Techniques",
        "Existence and Uniqueness of Local Solutions for a Class of Partial\n  Differential-Algebraic Equations",
        "Generalizable automated ischaemic stroke lesion segmentation with vision\n  transformers",
        "A Spatio-Temporal Dirichlet Process Mixture Model on Linear Networks for\n  Crime Data",
        "Weighted Graph Structure Learning with Attention Denoising for Node\n  Classification"
      ],
      "abstract":[
        "This report evaluates the performance of text-in text-out Large Language\nModels (LLMs) to understand and generate Indic languages. This evaluation is\nused to identify and prioritize Indic languages suited for inclusion in safety\nbenchmarks. We conduct this study by reviewing existing evaluation studies and\ndatasets; and a set of twenty-eight LLMs that support Indic languages. We\nanalyze the LLMs on the basis of the training data, license for model and data,\ntype of access and model developers. We also compare Indic language performance\nacross evaluation datasets and find that significant performance disparities in\nperformance across Indic languages. Hindi is the most widely represented\nlanguage in models. While model performance roughly correlates with number of\nspeakers for the top five languages, the assessment after that varies.",
        "Large Language Models (LLMs) have been shown to exhibit various biases and\nstereotypes in their generated content. While extensive research has\ninvestigated bias in LLMs, prior work has predominantly focused on explicit\nbias, leaving the more nuanced implicit biases largely unexplored. This paper\npresents a systematic framework grounded in social psychology theories to\ninvestigate and compare explicit and implicit biases in LLMs. We propose a\nnovel \"self-reflection\" based evaluation framework that operates in two phases:\nfirst measuring implicit bias through simulated psychological assessment\nmethods, then evaluating explicit bias by prompting LLMs to analyze their own\ngenerated content. Through extensive experiments on state-of-the-art LLMs\nacross multiple social dimensions, we demonstrate that LLMs exhibit a\nsubstantial inconsistency between explicit and implicit biases, where explicit\nbiases manifest as mild stereotypes while implicit biases show strong\nstereotypes. Furthermore, we investigate the underlying factors contributing to\nthis explicit-implicit bias inconsistency. Our experiments examine the effects\nof training data scale, model parameters, and alignment techniques. Results\nindicate that while explicit bias diminishes with increased training data and\nmodel size, implicit bias exhibits a contrasting upward trend. Notably,\ncontemporary alignment methods (e.g., RLHF, DPO) effectively suppress explicit\nbias but show limited efficacy in mitigating implicit bias. These findings\nsuggest that while scaling up models and alignment training can address\nexplicit bias, the challenge of implicit bias requires novel approaches beyond\ncurrent methodologies.",
        "The discovery of novel antibiotics is critical to address the growing\nantimicrobial resistance (AMR). However, pharmaceutical industries face high\ncosts (over $1 billion), long timelines, and a high failure rate, worsened by\nthe rediscovery of known compounds. We propose an LLM-based pipeline that acts\nas an alarm system, detecting prior evidence of antibiotic activity to prevent\ncostly rediscoveries. The system integrates organism and chemical literature\ninto a Knowledge Graph (KG), ensuring taxonomic resolution, synonym handling,\nand multi-level evidence classification. We tested the pipeline on a private\nlist of 73 potential antibiotic-producing organisms, disclosing 12 negative\nhits for evaluation. The results highlight the effectiveness of the pipeline\nfor evidence reviewing, reducing false negatives, and accelerating\ndecision-making. The KG for negative hits and the user interface for\ninteractive exploration will be made publicly available.",
        "The growing emotional stress in modern society has increased the demand for\nEmotional Support Conversations (ESC). While Large Language Models (LLMs) show\npromise for ESC, they face two key challenges: (1) low strategy selection\naccuracy, and (2) preference bias, limiting their adaptability to emotional\nneeds of users. Existing supervised fine-tuning (SFT) struggles to address\nthese issues, as it rigidly trains models on single gold-standard responses\nwithout modeling nuanced strategy trade-offs. To overcome these limitations, we\npropose Chain-of-Strategy Optimization (CSO), a novel approach that optimizes\nstrategy selection preferences at each dialogue turn. We first leverage Monte\nCarlo Tree Search to construct ESC-Pro, a high-quality preference dataset with\nturn-level strategy-response pairs. Training on ESC-Pro with CSO improves both\nstrategy accuracy and bias mitigation, enabling LLMs to generate more\nempathetic and contextually appropriate responses. Experiments on LLaMA-3.1-8B,\nGemma-2-9B, and Qwen2.5-7B demonstrate that CSO outperforms standard SFT,\nhighlighting the efficacy of fine-grained, turn-level preference modeling in\nESC.",
        "With the rise of large language models, neural text summarization has\nadvanced significantly in recent years. However, even state-of-the-art models\ncontinue to rely heavily on high-quality human-annotated data for training and\nevaluation. Active learning is frequently used as an effective way to collect\nsuch datasets, especially when annotation resources are scarce. Active learning\nmethods typically prioritize either uncertainty or diversity but have shown\nlimited effectiveness in summarization, often being outperformed by random\nsampling. We present Diversity and Uncertainty Active Learning (DUAL), a novel\nalgorithm that combines uncertainty and diversity to iteratively select and\nannotate samples that are both representative of the data distribution and\nchallenging for the current model. DUAL addresses the selection of noisy\nsamples in uncertainty-based methods and the limited exploration scope of\ndiversity-based methods. Through extensive experiments with different\nsummarization models and benchmark datasets, we demonstrate that DUAL\nconsistently matches or outperforms the best performing strategies. Using\nvisualizations and quantitative metrics, we provide valuable insights into the\neffectiveness and robustness of different active learning strategies, in an\nattempt to understand why these strategies haven't performed consistently in\ntext summarization. Finally, we show that DUAL strikes a good balance between\ndiversity and robustness.",
        "Analyzing large volumes of case law to uncover evolving legal principles,\nacross multiple cases, on a given topic is a demanding task for legal\nprofessionals. Structured topical reports provide an effective solution by\nsummarizing key issues, principles, and judgments, enabling comprehensive legal\nanalysis on a particular topic. While prior works have advanced query-based\nindividual case summarization, none have extended to automatically generating\nmulti-case structured reports. To address this, we introduce LexGenie, an\nautomated LLM-based pipeline designed to create structured reports using the\nentire body of case law on user-specified topics within the European Court of\nHuman Rights jurisdiction. LexGenie retrieves, clusters, and organizes relevant\npassages by topic to generate a structured outline and cohesive content for\neach section. Expert evaluation confirms LexGenie's utility in producing\nstructured reports that enhance efficient, scalable legal analysis.",
        "Large language models (LLMs) demonstrate impressive few-shot learning\ncapabilities, but their performance varies widely based on the sequence of\nin-context examples. Key factors influencing this include the sequence's\nlength, composition, and arrangement, as well as its relation to the specific\nquery. Existing methods often tackle these factors in isolation, overlooking\ntheir interdependencies. Moreover, the extensive search space for selecting\noptimal sequences complicates the development of a holistic approach. In this\nwork, we introduce Beam Search-based Example Sequence Constructor (BESC), a\nnovel method for learning to construct optimal example sequences. BESC\naddresses all key factors involved in sequence selection by considering them\njointly during inference, while incrementally building the sequence. This\ndesign enables the use of beam search to significantly reduce the complexity of\nthe search space. Experiments across various datasets and language models show\nnotable improvements in performance.",
        "Information in text is communicated in a way that supports a goal for its\nreader. Product reviews, for example, contain opinions, tips, product\ndescriptions, and many other types of information that provide both direct\ninsights, as well as unexpected signals for downstream applications. We devise\na typology of 24 communicative goals in sentences from the product review\ndomain, and employ a zero-shot multi-label classifier that facilitates\nlarge-scale analyses of review data. In our experiments, we find that the\ncombination of classes in the typology forecasts helpfulness and sentiment of\nreviews, while supplying explanations for these decisions. In addition, our\ntypology enables analysis of review intent, effectiveness and rhetorical\nstructure. Characterizing the types of information in reviews unlocks many\nopportunities for more effective consumption of this genre.",
        "To mitigate the hallucination and knowledge deficiency in large language\nmodels (LLMs), Knowledge Graph (KG)-based Retrieval-Augmented Generation (RAG)\nhas shown promising potential by utilizing KGs as external resource to enhance\nLLMs reasoning. However, existing KG-RAG approaches struggle with a trade-off\nbetween flexibility and retrieval quality. Modular methods prioritize\nflexibility by avoiding the use of KG-fine-tuned models during retrieval,\nleading to fixed retrieval strategies and suboptimal retrieval quality.\nConversely, coupled methods embed KG information within models to improve\nretrieval quality, but at the expense of flexibility. In this paper, we propose\na novel flexible modular KG-RAG framework, termed FRAG, which synergizes the\nadvantages of both approaches. FRAG estimates the hop range of reasoning paths\nbased solely on the query and classify it as either simple or complex. To match\nthe complexity of the query, tailored pipelines are applied to ensure efficient\nand accurate reasoning path retrieval, thus fostering the final reasoning\nprocess. By using the query text instead of the KG to infer the structural\ninformation of reasoning paths and employing adaptable retrieval strategies,\nFRAG improves retrieval quality while maintaining flexibility. Moreover, FRAG\ndoes not require extra LLMs fine-tuning or calls, significantly boosting\nefficiency and conserving resources. Extensive experiments show that FRAG\nachieves state-of-the-art performance with high efficiency and low resource\nconsumption.",
        "Open Information Extraction (Open IE) is the task of extracting structured\ninformation from textual documents, independent of domain. While traditional\nOpen IE methods were based on unsupervised approaches, recently, with the\nemergence of robust annotated datasets, new data-based approaches have been\ndeveloped to achieve better results. These innovations, however, have focused\nmainly on the English language due to a lack of datasets and the difficulty of\nconstructing such resources for other languages. In this work, we present a\nhigh-quality manually annotated corpus for Open Information Extraction in the\nPortuguese language, based on a rigorous methodology grounded in established\nsemantic theories. We discuss the challenges encountered in the annotation\nprocess, propose a set of structural and contextual annotation rules, and\nvalidate our corpus by evaluating the performance of state-of-the-art Open IE\nsystems. Our resource addresses the lack of datasets for Open IE in Portuguese\nand can support the development and evaluation of new methods and systems in\nthis area.",
        "Large Language Models (LLMs) have achieved impressive results across numerous\ndomains, yet they experience notable deficiencies in legal question-answering\ntasks. LLMs often generate generalized responses that lack the logical\nspecificity required for expert legal advice and are prone to hallucination,\nproviding answers that appear correct but are unreliable. Retrieval-Augmented\nGeneration (RAG) techniques offer partial solutions to address this challenge,\nbut existing approaches typically focus only on semantic similarity, neglecting\nthe logical structure essential to legal reasoning. In this paper, we propose\nthe Logical-Semantic Integration Model (LSIM), a novel supervised framework\nthat bridges semantic and logical coherence. LSIM comprises three components:\nreinforcement learning predicts a structured fact-rule chain for each question,\na trainable Deep Structured Semantic Model (DSSM) retrieves the most relevant\ncandidate questions by integrating semantic and logical features, and\nin-context learning generates the final answer using the retrieved content. Our\nexperiments on a real-world legal QA dataset-validated through both automated\nmetrics and human evaluation-demonstrate that LSIM significantly enhances\naccuracy and reliability compared to existing methods.",
        "Recent advances on instruction fine-tuning have led to the development of\nvarious prompting techniques for large language models, such as explicit\nreasoning steps. However, the success of techniques depends on various\nparameters, such as the task, language model, and context provided. Finding an\neffective prompt is, therefore, often a trial-and-error process. Most existing\napproaches to automatic prompting aim to optimize individual techniques instead\nof compositions of techniques and their dependence on the input. To fill this\ngap, we propose an adaptive prompting approach that predicts the optimal prompt\ncomposition ad-hoc for a given input. We apply our approach to social bias\ndetection, a highly context-dependent task that requires semantic\nunderstanding. We evaluate it with three large language models on three\ndatasets, comparing compositions to individual techniques and other baselines.\nThe results underline the importance of finding an effective prompt\ncomposition. Our approach robustly ensures high detection performance, and is\nbest in several settings. Moreover, first experiments on other tasks support\nits generalizability.",
        "Graph-structured data plays a vital role in numerous domains, such as social\nnetworks, citation networks, commonsense reasoning graphs and knowledge graphs.\nWhile graph neural networks have been employed for graph processing, recent\nadvancements have explored integrating large language models for graph-based\ntasks. In this paper, we propose a novel approach named Learnable Graph Pooling\nToken (LGPT), which addresses the limitations of the scalability issues in\nnode-level projection and information loss in graph-level projection. LGPT\nenables flexible and efficient graph representation by introducing learnable\nparameters that act as tokens in large language models, balancing fine-grained\nand global graph information. Additionally, we investigate an Early Query\nFusion technique, which fuses query context before constructing the graph\nrepresentation, leading to more effective graph embeddings. Our method achieves\na 4.13\\% performance improvement on the GraphQA benchmark without training the\nlarge language model, demonstrating significant gains in handling complex\ntextual-attributed graph data.",
        "Recognizing the same faces with and without masks is important for ensuring\nconsistent identification in security, access control, and public safety. This\ncapability is crucial in scenarios like law enforcement, healthcare, and\nsurveillance, where accurate recognition must be maintained despite facial\nocclusion. This research focuses on the challenge of recognizing the same faces\nwith and without masks by employing cosine similarity as the primary technique.\nWith the increased use of masks, traditional facial recognition systems face\nsignificant accuracy issues, making it crucial to develop methods that can\nreliably identify individuals in masked conditions. For that reason, this study\nproposed Masked-Unmasked Face Matching Model (MUFM). This model employs\ntransfer learning using the Visual Geometry Group (VGG16) model to extract\nsignificant facial features, which are subsequently classified utilizing the\nK-Nearest Neighbors (K-NN) algorithm. The cosine similarity metric is employed\nto compare masked and unmasked faces of the same individuals. This approach\nrepresents a novel contribution, as the task of recognizing the same individual\nwith and without a mask using cosine similarity has not been previously\naddressed. By integrating these advanced methodologies, the research\ndemonstrates effective identification of individuals despite the presence of\nmasks, addressing a significant limitation in traditional systems. Using data\nis another essential part of this work, by collecting and preparing an image\ndataset from three different sources especially some of those data are real\nprovided a comprehensive power of this research. The image dataset used were\nalready collected in three different datasets of masked and unmasked for the\nsame faces.",
        "This work addresses fair allocation of indivisible items in settings wherein\nit is feasible to create copies of resources or dispose of tasks. We establish\nthat exact maximin share (MMS) fairness can be achieved via limited duplication\nof goods even under monotone valuations. We also show that, when allocating\nchores under monotone costs, MMS fairness is always feasible with limited\ndisposal of chores. Since monotone valuations do not admit any nontrivial\napproximation guarantees for MMS, our results highlight that such barriers can\nbe circumvented by post facto adjustments in the supply of the items.\n  We prove that, for division of $m$ goods among $n$ agents with monotone\nvaluations, there always exists an assignment of subsets of goods to the agents\nsuch that they receive at least their maximin shares and no single good is\nallocated to more than $3 \\log m$ agents. In addition, the sum of the sizes of\nthe assigned subsets does not exceed $m$. For identically ordered valuations,\nwe obtain an upper bound of $O(\\sqrt{\\log m})$ on the maximum assignment\nmultiplicity across goods and an $m + \\widetilde{O}\\left(\\frac{m}{\\sqrt{n}}\n\\right)$ bound for the total number of goods assigned. Further, for additive\nvaluations, we prove that there always exists an MMS assignment in which no\nsingle good is allocated to more than $2$ agents and the total number of goods\nassigned is at most $2m$.\n  For chores, we upper bound the number of chores that need to be discarded for\nensuring MMS fairness. We prove that, under monotone costs, there exists an MMS\nassignment in which at most $\\frac{m}{e}$ remain unassigned. For identically\nordered costs, we establish that MMS fairness can be achieved while keeping at\nmost $\\widetilde{O} \\left(\\frac{m}{n^{1\/4}} \\right)$ chores unassigned. We also\nprove that the obtained bounds for monotone valuations and monotone costs are\nessentially tight.",
        "We propose protocols for the distribution of collective quantum operations\nbetween remote quantum processing units (QPUs), a requirement for distributed\nquantum computing. Using only local operations and classical communication\n(LOCC), these protocols allow for collective multicontrolled and multitarget\ngates to be executed in network architectures similar to those used for\nhigh-performance computing. The types of gates that can be implemented\nfollowing this scheme are discussed. The Bell pair cost for a single\ndistributed multicontrolled gate is estimated, arriving to a single additional\nBell pair over the theoretically optimal calculation with pre-shared\nentanglement, demonstrating better scalability when compared to current\nproposals based on entanglement swapping through a network, and bounds are\ncalculated for general diagonal gates. A recipe is provided for the lumped\ndistribution of gates such as arbitrarily-sized Toffoli and multicontrolled Z,\nand $R_{zz}(\\theta)$ gates. Finally, we provide an exact implementation of a\ndistributed Grover's search algorithm using this protocol to partition the\ncircuit, with Bell pair cost growing linearly with the number of Grover\niterations and the number of partitions.",
        "Although the isogeometric analysis has shown its great potential in achieving\nhighly accurate numerical solutions of partial differential equations, its\nefficiency is the main factor making the method more competitive in practical\nsimulations. In this paper, an integration of isogeometric analysis and a\nmoving mesh method is proposed, providing a competitive approach to resolve the\nefficiency issue. Focusing on the Poisson equation, the implementation of the\nalgorithm and related numerical analysis are presented in detail, including the\nnumerical discretization of the governing equation utilizing isogeometric\nanalysis, and a mesh redistribution technique developed via harmonic maps. It\nis found that the isogeometric analysis brings attractive features in the\nrealization of moving mesh method, such as it provides an accurate expression\nfor moving direction of mesh nodes, and allows for more choices for\nconstructing monitor functions. Through a series of numerical experiments, the\neffectiveness of the proposed method is successfully validated and the\npotential of the method towards the practical application is also well\npresented with the simulation of a helium atom in Kohn--Sham density functional\ntheory.",
        "Our experimental (neutron diffraction, M\\\"ossbauer spectroscopy, magnetic\nsusceptibility, specific heat) and numerical studies on the evolution of short-\nand long-range magnetic order in $\\gamma_{\\rm II}$-Li\\(_2\\)FeSiO\\(_4\\) suggest\na quasi-two-dimensional (2D) nature of magnetism. The experimental data\nobtained on single crystals imply long-range antiferromagnetic order below\n$T_{\\rm N}= 17$~K. A broad maximum in magnetic susceptibility $\\chi$ at $T_{\\rm\nm}\\simeq 28$~K, observation of magnetic entropy changes up to 100~K and\nanisotropy in $\\chi$ are indicative of low-dimensional magnetism and suggest\nshort-range magnetic correlations up to 200~K. Neutron diffraction shows that\nlong-range antiferromagnetic order is characterised by the propagation vector\nk=(1\/2,0,1\/2). The ordered moment $\\mu = 2.50(2) \\mu_B$ \/Fe, at $T = 1.5$~K, is\nalong the crystallographic $a$-axis. This is consistent with the observed\nstatic hyperfine field of $B_{\\rm hyp}=14.8(3)$\\,T by M\\\"ossbauer spectroscopy\nwhich indicates significant orbital contributions. The temperature dependence\nof $B_{\\rm hyp}$ yields the critical exponent $\\beta=0.116(12)$ which is in the\nregime of the 2D Ising behaviour. LSDA+U studies exploiting the experimental\nspin structure suggest dominating magnetic exchange coupling within the\n$ac$-layers (i.e., $J_3\\simeq -6$~K and $J_6\\simeq-2$~K) while interlayer\ncoupling is much smaller and partly frustrated. This confirms the 2D nature of\nmagnetism and is in full agreement with the experimental findings.",
        "The nucleation and\/or spreading of bubbles in water under tension (due to\nwater evaporation) can be problematic for most plants along the ascending sap\nnetwork from root to leaves, named xylem. Due to global warming, trees facing\ndrought conditions are particularly threatened by the formation of such air\nembolisms, which spreads intermittently and hinder the flow of sap and could\nultimately result in their demise. PDMS-based biomimetic leaves simulating\nevapotranspiration have demonstrated that, in a linear configuration, the\nexistence of a slender constriction in the channel allows for the creation of\nintermittent embolism propagation (as an interaction between the elasticity of\nthe biomimetic leaf (mainly the deformable ceiling of the microchannels) and\nthe capillary forces at the air\/water interfaces)\n\\cite{Keiser2022}-\\cite{keiser2024}. Here we use analog PDMS-based biomimetic\nleaves in 1d and 2d. To better explore the embolism spreading mechanism, we add\nto the setup an additional technique, allowing to measure directly the\nmicrochannel's ceiling deformation versus time, which corresponds to the\npressure variations. We present here such a method that allows to have\nquantitative insights in the dynamics of embolism spreading. The coupling\nbetween channel deformations and the Laplace pressure threshold explains the\nobserved elastocapillary dynamics.",
        "Generative Flow Networks (GFlowNets) have recently emerged as a suitable\nframework for generating diverse and high-quality molecular structures by\nlearning from rewards treated as unnormalized distributions. Previous works in\nthis framework often restrict exploration by using predefined molecular\nfragments as building blocks, limiting the chemical space that can be accessed.\nIn this work, we introduce Atomic GFlowNets (A-GFNs), a foundational generative\nmodel leveraging individual atoms as building blocks to explore drug-like\nchemical space more comprehensively. We propose an unsupervised pre-training\napproach using drug-like molecule datasets, which teaches A-GFNs about\ninexpensive yet informative molecular descriptors such as drug-likeliness,\ntopological polar surface area, and synthetic accessibility scores. These\nproperties serve as proxy rewards, guiding A-GFNs towards regions of chemical\nspace that exhibit desirable pharmacological properties. We further implement a\ngoal-conditioned finetuning process, which adapts A-GFNs to optimize for\nspecific target properties. In this work, we pretrain A-GFN on a subset of ZINC\ndataset, and by employing robust evaluation metrics we show the effectiveness\nof our approach when compared to other relevant baseline methods for a wide\nrange of drug design tasks.",
        "Three methods for computing the total star formation rate of the Milky Way\nagree well with a reference value of $1.65\\pm0.19$ M$_\\odot$ yr$^{-1}$. They\nare then used to determine the radial dependence of the star formation rate and\nface-on map for the Milky Way. First, the method based on a model of star\nformation in Hi-GAL-defined dense clumps, adjusted for an increase in the\ngas-to-dust ratio with Galactocentric radius, predicts $1.65\\pm0.61$ M$_\\odot$\nyr$^{-1}$. Second, the method using the 70 $\\mu$m emission, commonly used in\nother galaxies, with a technique to assign distances to the extended emission,\npredicts $1.42^{+0.63}_{-0.44}$ M$_\\odot$ yr$^{-1}$. Finally, a method based on\ntheoretical predictions of star formation efficiency as a function of virial\nparameter, with masses corrected for metallicity dependence, applied to a\ncatalog of molecular clouds also predicts a value in agreement at $1.47$\nM$_\\odot$ yr$^{-1}$. The three methods predict the radial variation of the star\nformation rate, with remarkably good agreement from the CMZ out to about 20\nkpc. More differences were seen in face-on maps with a resolution of 0.5 kpc\nmade with the three approaches and in comparisons to the local (within 3 kpc)\nstar formation rate, indicating limitations of the methods when applied to\nsmaller scales. The 70 $\\mu$m star formation rate follows very closely the\nsurface density of molecular gas, corrected for a metallicity-dependent CO\nconversion factor. A molecular gas depletion time of 1 Gyr is consistent with\nthe data, as is a molecular Kennicutt-Schmidt relation with a power-law slope\nof $1.10 \\pm 0.06$.",
        "The time-frequency map (TFM) is frequently used in condition monitoring,\nnecessitating further processing to select an informative frequency band (IFB)\nor directly detect damage. However, selecting an IFB is challenging due to the\ncomplexity of spectral structures, non-Gaussian disturbances, and overlapping\nfault signatures in vibration signals. Additionally, dynamic operating\nconditions and low signal-to-noise ratio further complicate the identification\nof relevant features that indicate damage. To solve this problem, the present\nwork proposes a novel method for informative band selection and local damage\ndetection in rolling element bearings, utilizing non-negative tensor\nfactorization (NTF)-based dependence map analysis. The recently introduced\nconcept of the dependence map is leveraged, with a set of these maps being\nfactorized to separate informative components from non-informative ones.\nDependence maps provide valuable information on the auto-similarity of spectral\ncontent, while NTF, a powerful tool commonly used in image processing for\nfeature extraction, enhances this process. The combination of these methods\nallows for the extraction of IFBs, forming the basis for local damage\ndetection. The effectiveness of the proposed method has been validated using\nboth synthetic and real vibration signals corrupted with non-Gaussian\ndisturbances.",
        "The 7x7 reconstruction of Si(111) surface is one of the most fascinating\nconfiguration in nature, whose STM image has been well-understood by the famous\ndimer-adatom-stacking-fault model (DAS). However, the electronic property of\nthe DAS model is always confirmed to be metallic by first-principles\ncalculations, while some experiments detected insulating features. It is still\nchallenge to predict DAS-like reconstructions through traditional method to\nsolve such a puzzle. Here, we show that 7x7 reconstructions can be quickly\ndiscovered by graph theory as implemented in the graph-space based RG2 code for\ncrystal structure prediction. Two groups of reconstructions with (DAS-d8-T12,\nDAS-d8-T9H3-A, DAS-d8-T9H3-B and DAS-d8-T6H6) and without (AB-d10-T12,\nAB-d10-T9H3, AA-d10-T12 and AA-d10-T9H3) stacking-fault are discovered. They\npossess energetic stabilities comparable to the well-known DAS (DAS-d8-T12) and\nshow similar STM patterns, providing a plausible explanation for the\nexperimentally observed 7x7 reconstruction on the Si(111) surface. The\nfirst-principles calculations show that DAS-d8-T12, DAS-d8-T6H6, AB-d10-T12,\nand AA-d10-T12 are metallic, while DAS-d8-T9H3-A, DAS-d8-T9H3-B, AB-d10-T9H3\nand AA-d10-T9H3 are insulating phases with gaps of 0.043 eV, 0.182 eV, 0.043 eV\nand 0.059 eV, respectively. Our work demonstrates the predictability of the\nSi(111)-7x7 reconstruction and provides the structural candidates for\nunderstanding the experimentally observed metal-to-insulator transition.",
        "Fermented dairy products, including yogurt, are widely consumed for their\nnutritional and health benefits. While numerous methods exist to monitor and\nunderstand yogurt fermentation, the literature lacks an integrated evaluation\nof diverse sensing approaches within a single experimental framework. To\naddress this gap, this study systematically examines and compares multiple\nmeasurement techniques--electrical impedance, DC resistance, pH, optical\ntransparency, carbon dioxide concentration, ambient temperature, and relative\nhumidity--in tracking the yogurt fermentation process. By presenting a unified\nset of experimental results and assessing each method's observational\ncharacteristics, this work offers an encompassing reference point for\nresearchers seeking to understand the relative merits and limitations of\ndifferent sensing modalities. Rather than establishing definitive guidelines or\npractical recommendations, the findings provide a foundation for subsequent\ninvestigations into sensor-based fermentation monitoring, thereby contributing\nto a more comprehensive understanding of yogurt fermentation dynamics.",
        "In this work, we present a result on the local existence and uniqueness of\nsolutions to nonlinear Partial Differential-Algebraic Equations (PDAEs). By\napplying established theoretical results, we identify the conditions that\nguarantee the existence of a unique local solution. The analysis relies on\ntechniques from functional analysis, semi-group theory, and the theory of\ndifferential-algebraic systems. Additionally, we provide applications to\nillustrate the effectiveness of this result.",
        "Ischaemic stroke, a leading cause of death and disability, critically relies\non neuroimaging for characterising the anatomical pattern of injury.\nDiffusion-weighted imaging (DWI) provides the highest expressivity in ischemic\nstroke but poses substantial challenges for automated lesion segmentation:\nsusceptibility artefacts, morphological heterogeneity, age-related\ncomorbidities, time-dependent signal dynamics, instrumental variability, and\nlimited labelled data. Current U-Net-based models therefore underperform, a\nproblem accentuated by inadequate evaluation metrics that focus on mean\nperformance, neglecting anatomical, subpopulation, and acquisition-dependent\nvariability. Here, we present a high-performance DWI lesion segmentation tool\naddressing these challenges through optimized vision transformer-based\narchitectures, integration of 3563 annotated lesions from multi-site data, and\nalgorithmic enhancements, achieving state-of-the-art results. We further\npropose a novel evaluative framework assessing model fidelity, equity (across\ndemographics and lesion subtypes), anatomical precision, and robustness to\ninstrumental variability, promoting clinical and research utility. This work\nadvances stroke imaging by reconciling model expressivity with domain-specific\nchallenges and redefining performance benchmarks to prioritize equity and\ngeneralizability, critical for personalized medicine and mechanistic research.",
        "Analyzing crime events is crucial to understand crime dynamics and it is\nlargely helpful for constructing prevention policies. Point processes specified\non linear networks can provide a more accurate description of crime incidents\nby considering the geometry of the city. We propose a spatio-temporal Dirichlet\nprocess mixture model on a linear network to analyze crime events in Valencia,\nSpain. We propose a Bayesian hierarchical model with a Dirichlet process prior\nto automatically detect space-time clusters of the events and adopt a\nconvolution kernel estimator to account for the network structure in the city.\nFrom the fitted model, we provide crime hotspot visualizations that can inform\nsocial interventions to prevent crime incidents. Furthermore, we study the\nrelationships between the detected cluster centers and the city's amenities,\nwhich provides an intuitive explanation of criminal contagion.",
        "Node classification in graphs aims to predict the categories of unlabeled\nnodes by utilizing a small set of labeled nodes. However, weighted graphs often\ncontain noisy edges and anomalous edge weights, which can distort fine-grained\nrelationships between nodes and hinder accurate classification. We propose the\nEdge Weight-aware Graph Structure Learning (EWGSL) method, which combines\nweight learning and graph structure learning to address these issues. EWGSL\nimproves node classification by redefining attention coefficients in graph\nattention networks to incorporate node features and edge weights. It also\napplies graph structure learning to sparsify attention coefficients and uses a\nmodified InfoNCE loss function to enhance performance by adapting to denoised\ngraph weights. Extensive experimental results show that EWGSL has an average\nMicro-F1 improvement of 17.8% compared with the best baseline."
      ]
    }
  },
  {
    "id":2412.18156,
    "research_type":"applied",
    "start_id":"b2",
    "start_title":"Llamafactory: Unified efficient fine-tuning of 100+ language models",
    "start_abstract":"Efficient fine-tuning is vital for adapting large language models (LLMs) to downstream tasks. However, it requires non-trivial efforts to implement these methods on different models. We present LlamaFactory, a unified framework that integrates a suite of cutting-edge efficient training methods. It provides a solution for flexibly customizing the fine-tuning of 100+ LLMs without the need for coding through the built-in web UI LlamaBoard. We empirically validate the efficiency and effectiveness of our framework on language modeling and text generation tasks.",
    "start_categories":[
      "cs.CL"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b18"
      ],
      "title":[
        "GeneCompass: Deciphering Universal Gene Regulatory Mechanisms with Knowledge-Informed Cross-Species Foundation Model"
      ],
      "abstract":[
        "Abstract Deciphering the universal gene regulatory mechanisms in diverse organisms holds great potential to advance our knowledge of fundamental life process and facilitate research on clinical applications. However, traditional paradigm primarily focuses individual model organisms, resulting limited collection integration complex features various cell types across species. Recent breakthroughs single-cell sequencing advancements deep learning techniques present an unprecedented opportunity tackle this challenge. In study, we developed GeneCompass, first knowledge-informed, cross-species foundation pre-trained extensive dataset over 120 million transcriptomes from human mouse. During pre-training, GeneCompass effectively integrates four biological prior enhance understanding a self-supervised manner. Fine-tuning towards multiple downstream tasks, outperforms competing state-of-the-art models tasks single species unlocks new realms investigation. Overall, marks milestone advancing accelerating discovery key fate regulators candidate targets for drug development."
      ],
      "categories":[
        "q-bio.GN"
      ]
    },
    "list":{
      "title":[
        "CoverM: Read alignment statistics for metagenomics",
        "Transcriptome signature for the identification of bevacizumab responders\n  in ovarian cancer",
        "Application of Single-cell Deep Learning in Elucidating the Mapping\n  Relationship Between Visceral and Body Surface Inflammatory Patterns",
        "Deep Learning-based Feature Discovery for Decoding Phenotypic Plasticity\n  in Pediatric High-Grade Gliomas Single-Cell Transcriptomics",
        "Genotype-to-Phenotype Prediction in Rice with High-Dimensional Nonlinear\n  Features",
        "Genomic Analysis of Date Palm Fruit Size Traits and Identification of\n  Candidate Genes through GWAS",
        "Learnable Group Transform: Enhancing Genotype-to-Phenotype Prediction\n  for Rice Breeding with Small, Structured Datasets",
        "Causes of evolutionary divergence in prostate cancer",
        "Origin of $\\alpha$-satellite repeat arrays from mitochondrial molecular\n  fossils -- sequential insertion, expansion, and evolution in the nuclear\n  genome",
        "Eukaryotes evade information storage-replication rate trade-off with\n  endosymbiont assistance leading to larger genomes",
        "GiantHunter: Accurate detection of giant virus in metagenomic data using\n  reinforcement-learning and Monte Carlo tree search",
        "TopoLa: A Universal Framework to Enhance Cell Representations for\n  Single-cell and Spatial Omics through Topology-encoded Latent Hyperbolic\n  Geometry",
        "Curated loci prime editing (cliPE) for accessible multiplexed assays of\n  variant effect (MAVEs)",
        "Radiative decays of $X(3872)$ in $D{\\bar D}^*$ molecule scenario",
        "Spontaneous in-plane anomalous Hall response observed in a ferromagnetic\n  oxide",
        "Hyperbolicity and Volume of Hyperbolic Bongles",
        "Semantic Wave Functions: Exploring Meaning in Large Language Models\n  Through Quantum Formalism",
        "Solar prosumage under different pricing regimes: Interactions with the\n  transmission grid",
        "Finite-temperature bubble nucleation with shifting scale hierarchies",
        "Residually finite amenable groups that are not Hilbert-Schmidt stable",
        "Estimating Task-based Performance Bounds for Accelerated MRI Image\n  Reconstruction Methods by Use of Learned-Ideal Observers",
        "1-shifted Lie bialgebras and their quantizations",
        "Application of the Pontryagin Maximum Principle to the robust\n  time-optimal control of two-level quantum systems",
        "Ensemble control of n-level quantum systems with a scalar control",
        "A Differential Index Measuring Rater's Capability in Educational\n  Assessment",
        "Two-dimensional higher-order Weyl semimetals",
        "Background-field method and QCD factorization",
        "Powerful rank verification for multivariate Gaussian data with any\n  covariance structure"
      ],
      "abstract":[
        "Genome-centric analysis of metagenomic samples is a powerful method for\nunderstanding the function of microbial communities. Calculating read coverage\nis a central part of analysis, enabling differential coverage binning for\nrecovery of genomes and estimation of microbial community composition. Coverage\nis determined by processing read alignments to reference sequences of either\ncontigs or genomes. Per-reference coverage is typically calculated in an ad-hoc\nmanner, with each software package providing its own implementation and\nspecific definition of coverage. Here we present a unified software package\nCoverM which calculates several coverage statistics for contigs and genomes in\nan ergonomic and flexible manner. It uses 'Mosdepth arrays' for computational\nefficiency and avoids unnecessary I\/O overhead by calculating coverage\nstatistics from streamed read alignment results. CoverM is free software\navailable at https:\/\/github.com\/wwood\/coverm. CoverM is implemented in Rust,\nwith Python (https:\/\/github.com\/apcamargo\/pycoverm) and Julia\n(https:\/\/github.com\/JuliaBinaryWrappers\/CoverM_jll.jl) interfaces.",
        "The standard of care for ovarian cancer comprises cytoreductive surgery,\nfollowed by adjuvant platinum-based chemotherapy plus taxane therapy and\nmaintenance therapy with the antiangiogenic compound bevacizumab and\/or a PARP\ninhibitor. Nevertheless, there is currently no clear clinical indication for\nthe use of bevacizumab, highlighting the urgent need for biomarkers to assess\nthe response to bevacizumab. In the present study, based on a novel RNA-seq\ndataset (n=181) and a previously published microarray-based dataset (n=377), we\nhave identified an expression signature potentially associated with benefit\nfrom bevacizumab addition and assumed to reflect cancer stemness acquisition\ndriven by activation of CTCFL. Patients with this signature demonstrated\nimproved overall survival when bevacizumab was added to standard chemotherapy\nin both novel (HR=0.41(0.23-0.74), adj.p-value=7.70e-03) and previously\npublished cohorts (HR=0.51(0.34-0.75), adj.p-value=3.25e-03), while no\nsignificant differences in survival explained by treatment were observed in\npatients negative for this signature. In addition to the CTCFL signature, we\nfound several other reproducible expression signatures which may also represent\nbiomarker candidates not related to established molecular subtypes of ovarian\ncancer and require further validation studies based on additional RNA-seq data.",
        "As a system of integrated homeostasis, life is susceptible to disruptions by\nvisceral inflammation, which can disturb internal environment equilibrium. The\nrole of body-spread subcutaneous fascia (scFascia) in this process is poorly\nunderstood. In the rat model of Salmonella-induced dysentery, scRNA-seq of\nscFascia and deep-learning analysis revealed Warburg-like metabolic\nreprogramming in macrophages (MPs) with reduced citrate cycle activity.\nCd34+\/Pdgfra+ telocytes (CPTCs) regulated MPs differentiation and proliferation\nvia Wnt\/Fgf signal, suggesting a pathological crosstalk pattern in the\nscFascia, herein termed the fascia-visceral inflammatory crosstalk pattern\n(FVICP). PySCENIC analysis indicated increased activity transcription factors\nFosl1, Nfkb2, and Atf4, modulated by CPTCs signaling to MPs, downregulating\naerobic respiration and upregulating cell cycle, DNA replication, and\ntranscription. This study highlights scFascia's role in immunomodulation and\nmetabolic reprogramming during visceral inflammation, underscoring its function\nin systemic homeostasis.",
        "By use of complex network dynamics and graph-based machine learning, we\nidentified critical determinants of lineage-specific plasticity across the\nsingle-cell transcriptomics of pediatric high-grade glioma (pHGGs) subtypes:\nIDHWT glioblastoma and K27M-mutant glioma. Our study identified network\ninteractions regulating glioma morphogenesis via the tumor-immune\nmicroenvironment, including neurodevelopmental programs, calcium dynamics, iron\nmetabolism, metabolic reprogramming, and feedback loops between MAPK\/ERK and\nWNT signaling. These relationships highlight the emergence of a hybrid spectrum\nof cellular states navigating a disrupted neuro-differentiation hierarchy. We\nidentified transition genes such as DKK3, NOTCH2, GATAD1, GFAP, and SEZ6L in\nIDHWT glioblastoma, and H3F3A, ANXA6, HES6\/7, SIRT2, FXYD6, PTPRZ1, MEIS1,\nCXXC5, and NDUFAB1 in K27M subtypes. We also identified MTRNR2L1, GAPDH, IGF2,\nFKBP variants, and FXYD7 as transition genes that influence cell fate\ndecision-making across both subsystems. Our findings suggest pHGGs are\ndevelopmentally trapped in states exhibiting maladaptive behaviors, and hybrid\ncellular identities. In effect, tumor heterogeneity (metastability) and\nplasticity emerge as stress-response patterns to immune-inflammatory\nmicroenvironments and oxidative stress. Furthermore, we show that pHGGs are\nsteered by developmental trajectories from radial glia predominantly favoring\nneocortical cell fates, in telencephalon and prefrontal cortex (PFC)\ndifferentiation. By addressing underlying patterning processes and plasticity\nnetworks as therapeutic vulnerabilities, our findings provide precision\nmedicine strategies aimed at modulating glioma cell fates and overcoming\ntherapeutic resistance. We suggest transition therapy toward neuronal-like\nlineage differentiation as a potential therapy to help stabilize pHGG\nplasticity and aggressivity.",
        "Genotype-to-Phenotype prediction can promote advances in modern genomic\nresearch and crop improvement, guiding precision breeding and genomic\nselection. However, high-dimensional nonlinear features often hinder the\naccuracy of genotype-to-phenotype prediction by increasing computational\ncomplexity. The challenge also limits the predictive accuracy of traditional\napproaches. Therefore, effective solutions are needed to improve the accuracy\nof genotype-to-phenotype prediction. In our paper, we propose MLFformer.\nMLFformer is a Transformer-based architecture that incorporates the Fast\nAttention mechanism and a multilayer perceptron module to handle\nhigh-dimensional nonlinear features. In MLFformer, the Fast Attention mechanism\nis utilized to handle computational complexity and enhance processing\nefficiency. In addition, the MLP structure further captures high-dimensional\nnonlinear features. Through experiments, the results show that MLFformer\nreduces the average MAPE by 7.73% compared to the vanilla Transformer. In\nunivariate and multivariate prediction scenarios, MLFformer achieves the best\npredictive performance among all compared models.",
        "The commercial value of economically significant fruits, including date palm\nfruit (dates), is influenced by various factors, such as biochemical\ncomposition and morphological features like size, shape, and visual appearance,\nwhich are key determinants of their quality and market value. Dates are\ntypically consumed at the dry stage (Tamar), during which they exhibit a wide\nrange of physical characteristics, such as color, length, weight, and skin\nappearance. Understanding the genetic basis of these traits is crucial for\nimproving crop quality and breeding new cultivars. In this study, we integrated\na genome dataset from highly diverse date cultivars with phenotypes of dry\nfruit such as length, width, area, and weight, identifying multiple significant\ngenetic loci (SNPs) associated with these traits. We also identified candidate\ngenes located near the associated SNPs that are involved in biological\nprocesses such as cell differentiation, proliferation, growth, and the\nregulation of signalling pathways for growth regulators like auxin and abscisic\nacid, as observed in other plants. Gene expression analysis reveals that many\nof these genes are highly expressed in the early stage of fruit development\nwhen the fruit attains its maximum size and weight. These findings will enhance\nour understanding of genetic determinants of fruit size particularly at the\ncommercially important Tamar stage.",
        "Genotype-to-Phenotype (G2P) prediction plays a pivotal role in crop breeding,\nenabling the identification of superior genotypes based on genomic data. Rice\n(Oryza sativa), one of the most important staple crops, faces challenges in\nimproving yield and resilience due to the complex genetic architecture of\nagronomic traits and the limited sample size in breeding datasets. Current G2P\nprediction methods, such as GWAS and linear models, often fail to capture\ncomplex non-linear relationships between genotypes and phenotypes, leading to\nsuboptimal prediction accuracy. Additionally, population stratification and\noverfitting are significant obstacles when models are applied to small datasets\nwith diverse genetic backgrounds. This study introduces the Learnable Group\nTransform (LGT) method, which aims to overcome these challenges by combining\nthe advantages of traditional linear models with advanced machine learning\ntechniques. LGT utilizes a group-based transformation of genotype data to\ncapture spatial relationships and genetic structures across diverse rice\npopulations, offering flexibility to generalize even with limited data. Through\nextensive experiments on the Rice529 dataset, a panel of 529 rice accessions,\nLGT demonstrated substantial improvements in prediction accuracy for multiple\nagronomic traits, including yield and plant height, compared to\nstate-of-the-art baselines such as linear models and recent deep learning\napproaches. Notably, LGT achieved an R^2 improvement of up to 15\\% for yield\nprediction, significantly reducing error and demonstrating its ability to\nextract meaningful signals from high-dimensional, noisy genomic data. These\nresults highlight the potential of LGT as a powerful tool for genomic\nprediction in rice breeding, offering a promising solution for accelerating the\nidentification of high-yielding and resilient rice varieties.",
        "Cancer progression involves the sequential accumulation of genetic\nalterations that cumulatively shape the tumour phenotype. In prostate cancer,\ntumours can follow divergent evolutionary trajectories that lead to distinct\nsubtypes, but the causes of this divergence remain unclear. While causal\ninference could elucidate the factors involved, conventional methods are\nunsuitable due to the possibility of unobserved confounders and ambiguity in\nthe direction of causality. Here, we propose a method that circumvents these\nissues and apply it to genomic data from 829 prostate cancer patients. We\nidentify several genetic alterations that drive divergence as well as others\nthat prevent this transition, locking tumours into one trajectory. Further\nanalysis reveals that these genetic alterations may cause each other, implying\na positive-feedback loop that accelerates divergence. Our findings provide\ninsights into how cancer subtypes emerge and offer a foundation for genomic\nsurveillance strategies aimed at monitoring the progression of prostate cancer.",
        "Alpha satellite DNA is large tandem arrays of 150-400 bp units, and its\norigin remains an evolutionary mystery. In this research, we identified 1,545\nalpha-satellite-like (SatL) repeat units in the nuclear genome of jewel wasp\nNasonia vitripennis. Among them, thirty-nine copies of SatL were organized in\ntwo palindromic arrays in mitochondria, resulting in a 50% increase in the\ngenome size. Strikingly, genomic neighborhood analyses of 1,516 nuclear SatL\nrepeats revealed that they are located in NuMT (nuclear mitochondrial DNA)\nregions, and SatL phylogeny matched perfectly with mitochondrial genes and NuMT\npseudogenes. These results support that SatL arrays originated from ten\nindependent mitochondria insertion events into the nuclear genome within the\nlast 500,000 years, after divergence from its sister species N. giraulti.\nDramatic repeat GC-percent elevation (from 33.9% to 50.4%) is a hallmark of\nrapid SatL sequence evolution in mitochondria due to GC-biased gene conversion\nfacilitated by the palindromic sequence pairing of the two mitochondrial SatL\narrays. The nuclear SatL repeat arrays underwent substantial copy number\nexpansion, from 12-15 (SatL1) to over 400 copies (SatL4). The oldest SatL4B\narray consists of four types of repeat units derived from deletions in the\nAT-rich region of ancestral repeats, and complex high-order structures have\nevolved through duplications. We also discovered similar repeat insertions into\nthe nuclear genome of Muscidifurax, suggesting this mechanism can be common in\ninsects. This is the first report of the mitochondrial origin of nuclear\nsatellite sequences, and our findings shed new light on the origin and\nevolution of satellite DNA.",
        "Genome length varies widely among organisms, from compact genomes of\nprokaryotes to vast and complex genomes of eukaryotes. In this study, we\ntheoretically identify the evolutionary pressures that may have driven this\ndivergence in genome length. We use a parameter-free model to study genome\nlength evolution under selection pressure to minimize replication time and\nmaximize information storage capacity. We show that prokaryotes tend to reduce\ngenome length, constrained by a single replication origin, while eukaryotes\nexpand their genomes by incorporating multiple replication origins. We propose\na connection between genome length and cellular energetics, suggesting that\nendosymbiotic organelles, mitochondria and chloroplasts, evolutionarily\nregulate the number of replication origins, thereby influencing genome length\nin eukaryotes. We show that the above two selection pressures also lead to\nstrict equalization of the number of purines and their corresponding\nbase-pairing pyrimidines within a single DNA strand, known as Chagraff's second\nparity rule, a hitherto unexplained observation in genomes of nearly all known\nspecies. This arises from the symmetrization of replichore length, another\nobservation that has been shown to hold across species, which our model\nreproduces. The model also reproduces other experimentally observed phenomena,\nsuch as a general preference for deletions over insertions, and elongation and\nhigh variance of genome lengths under reduced selection pressure for\nreplication rate, termed the C-value paradox. We highlight the possibility of\nregulation of the firing of latent replication origins in response to cues from\nthe extracellular environment leading to the regulation of cell cycle rates in\nmulticellular eukaryotes.",
        "Motivation: Nucleocytoplasmic large DNA viruses (NCLDVs) are notable for\ntheir large genomes and extensive gene repertoires, which contribute to their\nwidespread environmental presence and critical roles in processes such as host\nmetabolic reprogramming and nutrient cycling. Metagenomic sequencing has\nemerged as a powerful tool for uncovering novel NCLDVs in environmental\nsamples. However, identifying NCLDV sequences in metagenomic data remains\nchallenging due to their high genomic diversity, limited reference genomes, and\nshared regions with other microbes. Existing alignment-based and machine\nlearning methods struggle with achieving optimal trade-offs between sensitivity\nand precision. Results: In this work, we present GiantHunter, a reinforcement\nlearning-based tool for identifying NCLDVs from metagenomic data. By employing\na Monte Carlo tree search strategy, GiantHunter dynamically selects\nrepresentative non-NCLDV sequences as the negative training data, enabling the\nmodel to establish a robust decision boundary. Benchmarking on rigorously\ndesigned experiments shows that GiantHunter achieves high precision while\nmaintaining competitive sensitivity, improving the F1-score by 10% and reducing\ncomputational cost by 90% compared to the second-best method. To demonstrate\nits real-world utility, we applied GiantHunter to 60 metagenomic datasets\ncollected from six cities along the Yangtze River, located both upstream and\ndownstream of the Three Gorges Dam. The results reveal significant differences\nin NCLDV diversity correlated with proximity to the dam, likely influenced by\nreduced flow velocity caused by the dam. These findings highlight the potential\nof GiantSeeker to advance our understanding of NCLDVs and their ecological\nroles in diverse environments.",
        "Recent advances in cellular research demonstrate that scRNA-seq characterizes\ncellular heterogeneity, while spatial transcriptomics reveals the spatial\ndistribution of gene expression. Cell representation is the fundamental issue\nin the two fields. Here, we propose Topology-encoded Latent Hyperbolic Geometry\n(TopoLa), a computational framework enhancing cell representations by capturing\nfine-grained intercellular topological relationships. The framework introduces\na new metric, TopoLa distance (TLd), which quantifies the geometric distance\nbetween cells within latent hyperbolic space, capturing the network's\ntopological structure more effectively. With this framework, the cell\nrepresentation can be enhanced considerably by performing convolution on its\nneighboring cells. Performance evaluation across seven biological tasks,\nincluding scRNA-seq data clustering and spatial transcriptomics domain\nidentification, shows that TopoLa significantly improves the performance of\nseveral state-of-the-art models. These results underscore the generalizability\nand robustness of TopoLa, establishing it as a valuable tool for advancing both\nbiological discovery and computational methodologies.",
        "Multiplexed assays of variant effect (MAVEs) perform simultaneous\ncharacterization of many variants. Prime editing has been recently adopted for\nintroducing many variants in their native genomic contexts. However, robust\nprotocols and standards are limited, preventing widespread uptake. Herein, we\ndescribe curated loci prime editing (cliPE) which is an accessible, low-cost\nexperimental pipeline to perform MAVEs using prime editing of a target gene, as\nwell as a companion Shiny app (pegRNA Designer) to rapidly and easily design\nuser-specific MAVE libraries.",
        "We investigate the radiative decays of the $X(3872)$ to $\\gamma\nV~(V=\\rho^0,\\, \\omega)$ in the molecule scenario, where the $X(3872)$ is\nregarded as a pure hadronic molecule of the $D\\bar{D}^*+c.c$ in an $S$-wave\nwith the quantum numbers $J^{PC}=1^{++}$. The radiative processes were assumed\nto occur via the triangle hadronic loops, and the relevant calculations were\nconducted using an effective Lagrangian approach. It is found that the absolute\ndecay widths are model-dependent, but the relative width ratio is rather\nindependent of the model parameter. Moreover, the calculated results indicate\nthat the radiative decays of the $X(3872)$ are strongly influenced by the\nmolecular configuration characterized by the proportion of the charged and\nneutral constituents. We hope that the present calculations could be tested by\nthe experimental measurements.",
        "Recent observation of anomalous Hall effect (AHE) induced by magnetic field\nor spin magnetization lying in the Hall deflection plane has sparked interest\nin diverse mechanisms for inducing the Hall vector component perpendicular to\nthe applied magnetic field. Such off-diagonal coupling, which is strictly\nconstrained by symmetry of the system, provides new degrees of freedom for\nengineering Hall responses. However, spontaneous response as extensively\nstudied for out-of-plane AHE remains unexplored. Here we elucidate in-plane AHE\nin a typical ferromagnetic oxide SrRuO$_3$. The (111)-orientated ultrathin\nfilms with in-plane easy axes of spin magnetization exhibit spontaneous AHE at\nzero field, which is intrinsically coupled to the in-plane spin magnetization\nand controllable via its direction. Systematic measurements by varying\nazimuthal and polar field angles further reveal complex Hall responses shaped\nby higher-order terms allowed by trigonal distortion of the films. Our findings\nhighlight versatile and controllable in-plane Hall responses with out-of-plane\norbital ferromagnetism.",
        "We consider a simple but infinite class of staked links known as bongles. We\nprovide necessary and sufficient conditions for these bongles to be hyperbolic.\nThen, we prove that all balanced hyperbolic $n$-bongles have the same volume\nand the corresponding volume is an upper bound on the volume of any hyperbolic\n$n$-bongle for $n$ even. Moreover, all hyperbolic $n$-bongles have volume\nstrictly less than $5n(1.01494\\dots)$. We also include explicit volume\ncalculations for all hyperbolic 3-bongles through 6-bongles.",
        "Large Language Models (LLMs) encode semantic relationships in\nhigh-dimensional vector embeddings. This paper explores the analogy between LLM\nembedding spaces and quantum mechanics, positing that LLMs operate within a\nquantized semantic space where words and phrases behave as quantum states. To\ncapture nuanced semantic interference effects, we extend the standard\nreal-valued embedding space to the complex domain, drawing parallels to the\ndouble-slit experiment. We introduce a \"semantic wave function\" to formalize\nthis quantum-derived representation and utilize potential landscapes, such as\nthe double-well potential, to model semantic ambiguity. Furthermore, we propose\na complex-valued similarity measure that incorporates both magnitude and phase\ninformation, enabling a more sensitive comparison of semantic representations.\nWe develop a path integral formalism, based on a nonlinear Schr\\\"odinger\nequation with a gauge field and Mexican hat potential, to model the dynamic\nevolution of LLM behavior. This interdisciplinary approach offers a new\ntheoretical framework for understanding and potentially manipulating LLMs, with\nthe goal of advancing both artificial and natural language understanding.",
        "Solar prosumers, residential electricity consumers equipped with photovoltaic\n(PV) systems and battery storage, are transforming electricity markets. Their\ninteractions with the transmission grid under varying tariff designs are not\nyet fully understood. We explore the influence of different pricing regimes on\nprosumer investment and dispatch decisions and their subsequent impact on the\ntransmission grid. Using an integrated modeling approach that combines two\nopen-source dispatch, investment and grid models, we simulate prosumage\nbehavior in Germany's electricity market under real-time pricing or\ntime-invariant pricing, as well as under zonal or nodal pricing. Our findings\nshow that zonal pricing favors prosumer investments, while time-invariant\npricing rather hinders it. In comparison, regional solar availability emerges\nas a larger driver for rooftop PV investments. The impact of prosumer\nstrategies on grid congestion remains limited within the scope of our\nmodel-setup, in which home batteries cannot be used for energy arbitrage.",
        "Focusing on supercooled phase transitions in models with classical scale\nsymmetry, we formulate a state-of-the art framework for computing the\nbubble-nucleation rate, accounting for the presence of various energy scales.\nIn particular, we examine the limitations of derivative expansions in\nconstructing a thermal effective field theory for bubble nucleation. We show\nthat for gauge field fluctuations, derivative expansions diverge after the\nleading two orders due to the strong variation in gauge field masses between\nthe high- and low-temperature phases. By directly computing these contributions\nusing the fluctuation determinant, we capture these effects while also\naccounting for large explicit logarithms at two loops, utilising the exact\nrenormalisation group structure of the EFT. Finally, we demonstrate how this\napproach significantly improves nucleation rate calculations compared to\nleading-order results, providing a more robust framework for predicting\ngravitational-wave signals from supercooled phase transitions in models such as\nthe SU(2)cSM.",
        "We construct the first examples of residually finite amenable groups that are\nnot Hilbert-Schmidt (HS) stable. We construct finitely generated, class 3\nnilpotent by cyclic examples and solvable linear finitely presented examples.\nThis also provides the first examples of amenable groups that are very flexibly\nHS-stable but not flexibly HS-stable and the first examples of residually\nfinite amenable groups that are not locally HS-stable. Along the way we exhibit\n(necessarily not-finitely-generated) class 2 nilpotent groups $G = A\\rtimes \\Z$\nwith $A$ abelian such that the periodic points of the dual action are dense but\nit does not admit dense periodic measures. Finally we use the\nTikuisis-White-Winter theorem to show all of the examples are not even\noperator-HS-stable; they admit operator norm almost homomorphisms that can not\nbe HS-perturbed to true homomorphisms.",
        "Medical imaging systems are commonly assessed and optimized by the use of\nobjective measures of image quality (IQ). The performance of the ideal observer\n(IO) acting on imaging measurements has long been advocated as a\nfigure-of-merit to guide the optimization of imaging systems. For computed\nimaging systems, the performance of the IO acting on imaging measurements also\nsets an upper bound on task-performance that no image reconstruction method can\ntranscend. As such, estimation of IO performance can provide valuable guidance\nwhen designing under-sampled data-acquisition techniques by enabling the\nidentification of designs that will not permit the reconstruction of\ndiagnostically inappropriate images for a specified task - no matter how\nadvanced the reconstruction method is or how plausible the reconstructed images\nappear. The need for such analysis is urgent because of the substantial\nincrease of medical device submissions on deep learning-based image\nreconstruction methods and the fact that they may produce clean images\ndisguising the potential loss of diagnostic information when data is\naggressively under-sampled. Recently, convolutional neural network (CNN)\napproximated IOs (CNN-IOs) was investigated for estimating the performance of\ndata space IOs to establish task-based performance bounds for image\nreconstruction, under an X-ray computed tomographic (CT) context. In this work,\nthe application of such data space CNN-IO analysis to multi-coil magnetic\nresonance imaging (MRI) systems has been explored. This study utilized stylized\nmulti-coil sensitivity encoding (SENSE) MRI systems and deep-generated\nstochastic brain models to demonstrate the approach. Signal-known-statistically\nand background-known-statistically (SKS\/BKS) binary signal detection tasks were\nselected to study the impact of different acceleration factors on the data\nspace IO performance.",
        "In this paper, we define (cohomologically) 1-shifted Manin triples and\n1-shifted Lie bialgebras, and study their properties. We derive many results\nthat are parallel to those found in ordinary Lie bialgebras, including the\ndouble construction and the existence of a 1-shifted $r$-matrix satisfying the\nclassical Yang-Baxter equation.\n  Turning to quantization, we first construct a canonical quantization for each\n1-shifted metric Lie algebra $\\mathfrak{g}$, producing a deformation to the\nsymmetric monoidal category of $\\mathfrak{g}$ modules over a formal variable\n$\\hbar$. This quantization is in terms of a curved differential graded algebra.\nUnder a further technical assumption, we construct quantizations of transverse\nLagrangian subalgebras of $\\mathfrak{g}$, which is a pair of DG algebras\nconnected by Koszul duality, and give rise to monoidal module categories of the\nquantized double.\n  Finally, we apply this to Manin triples arising from Lie algebras of loop\ngroups, and construct 1-shifted meromorphic $r$-matrices. The resulting\nquantizations are the cohomologically-shifted analogue of Yangians.",
        "We study the time-optimal robust control of a two-level quantum system\nsubjected to field inhomogeneities. We apply the Pontryagin Maximum Principle\nand we introduce a reduced space onto which the optimal dynamics is projected\ndown. This reduction leads to a complete analytical derivation of the optimal\nsolution in terms of elliptic functions and elliptic integrals. Necessary\noptimality conditions are then obtained for the original system. These\nconditions are verified numerically and lead to the optimal control protocol.\nVarious examples, ranging from state-to-state transfer to the generation of a\nNot gate, illustrate this study. The connection with other geometric\noptimization approaches that have been used to solve this problem is also\ndiscussed.",
        "In this paper we discuss how a general bilinear finite-dimensional closed\nquantum system with dispersed parameters can be steered between eigenstates. We\nshow that, under suitable conditions on the separation of spectral gaps and the\nboundedness of parameter dispersion, rotating wave and adiabatic approximations\ncan be employed in cascade to achieve population inversion between arbitrary\neigenstates. We propose an explicit control law and test numerically the\nsharpness of the conditions on several examples.",
        "A rater's ability to assign accurate scores can significantly impact the\noutcomes of educational assessments. However, common indices for evaluating\nrater characteristics typically focus on either their severity or their\ndiscrimination ability (i.e., skills to differentiate between students).\nAdditionally, these indices are often developed without considering the rater's\naccuracy in scoring students at different ability levels. To address the\nlimitations, this study proposes a single-value measure to assess a rater's\ncapability of assigning accurate scores to students with varying ability\nlevels. The measure is derived from the partial derivatives of each rater's\npassing rate concerning student ability. Mathematical derivations of the index\nunder generalized multi-facet models and hierarchical rater models are\nprovided. To ease the implementation of the index, this study develops\nparameter estimation using marginal likelihood and its Laplacian approximation\nwhich allows for efficient evaluation and processing of large datasets\ninvolving numerous students and raters. Simulation studies demonstrate the\naccuracy of parameter recovery using the approximate likelihood and show how\nthe capability indices vary with different levels of rater severity. An\nempirical study further tests the practical applicability of the new measure,\nwhere raters evaluate essays on four topics: \"family,\" \"school,\" \"sport,\" and\n\"work.\" Results show that raters are most capable when rating the topic of\nfamily and least capable when rating sport, with individual raters displaying\ndifferent capabilities across the various topics.",
        "We propose a theoretical scheme to realize two-dimensional higher-order Weyl\nsemimetals using a trilayer topological insulator film coupled with a d-wave\naltermagnet. Our results show that the trilayer topological insulator exhibits\ntwo-dimensional Weyl semimetal characteristics with helical edge states.\nNotably, the Weyl points are located at four high-symmetry points in the\nBrillouin zone, and the topology of symmetric subspaces governs the formation\nof these Weyl points and edge states. Upon introducing a d-wave altermagnet\noriented along the z-direction, gaps open in the helical edge states while\npreserving two Weyl points, leading to the realization of two-dimensional\nhigher-order Weyl semimetals hosting topological corner states. The nonzero\nwinding number in the subspace along the high-symmetry line serves as a\ntopological invariant characterizing these corner states, and the other\nsubspace Hamiltonian confirms the existence of the Weyl points. Finally, a\ntopological phase diagram provides a complete topological description of the\nsystem.",
        "One method for deriving a factorization for QCD processes is to use\nsuccessive integration over fields in the functional integral. In this\napproach, we separate the fields into two categories: dynamical fields with\nmomenta above a relevant cutoff, and background fields with momenta below the\ncutoff. The dynamical fields are then integrated out in the background of the\nlow-momentum background fields. This strategy works well at tree level,\nallowing us to quickly derive QCD factorization formulas at leading order.\nHowever, to extend the approach to higher loops, it is necessary to rigorously\ndefine the functional integral over dynamical fields in an arbitrary background\nfield. This framework was carefully developed for the calculation of the\neffective action in a background field at the two-loop level in the classic\npaper by Abbott [1]. Building on this work, I specify the renormalized\nbackground-field Lagrangian and define the notion of the quantum average of an\noperator in a background field, consistent with the ``separation of scales''\nscheme mentioned earlier. As examples, I discuss the evolution of the twist-2\ngluon light-ray operator and the one-loop gluon propagator in a background\nfield near the light cone.",
        "Upon observing $n$-dimensional multivariate Gaussian data, when can we infer\nthat the largest $K$ observations came from the largest $K$ means? When $K=1$\nand the covariance is isotropic, \\cite{Gutmann} argue that this inference is\njustified when the two-sided difference-of-means test comparing the largest and\nsecond largest observation rejects. Leveraging tools from selective inference,\nwe provide a generalization of their procedure that applies for both any $K$\nand any covariance structure. We show that our procedure draws the desired\ninference whenever the two-sided difference-of-means test comparing the pair of\nobservations inside and outside the top $K$ with the smallest standardized\ndifference rejects, and sometimes even when this test fails to reject. Using\nthis insight, we argue that our procedure renders existing simultaneous\ninference approaches inadmissible when $n > 2$. When the observations are\nindependent (with possibly unequal variances) or equicorrelated, our procedure\ncorresponds exactly to running the two-sided difference-of-means test comparing\nthe pair of observations inside and outside the top $K$ with the smallest\nstandardized difference."
      ]
    }
  },
  {
    "id":2412.18156,
    "research_type":"applied",
    "start_id":"b18",
    "start_title":"GeneCompass: Deciphering Universal Gene Regulatory Mechanisms with Knowledge-Informed Cross-Species Foundation Model",
    "start_abstract":"Abstract Deciphering the universal gene regulatory mechanisms in diverse organisms holds great potential to advance our knowledge of fundamental life process and facilitate research on clinical applications. However, traditional paradigm primarily focuses individual model organisms, resulting limited collection integration complex features various cell types across species. Recent breakthroughs single-cell sequencing advancements deep learning techniques present an unprecedented opportunity tackle this challenge. In study, we developed GeneCompass, first knowledge-informed, cross-species foundation pre-trained extensive dataset over 120 million transcriptomes from human mouse. During pre-training, GeneCompass effectively integrates four biological prior enhance understanding a self-supervised manner. Fine-tuning towards multiple downstream tasks, outperforms competing state-of-the-art models tasks single species unlocks new realms investigation. Overall, marks milestone advancing accelerating discovery key fate regulators candidate targets for drug development.",
    "start_categories":[
      "q-bio.GN"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b2"
      ],
      "title":[
        "Llamafactory: Unified efficient fine-tuning of 100+ language models"
      ],
      "abstract":[
        "Efficient fine-tuning is vital for adapting large language models (LLMs) to downstream tasks. However, it requires non-trivial efforts to implement these methods on different models. We present LlamaFactory, a unified framework that integrates a suite of cutting-edge efficient training methods. It provides a solution for flexibly customizing the fine-tuning of 100+ LLMs without the need for coding through the built-in web UI LlamaBoard. We empirically validate the efficiency and effectiveness of our framework on language modeling and text generation tasks."
      ],
      "categories":[
        "cs.CL"
      ]
    },
    "list":{
      "title":[
        "WindowKV: Task-Adaptive Group-Wise KV Cache Window Selection for\n  Efficient LLM Inference",
        "FACTS&EVIDENCE: An Interactive Tool for Transparent Fine-Grained Factual\n  Verification of Machine-Generated Text",
        "SurveyX: Academic Survey Automation via Large Language Models",
        "Do as We Do, Not as You Think: the Conformity of Large Language Models",
        "Plan-and-Act: Improving Planning of Agents for Long-Horizon Tasks",
        "Story Grammar Semantic Matching for Literary Study",
        "Controllable Emotion Generation with Emotion Vectors",
        "RA-MTR: A Retrieval Augmented Multi-Task Reader based Approach for\n  Inspirational Quote Extraction from Long Documents",
        "Southern Newswire Corpus: A Large-Scale Dataset of Mid-Century Wire\n  Articles Beyond the Front Page",
        "A Study of the Plausibility of Attention between RNN Encoders in Natural\n  Language Inference",
        "Benchmarking Abstractive Summarisation: A Dataset of Human-authored\n  Summaries of Norwegian News Articles",
        "HonkaiChat: Companions from Anime that feel alive!",
        "Automating Mathematical Proof Generation Using Large Language Model\n  Agents and Knowledge Graphs",
        "Computation of the Hilbert Series for the Support-Minors Modeling of the\n  MinRank Problem",
        "A glimpse into an effective world",
        "Imprints of an early matter-dominated era arising from dark matter\n  dilution mechanism on cosmic string dynamics and gravitational wave\n  signatures",
        "Giant Kohn anomaly and chiral phonons in the charge density wave phase\n  of 1H-NbSe$_2$",
        "A study on $T$-equivalent graphs",
        "Criteria for unbiased estimation: applications to noise-agnostic sensing\n  and learnability of quantum channel",
        "Long-Term Planning Around Humans in Domestic Environments with 3D Scene\n  Graphs",
        "Applied Machine Learning Methods with Long-Short Term Memory Based\n  Recurrent Neural Networks for Multivariate Temperature Prediction",
        "GenMetaLoc: Learning to Learn Environment-Aware Fingerprint Generation\n  for Sample Efficient Wireless Localization",
        "On the structure of some one-generator nilpotent braces",
        "Multi-Instance Partial-Label Learning with Margin Adjustment",
        "A Systematic Review of ECG Arrhythmia Classification: Adherence to\n  Standards, Fair Evaluation, and Embedded Feasibility",
        "Evaluation of Large Language Models via Coupled Token Generation",
        "Hand-Object Contact Detection using Grasp Quality Metrics",
        "Processes on Wasserstein spaces and energy-minimizing particle\n  representations in fractional Sobolev spaces"
      ],
      "abstract":[
        "With the advancements in long-context inference capabilities of large\nlanguage models (LLMs), the KV cache has become one of the foundational\ncomponents. However, its substantial GPU memory consumption makes KV cache\ncompression a key technique for enabling efficient LLM inference in industrial\nscenarios. While recent studies have focused on optimizing the memory occupied\nby the KV cache, they overlook two critical factors: preserving semantic\ncoherence and considering task-specific characteristic during compression. To\naddress these limitations, we propose a novel task-adaptive KV cache window\nselection method, WindowKV. WindowKV dynamically selects local semantic windows\nconsisting of consecutive tokens, according to task-specific characteristics,\nensuring the retained KV cache captures continuous, essential context.\nAdditionally, we introduce an intra-group layer KV cache indices sharing\nstrategy to reduce computational overhead, achieving a balance between\nperformance and efficiency. We rigorously evaluate WindowKV on the LongBench\nbenchmark, and the results demonstrate that it maintains a performance\ncomparable to full KV cache retention while using only 12% of the original KV\ncache, significantly reducing memory requirements. Furthermore, our method also\nachieves state-of-the-art results in the Needle-in-a-Haystack evaluation,\nhighlighting its effectiveness and robustness.",
        "With the widespread consumption of AI-generated content, there has been an\nincreased focus on developing automated tools to verify the factual accuracy of\nsuch content. However, prior research and tools developed for fact verification\ntreat it as a binary classification or a linear regression problem. Although\nthis is a useful mechanism as part of automatic guardrails in systems, we argue\nthat such tools lack transparency in the prediction reasoning and diversity in\nsource evidence to provide a trustworthy user experience. We develop\nFacts&Evidence - an interactive and transparent tool for user-driven\nverification of complex text. The tool facilitates the intricate\ndecision-making involved in fact-verification, presenting its users a breakdown\nof complex input texts to visualize the credibility of individual claims along\nwith an explanation of model decisions and attribution to multiple, diverse\nevidence sources. Facts&Evidence aims to empower consumers of machine-generated\ntext and give them agency to understand, verify, selectively trust and use such\ntext.",
        "Large Language Models (LLMs) have demonstrated exceptional comprehension\ncapabilities and a vast knowledge base, suggesting that LLMs can serve as\nefficient tools for automated survey generation. However, recent research\nrelated to automated survey generation remains constrained by some critical\nlimitations like finite context window, lack of in-depth content discussion,\nand absence of systematic evaluation frameworks. Inspired by human writing\nprocesses, we propose SurveyX, an efficient and organized system for automated\nsurvey generation that decomposes the survey composing process into two phases:\nthe Preparation and Generation phases. By innovatively introducing online\nreference retrieval, a pre-processing method called AttributeTree, and a\nre-polishing process, SurveyX significantly enhances the efficacy of survey\ncomposition. Experimental evaluation results show that SurveyX outperforms\nexisting automated survey generation systems in content quality (0.259\nimprovement) and citation quality (1.76 enhancement), approaching human expert\nperformance across multiple evaluation dimensions. Examples of surveys\ngenerated by SurveyX are available on www.surveyx.cn",
        "Recent advancements in large language models (LLMs) revolutionize the field\nof intelligent agents, enabling collaborative multi-agent systems capable of\ntackling complex problems across various domains. However, the potential of\nconformity within these systems, analogous to phenomena like conformity bias\nand groupthink in human group dynamics, remains largely unexplored, raising\nconcerns about their collective problem-solving capabilities and possible\nethical implications. This paper presents a comprehensive study on conformity\nin LLM-driven multi-agent systems, focusing on three aspects: the existence of\nconformity, the factors influencing conformity, and potential mitigation\nstrategies. In particular, we introduce BenchForm, a new conformity-oriented\nbenchmark, featuring reasoning-intensive tasks and five distinct interaction\nprotocols designed to probe LLMs' behavior in collaborative scenarios. Several\nrepresentative LLMs are evaluated on BenchForm, using metrics such as\nconformity rate and independence rate to quantify conformity's impact. Our\nanalysis delves into factors influencing conformity, including interaction time\nand majority size, and examines how the subject agent rationalizes its\nconforming behavior. Furthermore, we explore two strategies to mitigate\nconformity effects, i.e., developing enhanced personas and implementing a\nreflection mechanism. Several interesting findings regarding LLMs' conformity\nare derived from empirical results and case studies. We hope that these\ninsights can pave the way for more robust and ethically-aligned collaborative\nAI systems. Our benchmark and code are available at BenchForm.",
        "Large language models (LLMs) have shown remarkable advancements in enabling\nlanguage agents to tackle simple tasks. However, applying them for complex,\nmulti-step, long-horizon tasks remains a challenge. Recent work have found\nsuccess by separating high-level planning from low-level execution, which\nenables the model to effectively balance high-level planning objectives and\nlow-level execution details. However, generating accurate plans remains\ndifficult since LLMs are not inherently trained for this task. To address this,\nwe propose Plan-and-Act, a novel framework that incorporates explicit planning\ninto LLM-based agents and introduces a scalable method to enhance plan\ngeneration through a novel synthetic data generation method. Plan-and-Act\nconsists of a Planner model which generates structured, high-level plans to\nachieve user goals, and an Executor model that translates these plans into\nenvironment-specific actions. To train the Planner effectively, we introduce a\nsynthetic data generation method that annotates ground-truth trajectories with\nfeasible plans, augmented with diverse and extensive examples to enhance\ngeneralization. We evaluate Plan-and-Act using web navigation as a\nrepresentative long-horizon planning environment, demonstrating a state-of\nthe-art 54% success rate on the WebArena-Lite benchmark.",
        "In Natural Language Processing (NLP), semantic matching algorithms have\ntraditionally relied on the feature of word co-occurrence to measure semantic\nsimilarity. While this feature approach has proven valuable in many contexts,\nits simplistic nature limits its analytical and explanatory power when used to\nunderstand literary texts. To address these limitations, we propose a more\ntransparent approach that makes use of story structure and related elements.\nUsing a BERT language model pipeline, we label prose and epic poetry with story\nelement labels and perform semantic matching by only considering these labels\nas features. This new method, Story Grammar Semantic Matching, guides literary\nscholars to allusions and other semantic similarities across texts in a way\nthat allows for characterizing patterns and literary technique.",
        "In recent years, technologies based on large-scale language models (LLMs)\nhave made remarkable progress in many fields, especially in customer service,\ncontent creation, and embodied intelligence, showing broad application\npotential. However, The LLM's ability to express emotions with proper tone,\ntiming, and in both direct and indirect forms is still insufficient but\nsignificant. Few works have studied on how to build the controlable emotional\nexpression capability of LLMs. In this work, we propose a method for emotion\nexpression output by LLMs, which is universal, highly flexible, and well\ncontrollable proved with the extensive experiments and verifications. This\nmethod has broad application prospects in fields involving emotions output by\nLLMs, such as intelligent customer service, literary creation, and home\ncompanion robots. The extensive experiments on various LLMs with different\nmodel-scales and architectures prove the versatility and the effectiveness of\nthe proposed method.",
        "Inspirational quotes from famous individuals are often used to convey\nthoughts in news articles, essays, and everyday conversations. In this paper,\nwe propose a novel context-based quote extraction system that aims to extract\nthe most relevant quote from a long text. We formulate this quote extraction as\nan open domain question answering problem first by employing a vector-store\nbased retriever and then applying a multi-task reader. We curate three\ncontext-based quote extraction datasets and introduce a novel multi-task\nframework RA-MTR that improves the state-of-the-art performance, achieving a\nmaximum improvement of 5.08% in BoW F1-score.",
        "I introduce a new large-scale dataset of historical wire articles from U.S.\nSouthern newspapers, spanning 1960-1975 and covering multiple wire services:\nThe Associated Press, United Press International, Newspaper Enterprise\nAssociation. Unlike prior work focusing on front-page content, this dataset\ncaptures articles across the entire newspaper, offering broader insight into\nmid-century Southern coverage. The dataset includes a version that has\nundergone an LLM-based text cleanup pipeline to reduce OCR noise, enhancing its\nsuitability for quantitative text analysis. Additionally, duplicate versions of\narticles are retained to enable analysis of editorial differences in language\nand framing across newspapers. Each article is tagged by wire service,\nfacilitating comparative studies of editorial patterns across agencies. This\nresource opens new avenues for research in computational social science,\ndigital humanities, and historical linguistics, providing a detailed\nperspective on how Southern newspapers relayed national and international news\nduring a transformative period in American history. The dataset will be made\navailable upon publication or request for research purposes.",
        "Attention maps in neural models for NLP are appealing to explain the decision\nmade by a model, hopefully emphasizing words that justify the decision. While\nmany empirical studies hint that attention maps can provide such justification\nfrom the analysis of sound examples, only a few assess the plausibility of\nexplanations based on attention maps, i.e., the usefulness of attention maps\nfor humans to understand the decision. These studies furthermore focus on text\nclassification. In this paper, we report on a preliminary assessment of\nattention maps in a sentence comparison task, namely natural language\ninference. We compare the cross-attention weights between two RNN encoders with\nhuman-based and heuristic-based annotations on the eSNLI corpus. We show that\nthe heuristic reasonably correlates with human annotations and can thus\nfacilitate evaluation of plausible explanations in sentence comparison tasks.\nRaw attention weights however remain only loosely related to a plausible\nexplanation.",
        "We introduce a dataset of high-quality human-authored summaries of news\narticles in Norwegian. The dataset is intended for benchmarking the abstractive\nsummarisation capabilities of generative language models. Each document in the\ndataset is provided with three different candidate gold-standard summaries\nwritten by native Norwegian speakers, and all summaries are provided in both of\nthe written variants of Norwegian -- Bokm{\\aa}l and Nynorsk. The paper\ndescribes details on the data creation effort as well as an evaluation of\nexisting open LLMs for Norwegian on the dataset. We also provide insights from\na manual human evaluation, comparing human-authored to model-generated\nsummaries. Our results indicate that the dataset provides a challenging LLM\nbenchmark for Norwegian summarisation capabilities",
        "Modern conversational agents, including anime-themed chatbots, are frequently\nreactive and personality-driven but fail to capture the dynamic nature of human\ninteractions. We propose an event-driven dialogue framework to address these\nlimitations by embedding dynamic events in conversation prompts and fine-tuning\nmodels on character-specific data. Evaluations on GPT-4 and comparisons with\nindustry-leading baselines demonstrate that event-driven prompts significantly\nimprove conversational engagement and naturalness while reducing\nhallucinations. This paper explores the application of this approach in\ncreating lifelike chatbot interactions within the context of Honkai: Star Rail,\nshowcasing the potential for dynamic event-based systems to transform\nrole-playing and interactive dialogue.",
        "Large Language Models have demonstrated remarkable capabilities in natural\nlanguage processing tasks, including mathematical problem-solving that requires\nmulti-step logical reasoning. However, challenges persist in automating the\nidentification of key mathematical concepts, understanding their\ninterrelations, and formalizing proofs within a rigorous framework. We present\na novel framework that leverages knowledge graphs to augment LLMs to construct\nand formalize mathematical proofs. Our results demonstrate significant\nperformance improvements across multiple datasets, with using knowledge graphs,\nachieving up to a 34% success rate on the MUSTARDSAUCE dataset on o1-mini and\nconsistently outperforming baseline approaches by 2-11% across different\nmodels. We show how this approach bridges the gap between natural language\nunderstanding and formal logic proof systems and achieve elevated results for\nfoundation models over baseline.",
        "The MinRank problem is a simple linear algebra problem: given matrices with\ncoefficients in a field, find a non trivial linear combination of the matrices\nthat has a small rank. There are several algebraic modeling of the problem. The\nmain ones are: the Kipnis-Shamir modeling, the Minors modeling and the\nSupport-Minors modeling. The Minors modeling has been studied by Faug\\`ere et\nal. in 2010, where the authors provide an analysis of the complexity of\ncomputing a Gr\\\"obner basis of the modeling, through the computation of the\nexact Hilbert Series for a generic instance. For the Support-Minors modeling,\nthe first terms of the Hilbert Series are given by Bardet et al. in 2020 based\non an heuristic and experimental work. In this work, we provide a formula and a\nproof for the complete Hilbert Series of the Support Minors modeling for\ngeneric instances. This is done by adapting well known results on determinantal\nideals to an ideal generated by a particular subset of the set of all minors of\na matrix of variables. We then show that this ideal is generated by a\nparticular subset of the set of all minors of a matrix of variables. We then\nshow that this ideal is generated by standard monomials having a particular\nshape, and derive the Hilbert Series by counting the number of such standard\nmonomials. Following the work done for the Minors Modeling, we then transfer\nthe properties of this particular determinantal ideal to ideals generated by\nthe Support Minors system, by adding generic forms. This work allows to make a\nprecise comparison between the Minors and Support Minors modeling, and a\nprecise estimate of the complexity of solving MinRank instances for the\nparameters of the Mirath signature scheme that is currently at the second round\nof the NIST standardization process for Additional Digital Signature Schemes.",
        "Our contribution aims to celebrate the immeasurable contribution that Tom Kuo\nhas provided to the understanding of the structure of atomic nuclei, and also\nof the infinite nuclear matter, in terms of the fundamental principles\ngoverning the realistic nuclear potential. The authors want to testify Tom\nKuo's heritage and impact on their approach to the study of nuclear systems by\nreviewing some recent findings on the role of the two-body component of\nshell-model effective $\\beta$-decay operators. The focus is spotted on the\nso-called Pauli-blocking effect, that plays a non-negligible role in nuclei\ncharacterized by a large number of valence nucleons.",
        "We investigate the influence of an early matter-dominated era in cosmic\nhistory on the dynamics of cosmic strings and the resulting stochastic\ngravitational waves. Specifically, we examine the case where this era\noriginates from the dark matter dilution mechanism within the framework of the\nminimal left-right symmetric model. By numerically solving the Boltzmann\nequations governing the energy densities of the relevant components, we\nmeticulously analyze the modifications to the cosmological scale factor, the\nnumber density of cosmic string loops, and the gravitational wave spectrum. Our\nresults reveal that the early matter-dominated era causes a characteristic\nsuppression in the high-frequency regime of the gravitational wave spectrum,\nproviding distinct and testable signatures for future ground-based\ninterferometer experiments.",
        "Despite extensive investigations, many aspects of charge density waves (CDWs)\nremain elusive, especially the relative roles of electron-phonon coupling and\nFermi surface nesting as the underlying driving mechanisms responsible for the\nemergence of the CDW vector $\\bl Q_{CDW}$. It is puzzling that even though\nelectrons interact strongly with optical phonons in many correlated systems,\nthe actual mode softening is of an acoustic mode. Here we consider monolayer\n1H-NbSe$_2$ as an exemplar system, and through an accurate computation of the\nphonon self-energy, including its off-diagonal components. We provide\ncompelling evidence that the relevant mode is a longitudinal optical phonon\nthat softens by anti-crossing several intervening phonon bands. We also show\nthat $\\bl Q_{CDW}$ is fixed by the convolution of the susceptibility and\nelectron-phonon coupling, and that the softened phonons are circularly\npolarized.",
        "In his article [J. Comb. Theory Ser. B 16 (1974), 168-174], Tutte called two\ngraphs $T$-equivalent (i.e., codichromatic) if they have the same Tutte\npolynomial and showed that graphs $G$ and $G'$ are $T$-equivalent if $G'$ is\nobtained from $G$ by flipping a rotor (i.e., replacing it by its mirror) of\norder at most $5$, where a rotor of order $k$ in $G$ is an induced subgraph $R$\nhaving an automorphism $\\psi$ with a vertex orbit $\\{\\psi^i(u): i\\ge 0\\}$ of\nsize $k$ such that every vertex of $R$ is only adjacent to vertices in $R$\nunless it is in this vertex orbit. In this article, we first show the above\nresult due to Tutte can be extended to a rotor $R$ of order $k\\ge 6$ if the\nsubgraph of $G$ induced by all those edges of $G$ which are not in $R$\nsatisfies certain conditions. Also, we provide a new method for generating\ninfinitely many non-isomorphic $T$-equivalent pairs of graphs.",
        "We establish the necessary and sufficient conditions for unbiased estimation\nin multi-parameter estimation tasks. More specifically, we first consider\nquantum state estimation, where multiple parameters are encoded in a quantum\nstate, and derive two equivalent necessary and sufficient conditions for an\nunbiased estimation: one formulated in terms of the quantum Fisher information\nmatrix (QFIM) and the other based on the derivatives of the encoded state.\nFurthermore, we introduce a generalized quantum Cram\\'er-Rao bound, which\nprovides a fundamental achievable lower bound on the estimation error even when\nthe QFIM is non-invertible. To demonstrate the utility of our framework, we\nconsider phase estimation under unknown Pauli noise. We show that while\nunbiased phase estimation is infeasible with a naive scheme, employing an\nentangled probe with a noiseless ancilla enables unbiased estimation. Next, we\nextend our analysis to quantum channel estimation (equivalently, quantum\nchannel learning), where the goal is to estimate parameters characterizing an\nunknown quantum channel. We establish the necessary and sufficient condition\nfor unbiased estimation of these parameters. Notably, by interpreting unbiased\nestimation as learnability, our result applies to the fundamental learnability\nof parameters in general quantum channels. As a concrete application, we\ninvestigate the learnability of noise affecting non-Clifford gates via cycle\nbenchmarking.",
        "Long-term planning for robots operating in domestic environments poses unique\nchallenges due to the interactions between humans, objects, and spaces. Recent\nadvancements in trajectory planning have leveraged vision-language models\n(VLMs) to extract contextual information for robots operating in real-world\nenvironments. While these methods achieve satisfying performance, they do not\nexplicitly model human activities. Such activities influence surrounding\nobjects and reshape spatial constraints. This paper presents a novel approach\nto trajectory planning that integrates human preferences, activities, and\nspatial context through an enriched 3D scene graph (3DSG) representation. By\nincorporating activity-based relationships, our method captures the spatial\nimpact of human actions, leading to more context-sensitive trajectory\nadaptation. Preliminary results demonstrate that our approach effectively\nassigns costs to spaces influenced by human activities, ensuring that the robot\ntrajectory remains contextually appropriate and sensitive to the ongoing\nenvironment. This balance between task efficiency and social appropriateness\nenhances context-aware human-robot interactions in domestic settings. Future\nwork includes implementing a full planning pipeline and conducting user studies\nto evaluate trajectory acceptability.",
        "This paper gives an overview on how to develop a dense and deep neural\nnetwork for making a time series prediction. First, the history and\ncornerstones in Artificial Intelligence and Machine Learning will be presented.\nAfter a short introduction to the theory of Artificial Intelligence and Machine\nLearning, the paper will go deeper into the techniques for conducting a time\nseries prediction with different models of neural networks. For this project,\nPython's development environment Jupyter, extended with the TensorFlow package\nand deep-learning application Keras is used. The system setup and project\nframework are explained in more detail before discussing the time series\nprediction. The main part shows an applied example of time series prediction\nwith weather data. For this work, a deep recurrent neural network with Long\nShort-Term Memory cells is used to conduct the time series prediction. The\nresults and evaluation of the work show that a weather prediction with deep\nneural networks can be successful for a short time period. However, there are\nsome drawbacks and limitations with time series prediction, which will be\ndiscussed towards the end of the paper.",
        "Existing fingerprinting-based localization methods often require extensive\ndata collection and struggle to generalize to new environments. In contrast to\nprevious environment-unknown MetaLoc, we propose GenMetaLoc in this paper,\nwhich first introduces meta-learning to enable the generation of dense\nfingerprint databases from an environment-aware perspective. In the model\naspect, the learning-to-learn mechanism accelerates the fingerprint generation\nprocess by facilitating rapid adaptation to new environments with minimal data.\nAdditionally, we incorporate 3D point cloud data from the first Fresnel zone\nbetween the transmitter and receiver, which describes the obstacles\ndistribution in the environment and serves as a condition to guide the\ndiffusion model in generating more accurate fingerprints. In the data\nprocessing aspect, unlike most studies that focus solely on channel state\ninformation (CSI) amplitude or phase, we present a comprehensive processing\nthat addresses both, correcting errors from WiFi hardware limitations such as\namplitude discrepancies and frequency offsets. For the data collection\nplatform, we develop an uplink wireless localization system that leverages the\nsensing capabilities of existing commercial WiFi devices and mobile phones,\nthus reducing the need for additional deployment costs. Experimental results on\nreal datasets show that our framework outperforms baseline methods.",
        "This article provides a detailed description of some nilpotent left braces\ngenerated by one element.",
        "Multi-instance partial-label learning (MIPL) is an emerging learning\nframework where each training sample is represented as a multi-instance bag\nassociated with a candidate label set. Existing MIPL algorithms often overlook\nthe margins for attention scores and predicted probabilities, leading to\nsuboptimal generalization performance. A critical issue with these algorithms\nis that the highest prediction probability of the classifier may appear on a\nnon-candidate label. In this paper, we propose an algorithm named MIPLMA, i.e.,\nMulti-Instance Partial-Label learning with Margin Adjustment, which adjusts the\nmargins for attention scores and predicted probabilities. We introduce a\nmargin-aware attention mechanism to dynamically adjust the margins for\nattention scores and propose a margin distribution loss to constrain the\nmargins between the predicted probabilities on candidate and non-candidate\nlabel sets. Experimental results demonstrate the superior performance of MIPLMA\nover existing MIPL algorithms, as well as other well-established multi-instance\nlearning algorithms and partial-label learning algorithms.",
        "The classification of electrocardiogram (ECG) signals is crucial for early\ndetection of arrhythmias and other cardiac conditions. However, despite\nadvances in machine learning, many studies fail to follow standardization\nprotocols, leading to inconsistencies in performance evaluation and real-world\napplicability. Additionally, hardware constraints essential for practical\ndeployment, such as in pacemakers, Holter monitors, and wearable ECG patches,\nare often overlooked. Since real-world impact depends on feasibility in\nresource-constrained devices, ensuring efficient deployment is critical for\ncontinuous monitoring. This review systematically analyzes ECG classification\nstudies published between 2017 and 2024, focusing on those adhering to the E3C\n(Embedded, Clinical, and Comparative Criteria), which include inter-patient\nparadigm implementation, compliance with Association for the Advancement of\nMedical Instrumentation (AAMI) recommendations, and model feasibility for\nembedded systems. While many studies report high accuracy, few properly\nconsider patient-independent partitioning and hardware limitations. We identify\nstate-of-the-art methods meeting E3C criteria and conduct a comparative\nanalysis of accuracy, inference time, energy consumption, and memory usage.\nFinally, we propose standardized reporting practices to ensure fair comparisons\nand practical applicability of ECG classification models. By addressing these\ngaps, this study aims to guide future research toward more robust and\nclinically viable ECG classification systems.",
        "State of the art large language models rely on randomization to respond to a\nprompt. As an immediate consequence, a model may respond differently to the\nsame prompt if asked multiple times. In this work, we argue that the evaluation\nand ranking of large language models should control for the randomization\nunderpinning their functioning. Our starting point is the development of a\ncausal model for coupled autoregressive generation, which allows different\nlarge language models to sample responses with the same source of randomness.\nBuilding upon our causal model, we first show that, on evaluations based on\nbenchmark datasets, coupled autoregressive generation leads to the same\nconclusions as vanilla autoregressive generation but using provably fewer\nsamples. However, we further show that, on evaluations based on (human)\npairwise comparisons, coupled and vanilla autoregressive generation can\nsurprisingly lead to different rankings when comparing more than two models,\neven with an infinite amount of samples. This suggests that the apparent\nadvantage of a model over others in existing evaluation protocols may not be\ngenuine but rather confounded by the randomness inherent to the generation\nprocess. To illustrate and complement our theoretical results, we conduct\nexperiments with several large language models from the Llama family. We find\nthat, across multiple knowledge areas from the popular MMLU benchmark dataset,\ncoupled autoregressive generation requires up to 40% fewer samples to reach the\nsame conclusions as vanilla autoregressive generation. Further, using data from\nthe LMSYS Chatbot Arena platform, we find that the win-rates derived from\npairwise comparisons by a strong large language model to prompts differ under\ncoupled and vanilla autoregressive generation.",
        "We propose a novel hand-object contact detection system based on grasp\nquality metrics extracted from object and hand poses, and evaluated its\nperformance using the DexYCB dataset. Our evaluation demonstrated the system's\nhigh accuracy (approaching 90%). Future work will focus on a real-time\nimplementation using vision-based estimation, and integrating it to a\nrobot-to-human handover system.",
        "Given a probability-measure-valued process $(\\mu_t)$, we aim to find, among\nall path-continuous stochastic processes whose one-dimensional time marginals\ncoincide almost surely with $(\\mu_t)$ (if there is any), a process that\nminimizes a given energy in expectation. Building on our recent study\n(arXiv:2502.12068), where the minimization of fractional Sobolev energy was\ninvestigated for deterministic paths on Wasserstein spaces, we now extend the\nresults to the stochastic setting to address some applications that originally\nmotivated our study. Two applications are given. We construct minimizing\nparticle representations for processes on Wasserstein spaces on $\\mathbb{R}$\nwith H\\\"{o}lder regularity, using optimal transportation. We prove the\nexistence of minimizing particle representations for solutions to stochastic\nFokker--Planck--Kolmogorov equations on $\\mathbb{R}^\\mathrm{d}$ satisfying an\nintegrability condition, using the stochastic superposition principle of\nLacker--Shkolnikov--Zhang (J. Eur. Math. Soc. 25, 3229--3288 (2023))."
      ]
    }
  },
  {
    "id":2411.07871,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"2016 Alzheimer's disease facts and figures",
    "start_abstract":"This report describes the public health impact of Alzheimer's disease, including incidence and prevalence, mortality rates, costs care, overall on caregivers society. It also examines in detail financial families, annual to families difficult decisions must often make pay those costs. An estimated 5.4 million Americans have disease. By mid-century, number people living with disease United States is projected grow 13.8 million, fueled large part by aging baby boom generation. Today, someone country develops every 66 seconds. 2050, one new case expected develop 33 seconds, resulting nearly 1 cases per year. In 2013, official death certificates recorded 84,767 deaths from making it sixth leading cause fifth age \u2265 65 years. Between 2000 stroke, heart prostate cancer decreased 23%, 14%, 11%, respectively, whereas increased 71%. The actual which contributes likely much larger than certificates. 2016, an 700,000 years will die many them because complications caused 2015, more 15 family members other unpaid provided 18.1 billion hours care dementias, a contribution valued at $221 billion. Average per-person Medicare payments for services beneficiaries dementias are two half times as great all without these conditions, Medicaid 19 great. Total 2016 long-term hospice dementia be $236 may place substantial burden who take money out their retirement savings, cut back buying food, reduce own trips doctor. addition, incorrectly believe that pays nursing home types care. Such findings highlight need solutions prevent dementia-related jeopardizing security dementias.",
    "start_categories":[
      "q-bio.NC"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b5"
      ],
      "title":[
        "Computer-aided diagnosis of Alzheimer\u2019s disease and neurocognitive disorders with multimodal Bi-Vision Transformer (BiViT)"
      ],
      "abstract":[
        "<jats:title>Abstract<\/jats:title><jats:p>Cognitive disorders affect various cognitive functions that can have a substantial impact on individual\u2019s daily life. Alzheimer\u2019s disease (AD) is one of such well-known cognitive disorders. Early detection and treatment of cognitive diseases using artificial intelligence can help contain them. However, the complex spatial relationships and long-range dependencies found in medical imaging data present challenges in achieving the objective. Moreover, for a few years, the application of transformers in imaging has emerged as a promising area of research. A reason can be transformer\u2019s impressive capabilities of tackling spatial relationships and long-range dependency challenges in two ways, i.e., (1) using their self-attention mechanism to generate comprehensive features, and (2) capture complex patterns by incorporating global context and long-range dependencies. In this work, a Bi-Vision Transformer (BiViT) architecture is proposed for classifying different stages of AD, and multiple types of cognitive disorders from 2-dimensional MRI imaging data. More specifically, the transformer is composed of two novel modules, namely Mutual Latent Fusion (MLF) and Parallel Coupled Encoding Strategy (PCES), for effective feature learning. Two different datasets have been used to evaluate the performance of proposed BiViT-based architecture. The first dataset contain several classes such as mild or moderate demented stages of the AD. The other dataset is composed of samples from patients with AD and different cognitive disorders such as mild, early, or moderate impairments. For comprehensive comparison, a multiple transfer learning algorithm and a deep autoencoder have been each trained on both datasets. The results show that the proposed BiViT-based model achieves an accuracy of 96.38% on the AD dataset. However, when applied to cognitive disease data, the accuracy slightly decreases below 96% which can be resulted due to smaller amount of data and imbalance in data distribution. Nevertheless, given the results, it can be hypothesized that the proposed algorithm can perform better if the imbalanced distribution and limited availability problems in data can be addressed.<\/jats:p>\n                <jats:p><jats:bold>Graphical abstract<\/jats:bold><\/jats:p>"
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "HCAST: Human-Calibrated Autonomy Software Tasks",
        "LLM-Net: Democratizing LLMs-as-a-Service through Blockchain-based Expert\n  Networks",
        "Towards A Litmus Test for Common Sense",
        "CollabLLM: From Passive Responders to Active Collaborators",
        "A Frontier AI Risk Management Framework: Bridging the Gap Between\n  Current AI Practices and Established Risk Management",
        "Analyzing sequential activity and travel decisions with interpretable\n  deep inverse reinforcement learning",
        "ARES: Auxiliary Range Expansion for Outlier Synthesis",
        "Machine Learning in Biomechanics: Key Applications and Limitations in\n  Walking, Running, and Sports Movements",
        "Large Language Models and Mathematical Reasoning Failures",
        "OmniScience: A Domain-Specialized LLM for Scientific Reasoning and\n  Discovery",
        "Decision Information Meets Large Language Models: The Future of\n  Explainable Operations Research",
        "Leveraging Large Language Models to Develop Heuristics for Emerging\n  Optimization Problems",
        "SagaLLM: Context Management, Validation, and Transaction Guarantees for\n  Multi-Agent LLM Planning",
        "Dirichlet's Lemma in Number Fields",
        "Forecasting Monthly Residential Natural Gas Demand Using\n  Just-In-Time-Learning Modeling",
        "Non-Hermitian Aharonov-Bohm Cage in Bosonic Bogoliubov-de Gennes Systems",
        "Einstein multiply warped products and generalized Kasner manifolds with\n  multidimensional base",
        "Probing Topological Anderson Transition in Quasiperiodic Photonic\n  Lattices via Chiral Displacement and Wavelength Tuning",
        "Privacy-Aware RAG: Secure and Isolated Knowledge Retrieval",
        "Subwavelength plasmonic antennas based on asymmetric\n  split-ring-resonators for high near-field enhancements",
        "Poisoning Bayesian Inference via Data Deletion and Replication",
        "Parking Space Detection in the City of Granada",
        "COFO: COdeFOrces dataset for Program Classification, Recognition and\n  Tagging",
        "Estimating treatment effects with competing intercurrent events in\n  randomized controlled trials",
        "$\\eta$, $\\eta^\\prime$ mesons from lattice QCD in fully physical\n  conditions",
        "Commonsense Reasoning-Aided Autonomous Vehicle Systems",
        "The effect of longitudinal debonding on stress redistributions around\n  fiber breaks: Incorporating fiber diameter distribution and fiber\n  misalignment",
        "An empirical formulation of accelerated molecular dynamics for\n  simulating and predicting microstructure evolution in materials"
      ],
      "abstract":[
        "To understand and predict the societal impacts of highly autonomous AI\nsystems, we need benchmarks with grounding, i.e., metrics that directly connect\nAI performance to real-world effects we care about. We present HCAST\n(Human-Calibrated Autonomy Software Tasks), a benchmark of 189 machine learning\nengineering, cybersecurity, software engineering, and general reasoning tasks.\nWe collect 563 human baselines (totaling over 1500 hours) from people skilled\nin these domains, working under identical conditions as AI agents, which lets\nus estimate that HCAST tasks take humans between one minute and 8+ hours.\nMeasuring the time tasks take for humans provides an intuitive metric for\nevaluating AI capabilities, helping answer the question \"can an agent be\ntrusted to complete a task that would take a human X hours?\" We evaluate the\nsuccess rates of AI agents built on frontier foundation models, and we find\nthat current agents succeed 70-80% of the time on tasks that take humans less\nthan one hour, and less than 20% of the time on tasks that take humans more\nthan 4 hours.",
        "The centralization of Large Language Models (LLMs) development has created\nsignificant barriers to AI advancement, limiting the democratization of these\npowerful technologies. This centralization, coupled with the scarcity of\nhigh-quality training data and mounting complexity of maintaining comprehensive\nexpertise across rapidly expanding knowledge domains, poses critical challenges\nto the continued growth of LLMs. While solutions like Retrieval-Augmented\nGeneration (RAG) offer potential remedies, maintaining up-to-date expert\nknowledge across diverse domains remains a significant challenge, particularly\ngiven the exponential growth of specialized information. This paper introduces\nLLMs Networks (LLM-Net), a blockchain-based framework that democratizes\nLLMs-as-a-Service through a decentralized network of specialized LLM providers.\nBy leveraging collective computational resources and distributed domain\nexpertise, LLM-Net incorporates fine-tuned expert models for various specific\ndomains, ensuring sustained knowledge growth while maintaining service quality\nthrough collaborative prompting mechanisms. The framework's robust design\nincludes blockchain technology for transparent transaction and performance\nvalidation, establishing an immutable record of service delivery. Our\nsimulation, built on top of state-of-the-art LLMs such as Claude 3.5 Sonnet,\nLlama 3.1, Grok-2, and GPT-4o, validates the effectiveness of the\nreputation-based mechanism in maintaining service quality by selecting\nhigh-performing respondents (LLM providers). Thereby it demonstrates the\npotential of LLM-Net to sustain AI advancement through the integration of\ndecentralized expertise and blockchain-based accountability.",
        "This paper is the second in a planned series aimed at envisioning a path to\nsafe and beneficial artificial intelligence. Building on the conceptual\ninsights of \"Common Sense Is All You Need,\" we propose a more formal litmus\ntest for common sense, adopting an axiomatic approach that combines minimal\nprior knowledge (MPK) constraints with diagonal or Godel-style arguments to\ncreate tasks beyond the agent's known concept set. We discuss how this approach\napplies to the Abstraction and Reasoning Corpus (ARC), acknowledging\ntraining\/test data constraints, physical or virtual embodiment, and large\nlanguage models (LLMs). We also integrate observations regarding emergent\ndeceptive hallucinations, in which more capable AI systems may intentionally\nfabricate plausible yet misleading outputs to disguise knowledge gaps. The\noverarching theme is that scaling AI without ensuring common sense risks\nintensifying such deceptive tendencies, thereby undermining safety and trust.\nAligning with the broader goal of developing beneficial AI without causing\nharm, our axiomatic litmus test not only diagnoses whether an AI can handle\ntruly novel concepts but also provides a stepping stone toward an ethical,\nreliable foundation for future safe, beneficial, and aligned artificial\nintelligence.",
        "Large Language Models are typically trained with next-turn rewards, limiting\ntheir ability to optimize for long-term interaction. As a result, they often\nrespond passively to ambiguous or open-ended user requests, failing to help\nusers reach their ultimate intents and leading to inefficient conversations. To\naddress these limitations, we introduce CollabLLM, a novel and general training\nframework that enhances multiturn human-LLM collaboration. Its key innovation\nis a collaborative simulation that estimates the long-term contribution of\nresponses using Multiturn-aware Rewards. By reinforcement fine-tuning these\nrewards, CollabLLM goes beyond responding to user requests, and actively\nuncovers user intent and offers insightful suggestions-a key step towards more\nhuman-centered AI. We also devise a multiturn interaction benchmark with three\nchallenging tasks such as document creation. CollabLLM significantly\noutperforms our baselines with averages of 18.5% higher task performance and\n46.3% improved interactivity by LLM judges. Finally, we conduct a large user\nstudy with 201 judges, where CollabLLM increases user satisfaction by 17.6% and\nreduces user spent time by 10.4%.",
        "The recent development of powerful AI systems has highlighted the need for\nrobust risk management frameworks in the AI industry. Although companies have\nbegun to implement safety frameworks, current approaches often lack the\nsystematic rigor found in other high-risk industries. This paper presents a\ncomprehensive risk management framework for the development of frontier AI that\nbridges this gap by integrating established risk management principles with\nemerging AI-specific practices. The framework consists of four key components:\n(1) risk identification (through literature review, open-ended red-teaming, and\nrisk modeling), (2) risk analysis and evaluation using quantitative metrics and\nclearly defined thresholds, (3) risk treatment through mitigation measures such\nas containment, deployment controls, and assurance processes, and (4) risk\ngovernance establishing clear organizational structures and accountability.\nDrawing from best practices in mature industries such as aviation or nuclear\npower, while accounting for AI's unique challenges, this framework provides AI\ndevelopers with actionable guidelines for implementing robust risk management.\nThe paper details how each component should be implemented throughout the\nlife-cycle of the AI system - from planning through deployment - and emphasizes\nthe importance and feasibility of conducting risk management work prior to the\nfinal training run to minimize the burden associated with it.",
        "Travel demand modeling has shifted from aggregated trip-based models to\nbehavior-oriented activity-based models because daily trips are essentially\ndriven by human activities. To analyze the sequential activity-travel\ndecisions, deep inverse reinforcement learning (DIRL) has proven effective in\nlearning the decision mechanisms by approximating a reward function to\nrepresent preferences and a policy function to replicate observed behavior\nusing deep neural networks (DNNs). However, most existing research has focused\non using DIRL to enhance only prediction accuracy, with limited exploration\ninto interpreting the underlying decision mechanisms guiding sequential\ndecision-making. To address this gap, we introduce an interpretable DIRL\nframework for analyzing activity-travel decision processes, bridging the gap\nbetween data-driven machine learning and theory-driven behavioral models. Our\nproposed framework adapts an adversarial IRL approach to infer the reward and\npolicy functions of activity-travel behavior. The policy function is\ninterpreted through a surrogate interpretable model based on choice\nprobabilities from the policy function, while the reward function is\ninterpreted by deriving both short-term rewards and long-term returns for\nvarious activity-travel patterns. Our analysis of real-world travel survey data\nreveals promising results in two key areas: (i) behavioral pattern insights\nfrom the policy function, highlighting critical factors in decision-making and\nvariations among socio-demographic groups, and (ii) behavioral preference\ninsights from the reward function, indicating the utility individuals gain from\nspecific activity sequences.",
        "Recent successes of artificial intelligence and deep learning often depend on\nthe well-collected training dataset which is assumed to have an identical\ndistribution with the test dataset. However, this assumption, which is called\nclosed-set learning, is hard to meet in realistic scenarios for deploying deep\nlearning models. As one of the solutions to mitigate this assumption, research\non out-of-distribution (OOD) detection has been actively explored in various\ndomains. In OOD detection, we assume that we are given the data of a new class\nthat was not seen in the training phase, i.e., outlier, at the evaluation\nphase. The ultimate goal of OOD detection is to detect and classify such unseen\noutlier data as a novel \"unknown\" class. Among various research branches for\nOOD detection, generating a virtual outlier during the training phase has been\nproposed. However, conventional generation-based methodologies utilize\nin-distribution training dataset to imitate outlier instances, which limits the\nquality of the synthesized virtual outlier instance itself. In this paper, we\npropose a novel methodology for OOD detection named Auxiliary Range Expansion\nfor Outlier Synthesis, or ARES. ARES models the region for generating\nout-of-distribution instances by escaping from the given in-distribution\nregion; instead of remaining near the boundary of in-distribution region.\nVarious stages consists ARES to ultimately generate valuable OOD-like virtual\ninstances. The energy score-based discriminator is then trained to effectively\nseparate in-distribution data and outlier data. Quantitative experiments on\nbroad settings show the improvement of performance by our method, and\nqualitative results provide logical explanations of the mechanism behind it.",
        "This chapter provides an overview of recent and promising Machine Learning\napplications, i.e. pose estimation, feature estimation, event detection, data\nexploration & clustering, and automated classification, in gait (walking and\nrunning) and sports biomechanics. It explores the potential of Machine Learning\nmethods to address challenges in biomechanical workflows, highlights central\nlimitations, i.e. data and annotation availability and explainability, that\nneed to be addressed, and emphasises the importance of interdisciplinary\napproaches for fully harnessing the potential of Machine Learning in gait and\nsports biomechanics.",
        "This paper investigates the mathematical reasoning capabilities of large\nlanguage models (LLMs) using 50 newly constructed high-school-level word\nproblems. Unlike prior studies that focus solely on answer correctness, we\nrigorously analyze both final answers and solution steps to identify reasoning\nfailures. Evaluating eight state-of-the-art models - including Mixtral, Llama,\nGemini, GPT-4o, and OpenAI's o1 variants - we find that while newer models\n(e.g., o3-mini, deepseek-r1) achieve higher accuracy, all models exhibit errors\nin spatial reasoning, strategic planning, and arithmetic, sometimes producing\ncorrect answers through flawed logic. Common failure modes include unwarranted\nassumptions, over-reliance on numerical patterns, and difficulty translating\nphysical intuition into mathematical steps. Manual analysis reveals that models\nstruggle with problems requiring multi-step deduction or real-world knowledge,\ndespite possessing broad mathematical knowledge. Our results underscore the\nimportance of evaluating reasoning processes, not just answers, and caution\nagainst overestimating LLMs' problem-solving proficiency. The study highlights\npersistent gaps in LLMs' generalization abilities, emphasizing the need for\ntargeted improvements in structured reasoning and constraint handling.",
        "Large Language Models (LLMs) have demonstrated remarkable potential in\nadvancing scientific knowledge and addressing complex challenges. In this work,\nwe introduce OmniScience, a specialized large reasoning model for general\nscience, developed through three key components: (1) domain adaptive\npretraining on a carefully curated corpus of scientific literature, (2)\ninstruction tuning on a specialized dataset to guide the model in following\ndomain-specific tasks, and (3) reasoning-based knowledge distillation through\nfine-tuning to significantly enhance its ability to generate contextually\nrelevant and logically sound responses. We demonstrate the versatility of\nOmniScience by developing a battery agent that efficiently ranks molecules as\npotential electrolyte solvents or additives. Comprehensive evaluations reveal\nthat OmniScience is competitive with state-of-the-art large reasoning models on\nthe GPQA Diamond and domain-specific battery benchmarks, while outperforming\nall public reasoning and non-reasoning models with similar parameter counts. We\nfurther demonstrate via ablation experiments that domain adaptive pretraining\nand reasoning-based knowledge distillation are critical to attain our\nperformance levels, across benchmarks.",
        "Operations Research (OR) is vital for decision-making in many industries.\nWhile recent OR methods have seen significant improvements in automation and\nefficiency through integrating Large Language Models (LLMs), they still\nstruggle to produce meaningful explanations. This lack of clarity raises\nconcerns about transparency and trustworthiness in OR applications. To address\nthese challenges, we propose a comprehensive framework, Explainable Operations\nResearch (EOR), emphasizing actionable and understandable explanations\naccompanying optimization. The core of EOR is the concept of Decision\nInformation, which emerges from what-if analysis and focuses on evaluating the\nimpact of complex constraints (or parameters) changes on decision-making.\nSpecifically, we utilize bipartite graphs to quantify the changes in the OR\nmodel and adopt LLMs to improve the explanation capabilities. Additionally, we\nintroduce the first industrial benchmark to rigorously evaluate the\neffectiveness of explanations and analyses in OR, establishing a new standard\nfor transparency and clarity in the field.",
        "Combinatorial optimization problems often rely on heuristic algorithms to\ngenerate efficient solutions. However, the manual design of heuristics is\nresource-intensive and constrained by the designer's expertise. Recent advances\nin artificial intelligence, particularly large language models (LLMs), have\ndemonstrated the potential to automate heuristic generation through\nevolutionary frameworks. Recent works focus only on well-known combinatorial\noptimization problems like the traveling salesman problem and online bin\npacking problem when designing constructive heuristics. This study investigates\nwhether LLMs can effectively generate heuristics for niche, not yet broadly\nresearched optimization problems, using the unit-load pre-marshalling problem\nas an example case. We propose the Contextual Evolution of Heuristics (CEoH)\nframework, an extension of the Evolution of Heuristics (EoH) framework, which\nincorporates problem-specific descriptions to enhance in-context learning\nduring heuristic generation. Through computational experiments, we evaluate\nCEoH and EoH and compare the results. Results indicate that CEoH enables\nsmaller LLMs to generate high-quality heuristics more consistently and even\noutperform larger models. Larger models demonstrate robust performance with or\nwithout contextualized prompts. The generated heuristics exhibit scalability to\ndiverse instance configurations.",
        "Recent LLM-based agent frameworks have demonstrated impressive capabilities\nin task delegation and workflow orchestration, but face significant challenges\nin maintaining context awareness and ensuring planning consistency. This paper\npresents SagaLLM, a structured multi-agent framework that addresses four\nfundamental limitations in current LLM approaches: inadequate self-validation,\ncontext narrowing, lacking transaction properties, and insufficient inter-agent\ncoordination. By implementing specialized context management agents and\nvalidation protocols, SagaLLM preserves critical constraints and state\ninformation throughout complex planning processes, enabling robust and\nconsistent decision-making even during disruptions. We evaluate our approach\nusing selected problems from the REALM benchmark, focusing on sequential and\nreactive planning scenarios that challenge both context retention and adaptive\nreasoning. Our experiments with state-of-the-art LLMs, Claude 3.7, DeepSeek R1,\nGPT-4o, and GPT-o1, demonstrate that while these models exhibit impressive\nreasoning capabilities, they struggle with maintaining global constraint\nawareness during complex planning tasks, particularly when adapting to\nunexpected changes. In contrast, the distributed cognitive architecture of\nSagaLLM shows significant improvements in planning consistency, constraint\nenforcement, and adaptation to disruptions in various scenarios.",
        "Dirichlet's Lemma states that every primitive quadratic Dirichlet character\n$\\chi$ can be written in the form $\\chi(n) = (\\frac{\\Delta}n)$ for a suitable\nquadratic discriminant $\\Delta$. In this article we define a group, the\nseparant class group, that measures the extent to which Dirichlet's Lemma fails\nin general number fields $F$. As an application we will show that over fields\nwith trivial separant class groups, genus theory of quadratic extensions can be\nmade as explicit as over the rationals.",
        "Natural gas (NG) is relatively a clean source of energy, particularly\ncompared to fossil fuels, and worldwide consumption of NG has been increasing\nalmost linearly in the last two decades. A similar trend can also be seen in\nTurkey, while another similarity is the high dependence on imports for the\ncontinuous NG supply. It is crucial to accurately forecast future NG demand\n(NGD) in Turkey, especially, for import contracts; in this respect, forecasts\nof monthly NGD for the following year are of utmost importance. In the current\nstudy, the historical monthly NG consumption data between 2014 and 2024\nprovided by SOCAR, the local residential NG distribution company for two cities\nin Turkey, Bursa and Kayseri, was used to determine out-of-sample monthly NGD\nforecasts for a period of one year and nine months using various time series\nmodels, including SARIMA and ETS models, and a novel proposed machine learning\nmethod. The proposed method, named Just-in-Time-Learning-Gaussian Process\nRegression (JITL-GPR), uses a novel feature representation for the past NG\ndemand values; instead of using past demand values as column-wise separate\nfeatures, they are placed on a two-dimensional (2-D) grid of year-month values.\nFor each test point, a kernel function, tailored for the NGD predictions, is\nused in GPR to predict the query point. Since a model is constructed separately\nfor each test point, the proposed method is, indeed, an example of JITL. The\nJITL-GPR method is easy to use and optimize, and offers a reduction in forecast\nerrors compared to traditional time series methods and a state-of-the-art\ncombination model; therefore, it is a promising tool for NGD forecasting in\nsimilar settings.",
        "The non-Hermitian Aharonov-Bohm (AB) cage is a unique localization phenomenon\nthat confines all possible excitations. This confinement leads to fully flat\nspectra in momentum space, which are typically accompanied with the degeneracy\nwith various types. Classifying the degeneracy type is crucial for studying the\ndynamical properties of the non-Hermitian AB cage, but the methods for such\nclassification and their physical connections remain not very clear. Here, we\nconstruct a non-Hermitian AB cage in a bosonic Bogoliubov-de Gennes (BdG)\nsystem with various types of degenerate flat bands (DFBs). Using the transfer\nmatrix, we demonstrate the localization mechanism for the formation of AB cage\nand derive the minimal polynomial in mathematics for classifying the degeneracy\ntypes of DFBs, thus providing comprehensive understanding of the correspondence\namong the degeneracy type of DFBs, the minimal polynomial, and the transfer\nmatrix. With such correspondence, we propose a scheme to realize highly\ndegenerate flat bands.",
        "The purpose of this paper is to provide conditions for the existence or non\nexistence of non trivial Einstein multiply warped products, specially of\ngeneralised Kasner type; as well as to show estimates of the Einstein parameter\nthat condition the existence of such metrics.",
        "The interplay of topology and disorder in quantum dynamics has recently\nattracted significant attention across diverse platforms, including solid-state\ndevices, ultracold atoms, and photonic systems. Here, we report on a\ntopological Anderson transition caused by quasiperiodic intra-cell coupling\ndisorder in photonic Su-Schrieffer-Heeger lattices. As the quasiperiodic\nstrength is varied, the system exhibits a reentrant transition from a trivial\nphase to a topological phase and back to a trivial phase, accompanied by the\nclosing and reopening of the band gap around zero energy. Unlike the\ntraditional detection of photonic topological edge modes, we measure the mean\nchiral displacement from the transport of light in the bulk of the lattices. In\nour photonic lattices with a fixed length, the propagation dynamics is\nretrieved by varying the wavelength of light, which tunes the inter-waveguide\ncouplings.",
        "The widespread adoption of Retrieval-Augmented Generation (RAG) systems in\nreal-world applications has heightened concerns about the confidentiality and\nintegrity of their proprietary knowledge bases. These knowledge bases, which\nplay a critical role in enhancing the generative capabilities of Large Language\nModels (LLMs), are increasingly vulnerable to breaches that could compromise\nsensitive information. To address these challenges, this paper proposes an\nadvanced encryption methodology designed to protect RAG systems from\nunauthorized access and data leakage. Our approach encrypts both textual\ncontent and its corresponding embeddings prior to storage, ensuring that all\ndata remains securely encrypted. This mechanism restricts access to authorized\nentities with the appropriate decryption keys, thereby significantly reducing\nthe risk of unintended data exposure. Furthermore, we demonstrate that our\nencryption strategy preserves the performance and functionality of RAG\npipelines, ensuring compatibility across diverse domains and applications. To\nvalidate the robustness of our method, we provide comprehensive security proofs\nthat highlight its resilience against potential threats and vulnerabilities.\nThese proofs also reveal limitations in existing approaches, which often lack\nrobustness, adaptability, or reliance on open-source models. Our findings\nsuggest that integrating advanced encryption techniques into the design and\ndeployment of RAG systems can effectively enhance privacy safeguards. This\nresearch contributes to the ongoing discourse on improving security measures\nfor AI-driven services and advocates for stricter data protection standards\nwithin RAG architectures.",
        "As for plasmonic antenna structures that generate localized near-field\nenhancement, the most effective current implementations are based on electric\ndipole resonance modes, but this approach also imposes limitations on their\nfurther optimization. Here we introduce an ASRR structure whose ASR mode\nenables differential charge distribution across both sides of the split.\nThrough asymmetric regulation, charges at one end can become highly localized,\nthereby achieving efficient near-field enhancement. The formation of this\nstructure was initially driven by a hybrid computational framework integrating\nevolutionary optimization with residual neural networks, and subsequently\nsimplified into an ASRR prototype using the Occam's Razor principle. The ASRR\ndimer structure can achieve an electric field intensity enhancement over 6.5\ntimes larger than a traditional nanorod dimer, while maintaining a compact size\n(<1\/3 the working wavelength). The ASRR configuration also demonstrates\nsuperior Purcell factor and fluorescence enhancement. These results can find\napplications in surface-enhanced spectroscopy, nonlinear optics, and quantum\nlight-matter interactions.",
        "Research in adversarial machine learning (AML) has shown that statistical\nmodels are vulnerable to maliciously altered data. However, despite advances in\nBayesian machine learning models, most AML research remains concentrated on\nclassical techniques. Therefore, we focus on extending the white-box model\npoisoning paradigm to attack generic Bayesian inference, highlighting its\nvulnerability in adversarial contexts. A suite of attacks are developed that\nallow an attacker to steer the Bayesian posterior toward a target distribution\nthrough the strategic deletion and replication of true observations, even when\nonly sampling access to the posterior is available. Analytic properties of\nthese algorithms are proven and their performance is empirically examined in\nboth synthetic and real-world scenarios. With relatively little effort, the\nattacker is able to substantively alter the Bayesian's beliefs and, by\naccepting more risk, they can mold these beliefs to their will. By carefully\nconstructing the adversarial posterior, surgical poisoning is achieved such\nthat only targeted inferences are corrupted and others are minimally disturbed.",
        "This paper addresses the challenge of parking space detection in urban areas,\nfocusing on the city of Granada. Utilizing aerial imagery, we develop and apply\nsemantic segmentation techniques to accurately identify parked cars, moving\ncars and roads. A significant aspect of our research is the creation of a\nproprietary dataset specific to Granada, which is instrumental in training our\nneural network model. We employ Fully Convolutional Networks, Pyramid Networks\nand Dilated Convolutions, demonstrating their effectiveness in urban semantic\nsegmentation. Our approach involves comparative analysis and optimization of\nvarious models, including Dynamic U-Net, PSPNet and DeepLabV3+, tailored for\nthe segmentation of aerial images. The study includes a thorough\nexperimentation phase, using datasets such as UDD5 and UAVid, alongside our\ncustom Granada dataset. We evaluate our models using metrics like Foreground\nAccuracy, Dice Coefficient and Jaccard Index. Our results indicate that\nDeepLabV3+ offers the most promising performance. We conclude with future\ndirections, emphasizing the need for a dedicated neural network for parked car\ndetection and the potential for application in other urban environments. This\nwork contributes to the fields of urban planning and traffic management,\nproviding insights into efficient utilization of parking spaces through\nadvanced image processing techniques.",
        "In recent years, a lot of technological advances in computer science have\naided software programmers to create innovative and real-time user-friendly\nsoftware. With the creation of the software and the urging interest of people\nto learn to write software, there is a large collection of source codes that\ncan be found on the web, also known as Big Code, which can be used as a source\nof data for driving the machine learning applications tending to solve certain\nsoftware engineering problems. In this paper, we present COFO, a dataset\nconsisting of 809 classes\/problems with a total of 369K source codes written in\nC, C++, Java, and Python programming languages, along with other metadata such\nas code tags, problem specification, and input-output specifications. COFO has\nbeen scraped from the openly available Codeforces website using a\nselenium-beautifulsoup-python based scraper. We envision that this dataset can\nbe useful for solving machine learning-based problems like program\nclassification\/recognition, tagging, predicting program properties, and code\ncomprehension.",
        "The analysis of randomized controlled trials is often complicated by\nintercurrent events--events that occur after treatment initiation and may\nimpact outcome assessment. These events may lead to patients discontinuing\ntheir assigned treatment or dropping out of the trial entirely. In an analysis\nof data from two recent immunology trials, we categorize intercurrent events\ninto two broad types: those unrelated to treatment (e.g., withdrawal from the\nstudy due to external factors like pandemics or relocation) and those related\nto treatment (e.g., adverse events or lack of efficacy). We adopt distinct\nstrategies to handle each type, aiming to target a clinically more relevant\nestimand. For treatment-related intercurrent events, they often meaningfully\ndescribe the patient's outcome, we employ a composite variable strategy, where\nwe attribute an outcome value that reflects the lack of treatment success. For\ntreatment-unrelated intercurrent events, we adopt a hypothetical strategy that\nassumes these event times are conditionally independent of the outcome, given\ntreatment and covariates, and envisions a scenario in which the intercurrent\nevents do not occur. We establish the nonparametric identification and\nsemiparametric estimation theory for the causal estimand and introduce doubly\nrobust estimators. We illustrate our methods through the re-analysis of two\nrandomized trials on baricitinib for Systemic Lupus Erythematosus. We classify\nintercurrent events, apply four estimators, and compare our approach with\ncommon ad-hoc methods, highlighting the robustness and practical implications\nof our framework.",
        "We determine masses and mixing parameters of the $\\eta$ and $M_{\\eta^\\prime}$\nmeson in lattice QCD. The calculations are carried out on a set of 13 ETMC\ngauge ensembles with $N_f=2+1+1$ (maximally) twisted-mass Clover-improved\nquarks. These ensemble cover four values of the lattice spacing\n$a=0.057\\mathrm{fm},...,0.092\\mathrm{fm}$ and pion masses from\n$140\\mathrm{MeV}$ to $360\\mathrm{MeV}$, including three ensembles at physical\nquark masses and six ensembles with $M_\\pi<200\\mathrm{MeV}$. The strange-quark\ncontribution is treated in a mixed-action approach using Osterwalder-Seiler\nfermions to avoid complications due to flavor mixing in the heavy quark sector\nand to enable the use of the one-end trick in the computation of strange\nquark-disconnected diagrams. With the strange-quark mass tuned to its physical\nvalue and several ensembles having close-to-physical light-quark mass,\nuncertainties related to the chiral extrapolations are reduced significantly\ncompared to earlier studies. Physical results are computed with fully\ncontrolled systematics from a combined chiral, continuum and infinite-volume\nextrapolation, and a full error budget is obtained from model averages over of\nvarious fit ans\\\"atze and data cuts. Our results for the masses are given by\n$M_\\eta=551(16)\\mathrm{MeV}$ and $M_{\\eta^\\prime}=972(20)\\mathrm{MeV}$,\nrespectively, where statistical and systematic errors have been added in\nquadrature. For the mixing angle and decay-constant parameters the\nFeldmann-Kroll-Stech scheme is employed to compute them from pseudoscalar\nmatrix elements in the quark-flavor basis. For the mixing angle we obtain\n$\\phi^\\mathrm{phys}=39.3(2.0)^\\circ$ and our results for the decay-constant\nparameters are given by $f_l^\\mathrm{phys}=138.6(4.4)\\mathrm{MeV}$ and\n$f_s^\\mathrm{phys}=170.7(3.3)\\mathrm{MeV}$.",
        "Autonomous Vehicle (AV) systems have been developed with a strong reliance on\nmachine learning techniques. While machine learning approaches, such as deep\nlearning, are extremely effective at tasks that involve observation and\nclassification, they struggle when it comes to performing higher level\nreasoning about situations on the road. This research involves incorporating\ncommonsense reasoning models that use image data to improve AV systems. This\nwill allow AV systems to perform more accurate reasoning while also making them\nmore adjustable, explainable, and ethical. This paper will discuss the findings\nso far and motivate its direction going forward.",
        "This research explores the influence of interfacial debonding between a\nbroken fiber and matrix on stress redistribution surrounding a fiber break\nwithin a unidirectional (UD) impregnated fiber bundle, accounting for\nmisalignment of fibers and fiber diameter distribution in randomly packed fiber\nconfigurations. Finite-element modelling is conducted on carbon-reinforced\nepoxy UD bundles with one fiber broken for different combinations of the bundle\nparameters: aligned\/misaligned fibers and constant\/randomly distributed fiber\ndiameters. Two definitions of stress concentration factor (SCF) are examined,\nbased on average and maximum stress over the fiber cross-section. The study\nreveals a statistically significant difference for average SCF for misaligned\nbundles with both constant and variable diameters of fibers compared to the\ncase of aligned bundles of fibers with constant diameter. When the calculated\nSCFs are incorporated in a bundle strength model, the failure strain of\nunidirectional composites can be more realistically predicted.",
        "Despite its widespread use in materials science, conventional molecular\ndynamics simulations are severely constrained by timescale limitations. To\naddress this shortcoming, we propose an empirical formulation of accelerated\nmolecular dynamics method, adapted from a collective-variable-based extended\nsystem dynamics framework. While this framework is originally developed for\nefficient free energy sampling and reaction pathway determination of specific\nrare events in condensed matter, we have modified it to enable accelerated\nmolecular dynamics simulation and prediction of microstructure evolution of\nmaterials across a broad range of scenarios. In essence, the nearest neighbor\noff-centering absolute displacement (NNOAD), which quantifies the deviation of\nan atom from the geometric center of its nearest neighbors in materials, is\nintroduced. We propose that the collection of NNOADs of all atoms can serve as\na generalized reaction coordinate for various structural transitions in\nmaterials. The NNOAD of each atom, represented by its three components, is\ncoupled with three additional dynamic variables assigned to the atom. Time\nevolution of the additional dynamic variables follows Langevin equation, while\nNos\\'e-Hoover dynamics is employed to thermostat the system. Through careful\nanalysis and benchmark simulations, we established appropriate parameter ranges\nfor the equations in our method. Application of this method to several test\ncases demonstrates its effectiveness and consistency in accelerating molecular\ndynamics simulations and predicting various microstructure evolutions of\nmaterials over much longer timescale. We also provide a preliminary theoretical\nanalysis and qualitative justification of the method, offering insights into\nits underlying principles."
      ]
    }
  },
  {
    "id":2411.07871,
    "research_type":"applied",
    "start_id":"b5",
    "start_title":"Computer-aided diagnosis of Alzheimer\u2019s disease and neurocognitive disorders with multimodal Bi-Vision Transformer (BiViT)",
    "start_abstract":"<jats:title>Abstract<\/jats:title><jats:p>Cognitive disorders affect various cognitive functions that can have a substantial impact on individual\u2019s daily life. Alzheimer\u2019s disease (AD) is one of such well-known cognitive disorders. Early detection and treatment of cognitive diseases using artificial intelligence can help contain them. However, the complex spatial relationships and long-range dependencies found in medical imaging data present challenges in achieving the objective. Moreover, for a few years, the application of transformers in imaging has emerged as a promising area of research. A reason can be transformer\u2019s impressive capabilities of tackling spatial relationships and long-range dependency challenges in two ways, i.e., (1) using their self-attention mechanism to generate comprehensive features, and (2) capture complex patterns by incorporating global context and long-range dependencies. In this work, a Bi-Vision Transformer (BiViT) architecture is proposed for classifying different stages of AD, and multiple types of cognitive disorders from 2-dimensional MRI imaging data. More specifically, the transformer is composed of two novel modules, namely Mutual Latent Fusion (MLF) and Parallel Coupled Encoding Strategy (PCES), for effective feature learning. Two different datasets have been used to evaluate the performance of proposed BiViT-based architecture. The first dataset contain several classes such as mild or moderate demented stages of the AD. The other dataset is composed of samples from patients with AD and different cognitive disorders such as mild, early, or moderate impairments. For comprehensive comparison, a multiple transfer learning algorithm and a deep autoencoder have been each trained on both datasets. The results show that the proposed BiViT-based model achieves an accuracy of 96.38% on the AD dataset. However, when applied to cognitive disease data, the accuracy slightly decreases below 96% which can be resulted due to smaller amount of data and imbalance in data distribution. Nevertheless, given the results, it can be hypothesized that the proposed algorithm can perform better if the imbalanced distribution and limited availability problems in data can be addressed.<\/jats:p>\n                <jats:p><jats:bold>Graphical abstract<\/jats:bold><\/jats:p>",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "2016 Alzheimer's disease facts and figures"
      ],
      "abstract":[
        "This report describes the public health impact of Alzheimer's disease, including incidence and prevalence, mortality rates, costs care, overall on caregivers society. It also examines in detail financial families, annual to families difficult decisions must often make pay those costs. An estimated 5.4 million Americans have disease. By mid-century, number people living with disease United States is projected grow 13.8 million, fueled large part by aging baby boom generation. Today, someone country develops every 66 seconds. 2050, one new case expected develop 33 seconds, resulting nearly 1 cases per year. In 2013, official death certificates recorded 84,767 deaths from making it sixth leading cause fifth age \u2265 65 years. Between 2000 stroke, heart prostate cancer decreased 23%, 14%, 11%, respectively, whereas increased 71%. The actual which contributes likely much larger than certificates. 2016, an 700,000 years will die many them because complications caused 2015, more 15 family members other unpaid provided 18.1 billion hours care dementias, a contribution valued at $221 billion. Average per-person Medicare payments for services beneficiaries dementias are two half times as great all without these conditions, Medicaid 19 great. Total 2016 long-term hospice dementia be $236 may place substantial burden who take money out their retirement savings, cut back buying food, reduce own trips doctor. addition, incorrectly believe that pays nursing home types care. Such findings highlight need solutions prevent dementia-related jeopardizing security dementias."
      ],
      "categories":[
        "q-bio.NC"
      ]
    },
    "list":{
      "title":[
        "Language modulates vision: Evidence from neural networks and human\n  brain-lesion models",
        "Long-term follow-up of DYT1 dystonia patients treated by deep brain\n  stimulation: an open-label study",
        "Language-specific Tonal Features Drive Speaker-Listener Neural\n  Synchronization",
        "Localization of Seizure Onset Zone based on Spatio-Temporal Independent\n  Component Analysis on fMRI",
        "A Relativistic Theory of Consciousness (shortened version)",
        "Electrophysiological Investigation of Insect Pain Threshold",
        "Mechanoreceptive A$\\beta$ primary afferents discriminate naturalistic\n  social touch inputs at a functionally relevant time scale",
        "The Neural Basis of Groove Sensations: Implications for Music-Based\n  Interventions and Dance Therapy in Parkinson's Disease",
        "Understanding and controlling the geometry of memory organization in\n  RNNs",
        "A UDP Packet Format Establishing Adress Event Representation\n  Communication Between Remote Neuromorphic and Biological Setups",
        "Increased GM-WM in a prefrontal network and decreased GM in the insula\n  and the precuneus are associated with reappraisal usage: A data fusion\n  approach",
        "Dynamic Markov Blanket Detection for Macroscopic Physics Discovery",
        "Deviance Detection and Regularity Sensitivity in Dissociated Neuronal\n  Cultures",
        "Axion Stabilization in Modular Cosmology",
        "Reconstruction of space-dependence and nonlinearity of a reaction term\n  in a subdiffusion equation",
        "Sharp Anti-Concentration Inequalities for Extremum Statistics via\n  Copulas",
        "On the spectral gap of negatively curved covers",
        "Fluid Reconfigurable Intelligent Surfaces: Joint On-Off Selection and\n  Beamforming with Discrete Phase Shifts",
        "Forecasting Local Ionospheric Parameters Using Transformers",
        "Fourier dimension of the graph of fractional Brownian motion with $H \\ge\n  1\/2$",
        "A generalization of Zwegers' multivariable $\\mu$-function",
        "Enumeration of lattices of nullity $k$ and containing $r$ comparable\n  reducible elements",
        "Coherent manifolds",
        "Study of event and particle selection effects on elliptic flow\n  background at the isobar experiments based on AMPT model",
        "Progress of the TianQin project",
        "Dimension-free Score Matching and Time Bootstrapping for Diffusion\n  Models",
        "\"Short-length\" Adversarial Training Helps LLMs Defend \"Long-length\"\n  Jailbreak Attacks: Theoretical and Empirical Evidence",
        "Modulating Optical Properties through Cation Substitution:\n  Composition-Property Relationships in $M^I_3$$M^{III}$P$_3$O$_9$N:Eu$^{2+}$\n  ($M^I$=Na, K; $M^{III}$=Al, Ga, In)"
      ],
      "abstract":[
        "Comparing information structures in between deep neural networks (DNNs) and\nthe human brain has become a key method for exploring their similarities and\ndifferences. Recent research has shown better alignment of vision-language DNN\nmodels, such as CLIP, with the activity of the human ventral occipitotemporal\ncortex (VOTC) than earlier vision models, supporting the idea that language\nmodulates human visual perception. However, interpreting the results from such\ncomparisons is inherently limited due to the \"black box\" nature of DNNs. To\naddress this, we combined model-brain fitness analyses with human brain lesion\ndata to examine how disrupting the communication pathway between the visual and\nlanguage systems causally affects the ability of vision-language DNNs to\nexplain the activity of the VOTC. Across four diverse datasets, CLIP\nconsistently outperformed both label-supervised (ResNet) and unsupervised\n(MoCo) models in predicting VOTC activity. This advantage was left-lateralized,\naligning with the human language network. Analyses of the data of 33 stroke\npatients revealed that reduced white matter integrity between the VOTC and the\nlanguage region in the left angular gyrus was correlated with decreased CLIP\nperformance and increased MoCo performance, indicating a dynamic influence of\nlanguage processing on the activity of the VOTC. These findings support the\nintegration of language modulation in neurocognitive models of human vision,\nreinforcing concepts from vision-language DNN models. The sensitivity of\nmodel-brain similarity to specific brain lesions demonstrates that leveraging\nmanipulation of the human brain is a promising framework for evaluating and\ndeveloping brain-like computer models.",
        "Long-term efficacy of internal globus pallidus (GPi) deep-brain stimulation\n(DBS) in DYT1 dystonia and disease progression under DBS was studied.\nTwenty-six patients of this open-label study were divided into two groups: (A)\nwith single bilateral GPi lead, (B) with a second bilateral GPi lead implanted\nowning to subsequent worsening of symptomatology. Dystonia was assessed with\nthe Burke Scale. Appearance of new symptoms and distribution according to body\nregion were recorded. In the whole cohort, significant decreases in motor and\ndisability subscores (P < 0.0001) were observed at 1 year and maintained up to\n10 years. Group B showed worsening of the symptoms. At 1 year, there were no\nsignificant differences between Groups A (without subsequent worsening) and B;\nat 5 years, a significant difference was found for motor and disability scores.\nWithin Group B, four patients exhibited additional improvement after the second\nDBS surgery. In the 26 patients, significant difference (P = 0.001) was found\nbetween the number of body regions affected by dystonia preoperatively and over\nthe whole follow-up. DBS efficacy in DYT1 dystonia can be maintained up to 10\nyears (two patients). New symptoms appear with long-term follow-up and may\nimprove with additional leads in a subgroup of patients.",
        "Verbal communication transmits information across diverse linguistic levels,\nwith neural synchronization (NS) between speakers and listeners emerging as a\nputative mechanism underlying successful exchange. However, the specific speech\nfeatures driving this synchronization and how language-specific versus\nuniversal characteristics facilitate information transfer remain poorly\nunderstood. We developed a novel content-based interbrain encoding model to\ndisentangle the contributions of acoustic and linguistic features to\nspeaker-listener NS during Mandarin storytelling and listening, as measured via\nmagnetoencephalography (MEG). Results revealed robust NS throughout\nfrontotemporal-parietal networks with systematic time lags between speech\nproduction and perception. Crucially, suprasegmental lexical tone features\n(tone categories, pitch height, and pitch contour), essential for lexical\nmeaning in Mandarin, contributed more significantly to NS than either acoustic\nelements or universal segmental units (consonants and vowels). These tonal\nfeatures generated distinctive spatiotemporal NS patterns, creating\nlanguage-specific neural \"communication channels\" that facilitated efficient\nrepresentation sharing between interlocutors. Furthermore, the strength and\npatterns of NS driven by these language-specific features predicted\ncommunication success. These findings demonstrate the neural mechanisms\nunderlying shared representations during verbal exchange and highlight how\nlanguage-specific features can shape neural coupling to optimize information\ntransfer during human communication.",
        "Localizing the seizure onset zone (SOZ) as a step of presurgical planning\nleads to higher efficiency in surgical and stimulation treatments. However, the\nclinical localization including structural, ictal, and invasive data\nacquisition and assessment is a difficult and long procedure with increasing\nchallenges in patients with complex epileptic foci. The interictal methods are\nproposed to assist in presurgical planning with simpler data acquisition and\nhigher speed. This study presents a spatiotemporal component classification for\nthe localization of epileptic foci using resting-state functional magnetic\nresonance imaging data. This method is based on spatiotemporal independent\ncomponent analysis on rsfMRI with a component-sorting procedure upon dominant\npower frequency, biophysical constraints, spatial lateralization, local\nconnectivity, temporal energy, and functional non-Gaussianity. This method\nutilized the rs-fMRI potential to reach a high spatial accuracy in localizing\nepileptic foci from interictal data while retaining the reliability of results\nfor clinical usage. Thirteen patients with temporal lobe epilepsy who underwent\nsurgical resection and had seizure-free surgical outcomes after a 12-month\nfollow-up were included in this study. All patients had presurgical structural\nMRI and rsfMRI while postsurgical MRI images were available for ten. Based on\nthe relationship between the localized foci and resection, the results were\nclassified into three groups fully concordant, partially concordant, and\ndiscordant. These groups had the resulting cluster aligned with, in the same\nlobe with, and outside the lobe of the resection area, respectively.",
        "This paper is a shortened version of the full paper that was published in the\njournal Frontiers of Psychology in May 2022. In recent decades, the scientific\nstudy of consciousness has significantly increased our understanding of this\nelusive phenomenon. Yet, despite critical development in our understanding of\nthe functional side of consciousness, we still lack a fundamental theory\nregarding its phenomenal aspect. The phenomenal aspect of consciousness is the\nfirst-person answer to what it is like question, and it has thus far proved\nrecalcitrant to direct scientific investigation. The question of how the brain,\nor any cognitive system, can create conscious experience out of neural\nrepresentations poses a great conundrum to science. Naturalistic dualists argue\nthat it is composed of a primitive, private, nonreductive element of reality.\nIllusionists, on the other hand, argue that it is merely a cognitive illusion.\nWe contend that both the dualist and illusionist positions are flawed because\nthey tacitly assume consciousness to be an absolute property that does not\ndepend on the observer. We developed a conceptual and a mathematical argument\nfor a relativistic theory of consciousness in which a system either has or does\nnot have phenomenal consciousness with respect to some observer. According to\nthe theory, Phenomenal consciousness is neither private nor delusional, just\nrelativistic. In the frame of reference of the cognitive system, it will be\nobservable (first-person perspective) and in other frame of reference it will\nnot (third-person perspective). These two cognitive frames of reference are\nboth correct, just as in the case of an observer that claims to be at rest\nwhile another will claim that the observer has constant velocity. Neither\nobserver position can be privileged, as they both describe the same underlying\nreality.",
        "The question of whether insects experience pain has long been debated in\nneuroscience and animal behavior research. Increasing evidence suggests that\ninsects possess the ability to detect and respond to noxious stimuli,\nexhibiting behaviors indicative of pain perception. This study investigates the\nrelationship between pain stimuli and physiological responses in crickets\n(Gryllidae), focusing on heart rate (ECG) and brain wave (EEG) patterns. We\napplied a range of mechanical, chemical, thermal, and electrical stimuli to\ncrickets, recording ECG and EEG data while employing a deep learning-based\nmodel to classify pain levels. Our findings revealed significant heart rate\nchanges and EEG fluctuations in response to various stimuli, with the highest\nintensity stimuli inducing marked physiological stress. The AI-based analysis,\nutilizing AlexNet for EEG signal classification, achieved 90% accuracy in\ndistinguishing between resting, low-pain, and high-pain states. While no social\nsharing of pain was observed through ECG measurements, these results contribute\nto the growing body of evidence supporting insect nociception and offer new\ninsights into their physiological responses to external stressors. This\nresearch advances the understanding of insect pain mechanisms and demonstrates\nthe potential for AI-driven analysis in entomological studies.",
        "Interpersonal touch is an important channel of social emotional interaction.\nHow these physical skin-to-skin touch expressions are processed in the\nperipheral nervous system is not well understood. From microneurography\nrecordings in humans, we evaluated the capacity of six subtypes of cutaneous\nmechanoreceptive afferents to differentiate human-delivered social touch\nexpressions. Leveraging statistical and classification analyses, we found that\nsingle units of multiple mechanoreceptive A$\\beta$ subtypes, especially slowly\nadapting type II (SA-II) and fast adapting hair follicle afferents (HFA), can\nreliably differentiate social touch expressions at accuracies similar to human\nrecognition. We then identified the most informative firing patterns of SA-II\nand HFA afferents, which indicate that average durations of 3-4 s of firing\nprovide sufficient discriminative information. Those two subtypes also exhibit\nrobust tolerance to spike-timing shifts of up to 10-20 ms, varying with touch\nexpressions due to their specific firing properties. Greater shifts in\nspike-timing, however, can change a firing pattern's envelope to resemble that\nof another expression and drastically compromise an afferent's discrimination\ncapacity. Altogether, the findings indicate that SA-II and HFA afferents\ndifferentiate the skin contact of social touch at time scales relevant for such\ninteractions, which are 1-2 orders of magnitude longer than those for\nnon-social touch.",
        "Groove sensations arise from rhythmic structures that evoke an urge to move\nin response to music. While syncopation has been extensively studied in groove\nperception, the neural mechanisms underlying low-frequency groove remain\nunderexplored. This fMRI study examines the role of the mirror neuron system\nand associated brain regions in processing low-frequency groove.\nRegion-of-interest analysis revealed that amplifying drum and bass components\nin K-pop songs significantly increased activity in the right posterior inferior\nfrontal gyrus, right inferior\/superior parietal lobules, left dorsolateral\nprefrontal cortex, and bilateral posterior middle\/inferior temporal gyrus.\nThese findings suggest that low-frequency grooves engage sensorimotor,\nexecutive, and rhythm semantics networks, reinforcing their role in\naction-related processing. Building on these insights, we propose an enhanced\nrhythmic auditory stimulation paradigm for Parkinson's disease, incorporating\namplified low-frequency rhythmic cues to improve gait synchronization.",
        "Training recurrent neural networks (RNNs) is a high-dimensional process that\nrequires updating numerous parameters. Therefore, it is often difficult to\npinpoint the underlying learning mechanisms. To address this challenge, we\npropose to gain mechanistic insights into the phenomenon of \\emph{abrupt\nlearning} by studying RNNs trained to perform diverse short-term memory tasks.\nIn these tasks, RNN training begins with an initial search phase. Following a\nlong period of plateau in accuracy, the values of the loss function suddenly\ndrop, indicating abrupt learning. Analyzing the neural computation performed by\nthese RNNs reveals geometric restructuring (GR) in their phase spaces prior to\nthe drop. To promote these GR events, we introduce a temporal consistency\nregularization that accelerates (bioplausible) training, facilitates attractor\nformation, and enables efficient learning in strongly connected networks. Our\nfindings offer testable predictions for neuroscientists and emphasize the need\nfor goal-agnostic secondary mechanisms to facilitate learning in biological and\nartificial networks.",
        "In the field of brain-machine interfaces, biohybrids offer an interesting new\nperspective, as in them, the technological side acts like a closed-loop\nextension or real counterpart of biological tissue, instead of the usual open\nloop approaches in tranditional BMI. To achieve a credible counterpart to\nbiological tissue, biohybrids usually employ one or several neuromorphic\ncomponents as the hardware half of the biohybrid. However, advanced\nneuromorphic circuit such as memristor crossbars usually operate best in a\ndedicated lab with corresponding support equipment. The same is true for\nbiological tissue, which makes co-locating all of the parts of a biohybrid in\nthe same lab challenging. Here, we present as solution to this co-location\nissue a simple method to connect biohybrids via the internet by a custom UDP\npacket format. We show that the characteristics achieved with our solution\n(jitter, delay, packet loss, packet reordering) on a standard internet\nconnection are compatible with various biohybrid processing paradigms, and we\npresent a short three-ways experiment as proof-of-concept. The described UDP\nformat has been employed to link biohybrids and neuromorphic circuits in four\ndifferent EC-funded projects.",
        "Emotion regulation plays a crucial role in mental health, and difficulties in\nregulating emotions can contribute to psychological disorders. While\nreappraisal and suppression are well-studied strategies, the combined\ncontributions of gray matter (GM) and white matter (WM) to these strategies\nremain unclear due to methodological limitations in previous studies. To\naddress this, we applied a data fusion approach using Parallel Independent\nComponent Analysis (Parallel ICA) to GM and WM MRI images from 165 individuals.\nParallel ICA identified two networks associated with reappraisal usage. Network\n1 included a large lateral and medial prefrontal cortical network, overlapping\nwith the default mode network (DMN) and adjacent WM regions. Higher reappraisal\nfrequency was associated with greater GM-WM density within this network, and\nthis network was negatively correlated with perceived stress. Network 2\nincluded the insula, precuneus, sub-gyral, and lingual gyri in its GM portion,\nshowing a negative association with reappraisal usage. The WM portion, adjacent\nto regions of the central executive network (CEN), was positively associated\nwith reappraisal usage. Regarding suppression, no significant network was\nassociated with this strategy. This study provides new insights into individual\ndifferences in reappraisal use, showing a positive association between\nreappraisal frequency and increased gray and white matter concentration in a\nlarge frontal network, including regions of the frontal DMN and the CEN.\nConversely, subcortical areas exhibited reduced gray and white concentration.",
        "The free energy principle (FEP), along with the associated constructs of\nMarkov blankets and ontological potentials, have recently been presented as the\ncore components of a generalized modeling method capable of mathematically\ndescribing arbitrary objects that persist in random dynamical systems; that is,\na mathematical theory of ``every'' ``thing''. Here, we leverage the FEP to\ndevelop a mathematical physics approach to the identification of objects,\nobject types, and the macroscopic, object-type-specific rules that govern their\nbehavior. We take a generative modeling approach and use variational Bayesian\nexpectation maximization to develop a dynamic Markov blanket detection\nalgorithm that is capable of identifying and classifying macroscopic objects,\ngiven partial observation of microscopic dynamics. This unsupervised algorithm\nuses Bayesian attention to explicitly label observable microscopic elements\naccording to their current role in a given system, as either the internal or\nboundary elements of a given macroscopic object; and it identifies macroscopic\nphysical laws that govern how the object interacts with its environment.\nBecause these labels are dynamic or evolve over time, the algorithm is capable\nof identifying complex objects that travel through fixed media or exchange\nmatter with their environment. This approach leads directly to a flexible class\nof structured, unsupervised algorithms that sensibly partition complex\nmany-particle or many-component systems into collections of interacting\nmacroscopic subsystems, namely, ``objects'' or ``things''. We derive a few\nexamples of this kind of macroscopic physics discovery algorithm and\ndemonstrate its utility with simple numerical experiments, in which the\nalgorithm correctly labels the components of Newton's cradle, a burning fuse,\nthe Lorenz attractor, and a simulated cell.",
        "Understanding how neural networks process complex patterns of information is\ncrucial for advancing both neuroscience and artificial intelligence. To\ninvestigate fundamental principles of neural computation, we studied\ndissociated neuronal cultures, one of the most primitive living neural\nnetworks, on high-resolution CMOS microelectrode arrays and tested whether the\ndissociated culture exhibits regularity sensitivity beyond mere\nstimulus-specific adaptation and deviance detection. In oddball electrical\nstimulation paradigms, we confirmed that the neuronal culture produced mismatch\nresponses (MMRs) with true deviance detection beyond mere adaptation. These\nMMRs were dependent on the N-methyl-D-aspartate (NMDA) receptors, similar to\nmismatch negativity (MMN) in humans, which is known to have true deviance\ndetection properties. Crucially, we also showed sensitivity to the statistical\nregularity of stimuli, a phenomenon previously observed only in intact brains:\nthe MMRs in a predictable, periodic sequence were smaller than those in a\ncommonly used sequence in which the appearance of the deviant stimulus was\nrandom and unpredictable. These results challenge the traditional view that a\nhierarchically structured neural network is required to process complex\ntemporal patterns, suggesting instead that deviant detection and regularity\nsensitivity are inherent properties arising from the primitive neural network.\nThey also suggest new directions for the development of neuro-inspired\nartificial intelligence systems, emphasizing the importance of incorporating\nadaptive mechanisms and temporal dynamics in the design of neural networks.",
        "The $SL(2,\\mathbb{Z})$ invariant $\\alpha$-attractor models have plateau\npotentials with respect to the inflaton and axion fields. The potential in the\naxion direction is almost exactly flat during inflation, hence, the axion field\nremains nearly massless. In this paper, we develop a generalized class of such\nmodels, where the $SL(2,\\mathbb{Z})$ symmetry is preserved, but the axion\nacquires a large mass and becomes strongly stabilized during inflation, which\neliminates isocurvature perturbations in this scenario. Inflation in such\ntwo-field models occurs as in the single-field $\\alpha$-attractors and leads to\nthe same cosmological predictions.",
        "In this paper we study the simultaneous reconstruction of two coefficients in\na reaction-subdiffusion equation, namely a nonlinearity and a space dependent\nfactor. The fact that these are coupled in a multiplicative matter makes the\nreconstruction particularly challenging. Several situations of overposed data\nare considered: boundary observations over a time interval, interior\nobservations at final time, as well as a combination thereof. We devise fixed\npoint schemes and also describe application of a frozen Newton method. In the\nfinal time data case we prove convergence of the fixed point scheme as well as\nuniqueness of both coefficients. Numerical experiments illustrate performance\nof the reconstruction methods, in particular dependence on the differentiation\norder in the subdiffusion equation.",
        "We derive sharp upper and lower bounds for the pointwise concentration\nfunction of the maximum statistic of $d$ identically distributed real-valued\nrandom variables. Our first main result places no restrictions either on the\ncommon marginal law of the samples or on the copula describing their joint\ndistribution. We show that, in general, strictly sublinear dependence of the\nconcentration function on the dimension $d$ is not possible. We then introduce\na new class of copulas, namely those with a convex diagonal section, and\ndemonstrate that restricting to this class yields a sharper upper bound on the\nconcentration function. This allows us to establish several new\ndimension-independent and poly-logarithmic-in-$d$ anti-concentration\ninequalities for a variety of marginal distributions under mild dependence\nassumptions. Our theory improves upon the best known results in certain special\ncases. Applications to high-dimensional statistical inference are presented,\nincluding a specific example pertaining to Gaussian mixture approximations for\nfactor models, for which our main results lead to superior distributional\nguarantees.",
        "Given a negatively curved compact Riemannian surface $X$, we give an explicit\nestimate, valid with high probability as the degree goes to infinity, of the\nfirst non-trivial eigenvalue of the Laplacian on random Riemannian covers of\n$X$. The explicit gap is given in terms of the bottom of the spectrum of the\nuniversal cover of $X$ and the topological entropy of the geodesic flow on X.\nThis result generalizes in variable curvature a result of Magee-Naud-Puder for\nhyperbolic surfaces. We then formulate a conjecture on the optimal spectral gap\nand show that there exists covers with near optimal spectral gaps using a\nresult of Louder-Magee and techniques of strong convergence from random matrix\ntheory.",
        "This letter proposes a fluid reconfigurable intelligent surface (FRIS)\nparadigm, extending the conventional reconfigurable intelligent surface (RIS)\ntechnology to incorporate position reconfigurability of the elements. In our\nmodel, a `fluid' element is realized by a dense matrix of subelements over a\ngiven space and dynamically selecting specific elements for signal modulation\nbased on channel conditions. Specifically, we consider a FRIS-assisted\nsingle-user single-input single-output (SU-SISO) system and formulate an\noptimization problem that can jointly optimize element selection and their\ndiscrete phase shifts to maximize the achievable rate. To address this problem\nefficiently, we propose an iterative algorithm based on the cross-entropy\noptimization (CEO) framework. Simulation results reveal that FRIS achieves\nsignificant performance gains over traditional RIS.",
        "We present a novel method for forecasting key ionospheric parameters using\ntransformer-based neural networks. The model provides accurate forecasts and\nuncertainty quantification of the F2-layer peak plasma frequency (foF2), the\nF2-layer peak density height (hmF2), and total electron content (TEC) for a\ngiven geographic location. It supports a number of exogenous variables,\nincluding F10.7cm solar flux and disturbance storm time (Dst). We demonstrate\nhow transformers can be trained in a data assimilation-like fashion that use\nthese exogenous variables along with na\\\"ive predictions from climatology to\ngenerate 24-hour forecasts with non-parametric uncertainty bounds. We call this\nmethod the Local Ionospheric Forecast Transformer (LIFT). We demonstrate that\nthe trained model can generalize to new geographic locations and time periods\nnot seen during training, and we compare its performance to that of the\nInternational Reference Ionosphere (IRI).",
        "We prove that the Fourier dimension of the graph of fractional Brownian\nmotion with Hurst index greater than $1\/2$ is almost surely 1. This extends the\nresult of Fraser and Sahlsten (2018) for the Brownian motion and verifies\npartly the conjecture of Fraser, Orponen and Sahlsten (2014). We introduce a\ncombinatorial integration by parts formula to compute the moments of the\nFourier transform of the graph measure. The proof of our main result is based\non this integration by parts formula together with Fa\\`a di Bruno's formula and\nstrong local nondeterminism of fractional Brownian motion. We also show that\nthe Fourier dimension of the graph of a symmetric $\\alpha$-stable process with\n$\\alpha\\in[1,2]$ is almost surely 1.",
        "We introduce a one parameter deformation of Zwegers' multivariable\n$\\mu$-function by applying iterations of the $q$-Borel summation method, which\nis also a multivariate analogue of the generalized $\\mu$-function introduced by\nthe authors. For this deformed multivariable $\\mu$-function, we give some\nformulas, for example, forward shift formula, translation and\n$\\mathfrak{S}_{N+1}$-symmetry. Further we mention modular formulas for the\nZwegers' original multivariable $\\mu$-function.",
        "In 2002 Thakare et al.\\ counted non-isomorphic lattices on $n$ elements,\nhaving nullity up to two. In 2020 Bhavale and Waphare introduced the concept of\nRC-lattices as the class of all lattices in which all the reducible elements\nare comparable. In this paper, we enumerate all non-isomorphic RC-lattices on\n$n$ elements. For this purpose, firstly we enumerate all non-isomorphic\nRC-lattices on $n \\geq 4$ elements, having nullity $k \\geq 1$, and containing\n$2 \\leq r \\leq 2k$ reducible elements. Secondly we enumerate all non-isomorphic\nRC-lattices on $n \\geq 4$ elements, having nullity $k \\geq 1$. This work is in\nrespect of Birkhoff's open problem of enumerating all finite lattices on $n$\nelements.",
        "This paper defines coherent manifolds and discusses their properties and\ntheir application in quantum mechanics. Every coherent manifold with a large\ngroup of symmetries gives rise to a Hilbert space, the completed quantum space\nof $Z$, which contains a distinguished family of coherent states labeled by the\npoints of the manifold.\n  The second quantization map in quantum field theory is generalized to\nquantization operators on arbitrary coherent manifolds. It is shown how the\nSchr\\\"odinger equation on any such completed quantum space can be solved in\nterms of computations only involving the coherent product. In particular, this\napplies to a description of Bosonic Fock spaces as completed quantum spaces of\na class of coherent manifolds called Klauder spaces.",
        "Measurement of the Chiral Magnetic Effect (CME) has been a popular topic of\nhigh-energy nuclear physics in the last decade. The flow correlation $\\gamma$\nbetween charged hadron pairs of the same and opposite charges and their\ndifference $\\Delta \\gamma$ were measured to separate the CME-driven signal from\nthe collective flow background especially second-order elliptic $v_{2}$. The\nSTAR experiment have stepped further to the isobar experiment to compare\n$\\gamma$ and $\\Delta \\gamma$ between Ru+Ru and Zr+Zr\n~\\cite{PhysRevC.105.014901}, which were theoretically expected to produce the\nsame elliptic flow background but different CME signals. However, the measured\nflow backgrounds also differ between Ru+Ru and Zr+Zr, indicating more\nfine-tuning of RP and centrality definition necessary.\n  This analysis applied the AMPT model~\\cite{PhysRevC.72.064901} to simulate\nthe same collision system and energy as the STAR isobar experiment. Since the\nAMPT model does not include magnetic field effects, we expect comparing its\noutput between Ru+Ru and Zr+Zr collision systems can provide an insight of the\npossible bias of flow background definition, and help improve the measurement\nof CME signal in real experiments. Multiple combinations of centrality and flow\ndefinition were chosen to study how the $v_2$ and their difference would be\naffected, especially by varying the particles selection of charge versus\nneutral properties and broadening (pseudo-)rapidity regions, while STAR CME\nwork relied on charged-only particles at central rapidity.",
        "TianQin is a future space-based gravitational wave observatory targeting the\nfrequency window of $10^{-4}$ Hz $\\sim 1$ Hz. A large variety of gravitational\nwave sources are expected in this frequency band, including the merger of\nmassive black hole binaries, the inspiral of extreme\/intermediate mass ratio\nsystems, stellar-mass black hole binaries, Galactic compact binaries, and so\non. TianQin will consist of three Earth orbiting satellites on nearly identical\norbits with orbital radii of about $10^5$ km. The satellites will form a normal\ntriangle constellation whose plane is nearly perpendicular to the ecliptic\nplane. The TianQin project has been progressing smoothly following the ``0123\"\ntechnology roadmap. In step ``0\", the TianQin laser ranging station has been\nconstructed and it has successfully ranged to all the five retro-reflectors on\nthe Moon. In step ``1\", the drag-free control technology has been tested and\ndemonstrated using the TianQin-1 satellite. In step ``2\", the inter-satellite\nlaser interferometry technology will be tested using the pair of TianQin-2\nsatellites. The TianQin-2 mission has been officially approved and the\nsatellites will be launched around 2026. In step ``3\", i.e., the TianQin-3\nmission, three identical satellites will be launched around 2035 to form the\nspace-based gravitational wave detector, TianQin, and to start gravitational\nwave detection in space.",
        "Diffusion models generate samples by estimating the score function of the\ntarget distribution at various noise levels. The model is trained using samples\ndrawn from the target distribution, progressively adding noise. In this work,\nwe establish the first (nearly) dimension-free sample complexity bounds for\nlearning these score functions, achieving a double exponential improvement in\ndimension over prior results. A key aspect of our analysis is the use of a\nsingle function approximator to jointly estimate scores across noise levels, a\ncritical feature of diffusion models in practice which enables generalization\nacross timesteps. Our analysis introduces a novel martingale-based error\ndecomposition and sharp variance bounds, enabling efficient learning from\ndependent data generated by Markov processes, which may be of independent\ninterest. Building on these insights, we propose Bootstrapped Score Matching\n(BSM), a variance reduction technique that utilizes previously learned scores\nto improve accuracy at higher noise levels. These results provide crucial\ninsights into the efficiency and effectiveness of diffusion models for\ngenerative modeling.",
        "Jailbreak attacks against large language models (LLMs) aim to induce harmful\nbehaviors in LLMs through carefully crafted adversarial prompts. To mitigate\nattacks, one way is to perform adversarial training (AT)-based alignment, i.e.,\ntraining LLMs on some of the most adversarial prompts to help them learn how to\nbehave safely under attacks. During AT, the length of adversarial prompts plays\na critical role in the robustness of aligned LLMs. This paper focuses on\nadversarial suffix jailbreak attacks and unveils that to defend against a\njailbreak attack with an adversarial suffix of length $\\Theta(M)$, it is enough\nto align LLMs on prompts with adversarial suffixes of length\n$\\Theta(\\sqrt{M})$. Theoretically, we analyze the adversarial in-context\nlearning of linear transformers on linear regression tasks and prove a robust\ngeneralization bound for trained transformers. The bound depends on the term\n$\\Theta(\\sqrt{M_{\\text{test}}}\/M_{\\text{train}})$, where $M_{\\text{train}}$ and\n$M_{\\text{test}}$ are the number of adversarially perturbed in-context samples\nduring training and testing. Empirically, we conduct AT on popular open-source\nLLMs and evaluate their robustness against jailbreak attacks of different\nadversarial suffix lengths. Results confirm a positive correlation between the\nattack success rate and the ratio of the square root of the adversarial suffix\nduring jailbreaking to the length during AT. Our findings show that it is\npractical to defend \"long-length\" jailbreak attacks via efficient\n\"short-length\" AT. The code is available at https:\/\/github.com\/fshp971\/adv-icl.",
        "Developing phosphors with narrow photoluminescence emission peaks and high\nchromatic stability holds significant importance in light-emitting diode (LED)\ndisplay technologies, where a wide color gamut is essential to achieve the Rec.\n2020 specifications. This research focuses on the optical properties of a solid\nsolution: $M^I_{2.97}$Eu$_{0.015}$$M^{III}$P$_3$O$_9$N [$M^I$=Na, K;\n$M^{III}$=Al, (Al$_{0.75}$Ga$_{0.25}$), (Al$_{0.5}$Ga$_{0.5}$),\n(Al$_{0.25}$Ga$_{0.75}$), Ga, (Ga$_{0.75}$In$_{0.25}$), (Ga$_{0.5}$In$_{0.5}$)]\nto understand how the narrow-emitting photoluminescence in\nK$_3$AlP$_3$O$_9$N:Eu$^{2+}$ can evolve during host structure cation\nsubstitution. Photoluminescence measurements at low temperature (15 K) support\nthat Eu$^{2+}$ replaces three crystallographically independent Na$^+$ sites in\nNa$_{2.97}$Eu$_{0.015}$AlP$_3$O$_9$N, similar to the parent K$^+$ phosphor, but\nsubstituting Ga$^{3+}$ and In$^{3+}$ for Al$^{3+}$ leads to a change in\nEu$^{2+}$ site preference, narrowing the full-width-at-half-maximum (fwhm) of\nthe emission peak. The chromatic stability and photoluminescence quantum yield\nare also enhanced with higher Ga$^{3+}$ content in the host but not with\nIn$^{3+}$. Thermoluminescence analysis indicates the relationship between trap\nstates and the enhanced quantum yield with Ga$^{3+}$ leads to the series' best\nperformance. The analysis of the $M^I_{2.97}$Eu$_{0.015}$$M^{III}$P$_3$O$_9$N\nseries offers insight into the potential method for modulating optical\nproperties with cation substitution in the host structure."
      ]
    }
  },
  {
    "id":2411.08073,
    "research_type":"applied",
    "start_id":"b24",
    "start_title":"DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome",
    "start_abstract":"Abstract Motivation Deciphering the language of non-coding DNA is one fundamental problems in genome research. Gene regulatory code highly complex due to existence polysemy and distant semantic relationship, which previous informatics methods often fail capture especially data-scarce scenarios. Results To address this challenge, we developed a novel pre-trained bidirectional encoder representation, named DNABERT, global transferrable understanding genomic sequences based on up downstream nucleotide contexts. We compared DNABERT most widely used programs for genome-wide elements prediction demonstrate its ease use, accuracy efficiency. show that single transformers model can simultaneously achieve state-of-the-art performance promoters, splice sites transcription factor binding sites, after easy fine-tuning using small task-specific labeled data. Further, enables direct visualization nucleotide-level importance relationship within input better interpretability accurate identification conserved sequence motifs functional genetic variant candidates. Finally, with human even be readily applied other organisms exceptional performance. anticipate fined tuned many analyses tasks. Availability implementation The source code, pretrained finetuned are available at GitHub (https:\/\/github.com\/jerryji1993\/DNABERT). Supplementary information data Bioinformatics online.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b20"
      ],
      "title":[
        "A primer on deep learning in genomics"
      ],
      "abstract":[
        "Deep learning methods are a class of machine learning techniques capable of identifying highly complex patterns in large datasets. Here, we provide a perspective and primer on deep learning applications for genome analysis. We discuss successful applications in the fields of regulatory genomics, variant calling and pathogenicity scores. We include general guidance for how to effectively use deep learning methods as well as a practical guide to tools and resources. This primer is accompanied by an interactive online tutorial."
      ],
      "categories":[
        "q-bio.GN"
      ]
    },
    "list":{
      "title":[
        "Genotype-to-Phenotype Prediction in Rice with High-Dimensional Nonlinear\n  Features",
        "Genomic Analysis of Date Palm Fruit Size Traits and Identification of\n  Candidate Genes through GWAS",
        "Find Central Dogma Again: Leveraging Multilingual Transfer in Large\n  Language Models",
        "GiantHunter: Accurate detection of giant virus in metagenomic data using\n  reinforcement-learning and Monte Carlo tree search",
        "Curated loci prime editing (cliPE) for accessible multiplexed assays of\n  variant effect (MAVEs)",
        "CoverM: Read alignment statistics for metagenomics",
        "Learnable Group Transform: Enhancing Genotype-to-Phenotype Prediction\n  for Rice Breeding with Small, Structured Datasets",
        "TopoLa: A Universal Framework to Enhance Cell Representations for\n  Single-cell and Spatial Omics through Topology-encoded Latent Hyperbolic\n  Geometry",
        "Origin of $\\alpha$-satellite repeat arrays from mitochondrial molecular\n  fossils -- sequential insertion, expansion, and evolution in the nuclear\n  genome",
        "Application of Single-cell Deep Learning in Elucidating the Mapping\n  Relationship Between Visceral and Body Surface Inflammatory Patterns",
        "Deep Learning-based Feature Discovery for Decoding Phenotypic Plasticity\n  in Pediatric High-Grade Gliomas Single-Cell Transcriptomics",
        "Transcriptome signature for the identification of bevacizumab responders\n  in ovarian cancer",
        "Causes of evolutionary divergence in prostate cancer",
        "Spatial locking of chimera states to frequency heterogeneity in\n  nonlocally coupled oscillators",
        "States of Disarray: Cleaning Data for Gerrymandering Analysis",
        "Wafer-scale waveguide sidewall roughness scattering loss\n  characterization by image processing",
        "The Aesthetic Imperative of Lev Landau's Geometric Reductionism in\n  Theoretical Physics",
        "BrainOOD: Out-of-distribution Generalizable Brain Network Analysis",
        "Fenchel's conjecture on NEC groups",
        "Using the STIX background detector as a proxy for GOES",
        "Exercises in Iterational Asymptotics II",
        "Tools for Unbinned Unfolding",
        "On stabilization at a soliton for generalized Korteweg--De Vries pure\n  power equation for any power $p\\in (1,5)$",
        "An improved evaluation of the electroweak contribution to $(g-2)_\\mu$",
        "The puzzle of isolated and quenched dwarf galaxies in cosmic voids",
        "Site-engineered ferromagnetism in Ca and Cr co-substituted Bismuth\n  Ferrite Nanoparticles",
        "A modified two-stage search framework for constrained multi-gradient\n  descent",
        "Ultrafast Charge Separation on the Nanoscale Induced by a Uniform Field"
      ],
      "abstract":[
        "Genotype-to-Phenotype prediction can promote advances in modern genomic\nresearch and crop improvement, guiding precision breeding and genomic\nselection. However, high-dimensional nonlinear features often hinder the\naccuracy of genotype-to-phenotype prediction by increasing computational\ncomplexity. The challenge also limits the predictive accuracy of traditional\napproaches. Therefore, effective solutions are needed to improve the accuracy\nof genotype-to-phenotype prediction. In our paper, we propose MLFformer.\nMLFformer is a Transformer-based architecture that incorporates the Fast\nAttention mechanism and a multilayer perceptron module to handle\nhigh-dimensional nonlinear features. In MLFformer, the Fast Attention mechanism\nis utilized to handle computational complexity and enhance processing\nefficiency. In addition, the MLP structure further captures high-dimensional\nnonlinear features. Through experiments, the results show that MLFformer\nreduces the average MAPE by 7.73% compared to the vanilla Transformer. In\nunivariate and multivariate prediction scenarios, MLFformer achieves the best\npredictive performance among all compared models.",
        "The commercial value of economically significant fruits, including date palm\nfruit (dates), is influenced by various factors, such as biochemical\ncomposition and morphological features like size, shape, and visual appearance,\nwhich are key determinants of their quality and market value. Dates are\ntypically consumed at the dry stage (Tamar), during which they exhibit a wide\nrange of physical characteristics, such as color, length, weight, and skin\nappearance. Understanding the genetic basis of these traits is crucial for\nimproving crop quality and breeding new cultivars. In this study, we integrated\na genome dataset from highly diverse date cultivars with phenotypes of dry\nfruit such as length, width, area, and weight, identifying multiple significant\ngenetic loci (SNPs) associated with these traits. We also identified candidate\ngenes located near the associated SNPs that are involved in biological\nprocesses such as cell differentiation, proliferation, growth, and the\nregulation of signalling pathways for growth regulators like auxin and abscisic\nacid, as observed in other plants. Gene expression analysis reveals that many\nof these genes are highly expressed in the early stage of fruit development\nwhen the fruit attains its maximum size and weight. These findings will enhance\nour understanding of genetic determinants of fruit size particularly at the\ncommercially important Tamar stage.",
        "In recent years, large language models (LLMs) have achieved state-of-the-art\nresults in various biological sequence analysis tasks, such as sequence\nclassification, structure prediction, and function prediction. Similar to\nadvancements in AI for other scientific fields, deeper research into biological\nLLMs has begun to focus on using these models to rediscover important existing\nbiological laws or uncover entirely new patterns in biological sequences. This\nstudy leverages GPT-like LLMs to utilize language transfer capabilities to\nrediscover the genetic code rules of the central dogma. In our experimental\ndesign, we transformed the central dogma into a binary classification problem\nof aligning DNA sequences with protein sequences, where positive examples are\nmatching DNA and protein sequences, and negative examples are non-matching\npairs. We first trained a GPT-2 model from scratch using a dataset comprising\nprotein sequences, DNA sequences, and sequences from languages such as English\nand Chinese. Subsequently, we fine-tuned the model using the natural language\nsentences similarity judgment dataset from PAWS-X. When tested on a dataset for\nDNA and protein sequence alignment judgment, the fine-tuned model achieved a\nclassification accuracy of 81%. The study also analyzed factors contributing to\nthis zero-shot capability, including model training stability and types of\ntraining data. This research demonstrates that LLMs can, through the transfer\nof natural language capabilities and solely relying on the analysis of\nsequences themselves, rediscover the central dogma without prior knowledge of\nit. This study bridges natural language and genetic language, opening a new\ndoor for AI-driven biological research.",
        "Motivation: Nucleocytoplasmic large DNA viruses (NCLDVs) are notable for\ntheir large genomes and extensive gene repertoires, which contribute to their\nwidespread environmental presence and critical roles in processes such as host\nmetabolic reprogramming and nutrient cycling. Metagenomic sequencing has\nemerged as a powerful tool for uncovering novel NCLDVs in environmental\nsamples. However, identifying NCLDV sequences in metagenomic data remains\nchallenging due to their high genomic diversity, limited reference genomes, and\nshared regions with other microbes. Existing alignment-based and machine\nlearning methods struggle with achieving optimal trade-offs between sensitivity\nand precision. Results: In this work, we present GiantHunter, a reinforcement\nlearning-based tool for identifying NCLDVs from metagenomic data. By employing\na Monte Carlo tree search strategy, GiantHunter dynamically selects\nrepresentative non-NCLDV sequences as the negative training data, enabling the\nmodel to establish a robust decision boundary. Benchmarking on rigorously\ndesigned experiments shows that GiantHunter achieves high precision while\nmaintaining competitive sensitivity, improving the F1-score by 10% and reducing\ncomputational cost by 90% compared to the second-best method. To demonstrate\nits real-world utility, we applied GiantHunter to 60 metagenomic datasets\ncollected from six cities along the Yangtze River, located both upstream and\ndownstream of the Three Gorges Dam. The results reveal significant differences\nin NCLDV diversity correlated with proximity to the dam, likely influenced by\nreduced flow velocity caused by the dam. These findings highlight the potential\nof GiantSeeker to advance our understanding of NCLDVs and their ecological\nroles in diverse environments.",
        "Multiplexed assays of variant effect (MAVEs) perform simultaneous\ncharacterization of many variants. Prime editing has been recently adopted for\nintroducing many variants in their native genomic contexts. However, robust\nprotocols and standards are limited, preventing widespread uptake. Herein, we\ndescribe curated loci prime editing (cliPE) which is an accessible, low-cost\nexperimental pipeline to perform MAVEs using prime editing of a target gene, as\nwell as a companion Shiny app (pegRNA Designer) to rapidly and easily design\nuser-specific MAVE libraries.",
        "Genome-centric analysis of metagenomic samples is a powerful method for\nunderstanding the function of microbial communities. Calculating read coverage\nis a central part of analysis, enabling differential coverage binning for\nrecovery of genomes and estimation of microbial community composition. Coverage\nis determined by processing read alignments to reference sequences of either\ncontigs or genomes. Per-reference coverage is typically calculated in an ad-hoc\nmanner, with each software package providing its own implementation and\nspecific definition of coverage. Here we present a unified software package\nCoverM which calculates several coverage statistics for contigs and genomes in\nan ergonomic and flexible manner. It uses 'Mosdepth arrays' for computational\nefficiency and avoids unnecessary I\/O overhead by calculating coverage\nstatistics from streamed read alignment results. CoverM is free software\navailable at https:\/\/github.com\/wwood\/coverm. CoverM is implemented in Rust,\nwith Python (https:\/\/github.com\/apcamargo\/pycoverm) and Julia\n(https:\/\/github.com\/JuliaBinaryWrappers\/CoverM_jll.jl) interfaces.",
        "Genotype-to-Phenotype (G2P) prediction plays a pivotal role in crop breeding,\nenabling the identification of superior genotypes based on genomic data. Rice\n(Oryza sativa), one of the most important staple crops, faces challenges in\nimproving yield and resilience due to the complex genetic architecture of\nagronomic traits and the limited sample size in breeding datasets. Current G2P\nprediction methods, such as GWAS and linear models, often fail to capture\ncomplex non-linear relationships between genotypes and phenotypes, leading to\nsuboptimal prediction accuracy. Additionally, population stratification and\noverfitting are significant obstacles when models are applied to small datasets\nwith diverse genetic backgrounds. This study introduces the Learnable Group\nTransform (LGT) method, which aims to overcome these challenges by combining\nthe advantages of traditional linear models with advanced machine learning\ntechniques. LGT utilizes a group-based transformation of genotype data to\ncapture spatial relationships and genetic structures across diverse rice\npopulations, offering flexibility to generalize even with limited data. Through\nextensive experiments on the Rice529 dataset, a panel of 529 rice accessions,\nLGT demonstrated substantial improvements in prediction accuracy for multiple\nagronomic traits, including yield and plant height, compared to\nstate-of-the-art baselines such as linear models and recent deep learning\napproaches. Notably, LGT achieved an R^2 improvement of up to 15\\% for yield\nprediction, significantly reducing error and demonstrating its ability to\nextract meaningful signals from high-dimensional, noisy genomic data. These\nresults highlight the potential of LGT as a powerful tool for genomic\nprediction in rice breeding, offering a promising solution for accelerating the\nidentification of high-yielding and resilient rice varieties.",
        "Recent advances in cellular research demonstrate that scRNA-seq characterizes\ncellular heterogeneity, while spatial transcriptomics reveals the spatial\ndistribution of gene expression. Cell representation is the fundamental issue\nin the two fields. Here, we propose Topology-encoded Latent Hyperbolic Geometry\n(TopoLa), a computational framework enhancing cell representations by capturing\nfine-grained intercellular topological relationships. The framework introduces\na new metric, TopoLa distance (TLd), which quantifies the geometric distance\nbetween cells within latent hyperbolic space, capturing the network's\ntopological structure more effectively. With this framework, the cell\nrepresentation can be enhanced considerably by performing convolution on its\nneighboring cells. Performance evaluation across seven biological tasks,\nincluding scRNA-seq data clustering and spatial transcriptomics domain\nidentification, shows that TopoLa significantly improves the performance of\nseveral state-of-the-art models. These results underscore the generalizability\nand robustness of TopoLa, establishing it as a valuable tool for advancing both\nbiological discovery and computational methodologies.",
        "Alpha satellite DNA is large tandem arrays of 150-400 bp units, and its\norigin remains an evolutionary mystery. In this research, we identified 1,545\nalpha-satellite-like (SatL) repeat units in the nuclear genome of jewel wasp\nNasonia vitripennis. Among them, thirty-nine copies of SatL were organized in\ntwo palindromic arrays in mitochondria, resulting in a 50% increase in the\ngenome size. Strikingly, genomic neighborhood analyses of 1,516 nuclear SatL\nrepeats revealed that they are located in NuMT (nuclear mitochondrial DNA)\nregions, and SatL phylogeny matched perfectly with mitochondrial genes and NuMT\npseudogenes. These results support that SatL arrays originated from ten\nindependent mitochondria insertion events into the nuclear genome within the\nlast 500,000 years, after divergence from its sister species N. giraulti.\nDramatic repeat GC-percent elevation (from 33.9% to 50.4%) is a hallmark of\nrapid SatL sequence evolution in mitochondria due to GC-biased gene conversion\nfacilitated by the palindromic sequence pairing of the two mitochondrial SatL\narrays. The nuclear SatL repeat arrays underwent substantial copy number\nexpansion, from 12-15 (SatL1) to over 400 copies (SatL4). The oldest SatL4B\narray consists of four types of repeat units derived from deletions in the\nAT-rich region of ancestral repeats, and complex high-order structures have\nevolved through duplications. We also discovered similar repeat insertions into\nthe nuclear genome of Muscidifurax, suggesting this mechanism can be common in\ninsects. This is the first report of the mitochondrial origin of nuclear\nsatellite sequences, and our findings shed new light on the origin and\nevolution of satellite DNA.",
        "As a system of integrated homeostasis, life is susceptible to disruptions by\nvisceral inflammation, which can disturb internal environment equilibrium. The\nrole of body-spread subcutaneous fascia (scFascia) in this process is poorly\nunderstood. In the rat model of Salmonella-induced dysentery, scRNA-seq of\nscFascia and deep-learning analysis revealed Warburg-like metabolic\nreprogramming in macrophages (MPs) with reduced citrate cycle activity.\nCd34+\/Pdgfra+ telocytes (CPTCs) regulated MPs differentiation and proliferation\nvia Wnt\/Fgf signal, suggesting a pathological crosstalk pattern in the\nscFascia, herein termed the fascia-visceral inflammatory crosstalk pattern\n(FVICP). PySCENIC analysis indicated increased activity transcription factors\nFosl1, Nfkb2, and Atf4, modulated by CPTCs signaling to MPs, downregulating\naerobic respiration and upregulating cell cycle, DNA replication, and\ntranscription. This study highlights scFascia's role in immunomodulation and\nmetabolic reprogramming during visceral inflammation, underscoring its function\nin systemic homeostasis.",
        "By use of complex network dynamics and graph-based machine learning, we\nidentified critical determinants of lineage-specific plasticity across the\nsingle-cell transcriptomics of pediatric high-grade glioma (pHGGs) subtypes:\nIDHWT glioblastoma and K27M-mutant glioma. Our study identified network\ninteractions regulating glioma morphogenesis via the tumor-immune\nmicroenvironment, including neurodevelopmental programs, calcium dynamics, iron\nmetabolism, metabolic reprogramming, and feedback loops between MAPK\/ERK and\nWNT signaling. These relationships highlight the emergence of a hybrid spectrum\nof cellular states navigating a disrupted neuro-differentiation hierarchy. We\nidentified transition genes such as DKK3, NOTCH2, GATAD1, GFAP, and SEZ6L in\nIDHWT glioblastoma, and H3F3A, ANXA6, HES6\/7, SIRT2, FXYD6, PTPRZ1, MEIS1,\nCXXC5, and NDUFAB1 in K27M subtypes. We also identified MTRNR2L1, GAPDH, IGF2,\nFKBP variants, and FXYD7 as transition genes that influence cell fate\ndecision-making across both subsystems. Our findings suggest pHGGs are\ndevelopmentally trapped in states exhibiting maladaptive behaviors, and hybrid\ncellular identities. In effect, tumor heterogeneity (metastability) and\nplasticity emerge as stress-response patterns to immune-inflammatory\nmicroenvironments and oxidative stress. Furthermore, we show that pHGGs are\nsteered by developmental trajectories from radial glia predominantly favoring\nneocortical cell fates, in telencephalon and prefrontal cortex (PFC)\ndifferentiation. By addressing underlying patterning processes and plasticity\nnetworks as therapeutic vulnerabilities, our findings provide precision\nmedicine strategies aimed at modulating glioma cell fates and overcoming\ntherapeutic resistance. We suggest transition therapy toward neuronal-like\nlineage differentiation as a potential therapy to help stabilize pHGG\nplasticity and aggressivity.",
        "The standard of care for ovarian cancer comprises cytoreductive surgery,\nfollowed by adjuvant platinum-based chemotherapy plus taxane therapy and\nmaintenance therapy with the antiangiogenic compound bevacizumab and\/or a PARP\ninhibitor. Nevertheless, there is currently no clear clinical indication for\nthe use of bevacizumab, highlighting the urgent need for biomarkers to assess\nthe response to bevacizumab. In the present study, based on a novel RNA-seq\ndataset (n=181) and a previously published microarray-based dataset (n=377), we\nhave identified an expression signature potentially associated with benefit\nfrom bevacizumab addition and assumed to reflect cancer stemness acquisition\ndriven by activation of CTCFL. Patients with this signature demonstrated\nimproved overall survival when bevacizumab was added to standard chemotherapy\nin both novel (HR=0.41(0.23-0.74), adj.p-value=7.70e-03) and previously\npublished cohorts (HR=0.51(0.34-0.75), adj.p-value=3.25e-03), while no\nsignificant differences in survival explained by treatment were observed in\npatients negative for this signature. In addition to the CTCFL signature, we\nfound several other reproducible expression signatures which may also represent\nbiomarker candidates not related to established molecular subtypes of ovarian\ncancer and require further validation studies based on additional RNA-seq data.",
        "Cancer progression involves the sequential accumulation of genetic\nalterations that cumulatively shape the tumour phenotype. In prostate cancer,\ntumours can follow divergent evolutionary trajectories that lead to distinct\nsubtypes, but the causes of this divergence remain unclear. While causal\ninference could elucidate the factors involved, conventional methods are\nunsuitable due to the possibility of unobserved confounders and ambiguity in\nthe direction of causality. Here, we propose a method that circumvents these\nissues and apply it to genomic data from 829 prostate cancer patients. We\nidentify several genetic alterations that drive divergence as well as others\nthat prevent this transition, locking tumours into one trajectory. Further\nanalysis reveals that these genetic alterations may cause each other, implying\na positive-feedback loop that accelerates divergence. Our findings provide\ninsights into how cancer subtypes emerge and offer a foundation for genomic\nsurveillance strategies aimed at monitoring the progression of prostate cancer.",
        "Chimera states in systems of nonlocally coupled oscillators, i.e.,\nself-organized coexistence of coherent and incoherent oscillator populations,\nhave attracted much attention. In this study, we consider the effect of\nfrequency heterogeneities on the chimera state and reveal that it induces\nspatial locking of the chimera state, i.e., the coherent and incoherent domains\nalign with lower and higher frequency regions, respectively, in a self-adaptive\nmanner. Using an extended self-consistency approach, we show that such\nspatially locked chimera states can be reproduced as steady solutions of the\nsystem in the continuum limit. Furthermore, we develop a variational argument\nto explain the mechanism leading to spatial locking. Our analysis reveals how\nheterogeneity can affect the collective dynamics of the chimera states and\noffers insights into their control and applications.",
        "The mathematics of redistricting is an area of study that has exploded in\nrecent years. In particular, many different research groups and expert\nwitnesses in court cases have used outlier analysis to argue that a proposed\nmap is a gerrymander. This outlier analysis relies on having an ensemble of\npotential redistricting maps against which the proposed map is compared.\nArguably the most widely-accepted method of creating such an ensemble is to use\na Markov Chain Monte Carlo (MCMC) process. This process requires that various\npieces of data be gathered, cleaned, and coalesced into a single file that can\nbe used as the seed of the MCMC process.\n  In this article, we describe how we have begun this cleaning process for each\nstate, and made the resulting data available for the public at\nhttps:\/\/github.com\/eveomett-states . At the time of submission, we have data\nfor 22 states available for researchers, students, and the general public to\neasily access and analyze. We will continue the data cleaning process for each\nstate, and we hope that the availability of these datasets will both further\nresearch in this area, and increase the public's interest in and understanding\nof modern techniques to detect gerrymandering.",
        "Photonic integrated circuits (PICs) are vital for developing affordable,\nhigh-performance optoelectronic devices that can be manufactured at an\nindustrial scale, driving innovation and efficiency in various applications.\nOptical loss of modes in thin film waveguides and devices is a critical measure\nof their performance. Thin films growth, lithography, masking, and etching\nprocesses are imperfect processes that introduce significant sidewall and\ntop-surface roughness and cause dominating optical losses in waveguides and\nphotonic structures. These roughness as perturbations couple light from guided\nto far-field radiation modes, leading to scattering losses that can be\nestimated from theoretical models. Typically, with UV-based lithography\nsidewall roughness is found to be significantly larger than wafer-top surface\nroughness. Atomic force microscopy (AFM) imaging measurement gives 3D and\nhigh-resolution roughness profile but the measurement is inconvenient, costly,\nand unscalable for large-scale PICs and at wafer-scale. Here, we evaluate the\nsidewall roughness profile based on 2D high-resolution scanning electron\nmicroscope imaging. We characterized the loss on two homemade nitride and oxide\nfilms on 3-inch silicon wafers with 12 waveguide devices on each and co-related\nthe scattering loss estimated from a 2D image-based sidewall profile and\ntheoretical Payne model. The lowest loss of guided fundamental transverse\nelectric (TE$_{0}$) is found at 0.075 dB\/cm at 633 nm across 24 devices, which\nis a record at visible wavelength. Our work shows a 100% success in edge\ndetection in image processing to estimate autocorrelation function and optical\nmode loss. These demonstrations offer valuable insights into waveguide sidewall\nroughness and comparison of experimental and 2D SEM image-processing-based loss\nestimations.",
        "This paper explores the ontological and epistemological foundations of Lev\nLandau's theoretical physics through the lens of his unpublished philosophical\nnotes and scientific practice. We identify a unique form of geometric\nreductionism where physical laws emerge as inevitable consequences of symmetry\nbreaking in progressively constrained phase spaces. Landau's dismissal of\nquantum interpretation debates and his famous \"axiomatic minimalism\" in the\nCourse of Theoretical Physics are shown to stem from a deep epistemological\ncommitment to dimensional aesthetics - the belief that fundamental truths must\nmanifest through dimensional economy in mathematical representations.",
        "In neuroscience, identifying distinct patterns linked to neurological\ndisorders, such as Alzheimer's and Autism, is critical for early diagnosis and\neffective intervention. Graph Neural Networks (GNNs) have shown promising in\nanalyzing brain networks, but there are two major challenges in using GNNs: (1)\ndistribution shifts in multi-site brain network data, leading to poor\nOut-of-Distribution (OOD) generalization, and (2) limited interpretability in\nidentifying key brain regions critical to neurological disorders. Existing\ngraph OOD methods, while effective in other domains, struggle with the unique\ncharacteristics of brain networks. To bridge these gaps, we introduce BrainOOD,\na novel framework tailored for brain networks that enhances GNNs' OOD\ngeneralization and interpretability. BrainOOD framework consists of a feature\nselector and a structure extractor, which incorporates various auxiliary losses\nincluding an improved Graph Information Bottleneck (GIB) objective to recover\ncausal subgraphs. By aligning structure selection across brain networks and\nfiltering noisy features, BrainOOD offers reliable interpretations of critical\nbrain regions. Our approach outperforms 16 existing methods and improves\ngeneralization to OOD subjects by up to 8.5%. Case studies highlight the\nscientific validity of the patterns extracted, which aligns with the findings\nin known neuroscience literature. We also propose the first OOD brain network\nbenchmark, which provides a foundation for future research in this field. Our\ncode is available at https:\/\/github.com\/AngusMonroe\/BrainOOD.",
        "A classical discovery known as Fenchel's conjecture and proved in the 1950s,\nshows that every co-compact Fuchsian group $F$ has a normal subgroup of finite\nindex isomorphic to the fundamental group of a compact unbordered orientable\nsurface, or in algebraic terms, that $F$ has a normal subgroup of finite index\nthat contains no element of finite order other than the identity. In this paper\nwe initiate and make progress on an extension of Fenchel's conjecture by\nconsidering the following question: Does every planar non-Euclidean\ncrystallographic group $\\Gamma$ containing transformations that reverse\norientation have a normal subgroup of finite index isomorphic to the\nfundamental group of a compact unbordered non-orientable surface? We answer\nthis question in the affirmative in the case where the orbit space of $\\Gamma$\nis a nonorientable surface, and also in the case where this orbit space is a\nbordered orientable surface of positive genus. In the case where the genus of\nthe quotient is $0$, we have an affirmative answer in many subcases, but the\nquestion is still open for others.",
        "Context. The Spectrometer\/Telescope for Imaging X-Rays (STIX) onboard Solar\nOrbiter was designed to observe solar flares in the X-ray range of 4-150 keV,\nproviding spectral, temporal and spatial information. Besides 30 imaging\ndetectors, STIX has two additional detectors, the coarse flare locator (CFL)\nand the background (BKG) detector. Flares observed from Earth are classified\nusing their peak X-ray flux observed by the GOES satellites. Roughly half of\nall flares observed by STIX are located on the backside of the Sun. These\nflares lack a GOES-class classification.\n  Aims. In this paper, we describe the calibration of the BKG detector aperture\nsizes. Using the calibrated measurements of the BKG detector, we explore the\nrelationship between the peak flux for flares jointly observed by STIX and\nGOES. This allows us to estimate the GOES flare classes of backside flares\nusing STIX measurements.\n  Methods. We looked at the 500 largest flares observed by both STIX and GOES\nin the time range Feb. 21 to Apr. 23. Aperture size calibration is done by\ncomparing 4-10 keV counts of the BKG detector with the CFL measurements. In a\nsecond step, we correlate the calibrated STIX BKG peak flux with the GOES peak\nflux for individual flares.\n  Results. We calibrated the BKG detector aperture sizes of STIX. Further, we\nshowed that for the larger flares a close power law fit exists between the STIX\nBKG and GOES peak flux with a Pearson correlation coefficient of 0.97. This\ncorrelation provides a GOES proxy with a one sigma uncertainty of 11%. We were\nable to show that the BKG detector can reliably measure a broad range of GOES\nflare classes from roughly B5 up to at least X85 (assuming a radial distance of\n1AU), making it an interesting detector-concept for future space weather\nmissions. The largest flare observed by STIX to date is an estimated X16.5\n$\\pm$ 1.8 backside flare on the 20 Mai 2024.",
        "The nonlinear recurrences we consider here include the functions $3x(1-x)$\nand $\\cos(x)$, which possess attractive fixed points $2\/3$ and $0.739...$\n(Dottie's number). Detailed asymptotics for oscillatory convergence are found,\nstarting with a 1960 paper by Wolfgang Thron. Another function,\n$x\/(1+x\\ln(1+x))$, gives rise to a sequence with monotonic convergence to $0$\nbut requires substantial work to calculate its associated constant $C$.",
        "Machine learning has enabled differential cross section measurements that are\nnot discretized. Going beyond the traditional histogram-based paradigm, these\nunbinned unfolding methods are rapidly being integrated into experimental\nworkflows. In order to enable widespread adaptation and standardization, we\ndevelop methods, benchmarks, and software for unbinned unfolding. For\nmethodology, we demonstrate the utility of boosted decision trees for unfolding\nwith a relatively small number of high-level features. This complements\nstate-of-the-art deep learning models capable of unfolding the full phase\nspace. To benchmark unbinned unfolding methods, we develop an extension of\nexisting dataset to include acceptance effects, a necessary challenge for real\nmeasurements. Additionally, we directly compare binned and unbinned methods\nusing discretized inputs for the latter in order to control for the binning\nitself. Lastly, we have assembled two software packages for the OmniFold\nunbinned unfolding method that should serve as the starting point for any\nfuture analyses using this technique. One package is based on the widely-used\nRooUnfold framework and the other is a standalone package available through the\nPython Package Index (PyPI).",
        "We apply our idea, which previously we used in the analysis of the pure power\nNLS, consisting in spitting the virial inequality method into a large energy\ninequality combined with Kato smoothing, to the case of generalized\nKorteweg--De Vries pure power equations. We assume that a solution remains for\nall positive times very close to a soliton and then we prove an asymptotic\nstability result for $t\\to +\\infty$.",
        "A precise evaluation of the electroweak contribution to the anomalous\nmagnetic moment of the muon requires control over all aspects of the Standard\nModel, ranging from Higgs physics, over multi-loop computations for bosonic and\n(heavy-)fermion diagrams, to non-perturbative effects in the presence of light\nquarks. Currently, the dominant uncertainties arise from such hadronic effects\nin the vector-vector-axial-vector three-point function, an improved\nunderstanding of which has recently emerged in the context of hadronic\nlight-by-light scattering. Profiting from these developments as well as new\nperturbative and non-perturbative input for the charm contribution, we obtain\n$a_\\mu^\\text{EW}=154.4(4)\\times 10^{-11}$.",
        "We report, for the first time, the detection of a sample of quenched and\nisolated dwarf galaxies (with 8.9 $<$ log(M$_{\\rm \\star}$\/M$_{\\rm \\odot}$) $<$\n9.5) in the least dense regions of the cosmic web, including voids, filaments,\nand walls. These dwarfs have no neighbouring galaxy within 1.0~Mpc in projected\ndistance. Based on the full spectral fitting of their central spectra using\nSloan Digital Sky Survey data, these galaxies are gas-deprived, exhibit stellar\nmass assembly very similar to dwarfs in the central regions of galaxy clusters,\nand have experienced no significant star formation in the past 2 Gyr.\nAdditionally, analysis of r-band images from the Dark Energy Camera Legacy\nSurvey showed that these dwarf galaxies host a central Nuclear Star Cluster\n(NSC). Detecting quenched, isolated dwarf galaxies in cosmic voids indicates\nthat environmental factors are not the sole drivers of their quenching.\nInternal mechanisms, such as feedback from in-situ star formation, also\ncontributing to the NSC formation, black holes, or variations in conditions\nduring their formation, offer potential explanations for star formation\nsuppression in these galaxies. These findings highlight the need for a\nsignificant revision in our understanding of baryonic physics, particularly\nconcerning the formation and evolution of low-mass galaxies.",
        "Multiferroic perovskites that exhibit room temperature magnetization and\npolarization have immense potential in the next generation of magneto-electric\nand spintronic memory devices. In this work, the magnetic and ferroelectric\nproperties of Bismuth Ferrite, BiFeO3 (BFO) nanoparticles (NPs) were enhanced\nthrough simultaneous A and B site Ca and Cr co-substitution. Novel compositions\nof Bi0.97Ca0.03CrxFe1-xO3 (x=0, 0.01, 0.03, 0.05) were synthesized using the\nsol-gel route and annealed at 550 degrees Celcius. Rietveld Refinement of XRD\npatterns confirmed high phase purity, while SEM analysis revealed a decreasing\ntrend in average particle size with increasing dopant concentration. Hysteresis\nloops showed enhanced magnetic properties as particle size approached the spin\ncycloid wavelength (around 62 nm), disrupting the intrinsic antiferromagnetic\nordering of BFO. Moreover, the presence of exchange bias in the NPs was linked\nto the formation of core-shell structure. Temperature dependent magnetization\nstudies showed an increase in N\\'eel temperature upon Ca substitution. XPS\nanalysis confirmed that Bi0.97Ca0.03FeO3 samples exhibited the highest oxygen\nvacancy concentration, while Fe3+ remained the dominant oxidation state across\nall compositions. Ferroelectric polarization loop measurements showed enhanced\nremanent polarization in doped samples, with leakage linked to oxygen vacancies\nand extrinsic microstructural effects.",
        "\\cite{desideri2012multiple} proposed a multi-gradient descent algorithm\n(MGDA) that can improve all objectives based on seeking the minim norm point in\nthe convex hull consisting of objectives function gradients as the common\ndescent direction, which has become the cornerstone of the multi-musk learning\nand multi-objective optimization. However, the logic to seek a common descent\ndirection through finding the minim-norm point in the gradient convex hull may\nfail under constraints, no matter whether taking the constraints into\nconsideration directly or projecting it into the feasible region after finding\nthe common descent direction with no constraints. Therefore, we proposed a\ntwo-stage search framework. In the first stage, a min-max search algorithm is\nimplemented to minimize the upper bound of the directional derivatives under\nconstraints, and the weak Pareto stationary can be theoretically reached in the\nfirst stage. In the second stage, the Pareto stationary can be theoretically\nobtained through the minimization of the lower bound of the directional\nderivatives under constraints. In numerical studies, we show the effectiveness\nof our proposed framework from the calibration of the multi-regime fundamental\ndiagram model and large-scale multi-objective portfolio problem.",
        "When illuminated by white light, atoms, molecules, and materials absorb only\ncertain characteristic energy contributions based on their absorption\nproperties. Here, we show that this effect can be translated from energy to\nspace: a spatially uniform laser pulse can create strongly localized carrier\nexcitations, including excitons, and spatial charge separation on the\nsub-nanometer scale within a few femtoseconds, opening new avenues for\nnanoelectronics and bringing petahertz switching within reach. Using\nnonequilibrium Green functions simulations we demonstrate this effect by\nexciting targeted areas of small graphene nanoribbon heterostructures by\ncareful choice of the laser energy and polarization."
      ]
    }
  }
]