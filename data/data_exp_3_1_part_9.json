[
  {
    "id":2411.06184,
    "research_type":"applied",
    "start_id":"b7",
    "start_title":"Taking the Human Out of the Loop: A Review of Bayesian Optimization",
    "start_abstract":"Big Data applications are typically associated with systems involving large numbers of users, massive complex software systems, and large-scale heterogeneous computing storage architectures. The construction such involves many distributed design choices. end products (e.g., recommendation medical analysis tools, real-time game engines, speech recognizers) thus involve tunable configuration parameters. These parameters often specified hard-coded into the by various developers or teams. If optimized jointly, these can result in significant improvements. Bayesian optimization is a powerful tool for joint choices that gaining great popularity recent years. It promises greater automation so as to increase both product quality human productivity. This review paper introduces optimization, highlights some its methodological aspects, showcases wide range applications.",
    "start_categories":[
      "math.OC"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "New Pathologic Classification of Lung Cancer: Relevance for Clinical Practice and Clinical Trials"
      ],
      "abstract":[
        "We summarize significant changes in pathologic classification of lung cancer resulting from the 2011 International Association for Study Lung Cancer\/American Thoracic Society\/European Respiratory Society (IASLC\/ATS\/ERS) adenocarcinoma classification. The was developed by an international core panel experts representing IASLC, ATS, and ERS with oncologists\/pulmonologists, pathologists, radiologists, molecular biologists, thoracic surgeons. Because 70% patients present advanced stages, a new approach to small biopsies cytology specific terminology criteria focused on need distinguishing squamous cell carcinoma testing EGFR mutations ALK rearrangement. Tumors previously classified as non-small-cell carcinoma, not otherwise specified, because lack clear or morphology should be further using limited immunohistochemical workup preserve tissue testing. terms \"bronchioloalveolar carcinoma\" \"mixed subtype adenocarcinoma\" have been discontinued. For resected adenocarcinomas, concepts situ minimally invasive define who, if they undergo complete resection, will 100% disease-free survival. Invasive adenocarcinomas are now predominant pattern after comprehensive histologic subtyping lepidic, acinar, papillary, solid patterns; micropapillary is added poor prognosis. Former mucinous bronchioloalveolar carcinomas called \"invasive adenocarcinoma.\" field rapidly evolving advances occurring frequent basis, particularly arena, this provides much needed standard diagnosis only patient care but also clinical trials TNM"
      ],
      "categories":[
        "q-bio.QM"
      ]
    },
    "list":{
      "title":[
        "Exact Learning of Permutations for Nonzero Binary Inputs with\n  Logarithmic Training Size and Quadratic Ensemble Complexity",
        "Entropy-Guided Attention for Private LLMs",
        "On the Uncertainty of a Simple Estimator for Remote Source Monitoring\n  over ALOHA Channels",
        "Unified Guidance for Geometry-Conditioned Molecular Generation",
        "In Situ Optimization of an Optoelectronic Reservoir Computer with\n  Digital Delayed Feedback",
        "Explaining 3D Computed Tomography Classifiers with Counterfactuals",
        "Detection of Rumors and Their Sources in Social Networks: A\n  Comprehensive Survey",
        "Anisotropic Exchange Spin Model to Investigate the Curie Temperature\n  Dispersion of Finite-Size L10-FePt Magnetic Nanoparticles",
        "PluralLLM: Pluralistic Alignment in LLMs via Federated Learning",
        "Fifteen Years of M31* X-ray Variability and Flares",
        "A Semantic-Loss Function Modeling Framework With Task-Oriented Machine\n  Learning Perspectives",
        "YOLO11-JDE: Fast and Accurate Multi-Object Tracking with Self-Supervised\n  Re-ID",
        "Robust Egoistic Rigid Body Localization",
        "Diminishing Waters: The Great Salt Lake's Desiccation and Its Mental\n  Health Consequences",
        "Code-Verification Techniques for an Arbitrary-Depth Electromagnetic Slot\n  Model",
        "H-MBR: Hypervisor-level Memory Bandwidth Reservation for Mixed\n  Criticality Systems",
        "From Trust to Truth: Actionable policies for the use of AI in\n  fact-checking in Germany and Ukraine",
        "The assembly of supermassive black holes at $z<1$ in early-type galaxies\n  from scaling relations",
        "Partially connected contributions to baryon masses in QCD+QED",
        "Reading the unreadable: Creating a dataset of 19th century English\n  newspapers using image-to-text language models",
        "Finite Sample Analysis of System Poles for Ho-Kalman Algorithm",
        "Does Generation Require Memorization? Creative Diffusion Models using\n  Ambient Diffusion",
        "Incorporating Cyclic Group Equivariance into Deep Learning for Reliable\n  Reconstruction of Rotationally Symmetric Tomography Systems",
        "The Fertile Steppe: Computability Logic and the decidability of one of\n  its fragments",
        "HuGDiffusion: Generalizable Single-Image Human Rendering via 3D Gaussian\n  Diffusion",
        "Mol-LLM: Generalist Molecular LLM with Improved Graph Utilization",
        "Policy Learning with a Natural Language Action Space: A Causal Approach",
        "Parabolic dielectric reflector for extreme on-chip spot-size conversion\n  with broad bandwidth",
        "Maximum force conjecture in curved spacetimes of stable self-gravitating\n  matter configurations"
      ],
      "abstract":[
        "The ability of an architecture to realize permutations is quite fundamental.\nFor example, Large Language Models need to be able to correctly copy (and\nperhaps rearrange) parts of the input prompt into the output. Classical\nuniversal approximation theorems guarantee the existence of parameter\nconfigurations that solve this task but offer no insights into whether\ngradient-based algorithms can find them. In this paper, we address this gap by\nfocusing on two-layer fully connected feed-forward neural networks and the task\nof learning permutations on nonzero binary inputs. We show that in the infinite\nwidth Neural Tangent Kernel (NTK) regime, an ensemble of such networks\nindependently trained with gradient descent on only the $k$ standard basis\nvectors out of $2^k - 1$ possible inputs successfully learns any fixed\npermutation of length $k$ with arbitrarily high probability. By analyzing the\nexact training dynamics, we prove that the network's output converges to a\nGaussian process whose mean captures the ground truth permutation via\nsign-based features. We then demonstrate how averaging these runs (an\n\"ensemble\" method) and applying a simple rounding step yields an arbitrarily\naccurate prediction on any possible input unseen during training. Notably, the\nnumber of models needed to achieve exact learning with high probability (which\nwe refer to as ensemble complexity) exhibits a linearithmic dependence on the\ninput size $k$ for a single test input and a quadratic dependence when\nconsidering all test inputs simultaneously.",
        "The pervasiveness of proprietary language models has raised critical privacy\nconcerns, necessitating advancements in private inference (PI), where\ncomputations are performed directly on encrypted data without revealing users'\nsensitive information. While PI offers a promising solution, its practical\ndeployment is hindered by substantial communication and latency overheads,\nprimarily stemming from nonlinear operations. To address this, we introduce an\ninformation-theoretic framework to characterize the role of nonlinearities in\ndecoder-only language models, laying a principled foundation for optimizing\ntransformer-architectures tailored to the demands of PI.\n  By leveraging Shannon's entropy as a quantitative measure, we uncover the\npreviously unexplored dual significance of nonlinearities: beyond ensuring\ntraining stability, they are crucial for maintaining attention head diversity.\nSpecifically, we find that their removal triggers two critical failure modes:\n{\\em entropy collapse} in deeper layers that destabilizes training, and {\\em\nentropic overload} in earlier layers that leads to under-utilization of\nMulti-Head Attention's (MHA) representational capacity.\n  We propose an entropy-guided attention mechanism paired with a novel entropy\nregularization technique to mitigate entropic overload. Additionally, we\nexplore PI-friendly alternatives to layer normalization for preventing entropy\ncollapse and stabilizing the training of LLMs with reduced-nonlinearities. Our\nstudy bridges the gap between information theory and architectural design,\nestablishing entropy dynamics as a principled guide for developing efficient PI\narchitectures. The code and implementation are available at\nhttps:\/\/github.com\/Nandan91\/entropy-guided-attention-llm",
        "Efficient remote monitoring of distributed sources is essential for many\nInternet of Things (IoT) applications. This work studies the uncertainty at the\nreceiver when tracking two-state Markov sources over a slotted random access\nchannel without feedback, using the conditional entropy as a performance\nindicator, and considering the last received value as current state estimate.\nWe provide an analytical characterization of the metric, and evaluate three\naccess strategies: (i) maximizing throughput, (ii) transmitting only on state\nchanges, and (iii) minimizing uncertainty through optimized access\nprobabilities. Our results reveal that throughput optimization does not always\nreduce uncertainty. Moreover, while reactive policies are optimal for symmetric\nsources, asymmetric processes benefit from mixed strategies allowing\ntransmissions during state persistence.",
        "Effectively designing molecular geometries is essential to advancing\npharmaceutical innovations, a domain, which has experienced great attention\nthrough the success of generative models and, in particular, diffusion models.\nHowever, current molecular diffusion models are tailored towards a specific\ndownstream task and lack adaptability. We introduce UniGuide, a framework for\ncontrolled geometric guidance of unconditional diffusion models that allows\nflexible conditioning during inference without the requirement of extra\ntraining or networks. We show how applications such as structure-based,\nfragment-based, and ligand-based drug design are formulated in the UniGuide\nframework and demonstrate on-par or superior performance compared to\nspecialised models. Offering a more versatile approach, UniGuide has the\npotential to streamline the development of molecular generative models,\nallowing them to be readily used in diverse application scenarios.",
        "Reservoir computing (RC) is an innovative paradigm in neuromorphic computing\nthat leverages fixed, randomized, internal connections to address the challenge\nof overfitting. RC has shown remarkable effectiveness in signal processing and\npattern recognition tasks, making it well-suited for hardware implementations\nacross various physical substrates, which promise enhanced computation speeds\nand reduced energy consumption. However, achieving optimal performance in RC\nsystems requires effective parameter optimization. Traditionally, this\noptimization has relied on software modeling, limiting the practicality of\nphysical computing approaches. Here, we report an \\emph{in situ} optimization\nmethod for an optoelectronic delay-based RC system with digital delayed\nfeedback. By simultaneously optimizing five parameters, normalized mean squared\nerror (NMSE) of 0.028, 0.561, and 0.271 is achieved in three benchmark tasks:\nwaveform classification, time series prediction, and speech recognition\noutperforming simulation-based optimization (NMSE 0.054, 0.543, and 0.329,\nrespectively) in the two of the three tasks. This method marks a significant\nadvancement in physical computing, facilitating the optimization of RC and\nneuromorphic systems without the need for simulation, thus enhancing their\npractical applicability.",
        "Counterfactual explanations in medical imaging are critical for understanding\nthe predictions made by deep learning models. We extend the Latent Shift\ncounterfactual generation method from 2D applications to 3D computed tomography\n(CT) scans. We address the challenges associated with 3D data, such as limited\ntraining samples and high memory demands, by implementing a slice-based\napproach. This method leverages a 2D encoder trained on CT slices, which are\nsubsequently combined to maintain 3D context. We demonstrate this technique on\ntwo models for clinical phenotype prediction and lung segmentation. Our\napproach is both memory-efficient and effective for generating interpretable\ncounterfactuals in high-resolution 3D medical imaging.",
        "With the recent advancements in social network platform technology, an\noverwhelming amount of information is spreading rapidly. In this situation, it\ncan become increasingly difficult to discern what information is false or true.\nIf false information proliferates significantly, it can lead to undesirable\noutcomes. Hence, when we receive some information, we can pose the following\ntwo questions: $(i)$ Is the information true? $(ii)$ If not, who initially\nspread that information? % The first problem is the rumor detection issue,\nwhile the second is the rumor source detection problem. A rumor-detection\nproblem involves identifying and mitigating false or misleading information\nspread via various communication channels, particularly online platforms and\nsocial media. Rumors can range from harmless ones to deliberately misleading\ncontent aimed at deceiving or manipulating audiences. Detecting misinformation\nis crucial for maintaining the integrity of information ecosystems and\npreventing harmful effects such as the spread of false beliefs, polarization,\nand even societal harm. Therefore, it is very important to quickly distinguish\nsuch misinformation while simultaneously finding its source to block it from\nspreading on the network. However, most of the existing surveys have analyzed\nthese two issues separately. In this work, we first survey the existing\nresearch on the rumor-detection and rumor source detection problems with joint\ndetection approaches, simultaneously. % This survey deals with these two issues\ntogether so that their relationship can be observed and it provides how the two\nproblems are similar and different. The limitations arising from the rumor\ndetection, rumor source detection, and their combination problems are also\nexplained, and some challenges to be addressed in future works are presented.",
        "We developed an anisotropic spin model that accounts for magnetic anisotropy\nand evaluated the Curie temperature (Tc) dispersion due to finite size effects\nin L10-FePt nanoparticles. In heat-assisted magnetic recording (HAMR) media, a\nnext-generation magnetic recording technology, high-density recording is\nachieved by locally heating L10-FePt nanoparticles near their Tc and rapidly\ncooling them. However, variations in Tc caused by differences in particle size\nand shape can compromise recording stability and areal density capacity, making\nthe control of Tc dispersion critical. In this study, we constructed atomistic\nLLG models to explicitly incorporate the spin exchange anisotropy of L10-FePt,\nbased on parameters determined by first-principles calculations. Using this\nmodel, we evaluated the impact of particle size on Tc dispersion. As a result,\n(1) the Tc dispersion critical to the performance of HAMR can be reproduced,\nwhereas it was previously underestimated by isotropic models and (2)\napproximately 70% of the experimentally observed Tc dispersion can be\nattributed to particle size effects. This research highlights the role of\nexchange anisotropy in amplifying finite-size effects and underscores the\nimportance of size control in HAMR media.",
        "Ensuring Large Language Models (LLMs) align with diverse human preferences\nwhile preserving privacy and fairness remains a challenge. Existing methods,\nsuch as Reinforcement Learning from Human Feedback (RLHF), rely on centralized\ndata collection, making them computationally expensive and privacy-invasive. We\nintroduce PluralLLM a federated learning-based approach that enables multiple\nuser groups to collaboratively train a transformer-based preference predictor\nwithout sharing sensitive data, which can also serve as a reward model for\naligning LLMs. Our method leverages Federated Averaging (FedAvg) to aggregate\npreference updates efficiently, achieving 46% faster convergence, a 4%\nimprovement in alignment scores, and nearly the same group fairness measure as\nin centralized training. Evaluated on a Q\/A preference alignment task,\nPluralLLM demonstrates that federated preference learning offers a scalable and\nprivacy-preserving alternative for aligning LLMs with diverse human values.",
        "We append an additional fifteen years (2009-2024) to the Chandra X-ray light\ncurve of M31*, the supermassive black hole at the center of M31, the Andromeda\ngalaxy. Extending and expanding on the work in Li et al. 2011, we show that\nM31* has remained in an elevated X-ray state from 2006 through at least 2016\n(when regular Chandra monitoring ceased) and likely through 2024, with the most\nrecent observations still showing an elevated X-ray flux. We identify one\nmoderate flare in 2013 where the other nuclear X-ray sources are in low-flux\nstates, making that flare a valuable target for followup with multiwavelength\nand multimessenger archival data. We extract a mostly uncontaminated spectrum\nfor M31* from this observation, showing that its X-ray properties are similar\nto those observed at Sgr A* in its quiescent state by Baganoff et al. 2003.\nFurthermore, we find no substantial change in the source's hardness ratio in\nthe 2006 and 2013 flares compared to the post-2006 elevated state, suggesting\nthe these flares are increases in the regular X-ray emission mechanisms instead\nof entirely new emission modes. Our extended light curve for M31* provides\nvaluable context for multimessenger or multiwavelength observations of nearby\nsupermassive black holes.",
        "The integration of machine learning (ML) has significantly enhanced the\ncapabilities of Earth Observation (EO) systems by enabling the extraction of\nactionable insights from complex datasets. However, the performance of\ndata-driven EO applications is heavily influenced by the data collection and\ntransmission processes, where limited satellite bandwidth and latency\nconstraints can hinder the full transmission of original data to the receivers.\nTo address this issue, adopting the concepts of Semantic Communication (SC)\noffers a promising solution by prioritizing the transmission of essential data\nsemantics over raw information. Implementing SC for EO systems requires a\nthorough understanding of the impact of data processing and communication\nchannel conditions on semantic loss at the processing center. This work\nproposes a novel data-fitting framework to empirically model the semantic loss\nusing real-world EO datasets and domain-specific insights. The framework\nquantifies two primary types of semantic loss: (1) source coding loss, assessed\nvia a data quality indicator measuring the impact of processing on raw source\ndata, and (2) transmission loss, evaluated by comparing practical transmission\nperformance against the Shannon limit. Semantic losses are estimated by\nevaluating the accuracy of EO applications using four task-oriented ML models,\nEfficientViT, MobileViT, ResNet50-DINO, and ResNet8-KD, on lossy image datasets\nunder varying channel conditions and compression ratios. These results underpin\na framework for efficient semantic-loss modeling in bandwidth-constrained EO\nscenarios, enabling more reliable and effective operations.",
        "We introduce YOLO11-JDE, a fast and accurate multi-object tracking (MOT)\nsolution that combines real-time object detection with self-supervised\nRe-Identification (Re-ID). By incorporating a dedicated Re-ID branch into\nYOLO11s, our model performs Joint Detection and Embedding (JDE), generating\nappearance features for each detection. The Re-ID branch is trained in a fully\nself-supervised setting while simultaneously training for detection,\neliminating the need for costly identity-labeled datasets. The triplet loss,\nwith hard positive and semi-hard negative mining strategies, is used for\nlearning discriminative embeddings. Data association is enhanced with a custom\ntracking implementation that successfully integrates motion, appearance, and\nlocation cues. YOLO11-JDE achieves competitive results on MOT17 and MOT20\nbenchmarks, surpassing existing JDE methods in terms of FPS and using up to ten\ntimes fewer parameters. Thus, making our method a highly attractive solution\nfor real-world applications.",
        "We consider a robust and self-reliant (or \"egoistic\") variation of the rigid\nbody localization (RBL) problem, in which a primary rigid body seeks to\nestimate the pose (i.e., location and orientation) of another rigid body (or\n\"target\"), relative to its own, without the assistance of external\ninfrastructure, without prior knowledge of the shape of the target, and taking\ninto account the possibility that the available observations are incomplete.\nThree complementary contributions are then offered for such a scenario. The\nfirst is a method to estimate the translation vector between the center point\nof both rigid bodies, which unlike existing techniques does not require that\nboth objects have the same shape or even the same number of landmark points.\nThis technique is shown to significantly outperform the state-of-the-art (SotA)\nunder complete information, but to be sensitive to data erasures, even when\nenhanced by matrix completion methods. The second contribution, designed to\noffer improved performance in the presence of incomplete information, offers a\nrobust alternative to the latter, at the expense of a slight relative loss\nunder complete information. Finally, the third contribution is a scheme for the\nestimation of the rotation matrix describing the relative orientation of the\ntarget rigid body with respect to the primary. Comparisons of the proposed\nschemes and SotA techniques demonstrate the advantage of the contributed\nmethods in terms of root mean square error (RMSE) performance under fully\ncomplete information and incomplete conditions.",
        "This study examines how the desiccation of Utah Great Salt Lake GSL,\nexacerbated by anthropogenic changes, poses significant health risks,\nparticularly communities mental health. Reduced water inflow has exposed the\nlakebed, increasing airborne particulate matter PM2.5 and dust storms, which\nimpact air quality. By integrating diverse datasets spanning from 1980 to\npresent including insitu measurements, satellite imagery, and reanalysis\nproducts this study synthesizes hydrological, atmospheric, and epidemiological\nvariables to comprehensively track the extent of the GSL surface water, local\nair quality fluctuations, and their effects on community mental health. The\nfindings indicate a clear relationship between higher pollution days and more\nsevere depressive symptoms. Specifically, individuals exposed to 22 days with\nPM2.5 levels above the World Health Organizations 24 hour guideline of 15 ug\nper m3 were more likely to experience severe depressive symptoms. Our results\nalso suggest that people experiencing more severe depression not only face a\nhigher number of high pollution days but also encounter such days more\nfrequently. The study highlights the interconnectedness of poor air quality,\nenvironmental degradation and mental health emphasizing the need for more\nsustainable economic growth in the region.",
        "Electromagnetic penetration through openings in an otherwise closed\nelectromagnetic scatterer is an important topic in computational\nelectromagnetics. To efficiently model this phenomenon, aperture or slot models\nare often used in conjunction with surface integral equations to solve\nMaxwell's equations. To establish the credibility of these models, code\nverification is necessary to assess the correctness of the implementation of\nthe underlying numerical methods. However, many characteristics of surface\nintegral equations and slot models render traditional code-verification\napproaches ineffective. In this paper, we present approaches to separately\nmeasure the different sources of numerical error arising from the\nmethod-of-moments implementation of the electric-field integral equation with\nan arbitrary-depth slot model. We demonstrate the effectiveness of these\napproaches for several cases.",
        "Recent advancements in fields such as automotive and aerospace have driven a\ngrowing demand for robust computational resources. Applications that were once\ndesigned for basic MCUs are now deployed on highly heterogeneous SoC platforms.\nWhile these platforms deliver the necessary computational performance, they\nalso present challenges related to resource sharing and predictability. These\nchallenges are particularly pronounced when consolidating safety and\nnon-safety-critical systems, the so-called Mixed-Criticality Systems (MCS) to\nadhere to strict SWaP-C requirements. MCS consolidation on shared platforms\nrequires stringent spatial and temporal isolation to comply with functional\nsafety standards. Virtualization, mainly leveraged by hypervisors, is a key\ntechnology that ensures spatial isolation across multiple OSes and\napplications; however, ensuring temporal isolation remains challenging due to\ncontention on shared hardwar resources, which impacts real-time performance and\npredictability. To mitigate this problem, several strategies as cache coloring\nand memory bandwidth reservation have been proposed. Although cache coloring is\ntypically implemented on state-of-the-art hypervisors, memory bandwidth\nreservation approaches are commonly implemented at the Linux kernel level or\nrely on dedicated hardware and typically do not consider the concept of VMs\nthat can run different OSes. To fill the gap between current memory bandwidth\nreservation solutions and the deployment of MCSs that operate on a hypervisor,\nthis work introduces H-MBR, an open-source VM-centric memory bandwidth\nreservation mechanism. H-MBR features (i) VM-centric bandwidth reservation,\n(ii) OS and platform agnosticism, and (iii) reduced overhead. Empirical results\nevidenced no overhead on non-regulated workloads, and negligible overhead (<1%)\nfor regulated workloads for regulation periods of 2 us or higher.",
        "The rise of Artificial Intelligence (AI) presents unprecedented opportunities\nand challenges for journalism, fact-checking and media regulation. While AI\noffers tools to combat disinformation and enhance media practices, its\nunregulated use and associated risks necessitate clear policies and\ncollaborative efforts. This policy paper explores the implications of\nartificial intelligence (AI) for journalism and fact-checking, with a focus on\naddressing disinformation and fostering responsible AI integration. Using\nGermany and Ukraine as key case studies, it identifies the challenges posed by\ndisinformation, proposes regulatory and funding strategies, and outlines\ntechnical standards to enhance AI adoption in media. The paper offers\nactionable recommendations to ensure AI's responsible and effective integration\ninto media ecosystems. AI presents significant opportunities to combat\ndisinformation and enhance journalistic practices. However, its implementation\nlacks cohesive regulation, leading to risks such as bias, transparency issues,\nand over-reliance on automated systems. In Ukraine, establishing an independent\nmedia regulatory framework adapted to its governance is crucial, while Germany\ncan act as a leader in advancing EU-wide collaborations and standards.\nTogether, these efforts can shape a robust AI-driven media ecosystem that\npromotes accuracy and trust.",
        "The assembly of supermassive black hole (SMBH) mass ($M_{\\bullet}$) and\nstellar mass ($M_{*}$) in galaxies can be studied via the redshift evolution of\nthe $M_{\\bullet}-M_{*}$ relation, but the ways in which selection bias and\nphysical assembly channels affect this evolution are uncertain. To address\nthis, we compare the $M_{\\bullet}-M_{*}$ relation for local massive\n($M_{*}>10^{10.5}$M$_{\\odot}$) quiescent early-type galaxies (ETGs) to that for\nmassive ETGs hosting active galactic nuclei (AGN) at $z\\sim0.8$. The\nrestrictions on stellar mass and galaxy type limit the assembly channels that\nmay connect the two relations. For the local sample we find $\\log(M_{\\bullet})\n= 8.80 + 1.10(\\log{M_{*}-11})$, in line with prior work. For the $z\\sim0.8$\nsample we find a bias-corrected relation: $\\log(M_{\\bullet}) = 7.80 +\n1.25(\\log{M_{*}-11})$. We show, however, that this relation depends on the\nstellar and SMBH mass functions used to compute the selection bias, the virial\nrelation, the virial factor, and the active fraction, which together introduce\nuncertainty of up to $\\sim0.6$\\,dex in the $z\\sim0.8$ relation. Adopting\nreasonable choices of these parameters then our $z\\sim0.8$ relation lies above\nthat for $z\\sim0$ AGN by $\\sim0.5$\\,dex, but below our $z\\sim0$ ETG relation by\n$0.4-1$\\,dex in SMBH mass. We discuss possible sources of this offset,\nincluding further bias corrections, `downsizing\" in SMBH mass assembly, and\npreferential SMBH growth. Our results highlight the need to reduce\nuncertainties from selection and measurement bias in SMBH and stellar masses at\nall redshifts.",
        "Full QCD+QED simulations allow to evaluate isospin breaking corrections to\nhadron masses. With the openQxD code, we are able to perform these simulations\nemploying C-periodic boundary conditions, implemented through a doubling of the\nphysical lattice along one spatial direction. The use of these boundary\nconditions introduces non-zero Wick contractions between two quark or two\nantiquark fields, that, in the case of the computation of baryon masses, lead\nto partially connected additional contributions that we expect to vanish in the\ninfinite volume limit. These contributions are challenging because they involve\nan all-to-all propagator connecting one point in the physical lattice and one\nin the mirror lattice. We present a way to compute these corrections to the\n$\\Omega^-$ baryon mass using a combination of point and stochastic source\ninversions. This work is part of the program of the RC* collaboration.",
        "Oscar Wilde said, \"The difference between literature and journalism is that\njournalism is unreadable, and literature is not read.\" Unfortunately, The\ndigitally archived journalism of Oscar Wilde's 19th century often has no or\npoor quality Optical Character Recognition (OCR), reducing the accessibility of\nthese archives and making them unreadable both figuratively and literally. This\npaper helps address the issue by performing OCR on \"The Nineteenth Century\nSerials Edition\" (NCSE), an 84k-page collection of 19th-century English\nnewspapers and periodicals, using Pixtral 12B, a pre-trained image-to-text\nlanguage model. The OCR capability of Pixtral was compared to 4 other OCR\napproaches, achieving a median character error rate of 1%, 5x lower than the\nnext best model. The resulting NCSE v2.0 dataset features improved article\nidentification, high-quality OCR, and text classified into four types and\nseventeen topics. The dataset contains 1.4 million entries, and 321 million\nwords. Example use cases demonstrate analysis of topic similarity, readability,\nand event tracking. NCSE v2.0 is freely available to encourage historical and\nsociological research. As a result, 21st-century readers can now share Oscar\nWilde's disappointment with 19th-century journalistic standards, reading the\nunreadable from the comfort of their own computers.",
        "This paper investigates the error analysis of system pole estimation in\n$n$-dimensional discrete-time Linear Time-Invariant systems with $m$ outputs\nand $p$ inputs, using the classical Ho-Kalman algorithm based on finite\ninput-output sample data. Building upon prior work, we establish end-to-end\nestimation guarantees for system poles under both single-trajectory and\nmultiple-trajectory settings. Specifically, we prove that, with high\nprobability, the estimation error of system poles decreases at a rate of at\nleast $\\mathcal{O}\\{T^{-\\frac{1}{2n}}\\}$ in the single-trajectory case and\n$\\mathcal{O}\\{N^{-\\frac{1}{2n}}\\}$ in the multiple-trajectory case, where $T$\nis the length of a single trajectory, and $N$ is the number of trajectories.\nFurthermore, we reveal that in both settings, achieving a constant estimation\naccuracy for system poles requires the sample size to grow super-polynomially\nwith respect to the larger of the two ratios, $ \\max\\{n\/m, n\/p\\} $. Numerical\nexperiments are conducted to validate the non-asymptotic results of system pole\nestimation.",
        "There is strong empirical evidence that the state-of-the-art diffusion\nmodeling paradigm leads to models that memorize the training set, especially\nwhen the training set is small. Prior methods to mitigate the memorization\nproblem often lead to a decrease in image quality. Is it possible to obtain\nstrong and creative generative models, i.e., models that achieve high\ngeneration quality and low memorization? Despite the current pessimistic\nlandscape of results, we make significant progress in pushing the trade-off\nbetween fidelity and memorization. We first provide theoretical evidence that\nmemorization in diffusion models is only necessary for denoising problems at\nlow noise scales (usually used in generating high-frequency details). Using\nthis theoretical insight, we propose a simple, principled method to train the\ndiffusion models using noisy data at large noise scales. We show that our\nmethod significantly reduces memorization without decreasing the image quality,\nfor both text-conditional and unconditional models and for a variety of data\navailability settings.",
        "Rotational symmetry is a defining feature of many tomography systems,\nincluding computed tomography (CT) and emission computed tomography (ECT),\nwhere detectors are arranged in a circular or periodically rotating\nconfiguration. This study revisits the image reconstruction process from the\nperspective of hardware-induced rotational symmetry and introduces a cyclic\ngroup equivariance framework for deep learning-based reconstruction.\nSpecifically, we derive a mathematical correspondence that couples cyclic\nrotations in the projection domain to discrete rotations in the image domain,\nboth arising from the same cyclic group inherent in the hardware design. This\ninsight also reveals the uniformly distributed circular structure of the\nprojection space. Building on this principle, we provide a cyclic rotation\nequivariant convolution design method to preserve projection domain symmetry\nand a cyclic group equivariance regularization approach that enforces\nconsistent rotational transformations across the entire network. We further\nintegrate these modules into a domain transform reconstruction framework and\nvalidate them using digital brain phantoms, training on discrete models and\ntesting on more complex and realistic fuzzy variants. Results indicate markedly\nimproved generalization and stability, with fewer artifacts and better detail\npreservation, especially under data distribution deviation. These findings\nhighlight the potential of cyclic group equivariance as a unifying principle\nfor tomographic reconstruction in rotationally symmetric systems, offering a\nflexible and interpretable solution for scenarios with limited data.",
        "The present work is devoted to Computability Logic (CoL), the young and\nvolcanic research-project developed by Giorgi Japaridze. Our main goal is to\nprovide the reader with a clear panoramic view of this vast new land, starting\nfrom its core knots and making our way towards the outer threads, in a somewhat\nthree-dimensional, spacial gait. Furthermore, through the present work, we\nprovide a tentative proof for the decidability of one of CoL's numerous\naxiomatisations, namely CL15. Thus, our expedition initially takes off for an\naerial, perusal overview of this fertile steppe. The first chapter introduces\nCoL in a philosophical fashion, exposing and arguing its main key points. We\nthen move over to unfold its semantics and syntax profiles, allowing the reader\nto become increasingly more familiar with this new environment. Landing on to\nthe second chapter, we thoroughly introduce Cirquent Calculus, the new\ndeductive system Japaridze has developed in order to axiomatise Computability\nLogic. Indeed, this new proof-system can also be a useful tool for many other\nlogics. We then review each of the 17 axiomatisations found so far. The third\nchapter zooms-in on CL15, in order to come up with a possible solution to its\nopen problem. We outline its soundness and completeness proofs; then provide\nsome few deductive examples; and, finally, build a tentative proof of its\ndecidability. Lastly, the fourth chapter focuses on the potential and actual\napplications of Computability Logic, both in arithmetic (clarithmetic) and in\nArtificial Intelligence systems (meaning knowledgebase and planning-and-action\nones). We close our journey with some final remarks on the richness of this\nframework and, hence, the research-worthiness it entails.",
        "We present HuGDiffusion, a generalizable 3D Gaussian splatting (3DGS)\nlearning pipeline to achieve novel view synthesis (NVS) of human characters\nfrom single-view input images. Existing approaches typically require monocular\nvideos or calibrated multi-view images as inputs, whose applicability could be\nweakened in real-world scenarios with arbitrary and\/or unknown camera poses. In\nthis paper, we aim to generate the set of 3DGS attributes via a diffusion-based\nframework conditioned on human priors extracted from a single image.\nSpecifically, we begin with carefully integrated human-centric feature\nextraction procedures to deduce informative conditioning signals. Based on our\nempirical observations that jointly learning the whole 3DGS attributes is\nchallenging to optimize, we design a multi-stage generation strategy to obtain\ndifferent types of 3DGS attributes. To facilitate the training process, we\ninvestigate constructing proxy ground-truth 3D Gaussian attributes as\nhigh-quality attribute-level supervision signals. Through extensive\nexperiments, our HuGDiffusion shows significant performance improvements over\nthe state-of-the-art methods. Our code will be made publicly available.",
        "Recent advances in Large Language Models (LLMs) have motivated the\ndevelopment of general LLMs for molecular tasks. While several studies have\ndemonstrated that fine-tuned LLMs can achieve impressive benchmark\nperformances, they are far from genuine generalist molecular LLMs due to a lack\nof fundamental understanding of molecular structure. Specifically, when given\nmolecular task instructions, LLMs trained with naive next-token prediction\ntraining assign similar likelihood scores to both original and negatively\ncorrupted molecules, revealing their lack of molecular structure understanding\nthat is crucial for reliable and general molecular LLMs. To overcome this\nlimitation and obtain a true generalist molecular LLM, we introduce a novel\nmulti-modal training method based on a thorough multi-modal instruction tuning\nas well as a molecular structure preference optimization between chosen and\nrejected graphs. On various molecular benchmarks, the proposed generalist\nmolecular LLM, called Mol-LLM, achieves state-of-the-art performances among\ngeneralist LLMs on most tasks, at the same time, surpassing or comparable to\nstate-of-the-art specialist LLMs. Moreover, Mol-LLM also shows superior\ngeneralization performances in reaction prediction tasks, demonstrating the\neffect of the molecular structure understanding for generalization perspective.",
        "This paper introduces a novel causal framework for multi-stage\ndecision-making in natural language action spaces where outcomes are only\nobserved after a sequence of actions. While recent approaches like Proximal\nPolicy Optimization (PPO) can handle such delayed-reward settings in\nhigh-dimensional action spaces, they typically require multiple models (policy,\nvalue, and reward) and substantial training data. Our approach employs\nQ-learning to estimate Dynamic Treatment Regimes (DTR) through a single model,\nenabling data-efficient policy learning via gradient ascent on language\nembeddings. A key technical contribution of our approach is a decoding strategy\nthat translates optimized embeddings back into coherent natural language. We\nevaluate our approach on mental health intervention, hate speech countering,\nand sentiment transfer tasks, demonstrating significant improvements over\ncompetitive baselines across multiple metrics. Notably, our method achieves\nsuperior transfer strength while maintaining content preservation and fluency,\nas validated through human evaluation. Our work provides a practical foundation\nfor learning optimal policies in complex language tasks where training data is\nlimited.",
        "Spot-size converters are key for efficient coupling of light between\nwaveguides of different sizes. While adiabatic tapers are well suited for small\nsize differences, they become impractically long for expansion factors around\nx100 which are often required when coupling integrated waveguides and\nfree-space beams. Evanescent couplers and bragg deflectors can be used in this\nscenario, but their operation is inherently limited in bandwidth. Here we\npropose a solution based on a parabolic dielectric interface that couples light\nfrom a 0.5 um-wide waveguide to a 285 um-wide waveguide, i.e. an expansion\nfactor of x570. We experimentally demonstrate an unprecedented bandwidth of\nmore than 380 nm with insertion losses below 0.35 dB. We furthermore provide\nanalytical expressions for the design of such parabolic spot-size-converters\nfor arbitrary expansion factors.",
        "Gibbons and Schiller have raised the physically interesting conjecture that\nforces in general relativity are bounded from above by the mathematically\ncompact relation ${\\cal F}\\leq c^4\/4G$. In the present compact paper we\nexplicitly prove, using the non-linearly coupled Einstein-matter field\nequations, that the force function ${\\cal F}\\equiv 4\\pi r^2 p(r)$ in {\\it\nstable} self-gravitating horizonless matter configurations is characterized by\nthe upper bound ${\\cal F}\\leq c^4\/G$ [here $p(r)$ is the radial pressure inside\nthe self-gravitating matter configuration]."
      ]
    }
  },
  {
    "id":2411.06184,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"New Pathologic Classification of Lung Cancer: Relevance for Clinical Practice and Clinical Trials",
    "start_abstract":"We summarize significant changes in pathologic classification of lung cancer resulting from the 2011 International Association for Study Lung Cancer\/American Thoracic Society\/European Respiratory Society (IASLC\/ATS\/ERS) adenocarcinoma classification. The was developed by an international core panel experts representing IASLC, ATS, and ERS with oncologists\/pulmonologists, pathologists, radiologists, molecular biologists, thoracic surgeons. Because 70% patients present advanced stages, a new approach to small biopsies cytology specific terminology criteria focused on need distinguishing squamous cell carcinoma testing EGFR mutations ALK rearrangement. Tumors previously classified as non-small-cell carcinoma, not otherwise specified, because lack clear or morphology should be further using limited immunohistochemical workup preserve tissue testing. terms \"bronchioloalveolar carcinoma\" \"mixed subtype adenocarcinoma\" have been discontinued. For resected adenocarcinomas, concepts situ minimally invasive define who, if they undergo complete resection, will 100% disease-free survival. Invasive adenocarcinomas are now predominant pattern after comprehensive histologic subtyping lepidic, acinar, papillary, solid patterns; micropapillary is added poor prognosis. Former mucinous bronchioloalveolar carcinomas called \"invasive adenocarcinoma.\" field rapidly evolving advances occurring frequent basis, particularly arena, this provides much needed standard diagnosis only patient care but also clinical trials TNM",
    "start_categories":[
      "q-bio.QM"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b6",
        "b7"
      ],
      "title":[
        "Multi-Task Bayesian Optimization",
        "Taking the Human Out of the Loop: A Review of Bayesian Optimization"
      ],
      "abstract":[
        "Bayesian optimization has recently been proposed as a framework for automatically tuning the hyperparameters of machine learning models and shown to yield state-of-the-art performance with impressive ease efficiency. In this paper, we explore whether it is possible transfer knowledge gained from previous optimizations new tasks in order find optimal hyperparameter settings more efficiently. Our approach based on extending multi-task Gaussian processes optimization. We show that method significantly speeds up process when compared standard single-task approach. further propose straightforward extension our algorithm jointly minimize average error across multiple demonstrate how can be used greatly speed k-fold cross-validation. Lastly, an adaptation developed acquisition function, entropy search, cost-sensitive, setting. utility function by leveraging small dataset hyper-parameter large dataset. dynamically chooses which query most information per unit cost.",
        "Big Data applications are typically associated with systems involving large numbers of users, massive complex software systems, and large-scale heterogeneous computing storage architectures. The construction such involves many distributed design choices. end products (e.g., recommendation medical analysis tools, real-time game engines, speech recognizers) thus involve tunable configuration parameters. These parameters often specified hard-coded into the by various developers or teams. If optimized jointly, these can result in significant improvements. Bayesian optimization is a powerful tool for joint choices that gaining great popularity recent years. It promises greater automation so as to increase both product quality human productivity. This review paper introduces optimization, highlights some its methodological aspects, showcases wide range applications."
      ],
      "categories":[
        "math.OC"
      ]
    },
    "list":{
      "title":[
        "Human-AI Collaboration in Cloud Security: Cognitive Hierarchy-Driven\n  Deep Reinforcement Learning",
        "Fluctuations of non-local branching Markov processes",
        "Understanding In-Context Machine Translation for Low-Resource Languages:\n  A Case Study on Manchu",
        "Differential virial analysis: a new technique to determine the dynamical\n  state of molecular clouds",
        "Quantum geometry of non-Hermitian systems",
        "Gravity-induced collisions of uncharged cloud droplets in an electric\n  field",
        "Breaking the Clusters: Uniformity-Optimization for Text-Based Sequential\n  Recommendation",
        "Imprints of an early matter-dominated era arising from dark matter\n  dilution mechanism on cosmic string dynamics and gravitational wave\n  signatures",
        "GRAPHGPT-O: Synergistic Multimodal Comprehension and Generation on\n  Graphs",
        "A Simple yet Effective DDG Predictor is An Unsupervised Antibody\n  Optimizer and Explainer",
        "Ways of Seeing, and Selling, AI Art",
        "Project portfolio planning in the pharmaceutical industry -- strategic\n  objectives and quantitative optimization",
        "Transforming Science with Large Language Models: A Survey on AI-assisted\n  Scientific Discovery, Experimentation, Content Generation, and Evaluation",
        "A Possible Four-Month Periodicity in the Activity of FRB 20240209A",
        "Hybrid Near\/Far-Field Frequency-Dependent Beamforming via Joint\n  Phase-Time Arrays",
        "Exploring the Collaborative Co-Creation Process with AI: A Case Study in\n  Novice Music Production",
        "Besov and Triebel-Lizorkin spaces on homogeneous groups",
        "Evaluating open-source Large Language Models for automated fact-checking",
        "A general form of Newton-Maclaurin type inequalities",
        "Recent Progress in Studies of Cobalt-based Quasi-1-dimensional Quantum\n  Magnets",
        "A Natural Transformation between the Completeness and Compactness\n  Theorems in Classical Logic",
        "Quasi-Fuchsian flows and the coupled vortex equations",
        "Billiard trajectories inside Cones",
        "Harvesting primordial black holes from stochastic trees with\n  $\\texttt{FOREST}$",
        "First direct observation of a wakefield generated with structured light",
        "Beyond Human Intervention: Algorithmic Collusion through Multi-Agent\n  Learning Strategies",
        "Walkthrough of Anthropomorphic Features in AI Assistant Tools",
        "Flat degenerate metrics and Riemannian foliations"
      ],
      "abstract":[
        "Given the complexity of multi-tenant cloud environments and the need for\nreal-time threat mitigation, Security Operations Centers (SOCs) must integrate\nAI-driven adaptive defenses against Advanced Persistent Threats (APTs).\nHowever, SOC analysts struggle with countering adaptive adversarial tactics,\nnecessitating intelligent decision-support frameworks. To enhance human-AI\ncollaboration in SOCs, we propose a Cognitive Hierarchy Theory-driven Deep\nQ-Network (CHT-DQN) framework that models SOC analysts' decision-making against\nAI-driven APT bots. The SOC analyst (defender) operates at cognitive level-1,\nanticipating attacker strategies, while the APT bot (attacker) follows a\nlevel-0 exploitative policy. By incorporating CHT into DQN, our framework\nenhances SOC defense strategies via Attack Graph (AG)-based reinforcement\nlearning. Simulation experiments across varying AG complexities show that\nCHT-DQN achieves higher data protection and lower action discrepancies compared\nto standard DQN. A theoretical lower bound analysis further validates its\nsuperior Q-value performance. A human-in-the-loop (HITL) evaluation on Amazon\nMechanical Turk (MTurk) reveals that SOC analysts using CHT-DQN-driven\ntransition probabilities align better with adaptive attackers, improving data\nprotection. Additionally, human decision patterns exhibit risk aversion after\nfailure and risk-seeking behavior after success, aligning with Prospect Theory.\nThese findings underscore the potential of integrating cognitive modeling into\ndeep reinforcement learning to enhance SOC operations and develop real-time\nadaptive cloud security mechanisms.",
        "The aim of this paper is to study the fluctuations of a general class of\nsupercritical branching Markov processes with non-local branching mechanisms.\nWe show the existence of three regimes according to the size of the spectral\ngap associated with the expectation semigroup of the branching process and\nestablish functional central limit theorems within each regime.",
        "In-context machine translation (MT) with large language models (LLMs) is a\npromising approach for low-resource MT, as it can readily take advantage of\nlinguistic resources such as grammar books and dictionaries. Such resources are\nusually selectively integrated into the prompt so that LLMs can directly\nperform translation without any specific training, via their in-context\nlearning capability (ICL). However, the relative importance of each type of\nresource e.g., dictionary, grammar book, and retrieved parallel examples, is\nnot entirely clear. To address this gap, this study systematically investigates\nhow each resource and its quality affects the translation performance, with the\nManchu language as our case study. To remove any prior knowledge of Manchu\nencoded in the LLM parameters and single out the effect of ICL, we also\nexperiment with an encrypted version of Manchu texts. Our results indicate that\nhigh-quality dictionaries and good parallel examples are very helpful, while\ngrammars hardly help. In a follow-up study, we showcase a promising application\nof in-context MT: parallel data augmentation as a way to bootstrap the\nconventional MT model. When monolingual data abound, generating synthetic\nparallel data through in-context MT offers a pathway to mitigate data scarcity\nand build effective and efficient low-resource neural MT systems.",
        "Since molecular clouds form stars, at least some parts of them must be in a\nstate of collapse. However, there is a long-standing debate as to whether that\ncollapse is local, involving only a small fraction of the cloud mass, or\nglobal, with most mass in a state of collapse up to the moment when it is\ndispersed by stellar feedback. In principle it is possible to distinguish these\npossibilities from clouds' virial ratios, which should be a factor of two\nlarger for collapse than for equilibrium, but systematic uncertainties have\nthus far prevented such measurements. Here we propose a new analysis method to\novercome this limitation: while the absolute value of a cloud's virial ratio is\ntoo uncertain to distinguish global from local collapse, the differential\nchange in virial ratio as a function of surface density is also diagnostic of\nclouds' dynamical state, and can be measured with far fewer systematic\nuncertainties. We demonstrate the basic principles of the method using simple\nanalytic models of supported and collapsing clouds, validate it from full 3D\nsimulations, and discuss possible challenges in applying the method to real\ndata. We then provide a preliminary application of the technique to recent\nobservations of the molecular clouds in Andromeda, showing that most of them\nare inconsistent with being in a state of global collapse.",
        "The Berry curvature characterizes one aspect of the geometry of quantum\nstates. It materializes, among other consequences, as an anomalous velocity of\nwave packets. In non-Hermitian systems, wave packet dynamics is enriched by\nadditional terms that can be expressed as generalizations of the Berry\nconnection to non-orthogonal eigenstates. Here, we contextualize these\nanomalous non-Hermitian contributions by showing that they directly arise from\nthe geometry of the underlying quantum states as corrections to the distance\nbetween left and perturbed right eigenstates. By calculating the electric\nsusceptibility for a single-band wave packet and comparing it with the wave\npacket's localization, we demonstrate that these terms can, in some\ncircumstances, lead to a violation of fluctuation-dissipation relations in\nnon-Hermitian systems. We discuss experimental signatures in terms of response\nfunctions and transport signatures.",
        "We investigate the collisions of uncharged, conducting droplets settling\nunder gravity in the presence of an external electric field. Previous studies\nhave derived a near-field asymptotic expression for the electric-field-induced\nattraction, suggesting that this force can overcome lubrication resistance and\ndrive surface-to-surface contact between two spherical conductors within a\nfinite time. However, for droplets moving in air, traditional lubrication\ntheory breaks down when the inter-droplet gap approaches the mean free path of\nair molecules. To account for this, we incorporate non-continuum hydrodynamic\neffects to estimate the gravity-driven collision efficiency under\nelectric-field-induced forces. This study examines how an external electric\nfield influences the trajectories of settling droplet pairs of unequal sizes.\nBy analyzing their motion, we compute collision efficiencies and explore their\ndependence on droplet size ratio, electric field strength, the angle between\nthe field and gravity, and key dimensionless parameters governing\nelectric-field-induced and van der Waals forces. Our findings reveal that\nelectric-field-induced forces significantly enhance collision efficiency,\nhighlighting their critical role in droplet coalescence dynamics.",
        "Traditional sequential recommendation (SR) methods heavily rely on explicit\nitem IDs to capture user preferences over time. This reliance introduces\ncritical limitations in cold-start scenarios and domain transfer tasks, where\nunseen items and new contexts often lack established ID mappings. To overcome\nthese limitations, recent studies have shifted towards leveraging text-only\ninformation for recommendation, thereby improving model generalization and\nadaptability across domains. Although promising, text-based SR faces unique\ndifficulties: items' text descriptions often share semantic similarities that\nlead to clustered item representations, compromising their uniformity, a\nproperty essential for promoting diversity and enhancing generalization in\nrecommendation systems. In this paper, we explore a novel framework to improve\nthe uniformity of item representations in text-based SR. Our analysis reveals\nthat items within a sequence exhibit marked semantic similarity, meaning they\nare closer in representation than items overall, and that this effect is more\npronounced for less popular items, which form tighter clusters compared to\ntheir more popular counterparts. Based on these findings, we propose UniT, a\nframework that employs three pairwise item sampling strategies: Unified General\nSampling Strategy, Sequence-Driven Sampling Strategy, and Popularity-Driven\nSampling Strategy. Each strategy applies varying degrees of repulsion to\nselectively adjust the distances between item pairs, thereby refining\nrepresentation uniformity while considering both sequence context and item\npopularity. Extensive experiments on multiple real-world datasets demonstrate\nthat our proposed approach outperforms state-of-the-art models, validating the\neffectiveness of UniT in enhancing both representation uniformity and\nrecommendation accuracy.The source code is available at\nhttps:\/\/github.com\/ccwwhhh\/Model-Rec.",
        "We investigate the influence of an early matter-dominated era in cosmic\nhistory on the dynamics of cosmic strings and the resulting stochastic\ngravitational waves. Specifically, we examine the case where this era\noriginates from the dark matter dilution mechanism within the framework of the\nminimal left-right symmetric model. By numerically solving the Boltzmann\nequations governing the energy densities of the relevant components, we\nmeticulously analyze the modifications to the cosmological scale factor, the\nnumber density of cosmic string loops, and the gravitational wave spectrum. Our\nresults reveal that the early matter-dominated era causes a characteristic\nsuppression in the high-frequency regime of the gravitational wave spectrum,\nproviding distinct and testable signatures for future ground-based\ninterferometer experiments.",
        "The rapid development of Multimodal Large Language Models (MLLMs) has enabled\nthe integration of multiple modalities, including texts and images, within the\nlarge language model (LLM) framework. However, texts and images are usually\ninterconnected, forming a multimodal attributed graph (MMAG). It is\nunderexplored how MLLMs can incorporate the relational information\n(\\textit{i.e.}, graph structure) and semantic information (\\textit{i.e.,} texts\nand images) on such graphs for multimodal comprehension and generation. In this\npaper, we propose GraphGPT-o, which supports omni-multimodal understanding and\ncreation on MMAGs. We first comprehensively study linearization variants to\ntransform semantic and structural information as input for MLLMs. Then, we\npropose a hierarchical aligner that enables deep graph encoding, bridging the\ngap between MMAGs and MLLMs. Finally, we explore the inference choices,\nadapting MLLM to interleaved text and image generation in graph scenarios.\nExtensive experiments on three datasets from different domains demonstrate the\neffectiveness of our proposed method. Datasets and codes will be open-sourced\nupon acceptance.",
        "The proteins that exist today have been optimized over billions of years of\nnatural evolution, during which nature creates random mutations and selects\nthem. The discovery of functionally promising mutations is challenged by the\nlimited evolutionary accessible regions, i.e., only a small region on the\nfitness landscape is beneficial. There have been numerous priors used to\nconstrain protein evolution to regions of landscapes with high-fitness\nvariants, among which the change in binding free energy (DDG) of protein\ncomplexes upon mutations is one of the most commonly used priors. However, the\nhuge mutation space poses two challenges: (1) how to improve the efficiency of\nDDG prediction for fast mutation screening; and (2) how to explain mutation\npreferences and efficiently explore accessible evolutionary regions. To address\nthese challenges, we propose a lightweight DDG predictor (Light-DDG), which\nadopts a structure-aware Transformer as the backbone and enhances it by\nknowledge distilled from existing powerful but computationally heavy DDG\npredictors. Additionally, we augmented, annotated, and released a large-scale\ndataset containing millions of mutation data for pre-training Light-DDG. We\nfind that such a simple yet effective Light-DDG can serve as a good\nunsupervised antibody optimizer and explainer. For the target antibody, we\npropose a novel Mutation Explainer to learn mutation preferences, which\naccounts for the marginal benefit of each mutation per residue. To further\nexplore accessible evolutionary regions, we conduct preference-guided antibody\noptimization and evaluate antibody candidates quickly using Light-DDG to\nidentify desirable mutations.",
        "In early 2025, Augmented Intelligence - Christie's first AI art auction -\ndrew criticism for showcasing a controversial genre. Amid wider legal\nuncertainty, artists voiced concerns over data mining practices, notably with\nrespect to copyright. The backlash could be viewed as a microcosm of AI's\ncontested position in the creative economy. Touching on the auction's\npresentation, reception, and results, this paper explores how, among social\ndissonance, machine learning finds its place in the artworld. Foregrounding\nresponsible innovation, the paper provides a balanced perspective that\nchampions creators' rights and brings nuance to this polarised debate. With a\nfocus on exhibition design, it centres framing, which refers to the way a piece\nis presented to influence consumer perception. Context plays a central role in\nshaping our understanding of how good, valuable, and even ethical an artwork\nis. In this regard, Augmented Intelligence situates AI art within a\nsurprisingly traditional framework, leveraging hallmarks of \"high art\" to\nestablish the genre's cultural credibility. Generative AI has a clear economic\ndimension, converging questions of artistic merit with those of monetary worth.\nScholarship on ways of seeing, or framing, could substantively inform the\ninterpretation and evaluation of creative outputs, including assessments of\ntheir aesthetic and commercial value.",
        "Many pharmaceutical companies face concerns with the maintenance of desired\nrevenue levels. Sales forecasts for the current portfolio of products and\nprojects may indicate a decline in revenue as the marketed products approach\npatent expiry. To counteract the potential downturn in revenue, and to\nestablish revenue growth, an in-flow of new projects into the development\nphases is required. In this article, we devise an approach with which the\nin-flow of new projects could be optimized, while adhering to the objectives\nand constraints set on revenue targets, budget limitations and strategic\nconsiderations on the composition of the company's portfolio.",
        "With the advent of large multimodal language models, science is now at a\nthreshold of an AI-based technological transformation. Recently, a plethora of\nnew AI models and tools has been proposed, promising to empower researchers and\nacademics worldwide to conduct their research more effectively and efficiently.\nThis includes all aspects of the research cycle, especially (1) searching for\nrelevant literature; (2) generating research ideas and conducting\nexperimentation; generating (3) text-based and (4) multimodal content (e.g.,\nscientific figures and diagrams); and (5) AI-based automatic peer review. In\nthis survey, we provide an in-depth overview over these exciting recent\ndevelopments, which promise to fundamentally alter the scientific research\nprocess for good. Our survey covers the five aspects outlined above, indicating\nrelevant datasets, methods and results (including evaluation) as well as\nlimitations and scope for future research. Ethical concerns regarding\nshortcomings of these tools and potential for misuse (fake science, plagiarism,\nharms to research integrity) take a particularly prominent place in our\ndiscussion. We hope that our survey will not only become a reference guide for\nnewcomers to the field but also a catalyst for new AI-based initiatives in the\narea of \"AI4Science\".",
        "Fast Radio Bursts (FRBs) are millisecond-duration radio transients from\ndistant galaxies. While most FRBs are singular events, repeaters emit multiple\nbursts, with only two-FRB 121102 and FRB 180916B-showing periodic activity (160\nand 16 days, respectively). FRB 20240209A, discovered by CHIME-FRB, is\nlocalized to the outskirts of a quiescent elliptical galaxy (z = 0.1384). We\ndiscovered a periodicity of ~ 126 days in the activity of the FRB 20240209A,\npotentially adding to the list of extremely rare periodic repeating FRBs. We\nused auto-correlation and Lomb-Scargle periodogram analyses, validated with\nrandomized control samples, to confirm the periodicity. The FRB's location in\nan old stellar population disfavors young progenitor models, instead pointing\nto scenarios involving globular clusters, late-stage magnetars, or low-mass\nX-ray binaries (LMXBs). Though deep X-ray or polarimetric observations are not\navailable, the localization of the FRB and a possible periodicity points to\nprogenitors likely to be a binary involving a compact object and a stellar\ncompanion or a precessing or rotating old neutron star.",
        "Joint phase-time arrays (JPTA) emerge as a cost-effective and\nenergy-efficient architecture for frequency-dependent beamforming in wideband\ncommunications by utilizing both true-time delay units and phase shifters. This\npaper exploits the potential of JPTA to simultaneously serve multiple users in\nboth near- and far-field regions with a single radio frequency chain. The goal\nis to jointly optimize JPTA-based beamforming and subband allocation to\nmaximize overall system performance. To this end, we formulate a system utility\nmaximization problem, including sum-rate maximization and proportional fairness\nas special cases. We develop a 3-step alternating optimization (AO) algorithm\nand an efficient deep learning (DL) method for this problem. The DL approach\nincludes a 2-layer convolutional neural network, a 3-layer graph attention\nnetwork (GAT), and a normalization module for resource and beamforming\noptimization. The GAT efficiently captures the interactions between resource\nallocation and analog beamformers. Simulation results confirm that JPTA\noutperforms conventional phased arrays (PA) in enhancing user rate and strikes\na good balance between PA and fully-digital approach in energy efficiency.\nEmploying a logarithmic utility function for user rates ensures greater\nfairness than maximizing sum-rates. Furthermore, the DL network achieves\ncomparable performance to the AO approach, while having orders of magnitude\nlower computational complexity.",
        "Artificial intelligence is reshaping creative domains, yet its co-creative\nprocesses, especially in group settings with novice users, remain under\nexplored. To bridge this gap, we conducted a case study in a college-level\ncourse where nine undergraduate students were tasked with creating three\noriginal music tracks using AI tools over 10 weeks. The study spanned the\nentire creative journey from ideation to releasing these songs on Spotify.\nParticipants leveraged AI for music and lyric production, cover art, and\ndistribution. Our findings highlight how AI transforms creative workflows:\naccelerating ideation but compressing the traditional preparation stage, and\nrequiring novices to navigate a challenging idea selection and validation\nphase. We also identified a new \"collaging and refinement\" stage, where\nparticipants creatively combined diverse AI-generated outputs into cohesive\nworks. Furthermore, AI influenced group social dynamics and role division among\nhuman creators. Based on these insights, we propose the Human-AI Co-Creation\nStage Model and the Human-AI Agency Model, offering new perspectives on\ncollaborative co-creation with AI.",
        "This paper develops a theory of Besov spaces $\\dot{\\mathbf{B}}^{\\sigma}_{p,q}\n(N)$ and Triebel-Lizorkin spaces $\\dot{\\mathbf{F}}^{\\sigma}_{p,q} (N)$ on an\narbitrary homogeneous group $N$ for the full range of parameters $p, q \\in (0,\n\\infty]$ and $\\sigma \\in \\mathbb{R}$. Among others, it is shown that these\nspaces are independent of the choice of the Littlewood-Paley decomposition and\nthat they admit characterizations in terms of continuous maximal functions and\nmolecular frame decompositions. The defined spaces include as special cases\nvarious classical function spaces, such as Hardy spaces on homogeneous groups\nand homogeneous Sobolev spaces and Lipschitz spaces associated to\nsub-Laplacians on stratified groups.",
        "The increasing prevalence of online misinformation has heightened the demand\nfor automated fact-checking solutions. Large Language Models (LLMs) have\nemerged as potential tools for assisting in this task, but their effectiveness\nremains uncertain. This study evaluates the fact-checking capabilities of\nvarious open-source LLMs, focusing on their ability to assess claims with\ndifferent levels of contextual information. We conduct three key experiments:\n(1) evaluating whether LLMs can identify the semantic relationship between a\nclaim and a fact-checking article, (2) assessing models' accuracy in verifying\nclaims when given a related fact-checking article, and (3) testing LLMs'\nfact-checking abilities when leveraging data from external knowledge sources\nsuch as Google and Wikipedia. Our results indicate that LLMs perform well in\nidentifying claim-article connections and verifying fact-checked stories but\nstruggle with confirming factual news, where they are outperformed by\ntraditional fine-tuned models such as RoBERTa. Additionally, the introduction\nof external knowledge does not significantly enhance LLMs' performance, calling\nfor more tailored approaches. Our findings highlight both the potential and\nlimitations of LLMs in automated fact-checking, emphasizing the need for\nfurther refinements before they can reliably replace human fact-checkers.",
        "In this paper, we extend the classical Newton-Maclaurin inequalities to\nfunctions $S_{k;s}(x)=E_k(x)+\\dsum_{i=1}^s \\al_i E_{k-i}(x)$, which are formed\nby linear combinations of multiple basic symmetric mean. We proved that when\nthe coefficients $\\al_1,\\al_2,\\cdots,\\al_s$ satisfy the condition that the\npolynomial $$t^s+\\al_1 t^{s-1}+\\al_2 t^{s-2}+\\cdots+\\al_s $$ has only real\nroots, the Newton-Maclaurin type inequalities hold for $S_{k;s}(x)$.",
        "The interplay of crystal electric field, temperature, and spin-orbit coupling\ncan yield a Kramers ion and thus an effective S = 1\/2 ground state for Co2+\nions (3d7), which is often the case for low dimensional materials. This is\nbecause a highly anisotropic structural motif can force the spins to point\neither up or down, hence becoming a system where spins communicate via Ising\ninteractions. Cobalt-based quasi-1-dimensional materials have been studied in\nthis context since the latter half of the 20th century, but due to the\ndevelopment of modern characterization techniques and advances in sample\npreparation, the exotic physical phenomena that have generated the most\ninterest have only emerged in the most recent three to four decades. This\ntopical review mainly summarizes progress in cobalt-based quasi-1-dimensional\nquantum magnets, and comments on a few research directions of potential future\ninterest.",
        "In this paper, we present a categorical framework that reveals the deep\nrelationship between the completeness and compactness theorems in classical\nlogic. Although these theorems have traditionally been approached by distinct\nmethods, our study shows that they are naturally equivalent through the lens of\ncategory theory. By introducing the basic concepts of categories, functors, and\nnatural transformations, we establish that the model constructions derived from\neach theorem can be viewed as functors from the category of logical theories to\nthe category of their models. In particular, one functor is defined using the\nHenkin construction from the completeness theorem, while another is defined\nbased on the finite satisfiability condition of the compactness theorem. We\nthen prove the existence of a natural transformation between these two\nfunctors, which demonstrates that the models produced by each method are\nisomorphic in a canonical way. This result not only bridges the gap between the\nproof-theoretic and model-theoretic perspectives but also provides a unified,\nconceptual framework that can be extended to other areas of mathematical logic.\nOur exposition is designed to be accessible, offering insight into both the\ncategorical formalism and its implications for understanding fundamental\nlogical theorems.",
        "We provide an alternative construction of the quasi-Fuchsian flows introduced\nby Ghys in \\cite{Ghys-92}. Our approach is based on the coupled vortex\nequations that allows to see these flows as thermostats on the unit tangent\nbundle of the Blaschke metric uniquely determined by a conformal class and a\nholomorphic quadratic differential. We also give formulas for the marked length\nspectrum of a quasi-Fuchsian flow in the thermostat parametrization.",
        "Recently it was proved that every billiard trajectory inside a $C^3$ convex\ncone has a finite number of reflections.\n  Here, by a $C^3$ convex cone, we mean a cone whose section with some\nhyperplane is a strictly convex closed $C^3$ submanifold of the hyperplane with\nnondegenerate second fundamental form.\n  In this paper,\n  we prove the existence of $C^2$ convex cones admitting billiard trajectories\nwith infinitely many reflections in finite time.\n  We also estimate the number of reflections of billiard trajectories in\nelliptic cones in $\\mathbb{R}^3$\n  using two first integrals.",
        "We introduce a novel framework to implement stochastic inflation on\nstochastic trees, modelling the inflationary expansion as a branching process.\nCombined with the $\\delta N$ formalism, this allows us to generate real-space\nmaps of the curvature perturbation that fully capture quantum diffusion and its\nnon-perturbative backreaction during inflation. Unlike lattice methods, trees\ndo not proceed on a fixed background since new spacetime units emerge\ndynamically as trees unfold, naturally incorporating metric fluctuations. The\nrecursive structure of stochastic trees also offers remarkable numerical\nefficiency, and we develop the FOrtran Recursive Exploration of Stochastic\nTrees ($\\texttt{FOREST}$) tool and demonstrate its performance. We show how\nprimordial black holes blossom at unbalanced nodes of the trees, and how their\nmass distribution can be obtained while automatically accounting for the\n\"cloud-in-cloud\" effect. In the \"quantum-well\" toy model, we find broad mass\ndistributions, with mild power laws terminated by exponential tails. We finally\ncompare our results with existing approximations in the literature and discuss\nseveral prospects.",
        "The use of structured light to control the phase velocity of the wake in\nlaser-wakefield accelerators has generated significant interest for its ability\nto mitigate electron dephasing. Combining the diffraction-free properties of\nBessel beams with spatio-temporal shaping of the pulse promises to enable\nacceleration with an unprecedented combination of long acceleration lengths and\nhigh gradients. This would facilitate the acceleration of electrons to energies\nabove 100 GeV in existing laser facilities. In-depth understanding of the\nphysical mechanisms involved is critical to achieving dephasing-free electron\nacceleration. Here we present the first experimental observation of wakefields\ngenerated by beams that were spatio-temporally sculpted and then focused with a\nlong-focal-depth mirror, known as an axiparabola, which generates a\nquasi-Bessel beam. The resulting wakefield was imaged using femtosecond\nrelativistic electron microscopy. Novel insights into this minimally explored\nregime include mapping the wakefield development over the focal depth and\nstudying the effects of spatio-temporal manipulations of the beam on the\nstructure and phase velocity of the wakefield. Such insights pave the way\ntowards realizing the potential of structured-light based solutions to\ndephasing in laser-wakefield acceleration.",
        "Collusion in market pricing is a concept associated with human actions to\nraise market prices through artificially limited supply. Recently, the idea of\nalgorithmic collusion was put forward, where the human action in the pricing\nprocess is replaced by automated agents. Although experiments have shown that\ncollusive market equilibria can be reached through such techniques, without the\nneed for human intervention, many of the techniques developed remain\nsusceptible to exploitation by other players, making them difficult to\nimplement in practice. In this article, we explore a situation where an agent\nhas a multi-objective strategy, and not only learns to unilaterally exploit\nmarket dynamics originating from other algorithmic agents, but also learns to\nmodel the behaviour of other agents directly. Our results show how common\ncritiques about the viability of algorithmic collusion in real-life settings\ncan be overcome through the usage of slightly more complex algorithms.",
        "In this paper, we attempt to understand the anthropomorphic features of\nchatbot outputs and how these features provide a discursive frame for human-AI\ninteractions. To do so, we explore the use of a prompt-based walkthrough method\nwith two phases: (1) interview-style prompting to reveal the chatbots' context\nof expected use and (2) roleplaying-type prompting to evoke everyday use\nscenarios and typical chatbot outputs. We applied this method to catalogue\nanthropomorphic features across four different LLM chatbots, finding that\nanthropomorphism was exhibited as both subjective language and a sympathetic\nconversational tone. We also found that socio-emotional cues in prompts\nincrease the incidence of anthropomorphic expressions in outputs. We argue that\nthe prompt-based walkthrough method was successful in stimulating social role\nperformance in LLM chatbots and in eliciting a variety of anthropomorphic\nfeatures, making it useful in the study of interaction-based algorithmic harms\nwhere users project inappropriate social roles onto LLM-based tools.",
        "Bandyopadhyay, Dacorogna, Matveev and Troyanov conjectured that a closed\nmanifold admitting a flat, non-negative definite metric of constant rank $m$\nshould be finitely covered by a fiber bundle over the $m$-torus. We give a\ncounter-example to this statement and we discuss the link between this problem\nand the study of transversely flat Riemannian foliations."
      ]
    }
  },
  {
    "id":2412.20675,
    "research_type":"applied",
    "start_id":"b13",
    "start_title":"Discriminative feature learning using a multiscale convolutional capsule network from attitude data for fault diagnosis of industrial robots",
    "start_abstract":"Effective fault diagnosis is important to ensure the reliability, safety, and efficiency of industrial robots. This article proposes a simple yet effective data acquisition strategy based on transmission mechanism analysis, using only one attitude sensor mounted on an end effector or an output component to monitor the attitude of all transmission components. Unlike widely used vibration-monitoring signals, attitude signals can provide fault features reflecting spatial relationships. Using one attitude sensor facilitates the data collection, but weakens fault features and introduces strong background noise in attitude signals. To learn discriminative features from the attitude data collected by the attitude sensor, a multiscale convolutional capsule network (MCCN) is proposed. In MCCN, integrating low-level and high-level features in a convolutional neural network (CNN) as multiscale features is conductive to noise reduction and robust feature extraction, and a capsule network (CapsNet) is used to recognize the spatial relationships in attitude data. The extracted multiscale features in CNN and the spatial-relational features in CapsNet are fused for effective fault diagnosis of industrial robots. The performance of MCCN is evaluated by attaching a softmax-based classifier and integrating it into different transfer learning frameworks to diagnose faults in industrial robots under single and variable working conditions, respectively. Fault diagnosis experiments were conducted on a 6-axis series industrial robot and a parallel robot-driven 3D printer. The superiority of the proposed MCCN was demonstrated by comparing its performance with the other feature learning methods.",
    "start_categories":[
      "eess.SP"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b21"
      ],
      "title":[
        "LSTM Based Bearing Fault Diagnosis of Electrical Machines using Motor Current Signal"
      ],
      "abstract":[
        "Rolling element bearings are one of the most critical components rotating machinery, with bearing faults amounting up to 50% in electrical machines. Therefore, fault diagnosis has attracted attention many researchers. Typically, is performed using vibration signals from machine. In addition, by deep learning algorithms on signals, detection accuracy close 100% can be achieved. However, measurement requires an additional sensor, which not present majority Nevertheless, alternative approach, stator current used for diagnosis. paper emphasizes current. The signal processing signature extraction that buried underneath noise signal. uses Paderborn University damaged dataset, contains data healthy, real inner raceway and outer different severity. For redundant frequencies filtered, then filtered eight features extracted time time-frequency domain wavelet packet decomposition (WPD). Then, these well known algorithm Long Short-Term Memory (LSTM), classification made. LSTM mostly speech recognition due its coherence, but this paper, ability also demonstrated 96%, outperforms perform method developed independent speed loading conditions."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "Love numbers beyond GR from the modified Teukolsky equation",
        "Formation of Be star decretion discs through boundary layer effects",
        "Action representability in categories of unitary algebras",
        "Impact of the Pandemic on Currency Circulation in Brazil: Projections\n  using the SARIMA Model",
        "A precise asymptotic analysis of learning diffusion models: theory and\n  insights",
        "PINK: physical-informed machine learning for lattice thermal\n  conductivity",
        "Search for doubly charmed dibaryons in baryon-baryon scattering",
        "Nonlinear Nanophotonics for High-Dimensional Quantum States",
        "From discrete-time policies to continuous-time diffusion samplers:\n  Asymptotic equivalences and faster training",
        "Linear-Quadratic Optimal Control for Mean-Field Stochastic Differential\n  Equations in Infinite-Horizon with Regime Switching",
        "Pair correlations of one-dimensional model sets and monstrous\n  covariograms of Rauzy fractals",
        "A population synthesis study of the Gaia 100 pc unresolved white\n  dwarf-main sequence binary population",
        "Max-Min Fairness for IRS-Assisted Secure Two-Way Communications",
        "On Coordinated Drone-Courier Logistics for Intra-city Express Services",
        "Extinction and Extirpation Conditions in Coalescent and Ecotonal\n  Metacommunities",
        "The Effect of Optimal Self-Distillation in Noisy Gaussian Mixture Model",
        "Robust design of bicycle infrastructure networks",
        "Single-Source Localization as an Eigenvalue Problem",
        "Simulating Raman Scattering Impairments with Depolarization Noise in\n  Quantum-Classical Links",
        "Quantized Analog Beamforming Enabled Multi-task Federated Learning\n  Over-the-air",
        "Incentive-Compatible Recovery from Manipulated Signals, with\n  Applications to Decentralized Physical Infrastructure",
        "MDCrow: Automating Molecular Dynamics Workflows with Large Language\n  Models",
        "Turbulent-like flows in quasi two-dimensional dense suspensions of\n  motile colloids",
        "Vanishing theorems for Hodge numbers and the Calabi curvature operator",
        "GPU-accelerated LISA parameter estimation with full time domain response",
        "On the number of cofinalities of cuts in ultraproducts of linear orders",
        "Towards an AI co-scientist",
        "Well-posedness and blowup of 1D electron magnetohydrodynamics",
        "Bayesian estimation of Unit-Weibull distribution based on dual\n  generalized order statistics with application to the Cotton Production Data"
      ],
      "abstract":[
        "We obtain the full set of tidal Love numbers of non-rotating black holes in\nan effective field theory extension of general relativity. We achieve our\nresults using a recently introduced modified Teukolsky equation that describes\nthe perturbations of black holes in this theory. We show how to identify the\nLove numbers and their beta functions in a systematic and gauge invariant way,\napplying analytic continuation on the angular number $\\ell$ when necessary. We\nobserve that there are three types of Love numbers: electric, magnetic, and a\n``mixing'' type, associated to parity-breaking theories, that we identify here\nfor the first time. The modified Teukolsky equation proves to be very useful as\nit allows us to obtain all the different Love numbers in a unified framework.\nWe compare our results with previous literature that utilized the\nRegge-Wheeler-Zerilli equations to compute Love numbers, finding perfect\nagreement. The method introduced here paves the way towards the computation of\nLove numbers of rotating black holes beyond general relativity.",
        "Be stars are rapidly rotating, with angular frequency around $0.7-0.8$ of\ntheir Keplerian break up frequency, as a result of significant accretion during\nthe earlier stellar evolution of a companion star. Material from the equator of\nthe Be star is ejected and forms a decretion disc, although the mechanism for\nthe disc formation has remained elusive. We find one-dimensional steady state\ndecretion disc solutions that smoothly transition from a rapidly rotating star\nthat is in hydrostatic balance. Boundary layer effects in a geometrically thick\ndisc which connects to a rotationally flattened star enable the formation of a\ndecretion disc at stellar spin rates below the break up rate. For a disc with\nan aspect ratio $H\/R\\approx 0.1$ at the inner edge, the torque from the disc on\nthe star slows the stellar spin to the observed range and mass ejection\ncontinues at a rate consistent with observed decretion rates. The critical\nrotation rate, to which the star slows down to, decreases as the disc aspect\nratio increases. More generally, steady state accretion and decretion disc\nsolutions can be found for all stellar spin rates. The outcome for a particular\nsystem depends upon the balance between the decretion rate and any external\ninfall accretion rate.",
        "In a recent article [13], G. Janelidze introduced the concept of ideally\nexact categories as a generalization of semi-abelian categories, aiming to\nincorporate relevant examples of non-pointed categories, such as the categories\n$\\textbf{Ring}$ and $\\textbf{CRing}$ of unitary (commutative) rings. He also\nextended the notion of action representability to this broader framework,\nproving that both $\\textbf{Ring}$ and $\\textbf{CRing}$ are action\nrepresentable.\n  This article investigates the representability of actions of unitary\nnon-associative algebras. After providing a detailed description of the monadic\nadjunction associated with any category of unitary algebra, we use the\nconstruction of the external weak actor [4] in order to prove that the\ncategories of unitary (commutative) associative algebras and that of unitary\nalternative algebras are action representable. The result is then extended for\nunitary (commutative) Poisson algebras, where the explicit construction of the\nuniversal strict general actor is employed.",
        "This study analyzes the impact of the COVID-19 pandemic on currency\ncirculation in Brazil by comparing actual data from 2000 to 2023 with\ncounterfactual projections using the\n\\textbf{SARIMA(3,1,1)(3,1,4)\\textsubscript{12}} model. The model was selected\nbased on an extensive parameter search, balancing accuracy and simplicity, and\nvalidated through the metrics MAPE, RMSE, and AIC. The results indicate a\nsignificant deviation between projected and observed values, with an average\ndifference of R\\$ 47.57 billion (13.95\\%). This suggests that the pandemic,\nemergency policies, and the introduction of \\textit{Pix} had a substantial\nimpact on currency circulation. The robustness of the SARIMA model was\nconfirmed, effectively capturing historical trends and seasonality, though\nfindings emphasize the importance of considering exogenous variables, such as\ninterest rates and macroeconomic policies, in future analyses. Future research\nshould explore multivariate models incorporating economic indicators, long-term\nanalysis of post-pandemic currency circulation trends, and studies on public\ncash-holding behavior. The results reinforce the need for continuous monitoring\nand econometric modeling to support decision-making in uncertain economic\ncontexts.",
        "In this manuscript, we consider the problem of learning a flow or\ndiffusion-based generative model parametrized by a two-layer auto-encoder,\ntrained with online stochastic gradient descent, on a high-dimensional target\ndensity with an underlying low-dimensional manifold structure. We derive a\ntight asymptotic characterization of low-dimensional projections of the\ndistribution of samples generated by the learned model, ascertaining in\nparticular its dependence on the number of training samples. Building on this\nanalysis, we discuss how mode collapse can arise, and lead to model collapse\nwhen the generative model is re-trained on generated synthetic data.",
        "Lattice thermal conductivity ($\\kappa_L$) is crucial for efficient thermal\nmanagement in electronics and energy conversion technologies. Traditional\nmethods for predicting \\k{appa}L are often computationally expensive, limiting\ntheir scalability for large-scale material screening. Empirical models, such as\nthe Slack model, offer faster alternatives but require time-consuming\ncalculations for key parameters such as sound velocity and the Gruneisen\nparameter. This work presents a high-throughput framework, physical-informed\nkappa (PINK), which combines the predictive power of crystal graph\nconvolutional neural networks (CGCNNs) with the physical interpretability of\nthe Slack model to predict \\k{appa}L directly from crystallographic information\nfiles (CIFs). Unlike previous approaches, PINK enables rapid, batch predictions\nby extracting material properties such as bulk and shear modulus from CIFs\nusing a well-trained CGCNN model. These properties are then used to compute the\nnecessary parameters for $\\kappa_L$ calculation through a simplified physical\nformula. PINK was applied to a dataset of 377,221 stable materials, enabling\nthe efficient identification of promising candidates with ultralow $\\kappa_L$\nvalues, such as Ag$_3$Te$_4$W and Ag$_3$Te$_4$Ta. The platform, accessible via\na user-friendly interface, offers an unprecedented combination of speed,\naccuracy, and scalability, significantly accelerating material discovery for\nthermal management and energy conversion applications.",
        "We perform a systematical investigation of the doubly charmed dibaryon system\nwith quantum numbers $IJ=01$, and strangeness numbers $S=0$, $-2$ and $-4$ in\nthe framework of the chiral quark model. Two resonance states with strangeness\nnumbers $S=-2$ is obtained in the $\\Lambda\\Omega_{cc}$ scattering channel,\nwhich are $\\Xi_{cc}^{\\ast}\\Xi$ with resonance mass 5081 MeV and decay width 0.3\nMeV, and the $\\Xi_{c}\\Xi_{c}^{\\ast}$ state with the mass 5213 MeV and decay\nwidth 19.8 MeV, respectively. These two predicted charmed dibaryon candidates\nare worth searching for experimentally. Besides, we would like to emphasize\nthat the multi-channel coupling calculation is important to confirm the\nexistence of multiquark states. The coupling can shift the energy of the\nresonance, give the width to the resonance and even destroy the resonance.\nTherefore, to provide the necessary information for experiments to search for\nexotic hadron states, the coupling calculation between the bound channels and\nopen channels is indispensable.",
        "Quantum nanophotonics merges the precision of nanoscale light manipulation\nwith the capabilities of quantum technologies, offering a pathway for enhanced\nlight-matter interaction and compact realization of quantum devices. Here, we\nshow how a recently-demonstrated nonlinear nanophotonic process can be employed\nto selectively create photonic high-dimensional quantum states (qudits). We\nutilize the nonlinearity on the surface of the nanophotonic device to dress,\nthrough the polarization of the pump field, the near-field modes carrying\nangular momentum and their superpositions. We then use this approach for the\nrealization of a multilevel quantum key distribution protocol, which doubles\nthe key rate compared to standard schemes. This idea is an important step\ntowards experimental realizations of quantum state generation and manipulation\nthrough nonlinearity within nanophotonic platforms, and enables new\ncapabilities for on-chip quantum devices.",
        "We study the problem of training neural stochastic differential equations, or\ndiffusion models, to sample from a Boltzmann distribution without access to\ntarget samples. Existing methods for training such models enforce time-reversal\nof the generative and noising processes, using either differentiable simulation\nor off-policy reinforcement learning (RL). We prove equivalences between\nfamilies of objectives in the limit of infinitesimal discretization steps,\nlinking entropic RL methods (GFlowNets) with continuous-time objects (partial\ndifferential equations and path space measures). We further show that an\nappropriate choice of coarse time discretization during training allows greatly\nimproved sample efficiency and the use of time-local objectives, achieving\ncompetitive performance on standard sampling benchmarks with reduced\ncomputational cost.",
        "This paper is concerned with stochastic linear quadratic (LQ, for short)\noptimal control problems in an infinite horizon with conditional mean-field\nterm in a switching regime environment. The orthogonal decomposition introduced\nin [21] has been adopted. Desired algebraic Riccati equations (AREs, for short)\nand a system of backward stochastic differential equations (BSDEs, for short)\nin infinite time horizon with the coefficients depending on the Markov chain\nhave been derived. The determination of closed-loop optimal strategy follows\nfrom the solvability of ARE and BSDE. Moreover, the solvability of BSDEs leads\nto a characterization of open-loop solvability of the optimal control problem.",
        "The averaged distance structure of one-dimensional regular model sets is\ndetermined via their pair correlation functions. The latter lead to\ncovariograms and cross covariograms of the windows, which give continuous\nfunctions in internal space. While they are simple tent-shaped, piecewise\nlinear functions for intervals, the typical case for inflation systems leads to\nconvolutions of Rauzy fractals, which are difficult to compute. In the presence\nof an inflation structure, an alternative path is possible via the exact\nrenormalisation structures of the pair correlation functions. We introduce this\napproach and derive two concrete examples, which display an unexpectedly\ncomplex and wild behaviour.",
        "Binary stars consisting of a white dwarf and a main sequence star (WDMS) are\nvaluable for studying key astrophysical questions. However, observational\nbiases strongly affect the known population, particularly unresolved systems\nwhere the main sequence star outshines the white dwarf. This work aims to\ncomprehensively simulate the population of unresolved WDMS binaries within 100\npc of the Sun and to compare the outcome with the currently most complete\nvolume-limited sample available from Gaia data. We employ a population\nsynthesis code, MRBIN, extensively developed by our group and based on Monte\nCarlo techniques, which uses a standard binary stellar evolutionary code\nadapted to cover a wide range of stars across all ages, masses, and\nmetallicities. Selection criteria matching those of Gaia observations are\napplied to generate synthetic populations comparable to the observed WDMS\nsample. The synthetic data accurately populate the expected regions in the Gaia\ncolor-magnitude diagram. However, simulations predict a lower number of\nextremely low-mass white dwarfs, suggesting potential issues in observed mass\nderivations. Additionally, our analysis constrains the common envelope\nefficiency to 0.1-0.4, consistent with previous findings, and estimates a total\ncompleteness of about 25% for the observed sample, confirming the strong\nobservational limitations for unresolved WDMS.",
        "This paper investigates an intelligent reflective surface (IRS) assisted\nsecure multi-user two-way communication system. The aim of this paper is to\nenhance the physical layer security by optimizing the minimum secrecy-rate\namong all user-pairs in the presence of a malicious user. The optimization\nproblem is converted into an alternating optimization problem consisting of two\nsub-problems. Transmit power optimization is handled using a fractional\nprogramming method, whereas IRS phase shift optimization is handled with\nsemi-definite programming. The convergence of the proposed algorithm is\ninvestigated numerically. The performance gain in minimum secrecy-rate is\nquantified for four different user configurations in comparison to the baseline\nscheme. Results indicate a 3.6-fold gain in minimum secrecy rate over the\nbaseline scheme when the IRS is positioned near a legitimate user, even when\nthe malicious user is located close to the same legitimate user.",
        "Problem definition: Drones, despite being acknowledged as a transformative\nforce in the city logistics sector, are unable to execute the\n\\textit{last-meter delivery} (unloading goods directly to customers' doorsteps)\ndue to airspace restrictions and safety concerns. To leverage advancements and\novercome the limitations of drones in providing intra-city express services, we\nintroduce a coordinated drone-courier logistics system where drones operate\nwithin a closed network among vertiports, while couriers connect customers to\nthe drone delivery system. This paper aims to shed light on this coordinated\nsystem in terms of system feasibility, network interactivity, and long-term\nsustainability. Methodology\/Results: We develop an integrated optimization\nmodel to optimize the network planning of the coordinated logistics system. The\ninterplay between network planning and tactical operations is mirrored by a\nqueueing network model, resulting in the nonlinear and nonconvex (partially\nconvex and partially concave) feasible region of the optimization model. An\niterative exact algorithm that tightens lower and upper bounds by adaptively\nrefining the linear approximations of nonlinear constraints is developed to\nprovide optimality-guaranteed solutions with finite convergence. The\ncomputational experiments demonstrate the scalability and robustness of our\nalgorithm across various network configurations and scenarios.Managerial\nimplications: The case study, based on a real-world dataset from SF Express, a\nlogistics giant in China, validates that the coordinated logistics system\nefficiently attains cost and time savings by leveraging the effective turnover\nof drones and the coordination between drones and couriers. The optimal network\ndesign features a concentrated structure, streamlining demand consolidation and\nreducing deadhead repositioning.",
        "Here we present extinction, extirpation and coexistence conditions where \/\nwhen two communities combine. We consider one specific model where two\ncommunities coalesce, and another model where the communities coexist side by\nside, blending in a transitionary zone called the ecotone. Specifically, (1) we\nanalytically calculate the shifts in abundances as a function of mixing\nstrength. (2) Obtain a critical value for the mixing strength leading to\nextinction. (3) Derive an inequality condition for full coexistent mixing. (4)\nfind how the individual communities penetrate into one other as a function of\nmixing strength. (5) derive the conditions for one species to cross the ecotone\nand invade an neighboring community and (6) conditions for a native species to\nget extirpated. Lastly, (7) we spatially investigate the species richness\nwithin the ecotone and derive a condition that determines whether the ecotone\nwill have higher or lower richness compared to its surrounding habitats.",
        "Self-distillation (SD), a technique where a model refines itself from its own\npredictions, has garnered attention as a simple yet powerful approach in\nmachine learning. Despite its widespread use, the mechanisms underlying its\neffectiveness remain unclear. In this study, we investigate the efficacy of\nhyperparameter-tuned multi-stage SD in binary classification tasks with noisy\nlabeled Gaussian mixture data, utilizing a replica theory. Our findings reveals\nthat the primary driver of SD's performance improvement is denoising through\nhard pseudo-labels, with the most notable gains observed in moderately sized\ndatasets. We also demonstrate the efficacy of practical heuristics, such as\nearly stopping for extracting meaningful signal and bias fixation for\nimbalanced data. These results provide both theoretical guarantees and\npractical insights, advancing our understanding and application of SD in noisy\nsettings.",
        "Promoting active mobility like cycling relies on the availability of\nwell-connected, high-quality bicycle networks. However, expanding these\nnetworks over an extended planning horizon presents one of the most complex\nchallenges in transport science. This complexity arises from the intricate\ninteractions between infrastructure availability and usage, such as network\nspillover effects and mode choice substitutions. In this paper, we approach the\nproblem from two perspectives: direct optimization methods, which generate\nnear-optimal solutions using operations research techniques, and conceptual\nheuristics, which offer intuitive and scalable algorithms grounded in network\nscience. Specifically, we compare direct welfare optimization with an inverse\nnetwork percolation approach to planning cycle superhighway extensions in\nCopenhagen. Interestingly, while the more complex optimization models yield\nbetter overall welfare results, the improvements over simpler methods are\nsmall. More importantly, we demonstrate that the increased complexity of\nplanning approaches generally makes them more vulnerable to input uncertainty,\nreflecting the bias-variance tradeoff. This issue is particularly relevant in\nthe context of long-term planning, where conditions change during the\nimplementation of the planned infrastructure expansions. Therefore, while\nplanning bicycle infrastructure is important and renders exceptionally high\nbenefit-cost ratios, considerations of robustness and ease of implementation\nmay justify the use of more straightforward network-based methods.",
        "This paper introduces a novel method for solving the single-source\nlocalization problem, specifically addressing the case of trilateration. We\nformulate the problem as a weighted least-squares problem in the squared\ndistances and demonstrate how suitable weights are chosen to accommodate\ndifferent noise distributions. By transforming this formulation into an\neigenvalue problem, we leverage existing eigensolvers to achieve a fast,\nnumerically stable, and easily implemented solver. Furthermore, our theoretical\nanalysis establishes that the globally optimal solution corresponds to the\nlargest real eigenvalue, drawing parallels to the existing literature on the\ntrust-region subproblem. Unlike previous works, we give special treatment to\ndegenerate cases, where multiple and possibly infinitely many solutions exist.\nWe provide a geometric interpretation of the solution sets and design the\nproposed method to handle these cases gracefully. Finally, we validate against\na range of state-of-the-art methods using synthetic and real data,\ndemonstrating how the proposed method is among the fastest and most numerically\nstable.",
        "We model spontaneous Raman scattering noise in polarization-encoded quantum\ncommunication channels co-propagating with classical signals using the\ndepolarization channel. Utilizing NetSquid simulations, we validate the model\nagainst demonstrations of qubit transmission, entanglement distribution, and\nteleportation.",
        "Over-the-air computation (AirComp) has recently emerged as a pivotal\ntechnique for communication-efficient federated learning (FL) in\nresource-constrained wireless networks. Though AirComp leverages the\nsuperposition property of multiple access channels for computation, it\ninherently limits its ability to manage inter-task interference in multi-task\ncomputing. In this paper, we propose a quantized analog beamforming scheme at\nthe receiver to enable simultaneous multi-task FL. Specifically, inspiring by\nthe favorable propagation and channel hardening properties of large-scale\nantenna arrays, a targeted analog beamforming method in closed form is proposed\nfor statistical interference elimination. Analytical results reveal that the\ninterference power vanishes by an order of $\\mathcal{O}\\left(1\/N_r\\right)$ with\nthe number of analog phase shifters, $N_r$, irrespective of their quantization\nprecision. Numerical results demonstrate the effectiveness of the proposed\nanalog beamforming method and show that the performance upper bound of ideal\nlearning without errors can be achieved by increasing the number of\nlow-precision analog phase shifters.",
        "We introduce the first formal model capturing the elicitation of unverifiable\ninformation from a party (the \"source\") with implicit signals derived by other\nplayers (the \"observers\"). Our model is motivated in part by applications in\ndecentralized physical infrastructure networks (a.k.a. \"DePIN\"), an emerging\napplication domain in which physical services (e.g., sensor information,\nbandwidth, or energy) are provided at least in part by untrusted and\nself-interested parties. A key challenge in these signal network applications\nis verifying the level of service that was actually provided by network\nparticipants.\n  We first establish a condition called source identifiability, which we show\nis necessary for the existence of a mechanism for which truthful signal\nreporting is a strict equilibrium. For a converse, we build on techniques from\npeer prediction to show that in every signal network that satisfies the source\nidentifiability condition, there is in fact a strictly truthful mechanism,\nwhere truthful signal reporting gives strictly higher total expected payoff\nthan any less informative equilibrium. We furthermore show that this truthful\nequilibrium is in fact the unique equilibrium of the mechanism if there is\npositive probability that any one observer is unconditionally honest (e.g., if\nan observer were run by the network owner). Also, by extending our condition to\ncoalitions, we show that there are generally no collusion-resistant mechanisms\nin the settings that we consider.\n  We apply our framework and results to two DePIN applications: proving\nlocation, and proving bandwidth. In the location-proving setting observers\nlearn (potentially enlarged) Euclidean distances to the source. Here, our\ncondition has an appealing geometric interpretation, implying that the source's\nlocation can be truthfully elicited if and only if it is guaranteed to lie\ninside the convex hull of the observers.",
        "Molecular dynamics (MD) simulations are essential for understanding\nbiomolecular systems but remain challenging to automate. Recent advances in\nlarge language models (LLM) have demonstrated success in automating complex\nscientific tasks using LLM-based agents. In this paper, we introduce MDCrow, an\nagentic LLM assistant capable of automating MD workflows. MDCrow uses\nchain-of-thought over 40 expert-designed tools for handling and processing\nfiles, setting up simulations, analyzing the simulation outputs, and retrieving\nrelevant information from literature and databases. We assess MDCrow's\nperformance across 25 tasks of varying required subtasks and difficulty, and we\nevaluate the agent's robustness to both difficulty and prompt style.\n\\texttt{gpt-4o} is able to complete complex tasks with low variance, followed\nclosely by \\texttt{llama3-405b}, a compelling open-source model. While prompt\nstyle does not influence the best models' performance, it has significant\neffects on smaller models.",
        "Dense bacterial suspensions exhibit turbulent-like flows at low Reynolds\nnumbers, driven by the activity of the microswimmers. In this study, we develop\na model system to examine these dynamics using motile colloids that mimic\nbacterial locomotion. The colloids are powered by the Quincke instability,\nwhich causes them to spontaneously roll in a random-walk pattern when exposed\nto a square-wave electric field. We experimentally investigate the flow\ndynamics in dense suspensions of these Quincke random walkers under quasi\ntwo-dimensional conditions, where the particle size is comparable to the gap\nbetween the electrodes. Our results reveal an energy spectrum scaling at high\nwavenumbers as $ \\sim k^{-4}$, which holds across a broad range of activity\nlevels -- controlled by the field strength -- and particle concentrations. We\nobserve that velocity time correlations decay within a single period of the\nsquare-wave field, yet an anti-correlation appears between successive field\napplications, indicative of a dynamic structural memory of the ensemble.",
        "It is shown that a compact $n$-dimensional K\\\"ahler manifold with\n$\\frac{n}{2}$-positive Calabi curvature operator has the rational cohomology of\ncomplex projective space. For even $n,$ this is sharp in the sense that the\ncomplex quadric with its symmetric metric has $\\frac{n}{2}$-nonnegative Calabi\ncurvature operator, yet $b_n =2.$ Furthermore, the compact K\\\"ahler manifolds\nwith $\\frac{n}{2}$-nonnegative Calabi curvature operator are completely\nclassified. In addition, the previously known results for the K\\\"ahler\ncurvature operator are improved when the metric is K\\\"ahler-Einstein.",
        "We conduct the first full Bayesian inference analysis for LISA parameter\nestimation incorporating the effects of subdominant harmonics and\nspin-precession through a full time domain response. The substantial\ncomputational demands of using time domain waveforms for LISA are significantly\nmitigated by implementing a novel Python version of the IMRPhenomT family of\nwaveform models and the LISA response with GPU acceleration. This time domain\nresponse alleviates the theoretical necessity of developing specific transfer\nfunctions to approximate the LISA response in the Fourier domain for each\nspecific type of system and allows for the use of unequal arms configurations\nand realistic LISA orbits. Our analysis includes a series of zero-noise\ninjections for a Massive Black Hole Binary with aligned and precessing spins.\nWe investigate the impact of including subdominant harmonics, compare equal and\nunequal arm configurations, and analyze different Time-Delay-Interferometry\n(TDI) configurations. We utilize full and uniform priors, with a lower\nfrequency cutoff of 0.1mHz, and a signal duration of approximately two months,\nsampled every 5 seconds. The sampler is initialized based on Fisher estimates.\nOur results demonstrate LISA capability to measure the two spin magnitudes and\nthe primary spin tilt angle, alongside sky localization, with percent-level\nprecision, while component masses are determined with sub-percent accuracy.",
        "Suppose $\\kappa$ is a regular cardinal and $\\bar a=\\langle \\mu_i: i<\\kappa\n\\rangle$ is a non-decreasing sequence of regular cardinals. We study the set of\npossible cofinalities of cuts Pcut$(\\bar a)=\\{(\\lambda_1, \\lambda_2):$ for some\nultrafilter $D$ on $\\kappa$, $(\\lambda_1, \\lambda_2)$ is the cofinality of a\ncut of $\\prod\\limits_{i<\\kappa} \\mu_i \/ D \\}$.",
        "Scientific discovery relies on scientists generating novel hypotheses that\nundergo rigorous experimental validation. To augment this process, we introduce\nan AI co-scientist, a multi-agent system built on Gemini 2.0. The AI\nco-scientist is intended to help uncover new, original knowledge and to\nformulate demonstrably novel research hypotheses and proposals, building upon\nprior evidence and aligned to scientist-provided research objectives and\nguidance. The system's design incorporates a generate, debate, and evolve\napproach to hypothesis generation, inspired by the scientific method and\naccelerated by scaling test-time compute. Key contributions include: (1) a\nmulti-agent architecture with an asynchronous task execution framework for\nflexible compute scaling; (2) a tournament evolution process for self-improving\nhypotheses generation. Automated evaluations show continued benefits of\ntest-time compute, improving hypothesis quality. While general purpose, we\nfocus development and validation in three biomedical areas: drug repurposing,\nnovel target discovery, and explaining mechanisms of bacterial evolution and\nanti-microbial resistance. For drug repurposing, the system proposes candidates\nwith promising validation findings, including candidates for acute myeloid\nleukemia that show tumor inhibition in vitro at clinically applicable\nconcentrations. For novel target discovery, the AI co-scientist proposed new\nepigenetic targets for liver fibrosis, validated by anti-fibrotic activity and\nliver cell regeneration in human hepatic organoids. Finally, the AI\nco-scientist recapitulated unpublished experimental results via a parallel in\nsilico discovery of a novel gene transfer mechanism in bacterial evolution.\nThese results, detailed in separate, co-timed reports, demonstrate the\npotential to augment biomedical and scientific discovery and usher an era of AI\nempowered scientists.",
        "The one-dimensional toy models proposed for the three-dimensional electron\nmagnetohydrodynamics in our previous work share some similarities with the\noriginal dynamics under certain symmetry. We continue to study the\nwell-posedness issue and explore the potential singularity formation scenario\nfor these models.",
        "The Unit Weibull distribution with parameters $\\alpha$ and $\\beta$ is\nconsidered to study in the context of dual generalized order statistics. For\nthe analysis purpose, Bayes estimators based on symmetric and asymmetric loss\nfunctions are obtained. The methods which are utilized for Bayesian estimation\nare approximation and simulation tools such as Lindley, Tierney-Kadane and\nMarkov chain Monte Carlo methods. The authors have considered squared error\nloss function as symmetric and LINEX and general entropy loss function as\nasymmetric loss functions. After presenting the mathematical results, a\nsimulation study is conducted to exhibit the performances of various derived\nestimators. As this study is considered for the dual generalized order\nstatistics that is unification of models based distinct ordered random variable\nsuch as order statistics, record values, etc. This provides flexibility in our\nresults and in continuation of this, the cotton production data of USA is\nanalyzed for both submodels of ordered random variables: order statistics and\nrecord values."
      ]
    }
  },
  {
    "id":2412.20675,
    "research_type":"applied",
    "start_id":"b21",
    "start_title":"LSTM Based Bearing Fault Diagnosis of Electrical Machines using Motor Current Signal",
    "start_abstract":"Rolling element bearings are one of the most critical components rotating machinery, with bearing faults amounting up to 50% in electrical machines. Therefore, fault diagnosis has attracted attention many researchers. Typically, is performed using vibration signals from machine. In addition, by deep learning algorithms on signals, detection accuracy close 100% can be achieved. However, measurement requires an additional sensor, which not present majority Nevertheless, alternative approach, stator current used for diagnosis. paper emphasizes current. The signal processing signature extraction that buried underneath noise signal. uses Paderborn University damaged dataset, contains data healthy, real inner raceway and outer different severity. For redundant frequencies filtered, then filtered eight features extracted time time-frequency domain wavelet packet decomposition (WPD). Then, these well known algorithm Long Short-Term Memory (LSTM), classification made. LSTM mostly speech recognition due its coherence, but this paper, ability also demonstrated 96%, outperforms perform method developed independent speed loading conditions.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b13"
      ],
      "title":[
        "Discriminative feature learning using a multiscale convolutional capsule network from attitude data for fault diagnosis of industrial robots"
      ],
      "abstract":[
        "Effective fault diagnosis is important to ensure the reliability, safety, and efficiency of industrial robots. This article proposes a simple yet effective data acquisition strategy based on transmission mechanism analysis, using only one attitude sensor mounted on an end effector or an output component to monitor the attitude of all transmission components. Unlike widely used vibration-monitoring signals, attitude signals can provide fault features reflecting spatial relationships. Using one attitude sensor facilitates the data collection, but weakens fault features and introduces strong background noise in attitude signals. To learn discriminative features from the attitude data collected by the attitude sensor, a multiscale convolutional capsule network (MCCN) is proposed. In MCCN, integrating low-level and high-level features in a convolutional neural network (CNN) as multiscale features is conductive to noise reduction and robust feature extraction, and a capsule network (CapsNet) is used to recognize the spatial relationships in attitude data. The extracted multiscale features in CNN and the spatial-relational features in CapsNet are fused for effective fault diagnosis of industrial robots. The performance of MCCN is evaluated by attaching a softmax-based classifier and integrating it into different transfer learning frameworks to diagnose faults in industrial robots under single and variable working conditions, respectively. Fault diagnosis experiments were conducted on a 6-axis series industrial robot and a parallel robot-driven 3D printer. The superiority of the proposed MCCN was demonstrated by comparing its performance with the other feature learning methods."
      ],
      "categories":[
        "eess.SP"
      ]
    },
    "list":{
      "title":[
        "Love numbers beyond GR from the modified Teukolsky equation",
        "Formation of Be star decretion discs through boundary layer effects",
        "Action representability in categories of unitary algebras",
        "Impact of the Pandemic on Currency Circulation in Brazil: Projections\n  using the SARIMA Model",
        "A precise asymptotic analysis of learning diffusion models: theory and\n  insights",
        "PINK: physical-informed machine learning for lattice thermal\n  conductivity",
        "Search for doubly charmed dibaryons in baryon-baryon scattering",
        "Nonlinear Nanophotonics for High-Dimensional Quantum States",
        "From discrete-time policies to continuous-time diffusion samplers:\n  Asymptotic equivalences and faster training",
        "Linear-Quadratic Optimal Control for Mean-Field Stochastic Differential\n  Equations in Infinite-Horizon with Regime Switching",
        "Pair correlations of one-dimensional model sets and monstrous\n  covariograms of Rauzy fractals",
        "A population synthesis study of the Gaia 100 pc unresolved white\n  dwarf-main sequence binary population",
        "Max-Min Fairness for IRS-Assisted Secure Two-Way Communications",
        "On Coordinated Drone-Courier Logistics for Intra-city Express Services",
        "Extinction and Extirpation Conditions in Coalescent and Ecotonal\n  Metacommunities",
        "The Effect of Optimal Self-Distillation in Noisy Gaussian Mixture Model",
        "Robust design of bicycle infrastructure networks",
        "Single-Source Localization as an Eigenvalue Problem",
        "Simulating Raman Scattering Impairments with Depolarization Noise in\n  Quantum-Classical Links",
        "Quantized Analog Beamforming Enabled Multi-task Federated Learning\n  Over-the-air",
        "Incentive-Compatible Recovery from Manipulated Signals, with\n  Applications to Decentralized Physical Infrastructure",
        "MDCrow: Automating Molecular Dynamics Workflows with Large Language\n  Models",
        "Turbulent-like flows in quasi two-dimensional dense suspensions of\n  motile colloids",
        "Vanishing theorems for Hodge numbers and the Calabi curvature operator",
        "GPU-accelerated LISA parameter estimation with full time domain response",
        "On the number of cofinalities of cuts in ultraproducts of linear orders",
        "Towards an AI co-scientist",
        "Well-posedness and blowup of 1D electron magnetohydrodynamics",
        "Bayesian estimation of Unit-Weibull distribution based on dual\n  generalized order statistics with application to the Cotton Production Data"
      ],
      "abstract":[
        "We obtain the full set of tidal Love numbers of non-rotating black holes in\nan effective field theory extension of general relativity. We achieve our\nresults using a recently introduced modified Teukolsky equation that describes\nthe perturbations of black holes in this theory. We show how to identify the\nLove numbers and their beta functions in a systematic and gauge invariant way,\napplying analytic continuation on the angular number $\\ell$ when necessary. We\nobserve that there are three types of Love numbers: electric, magnetic, and a\n``mixing'' type, associated to parity-breaking theories, that we identify here\nfor the first time. The modified Teukolsky equation proves to be very useful as\nit allows us to obtain all the different Love numbers in a unified framework.\nWe compare our results with previous literature that utilized the\nRegge-Wheeler-Zerilli equations to compute Love numbers, finding perfect\nagreement. The method introduced here paves the way towards the computation of\nLove numbers of rotating black holes beyond general relativity.",
        "Be stars are rapidly rotating, with angular frequency around $0.7-0.8$ of\ntheir Keplerian break up frequency, as a result of significant accretion during\nthe earlier stellar evolution of a companion star. Material from the equator of\nthe Be star is ejected and forms a decretion disc, although the mechanism for\nthe disc formation has remained elusive. We find one-dimensional steady state\ndecretion disc solutions that smoothly transition from a rapidly rotating star\nthat is in hydrostatic balance. Boundary layer effects in a geometrically thick\ndisc which connects to a rotationally flattened star enable the formation of a\ndecretion disc at stellar spin rates below the break up rate. For a disc with\nan aspect ratio $H\/R\\approx 0.1$ at the inner edge, the torque from the disc on\nthe star slows the stellar spin to the observed range and mass ejection\ncontinues at a rate consistent with observed decretion rates. The critical\nrotation rate, to which the star slows down to, decreases as the disc aspect\nratio increases. More generally, steady state accretion and decretion disc\nsolutions can be found for all stellar spin rates. The outcome for a particular\nsystem depends upon the balance between the decretion rate and any external\ninfall accretion rate.",
        "In a recent article [13], G. Janelidze introduced the concept of ideally\nexact categories as a generalization of semi-abelian categories, aiming to\nincorporate relevant examples of non-pointed categories, such as the categories\n$\\textbf{Ring}$ and $\\textbf{CRing}$ of unitary (commutative) rings. He also\nextended the notion of action representability to this broader framework,\nproving that both $\\textbf{Ring}$ and $\\textbf{CRing}$ are action\nrepresentable.\n  This article investigates the representability of actions of unitary\nnon-associative algebras. After providing a detailed description of the monadic\nadjunction associated with any category of unitary algebra, we use the\nconstruction of the external weak actor [4] in order to prove that the\ncategories of unitary (commutative) associative algebras and that of unitary\nalternative algebras are action representable. The result is then extended for\nunitary (commutative) Poisson algebras, where the explicit construction of the\nuniversal strict general actor is employed.",
        "This study analyzes the impact of the COVID-19 pandemic on currency\ncirculation in Brazil by comparing actual data from 2000 to 2023 with\ncounterfactual projections using the\n\\textbf{SARIMA(3,1,1)(3,1,4)\\textsubscript{12}} model. The model was selected\nbased on an extensive parameter search, balancing accuracy and simplicity, and\nvalidated through the metrics MAPE, RMSE, and AIC. The results indicate a\nsignificant deviation between projected and observed values, with an average\ndifference of R\\$ 47.57 billion (13.95\\%). This suggests that the pandemic,\nemergency policies, and the introduction of \\textit{Pix} had a substantial\nimpact on currency circulation. The robustness of the SARIMA model was\nconfirmed, effectively capturing historical trends and seasonality, though\nfindings emphasize the importance of considering exogenous variables, such as\ninterest rates and macroeconomic policies, in future analyses. Future research\nshould explore multivariate models incorporating economic indicators, long-term\nanalysis of post-pandemic currency circulation trends, and studies on public\ncash-holding behavior. The results reinforce the need for continuous monitoring\nand econometric modeling to support decision-making in uncertain economic\ncontexts.",
        "In this manuscript, we consider the problem of learning a flow or\ndiffusion-based generative model parametrized by a two-layer auto-encoder,\ntrained with online stochastic gradient descent, on a high-dimensional target\ndensity with an underlying low-dimensional manifold structure. We derive a\ntight asymptotic characterization of low-dimensional projections of the\ndistribution of samples generated by the learned model, ascertaining in\nparticular its dependence on the number of training samples. Building on this\nanalysis, we discuss how mode collapse can arise, and lead to model collapse\nwhen the generative model is re-trained on generated synthetic data.",
        "Lattice thermal conductivity ($\\kappa_L$) is crucial for efficient thermal\nmanagement in electronics and energy conversion technologies. Traditional\nmethods for predicting \\k{appa}L are often computationally expensive, limiting\ntheir scalability for large-scale material screening. Empirical models, such as\nthe Slack model, offer faster alternatives but require time-consuming\ncalculations for key parameters such as sound velocity and the Gruneisen\nparameter. This work presents a high-throughput framework, physical-informed\nkappa (PINK), which combines the predictive power of crystal graph\nconvolutional neural networks (CGCNNs) with the physical interpretability of\nthe Slack model to predict \\k{appa}L directly from crystallographic information\nfiles (CIFs). Unlike previous approaches, PINK enables rapid, batch predictions\nby extracting material properties such as bulk and shear modulus from CIFs\nusing a well-trained CGCNN model. These properties are then used to compute the\nnecessary parameters for $\\kappa_L$ calculation through a simplified physical\nformula. PINK was applied to a dataset of 377,221 stable materials, enabling\nthe efficient identification of promising candidates with ultralow $\\kappa_L$\nvalues, such as Ag$_3$Te$_4$W and Ag$_3$Te$_4$Ta. The platform, accessible via\na user-friendly interface, offers an unprecedented combination of speed,\naccuracy, and scalability, significantly accelerating material discovery for\nthermal management and energy conversion applications.",
        "We perform a systematical investigation of the doubly charmed dibaryon system\nwith quantum numbers $IJ=01$, and strangeness numbers $S=0$, $-2$ and $-4$ in\nthe framework of the chiral quark model. Two resonance states with strangeness\nnumbers $S=-2$ is obtained in the $\\Lambda\\Omega_{cc}$ scattering channel,\nwhich are $\\Xi_{cc}^{\\ast}\\Xi$ with resonance mass 5081 MeV and decay width 0.3\nMeV, and the $\\Xi_{c}\\Xi_{c}^{\\ast}$ state with the mass 5213 MeV and decay\nwidth 19.8 MeV, respectively. These two predicted charmed dibaryon candidates\nare worth searching for experimentally. Besides, we would like to emphasize\nthat the multi-channel coupling calculation is important to confirm the\nexistence of multiquark states. The coupling can shift the energy of the\nresonance, give the width to the resonance and even destroy the resonance.\nTherefore, to provide the necessary information for experiments to search for\nexotic hadron states, the coupling calculation between the bound channels and\nopen channels is indispensable.",
        "Quantum nanophotonics merges the precision of nanoscale light manipulation\nwith the capabilities of quantum technologies, offering a pathway for enhanced\nlight-matter interaction and compact realization of quantum devices. Here, we\nshow how a recently-demonstrated nonlinear nanophotonic process can be employed\nto selectively create photonic high-dimensional quantum states (qudits). We\nutilize the nonlinearity on the surface of the nanophotonic device to dress,\nthrough the polarization of the pump field, the near-field modes carrying\nangular momentum and their superpositions. We then use this approach for the\nrealization of a multilevel quantum key distribution protocol, which doubles\nthe key rate compared to standard schemes. This idea is an important step\ntowards experimental realizations of quantum state generation and manipulation\nthrough nonlinearity within nanophotonic platforms, and enables new\ncapabilities for on-chip quantum devices.",
        "We study the problem of training neural stochastic differential equations, or\ndiffusion models, to sample from a Boltzmann distribution without access to\ntarget samples. Existing methods for training such models enforce time-reversal\nof the generative and noising processes, using either differentiable simulation\nor off-policy reinforcement learning (RL). We prove equivalences between\nfamilies of objectives in the limit of infinitesimal discretization steps,\nlinking entropic RL methods (GFlowNets) with continuous-time objects (partial\ndifferential equations and path space measures). We further show that an\nappropriate choice of coarse time discretization during training allows greatly\nimproved sample efficiency and the use of time-local objectives, achieving\ncompetitive performance on standard sampling benchmarks with reduced\ncomputational cost.",
        "This paper is concerned with stochastic linear quadratic (LQ, for short)\noptimal control problems in an infinite horizon with conditional mean-field\nterm in a switching regime environment. The orthogonal decomposition introduced\nin [21] has been adopted. Desired algebraic Riccati equations (AREs, for short)\nand a system of backward stochastic differential equations (BSDEs, for short)\nin infinite time horizon with the coefficients depending on the Markov chain\nhave been derived. The determination of closed-loop optimal strategy follows\nfrom the solvability of ARE and BSDE. Moreover, the solvability of BSDEs leads\nto a characterization of open-loop solvability of the optimal control problem.",
        "The averaged distance structure of one-dimensional regular model sets is\ndetermined via their pair correlation functions. The latter lead to\ncovariograms and cross covariograms of the windows, which give continuous\nfunctions in internal space. While they are simple tent-shaped, piecewise\nlinear functions for intervals, the typical case for inflation systems leads to\nconvolutions of Rauzy fractals, which are difficult to compute. In the presence\nof an inflation structure, an alternative path is possible via the exact\nrenormalisation structures of the pair correlation functions. We introduce this\napproach and derive two concrete examples, which display an unexpectedly\ncomplex and wild behaviour.",
        "Binary stars consisting of a white dwarf and a main sequence star (WDMS) are\nvaluable for studying key astrophysical questions. However, observational\nbiases strongly affect the known population, particularly unresolved systems\nwhere the main sequence star outshines the white dwarf. This work aims to\ncomprehensively simulate the population of unresolved WDMS binaries within 100\npc of the Sun and to compare the outcome with the currently most complete\nvolume-limited sample available from Gaia data. We employ a population\nsynthesis code, MRBIN, extensively developed by our group and based on Monte\nCarlo techniques, which uses a standard binary stellar evolutionary code\nadapted to cover a wide range of stars across all ages, masses, and\nmetallicities. Selection criteria matching those of Gaia observations are\napplied to generate synthetic populations comparable to the observed WDMS\nsample. The synthetic data accurately populate the expected regions in the Gaia\ncolor-magnitude diagram. However, simulations predict a lower number of\nextremely low-mass white dwarfs, suggesting potential issues in observed mass\nderivations. Additionally, our analysis constrains the common envelope\nefficiency to 0.1-0.4, consistent with previous findings, and estimates a total\ncompleteness of about 25% for the observed sample, confirming the strong\nobservational limitations for unresolved WDMS.",
        "This paper investigates an intelligent reflective surface (IRS) assisted\nsecure multi-user two-way communication system. The aim of this paper is to\nenhance the physical layer security by optimizing the minimum secrecy-rate\namong all user-pairs in the presence of a malicious user. The optimization\nproblem is converted into an alternating optimization problem consisting of two\nsub-problems. Transmit power optimization is handled using a fractional\nprogramming method, whereas IRS phase shift optimization is handled with\nsemi-definite programming. The convergence of the proposed algorithm is\ninvestigated numerically. The performance gain in minimum secrecy-rate is\nquantified for four different user configurations in comparison to the baseline\nscheme. Results indicate a 3.6-fold gain in minimum secrecy rate over the\nbaseline scheme when the IRS is positioned near a legitimate user, even when\nthe malicious user is located close to the same legitimate user.",
        "Problem definition: Drones, despite being acknowledged as a transformative\nforce in the city logistics sector, are unable to execute the\n\\textit{last-meter delivery} (unloading goods directly to customers' doorsteps)\ndue to airspace restrictions and safety concerns. To leverage advancements and\novercome the limitations of drones in providing intra-city express services, we\nintroduce a coordinated drone-courier logistics system where drones operate\nwithin a closed network among vertiports, while couriers connect customers to\nthe drone delivery system. This paper aims to shed light on this coordinated\nsystem in terms of system feasibility, network interactivity, and long-term\nsustainability. Methodology\/Results: We develop an integrated optimization\nmodel to optimize the network planning of the coordinated logistics system. The\ninterplay between network planning and tactical operations is mirrored by a\nqueueing network model, resulting in the nonlinear and nonconvex (partially\nconvex and partially concave) feasible region of the optimization model. An\niterative exact algorithm that tightens lower and upper bounds by adaptively\nrefining the linear approximations of nonlinear constraints is developed to\nprovide optimality-guaranteed solutions with finite convergence. The\ncomputational experiments demonstrate the scalability and robustness of our\nalgorithm across various network configurations and scenarios.Managerial\nimplications: The case study, based on a real-world dataset from SF Express, a\nlogistics giant in China, validates that the coordinated logistics system\nefficiently attains cost and time savings by leveraging the effective turnover\nof drones and the coordination between drones and couriers. The optimal network\ndesign features a concentrated structure, streamlining demand consolidation and\nreducing deadhead repositioning.",
        "Here we present extinction, extirpation and coexistence conditions where \/\nwhen two communities combine. We consider one specific model where two\ncommunities coalesce, and another model where the communities coexist side by\nside, blending in a transitionary zone called the ecotone. Specifically, (1) we\nanalytically calculate the shifts in abundances as a function of mixing\nstrength. (2) Obtain a critical value for the mixing strength leading to\nextinction. (3) Derive an inequality condition for full coexistent mixing. (4)\nfind how the individual communities penetrate into one other as a function of\nmixing strength. (5) derive the conditions for one species to cross the ecotone\nand invade an neighboring community and (6) conditions for a native species to\nget extirpated. Lastly, (7) we spatially investigate the species richness\nwithin the ecotone and derive a condition that determines whether the ecotone\nwill have higher or lower richness compared to its surrounding habitats.",
        "Self-distillation (SD), a technique where a model refines itself from its own\npredictions, has garnered attention as a simple yet powerful approach in\nmachine learning. Despite its widespread use, the mechanisms underlying its\neffectiveness remain unclear. In this study, we investigate the efficacy of\nhyperparameter-tuned multi-stage SD in binary classification tasks with noisy\nlabeled Gaussian mixture data, utilizing a replica theory. Our findings reveals\nthat the primary driver of SD's performance improvement is denoising through\nhard pseudo-labels, with the most notable gains observed in moderately sized\ndatasets. We also demonstrate the efficacy of practical heuristics, such as\nearly stopping for extracting meaningful signal and bias fixation for\nimbalanced data. These results provide both theoretical guarantees and\npractical insights, advancing our understanding and application of SD in noisy\nsettings.",
        "Promoting active mobility like cycling relies on the availability of\nwell-connected, high-quality bicycle networks. However, expanding these\nnetworks over an extended planning horizon presents one of the most complex\nchallenges in transport science. This complexity arises from the intricate\ninteractions between infrastructure availability and usage, such as network\nspillover effects and mode choice substitutions. In this paper, we approach the\nproblem from two perspectives: direct optimization methods, which generate\nnear-optimal solutions using operations research techniques, and conceptual\nheuristics, which offer intuitive and scalable algorithms grounded in network\nscience. Specifically, we compare direct welfare optimization with an inverse\nnetwork percolation approach to planning cycle superhighway extensions in\nCopenhagen. Interestingly, while the more complex optimization models yield\nbetter overall welfare results, the improvements over simpler methods are\nsmall. More importantly, we demonstrate that the increased complexity of\nplanning approaches generally makes them more vulnerable to input uncertainty,\nreflecting the bias-variance tradeoff. This issue is particularly relevant in\nthe context of long-term planning, where conditions change during the\nimplementation of the planned infrastructure expansions. Therefore, while\nplanning bicycle infrastructure is important and renders exceptionally high\nbenefit-cost ratios, considerations of robustness and ease of implementation\nmay justify the use of more straightforward network-based methods.",
        "This paper introduces a novel method for solving the single-source\nlocalization problem, specifically addressing the case of trilateration. We\nformulate the problem as a weighted least-squares problem in the squared\ndistances and demonstrate how suitable weights are chosen to accommodate\ndifferent noise distributions. By transforming this formulation into an\neigenvalue problem, we leverage existing eigensolvers to achieve a fast,\nnumerically stable, and easily implemented solver. Furthermore, our theoretical\nanalysis establishes that the globally optimal solution corresponds to the\nlargest real eigenvalue, drawing parallels to the existing literature on the\ntrust-region subproblem. Unlike previous works, we give special treatment to\ndegenerate cases, where multiple and possibly infinitely many solutions exist.\nWe provide a geometric interpretation of the solution sets and design the\nproposed method to handle these cases gracefully. Finally, we validate against\na range of state-of-the-art methods using synthetic and real data,\ndemonstrating how the proposed method is among the fastest and most numerically\nstable.",
        "We model spontaneous Raman scattering noise in polarization-encoded quantum\ncommunication channels co-propagating with classical signals using the\ndepolarization channel. Utilizing NetSquid simulations, we validate the model\nagainst demonstrations of qubit transmission, entanglement distribution, and\nteleportation.",
        "Over-the-air computation (AirComp) has recently emerged as a pivotal\ntechnique for communication-efficient federated learning (FL) in\nresource-constrained wireless networks. Though AirComp leverages the\nsuperposition property of multiple access channels for computation, it\ninherently limits its ability to manage inter-task interference in multi-task\ncomputing. In this paper, we propose a quantized analog beamforming scheme at\nthe receiver to enable simultaneous multi-task FL. Specifically, inspiring by\nthe favorable propagation and channel hardening properties of large-scale\nantenna arrays, a targeted analog beamforming method in closed form is proposed\nfor statistical interference elimination. Analytical results reveal that the\ninterference power vanishes by an order of $\\mathcal{O}\\left(1\/N_r\\right)$ with\nthe number of analog phase shifters, $N_r$, irrespective of their quantization\nprecision. Numerical results demonstrate the effectiveness of the proposed\nanalog beamforming method and show that the performance upper bound of ideal\nlearning without errors can be achieved by increasing the number of\nlow-precision analog phase shifters.",
        "We introduce the first formal model capturing the elicitation of unverifiable\ninformation from a party (the \"source\") with implicit signals derived by other\nplayers (the \"observers\"). Our model is motivated in part by applications in\ndecentralized physical infrastructure networks (a.k.a. \"DePIN\"), an emerging\napplication domain in which physical services (e.g., sensor information,\nbandwidth, or energy) are provided at least in part by untrusted and\nself-interested parties. A key challenge in these signal network applications\nis verifying the level of service that was actually provided by network\nparticipants.\n  We first establish a condition called source identifiability, which we show\nis necessary for the existence of a mechanism for which truthful signal\nreporting is a strict equilibrium. For a converse, we build on techniques from\npeer prediction to show that in every signal network that satisfies the source\nidentifiability condition, there is in fact a strictly truthful mechanism,\nwhere truthful signal reporting gives strictly higher total expected payoff\nthan any less informative equilibrium. We furthermore show that this truthful\nequilibrium is in fact the unique equilibrium of the mechanism if there is\npositive probability that any one observer is unconditionally honest (e.g., if\nan observer were run by the network owner). Also, by extending our condition to\ncoalitions, we show that there are generally no collusion-resistant mechanisms\nin the settings that we consider.\n  We apply our framework and results to two DePIN applications: proving\nlocation, and proving bandwidth. In the location-proving setting observers\nlearn (potentially enlarged) Euclidean distances to the source. Here, our\ncondition has an appealing geometric interpretation, implying that the source's\nlocation can be truthfully elicited if and only if it is guaranteed to lie\ninside the convex hull of the observers.",
        "Molecular dynamics (MD) simulations are essential for understanding\nbiomolecular systems but remain challenging to automate. Recent advances in\nlarge language models (LLM) have demonstrated success in automating complex\nscientific tasks using LLM-based agents. In this paper, we introduce MDCrow, an\nagentic LLM assistant capable of automating MD workflows. MDCrow uses\nchain-of-thought over 40 expert-designed tools for handling and processing\nfiles, setting up simulations, analyzing the simulation outputs, and retrieving\nrelevant information from literature and databases. We assess MDCrow's\nperformance across 25 tasks of varying required subtasks and difficulty, and we\nevaluate the agent's robustness to both difficulty and prompt style.\n\\texttt{gpt-4o} is able to complete complex tasks with low variance, followed\nclosely by \\texttt{llama3-405b}, a compelling open-source model. While prompt\nstyle does not influence the best models' performance, it has significant\neffects on smaller models.",
        "Dense bacterial suspensions exhibit turbulent-like flows at low Reynolds\nnumbers, driven by the activity of the microswimmers. In this study, we develop\na model system to examine these dynamics using motile colloids that mimic\nbacterial locomotion. The colloids are powered by the Quincke instability,\nwhich causes them to spontaneously roll in a random-walk pattern when exposed\nto a square-wave electric field. We experimentally investigate the flow\ndynamics in dense suspensions of these Quincke random walkers under quasi\ntwo-dimensional conditions, where the particle size is comparable to the gap\nbetween the electrodes. Our results reveal an energy spectrum scaling at high\nwavenumbers as $ \\sim k^{-4}$, which holds across a broad range of activity\nlevels -- controlled by the field strength -- and particle concentrations. We\nobserve that velocity time correlations decay within a single period of the\nsquare-wave field, yet an anti-correlation appears between successive field\napplications, indicative of a dynamic structural memory of the ensemble.",
        "It is shown that a compact $n$-dimensional K\\\"ahler manifold with\n$\\frac{n}{2}$-positive Calabi curvature operator has the rational cohomology of\ncomplex projective space. For even $n,$ this is sharp in the sense that the\ncomplex quadric with its symmetric metric has $\\frac{n}{2}$-nonnegative Calabi\ncurvature operator, yet $b_n =2.$ Furthermore, the compact K\\\"ahler manifolds\nwith $\\frac{n}{2}$-nonnegative Calabi curvature operator are completely\nclassified. In addition, the previously known results for the K\\\"ahler\ncurvature operator are improved when the metric is K\\\"ahler-Einstein.",
        "We conduct the first full Bayesian inference analysis for LISA parameter\nestimation incorporating the effects of subdominant harmonics and\nspin-precession through a full time domain response. The substantial\ncomputational demands of using time domain waveforms for LISA are significantly\nmitigated by implementing a novel Python version of the IMRPhenomT family of\nwaveform models and the LISA response with GPU acceleration. This time domain\nresponse alleviates the theoretical necessity of developing specific transfer\nfunctions to approximate the LISA response in the Fourier domain for each\nspecific type of system and allows for the use of unequal arms configurations\nand realistic LISA orbits. Our analysis includes a series of zero-noise\ninjections for a Massive Black Hole Binary with aligned and precessing spins.\nWe investigate the impact of including subdominant harmonics, compare equal and\nunequal arm configurations, and analyze different Time-Delay-Interferometry\n(TDI) configurations. We utilize full and uniform priors, with a lower\nfrequency cutoff of 0.1mHz, and a signal duration of approximately two months,\nsampled every 5 seconds. The sampler is initialized based on Fisher estimates.\nOur results demonstrate LISA capability to measure the two spin magnitudes and\nthe primary spin tilt angle, alongside sky localization, with percent-level\nprecision, while component masses are determined with sub-percent accuracy.",
        "Suppose $\\kappa$ is a regular cardinal and $\\bar a=\\langle \\mu_i: i<\\kappa\n\\rangle$ is a non-decreasing sequence of regular cardinals. We study the set of\npossible cofinalities of cuts Pcut$(\\bar a)=\\{(\\lambda_1, \\lambda_2):$ for some\nultrafilter $D$ on $\\kappa$, $(\\lambda_1, \\lambda_2)$ is the cofinality of a\ncut of $\\prod\\limits_{i<\\kappa} \\mu_i \/ D \\}$.",
        "Scientific discovery relies on scientists generating novel hypotheses that\nundergo rigorous experimental validation. To augment this process, we introduce\nan AI co-scientist, a multi-agent system built on Gemini 2.0. The AI\nco-scientist is intended to help uncover new, original knowledge and to\nformulate demonstrably novel research hypotheses and proposals, building upon\nprior evidence and aligned to scientist-provided research objectives and\nguidance. The system's design incorporates a generate, debate, and evolve\napproach to hypothesis generation, inspired by the scientific method and\naccelerated by scaling test-time compute. Key contributions include: (1) a\nmulti-agent architecture with an asynchronous task execution framework for\nflexible compute scaling; (2) a tournament evolution process for self-improving\nhypotheses generation. Automated evaluations show continued benefits of\ntest-time compute, improving hypothesis quality. While general purpose, we\nfocus development and validation in three biomedical areas: drug repurposing,\nnovel target discovery, and explaining mechanisms of bacterial evolution and\nanti-microbial resistance. For drug repurposing, the system proposes candidates\nwith promising validation findings, including candidates for acute myeloid\nleukemia that show tumor inhibition in vitro at clinically applicable\nconcentrations. For novel target discovery, the AI co-scientist proposed new\nepigenetic targets for liver fibrosis, validated by anti-fibrotic activity and\nliver cell regeneration in human hepatic organoids. Finally, the AI\nco-scientist recapitulated unpublished experimental results via a parallel in\nsilico discovery of a novel gene transfer mechanism in bacterial evolution.\nThese results, detailed in separate, co-timed reports, demonstrate the\npotential to augment biomedical and scientific discovery and usher an era of AI\nempowered scientists.",
        "The one-dimensional toy models proposed for the three-dimensional electron\nmagnetohydrodynamics in our previous work share some similarities with the\noriginal dynamics under certain symmetry. We continue to study the\nwell-posedness issue and explore the potential singularity formation scenario\nfor these models.",
        "The Unit Weibull distribution with parameters $\\alpha$ and $\\beta$ is\nconsidered to study in the context of dual generalized order statistics. For\nthe analysis purpose, Bayes estimators based on symmetric and asymmetric loss\nfunctions are obtained. The methods which are utilized for Bayesian estimation\nare approximation and simulation tools such as Lindley, Tierney-Kadane and\nMarkov chain Monte Carlo methods. The authors have considered squared error\nloss function as symmetric and LINEX and general entropy loss function as\nasymmetric loss functions. After presenting the mathematical results, a\nsimulation study is conducted to exhibit the performances of various derived\nestimators. As this study is considered for the dual generalized order\nstatistics that is unification of models based distinct ordered random variable\nsuch as order statistics, record values, etc. This provides flexibility in our\nresults and in continuation of this, the cotton production data of USA is\nanalyzed for both submodels of ordered random variables: order statistics and\nrecord values."
      ]
    }
  },
  {
    "id":2411.10403,
    "research_type":"applied",
    "start_id":"b14",
    "start_title":"Sparse MRI: The application of compressed sensing for rapid MR imaging",
    "start_abstract":"Abstract The sparsity which is implicit in MR images exploited to significantly undersample k \u2010space. Some such as angiograms are already sparse the pixel representation; other, more complicated have a representation some transform domain\u2013for example, terms of spatial finite\u2010differences or their wavelet coefficients. According recently developed mathematical theory compressed\u2010sensing, with can be recovered from randomly undersampled \u2010space data, provided an appropriate nonlinear recovery scheme used. Intuitively, artifacts due random undersampling add noise\u2010like interference. In domain significant coefficients stand out above A thresholding recover coefficients, effectively recovering image itself. this article, practical incoherent schemes and analyzed by means aliasing Incoherence introduced pseudo\u2010random variable\u2010density phase\u2010encodes. reconstruction performed minimizing \u2113 1 norm transformed image, subject data fidelity constraints. Examples demonstrate improved resolution accelerated acquisition for multislice fast spin\u2010echo brain imaging 3D contrast enhanced angiography. Magn Reson Med, 2007. \u00a9 2007 Wiley\u2010Liss, Inc.",
    "start_categories":[
      "stat.CO",
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b13"
      ],
      "title":[
        "Cardiovascular Flow Measurement with Phase-Contrast MR Imaging: Basic Facts and Implementation"
      ],
      "abstract":[
        "Phase-contrast magnetic resonance (MR) imaging is a well-known but undervalued method of obtaining quantitative information on blood flow. Applications this technique in cardiovascular MR are expanding. According to the sequences available, phase-contrast measurement can be performed breath hold or during normal respiration. Prospective as well retrospective gating techniques used. Common errors include mismatched encoding velocity, deviation plane, inadequate temporal resolution, spatial accelerated flow and misregistration, phase offset errors. Flow measurements most precise if plane perpendicular vessel interest set through-plane The sequence should repeated at least once, with high velocity used initially. If peak has estimated, an adapted velocity. overall error comprises prescription that occur image analysis data. With imaging, reduced less than 10%, acceptable level for routine clinical use."
      ],
      "categories":[
        "q-bio.QM"
      ]
    },
    "list":{
      "title":[
        "Evaluating Answer Reranking Strategies in Time-sensitive Question\n  Answering",
        "On the Potential Galactic Origin of the Ultra-High-Energy Event\n  KM3-230213A",
        "Plantation Monitoring Using Drone Images: A Dataset and Performance\n  Review",
        "ML-Based Optimum Number of CUDA Streams for the GPU Implementation of\n  the Tridiagonal Partition Method",
        "Exact Decoding of Repetition Code under Circuit Level Noise",
        "Audio-to-Image Encoding for Improved Voice Characteristic Detection\n  Using Deep Convolutional Neural Networks",
        "Context-Enhanced Memory-Refined Transformer for Online Action Detection",
        "AI-assisted design of experiments at the frontiers of computation:\n  methods and new perspectives",
        "BioMamba: Leveraging Spectro-Temporal Embedding in Bidirectional Mamba\n  for Enhanced Biosignal Classification",
        "CDI3D: Cross-guided Dense-view Interpolation for 3D Reconstruction",
        "Quantum teleportation between simulated binary black holes",
        "EDGE: Efficient Data Selection for LLM Agents via Guideline\n  Effectiveness",
        "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay\n  Scoring Capabilities of Multimodal Large Language Models",
        "Definable Obstruction Theory",
        "Towards more accurate $B_{(s)}\\rightarrow\\pi(K)$ and\n  $D_{(s)}\\rightarrow\\pi(K)$ form factors",
        "Faster-Than-Nyquist Equalization with Convolutional Neural Networks",
        "Interplay of Kondo Physics with Incommensurate Charge Density Waves in\n  CeTe$_3$",
        "Rescaled Einstein-Gauss-Bonnet Gravity Inflation",
        "Unsupervised CP-UNet Framework for Denoising DAS Data with Decay Noise",
        "A Neural Symbolic Model for Space Physics",
        "Vibration Analysis and Mitigation in Semiconductor Motion Stages Using\n  DMAIC Methodology- A Case Study",
        "Col-OLHTR: A Novel Framework for Multimodal Online Handwritten Text\n  Recognition",
        "A stable phase-locking-free single beam optical lattice with multiple\n  configurations",
        "Enhanced Min-Sum Decoding of Quantum Codes Using Previous Iteration\n  Dynamics",
        "Scaling Semantic Categories: Investigating the Impact on Vision\n  Transformer Labeling Performance",
        "A general approach to quantum integration of cross sections in\n  high-energy physics",
        "Modeling metaorder impact with a Non-Markovian Zero Intelligence model",
        "Physics, Environment and Environmental Education; Perceptions from\n  trainee Natural Science teachers",
        "Leveraging Chain of Thought towards Empathetic Spoken Dialogue without\n  Corresponding Question-Answering Data"
      ],
      "abstract":[
        "Despite advancements in state-of-the-art models and information retrieval\ntechniques, current systems still struggle to handle temporal information and\nto correctly answer detailed questions about past events. In this paper, we\ninvestigate the impact of temporal characteristics of answers in Question\nAnswering (QA) by exploring several simple answer selection techniques. Our\nfindings emphasize the role of temporal features in selecting the most relevant\nanswers from diachronic document collections and highlight differences between\nexplicit and implicit temporal questions.",
        "The KM3NeT observatory detected the most energetic neutrino candidate ever\nobserved, with an energy between 72 PeV and 2.6 EeV at the 90% confidence\nlevel. The observed neutrino is likely of cosmic origin. In this article, it is\ninvestigated if the neutrino could have been produced within the Milky Way.\nConsidering the low fluxes of the Galactic diffuse emission at these energies,\nthe lack of a nearby potential Galactic particle accelerator in the direction\nof the event and the difficulty to accelerate particles to such high energies\nin Galactic systems, we conclude that if the event is indeed cosmic, it is most\nlikely of extragalactic origin.",
        "Automatic monitoring of tree plantations plays a crucial role in agriculture.\nFlawless monitoring of tree health helps farmers make informed decisions\nregarding their management by taking appropriate action. Use of drone images\nfor automatic plantation monitoring can enhance the accuracy of the monitoring\nprocess, while still being affordable to small farmers in developing countries\nsuch as India. Small, low cost drones equipped with an RGB camera can capture\nhigh-resolution images of agricultural fields, allowing for detailed analysis\nof the well-being of the plantations. Existing methods of automated plantation\nmonitoring are mostly based on satellite images, which are difficult to get for\nthe farmers. We propose an automated system for plantation health monitoring\nusing drone images, which are becoming easier to get for the farmers. We\npropose a dataset of images of trees with three categories: ``Good health\",\n``Stunted\", and ``Dead\". We annotate the dataset using CVAT annotation tool,\nfor use in research purposes. We experiment with different well-known CNN\nmodels to observe their performance on the proposed dataset. The initial low\naccuracy levels show the complexity of the proposed dataset. Further, our study\nrevealed that, depth-wise convolution operation embedded in a deep CNN model,\ncan enhance the performance of the model on drone dataset. Further, we apply\nstate-of-the-art object detection models to identify individual trees to better\nmonitor them automatically.",
        "This paper presents a heuristic for finding the optimum number of CUDA\nstreams by using tools common to the modern AI-oriented approaches and applied\nto the parallel partition algorithm. A time complexity model for the GPU\nrealization of the partition method is built. Further, a refined time\ncomplexity model for the partition algorithm being executed on multiple CUDA\nstreams is formulated. Computational experiments for different SLAE sizes are\nconducted, and the optimum number of CUDA streams for each of them is found\nempirically. Based on the collected data a model for the sum of the times for\nthe non-dominant GPU operations (that take part in the stream overlap) is\nformulated using regression analysis. A fitting non-linear model for the\noverhead time connected with the creation of CUDA streams is created.\nStatistical analysis is done for all the built models. An algorithm for finding\nthe optimum number of CUDA streams is formulated. Using this algorithm,\ntogether with the two models mentioned above, predictions for the optimum\nnumber of CUDA streams are made. Comparing the predicted values with the actual\ndata, the algorithm is deemed to be acceptably good.",
        "Repetition code forms a fundamental basis for quantum error correction\nexperiments. To date, it stands as the sole code that has achieved large\ndistances and extremely low error rates. Its applications span the spectrum of\nevaluating hardware limitations, pinpointing hardware defects, and detecting\nrare events. However, current methods for decoding repetition codes under\ncircuit level noise are suboptimal, leading to inaccurate error correction\nthresholds and introducing additional errors in event detection. In this work,\nwe establish that repetition code under circuit level noise has an exact\nsolution, and we propose an optimal maximum likelihood decoding algorithm\ncalled planar. The algorithm is based on the exact solution of the spin glass\npartition function on planar graphs and has polynomial computational\ncomplexity. Through extensive numerical experiments, we demonstrate that our\nalgorithm uncovers the exact threshold for depolarizing noise and realistic\nsuperconductor SI1000 noise. Furthermore, we apply our method to analyze data\nfrom recent quantum memory experiments conducted by Google Quantum AI,\nrevealing that part of the error floor was attributed to the decoding algorithm\nused by Google. Finally, we implemented the repetition code quantum memory on\nsuperconducting systems with a 72-qubit quantum chip lacking reset gates,\ndemonstrating that even with an unknown error model, the proposed algorithm\nachieves a significantly lower logical error rate than the matching-based\nalgorithm.",
        "This paper introduces a novel audio-to-image encoding framework that\nintegrates multiple dimensions of voice characteristics into a single RGB image\nfor speaker recognition. In this method, the green channel encodes raw audio\ndata, the red channel embeds statistical descriptors of the voice signal\n(including key metrics such as median and mean values for fundamental\nfrequency, spectral centroid, bandwidth, rolloff, zero-crossing rate, MFCCs,\nRMS energy, spectral flatness, spectral contrast, chroma, and harmonic-to-noise\nratio), and the blue channel comprises subframes representing these features in\na spatially organized format. A deep convolutional neural network trained on\nthese composite images achieves 98% accuracy in speaker classification across\ntwo speakers, suggesting that this integrated multi-channel representation can\nprovide a more discriminative input for voice recognition tasks.",
        "Online Action Detection (OAD) detects actions in streaming videos using past\nobservations. State-of-the-art OAD approaches model past observations and their\ninteractions with an anticipated future. The past is encoded using short- and\nlong-term memories to capture immediate and long-range dependencies, while\nanticipation compensates for missing future context. We identify a\ntraining-inference discrepancy in existing OAD methods that hinders learning\neffectiveness. The training uses varying lengths of short-term memory, while\ninference relies on a full-length short-term memory. As a remedy, we propose a\nContext-enhanced Memory-Refined Transformer (CMeRT). CMeRT introduces a\ncontext-enhanced encoder to improve frame representations using additional\nnear-past context. It also features a memory-refined decoder to leverage\nnear-future generation to enhance performance. CMeRT achieves state-of-the-art\nin online detection and anticipation on THUMOS'14, CrossTask, and\nEPIC-Kitchens-100.",
        "Designing the next generation colliders and detectors involves solving\noptimization problems in high-dimensional spaces where the optimal solutions\nmay nest in regions that even a team of expert humans would not explore.\n  Resorting to Artificial Intelligence to assist the experimental design\nintroduces however significant computational challenges in terms of generation\nand processing of the data required to perform such optimizations: from the\nsoftware point of view, differentiable programming makes the exploration of\nsuch spaces with gradient descent feasible; from the hardware point of view,\nthe complexity of the resulting models and their optimization is prohibitive.\nTo scale up to the complexity of the typical HEP collider experiment, a change\nin paradigma is required.\n  In this contribution I will describe the first proofs-of-concept of\ngradient-based optimization of experimental design and implementations in\nneuromorphic hardware architectures, paving the way to more complex challenges.",
        "Biological signals, such as electroencephalograms (EEGs) and\nelectrocardiograms (ECGs), play a pivotal role in numerous clinical practices,\nsuch as diagnosing brain and cardiac arrhythmic diseases. Existing methods for\nbiosignal classification rely on Attention-based frameworks with dense Feed\nForward layers, which lead to inefficient learning, high computational\noverhead, and suboptimal performance. In this work, we introduce BioMamba, a\nSpectro-Temporal Embedding strategy applied to the Bidirectional Mamba\nframework with Sparse Feed Forward layers to enable effective learning of\nbiosignal sequences. By integrating these three key components, BioMamba\neffectively addresses the limitations of existing methods. Extensive\nexperiments demonstrate that BioMamba significantly outperforms\nstate-of-the-art methods with marked improvement in classification performance.\nThe advantages of the proposed BioMamba include (1) Reliability: BioMamba\nconsistently delivers robust results, confirmed across six evaluation metrics.\n(2) Efficiency: We assess both model and training efficiency, the BioMamba\ndemonstrates computational effectiveness by reducing model size and resource\nconsumption compared to existing approaches. (3) Generality: With the capacity\nto effectively classify a diverse set of tasks, BioMamba demonstrates\nadaptability and effectiveness across various domains and applications.",
        "3D object reconstruction from single-view image is a fundamental task in\ncomputer vision with wide-ranging applications. Recent advancements in Large\nReconstruction Models (LRMs) have shown great promise in leveraging multi-view\nimages generated by 2D diffusion models to extract 3D content. However,\nchallenges remain as 2D diffusion models often struggle to produce dense images\nwith strong multi-view consistency, and LRMs tend to amplify these\ninconsistencies during the 3D reconstruction process. Addressing these issues\nis critical for achieving high-quality and efficient 3D reconstruction. In this\npaper, we present CDI3D, a feed-forward framework designed for efficient,\nhigh-quality image-to-3D generation with view interpolation. To tackle the\naforementioned challenges, we propose to integrate 2D diffusion-based view\ninterpolation into the LRM pipeline to enhance the quality and consistency of\nthe generated mesh. Specifically, our approach introduces a Dense View\nInterpolation (DVI) module, which synthesizes interpolated images between main\nviews generated by the 2D diffusion model, effectively densifying the input\nviews with better multi-view consistency. We also design a tilt camera pose\ntrajectory to capture views with different elevations and perspectives.\nSubsequently, we employ a tri-plane-based mesh reconstruction strategy to\nextract robust tokens from these interpolated and original views, enabling the\ngeneration of high-quality 3D meshes with superior texture and geometry.\nExtensive experiments demonstrate that our method significantly outperforms\nprevious state-of-the-art approaches across various benchmarks, producing 3D\ncontent with enhanced texture fidelity and geometric accuracy.",
        "The quantum description of a black hole predicts that quantum information\nhidden behind the event horizon can be teleported outside almost\ninstantaneously. In this work, we demonstrate that a chiral spin-chain model,\nwhich naturally simulates a binary black hole system, can realise this\nteleportation process. Our system captures two essential components of this\nprotocol: Hawking radiation, which generates the necessary entanglement between\nthe black holes, and optimal scrambling, which enables high-fidelity\nteleportation on short timescales. Through numerical simulations, we quantify\nthe key timescales governing the process, including the Page time, radiation\ntime, scrambling time, and butterfly velocity, showing their universal\ndependence on the chiral coupling strength. Our results establish the\nfeasibility of simulating quantum properties of black holes within condensed\nmatter systems, offering an experimentally accessible platform for probing\notherwise inaccessible high-energy phenomena.",
        "Large Language Models (LLMs) have shown remarkable capabilities as AI agents.\nHowever, existing methods for enhancing LLM-agent abilities often lack a focus\non data quality, leading to inefficiencies and suboptimal results in both\nfine-tuning and prompt engineering. To address this issue, we introduce EDGE, a\nnovel approach for identifying informative samples without needing golden\nanswers. We propose the Guideline Effectiveness (GE) metric, which selects\nchallenging samples by measuring the impact of human-provided guidelines in\nmulti-turn interaction tasks. A low GE score indicates that the human expertise\nrequired for a sample is missing from the guideline, making the sample more\ninformative. By selecting samples with low GE scores, we can improve the\nefficiency and outcomes of both prompt engineering and fine-tuning processes\nfor LLMs. Extensive experiments validate the performance of our method. Our\nmethod achieves competitive results on the HotpotQA and WebShop and datasets,\nrequiring 75\\% and 50\\% less data, respectively, while outperforming existing\nmethods. We also provide a fresh perspective on the data quality of LLM-agent\nfine-tuning.",
        "Automated Essay Scoring (AES) plays a crucial role in educational assessment\nby providing scalable and consistent evaluations of writing tasks. However,\ntraditional AES systems face three major challenges: (1) reliance on\nhandcrafted features that limit generalizability, (2) difficulty in capturing\nfine-grained traits like coherence and argumentation, and (3) inability to\nhandle multimodal contexts. In the era of Multimodal Large Language Models\n(MLLMs), we propose EssayJudge, the first multimodal benchmark to evaluate AES\ncapabilities across lexical-, sentence-, and discourse-level traits. By\nleveraging MLLMs' strengths in trait-specific scoring and multimodal context\nunderstanding, EssayJudge aims to offer precise, context-rich evaluations\nwithout manual feature engineering, addressing longstanding AES limitations.\nOur experiments with 18 representative MLLMs reveal gaps in AES performance\ncompared to human evaluation, particularly in discourse-level traits,\nhighlighting the need for further advancements in MLLM-based AES research. Our\ndataset and code will be available upon acceptance.",
        "A series of recent papers by Bergfalk, Lupini and Panagiotopoulus developed\nthe foundations of a field known as `definable algebraic topology,' in which\nclassical cohomological invariants are enriched by viewing them as groups with\na Polish cover. This allows one to apply techniques from descriptive set theory\nto the study of cohomology theories. In this paper, we will establish a\n`definable' version of a classical theorem from obstruction theory, and use\nthis to study the potential complexity of the homotopy relation on the space of\ncontinuous maps $C(X, |K|)$, where $X$ is a locally compact Polish space, and K\nis a locally finite countable simplicial complex. We will also characterize the\nSolecki Groups of the Cech cohomology of X, which are the canonical chain of\nsubgroups with a Polish cover that are least among those of a given complexity.",
        "We present progress on the calculation of scalar, vector, and tensor form\nfactors for the following meson decays: $B\\rightarrow\\pi$, $B_s\\rightarrow K$,\n$D\\rightarrow\\pi$ and $D_s\\rightarrow K$. This calculation uses the MILC HISQ\ngluon field ensembles with HISQ valence quarks. We generate ensembles of\ncorrelator data with varying lattice spacings, some as small as 0.044 fm. Some\nensembles have a strange-to-light quark mass ratio of 5:1 and others use the\nphysical light quark mass. The fully-relativistic, heavy-HISQ approach is used\nfor the heavy quark, with simulation masses ranging from the charm to near the\nbottom. This heavy-HISQ approach provides nearly full coverage of the kinematic\nrange.",
        "Faster-than-Nyquist (FTN) signaling aims at improving the spectral efficiency\nof wireless communication systems by exceeding the boundaries set by the\nNyquist-Shannon sampling theorem. 50 years after its first introduction in the\nscientific literature, wireless communications have significantly changed, but\nspectral efficiency remains one of the key challenges. To adopt FTN signaling,\ninter-symbol interference (ISI) patterns need to be equalized at the receiver.\nMotivated by the pattern recognition capabilities of convolutional neural\nnetworks with skip connections, we propose such deep learning architecture for\nISI equalization and symbol demodulation in FTN receivers. We investigate the\nperformance of the proposed model considering quadrature phase shift keying\nmodulation and low density parity check coding, and compare it to a set of\nbenchmarks, including frequency-domain equalization, a\nquadratic-programming-based receiver, and an equalization scheme based on a\ndeep neural network. We show that our receiver outperforms any benchmark,\nachieving error rates comparable to those in additive white Gaussian noise\nchannel, and higher effective throughput, thanks to the increased spectral\nefficiency of FTN signaling. With a compression factor of 60% and code rate\n3\/4, the proposed model achieves a peak effective throughput of 2.5 Mbps at\njust 10dB of energy per bit over noise power spectral density ratio, with other\nreceivers being limited by error floors due to the strong inter-symbol\ninterference. To promote reproducibility in deep learning for wireless\ncommunications, our code is open source at the repository provided in the\nreferences.",
        "CeTe$_3$ is a 2-dimensional (2D) Van der Waals (VdW) material with\nincommensurate charge density waves (CDW), extremely high transition\ntemperature ($T_{CDW}$) and a large momentum-dependent CDW gap that leaves a\nsignificant portion of the Fermi surface intact. It is also considered to be a\nweak Kondo system, a property unexpected for a material with incommensurate\nCDW, where each atomic site is slightly different. Here, we study the\nproperties of the CDW state in several RTe$_3$ (R is rare earth) materials and\nexamine the hybridization of itinerant states with the localized Ce $4f$\nmultiplet in CeTe$_3$ by using angle resolved photoemission spectroscopy\n(ARPES). We find that the renormalization of the itinerant states originating\nfrom the hybridization with the localized $4f$ states at $-260$ meV extends to\nthe Fermi level. This, with remnants of another localized state at the Fermi\nlevel, supports the characterization of CeTe$_3$ as a weak Kondo material.\nFurthermore, we uncover a $k$-dependence of the hybridization with the states\n$-260$ meV, indicating that similar effect could be the reason for discrepancy\nbetween the heavy masses in specific heat and light ones in Shubnikov de Haas\noscillations observed in other heavy fermion materials.",
        "We study the inflationary phenomenology of a rescaled Einstein-Gauss-Bonnet\ngravity. In this framework, the gravitational constant of the Einstein-Hilbert\nterm is rescaled due to effective terms active in the high curvature era.\nBasically, the total theory is an $F(R,G,\\phi)$ theory with the Gauss-Bonnet\npart contributing only a non-minimal coupling to the scalar field, so it is a\ntheory with string theory origins and with a non-trivial $F(R)$ gravity part.\nThe $F(R)$ gravity part in the high curvature regime contributes only a\nrescaled Einstein-Hilbert term and thus the resulting theory is effectively a\nrescaled version of a standard Einstein-Gauss-Bonnet theory. We develop the\nformalism of rescaled Einstein-Gauss-Bonnet gravity, taking in account the\nGW170817 constraints on the gravitational wave speed. We show explicitly how\nthe rescaled theory affects directly the primordial scalar and tensor\nperturbations, and how the slow-roll and observational indices of inflation are\naffected by the rescaling of the theory. We perform a thorough phenomenological\nanalysis of several models of interest and we show that is it possible to\nobtain viable inflationary theories compatible with the latest Planck data.\nAlso among the studied models there are cases that yield a relatively large\nblue tilted tensor spectral index and we demonstrate that these models can lead\nto detectable primordial gravitational waves in the future gravitational wave\nexperiments. Some of the scenarios examined, for specific values of the\nreheating temperature may be detectable by SKA, LISA, BBO, DECIGO and the\nEinstein Telescope.",
        "Distributed acoustic sensor (DAS) technology leverages optical fiber cables\nto detect acoustic signals, providing cost-effective and dense monitoring\ncapabilities. It offers several advantages including resistance to extreme\nconditions, immunity to electromagnetic interference, and accurate detection.\nHowever, DAS typically exhibits a lower signal-to-noise ratio (S\/N) compared to\ngeophones and is susceptible to various noise types, such as random noise,\nerratic noise, level noise, and long-period noise. This reduced S\/N can\nnegatively impact data analyses containing inversion and interpretation. While\nartificial intelligence has demonstrated excellent denoising capabilities, most\nexisting methods rely on supervised learning with labeled data, which imposes\nstringent requirements on the quality of the labels. To address this issue, we\ndevelop a label-free unsupervised learning (UL) network model based on\nContext-Pyramid-UNet (CP-UNet) to suppress erratic and random noises in DAS\ndata. The CP-UNet utilizes the Context Pyramid Module in the encoding and\ndecoding process to extract features and reconstruct the DAS data. To enhance\nthe connectivity between shallow and deep features, we add a Connected Module\n(CM) to both encoding and decoding section. Layer Normalization (LN) is\nutilized to replace the commonly employed Batch Normalization (BN),\naccelerating the convergence of the model and preventing gradient explosion\nduring training. Huber-loss is adopted as our loss function whose parameters\nare experimentally determined. We apply the network to both the 2-D synthetic\nand filed data. Comparing to traditional denoising methods and the latest UL\nframework, our proposed method demonstrates superior noise reduction\nperformance.",
        "In this study, we unveil a new AI model, termed PhyE2E, to discover physical\nformulas through symbolic regression. PhyE2E simplifies symbolic regression by\ndecomposing it into sub-problems using the second-order derivatives of an\noracle neural network, and employs a transformer model to translate data into\nsymbolic formulas in an end-to-end manner. The resulting formulas are refined\nthrough Monte-Carlo Tree Search and Genetic Programming. We leverage a large\nlanguage model to synthesize extensive symbolic expressions resembling real\nphysics, and train the model to recover these formulas directly from data. A\ncomprehensive evaluation reveals that PhyE2E outperforms existing\nstate-of-the-art approaches, delivering superior symbolic accuracy, precision\nin data fitting, and consistency in physical units. We deployed PhyE2E to five\napplications in space physics, including the prediction of sunspot numbers,\nsolar rotational angular velocity, emission line contribution functions,\nnear-Earth plasma pressure, and lunar-tide plasma signals. The physical\nformulas generated by AI demonstrate a high degree of accuracy in fitting the\nexperimental data from satellites and astronomical telescopes. We have\nsuccessfully upgraded the formula proposed by NASA in 1993 regarding solar\nactivity, and for the first time, provided the explanations for the long cycle\nof solar activity in an explicit form. We also found that the decay of\nnear-Earth plasma pressure is proportional to r^2 to Earth, where subsequent\nmathematical derivations are consistent with satellite data from another\nindependent study. Moreover, we found physical formulas that can describe the\nrelationships between emission lines in the extreme ultraviolet spectrum of the\nSun, temperatures, electron densities, and magnetic fields. The formula\nobtained is consistent with the properties that physicists had previously\nhypothesized it should possess.",
        "Motion stages are critical in semiconductor manufacturing equipment for\nprocesses like die bonding, wafer loading, and chip packaging, as their\nperformance must meet the industry's stringent precision requirements.\nVibration, a significant yet often overlooked adversary to precision motion\nstages, is challenging to identify and mitigate due to its subtle nature. This\nstudy, conducted at a motion stage manufacturer facing frequent\nvibration-related complaints, proposes a novel approach to resolving vibration\nissues. By leveraging the DMAIC methodology, it introduces VIBGUARD, an active\nvibration monitoring and mitigation solution, instead of solely focusing on\ntraditional hardware vibration control. This comprehensive strategy enhances\nvalue and competitiveness, increasing UPH (units per hour) by 15.3% from 8,500\nto 9,800 and reducing downtime by 68.2% from 2.2 to 0.7 occurrences per month.\nThis case study and the DMAIC methodology offer valuable resources for quality\ncontrol and problem analysis in the semiconductor industry.",
        "Online Handwritten Text Recognition (OLHTR) has gained considerable attention\nfor its diverse range of applications. Current approaches usually treat OLHTR\nas a sequence recognition task, employing either a single trajectory or image\nencoder, or multi-stream encoders, combined with a CTC or attention-based\nrecognition decoder. However, these approaches face several drawbacks: 1)\nsingle encoders typically focus on either local trajectories or visual regions,\nlacking the ability to dynamically capture relevant global features in\nchallenging cases; 2) multi-stream encoders, while more comprehensive, suffer\nfrom complex structures and increased inference costs. To tackle this, we\npropose a Collaborative learning-based OLHTR framework, called Col-OLHTR, that\nlearns multimodal features during training while maintaining a single-stream\ninference process. Col-OLHTR consists of a trajectory encoder, a\nPoint-to-Spatial Alignment (P2SA) module, and an attention-based decoder. The\nP2SA module is designed to learn image-level spatial features through\ntrajectory-encoded features and 2D rotary position embeddings. During training,\nan additional image-stream encoder-decoder is collaboratively trained to\nprovide supervision for P2SA features. At inference, the extra streams are\ndiscarded, and only the P2SA module is used and merged before the decoder,\nsimplifying the process while preserving high performance. Extensive\nexperimental results on several OLHTR benchmarks demonstrate the\nstate-of-the-art (SOTA) performance, proving the effectiveness and robustness\nof our design.",
        "Optical lattices formed by interfering laser beams are widely used to trap\nand manipulate atoms for quantum simulation, metrology, and computation. To\nstabilize optical lattices in experiments, it is usually challenging to\nimplement delicate phase-locking systems with complicated optics and\nelectronics to reduce the relative phase fluctuation of multiple laser beams.\nHere we report a phase-locking-free scheme to implement optical lattices by\npassing a single laser beam through a prism with n-fold symmetric facets and\nlarge apex angles. The scheme ensures a stable optical lattice since the\ninterference occurs among different deflected parts of a single laser beam\nwithout any moving component. Various lattice configurations, including a\ntriangular lattice and a quasi-crystal lattice with ten-fold symmetry are\ndemonstrated. In both cases, stability measurements show a change of lattice\nconstant in less than 1.14%, and a drift of lattice position in less than\n1.61%.",
        "In this paper, we propose a novel message-passing decoding approach that\nleverages the degeneracy of quantum low-density parity-check codes to enhance\ndecoding performance, eliminating the need for serial scheduling or\npost-processing. Our focus is on two-block Calderbank-Shor-Steane (CSS) codes,\nwhich are composed of symmetric stabilizers that hinder the performance of\nconventional iterative decoders with uniform update rules. Specifically, our\nanalysis shows that, under the isolation assumption, the min-sum decoder fails\nto converge when constant-weight errors are applied to symmetric stabilizers,\nas variable-to-check messages oscillate in every iteration. To address this, we\nintroduce a decoding technique that exploits this oscillatory property by\napplying distinct update rules: variable nodes in one block utilize messages\nfrom previous iterations, while those in the other block are updated\nconventionally. Logical error-rate results demonstrate that the proposed\ndecoder significantly outperforms the normalized min-sum decoder and achieves\ncompetitive performance with belief propagation enhanced by order-zero ordered\nstatistics decoding, all while maintaining linear complexity in the code's\nblock length.",
        "This study explores the impact of scaling semantic categories on the image\nclassification performance of vision transformers (ViTs). In this specific\ncase, the CLIP server provided by Jina AI is used for experimentation. The\nresearch hypothesizes that as the number of ground truth and artificially\nintroduced semantically equivalent categories increases, the labeling accuracy\nof ViTs improves until a theoretical maximum or limit is reached. A wide\nvariety of image datasets were chosen to test this hypothesis. These datasets\nwere processed through a custom function in Python designed to evaluate the\nmodel's accuracy, with adjustments being made to account for format differences\nbetween datasets. By exponentially introducing new redundant categories, the\nexperiment assessed accuracy trends until they plateaued, decreased, or\nfluctuated inconsistently. The findings show that while semantic scaling\ninitially increases model performance, the benefits diminish or reverse after\nsurpassing a critical threshold, providing insight into the limitations and\npossible optimization of category labeling strategies for ViTs.",
        "We present universal \\emph{building blocks} for the quantum integration of\ngeneric cross sections in high-energy physics. We make use of Fourier Quantum\nMonte Carlo Integration as implemented in {\\sc Quantinuum}'s Quantum Monte\nCarlo Integration engine to provide an extendable methodology for generating\nefficient circuits that can implement generic cross-section calculations,\nproviding a quadratic speed-up in root mean-squared error convergence with\nrespect to classical Monte Carlo integration. We focus on a concrete example of\na $1\\to 3$ decay process to illustrate our work.",
        "Devising models of the limit order book that realistically reproduce the\nmarket response to exogenous trades is extremely challenging and fundamental in\norder to test trading strategies. We propose a novel explainable model for\nsmall tick assets, the Non-Markovian Zero Intelligence, which is a variant of\nthe well-known Zero Intelligence model. The main modification is that the\nprobability of limit orders' signs (buy\/sell) is not constant but is a function\nof the exponentially weighted mid-price return, representing the past price\ndynamics, and can be interpreted as the reaction of traders with reservation\nprices to the price trend. With numerical simulations and analytical arguments,\nwe show that the model predicts a concave price path during a metaorder\nexecution and to a price reversion after the execution ends, as empirically\nobserved. We analyze in-depth the mechanism at the root of the arising\nconcavity, the components which constitute the price impact in our model, and\nthe dependence of the results on the two main parameters, namely the time scale\nand the strength of the reaction of traders to the price trend.",
        "Environmental Education (EE) is vital for shaping citizens who understand and\nvalue sustainability as an epistemological and practical alternative to\nmitigate current environmental issues. This research was prompted by the\nexploration of the relationship between EE and the physical sciences,\nconnections that are often overlooked in curriculums and in the teaching\nprocesses of both this science and EE. It is essential to emphasize that\nphysics provides conceptual frameworks and methodological tools that can\nenhance the understanding of environmental phenomena from a broad and\nmultidimensional perspective. To delve into these connections, a study with a\nhermeneutic interpretative nuance was conducted. Through a questionnaire, the\nperceptions of prospective teachers in the natural sciences field regarding\nthis topic were gathered. The findings revealed that a significant number of\nthem recognize and value the correlation between physics and EE. From their\nperspective, this linkage is not only crucial for a comprehensive view of\nenvironmental dynamics but also to encourage students to develop critical,\narticulated, and well-founded thinking about environmental balance. The\nresearch also highlighted the didactic opportunities presented when\nintertwining physics with EE. By associating physical concepts with real\nenvironmental issues, learning can be reinforced, making it meaningful and\nenduring over time. This interdisciplinary fusion also holds the potential to\nincrease students' motivation and interest, fostering a more active and engaged\nattitude in their educational journey",
        "Empathetic dialogue is crucial for natural human-computer interaction,\nallowing the dialogue system to respond in a more personalized and emotionally\naware manner, improving user satisfaction and engagement. The emergence of\nlarge language models (LLMs) has revolutionized dialogue generation by\nharnessing their powerful capabilities and shown its potential in multimodal\ndomains. Many studies have integrated speech with text-based LLMs to take\nspeech question as input and output text response. However, the lack of spoken\nquestion-answering datasets that include speech style information to supervised\nfine-tuning (SFT) limits the performance of these systems. As a result, while\nthese systems excel at understanding speech content, they often struggle to\ngenerate empathetic responses. In response, we propose a novel approach that\ncircumvents the need for question-answering data, called Listen, Perceive, and\nExpress (LPE). Our method employs a two-stage training process, initially\nguiding the LLM to listen the content and perceive the emotional aspects of\nspeech. Subsequently, we utilize Chain-of-Thought (CoT) prompting to unlock the\nmodel's potential for expressing empathetic responses based on listened spoken\ncontent and perceived emotional cues. We employ experiments to prove the\neffectiveness of proposed method. To our knowledge, this is the first attempt\nto leverage CoT for speech-based dialogue."
      ]
    }
  },
  {
    "id":2411.10403,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"MoDL: Model-Based Deep Learning Architecture for Inverse Problems",
    "start_abstract":"We introduce a model-based image reconstruction framework with convolution neural network (CNN)-based regularization prior. The proposed formulation provides systematic approach for deriving deep architectures inverse problems the arbitrary structure. Since forward model is explicitly accounted for, smaller fewer parameters sufficient to capture information compared direct inversion approaches. Thus, reducing demand training data and time. we rely on end-to-end weight sharing across iterations, CNN weights are customized model, thus offering improved performance over approaches that pre-trained denoisers. Our experiments show decoupling of number iterations from complexity offered by this benefits, including lower data, reduced risk overfitting, implementations significantly memory footprint. propose enforce data-consistency using numerical optimization blocks, such as conjugate gradients algorithm within network. This offers faster convergence per iteration, methods proximal steps consistency. translates performance, primarily when available GPU restricts iterations.",
    "start_categories":[
      "stat.CO",
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science",
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b13"
      ],
      "title":[
        "Cardiovascular Flow Measurement with Phase-Contrast MR Imaging: Basic Facts and Implementation"
      ],
      "abstract":[
        "Phase-contrast magnetic resonance (MR) imaging is a well-known but undervalued method of obtaining quantitative information on blood flow. Applications this technique in cardiovascular MR are expanding. According to the sequences available, phase-contrast measurement can be performed breath hold or during normal respiration. Prospective as well retrospective gating techniques used. Common errors include mismatched encoding velocity, deviation plane, inadequate temporal resolution, spatial accelerated flow and misregistration, phase offset errors. Flow measurements most precise if plane perpendicular vessel interest set through-plane The sequence should repeated at least once, with high velocity used initially. If peak has estimated, an adapted velocity. overall error comprises prescription that occur image analysis data. With imaging, reduced less than 10%, acceptable level for routine clinical use."
      ],
      "categories":[
        "q-bio.QM"
      ]
    },
    "list":{
      "title":[
        "Evaluating Answer Reranking Strategies in Time-sensitive Question\n  Answering",
        "On the Potential Galactic Origin of the Ultra-High-Energy Event\n  KM3-230213A",
        "Plantation Monitoring Using Drone Images: A Dataset and Performance\n  Review",
        "ML-Based Optimum Number of CUDA Streams for the GPU Implementation of\n  the Tridiagonal Partition Method",
        "Exact Decoding of Repetition Code under Circuit Level Noise",
        "Audio-to-Image Encoding for Improved Voice Characteristic Detection\n  Using Deep Convolutional Neural Networks",
        "Context-Enhanced Memory-Refined Transformer for Online Action Detection",
        "AI-assisted design of experiments at the frontiers of computation:\n  methods and new perspectives",
        "BioMamba: Leveraging Spectro-Temporal Embedding in Bidirectional Mamba\n  for Enhanced Biosignal Classification",
        "CDI3D: Cross-guided Dense-view Interpolation for 3D Reconstruction",
        "Quantum teleportation between simulated binary black holes",
        "EDGE: Efficient Data Selection for LLM Agents via Guideline\n  Effectiveness",
        "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay\n  Scoring Capabilities of Multimodal Large Language Models",
        "Definable Obstruction Theory",
        "Towards more accurate $B_{(s)}\\rightarrow\\pi(K)$ and\n  $D_{(s)}\\rightarrow\\pi(K)$ form factors",
        "Faster-Than-Nyquist Equalization with Convolutional Neural Networks",
        "Interplay of Kondo Physics with Incommensurate Charge Density Waves in\n  CeTe$_3$",
        "Rescaled Einstein-Gauss-Bonnet Gravity Inflation",
        "Unsupervised CP-UNet Framework for Denoising DAS Data with Decay Noise",
        "A Neural Symbolic Model for Space Physics",
        "Vibration Analysis and Mitigation in Semiconductor Motion Stages Using\n  DMAIC Methodology- A Case Study",
        "Col-OLHTR: A Novel Framework for Multimodal Online Handwritten Text\n  Recognition",
        "A stable phase-locking-free single beam optical lattice with multiple\n  configurations",
        "Enhanced Min-Sum Decoding of Quantum Codes Using Previous Iteration\n  Dynamics",
        "Scaling Semantic Categories: Investigating the Impact on Vision\n  Transformer Labeling Performance",
        "A general approach to quantum integration of cross sections in\n  high-energy physics",
        "Modeling metaorder impact with a Non-Markovian Zero Intelligence model",
        "Physics, Environment and Environmental Education; Perceptions from\n  trainee Natural Science teachers",
        "Leveraging Chain of Thought towards Empathetic Spoken Dialogue without\n  Corresponding Question-Answering Data"
      ],
      "abstract":[
        "Despite advancements in state-of-the-art models and information retrieval\ntechniques, current systems still struggle to handle temporal information and\nto correctly answer detailed questions about past events. In this paper, we\ninvestigate the impact of temporal characteristics of answers in Question\nAnswering (QA) by exploring several simple answer selection techniques. Our\nfindings emphasize the role of temporal features in selecting the most relevant\nanswers from diachronic document collections and highlight differences between\nexplicit and implicit temporal questions.",
        "The KM3NeT observatory detected the most energetic neutrino candidate ever\nobserved, with an energy between 72 PeV and 2.6 EeV at the 90% confidence\nlevel. The observed neutrino is likely of cosmic origin. In this article, it is\ninvestigated if the neutrino could have been produced within the Milky Way.\nConsidering the low fluxes of the Galactic diffuse emission at these energies,\nthe lack of a nearby potential Galactic particle accelerator in the direction\nof the event and the difficulty to accelerate particles to such high energies\nin Galactic systems, we conclude that if the event is indeed cosmic, it is most\nlikely of extragalactic origin.",
        "Automatic monitoring of tree plantations plays a crucial role in agriculture.\nFlawless monitoring of tree health helps farmers make informed decisions\nregarding their management by taking appropriate action. Use of drone images\nfor automatic plantation monitoring can enhance the accuracy of the monitoring\nprocess, while still being affordable to small farmers in developing countries\nsuch as India. Small, low cost drones equipped with an RGB camera can capture\nhigh-resolution images of agricultural fields, allowing for detailed analysis\nof the well-being of the plantations. Existing methods of automated plantation\nmonitoring are mostly based on satellite images, which are difficult to get for\nthe farmers. We propose an automated system for plantation health monitoring\nusing drone images, which are becoming easier to get for the farmers. We\npropose a dataset of images of trees with three categories: ``Good health\",\n``Stunted\", and ``Dead\". We annotate the dataset using CVAT annotation tool,\nfor use in research purposes. We experiment with different well-known CNN\nmodels to observe their performance on the proposed dataset. The initial low\naccuracy levels show the complexity of the proposed dataset. Further, our study\nrevealed that, depth-wise convolution operation embedded in a deep CNN model,\ncan enhance the performance of the model on drone dataset. Further, we apply\nstate-of-the-art object detection models to identify individual trees to better\nmonitor them automatically.",
        "This paper presents a heuristic for finding the optimum number of CUDA\nstreams by using tools common to the modern AI-oriented approaches and applied\nto the parallel partition algorithm. A time complexity model for the GPU\nrealization of the partition method is built. Further, a refined time\ncomplexity model for the partition algorithm being executed on multiple CUDA\nstreams is formulated. Computational experiments for different SLAE sizes are\nconducted, and the optimum number of CUDA streams for each of them is found\nempirically. Based on the collected data a model for the sum of the times for\nthe non-dominant GPU operations (that take part in the stream overlap) is\nformulated using regression analysis. A fitting non-linear model for the\noverhead time connected with the creation of CUDA streams is created.\nStatistical analysis is done for all the built models. An algorithm for finding\nthe optimum number of CUDA streams is formulated. Using this algorithm,\ntogether with the two models mentioned above, predictions for the optimum\nnumber of CUDA streams are made. Comparing the predicted values with the actual\ndata, the algorithm is deemed to be acceptably good.",
        "Repetition code forms a fundamental basis for quantum error correction\nexperiments. To date, it stands as the sole code that has achieved large\ndistances and extremely low error rates. Its applications span the spectrum of\nevaluating hardware limitations, pinpointing hardware defects, and detecting\nrare events. However, current methods for decoding repetition codes under\ncircuit level noise are suboptimal, leading to inaccurate error correction\nthresholds and introducing additional errors in event detection. In this work,\nwe establish that repetition code under circuit level noise has an exact\nsolution, and we propose an optimal maximum likelihood decoding algorithm\ncalled planar. The algorithm is based on the exact solution of the spin glass\npartition function on planar graphs and has polynomial computational\ncomplexity. Through extensive numerical experiments, we demonstrate that our\nalgorithm uncovers the exact threshold for depolarizing noise and realistic\nsuperconductor SI1000 noise. Furthermore, we apply our method to analyze data\nfrom recent quantum memory experiments conducted by Google Quantum AI,\nrevealing that part of the error floor was attributed to the decoding algorithm\nused by Google. Finally, we implemented the repetition code quantum memory on\nsuperconducting systems with a 72-qubit quantum chip lacking reset gates,\ndemonstrating that even with an unknown error model, the proposed algorithm\nachieves a significantly lower logical error rate than the matching-based\nalgorithm.",
        "This paper introduces a novel audio-to-image encoding framework that\nintegrates multiple dimensions of voice characteristics into a single RGB image\nfor speaker recognition. In this method, the green channel encodes raw audio\ndata, the red channel embeds statistical descriptors of the voice signal\n(including key metrics such as median and mean values for fundamental\nfrequency, spectral centroid, bandwidth, rolloff, zero-crossing rate, MFCCs,\nRMS energy, spectral flatness, spectral contrast, chroma, and harmonic-to-noise\nratio), and the blue channel comprises subframes representing these features in\na spatially organized format. A deep convolutional neural network trained on\nthese composite images achieves 98% accuracy in speaker classification across\ntwo speakers, suggesting that this integrated multi-channel representation can\nprovide a more discriminative input for voice recognition tasks.",
        "Online Action Detection (OAD) detects actions in streaming videos using past\nobservations. State-of-the-art OAD approaches model past observations and their\ninteractions with an anticipated future. The past is encoded using short- and\nlong-term memories to capture immediate and long-range dependencies, while\nanticipation compensates for missing future context. We identify a\ntraining-inference discrepancy in existing OAD methods that hinders learning\neffectiveness. The training uses varying lengths of short-term memory, while\ninference relies on a full-length short-term memory. As a remedy, we propose a\nContext-enhanced Memory-Refined Transformer (CMeRT). CMeRT introduces a\ncontext-enhanced encoder to improve frame representations using additional\nnear-past context. It also features a memory-refined decoder to leverage\nnear-future generation to enhance performance. CMeRT achieves state-of-the-art\nin online detection and anticipation on THUMOS'14, CrossTask, and\nEPIC-Kitchens-100.",
        "Designing the next generation colliders and detectors involves solving\noptimization problems in high-dimensional spaces where the optimal solutions\nmay nest in regions that even a team of expert humans would not explore.\n  Resorting to Artificial Intelligence to assist the experimental design\nintroduces however significant computational challenges in terms of generation\nand processing of the data required to perform such optimizations: from the\nsoftware point of view, differentiable programming makes the exploration of\nsuch spaces with gradient descent feasible; from the hardware point of view,\nthe complexity of the resulting models and their optimization is prohibitive.\nTo scale up to the complexity of the typical HEP collider experiment, a change\nin paradigma is required.\n  In this contribution I will describe the first proofs-of-concept of\ngradient-based optimization of experimental design and implementations in\nneuromorphic hardware architectures, paving the way to more complex challenges.",
        "Biological signals, such as electroencephalograms (EEGs) and\nelectrocardiograms (ECGs), play a pivotal role in numerous clinical practices,\nsuch as diagnosing brain and cardiac arrhythmic diseases. Existing methods for\nbiosignal classification rely on Attention-based frameworks with dense Feed\nForward layers, which lead to inefficient learning, high computational\noverhead, and suboptimal performance. In this work, we introduce BioMamba, a\nSpectro-Temporal Embedding strategy applied to the Bidirectional Mamba\nframework with Sparse Feed Forward layers to enable effective learning of\nbiosignal sequences. By integrating these three key components, BioMamba\neffectively addresses the limitations of existing methods. Extensive\nexperiments demonstrate that BioMamba significantly outperforms\nstate-of-the-art methods with marked improvement in classification performance.\nThe advantages of the proposed BioMamba include (1) Reliability: BioMamba\nconsistently delivers robust results, confirmed across six evaluation metrics.\n(2) Efficiency: We assess both model and training efficiency, the BioMamba\ndemonstrates computational effectiveness by reducing model size and resource\nconsumption compared to existing approaches. (3) Generality: With the capacity\nto effectively classify a diverse set of tasks, BioMamba demonstrates\nadaptability and effectiveness across various domains and applications.",
        "3D object reconstruction from single-view image is a fundamental task in\ncomputer vision with wide-ranging applications. Recent advancements in Large\nReconstruction Models (LRMs) have shown great promise in leveraging multi-view\nimages generated by 2D diffusion models to extract 3D content. However,\nchallenges remain as 2D diffusion models often struggle to produce dense images\nwith strong multi-view consistency, and LRMs tend to amplify these\ninconsistencies during the 3D reconstruction process. Addressing these issues\nis critical for achieving high-quality and efficient 3D reconstruction. In this\npaper, we present CDI3D, a feed-forward framework designed for efficient,\nhigh-quality image-to-3D generation with view interpolation. To tackle the\naforementioned challenges, we propose to integrate 2D diffusion-based view\ninterpolation into the LRM pipeline to enhance the quality and consistency of\nthe generated mesh. Specifically, our approach introduces a Dense View\nInterpolation (DVI) module, which synthesizes interpolated images between main\nviews generated by the 2D diffusion model, effectively densifying the input\nviews with better multi-view consistency. We also design a tilt camera pose\ntrajectory to capture views with different elevations and perspectives.\nSubsequently, we employ a tri-plane-based mesh reconstruction strategy to\nextract robust tokens from these interpolated and original views, enabling the\ngeneration of high-quality 3D meshes with superior texture and geometry.\nExtensive experiments demonstrate that our method significantly outperforms\nprevious state-of-the-art approaches across various benchmarks, producing 3D\ncontent with enhanced texture fidelity and geometric accuracy.",
        "The quantum description of a black hole predicts that quantum information\nhidden behind the event horizon can be teleported outside almost\ninstantaneously. In this work, we demonstrate that a chiral spin-chain model,\nwhich naturally simulates a binary black hole system, can realise this\nteleportation process. Our system captures two essential components of this\nprotocol: Hawking radiation, which generates the necessary entanglement between\nthe black holes, and optimal scrambling, which enables high-fidelity\nteleportation on short timescales. Through numerical simulations, we quantify\nthe key timescales governing the process, including the Page time, radiation\ntime, scrambling time, and butterfly velocity, showing their universal\ndependence on the chiral coupling strength. Our results establish the\nfeasibility of simulating quantum properties of black holes within condensed\nmatter systems, offering an experimentally accessible platform for probing\notherwise inaccessible high-energy phenomena.",
        "Large Language Models (LLMs) have shown remarkable capabilities as AI agents.\nHowever, existing methods for enhancing LLM-agent abilities often lack a focus\non data quality, leading to inefficiencies and suboptimal results in both\nfine-tuning and prompt engineering. To address this issue, we introduce EDGE, a\nnovel approach for identifying informative samples without needing golden\nanswers. We propose the Guideline Effectiveness (GE) metric, which selects\nchallenging samples by measuring the impact of human-provided guidelines in\nmulti-turn interaction tasks. A low GE score indicates that the human expertise\nrequired for a sample is missing from the guideline, making the sample more\ninformative. By selecting samples with low GE scores, we can improve the\nefficiency and outcomes of both prompt engineering and fine-tuning processes\nfor LLMs. Extensive experiments validate the performance of our method. Our\nmethod achieves competitive results on the HotpotQA and WebShop and datasets,\nrequiring 75\\% and 50\\% less data, respectively, while outperforming existing\nmethods. We also provide a fresh perspective on the data quality of LLM-agent\nfine-tuning.",
        "Automated Essay Scoring (AES) plays a crucial role in educational assessment\nby providing scalable and consistent evaluations of writing tasks. However,\ntraditional AES systems face three major challenges: (1) reliance on\nhandcrafted features that limit generalizability, (2) difficulty in capturing\nfine-grained traits like coherence and argumentation, and (3) inability to\nhandle multimodal contexts. In the era of Multimodal Large Language Models\n(MLLMs), we propose EssayJudge, the first multimodal benchmark to evaluate AES\ncapabilities across lexical-, sentence-, and discourse-level traits. By\nleveraging MLLMs' strengths in trait-specific scoring and multimodal context\nunderstanding, EssayJudge aims to offer precise, context-rich evaluations\nwithout manual feature engineering, addressing longstanding AES limitations.\nOur experiments with 18 representative MLLMs reveal gaps in AES performance\ncompared to human evaluation, particularly in discourse-level traits,\nhighlighting the need for further advancements in MLLM-based AES research. Our\ndataset and code will be available upon acceptance.",
        "A series of recent papers by Bergfalk, Lupini and Panagiotopoulus developed\nthe foundations of a field known as `definable algebraic topology,' in which\nclassical cohomological invariants are enriched by viewing them as groups with\na Polish cover. This allows one to apply techniques from descriptive set theory\nto the study of cohomology theories. In this paper, we will establish a\n`definable' version of a classical theorem from obstruction theory, and use\nthis to study the potential complexity of the homotopy relation on the space of\ncontinuous maps $C(X, |K|)$, where $X$ is a locally compact Polish space, and K\nis a locally finite countable simplicial complex. We will also characterize the\nSolecki Groups of the Cech cohomology of X, which are the canonical chain of\nsubgroups with a Polish cover that are least among those of a given complexity.",
        "We present progress on the calculation of scalar, vector, and tensor form\nfactors for the following meson decays: $B\\rightarrow\\pi$, $B_s\\rightarrow K$,\n$D\\rightarrow\\pi$ and $D_s\\rightarrow K$. This calculation uses the MILC HISQ\ngluon field ensembles with HISQ valence quarks. We generate ensembles of\ncorrelator data with varying lattice spacings, some as small as 0.044 fm. Some\nensembles have a strange-to-light quark mass ratio of 5:1 and others use the\nphysical light quark mass. The fully-relativistic, heavy-HISQ approach is used\nfor the heavy quark, with simulation masses ranging from the charm to near the\nbottom. This heavy-HISQ approach provides nearly full coverage of the kinematic\nrange.",
        "Faster-than-Nyquist (FTN) signaling aims at improving the spectral efficiency\nof wireless communication systems by exceeding the boundaries set by the\nNyquist-Shannon sampling theorem. 50 years after its first introduction in the\nscientific literature, wireless communications have significantly changed, but\nspectral efficiency remains one of the key challenges. To adopt FTN signaling,\ninter-symbol interference (ISI) patterns need to be equalized at the receiver.\nMotivated by the pattern recognition capabilities of convolutional neural\nnetworks with skip connections, we propose such deep learning architecture for\nISI equalization and symbol demodulation in FTN receivers. We investigate the\nperformance of the proposed model considering quadrature phase shift keying\nmodulation and low density parity check coding, and compare it to a set of\nbenchmarks, including frequency-domain equalization, a\nquadratic-programming-based receiver, and an equalization scheme based on a\ndeep neural network. We show that our receiver outperforms any benchmark,\nachieving error rates comparable to those in additive white Gaussian noise\nchannel, and higher effective throughput, thanks to the increased spectral\nefficiency of FTN signaling. With a compression factor of 60% and code rate\n3\/4, the proposed model achieves a peak effective throughput of 2.5 Mbps at\njust 10dB of energy per bit over noise power spectral density ratio, with other\nreceivers being limited by error floors due to the strong inter-symbol\ninterference. To promote reproducibility in deep learning for wireless\ncommunications, our code is open source at the repository provided in the\nreferences.",
        "CeTe$_3$ is a 2-dimensional (2D) Van der Waals (VdW) material with\nincommensurate charge density waves (CDW), extremely high transition\ntemperature ($T_{CDW}$) and a large momentum-dependent CDW gap that leaves a\nsignificant portion of the Fermi surface intact. It is also considered to be a\nweak Kondo system, a property unexpected for a material with incommensurate\nCDW, where each atomic site is slightly different. Here, we study the\nproperties of the CDW state in several RTe$_3$ (R is rare earth) materials and\nexamine the hybridization of itinerant states with the localized Ce $4f$\nmultiplet in CeTe$_3$ by using angle resolved photoemission spectroscopy\n(ARPES). We find that the renormalization of the itinerant states originating\nfrom the hybridization with the localized $4f$ states at $-260$ meV extends to\nthe Fermi level. This, with remnants of another localized state at the Fermi\nlevel, supports the characterization of CeTe$_3$ as a weak Kondo material.\nFurthermore, we uncover a $k$-dependence of the hybridization with the states\n$-260$ meV, indicating that similar effect could be the reason for discrepancy\nbetween the heavy masses in specific heat and light ones in Shubnikov de Haas\noscillations observed in other heavy fermion materials.",
        "We study the inflationary phenomenology of a rescaled Einstein-Gauss-Bonnet\ngravity. In this framework, the gravitational constant of the Einstein-Hilbert\nterm is rescaled due to effective terms active in the high curvature era.\nBasically, the total theory is an $F(R,G,\\phi)$ theory with the Gauss-Bonnet\npart contributing only a non-minimal coupling to the scalar field, so it is a\ntheory with string theory origins and with a non-trivial $F(R)$ gravity part.\nThe $F(R)$ gravity part in the high curvature regime contributes only a\nrescaled Einstein-Hilbert term and thus the resulting theory is effectively a\nrescaled version of a standard Einstein-Gauss-Bonnet theory. We develop the\nformalism of rescaled Einstein-Gauss-Bonnet gravity, taking in account the\nGW170817 constraints on the gravitational wave speed. We show explicitly how\nthe rescaled theory affects directly the primordial scalar and tensor\nperturbations, and how the slow-roll and observational indices of inflation are\naffected by the rescaling of the theory. We perform a thorough phenomenological\nanalysis of several models of interest and we show that is it possible to\nobtain viable inflationary theories compatible with the latest Planck data.\nAlso among the studied models there are cases that yield a relatively large\nblue tilted tensor spectral index and we demonstrate that these models can lead\nto detectable primordial gravitational waves in the future gravitational wave\nexperiments. Some of the scenarios examined, for specific values of the\nreheating temperature may be detectable by SKA, LISA, BBO, DECIGO and the\nEinstein Telescope.",
        "Distributed acoustic sensor (DAS) technology leverages optical fiber cables\nto detect acoustic signals, providing cost-effective and dense monitoring\ncapabilities. It offers several advantages including resistance to extreme\nconditions, immunity to electromagnetic interference, and accurate detection.\nHowever, DAS typically exhibits a lower signal-to-noise ratio (S\/N) compared to\ngeophones and is susceptible to various noise types, such as random noise,\nerratic noise, level noise, and long-period noise. This reduced S\/N can\nnegatively impact data analyses containing inversion and interpretation. While\nartificial intelligence has demonstrated excellent denoising capabilities, most\nexisting methods rely on supervised learning with labeled data, which imposes\nstringent requirements on the quality of the labels. To address this issue, we\ndevelop a label-free unsupervised learning (UL) network model based on\nContext-Pyramid-UNet (CP-UNet) to suppress erratic and random noises in DAS\ndata. The CP-UNet utilizes the Context Pyramid Module in the encoding and\ndecoding process to extract features and reconstruct the DAS data. To enhance\nthe connectivity between shallow and deep features, we add a Connected Module\n(CM) to both encoding and decoding section. Layer Normalization (LN) is\nutilized to replace the commonly employed Batch Normalization (BN),\naccelerating the convergence of the model and preventing gradient explosion\nduring training. Huber-loss is adopted as our loss function whose parameters\nare experimentally determined. We apply the network to both the 2-D synthetic\nand filed data. Comparing to traditional denoising methods and the latest UL\nframework, our proposed method demonstrates superior noise reduction\nperformance.",
        "In this study, we unveil a new AI model, termed PhyE2E, to discover physical\nformulas through symbolic regression. PhyE2E simplifies symbolic regression by\ndecomposing it into sub-problems using the second-order derivatives of an\noracle neural network, and employs a transformer model to translate data into\nsymbolic formulas in an end-to-end manner. The resulting formulas are refined\nthrough Monte-Carlo Tree Search and Genetic Programming. We leverage a large\nlanguage model to synthesize extensive symbolic expressions resembling real\nphysics, and train the model to recover these formulas directly from data. A\ncomprehensive evaluation reveals that PhyE2E outperforms existing\nstate-of-the-art approaches, delivering superior symbolic accuracy, precision\nin data fitting, and consistency in physical units. We deployed PhyE2E to five\napplications in space physics, including the prediction of sunspot numbers,\nsolar rotational angular velocity, emission line contribution functions,\nnear-Earth plasma pressure, and lunar-tide plasma signals. The physical\nformulas generated by AI demonstrate a high degree of accuracy in fitting the\nexperimental data from satellites and astronomical telescopes. We have\nsuccessfully upgraded the formula proposed by NASA in 1993 regarding solar\nactivity, and for the first time, provided the explanations for the long cycle\nof solar activity in an explicit form. We also found that the decay of\nnear-Earth plasma pressure is proportional to r^2 to Earth, where subsequent\nmathematical derivations are consistent with satellite data from another\nindependent study. Moreover, we found physical formulas that can describe the\nrelationships between emission lines in the extreme ultraviolet spectrum of the\nSun, temperatures, electron densities, and magnetic fields. The formula\nobtained is consistent with the properties that physicists had previously\nhypothesized it should possess.",
        "Motion stages are critical in semiconductor manufacturing equipment for\nprocesses like die bonding, wafer loading, and chip packaging, as their\nperformance must meet the industry's stringent precision requirements.\nVibration, a significant yet often overlooked adversary to precision motion\nstages, is challenging to identify and mitigate due to its subtle nature. This\nstudy, conducted at a motion stage manufacturer facing frequent\nvibration-related complaints, proposes a novel approach to resolving vibration\nissues. By leveraging the DMAIC methodology, it introduces VIBGUARD, an active\nvibration monitoring and mitigation solution, instead of solely focusing on\ntraditional hardware vibration control. This comprehensive strategy enhances\nvalue and competitiveness, increasing UPH (units per hour) by 15.3% from 8,500\nto 9,800 and reducing downtime by 68.2% from 2.2 to 0.7 occurrences per month.\nThis case study and the DMAIC methodology offer valuable resources for quality\ncontrol and problem analysis in the semiconductor industry.",
        "Online Handwritten Text Recognition (OLHTR) has gained considerable attention\nfor its diverse range of applications. Current approaches usually treat OLHTR\nas a sequence recognition task, employing either a single trajectory or image\nencoder, or multi-stream encoders, combined with a CTC or attention-based\nrecognition decoder. However, these approaches face several drawbacks: 1)\nsingle encoders typically focus on either local trajectories or visual regions,\nlacking the ability to dynamically capture relevant global features in\nchallenging cases; 2) multi-stream encoders, while more comprehensive, suffer\nfrom complex structures and increased inference costs. To tackle this, we\npropose a Collaborative learning-based OLHTR framework, called Col-OLHTR, that\nlearns multimodal features during training while maintaining a single-stream\ninference process. Col-OLHTR consists of a trajectory encoder, a\nPoint-to-Spatial Alignment (P2SA) module, and an attention-based decoder. The\nP2SA module is designed to learn image-level spatial features through\ntrajectory-encoded features and 2D rotary position embeddings. During training,\nan additional image-stream encoder-decoder is collaboratively trained to\nprovide supervision for P2SA features. At inference, the extra streams are\ndiscarded, and only the P2SA module is used and merged before the decoder,\nsimplifying the process while preserving high performance. Extensive\nexperimental results on several OLHTR benchmarks demonstrate the\nstate-of-the-art (SOTA) performance, proving the effectiveness and robustness\nof our design.",
        "Optical lattices formed by interfering laser beams are widely used to trap\nand manipulate atoms for quantum simulation, metrology, and computation. To\nstabilize optical lattices in experiments, it is usually challenging to\nimplement delicate phase-locking systems with complicated optics and\nelectronics to reduce the relative phase fluctuation of multiple laser beams.\nHere we report a phase-locking-free scheme to implement optical lattices by\npassing a single laser beam through a prism with n-fold symmetric facets and\nlarge apex angles. The scheme ensures a stable optical lattice since the\ninterference occurs among different deflected parts of a single laser beam\nwithout any moving component. Various lattice configurations, including a\ntriangular lattice and a quasi-crystal lattice with ten-fold symmetry are\ndemonstrated. In both cases, stability measurements show a change of lattice\nconstant in less than 1.14%, and a drift of lattice position in less than\n1.61%.",
        "In this paper, we propose a novel message-passing decoding approach that\nleverages the degeneracy of quantum low-density parity-check codes to enhance\ndecoding performance, eliminating the need for serial scheduling or\npost-processing. Our focus is on two-block Calderbank-Shor-Steane (CSS) codes,\nwhich are composed of symmetric stabilizers that hinder the performance of\nconventional iterative decoders with uniform update rules. Specifically, our\nanalysis shows that, under the isolation assumption, the min-sum decoder fails\nto converge when constant-weight errors are applied to symmetric stabilizers,\nas variable-to-check messages oscillate in every iteration. To address this, we\nintroduce a decoding technique that exploits this oscillatory property by\napplying distinct update rules: variable nodes in one block utilize messages\nfrom previous iterations, while those in the other block are updated\nconventionally. Logical error-rate results demonstrate that the proposed\ndecoder significantly outperforms the normalized min-sum decoder and achieves\ncompetitive performance with belief propagation enhanced by order-zero ordered\nstatistics decoding, all while maintaining linear complexity in the code's\nblock length.",
        "This study explores the impact of scaling semantic categories on the image\nclassification performance of vision transformers (ViTs). In this specific\ncase, the CLIP server provided by Jina AI is used for experimentation. The\nresearch hypothesizes that as the number of ground truth and artificially\nintroduced semantically equivalent categories increases, the labeling accuracy\nof ViTs improves until a theoretical maximum or limit is reached. A wide\nvariety of image datasets were chosen to test this hypothesis. These datasets\nwere processed through a custom function in Python designed to evaluate the\nmodel's accuracy, with adjustments being made to account for format differences\nbetween datasets. By exponentially introducing new redundant categories, the\nexperiment assessed accuracy trends until they plateaued, decreased, or\nfluctuated inconsistently. The findings show that while semantic scaling\ninitially increases model performance, the benefits diminish or reverse after\nsurpassing a critical threshold, providing insight into the limitations and\npossible optimization of category labeling strategies for ViTs.",
        "We present universal \\emph{building blocks} for the quantum integration of\ngeneric cross sections in high-energy physics. We make use of Fourier Quantum\nMonte Carlo Integration as implemented in {\\sc Quantinuum}'s Quantum Monte\nCarlo Integration engine to provide an extendable methodology for generating\nefficient circuits that can implement generic cross-section calculations,\nproviding a quadratic speed-up in root mean-squared error convergence with\nrespect to classical Monte Carlo integration. We focus on a concrete example of\na $1\\to 3$ decay process to illustrate our work.",
        "Devising models of the limit order book that realistically reproduce the\nmarket response to exogenous trades is extremely challenging and fundamental in\norder to test trading strategies. We propose a novel explainable model for\nsmall tick assets, the Non-Markovian Zero Intelligence, which is a variant of\nthe well-known Zero Intelligence model. The main modification is that the\nprobability of limit orders' signs (buy\/sell) is not constant but is a function\nof the exponentially weighted mid-price return, representing the past price\ndynamics, and can be interpreted as the reaction of traders with reservation\nprices to the price trend. With numerical simulations and analytical arguments,\nwe show that the model predicts a concave price path during a metaorder\nexecution and to a price reversion after the execution ends, as empirically\nobserved. We analyze in-depth the mechanism at the root of the arising\nconcavity, the components which constitute the price impact in our model, and\nthe dependence of the results on the two main parameters, namely the time scale\nand the strength of the reaction of traders to the price trend.",
        "Environmental Education (EE) is vital for shaping citizens who understand and\nvalue sustainability as an epistemological and practical alternative to\nmitigate current environmental issues. This research was prompted by the\nexploration of the relationship between EE and the physical sciences,\nconnections that are often overlooked in curriculums and in the teaching\nprocesses of both this science and EE. It is essential to emphasize that\nphysics provides conceptual frameworks and methodological tools that can\nenhance the understanding of environmental phenomena from a broad and\nmultidimensional perspective. To delve into these connections, a study with a\nhermeneutic interpretative nuance was conducted. Through a questionnaire, the\nperceptions of prospective teachers in the natural sciences field regarding\nthis topic were gathered. The findings revealed that a significant number of\nthem recognize and value the correlation between physics and EE. From their\nperspective, this linkage is not only crucial for a comprehensive view of\nenvironmental dynamics but also to encourage students to develop critical,\narticulated, and well-founded thinking about environmental balance. The\nresearch also highlighted the didactic opportunities presented when\nintertwining physics with EE. By associating physical concepts with real\nenvironmental issues, learning can be reinforced, making it meaningful and\nenduring over time. This interdisciplinary fusion also holds the potential to\nincrease students' motivation and interest, fostering a more active and engaged\nattitude in their educational journey",
        "Empathetic dialogue is crucial for natural human-computer interaction,\nallowing the dialogue system to respond in a more personalized and emotionally\naware manner, improving user satisfaction and engagement. The emergence of\nlarge language models (LLMs) has revolutionized dialogue generation by\nharnessing their powerful capabilities and shown its potential in multimodal\ndomains. Many studies have integrated speech with text-based LLMs to take\nspeech question as input and output text response. However, the lack of spoken\nquestion-answering datasets that include speech style information to supervised\nfine-tuning (SFT) limits the performance of these systems. As a result, while\nthese systems excel at understanding speech content, they often struggle to\ngenerate empathetic responses. In response, we propose a novel approach that\ncircumvents the need for question-answering data, called Listen, Perceive, and\nExpress (LPE). Our method employs a two-stage training process, initially\nguiding the LLM to listen the content and perceive the emotional aspects of\nspeech. Subsequently, we utilize Chain-of-Thought (CoT) prompting to unlock the\nmodel's potential for expressing empathetic responses based on listened spoken\ncontent and perceived emotional cues. We employ experiments to prove the\neffectiveness of proposed method. To our knowledge, this is the first attempt\nto leverage CoT for speech-based dialogue."
      ]
    }
  },
  {
    "id":2411.10403,
    "research_type":"applied",
    "start_id":"b13",
    "start_title":"Cardiovascular Flow Measurement with Phase-Contrast MR Imaging: Basic Facts and Implementation",
    "start_abstract":"Phase-contrast magnetic resonance (MR) imaging is a well-known but undervalued method of obtaining quantitative information on blood flow. Applications this technique in cardiovascular MR are expanding. According to the sequences available, phase-contrast measurement can be performed breath hold or during normal respiration. Prospective as well retrospective gating techniques used. Common errors include mismatched encoding velocity, deviation plane, inadequate temporal resolution, spatial accelerated flow and misregistration, phase offset errors. Flow measurements most precise if plane perpendicular vessel interest set through-plane The sequence should repeated at least once, with high velocity used initially. If peak has estimated, an adapted velocity. overall error comprises prescription that occur image analysis data. With imaging, reduced less than 10%, acceptable level for routine clinical use.",
    "start_categories":[
      "q-bio.QM"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b14",
        "b1"
      ],
      "title":[
        "Sparse MRI: The application of compressed sensing for rapid MR imaging",
        "MoDL: Model-Based Deep Learning Architecture for Inverse Problems"
      ],
      "abstract":[
        "Abstract The sparsity which is implicit in MR images exploited to significantly undersample k \u2010space. Some such as angiograms are already sparse the pixel representation; other, more complicated have a representation some transform domain\u2013for example, terms of spatial finite\u2010differences or their wavelet coefficients. According recently developed mathematical theory compressed\u2010sensing, with can be recovered from randomly undersampled \u2010space data, provided an appropriate nonlinear recovery scheme used. Intuitively, artifacts due random undersampling add noise\u2010like interference. In domain significant coefficients stand out above A thresholding recover coefficients, effectively recovering image itself. this article, practical incoherent schemes and analyzed by means aliasing Incoherence introduced pseudo\u2010random variable\u2010density phase\u2010encodes. reconstruction performed minimizing \u2113 1 norm transformed image, subject data fidelity constraints. Examples demonstrate improved resolution accelerated acquisition for multislice fast spin\u2010echo brain imaging 3D contrast enhanced angiography. Magn Reson Med, 2007. \u00a9 2007 Wiley\u2010Liss, Inc.",
        "We introduce a model-based image reconstruction framework with convolution neural network (CNN)-based regularization prior. The proposed formulation provides systematic approach for deriving deep architectures inverse problems the arbitrary structure. Since forward model is explicitly accounted for, smaller fewer parameters sufficient to capture information compared direct inversion approaches. Thus, reducing demand training data and time. we rely on end-to-end weight sharing across iterations, CNN weights are customized model, thus offering improved performance over approaches that pre-trained denoisers. Our experiments show decoupling of number iterations from complexity offered by this benefits, including lower data, reduced risk overfitting, implementations significantly memory footprint. propose enforce data-consistency using numerical optimization blocks, such as conjugate gradients algorithm within network. This offers faster convergence per iteration, methods proximal steps consistency. translates performance, primarily when available GPU restricts iterations."
      ],
      "categories":[
        "stat.CO",
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "CausalGeD: Blending Causality and Diffusion for Spatial Gene Expression\n  Generation",
        "Pressure-Induced Structural and Dielectric Changes in Liquid Water at\n  Room Temperature",
        "AugGen: Synthetic Augmentation Can Improve Discriminative Models",
        "CAPOS: The bulge Cluster APOGEE Survey VII: First detailed chemical\n  analysis of NGC 6316",
        "Single-Satellite-Based Geolocation of Broadcast GNSS Spoofers from Low\n  Earth Orbit",
        "The Impact of Building-Induced Visibility Restrictions on Intersection\n  Accidents",
        "Scientific literature cited in patents: A Technology Transfer indicator\n  in Portuguese universities",
        "An atomistic approach for modeling of polarizability and Raman\n  scattering of water clusters and liquid water",
        "VEGA: Voids idEntification using Genetic Algorithm",
        "Sobol-CPI: a Doubly Robust Conditional Permutation Importance Statistic",
        "AdaCS: Adaptive Normalization for Enhanced Code-Switching ASR",
        "An Optimal Cascade Feature-Level Spatiotemporal Fusion Strategy for\n  Anomaly Detection in CAN Bus",
        "X-ray Thomson scattering studies on spin-singlet stabilization of highly\n  compressed H-like Be ions heated to two million degrees Kelvin",
        "Joint Optimization of Resource Allocation and Radar Receiver Selection\n  in Integrated Communication-Radar Systems",
        "Personalize Your LLM: Fake it then Align it",
        "New envelope equations for shallow water waves and modulational\n  instability",
        "Numerical analysis of a finite volume method for a 1-D wave equation\n  with non smooth wave speed and localized Kelvin-Voigt damping",
        "Measurement-Based Modeling and Analysis of UAV Air-Ground Channels at 1\n  and 4 GHz",
        "Direct Detection of Fast-Moving Low-Mass Dark Matter",
        "Join the Chat: How Curiosity Sparks Participation in Telegram Groups",
        "On zero-sum Ramsey numbers modulo 3",
        "Towards Explainable Spoofed Speech Attribution and Detection:a\n  Probabilistic Approach for Characterizing Speech Synthesizer Components",
        "A Minimalist Example of Edge-of-Stability and Progressive Sharpening",
        "Traversable Wormhole in AdS and Entanglement",
        "MoireDB: Formula-generated Interference-fringe Image Dataset",
        "Chiral and deconfinement thermal transitions at finite quark spin\n  polarization in lattice QCD simulations",
        "Continuous Observability Assurance in Cloud-Native Applications",
        "Towards shell model interactions with credible uncertainties"
      ],
      "abstract":[
        "The integration of single-cell RNA sequencing (scRNA-seq) and spatial\ntranscriptomics (ST) data is crucial for understanding gene expression in\nspatial context. Existing methods for such integration have limited\nperformance, with structural similarity often below 60\\%, We attribute this\nlimitation to the failure to consider causal relationships between genes. We\npresent CausalGeD, which combines diffusion and autoregressive processes to\nleverage these relationships. By generalizing the Causal Attention Transformer\nfrom image generation to gene expression data, our model captures regulatory\nmechanisms without predefined relationships. Across 10 tissue datasets,\nCausalGeD outperformed state-of-the-art baselines by 5- 32\\% in key metrics,\nincluding Pearson's correlation and structural similarity, advancing both\ntechnical and biological insights.",
        "Understanding the pressure-dependent dielectric properties of water is\ncrucial for a wide range of scientific and practical applications. In this\nstudy, we employ a deep neural network trained on density functional theory\ndata to investigate the dielectric properties of liquid water at room\ntemperature across a pressure range of 0.1 MPa to 1000 MPa. We observe a\nnonlinear increase in the static dielectric constant $\\epsilon_0$ with\nincreasing pressure, a trend that is qualitatively consistent with experimental\nobservations. This increase in $\\epsilon_0$ is primarily attributed to the\nincrease in water density under compression, which enhances collective dipole\nfluctuations within the hydrogen-bonding network as well as the dielectric\nresponse. Despite the increase in $\\epsilon_0$, our results reveal a decrease\nin the Kirkwood correlation factor $G_K$ with increasing pressure. This\ndecrease in $G_K$ is attributed to pressure-induced structural distortions in\nthe hydrogen-bonding network, which weaken dipolar correlations by disrupting\nthe ideal tetrahedral arrangement of water molecules.",
        "The increasing dependence on large-scale datasets in machine learning\nintroduces significant privacy and ethical challenges. Synthetic data\ngeneration offers a promising solution; however, most current methods rely on\nexternal datasets or pre-trained models, which add complexity and escalate\nresource demands. In this work, we introduce a novel self-contained synthetic\naugmentation technique that strategically samples from a conditional generative\nmodel trained exclusively on the target dataset. This approach eliminates the\nneed for auxiliary data sources. Applied to face recognition datasets, our\nmethod achieves 1--12\\% performance improvements on the IJB-C and IJB-B\nbenchmarks. It outperforms models trained solely on real data and exceeds the\nperformance of state-of-the-art synthetic data generation baselines. Notably,\nthese enhancements often surpass those achieved through architectural\nimprovements, underscoring the significant impact of synthetic augmentation in\ndata-scarce environments. These findings demonstrate that carefully integrated\nsynthetic data not only addresses privacy and resource constraints but also\nsubstantially boosts model performance. Project page\nhttps:\/\/parsa-ra.github.io\/auggen",
        "As part of the bulge Cluster APOgee Survey (CAPOS), high-resolution, high\nSignal-to-Noise Ratio Near-Infrared spectroscopy, we aim to conduct the most\nrobust chemical study to date for NGC 6316, deriving abundances for a number of\nelements with a variety of nucleosynthetic origins, most of which have never\nbeen studied before in this cluster. We use the Brussels Automatic Code for\nCharacterizing High accuracy Spectra (BACCHUS) with atmospheric parameters\nphotometrically obtained in order to determine, for the first time, abundances\nfor C, N, O, Mg, Al, Si, P, K, Ca, Ti, V, Cr, Mn, Fe, Ni and Ce for this\ncluster. We obtained a mean metallicity [Fe\/H] = -0.87 +- 0.02, finding no\nindication of an intrinsic metallicity spread. Our metallicity agrees with the\nmost recent values from other studies, revising earlier values that were ~0.5\ndex metal-richer. With this new value, this cluster, long believed to be a\nmember of the classical metal-rich group of bulge GCs around -0.5, now falls in\nthe dominant bulge globular cluster peak around [Fe\/H] = -1. The cluster\npresents a clear C-N anticorrelation. We also found a [{\\alpha}\/Fe] = 0.3 +-\n0.02. Our abundances show similar behaviour to other in situ globular clusters\nwith comparable metallicity. We obtained E(B-V) = 0.71 and (M-m)_0 = 15.32 +-\n0.05 by isochrone fitting, in good agreement with the recent determinations\nfrom other works. We derive an overall metallicity [M\/H] = -0.6 +- 0.05 by\nisochrone fitting, in agreement with our abundance determination. According to\nthe mean [Mg\/Fe] and [Al\/Fe] abundances from first population stars, NGC 6316\nis an in-situ globular cluster, in accordance with various dynamical\nclassifications.",
        "This paper presents an analysis and experimental demonstration of\nsingle-satellite single-pass geolocation of a terrestrial broadcast Global\nNavigation Satellite System (GNSS) spoofer from Low Earth Orbit (LEO). The\nproliferation of LEO-based GNSS receivers offers the prospect of unprecedented\nspectrum awareness, enabling persistent GNSS interference detection and\ngeolocation. Accurate LEO-based single-receiver emitter geolocation is possible\nwhen a range-rate time history can be extracted for the emitter. This paper\npresents a technique crafted specifically for indiscriminate broadcast-type\nGNSS spoofing signals. Furthermore, it explores how unmodeled oscillator\ninstability and worst-case spoofer-introduced signal variations degrade the\ngeolocation estimate. The proposed geolocation technique is validated by a\ncontrolled experiment, in partnership with Spire Global, in which a LEO-based\nreceiver captures broadcast GNSS spoofing signals transmitted from a known\nground station on a non-GNSS frequency band.",
        "Traffic accidents, especially at intersections, are a major road safety\nconcern. Previous research has extensively studied intersection-related\naccidents, but the effect of building-induced visibility restrictions at\nintersections on accident rates has been under-explored, particularly in urban\ncontexts. Using OpenStreetMap data, the UK's geographic and accident datasets,\nand the UK Traffic Count Dataset, we formulated a novel approach to estimate\naccident risk at intersections. This method factors in the area visible to\ndrivers, accounting for views blocked by buildings - a distinctive aspect in\ntraffic accident analysis. Our findings reveal a notable correlation between\nthe road visible percentage and accident frequency. In the model, the\ncoefficient for \"road visible percentage\" is 1.7450, implying a strong positive\nrelationship. Incorporating this visibility factor enhances the model's\nexplanatory power, with increased R-square values and reduced AIC and BIC,\nindicating a better data fit. This study underscores the essential role of\narchitectural layouts in road safety and suggests that urban planning\nstrategies should consider building-induced visibility restrictions. Such\nconsideration could be an effective approach to mitigate accident rates at\nintersections. This research opens up new avenues for innovative, data-driven\nurban planning and traffic management strategies, highlighting the importance\nof visibility enhancements for safer roads.",
        "The study aims to identify the process of transfer from science to technology\nthat occurs in the main Portuguese public universities. The methodology was\nbased on the analysis of the scientific literature cited in patents. Data was\nobtained from the Lens patent database. 10,514 scientific articles cited in\npatents were retrieved. A descriptive analysis of the data was performed.\nScience maps were created to visualize the main research trends. The results\nshowed a valuable impact of academic research in certain scientific\ndisciplines, such as Chemistry, Biology, Materials Sciences and Medicine. The\nmain research fronts were cancer, nanoparticles, biomaterials, tissue\nengineering or molecular biology. In conclusion, the research produced by\nPortuguese universities has generated relevant knowledge for patented\ninventions and the science-technology flow within specific areas.",
        "In this work, we develop a framework for atomistic modeling of electronic\npolarizability to predict the Raman spectra of hydrogen-bonded clusters and\nliquids from molecular dynamics (MD) simulations. The total polarizability of\nthe system is assumed to arise from contributions of both the monomer unit and\nintermolecular interactions. The generalized bond-polarizability model (GBPM),\ninspired by the classic bond-polarizability model, effectively describes the\nelectronic polarizability of a monomer. To account for the electronic\npolarizability arising from intermolecular interactions, we use a basis set of\nrapidly decaying functions of interatomic distances. We apply this model to\ncalculate the electronic polarizability and Raman spectra of water clusters\n((H2O)r, r = 2, 3, 4, 5, 6) and liquid water. The computational results are\ncompared with the results of quantum-mechanical calculations for clusters and\nto experimental data for the liquid. It is demonstrated that this simple and\nphysically motivated model, which relies on a small number of parameters,\nperforms well for clusters at both low and high temperatures, capturing strong\nanharmonic effects. Moreover, its high transferability suggests its\napplicability to other water clusters. These results suggest that a\nhierarchical approach based on the Jacob's ladder of increasingly sophisticated\nand accurate atomistic polarizability models incorporating additional effects\ncan be used for efficient modeling of Raman spectra from MD simulations of\nclusters, liquids and solids.",
        "Cosmic voids, the nearly empty regions nestled between walls and filaments,\nare recognized for their extensive applications in the field of cosmology and\nastrophysics. However, a consensus on the definition of voids remains elusive,\nas various void-finding methods identify different types of voids, each\ndiffering in shape and density based on the method that were used. In this\npaper, we introduce an innovative void identification method that utilizes\nGenetic Algorithm analysis. VEGA employs the Voronoi tessellation technique and\nthe Convex Hull algorithm to partition the dataset plane into distinct regions\nand calculate the volume of each region. For the first time, VEGA integrates\nGenetic Algorithm analysis with the luminosity density contrast function to\nidentify and locate the possible void region candidates. This method utilizes a\nset of grid points, which enhances the implementation of Voronoi tessellation\nand enables VEGA to more effectively access the dataset space for the\nidentification of void regions candidates, finding the center and the ultimate\nstructure of voids. Finally, we applied the VEGA and Aikio-M\\\"ah\\\"onen (AM)\nmethods to the same test dataset and compared the cosmic voids identified by\nVEGA with those identified by the AM method. This comparison demonstrated that\nthe VEGA void-finding method yields reliable results and can be effectively\napplied to various particle distributions.",
        "Conditional Permutation Importance (CPI) has been recently introduced for\nVariable Importance analysis with good empirical results. In this work, we\nfirst provide theoretical guarantees for CPI. We establish a double robustness\nproperty to detect null covariates, making it a suitable model for variable\nselection. We then present a modified and still computationally efficient\nversion, Sobol-CPI, that aims to estimate a well-known variable importance\nmeasure, the Total Sobol Index (TSI). We prove that it is nonparametrically\nefficient, and we provide a procedure to control the type-I error. Through\nnumerical experiments, we show that Sobol-CPI preserves the double robustness\nproperty in practice.",
        "Intra-sentential code-switching (CS) refers to the alternation between\nlanguages that happens within a single utterance and is a significant challenge\nfor Automatic Speech Recognition (ASR) systems. For example, when a Vietnamese\nspeaker uses foreign proper names or specialized terms within their speech. ASR\nsystems often struggle to accurately transcribe intra-sentential CS due to\ntheir training on monolingual data and the unpredictable nature of CS. This\nissue is even more pronounced for low-resource languages, where limited data\navailability hinders the development of robust models. In this study, we\npropose AdaCS, a normalization model integrates an adaptive bias attention\nmodule (BAM) into encoder-decoder network. This novel approach provides a\nrobust solution to CS ASR in unseen domains, thereby significantly enhancing\nour contribution to the field. By utilizing BAM to both identify and normalize\nCS phrases, AdaCS enhances its adaptive capabilities with a biased list of\nwords provided during inference. Our method demonstrates impressive performance\nand the ability to handle unseen CS phrases across various domains. Experiments\nshow that AdaCS outperforms previous state-of-the-art method on Vietnamese CS\nASR normalization by considerable WER reduction of 56.2% and 36.8% on the two\nproposed test sets.",
        "Autonomous vehicles represent a revolutionary advancement driven by the\nintegration of artificial intelligence within intelligent transportation\nsystems. However, they remain vulnerable due to the absence of robust security\nmechanisms in the Controller Area Network (CAN) bus. In order to mitigate the\nsecurity issue, many machine learning models and strategies have been proposed,\nwhich primarily focus on a subset of dominant patterns of anomalies and lack\nrigorous evaluation in terms of reliability and robustness. Therefore, to\naddress the limitations of previous works and mitigate the security\nvulnerability in CAN bus, the current study develops a model based on the\nintrinsic nature of the problem to cover all dominant patterns of anomalies. To\nachieve this, a cascade feature-level fusion strategy optimized by a\ntwo-parameter genetic algorithm is proposed to combine temporal and spatial\ninformation. Subsequently, the model is evaluated using a paired t-test to\nensure reliability and robustness. Finally, a comprehensive comparative\nanalysis conducted on two widely used datasets advocates that the proposed\nmodel outperforms other models and achieves superior accuracy and F1-score,\ndemonstrating the best performance among all models presented to date.",
        "Experiments at the US National Ignition Facility (NIF) [D\\\"{o}ppner et al.,\nNature {\\bf 618}, 270-275 (2023)] have created highly compressed hot\nhydrogen-like Be plasmas. Published analyses of the the NIF experiment have\nused finite-$T$ multi-atom density-functional theory (DFT) with Molecular\ndynamics (MD), and Path-Integral Monte Carlo (PIMC) simulations. These methods\nare very expensive to implement and often lack physical transparency. Here we\n(i) relate their results to simpler first-principles average-atom results, (ii)\nestablish the feasibility of rapid data analysis, with good accuracy and gain\nin physical transparency, and (iii) show that the NIF experiment reveals\nhigh-$T$ spin-singlet pairing of hydrogen-like Be ions with near neighbours.\nOur analysis predicts such stabilization over a wide range of compressed\ndensities for temperatures close to two million Kelvin. Calculations of\nstructure factors $S(k)$ for electrons or ions, the Raleigh weight and other\nquantities of interest to X-ray Thomson scattering are presented. We find that\nthe NIF data at the scattering wavevector $k_{sc}$ of 7.89 \\AA$^{-1}$ are more\nconsistent with a density of $20\\pm2$ g\/cm$^3$, mean ionization $\\bar{Z}=$3.25,\nat a temperature of $\\simeq$ 1,800,000 K than the 34 g\/cm$^3, \\bar{Z}=3.4$\nproposed by the NIF team. The relevance of ion-electron coupled-modes in\nstudying small $k_{sc}$ data is indicated.",
        "In this paper, we investigate a distributed multi-input multi-output and\northogonal frequency division multiplexing (MIMO-OFDM) dual-function\nradar-communication (DFRC) system, which enables simultaneous communication and\nsensing in different subcarrier sets. To obtain the best tradeoff between\ncommunication and sensing performance, we first derive Cramer-Rao Bound (CRB)\nof targets in the detection area, and then maximize the transmission rate by\njointly optimizing the power\/subcarriers allocation and the selection of radar\nreceivers under the constraints of detection performance and total transmit\npower. To tackle the non-convex mixed integer programming problem, we decompose\nthe original problem into a semidefinite programming (SDP) problem and a convex\nquadratic integer problem and solve them iteratively. The numerical results\ndemonstrate the effectiveness of our proposed algorithm, as well as the\nperformance improvement brought by optimizing radar receivers selection.",
        "Personalizing large language models (LLMs) is essential for delivering\ntailored interactions that improve user experience. Many existing\npersonalization methods require fine-tuning LLMs for each user, rendering them\nprohibitively expensive for widespread adoption. Although retrieval-based\napproaches offer a more compute-efficient alternative, they still depend on\nlarge, high-quality datasets that are not consistently available for all users.\nTo address this challenge, we propose CHAMELEON, a scalable and efficient\npersonalization approach that uses (1) self-generated personal preference data\nand (2) representation editing to enable quick and cost-effective\npersonalization. Our experiments on various tasks, including those from the\nLaMP personalization benchmark, show that CHAMELEON efficiently adapts models\nto personal preferences, improving instruction-tuned models and outperforms two\npersonalization baselines by an average of 40% across two model architectures.",
        "The dynamics of wave groups is studied for long waves, using the framework of\nthe Benjamin-Bona-Mahony (BBM) equation and its generalizations. It is shown\nthat the dynamics are richer than the corresponding results obtained just from\nthe Korteweg-de Vries-type equation. First, a reduction to a nonlinear\nSchr\\\"odinger equation is obtained for weakly nonlinear wave packets, and it is\ndemonstrated that either the focusing or the defocusing case can be obtained.\nThis is in contrast to the corresponding reduction for the Korteweg-de Vries\nequation, where only the defocusing case is found. The focusing regime displays\nmodulational instability responsible for the appearance of rogue waves. Next,\nthe condition for modulational instability is obtained in the case of one and\ntwo monochromatic waves in interaction at slow space-time coordinates with\nequal scalings. Other new envelope equations are obtained starting from the\ngeneral system describing shallow water waves found by Bona et al. [3]. A\npresumably integrable system is obtained form the integrable Kaup-Boussinesq\none.",
        "In this paper, we study the numerical solution of an elastic\/viscoelastic\nwave equation with non smooth wave speed and internal localized distributed\nKelvin-Voigt damping acting faraway from the boundary. Our method is based on\nthe Finite Volume Method (FVM) and we are interested in deriving the stability\nestimates and the convergence of the numerical solution to the continuous one.\nNumerical experiments are performed to confirm the theoretical study on the\ndecay rate of the solution to the null one when a localized damping acts.",
        "In the design of unmanned aerial vehicle (UAV) wireless communications, a\nbetter understanding of propagation characteristics and an accurate channel\nmodel are required. Measurements and comprehensive analysis for the UAV-based\nair-ground (AG) propagation channel in the vertical dimension are presented in\nthis letter. Based on the measurement data at 1 and 4 GHz, the large-scale and\nsmall-scale channel parameters are extracted in the line-of-sight (LOS) and\nnonLOS case, respectively. The altitude-dependent path loss model is proposed\nherein. Furthermore, shadow fading and fast fading are statistically analyzed\nfor comprehensively describing the fading behavior. Our results will be useful\nin the modeling of AG channels and the performance analysis for UAV-enabled\nwireless communication systems.",
        "We examine the signals produced by dark matter interactions with electrons,\nwhich play a crucial role in direct detection experiments employing heavy\ntarget materials, particularly in many well-motivated sub-GeV dark matter\nscenarios. When the momentum transfer to target electrons is comparable to or\nexceeds their binding energy, atomic effects related to electron ionization\nbecome essential for accurately determining signal rates - especially in the\ncase of fast-moving dark matter. In this paper, we revisit and extend the\natomic ionization formalism, systematically comparing different approaches used\nto formulate the ionization form factor and identifying their respective\ndomains of validity. As practical applications, we explore detection prospects\nin xenon target experiments. To illustrate our findings, we consider a specific\nscenario involving boosted dark matter, which often leads to high-momentum\nelectron recoils. Our analysis demonstrates that the choice of formalism can\nsignificantly influence the interpretation of experimental data, depending on\nthe regions of parameter space.",
        "This study delves into the mechanisms that spark user curiosity driving\nactive engagement within public Telegram groups. By analyzing approximately 6\nmillion messages from 29,196 users across 409 groups, we identify and quantify\nthe key factors that stimulate users to actively participate (i.e., send\nmessages) in group discussions. These factors include social influence,\nnovelty, complexity, uncertainty, and conflict, all measured through metrics\nderived from message sequences and user participation over time. After\nclustering the messages, we apply explainability techniques to assign\nmeaningful labels to the clusters. This approach uncovers macro categories\nrepresenting distinct curiosity stimulation profiles, each characterized by a\nunique combination of various stimuli. Social influence from peers and\ninfluencers drives engagement for some users, while for others, rare media\ntypes or a diverse range of senders and media sparks curiosity. Analyzing\npatterns, we found that user curiosity stimuli are mostly stable, but, as the\ntime between the initial message increases, curiosity occasionally shifts. A\ngraph-based analysis of influence networks reveals that users motivated by\ndirect social influence tend to occupy more peripheral positions, while those\nwho are not stimulated by any specific factors are often more central,\npotentially acting as initiators and conversation catalysts. These findings\ncontribute to understanding information dissemination and spread processes on\nsocial media networks, potentially contributing to more effective communication\nstrategies.",
        "We start with a systematic study of the zero-sum Ramsey numbers. For a graph\n$G$ with $0 \\ (\\!\\!\\!\\!\\mod 3)$ edges, the zero-sum Ramsey number is defined as\nthe smallest positive integer $R(G, \\mathbb{Z}_3)$ such that for every $n \\geq\nR(G, \\mathbb{Z}_3)$ and every edge-colouring $f$ of $K_n$ using $\\mathbb{Z}_3$,\nthere is a zero-sum copy of $G$ in $K_n$ coloured by $f$, that is: $\\sum_{e \\in\nE(G)} f(e) \\equiv 0 \\ (\\!\\!\\!\\!\\mod 3)$.\n  Only sporadic results are known for these Ramsey numbers, and we discover\nmany new ones. In particular we prove that for every forest $F$ on $n$ vertices\nand with $0 \\ (\\!\\!\\!\\!\\mod 3)$ edges, $R(F, \\mathbb{Z}_3) \\leq n+2$, and this\nbound is tight if all the vertices of $F$ have degrees $1 \\ (\\!\\!\\!\\!\\mod 3)$.\nWe also determine exact values of $R(T, \\mathbb{Z}_3)$ for infinite families of\ntrees.",
        "We propose an explainable probabilistic framework for characterizing spoofed\nspeech by decomposing it into probabilistic attribute embeddings. Unlike raw\nhigh-dimensional countermeasure embeddings, which lack interpretability, the\nproposed probabilistic attribute embeddings aim to detect specific speech\nsynthesizer components, represented through high-level attributes and their\ncorresponding values. We use these probabilistic embeddings with four\nclassifier back-ends to address two downstream tasks: spoofing detection and\nspoofing attack attribution. The former is the well-known bonafide-spoof\ndetection task, whereas the latter seeks to identify the source method\n(generator) of a spoofed utterance. We additionally use Shapley values, a\nwidely used technique in machine learning, to quantify the relative\ncontribution of each attribute value to the decision-making process in each\ntask. Results on the ASVspoof2019 dataset demonstrate the substantial role of\nduration and conversion modeling in spoofing detection; and waveform generation\nand speaker modeling in spoofing attack attribution. In the detection task, the\nprobabilistic attribute embeddings achieve $99.7\\%$ balanced accuracy and\n$0.22\\%$ equal error rate (EER), closely matching the performance of raw\nembeddings ($99.9\\%$ balanced accuracy and $0.22\\%$ EER). Similarly, in the\nattribution task, our embeddings achieve $90.23\\%$ balanced accuracy and\n$2.07\\%$ EER, compared to $90.16\\%$ and $2.11\\%$ with raw embeddings. These\nresults demonstrate that the proposed framework is both inherently explainable\nby design and capable of achieving performance comparable to raw CM embeddings.",
        "Recent advances in deep learning optimization have unveiled two intriguing\nphenomena under large learning rates: Edge of Stability (EoS) and Progressive\nSharpening (PS), challenging classical Gradient Descent (GD) analyses. Current\nresearch approaches, using either generalist frameworks or minimalist examples,\nface significant limitations in explaining these phenomena. This paper advances\nthe minimalist approach by introducing a two-layer network with a\ntwo-dimensional input, where one dimension is relevant to the response and the\nother is irrelevant. Through this model, we rigorously prove the existence of\nprogressive sharpening and self-stabilization under large learning rates, and\nestablish non-asymptotic analysis of the training dynamics and sharpness along\nthe entire GD trajectory. Besides, we connect our minimalist example to\nexisting works by reconciling the existence of a well-behaved ``stable set\"\nbetween minimalist and generalist analyses, and extending the analysis of\nGradient Flow Solution sharpness to our two-dimensional input scenario. These\nfindings provide new insights into the EoS phenomenon from both parameter and\ninput data distribution perspectives, potentially informing more effective\noptimization strategies in deep learning practice.",
        "A traversable wormhole generally violates the averaged null energy condition,\nusually requiring exotic matter. Recently, it has been found that the\ntraversable wormhole can be realized by non-exotic matter in\nEinstein-Dirac-Maxwell theories in flat space. This paper generalizes\ndiscussions to the AdS spacetime and finds traversable wormholes with spherical\nand planar topologies. Furthermore, based on the AdS\/CFT correspondence, we\ncompute the entanglement entropy of strips and disks on two AdS boundaries of\nthe wormhole. We find that entanglement entropy undergoes a phase transition as\nthe subsystem size increases.",
        "Image recognition models have struggled to treat recognition robustness to\nreal-world degradations. In this context, data augmentation methods like PixMix\nimprove robustness but rely on generative arts and feature visualizations\n(FVis), which have copyright, drawing cost, and scalability issues. We propose\nMoireDB, a formula-generated interference-fringe image dataset for image\naugmentation enhancing robustness. MoireDB eliminates copyright concerns,\nreduces dataset assembly costs, and enhances robustness by leveraging illusory\npatterns. Experiments show that MoireDB augmented images outperforms\ntraditional Fractal arts and FVis-based augmentations, making it a scalable and\neffective solution for improving model robustness against real-world\ndegradations.",
        "We study the effect of finite spin quark density on the chiral and\ndeconfinement thermal transitions using numerical simulations of lattice QCD\nwith two dynamical light quarks. The finite spin density is introduced by the\nquark spin potential in the canonical formulation of the spin operator. We show\nthat both chiral and deconfinement temperatures are decreasing functions of the\nspin potential. We determine the parabolic curvatures of transition\ntemperatures in a limit of physical quark masses.",
        "When faults occur in microservice applications -- as they inevitably do --\ndevelopers depend on observability data to quickly identify and diagnose the\nissue. To collect such data, microservices need to be instrumented and the\nrespective infrastructure configured. This task is often underestimated and\nerror-prone, typically relying on many ad-hoc decisions. However, some of these\ndecisions can significantly affect how quickly faults are detected and also\nimpact the cost and performance of the application.\n  Given its importance, we emphasize the need for a method to guide the\nobservability design process. In this paper, we build on previous work and\nintegrate our observability experiment tool OXN into a novel method for\ncontinuous observability assurance. We demonstrate its use and discuss future\ndirections.",
        "Background: The nuclear shell model offers realistic predictions of nuclear\nstructure starting from (quasi-) proton and neutron degrees of freedom, but\nrelies on coupling constants (interaction matrix elements) that must be fit to\nexperiment. To extend the shell model's applicability across the nuclear chart,\nand specifically toward the driplines, we must first be able to efficiently\ntest new interaction matrix elements and assign credible uncertainties.\n  Purpose: We develop and test a framework to efficiently fit new shell model\ninteractions and obtain credible uncertainties. We further demonstrate its use\nby validating the uncertainty estimates of the known \\textit{sd}-shell\neffective interactions.\n  Methods: We use eigenvector continuation to emulate solutions to the exact\nshell model. First, we use the emulator to replicate earlier results using a\nwell-known linear-combination chi-squared minimization algorithm. Then, we\nemploy a modern Markov Chain Monte Carlo method to test for nonlinearities in\nthe observable posterior distributions, which previous sensitivity analyses\nprecluded.\n  Results: The emulator reproduces the USDB interaction within a small margin\nof error, allowing for the quantification of the matrix element uncertainty.\nHowever, we find that to obtain credible predictive intervals the model defect\nof the shell model itself, rather than experimental or emulator\nuncertainty\/error, must be taken into account.\n  Conclusions: Eigenvector continuation can be used to accelerate fitting shell\nmodel interactions. We confirm that the linear approximation used to develop\ninteractions in the past is indeed sufficient. However, we find that typical\nassumptions about the likelihood function must be modified in order to obtain a\ncredible uncertainty-quantified interaction."
      ]
    }
  },
  {
    "id":2411.15202,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"The Great Barrier Reef: an environmental history",
    "start_abstract":"Reconstructing changes in the Great Barrier Reef 15 3 The natural context of 33 4 spread European settlement coastal Queensland 43 5 beche--de ... mer, pearl shell and trochus fisheries 55 6 Impacts on marine turtles 72 7 dugongs 95 8 whales, sharks fish 9 impacts coral collecting 10 guano rock phosphate mining 11 vi Contents 12 Other reefs 13 Changes island biota 14 Conclusion",
    "start_categories":[
      "q-bio.QM"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b2"
      ],
      "title":[
        "Ilf-lstm: Enhanced loss function in lstm to predict the sea surface temperature"
      ],
      "abstract":[
        "Globe's primary issue is global warming, water temperatures have accompanied it as the sea surface temperature, and it is the primary attribute to balance the energy on the earth's surface. Sea surface temperature prediction is vital to climate forecast. Downwelling currents carry some of this heat to the ocean's bottom layers, which are also heating, covering far behind the increase in sea surface temperature. In deep learning models, the correct loss function will try to reduce the error and converge fast. The proposed improved loss function correctly estimates how close the predictions made by the long short-term memory match the observed values in the training data. This research considers location-specific sea surface temperature predictions using the improved loss function in the long short-term memory neural network at six different locations around India for daily, weekly, and monthly time horizons. Most existing research concentrated on periodic forecasts, but this paper focused on daily, weekly, and monthly predictions. The improved loss function\u2014long short-term memory, achieved 98.7% accuracy, and this improved loss function overcomes the limitations of the existing techniques and reduces the processing time to\u2009~\u20090.35 s. In this research, the sea surface temperature prediction using the improved loss function in the long short-term memory neural network gives better results than the standard prediction models and other existing techniques by considering the long-time dependencies and obtaining features from the spatial data."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "Resolution Invariant Autoencoder",
        "Extending Dense Passage Retrieval with Temporal Information",
        "Exploring constraints on the core radius and density jumps inside Earth\n  using atmospheric neutrino oscillations",
        "The Kodaira dimension of Hilbert modular threefolds",
        "Some NP Complete Problems Based on Algebra and Algebraic Geometry",
        "Lagrangian chaos and unique ergodicity for stochastic primitive\n  equations",
        "Simpliciality of vector-valued function spaces",
        "Graphy'our Data: Towards End-to-End Modeling, Exploring and Generating\n  Report from Raw Data",
        "Block Flow: Learning Straight Flow on Data Blocks",
        "LiDAR Remote Sensing Meets Weak Supervision: Concepts, Methods, and\n  Perspectives",
        "Training Consistency Models with Variational Noise Coupling",
        "Formulas as Processes, Deadlock-Freedom as Choreographies (Extended\n  Version)",
        "CoMoGaussian: Continuous Motion-Aware Gaussian Splatting from\n  Motion-Blurred Images",
        "Optimal Control of Fluid Restless Multi-armed Bandits: A Machine\n  Learning Approach",
        "Central-moment-based discrete Boltzmann modeling of compressible flows",
        "Accuracy Can Lie: On the Impact of Surrogate Model in Configuration\n  Tuning",
        "Resilient Distributed Control for Uncertain Nonlinear Interconnected\n  Systems under Network Anomaly",
        "YUNet: Improved YOLOv11 Network for Skyline Detection",
        "Bounded powers of edge ideals: regularity and linear quotients",
        "Ukrainian contribution to particle physics: historical perspective and\n  prospects",
        "Double-Scaled SYK, QCD, and the Flat Space Limit of de Sitter Space",
        "Cyclicity of sliding cycles with singularities of regularized piecewise\n  smooth visible-invisible two-folds",
        "Robust Cross-Etiology and Speaker-Independent Dysarthric Speech\n  Recognition",
        "Deep learning-based holography for T-linear resistivity",
        "GNSS\/GPS Spoofing and Jamming Identification Using Machine Learning and\n  Deep Learning",
        "Structure factors and quantum geometry in multiband BCS superconductors",
        "Patch-Depth Fusion: Dichotomous Image Segmentation via Fine-Grained\n  Patch Strategy and Depth Integrity-Prior",
        "Foundation Inference Models for Stochastic Differential Equations: A\n  Transformer-based Approach for Zero-shot Function Estimation",
        "A Volumetric Approach to Privacy of Dynamical Systems"
      ],
      "abstract":[
        "Deep learning has significantly advanced medical imaging analysis, yet\nvariations in image resolution remain an overlooked challenge. Most methods\naddress this by resampling images, leading to either information loss or\ncomputational inefficiencies. While solutions exist for specific tasks, no\nunified approach has been proposed. We introduce a resolution-invariant\nautoencoder that adapts spatial resizing at each layer in the network via a\nlearned variable resizing process, replacing fixed spatial down\/upsampling at\nthe traditional factor of 2. This ensures a consistent latent space resolution,\nregardless of input or output resolution. Our model enables various downstream\ntasks to be performed on an image latent whilst maintaining performance across\ndifferent resolutions, overcoming the shortfalls of traditional methods. We\ndemonstrate its effectiveness in uncertainty-aware super-resolution,\nclassification, and generative modelling tasks and show how our method\noutperforms conventional baselines with minimal performance loss across\nresolutions.",
        "Temporal awareness is crucial in many information retrieval tasks,\nparticularly in scenarios where the relevance of documents depends on their\nalignment with the query's temporal context. Traditional retrieval methods such\nas BM25 and Dense Passage Retrieval (DPR) excel at capturing lexical and\nsemantic relevance but fall short in addressing time-sensitive queries. To\nbridge this gap, we introduce the temporal retrieval model that integrates\nexplicit temporal signals by incorporating query timestamps and document dates\ninto the representation space. Our approach ensures that retrieved passages are\nnot only topically relevant but also temporally aligned with user intent. We\nevaluate our approach on two large-scale benchmark datasets, ArchivalQA and\nChroniclingAmericaQA, achieving substantial performance gains over standard\nretrieval baselines. In particular, our model improves Top-1 retrieval accuracy\nby 6.63% and NDCG@10 by 3.79% on ArchivalQA, while yielding a 9.56% boost in\nTop-1 retrieval accuracy and 4.68% in NDCG@10 on ChroniclingAmericaQA.\nAdditionally, we introduce a time-sensitive negative sampling strategy, which\nrefines the model's ability to distinguish between temporally relevant and\nirrelevant documents during training. Our findings highlight the importance of\nexplicitly modeling time in retrieval systems and set a new standard for\nhandling temporally grounded queries.",
        "Atmospheric neutrinos, through their weak interactions, can serve as an\nindependent tool for exploring the internal structure of Earth. The information\nobtained would be complementary to that provided by seismic and gravitational\nmeasurements. The Earth matter effects in neutrino oscillations depend upon the\nenergy of neutrinos and the electron density distribution that they encounter\nduring their journey through Earth, and hence, can be used to probe the inner\nstructure of Earth. In this contribution, we demonstrate how well an\natmospheric neutrino experiment, such as an iron calorimeter detector (ICAL),\nwould simultaneously constrain the density jumps inside Earth and determine the\nlocation of the core-mantle boundary. In this work, we employ a five-layered\ndensity model of Earth, where the layer densities and core radius are modified\nto explore the parameter space, ensuring that the mass and moment of inertia of\nEarth remain constant while satisfying the hydrostatic equilibrium condition.\nWe further demonstrate that the charge identification capability of an\nICAL-like detector would play a crucial role in obtaining these correlated\nconstraints.",
        "Following a method introduced by Thomas-Vasquez and developed by Grundman, we\nprove that many Hilbert modular threefolds of arithmetic genus $0$ and $1$ are\nof general type, and that some are of nonnegative Kodaira dimension. The new\ningredient is a detailed study of the geometry and combinatorics of totally\npositive integral elements $x$ of a fractional ideal $I$ in a totally real\nnumber field $K$ with the property that $\\mathop{\\mathrm{tr}} xy <\n\\mathop{\\mathrm{min}} I \\mathop{\\mathrm{tr}} y$ for some $y \\gg 0 \\in K$.",
        "This paper describes several new problems and ideas concerning algebraic\ngeometry and complexity theory. It first uses the idea of coloring graphs with\nelements of finite fields. This procedure then shows that graph coloring\nproblems can be converted into membership problems for a new family of\nalgebraic varieties, coloring varieties, which are closely related to\ndeterminantal varieties. This in turn shows that the problem of NP vs P can be\nconverted into questions of if certain polynomials of large degree over finite\nfields have low multiplicative complexity.",
        "We show that the Lagrangian flow associated with the stochastic 3D primitive\nequations (PEs) with non-degenerate noise is chaotic, i.e., the corresponding\ntop Lyapunov exponent is strictly positive almost surely. This result builds on\nthe landmark work by Bedrossian, Blumenthal, and Punshon-Smith on Lagrangian\nchaos in stochastic fluid mechanics. Our primary contribution is establishing\nan instance where Lagrangian chaos can be proven for a fluid flow with\nsupercritical energy, a key characteristic of 3D fluid dynamics. For the 3D\nPEs, establishing the existence of the top Lyapunov exponent is already a\nchallenging task. We address this difficulty by deriving new estimates for the\ninvariant measures of the 3D PEs, which capture the anisotropic smoothing in\nthe dynamics of the PEs. As a by-product of our results, we also obtain the\nfirst uniqueness result for invariant measures of stochastic PEs.",
        "We investigate integral representation of vector-valued function spaces,\ni.e., of subspaces $H\\subset C(K,E)$, where $K$ is a compact space and $E$ is a\n(real or complex) Banach space. We point out that there are two possible ways\nof generalizing representation theorems known from the scalar case -- either\none may represent (all) functionals from $H^*$ using $E^*$-valued vector\nmeasures on $K$ (as it is done in the literature) or one may represent (some)\noperators from $L(H,E)$ by scalar measures on $K$ using the Bochner integral.\nThese two ways lead to two different notions of simpliciality which we call\n`vector simpliciality' and `weak simpliciality'. It turns out that these two\nnotions are in general incomparable. Moreover, the weak simpliciality is not\naffected by renorming the target space $E$, while vector simpliciality may be\naffected. Further, if $H$ contains constants, vector simpliciality is strictly\nstronger and admits several characterizations (partially analogous to the\ncharacterizations known in the scalar case). We also study orderings of\nmeasures inspired by C.J.K.~Batty which may be (in special cases) used to\ncharacterize $H$-boundary measures. Finally, we give a finer version of\nrepresentation theorem using positive measures on $K\\times B_{E^*}$ and\ncharacterize uniqueness in this case.",
        "Large Language Models (LLMs) have recently demonstrated remarkable\nperformance in tasks such as Retrieval-Augmented Generation (RAG) and\nautonomous AI agent workflows. Yet, when faced with large sets of unstructured\ndocuments requiring progressive exploration, analysis, and synthesis, such as\nconducting literature survey, existing approaches often fall short. We address\nthis challenge -- termed Progressive Document Investigation -- by introducing\nGraphy, an end-to-end platform that automates data modeling, exploration and\nhigh-quality report generation in a user-friendly manner. Graphy comprises an\noffline Scrapper that transforms raw documents into a structured graph of Fact\nand Dimension nodes, and an online Surveyor that enables iterative exploration\nand LLM-driven report generation. We showcase a pre-scrapped graph of over\n50,000 papers -- complete with their references -- demonstrating how Graphy\nfacilitates the literature-survey scenario. The demonstration video can be\nfound at https:\/\/youtu.be\/uM4nzkAdGlM.",
        "Flow-matching models provide a powerful framework for various applications,\noffering efficient sampling and flexible probability path modeling. These\nmodels are characterized by flows with low curvature in learned generative\ntrajectories, which results in reduced truncation error at each sampling step.\nTo further reduce curvature, we propose block matching. This novel approach\nleverages label information to partition the data distribution into blocks and\nmatch them with a prior distribution parameterized using the same label\ninformation, thereby learning straighter flows. We demonstrate that the\nvariance of the prior distribution can control the curvature upper bound of\nforward trajectories in flow-matching models. By designing flexible\nregularization strategies to adjust this variance, we achieve optimal\ngeneration performance, effectively balancing the trade-off between maintaining\ndiversity in generated samples and minimizing numerical solver errors. Our\nresults demonstrate competitive performance with models of the same parameter\nscale.Code is available at \\url{https:\/\/github.com\/wpp13749\/block_flow}.",
        "LiDAR (Light Detection and Ranging) enables rapid and accurate acquisition of\nthree-dimensional spatial data, widely applied in remote sensing areas such as\nsurface mapping, environmental monitoring, urban modeling, and forestry\ninventory. LiDAR remote sensing primarily includes data interpretation and\nLiDAR-based inversion. However, LiDAR interpretation typically relies on dense\nand precise annotations, which are costly and time-consuming. Similarly, LiDAR\ninversion depends on scarce supervisory signals and expensive field surveys for\nannotations. To address this challenge, weakly supervised learning has gained\nsignificant attention in recent years, with many methods emerging to tackle\nLiDAR remote sensing tasks using incomplete, inaccurate, and inexact\nannotations, as well as annotations from other domains. Existing review\narticles treat LiDAR interpretation and inversion as separate tasks. This\nreview, for the first time, adopts a unified weakly supervised learning\nperspective to systematically examine research on both LiDAR interpretation and\ninversion. We summarize the latest advancements, provide a comprehensive review\nof the development and application of weakly supervised techniques in LiDAR\nremote sensing, and discuss potential future research directions in this field.",
        "Consistency Training (CT) has recently emerged as a promising alternative to\ndiffusion models, achieving competitive performance in image generation tasks.\nHowever, non-distillation consistency training often suffers from high variance\nand instability, and analyzing and improving its training dynamics is an active\narea of research. In this work, we propose a novel CT training approach based\non the Flow Matching framework. Our main contribution is a trained\nnoise-coupling scheme inspired by the architecture of Variational Autoencoders\n(VAE). By training a data-dependent noise emission model implemented as an\nencoder architecture, our method can indirectly learn the geometry of the\nnoise-to-data mapping, which is instead fixed by the choice of the forward\nprocess in classical CT. Empirical results across diverse image datasets show\nsignificant generative improvements, with our model outperforming baselines and\nachieving the state-of-the-art (SoTA) non-distillation CT FID on CIFAR-10, and\nattaining FID on par with SoTA on ImageNet at $64 \\times 64$ resolution in\n2-step generation. Our code is available at https:\/\/github.com\/sony\/vct .",
        "We introduce a novel approach to studying properties of processes in the\n{\\pi}-calculus based on a processes-as-formulas interpretation, by establishing\na correspondence between specific sequent calculus derivations and computation\ntrees in the reduction semantics of the recursion-free {\\pi}-calculus. Our\nmethod provides a simple logical characterisation of deadlock-freedom for the\nrecursion- and race-free fragment of the {\\pi}-calculus, supporting key\nfeatures such as cyclic dependencies and an independence of the name\nrestriction and parallel operators. Based on this technique, we establish a\nstrong completeness result for a nontrivial choreographic language: all\ndeadlock-free and race-free finite {\\pi}-calculus processes composed in\nparallel at the top level can be faithfully represented by a choreography. With\nthese results, we show how the paradigm of computation-as-derivation extends\nthe reach of logical methods for the study of concurrency, by bridging\nimportant gaps between logic, the expressiveness of the {\\pi}-calculus, and the\nexpressiveness of choreographic languages.",
        "3D Gaussian Splatting (3DGS) has gained significant attention for their\nhigh-quality novel view rendering, motivating research to address real-world\nchallenges. A critical issue is the camera motion blur caused by movement\nduring exposure, which hinders accurate 3D scene reconstruction. In this study,\nwe propose CoMoGaussian, a Continuous Motion-Aware Gaussian Splatting that\nreconstructs precise 3D scenes from motion-blurred images while maintaining\nreal-time rendering speed. Considering the complex motion patterns inherent in\nreal-world camera movements, we predict continuous camera trajectories using\nneural ordinary differential equations (ODEs). To ensure accurate modeling, we\nemploy rigid body transformations, preserving the shape and size of the object\nbut rely on the discrete integration of sampled frames. To better approximate\nthe continuous nature of motion blur, we introduce a continuous motion\nrefinement (CMR) transformation that refines rigid transformations by\nincorporating additional learnable parameters. By revisiting fundamental camera\ntheory and leveraging advanced neural ODE techniques, we achieve precise\nmodeling of continuous camera trajectories, leading to improved reconstruction\naccuracy. Extensive experiments demonstrate state-of-the-art performance both\nquantitatively and qualitatively on benchmark datasets, which include a wide\nrange of motion blur scenarios, from moderate to extreme blur.",
        "We propose a machine learning approach to the optimal control of fluid\nrestless multi-armed bandits (FRMABs) with state equations that are either\naffine or quadratic in the state variables. By deriving fundamental properties\nof FRMAB problems, we design an efficient machine learning based algorithm.\nUsing this algorithm, we solve multiple instances with varying initial states\nto generate a comprehensive training set. We then learn a state feedback policy\nusing Optimal Classification Trees with hyperplane splits (OCT-H). We test our\napproach on machine maintenance, epidemic control and fisheries control\nproblems. Our method yields high-quality state feedback policies and achieves a\nspeed-up of up to 26 million times compared to a direct numerical algorithm for\nfluid problems.",
        "In this work, a central-moment-based discrete Boltzmann method (CDBM) is\nconstructed for fluid flows with variable specific heat ratios. The central\nkinetic moments are employed to calculate the equilibrium discrete velocity\ndistribution function in the CDBM. In comparison to previous incompressible\ncentral-moment-based lattice Boltzmann method, the CDBM possesses the\ncapability of investigating compressible flows with thermodynamic\nnonequilibrium effects beyond conventional hydrodynamic models. Unlike all\nexisting DBMs which are constructed in raw-moment space, the CDBM stands out by\ndirectly providing the nonequilibrium effects related to the thermal\nfluctuation. The proposed method has been rigorously validated using benchmarks\nof the Sod shock tube, Lax shock tube, shock wave phenomena, two-dimensional\nsound wave, and the Taylor-Green vortex flow. The numerical results exhibit an\nexceptional agreement with theoretical predictions.",
        "To ease the expensive measurements during configuration tuning, it is natural\nto build a surrogate model as the replacement of the system, and thereby the\nconfiguration performance can be cheaply evaluated. Yet, a stereotype therein\nis that the higher the model accuracy, the better the tuning result would be.\nThis \"accuracy is all\" belief drives our research community to build more and\nmore accurate models and criticize a tuner for the inaccuracy of the model\nused. However, this practice raises some previously unaddressed questions,\ne.g., Do those somewhat small accuracy improvements reported in existing work\nreally matter much to the tuners? What role does model accuracy play in the\nimpact of tuning quality? To answer those related questions, we conduct one of\nthe largest-scale empirical studies to date-running over the period of 13\nmonths 24*7-that covers 10 models, 17 tuners, and 29 systems from the existing\nworks while under four different commonly used metrics, leading to 13,612 cases\nof investigation. Surprisingly, our key findings reveal that the accuracy can\nlie: there are a considerable number of cases where higher accuracy actually\nleads to no improvement in the tuning outcomes (up to 58% cases under certain\nsetting), or even worse, it can degrade the tuning quality (up to 24% cases\nunder certain setting). We also discover that the chosen models in most\nproposed tuners are sub-optimal and that the required % of accuracy change to\nsignificantly improve tuning quality varies according to the range of model\naccuracy. Deriving from the fitness landscape analysis, we provide in-depth\ndiscussions of the rationale behind, offering several lessons learned as well\nas insights for future opportunities. Most importantly, this work poses a clear\nmessage to the community: we should take one step back from the natural\n\"accuracy is all\" belief for model-based configuration tuning.",
        "We address a distributed adaptive control methodology for nonlinear\ninterconnected systems possibly affected by network anomalies. In the framework\nof adaptive approximation, the distributed controller and parameter estimator\nare designed by exploiting a backstepping approach. The stability of the\ndistributed control system under anomalies is analyzed, where both local and\nneighboring anomaly effects are considered. To quantify the resilience of the\ninterconnected system under the action of network anomalies, we derive bounds\non the duration of each anomaly and the resting time between two consecutive\nanomalies. Specifically, when each anomaly duration is smaller than our\ndesigned upper bound, the interconnected system controlled by the distributed\napproximation-based controller remains asymptotically stable. Moreover, if the\nresting time between two consecutive anomalies is larger than the proposed\nbound, then all signals of the control system are guaranteed to be bounded. In\nthe paper, we show that under the action of the proposed distributed adaptive\ncontroller, the interconnected system remains stable in the presence of network\nanomalies, with both the qualitative and quantitative resilient conditions.\nExtensive simulation results show the effectiveness of our theoretical results.",
        "Skyline detection plays an important role in geolocalizaion, flight control,\nvisual navigation, port security, etc. The appearance of the sky and non-sky\nareas are variable, because of different weather or illumination environment,\nwhich brings challenges to skyline detection. In this research, we proposed the\nYUNet algorithm, which improved the YOLOv11 architecture to segment the sky\nregion and extract the skyline in complicated and variable circumstances. To\nimprove the ability of multi-scale and large range contextual feature fusion,\nthe YOLOv11 architecture is extended as an UNet-like architecture, consisting\nof an encoder, neck and decoder submodule. The encoder extracts the multi-scale\nfeatures from the given images. The neck makes fusion of these multi-scale\nfeatures. The decoder applies the fused features to complete the prediction\nrebuilding. To validate the proposed approach, the YUNet was tested on\nSkyfinder and CH1 datasets for segmentation and skyline detection respectively.\nOur test shows that the IoU of YUnet segmentation can reach 0.9858, and the\naverage error of YUnet skyline detection is just 1.36 pixels. The\nimplementation is published at\nhttps:\/\/github.com\/kuazhangxiaoai\/SkylineDet-YOLOv11Seg.git.",
        "Let $S=K[x_1, \\ldots,x_n]$ denote the polynomial ring in $n$ variables over a\nfield $K$ and let $I \\subset S$ be a monomial ideal. For a vector\n$\\mathfrak{c}\\in\\mathbb{N}^n$, we set $I_{\\mathfrak{c}}$ to be the ideal\ngenerated by monomials belonging to $I$ whose exponent vectors are\ncomponentwise bounded above by $\\mathfrak{c}$. Also, let\n$\\delta_{\\mathfrak{c}}(I)$ be the largest integer $k$ such that\n$(I^k)_{\\mathfrak{c}}\\neq 0$. It is shown that for every graph $G$ with edge\nideal $I(G)$, the ideal $(I(G)^{\\delta_{\\mathfrak{c}}(I)})_{\\mathfrak{c}}$ is a\npolymatroidal ideal. Moreover, we show that for each integer $s=1, \\ldots\n\\delta_{\\mathfrak{c}}(I(G))$, the Castelnuovo--Mumford regularity of\n$(I(G)^s)_{\\mathfrak{c}}$ is bounded above by $\\delta_{\\mathfrak{c}}(I(G))+s$.",
        "Many world-known scientists and engineers like G. Breit, G. Budker, G.\nCharpak, G. Gamow, M. Goldhaber, A. Ioffe, S. Korolyov, E. Lifshitz, M.\nOstrogradsky, S. Timoshenko, V. Veksler were born in Ukraine, while some, like\nL. Landau and M. Bogolyubov, started their career there. Reclaiming their\nscientific legacy as well as that of many others helps to promote Ukrainian\ncontributions to particle physics both inside and outside of Ukraine and to\nmotivate the next generation of Ukrainian scientists in the time of war. We\nwill present the status of Ukrainian scientific infrastructure two years after\nthe start of the full-scale invasion and past, present and expected future\ncontributions of Ukrainian scientists to CERN.",
        "A surprising connection exists between double-scaled SYK at infinite\ntemperature, and large N QCD. The large N expansions of the two theories have\nthe same form; the 't Hooft limit of QCD parallels the fixed p limit of SYK\n(for a theory with p-fermion interactions), and the limit of fixed gauge\ncoupling g -- the flat space limit in AdS\/CFT -- parallels the double-scaled\nlimit of SYK. From the holographic perspective fixed g is the far more\ninteresting limit of gauge theory, but very little is known about it. DSSYK\nallows us to explore it in a more tractable example. The connection is\nillustrated by perturbative and non-perturbative DSSYK calculations, and\ncomparing the results with known properties of Yang Mills theory. The\ncorrespondence is largely independent of the conjectured duality between DSSYK\nand de Sitter space, but may have a good deal to tell us about it.",
        "In this paper we study the cyclicity of sliding cycles for regularized\npiecewise smooth visible-invisible two-folds, in the presence of singularities\nof the Filippov sliding vector field located away from two-folds. We obtain a\nslow-fast system after cylindrical blow-up and use a well-known connection\nbetween the divergence integral along orbits and transition maps for vector\nfields. Since properties of the divergence integral depend on the location and\nmultiplicity of singularities, we divide the sliding cycles into different\nclasses, which can then produce different types of cyclicity results. As an\nexample, we apply our results to regularized piecewise linear systems.",
        "In this paper, we present a speaker-independent dysarthric speech recognition\nsystem, with a focus on evaluating the recently released Speech Accessibility\nProject (SAP-1005) dataset, which includes speech data from individuals with\nParkinson's disease (PD). Despite the growing body of research in dysarthric\nspeech recognition, many existing systems are speaker-dependent and adaptive,\nlimiting their generalizability across different speakers and etiologies. Our\nprimary objective is to develop a robust speaker-independent model capable of\naccurately recognizing dysarthric speech, irrespective of the speaker.\nAdditionally, as a secondary objective, we aim to test the cross-etiology\nperformance of our model by evaluating it on the TORGO dataset, which contains\nspeech samples from individuals with cerebral palsy (CP) and amyotrophic\nlateral sclerosis (ALS). By leveraging the Whisper model, our\nspeaker-independent system achieved a CER of 6.99% and a WER of 10.71% on the\nSAP-1005 dataset. Further, in cross-etiology settings, we achieved a CER of\n25.08% and a WER of 39.56% on the TORGO dataset. These results highlight the\npotential of our approach to generalize across unseen speakers and different\netiologies of dysarthria.",
        "We employ deep learning within holographic duality to investigate $T$-linear\nresistivity, a hallmark of strange metals. Utilizing Physics-Informed Neural\nNetworks, we incorporate boundary data for $T$-linear resistivity and bulk\ndifferential equations into a loss function. This approach allows us to derive\ndilaton potentials in Einstein-Maxwell-Dilaton-Axion theories, capturing\nessential features of strange metals, such as $T$-linear resistivity and linear\nspecific heat scaling. We also explore the impact of the resistivity slope on\ndilaton potentials. Regardless of slope, dilaton potentials exhibit universal\nexponential growth at low temperatures, driving $T$-linear resistivity and\nmatching infrared geometric analyses. At a specific slope, our method\nrediscovers the Gubser-Rocha model, a well-known holographic model of strange\nmetals. Additionally, the robustness of $T$-linear resistivity at higher\ntemperatures correlates with the asymptotic AdS behavior of the dilaton\ncoupling to the Maxwell term. Our findings suggest that deep learning could\nhelp uncover mechanisms in holographic condensed matter systems and advance our\nunderstanding of strange metals.",
        "The increasing reliance on Global Navigation Satellite Systems (GNSS),\nparticularly the Global Positioning System (GPS), underscores the urgent need\nto safeguard these technologies against malicious threats such as spoofing and\njamming. As the backbone for positioning, navigation, and timing (PNT) across\nvarious applications including transportation, telecommunications, and\nemergency services GNSS is vulnerable to deliberate interference that poses\nsignificant risks. Spoofing attacks, which involve transmitting counterfeit\nGNSS signals to mislead receivers into calculating incorrect positions, can\nresult in serious consequences, from navigational errors in civilian aviation\nto security breaches in military operations. Furthermore, the lack of inherent\nsecurity measures within GNSS systems makes them attractive targets for\nadversaries. While GNSS\/GPS jamming and spoofing systems consist of numerous\ncomponents, the ability to distinguish authentic signals from malicious ones is\nessential for maintaining system integrity. Recent advancements in machine\nlearning and deep learning provide promising avenues for enhancing detection\nand mitigation strategies against these threats. This paper addresses both\nspoofing and jamming by tackling real-world challenges through machine\nlearning, deep learning, and computer vision techniques. Through extensive\nexperiments on two real-world datasets related to spoofing and jamming\ndetection using advanced algorithms, we achieved state of the art results. In\nthe GNSS\/GPS jamming detection task, we attained approximately 99% accuracy,\nimproving performance by around 5% compared to previous studies. Additionally,\nwe addressed a challenging tasks related to spoofing detection, yielding\nresults that underscore the potential of machine learning and deep learning in\nthis domain.",
        "We consider multiband BCS superconductors that exhibit time-reversal symmetry\nand uniform pairing, and analyze their dynamic density and spin structure\nfactors using linear-response theory within the mean-field BCS-BEC crossover\nframework at zero temperature. Our results for the multi-orbital Hubbard model\nsatisfy the associated f-sum rules in several limits. In particular, in the\nstrong-coupling limit, they coincide with those of a weakly-interacting Bose\ngas of Cooper pairs, where the low-energy collective Goldstone modes serve as\nBogoliubov phonons. We further reveal that the quantum-geometric origin of the\nlow-energy structure factors, along with related observables such as the\nsuperfluid-weight tensor and the effective-mass tensor of Cooper pairs, can be\ntraced all the way back to the effective-mass theorem for Bloch bands in this\nlimit. As an illustration, we investigate the pyrochlore-Hubbard model\nnumerically and demonstrate that the Goldstone modes are the only relevant\ncollective degrees of freedom in the flat-band regime.",
        "Dichotomous Image Segmentation (DIS) is a high-precision object segmentation\ntask for high-resolution natural images. The current mainstream methods focus\non the optimization of local details but overlook the fundamental challenge of\nmodeling the integrity of objects. We have found that the depth integrity-prior\nimplicit in the the pseudo-depth maps generated by Depth Anything Model v2 and\nthe local detail features of image patches can jointly address the above\ndilemmas. Based on the above findings, we have designed a novel Patch-Depth\nFusion Network (PDFNet) for high-precision dichotomous image segmentation. The\ncore of PDFNet consists of three aspects. Firstly, the object perception is\nenhanced through multi-modal input fusion. By utilizing the patch fine-grained\nstrategy, coupled with patch selection and enhancement, the sensitivity to\ndetails is improved. Secondly, by leveraging the depth integrity-prior\ndistributed in the depth maps, we propose an integrity-prior loss to enhance\nthe uniformity of the segmentation results in the depth maps. Finally, we\nutilize the features of the shared encoder and, through a simple depth\nrefinement decoder, improve the ability of the shared encoder to capture subtle\ndepth-related information in the images. Experiments on the DIS-5K dataset show\nthat PDFNet significantly outperforms state-of-the-art non-diffusion methods.\nDue to the incorporation of the depth integrity-prior, PDFNet achieves or even\nsurpassing the performance of the latest diffusion-based methods while using\nless than 11% of the parameters of diffusion-based methods. The source code at\nhttps:\/\/github.com\/Tennine2077\/PDFNet.",
        "Stochastic differential equations (SDEs) describe dynamical systems where\ndeterministic flows, governed by a drift function, are superimposed with random\nfluctuations dictated by a diffusion function. The accurate estimation (or\ndiscovery) of these functions from data is a central problem in machine\nlearning, with wide application across natural and social sciences alike. Yet\ncurrent solutions are brittle, and typically rely on symbolic regression or\nBayesian non-parametrics. In this work, we introduce FIM-SDE (Foundation\nInference Model for SDEs), a transformer-based recognition model capable of\nperforming accurate zero-shot estimation of the drift and diffusion functions\nof SDEs, from noisy and sparse observations on empirical processes of different\ndimensionalities. Leveraging concepts from amortized inference and neural\noperators, we train FIM-SDE in a supervised fashion, to map a large set of\nnoisy and discretely observed SDE paths to their corresponding drift and\ndiffusion functions. We demonstrate that one and the same (pretrained) FIM-SDE\nachieves robust zero-shot function estimation (i.e. without any parameter\nfine-tuning) across a wide range of synthetic and real-world processes, from\ncanonical SDE systems (e.g. double-well dynamics or weakly perturbed Hopf\nbifurcations) to human motion recordings and oil price and wind speed\nfluctuations.",
        "Information-theoretic metrics, such as mutual information, have been widely\nused to evaluate privacy leakage in dynamic systems. However, these approaches\nare typically limited to stochastic systems and face computational challenges.\nIn this paper, we introduce a novel volumetric framework for analyzing privacy\nin systems affected by unknown but bounded noise. Our model considers a dynamic\nsystem comprising public and private states, where an observation set of the\npublic state is released. An adversary utilizes the observed public state to\ninfer an uncertainty set of the private state, referred to as the inference\nattack. We define the evolution dynamics of these inference attacks and\nquantify the privacy level of the private state using the volume of its\nuncertainty sets. For linear scalar systems, we derive an explicit formulation\nof the uncertainty set. For multi-dimensional linear systems, we develop an\napproximate computation method leveraging interval analysis. We investigate the\nproperties of the proposed volumetric privacy measure and demonstrate that it\nis bounded by the information gain derived from the observation set.\nFurthermore, we propose an optimization approach to designing privacy filter\nusing randomization and linear programming based on the proposed privacy\nmeasure. The effectiveness of the optimal privacy filter design is evaluated\nthrough a production-inventory case study, illustrating its robustness against\nthe inference attack."
      ]
    }
  },
  {
    "id":2411.15202,
    "research_type":"applied",
    "start_id":"b2",
    "start_title":"Ilf-lstm: Enhanced loss function in lstm to predict the sea surface temperature",
    "start_abstract":"Globe's primary issue is global warming, water temperatures have accompanied it as the sea surface temperature, and it is the primary attribute to balance the energy on the earth's surface. Sea surface temperature prediction is vital to climate forecast. Downwelling currents carry some of this heat to the ocean's bottom layers, which are also heating, covering far behind the increase in sea surface temperature. In deep learning models, the correct loss function will try to reduce the error and converge fast. The proposed improved loss function correctly estimates how close the predictions made by the long short-term memory match the observed values in the training data. This research considers location-specific sea surface temperature predictions using the improved loss function in the long short-term memory neural network at six different locations around India for daily, weekly, and monthly time horizons. Most existing research concentrated on periodic forecasts, but this paper focused on daily, weekly, and monthly predictions. The improved loss function\u2014long short-term memory, achieved 98.7% accuracy, and this improved loss function overcomes the limitations of the existing techniques and reduces the processing time to\u2009~\u20090.35 s. In this research, the sea surface temperature prediction using the improved loss function in the long short-term memory neural network gives better results than the standard prediction models and other existing techniques by considering the long-time dependencies and obtaining features from the spatial data.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "The Great Barrier Reef: an environmental history"
      ],
      "abstract":[
        "Reconstructing changes in the Great Barrier Reef 15 3 The natural context of 33 4 spread European settlement coastal Queensland 43 5 beche--de ... mer, pearl shell and trochus fisheries 55 6 Impacts on marine turtles 72 7 dugongs 95 8 whales, sharks fish 9 impacts coral collecting 10 guano rock phosphate mining 11 vi Contents 12 Other reefs 13 Changes island biota 14 Conclusion"
      ],
      "categories":[
        "q-bio.QM"
      ]
    },
    "list":{
      "title":[
        "Existence of Viscosity Solutions to Abstract Cauchy Problems via\n  Nonlinear Semigroups",
        "On the Baum-Connes conjecture for $D_{\\infty}$",
        "Abundance of spin liquids in the $S=1$ bilinear-biquadratic model on the\n  pyrochlore lattice, and its application to $\\mathrm{NaCaNi}_2\\mathrm{F}_7$",
        "Coherence DeepClean: Toward autonomous denoising of gravitational-wave\n  detector data",
        "On Stein spaces with finite homotopy rank-sum",
        "Distribution amplitudes of heavy-light pseudo-scalar and vector mesons\n  from Dyson-Schwinger equations framework",
        "Effects of particle angularity on granular self-organization",
        "Random matrices acting on sets: Independent columns",
        "Preons, Braid Topology, and Representations of Fundamental Particles",
        "COMPLETED CYCLES LEAKY HURWITZ NUMBERS",
        "Oriented diameter of the complete tripartite graph (III)",
        "Trends and Reversion in Financial Markets on Time Scales from Minutes to\n  Decades",
        "Thermostats without conjugate points",
        "Rethinking Approximate Gaussian Inference in Classification",
        "Dynamically Learning to Integrate in Recurrent Neural Networks",
        "Ultrafast neural sampling with spiking nanolasers",
        "Probing the Merger Rates of Supermassive Black Holes and Galaxies with\n  Gravitational Waves",
        "Discovering Dynamics with Kolmogorov Arnold Networks: Linear Multistep\n  Method-Based Algorithms and Error Estimation",
        "Probing $D_s^*$-meson longitudinal twist-2 LCDA",
        "Gradient-based Explanations for Deep Learning Survival Models",
        "Puncture loops on a non-orientable surface",
        "Understanding the Capabilities and Limitations of Weak-to-Strong\n  Generalization",
        "On Fractional Generalizations of the Logistic Map and their Applications",
        "Thermoelectric and heavy quark transport coefficients of hot QCD matter\n  in the presence of magnetic field",
        "When Do Transformers Outperform Feedforward and Recurrent Networks? A\n  Statistical Perspective",
        "Observation of the $\\Lambda_b^0 \\to J\/\\psi \\Xi^- K^+$ and $\\Xi_b^0 \\to\n  J\/\\psi \\Xi^- \\pi^+$ decays",
        "Study of 14.1 MeV Neutron Moderation in Beryllium",
        "Characterising the Surface Resistance of Laser-Treated LHC Beam Screens\n  with the Shielded Pair Method",
        "Towards an integrative approach to the study of brain-environment\n  interactions in human and non-human primate"
      ],
      "abstract":[
        "In this work, we provide conditions for nonlinear monotone semigroups on\nlocally convex vector lattices to give rise to a generalized notion of\nviscosity solutions to a related nonlinear partial differential equation. The\nsemigroup needs to satisfy a convexity estimate, so called $K$-convexity,\nw.r.t. another family of operators, defined on a potentially larger locally\nconvex vector lattice. We then show that, under mild continuity requirements on\nthe bounding family of operators, the semigroup yields viscosity solutions to\nthe abstract Cauchy problem given in terms of its generator in the larger\nlocally convex vector lattice. We apply our results to drift control problems\nfor infinite-dimensional L\\'evy processes and robust optimal control problems\nfor infinite-dimensional Ornstein-Uhlenbeck processes.",
        "We make an exposition of the proof of the Baum-Connes conjecture for the\ninfinite dihedral group following the ideas of Higson and Kasparov.",
        "Long considered the ''poor cousins'' of spin-1\/2 systems, magnets built of\nspin-1 moments have recently come to fore as a rich source of novel phases of\nmatter. Here we explore the phases which arise in a spin-1 magnet on the\npyrochlore lattice, once biquadratic interactions are taken into account. Using\na combination of variational and Monte Carlo techniques, built around the exact\ntreatment of spin-1 at the level of a single site, we uncover seven distinct\nspin liquid phases. Dynamical calculations for one of these spin liquids are\nshown to be in good agreement with inelastic neutron scattering on the spin-1\npyrochlore $\\mathrm{NaCaNi}_2\\mathrm{F}_7$. These results suggest that the\nrange of spin liquid phases found in spin-1 pyrochlores may be even richer than\nin materials with (pseudo-)spin-1\/2 moments.",
        "Technical and environmental noise in ground-based laser interferometers\ndesigned for gravitational-wave observations like Advanced LIGO, Advanced Virgo\nand KAGRA, can manifest as narrow (<1Hz) or broadband ($10'$s or even $100'$s\nof Hz) spectral lines and features in the instruments' strain amplitude\nspectral density. When the sources of this noise cannot be identified or\nremoved, in cases where there are witness sensors sensitive to this noise\nsource, denoising of the gravitational-wave strain channel can be performed in\nsoftware, enabling recovery of instrument sensitivity over affected frequency\nbands. This noise hunting and removal process can be particularly challenging\ndue to the wealth of auxiliary channels monitoring the interferometry and the\nenvironment and the non-linear couplings that may be present. In this work, we\npresent a comprehensive analysis approach and corresponding cyberinfrastructure\nto promptly identify and remove noise in software using machine learning\ntechniques. The approach builds on earlier work (referred to as DeepClean) in\nusing machine learning methods for linear and non-linear regression of noise.\nWe demonstrate how this procedure can be operated and optimized in a tandem\nfashion close to online data taking; it starts off with a coherence monitoring\nanalysis that first singles out and prioritizes witness channels that can then\nbe used by DeepClean. The resulting denoised strain by DeepClean reflects a\n1.4\\% improvement in the binary neutron star range, which can translate into a\n4.3\\% increase in the sensitive volume. This cyber infrastructure we refer to\nas Coherence DeepClean, or CDC, is a significant step toward autonomous\noperations of noise subtraction for ground-based interferometers.",
        "A topological space (not necessarily simply connected) is said to have finite\nhomotopy rank-sum if the sum of the ranks of all higher homotopy groups (from\nthe second homotopy group onward) is finite. In this article, we consider Stein\nspaces of arbitrary dimension satisfying the above rational homotopy theoretic\nproperty, although most of this article focuses on Stein surfaces only. We\ncharacterize all Stein surfaces satisfying the finite homotopy rank-sum\nproperty. In particular, if such a Stein surface is affine and every element of\nits fundamental group is finite, it is either simply connected or has a\nfundamental group of order $2$. A detailed classification of the smooth complex\naffine surfaces of the non-general type satisfying the finite homotopy rank-sum\nproperty is obtained. It turns out that these affine surfaces are\nEilenberg--MacLane spaces whenever the fundamental group is infinite.",
        "We systematically investigate leading-twist distribution amplitudes of ground\nstate heavy-light pseudo-scalar and vector mesons, the results of $B^*$,\n$B^*_s$, $B_c^*$ mesons are reported for the first time within the\nDyson-Schwinger equations framework. A novel numerical method for calculating\nMellin moments is proposed, which can avoid extrapolation or fitting in\nprevious similar studies. Based on it, we calculate the first eight Mellin\nmoments of mesons and reconstruct their distribution amplitudes. It is found\nthat, in flavor-asymmetric systems, distribution amplitude $\\phi(x)$ is skewed\nto one side, with the position of the maximum $\\sim M^f_E\/(M^f_E+M^g_E)$, where\n$M_E$ is Euclidean constituent quark mass and $f\/g$ denote the flavor of\nheavier\/lighter quark in the meson, respectively. For systems with the same\nvalence quark structure, the first Mellin moments follow the relation $\\langle\n\\xi \\rangle_{0^-} < \\langle \\xi \\rangle^{\\|}_{1^-} < \\langle \\xi\n\\rangle^{\\perp}_{1^-}$, where $\\xi = 2x - 1$ and $x$ is the momentum fraction\ncarried by the heavier quark. Our predictions can be compared with experimental\ndata and further theoretical calculations in the future, and the results of\nlight mesons such as $\\pi$, $K$, $\\rho$ are consistent with recent lattice\ndata.",
        "Recent studies of two-dimensional poly-disperse disc systems revealed a\ncoordinated self-organisation of cell stresses and shapes, with certain\ndistributions collapsing onto a master form for many processes, size\ndistributions, friction coefficients, and cell orders. Here we examine the\neffects of grain angularity on the indicators of self-organisation, using\nsimulations of bi-disperse regular $N$-polygons and varying $N$ systematically.\nWe find that: the strong correlation between local cell stresses and\norientations, as well as the collapses of the conditional distributions of\nscaled cell stress ratios to a master Weibull form for all cell orders $k$, are\nindependent of angularity and friction coefficient. In contrast, increasing\nangularity makes the collapses of the conditional distributions sensitive to\nchanges in the friction coefficient.",
        "We study random matrices with independent subgaussian columns. Assuming each\ncolumn has a fixed Euclidean norm, we establish conditions under which such\nmatrices act as near-isometries when restricted to a given subset of their\ndomain. We show that, with high probability, the maximum distortion caused by\nsuch a matrix is proportional to the Gaussian complexity of the subset, scaled\nby the subgaussian norm of the matrix columns. This linear dependence on the\nsubgaussian norm is a new phenomenon, as random matrices with independent rows\nor independent entries typically exhibit superlinear dependence. As a\nconsequence, normalizing the columns of random sparse matrices leads to\nstronger embedding guarantees.",
        "In particle phenomenology, preon models study compositional rules of standard\nmodel interactions. In spite of empirical success, mathematical underpinnings\nof preon models in terms of group representation theory have not been fully\nworked out. Here, we address this issue while clarifying the relation between\ndifferent preon models. In particular, we focus on two prominent models:\nBilson-Thompson's helon model, and Lambek's 4-vector model. We determine the\nmapping between helon model particle states and representation theory of Lie\nalgebras. Braided ribbon diagrams of the former represent on-shell states of\nspinors of the Lorentz group. Braids correspond to chirality, and twists, to\ncharges. We note that this model captures only the $SU(3)_c\\times U(1)_{em}$\nsector of the standard model. We then map the twists of helon diagrams to the\nweight polytope of $SU(3)_c \\times U(1)_{em}$. The braid structure maps to\nchiral states of fermions. We also show that Lambek's 4-vector can be recovered\nfrom helon diagrams. Alongside, we introduce a new 5-vector representation\nderived from the weight lattice. This representation contains both, the correct\ninteractions found in 4-vectors and the inclusion of chirality found in helons.\nAdditionally, we demonstrate topological analogues of CPT transformations in\nhelon diagrams. Interestingly, the braid diagrams of the helon model are the\nonly ones that are self-consistent with CPT invariance. In contrast to\nfield-theoretic approaches, the compositional character of preon models offers\nan analogous particle-centric perspective on fundamental interactions.",
        "We introduce $(r+1)$-completed cycles $k$-leaky Hurwitz numbers and prove\npiecewise polynomiality as well as establishing their chamber polynomiality\nstructure and their wall crossing formulae. For $k=0$ the results recover\nprevious results of Shadrin-Spitz-Zvonkine. The specialization for $r=1$\nrecovers Hurwitz numbers that are close to the ones studied by\nCavalieri-Markwig-Ranganathan and Cavalieri-Markwig-Schmitt. The ramifications\ndiffer by a lower order torus correction, natural from the Fock space\nperspective, not affecting the genus zero enumeration, nor the enumeration for\nleaky parameter values $k = \\pm 1$ in all genera.",
        "Given a bridgeless graph $G$, let $\\mathbb{D}(G)$ be the set of all strong\norientations of $G$, and define the oriented diameter $f(G)$ of $G$ to be the\nminimum of diameters $diam(D)$ among all the strong orientations $D\\in\n\\mathbb{D}(G)$, i.e., $f(G)=\\min\\{diam(D)\\mid D\\in \\mathbb{D}(G)\\}$. In this\npaper, we determine the oriented diameter of complete tripartite graph\n$K(3,p,q)$ for $p\\geqslant 5$. Combining with the previous results, the\noriented diameter of complete tripartite graph $K(3,p,q)$ are known.",
        "We empirically analyze the reversion of financial market trends with time\nhorizons ranging from minutes to decades. The analysis covers equities,\ninterest rates, currencies and commodities and combines 14 years of futures\ntick data, 30 years of daily futures prices, 330 years of monthly asset prices,\nand yearly financial data since medieval times.\n  Across asset classes, we find that markets are in a trending regime on time\nscales that range from a few hours to a few years, while they are in a\nreversion regime on shorter and longer time scales. In the trending regime,\nweak trends tend to persist, which can be explained by herding behavior of\ninvestors. However, in this regime trends tend to revert before they become\nstrong enough to be statistically significant, which can be interpreted as a\nreturn of asset prices to their intrinsic value. In the reversion regime, we\nfind the opposite pattern: weak trends tend to revert, while those trends that\nbecome statistically significant tend to persist.\n  Our results provide a set of empirical tests of theoretical models of\nfinancial markets. We interpret them in the light of a recently proposed\nlattice gas model, where the lattice represents the social network of traders,\nthe gas molecules represent the shares of financial assets, and efficient\nmarkets correspond to the critical point. If this model is accurate, the\nlattice gas must be near this critical point on time scales from 1 hour to a\nfew days, with a correlation time of a few years.",
        "We generalize Hopf's theorem to thermostats: the total thermostat curvature\nof a thermostat without conjugate points is non-positive, and vanishes only if\nthe thermostat curvature is identically zero. We further show that, if the\nthermostat curvature is zero, then the flow has no conjugate points, and the\nGreen bundles collapse almost everywhere. Given a thermostat without conjugate\npoints, we prove that the Green bundles are transversal everywhere if and only\nif it admits a dominated splitting. Finally, we provide an example showing that\nHopf's rigidity theorem on the 2-torus cannot be extended to thermostats. It is\nalso the first example of a thermostat with a dominated splitting which is not\nAnosov.",
        "In classification tasks, softmax functions are ubiquitously used as output\nactivations to produce predictive probabilities. Such outputs only capture\naleatoric uncertainty. To capture epistemic uncertainty, approximate Gaussian\ninference methods have been proposed, which output Gaussian distributions over\nthe logit space. Predictives are then obtained as the expectations of the\nGaussian distributions pushed forward through the softmax. However, such\nsoftmax Gaussian integrals cannot be solved analytically, and Monte Carlo (MC)\napproximations can be costly and noisy. We propose a simple change in the\nlearning objective which allows the exact computation of predictives and enjoys\nimproved training dynamics, with no runtime or memory overhead. This framework\nis compatible with a family of output activation functions that includes the\nsoftmax, as well as element-wise normCDF and sigmoid. Moreover, it allows for\napproximating the Gaussian pushforwards with Dirichlet distributions by\nanalytic moment matching. We evaluate our approach combined with several\napproximate Gaussian inference methods (Laplace, HET, SNGP) on large- and\nsmall-scale datasets (ImageNet, CIFAR-10), demonstrating improved uncertainty\nquantification capabilities compared to softmax MC sampling. Code is available\nat https:\/\/github.com\/bmucsanyi\/probit.",
        "Learning to remember over long timescales is fundamentally challenging for\nrecurrent neural networks (RNNs). While much prior work has explored why RNNs\nstruggle to learn long timescales and how to mitigate this, we still lack a\nclear understanding of the dynamics involved when RNNs learn long timescales\nvia gradient descent. Here we build a mathematical theory of the learning\ndynamics of linear RNNs trained to integrate white noise. We show that when the\ninitial recurrent weights are small, the dynamics of learning are described by\na low-dimensional system that tracks a single outlier eigenvalue of the\nrecurrent weights. This reveals the precise manner in which the long timescale\nassociated with white noise integration is learned. We extend our analyses to\nRNNs learning a damped oscillatory filter, and find rich dynamical equations\nfor the evolution of a conjugate pair of outlier eigenvalues. Taken together,\nour analyses build a rich mathematical framework for studying dynamical\nlearning problems salient for both machine learning and neuroscience.",
        "Owing to their significant advantages in terms of bandwidth, power efficiency\nand especially speed, optical neuromorphic systems have arisen as interesting\nalternatives to conventional semiconductor devices. Recently, photonic crystal\nnanolasers with excitable behaviour were first demonstrated. Depending on the\npumping strength, they emit short optical pulses -- spikes -- at various\nintervals on a nanosecond timescale. In this theoretical work, we show how\nnetworks of such photonic spiking neurons can be used for Bayesian inference\nthrough sampling from learned probability distributions. We provide a detailed\nderivation of translation rules from conventional sampling networks such as\nBoltzmann machines to photonic spiking networks and demonstrate their\nfunctionality across a range of generative tasks. Finally, we provide estimates\nof processing speed and power consumption, for which we expect improvements of\nseveral orders of magnitude over current state-of-the-art neuromorphic systems.",
        "The mergers of galaxies and supermassive black holes (SMBHs) are key drivers\nof galaxy evolution, contributing to the growth of both galaxies and their\ncentral black holes. Current projects like Pulsar Timing Arrays (PTAs) and\nupcoming missions such as the Laser Interferometer Space Antenna (LISA), Taiji,\nand Tianqin are designed to detect gravitational waves (GWs) emitted by SMBH\nbinaries during their inspiral and merger phases. We investigate the capability\nto probe the merger rates of SMBHs and their host galaxies by combining current\nPTA detections and mock GW data for LISA-like detectors, while incorporating\nobservational constraints from the $M_{\\bullet}-M_*$ relationship and galaxy\nstellar mass functions. Our findings highlight the critical role of GW\ndetections with LISA-like detectors in exploring the merger rates of galaxies\nand SMBHs and the timescale of SMBH mergers. Additionally, incorporating PTA\nconstraints on the stochastic gravitational wave background further refines\nmodel parameters and reduces uncertainties. Gravitational wave detections offer\nan independent method for estimating galaxy merger rates, providing a valuable\nconsistency check against rates derived from galaxy pair observations and\ncosmological simulations. Furthermore, comparing SMBH mass assembly through\nmergers with growth via accretion provides key insights into the evolutionary\nhistory of SMBHs, with the timescale of SMBH binary mergers playing a\nsignificant role in shaping their merger rates and merger mass assembly.",
        "Uncovering the underlying dynamics from observed data is a critical task in\nvarious scientific fields. Recent advances have shown that combining deep\nlearning techniques with linear multistep methods (LMMs) can be highly\neffective for this purpose. In this work, we propose a novel framework that\nintegrates Kolmogorov Arnold Networks (KANs) with LMMs for the discovery and\napproximation of dynamical systems' vector fields. Specifically, we begin by\nestablishing precise error bounds for two-layer B-spline KANs when\napproximating the governing functions of dynamical systems. Leveraging the\napproximation capabilities of KANs, we demonstrate that for certain families of\nLMMs, the total error is constrained within a specific range that accounts for\nboth the method's step size and the network's approximation accuracy.\nAdditionally, we analyze the difference between the numerical solution obtained\nfrom solving the ordinary differential equations with the fitted vector fields\nand the true solution of the dynamical system. To validate our theoretical\nresults, we provide several numerical examples that highlight the effectiveness\nof our approach.",
        "In this paper, we carry on an investigation of the semileptonic decays\n$B_s\\to D_s^*\\ell \\bar\\nu_{\\ell}$. Firstly, we derive the moments of the\n$D_s^*$-meson longitudinal leading-twist light-cone distribution amplitude\n(LCDA) based on QCD sum rules within background field theory framework.\nConsidering the contributions of the vacuum condensates up to dimension-six,\nits first ten non-zero $\\xi$-moments are given. Meanwhile, we construct the\n$D_s^*$-meson longitudinal leading-twist LCDA by using the light-cone harmonic\noscillator model. Then, using those moments, we fix the model parameters\n$\\alpha_{2;D_s^*}$ and $B_1^{2;D_s^*}$ by the least square method and apply\nthem to calculate $B_s \\to D_s^*$ transition form factors $A_1(q^2), A_2(q^2)$\nand $V(q^2)$ that are derived by using the QCD light-cone sum rules. At the\nlarge recoil region, we obtain $A_1(0) =0.632_{-0.135}^{+0.228}, A_2(0)\n=0.706_{-0.092}^{+0.109}$ and $V(0) =0.647_{-0.069}^{+0.076}$. Those form\nfactors are then extrapolated to the allowed whole physical $q^2$-region\nthrough the simplified series expansion. Finally, we obtain the branching\nfractions for the two decay channels $B_s\\to D_s^*\\ell\\bar\\nu_\\ell$, $\\it i.e.$\n${\\cal B}(B_s^0 \\to D_s^{*+}e^-\\bar\\nu_e)=(5.45_{-1.57}^{+2.15})\\times\n10^{-2}$, ${\\cal B}(B_s^0 \\to\nD_s^{*+}\\mu^-\\bar\\nu_\\mu)=(5.43_{-1.57}^{+2.14})\\times 10^{-2}$.",
        "Deep learning survival models often outperform classical methods in\ntime-to-event predictions, particularly in personalized medicine, but their\n\"black box\" nature hinders broader adoption. We propose a framework for\ngradient-based explanation methods tailored to survival neural networks,\nextending their use beyond regression and classification. We analyze the\nimplications of their theoretical assumptions for time-dependent explanations\nin the survival setting and propose effective visualizations incorporating the\ntemporal dimension. Experiments on synthetic data show that gradient-based\nmethods capture the magnitude and direction of local and global feature\neffects, including time dependencies. We introduce GradSHAP(t), a\ngradient-based counterpart to SurvSHAP(t), which outperforms SurvSHAP(t) and\nSurvLIME in a computational speed vs. accuracy trade-off. Finally, we apply\nthese methods to medical data with multi-modal inputs, revealing relevant\ntabular features and visual patterns, as well as their temporal dynamics.",
        "On a connected surface $N$ with negative Euler characteristic, the free\nhomotopy class of a loop obtained by smoothing an intersection of two closed\ngeodesics may wind around a puncture. Chas and Kabiraj showed that this\nphenomenon does not occur when the surface $N$ is orientable. In this paper, we\nprove that it occurs when $N$ is non-orientable and both geodesics involved in\nthe smoothing are actually one-sided. In particular, we study a loop obtained\nby traversing a one-sided closed geodesic and the $m$-th power of another\none-sided closed geodesic for odd $m$. Then we show that its free homotopy\nclass may wind aroud a puncture at most two values of $m$. Furthermore, if two\nsuch $m$'s exist, they are consecutive odd integers.",
        "Weak-to-strong generalization, where weakly supervised strong models\noutperform their weaker teachers, offers a promising approach to aligning\nsuperhuman models with human values. To deepen the understanding of this\napproach, we provide theoretical insights into its capabilities and\nlimitations. First, in the classification setting, we establish upper and lower\ngeneralization error bounds for the strong model, identifying the primary\nlimitations as stemming from the weak model's generalization error and the\noptimization objective itself. Additionally, we derive lower and upper bounds\non the calibration error of the strong model. These theoretical bounds reveal\ntwo critical insights: (1) the weak model should demonstrate strong\ngeneralization performance and maintain well-calibrated predictions, and (2)\nthe strong model's training process must strike a careful balance, as excessive\noptimization could undermine its generalization capability by over-relying on\nthe weak supervision signals. Finally, in the regression setting, we extend the\nwork of Charikar et al. (2024) to a loss function based on Kullback-Leibler\n(KL) divergence, offering guarantees that the strong student can outperform its\nweak teacher by at least the magnitude of their disagreement. We conduct\nsufficient experiments to validate our theory.",
        "The regular logistic map was introduced in 1960s, served as an example of a\ncomplex system, and was used as an instrument to demonstrate and investigate\nthe period doubling cascade of bifurcations scenario of transition to chaos. In\nthis paper, we review various fractional generalizations of the logistic map\nand their applications.",
        "The aim of this thesis is twofold: a) A comprehensive study of the\nthermoelectric response in QGP in the absence and presence of a background\nmagnetic field, b) Exploring the dynamics of heavy quarks traversing in QGP in\nthe presence of a weak background magnetic field.\n  We have evaluated the strength of the thermoelectric response in QGP\nquantified by the Seebeck and Nernst coefficients, first in the absence of a\nbackground magnetic field, and then in the presence of a strong magnetic field.\nThis is followed by the evaluation of the coefficients in the presence of a\nweak magnetic field. Each of the above-mentioned scenarios is investigated\nunder the assumption that the QGP is isotropic. This assumption is then relaxed\nby using an anisotropic distribution for the quarks, and the calculations are\nrepeated. The formalism adopted in the calculation of these coefficients is\nthat of kinetic theory, particularly, the relativistic Boltzmann equation in\nthe relaxation time approximation.\n  The other part of this thesis deals with heavy quark (HQ) dynamics in the\nQGP. HQs have been recognised as very good probes of the QGP owing to their\nlarge masses. We have calculated the HQ energy loss $dE\/dx$, longitudinal and\ntransverse momentum diffusion coeffcients $\\kappa L\/T^3$ , and spatial\ndiffusion coefficient $D_s$ , to leading order in the strong coupling $\\alpha$,\nfor both charm and bottom quarks. We consider Coulomb scattering of the HQ with\nthermal quarks and gluons to evaluate the scattering rate from which, all the\naforementioned coefficients are obtained. We find that the values of $\\kappa$'s\nincrease in the presence of a weak magnetic field (compared to the $B = 0$\ncase), and the anisotropy therein is also heightened in the presence of the\nmagnetic field. $D_s$ is found to decrease in the presence of magnetic field,\ncompared to its value at $B = 0$.",
        "Theoretical efforts to prove advantages of Transformers in comparison with\nclassical architectures such as feedforward and recurrent neural networks have\nmostly focused on representational power. In this work, we take an alternative\nperspective and prove that even with infinite compute, feedforward and\nrecurrent networks may suffer from larger sample complexity compared to\nTransformers, as the latter can adapt to a form of dynamic sparsity.\nSpecifically, we consider a sequence-to-sequence data generating model on\nsequences of length $N$, in which the output at each position depends only on\n$q$ relevant tokens with $q \\ll N$, and the positions of these tokens are\ndescribed in the input prompt. We prove that a single-layer Transformer can\nlearn this model if and only if its number of attention heads is at least $q$,\nin which case it achieves a sample complexity almost independent of $N$, while\nrecurrent networks require $N^{\\Omega(1)}$ samples on the same problem. If we\nsimplify this model, recurrent networks may achieve a complexity almost\nindependent of $N$, while feedforward networks still require $N$ samples.\nConsequently, our proposed sparse retrieval model illustrates a natural\nhierarchy in sample complexity across these architectures.",
        "The first observation of the $\\Xi_b^0 \\to J\/\\psi \\Xi^- \\pi^+$ decay and the\nmost precise measurement of the branching fraction of the $\\Lambda_b^0 \\to\nJ\/\\psi \\Xi^- K^+$ decay are reported, using proton-proton collision data from\nthe LHCb experiment collected in 2016--2018 at a centre-of-mass energy of\n13~TeV, corresponding to an integrated luminosity of 5.4~fb$^{-1}$. Using the\n$\\Lambda_b^0 \\to J\/\\psi \\Lambda$ and $\\Xi_b^0 \\to J\/\\psi \\Xi^-$ decays as\nnormalisation channels, the ratios of branching fractions are measured to be:\n\\[ \\frac{\\mathcal{B}(\\Lambda_b^0 \\to J\/\\psi \\Xi^- K^+)}{\\mathcal{B}(\\Lambda_b^0\n\\to J\/\\psi \\Lambda)} = (1.17 \\pm 0.14 \\pm 0.08)\\times 10^{-2} \\, , \\] \\[\n\\frac{\\mathcal{B}(\\Xi_b^0 \\to J\/\\psi \\Xi^- \\pi^+)}{\\mathcal{B}(\\Xi_b^0 \\to\nJ\/\\psi \\Xi^-)} = (11.9 \\pm 1.4 \\pm 0.6)\\times 10^{-2}\\, , \\] where the first\nuncertainty is statistical and the second systematic.",
        "This study investigates the moderation of 14.1 MeV neutrons in a natural\nberyllium moderator arranged in a spherical geometry. The neutron interactions\nand moderation efficiency were analyzed using Monte Carlo simulations with the\nGEANT4 toolkit. Various sphere radii were tested to determine the optimal\nmoderator thickness for neutron thermalization.",
        "The presence of strong electron clouds in the quadrupole magnetic field\nregions of the Large Hadron Collider (LHC) leads to considerable heating that\nposes challenges for the cryogenic cooling system, and under certain conditions\nto proton beam quality deterioration. Research is being conducted on\nlaser-treated inner beam screen surfaces for the upgraded High-Luminosity LHC\nto mitigate this issue. Laser-induced surface structuring, a technique that\neffectively roughens surfaces, has been shown to reduce secondary electron\nemission; an essential factor in controlling electron cloud formation.\nConversely, the resulting surface roughening also alters the material's surface\nimpedance, potentially impacting beam stability and increasing beam-induced\nresistive wall heating. Different laser treatment patterns have been applied to\nLHC beam screens to estimate this potential impact and assessed for their\nmicrowave responses.",
        "By retracing my scientific journey that began 20 years ago, I highlight in\nthis thesis the need to consider the organization of the brain, which is\ncertainly globally hierarchical, but also highly distributed and mixed in the\nneuronal response of its different areas (by focusing on the sensorimotor\ncortex-basal ganglia network). I also emphasize the importance of adopting\nbehavioral paradigms that reflect as much as possible the characteristics of\nthe scenarios encountered in the real life of animals. And finally, I mention\nthe importance of \"disintegrating\" the way data analysis is traditionally\ncarried out, and of taking into account the dynamic nature of behavior, for\nexample by favoring the study of behavioral variables and so-called \"latent\"\nneuronal activities. I conclude this thesis by presenting the vision of my\nideal laboratory in a perspective of 5 to 10 years from today. This laboratory\nwould have two experimental devices, a first, classic, for carrying out tests\nin a constrained and therefore unnatural environment, the data of which would\nbe compared to those from a second device called \"naturalistic\" in which the\nanimals could potentially express their entire behavioral repertoire. The tasks\ntested would be characterized by strong ecological principles, such as in those\nsimulating the properties of foraging, and the data would make it possible to\ntest hypotheses based on the progressive addition of ecological components in\nthese tasks, all this in order to maintain control over the interpretability of\nthese data which are complex by nature."
      ]
    }
  },
  {
    "id":2411.06741,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"Learning Polynomials with Neural Networks",
    "start_abstract":"We study the effectiveness of learning low degree polynomials using neural networks by gradient descent method. While have been shown to great expressive power, and has widely used in practice for networks, few theoretical guarantees are known such methods. In particular, it is well that can get stuck at local minima, even simple classes target functions. this paper, we present several positive results support networks. focus on twolayer where bottom layer a set non-linear hidden nodes, top node linear function, similar Barron (1993). First show randomly initialized network with sufficiently many units, generic algorithm learns any polynomial, assuming initialize weights randomly. Secondly, if use complex-valued (the function still be real), then under suitable conditions, there no robust minima: always escape minimum performing random perturbation. This property does not hold real-valued weights. Thirdly, discuss whether sparse learned small size dependent sparsity function.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b20"
      ],
      "title":[
        "Physics-informed machine learning"
      ],
      "abstract":[
        "Despite great progress in simulating multiphysics problems using the numerical discretization of partial differential equations (PDEs), one still cannot seamlessly incorporate noisy data into existing algorithms, mesh generation remains complex, and high-dimensional problems governed by parameterized PDEs cannot be tackled. Moreover, solving inverse problems with hidden physics is often prohibitively expensive and requires different formulations and elaborate computer codes. Machine learning has emerged as a promising alternative, but training deep neural networks requires big data, not always available for scientific problems. Instead, such networks can be trained from additional information obtained by enforcing the physical laws (for example, at random points in the continuous space-time domain). Such physics-informed learning integrates (noisy) data and mathematical models, and implements them through neural networks or other kernel-based regression networks. Moreover, it may be possible to design specialized network architectures that automatically satisfy some of the physical invariants for better accuracy, faster training and improved generalization. Here, we review some of the prevailing trends in embedding physics into machine learning, present some of the current capabilities and limitations and discuss diverse applications of physics-informed learning both for forward and inverse problems, including discovering hidden physics and tackling high-dimensional problems."
      ],
      "categories":[
        "physics.chem-ph"
      ]
    },
    "list":{
      "title":[
        "Non-Commutative fluid: an alternative source of cosmic acceleration",
        "Gradient estimates for the fractional $p$-Poisson equation",
        "Optimizing compilation of error correction codes for 2xN quantum dot\n  arrays and its NP-hardness",
        "FOSS solution for Molecular Dynamics Simulation Automation and\n  Collaboration with MDSGAT",
        "Leveraging the Bias-Variance Tradeoff in Quantum Chemistry for Accurate\n  Negative Singlet-Triplet Gap Predictions: A Case for Double-Hybrid DFT",
        "Galaxy mass profiles with convolutional neural networks",
        "Auto-Balancer: Harnessing idle network resources for enhanced market\n  stability",
        "On a planetary forcing of global seismicity",
        "Magnetic Interactions in the Polar Ferrimagnet with a Bipartite\n  Structure",
        "Chiral supersolid and dissipative time crystal in Rydberg-dressed\n  Bose-Einstein condensates with Raman-induced spin-orbit coupling",
        "Sub-MHz Radio Background from Ultralight Dark Photon Dark Matter",
        "Kolyvagin's conjecture for modular forms at non-ordinary primes",
        "Global $C^{1,\\alpha}$ regularity for Monge-Amp\\`ere equations on planar\n  convex domains",
        "Investigation of Inverse Velocity Dispersion in a Solar Energetic\n  Particle Event Observed by Solar Orbiter",
        "Averaging over the circles the gaussian free field in the Poincar{\\'e}\n  disk",
        "Functional limit theorems for Gaussian-fed queueing network in light and\n  heavy traffic",
        "Orbits of very distant asteroid satellites",
        "VEGA: Voids idEntification using Genetic Algorithm",
        "Can circumstellar interaction explain the strange light curve features\n  of Type Ib\/c supernovae?",
        "Entropic costs of the quantum-to-classical transition in a microscopic\n  clock",
        "Partial Resolution of the Erd\\\"os-Straus, Sierpinski, and Generalized\n  Erd\\\"os-Straus Conjectures Using New Analytical Formulas",
        "Irreducible components of affine Lusztig varieties",
        "Network Goodness-of-Fit for the block-model family",
        "A multi-component phase-field model for T1 precipitates in Al-Cu-Li\n  alloys",
        "Framed Blob Monoids",
        "Two- and three-meson scattering amplitudes with physical quark masses\n  from lattice QCD",
        "Magnetic imaging under high pressure with a spin-based quantum sensor\n  integrated in a van der Waals heterostructure",
        "Multi-Scale Conformal Prediction: A Theoretical Framework with Coverage\n  Guarantees",
        "Observable Primordial Gravitational Waves from Non-minimally Coupled\n  $R^2$ Palatini Modified Gravity"
      ],
      "abstract":[
        "We have developed a Hubble function based on Newtonian Cosmology using\nnon-commutative fluid equations. Our Hubble function contains cosmic fluids\nwith the signature of a new cosmological parameter $\\sigma$, motivated by a\nnon-commutative Poisson bracket structure. Interestingly, this Hubble function\ndoes not include any external fluid content related to dark energy or the\nCosmological constant; the parameter $\\sigma$ acts as the source of accelerated\nexpansion. In this work, we aim to explain the phenomenon of the accelerating\nexpansion of the universe without \"dark energy\". Additionally, we have verified\nthe observational bounds for $\\sigma$ to assess its potential in explaining the\naccelerated expansion.",
        "We consider local weak solutions to the fractional $p$-Poisson equation of\norder $s$, i.e. $\\left( - \\Delta_p\\right)^s u = f$. In the range $p>1$ and\n$s\\in \\big(\\frac{p-1}{p},1\\big)$ we prove Calder\\'on & Zygmund type estimates\nat the gradient level. More precisely, we show for any $q>1$ that\n\\begin{equation*}\n  f\\in L^{\\frac{qp}{p-1}}_{\\rm loc}\n  \\quad\\Longrightarrow\\quad\n  \\nabla u\\in L^{qp}_{\\rm loc}. \\end{equation*} The qualitative result is\naccompanied by a local quantitative estimate.",
        "The ability to physically move qubits within a register allows the design of\nhardware-specific error correction codes which can achieve fault-tolerance\nwhile respecting other constraints. In particular, recent advancements have\ndemonstrated the shuttling of electron and hole spin qubits through a quantum\ndot array with high fidelity. Exploiting this, we design an error correction\narchitecture, consisting merely of two parallel quantum dot arrays, an\nexperimentally validated architecture compatible with classical wiring and\ncontrol constraints. We develop a suite of heuristic methods for compiling any\nstabilizer error-correcting code's syndrome-extraction circuit to run with a\nminimal number of shuttling operations. In simulation, these heuristics show\nthat fault tolerance can be achieved on several contemporary quantum\nerror-correcting codes requiring only modestly-optimistic noise parameters.\nFurthermore, we demonstrate how constant column-weight qLDPC codes can be\ncompiled in a provably minimal number of shuttles that scales constantly with\ncode size using Shor-style syndrome extraction. In addition, we provide a proof\nof the NP hardness of minimizing the number of shuttle operations for codes not\nin that class.",
        "The process of setting up and successfully running Molecular Dynamics\nSimulations (MDS) is outlined to be incredibly labour and computationally\nexpensive with a very high barrier to entry for newcomers wishing to utilise\nthe benefits and insights of MDS. Here, presented, is a unique Free and\nOpen-Source Software (FOSS) solution that aims to not only reduce the barrier\nof entry for new Molecular Dynamics (MD) users, but also significantly reduce\nthe setup time and hardware utilisation overhead for even highly experienced MD\nresearchers. This is accomplished through the creation of the Molecular\nDynamics Simulation Generator and Analysis Tool (MDSGAT) which currently serves\nas a viable alternative to other restrictive or privatised MDS Graphical\nsolutions with a unique design that allows for seamless collaboration and\ndistribution of exact MD simulation setups and initialisation parameters\nthrough a single setup file. This solution is designed from the start with a\nmodular mindset allowing for additional software expansion to incorporate\nnumerous extra MDS packages and analysis methods over time",
        "Molecules that violate Hund's rule -- having first excited singlet state\n(S$_1$) below the triplet state (T$_1$) -- are rare yet promising as efficient\nlight emitters. Their high-throughput identification demands exceptionally\naccurate excited-state modeling to minimize false positives and negatives.\nBenchmarking twelve S$_1$-T$_1$ energy gaps, we find that local variants of\nADC(2) and CC2 deliver excellent accuracy and speed for screening medium-sized\nmolecules. Notably, while double-hybrid DFT approximations (e.g., B2GP-PLYP and\nPBE-QIDH) exhibit high mean errors ($>100$ meV) despite very low standard\ndeviations ($\\approx10$ meV), exploring their parameter space reveals that a\nconfiguration with 75% exchange and 55% correlation reduces the mean error to\nbelow $5$ meV -- albeit with increased variance. Using this low-bias\nparameterization as an internal reference, we correct the systematic error\nwhile maintaining low variance, effectively combining the strengths of both\nlow-bias and low-variance DFT parameterizations to enhance overall accuracy.\nOur findings suggest that low-variance DFT methods -- often overlooked due to\nhigh bias -- can serve as reliable tools for predictive modeling in\nfirst-principles molecular design.",
        "Determining the dynamical mass profiles of dispersion-supported galaxies is\nparticularly challenging due to projection effects and the unknown shape of\ntheir velocity anisotropy profile. Our goal is to develop a machine learning\nalgorithm capable of recovering dynamical mass profiles of dispersion-supported\ngalaxies from line-of-sight stellar data. Traditionally, this task relies on\ntime-consuming methods that require profile parameterization and assume\ndynamical equilibrium and spherical symmetry. We train a convolutional neural\nnetwork model using various sets of cosmological hydrodynamical simulations of\ngalaxies. By extracting projected stellar data from the simulated galaxies and\nfeeding it into the model, we obtain the posterior distribution of the\ndynamical mass profile at ten different radii. Additionally, we evaluate the\nperformance of existing literature mass estimators on our dataset. Our model\nachieves more accurate results than any literature mass estimator while also\nproviding enclosed mass estimates at radii where no previous estimators exist.\nWe confirm that the posterior distributions produced by the model are\nwell-calibrated, ensuring they provide meaningful uncertainties. However,\nissues remain, as the method loses performance when trained on one set of\nsimulations and applied to another, highlighting the importance of improving\nthe generalization of ML methods trained on specific galaxy simulations.",
        "We propose a mechanism embedded into the foundational infrastructure of a\nblockchain network, designed to improve the utility of idle network resources,\nwhilst enhancing market microstructure efficiency during block production by\nleveraging both network-owned and external capital. By systematically seeking\nto use idle network resources for internally capture arbitrageable\ninefficiencies, the mechanism mitigates extractable value leakage, reduces\nexecution frictions, and improves price formation across venues. This framework\noptimises resource allocation by incentivising an ordered set of transactions\nto be identified and automatically executed at the end of each block,\nredirecting any realised arbitrage income - to marketplaces operating on the\nhost blockchain network (and other stakeholders), which may have otherwise been\nextracted as rent by external actors. Crucially, this process operates without\nintroducing additional inventory risk, ensuring that the network remains a\nneutral facilitator of price discovery. While the systematic framework\ngoverning the distribution of these internally captured returns is beyond the\nscope of this work, reinvesting them to support the ecosystem deployed on the\nhost blockchain network is envisioned to endogenously enhance liquidity,\nstrengthen transactional efficiency, and promote the organic adoption of the\nblockchain for end users. This mechanism is designed specifically for Supra's\nblockchain and seeks to maximally utilise its highly efficient automation\nframework to enhance the blockchain network's efficiency.",
        "We have explored the temporal variability of the seismicity at global scale\nover the last 124 years, as well as its potential drivers. To achieve this, we\nconstructed and analyzed an averaged global seismicity curve for earthquakes of\nmagnitude equal or greater than 6 since 1900. Using Singular Spectrum Analysis,\nwe decomposed this curve and compared the extracted pseudo-cycles with two\nglobal geophysical parameters associated with Earth's tides: length-of-day\nvariations and sea-level changes. Our results reveal that these three\ngeophysical phenomena can be be explained with 90% accuracy, as the sum of up\nto seven periodic components, largely aligned with planetary ephemerides: 1\nyear, 3.4 years (Quasi-Biennial Oscillation, QBO), $\\sim$11 years, $\\sim$14\nyears, $\\sim$18.6 years (lunar nodal cycle), $\\sim$33 years, and $\\sim$60\nyears. We discuss these results in the framework of Laplace's theory, with a\nparticular focus on the phase relationships between seismicity, length-of-day\nvariations, and sea-level changes to further elucidate the underlying physical\nmechanisms. Finally,integrating observations from seismogenic regions, we\npropose a trigger mechanism based on solid Earth-hydrosphere interactions,\nemphasizing the key role of water-rock interactions in modulating earthquake\noccurrence.",
        "The polar magnets A$_2$Mo$_3$O$_8$ (A=Fe, Mn, Co, and Ni) feature a bipartite\nstructure, where the magnetic A$^{2+}$ ions occupy two different sites with\noctahedral and tetrahedral oxygen coordinations. This bipartite structure\nprovides a platform for the emergence of nontrivial magnetoelectric (ME)\neffects and intriguing excitation behaviors, and thus creates significant\nresearch interest. In this study, we conduct inelastic neutron scattering\nmeasurements on single crystals of Mn$_2$Mo$_3$O$_8$, an L-type ferrimagnet in\nthe A$_2$Mo$_3$O$_8$ family, to investigate its spin dynamics. The obtained\nmagnetic excitation spectra reveal two distinct magnon dispersions\ncorresponding to the octahedral and tetrahedral spins in Mn$_2$Mo$_3$O$_8$.\nThese magnon bands can be well described by a spin Hamiltonian including\nHeisenberg and single-ion anisotropy terms. Employing our effective spin model,\nwe successfully reproduce the unusual temperature dependence of the L-type\nferrimagnetic susceptibility through self-consistent mean-field theory. This\nresearch reveals the significance of the bipartite structure in determining the\nexcitation properties of the polar magnets $\\rm{A_{2}Mo_{3}O_{8}}$ and provides\nvaluable insights into the spin dynamics of L-type ferrimagnets.",
        "Spin-orbit coupling (SOC) is one of the key factors that affect the chiral\nsymmetry of matter by causing the spatial symmetry breaking of the system. We\nfind that Raman-induced SOC can induce a chiral supersolid phase with a helical\nantiskyrmion lattice in balanced Rydberg-dressed two-component Bose-Einstein\ncondensates (BECs) in a harmonic trap by modulating the Raman coupling\nstrength, strong contrast with the mirror symmetric supersolid phase containing\nskyrmion-antiskyrmion lattice pair for the case of Rashba SOC. Two ground-state\nphase diagrams are presented as a function of the Rydberg interaction strength\nand the SOC strength, as well as that of the Rydberg interaction strength and\nthe Raman coupling strength, respectively. It is shown that the interplay among\nRaman-induced SOC, soft-core long-range Rydberg interactions, and contact\ninteractions favors rich ground-state structures including half-quantum vortex\nphase, stripe supersolid phase, toroidal stripe phase with a central\nAnderson-Toulouse coreless vortex, checkerboard supersolid phase, mirror\nsymmetric supersolid phase, chiral supersolid phase and standing-wave\nsupersolid phase. In addition, the effects of rotation and in-plane quadrupole\nmagnetic field on the ground state of the system are analyzed. In these two\ncases, the chiral supersolid phase is broken and the ground state tends to form\na miscible phase. Furthermore, the stability and superfluid properties of the\ntwo-component BECs with Raman-induced SOC and Rydberg interactions in free\nspace are revealed by solving the Bogoliubov-de Gennes equation. Finally, we\ndemonstrate that when the initial state is a chiral supersolid phase the\nrotating harmonic trapped system sustains dissipative continuous time crystal\nby studying the rotational dynamic behaviors of the system.",
        "Dark photons are a well-motivated candidate for dark matter, but their\ndetection becomes challenging for ultralight masses with both experimental and\nastrophysical probes. In this work, we propose a new approach to explore this\nregime through the dark inverse Compton scattering of ultralight dark photons\nwith cosmic ray electrons and positrons. We show this process generates a\npotentially observable background radiation that is most prominent at\nfrequencies below MHz. We compute this effect using the latest cosmic ray\nmodels and radio absorption maps. Comparing it to observations of the Milky\nWay's radio spectrum from Explorer 43, Radio Astronomy Explorer 2, and the\nParker Solar Probe, we place leading constraints on the kinetic mixing of dark\nphoton dark matter for masses $\\lesssim 2 \\times 10^{-17} \\ \\rm eV$.",
        "In this article we prove a version of Kolyvagin's conjecture for modular\nforms at non-ordinary primes. In particular, we generalize the work of Wang on\na converse to a higher weight Gross-Zagier-Kolyvagin theorem in order to prove\nthe conjecture under the hypothesis that some Selmer group has rank one. The\nmain ingredients that we use in non-ordinary setting are the signed Selmer\ngroups introduced by Lei, Loeffler and Zerbes. We will also use a result of\nWan, i.e., the $p$-part of the Tamagawa number conjecture for non-ordinary\nmodular forms with analytic rank zero. Starting from the rank one case we will\nshow how to prove the full version of the conjecture.",
        "In this paper, we establish the global H\\\"older gradient estimate for\nsolutions to the Dirichlet problem of the Monge-Amp\\`ere equation $\\det D^2u =\nf$ on strictly convex but not uniformly convex domain $\\Omega$.",
        "Inverse velocity dispersion (IVD) events, characterized by higher-energy\nparticles arriving later than lower-energy particles, challenge the classical\nunderstanding of SEP events and are increasingly observed by spacecraft, such\nas Parker Solar Probe (PSP) and Solar Orbiter (SolO). However, the mechanisms\nunderlying IVD events remain poorly understood. This study aims to investigate\nthe physical processes responsible for long-duration IVD events by analyzing\nthe SEP event observed by SolO on 2022 June 7. We explore the role of evolving\nshock connectivity, particle acceleration at interplanetary (IP) shocks, and\ncross-field transport in shaping the observed particle profiles.We utilize data\nfrom Energetic Particle Detector (EPD) suite onboard SolO to analyze the\ncharacteristics of the IVD, and model the event using the Heliospheric\nEnergetic Particle Acceleration and Transport (HEPAT) model. The IVD event\nexhibited a distinct and long-duration IVD signature, across proton energies\nfrom 1 to 20 MeV and lasting for approximately 10 hours. Simulations suggest\nthat evolving shock connectivity and the evolution of shock play a primary role\nin the IVD signature, with SolO transitioning from shock flank to nose over\ntime, resulting in a gradual increase in maximum particle energy along the\nfield line. Furthermore, model results show that limited cross-field diffusion\ncan influence both the nose energy and the duration of the IVD event. This\nstudy demonstrates that long-duration IVD events are primarily driven by\nevolving magnetic connectivity along a non-uniform shock that evolves over\ntime, where the connection moves to more efficient acceleration sites as the\nshock propagates farther from the Sun. Other mechanisms, such as acceleration\ntime at the shock, may also contribute to the observed IVD features.",
        "The gaussian free field on the unit disk $D$ can be seen as a two-dimensional\nversion of the Brownian bridge on the interval [0,1]. It is intrinsically\nassociated with the Sobolev space $H_0^1 (D)$. To define the latter, we can\nchoose any metric conformally equivalent to the Euclidean metric on $D$. This\nnote is an introduction to the gaussian free field on the unit disk whose aim\nis to highlight some of the conveniences offered by hyperbolic geometryon $D$\nto describe the first properties of this probabilistic object.",
        "We consider a queueing network operating under a strictly upper-triangular\nrouting matrix with per column at most one non-negative entry. The root node is\nfed by a Gaussian process with stationary increments. Our aim is to\ncharacterize the distribution of the multivariate stationary workload process\nunder a specific scaling of the queue's service rates. In the main results of\nthis paper we identify, under mild conditions on the standard deviation\nfunction of the driving Gaussian process, in both light and heavy traffic\nparameterization, the limiting law of an appropriately scaled version (in both\ntime and space) of the joint stationary workload process. In particular, we\ndevelop conditions under which specific queueing processes of the network\neffectively decouple, i.e., become independent in the limiting regime.",
        "The very wide binary asteroid (VWBA) population is a small subset of the\npopulation of known binary and multiple asteroids made of systems with very\nwidely orbiting satellites and long orbital periods, on the order of tens to\nhundreds of days. The origin of these systems is debatable, and most members of\nthis population are poorly characterized. We have compiled all available\nhigh-angular resolution imaging archival data of VWBA systems from large\nground- and space-based telescopes. We measure the astrometric positions of the\nsatellite relative to the primary and analyze the dynamics of the satellites\nusing the Genoid genetic algorithm. Additionally, we use a NEATM thermal model\nto estimate the diameters of two systems, and we model the orbit of Litva's\ninner satellite using photometric lightcurve observations. We determine the\neffective diameters of binary systems Christophedumas and Alconrad to be 4.7 +\n0.4km and 5.2 + 0.3km respectively. We determine new orbital solutions for five\nsystems, Huenna, Litva, (3548) Eurybates, Pauling, and Alconrad. We find a\nsignificantly eccentric best-fit orbital solution for the outer satellite of\nLitva, a moderately eccentric solution for Alconrad, and a nearly circular\nsolution for Pauling. We also confirm previously reported orbital solutions for\n(379) Huenna and Eurybates. It is unlikely that BYORP expansion could be solely\nresponsible for the formation of VWBAs. It is possible that the satellites of\nthese systems were formed through YORP spin-up and then later scattered onto\nvery wide orbits. Additionally, we find that some members of the population are\nunlikely to have formed satellites through YORP spin-up, and a collisional\nformation history is favored. In particular, this applies to VWBAs within large\ndynamical families, or large VWBA systems such as Huenna and NASA's Lucy\nmission target Eurybates.",
        "Cosmic voids, the nearly empty regions nestled between walls and filaments,\nare recognized for their extensive applications in the field of cosmology and\nastrophysics. However, a consensus on the definition of voids remains elusive,\nas various void-finding methods identify different types of voids, each\ndiffering in shape and density based on the method that were used. In this\npaper, we introduce an innovative void identification method that utilizes\nGenetic Algorithm analysis. VEGA employs the Voronoi tessellation technique and\nthe Convex Hull algorithm to partition the dataset plane into distinct regions\nand calculate the volume of each region. For the first time, VEGA integrates\nGenetic Algorithm analysis with the luminosity density contrast function to\nidentify and locate the possible void region candidates. This method utilizes a\nset of grid points, which enhances the implementation of Voronoi tessellation\nand enables VEGA to more effectively access the dataset space for the\nidentification of void regions candidates, finding the center and the ultimate\nstructure of voids. Finally, we applied the VEGA and Aikio-M\\\"ah\\\"onen (AM)\nmethods to the same test dataset and compared the cosmic voids identified by\nVEGA with those identified by the AM method. This comparison demonstrated that\nthe VEGA void-finding method yields reliable results and can be effectively\napplied to various particle distributions.",
        "Context. The evolution and the surrounding of stripped-envelope supernova\nprogenitors are still under debate: some studies suggest single-star, while\nothers prefer massive binary progenitors. Moreover, the basic physical\nproperties of the exploding star and its interaction with circumstellar matter\ncould significantly modify the overall light curve features of these objects.\nTo better understand the effect of stellar evolution and circumstellar\ninteraction, systematic hydrodynamic calculations are needed.\n  Aims. Here, we test the hypothesis that circumstellar matter generated by an\nextreme episodic $\\eta$ Carinae-like eruption that occurs days or weeks before\nthe supernova explosion may explain the controversies related to the general\nlight curve features of stripped-envelope supernovae.\n  Methods. In this work, we present our bolometric light curve calculations of\nboth single- and binary progenitors generated by hydrodynamic simulations via\nMESA and SNEC. We also studied the effect of an interaction with a close,\nlow-mass circumstellar matter assumed to be created just a few days or weeks\nbefore the explosion. Beyond generating a model light curve grid, we compared\nour results with some observational data.\n  Results. We found that merely the shape of the supernova light curve could\nindicate that the cataclysmic death of the massive star happened in a binary\nsystem or was related to the explosion of a single star. Moreover, our study\nalso shows that a confined dense circumstellar matter may be responsible for\nthe strange light curve features (bumps, re-brightening, or steeper tail) of\nsome Type Ib\/c supernovae.",
        "We experimentally realize a quantum clock by using a charge sensor to count\ncharges tunneling through a double quantum dot (DQD). Individual tunneling\nevents are used as the clock's ticks. We quantify the clock's precision while\nmeasuring the power dissipated by the DQD and, separately, the charge sensor in\nboth direct-current and radio-frequency readout modes. This allows us to probe\nthe thermodynamic cost of creating ticks microscopically and recording them\nmacroscopically, which we refer to as the quantum-to-classical transition. Our\nexperiment is the first to explore the interplay between the entropy produced\nby a microscopic clockwork and its macroscopic measurement apparatus. We show\nthat the latter contribution not only dwarfs the former but also unlocks\ngreatly increased precision, because the measurement record can be exploited to\noptimally estimate time even when the DQD is at equilibrium. Our results\nsuggest that the entropy produced by the amplification and measurement of a\nclock's ticks, which has often been ignored in the literature, is the most\nimportant and fundamental thermodynamic cost of timekeeping at the quantum\nscale.",
        "This article proposes a unified analytical approach leading to a partial\nresolution of the Erdos-Straus, Sierpinski conjectures, and their\ngeneralization. We introduce two new analytical formulas under specific\nconditions of divisibility and the existence of perfect squares. Under these\nconditions, the formulas verify the conjectures even for very large numerical\nvalues. Moreover, our method reduces the problem to the search for a suitable\nperfect square, thereby opening the way to a complete proof of these\nconjectures. Notably, our second formula significantly improves upon Mordell's\nwork by demonstrating analytically the conjecture in the majority of cases\nwhere Mordell's approach fails. Furthermore, these formulas are highly\nversatile, as they provide, under the established conditions, a systematic\nmethod to decompose any fraction a\/n into a sum of three Egyptian fractions. In\nconclusion, we present open questions and conjectures to the mathematical\ncommunity regarding the generalization of these formulas.",
        "Let $\\breve{G}$ be a loop group and $\\tilde W$ be its Iwahori-Weyl group. The\naffine Lusztig variety $Y_w(\\gamma)$ describes the intersection of the Bruhat\ncell $\\mathcal{I} \\dot{w} \\mathcal{I}$ for $w \\in \\tilde W$ with the conjugacy\nclass of $\\gamma \\in \\breve{G}$, while the affine Deligne-Lusztig variety\n$X_w(b)$ describes the intersection of the Bruhat cell $\\mathcal{I} \\dot{w}\n\\mathcal{I}$ with the Frobenius-twisted conjugacy class of $b \\in \\breve{G}$.\nAlthough the geometric connections between these varieties are unknown,\nnumerical relations exist in their geometric properties.\n  This paper explores the irreducible components of affine Lusztig varieties.\nThe centralizer of $\\g$ acts on $Y_w(\\g)$ and the Frobenius-twisted centralizer\nof $b$ acts on $X_w(b)$. We relate the number of orbits on the top-dimensional\ncomponents of $Y_w(\\gamma)$ to the numbers of orbits on top-dimensional\ncomponents of $X_w(b)$ and the affine Springer fibers. For split groups and\nelements $\\gamma$ with integral Newton points, we show that, for most $w$, the\nnumbers of orbits for the affine Lusztig variety and the associated affine\nDeligne-Lusztig variety match. Moreover, for these $\\g$, we verify Chi's\nconjecture that the number of top-dimensional components in $Y_\\mu(\\gamma)$\nwithin the affine Grassmannian equals to the dimension of a specific weight\nspace in a representation of the Langlands dual group.",
        "The block-model family has four popular network models (SBM, DCBM, MMSBM, and\nDCMM). A fundamental problem is, how well each of these models fits with real\nnetworks. We propose GoF-MSCORE as a new Goodness-of-Fit (GoF) metric for DCMM\n(the broadest one among the four), with two main ideas. The first is to use\ncycle count statistics as a general recipe for GoF. The second is a novel\nnetwork fitting scheme. GoF-MSCORE is a flexible GoF approach, and we further\nextend it to SBM, DCBM, and MMSBM. This gives rise to a series of GoF metrics\ncovering each of the four models in the block-model family.\n  We show that for each of the four models, if the assumed model is correct,\nthen the corresponding GoF metric converges to $N(0, 1)$ as the network sizes\ndiverge. We also analyze the powers and show that these metrics are optimal in\nmany settings. In comparison, many other GoF ideas face challenges: they may\nlack a parameter-free limiting null, or are non-optimal in power, or face an\nanalytical hurdle. Note that a parameter-free limiting null is especially\ndesirable as many network models have a large number of unknown parameters. The\nlimiting nulls of our GoF metrics are always $N(0,1)$, which are parameter-free\nas desired.\n  For 12 frequently-used real networks, we use the proposed GoF metrics to show\nthat DCMM fits well with almost all of them. We also show that SBM, DCBM, and\nMMSBM do not fit well with many of these networks, especially when the networks\nare relatively large. To complement with our study on GoF, we also show that\nthe DCMM is nearly as broad as the rank-$K$ network model. Based on these\nresults, we recommend the DCMM as a promising model for undirected networks.",
        "In this study, the role of elastic and interfacial energies in the shape\nevolution of T1 precipitates in Al-Cu-Li alloys is investigated using\nphase-field modeling. We employ a formulation considering the stoichiometric\nnature of the precipitate phase explicitly, including coupled equation systems\nfor various order parameters. Inputs such as elastic properties are derived\nfrom DFT calculations, while chemical potentials are obtained from CALPHAD\ndatabases. This methodology provides a framework that is consistent with the\nderived chemical potentials to study the interplay of thermodynamic, kinetic,\nand elastic effects on T1 precipitate evolution in Al-Cu-Li alloys. It is shown\nthat diffusion-controlled lengthening and interface-controlled thickening are\nimportant mechanisms to describe the growth of T1 precipitates. Furthermore,\nthe study illustrates that the precipitate shape is significantly influenced by\nthe anisotropy in interfacial energy and linear reaction rate, however, elastic\neffects only play a minor role.",
        "We introduce and study blob and framed blob monoids. In particular, several\nrealizations of these monoids are given. We compute the cardinality of the\nframed blob monoid and derive some combinatorial formulas involving this\ncardinality.",
        "We study systems of two and three mesons composed of pions and kaons at\nmaximal isospin using four CLS ensembles with $a\\approx 0.063\\;$fm, including\none with approximately physical quark masses. Using the stochastic\nLaplacian-Heaviside method, we determine the energy spectrum of these systems\nincluding many levels in different momentum frames and irreducible\nrepresentations. Using the relativistic two- and three-body finite-volume\nformalism, we constrain the two and three-meson K matrices, including not only\nthe leading $s$ wave, but also $p$ and $d$ waves. By solving the three-body\nintegral equations, we determine, for the first time, the physical-point\nscattering amplitudes for $3\\pi^+$, $3K^+$, $\\pi^+\\pi^+ K^+$ and $K^+ K^+\n\\pi^+$ systems. These are determined for total angular momentum $J^P=0^-$,\n$1^+$, and $2^-$. We also obtain accurate results for $2\\pi^+$, $\\pi^+ K^+$,\nand $2K^+$ phase shifts. We compare our results to Chiral Perturbation Theory,\nand to phenomenological fits.",
        "Pressure is a powerful thermodynamic parameter for tuning the magnetic\nproperties of van der Waals magnets owing to their weak interlayer bonding.\nHowever, local magnetometry measurements under high pressure still remain\nelusive for this important class of emerging materials. Here we introduce a\nmethod enabling in situ magnetic imaging of van der Waals magnets under high\npressure with sub-micron spatial resolution. Our approach relies on a quantum\nsensing platform based on boron-vacancy (V$_\\text{B}^-$) centers in hexagonal\nboron nitride (hBN), which can be placed in atomic contact of any type of\ntwo-dimensional (2D) material within a van der Waals heterostructure. We first\nshow that the V$_\\text{B}^-$ center can be used as a magnetic field sensor up\nto pressures of a few GPa, a pressure range for which the properties of a wide\nvariety of van der Waals magnets are efficiently altered. We then use\nV$_\\text{B}^-$ centers in a thin hBN layer to perform magnetic imaging of a van\nder Waals magnet under pressure. As a proof of concept, we study the\npressure-dependent magnetization in micrometer-sized flakes of $1T$-CrTe$_2$,\nwhose evolution is explained by a shift of the Curie temperature. Besides\nproviding a new path for studying pressure-induced phase transitions in van der\nWaals magnets, this work also opens up interesting perspectives for exploring\nthe physics of 2D superconductors under pressure via local measurements of the\nMeissner effect.",
        "We propose a multi-scale extension of conformal prediction, an approach that\nconstructs prediction sets with finite-sample coverage guarantees under minimal\nstatistical assumptions. Classic conformal prediction relies on a single notion\nof conformity, overlooking the multi-level structures that arise in\napplications such as image analysis, hierarchical data exploration, and\nmulti-resolution time series modeling. In contrast, the proposed framework\ndefines a distinct conformity function at each relevant scale or resolution,\nproducing multiple conformal predictors whose prediction sets are then\nintersected to form the final multi-scale output. We establish theoretical\nresults confirming that the multi-scale prediction set retains the marginal\ncoverage guarantees of the original conformal framework and can, in fact, yield\nsmaller or more precise sets in practice. By distributing the total miscoverage\nprobability across scales in proportion to their informative power, the method\nfurther refines the set sizes. We also show that dependence between scales can\nlead to conservative coverage, ensuring that the actual coverage exceeds the\nnominal level. Numerical experiments in a synthetic classification setting\ndemonstrate that multi-scale conformal prediction achieves or surpasses the\nnominal coverage level while generating smaller prediction sets compared to\nsingle-scale conformal methods.",
        "We probe the spectrum of primordial gravitational waves (GWs) produced during\nthe eras of hyperkination, kination, and reheating in a non-minimally coupled,\n$\\mathcal{L} \\propto (1+ \\xi \\chi \/M_{\\text{Pl}})^t (R+\\alpha R^2)$, modified\ngravity using the Palatini formulation. We consider a runaway potential, which\ngives an era of kinetic domination after the end of inflation. The coupling\norder $t$ is varied to examine a large class of theories up to $\\chi^2 R^2$.\nFor models with $t>0$, reheating is not achieved naturally; hence, we\nsupplement such theories with a reheating mechanism based on the interaction of\ninflaton and radiation produced at the end of inflation due to cosmological\nexpansion. We demonstrate that the energy density of the GWs is enhanced as a\nfunction of the coupling during kination for all considered theories, and a\nshort-lived phase of hyperkination truncates the boost and avoids the\nover-production of GWs. Hyperkination, and thus the $R^2$ term, should be\ndeemed necessary in all theories with a runaway potential as it prevents the GW\nenhancement during kination from destabilizing the Big Bang Nucleosynthesis.\nThe spectrum remains flat for the period of hyperkination and reheating. We\nexamine the available parameter space for which the theories remain valid and\nplace bounds on the Hubble parameter ($H$) and radiation energy density\n($\\Omega_r^{\\text{end}}$) at the end of inflation. We find that as we decrease\nthe order of the coupling, the spectra shift towards a more observable regime\nof future GW experiments. The observation of the plateau during reheating will\nconstrain the $H$ and $\\Omega_r^{\\text{end}}$ values, while the spectral shape\nof the boost obtained during kination will confirm the nature of the theory.\nThe bounds from hyperkination lie in the kHz-GHz frequency range whose\ndetection can be positively anticipated via resonant cavities."
      ]
    }
  },
  {
    "id":2411.06741,
    "research_type":"applied",
    "start_id":"b20",
    "start_title":"Physics-informed machine learning",
    "start_abstract":"Despite great progress in simulating multiphysics problems using the numerical discretization of partial differential equations (PDEs), one still cannot seamlessly incorporate noisy data into existing algorithms, mesh generation remains complex, and high-dimensional problems governed by parameterized PDEs cannot be tackled. Moreover, solving inverse problems with hidden physics is often prohibitively expensive and requires different formulations and elaborate computer codes. Machine learning has emerged as a promising alternative, but training deep neural networks requires big data, not always available for scientific problems. Instead, such networks can be trained from additional information obtained by enforcing the physical laws (for example, at random points in the continuous space-time domain). Such physics-informed learning integrates (noisy) data and mathematical models, and implements them through neural networks or other kernel-based regression networks. Moreover, it may be possible to design specialized network architectures that automatically satisfy some of the physical invariants for better accuracy, faster training and improved generalization. Here, we review some of the prevailing trends in embedding physics into machine learning, present some of the current capabilities and limitations and discuss diverse applications of physics-informed learning both for forward and inverse problems, including discovering hidden physics and tackling high-dimensional problems.",
    "start_categories":[
      "physics.chem-ph"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Learning Polynomials with Neural Networks"
      ],
      "abstract":[
        "We study the effectiveness of learning low degree polynomials using neural networks by gradient descent method. While have been shown to great expressive power, and has widely used in practice for networks, few theoretical guarantees are known such methods. In particular, it is well that can get stuck at local minima, even simple classes target functions. this paper, we present several positive results support networks. focus on twolayer where bottom layer a set non-linear hidden nodes, top node linear function, similar Barron (1993). First show randomly initialized network with sufficiently many units, generic algorithm learns any polynomial, assuming initialize weights randomly. Secondly, if use complex-valued (the function still be real), then under suitable conditions, there no robust minima: always escape minimum performing random perturbation. This property does not hold real-valued weights. Thirdly, discuss whether sparse learned small size dependent sparsity function."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Towards Data-Efficient Pretraining for Atomic Property Prediction",
        "MMXU: A Multi-Modal and Multi-X-ray Understanding Dataset for Disease\n  Progression",
        "Activation Steering in Neural Theorem Provers",
        "Optimal Actuator Attacks on Autonomous Vehicles Using Reinforcement\n  Learning",
        "Spectral Unmixing Comparison with Sparse, Iterative and Mixed Integer\n  Programming Models",
        "Convergence analysis for a variant of manifold proximal point algorithm\n  based on Kurdyka-{\\L}ojasiewicz property",
        "Structure-Aware Correspondence Learning for Relative Pose Estimation",
        "Fixed point results for single and multi-valued three-points\n  contractions",
        "Data-driven Control of T-Product-based Dynamical Systems",
        "Exploring the Effects of Level of Control in the Initialization of\n  Shared Whiteboarding Sessions in Collaborative Augmented Reality",
        "Unveiling Hidden Pivotal Players with GoalNet: A GNN-Based Soccer Player\n  Evaluation System",
        "Incremental Approximate Single-Source Shortest Paths with Predictions",
        "How does the restriction of representations change under translations? A\n  story for the general linear groups and the unitary groups",
        "Understanding SGD with Exponential Moving Average: A Case Study in\n  Linear Regression",
        "Modeling of stochastic processes in $L_p(T)$ using orthogonal\n  polynomials",
        "ImageScope: Unifying Language-Guided Image Retrieval via Large\n  Multimodal Model Collective Reasoning",
        "Mutating ordered $\\tau$-rigid modules with applications to Nakayama\n  algebras",
        "Comparing Human Expertise and Large Language Models Embeddings in\n  Content Validity Assessment of Personality Tests",
        "Deep ARTMAP: Generalized Hierarchical Learning with Adaptive Resonance\n  Theory",
        "A Novel Observer Design for LuGre Friction Estimation and Control",
        "Real Time Control of Tandem-Wing Experimental Platform Using Concerto\n  Reinforcement Learning",
        "Re-examining Double Descent and Scaling Laws under Norm-based Capacity\n  via Deterministic Equivalence",
        "Towards Auto-Regressive Next-Token Prediction: In-Context Learning\n  Emerges from Generalization",
        "Efficient Phishing URL Detection Using Graph-based Machine Learning and\n  Loopy Belief Propagation",
        "Robust Federated Learning with Global Sensitivity Estimation for\n  Financial Risk Management",
        "A binary PSO based ensemble under-sampling model for rebalancing\n  imbalanced training data",
        "Language-Parametric Reference Synthesis (Extended)",
        "In Praise of Stubbornness: The Case for Cognitive-Dissonance-Aware\n  Knowledge Updates in LLMs",
        "Multiple Linked Tensor Factorization"
      ],
      "abstract":[
        "This paper challenges the recent paradigm in atomic property prediction that\nlinks progress to growing dataset sizes and computational resources. We show\nthat pretraining on a carefully selected, task-relevant dataset can match or\neven surpass large-scale pretraining, while using as little as 1\/24th of the\ncomputational cost. We introduce the Chemical Similarity Index (CSI), a novel\nmetric inspired by computer vision's Fr\\'echet Inception Distance, for\nmolecular graphs which quantifies the alignment between upstream pretraining\ndatasets and downstream tasks. By selecting the most relevant dataset with\nminimal CSI distance, we show that models pretrained on a smaller, focused\ndataset consistently outperform those pretrained on massive, mixed datasets\nsuch as JMP, even when those larger datasets include the relevant dataset.\nCounterintuitively, we also find that indiscriminately adding more data can\ndegrade model performance when the additional data poorly aligns with the task\nat hand. Our findings highlight that quality often outperforms quantity in\npretraining for atomic property prediction.",
        "Large vision-language models (LVLMs) have shown great promise in medical\napplications, particularly in visual question answering (MedVQA) and diagnosis\nfrom medical images. However, existing datasets and models often fail to\nconsider critical aspects of medical diagnostics, such as the integration of\nhistorical records and the analysis of disease progression over time. In this\npaper, we introduce MMXU (Multimodal and MultiX-ray Understanding), a novel\ndataset for MedVQA that focuses on identifying changes in specific regions\nbetween two patient visits. Unlike previous datasets that primarily address\nsingle-image questions, MMXU enables multi-image questions, incorporating both\ncurrent and historical patient data. We demonstrate the limitations of current\nLVLMs in identifying disease progression on MMXU-\\textit{test}, even those that\nperform well on traditional benchmarks. To address this, we propose a\nMedRecord-Augmented Generation (MAG) approach, incorporating both global and\nregional historical records. Our experiments show that integrating historical\nrecords significantly enhances diagnostic accuracy by at least 20\\%, bridging\nthe gap between current LVLMs and human expert performance. Additionally, we\nfine-tune models with MAG on MMXU-\\textit{dev}, which demonstrates notable\nimprovements. We hope this work could illuminate the avenue of advancing the\nuse of LVLMs in medical diagnostics by emphasizing the importance of historical\ncontext in interpreting medical images. Our dataset is released at\n\\href{https:\/\/github.com\/linjiemu\/MMXU}{https:\/\/github.com\/linjiemu\/MMXU}.",
        "Large Language Models (LLMs) have shown promise in proving formal theorems\nusing proof assistants like Lean. However, current state of the art language\nmodels struggles to predict next step in proofs leading practitioners to use\ndifferent sampling techniques to improve LLMs capabilities. We observe that the\nLLM is capable of predicting the correct tactic; however, it faces challenges\nin ranking it appropriately within the set of candidate tactics, affecting the\noverall selection process. To overcome this hurdle, we use activation steering\nto guide LLMs responses to improve the generations at the time of inference.\nOur results suggest that activation steering offers a promising lightweight\nalternative to specialized fine-tuning for enhancing theorem proving\ncapabilities in LLMs, particularly valuable in resource-constrained\nenvironments.",
        "With the increasing prevalence of autonomous vehicles (AVs), their\nvulnerability to various types of attacks has grown, presenting significant\nsecurity challenges. In this paper, we propose a reinforcement learning\n(RL)-based approach for designing optimal stealthy integrity attacks on AV\nactuators. We also analyze the limitations of state-of-the-art RL-based secure\ncontrollers developed to counter such attacks. Through extensive simulation\nexperiments, we demonstrate the effectiveness and efficiency of our proposed\nmethod.",
        "Hyperspectral unmixing is the analytical process of determining the pure\nmaterials and estimating the proportions of such materials composed within an\nobserved mixed pixel spectrum. We can unmix mixed pixel spectra using linear\nand nonlinear mixture models. Ordinary least squares (OLS) regression serves as\nthe foundation for many linear mixture models employed in Hyperspectral Image\nanalysis. Though variations of OLS are implemented, studies rarely address the\nunderlying assumptions that affect results. This paper provides an in depth\ndiscussion on the assumptions inherently endorsed by the application of OLS\nregression. We also examine variations of OLS models stemming from highly\neffective approaches in spectral unmixing -- sparse regression, iterative\nfeature search strategies and Mathematical programming. These variations are\ncompared to a novel unmixing approach called HySUDeB. We evaluated each\napproach's performance by computing the average error and precision of each\nmodel. Additionally, we provide a taxonomy of the molecular structure of each\nmineral to derive further understanding into the detection of the target\nmaterials.",
        "We incorporate an iteratively reweighted strategy in the manifold proximal\npoint algorithm (ManPPA) in [12] to solve an enhanced sparsity inducing model\nfor identifying sparse yet nonzero vectors in a given subspace. We establish\nthe global convergence of the whole sequence generated by our algorithm by\nassuming the Kurdyka-Lojasiewicz (KL) properties of suitable potential\nfunctions. We also study how the KL exponents of the different potential\nfunctions are related. More importantly, when our enhanced model and algorithm\nreduce, respectively, to the model and ManPPA with constant stepsize considered\nin [12], we show that the sequence generated converges linearly as long as the\noptimal value of the model is positive, and converges finitely when the limit\nof the sequence lies in a set of weak sharp minima. Our results improve [13,\nTheorem 2.4], which asserts local quadratic convergence in the presence of weak\nsharp minima when the constant stepsize is small.",
        "Relative pose estimation provides a promising way for achieving\nobject-agnostic pose estimation. Despite the success of existing 3D\ncorrespondence-based methods, the reliance on explicit feature matching suffers\nfrom small overlaps in visible regions and unreliable feature estimation for\ninvisible regions. Inspired by humans' ability to assemble two object parts\nthat have small or no overlapping regions by considering object structure, we\npropose a novel Structure-Aware Correspondence Learning method for Relative\nPose Estimation, which consists of two key modules. First, a structure-aware\nkeypoint extraction module is designed to locate a set of kepoints that can\nrepresent the structure of objects with different shapes and appearance, under\nthe guidance of a keypoint based image reconstruction loss. Second, a\nstructure-aware correspondence estimation module is designed to model the\nintra-image and inter-image relationships between keypoints to extract\nstructure-aware features for correspondence estimation. By jointly leveraging\nthese two modules, the proposed method can naturally estimate 3D-3D\ncorrespondences for unseen objects without explicit feature matching for\nprecise relative pose estimation. Experimental results on the CO3D, Objaverse\nand LineMOD datasets demonstrate that the proposed method significantly\noutperforms prior methods, i.e., with 5.7{\\deg}reduction in mean angular error\non the CO3D dataset.",
        "In this paper, we are concerned with the study of the existence of fixed\npoints for single and multi-valued three-points contractions. Namely, we first\nintroduce a new class of single-valued mappings defined on a metric space\nequipped with three metrics. A fixed point theorem is established for such\nmappings. The obtained result recovers that established recently by the second\nauthor [J. Fixed Point Theory Appl. 25 (2023) 74] for the class of\nsingle-valued mappings contracting perimeters of triangles. We next extend our\nstudy by introducing the class of multivalued three points contractions. A\nfixed point theorem, which is a multi-valued version of that obtained in the\nabove reference, is established. Some examples showing the validity of our\nobtained results are provided.",
        "Data-driven control is a powerful tool that enables the design and\nimplementation of control strategies directly from data without explicitly\nidentifying the underlying system dynamics. While various data-driven control\ntechniques, such as stabilization, linear quadratic regulation, and model\npredictive control, have been extensively developed, these methods are not\ninherently suited for multi-linear dynamical systems, where the states are\nrepresented as higher-order tensors. In this article, we propose a novel\nframework for data-driven control of T-product-based dynamical systems (TPDSs),\nwhere the system evolution is governed by the T-product between a third-order\ndynamic tensor and a third-order state tensor. In particular, we offer\nnecessary and sufficient conditions to determine the data informativity for\nsystem identification, stabilization by state feedback, and T-product quadratic\nregulation of TPDSs with detailed complexity analyses. Finally, we validate our\nframework through numerical examples.",
        "Augmented Reality (AR) collaboration can benefit from a shared 2D surface,\nsuch as a whiteboard. However, many features of each collaborators physical\nenvironment must be considered in order to determine the best placement and\nshape of the shared surface. We explored the effects of three methods for\nbeginning a collaborative whiteboarding session with varying levels of user\ncontrol: MANUAL, DISCRETE CHOICE, and AUTOMATIC by conducting a simulated AR\nstudy within Virtual Reality (VR). In the MANUAL method, users draw their own\nsurfaces directly in the environment until they agree on the placement; in the\nDISCRETE CHOICE method, the system provides three options for whiteboard size\nand location; and in the AUTOMATIC method, the system automatically creates a\nwhiteboard that fits within each collaborators environment. We evaluate these\nthree conditions in a study in which two collaborators used each method to\nbegin collaboration sessions. After establishing a session, the users worked\ntogether to complete an affinity diagramming task using the shared whiteboard.\nWe found that the majority of participants preferred to have direct control\nduring the initialization of a new collaboration session, despite the\nadditional workload induced by the Manual method.",
        "Soccer analysis tools emphasize metrics such as expected goals, leading to an\noverrepresentation of attacking players' contributions and overlooking players\nwho facilitate ball control and link attacks. Examples include Rodri from\nManchester City and Palhinha who just transferred to Bayern Munich. To address\nthis bias, we aim to identify players with pivotal roles in a soccer team,\nincorporating both spatial and temporal features.\n  In this work, we introduce a GNN-based framework that assigns individual\ncredit for changes in expected threat (xT), thus capturing overlooked yet vital\ncontributions in soccer. Our pipeline encodes both spatial and temporal\nfeatures in event-centric graphs, enabling fair attribution of non-scoring\nactions such as defensive or transitional plays. We incorporate centrality\nmeasures into the learned player embeddings, ensuring that ball-retaining\ndefenders and defensive midfielders receive due recognition for their overall\nimpact. Furthermore, we explore diverse GNN variants-including Graph Attention\nNetworks and Transformer-based models-to handle long-range dependencies and\nevolving match contexts, discussing their relative performance and\ncomputational complexity. Experiments on real match data confirm the robustness\nof our approach in highlighting pivotal roles that traditional attacking\nmetrics typically miss, underscoring the model's utility for more comprehensive\nsoccer analytics.",
        "The algorithms-with-predictions framework has been used extensively to\ndevelop online algorithms with improved beyond-worst-case competitive ratios.\nRecently, there is growing interest in leveraging predictions for designing\ndata structures with improved beyond-worst-case running times. In this paper,\nwe study the fundamental data structure problem of maintaining approximate\nshortest paths in incremental graphs in the algorithms-with-predictions model.\nGiven a sequence $\\sigma$ of edges that are inserted one at a time, the goal is\nto maintain approximate shortest paths from the source to each vertex in the\ngraph at each time step. Before any edges arrive, the data structure is given a\nprediction of the online edge sequence $\\hat{\\sigma}$ which is used to ``warm\nstart'' its state.\n  As our main result, we design a learned algorithm that maintains\n$(1+\\epsilon)$-approximate single-source shortest paths, which runs in\n$\\tilde{O}(m \\eta \\log W\/\\epsilon)$ time, where $W$ is the weight of the\nheaviest edge and $\\eta$ is the prediction error. We show these techniques\nimmediately extend to the all-pairs shortest-path setting as well. Our\nalgorithms are consistent (performing nearly as fast as the offline algorithm)\nwhen predictions are nearly perfect, have a smooth degradation in performance\nwith respect to the prediction error and, in the worst case, match the best\noffline algorithm up to logarithmic factors.\n  As a building block, we study the offline incremental approximate\nsingle-source shortest-paths problem. In this problem, the edge sequence\n$\\sigma$ is known a priori and the goal is to efficiently return the length of\nthe shortest paths in the intermediate graph $G_t$ consisting of the first $t$\nedges, for all $t$. Note that the offline incremental problem is defined in the\nworst-case setting (without predictions) and is of independent interest.",
        "We present a new approach to symmetry breaking for pairs of real forms of\n$(GL(n, \\mathbb{C}), GL(n-1, \\mathbb{C}))$. While translation functors are a\nuseful tool for studying a family of representations of a single reductive\ngroup $G$, when applied to a pair of groups $G \\supset G'$,translation functors\ncan significantly alter the nature of symmetry breaking between the\nrepresentations of $G$ and $G'$, even within the same Weyl chamber of the\ndirect product group $G \\times G'$. We introduce the concept of \\lq\\lq{fences\nfor the interlacing pattern}\\rq\\rq,which provides a refinement of the usual\nnotion of \\lq\\lq{walls for Weyl chambers}\\rq\\rq. We then present a theorem that\nstates that multiplicity is constant unless these \\lq\\lq{fences}\\rq\\rq\\ are\ncrossed. This general theorem is illustrated with examples of both tempered and\nnon-tempered representations. Additionally,we provide a new non-vanishing\ntheorem of period integrals for pairs of reductive symmetric spaces,which is\nfurther strengthened through this approach.",
        "Exponential moving average (EMA) has recently gained significant popularity\nin training modern deep learning models, especially diffusion-based generative\nmodels. However, there have been few theoretical results explaining the\neffectiveness of EMA. In this paper, to better understand EMA, we establish the\nrisk bound of online SGD with EMA for high-dimensional linear regression, one\nof the simplest overparameterized learning tasks that shares similarities with\nneural networks. Our results indicate that (i) the variance error of SGD with\nEMA is always smaller than that of SGD without averaging, and (ii) unlike SGD\nwith iterate averaging from the beginning, the bias error of SGD with EMA\ndecays exponentially in every eigen-subspace of the data covariance matrix.\nAdditionally, we develop proof techniques applicable to the analysis of a broad\nclass of averaging schemes.",
        "In this paper, models that approximate stochastic processes from the space\n$Sub_\\varphi(\\Omega)$ with given reliability and accuracy in $L_p(T)$ are\nconsidered for some specific functions $\\varphi(t)$. For processes that are\ndecomposited in series using orthonormal bases, such models are constructed in\nthe case where elements of such decomposition cannot be found explicitly.",
        "With the proliferation of images in online content, language-guided image\nretrieval (LGIR) has emerged as a research hotspot over the past decade,\nencompassing a variety of subtasks with diverse input forms. While the\ndevelopment of large multimodal models (LMMs) has significantly facilitated\nthese tasks, existing approaches often address them in isolation, requiring the\nconstruction of separate systems for each task. This not only increases system\ncomplexity and maintenance costs, but also exacerbates challenges stemming from\nlanguage ambiguity and complex image content, making it difficult for retrieval\nsystems to provide accurate and reliable results. To this end, we propose\nImageScope, a training-free, three-stage framework that leverages collective\nreasoning to unify LGIR tasks. The key insight behind the unification lies in\nthe compositional nature of language, which transforms diverse LGIR tasks into\na generalized text-to-image retrieval process, along with the reasoning of LMMs\nserving as a universal verification to refine the results. To be specific, in\nthe first stage, we improve the robustness of the framework by synthesizing\nsearch intents across varying levels of semantic granularity using\nchain-of-thought (CoT) reasoning. In the second and third stages, we then\nreflect on retrieval results by verifying predicate propositions locally, and\nperforming pairwise evaluations globally. Experiments conducted on six LGIR\ndatasets demonstrate that ImageScope outperforms competitive baselines.\nComprehensive evaluations and ablation studies further confirm the\neffectiveness of our design.",
        "A mutation operation for $\\tau$-exceptional sequences of modules over any\nfinite-dimensional algebra was recently introduced, generalising the mutation\nfor exceptional sequences of modules over hereditary algebras. We interpret\nthis mutation in terms of TF-ordered $\\tau$-rigid modules, which are in\nbijection with $\\tau$-exceptional sequences. As an application we show that the\nmutation is transitive for Nakayama algebras, by providing an explicit\ncombinatorial description of mutation over this class of algebras.",
        "In this article we explore the application of Large Language Models (LLMs) in\nassessing the content validity of psychometric instruments, focusing on the Big\nFive Questionnaire (BFQ) and Big Five Inventory (BFI). Content validity, a\ncornerstone of test construction, ensures that psychological measures\nadequately cover their intended constructs. Using both human expert evaluations\nand advanced LLMs, we compared the accuracy of semantic item-construct\nalignment. Graduate psychology students employed the Content Validity Ratio\n(CVR) to rate test items, forming the human baseline. In parallel,\nstate-of-the-art LLMs, including multilingual and fine-tuned models, analyzed\nitem embeddings to predict construct mappings. The results reveal distinct\nstrengths and limitations of human and AI approaches. Human validators excelled\nin aligning the behaviorally rich BFQ items, while LLMs performed better with\nthe linguistically concise BFI items. Training strategies significantly\ninfluenced LLM performance, with models tailored for lexical relationships\noutperforming general-purpose LLMs. Here we highlights the complementary\npotential of hybrid validation systems that integrate human expertise and AI\nprecision. The findings underscore the transformative role of LLMs in\npsychological assessment, paving the way for scalable, objective, and robust\ntest development methodologies.",
        "This paper presents Deep ARTMAP, a novel extension of the ARTMAP architecture\nthat generalizes the self-consistent modular ART (SMART) architecture to enable\nhierarchical learning (supervised and unsupervised) across arbitrary\ntransformations of data. The Deep ARTMAP framework operates as a divisive\nclustering mechanism, supporting an arbitrary number of modules with\ncustomizable granularity within each module. Inter-ART modules regulate the\nclustering at each layer, permitting unsupervised learning while enforcing a\none-to-many mapping from clusters in one layer to the next. While Deep ARTMAP\nreduces to both ARTMAP and SMART in particular configurations, it offers\nsignificantly enhanced flexibility, accommodating a broader range of data\ntransformations and learning modalities.",
        "Dynamic components of the friction may directly impact the stability and\nperformance of the motion control systems. The LuGre model is a prevalent\nfriction model utilized to express this dynamic behavior. Since the LuGre model\nis very comprehensive, friction compensation based on it might be challenging.\nInspired by this, we develop a novel observer to estimate and compensate for\nLuGre friction. Furthermore, we present a Lyapunov stability analysis to show\nthat observer dynamics are asymptotically stable under certain conditions.\nCompared to its counterparts, the proposed observer constitutes a simple and\nstandalone scheme that can be utilized with arbitrary control inputs in a\nstraightforward way. As a primary difference, the presented observer estimates\nvelocity and uses the velocity error to estimate friction in addition to\ncontrol input. The extensive simulations revealed that the introduced observer\nenhances position and velocity tracking performance in the presence of\nfriction.",
        "This paper introduces the CRL2RT algorithm, an advanced reinforcement\nlearning method aimed at improving the real-time control performance of the\nDirect-Drive Tandem-Wing Experimental Platform (DDTWEP). Inspired by dragonfly\nflight, DDTWEP's tandem wing structure causes nonlinear and unsteady\naerodynamic interactions, leading to complex load behaviors during pitch, roll,\nand yaw maneuvers. These complexities challenge stable motion control at high\nfrequencies (2000 Hz). To overcome these issues, we developed the CRL2RT\nalgorithm, which combines classical control elements with reinforcement\nlearning-based controllers using a time-interleaved architecture and a\nrule-based policy composer. This integration ensures finite-time convergence\nand single-life adaptability. Experimental results under various conditions,\nincluding different flapping frequencies and yaw disturbances, show that CRL2RT\nachieves a control frequency surpassing 2500 Hz on standard CPUs. Additionally,\nwhen integrated with classical controllers like PID, Adaptive PID, and Model\nReference Adaptive Control (MRAC), CRL2RT enhances tracking performance by\n18.3% to 60.7%. These findings demonstrate CRL2RT's broad applicability and\nsuperior performance in complex real-time control scenarios, validating its\neffectiveness in overcoming existing control strategy limitations and advancing\nrobust, efficient real-time control for biomimetic aerial vehicles.",
        "We investigate double descent and scaling laws in terms of weights rather\nthan the number of parameters. Specifically, we analyze linear and random\nfeatures models using the deterministic equivalence approach from random matrix\ntheory. We precisely characterize how the weights norm concentrate around\ndeterministic quantities and elucidate the relationship between the expected\ntest error and the norm-based capacity (complexity). Our results rigorously\nanswer whether double descent exists under norm-based capacity and reshape the\ncorresponding scaling laws. Moreover, they prompt a rethinking of the\ndata-parameter paradigm - from under-parameterized to over-parameterized\nregimes - by shifting the focus to norms (weights) rather than parameter count.",
        "Large language models (LLMs) have demonstrated remarkable in-context learning\n(ICL) abilities. However, existing theoretical analysis of ICL primarily\nexhibits two limitations: (a) Limited i.i.d. Setting. Most studies focus on\nsupervised function learning tasks where prompts are constructed with i.i.d.\ninput-label pairs. This i.i.d. assumption diverges significantly from real\nlanguage learning scenarios where prompt tokens are interdependent. (b) Lack of\nEmergence Explanation. Most literature answers what ICL does from an implicit\noptimization perspective but falls short in elucidating how ICL emerges and the\nimpact of pre-training phase on ICL. In our paper, to extend (a), we adopt a\nmore practical paradigm, auto-regressive next-token prediction (AR-NTP), which\nclosely aligns with the actual training of language models. Specifically,\nwithin AR-NTP, we emphasize prompt token-dependency, which involves predicting\neach subsequent token based on the preceding sequence. To address (b), we\nformalize a systematic pre-training and ICL framework, highlighting the\nlayer-wise structure of sequences and topics, alongside a two-level\nexpectation. In conclusion, we present data-dependent, topic-dependent and\noptimization-dependent PAC-Bayesian generalization bounds for pre-trained LLMs,\ninvestigating that ICL emerges from the generalization of sequences and topics.\nOur theory is supported by experiments on numerical linear dynamic systems,\nsynthetic GINC and real-world language datasets.",
        "The proliferation of mobile devices and online interactions have been\nthreatened by different cyberattacks, where phishing attacks and malicious\nUniform Resource Locators (URLs) pose significant risks to user security.\nTraditional phishing URL detection methods primarily rely on URL string-based\nfeatures, which attackers often manipulate to evade detection. To address these\nlimitations, we propose a novel graph-based machine learning model for phishing\nURL detection, integrating both URL structure and network-level features such\nas IP addresses and authoritative name servers. Our approach leverages Loopy\nBelief Propagation (LBP) with an enhanced convergence strategy to enable\neffective message passing and stable classification in the presence of complex\ngraph structures. Additionally, we introduce a refined edge potential mechanism\nthat dynamically adapts based on entity similarity and label relationships to\nfurther improve classification accuracy. Comprehensive experiments on\nreal-world datasets demonstrate our model's effectiveness by achieving F1 score\nof up to 98.77\\%. This robust and reproducible method advances phishing\ndetection capabilities, offering enhanced reliability and valuable insights in\nthe field of cybersecurity.",
        "In decentralized financial systems, robust and efficient Federated Learning\n(FL) is promising to handle diverse client environments and ensure resilience\nto systemic risks. We propose Federated Risk-Aware Learning with Central\nSensitivity Estimation (FRAL-CSE), an innovative FL framework designed to\nenhance scalability, stability, and robustness in collaborative financial\ndecision-making. The framework's core innovation lies in a central acceleration\nmechanism, guided by a quadratic sensitivity-based approximation of global\nmodel dynamics. By leveraging local sensitivity information derived from robust\nrisk measurements, FRAL-CSE performs a curvature-informed global update that\nefficiently incorporates second-order information without requiring repeated\nlocal re-evaluations, thereby enhancing training efficiency and improving\noptimization stability. Additionally, distortion risk measures are embedded\ninto the training objectives to capture tail risks and ensure robustness\nagainst extreme scenarios. Extensive experiments validate the effectiveness of\nFRAL-CSE in accelerating convergence and improving resilience across\nheterogeneous datasets compared to state-of-the-art baselines.",
        "Ensemble technique and under-sampling technique are both effective tools used\nfor imbalanced dataset classification problems. In this paper, a novel ensemble\nmethod combining the advantages of both ensemble learning for biasing\nclassifiers and a new under-sampling method is proposed. The under-sampling\nmethod is named Binary PSO instance selection; it gathers with ensemble\nclassifiers to find the most suitable length and combination of the majority\nclass samples to build a new dataset with minority class samples. The proposed\nmethod adopts multi-objective strategy, and contribution of this method is a\nnotable improvement of the performances of imbalanced classification, and in\nthe meantime guaranteeing a best integrity possible for the original dataset.\nWe experimented the proposed method and compared its performance of processing\nimbalanced datasets with several other conventional basic ensemble methods.\nExperiment is also conducted on these imbalanced datasets using an improved\nversion where ensemble classifiers are wrapped in the Binary PSO instance\nselection. According to experimental results, our proposed methods outperform\nsingle ensemble methods, state-of-the-art under-sampling methods, and also\ncombinations of these methods with the traditional PSO instance selection\nalgorithm.",
        "Modern Integrated Development Environments (IDEs) offer automated\nrefactorings to aid programmers in developing and maintaining software.\nHowever, implementing sound automated refactorings is challenging, as\nrefactorings may inadvertently introduce name-binding errors or cause\nreferences to resolve to incorrect declarations. To address these issues,\nprevious work by Sch\\\"afer et al. proposed replacing concrete references with\nlocked references to separate binding preservation from transformation. Locked\nreferences vacuously resolve to a specific declaration, and after\ntransformation must be replaced with concrete references that also resolve to\nthat declaration. Synthesizing these references requires a faithful inverse of\nthe name lookup functions of the underlying language.\n  Manually implementing such inverse lookup functions is challenging due to the\ncomplex name-binding features in modern programming languages. Instead, we\npropose to automatically derive this function from type system specifications\nwritten in the Statix meta-DSL. To guide the synthesis of qualified references\nwe use scope graphs, which represent the binding structure of a program, to\ninfer their names and discover their syntactic structure.\n  We evaluate our approach by synthesizing concrete references for locked\nreferences in 2528 Java, 196 ChocoPy, and 49 Featherweight Generic Java test\nprograms. Our approach yields a principled language-parametric method for\nsynthesizing references.",
        "Despite remarkable capabilities, large language models (LLMs) struggle to\ncontinually update their knowledge without catastrophic forgetting. In\ncontrast, humans effortlessly integrate new information, detect conflicts with\nexisting beliefs, and selectively update their mental models. This paper\nintroduces a cognitive-inspired investigation paradigm to study continual\nknowledge updating in LLMs. We implement two key components inspired by human\ncognition: (1) Dissonance and Familiarity Awareness, analyzing model behavior\nto classify information as novel, familiar, or dissonant; and (2) Targeted\nNetwork Updates, which track neural activity to identify frequently used\n(stubborn) and rarely used (plastic) neurons. Through carefully designed\nexperiments in controlled settings, we uncover a number of empirical findings\ndemonstrating the potential of this approach. First, dissonance detection is\nfeasible using simple activation and gradient features, suggesting potential\nfor cognitive-inspired training. Second, we find that non-dissonant updates\nlargely preserve prior knowledge regardless of targeting strategy, revealing\ninherent robustness in LLM knowledge integration. Most critically, we discover\nthat dissonant updates prove catastrophically destructive to the model's\nknowledge base, indiscriminately affecting even information unrelated to the\ncurrent updates. This suggests fundamental limitations in how neural networks\nhandle contradictions and motivates the need for new approaches to knowledge\nupdating that better mirror human cognitive mechanisms.",
        "In biomedical research and other fields, it is now common to generate high\ncontent data that are both multi-source and multi-way. Multi-source data are\ncollected from different high-throughput technologies while multi-way data are\ncollected over multiple dimensions, yielding multiple tensor arrays.\nIntegrative analysis of these data sets is needed, e.g., to capture and\nsynthesize different facets of complex biological systems. However, despite\ngrowing interest in multi-source and multi-way factorization techniques,\nmethods that can handle data that are both multi-source and multi-way are\nlimited. In this work, we propose a Multiple Linked Tensors Factorization\n(MULTIFAC) method extending the CANDECOMP\/PARAFAC (CP) decomposition to\nsimultaneously reduce the dimension of multiple multi-way arrays and\napproximate underlying signal. We first introduce a version of the CP\nfactorization with L2 penalties on the latent factors, leading to rank\nsparsity. When extended to multiple linked tensors, the method automatically\nreveals latent components that are shared across data sources or individual to\neach data source. We also extend the decomposition algorithm to its\nexpectation-maximization (EM) version to handle incomplete data with\nimputation. Extensive simulation studies are conducted to demonstrate\nMULTIFAC's ability to (i) approximate underlying signal, (ii) identify shared\nand unshared structures, and (iii) impute missing data. The approach yields an\ninterpretable decomposition on multi-way multi-omics data for a study on\nearly-life iron deficiency."
      ]
    }
  },
  {
    "id":2411.18259,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"Physics-informed machine learning",
    "start_abstract":"Despite great progress in simulating multiphysics problems using the numerical discretization of partial differential equations (PDEs), one still cannot seamlessly incorporate noisy data into existing algorithms, mesh generation remains complex, and high-dimensional problems governed by parameterized PDEs cannot be tackled. Moreover, solving inverse problems with hidden physics is often prohibitively expensive and requires different formulations and elaborate computer codes. Machine learning has emerged as a promising alternative, but training deep neural networks requires big data, not always available for scientific problems. Instead, such networks can be trained from additional information obtained by enforcing the physical laws (for example, at random points in the continuous space-time domain). Such physics-informed learning integrates (noisy) data and mathematical models, and implements them through neural networks or other kernel-based regression networks. Moreover, it may be possible to design specialized network architectures that automatically satisfy some of the physical invariants for better accuracy, faster training and improved generalization. Here, we review some of the prevailing trends in embedding physics into machine learning, present some of the current capabilities and limitations and discuss diverse applications of physics-informed learning both for forward and inverse problems, including discovering hidden physics and tackling high-dimensional problems.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b2"
      ],
      "title":[
        "Quantifying the performance of machine learning models in materials discovery"
      ],
      "abstract":[
        "The predictive capabilities of machine learning (ML) models used in materials discovery are typically measured using simple statistics such as the root-mean-square error (RMSE) or coefficient determination ($r^2$) between ML-predicted property values and their known values. A tempting assumption is that with low should be effective at guiding discovery, conversely, high give poor performance. However, we observe no clear connection exists a \"static\" quantity averaged across an entire training set, RMSE, ML model's ability to dynamically guide iterative (and often extrapolative) novel targeted properties. In this work, simulate sequential (SL)-guided process demonstrate decoupling traditional model metrics performance discoveries. We show depends strongly on (1) target range within distribution (e.g., whether 1st 10th decile material desired); (2) incorporation uncertainty estimates SL acquisition function; (3) scientist interested one many targets; (4) how iterations allowed. To overcome limitations static robustly capture performance, recommend Discovery Yield ($DY$), measure high-performing were discovered during SL, Probability ($DP$), likelihood discovering any point process."
      ],
      "categories":[
        "cond-mat.mtrl-sci"
      ]
    },
    "list":{
      "title":[
        "On the speed of coming down from infinity for (sub)critical branching\n  processes with pairwise interactions",
        "Comment on \"Optimal conversion of Kochen-Specker sets into bipartite\n  perfect quantum strategies\"",
        "A simple extrapolation criterion with an application to wavelet\n  characterization of various function spaces",
        "Predicted Neutrino Signal Features of Core-Collapse Supernovae",
        "A Space Mapping approach for the calibration of financial models with\n  the application to the Heston model",
        "Euclid Quick Data Release (Q1), A first look at the fraction of bars in\n  massive galaxies at $z<1$",
        "H$\\alpha$ Variability of AB Aur b with the Hubble Space Telescope:\n  Probing the Nature of a Protoplanet Candidate with Accretion Light Echoes",
        "Surfaces in 4-manifolds and extendible mapping classes",
        "Discrete Level Set Persistence for Finite Discrete Functions",
        "Monge-Kantorovich quantiles and ranks for image data",
        "Interpreting and Steering Protein Language Models through Sparse\n  Autoencoders",
        "Minimax rates of convergence for the nonparametric estimation of the\n  diffusion coefficient from time-homogeneous SDE paths",
        "Stochastic Equilibrium Raman Spectroscopy (STERS)",
        "The Light Neutralino Dark Matter in the Generalized Minimal Supergravity\n  (GmSUGRA)",
        "Fusion Dynamics of Majorana Zero Modes",
        "Error bounds for composite quantum hypothesis testing and a new\n  characterization of the weighted Kubo-Ando geometric means",
        "Part-Time Penalties and Heterogeneous Retirement Decisions",
        "Environmental Co-design: Fish-Blade Collision Model for Hydrokinetic\n  Turbines",
        "Noise equals endogenous control",
        "On regular charged black holes in three dimensions",
        "Advances in modeling complex materials: The rise of neuroevolution\n  potentials",
        "A numerical scheme for a multi-scale model of thrombus in arteries",
        "Parameter Symmetry Breaking and Restoration Determines the Hierarchical\n  Learning in AI Systems",
        "ELT-METIS imaging simulations for disks and envelopes associated with FU\n  Ori-type objects",
        "On meromorphic solutions of certain Fermat-type difference and analogues\n  equations concerning open problems",
        "Accelerating Combinatorial Electrocatalyst Discovery with Bayesian\n  Optimization: A Case Study in the Quaternary System Ni-Pd-Pt-Ru for the\n  Oxygen Evolution Reaction",
        "On the hydrostatic approximation of 3D Oldroyd-B model",
        "Metrological symmetries in singular quantum multi-parameter estimation",
        "Desorption-induced decoherence of nanoparticle motion"
      ],
      "abstract":[
        "In this paper, we investigate the phenomenon of coming down from infinity for\n(sub)critical cooperative branching processes with pairwise interactions (BPI\nprocesses for short) under appropriate conditions. BPI processes are\ncontinuous-time Markov chains that extend pure branching dynamics by\nincorporating additional mechanisms that allow both competition and cooperation\nevents between pairs of individuals.\n  Specifically, we focus on characterising the speed at which BPI processes\nevolve when starting from a very large initial population in the subcritical\nand critical cooperative regimes. Further, in the subcritical cooperative\nregime, we analyse their second-order fluctuations.",
        "A recent paper of Trandafir and Cabello [Phys. Rev. A, 111, 022408 (2025)]\ncontains a number of errors, inconsistencies, and inefficiencies. They are too\nnumerous to be listed here, so we identify and discuss them in the main body of\nthe comment.",
        "The aim of this paper is to obtain an extrapolation result without using\nconvexification. What is new about this criterion is that the convexification\nof Banach spaces does not come into play. As an application, a characterization\nof ball Banach function spaces in terms of wavelets can be obtained. The result\ncan be formulated so that we can take into account the smoothness property of\nthe function spaces under consideration. The same technique can be used for the\nproof of the vector-valued inequalities for example. Also, the result in the\npresent paper refines a recent result on the extension operator.",
        "In this paper, we examine the neutrino signals from 24 initially\nnon-rotating, three-dimensional core-collapse supernova (CCSN) simulations\ncarried to late times. We find that not only does the neutrino luminosity\nsignal encode information about each stage of the CCSN process, but that the\nmonotonic dependence of the luminosity peak height with compactness enables one\nto infer the progenitor core structure from the neutrino signal. We highlight a\nsystematic relationship between the luminosity peak height with its timing.\nAdditionally, we emphasize that the total energy radiated in neutrinos is\nmonotonic with progenitor compactness, and that the mean neutrino energy\ncontains a unique spiral SASI signature for nonexploding, BH-forming models. We\nalso find that neutrino emissions are not isotropic and that the anisotropy\nincreases roughly with progenitor compactness. To assess the detectability of\nthese neutrino signal features, we provide examples of the event rates for our\nmodels for the JUNO, DUNE, SK, and IceCube detectors using the SNEWPY software,\nand find that many of the trends in the luminosity signal can be detectable\nacross several detectors and oscillation models. Finally, we discuss\ncorrelations between the radiated neutrino energy and the evolution of the\ngravitational-wave f-mode.",
        "We present a novel approach for parameter calibration of the Heston model for\npricing an Asian put option, namely space mapping. Since few parameters of the\nHeston model can be directly extracted from real market data, calibration to\nreal market data is implicit and therefore a challenging task. In addition,\nsome of the parameters in the model are non-linear, which makes it difficult to\nfind the global minimum of the optimization problem within the calibration. Our\napproach is based on the idea of space mapping, exploiting the residuum of a\ncoarse surrogate model that allows optimization and a fine model that needs to\nbe calibrated. In our case, the pricing of an Asian option using the Heston\nmodel SDE is the fine model, and the surrogate is chosen to be the Heston model\nPDE pricing a European option. We formally derive a gradient descent algorithm\nfor the PDE constrained calibration model using well-known techniques from\noptimization with PDEs. Our main goal is to provide evidence that the space\nmapping approach can be useful in financial calibration tasks. Numerical\nresults underline the feasibility of our approach.",
        "Stellar bars are key structures in disc galaxies, driving angular momentum\nredistribution and influencing processes such as bulge growth and star\nformation. Quantifying the bar fraction as a function of redshift and stellar\nmass is therefore important for constraining the physical processes that drive\ndisc formation and evolution across the history of the Universe. Leveraging the\nunprecedented resolution and survey area of the Euclid Q1 data release combined\nwith the Zoobot deep-learning model trained on citizen-science labels, we\nidentify 7711 barred galaxies with $M_* \\gtrsim 10^{10}M_\\odot$ in a\nmagnitude-selected sample $I_E < 20.5$ spanning $63.1 deg^2$. We measure a mean\nbar fraction of $0.2-0.4$, consistent with prior studies. At fixed redshift,\nmassive galaxies exhibit higher bar fractions, while lower-mass systems show a\nsteeper decline with redshift, suggesting earlier disc assembly in massive\ngalaxies. Comparisons with cosmological simulations (e.g., TNG50, Auriga)\nreveal a broadly consistent bar fraction, but highlight overpredictions for\nhigh-mass systems, pointing to potential over-efficiency in central stellar\nmass build-up in simulations. These findings demonstrate Euclid's\ntransformative potential for galaxy morphology studies and underscore the\nimportance of refining theoretical models to better reproduce observed trends.\nFuture work will explore finer mass bins, environmental correlations, and\nadditional morphological indicators.",
        "Giant planets generate accretion luminosity as they form. Much of this energy\nis radiated in strong H$\\alpha$ line emission, which has motivated direct\nimaging surveys at optical wavelengths to search for accreting protoplanets.\nHowever, compact disk structures can mimic accreting planets by scattering\nemission from the host star. This can complicate the interpretation of\nH$\\alpha$ point sources, especially if the host star itself is accreting. We\ndescribe an approach to distinguish accreting protoplanets from scattered-light\ndisk features using \"accretion light echoes.\" This method relies on variable\nH$\\alpha$ emission from a stochastically accreting host star to search for a\ndelayed brightness correlation with a candidate protoplanet. We apply this\nmethod to the candidate protoplanet AB Aur b with a dedicated Hubble Space\nTelescope Wide Field Camera 3 program designed to sequentially sample the host\nstar and the candidate planet in H$\\alpha$ while accounting for the light\ntravel time delay and orbital geometry of the source within the protoplanetary\ndisk. Across five epochs spanning 14 months, AB Aur b is over 20 times more\nvariable than its host star; AB Aur's H$\\alpha$ emission changes by 15% while\nAB Aur b varies by 330%. These brightness changes are not correlated, which\nrules out unobstructed scattered starlight from the host star as the only\nsource of AB Aur b's H$\\alpha$ emission and is consistent with tracing emission\nfrom an independently accreting protoplanet, inner disk shadowing effects, or a\nphysically evolving compact disk structure. More broadly, accretion light\nechoes offer a novel tool to explore the nature of protoplanet candidates with\nwell-timed observations of the host star prior to deep imaging in H$\\alpha$.",
        "We study smooth proper embeddings of compact orientable surfaces in compact\norientable $4$-manifolds and elements in the mapping class group of that\nsurface which are induced by diffeomorphisms of the ambient $4$-manifolds. We\ncall such mapping classes extendible. An embedding for which all mapping\nclasses are extendible is called flexible. We show that for most of the\nsurfaces there exists no flexible embedding in a $4$-manifold with homology\ntype of a $4$-ball or of a $4$-sphere. As an application of our method, we\naddress a question of Etnyre and Lekili and show that there exists no simple\nopen book decomposition of $S^5$ with a spin page where all $3$-dimensional\nopen books admit open book embeddings. We also provide many constructions and\ncriteria for extendible and non-extendible mapping classes, and discuss a\nconnection between extendibility and sliceness of links in a homology $4$-ball\nwith $S^3$ boundary. Finally, we give a new generating set of the group of\nextendible mapping classes for the trivial embedding of a closed genus $g$\nsurface in $S^4$, consisting of $3g$ generators. This improves a previous\nresult of Hirose giving a generating set of size $6g-1$.",
        "We study sublevel set and superlevel set persistent homology on discrete\nfunctions through the perspective of finite ordered sets of both linearly\nordered and cyclically ordered domains. Finite ordered sets also serve as the\ncodomain of our functions making all arguments finite and discrete. We prove\nduality of filtrations of sublevel sets and superlevel sets that undergirths a\nrange of duality results of sublevel set persistent homology without the need\nto invoke complications of continuous functions or classical Morse theory. We\nshow that Morse-like behavior can be achieved for flat extrema without assuming\ngenericity. Additionally, we show that with inversion of order, one can compute\nsublevel set persistence from superlevel set persistence, and vice versa via a\nduality result that does not require the boundary to be treated as a special\ncase. Furthermore, we discuss aspects of barcode construction rules, surgery of\ncircular and linearly ordered sets, as well as surgery on auxiliary structures\nsuch as box snakes, which segment the ordered set by extrema and monotones.",
        "This paper defines quantiles, ranks and statistical depths for image data by\nleveraging ideas from measure transportation. The first step is to embed a\ndistribution of images in a tangent space, with the framework of linear optimal\ntransport. Therein, Monge-Kantorovich quantiles are shown to provide a\nmeaningful ordering of image data, with outward images having unusual shapes.\nNumerical experiments showcase the relevance of the proposed procedure, for\ndescriptive analysis, outlier detection or statistical testing.",
        "The rapid advancements in transformer-based language models have\nrevolutionized natural language processing, yet understanding the internal\nmechanisms of these models remains a significant challenge. This paper explores\nthe application of sparse autoencoders (SAE) to interpret the internal\nrepresentations of protein language models, specifically focusing on the ESM-2\n8M parameter model. By performing a statistical analysis on each latent\ncomponent's relevance to distinct protein annotations, we identify potential\ninterpretations linked to various protein characteristics, including\ntransmembrane regions, binding sites, and specialized motifs.\n  We then leverage these insights to guide sequence generation, shortlisting\nthe relevant latent components that can steer the model towards desired targets\nsuch as zinc finger domains. This work contributes to the emerging field of\nmechanistic interpretability in biological sequence models, offering new\nperspectives on model steering for sequence design.",
        "Consider a diffusion process X, solution of a time-homogeneous stochastic\ndifferential equation. We assume that the diffusion process X is observed at\ndiscrete times, at high frequency, which means that the time step tends toward\nzero. In addition, the drift and diffusion coefficients of the process X are\nassumed to be unknown. In this paper, we study the minimax rates of convergence\nof the nonparametric estimators of the square of the diffusion coefficient. Two\nobservation schemes are considered depending on the estimation interval. The\nsquare of the diffusion coefficient is estimated on the real line from repeated\nobservations of the process X, where the number of diffusion paths tends to\ninfinity. For the case of a compact estimation interval, we study the\nnonparametric estimation of the square of the diffusion coefficient constructed\nfrom a single diffusion path on one side and from repeated observations on the\nother side, where the number of trajectories tends to infinity. In each of\nthese cases, we establish minimax convergence rates of the risk of estimation\nof the diffusion coefficient over a space of Holder functions.",
        "We theoretically propose a new method in cavity- and surface-enhanced Raman\nspectroscopy (SERS) with improved temporal resolution in the measurement of\nstochastic Raman spectral fluctuations. Our approach combines Fourier\nspectroscopy and photon correlation to decouple the integration time from the\ntemporal resolution. Using statistical optics simulations, we establish the\nrelationship between time resolution and Raman signal strength, revealing that\ntypical Raman spectral fluctuations, commensurate with molecular conformational\ndynamics, can theoretically be resolved on micro- to millisecond timescales.\nThe method can further extract average single-molecule dynamics from small\nsub-ensembles, thereby potentially mitigating challenges in achieving strictly\nsingle-molecule isolation on SERS substrates.",
        "We investigate both the $Z$ and $H$ poles solutions for the Higgsino mass\nparameter $\\mu>0$ and $\\mu<0$ for the neutralino dark matter in light of the\nLHC supersymmetry searches and the direct detection dark matter experiments,\nLUX-ZEPLIN (LZ), in the Generalized Minimal Supergravity (GmSUGRA). Our study\nindicates that the latest experimental constraints from the LHC and LZ\nCollaborations exclude the light Higgsinos in the $Z$ and $H$ pole regions for\nthe $\\mu>0$ case. Interestingly, for the $\\mu < 0$ case, a very light Higgsinos\ncan still be consistent with the current constraints from the electroweakino\nsearches and LZ experiment in the $Z$ and $H$ poles. Consequently, the $\\mu <\n0$ case appears more promising and thus requires the dedicated efforts to make\ndefinitive conclusions about their current status from the experimental\nCollaborations. In this framework, our findings indicate a deviation of up to\n$2\\sigma$ from the central value of \\( a_\\mu \\equiv (g-2)_\\mu\/2 \\), resonating\nwith the experimental results reported by CMD and BDM.",
        "Braiding and fusion of Majorana zero modes are key elements of any future\ntopological Majorana-based quantum computer. Here, we investigate the fusion\ndynamics of Majorana zero modes in the spinless Kitaev model, as well as in a\nspinfull model describing magnet-superconductor hybrid structures. We consider\nvarious scenarios allowing us to reproduce the fusion rules of the Ising anyon\nmodel. Particular emphasis is given to the charge of the fermion obtained after\nfusing two Majorana zero modes: as long as it remains on the superconductor,\ncharge quantization is absent. When moving the fermion to a non-superconducting\nregion, such as a quantum dot, nearly-quantized charge can be measured. Our\nfindings confirm for both platforms that fusion dynamics of Majorana zero modes\ncan indeed be used for the readout of Majorana qubits.",
        "The optimal error exponents of binary composite i.i.d. state discrimination\nare trivially bounded by the worst-case pairwise exponents of discriminating\nindividual elements of the sets representing the two hypotheses, and in the\nfinite-dimensional classical case, these bounds in fact give exact single-copy\nexpressions for the error exponents. In contrast, in the non-commutative case,\nthe optimal exponents are only known to be expressible in terms of regularized\ndivergences, resulting in formulas that, while conceptually relevant,\npractically not very useful. In this paper, we develop further an approach\ninitiated in [Mosonyi, Szil\\'agyi, Weiner, IEEE Trans. Inf. Th.\n68(2):1032--1067, 2022] to give improved single-copy bounds on the error\nexponents by comparing not only individual states from the two hypotheses, but\nalso various unnormalized positive semi-definite operators associated to them.\nHere, we show a number of equivalent characterizations of such operators giving\nvalid bounds, and show that in the commutative case, considering weighted\ngeometric means of the states, and in the case of two states per hypothesis,\nconsidering weighted Kubo-Ando geometric means, are optimal for this approach.\nAs a result, we give a new characterization of the weighted Kubo-Ando geometric\nmeans as the only $2$-variable operator geometric means that are block\nadditive, tensor multiplicative, and satisfy the arithmetic-geometric mean\ninequality. We also extend our results to composite quantum channel\ndiscrimination, and show an analogous optimality property of the weighted\nKubo-Ando geometric means of two quantum channels, a notion that seems to be\nnew. We extend this concept to defining the notion of superoperator perspective\nfunction and establish some of its basic properties, which may be of\nindependent interest.",
        "Older male workers exhibit diverse retirement behaviors across occupations\nand respond differently to policy changes, influenced significantly by the\npart-time penalty, wage reduction faced by part-time workers compared to their\nfull-time counterparts. Many older individuals reduce their working hours, and\nin occupations with high part-time penalties, they tend to retire earlier, as\nobserved in data from Japan and the United States. This study develops a\ngeneral equilibrium model that incorporates occupational choices, endogenous\nlabor supply, highlighting that the impact on the retirement decision is\namplified by the presence of assets and pensions. I find that cutting\nemployees' pension benefits reduce aggregate labor supply in occupations with\nhigh part-time penalties in Japan, reducing overall welfare across the economy.\nIn contrast, increasing income tax credits and exempting pension form income\ntax boost labor supply across all occupations and enhance welfare by raising\ndisposable wages relative to the reservation wage. Reducing part-time penalties\nin high-penalty occupations also stimulate the labor supply in high-penalty\noccupations and improve long-term welfare.",
        "A major challenge in the deployment of hydrokinetic turbines in aquatic\nenvironments is the risk of fish collisions. Traditional fish collision models\noften oversimplify this risk by neglecting critical factors, such as the\nthickness of the turbine and accessory structures. Additionally, variations in\nfish size and species are frequently overlooked. This study addresses these\ngaps by developing a swimming mechanics-based fish-blade collision model. Using\na Lagrangian particle tracking approach, we simulate fish movements and\nevaluate collision risks with a representative hydrokinetic turbine, both with\nand without ducts. The model is applied to the velocity field at Baton Rouge,\nLouisiana, allowing for the assessment of collision risks across different fish\nspecies. The results offer valuable insights for turbine siting, optimization\nof turbine placement, and evaluation of protective designs to reduce\nenvironmental impacts in complex flow environments.",
        "Stochastic systems have a control-theoretic interpretation in which noise\nplays the role of endogenous control. In the weak-noise limit, relevant at low\ntemperatures or in large populations, control is optimal and an exact\nmathematical mapping from noise to control is described, where the maximizing\nthe probability of a state becomes the control objective. In Langevin dynamics\nnoise is identified directly with control, while in general Markov jump\nprocesses, which include chemical reaction networks and electronic circuits, we\nuse the Doi-Zel'dovich-Grassberger-Goldenfeld-Peliti path integral to identify\nthe `response' or `tilt' field $\\pi$ as control, which is proportional to the\nnoise in the semiclassical limit. This solves the longstanding problem of\ninterpreting $\\pi$. We illustrate the mapping on multistable chemical reaction\nnetworks and systems with unstable fixed points. The noise-control mapping\nbuilds intuition for otherwise puzzling phenomena of stochastic systems: why\nthe probability is generically a non-smooth function of state out of thermal\nequilibrium; why biological mechanisms can work better in the presence of\nnoise; and how agentic behavior emerges naturally without recourse to\nmysticism.",
        "As argued in arXiv:2104.10172, introducing a non-minimally coupled scalar\nfield, three-dimensional Einstein gravity can be extended by infinite families\nof theories which admit simple analytic generalizations of the charged BTZ\nblack hole. Depending on the gravitational couplings, the solutions may\ndescribe black holes with one or several horizons and with curvature or\nBTZ-like singularities. In other cases, the metric function behaves as\n$f(r)\\overset{(r\\rightarrow 0)}{\\sim} \\mathcal{O}(r^{2s})$ with $s\\geq 1$, and\nthe black holes are completely regular -- a feature unique to three dimensions.\nRegularity arises generically i.e., without requiring any fine-tuning of\nparameters. In this paper we show that all these theories satisfy Birkhoff\ntheorems, so that the most general spherically-symmetric solutions are given by\nthe corresponding static black holes. We perform a thorough characterization of\nthe Penrose diagrams of the solutions, finding a rich structure which includes,\nin particular, cases which tessellate the plane and others in which the\ndiagrams cannot be drawn in a single plane. We also study the motion of probe\nparticles on the black holes, finding that observers falling to regular black\nholes reach the center after a finite proper time. Contrary to the singular\ncases, the particles are not torn apart by tidal forces, so they oscillate\nbetween antipodal points describing many-universe orbits. We argue that in\nthose cases the region $r=0$ can be interpreted as a horizon with vanishing\nsurface gravity, giving rise to generic inner-extremal regular black hole\nsolutions. We also analyze the deep interior region of the solutions\nidentifying the presence of Kasner eons and the conditions under which they\ntake place. Finally, we construct new black hole solutions in the case in which\ninfinite towers of terms are included in the action.",
        "Interatomic potentials are essential for driving molecular dynamics (MD)\nsimulations, directly impacting the reliability of predictions regarding the\nphysical and chemical properties of materials. In recent years, machine-learned\npotentials (MLPs), trained against first-principles calculations, have become a\nnew paradigm in materials modeling as they provide a desirable balance between\naccuracy and computational cost. The neuroevolution potential (NEP) approach,\nimplemented in the open-source GPUMD software, has emerged as a promising\nmachine-learned potential, exhibiting impressive accuracy and exceptional\ncomputational efficiency. This review provides a comprehensive discussion on\nthe methodological and practical aspects of the NEP approach, along with a\ndetailed comparison with other representative state-of-the-art MLP approaches\nin terms of training accuracy, property prediction, and computational\nefficiency. We also demonstrate the application of the NEP approach to perform\naccurate and efficient MD simulations, addressing complex challenges that\ntraditional force fields typically can not tackle. Key examples include\nstructural properties of liquid and amorphous materials, chemical order in\ncomplex alloy systems, phase transitions, surface reconstruction, material\ngrowth, primary radiation damage, fracture in two-dimensional materials,\nnanoscale tribology, and mechanical behavior of compositionally complex alloys\nunder various mechanical loadings. This review concludes with a summary and\nperspectives on future extensions to further advance this rapidly evolving\nfield.",
        "In this article, the time-discretization of the fluid structure interaction\nmodel in the three-dimensional boundary domain is taken into account, which\nexplains the mechanical interaction between the blood flow and the Hookean\nelasticity. The interface between the two phases is given by a soft transition\nlayer and spreads to the finite thickness. On the implicit Euler scheme for\nthis discretization, We derive a variety of priori estimates and then use the\nFaedo-Galerkin method to prove the local well-poseedness results.",
        "The dynamics of learning in modern large AI systems is hierarchical, often\ncharacterized by abrupt, qualitative shifts akin to phase transitions observed\nin physical systems. While these phenomena hold promise for uncovering the\nmechanisms behind neural networks and language models, existing theories remain\nfragmented, addressing specific cases. In this paper, we posit that parameter\nsymmetry breaking and restoration serve as a unifying mechanism underlying\nthese behaviors. We synthesize prior observations and show how this mechanism\nexplains three distinct hierarchies in neural networks: learning dynamics,\nmodel complexity, and representation formation. By connecting these\nhierarchies, we highlight symmetry -- a cornerstone of theoretical physics --\nas a potential fundamental principle in modern AI.",
        "We investigate the detectability of extended mid-infrared (MIR) emission\nassociated with FU-Ori type objects (FUors) using the METIS coronagraphs on the\n39-m Extremely Large Telescope (ELT). The imaging simulations were made for\nthree representative filters ($\\lambda$=3.8, 4.8, and 11.3 micron) of the METIS\ninstrument. We demonstrate that the detectability of the extended MIR emission\nusing these coronagraphs is highly dependent on the uncertain nature of the\ncentral FUor and its circumstellar environment in various contexts. These\ncontexts are: (A) whether the central radiation source is either a flat\nself-luminous accretion disk or a star at near-infrared (NIR) wavelengths, (B)\nthe size of the accretion disk for the bright central MIR emission at\nmilliarcsecond scales, (C) whether the extended emission is due to either an\noptically thick disk or an optically thin envelope, and (D) dust grain models.\nObservations at $\\lambda$=3.8 micron will allow us to detect the extended\nemission in many cases, while the number of cases with detection may\nsignificantly decrease toward longer wavelengths due to the fainter nature of\nthe extended emission and high thermal background noise. In some cases, the\npresence of a binary companion can significantly hamper detections of the\nextended MIR emission. NIR and MIR imaging observations at existing 8-m class\ntelescopes, prior to the METIS observations, will be useful for (1) reducing\nthe many model uncertainties and (2) searching for binary companions associated\nwith FUors, therefore determining the best observing strategy using METIS.",
        "In this paper, we have found that some certain Fermat-type shift and\ndifference equations have the meromorphic solutions generated by Riccati type\nfunctions. Also we have solved the open problems posed by Liu and Yang (A note\non meromorphic solutions of Fermat types equations, An. Stiint. Univ. Al. I.\nCuza Lasi Mat. (N. S.), 62(2)(1), 317-325 (2016)). We have fortified the claims\nby some examples.",
        "The discovery of high-performance electrocatalysts is crucial for advancing\nsustainable energy technologies. Compositionally complex solid solutions\ncomprising multiple metals offer promising catalytic properties, yet their\nexploration is challenging due to the combinatorial explosion of possible\ncompositions. In this work, we combine combinatorial sputtering of thin-film\nmaterials libraries and their high-throughput characterization with Bayesian\noptimization to efficiently explore the quaternary composition space\nNi-Pd-Pt-Ru for the oxygen evolution reaction in alkaline media. Using this\nmethod, the global activity optimum of pure Ru was identified after covering\nless than 20% of the complete composition space with six materials libraries.\nSix additional libraries were fabricated to validate the activity trend. The\nresulting dataset is used to formulate general guidelines for the efficient\ncomposition space exploration using combinatorial synthesis paired with\nBayesian optimization.",
        "In this paper, we study the hydrostatic approximation for the 3D Oldroyd-B\nmodel. Firstly, we derive the hydrostatic approximate system for this model and\nprove the global well-posedness of the limit system with small analytic initial\ndata in horizontal variable. Then we justify the hydrostatic limit strictly\nfrom the re-scaled Oldroyd-B model to the hydrostatic Oldroyd-B model and\nobtain the precise convergence rate.",
        "The theoretical foundation of quantum sensing is rooted in the Cram\\'er-Rao\nformalism, which establishes quantitative precision bounds for a given quantum\nprobe. In many practical scenarios, where more than one parameter is unknown,\nthe multi-parameter Cram\\'er-Rao bound (CRB) applies. Since this is a matrix\ninequality involving the inverse of the quantum Fisher information matrix\n(QFIM), the formalism breaks down when the QFIM is singular. In this paper, we\nexamine the physical origins of such singularities, showing that they result\nfrom an over-parametrization on the metrological level. This is itself caused\nby emergent metrological symmetries, whereby the same set of measurement\noutcomes are obtained for different combinations of system parameters. Although\nthe number of effective parameters is equal to the number of non-zero QFIM\neigenvalues, the Cram\\'er-Rao formalism typically does not provide information\nabout the effective parameter encoding. Instead, we demonstrate through a\nseries of concrete examples that Bayesian estimation can provide deep insights.\nIn particular, the metrological symmetries appear in the Bayesian posterior\ndistribution as lines of persistent likelihood running through the space of\nunknown parameters. These lines are contour lines of the effective parameters\nwhich, through suitable parameter transformations, can be estimated and follow\ntheir own effective CRBs.",
        "We derive the quantum master equation predicting how the translational and\nrotational dynamics of a nanoparticle is affected by the emission of surface\nadsorbates. This is motivated by recent experiments which prepared the motion\nof internally hot silica particles in the deep quantum regime. In the limit of\na well localized nanoparticle the ro-translational dynamics can be\ncharacterized by diffusion rates in quantitative agreement with classical\nexpectations. The theory is also suited to describe the decoherence effect of\noutgassing and sublimation."
      ]
    }
  },
  {
    "id":2411.18259,
    "research_type":"applied",
    "start_id":"b2",
    "start_title":"Quantifying the performance of machine learning models in materials discovery",
    "start_abstract":"The predictive capabilities of machine learning (ML) models used in materials discovery are typically measured using simple statistics such as the root-mean-square error (RMSE) or coefficient determination ($r^2$) between ML-predicted property values and their known values. A tempting assumption is that with low should be effective at guiding discovery, conversely, high give poor performance. However, we observe no clear connection exists a \"static\" quantity averaged across an entire training set, RMSE, ML model's ability to dynamically guide iterative (and often extrapolative) novel targeted properties. In this work, simulate sequential (SL)-guided process demonstrate decoupling traditional model metrics performance discoveries. We show depends strongly on (1) target range within distribution (e.g., whether 1st 10th decile material desired); (2) incorporation uncertainty estimates SL acquisition function; (3) scientist interested one many targets; (4) how iterations allowed. To overcome limitations static robustly capture performance, recommend Discovery Yield ($DY$), measure high-performing were discovered during SL, Probability ($DP$), likelihood discovering any point process.",
    "start_categories":[
      "cond-mat.mtrl-sci"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "Physics-informed machine learning"
      ],
      "abstract":[
        "Despite great progress in simulating multiphysics problems using the numerical discretization of partial differential equations (PDEs), one still cannot seamlessly incorporate noisy data into existing algorithms, mesh generation remains complex, and high-dimensional problems governed by parameterized PDEs cannot be tackled. Moreover, solving inverse problems with hidden physics is often prohibitively expensive and requires different formulations and elaborate computer codes. Machine learning has emerged as a promising alternative, but training deep neural networks requires big data, not always available for scientific problems. Instead, such networks can be trained from additional information obtained by enforcing the physical laws (for example, at random points in the continuous space-time domain). Such physics-informed learning integrates (noisy) data and mathematical models, and implements them through neural networks or other kernel-based regression networks. Moreover, it may be possible to design specialized network architectures that automatically satisfy some of the physical invariants for better accuracy, faster training and improved generalization. Here, we review some of the prevailing trends in embedding physics into machine learning, present some of the current capabilities and limitations and discuss diverse applications of physics-informed learning both for forward and inverse problems, including discovering hidden physics and tackling high-dimensional problems."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Attention to Trajectory: Trajectory-Aware Open-Vocabulary Tracking",
        "One citation, one vote! A new approach for analysing\n  check-all-that-apply (CATA) data in sensometrics, using L1 norm methods",
        "Split-n-Chain: Privacy-Preserving Multi-Node Split Learning with\n  Blockchain-Based Auditability",
        "Analysis of kinematics of mechanisms containing revolute joints",
        "Controllable Emotion Generation with Emotion Vectors",
        "Automated Refactoring of Non-Idiomatic Python Code: A Differentiated\n  Replication with LLMs",
        "DanmuA11y: Making Time-Synced On-Screen Video Comments (Danmu)\n  Accessible to Blind and Low Vision Users via Multi-Viewer Audio Discussions",
        "Mixing Any Cocktail with Limited Ingredients: On the Structure of Payoff\n  Sets in Multi-Objective MDPs and its Impact on Randomised Strategies",
        "Asymptotic Optimism of Random-Design Linear and Kernel Regression Models",
        "On Nash Equilibria in Play-Once and Terminal Deterministic Graphical\n  Games",
        "Optimal generalisation and learning transition in extensive-width\n  shallow neural networks near interpolation",
        "MAST-Pro: Dynamic Mixture-of-Experts for Adaptive Segmentation of\n  Pan-Tumors with Knowledge-Driven Prompts",
        "VideoAnydoor: High-fidelity Video Object Insertion with Precise Motion\n  Control",
        "A class of anisotropic diffusion-transport equations in non-divergence\n  form",
        "A conjecture on monomial realizations and polyhedral realizations for\n  crystal bases",
        "Maximal Representation Dimensions of Algebraic Tori of Fixed Dimension\n  Over Arbitrary Fields",
        "Duals of limiting interpolation spaces",
        "THOR: A Non-Speculative Value Dependent Timing Side Channel Attack\n  Exploiting Intel AMX",
        "MHAF-YOLO: Multi-Branch Heterogeneous Auxiliary Fusion YOLO for accurate\n  object detection",
        "HybridLinker: Topology-Guided Posterior Sampling for Enhanced Diversity\n  and Validity in 3D Molecular Linker Generation",
        "Utilizing API Response for Test Refinement",
        "Runway capacity expansion planning for public airports under demand\n  uncertainty",
        "Exponentially accurate spectral Monte Carlo method for linear PDEs and\n  their error estimates",
        "Problems on handlebody groups",
        "Recursive decoding of projective Reed-Muller codes",
        "SkyRover: A Modular Simulator for Cross-Domain Pathfinding",
        "LLaVA-Octopus: Unlocking Instruction-Driven Adaptive Projector Fusion\n  for Video Understanding",
        "Debate Helps Weak-to-Strong Generalization",
        "Toeplitz Operators on Fock Space $F_{\\alpha}^{\\infty}$"
      ],
      "abstract":[
        "Open-Vocabulary Multi-Object Tracking (OV-MOT) aims to enable approaches to\ntrack objects without being limited to a predefined set of categories. Current\nOV-MOT methods typically rely primarily on instance-level detection and\nassociation, often overlooking trajectory information that is unique and\nessential for object tracking tasks. Utilizing trajectory information can\nenhance association stability and classification accuracy, especially in cases\nof occlusion and category ambiguity, thereby improving adaptability to novel\nclasses. Thus motivated, in this paper we propose \\textbf{TRACT}, an\nopen-vocabulary tracker that leverages trajectory information to improve both\nobject association and classification in OV-MOT. Specifically, we introduce a\n\\textit{Trajectory Consistency Reinforcement} (\\textbf{TCR}) strategy, that\nbenefits tracking performance by improving target identity and category\nconsistency. In addition, we present \\textbf{TraCLIP}, a plug-and-play\ntrajectory classification module. It integrates \\textit{Trajectory Feature\nAggregation} (\\textbf{TFA}) and \\textit{Trajectory Semantic Enrichment}\n(\\textbf{TSE}) strategies to fully leverage trajectory information from visual\nand language perspectives for enhancing the classification results. Extensive\nexperiments on OV-TAO show that our TRACT significantly improves tracking\nperformance, highlighting trajectory information as a valuable asset for\nOV-MOT. Code will be released.",
        "A unified framework is provided for analysing check-all-that-apply (CATA)\nproduct data following the ``one citation, one vote\" principle. CATA data arise\nfrom studies where A consumers evaluate P products by describing samples by\nchecking all of the T terms that apply. Giving every citation the same weight,\nregardless of the assessor, product, or term, leads to analyses based on the L1\nnorm where the median absolute deviation is the measure of dispersion. Five\npermutation tests are proposed to answer the following questions. Do any\nproducts differ? For which terms do products differ? Within each of the terms,\nwhich products differ? Which product pairs differ? On which terms does each\nproduct pair differ? Additionally, we show how products and terms can be\nclustered following the ``one citation, one vote\" principle and how L1-norm\nprincipal component analysis (L1-norm PCA) can be applied to visualize CATA\nresults in few dimensions. Together, the permutation tests, clustering methods,\nand L1-norm PCA provide a unified approach. The proposed methods are\nillustrated using a data set in which 100 consumers evaluated 11 products using\n34 CATA terms.R code is provided to perform the analyses.",
        "Deep learning, when integrated with a large amount of training data, has the\npotential to outperform machine learning in terms of high accuracy. Recently,\nprivacy-preserving deep learning has drawn significant attention of the\nresearch community. Different privacy notions in deep learning include privacy\nof data provided by data-owners and privacy of parameters and\/or\nhyperparameters of the underlying neural network. Federated learning is a\npopular privacy-preserving execution environment where data-owners participate\nin learning the parameters collectively without leaking their respective data\nto other participants. However, federated learning suffers from certain\nsecurity\/privacy issues. In this paper, we propose Split-n-Chain, a variant of\nsplit learning where the layers of the network are split among several\ndistributed nodes. Split-n-Chain achieves several privacy properties:\ndata-owners need not share their training data with other nodes, and no nodes\nhave access to the parameters and hyperparameters of the neural network (except\nthat of the respective layers they hold). Moreover, Split-n-Chain uses\nblockchain to audit the computation done by different nodes. Our experimental\nresults show that: Split-n-Chain is efficient, in terms of time required to\nexecute different phases, and the training loss trend is similar to that for\nthe same neural network when implemented in a monolithic fashion.",
        "Kinematics of rigid bodies can be analyzed in many different ways. The\nadvantage of using Euler parameters is that the resulting equations are\npolynomials and hence computational algebra, in particular Gr\\\"obner bases, can\nbe used to study them. The disadvantage of the Gr\\\"obner basis methods is that\nthe computational complexity grows quite fast in the worst case in the number\nof variables and the degree of polynomials. In the present article we show how\nto simplify computations when the mechanism contains revolute joints. The idea\nis based on the fact that the ideal representing the constraints of the\nrevolute joint is not prime. Choosing the appropriate prime component reduces\nsignificantly the computational cost. We illustrate the method by applying it\nto the well known Bennett's and Bricard's mechanisms, but it can be applied to\nany mechanism which has revolute joints.",
        "In recent years, technologies based on large-scale language models (LLMs)\nhave made remarkable progress in many fields, especially in customer service,\ncontent creation, and embodied intelligence, showing broad application\npotential. However, The LLM's ability to express emotions with proper tone,\ntiming, and in both direct and indirect forms is still insufficient but\nsignificant. Few works have studied on how to build the controlable emotional\nexpression capability of LLMs. In this work, we propose a method for emotion\nexpression output by LLMs, which is universal, highly flexible, and well\ncontrollable proved with the extensive experiments and verifications. This\nmethod has broad application prospects in fields involving emotions output by\nLLMs, such as intelligent customer service, literary creation, and home\ncompanion robots. The extensive experiments on various LLMs with different\nmodel-scales and architectures prove the versatility and the effectiveness of\nthe proposed method.",
        "In the Python ecosystem, the adoption of idiomatic constructs has been\nfostered because of their expressiveness, increasing productivity and even\nefficiency, despite controversial arguments concerning familiarity or\nunderstandability issues. Recent research contributions have proposed\napproaches -- based on static code analysis and transformation -- to\nautomatically identify and enact refactoring opportunities of non-idiomatic\ncode into idiomatic ones. Given the potential recently offered by Large\nLanguage Models (LLMs) for code-related tasks, in this paper, we present the\nresults of a replication study in which we investigate GPT-4 effectiveness in\nrecommending and suggesting idiomatic refactoring actions. Our results reveal\nthat GPT-4 not only identifies idiomatic constructs effectively but frequently\nexceeds the benchmark in proposing refactoring actions where the existing\nbaseline failed. A manual analysis of a random sample shows the correctness of\nthe obtained recommendations. Our findings underscore the potential of LLMs to\nachieve tasks where, in the past, implementing recommenders based on complex\ncode analyses was required.",
        "By overlaying time-synced user comments on videos, Danmu creates a\nco-watching experience for online viewers. However, its visual-centric design\nposes significant challenges for blind and low vision (BLV) viewers. Our\nformative study identified three primary challenges that hinder BLV viewers'\nengagement with Danmu: the lack of visual context, the speech interference\nbetween comments and videos, and the disorganization of comments. To address\nthese challenges, we present DanmuA11y, a system that makes Danmu accessible by\ntransforming it into multi-viewer audio discussions. DanmuA11y incorporates\nthree core features: (1) Augmenting Danmu with visual context, (2) Seamlessly\nintegrating Danmu into videos, and (3) Presenting Danmu via multi-viewer\ndiscussions. Evaluation with twelve BLV viewers demonstrated that DanmuA11y\nsignificantly improved Danmu comprehension, provided smooth viewing\nexperiences, and fostered social connections among viewers. We further\nhighlight implications for enhancing commentary accessibility in video-based\nsocial media and live-streaming platforms.",
        "We consider multi-dimensional payoff functions in Markov decision processes,\nand ask whether a given expected payoff vector can be achieved or not. In\ngeneral, pure strategies (i.e., not resorting to randomisation) do not suffice\nfor this problem.\n  We study the structure of the set of expected payoff vectors of all\nstrategies given a multi-dimensional payoff function and its consequences\nregarding randomisation requirements for strategies. In particular, we prove\nthat for any payoff for which the expectation is well-defined under all\nstrategies, it is sufficient to mix (i.e., randomly select a pure strategy at\nthe start of a play and committing to it for the rest of the play) finitely\nmany pure strategies to approximate any expected payoff vector up to any\nprecision. Furthermore, for any payoff for which the expected payoff is finite\nunder all strategies, any expected payoff can be obtained exactly by mixing\nfinitely many strategies.",
        "We derived the closed-form asymptotic optimism of linear regression models\nunder random designs, and generalizes it to kernel ridge regression. Using\nscaled asymptotic optimism as a generic predictive model complexity measure, we\nstudied the fundamental different behaviors of linear regression model, tangent\nkernel (NTK) regression model and three-layer fully connected neural networks\n(NN). Our contribution is two-fold: we provided theoretical ground for using\nscaled optimism as a model predictive complexity measure; and we show\nempirically that NN with ReLUs behaves differently from kernel models under\nthis measure. With resampling techniques, we can also compute the optimism for\nregression models with real data.",
        "We consider finite $n$-person deterministic graphical games and study the\nexistence of pure stationary Nash-equilibrium in such games. We assume that all\ninfinite plays are equivalent and form a unique outcome, while each terminal\nposition is a separate outcome. It is known that for $n=2$ such a game always\nhas a Nash equilibrium, while that may not be true for $n > 2$.\n  A game is called {\\em play-once} if each player controls a unique position\nand {\\em terminal} if any terminal outcome is better than the infinite one for\neach player. We prove in this paper that play-once games have Nash equilibria.\n  We also show that terminal games have Nash equilibria if they have at most\nthree terminals.",
        "We consider a teacher-student model of supervised learning with a\nfully-trained 2-layer neural network whose width $k$ and input dimension $d$\nare large and proportional. We compute the Bayes-optimal generalisation error\nof the network for any activation function in the regime where the number of\ntraining data $n$ scales quadratically with the input dimension, i.e., around\nthe interpolation threshold where the number of trainable parameters $kd+k$ and\nof data points $n$ are comparable. Our analysis tackles generic weight\ndistributions. Focusing on binary weights, we uncover a discontinuous phase\ntransition separating a \"universal\" phase from a \"specialisation\" phase. In the\nfirst, the generalisation error is independent of the weight distribution and\ndecays slowly with the sampling rate $n\/d^2$, with the student learning only\nsome non-linear combinations of the teacher weights. In the latter, the error\nis weight distribution-dependent and decays faster due to the alignment of the\nstudent towards the teacher network. We thus unveil the existence of a highly\npredictive solution near interpolation, which is however potentially hard to\nfind.",
        "Accurate tumor segmentation is crucial for cancer diagnosis and treatment.\nWhile foundation models have advanced general-purpose segmentation, existing\nmethods still struggle with: (1) limited incorporation of medical priors, (2)\nimbalance between generic and tumor-specific features, and (3) high\ncomputational costs for clinical adaptation. To address these challenges, we\npropose MAST-Pro (Mixture-of-experts for Adaptive Segmentation of pan-Tumors\nwith knowledge-driven Prompts), a novel framework that integrates dynamic\nMixture-of-Experts (D-MoE) and knowledge-driven prompts for pan-tumor\nsegmentation. Specifically, text and anatomical prompts provide domain-specific\npriors, guiding tumor representation learning, while D-MoE dynamically selects\nexperts to balance generic and tumor-specific feature learning, improving\nsegmentation accuracy across diverse tumor types. To enhance efficiency, we\nemploy Parameter-Efficient Fine-Tuning (PEFT), optimizing MAST-Pro with\nsignificantly reduced computational overhead. Experiments on multi-anatomical\ntumor datasets demonstrate that MAST-Pro outperforms state-of-the-art\napproaches, achieving up to a 5.20% improvement in average DSC while reducing\ntrainable parameters by 91.04%, without compromising accuracy.",
        "Despite significant advancements in video generation, inserting a given\nobject into videos remains a challenging task. The difficulty lies in\npreserving the appearance details of the reference object and accurately\nmodeling coherent motions at the same time. In this paper, we propose\nVideoAnydoor, a zero-shot video object insertion framework with high-fidelity\ndetail preservation and precise motion control. Starting from a text-to-video\nmodel, we utilize an ID extractor to inject the global identity and leverage a\nbox sequence to control the overall motion. To preserve the detailed appearance\nand meanwhile support fine-grained motion control, we design a pixel warper. It\ntakes the reference image with arbitrary key-points and the corresponding\nkey-point trajectories as inputs. It warps the pixel details according to the\ntrajectories and fuses the warped features with the diffusion U-Net, thus\nimproving detail preservation and supporting users in manipulating the motion\ntrajectories. In addition, we propose a training strategy involving both videos\nand static images with a weighted loss to enhance insertion quality.\nVideoAnydoor demonstrates significant superiority over existing methods and\nnaturally supports various downstream applications (e.g., talking head\ngeneration, video virtual try-on, multi-region editing) without task-specific\nfine-tuning.",
        "We generalize Einstein's probabilistic method for the Brownian motion to\nstudy compressible fluids in porous media. The multi-dimensional case is\nconsidered with general probability distribution functions. By relating the\nexpected displacement per unit time with the velocity of the fluid, we derive\nan anisotropic diffusion equation in non-divergence form that contains a\ntransport term. Under the Darcy law assumption, a corresponding nonlinear\npartial differential equations for the density function is obtained. The\nclassical solutions of this equation are studied, and the maximum and strong\nmaximum principles are established. We also obtain exponential decay estimates\nfor the solutions for all time, and particularly, their exponential convergence\nas time tends to infinity. Our analysis uses some transformations of the\nBernstein-Cole--Hopf type which are explicitly constructed even for very\ngeneral equation of state. Moreover, the Lemma of Growth in time is proved and\nutilized in order to achieve the above decaying estimates.",
        "Crystal bases are powerful combinatorial tools in the representation theory\nof quantum groups $U_q(\\mathfrak{g})$ for a symmetrizable Kac-Moody algebras\n$\\mathfrak{g}$. The polyhedral realizations are combinatorial descriptions of\nthe crystal base $B(\\infty)$ for Verma modules in terms of the set of integer\npoints of a polyhedral cone, which equals the string cone when $\\mathfrak{g}$\nis finite dimensional simple. It is a fundamental and natural problem to find\nexplicit forms of the polyhedral cone. The monomial realization expresses\ncrystal bases $B(\\lambda)$ of integrable highest weight representations as\nLaurent monomials with double indexed variables. In this paper, we give a\nconjecture between explicit forms of the polyhedral cones and monomial\nrealizations. We prove the conjecture is true when $\\mathfrak{g}$ is a\nclassical Lie algebra, a rank $2$ Kac-Moody algebra or a classical affine Lie\nalgebra.",
        "We define the representation dimension of an algebraic torus $T$ to be the\nminimal positive integer $r$ such that there exists a faithful embedding $T\n\\hookrightarrow \\operatorname{GL}_r$. Given a positive integer $n$, there\nexists a maximal representation dimension of all $n$-dimensional algebraic tori\nover all fields. In this paper, we use the theory of group actions on lattices\nto find lower bounds on this maximum for all $n$. Further, we find the exact\nmaximum value for irreducible tori for all $n \\in \\left\\lbrace 1, 2, \\dots, 10,\n11, 13, 17, 19, 23\\right\\rbrace$ and conjecturally infinitely many primes $n$.",
        "The aim of the paper is to establish duals of the limiting real interpolation\n$K$- and $J$-spaces $(X_0,X_1)_{0,q,v;K}$ and $(X_0,X_1)_{0,q,v;J}$, where\n$(X_0,X_1)$ is a compatible couple of Banach spaces, $1\\le q<\\infty$, $v$ is a\nslowly varying function on the interval $(0,\\infty)$, and the symbols $K$ and\n$J$ stand for the Peetre $K$- and $J$-functionals. In the case of the classical\nreal interpolation method $(X_0,X_1)_{\\theta,q}$, where $\\theta \\in (0,1)$ and\n$1\\le q < \\infty$, this problem was solved by Lions and Peetre.",
        "The rise of on-chip accelerators signifies a major shift in computing, driven\nby the growing demands of artificial intelligence (AI) and specialized\napplications. These accelerators have gained popularity due to their ability to\nsubstantially boost performance, cut energy usage, lower total cost of\nownership (TCO), and promote sustainability. Intel's Advanced Matrix Extensions\n(AMX) is one such on-chip accelerator, specifically designed for handling tasks\ninvolving large matrix multiplications commonly used in machine learning (ML)\nmodels, image processing, and other computational-heavy operations. In this\npaper, we introduce a novel value-dependent timing side-channel vulnerability\nin Intel AMX. By exploiting this weakness, we demonstrate a software-based,\nvalue-dependent timing side-channel attack capable of inferring the sparsity of\nneural network weights without requiring any knowledge of the confidence score,\nprivileged access or physical proximity. Our attack method can fully recover\nthe sparsity of weights assigned to 64 input elements within 50 minutes, which\nis 631% faster than the maximum leakage rate achieved in the Hertzbleed attack.",
        "Due to the effective multi-scale feature fusion capabilities of the Path\nAggregation FPN (PAFPN), it has become a widely adopted component in YOLO-based\ndetectors. However, PAFPN struggles to integrate high-level semantic cues with\nlow-level spatial details, limiting its performance in real-world applications,\nespecially with significant scale variations. In this paper, we propose\nMHAF-YOLO, a novel detection framework featuring a versatile neck design called\nthe Multi-Branch Auxiliary FPN (MAFPN), which consists of two key modules: the\nSuperficial Assisted Fusion (SAF) and Advanced Assisted Fusion (AAF). The SAF\nbridges the backbone and the neck by fusing shallow features, effectively\ntransferring crucial low-level spatial information with high fidelity.\nMeanwhile, the AAF integrates multi-scale feature information at deeper neck\nlayers, delivering richer gradient information to the output layer and further\nenhancing the model learning capacity. To complement MAFPN, we introduce the\nGlobal Heterogeneous Flexible Kernel Selection (GHFKS) mechanism and the\nReparameterized Heterogeneous Multi-Scale (RepHMS) module to enhance feature\nfusion. RepHMS is globally integrated into the network, utilizing GHFKS to\nselect larger convolutional kernels for various feature layers, expanding the\nvertical receptive field and capturing contextual information across spatial\nhierarchies. Locally, it optimizes convolution by processing both large and\nsmall kernels within the same layer, broadening the lateral receptive field and\npreserving crucial details for detecting smaller targets. The source code of\nthis work is available at: https:\/\/github.com\/yang-0201\/MHAF-YOLO.",
        "Linker generation is critical in drug discovery applications such as lead\noptimization and PROTAC design, where molecular fragments are assembled into\ndiverse drug candidates. Existing methods fall into PC-Free and PC-Aware\ncategories based on their use of 3D point clouds (PC). PC-Free models\nprioritize diversity but suffer from lower validity due to overlooking PC\nconstraints, while PC-Aware models ensure higher validity but restrict\ndiversity by enforcing strict PC constraints. To overcome these trade-offs\nwithout additional training, we propose HybridLinker, a framework that enhances\nPC-Aware inference by providing diverse bonding topologies from a pretrained\nPC-Free model as guidance. At its core, we propose LinkerDPS, the first\ndiffusion posterior sampling (DPS) method operating across PC-Free and PC-Aware\nspaces, bridging molecular topology with 3D point clouds via an energy-inspired\nfunction. By transferring the diverse sampling distribution of PC-Free models\ninto the PC-Aware distribution, HybridLinker significantly and consistently\nsurpasses baselines, improving both validity and diversity in foundational\nmolecular design and applied property optimization tasks, establishing a new\nDPS framework in the molecular and graph domains beyond imaging.",
        "Most of the web services are offered in the form of RESTful APIs. This has\nled to an active research interest in API testing to ensure the reliability of\nthese services. While most of the testing techniques proposed in the past rely\non the API specification to generate the test cases, a major limitation of such\nan approach is that in the case of an incomplete or inconsistent specification,\nthe test cases may not be realistic in nature and would result in a lot of 4xx\nresponse due to invalid input. This is indicative of poor test quality.\nLearning-based approaches may learn about valid inputs but often require a\nlarge number of request-response pairs to learn the constraints, making it\ninfeasible to be readily used in the industry. To address this limitation, this\npaper proposes a dynamic test refinement approach that leverages the response\nmessage. The response is used to infer the point in the API testing flow where\na test scenario fix is required. Using an intelligent agent, the approach adds\nconstraints to the API specification that are further used to generate a test\nscenario accounting for the learned constraint from the response. Following a\ngreedy approach, the iterative learning and refinement of test scenarios are\nobtained from the API testing system. The proposed approach led to a decrease\nin the number of 4xx responses, taking a step closer to generating more\nrealistic test cases with high coverage that would aid in functional testing. A\nhigh coverage was obtained from a lesser number of API requests, as compared\nwith the state-of-the-art search-based API Testing tools.",
        "Flight delay is a significant issue affecting air travel. The runway system,\nfrequently falling short of demand, serves as a bottleneck. As demand\nincreases, runway capacity expansion becomes imperative to mitigate congestion.\nHowever, the decision to expand runway capacity is challenging due to inherent\nuncertainties in demand forecasts. This paper presents a novel approach to\nmodeling air traffic demand growth as a jump diffusion process, incorporating\ntwo layers of uncertainty: Geometric Brownian Motion (GBM) for continuous\nvariability and a Poisson process to capture the impact of crisis events, such\nas natural disasters or public health emergencies, on decision-making. We\npropose a real options model to jointly evaluate the interrelated factors of\noptimal runway capacity and investment timing under uncertainty, with\ninvestment timing linked to trigger demand. The findings suggest that increased\nuncertainty indicates more conservative decision-making. Furthermore, the\nrelationship between optimal investment timing and expansion size is complex:\nif the expansion size remains unchanged, the trigger demand decreases as the\ndemand growth rate increases; if the expansion size experiences a jump, the\ntrigger demand also exhibits a sharp rise. This work provides valuable insights\nfor airport authorities for informed capacity expansion decision-making.",
        "This paper introduces a spectral Monte Carlo iterative method (SMC) for\nsolving linear Poisson and parabolic equations driven by $\\alpha$-stable L\\'evy\nprocess with $\\alpha\\in (0,2)$, which was initially proposed and developed by\nGobet and Maire in their pioneering works (Monte Carlo Methods Appl 10(3-4),\n275--285, 2004, and SIAM J Numer Anal 43(3), 1256--1275, 2005) for the case\n$\\alpha=2$. The novel method effectively integrates multiple computational\ntechniques, including the interpolation based on generalized Jacobi functions\n(GJFs), space-time spectral methods, control variates techniques, and a novel\nwalk-on-sphere method (WOS). The exponential convergence of the error bounds is\nrigorously established through finite iterations for both Poisson and parabolic\nequations involving the integral fractional Laplacian operator. Remarkably, the\nproposed space-time spectral Monte Carlo method (ST-SMC) for the parabolic\nequation is unified for both $\\alpha\\in(0,2)$ and $\\alpha=2$. Extensive\nnumerical results are provided to demonstrate the spectral accuracy and\nefficiency of the proposed method, thereby validating the theoretical findings.",
        "We survey a number of constructions and open problems related to the\nhandlebody group, with a focus on recent trends in geometric group theory,\n(co)homological properties, and its relationship to outer automorphism groups\nof free groups. We also briefly describe how the \\emph{cheap\n$\\alpha$-rebuilding property} of Abert, Bergeron, Fraczyk, and Gaboriau can be\napplied using the disc complex to deduce results about the homology growth of\nthe handlebody group.",
        "We give a recursive decoding algorithm for projective Reed-Muller codes\nmaking use of a decoder for affine Reed-Muller codes. We determine the number\nof errors that can be corrected in this way, which is the current highest for\ndecoders of projective Reed-Muller codes. We show when we can decode up to the\nerror correction capability of these codes, and we compute the order of\ncomplexity of the algorithm, which is given by that of the chosen decoder for\naffine Reed-Muller codes.",
        "Unmanned Aerial Vehicles (UAVs) and Automated Guided Vehicles (AGVs)\nincreasingly collaborate in logistics, surveillance, inspection tasks and etc.\nHowever, existing simulators often focus on a single domain, limiting\ncross-domain study. This paper presents the SkyRover, a modular simulator for\nUAV-AGV multi-agent pathfinding (MAPF). SkyRover supports realistic agent\ndynamics, configurable 3D environments, and convenient APIs for external\nsolvers and learning methods. By unifying ground and aerial operations, it\nfacilitates cross-domain algorithm design, testing, and benchmarking.\nExperiments highlight SkyRover's capacity for efficient pathfinding and\nhigh-fidelity simulations in UAV-AGV coordination. Project is available at\nhttps:\/\/sites.google.com\/view\/mapf3d\/home.",
        "In this paper, we introduce LLaVA-Octopus, a novel video multimodal large\nlanguage model. LLaVA-Octopus adaptively weights features from different visual\nprojectors based on user instructions, enabling us to leverage the\ncomplementary strengths of each projector. We observe that different visual\nprojectors exhibit distinct characteristics when handling specific tasks. For\ninstance, some projectors excel at capturing static details, while others are\nmore effective at processing temporal information, and some are better suited\nfor tasks requiring temporal coherence. By dynamically adjusting feature\nweights according to user instructions, LLaVA-Octopus dynamically selects and\ncombines the most suitable features, significantly enhancing the model's\nperformance in multimodal tasks. Experimental results demonstrate that\nLLaVA-Octopus achieves excellent performance across multiple benchmarks,\nespecially in tasks such as video question answering, long video understanding,\nand comprehensive multi-choices benchmarks, highlighting its broad application\npotential.",
        "Common methods for aligning already-capable models with desired behavior rely\non the ability of humans to provide supervision. However, future superhuman\nmodels will surpass the capability of humans. Therefore, humans will only be\nable to weakly supervise superhuman models. This expected deficiency of human\nevaluation would weaken the safety of future AI systems. Scalable oversight and\nweak-to-strong generalization are two complementary approaches to tackle this\nissue. In this paper, we attempt to combine the strengths of these two\napproaches to further improve alignment. Specifically, we investigate ways of\nimproving human supervision with a strong pretrained model and then supervise\nthe strong model with enhanced weak human supervision. To make iterative\nempirical progress, we consider an analogy: can we use a strong model to\nimprove weak model supervision and then use it to supervise the strong model?\nWe empirically test it by finetuning a small weak model on ground truth labels\nwith the additional help from a large strong model, and then finetuning the\nstrong model on labels generated by the weak model. We find that debate can\nassist a weak model in extracting trustworthy information from an untrustworthy\nstrong model, which provides leverage as context on samples when training a\nweak model. We also show that an ensemble of weak models helps exploit long\narguments generated by strong model debaters and obtain a more robust\nsupervision estimate. Extensive experiments on the OpenAI weak-to-strong NLP\nbenchmarks show that the combination approach leads to better alignment, which\nindicates that debate has the potential to help weak-to-strong generalization.",
        "In this paper, we study necessary and sufficient conditions for a positive\nBorel measure $\\mu$ on the complex space $\\mathbb{C}$ to be a $(\\infty,q)$ or\n$(p,\\infty)$ (vanishing) Fock-Carleson measure through its Berezin transform.\nThen we discuss boundedness and compactness of the Toeplitz operator $T_{\\mu}$\nwith a positive Borel measure $\\mu$ as symbol on Fock space\n$F_{\\alpha}^{\\infty}$. Furthermore, we charaterize these properties of the\nToeplitz operator T_{\\varphi}$ with a symbol $\\varphi$ which is in $BMO$."
      ]
    }
  },
  {
    "id":2411.18253,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"Multimodal data fusion for cancer biomarker discovery with deep learning",
    "start_abstract":"Technological advances have made it possible to study a patient from multiple angles with high-dimensional, high-throughput multiscale biomedical data. In oncology, massive amounts of data are being generated, ranging from molecular, histopathology, radiology to clinical records. The introduction of deep learning has greatly advanced the analysis of biomedical data. However, most approaches focus on single data modalities, leading to slow progress in methods to integrate complementary data types. Development of effective multimodal fusion approaches is becoming increasingly important as a single modality might not be consistent and sufficient to capture the heterogeneity of complex diseases to tailor medical care and improve personalized medicine. Many initiatives now focus on integrating these disparate modalities to unravel the biological processes involved in multifactorial diseases such as cancer. However, many obstacles remain, including lack of usable data as well as methods for clinical validation and interpretation. Here, we cover these current challenges and reflect on opportunities through deep learning to tackle data sparsity and scarcity, multimodal interpretability and standardization of datasets.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Artificial intelligence for predictive biomarker discovery in immuno-oncology: a systematic review"
      ],
      "abstract":[
        "Background: The widespread use of immune checkpoint inhibitors (ICIs) has revolutionised treatment of multiple cancer types. However, selecting patients who may benefit from ICI remains challenging. Artificial intelligence (AI) approaches allow exploitation of high-dimension oncological data in research and development of precision immuno-oncology. Materials and methods: We conducted a systematic literature review of peer-reviewed original articles studying the ICI efficacy prediction in cancer patients across five data modalities: genomics (including genomics, transcriptomics, and epigenomics), radiomics, digital pathology (pathomics), and real-world and multimodality data. Results: A total of 90 studies were included in this systematic review, with 80% published in 2021-2022. Among them, 37 studies included genomic, 20 radiomic, 8 pathomic, 20 real-world, and 5 multimodal data. Standard machine learning (ML) methods were used in 72% of studies, deep learning (DL) methods in 22%, and both in 6%. The most frequently studied cancer type was non-small-cell lung cancer (36%), followed by melanoma (16%), while 25% included pan-cancer studies. No prospective study design incorporated AI-based methodologies from the outset; rather, all implemented AI as a post hoc analysis. Novel biomarkers for ICI in radiomics and pathomics were identified using AI approaches, and molecular biomarkers have expanded past genomics into transcriptomics and epigenomics. Finally, complex algorithms and new types of AI-based markers, such as meta-biomarkers, are emerging by integrating multimodal\/multi-omics data. Conclusion: AI-based methods have expanded the horizon for biomarker discovery, demonstrating the power of integrating multimodal data from existing datasets to discover new meta-biomarkers. While most of the included studies showed promise for AI-based prediction of benefit from immunotherapy, none provided high-level evidence for immediate practice change. A priori planned prospective trial designs are needed to cover all lifecycle steps of these software biomarkers, from development and validation to integration into clinical practice."
      ],
      "categories":[
        "Immunotherapy"
      ]
    },
    "list":{
      "title":[
        "One-dimensional linear analysis and numerical simulations of Alfven\n  waves in a force-free magnetosphere around a Kerr black hole",
        "MEMSDuino: An Arduino-Based MEMS Switch Controller",
        "On binomial edge ideals of corona of graphs",
        "A Frequency-Domain Opportunistic Approach for Spectral-Efficient\n  Cell-Free Massive MIMO",
        "On finitely many base $q$ expansions",
        "Electronic structures of crystalline and amorphous GeSe and GeSbTe\n  compounds using machine learning empirical pseudopotentials",
        "DINAMO: Dynamic and INterpretable Anomaly MOnitoring for Large-Scale\n  Particle Physics Experiments",
        "Parallactic delay for geodetic VLBI and non-orthogonality of the\n  fundamental axes",
        "A knot-theoretic tour of dimension four",
        "Optimization Methods for Joint Eigendecomposition",
        "Homogeneous analytic Hilbert modules -- the case of non-transitive\n  action",
        "Detecting Destabilizing Nonlinearities in Absolute Stability Analysis of\n  Discrete-Time Feedback Systems",
        "Stability of propagating terraces in spatially periodic multistable\n  equations in $\\mathbb{R}^N$",
        "Mean-Field Analysis of Latent Variable Process Models on Dynamically\n  Evolving Graphs with Feedback Effects",
        "Coalescence production of sexaquark with three diquarks in high-energy\n  nuclear collisions",
        "Uniqueness of the strong positive solution for a general quasilinear\n  elliptic problem with variable exponents and homogeneous Neumann boundary\n  conditions using a generalization of the $p(x)$-D\\'{i}az-Saa inequality",
        "Reproducing EPR correlations without superluminal signalling: backward\n  conditional probabilities and Statistical Independence",
        "Linear Bandits with Partially Observable Features",
        "Correlative and in situ microscopy investigation of phase\n  transformation, crystal growth and degradation of antimony sulfide thin films",
        "Four-dimensional QCD equation of state at finite chemical potentials",
        "Defining Determinism",
        "Lunar Laser Ranging with High-Power CW Lasers",
        "Parameter Expanded Stochastic Gradient Markov Chain Monte Carlo",
        "ST-FlowNet: An Efficient Spiking Neural Network for Event-Based Optical\n  Flow Estimation",
        "A nodally bound-preserving finite element method for hyperbolic\n  convection-reaction problems",
        "On the Set of Balanced Games",
        "Modular transport in two-dimensional conformal field theory",
        "Statistical mechanics of a cold tracer in a hot bath",
        "Probabilistic Assessment of West Nile Virus Spillover Risk Using a\n  Compartmental Mechanistic Model"
      ],
      "abstract":[
        "We perform one-dimensional linear analysis and numerical simulations of the\npropagation of Alfven waves in a force-free magnetosphere along magnetic field\nlines around a spinning black hole. We use the results to investigate the\ndynamic process of wave propagation and energy transport for Alfven waves. As\nin a previous study using the Banados--Teitelboim--Zanelli spacetime (Koide et\nal. 2022), the Alfven wave induces a fast magnetosonic wave in the case of a\nspinning black hole. Energy conservation is confirmed when this additional\ninduced magnetosonic wave is considered. We also observe the reflection of the\ninwardly propagating Alfven wave around the static limit, which is prohibited\nin theory when using the eikonal approximation.",
        "Radio frequency cryogenic switches are a critical enabling technology for\nquantum information science for both calibration and high throughput testing of\nsamples. Traditionally, solenoid-based switches have been used [1,2], but a\ntransition is being made to MEMS-based (Micro Electro Mechanical Systems)\nswitches due to their lower power dissipation and smaller size, and to minimize\nthe risk that solenoid switches tend to produce current pulses that destroy\nexpensive cryogenic amplifiers and can cause electrostatic damage to devices.\nThese MEMS switches require a 90-volt signal to be applied to the control lines\nto determine the state of the switches. Switches exist that have built-in\nCMOS-based (Complimentary Metal Oxide Semiconductor) control electronics to\ndrive the 90 V, but these do not work at the cryogenic temperatures used in\nquantum information science.\n  There is no currently available room temperature control system with direct\ncontrol of the switches. The instrument presented here is a 19-inch rack-mount\ncontroller for a cryogenic MEMS switch network that allows a human operator to\nsee the state of the switch via a row of clearly marked indicator lights and to\nchange the state manually via buttons on an LED-based indicator board or\nautomatically via Python-based serial port commands to the Arduino, an open\nsource microcontroller platform available from multiple vendors. The design can\nalso be modified to control other switches that require either a large voltage\nor current to switch.",
        "For a simple graph $G$, let $J_G$ denote the corresponding binomial edge\nideal. This article considers the binomial edge ideal of the corona product of\ntwo connected graphs $G$ and $H$. The corona product of $G$ and $H$, denoted by\n$G\\circ H$, is a construction where each vertex of $G$ is connected (via the\nconing-off) to an entire copy of $H$. This is a direct generalization of a cone\nconstruction. Previous studies have shown that for $J_{G \\circ H}$ to be\nCohen-Macaulay, both $G$ and $H$ must be complete graphs. However, there are no\ngeneral formulae for the dimension, depth, or Castelnuovo-Mumford regularity of\n$J_{G\\circ H}$ for all graphs $G$ and $H$. In this article, we provide a\ngeneral formula for the dimension, depth and Castelnuovo-Mumford regularity of\nthe binomial edge ideals of certain corona and corona-type (somewhat a\ngeneralization of corona) products of special interests. Additionally, we study\nthe Cohen-Macaulayness, unmixedness and related properties of binomial edge\nideals corresponding to above class of graphs. We have also added a short note\non the reduction of the Bolognini-Macchia-Strazzanti Conjecture to all graphs\nwith a diameter of $3$.",
        "Constrained by weak signal strength and significant inter-cell interference,\nusers located at the cell edge in a cellular network suffer from inferior\nservice quality. Recently, cell-free massive MIMO (CFmMIMO) has gained\nconsiderable attention due to its capability to offer uniform quality of\nservice, alleviating the cell-edge problem. In contrast to previous studies\nfocused on narrow-band CFmMIMO systems, this paper studies wideband CFmMIMO\ncommunications against channel frequency selectivity. By exploiting the\nfrequency-domain flexibility offered by orthogonal frequency-division\nmultiplexing (OFDM), and leveraging a particular spatial characteristic in the\ncell-free structure -- namely, the near-far effect among distributed access\npoints (APs) -- we propose an opportunistic approach to boost spectral\nefficiency. The core concept lies in opportunistically activating nearby APs\nfor certain users across their assigned OFDM subcarriers while deactivating\ndistant APs to prevent power wastage and lower inter-user interference.\nFurthermore, this approach enables the use of downlink pilots by reducing the\nnumber of active APs per subcarrier to a small subset, thereby substantially\nimproving downlink performance through coherent detection at the user receiver.\nVerified by numerical results, our proposed approach demonstrates considerable\nperformance improvement compared to the two benchmark approaches.",
        "Given some integer $m \\geq 3$, we find the first explicit collection of\ncountably many intervals in $(1,2)$ such that for any $q$ in one of these\nintervals, the set of points with exactly $m$ base $q$ expansions is nonempty\nand moreover has positive Hausdorff dimension. Our method relies on an\napplication of a theorem proved by Falconer and Yavicoli, which guarantees that\nthe intersection of a family of compact subsets of $\\mathbb{R}^d$ has positive\nHausdorff dimension under certain conditions.",
        "The newly developed machine learning (ML) empirical pseudopotential (EP)\nmethod overcomes the poor transferability of the traditional EP method with the\nhelp of ML techniques while preserving its formal simplicity and computational\nefficiency. We apply the new method to binary and ternary systems such as GeSe\nand Ge-Sb-Te (GST) compounds, well-known materials for non-volatile\nphase-change memory and related technologies. Using a training set of {\\it ab\ninitio} electronic energy bands and rotation-covariant descriptors for various\nGeSe and GST compounds, we generate transferable EPs for Ge, Se, Sb, and Te. We\ndemonstrate that the new ML model accurately reproduces the energy bands and\nwavefunctions of structures outside the training set, closely matching\nfirst-principles calculations. This accuracy is achieved with significantly\nlower computational costs due to the elimination of self-consistency iterations\nand the reduced size of the plane-wave basis set. Notably, the method maintains\naccuracy even for diverse local atomic environments, such as amorphous phases\nor larger systems not explicitly included in the training set.",
        "Ensuring reliable data collection in large-scale particle physics experiments\ndemands Data Quality Monitoring (DQM) procedures to detect possible detector\nmalfunctions and preserve data integrity. Traditionally, this\nresource-intensive task has been handled by human shifters that struggle with\nfrequent changes in operational conditions. We present novel, interpretable,\nrobust, and scalable DQM algorithms designed to automate anomaly detection in\ntime-dependent settings. Our approach constructs evolving histogram templates\nwith built-in uncertainties, featuring both a statistical variant - extending\nthe classical Exponentially Weighted Moving Average (EWMA) - and a machine\nlearning (ML)-enhanced version that leverages a transformer encoder for\nimproved adaptability. Experimental validations on synthetic datasets\ndemonstrate the high accuracy, adaptability, and interpretability of these\nmethods, with the statistical variant being commissioned in the LHCb experiment\nat the Large Hadron Collider, underscoring its real-world impact. The code used\nin this study is available at https:\/\/github.com\/ArseniiGav\/DINAMO.",
        "The Gaia optical astrometric mission has measured the precise positions of\nmillions of objects in the sky, including extragalactic sources also observed\nby Very Long Baseline Interferometry (VLBI). In the recent Gaia EDR3 release,\nan effect of negative parallax with a magnitude of approximately -17 $\\mu$as\nwas reported, presumably due to technical reasons related to the relativistic\ndelay model. A recent analysis of a 30-year set of geodetic VLBI data revealed\na similar negative parallax with an amplitude of $-15.8 \\pm 0.5$ $ \\mu$as.\nSince both astrometric techniques, optical and radio, provide consistent\nestimates of this negative parallax, it is necessary to investigate the\npotential origin of this effect. We developed the extended group relativistic\ndelay model to incorporate the additional parallactic effect for radio sources\nat distances less than 1 Mpc and found that the apparent annual signal might\nappear due the non-orthogonality of the fundamental axes, which are defined by\nthe positions of the reference radio sources themselves. Unlike the\nconventional parallactic ellipse, the apparent annual effect in this case\nappears as a circular motion for all objects independently of their ecliptic\nlatitude. The measured amplitude of this circular effect is within a range of\n10-15 $\\mu$as that is consistent with the ICRF3 stability of the fundamental\naxis. This annual circular effect could also arise if a G\\\"odel-type\ncosmological metric were applied, suggesting that, in the future, this\nphenomenon could be used to indicate global cosmic rotation.",
        "These notes follow a lecture series at the \"Singularities and low dimensional\ntopology\" winter school at the R\\'enyi Institute in January 2023, with a target\naudience of graduate students in singularity theory and low-dimensional\ntopology. The lectures discuss the basics of four-dimensional manifold\ntopology, connecting this rich subject to knot theory on one side and to\ncontact, symplectic, and complex geometry (through Stein surfaces) on the other\nside of the spectrum.",
        "Joint diagonalization, the process of finding a shared set of approximate\neigenvectors for a collection of matrices, arises in diverse applications such\nas multidimensional harmonic analysis or quantum information theory. This task\nis typically framed as an optimization problem: minimizing a non-convex\nfunction that quantifies off-diagonal matrix elements across possible bases. In\nthis work, we introduce a suite of efficient algorithms designed to locate\nlocal minimizers of this functional. Our methods leverage the Hessian's\nstructure to bypass direct computation of second-order derivatives, evaluating\nit as either an operator or bilinear form - a strategy that remains\ncomputationally feasible even for large-scale applications. Additionally, we\ndemonstrate that this Hessian-based information enables precise estimation of\nparameters, such as step-size, in first-order optimization techniques like\nGradient Descent and Conjugate Gradient, and the design of second-order methods\nsuch as (Quasi-)Newton. The resulting algorithms for joint diagonalization\noutperform existing techniques, and we provide comprehensive numerical evidence\nof their superior performance.",
        "This work investigates analytic Hilbert modules $\\mathcal{H}$, over the\npolynomial ring, consisting of holomorphic functions on a $G$-space $\\Omega\n\\subset \\mathbb{C}^m$ that are homogeneous under the natural action of the\ngroup $G$. In a departure from the past studies of such questions, here we\ndon't assume transitivity of the group action. The primary finding reveals that\nunitary invariants such as curvature and the reproducing kernel of a\nhomogeneous analytic Hilbert module can be deduced from their values on a\nfundamental set $\\Lambda$ of the group action. Next, utilizing these\ntechniques, we examine the analytic Hilbert modules associated with the\nsymmetrized bi-disc $\\mathbb{G}_2$ and its homogeneity under the automorphism\ngroup of $\\mathbb{G}_2$. It follows from one of our main theorems that none of\nthe weighted Bergman metrics on the symmetrized bi-disc is K\\\"{a}hler-Einstein.",
        "This paper is concerned with the absolute stability analysis of discrete-time\nfeedback systems with slope-restricted nonlinearities. By employing static\nO'Shea-Zames-Falb multipliers in the framework of integral quadratic\nconstraints, we can obtain a certificate for the absolute stability in the form\nof a linear matrix inequality (LMI). However, since this LMI certificate is\nonly a sufficient condition, we cannot draw any definite conclusion if the LMI\nturns out to be infeasible. To address this issue, we focus on the dual LMI\nthat is feasible if and only if the original (primal) LMI is infeasible. As the\nmain result, if the dual solution satisfies a certain rank condition, we prove\nthat we can detect a destabilizing nonlinearity within the assumed class of\nslope-restricted nonlinearities as well as a non-zero equilibrium point of the\nresulting feedback system, thereby we can conclude that the system of interest\nis never absolutely stable. The effectiveness of the technical results is\ndemonstrated through numerical examples.",
        "In this paper, we study the large time behaviour of solutions of multistable\nreaction-diffusion equations in $\\mathbb{R}^N$, with a spatially periodic\nheterogeneity. By multistable, we mean that the problem admits a finite -- but\narbitrarily large -- number of stable, periodic steady states. In contrast with\nthe more classical monostable and bistable frameworks, which exhibit the\nemergence of a single travelling front in the long run, in the present case the\nlarge time dynamics is governed by a family of stacked travelling fronts,\ninvolving intermediate steady states, called propagating terrace. Their\nexistence in the multidimensional case has been established in our previous\nwork [13]. The first result of the present paper is their uniqueness. Next, we\nshow that the speeds of the propagating terraces in different directions\ndictate the spreading speeds of solutions of the Cauchy problem, for both\nplanar-like and compactly supported initial data. The latter case turns out to\nbe much more intricate than the former, due to the fact that the propagating\nterraces in distinct directions may involve different sets of intermediate\nsteady states. Another source of difficulty is that the Wulff shape of the\nspeeds of travelling fronts can be non-smooth, as we show in the bistable case\nusing a result of [4].",
        "In this paper, we study the asymptotic behavior of a class of dynamic\nco-evolving latent space networks. The model we study is subject to\nbi-directional feedback effects, meaning that at any given time, the latent\nprocess depends on its own value and the graph structure at the previous time\nstep, and the graph structure at the current time depends on the value of the\nlatent processes at the current time but also on the graph structure at the\nprevious time instance (sometimes called a persistence effect). We construct\nthe mean-field limit of this model, which we use to characterize the limiting\nbehavior of a random sample taken from the latent space network in the limit as\nthe number of nodes in the network diverges. From this limiting model, we can\nderive the limiting behavior of the empirical measure of the latent process and\nestablish the related graphon limit of the latent particle network process. We\nalso provide a description of the rich conditional probabilistic structure of\nthe limiting model. The inherent dependence structure complicates the\nmathematical analysis significantly. In the process of proving our main\nresults, we derive a general conditional propagation of chaos result, which is\nof independent interest. In addition, our novel approach of studying the\nlimiting behavior of random samples proves to be a very useful methodology for\nfully grasping the asymptotic behavior of co-evolving particle systems.\nNumerical results are included to illustrate the theoretical findings.",
        "The coalescence production of sexaquark, a hypothetical stable state with\nquark content $(uuddss)$, is investigated by the parton and hadron cascade\nmodel PACIAE in $pp$ collisions at $\\sqrt s = 7$ TeV. In this work, the compact\nsexaquark bound state of three diquarks is formed in the final partonic state\nby a two-step approach, which involves ``diquark\" formation via partonic\ncoalescence and sexaquark construction with dynamically constrained phase-space\ncoalescence model successively. The yields, yield ratios, and dependences of\nspatial parameters (the size of diquark $D_{0}$ and the radius of sexaquark\n$R_{0}$) of (anti-)sexaquark are predicted. The yields of a hadronic molecule\nH-dibaryon $\\mathrm{H}(\\Lambda\\Lambda)$ generated in the final hadronic state\nare also compared. These estimates provide references for future sexaquark\nsearches and other exotic state studies, such as dibaryons.",
        "In this paper, we study a generalization of the D\\'iaz-Saa inequality and its\napplications to nonlinear elliptic problems. We first present the necessary\nhypotheses and preliminary results before introducing an improved version of\nthe inequality, which holds in a broader functional setting and allows\napplications to problems with homogeneous Neumann boundary conditions. The\nsignificance of cases where the inequality becomes an equality is also\nanalyzed, leading to uniqueness results for certain classes of partial\ndifferential equations. Furthermore, we provide a detailed proof of a\nuniqueness theorem for strong positive solutions and illustrate our findings\nwith two concrete applications: a multiple-phase problem and an elliptic\nquasilinear equation relevant to image processing. The paper concludes with\npossible directions for future research.",
        "Bell's theorem states that no model that respects Local Causality and\nStatistical Independence can account for the correlations predicted by quantum\nmechanics via entangled states. This paper proposes a new approach, using\nbackward-in-time conditional probabilities, which relaxes conventional\nassumptions of temporal ordering while preserving Statistical Independence as a\n\"fine-tuning condition. It is shown how such models can account for EPR\/Bell\ncorrelations and, analogously, the GHZ predictions while nevertheless\nforbidding superluminal signalling.",
        "We introduce a novel linear bandit problem with partially observable\nfeatures, resulting in partial reward information and spurious estimates.\nWithout proper address for latent part, regret possibly grows linearly in\ndecision horizon $T$, as their influence on rewards are unknown. To tackle\nthis, we propose a novel analysis to handle the latent features and an\nalgorithm that achieves sublinear regret. The core of our algorithm involves\n(i) augmenting basis vectors orthogonal to the observed feature space, and (ii)\nintroducing an efficient doubly robust estimator. Our approach achieves a\nregret bound of $\\tilde{O}(\\sqrt{(d + d_h)T})$, where $d$ is the dimension of\nobserved features, and $d_h$ is the unknown dimension of the subspace of the\nunobserved features. Notably, our algorithm requires no prior knowledge of the\nunobserved feature space, which may expand as more features become hidden.\nNumerical experiments confirm that our algorithm outperforms both\nnon-contextual multi-armed bandits and linear bandit algorithms depending\nsolely on observed features.",
        "Antimony sulfide (Sb$_2$S$_3$), a compound of earth-abundant elements with\nhighly anisotropic, quasi-layered crystal structure, triggered growing interest\nas a solar absorber in photovoltaics and as a phase change material in memory\ndevices, yet challenges remain in achieving high-quality thin films with\ncontrolled nucleation and growth for optimal performance. Here, we investigate\nthe phase transformation, crystal structure and properties, growth and\ndegradation of atomic layer deposited Sb$_2$S$_3$ thin films using in situ TEM\nand correlative ex situ analysis. The as-deposited amorphous films crystallized\nat 243{\\deg}C, forming grains with an [100] out-of-plane texture and developed\ninto tens to hundreds of micrometer, leaves-shaped grains. Introducing an\nultra-thin ZnS interfacial layer increased nucleation density, and resulted in\na few micrometer-sized, more uniform grains while retaining the overall [100]\ntexture. In situ observations and subsequent crystal orientation analysis with\ncutting-edge 4D-STEM and EBSD revealed that the grains grew faster along the\n[010] ribbon direction and that the bare films underwent early-stage\ndegradation, forming holes in amorphous regions during annealing. The ZnS\ninterlayer mitigated degradation, stabilizing the films and improving their\nuniformity. These findings offer valuable insights for optimizing Sb$_2$S$_3$\nthin films for applications both as solar cell materials and phase change\nmaterials.",
        "Exploration of the QCD phase diagram is pivotal in particle and nuclear\nphysics. We construct a full four-dimensional equation of state of QCD with net\nbaryon, electric charge, and strangeness by extending the NEOS model beyond the\nconventional two-dimensional approximation. Lattice QCD calculations based on\nthe Taylor expansion method and the hadron resonance gas model are considered\nfor the construction. We also develop an efficient numerical method for\napplying the four-dimensional equation of state to relativistic hydrodynamic\nsimulations, which can be used for the analysis of nuclear collisions at beam\nenergy scan energies and for different nuclear species at the BNL Relativistic\nHeavy Ion Collider.",
        "Determinism is the thesis that the past determines the future, but efforts to\ndefine it precisely have exposed deep methodological disagreements. Standard\npossible-worlds formulations of determinism presuppose an \"agreement\" relation\nbetween worlds, but this relation can be understood in multiple ways -- none of\nwhich is particularly clear. We critically examine the proliferation of\ndefinitions of determinism in the recent literature, arguing that these\ndefinitions fail to deliver clear verdicts about actual scientific theories. We\nadvocate a return to a formal approach, in the logical tradition of Carnap,\nthat treats determinism as a property of scientific theories, rather than an\nelusive metaphysical doctrine.\n  We highlight two key distinctions: (1) the difference between qualitative and\n\"full\" determinism, as emphasized in recent discussions of physics and\nmetaphysics, and (2) the distinction between weak and strong formal conditions\non the uniqueness of world extensions. We argue that defining determinism in\nterms of metaphysical notions such as haecceities is unhelpful, whereas\nrigorous formal criteria -- such as Belot's D1 and D3 -- offer a tractable and\nscientifically relevant account. By clarifying what it means for a theory to be\ndeterministic, we set the stage for a fruitful interaction between physics and\nmetaphysics.",
        "We present a high-power continuous-wave (CW) lunar laser ranging (LLR)\ntechnique that has the potential to significantly improve Earth--Moon distance\nmeasurements. Using a 1 kW CW laser at 1064 nm and a 1 m-aperture telescope as\nan example, we develop a detailed link budget and analyze the prevailing noise\nsources to assess system performance when ranging to next-generation ~10 cm\ncorner-cube retroreflectors (CCRs). Unlike legacy arrays, these smaller CCRs\nare designed to yield lower intrinsic range errors, yet their reduced\nreflective area results in lower photon return rates, posing challenges for\npulsed LLR systems. The photon-rich CW approach, by providing continuous\nhigh-power illumination, overcomes this limitation, reducing shot noise and\nenabling sustained millimeter-level ranging with a pathway to sub-0.1 mm\nprecision. Furthermore, by alternating measurements between widely separated\nlunar reflectors, differential LLR mitigates common-mode station errors to\nachieve tens-of-micrometer precision, limited primarily by uncorrelated\natmospheric turbulence. This scalable approach -- integrating high-power CW\nlasers, narrowband filtering, and rapid atmospheric turbulence averaging --\nenables next-generation gravitational tests, precision lunar geodesy, and\nimproved lunar reference frames in support of planetary exploration.",
        "Bayesian Neural Networks (BNNs) provide a promising framework for modeling\npredictive uncertainty and enhancing out-of-distribution robustness (OOD) by\nestimating the posterior distribution of network parameters. Stochastic\nGradient Markov Chain Monte Carlo (SGMCMC) is one of the most powerful methods\nfor scalable posterior sampling in BNNs, achieving efficiency by combining\nstochastic gradient descent with second-order Langevin dynamics. However,\nSGMCMC often suffers from limited sample diversity in practice, which affects\nuncertainty estimation and model performance. We propose a simple yet effective\napproach to enhance sample diversity in SGMCMC without the need for tempering\nor running multiple chains. Our approach reparameterizes the neural network by\ndecomposing each of its weight matrices into a product of matrices, resulting\nin a sampling trajectory that better explores the target parameter space. This\napproach produces a more diverse set of samples, allowing faster mixing within\nthe same computational budget. Notably, our sampler achieves these improvements\nwithout increasing the inference cost compared to the standard SGMCMC.\nExtensive experiments on image classification tasks, including OOD robustness,\ndiversity, loss surface analyses, and a comparative study with Hamiltonian\nMonte Carlo, demonstrate the superiority of the proposed approach.",
        "Spiking Neural Networks (SNNs) have emerged as a promising tool for\nevent-based optical flow estimation tasks due to their ability to leverage\nspatio-temporal information and low-power capabilities. However, the\nperformance of SNN models is often constrained, limiting their application in\nreal-world scenarios. In this work, we address this gap by proposing a novel\nneural network architecture, ST-FlowNet, specifically tailored for optical flow\nestimation from event-based data. The ST-FlowNet architecture integrates\nConvGRU modules to facilitate cross-modal feature augmentation and temporal\nalignment of the predicted optical flow, improving the network's ability to\ncapture complex motion dynamics. Additionally, to overcome the challenges\nassociated with training SNNs, we introduce a novel approach to derive SNN\nmodels from pre-trained artificial neural networks (ANNs) through ANN-to-SNN\nconversion or our proposed BISNN method. Notably, the BISNN method alleviates\nthe complexities involved in biological parameter selection, further enhancing\nthe robustness of SNNs in optical flow estimation tasks. Extensive evaluations\non three benchmark event-based datasets demonstrate that the SNN-based\nST-FlowNet model outperforms state-of-the-art methods, delivering superior\nperformance in accurate optical flow estimation across a diverse range of\ndynamic visual scenes. Furthermore, the inherent energy efficiency of SNN\nmodels is highlighted, establishing a compelling advantage for their practical\ndeployment. Overall, our work presents a novel framework for optical flow\nestimation using SNNs and event-based data, contributing to the advancement of\nneuromorphic vision applications.",
        "In this article, we present a numerical approach to ensure the preservation\nof physical bounds on the solutions to linear and nonlinear hyperbolic\nconvection-reaction problems at the discrete level. We provide a rigorous\nframework for error analysis, formulating the discrete problem as a variational\ninequality and demonstrate optimal convergence rates in a natural norm. We\nsummarise extensive numerical experiments validating the effectiveness of the\nproposed methods in preserving physical bounds and preventing unphysical\noscillations, even in challenging scenarios involving highly nonlinear reaction\nterms.",
        "We study the geometric structure of the set of cooperative transferable\nutility games having a nonempty core, characterized by Bondareva and Shapley as\nbalanced games. We show that this set is a nonpointed polyhedral cone, and we\nfind the set of its extremal rays and facets. This study is also done for the\nset of balanced games whose value for the grand coalition is fixed, which\nyields an affine nonpointed polyhedral cone. Finally, the case of nonnegative\nbalanced games with fixed value for the grand coalition is tackled. This set is\na convex polytope, with remarkable properties. We characterize its vertices and\nfacets, study the adjacency structure of vertices, develop an algorithm for\ngenerating vertices in a random uniform way, and show that this polytope is\ncombinatorial and its adjacency graph is Hamiltonian. Last, we give a\ncharacterization of the set of games having a core reduced to a singleton.\nFunding: This work was supported by the Spanish Government [Grant\nPID2021-124933NB-I00].",
        "We study the quantum transport generated by the bipartite entanglement in\ntwo-dimensional conformal field theory at finite density with the $U(1) \\times\nU(1)$ symmetry associated to the conservation of the electric charge and of the\nhelicity. The bipartition given by an interval is considered, either on the\nline or on the circle. The continuity equations and the corresponding conserved\nquantities for the modular flows of the currents and of the energy-momentum\ntensor are derived. We investigate the mean values of the associated currents\nand their quantum fluctuations in the finite density representation, which\ndescribe the properties of the modular quantum transport. The modular analogues\nof the Johnson-Nyquist law and of the fluctuation-dissipation relation are\nfound, which encode the thermal nature of the modular evolution.",
        "We study the dynamics of a zero-temperature particle interacting linearly\nwith a bath of hot Brownian particles. Starting with the most general model of\na linearly-coupled bath, we eliminate the bath degrees of freedom exactly to\nmap the tracer dynamics onto a generalized Langevin equation, allowing for an\narbitrary external potential on the tracer. We apply this result to determine\nthe fate of a tracer connected by springs to $N$ identical bath particles or\ninserted within a harmonic chain of hot particles. In the former\n\"fully-connected\" case, we find the tracer to transition between an effective\nequilibrium regime at large $N$ and an FDT-violating regime at finite $N$,\nwhile in the latter \"loop\" model the tracer never satisfies an FDT. We then\nstudy the fully-connected model perturbatively for large but finite $N$,\ndemonstrating signatures of irreversibility such as ratchet currents,\nnon-Boltzmann statistics, and positive entropy production. Finally, we\nspecialize to harmonic external potentials on the tracer, allowing us to\nexactly solve the dynamics of both the tracer and the bath for an arbitrary\nlinear model. We apply our findings to show that a cold tracer in a hot lattice\nsuppresses the fluctuations of the lattice in a long-ranged manner, and we\ngeneralize this result to linear elastic field theories.",
        "This paper presents a novel probabilistic approach for assessing the risk of\nWest Nile Disease (WND) spillover to the human population. The assessment has\nbeen conducted under two different scenarios: (1) assessment of the onset of\nspillover, and (2) assessment of the severity of the epidemic after the onset\nof the disease. A compartmental model of differential equations is developed to\ndescribe the disease transmission mechanism, and a probability density function\nfor pathogen spillover to humans is derived based on the model for the\nassessment of the risk of the spillover onset and the severity of the epidemic.\nThe prediction strategy involves making a long-term forecast and then updating\nit with a short-term (lead time of two weeks or daily). The methodology is\ndemonstrated using detailed outbreak data from high-case counties in\nCalifornia, including Orange County, Los Angeles County, and Kern County. The\npredicted results are compared with actual infection dates reported by the\nCalifornia Department of Public Health for 2022-2024 to assess prediction\naccuracy. The performance accuracy is evaluated using a logarithmic scoring\nsystem and compared with one of the most renowned predictive models to assess\nits effectiveness. In all prediction scenarios, the model demonstrated strong\nperformance. Lastly, the method is applied to explore the impact of global\nwarming on spillover risk, revealing an increasing trend in the number of\nhigh-risk days and a shift toward a greater proportion of these days over time\nfor the onset of the disease."
      ]
    }
  },
  {
    "id":2411.18253,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"Artificial intelligence for predictive biomarker discovery in immuno-oncology: a systematic review",
    "start_abstract":"Background: The widespread use of immune checkpoint inhibitors (ICIs) has revolutionised treatment of multiple cancer types. However, selecting patients who may benefit from ICI remains challenging. Artificial intelligence (AI) approaches allow exploitation of high-dimension oncological data in research and development of precision immuno-oncology. Materials and methods: We conducted a systematic literature review of peer-reviewed original articles studying the ICI efficacy prediction in cancer patients across five data modalities: genomics (including genomics, transcriptomics, and epigenomics), radiomics, digital pathology (pathomics), and real-world and multimodality data. Results: A total of 90 studies were included in this systematic review, with 80% published in 2021-2022. Among them, 37 studies included genomic, 20 radiomic, 8 pathomic, 20 real-world, and 5 multimodal data. Standard machine learning (ML) methods were used in 72% of studies, deep learning (DL) methods in 22%, and both in 6%. The most frequently studied cancer type was non-small-cell lung cancer (36%), followed by melanoma (16%), while 25% included pan-cancer studies. No prospective study design incorporated AI-based methodologies from the outset; rather, all implemented AI as a post hoc analysis. Novel biomarkers for ICI in radiomics and pathomics were identified using AI approaches, and molecular biomarkers have expanded past genomics into transcriptomics and epigenomics. Finally, complex algorithms and new types of AI-based markers, such as meta-biomarkers, are emerging by integrating multimodal\/multi-omics data. Conclusion: AI-based methods have expanded the horizon for biomarker discovery, demonstrating the power of integrating multimodal data from existing datasets to discover new meta-biomarkers. While most of the included studies showed promise for AI-based prediction of benefit from immunotherapy, none provided high-level evidence for immediate practice change. A priori planned prospective trial designs are needed to cover all lifecycle steps of these software biomarkers, from development and validation to integration into clinical practice.",
    "start_categories":[
      "Immunotherapy"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "Multimodal data fusion for cancer biomarker discovery with deep learning"
      ],
      "abstract":[
        "Technological advances have made it possible to study a patient from multiple angles with high-dimensional, high-throughput multiscale biomedical data. In oncology, massive amounts of data are being generated, ranging from molecular, histopathology, radiology to clinical records. The introduction of deep learning has greatly advanced the analysis of biomedical data. However, most approaches focus on single data modalities, leading to slow progress in methods to integrate complementary data types. Development of effective multimodal fusion approaches is becoming increasingly important as a single modality might not be consistent and sufficient to capture the heterogeneity of complex diseases to tailor medical care and improve personalized medicine. Many initiatives now focus on integrating these disparate modalities to unravel the biological processes involved in multifactorial diseases such as cancer. However, many obstacles remain, including lack of usable data as well as methods for clinical validation and interpretation. Here, we cover these current challenges and reflect on opportunities through deep learning to tackle data sparsity and scarcity, multimodal interpretability and standardization of datasets."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "A New Era of Elections: Leveraging Blockchain for Fair and Transparent\n  Voting",
        "Twists of representations of complex reflection groups and rational\n  Cherednik algebras",
        "Generating Physically Realistic and Directable Human Motions from\n  Multi-Modal Inputs",
        "The GigaMIDI Dataset with Features for Expressive Music Performance\n  Detection",
        "On alternating-conjugate splitting methods",
        "Cluster algebras and skein algebras for surfaces",
        "Spring-mass behavior of solitons under the influence of an external\n  force field within the modified Korteweg-de Vries equation",
        "PanoGen++: Domain-Adapted Text-Guided Panoramic Environment Generation\n  for Vision-and-Language Navigation",
        "Subcodes of Second-Order Reed-Muller Codes via Recursive Subproducts",
        "Neural ODE Transformers: Analyzing Internal Dynamics and Adaptive\n  Fine-tuning",
        "Robust Multi-Objective Preference Alignment with Online DPO",
        "The Striking Impact of Natural Hazard Risk on Global Green Hydrogen Cost",
        "Tunable magnon band topology and magnon orbital Nernst effect in\n  noncollinear antiferromagnets",
        "Automation of Electroweak Corrections",
        "Comparing strange and non-strange quark stars within resummed QCD at NLO",
        "Movable Antenna Aided Multiuser Communications: Antenna Position\n  Optimization Based on Statistical Channel Information",
        "ShapeShift: Towards Text-to-Shape Arrangement Synthesis with\n  Content-Aware Geometric Constraints",
        "A machine-learning study of phase transitions in Ising, Blume-Capel, and\n  Ising-metamagnet models",
        "GS-DiT: Advancing Video Generation with Pseudo 4D Gaussian Fields\n  through Efficient Dense 3D Point Tracking",
        "Study of SARS-CoV-2 Spike Protein by Surface Enhanced Raman Spectroscopy\n  and Transmission Electron Microscopy",
        "A Review of Brain-Computer Interface Technologies: Signal Acquisition\n  Methods and Interaction Paradigms",
        "Brain Effective Connectivity Estimation via Fourier Spatiotemporal\n  Attention",
        "MechIR: A Mechanistic Interpretability Framework for Information\n  Retrieval",
        "Irrationality of the Length Spectrum",
        "On the eigenvalues and Fu\\v{c}\\'{\\i}k spectrum of $p$-Laplace local and\n  nonlocal operator with mixed interpolated Hardy term",
        "Correlated flat-band physics in a bilayer kagome metal based on compact\n  molecular orbitals",
        "Continuous Approach to Phase (Norm) Retrieval Frames",
        "Gradient estimates for the fractional $p$-Poisson equation",
        "Pump-intensity-scaling of Two-Photon-Absorption and Photon Statistics of\n  Entangled-Photon Fields"
      ],
      "abstract":[
        "This study presents a blockchain-based voting system aimed at enhancing\nelection security, transparency, and integrity. Traditional voting methods face\ngrowing risks of tampering, making it crucial to explore innovative solutions.\nOur proposed system combines blockchain's immutable, decentralized ledger with\nadvanced voter identity verification techniques, including digital identity\nvalidation through Aadhaar and Driving Licenses (secured via BLAKE2b-512\nhashing), biometric fingerprint authentication, and a picture rotation pattern\nfor added security. Votes are recorded transparently and securely on a\nblockchain, with a consensus mechanism ensuring data integrity and reducing the\nrisk of unauthorized alterations. Security analysis indicates that this\nmulti-layered approach significantly reduces impersonation risks, while\nblockchain ensures accurate, private, and tamper-resistant vote recording. The\nfindings support that a blockchain-based voting system with robust identity\nchecks offers a trustworthy alternative to traditional methods, with potential\nfor even greater refinement in secure and transparent elections.",
        "Drinfeld twists, and the twists of Giaquinto and Zhang, allow for algebras\nand their modules to be deformed by a cocycle. We prove general results about\ncocycle twists of algebra factorisations and induced representations and apply\nthem to reflection groups and rational Cherednik algebras. In particular, we\ndescribe how a twist acts on characters of Coxeter groups of type $B_n$ and\n$D_n$ and relate them to characters of mystic reflection groups. This is used\nto characterise twists of standard modules of rational Cherednik algebras as\nstandard modules for certain braided Cherednik algebras. We introduce the\ncoinvariant algebra of a mystic reflection group and use a twist to show that\nan analogue of Chevalley's theorem holds for these noncommutative algebras. We\nalso discuss several cases where the negative braided Cherednik algebras are,\nand are not, isomorphic to rational Cherednik algebras.",
        "This work focuses on generating realistic, physically-based human behaviors\nfrom multi-modal inputs, which may only partially specify the desired motion.\nFor example, the input may come from a VR controller providing arm motion and\nbody velocity, partial key-point animation, computer vision applied to videos,\nor even higher-level motion goals. This requires a versatile low-level humanoid\ncontroller that can handle such sparse, under-specified guidance, seamlessly\nswitch between skills, and recover from failures. Current approaches for\nlearning humanoid controllers from demonstration data capture some of these\ncharacteristics, but none achieve them all. To this end, we introduce the\nMasked Humanoid Controller (MHC), a novel approach that applies multi-objective\nimitation learning on augmented and selectively masked motion demonstrations.\nThe training methodology results in an MHC that exhibits the key capabilities\nof catch-up to out-of-sync input commands, combining elements from multiple\nmotion sequences, and completing unspecified parts of motions from sparse\nmultimodal input. We demonstrate these key capabilities for an MHC learned over\na dataset of 87 diverse skills and showcase different multi-modal use cases,\nincluding integration with planning frameworks to highlight MHC's ability to\nsolve new user-defined tasks without any finetuning.",
        "The Musical Instrument Digital Interface (MIDI), introduced in 1983,\nrevolutionized music production by allowing computers and instruments to\ncommunicate efficiently. MIDI files encode musical instructions compactly,\nfacilitating convenient music sharing. They benefit Music Information Retrieval\n(MIR), aiding in research on music understanding, computational musicology, and\ngenerative music. The GigaMIDI dataset contains over 1.4 million unique MIDI\nfiles, encompassing 1.8 billion MIDI note events and over 5.3 million MIDI\ntracks. GigaMIDI is currently the largest collection of symbolic music in MIDI\nformat available for research purposes under fair dealing. Distinguishing\nbetween non-expressive and expressive MIDI tracks is challenging, as MIDI files\ndo not inherently make this distinction. To address this issue, we introduce a\nset of innovative heuristics for detecting expressive music performance. These\ninclude the Distinctive Note Velocity Ratio (DNVR) heuristic, which analyzes\nMIDI note velocity; the Distinctive Note Onset Deviation Ratio (DNODR)\nheuristic, which examines deviations in note onset times; and the Note Onset\nMedian Metric Level (NOMML) heuristic, which evaluates onset positions relative\nto metric levels. Our evaluation demonstrates these heuristics effectively\ndifferentiate between non-expressive and expressive MIDI tracks. Furthermore,\nafter evaluation, we create the most substantial expressive MIDI dataset,\nemploying our heuristic, NOMML. This curated iteration of GigaMIDI encompasses\nexpressively-performed instrument tracks detected by NOMML, containing all\nGeneral MIDI instruments, constituting 31% of the GigaMIDI dataset, totalling\n1,655,649 tracks.",
        "The new class of alternating-conjugate splitting methods is presented and\nanalyzed. They are obtained by concatenating a given composition involving\ncomplex coefficients with the same composition but with the complex conjugate\ncoefficients. We show that schemes of this type exhibit a good long time\nbehavior when applied to linear unitary and linear Hamiltonian systems, in\ncontrast with other methods based on complex coefficients, and study in detail\ntheir preservation properties. We also present new schemes within this class up\nto order 6 that exhibit a better efficiency than state-of-the-art splitting\nmethods with real coefficients for some classes of problems.",
        "We consider two algebras of curves associated to an oriented surface of\nfinite type - the cluster algebra from combinatorial algebra, and the skein\nalgebra from quantum topology. We focus on generalizations of cluster algebras\nand generalizations of skein algebras that include arcs whose endpoints are\nmarked points on the boundary or in the interior of the surface. We show that\nthe generalizations are closely related by maps that can be explicitly defined,\nand we explore the structural implications, including (non-)finite generation.\nWe also discuss open questions about the algebraic structure of the algebras.",
        "We investigate the interaction of solitons with an external periodic field\nwithin the framework of the modified Korteweg-de Vries (mKdV) equation. In the\ncase of small perturbation a simple dynamical system is used to describe the\nsoliton behaviour. Equilibrium points of this dynamical system are computed\nwhen the external force travels at a constant speed. Assuming that the external\nforce moves with sinusoidal speed, we demonstrate that the soliton behavior is\nqualitatively similar to the constant-speed case. Besides, a resonant frequency\nis derived from the asymptotic theory without using the classical broad force\napproximation. The results obtained from the dynamical system are compared with\nfully direct numerical simulations, which reveal that the soliton solution\nexhibits spiral-like behavior in the soliton amplitude versus soliton phase\nspace. Moreover, when the external force oscillates at the resonant frequency,\nthe trajectories in the soliton phase versus soliton amplitude exhibit chaotic\nbehavior.",
        "Vision-and-language navigation (VLN) tasks require agents to navigate\nthree-dimensional environments guided by natural language instructions,\noffering substantial potential for diverse applications. However, the scarcity\nof training data impedes progress in this field. This paper introduces\nPanoGen++, a novel framework that addresses this limitation by generating\nvaried and pertinent panoramic environments for VLN tasks. PanoGen++\nincorporates pre-trained diffusion models with domain-specific fine-tuning,\nemploying parameter-efficient techniques such as low-rank adaptation to\nminimize computational costs. We investigate two settings for environment\ngeneration: masked image inpainting and recursive image outpainting. The former\nmaximizes novel environment creation by inpainting masked regions based on\ntextual descriptions, while the latter facilitates agents' learning of spatial\nrelationships within panoramas. Empirical evaluations on room-to-room (R2R),\nroom-for-room (R4R), and cooperative vision-and-dialog navigation (CVDN)\ndatasets reveal significant performance enhancements: a 2.44% increase in\nsuccess rate on the R2R test leaderboard, a 0.63% improvement on the R4R\nvalidation unseen set, and a 0.75-meter enhancement in goal progress on the\nCVDN validation unseen set. PanoGen++ augments the diversity and relevance of\ntraining environments, resulting in improved generalization and efficacy in VLN\ntasks.",
        "We use a simple construction called `recursive subproducts' (that is known to\nyield good codes of lengths $n^m$, $n \\geq 3$) to identify a family of codes\nsandwiched between first-order and second-order Reed-Muller (RM) codes. These\ncodes are subcodes of multidimensional product codes that use first-order RM\ncodes as components. We identify the minimum weight codewords of all the codes\nin this family, and numerically determine the weight distribution of some of\nthem. While these codes have the same minimum distance and a smaller rate than\nsecond-order RM codes, they have significantly fewer minimum weight codewords.\nFurther, these codes can be decoded via modifications to known RM decoders\nwhich yield codeword error rates within 0.25 dB of second-order RM codes and\nbetter than CRC-aided Polar codes (in terms of $E_b\/N_o$ for lengths $256, 512,\n1024$), thereby offering rate adaptation options for RM codes in low-capacity\nscenarios.",
        "Recent advancements in large language models (LLMs) based on transformer\narchitectures have sparked significant interest in understanding their inner\nworkings. In this paper, we introduce a novel approach to modeling transformer\narchitectures using highly flexible non-autonomous neural ordinary differential\nequations (ODEs). Our proposed model parameterizes all weights of attention and\nfeed-forward blocks through neural networks, expressing these weights as\nfunctions of a continuous layer index. Through spectral analysis of the model's\ndynamics, we uncover an increase in eigenvalue magnitude that challenges the\nweight-sharing assumption prevalent in existing theoretical studies. We also\nleverage the Lyapunov exponent to examine token-level sensitivity, enhancing\nmodel interpretability. Our neural ODE transformer demonstrates performance\ncomparable to or better than vanilla transformers across various configurations\nand datasets, while offering flexible fine-tuning capabilities that can adapt\nto different architectural constraints.",
        "Multi-objective preference alignment of large language models (LLMs) is\ncritical for developing AI systems that are more configurable, personalizable,\nhelpful, and safe. However, optimizing model outputs to satisfy diverse\nobjectives with variable weights at inference time for truly personalized\nmodels presents a significant challenge. Existing approaches are either\ncomputationally expensive to train or do not sufficiently steer model\nbehaviors. This paper introduces the Multi-Objective Online DPO (MO-ODPO)\nalgorithm, designed to robustly and efficiently align model behaviors with\nmultiple, potentially conflicting human preferences. Our approach incorporates\na prompt conditioning mechanism, allowing us to train a single\npreference-conditional policy, that can adapt to new preference combinations at\ninference. Experiments on two popular benchmarks show that MO-ODPO\nPareto-dominates existing baselines while providing excellent inference-time\nsteerability between diverse objectives.",
        "Due to climate change, natural hazards that affect energy infrastructure will\nbecome more frequent in the future. However, to incorporate natural hazard risk\ninto infrastructure investment decisions, we develop an approach to translate\nthis risk into discount rates. Thus, our newly developed discount rate approach\nincorporates both economic risk and natural hazard risk. To illustrate the\nimpact of including the risk of natural hazards, we apply country-specific\ndiscount rates for hydrogen production costs. The country-specific relative\ndifference in hydrogen generation cost ranges from a 96% surplus in the\nPhilippines to a -63% cost reduction in Kyrgyzstan compared to a discount rate\nthat only consists of economic risks. The inclusion of natural hazard risk\nchanges the cost ranking of technologies as outcome of energy system models and\nthus policy recommendations. The derived discount rates for 254 countries\nworldwide are published in this publication for further use.",
        "We theoretically investigate the intrinsic magnon orbital Nernst effect (ONE)\nin noncollinear antiferromagnets with Kagom\\'e spin systems. Our analysis\nreveals that an externally applied magnetic field induces topological phase\ntransitions in the magnonic system, characterized by the closing and reopening\nof the band gap between distinct magnon bands. These transitions enable tunable\ncontrol of the magnon orbital Nernst effect with applied magnetic field, with a\npronounced enhancement in magnon orbital Nernst conductivity near the phase\ntransition points. This tunability presents a promising direction for\nexperimental detection of the magnon ONE.",
        "This dissertation addresses a topic that I have worked on over the past\ndecade: the automation of next-to-leading order electroweak corrections in the\nStandard Model of particle physics. After introducing the basic concepts and\ntechniques of next-to-leading order QCD calculations that underpin the\nMadGraph5_aMC@NLO framework, I present a few key features relevant to the\nautomated next-to-leading order electroweak contributions to short-distance\ncross sections, with an emphasis on the mixed QCD and electroweak coupling\nexpansions. These include the FKS subtraction, the renormalization and\nelectroweak input parameter schemes, and the complex mass scheme for dealing\nwith unstable particles. Issues related to the initial or final photons and\nleptons are also discussed. Two remaining challenges are highlighted if one\nwishes to go beyond next-to-leading order computations. Some phenomenological\napplications at the LHC are given to demonstrate the relevance of electroweak\ncorrections at colliders. Finally, an outlook on future studies concludes the\ndissertation.",
        "We employ the renormalization group optimized perturbation theory (RGOPT)\nresummation method to evaluate the equation of state (EoS) for strange\n($N_f=2+1$) and non-strange ($N_f=2$) cold quark matter at NLO. This allows us\nto obtain the mass-radius relation for pure quark stars and compare the results\nwith the predictions from perturbative QCD (pQCD) at NNLO. Choosing the\nrenormalization scale to generate maximum star masses of order $M=2 - 2.6\nM_\\odot$, we show that the RGOPT can produce mass-radius curves compatible with\nthe masses and radii of some recently observed pulsars, regardless of their\nstrangeness content. The scale values required to produce the desired maximum\nmasses are higher in the strange scenario since the EoS is softer in this case.\nThe possible reasons for such behavior are discussed. Our results also show\nthat, as expected, the RGOPT predictions for the relevant observables are less\nsensitive to scale variations than those furnished by pQCD.",
        "The movable antenna (MA) technology has attracted great attention recently\ndue to its promising capability in improving wireless channel conditions by\nflexibly adjusting antenna positions. To reap maximal performance gains of MA\nsystems, existing works mainly focus on MA position optimization to cater to\nthe instantaneous channel state information (CSI). However, the resulting\nreal-time antenna movement may face challenges in practical implementation due\nto the additional time overhead and energy consumption required, especially in\nfast time-varying channel scenarios. To address this issue, we propose in this\npaper a new approach to optimize the MA positions based on the users'\nstatistical CSI over a large timescale. In particular, we propose a general\nfield response based statistical channel model to characterize the random\nchannel variations caused by the local movement of users. Based on this model,\na two-timescale optimization problem is formulated to maximize the ergodic sum\nrate of multiple users, where the precoding matrix and the positions of MAs at\nthe base station (BS) are optimized based on the instantaneous and statistical\nCSI, respectively. To solve this non-convex optimization problem, a log-barrier\npenalized gradient ascent algorithm is developed to optimize the MA positions,\nwhere two methods are proposed to approximate the ergodic sum rate and its\ngradients with different complexities. Finally, we present simulation results\nto evaluate the performance of the proposed design and algorithms based on\npractical channels generated by ray-tracing. The results verify the performance\nadvantages of MA systems compared to their fixed-position antenna (FPA)\ncounterparts in terms of long-term rate improvement, especially for scenarios\nwith more diverse channel power distributions in the angular domain.",
        "While diffusion-based models excel at generating photorealistic images from\ntext, a more nuanced challenge emerges when constrained to using only a fixed\nset of rigid shapes, akin to solving tangram puzzles or arranging real-world\nobjects to match semantic descriptions. We formalize this problem as\nshape-based image generation, a new text-guided image-to-image translation task\nthat requires rearranging the input set of rigid shapes into non-overlapping\nconfigurations and visually communicating the target concept. Unlike\npixel-manipulation approaches, our method, ShapeShift, explicitly parameterizes\neach shape within a differentiable vector graphics pipeline, iteratively\noptimizing placement and orientation through score distillation sampling from\npretrained diffusion models. To preserve arrangement clarity, we introduce a\ncontent-aware collision resolution mechanism that applies minimal semantically\ncoherent adjustments when overlaps occur, ensuring smooth convergence toward\nphysically valid configurations. By bridging diffusion-based semantic guidance\nwith explicit geometric constraints, our approach yields interpretable\ncompositions where spatial relationships clearly embody the textual prompt.\nExtensive experiments demonstrate compelling results across diverse scenarios,\nwith quantitative and qualitative advantages over alternative techniques.",
        "We combine machine-learning (ML) techniques with Monte Carlo (MC) simulations\nand finite-size scaling (FSS) to study continuous and first-order phase\ntransitions in Ising, Blume-Capel, and Ising-metamagnet spin models. We go\nbeyond earlier studies that had concentrated on obtaining the\ncorrelation-length exponent $\\nu$. In particular, we show (a) how to combine\nneural networks (NNs), trained with data from MC simulations of Ising-type spin\nmodels on finite lattices, with FSS to obtain both thermal magnetic exponents\n$y_t = 1\/\\nu$ and $y_h$, respectively, at both critical and tricritical points,\n(b) how to obtain the NN counterpart of two-scale-factor universality at an\nIsing-type critical point, and (c) FSS at a first-order transition. We also\nobtain the FSS forms for the output of our trained NNs as functions of both the\ntemperature and the magnetic field.",
        "4D video control is essential in video generation as it enables the use of\nsophisticated lens techniques, such as multi-camera shooting and dolly zoom,\nwhich are currently unsupported by existing methods. Training a video Diffusion\nTransformer (DiT) directly to control 4D content requires expensive multi-view\nvideos. Inspired by Monocular Dynamic novel View Synthesis (MDVS) that\noptimizes a 4D representation and renders videos according to different 4D\nelements, such as camera pose and object motion editing, we bring pseudo 4D\nGaussian fields to video generation. Specifically, we propose a novel framework\nthat constructs a pseudo 4D Gaussian field with dense 3D point tracking and\nrenders the Gaussian field for all video frames. Then we finetune a pretrained\nDiT to generate videos following the guidance of the rendered video, dubbed as\nGS-DiT. To boost the training of the GS-DiT, we also propose an efficient Dense\n3D Point Tracking (D3D-PT) method for the pseudo 4D Gaussian field\nconstruction. Our D3D-PT outperforms SpatialTracker, the state-of-the-art\nsparse 3D point tracking method, in accuracy and accelerates the inference\nspeed by two orders of magnitude. During the inference stage, GS-DiT can\ngenerate videos with the same dynamic content while adhering to different\ncamera parameters, addressing a significant limitation of current video\ngeneration models. GS-DiT demonstrates strong generalization capabilities and\nextends the 4D controllability of Gaussian splatting to video generation beyond\njust camera poses. It supports advanced cinematic effects through the\nmanipulation of the Gaussian field and camera intrinsics, making it a powerful\ntool for creative video production. Demos are available at\nhttps:\/\/wkbian.github.io\/Projects\/GS-DiT\/.",
        "The spike protein (SP) of SARS-CoV-2 is the major molecular target for making\ndiagnostic tests, vaccines, and therapeutic development. We used a combination\nof transmission electron microscopy (TEM) and surface enhanced Raman microscopy\n(SERS) to study its structure. Using SERS on an aluminum substrate, we were\nable to detect a characteristic spectrum of SP mostly due to vibration of three\naromatic amino acids producing Raman shifts at 466 cm-1, 524 cm-1, 773 cm-1,\n831 cm-1, 1048 cm-1, 1308 cm-1, 1457 cm-1, and 1610 cm-1. Transmission Electron\nMicroscopy (TEM) of the SP showed periodic 2D-lattice orientation. The findings\nfrom this study have translational values for developing surface-enhanced Raman\nspectroscopy (SERS) based detectors for screening and testing SARS-CoV-2\nsignatures in diagnostic settings and contamination tracking.",
        "Brain-Computer Interface (BCI) technology facilitates direct communication\nbetween the human brain and external devices, representing a substantial\nadvancement in human-machine interaction. This review provides an in-depth\nanalysis of various BCI paradigms, including classic paradigms, current\nclassifications, and hybrid paradigms, each with distinct characteristics and\napplications. Additionally, we explore a range of signal acquisition methods,\nclassified into non-implantation, intervention, and implantation techniques,\nelaborating on their principles and recent advancements. By examining the\ninterdependence between paradigms and signal acquisition technologies, this\nreview offers a comprehensive perspective on how innovations in one domain\npropel progress in the other. The goal is to present insights into the future\ndevelopment of more efficient, user-friendly, and versatile BCI systems,\nemphasizing the synergy between paradigm design and signal acquisition\ntechniques and their potential to transform the field.",
        "Estimating brain effective connectivity (EC) from functional magnetic\nresonance imaging (fMRI) data can aid in comprehending the neural mechanisms\nunderlying human behavior and cognition, providing a foundation for disease\ndiagnosis. However, current spatiotemporal attention modules handle temporal\nand spatial attention separately, extracting temporal and spatial features\neither sequentially or in parallel. These approaches overlook the inherent\nspatiotemporal correlations present in real world fMRI data. Additionally, the\npresence of noise in fMRI data further limits the performance of existing\nmethods. In this paper, we propose a novel brain effective connectivity\nestimation method based on Fourier spatiotemporal attention (FSTA-EC), which\ncombines Fourier attention and spatiotemporal attention to simultaneously\ncapture inter-series (spatial) dynamics and intra-series (temporal)\ndependencies from high-noise fMRI data. Specifically, Fourier attention is\ndesigned to convert the high-noise fMRI data to frequency domain, and map the\ndenoised fMRI data back to physical domain, and spatiotemporal attention is\ncrafted to simultaneously learn spatiotemporal dynamics. Furthermore, through a\nseries of proofs, we demonstrate that incorporating learnable filter into fast\nFourier transform and inverse fast Fourier transform processes is\nmathematically equivalent to performing cyclic convolution. The experimental\nresults on simulated and real-resting-state fMRI datasets demonstrate that the\nproposed method exhibits superior performance when compared to state-of-the-art\nmethods.",
        "Mechanistic interpretability is an emerging diagnostic approach for neural\nmodels that has gained traction in broader natural language processing domains.\nThis paradigm aims to provide attribution to components of neural systems where\ncausal relationships between hidden layers and output were previously\nuninterpretable. As the use of neural models in IR for retrieval and evaluation\nbecomes ubiquitous, we need to ensure that we can interpret why a model\nproduces a given output for both transparency and the betterment of systems.\nThis work comprises a flexible framework for diagnostic analysis and\nintervention within these highly parametric neural systems specifically\ntailored for IR tasks and architectures. In providing such a framework, we look\nto facilitate further research in interpretable IR with a broader scope for\npractical interventions derived from mechanistic interpretability. We provide\npreliminary analysis and look to demonstrate our framework through an axiomatic\nlens to show its applications and ease of use for those IR practitioners\ninexperienced in this emerging paradigm.",
        "It is a classical result of Dal'Bo that the length spectrum of a\nnon-elementary Fuchsian group is non-arithmetic, namely, it generates a dense\nadditive subgroup of $\\mathbb{R}$. In this note we observe the following\nextension of this fact: a non-elementary Fuchsian group contains two elements\nwhose lengths are linearly independent over $\\mathbb{Q}$.",
        "In this article, we are concerned with the eigenvalue problem driven by the\nmixed local and nonlocal $p$-Laplacian operator having the interpolated Hardy\nterm \\begin{equation*} \\mathcal{T}(u) :=- \\Delta_p u + (- \\Delta_p)^s u - \\mu\n\\frac{|u|^{p-2}u}{|x|^{p \\theta}}, \\end{equation*} where $0<s<1<p<N$, $\\theta\n\\in [s,1]$, and $\\mu \\in (0,\\mu_0(\\theta))$. First, we establish a mixed\ninterpolated Hardy inequality and then show the existence of eigenvalues and\ntheir properties. We also investigate the Fu\\v{c}\\'{\\i}k spectrum, the\nexistence of the first nontrivial curve in the Fu\\v{c}\\'{\\i}k spectrum, and\nprove some of its properties. Moreover, we study the shape optimization of the\ndomain with respect to the first two eigenvalues, the regularity of the\neigenfunctions, the Faber-Krahn inequality, and a variational characterization\nof the second eigenvalue.",
        "Flat bands, when located close to the Fermi energy, can considerably enhance\nthe influence of electron correlations on the low energy physics in kagome and\nother frustrated-lattice metals. A major challenge in describing the\ninteraction effects in such bulk materials is that the flat band is often\nintermixed with a large number of other bands. Here we show that the recently\nintroduced notion of compact molecular orbitals (CMOs) enable a path forward in\ndescribing the dominant effect of the Coulomb interactions in spite of the\ncomplexity of the bandstructure. Our materials-based analysis allows for the\nunderstanding of the scanning-tunneling-microscopy experiment [J. C. Souza et\nal., preprint (2024)] of the bilayer kagome metal Ni$_3$In in terms of the CMO\nnotion. From the resulting CMO, an effective Anderson lattice model can be set\nup. This CMO-based approach enables the calculation of correlation effects that\nis difficult to do based on the atomic orbitals. Furthermore, it suggests an\nenriched phase diagram for the strange metal physics of the kagome metal, which\ncan be tested by future experiments. We discuss the implications of our results\nfor the general correlation physics of flat band systems and beyond.",
        "This paper investigates the properties of continuous frames, with a\nparticular focus on phase retrieval and norm retrieval in the context of\nHilbert spaces. We introduce the concept of continuous near-Riesz bases and\nprove their invariance under invertible operators. Some equivalent conditions\nfor phase and norm retrieval property of continuous frames are presented. We\nstudy the stability of phase retrieval under perturbations. Furthermore, tensor\nproduct frames for separable Hilbert spaces are studied, and we establish the\nequivalence of phase retrieval and norm retrieval properties between components\nand their tensor products.",
        "We consider local weak solutions to the fractional $p$-Poisson equation of\norder $s$, i.e. $\\left( - \\Delta_p\\right)^s u = f$. In the range $p>1$ and\n$s\\in \\big(\\frac{p-1}{p},1\\big)$ we prove Calder\\'on & Zygmund type estimates\nat the gradient level. More precisely, we show for any $q>1$ that\n\\begin{equation*}\n  f\\in L^{\\frac{qp}{p-1}}_{\\rm loc}\n  \\quad\\Longrightarrow\\quad\n  \\nabla u\\in L^{qp}_{\\rm loc}. \\end{equation*} The qualitative result is\naccompanied by a local quantitative estimate.",
        "We use a non-perturbative theoretical approach to the parametric\ndown-conversion (PDC) process, which generates entangled-photon field for an\narbitrarily strong pump-pulse. This approach can be used to evaluate\nmulti-point field correlation functions to compute nonlinear spectroscopic\nsignals induced by a strong pump. The entangled-photon statistics is studied\nusing Glauber's $g^{(2)}$ function, which helps understand the significance of\nthe photon entanglement-time and the pump-pulse intensity on spectroscopic\nsignals. Under the non-perturbative treatment of the entangled field, the\ntwo-photon absorption (TPA) signal shows linear to strongly non-linear growth\nwith the pump intensity, rather than linear to quadratic scaling reported\npreviously. An increase in the range of pump intensity for the linear scaling\nis observed as the pump band-width is increased. We propose an experimental\nscheme that can select contributions to the TPA signal that arise solely from\ninteractions with the entangled photons, and filter out unentangled photon\ncontributions, which are dominant at higher pump intensities, paving a way to\nexplore the entanglement effects at higher intensities."
      ]
    }
  },
  {
    "id":2411.17617,
    "research_type":"applied",
    "start_id":"b5",
    "start_title":"ALL-Net: Anatomical information lesion-wise loss function integrated into neural network for multiple sclerosis lesion segmentation",
    "start_abstract":"Accurate detection and segmentation of multiple sclerosis (MS) brain lesions on magnetic resonance images are important for disease diagnosis treatment. This is a challenging task as vary greatly in size, shape, location, image contrast. The objective our study was to develop an algorithm based deep convolutional neural network integrated with anatomic information lesion-wise loss function (ALL-Net) fast accurate automated MS lesions. Distance transformation mapping used construct module that encoded lesion-specific anatomical information. To overcome the lesion size imbalance during training improve small lesions, developed which individual were modeled spheres equal size. On ISBI-2015 longitudinal challenge dataset (19 subjects total), ALL-Net achieved overall score 93.32 amongst top performing methods. larger Cornell (176 significantly improved both voxel-wise metrics (Dice improvement 3.9% 35.3% p-values ranging from p < 0.01 0.0001, AUC precision-recall curve 2.1% 29.8%) (lesion-wise F1 12.6% 29.8% all ROC 1.4% 20.0%) compared leading publicly available tools.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b8"
      ],
      "title":[
        "Deep learning-Based 3D inpainting of brain MR images"
      ],
      "abstract":[
        "Abstract The detailed anatomical information of the brain provided by 3D magnetic resonance imaging (MRI) enables various neuroscience research. However, due to long scan time for MR images, 2D images are mainly obtained in clinical environments. purpose this study is generate from a sparsely sampled using an inpainting deep neural network that has U-net-like structure and DenseNet sub-blocks. To train network, not only fidelity loss but also perceptual based on VGG were considered. Various methods used assess overall similarity between inpainted original data. In addition, morphological analyzes performed investigate whether data produced local features similar diagnostic ability was evaluated investigating pattern changes disease groups. Brain anatomy details efficiently recovered proposed network. voxel-based analysis gray matter volume cortical thickness, differences observed small clusters. method will be useful utilizing advanced neuroimaging techniques with MRI"
      ],
      "categories":[
        "q-bio.CB"
      ]
    },
    "list":{
      "title":[
        "On the Ising Phase Transition in the Infrared-Divergent Spin Boson Model",
        "Rotational decoherence dynamics in ultracold molecules induced by a\n  tunable spin environment: The Central Rotor Model",
        "Pseudorapidity density distributions of charged particles and transverse\n  momentum spectra of identified particles in pp collisions in PACIAE 4.0 model",
        "Laser-based aberration corrector",
        "Dissociated Neuronal Cultures as Model Systems for Self-Organized\n  Prediction",
        "Geometric origin of supercurrents in Berry phase: Formula for computing\n  currents from wavefunctions with correlation and particle number variation",
        "Center vortices and the $\\mathrm{SU}(3)$ conformal window",
        "High-aspect-ratio silica meta-optics for high-intensity structured light",
        "Reporting on pTP sublimation during evaporation deposition",
        "Observation of a zero-energy excitation mode in the open Dicke model",
        "X-ray Thomson scattering studies on spin-singlet stabilization of highly\n  compressed H-like Be ions heated to two million degrees Kelvin",
        "Comment on: \"2005 VL1 is not Venera-2\"",
        "An Approach to Use Depletion Charges for Modifying Band Profiles for\n  Field-Effect Transistors",
        "On the Gauge Invariance of Secondary Gravitational Waves",
        "A physical model approach to order lot sizing",
        "Data driven discovery of human mobility models",
        "Refining Au\/Sb alloyed ohmic contacts in undoped Si\/SiGe strained\n  quantum wells",
        "Incompleteness of Bell's theorem",
        "Proceedings of the Erice Workshop: A new baseline for the hybrid,\n  asymmetric, linear Higgs factory HALHF",
        "An Adaptive Collocation Point Strategy For Physics Informed Neural\n  Networks via the QR Discrete Empirical Interpolation Method",
        "Demystifying 5G Polar and LDPC Codes: A Comprehensive Review and\n  Foundations",
        "Scalable Discovery of Fundamental Physical Laws: Learning\n  Magnetohydrodynamics from 3D Turbulence Data",
        "Modular covariant torus partition functions of dense $A_1^{(1)}$ and\n  dilute $A_2^{(2)}$ loop models",
        "Realistic predictions for Gaia black hole discoveries: comparison of\n  isolated binary and dynamical formation models",
        "Precision Higgs Constraints in U(1) Extensions of the Standard Model\n  with a Light Z'-Boson",
        "Bidirectional quantitative scattering microscopy",
        "The Motzkin subproduct system",
        "Sub-GeV Dark Matter Direct Detection with Neutrino Observatories",
        "Statistical inference for interacting innovation processes and related\n  general results"
      ],
      "abstract":[
        "We prove absence of ground states in the infrared-divergent spin boson model\nat large coupling. Our key argument reduces the proof to verifying long range\norder in the dual one-dimensional continuum Ising model, i.e., to showing that\nthe respective two point function is lower bounded by a strictly positive\nconstant. We can then use known results from percolation theory to establish\nlong range order at large coupling. Combined with the known existence of ground\nstates at small coupling, our result proves that the spin boson model undergoes\na phase transition with respect to the coupling strength. We also present an\nexpansion for the vacuum overlap of the spin boson ground state in terms of the\nIsing $n$-point functions, which implies that the phase transition is unique,\ni.e., that there is a critical coupling constant below which a ground state\nexists and above which none can exist.",
        "We show that quantum rotational wavepacket dynamics in molecules can be\ndescribed by a new system-environment model, which consists of a rotational\nsubsystem coupled to a magnetically tunable spin bath formed by the nuclear\nspins within the molecule. The central rotor model shares similarities with the\nparadigmatic central spin model, but features much richer rotational dynamics\nthat is sensitive to the molecule's environment, which can be initiated and\nprobed with short laser pulses used to control molecular orientation and\nalignment. We present numerical simulations of the nuclear-spin-bath-induced\nrotational decoherence dynamics of KRb molecules, which exhibit remarkable\nsensitivity to an external magnetic field. Our results show that ultracold\nmolecular gases provide a natural platform for the experimental realization of\nthe CRM.",
        "The pseudorapidity density distributions of charged particles and the\ntransverse momentum spectra of identified particles in proton-proton (pp)\ncollisions at the center-of-mass energies ranging from $\\sqrt{s}=200$ GeV to 13\nTeV have been systematically studied using the newly released parton and\ncascade model PACIAE 4.0 based on PYTHIA 8.3. The available experimental data\nare well reproduced across all analyzed aspects. This theoretical method can be\neasily extended to anywhere the experimental data for pp collisions are\ncurrently unavailable. Furthermore, since pp collisions serve as the baseline\nfor heavy-ion collisions, our results can provide a valuable resource for both\nexperimentalists and theorists.",
        "Aberration correctors are essential elements for achieving atomic resolution\nin state-of-the-art electron microscopes. Conventional correctors are based on\na series of multipolar electron lenses, but more versatile alternatives are\nintensively sought. Here we suggest spatially tailored intense laser pulses as\none such alternative. Our simulations demonstrate that the free-space\nelectron-photon interaction can be used to compensate for spherical and\nchromatic aberrations of subsequent electron lenses. We show a significant\nimprovement in the simulated electron probe sizes and discuss the prospects of\nutilizing the tailored laser fields as a platform for novel electron optics in\nultrafast electron microscope setups.",
        "Dissociated neuronal cultures provide a simplified yet effective model system\nfor investigating self-organized prediction and information processing in\nneural networks. This review consolidates current research demonstrating that\nthese in vitro networks display fundamental computational capabilities,\nincluding predictive coding, adaptive learning, goal-directed behavior, and\ndeviance detection. We examine how these cultures develop critical dynamics\noptimized for information processing, detail the mechanisms underlying learning\nand memory formation, and explore the relevance of the free energy principle\nwithin these systems. Building on these insights, we discuss how findings from\ndissociated neuronal cultures inform the design of neuromorphic and reservoir\ncomputing architectures, with the potential to enhance energy efficiency and\nadaptive functionality in artificial intelligence. The reduced complexity of\nneuronal cultures allows for precise manipulation and systematic investigation,\nbridging theoretical frameworks with practical implementations in bio-inspired\ncomputing. Finally, we highlight promising future directions, emphasizing\nadvancements in three-dimensional culture techniques, multi-compartment models,\nand brain organoids that deepen our understanding of hierarchical and\npredictive processes in both biological and artificial systems. This review\naims to provide a comprehensive overview of how dissociated neuronal cultures\ncontribute to neuroscience and artificial intelligence, ultimately paving the\nway for biologically inspired computing solutions.",
        "The complexity of itinerant and many-body nature in Bardeen-Cooper-Schrieffer\n(BCS) wavefunctions has traditionally led to the use of coarse-grained order\nparameters for describing currents in superconductors (SC), rather than\ndirectly utilizing wavefunctions. In this work, we introduce a phase-based\nformula that enables the direct computation of currents from microscopic\nwavefunctions, accounting for correlation and particle number variations.\nInterestingly, the formulation draws parallels with insulators, suggesting a\nunified framework for understanding (intra-band) charge transport across two\nextremes of conductivity. A group velocity current\n$J_{band}{\\propto}\\frac{1}{\\hbar}{\\partial}_kE(k)$ is derived from Berry phase,\nindependent of wave package dynamics, robust against correlation. Additionally,\nwe identify a correlation-driven contribution, $J_{corr}$, which reveals that\nthe pairing correlations ${\\langle}c_kc_{-k}{\\rangle}$ among dancing partners\nprovide a current component beyond the velocity operator.",
        "A novel approach for estimating the lower end of the $\\mathrm{SU}(3)$\nconformal window is presented through the study of center vortex geometry and\nits dependence on the number of fermion flavors $N_f$. Values ranging from $N_f\n= 2$--$8$ are utilized to infer an upper limit for vortex behavior in the low\n$N_f$ phase, which may inform the transition to the conformal window. The\nsimulations are performed at a single lattice spacing and pion mass, both fixed\nfor all $N_f$. Visualizations of the center vortex structure in\nthree-dimensional slices of the lattice reveal a growing roughness in the\nvortex matter as a function of $N_f$, embodied by an increase in the density of\nvortex matter in the percolating cluster and a simultaneous reduction in\nsecondary clusters disconnected from the percolating cluster in 3D slices. This\nis quantified by various bulk properties, including the vortex and branching\npoint densities. A correlation of the vortex structure reveals a turning point\nnear $N_f \\simeq 5$ past which a randomness in the vortex field becomes the\ndominant aspect of its evolution with $N_f$. As a byproduct, extrapolations to\nthe vortex content of a uniform-random gauge field provide a critical point at\nwhich there must be a drastic shift in vacuum field structure. A precise\nestimate for the critical value is extracted as $N_f^* = 11.43(16)(17)$, close\nto various other estimates.",
        "Structured light and high-intensity ultrafast lasers are two rapidly\nadvancing frontiers in photonics, yet their intersection remains largely\nunexplored. While ultrafast lasers continue to push the boundaries of peak\nintensities, structured light has enabled unprecedented control over light's\nspatial, temporal, and polarization properties. However, the lack of robust\noptical devices capable of bridging structured light with the high-intensity\ndomain has constrained progress in combining these directions. Here, we\ndemonstrate high-aspect-ratio silica meta-optics, which close this gap by\ncombining silica's extraordinary damage resistance with the advanced phase and\npolarization control offered by metasurfaces. By leveraging anisotropic etching\ntechniques, we fabricate nanopillars exceeding 3 $\\mu$m in height with aspect\nratios up to 14, enabling precise manipulation of complex light fields at\nintensities far beyond the thresholds of conventional metasurfaces. We showcase\ntheir functionality in generating vortex beams and achieving polarization\nmanipulation with large phase retardance at challenging long-visible\nwavelengths. High-aspect-ratio silica meta-optics unlock structured\nlaser-matter interactions in extreme regimes, that will surpass plasma\nionization thresholds and enable applications such as relativistic particle\nacceleration and high-harmonic generation with structured beams, for both\ntabletop ultrafast systems and large-scale laser facilities.",
        "Noble liquid detectors rely on wavelength shifter materials, such as\np-terphenyl (pTP) and Tetraphenyl-butadiene (TPB), which are widely used in\nneutrino and dark matter experiments. Given their importance, a thorough\nunderstanding and characterization of these compounds are essential for\noptimizing experimental techniques and enhancing detector performance. In this\nstudy, we report a novel phenomenon in which commonly used wavelength shifters\nundergo spontaneous sublimation under high vacuum conditions. We quantify the\nsublimation rates of pTP and TPB as a function of pressure and temperature,\nassessing their impact on material growth and physical properties.\nAdditionally, we investigate how variations in film thickness and growth rate\ninfluence the sublimation process. These findings provide critical insights\ninto improving the handling and preparation of wavelength shifters during the\nfabrication of light detectors for these experiments, ensuring their stability\nand performance in low-background photodetection systems.",
        "Approaching phase boundaries in many-body systems can give rise to intriguing\nsignatures in their excitation spectra. Here, we explore the excitation\nspectrum of a Bose-Einstein condensate strongly coupled to an optical cavity\nand pumped by an optical standing wave, which simulates the famous\nDicke-Hepp-Lieb phase transition of the open Dicke model with dissipation\narising due to photon leakage from the cavity. For weak dissipation, the\nexcitation spectrum displays two strongly polaritonic modes. Close to the phase\nboundary, we observe an intriguing regime where the lower-energetic of these\nmodes, instead of showing the expected roton-type mode softening, is found to\napproach and persist at zero energy, well before the critical pump strength for\nthe Dicke-Hepp-Lieb transition boundary is reached. Hence, a peculiar situation\narises, where an excitation is possible at zero energy cost, but nevertheless\nno instability of the system is created.",
        "Experiments at the US National Ignition Facility (NIF) [D\\\"{o}ppner et al.,\nNature {\\bf 618}, 270-275 (2023)] have created highly compressed hot\nhydrogen-like Be plasmas. Published analyses of the the NIF experiment have\nused finite-$T$ multi-atom density-functional theory (DFT) with Molecular\ndynamics (MD), and Path-Integral Monte Carlo (PIMC) simulations. These methods\nare very expensive to implement and often lack physical transparency. Here we\n(i) relate their results to simpler first-principles average-atom results, (ii)\nestablish the feasibility of rapid data analysis, with good accuracy and gain\nin physical transparency, and (iii) show that the NIF experiment reveals\nhigh-$T$ spin-singlet pairing of hydrogen-like Be ions with near neighbours.\nOur analysis predicts such stabilization over a wide range of compressed\ndensities for temperatures close to two million Kelvin. Calculations of\nstructure factors $S(k)$ for electrons or ions, the Raleigh weight and other\nquantities of interest to X-ray Thomson scattering are presented. We find that\nthe NIF data at the scattering wavevector $k_{sc}$ of 7.89 \\AA$^{-1}$ are more\nconsistent with a density of $20\\pm2$ g\/cm$^3$, mean ionization $\\bar{Z}=$3.25,\nat a temperature of $\\simeq$ 1,800,000 K than the 34 g\/cm$^3, \\bar{Z}=3.4$\nproposed by the NIF team. The relevance of ion-electron coupled-modes in\nstudying small $k_{sc}$ data is indicated.",
        "I show that the small differences between the orbital parameters of the dark\ncomet 2005 VL1 and the Venera 2 spacecraft (reported in arXiv:2503.07972) are\nof the magnitude expected from gravitational deflection by a close encounter of\nVenera 2 with Venus.",
        "We present the study of using depletion charges for tailoring lateral band\nprofiles and applying it to the promising gate-all-around field-effect\ntransistors (GAAFET). Specifically, we introduce heavily p-type doped Si next\nto the channel, but outside the channel, of a transistor. They are connected to\nthe heavily n-type doped source and drain for generating the depletion charges.\nThe finite difference method was used for simulations and the results show\nsignificant modifications of the conduction band along the channel. The\ndepletion charges act as built-in electrodes capable of significantly modifying\nthe band profiles of field-effect transistors. Quantum confinement within the\nchannel has been attempted with different approaches, such as additional\nelectrodes and point contacts. The results presented show two aspects of this\napproach, namely, realizing quantum confinement in an all-Si structure and\ntailoring band profiles within channels to modify their transport properties.",
        "Second-order tensor perturbations induced by primordial fluctuations play a\ncrucial role in probing small-scale physics, but gauge dependence of their\nenergy density has remained a fundamental challenge in cosmological\nperturbation theory. We address this issue by introducing a boundary\ncondition-based filtering method that extracts physical radiation through the\nSommerfeld criterion. We demonstrate that after filtering non-physical modes,\nthe energy density of secondary gravitational waves becomes gauge-invariant and\nexhibits physically consistent behavior in the sub-horizon limit. This approach\nprovides a unified framework for both adiabatic and isocurvature perturbations,\nenhancing theoretical predictions and observational signatures of early\nuniverse physics.",
        "The growing need for companies to reduce costs and maximize profits has led\nto an increased focus on logistics activities. Among these, inventory\nmanagement plays a crucial role in minimizing organizational expenses by\noptimizing the storage and transportation of materials. In this context, this\nstudy introduces an optimization model for the lot-sizing problem based on a\nphysical system approach. By establishing that the material supply problem is\nisomorphic to a one-dimensional mechanical system of point particles connected\nby elastic elements, we leverage this analogy to derive cost optimization\nconditions naturally and obtain an exact solution. This approach determines lot\nsizes that minimize the combined ordering and inventory holding costs in a\nsignificantly shorter time, eliminating the need for heuristic methods. The\noptimal lot sizes are defined in terms of the parameter $ \\gamma = 2C_O \/ C_H\n$, which represents the relationship between the ordering cost per order ($ C_O\n$) and the holding cost per period for the material required in one period ($\nC_H $). This parameter fully dictates the system's behavior: when $ \\gamma \\leq\n1 $, the optimal strategy is to place one order per period, whereas for $\n\\gamma > 1 $, the number of orders $ N $ is reduced relative to the planning\nhorizon $ M $, meaning $ N < M $. By formulating the total cost function in\nterms of the intensive variable $ N\/M $, we consolidate the entire optimization\nproblem into a single function of $ \\gamma $. This eliminates the need for\ncomplex algorithms, enabling faster and more precise purchasing decisions. The\nproposed model was validated through a real-world case study and benchmarked\nagainst classical algorithms, demonstrating superior cost optimization and\nreduced execution time. These findings underscore the potential of this\napproach for improving material lot-sizing strategies.",
        "Human mobility is a fundamental aspect of social behavior, with broad\napplications in transportation, urban planning, and epidemic modeling. However,\nfor decades new mathematical formulas to model mobility phenomena have been\nscarce and usually discovered by analogy to physical processes, such as the\ngravity model and the radiation model. These sporadic discoveries are often\nthought to rely on intuition and luck in fitting empirical data. Here, we\npropose a systematic approach that leverages symbolic regression to\nautomatically discover interpretable models from human mobility data. Our\napproach finds several well-known formulas, such as the distance decay effect\nand classical gravity models, as well as previously unknown ones, such as an\nexponential-power-law decay that can be explained by the maximum entropy\nprinciple. By relaxing the constraints on the complexity of model expressions,\nwe further show how key variables of human mobility are progressively\nincorporated into the model, making this framework a powerful tool for\nrevealing the underlying mathematical structures of complex social phenomena\ndirectly from observational data.",
        "Shallow undoped Si\/SiGe quantum wells are the leading platforms for hosting\nquantum processors based on spin-qubits. The ohmic contacts to the electron gas\nin these systems are accomplished by ion-implantation technique since the\nconventional Au\/Sb alloyed contacts present a rough surface consisting of sharp\nislands and pits. These sharp protrusions cause electrical discharge across the\ngate-dielectric between the ohmic contacts and the accumulation-gates causing\ndevice break-down. A clear understanding of the surface morphology, elemental,\ncompositional and electrical characterization of the alloyed region would\nenable one to engineer a smoother post alloyed surface. In this work, we find\nthat the rough surface morphology is a cumulative effect of the Au\/Si eutectic\nreaction and the threading dislocations inherent in the heterostructure. The\nstructural, elemental, and chemical-state analysis show that the inverted\npyramidal pits are resulting from the enhanced Au\/Si eutectic reaction at the\nthreading dislocations stemming from the heterostructure interface, while, the\nsharp protrusions causing accumulation gate-leakage are gold-rich\nprecipitations. The protrusions are removed using an aqua regia treatment prior\nto the deposition of the gate-oxide and gate electrode. Exploiting a Hall bar\ndevice, we analyse the mobility and carrier concentration of the undoped\nSi\/SiGe consisting of Au\/Sb alloyed contacts down to 1.5 K. The measured\nmobility ~10^5 cm^2\/Vs and carrier concentration of ~10^11\/cm^2are comparable\nto the reported values on similar high-mobility heterostructures confirming the\nefficacy of our modified Au\/Sb alloy technique in accomplishing high-efficiency\ncontacts to undoped Si\/SiGe heterostructures.",
        "In the paper it is reported that Bell's correlation formula allows an\nEinstein local hidden variables explanation. The key is the application of\nPetis integration.",
        "The HALHF collaboration has discussed a new baseline for the project, taking\ninto account comments from the accelerator community on various aspects of the\noriginal design. In particular, these concerned the practicality of the\ndual-purpose linac to accelerate both colliding positron bunches and the drive\nbeams required for the plasma linac. In addition, many other aspects of the\nproject were also considered; the discussion and conclusions are documented in\nthis paper. Finally, a new baseline is outlined that has been optimised and\naddresses several weaknesses in the original design, has higher luminosity,\nreduced centre-of-mass energy boost and additional features such as positron\npolarization as well as electron polarization. Although HALHF has become longer\nand more expensive, it remains significantly smaller and cheaper than other\nmature Higgs factory designs currently under discussion.",
        "Physics-informed neural networks (PINNs) have gained significant attention\nfor solving forward and inverse problems related to partial differential\nequations (PDEs). While advancements in loss functions and network\narchitectures have improved PINN accuracy, the impact of collocation point\nsampling on their performance remains underexplored. Fixed sampling methods,\nsuch as uniform random sampling and equispaced grids, can fail to capture\ncritical regions with high solution gradients, limiting their effectiveness for\ncomplex PDEs. Adaptive methods, inspired by adaptive mesh refinement from\ntraditional numerical methods, address this by dynamically updating collocation\npoints during training but may overlook residual dynamics between updates,\npotentially losing valuable information. To overcome this limitation, we\npropose an adaptive collocation point selection strategy utilizing the QR\nDiscrete Empirical Interpolation Method (QR-DEIM), a reduced-order modeling\ntechnique for efficiently approximating nonlinear functions. Our results on\nbenchmark PDEs, including the wave, Allen-Cahn, and Burgers' equations,\ndemonstrate that our QR-DEIM-based approach improves PINN accuracy compared to\nexisting methods, offering a promising direction for adaptive collocation point\nstrategies.",
        "This paper serves as a comprehensive guide for practitioners and scholars\naiming to understand the channel coding and decoding schemes integral to the 5G\nNR standard, with a particular focus on LDPC and polar codes. We start by\nexplaining the design procedures that underlie these channel codes, offering\nfundamental information from the perspectives of both encoding and decoding. In\norder to determine the present status of research in this area, we also provide\na thorough literature review. Notably, we add comprehensive, standard-specific\ninformation to these foundational evaluations that is frequently difficult to\nextract from technical specification documents. The significance of reviewing\nand refining the foundations of the aforementioned codes lies in their\npotential to serve as candidate error-correcting codes for the future 6G\nstandard and beyond.",
        "The discovery of dynamical models from data represents a crucial step in\nadvancing our understanding of physical systems. Library-based sparse\nregression has emerged as a powerful method for inferring governing equations\ndirectly from spatiotemporal data, but current model-agnostic implementations\nremain computationally expensive, limiting their applicability to data that\nlack substantial complexity. To overcome these challenges, we introduce a\nscalable framework that enables efficient discovery of complex dynamical models\nacross a wide range of applications. We demonstrate the capabilities of our\napproach, by ``discovering'' the equations of magnetohydrodynamics (MHD) from\nsynthetic data generated by high-resolution simulations of turbulent MHD flows\nwith viscous and Ohmic dissipation. Using a library of candidate terms that is\n$\\gtrsim 10$ times larger than those in previous studies, we accurately recover\nthe full set of MHD equations, including the subtle dissipative terms that are\ncritical to the dynamics of the system. Our results establish sparse regression\nas a practical tool for uncovering fundamental physical laws from complex,\nhigh-dimensional data without assumptions on the underlying symmetry or the\nform of any governing equation.",
        "Yang-Baxter integrable dense $A_1^{(1)}$ and dilute $A_2^{(2)}$ loop models\nare considered on the torus in their simplest physical regimes. A combination\nof boundary conditions $(h,v)$ is applied in the horizontal and vertical\ndirections with $h,v=0$ and $1$ for periodic and antiperiodic boundary\nconditions respectively. The fugacities of non-contractible and contractible\nloops are denoted by $\\alpha$ and $\\beta$ respectively where $\\beta$ is simply\nrelated to the crossing parameter $\\lambda$. At roots of unity, when\n$\\lambda\/\\pi\\in\\mathbb Q$, these models are the dense ${\\cal LM}(p,p')$ and\ndilute ${\\cal DLM}(p,p')$ logarithmic minimal models with $p,p'$ coprime\nintegers. We conjecture the scaling limits of the transfer matrix traces in the\nstandard modules with $d$ defects and deduce the conformal partition functions\n${\\cal Z}_{\\textrm{dense}}^{(h,v)}(\\alpha)$ and ${\\cal\nZ}_{\\textrm{dilute}}^{(h,v)}(\\alpha)$ using Markov traces. These are expressed\nin terms of functions ${\\cal Z}_{m,m'}(g)$ known from the Coulomb gas arguments\nof Di Francesco, Saleur and Zuber and subsequently as sesquilinear forms in\nVerma characters. Crucially, we find that the partition functions are identical\nfor the dense and dilute models. The coincidence of these conformal partition\nfunctions provides compelling evidence that, for given $(p,p')$, these dense\nand dilute theories lie in the same universality class. In root of unity cases\nwith $\\alpha=2$, the $(h,v)$ modular covariant partition functions are also\nexpressed as sesquilinear forms in affine $u(1)$ characters involving\ngeneralized Bezout conjugates. These also give the modular covariant partition\nfunctions for the 6-vertex and Izergin-Korepin 19-vertex models in the\ncorresponding regimes.",
        "Astrometry from Gaia has enabled discovery of three dormant black holes (BHs)\nin au-scale binaries. Numerous models have been proposed to explain their\nformation, including several that have forecasted Gaia detections. However,\nprevious works have used simplified detectability metrics that do not capture\nkey elements of the Gaia astrometric orbit selection function. We apply a\nrealistic forward-model of Gaia astrometric orbit catalogs to BH binary\npopulations generated through (a) isolated binary evolution (IBE) and (b)\ndynamical formation in star clusters. For both formation channels, we analyze\nbinary populations in a simulated Milky Way-like galaxy with a realistic\nmetallicity-dependent star formation history and 3D dust map. We generate epoch\nastrometry for each binary from the Gaia scanning law and fit it with the\ncascade of astrometric models used in Gaia DR3. The IBE model of Chawla et al.\n(2022) predicts that no BH binaries should have been detected in DR3 and thus\nsignificantly underpredicts the formation rate of Gaia BHs. In contrast, the\ndynamical model of Di Carlo et al. (2024) overpredicts the number of BHs\nreceiving DR3 orbital solutions by a factor of $\\sim$8. The two models predict\nvery different orbital period distributions, with the IBE model predicting only\nbinaries that avoided common envelope evolution and have $P_{\\text{orb}}\n\\gtrsim 2,000$ d to be detectable, and the dynamical formation model predicting\na period distribution that is roughly log-uniform. Adopting the dynamical\nchannel as a fiducial model and rescaling by a factor of 1\/8 to match DR3, we\npredict that $\\sim$30 BH binaries will be detected in Gaia DR4, representing\n$\\sim0.1\\%$ of Milky Way BHs with luminous companions in au-scale orbits.",
        "Anomaly free $U(1)$ extensions of the standard model (SM) predict a new\nneutral gauge boson $Z'$. When the $Z'$ obtains its mass from the spontaneous\nbreaking of the new $U(1)$ symmetry by a new complex scalar field, the model\nalso predicts a second real scalar $s$ and the searches for the new scalar and\nthe new gauge boson become intertwined. We present the computation of\nproduction cross sections and decay widths of such a scalar $s$ in models with\na light $Z'$ boson, when the decay $h\\to Z' Z'$ may have a sizeable branching\nratio. We show how Higgs signal strength measurement in this channel can\nprovide stricter exclusion bounds on the parameters of the model than those\nobtained from the total signal strength for Higgs boson production.",
        "Quantitative phase microscopy (QPM) and interferometric scattering (iSCAT)\nmicroscopy are powerful label-free imaging techniques and are widely used for\nbiomedical applications. Each method, however, possesses distinct limitations:\nQPM, which measures forward scattering (FS), excels at imaging microscale\nstructures but struggles with rapidly moving nanoscale objects, while iSCAT,\nbased on backward scattering (BS), is highly sensitive to nanoscale dynamics\nbut lacks the ability to image microscale structures comprehensively. Here, we\nintroduce bidirectional quantitative scattering microscopy (BiQSM), an\ninnovative approach that integrates FS and BS detection using off-axis digital\nholography with bidirectional illumination and spatial-frequency multiplexing.\nBiQSM achieves spatiotemporal consistency and a dynamic range 14 times wider\nthan QPM, enabling simultaneous imaging of nanoscale and microscale cellular\ncomponents. We demonstrate BiQSM's ability to reveal spatiotemporal behaviors\nof intracellular structures, with FS-BS correlation analysis providing insights\ninto proteins, lipids, and membranes. Time-lapse imaging of dying cells further\nhighlights BiQSM's potential as a label-free tool for monitoring cellular vital\nstates through structural and motion-related changes. By bridging the strengths\nof QPM and iSCAT, BiQSM advances quantitative cellular imaging and opens new\navenues for studying dynamic biological processes.",
        "We introduce a subproduct system of finite-dimensional Hilbert spaces using\nthe Motzkin planar algebra and its Jones-Wenzl idempotents, which generalizes\nthe Temperley-Lieb subproduct system of Habbestad and Neshveyev. We provide a\ndescription of the corresponding Toeplitz C$^*$-algebra as a universal\nC$^*$-algebra, defined in terms of generators and relations, and we highlight\nproperties of its representation theory.",
        "We present a new technique for sub-GeV dark matter (DM) searches and a new\nuse of neutrino observatories. DM-electron scattering can excite or ionize\ntarget molecules in the observatory, which then produce light that can be\ndetected by its photomultiplier tubes (PMTs). While individual DM scatterings\nare undetectable, the aggregate rate from many independent scatterings can be\nisolated from the total PMT dark rate using the expected DM annual modulation.\nWe showcase this technique with the example of JUNO, a 20,000 ton scintillator\ndetector, showing that its sensitivity in some mass ranges exceeds all other\ntechniques and reaches key particle-theory targets.",
        "Given the importance of understanding how different innovation processes\naffect each other, we have introduced a model for a finite system of\ninteracting innovation processes. The present work focuses on the second-order\nasymptotic properties of the model and illustrates how to leverage the\ntheoretical results in order to make statistical inference on the intensity of\nthe interaction. This methodology is presented within a general framework in\nthe supplementary material to ensure its broad applicability across various\ncontexts. We apply the proposed tools to two real data sets (from Reddit and\nGutenberg)."
      ]
    }
  },
  {
    "id":2411.17617,
    "research_type":"applied",
    "start_id":"b8",
    "start_title":"Deep learning-Based 3D inpainting of brain MR images",
    "start_abstract":"Abstract The detailed anatomical information of the brain provided by 3D magnetic resonance imaging (MRI) enables various neuroscience research. However, due to long scan time for MR images, 2D images are mainly obtained in clinical environments. purpose this study is generate from a sparsely sampled using an inpainting deep neural network that has U-net-like structure and DenseNet sub-blocks. To train network, not only fidelity loss but also perceptual based on VGG were considered. Various methods used assess overall similarity between inpainted original data. In addition, morphological analyzes performed investigate whether data produced local features similar diagnostic ability was evaluated investigating pattern changes disease groups. Brain anatomy details efficiently recovered proposed network. voxel-based analysis gray matter volume cortical thickness, differences observed small clusters. method will be useful utilizing advanced neuroimaging techniques with MRI",
    "start_categories":[
      "q-bio.CB"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b5"
      ],
      "title":[
        "ALL-Net: Anatomical information lesion-wise loss function integrated into neural network for multiple sclerosis lesion segmentation"
      ],
      "abstract":[
        "Accurate detection and segmentation of multiple sclerosis (MS) brain lesions on magnetic resonance images are important for disease diagnosis treatment. This is a challenging task as vary greatly in size, shape, location, image contrast. The objective our study was to develop an algorithm based deep convolutional neural network integrated with anatomic information lesion-wise loss function (ALL-Net) fast accurate automated MS lesions. Distance transformation mapping used construct module that encoded lesion-specific anatomical information. To overcome the lesion size imbalance during training improve small lesions, developed which individual were modeled spheres equal size. On ISBI-2015 longitudinal challenge dataset (19 subjects total), ALL-Net achieved overall score 93.32 amongst top performing methods. larger Cornell (176 significantly improved both voxel-wise metrics (Dice improvement 3.9% 35.3% p-values ranging from p < 0.01 0.0001, AUC precision-recall curve 2.1% 29.8%) (lesion-wise F1 12.6% 29.8% all ROC 1.4% 20.0%) compared leading publicly available tools."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "MI-DETR: An Object Detection Model with Multi-time Inquiries Mechanism",
        "Non-linear Partition of Unity method",
        "Linear statistics at the microscopic scale for the 2D Coulomb gas",
        "The role of effective mass and long-range interactions in the band-gap\n  renormalization of photo-excited semiconductors",
        "Machine-learning potentials for structurally and chemically complex MAB\n  phases: strain hardening and ripplocation-mediated plasticity",
        "On Elephant Random Walk with Random Memory",
        "Theoretical Data-Driven MobilePosenet: Lightweight Neural Network for\n  Accurate Calibration-Free 5-DOF Magnet Localization",
        "Security and Quality in LLM-Generated Code: A Multi-Language,\n  Multi-Model Analysis",
        "ProJudge: A Multi-Modal Multi-Discipline Benchmark and\n  Instruction-Tuning Dataset for MLLM-based Process Judges",
        "Analysis of Information Loss on Composition Measurement in Stiff\n  Chemically Reacting Systems",
        "ReVeil: Unconstrained Concealed Backdoor Attack on Deep Neural Networks\n  using Machine Unlearning",
        "Uni-Retrieval: A Multi-Style Retrieval Framework for STEM's Education",
        "Non-ergodic Phase Transition in the Global Hysteresis of the Frustrated\n  Magnet DyRu2Si2",
        "Evaluation for Regression Analyses on Evolving Data Streams",
        "Many-body perturbation theory for moir\\'{e} systems",
        "Zooming into the horizon region of black hole-type objects",
        "ARCAS: Adaptive Runtime System for Chiplet-Aware Scheduling",
        "Murmurations and Sato-Tate Conjectures for High Rank Zetas of Elliptic\n  Curves II: Beyond Riemann Hypothesis",
        "Weak mixing angle under $\\text{U}(1, 3)$ colored gravity",
        "An Elementary Microscopic Model of Sympatric Speciation",
        "Performance Analysis of Traditional VQA Models Under Limited\n  Computational Resources",
        "Compressing Language Models for Specialized Domains",
        "Efficient and Accurate Estimation of Lipschitz Constants for Hybrid\n  Quantum-Classical Decision Models",
        "Bruhat-Tits buildings and $p$-adic period domains",
        "Big algebra in type $A$ for the coordinate ring of the matrix space",
        "Nonlinear optical response in a ferromagnetic insulating manganite:\n  Pr$_{0.8}$Ca$_{0.2}$MnO$_{3}$",
        "Can Large Language Models Outperform Non-Experts in Poetry Evaluation? A\n  Comparative Study Using the Consensual Assessment Technique",
        "Nucleation line in three-component Bose-Einstein condensates in\n  Gross-Pitaevskii theory",
        "A Unified Framework for Entropy Search and Expected Improvement in\n  Bayesian Optimization"
      ],
      "abstract":[
        "Based on analyzing the character of cascaded decoder architecture commonly\nadopted in existing DETR-like models, this paper proposes a new decoder\narchitecture. The cascaded decoder architecture constrains object queries to\nupdate in the cascaded direction, only enabling object queries to learn\nrelatively-limited information from image features. However, the challenges for\nobject detection in natural scenes (e.g., extremely-small, heavily-occluded,\nand confusingly mixed with the background) require an object detection model to\nfully utilize image features, which motivates us to propose a new decoder\narchitecture with the parallel Multi-time Inquiries (MI) mechanism. MI enables\nobject queries to learn more comprehensive information, and our MI based model,\nMI-DETR, outperforms all existing DETR-like models on COCO benchmark under\ndifferent backbones and training epochs, achieving +2.3 AP and +0.6 AP\nimprovements compared to the most representative model DINO and SOTA model\nRelation-DETR under ResNet-50 backbone. In addition, a series of diagnostic and\nvisualization experiments demonstrate the effectiveness, rationality, and\ninterpretability of MI.",
        "This paper introduces the Non-linear Partition of Unity Method, a novel\ntechnique integrating Radial Basis Function interpolation and Weighted\nEssentially Non-Oscillatory algorithms. It addresses challenges in\nhigh-accuracy approximations, particularly near discontinuities, by adapting\nweights dynamically. The method is rooted in the Partition of Unity framework,\nenabling efficient decomposition of large datasets into subproblems while\nmaintaining accuracy. Smoothness indicators and compactly supported functions\nensure precision in regions with discontinuities. Error bounds are calculated\nand validate its effectiveness, showing improved interpolation in discontinuous\nand smooth regions. Some numerical experiments are performed to check the\ntheoretical results.",
        "We consider the classical Coulomb gas in two dimensions at the inverse\ntemperature $\\beta=2$, confined within a droplet of radius $R$ by a\nrotationally invariant potential $U(r)$. For $U(r)\\sim r^2$ this describes the\neigenvalues of the complex Ginibre ensemble of random matrices. We study linear\nstatistics of the form ${\\cal L}_N = \\sum_{i=1}^N f(|{\\bf x}_i|)$, where ${\\bf\nx}_i$'s are the positions of the $N$ particles, in the large $N$ limit with\n$R=O(1)$. It is known that for smooth functions $f(r)$ the variance ${\\rm Var}\n\\,{\\cal L}_N= O(1)$, while for an indicator function relevant for the disk\ncounting statistics, all cumulants of ${\\cal L}_N$ of order $q \\geq 2$ behave\nas $\\sim \\sqrt{N}$. In addition, for smooth functions, it was shown that the\ncumulants of ${\\cal L}_N$ of order $q \\geq 3$ scale as $\\sim N^{2-q}$.\nSurprisingly it was found that they depend only on $f'(|\\bf x|)$ and its\nderivatives evaluated exactly at the boundary of the droplet. To understand\nthis property, and interpolate between the two behaviors (smooth versus\nstep-like), we study the microscopic linear statistics given by $f(r) \\to\nf_N(r) = \\phi((r-\\hat r) \\sqrt{N}\/\\xi)$, which probes the fluctuations at the\nscale of the inter-particle distance. We compute the cumulants of ${\\cal L}_N$\nat large $N$ for a fixed $\\phi(u)$ at arbitrary $\\xi$. For large $\\xi$ they\nmatch the predictions for smooth functions which shows that the leading\ncontribution in that case comes from a boundary layer of size $1\/\\sqrt{N}$ near\nthe boundary of the droplet. Finally we show that the full probability\ndistribution of ${\\cal L}_N$ take two distinct large deviation forms, in the\nregime ${\\cal L}_N \\sim \\sqrt{N}$ and ${\\cal L}_N \\sim N$ respectively. We also\ndiscuss applications of our results to fermions in a rotating harmonic trap and\nto the Ginibre symplectic ensemble.",
        "Understanding how to control changes in electronic structure and related\ndynamical renormalizations by external driving fields is the key for\nunderstanding ultrafast spectroscopy and applications in electronics. Here we\nfocus on the band-gap's modulation by external electric fields and uncover the\neffect of band dispersion on the gap renormalization. We employ the Green's\nfunction formalism using the real-time Dyson expansion to account for dynamical\ncorrelations induced by photodoping. The many-body formalism captures the\ndynamics of systems with long-range interactions, carrier mobility, and\nvariable electron and hole effective mass. We also demonstrate that mean-field\nsimulations based on the Hartree-Fock Hamiltonian, which lacks dynamical\ncorrelations, yields a qualitatively incorrect picture of band-gap\nrenormalization. We find the trend that increasing effective mass, thus\ndecreasing mobility, leads to as much as a 6\\% enhancement in band-gap\nrenormalization. Further, the renormalization is strongly dependent on the\ndegree of photodoping. As the screening induced by free electrons and holes\neffectively reduces any long-range and interband interactions for highly\nexcited systems, we show that there is a specific turnover point with minimal\nband-gap. We further demonstrate that the optical gap renormalization follows\nthe same trend though its magnitude is altered by the Moss-Burstein effect.",
        "Though offering unprecedented pathways to molecular dynamics (MD) simulations\nof technologically-relevant materials and conditions, machine-learning\ninteratomic potentials (MLIPs) are typically trained for ``simple'' materials\nand properties with minor size effects. Our study of MAB phases (MABs) -\nalternating transition metal boride (MB) and group A element layers -\nexemplifies that MLIPs for complex materials can be fitted and used in a\nhigh-throughput fashion: for predicting structural and mechanical properties\nacross a large chemical\/phase\/temperature space. Considering group 4-6\ntransition metal based MABs, with A=Al and the 222, 212, and 314 type phases,\nthree MLIPs are trained and tested, including lattice and elastic constants\ncalculations at temperatures $T\\in\\{0,300,1200\\}$ K, extrapolation grade and\nenergy (force, stress) error analysis for $\\approx{3\\cdot10^6}$ ab initio MD\nsnapshots. Subsequently, nanoscale tensile tests serve to quantify upper limits\nof strength and toughness attainable in single-crystal MABs at 300~K as well as\ntheir temperature evolution. In-plane tensile deformation is characterised by\nrelatively high strength, {110}$\\langle001\\rangle$ type slipping, and failure\nby shear banding. The response to [001] loading is softer, triggers work\nhardening, and failure by kinking and layer delamination. Furthermore,\nW$_2$AlB$_2$ able to retard fracture via ripplocations and twinning from 300 up\nto 1200~K.",
        "In this paper, we introduce the elephant random walk (ERW) with memory\nconsisting of randomly selected steps from its history. It is a time-changed\nvariant of the standard elephant random walk with memory consisting of its full\nhistory. At each time point, the time changing component is the composition of\ntwo uniformly distributed independent random variables with support over all\nthe past steps. Several conditional distributional properties including the\nconditional mean increments and conditional displacement of ERW with random\nmemory are obtained. Using these conditional results, we derive the recursive\nand explicit expressions for the mean increments and mean displacement of the\nwalk.",
        "Permanent magnet tracking using the external sensor array is crucial for the\naccurate localization of wireless capsule endoscope robots. Traditional\ntracking algorithms, based on the magnetic dipole model and Levenberg-Marquardt\n(LM) algorithm, face challenges related to computational delays and the need\nfor initial position estimation. More recently proposed neural network-based\napproaches often require extensive hardware calibration and real-world data\ncollection, which are time-consuming and labor-intensive. To address these\nchallenges, we propose MobilePosenet, a lightweight neural network architecture\nthat leverages depthwise separable convolutions to minimize computational cost\nand a channel attention mechanism to enhance localization accuracy. Besides,\nthe inputs to the network integrate the sensors' coordinate information and\nrandom noise, compensating for the discrepancies between the theoretical model\nand the actual magnetic fields and thus allowing MobilePosenet to be trained\nentirely on theoretical data. Experimental evaluations conducted in a \\(90\n\\times 90 \\times 80\\) mm workspace demonstrate that MobilePosenet exhibits\nexcellent 5-DOF localization accuracy ($1.54 \\pm 1.03$ mm and $2.24 \\pm\n1.84^{\\circ}$) and inference speed (0.9 ms) against state-of-the-art methods\ntrained on real-world data. Since network training relies solely on theoretical\ndata, MobilePosenet can eliminate the hardware calibration and real-world data\ncollection process, improving the generalizability of this permanent magnet\nlocalization method and the potential for rapid adoption in different clinical\nsettings.",
        "Artificial Intelligence (AI)-driven code generation tools are increasingly\nused throughout the software development lifecycle to accelerate coding tasks.\nHowever, the security of AI-generated code using Large Language Models (LLMs)\nremains underexplored, with studies revealing various risks and weaknesses.\nThis paper analyzes the security of code generated by LLMs across different\nprogramming languages. We introduce a dataset of 200 tasks grouped into six\ncategories to evaluate the performance of LLMs in generating secure and\nmaintainable code. Our research shows that while LLMs can automate code\ncreation, their security effectiveness varies by language. Many models fail to\nutilize modern security features in recent compiler and toolkit updates, such\nas Java 17. Moreover, outdated methods are still commonly used, particularly in\nC++. This highlights the need for advancing LLMs to enhance security and\nquality while incorporating emerging best practices in programming languages.",
        "As multi-modal large language models (MLLMs) frequently exhibit errors when\nsolving scientific problems, evaluating the validity of their reasoning\nprocesses is critical for ensuring reliability and uncovering fine-grained\nmodel weaknesses. Since human evaluation is laborious and costly, prompting\nMLLMs as automated process judges has become a common practice. However, the\nreliability of these model-based judges remains uncertain. To address this, we\nintroduce ProJudgeBench, the first comprehensive benchmark specifically\ndesigned for evaluating abilities of MLLM-based process judges. ProJudgeBench\ncomprises 2,400 test cases and 50,118 step-level labels, spanning four\nscientific disciplines with diverse difficulty levels and multi-modal content.\nIn ProJudgeBench, each step is meticulously annotated by human experts for\ncorrectness, error type, and explanation, enabling a systematic evaluation of\njudges' capabilities to detect, classify and diagnose errors. Evaluation on\nProJudgeBench reveals a significant performance gap between open-source and\nproprietary models. To bridge this gap, we further propose ProJudge-173k, a\nlarge-scale instruction-tuning dataset, and a Dynamic Dual-Phase fine-tuning\nstrategy that encourages models to explicitly reason through problem-solving\nbefore assessing solutions. Both contributions significantly enhance the\nprocess evaluation capabilities of open-source models. All the resources will\nbe released to foster future research of reliable multi-modal process\nevaluation.",
        "Gas sampling methods have been crucial for the advancement of combustion\nscience, enabling analysis of reaction kinetics and pollutant formation.\nHowever, the measured composition can deviate from the true one because of the\npotential residual reactions in the sampling probes. This study formulates the\ninitial composition estimation in stiff chemically reacting systems as a\nBayesian inference problem, solved using the No-U-Turn Sampler (NUTS).\nInformation loss arises from the restriction of system dynamics by low\ndimensional attracting manifold, where constrained evolution causes initial\nperturbations to decay or vanish in fast eigen-directions in composition space.\nThis study systematically investigates the initial value inference in\ncombustion systems and successfully validates the methodological framework in\nthe Robertson toy system and hydrogen autoignition. Furthermore, a gas sample\ncollected from a one-dimensional hydrogen diffusion flame is analyzed to\ninvestigate the effect of frozen temperature on information loss. The research\nhighlights the importance of species covariance information from observations\nin improving estimation accuracy and identifies how the rank reduction in the\nsensitivity matrix leads to inference failures. Critical failure times for\nspecies inference in the Robertson and hydrogen autoignition systems are\nanalyzed, providing insights into the limits of inference reliability and its\nphysical significance.",
        "Backdoor attacks embed hidden functionalities in deep neural networks (DNN),\ntriggering malicious behavior with specific inputs. Advanced defenses monitor\nanomalous DNN inferences to detect such attacks. However, concealed backdoors\nevade detection by maintaining a low pre-deployment attack success rate (ASR)\nand restoring high ASR post-deployment via machine unlearning. Existing\nconcealed backdoors are often constrained by requiring white-box or black-box\naccess or auxiliary data, limiting their practicality when such access or data\nis unavailable. This paper introduces ReVeil, a concealed backdoor attack\ntargeting the data collection phase of the DNN training pipeline, requiring no\nmodel access or auxiliary data. ReVeil maintains low pre-deployment ASR across\nfour datasets and four trigger patterns, successfully evades three popular\nbackdoor detection methods, and restores high ASR post-deployment through\nmachine unlearning.",
        "In AI-facilitated teaching, leveraging various query styles to interpret\nabstract text descriptions is crucial for ensuring high-quality teaching.\nHowever, current retrieval models primarily focus on natural text-image\nretrieval, making them insufficiently tailored to educational scenarios due to\nthe ambiguities in the retrieval process. In this paper, we propose a diverse\nexpression retrieval task tailored to educational scenarios, supporting\nretrieval based on multiple query styles and expressions. We introduce the STEM\nEducation Retrieval Dataset (SER), which contains over 24,000 query pairs of\ndifferent styles, and the Uni-Retrieval, an efficient and style-diversified\nretrieval vision-language model based on prompt tuning. Uni-Retrieval extracts\nquery style features as prototypes and builds a continuously updated Prompt\nBank containing prompt tokens for diverse queries. This bank can updated during\ntest time to represent domain-specific knowledge for different subject\nretrieval scenarios. Our framework demonstrates scalability and robustness by\ndynamically retrieving prompt tokens based on prototype similarity, effectively\nfacilitating learning for unknown queries. Experimental results indicate that\nUni-Retrieval outperforms existing retrieval models in most retrieval tasks.\nThis advancement provides a scalable and precise solution for diverse\neducational needs.",
        "Some frustrated magnets exhibit a huge hysteresis called \"global hysteresis\n(GH)\", where the magnetic plateaus appearing in the increasing field process\nare skipped in the decreasing field process from the high magnetic field state.\nIn this paper, we focused on the frustrated magnet DyRu2Si2 and measured\nmagnetization relaxations from two plateau states inside the GH loop, the\nphases III and IV, and investigated the phase transitions into them. As a\nresult of the relaxation measurements, no relaxation is observed in the phase\nIII, whereas long-time relaxations of more than 105 sec are observed at the\nphase IV plateau. Moreover, a Mpemba-effect-like relaxation phenomenon where\nthe relaxation from an initial state prepared in the zero-field-cooled\ncondition overtakes that from an initial state prepared in the field-cooled\ncondition is observed. These results indicate that the phase IV is the\nnon-ergodic state with a complex free-energy landscape with multiple local\nminima, while the phase III has a simple free energy structure. Therefore, the\nIII-IV phase transition is considered to be the ergodic to non-ergodic phase\ntransition. Although this type of phase transition typically occurs in random\nglassy systems, the phase IV in DyRu2Si2 has a regular long-range ordered\nmagnetic structure and yet exhibits non-ergodic properties, which is highly\nnontrivial. Our findings open the possibility of observing non-ergodic states\nin frustrated magnets with regular long-range orders.",
        "The paper explores the challenges of regression analysis in evolving data\nstreams, an area that remains relatively underexplored compared to\nclassification. We propose a standardized evaluation process for regression and\nprediction interval tasks in streaming contexts. Additionally, we introduce an\ninnovative drift simulation strategy capable of synthesizing various drift\ntypes, including the less-studied incremental drift. Comprehensive experiments\nwith state-of-the-art methods, conducted under the proposed process, validate\nthe effectiveness and robustness of our approach.",
        "Moir\\'{e} systems such as magic-angle twisted bilayer graphene have attracted\nsignificant attention due to their ability to host correlated phenomena\nincluding superconductivity and strongly correlated insulating states. By\ndefining the single-particle Green's function in the band basis, we\nsystematically develop a many-body perturbation theory framework to address\ncorrelations beyond the usual mean-field Hartree-Fock approaches. As a specific\nexample, we first analyze twisted bilayer graphene within the Hartree-Fock\napproximation. We derive analytical solutions for symmetry-breaking states at\ninteger fillings and the finite-temperature metal-insulator transition that\nclosely match previously known numerical results in the literature. Moving\nbeyond Hartree-Fock, we incorporate self-consistent GW corrections\ndemonstrating that first-order diagrams significantly overestimate the\nfilling-dependent fluctuations in the electronic compressibility. This\nframework provides a comprehensive pathway for exploring strong electronic\ncorrelations in moir\\'{e} systems beyond mean-field, giving new insights into\nthe interplay of symmetry breaking and electron correlations.",
        "A universal prediction of quantum gravity is that the dynamics of general\nrelativity is augmented by interactions that are of higher order in the\nspacetime curvature. Numerical explorations indicate that such terms may have a\ndrastic impact on black hole-type solutions by modifying the geometry close to\nthe would-be event horizon in a substantial way. In this work, we perform the\nfirst systematic investigation of this blow-up mechanism within general\nrelativity supplemented by quadratic gravity terms, the Goroff-Sagnotti\ncounterterm, the combination of the two, and Einstein-Cubic Gravity. By\nstudying linear perturbations of the Schwarzschild solution close to the\nSchwarzschild radius, we discover the following picture: the higher-derivative\nterms giving rise to extra degrees of freedom play a distinguished role. Once\ncouplings associated with these terms enter the solution in the asymptotically\nflat region, a blow-up mechanism removes the event horizon and one deals with\neither a naked singularity or a wormhole. We believe that this finding is\nhighly relevant when constraining the coefficients appearing in the Wilsonian\ndescription of gravity by observations.",
        "The growing disparity between CPU core counts and available memory bandwidth\nhas intensified memory contention in servers. This particularly affects highly\nparallelizable applications, which must achieve efficient cache utilization to\nmaintain performance as CPU core counts grow. Optimizing cache utilization,\nhowever, is complex for recent chiplet-based CPUs, whose partitioned L3 caches\nlead to varying latencies and bandwidths, even within a single NUMA domain.\nClassical NUMA optimizations and task scheduling approaches unfortunately fail\nto address the performance issues of chiplet-based CPUs.\n  We describe Adaptive Runtime system for Chiplet-Aware Scheduling (ARCAS), a\nnew runtime system designed for chiplet-based CPUs. ARCAS combines\nchiplet-aware task scheduling heuristics, hardware-aware memory allocation, and\nfine-grained performance monitoring to optimize workload execution. It\nimplements a lightweight concurrency model that combines user-level thread\nfeatures-such as individual stacks, per-task scheduling, and state\nmanagement-with coroutine-like behavior, allowing tasks to suspend and resume\nexecution at defined points while efficiently managing task migration across\nchiplets. Our evaluation across diverse scenarios shows ARCAS's effectiveness\nfor optimizing the performance of memory-intensive parallel applications.",
        "As a continuation of our earlier paper, we offer a new approach to\nmurmurations and Sato-Tate laws for higher rank zetas of elliptic curves. Our\napproach here does not depend on the Riemann hypothesis for the so-called\na-invariant in rank n>2 even for the Sato-Tate law, rather, on a much refined\nstructure, a similar version of which was already observed by Zagier and the\nsenior author when the rank n Riemann hypothesis was established. Namely,\ninstead of the rank n Riemann hypothesis bounds, we use much stronger\nasymptotic bounds. Accordingly, rank n Sato-Tate law can be established and\nrank n murmuration can be formulated equally well, similar to the corresponding\nstructures in the abelian framework for Artin zetas of elliptic curves.",
        "Colored gravity, based on $\\text{U}(1,3)$ symmetry, emerges naturally in the\ncomplexification of Lorentzian manifolds and integrates U(1) electromagnetism\nas a subcase. This work explores the viability of also including strong and\nelectroweak interactions under the $\\text{U}(1,3)$ gauge group of colored\ngravity. We identify specific generators linked to leptonic and quark\ninteractions and embed the standard Higgs mechanism. Crucially, the weak mixing\nangle ($\\sin^2\\theta_W$) is predicted to exhibit about $\\sim0.231$ for\nlepton-lepton interactions (close to observations) and $\\sim0.222$ for\nhadron-lepton interactions, which is in 3$\\sigma$ tension with some\nobservations. These findings open pathways for reconciling experimental data\nwith colored gravity and suggest avenues for quantum correction studies.",
        "Using as a narrative theme the example of Darwin's finches, a microscopic\nagent-based model is introduces to study sympatric speciation as a result of\ncompetition for resources in the same ecological niche. Varying competition\namong individuals and resource distribution, the model exhibits some of the\nmain features of evolutionary branching processing. The model can be extended\nto include spatial effects, different genetic loci, sexual mating and\nrecombination, etc.",
        "In real-world applications where computational resources are limited,\neffectively integrating visual and textual information for Visual Question\nAnswering (VQA) presents significant challenges. This paper investigates the\nperformance of traditional models under computational constraints, focusing on\nenhancing VQA performance, particularly for numerical and counting questions.\nWe evaluate models based on Bidirectional GRU (BidGRU), GRU, Bidirectional LSTM\n(BidLSTM), and Convolutional Neural Networks (CNN), analyzing the impact of\ndifferent vocabulary sizes, fine-tuning strategies, and embedding dimensions.\nExperimental results show that the BidGRU model with an embedding dimension of\n300 and a vocabulary size of 3000 achieves the best overall performance without\nthe computational overhead of larger models. Ablation studies emphasize the\nimportance of attention mechanisms and counting information in handling complex\nreasoning tasks under resource limitations. Our research provides valuable\ninsights for developing more efficient VQA models suitable for deployment in\nenvironments with limited computational capacity.",
        "Compression techniques such as pruning and quantization offer a solution for\nmore efficient deployment of language models (LMs), albeit with small\nperformance drops in benchmark performance. However, general-purpose LM\ncompression methods can negatively affect performance in specialized domains\n(e.g. biomedical or legal). Recent work has sought to address this, yet\nrequires computationally expensive full-parameter fine-tuning. To this end, we\npropose cross-calibration, a novel training-free approach for improving the\ndomain performance of compressed LMs. Our approach effectively leverages\nHessian-based sensitivity to identify weights that are influential for both\nin-domain and general performance. Through extensive experimentation, we\ndemonstrate that cross-calibration substantially outperforms existing\napproaches on domain-specific tasks, without compromising general performance.\nNotably, these gains come without additional computational overhead, displaying\nremarkable potential towards extracting domain-specialized compressed models\nfrom general-purpose LMs.",
        "In this paper, we propose a novel framework for efficiently and accurately\nestimating Lipschitz constants in hybrid quantum-classical decision models. Our\napproach integrates classical neural network with quantum variational circuits\nto address critical issues in learning theory such as fairness verification,\nrobust training, and generalization.\n  By a unified convex optimization formulation, we extend existing classical\nmethods to capture the interplay between classical and quantum layers. This\nintegrated strategy not only provide a tight bound on the Lipschitz constant\nbut also improves computational efficiency with respect to the previous\nmethods.",
        "Let $G$ be a connected reductive group over a $p$-adic local field $F$.\nR\\'emy-Thuillier-Werner constructed embeddings of the (reduced) Bruhat-Tits\nbuilding $\\mathcal{B}(G,F)$ into the Berkovich spaces associated to suitable\nflag varieties of $G$, generalizing the work of Berkovich in split case. They\ndefined compactifications of $\\mathcal{B}(G,F)$ by taking closure inside these\nBerkovich flag varieties. We show that, in the setting of a basic local Shimura\ndatum, the R\\'emy-Thuillier-Werner embedding factors through the associated\n$p$-adic Hodge-Tate period domain. Moreover, we compare the boundaries of the\nBerkovich compactification of $\\mathcal{B}(G,F)$ with non basic Newton strata.\nIn the case of $\\mathrm{GL}_n$ and the cocharacter $\\mu=(1^d, 0^{n-d})$ for an\ninteger $d$ which is coprime to $n$, we further construct a continuous\nretraction map from the $p$-adic period domain to the building. This reveals\nnew information on these $p$-adic period domains, which share many similarities\nwith the Drinfeld spaces.",
        "In this note we consider the big algebra recently introduced by Hausel for\nthe $\\mathrm{GL}_n$-action on the coordinate ring of the matrix space\n$\\operatorname{Mat}(n,r)$. In particular, we obtain explicit formulas for the\nbig algebra generators in terms of differential operators with polynomial\ncoefficients. We show that big algebras in type $A$ are commutative and relate\nthem to the Bethe subalgebra in the Yangian $\\mathrm{Y}(\\mathfrak{gl}_{n})$. We\napply these results to big algebras of symmetric powers of the standard\nrepresentation of $\\mathrm{GL}_n$.",
        "High harmonic generation from Pr$_{0.8}$Ca$_{0.2}$MnO$_{3}$ was investigated\nacross a high-temperature paramagnetic phase and a low-temperature\nferromagnetic phase. As the temperature decreases, the harmonic intensity\ngradually increases in the paramagnetic phase like that in different\ncomposition material Pr$_{0.6}$Ca$_{0.4}$MnO$_{3}$. However, it turns to a\ndecrease in the ferromagnetic phase. We propose a possible interpretation of\nthe anomaly around the ferromagnetic transition temperature considering the\nthermal fluctuation of orbital order and the metal-insulator phase separation\nin the ferromagnetic insulating phase.",
        "The Consensual Assessment Technique (CAT) evaluates creativity through\nholistic expert judgments. We investigate the use of two advanced Large\nLanguage Models (LLMs), Claude-3-Opus and GPT-4o, to evaluate poetry by a\nmethodology inspired by the CAT. Using a dataset of 90 poems, we found that\nthese LLMs can surpass the results achieved by non-expert human judges at\nmatching a ground truth based on publication venue, particularly when assessing\nsmaller subsets of poems. Claude-3-Opus exhibited slightly superior performance\nthan GPT-4o. We show that LLMs are viable tools for accurately assessing\npoetry, paving the way for their broader application into other creative\ndomains.",
        "By means of Gross-Pitaevskii theory we investigate the possibility of wetting\nphase transition in three-component Bose-Einstein condensates within the strong\nsegregation between components 1 and 2. Third component plays the role of a\nsurfactant, which can wet the interface formed by components 1 and 2. The case\nof symmetry of components 1 and 2, the equation for nucleation line and wetting\nphase diagram are studied. By linearizing the set of three-coupled\nGross-Pitaevskii equations, the wave functions of the components are found. An\nanalytical approximation for the surfactant thickness in the prewetting phase\nis presented. The results indicate that the surfactant thickness in the\nprewtting phase varies linearly with the chemical potential ratio in the\nlogarithmic scale.",
        "Bayesian optimization is a widely used method for optimizing expensive\nblack-box functions, with Expected Improvement being one of the most commonly\nused acquisition functions. In contrast, information-theoretic acquisition\nfunctions aim to reduce uncertainty about the function's optimum and are often\nconsidered fundamentally distinct from EI. In this work, we challenge this\nprevailing perspective by introducing a unified theoretical framework,\nVariational Entropy Search, which reveals that EI and information-theoretic\nacquisition functions are more closely related than previously recognized. We\ndemonstrate that EI can be interpreted as a variational inference approximation\nof the popular information-theoretic acquisition function, named Max-value\nEntropy Search. Building on this insight, we propose VES-Gamma, a novel\nacquisition function that balances the strengths of EI and MES. Extensive\nempirical evaluations across both low- and high-dimensional synthetic and\nreal-world benchmarks demonstrate that VES-Gamma is competitive with\nstate-of-the-art acquisition functions and in many cases outperforms EI and\nMES."
      ]
    }
  },
  {
    "id":2412.00252,
    "research_type":"basic",
    "start_id":"b15",
    "start_title":"Analytic perturbation theory for matrices and operators",
    "start_abstract":"Perturbation theory is the study of the behavior of mathematical objects under the influence of perturbations. It is not a well-defined mathematical topic with specific objects and methods, but rather a method of investigation. In this book, perturbation theory will be developed for linear operators. First, inter-est focuses on the properties of spectral objects, such as eigenvalues, eigenprojections, eigenvectors and Jordan vectors, under perturbation of the underlying operator. This study encompasses some difficult problems. On the one hand, variations of the spectral objects need to be calculated quantitatively. The spectral objects are assumed known for the unperturbed operator, the determination (or at least the approximation) of the spectral objects for the perturbed operator is at issue. This is the starting point for the perturbation theory of L. RAYLEIGH [1] (see also R. COURANT and D. HTT.BEBT [1, p. 296 sqq]). On the other hand, the spectral objects often undergo abrupt qualitative changes, even in the case of small perturbations. These changes cause significant complications. Usually, the behavior of the spectral objects depends strongly on the assumptions about the nature of the perturbation. For example, one can assume continuity, differ-entiability, smoothness (i.e. arbitrary differentiability) or analyticity. In the following discussion, only the case of analytic (holomorphic) perturbations will be investigated, even if this strong restriction is applied, the problems remain difficult enough. On the one hand, solving problems of perturbation theory is of conceptual interest. The study of intrinsic spectral properties of a linear operator undergoing perturbation leads to deeper insights and understanding of the structure of the operator. It also leads to the development of new tools for further investigations. On the other hand, applications (inside and outside of mathematics) lead to new questions in perturbation theory. One of the first calculations of perturbation theory was given by L. Rayleigh, who determined the eigenfrequencies and eigenmodes of an oscillating string, fixed at x = 0 and x = n, whose elasticity modulus is constant and whose mass density Q(X) has only a small deviation from a constant value for all x, 0 ^ x 5S n. (That is, the density o(x) is of the form Q(X) = p0 + ea(x), where a(x) is a given function and where e is a small perturbation parameter.) Actually, as this example indicates, the starting point for the development of per-turbation theory was the study of perturbations of spectral objects (eigenvalues and eigenvectors) for concrete classes of operators, for example, Fredholm integral operators or Sturm-Liouville differential operators (for example, see L. LIECHTENSTEIN [1]).",
    "start_categories":[
      "Analytic Perturbation Theory "
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b5"
      ],
      "title":[
        "Localization and landscape functions for graph laplacians"
      ],
      "abstract":[
        "We discuss explicit landscape functions for quantum graphs. By a 'landscape function' Upsilon(x) we mean a function that controls the localization properties of normalized eigenfunctions psi(x) through a pointwise inequality of the form |psi(x)| le Upsilon(x). The ideal Upsilon is a function that a) responds to the potential energy V(x) and to the structure of the graph in some formulaic way; b) is small in examples where eigenfunctions are suppressed by the tunneling effect, and c) relatively large in regions where eigenfunctions may - or may not - be concentrated, as observed in specific examples. It turns out that the connectedness of a graph can present a barrier to the existence of universal landscape functions in the high-energy r\u00e9gime, as we show with simple examples. We therefore apply different methods in different r\u00e9gimes determined by the values of the potential energy V(x) and the eigenvalue parameter E."
      ],
      "categories":[
        "Anderson Localization"
      ]
    },
    "list":{
      "title":[
        "GSVC: Efficient Video Representation and Compression Through 2D Gaussian\n  Splatting",
        "On finite approximations of transitive graphs",
        "Solvable Dynamics of Self-Supervised Word Embeddings and the Emergence\n  of Analogical Reasoning",
        "How to Tune a Multilingual Encoder Model for Germanic Languages: A Study\n  of PEFT, Full Fine-Tuning, and Language Adapters",
        "Dynamic realization of emergent high-dimensional optical vortices",
        "Rigidity results for free boundary hypersurfaces in initial data sets\n  with boundary",
        "Int2Int: a framework for mathematics with transformers",
        "Eliciting Language Model Behaviors with Investigator Agents",
        "Applying the Liouville-Lanczos Method of Time-Dependent\n  Density-Functional Theory to Warm Dense Matter",
        "TigerVector: Supporting Vector Search in Graph Databases for Advanced\n  RAGs",
        "qReduMIS: A Quantum-Informed Reduction Algorithm for the Maximum\n  Independent Set Problem",
        "Algebra and geometry of ASM weak order",
        "BPS invariants from framed links",
        "Overcoming Semantic Dilution in Transformer-Based Next Frame Prediction",
        "Non-convergence to the optimal risk for Adam and stochastic gradient\n  descent optimization in the training of deep neural networks",
        "On the Role of Individual Differences in Current Approaches to\n  Computational Image Aesthetics",
        "UAV-Enabled IoT Networks: A SWIPT Energy Harvesting Architecture with\n  Relay Support for Disaster Response",
        "JWST's little red dots: an emerging population of young, low-mass AGN\n  cocooned in dense ionized gas",
        "Understanding the Quality-Diversity Trade-off in Diffusion Language\n  Models",
        "Advanced Weakly-Supervised Formula Exploration for Neuro-Symbolic\n  Mathematical Reasoning",
        "Cooperative Behavior in Pre-State Societies: An Agent-Based Approach of\n  the Aksum Civilization",
        "Improved SED-Fitting Assumptions Result in Inside-Out Quenching at\n  $z\\sim0.5$ and Quenching at All Radii Simultaneously at $z\\sim1$",
        "Displacement-Sparse Neural Optimal Transport",
        "Quantifying Correlations of Machine Learning Models",
        "Machine learning the vanishing order of rational L-functions",
        "Fairness Aware Reinforcement Learning via Proximal Policy Optimization",
        "Gravitational Effects of a Small Primordial Black Hole Passing Through\n  the Human Body",
        "An Ontology for Social Determinants of Education (SDoEd) based on\n  Human-AI Collaborative Approach",
        "Benchmarking MedMNIST dataset on real quantum hardware"
      ],
      "abstract":[
        "3D Gaussian splats have emerged as a revolutionary, effective, learned\nrepresentation for static 3D scenes. In this work, we explore using 2D Gaussian\nsplats as a new primitive for representing videos. We propose GSVC, an approach\nto learning a set of 2D Gaussian splats that can effectively represent and\ncompress video frames. GSVC incorporates the following techniques: (i) To\nexploit temporal redundancy among adjacent frames, which can speed up training\nand improve the compression efficiency, we predict the Gaussian splats of a\nframe based on its previous frame; (ii) To control the trade-offs between file\nsize and quality, we remove Gaussian splats with low contribution to the video\nquality; (iii) To capture dynamics in videos, we randomly add Gaussian splats\nto fit content with large motion or newly-appeared objects; (iv) To handle\nsignificant changes in the scene, we detect key frames based on loss\ndifferences during the learning process. Experiment results show that GSVC\nachieves good rate-distortion trade-offs, comparable to state-of-the-art video\ncodecs such as AV1 and VVC, and a rendering speed of 1500 fps for a 1920x1080\nvideo.",
        "In this note we answer a question of Johannes Carmesin, which was circulated\nat the Oberwolfach Workshop on \"Graph Theory\" in January 2025. We provide a\nunimodular, locally finite, and vertex-transitive graph without any perfect\nfinite $r$-local model for $r \\in \\mathbb N$ large enough.",
        "The remarkable success of large language models relies on their ability to\nimplicitly learn structured latent representations from the pretraining corpus.\nAs a simpler surrogate for representation learning in language modeling, we\nstudy a class of solvable contrastive self-supervised algorithms which we term\nquadratic word embedding models. These models resemble the word2vec algorithm\nand perform similarly on downstream tasks. Our main contributions are\nanalytical solutions for both the training dynamics (under certain\nhyperparameter choices) and the final word embeddings, given in terms of only\nthe corpus statistics. Our solutions reveal that these models learn orthogonal\nlinear subspaces one at a time, each one incrementing the effective rank of the\nembeddings until model capacity is saturated. Training on WikiText, we find\nthat the top subspaces represent interpretable concepts. Finally, we use our\ndynamical theory to predict how and when models acquire the ability to complete\nanalogies.",
        "This paper investigates the optimal use of the multilingual encoder model\nmDeBERTa for tasks in three Germanic languages -- German, Swedish, and\nIcelandic -- representing varying levels of presence and likely data quality in\nmDeBERTas pre-training data. We compare full fine-tuning with the\nparameter-efficient fine-tuning (PEFT) methods LoRA and Pfeiffer bottleneck\nadapters, finding that PEFT is more effective for the higher-resource language,\nGerman. However, results for Swedish and Icelandic are less consistent. We also\nobserve differences between tasks: While PEFT tends to work better for question\nanswering, full fine-tuning is preferable for named entity recognition.\nInspired by previous research on modular approaches that combine task and\nlanguage adapters, we evaluate the impact of adding PEFT modules trained on\nunstructured text, finding that this approach is not beneficial.",
        "The dimensionality of vortical structures has recently been extended beyond\ntwo dimensions, providing higher-order topological characteristics and\nrobustness for high-capacity information processing and turbulence control. The\ngeneration of high-dimensional vortical structures has mostly been demonstrated\nin classical systems through the complex interference of fluidic, acoustic, or\nelectromagnetic waves. However, natural materials rarely support three- or\nhigher-dimensional vortical structures and their physical interactions. Here,\nwe present a high-dimensional gradient thickness optical cavity (GTOC) in which\nthe optical coupling of planar metal-dielectric multilayers implements\ntopological interactions across multiple dimensions. Topological interactions\nin high-dimensional GTOC construct non-trivial topological phases, which induce\nhigh-dimensional vortical structures in generalized parameter space in three,\nfour dimensions, and beyond. These emergent high-dimensional vortical\nstructures are observed under electro-optic tomography as optical vortex\ndynamics in two-dimensional real-space, employing the optical thicknesses of\nthe dielectric layers as synthetic dimensions. We experimentally demonstrate\nemergent vortical structures, optical vortex lines and vortex rings, in a\nthree-dimensional generalized parameter space and their topological\ntransitions. Furthermore, we explore four-dimensional vortical structures,\ntermed optical vortex sheets, which provide the programmability of real-space\noptical vortex dynamics. Our findings hold significant promise for emulating\nhigh-dimensional physics and developing active topological photonic devices.",
        "In this work, we present several rigidity results for compact free boundary\nhypersurfaces in initial data sets with boundary. Specifically, in the first\npart of the paper, we extend the local splitting theorems from [G. J. Galloway\nand H. C. Jang, Some scalar curvature warped product splitting theorems, Proc.\nAm. Math. Soc. 148 (2020), no. 6, 2617-2629] to the setting of manifolds with\nboundary. To achieve this, we build on the approach of the original paper,\nutilizing results on free boundary marginally outer trapped surfaces (MOTS)\napplied to specific initial data sets. In the second part, we extend the main\nresults from [A. Barros and C. Cruz, Free boundary hypersurfaces with\nnon-positive Yamabe invariant in mean convex manifolds, J. Geom. Anal. 30\n(2020), no. 4, 3542-3562] to the context of free boundary MOTS in initial data\nsets with boundary.",
        "This paper documents Int2Int, an open source code base for using transformers\non problems of mathematical research, with a focus on number theory and other\nproblems involving integers. Int2Int is a complete PyTorch implementation of a\ntransformer architecture, together with training and evaluation loops, and\nclasses and functions to represent, generate and decode common mathematical\nobjects. Ancillary code for data preparation, and Jupyter Notebooks for\nvisualizing experimental results are also provided. This document presents the\nmain features of Int2Int, serves as its user manual, and provides guidelines on\nhow to extend it. Int2Int is released under the MIT licence, at\nhttps:\/\/github.com\/FacebookResearch\/Int2Int.",
        "Language models exhibit complex, diverse behaviors when prompted with\nfree-form text, making it difficult to characterize the space of possible\noutputs. We study the problem of behavior elicitation, where the goal is to\nsearch for prompts that induce specific target behaviors (e.g., hallucinations\nor harmful responses) from a target language model. To navigate the\nexponentially large space of possible prompts, we train investigator models to\nmap randomly-chosen target behaviors to a diverse distribution of outputs that\nelicit them, similar to amortized Bayesian inference. We do this through\nsupervised fine-tuning, reinforcement learning via DPO, and a novel Frank-Wolfe\ntraining objective to iteratively discover diverse prompting strategies. Our\ninvestigator models surface a variety of effective and human-interpretable\nprompts leading to jailbreaks, hallucinations, and open-ended aberrant\nbehaviors, obtaining a 100% attack success rate on a subset of AdvBench\n(Harmful Behaviors) and an 85% hallucination rate.",
        "Ab initio modeling of dynamic structure factors (DSF) and related density\nresponse properties in the warm dense matter (WDM) regime is a challenging\ncomputational task. The DSF, convolved with a probing X-ray beam and instrument\nfunction, is measured in X-ray Thomson scattering (XRTS) experiments, which\nallows for the study of electronic structure properties at the microscopic\nlevel. Among the various ab initio methods, linear response time-dependent\ndensity functional theory (LR-TDDFT) is a key framework for simulating the DSF.\nThe standard approach in LR-TDDFT for computing the DSF relies on the orbital\nrepresentation. A significant drawback of this method is the unfavorable\nscaling of the number of required empty bands as the wavenumber increases,\nmaking LR-TDDFT impractical for modeling XRTS measurements over large energy\nscales, such as in backward scattering geometry. We consider and test an\nalternative approach that employs the Liouville-Lanczos (LL) method for\nsimulating the DSF. This approach does not require empty states and allows the\nDSF at large momentum transfer values and over a broad frequency range to be\naccessed. We compare the results obtained from the LL method with those from\nthe standard LR-TDDFT within the projector augmented-wave formalism for\nisochorically heated aluminum and warm dense hydrogen. Additionally, we utilize\nexact path integral Monte Carlo (PIMC) results for the imaginary-time\ndensity-density correlation function (ITCF) of warm dense hydrogen to\nrigorously benchmark the LL approach. We discuss the application of the LL\nmethod for calculating DSFs and ITCFs at different wavenumbers, the effects of\npseudopotentials, and the role of Lorentzian smearing. The successful\nvalidation of the LL method under WDM conditions makes it a valuable addition\nto the ab initio simulation landscape, supporting experimental efforts and\nadvancing WDM theory.",
        "In this paper, we introduce TigerVector, a system that integrates vector\nsearch and graph query within TigerGraph, a Massively Parallel Processing (MPP)\nnative graph database. We extend the vertex attribute type with the embedding\ntype. To support fast vector search, we devise an MPP index framework that\ninteroperates efficiently with the graph engine. The graph query language GSQL\nis enhanced to support vector type expressions and enable query compositions\nbetween vector search results and graph query blocks. These advancements\nelevate the expressive power and analytical capabilities of graph databases,\nenabling seamless fusion of unstructured and structured data in ways previously\nunattainable. Through extensive experiments, we demonstrate TigerVector's\nhybrid search capability, scalability, and superior performance compared to\nother graph databases (including Neo4j and Amazon Neptune) and a highly\noptimized specialized vector database (Milvus). TigerVector was integrated into\nTigerGraph v4.2, the latest release of TigerGraph, in December 2024.",
        "We propose and implement a quantum-informed reduction algorithm for the\nmaximum independent set problem that integrates classical kernelization\ntechniques with information extracted from quantum devices. Our larger\nframework consists of dedicated application, algorithm, and hardware layers,\nand easily generalizes to the maximum weight independent set problem. In this\nhybrid quantum-classical framework, which we call qReduMIS, the quantum\ncomputer is used as a co-processor to inform classical reduction logic about\nfrozen vertices that are likely (or unlikely) to be in large independent sets,\nthereby opening up the reduction space after removal of targeted subgraphs. We\nsystematically assess the performance of qReduMIS based on experiments with up\nto 231 qubits run on Rydberg quantum hardware available through Amazon Braket.\nOur experiments show that qReduMIS can help address fundamental performance\nlimitations faced by a broad set of (quantum) solvers including Rydberg quantum\ndevices. We outline implementations of qReduMIS with alternative platforms,\nsuch as superconducting qubits or trapped ions, and we discuss potential future\nextensions.",
        "Much of modern Schubert calculus is centered on Schubert varieties in the\ncomplete flag variety and on their classes in its integral cohomology ring.\nUnder the Borel isomorphism, these classes are represented by distinguished\npolynomials called Schubert polynomials, introduced by Lascoux and\nSch\\\"utzenberger.\n  Knutson and Miller showed that Schubert polynomials are multidegrees of\nmatrix Schubert varieties, affine varieties introduced by Fulton, which are\nclosely related to Schubert varieties. Many roads to studying Schubert\npolynomials pass through unions and intersections of matrix Schubert varieties.\nThe third author showed that the natural indexing objects of arbitrary\nintersections of matrix Schubert varieties are alternating sign matrices\n(ASMs). Every ASM variety is expressible as a union of matrix Schubert\nvarieties.\n  Many fundamental algebro-geometric invariants (e.g., codimension, degree, and\nCastelnuovo--Mumford regularity) are well understood combinatorially for matrix\nSchubert varieties, substantially via the combinatorics of strong Bruhat order\non $S_n$. The extension of strong order to ASM(n), the set of $n \\times n$\nASMs, has so far not borne as much algebro-geometric fruit for ASM varieties.\n  Hamaker and Reiner proposed an extension of weak Bruhat order from $S_n$ to\nASM(n), which they studied from a combinatorial perspective. In the present\npaper, we place this work on algebro-geometric footing. We use weak order on\nASMs to give a characterization of codimension of ASM varieties. We also show\nthat weak order operators commute with K-theoretic divided difference operators\nand that they satisfy the same derivative formula that facilitated the first\ngeneral combinatorial computation of Castelnuovo--Mumford regularity of matrix\nSchubert varieties. Finally, we build from these results to generalizations\nthat apply to arbitrary unions of matrix Schubert varieties.",
        "In this article, we investigate the BPS invariants associated with framed\nlinks. We extend the relationship between the algebraic curve (i.e. dual\n$A$-polynomial) and the BPS invariants of a knot investigated in \\cite{GKS} to\nthe case of a framed knot. With the help of the framing change formula for the\ndual $A$-polynomial of a framed knot, we give several explicit formulas for the\nextremal $A$-polynomials and the BPS invariants of framed knots. As to the\nframed links, we present several numerical calculations for the Ooguri-Vafa\ninvariants and BPS invariants for framed Whitehead links and Borromean rings\nand verify the integrality property for them.",
        "Next-frame prediction in videos is crucial for applications such as\nautonomous driving, object tracking, and motion prediction. The primary\nchallenge in next-frame prediction lies in effectively capturing and processing\nboth spatial and temporal information from previous video sequences. The\ntransformer architecture, known for its prowess in handling sequence data, has\nmade remarkable progress in this domain. However, transformer-based next-frame\nprediction models face notable issues: (a) The multi-head self-attention (MHSA)\nmechanism requires the input embedding to be split into $N$ chunks, where $N$\nis the number of heads. Each segment captures only a fraction of the original\nembeddings information, which distorts the representation of the embedding in\nthe latent space, resulting in a semantic dilution problem; (b) These models\npredict the embeddings of the next frames rather than the frames themselves,\nbut the loss function based on the errors of the reconstructed frames, not the\npredicted embeddings -- this creates a discrepancy between the training\nobjective and the model output. We propose a Semantic Concentration Multi-Head\nSelf-Attention (SCMHSA) architecture, which effectively mitigates semantic\ndilution in transformer-based next-frame prediction. Additionally, we introduce\na loss function that optimizes SCMHSA in the latent space, aligning the\ntraining objective more closely with the model output. Our method demonstrates\nsuperior performance compared to the original transformer-based predictors.",
        "Despite the omnipresent use of stochastic gradient descent (SGD) optimization\nmethods in the training of deep neural networks (DNNs), it remains, in\nbasically all practically relevant scenarios, a fundamental open problem to\nprovide a rigorous theoretical explanation for the success (and the\nlimitations) of SGD optimization methods in deep learning. In particular, it\nremains an open question to prove or disprove convergence of the true risk of\nSGD optimization methods to the optimal true risk value in the training of\nDNNs. In one of the main results of this work we reveal for a general class of\nactivations, loss functions, random initializations, and SGD optimization\nmethods (including, for example, standard SGD, momentum SGD, Nesterov\naccelerated SGD, Adagrad, RMSprop, Adadelta, Adam, Adamax, Nadam, Nadamax, and\nAMSGrad) that in the training of any arbitrary fully-connected feedforward DNN\nit does not hold that the true risk of the considered optimizer converges in\nprobability to the optimal true risk value. Nonetheless, the true risk of the\nconsidered SGD optimization method may very well converge to a strictly\nsuboptimal true risk value.",
        "Image aesthetic assessment (IAA) evaluates image aesthetics, a task\ncomplicated by image diversity and user subjectivity. Current approaches\naddress this in two stages: Generic IAA (GIAA) models estimate mean aesthetic\nscores, while Personal IAA (PIAA) models adapt GIAA using transfer learning to\nincorporate user subjectivity. However, a theoretical understanding of transfer\nlearning between GIAA and PIAA, particularly concerning the impact of group\ncomposition, group size, aesthetic differences between groups and individuals,\nand demographic correlations, is lacking. This work establishes a theoretical\nfoundation for IAA, proposing a unified model that encodes individual\ncharacteristics in a distributional format for both individual and group\nassessments. We show that transferring from GIAA to PIAA involves\nextrapolation, while the reverse involves interpolation, which is generally\nmore effective for machine learning. Experiments with varying group\ncompositions, including sub-sampling by group size and disjoint demographics,\nreveal significant performance variation even for GIAA, indicating that mean\nscores do not fully eliminate individual subjectivity. Performance variations\nand Gini index analysis reveal education as the primary factor influencing\naesthetic differences, followed by photography and art experience, with\nstronger individual subjectivity observed in artworks than in photos. Our model\nuniquely supports both GIAA and PIAA, enhancing generalization across\ndemographics.",
        "Due to the wide application of unmanned aerial vehicles (UAVs) as relays to\nestablish Disaster Response Networks (DRNs), an effective model of energy\nharvesting (EH) and energy consumption for the UAV-aided Disaster Response\nNetwork (DRN) is rising to be a challenging issue. This is mainly manifest in\nInternet of Things (IoT) scenarios where multiple users are looking to\ncommunicate with the UAV. In this paper, the possibility of connecting an UAV\nwith several users is investigated where the UAV as a relay receives data from\na DRN and delivers to another network considering two IoT scenarios. The first\nscenario represents a conventional method with limited UAV energy where low\ncommunication rates and inadequate service coverage for all users are\nchallenges. But in the second scenario, a Simultaneous Wireless Information and\nPower Transmission (SWIPT) technique is used to serve users. Considering\npotential limitations in transmission energy of users within disaster networks,\nthe SWIPT technique is applied to maximize energy acquisition by the UAV,\nleading to improve the efficiency of the investigated scenario. Finally, the\nrequired energy of the UAV to serve the largest number of users in the shortest\npossible time is clarified. Furthermore, by Considering the relationship\nbetween energy and UAV flight time and defining the UAV flight time\noptimization problem, optimal network parameters are obtained. Simulation\nresults show the effectiveness of the proposed scenario.",
        "JWST has uncovered large numbers of compact galaxies at high redshift with\nbroad hydrogen\/helium lines. These include the enigmatic population known as\n\"little red dots\" (LRDs). Their nature is debated, but they are thought to be\npowered by supermassive black holes (SMBHs) or intense star formation. They\nexhibit unusual properties for SMBHs, such as black holes that are overmassive\nfor their host galaxies and extremely weak X-ray and radio emission. Using the\nhighest-quality JWST spectra, we show here that the lines are broadened by\nelectron scattering with a narrow intrinsic line core. The data require high\nelectron column densities and compact sizes (light days), which, when coupled\nwith their high luminosities can only be explained by SMBH accretion. The\nnarrow intrinsic cores of the lines imply upper limits on the black hole masses\nof $10^{5-7}$ $M_{\\odot}$, two orders of magnitude lower than previous\nestimates. These are among the lowest mass SMBHs known at high redshift and\nsuggest that this is a population of young, rapidly growing SMBHs. They are\nenshrouded in a dense cocoon of ionized gas, probably related to their youth,\nfrom which they are accreting close to the Eddington limit. Reprocessed nebular\nemission from the dense cocoon dominates the optical spectrum, explaining most\nLRD spectral characteristics and helping to suppress radio and X-ray emission.",
        "Diffusion models have seen immense success in modelling continuous data\nacross a range of domains such as vision and audio. Despite the challenges of\nadapting diffusion models to discrete data, recent work explores their\napplication to text generation by working in the continuous embedding space.\nHowever, these models lack a natural means to control the inherent trade-off\nbetween quality and diversity as afforded by the temperature hyperparameter in\nautoregressive models, hindering understanding of model performance and\nrestricting generation quality. This work proposes the use of classifier-free\nguidance and stochastic clamping for manipulating the quality-diversity\ntrade-off on sequence-to-sequence tasks, demonstrating that these techniques\nmay be used to improve the performance of a diffusion language model.",
        "In recent years, neuro-symbolic methods have become a popular and powerful\napproach that augments artificial intelligence systems with the capability to\nperform abstract, logical, and quantitative deductions with enhanced precision\nand controllability. Recent studies successfully performed symbolic reasoning\nby leveraging various machine learning models to explicitly or implicitly\npredict intermediate labels that provide symbolic instructions. However, these\nintermediate labels are not always prepared for every task as a part of\ntraining data, and pre-trained models, represented by Large Language Models\n(LLMs), also do not consistently generate valid symbolic instructions with\ntheir intrinsic knowledge. On the other hand, existing work developed\nalternative learning techniques that allow the learning system to autonomously\nuncover optimal symbolic instructions. Nevertheless, their performance also\nexhibits limitations when faced with relatively huge search spaces or more\nchallenging reasoning problems. In view of this, in this work, we put forward\nan advanced practice for neuro-symbolic reasoning systems to explore the\nintermediate labels with weak supervision from problem inputs and final\noutputs. Our experiments on the Mathematics dataset illustrated the\neffectiveness of our proposals from multiple aspects.",
        "This study intends to test the hypothesis that, contrary to traditional\ninterpretation, the social structure of the polity of Aksum - especially in its\nearly stages - was not characterized by a vertical hierarchy with highly\ncentralized administrative power, and that the leaders mentioned in the few\navailable inscriptions were predominantly ritual leaders with religious rather\nthan coercive political authority. This hypothesis, suggested by the available\narchaeological evidence, is grounded in Charles Stanish's model, which posits\nthat pre-state societies could achieve cooperative behavior without the\npresence of coercive authority. Using agent-based modeling applied to data\ninspired by the Aksum civilization, we examine the dynamics of cooperation in\nthe presence and absence of a Public Goods Game. Results show that while\ncooperative behavior can emerge in the short term without coercive power, it\nmay not be sustainable over the long term, suggesting a need for centralized\nauthority to foster stable, complex societies. These findings provide insights\ninto the evolutionary pathways that lead to state formation and complex social\nstructures.",
        "Many studies conclude that galaxies quench from the inside-out by examining\nprofiles of specific star-formation rate (sSFR). These are usually measured by\nfitting spectral energy distributions (SEDs) assuming a fixed dust law and\nuniform priors on all parameters. Here, we examine the effects of more\nphysically motivated priors: a flexible dust law, an exponential prior on the\ndust attenuation $A_V$, and Gaussian priors that favor extended star-formation\nhistories. This results in model colors that better trace observations. We then\nperform radial SED fits to multiband flux profiles measured from Hubble Space\nTelescope images for 1,440 galaxies at $0.4<z<1.5$ of stellar masses\n$10^{10}-10^{11.5}\\ M_{\\odot}$ using both the traditional and the more\nphysically motivated assumptions. The latter results in star formation rate and\n$A_V$ profiles that agree with measurements from spectroscopy and $A_V$\nprofiles that behave correctly as a function of inclination. Since green valley\ngalaxies at $z\\sim1.3$ are expected to evolve into quiescent galaxies at\n$z\\sim0.9$, we compare their sSFR profiles using the more physically motivated\nassumptions. Their slopes are similar at all masses ($0.06 -\n0.08~\\textrm{dex}~\\textrm{kpc}^{-1}$), and the normalizations for the quiescent\ngalaxies are lower. Therefore, the sSFR profiles decline with time as quenching\noccurs at all radii simultaneously. We compare profiles of green valley\ngalaxies at $z\\sim0.9$ and quiescent galaxies at $z\\sim0.5$. The former are\nshallower at all masses by $\\sim0.1~\\textrm{dex}~\\textrm{kpc}^{-1}$. The sSFR\nprofiles steepen with time as galaxies quench from the inside-out. In summary,\nat $z\\sim0.9-1.3$, galaxies quench at all radii simultaneously, and at\n$z\\sim0.5-0.9$, they quench from the inside-out.",
        "Optimal Transport (OT) theory seeks to determine the map $T:X \\to Y$ that\ntransports a source measure $P$ to a target measure $Q$, minimizing the cost\n$c(\\mathbf{x}, T(\\mathbf{x}))$ between $\\mathbf{x}$ and its image\n$T(\\mathbf{x})$. Building upon the Input Convex Neural Network OT solver and\nincorporating the concept of displacement-sparse maps, we introduce a sparsity\npenalty into the minimax Wasserstein formulation, promote sparsity in\ndisplacement vectors $\\Delta(\\mathbf{x}) := T(\\mathbf{x}) - \\mathbf{x}$, and\nenhance the interpretability of the resulting map. However, increasing sparsity\noften reduces feasibility, causing $T_{\\#}(P)$ to deviate more significantly\nfrom the target measure. In low-dimensional settings, we propose a heuristic\nframework to balance the trade-off between sparsity and feasibility by\ndynamically adjusting the sparsity intensity parameter during training. For\nhigh-dimensional settings, we directly constrain the dimensionality of\ndisplacement vectors by enforcing $\\dim(\\Delta(\\mathbf{x})) \\leq l$, where $l <\nd$ for $X \\subseteq \\mathbb{R}^d$. Among maps satisfying this constraint, we\naim to identify the most feasible one. This goal can be effectively achieved by\nadapting our low-dimensional heuristic framework without resorting to\ndimensionality reduction. We validate our method on both synthesized sc-RNA and\nreal 4i cell perturbation datasets, demonstrating improvements over existing\nmethods.",
        "Machine Learning models are being extensively used in safety critical\napplications where errors from these models could cause harm to the user. Such\nrisks are amplified when multiple machine learning models, which are deployed\nconcurrently, interact and make errors simultaneously. This paper explores\nthree scenarios where error correlations between multiple models arise,\nresulting in such aggregated risks. Using real-world data, we simulate these\nscenarios and quantify the correlations in errors of different models. Our\nfindings indicate that aggregated risks are substantial, particularly when\nmodels share similar algorithms, training datasets, or foundational models.\nOverall, we observe that correlations across models are pervasive and likely to\nintensify with increased reliance on foundational models and widely used public\ndatasets, highlighting the need for effective mitigation strategies to address\nthese challenges.",
        "In this paper, we study the vanishing order of rational $L$-functions from a\ndata scientific perspective. Each $L$-function is represented in our data by\nfinitely many Dirichlet coefficients, the normalisation of which depends on the\ncontext. We observe murmuration-like patterns in averages across our dataset,\nfind that PCA clusters rational $L$-functions by their vanishing order, and\nrecord that LDA and neural networks may accurately predict this quantity.",
        "Fairness in multi-agent systems (MAS) focuses on equitable reward\ndistribution among agents in scenarios involving sensitive attributes such as\nrace, gender, or socioeconomic status. This paper introduces fairness in\nProximal Policy Optimization (PPO) with a penalty term derived from demographic\nparity, counterfactual fairness, and conditional statistical parity. The\nproposed method balances reward maximisation with fairness by integrating two\npenalty components: a retrospective component that minimises disparities in\npast outcomes and a prospective component that ensures fairness in future\ndecision-making. We evaluate our approach in the Allelopathic Harvest game, a\ncooperative and competitive MAS focused on resource collection, where some\nagents possess a sensitive attribute. Experiments demonstrate that fair-PPO\nachieves fairer policies across all fairness metrics than classic PPO. Fairness\ncomes at the cost of reduced rewards, namely the Price of Fairness, although\nagents with and without the sensitive attribute renounce comparable amounts of\nrewards. Additionally, the retrospective and prospective penalties effectively\nchange the agents' behaviour and improve fairness. These findings underscore\nthe potential of fair-PPO to address fairness challenges in MAS.",
        "The gravitational effects of a primordial black hole (PBH) passing through\nthe human body are examined, with the goal of determining the minimum mass\nnecessary to produce significant injury or death. Two effects are examined: the\ndamage caused by a shock wave propagating outward from the black hole\ntrajectory, and the dissociation of brain cells from tidal forces produced by\nthe black hole on its passage through the human body. It is found that the\nformer is the dominant effect, with a cutoff mass for serious injury or death\nof approximately $M_{PBH} > 1.4 \\times 10^{17} {\\rm g}$. The number density of\nprimordial black holes with a mass above this cutoff is far too small to\nproduce any observable effects on the human population.",
        "The use of computational ontologies is well-established in the field of\nMedical Informatics. The topic of Social Determinants of Health (SDoH) has also\nreceived extensive attention. Work at the intersection of ontologies and SDoH\nhas been published. However, a standardized framework for Social Determinants\nof Education (SDoEd) is lacking. In this paper, we are closing the gap by\nintroducing an SDoEd ontology for creating a precise conceptualization of the\ninterplay between life circumstances of students and their possible educational\nachievements. The ontology was developed utilizing suggestions from\nChatGPT-3.5-010422 and validated using peer-reviewed research articles. The\nfirst version of developed ontology was evaluated by human experts in the field\nof education and validated using standard ontology evaluation software. This\nversion of the SDoEd ontology contains 231 domain concepts, 10 object\nproperties, and 24 data properties",
        "Quantum machine learning (QML) has emerged as a promising domain to leverage\nthe computational capabilities of quantum systems to solve complex\nclassification tasks. In this work, we present the first comprehensive QML\nstudy by benchmarking the MedMNIST-a diverse collection of medical imaging\ndatasets on a 127-qubit real IBM quantum hardware, to evaluate the feasibility\nand performance of quantum models (without any classical neural networks) in\npractical applications. This study explores recent advancements in quantum\ncomputing such as device-aware quantum circuits, error suppression, and\nmitigation for medical image classification. Our methodology is comprised of\nthree stages: preprocessing, generation of noise-resilient and\nhardware-efficient quantum circuits, optimizing\/training of quantum circuits on\nclassical hardware, and inference on real IBM quantum hardware. Firstly, we\nprocess all input images in the preprocessing stage to reduce the spatial\ndimension due to quantum hardware limitations. We generate hardware-efficient\nquantum circuits using backend properties expressible to learn complex patterns\nfor medical image classification. After classical optimization of QML models,\nwe perform inference on real quantum hardware. We also incorporate advanced\nerror suppression and mitigation techniques in our QML workflow, including\ndynamical decoupling (DD), gate twirling, and matrix-free measurement\nmitigation (M3) to mitigate the effects of noise and improve classification\nperformance. The experimental results showcase the potential of quantum\ncomputing for medical imaging and establish a benchmark for future advancements\nin QML applied to healthcare."
      ]
    }
  },
  {
    "id":2412.00252,
    "research_type":"basic",
    "start_id":"b5",
    "start_title":"Localization and landscape functions for graph laplacians",
    "start_abstract":"We discuss explicit landscape functions for quantum graphs. By a 'landscape function' Upsilon(x) we mean a function that controls the localization properties of normalized eigenfunctions psi(x) through a pointwise inequality of the form |psi(x)| le Upsilon(x). The ideal Upsilon is a function that a) responds to the potential energy V(x) and to the structure of the graph in some formulaic way; b) is small in examples where eigenfunctions are suppressed by the tunneling effect, and c) relatively large in regions where eigenfunctions may - or may not - be concentrated, as observed in specific examples. It turns out that the connectedness of a graph can present a barrier to the existence of universal landscape functions in the high-energy r\u00e9gime, as we show with simple examples. We therefore apply different methods in different r\u00e9gimes determined by the values of the potential energy V(x) and the eigenvalue parameter E.",
    "start_categories":[
      "Anderson Localization"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b15"
      ],
      "title":[
        "Analytic perturbation theory for matrices and operators"
      ],
      "abstract":[
        "Perturbation theory is the study of the behavior of mathematical objects under the influence of perturbations. It is not a well-defined mathematical topic with specific objects and methods, but rather a method of investigation. In this book, perturbation theory will be developed for linear operators. First, inter-est focuses on the properties of spectral objects, such as eigenvalues, eigenprojections, eigenvectors and Jordan vectors, under perturbation of the underlying operator. This study encompasses some difficult problems. On the one hand, variations of the spectral objects need to be calculated quantitatively. The spectral objects are assumed known for the unperturbed operator, the determination (or at least the approximation) of the spectral objects for the perturbed operator is at issue. This is the starting point for the perturbation theory of L. RAYLEIGH [1] (see also R. COURANT and D. HTT.BEBT [1, p. 296 sqq]). On the other hand, the spectral objects often undergo abrupt qualitative changes, even in the case of small perturbations. These changes cause significant complications. Usually, the behavior of the spectral objects depends strongly on the assumptions about the nature of the perturbation. For example, one can assume continuity, differ-entiability, smoothness (i.e. arbitrary differentiability) or analyticity. In the following discussion, only the case of analytic (holomorphic) perturbations will be investigated, even if this strong restriction is applied, the problems remain difficult enough. On the one hand, solving problems of perturbation theory is of conceptual interest. The study of intrinsic spectral properties of a linear operator undergoing perturbation leads to deeper insights and understanding of the structure of the operator. It also leads to the development of new tools for further investigations. On the other hand, applications (inside and outside of mathematics) lead to new questions in perturbation theory. One of the first calculations of perturbation theory was given by L. Rayleigh, who determined the eigenfrequencies and eigenmodes of an oscillating string, fixed at x = 0 and x = n, whose elasticity modulus is constant and whose mass density Q(X) has only a small deviation from a constant value for all x, 0 ^ x 5S n. (That is, the density o(x) is of the form Q(X) = p0 + ea(x), where a(x) is a given function and where e is a small perturbation parameter.) Actually, as this example indicates, the starting point for the development of per-turbation theory was the study of perturbations of spectral objects (eigenvalues and eigenvectors) for concrete classes of operators, for example, Fredholm integral operators or Sturm-Liouville differential operators (for example, see L. LIECHTENSTEIN [1])."
      ],
      "categories":[
        "Analytic Perturbation Theory "
      ]
    },
    "list":{
      "title":[
        "GSVC: Efficient Video Representation and Compression Through 2D Gaussian\n  Splatting",
        "On finite approximations of transitive graphs",
        "Solvable Dynamics of Self-Supervised Word Embeddings and the Emergence\n  of Analogical Reasoning",
        "How to Tune a Multilingual Encoder Model for Germanic Languages: A Study\n  of PEFT, Full Fine-Tuning, and Language Adapters",
        "Dynamic realization of emergent high-dimensional optical vortices",
        "Rigidity results for free boundary hypersurfaces in initial data sets\n  with boundary",
        "Int2Int: a framework for mathematics with transformers",
        "Eliciting Language Model Behaviors with Investigator Agents",
        "Applying the Liouville-Lanczos Method of Time-Dependent\n  Density-Functional Theory to Warm Dense Matter",
        "TigerVector: Supporting Vector Search in Graph Databases for Advanced\n  RAGs",
        "qReduMIS: A Quantum-Informed Reduction Algorithm for the Maximum\n  Independent Set Problem",
        "Algebra and geometry of ASM weak order",
        "BPS invariants from framed links",
        "Overcoming Semantic Dilution in Transformer-Based Next Frame Prediction",
        "Non-convergence to the optimal risk for Adam and stochastic gradient\n  descent optimization in the training of deep neural networks",
        "On the Role of Individual Differences in Current Approaches to\n  Computational Image Aesthetics",
        "UAV-Enabled IoT Networks: A SWIPT Energy Harvesting Architecture with\n  Relay Support for Disaster Response",
        "JWST's little red dots: an emerging population of young, low-mass AGN\n  cocooned in dense ionized gas",
        "Understanding the Quality-Diversity Trade-off in Diffusion Language\n  Models",
        "Advanced Weakly-Supervised Formula Exploration for Neuro-Symbolic\n  Mathematical Reasoning",
        "Cooperative Behavior in Pre-State Societies: An Agent-Based Approach of\n  the Aksum Civilization",
        "Improved SED-Fitting Assumptions Result in Inside-Out Quenching at\n  $z\\sim0.5$ and Quenching at All Radii Simultaneously at $z\\sim1$",
        "Displacement-Sparse Neural Optimal Transport",
        "Quantifying Correlations of Machine Learning Models",
        "Machine learning the vanishing order of rational L-functions",
        "Fairness Aware Reinforcement Learning via Proximal Policy Optimization",
        "Gravitational Effects of a Small Primordial Black Hole Passing Through\n  the Human Body",
        "An Ontology for Social Determinants of Education (SDoEd) based on\n  Human-AI Collaborative Approach",
        "Benchmarking MedMNIST dataset on real quantum hardware"
      ],
      "abstract":[
        "3D Gaussian splats have emerged as a revolutionary, effective, learned\nrepresentation for static 3D scenes. In this work, we explore using 2D Gaussian\nsplats as a new primitive for representing videos. We propose GSVC, an approach\nto learning a set of 2D Gaussian splats that can effectively represent and\ncompress video frames. GSVC incorporates the following techniques: (i) To\nexploit temporal redundancy among adjacent frames, which can speed up training\nand improve the compression efficiency, we predict the Gaussian splats of a\nframe based on its previous frame; (ii) To control the trade-offs between file\nsize and quality, we remove Gaussian splats with low contribution to the video\nquality; (iii) To capture dynamics in videos, we randomly add Gaussian splats\nto fit content with large motion or newly-appeared objects; (iv) To handle\nsignificant changes in the scene, we detect key frames based on loss\ndifferences during the learning process. Experiment results show that GSVC\nachieves good rate-distortion trade-offs, comparable to state-of-the-art video\ncodecs such as AV1 and VVC, and a rendering speed of 1500 fps for a 1920x1080\nvideo.",
        "In this note we answer a question of Johannes Carmesin, which was circulated\nat the Oberwolfach Workshop on \"Graph Theory\" in January 2025. We provide a\nunimodular, locally finite, and vertex-transitive graph without any perfect\nfinite $r$-local model for $r \\in \\mathbb N$ large enough.",
        "The remarkable success of large language models relies on their ability to\nimplicitly learn structured latent representations from the pretraining corpus.\nAs a simpler surrogate for representation learning in language modeling, we\nstudy a class of solvable contrastive self-supervised algorithms which we term\nquadratic word embedding models. These models resemble the word2vec algorithm\nand perform similarly on downstream tasks. Our main contributions are\nanalytical solutions for both the training dynamics (under certain\nhyperparameter choices) and the final word embeddings, given in terms of only\nthe corpus statistics. Our solutions reveal that these models learn orthogonal\nlinear subspaces one at a time, each one incrementing the effective rank of the\nembeddings until model capacity is saturated. Training on WikiText, we find\nthat the top subspaces represent interpretable concepts. Finally, we use our\ndynamical theory to predict how and when models acquire the ability to complete\nanalogies.",
        "This paper investigates the optimal use of the multilingual encoder model\nmDeBERTa for tasks in three Germanic languages -- German, Swedish, and\nIcelandic -- representing varying levels of presence and likely data quality in\nmDeBERTas pre-training data. We compare full fine-tuning with the\nparameter-efficient fine-tuning (PEFT) methods LoRA and Pfeiffer bottleneck\nadapters, finding that PEFT is more effective for the higher-resource language,\nGerman. However, results for Swedish and Icelandic are less consistent. We also\nobserve differences between tasks: While PEFT tends to work better for question\nanswering, full fine-tuning is preferable for named entity recognition.\nInspired by previous research on modular approaches that combine task and\nlanguage adapters, we evaluate the impact of adding PEFT modules trained on\nunstructured text, finding that this approach is not beneficial.",
        "The dimensionality of vortical structures has recently been extended beyond\ntwo dimensions, providing higher-order topological characteristics and\nrobustness for high-capacity information processing and turbulence control. The\ngeneration of high-dimensional vortical structures has mostly been demonstrated\nin classical systems through the complex interference of fluidic, acoustic, or\nelectromagnetic waves. However, natural materials rarely support three- or\nhigher-dimensional vortical structures and their physical interactions. Here,\nwe present a high-dimensional gradient thickness optical cavity (GTOC) in which\nthe optical coupling of planar metal-dielectric multilayers implements\ntopological interactions across multiple dimensions. Topological interactions\nin high-dimensional GTOC construct non-trivial topological phases, which induce\nhigh-dimensional vortical structures in generalized parameter space in three,\nfour dimensions, and beyond. These emergent high-dimensional vortical\nstructures are observed under electro-optic tomography as optical vortex\ndynamics in two-dimensional real-space, employing the optical thicknesses of\nthe dielectric layers as synthetic dimensions. We experimentally demonstrate\nemergent vortical structures, optical vortex lines and vortex rings, in a\nthree-dimensional generalized parameter space and their topological\ntransitions. Furthermore, we explore four-dimensional vortical structures,\ntermed optical vortex sheets, which provide the programmability of real-space\noptical vortex dynamics. Our findings hold significant promise for emulating\nhigh-dimensional physics and developing active topological photonic devices.",
        "In this work, we present several rigidity results for compact free boundary\nhypersurfaces in initial data sets with boundary. Specifically, in the first\npart of the paper, we extend the local splitting theorems from [G. J. Galloway\nand H. C. Jang, Some scalar curvature warped product splitting theorems, Proc.\nAm. Math. Soc. 148 (2020), no. 6, 2617-2629] to the setting of manifolds with\nboundary. To achieve this, we build on the approach of the original paper,\nutilizing results on free boundary marginally outer trapped surfaces (MOTS)\napplied to specific initial data sets. In the second part, we extend the main\nresults from [A. Barros and C. Cruz, Free boundary hypersurfaces with\nnon-positive Yamabe invariant in mean convex manifolds, J. Geom. Anal. 30\n(2020), no. 4, 3542-3562] to the context of free boundary MOTS in initial data\nsets with boundary.",
        "This paper documents Int2Int, an open source code base for using transformers\non problems of mathematical research, with a focus on number theory and other\nproblems involving integers. Int2Int is a complete PyTorch implementation of a\ntransformer architecture, together with training and evaluation loops, and\nclasses and functions to represent, generate and decode common mathematical\nobjects. Ancillary code for data preparation, and Jupyter Notebooks for\nvisualizing experimental results are also provided. This document presents the\nmain features of Int2Int, serves as its user manual, and provides guidelines on\nhow to extend it. Int2Int is released under the MIT licence, at\nhttps:\/\/github.com\/FacebookResearch\/Int2Int.",
        "Language models exhibit complex, diverse behaviors when prompted with\nfree-form text, making it difficult to characterize the space of possible\noutputs. We study the problem of behavior elicitation, where the goal is to\nsearch for prompts that induce specific target behaviors (e.g., hallucinations\nor harmful responses) from a target language model. To navigate the\nexponentially large space of possible prompts, we train investigator models to\nmap randomly-chosen target behaviors to a diverse distribution of outputs that\nelicit them, similar to amortized Bayesian inference. We do this through\nsupervised fine-tuning, reinforcement learning via DPO, and a novel Frank-Wolfe\ntraining objective to iteratively discover diverse prompting strategies. Our\ninvestigator models surface a variety of effective and human-interpretable\nprompts leading to jailbreaks, hallucinations, and open-ended aberrant\nbehaviors, obtaining a 100% attack success rate on a subset of AdvBench\n(Harmful Behaviors) and an 85% hallucination rate.",
        "Ab initio modeling of dynamic structure factors (DSF) and related density\nresponse properties in the warm dense matter (WDM) regime is a challenging\ncomputational task. The DSF, convolved with a probing X-ray beam and instrument\nfunction, is measured in X-ray Thomson scattering (XRTS) experiments, which\nallows for the study of electronic structure properties at the microscopic\nlevel. Among the various ab initio methods, linear response time-dependent\ndensity functional theory (LR-TDDFT) is a key framework for simulating the DSF.\nThe standard approach in LR-TDDFT for computing the DSF relies on the orbital\nrepresentation. A significant drawback of this method is the unfavorable\nscaling of the number of required empty bands as the wavenumber increases,\nmaking LR-TDDFT impractical for modeling XRTS measurements over large energy\nscales, such as in backward scattering geometry. We consider and test an\nalternative approach that employs the Liouville-Lanczos (LL) method for\nsimulating the DSF. This approach does not require empty states and allows the\nDSF at large momentum transfer values and over a broad frequency range to be\naccessed. We compare the results obtained from the LL method with those from\nthe standard LR-TDDFT within the projector augmented-wave formalism for\nisochorically heated aluminum and warm dense hydrogen. Additionally, we utilize\nexact path integral Monte Carlo (PIMC) results for the imaginary-time\ndensity-density correlation function (ITCF) of warm dense hydrogen to\nrigorously benchmark the LL approach. We discuss the application of the LL\nmethod for calculating DSFs and ITCFs at different wavenumbers, the effects of\npseudopotentials, and the role of Lorentzian smearing. The successful\nvalidation of the LL method under WDM conditions makes it a valuable addition\nto the ab initio simulation landscape, supporting experimental efforts and\nadvancing WDM theory.",
        "In this paper, we introduce TigerVector, a system that integrates vector\nsearch and graph query within TigerGraph, a Massively Parallel Processing (MPP)\nnative graph database. We extend the vertex attribute type with the embedding\ntype. To support fast vector search, we devise an MPP index framework that\ninteroperates efficiently with the graph engine. The graph query language GSQL\nis enhanced to support vector type expressions and enable query compositions\nbetween vector search results and graph query blocks. These advancements\nelevate the expressive power and analytical capabilities of graph databases,\nenabling seamless fusion of unstructured and structured data in ways previously\nunattainable. Through extensive experiments, we demonstrate TigerVector's\nhybrid search capability, scalability, and superior performance compared to\nother graph databases (including Neo4j and Amazon Neptune) and a highly\noptimized specialized vector database (Milvus). TigerVector was integrated into\nTigerGraph v4.2, the latest release of TigerGraph, in December 2024.",
        "We propose and implement a quantum-informed reduction algorithm for the\nmaximum independent set problem that integrates classical kernelization\ntechniques with information extracted from quantum devices. Our larger\nframework consists of dedicated application, algorithm, and hardware layers,\nand easily generalizes to the maximum weight independent set problem. In this\nhybrid quantum-classical framework, which we call qReduMIS, the quantum\ncomputer is used as a co-processor to inform classical reduction logic about\nfrozen vertices that are likely (or unlikely) to be in large independent sets,\nthereby opening up the reduction space after removal of targeted subgraphs. We\nsystematically assess the performance of qReduMIS based on experiments with up\nto 231 qubits run on Rydberg quantum hardware available through Amazon Braket.\nOur experiments show that qReduMIS can help address fundamental performance\nlimitations faced by a broad set of (quantum) solvers including Rydberg quantum\ndevices. We outline implementations of qReduMIS with alternative platforms,\nsuch as superconducting qubits or trapped ions, and we discuss potential future\nextensions.",
        "Much of modern Schubert calculus is centered on Schubert varieties in the\ncomplete flag variety and on their classes in its integral cohomology ring.\nUnder the Borel isomorphism, these classes are represented by distinguished\npolynomials called Schubert polynomials, introduced by Lascoux and\nSch\\\"utzenberger.\n  Knutson and Miller showed that Schubert polynomials are multidegrees of\nmatrix Schubert varieties, affine varieties introduced by Fulton, which are\nclosely related to Schubert varieties. Many roads to studying Schubert\npolynomials pass through unions and intersections of matrix Schubert varieties.\nThe third author showed that the natural indexing objects of arbitrary\nintersections of matrix Schubert varieties are alternating sign matrices\n(ASMs). Every ASM variety is expressible as a union of matrix Schubert\nvarieties.\n  Many fundamental algebro-geometric invariants (e.g., codimension, degree, and\nCastelnuovo--Mumford regularity) are well understood combinatorially for matrix\nSchubert varieties, substantially via the combinatorics of strong Bruhat order\non $S_n$. The extension of strong order to ASM(n), the set of $n \\times n$\nASMs, has so far not borne as much algebro-geometric fruit for ASM varieties.\n  Hamaker and Reiner proposed an extension of weak Bruhat order from $S_n$ to\nASM(n), which they studied from a combinatorial perspective. In the present\npaper, we place this work on algebro-geometric footing. We use weak order on\nASMs to give a characterization of codimension of ASM varieties. We also show\nthat weak order operators commute with K-theoretic divided difference operators\nand that they satisfy the same derivative formula that facilitated the first\ngeneral combinatorial computation of Castelnuovo--Mumford regularity of matrix\nSchubert varieties. Finally, we build from these results to generalizations\nthat apply to arbitrary unions of matrix Schubert varieties.",
        "In this article, we investigate the BPS invariants associated with framed\nlinks. We extend the relationship between the algebraic curve (i.e. dual\n$A$-polynomial) and the BPS invariants of a knot investigated in \\cite{GKS} to\nthe case of a framed knot. With the help of the framing change formula for the\ndual $A$-polynomial of a framed knot, we give several explicit formulas for the\nextremal $A$-polynomials and the BPS invariants of framed knots. As to the\nframed links, we present several numerical calculations for the Ooguri-Vafa\ninvariants and BPS invariants for framed Whitehead links and Borromean rings\nand verify the integrality property for them.",
        "Next-frame prediction in videos is crucial for applications such as\nautonomous driving, object tracking, and motion prediction. The primary\nchallenge in next-frame prediction lies in effectively capturing and processing\nboth spatial and temporal information from previous video sequences. The\ntransformer architecture, known for its prowess in handling sequence data, has\nmade remarkable progress in this domain. However, transformer-based next-frame\nprediction models face notable issues: (a) The multi-head self-attention (MHSA)\nmechanism requires the input embedding to be split into $N$ chunks, where $N$\nis the number of heads. Each segment captures only a fraction of the original\nembeddings information, which distorts the representation of the embedding in\nthe latent space, resulting in a semantic dilution problem; (b) These models\npredict the embeddings of the next frames rather than the frames themselves,\nbut the loss function based on the errors of the reconstructed frames, not the\npredicted embeddings -- this creates a discrepancy between the training\nobjective and the model output. We propose a Semantic Concentration Multi-Head\nSelf-Attention (SCMHSA) architecture, which effectively mitigates semantic\ndilution in transformer-based next-frame prediction. Additionally, we introduce\na loss function that optimizes SCMHSA in the latent space, aligning the\ntraining objective more closely with the model output. Our method demonstrates\nsuperior performance compared to the original transformer-based predictors.",
        "Despite the omnipresent use of stochastic gradient descent (SGD) optimization\nmethods in the training of deep neural networks (DNNs), it remains, in\nbasically all practically relevant scenarios, a fundamental open problem to\nprovide a rigorous theoretical explanation for the success (and the\nlimitations) of SGD optimization methods in deep learning. In particular, it\nremains an open question to prove or disprove convergence of the true risk of\nSGD optimization methods to the optimal true risk value in the training of\nDNNs. In one of the main results of this work we reveal for a general class of\nactivations, loss functions, random initializations, and SGD optimization\nmethods (including, for example, standard SGD, momentum SGD, Nesterov\naccelerated SGD, Adagrad, RMSprop, Adadelta, Adam, Adamax, Nadam, Nadamax, and\nAMSGrad) that in the training of any arbitrary fully-connected feedforward DNN\nit does not hold that the true risk of the considered optimizer converges in\nprobability to the optimal true risk value. Nonetheless, the true risk of the\nconsidered SGD optimization method may very well converge to a strictly\nsuboptimal true risk value.",
        "Image aesthetic assessment (IAA) evaluates image aesthetics, a task\ncomplicated by image diversity and user subjectivity. Current approaches\naddress this in two stages: Generic IAA (GIAA) models estimate mean aesthetic\nscores, while Personal IAA (PIAA) models adapt GIAA using transfer learning to\nincorporate user subjectivity. However, a theoretical understanding of transfer\nlearning between GIAA and PIAA, particularly concerning the impact of group\ncomposition, group size, aesthetic differences between groups and individuals,\nand demographic correlations, is lacking. This work establishes a theoretical\nfoundation for IAA, proposing a unified model that encodes individual\ncharacteristics in a distributional format for both individual and group\nassessments. We show that transferring from GIAA to PIAA involves\nextrapolation, while the reverse involves interpolation, which is generally\nmore effective for machine learning. Experiments with varying group\ncompositions, including sub-sampling by group size and disjoint demographics,\nreveal significant performance variation even for GIAA, indicating that mean\nscores do not fully eliminate individual subjectivity. Performance variations\nand Gini index analysis reveal education as the primary factor influencing\naesthetic differences, followed by photography and art experience, with\nstronger individual subjectivity observed in artworks than in photos. Our model\nuniquely supports both GIAA and PIAA, enhancing generalization across\ndemographics.",
        "Due to the wide application of unmanned aerial vehicles (UAVs) as relays to\nestablish Disaster Response Networks (DRNs), an effective model of energy\nharvesting (EH) and energy consumption for the UAV-aided Disaster Response\nNetwork (DRN) is rising to be a challenging issue. This is mainly manifest in\nInternet of Things (IoT) scenarios where multiple users are looking to\ncommunicate with the UAV. In this paper, the possibility of connecting an UAV\nwith several users is investigated where the UAV as a relay receives data from\na DRN and delivers to another network considering two IoT scenarios. The first\nscenario represents a conventional method with limited UAV energy where low\ncommunication rates and inadequate service coverage for all users are\nchallenges. But in the second scenario, a Simultaneous Wireless Information and\nPower Transmission (SWIPT) technique is used to serve users. Considering\npotential limitations in transmission energy of users within disaster networks,\nthe SWIPT technique is applied to maximize energy acquisition by the UAV,\nleading to improve the efficiency of the investigated scenario. Finally, the\nrequired energy of the UAV to serve the largest number of users in the shortest\npossible time is clarified. Furthermore, by Considering the relationship\nbetween energy and UAV flight time and defining the UAV flight time\noptimization problem, optimal network parameters are obtained. Simulation\nresults show the effectiveness of the proposed scenario.",
        "JWST has uncovered large numbers of compact galaxies at high redshift with\nbroad hydrogen\/helium lines. These include the enigmatic population known as\n\"little red dots\" (LRDs). Their nature is debated, but they are thought to be\npowered by supermassive black holes (SMBHs) or intense star formation. They\nexhibit unusual properties for SMBHs, such as black holes that are overmassive\nfor their host galaxies and extremely weak X-ray and radio emission. Using the\nhighest-quality JWST spectra, we show here that the lines are broadened by\nelectron scattering with a narrow intrinsic line core. The data require high\nelectron column densities and compact sizes (light days), which, when coupled\nwith their high luminosities can only be explained by SMBH accretion. The\nnarrow intrinsic cores of the lines imply upper limits on the black hole masses\nof $10^{5-7}$ $M_{\\odot}$, two orders of magnitude lower than previous\nestimates. These are among the lowest mass SMBHs known at high redshift and\nsuggest that this is a population of young, rapidly growing SMBHs. They are\nenshrouded in a dense cocoon of ionized gas, probably related to their youth,\nfrom which they are accreting close to the Eddington limit. Reprocessed nebular\nemission from the dense cocoon dominates the optical spectrum, explaining most\nLRD spectral characteristics and helping to suppress radio and X-ray emission.",
        "Diffusion models have seen immense success in modelling continuous data\nacross a range of domains such as vision and audio. Despite the challenges of\nadapting diffusion models to discrete data, recent work explores their\napplication to text generation by working in the continuous embedding space.\nHowever, these models lack a natural means to control the inherent trade-off\nbetween quality and diversity as afforded by the temperature hyperparameter in\nautoregressive models, hindering understanding of model performance and\nrestricting generation quality. This work proposes the use of classifier-free\nguidance and stochastic clamping for manipulating the quality-diversity\ntrade-off on sequence-to-sequence tasks, demonstrating that these techniques\nmay be used to improve the performance of a diffusion language model.",
        "In recent years, neuro-symbolic methods have become a popular and powerful\napproach that augments artificial intelligence systems with the capability to\nperform abstract, logical, and quantitative deductions with enhanced precision\nand controllability. Recent studies successfully performed symbolic reasoning\nby leveraging various machine learning models to explicitly or implicitly\npredict intermediate labels that provide symbolic instructions. However, these\nintermediate labels are not always prepared for every task as a part of\ntraining data, and pre-trained models, represented by Large Language Models\n(LLMs), also do not consistently generate valid symbolic instructions with\ntheir intrinsic knowledge. On the other hand, existing work developed\nalternative learning techniques that allow the learning system to autonomously\nuncover optimal symbolic instructions. Nevertheless, their performance also\nexhibits limitations when faced with relatively huge search spaces or more\nchallenging reasoning problems. In view of this, in this work, we put forward\nan advanced practice for neuro-symbolic reasoning systems to explore the\nintermediate labels with weak supervision from problem inputs and final\noutputs. Our experiments on the Mathematics dataset illustrated the\neffectiveness of our proposals from multiple aspects.",
        "This study intends to test the hypothesis that, contrary to traditional\ninterpretation, the social structure of the polity of Aksum - especially in its\nearly stages - was not characterized by a vertical hierarchy with highly\ncentralized administrative power, and that the leaders mentioned in the few\navailable inscriptions were predominantly ritual leaders with religious rather\nthan coercive political authority. This hypothesis, suggested by the available\narchaeological evidence, is grounded in Charles Stanish's model, which posits\nthat pre-state societies could achieve cooperative behavior without the\npresence of coercive authority. Using agent-based modeling applied to data\ninspired by the Aksum civilization, we examine the dynamics of cooperation in\nthe presence and absence of a Public Goods Game. Results show that while\ncooperative behavior can emerge in the short term without coercive power, it\nmay not be sustainable over the long term, suggesting a need for centralized\nauthority to foster stable, complex societies. These findings provide insights\ninto the evolutionary pathways that lead to state formation and complex social\nstructures.",
        "Many studies conclude that galaxies quench from the inside-out by examining\nprofiles of specific star-formation rate (sSFR). These are usually measured by\nfitting spectral energy distributions (SEDs) assuming a fixed dust law and\nuniform priors on all parameters. Here, we examine the effects of more\nphysically motivated priors: a flexible dust law, an exponential prior on the\ndust attenuation $A_V$, and Gaussian priors that favor extended star-formation\nhistories. This results in model colors that better trace observations. We then\nperform radial SED fits to multiband flux profiles measured from Hubble Space\nTelescope images for 1,440 galaxies at $0.4<z<1.5$ of stellar masses\n$10^{10}-10^{11.5}\\ M_{\\odot}$ using both the traditional and the more\nphysically motivated assumptions. The latter results in star formation rate and\n$A_V$ profiles that agree with measurements from spectroscopy and $A_V$\nprofiles that behave correctly as a function of inclination. Since green valley\ngalaxies at $z\\sim1.3$ are expected to evolve into quiescent galaxies at\n$z\\sim0.9$, we compare their sSFR profiles using the more physically motivated\nassumptions. Their slopes are similar at all masses ($0.06 -\n0.08~\\textrm{dex}~\\textrm{kpc}^{-1}$), and the normalizations for the quiescent\ngalaxies are lower. Therefore, the sSFR profiles decline with time as quenching\noccurs at all radii simultaneously. We compare profiles of green valley\ngalaxies at $z\\sim0.9$ and quiescent galaxies at $z\\sim0.5$. The former are\nshallower at all masses by $\\sim0.1~\\textrm{dex}~\\textrm{kpc}^{-1}$. The sSFR\nprofiles steepen with time as galaxies quench from the inside-out. In summary,\nat $z\\sim0.9-1.3$, galaxies quench at all radii simultaneously, and at\n$z\\sim0.5-0.9$, they quench from the inside-out.",
        "Optimal Transport (OT) theory seeks to determine the map $T:X \\to Y$ that\ntransports a source measure $P$ to a target measure $Q$, minimizing the cost\n$c(\\mathbf{x}, T(\\mathbf{x}))$ between $\\mathbf{x}$ and its image\n$T(\\mathbf{x})$. Building upon the Input Convex Neural Network OT solver and\nincorporating the concept of displacement-sparse maps, we introduce a sparsity\npenalty into the minimax Wasserstein formulation, promote sparsity in\ndisplacement vectors $\\Delta(\\mathbf{x}) := T(\\mathbf{x}) - \\mathbf{x}$, and\nenhance the interpretability of the resulting map. However, increasing sparsity\noften reduces feasibility, causing $T_{\\#}(P)$ to deviate more significantly\nfrom the target measure. In low-dimensional settings, we propose a heuristic\nframework to balance the trade-off between sparsity and feasibility by\ndynamically adjusting the sparsity intensity parameter during training. For\nhigh-dimensional settings, we directly constrain the dimensionality of\ndisplacement vectors by enforcing $\\dim(\\Delta(\\mathbf{x})) \\leq l$, where $l <\nd$ for $X \\subseteq \\mathbb{R}^d$. Among maps satisfying this constraint, we\naim to identify the most feasible one. This goal can be effectively achieved by\nadapting our low-dimensional heuristic framework without resorting to\ndimensionality reduction. We validate our method on both synthesized sc-RNA and\nreal 4i cell perturbation datasets, demonstrating improvements over existing\nmethods.",
        "Machine Learning models are being extensively used in safety critical\napplications where errors from these models could cause harm to the user. Such\nrisks are amplified when multiple machine learning models, which are deployed\nconcurrently, interact and make errors simultaneously. This paper explores\nthree scenarios where error correlations between multiple models arise,\nresulting in such aggregated risks. Using real-world data, we simulate these\nscenarios and quantify the correlations in errors of different models. Our\nfindings indicate that aggregated risks are substantial, particularly when\nmodels share similar algorithms, training datasets, or foundational models.\nOverall, we observe that correlations across models are pervasive and likely to\nintensify with increased reliance on foundational models and widely used public\ndatasets, highlighting the need for effective mitigation strategies to address\nthese challenges.",
        "In this paper, we study the vanishing order of rational $L$-functions from a\ndata scientific perspective. Each $L$-function is represented in our data by\nfinitely many Dirichlet coefficients, the normalisation of which depends on the\ncontext. We observe murmuration-like patterns in averages across our dataset,\nfind that PCA clusters rational $L$-functions by their vanishing order, and\nrecord that LDA and neural networks may accurately predict this quantity.",
        "Fairness in multi-agent systems (MAS) focuses on equitable reward\ndistribution among agents in scenarios involving sensitive attributes such as\nrace, gender, or socioeconomic status. This paper introduces fairness in\nProximal Policy Optimization (PPO) with a penalty term derived from demographic\nparity, counterfactual fairness, and conditional statistical parity. The\nproposed method balances reward maximisation with fairness by integrating two\npenalty components: a retrospective component that minimises disparities in\npast outcomes and a prospective component that ensures fairness in future\ndecision-making. We evaluate our approach in the Allelopathic Harvest game, a\ncooperative and competitive MAS focused on resource collection, where some\nagents possess a sensitive attribute. Experiments demonstrate that fair-PPO\nachieves fairer policies across all fairness metrics than classic PPO. Fairness\ncomes at the cost of reduced rewards, namely the Price of Fairness, although\nagents with and without the sensitive attribute renounce comparable amounts of\nrewards. Additionally, the retrospective and prospective penalties effectively\nchange the agents' behaviour and improve fairness. These findings underscore\nthe potential of fair-PPO to address fairness challenges in MAS.",
        "The gravitational effects of a primordial black hole (PBH) passing through\nthe human body are examined, with the goal of determining the minimum mass\nnecessary to produce significant injury or death. Two effects are examined: the\ndamage caused by a shock wave propagating outward from the black hole\ntrajectory, and the dissociation of brain cells from tidal forces produced by\nthe black hole on its passage through the human body. It is found that the\nformer is the dominant effect, with a cutoff mass for serious injury or death\nof approximately $M_{PBH} > 1.4 \\times 10^{17} {\\rm g}$. The number density of\nprimordial black holes with a mass above this cutoff is far too small to\nproduce any observable effects on the human population.",
        "The use of computational ontologies is well-established in the field of\nMedical Informatics. The topic of Social Determinants of Health (SDoH) has also\nreceived extensive attention. Work at the intersection of ontologies and SDoH\nhas been published. However, a standardized framework for Social Determinants\nof Education (SDoEd) is lacking. In this paper, we are closing the gap by\nintroducing an SDoEd ontology for creating a precise conceptualization of the\ninterplay between life circumstances of students and their possible educational\nachievements. The ontology was developed utilizing suggestions from\nChatGPT-3.5-010422 and validated using peer-reviewed research articles. The\nfirst version of developed ontology was evaluated by human experts in the field\nof education and validated using standard ontology evaluation software. This\nversion of the SDoEd ontology contains 231 domain concepts, 10 object\nproperties, and 24 data properties",
        "Quantum machine learning (QML) has emerged as a promising domain to leverage\nthe computational capabilities of quantum systems to solve complex\nclassification tasks. In this work, we present the first comprehensive QML\nstudy by benchmarking the MedMNIST-a diverse collection of medical imaging\ndatasets on a 127-qubit real IBM quantum hardware, to evaluate the feasibility\nand performance of quantum models (without any classical neural networks) in\npractical applications. This study explores recent advancements in quantum\ncomputing such as device-aware quantum circuits, error suppression, and\nmitigation for medical image classification. Our methodology is comprised of\nthree stages: preprocessing, generation of noise-resilient and\nhardware-efficient quantum circuits, optimizing\/training of quantum circuits on\nclassical hardware, and inference on real IBM quantum hardware. Firstly, we\nprocess all input images in the preprocessing stage to reduce the spatial\ndimension due to quantum hardware limitations. We generate hardware-efficient\nquantum circuits using backend properties expressible to learn complex patterns\nfor medical image classification. After classical optimization of QML models,\nwe perform inference on real quantum hardware. We also incorporate advanced\nerror suppression and mitigation techniques in our QML workflow, including\ndynamical decoupling (DD), gate twirling, and matrix-free measurement\nmitigation (M3) to mitigate the effects of noise and improve classification\nperformance. The experimental results showcase the potential of quantum\ncomputing for medical imaging and establish a benchmark for future advancements\nin QML applied to healthcare."
      ]
    }
  },
  {
    "id":2411.06513,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"Advances and Open Problems in Federated Learning",
    "start_abstract":"The term Federated Learning was coined as recently 2016 to describe a machine learning setting where multiple entities collaborate in solving problem, under the coordination of central server or service provider. Each client\u2019s raw data is stored locally and not exchanged transferred; instead, focused updates intended for immediate aggregation are used achieve objective. Since then, topic has gathered much interest across many different disciplines realization that these interdisciplinary problems likely requires just but techniques from distributed optimization, cryptography, security, differential privacy, fairness, compressed sensing, systems, information theory, statistics, more. This monograph contributions leading experts disciplines, who latest state-of-the art their perspective. These have been carefully curated into comprehensive treatment enables reader understand work done get pointers effort required solve before can become reality practical systems. Researchers working area systems will find this an enlightening read may inspire them on challenging issues outlined. up speed quickly easily what increasingly important topic: Learning.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "The future of digital health with federated learning"
      ],
      "abstract":[
        "Abstract Data-driven machine learning (ML) has emerged as a promising approach for building accurate and robust statistical models from medical data, which is collected in huge volumes by modern healthcare systems. Existing data not fully exploited ML primarily because it sits silos privacy concerns restrict access to this data. However, without sufficient will be prevented reaching its full potential and, ultimately, making the transition research clinical practice. This paper considers key factors contributing issue, explores how federated (FL) may provide solution future of digital health highlights challenges considerations that need addressed."
      ],
      "categories":[
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "Iterative phase retrieval algorithm for space-variant PSF in optical\n  systems with aberrations",
        "Network-Driven Global Stability Analysis: SVIRS Epidemic Model",
        "VAEs and GANs: Implicitly Approximating Complex Distributions with\n  Simple Base Distributions and Deep Neural Networks -- Principles, Necessity,\n  and Limitations",
        "The Equation of State of QCD up to very high temperatures",
        "Toward Large-Scale Distributed Quantum Long Short-Term Memory with\n  Modular Quantum Computers",
        "Domination Parameters of Graph Covers",
        "$L^2$-estimates on flat vector bundles and Pr\\'ekopa's theorem",
        "Unveiling potential candidates for rare-earth-free permanent magnet and\n  magnetocaloric effect applications: a high throughput screening in Fe-N\n  alloys",
        "QPEs as Lense-Thirring precession of super-Eddington flows",
        "On monotonicity of heat kernels: a new example and counterexamples",
        "3D Surface Reconstruction and Volume Approximation via the meshless\n  methods",
        "SOE's ESG Performance on Financial Flexibility: The Evidence from the\n  Hong Kong Stock Market",
        "Unsupervised optimal deep transfer learning for classification under\n  general conditional shift",
        "Multivariable Behavioral Change Modeling of Epidemics in the Presence of\n  Undetected Infections",
        "SWIFT: Mapping Sub-series with Wavelet Decomposition Improves Time\n  Series Forecasting",
        "Sabotage and Free Riding in Contests with a Group-Specific\n  Public-Good\/Bad Prize",
        "Purported quantitative support for multiple introductions of SARS-CoV-2\n  into humans is an artefact of an imbalanced hypothesis testing framework",
        "Stretching the Printability Metric in Direct-ink Writing with Highly\n  Extensible Yield-Stress Fluids",
        "On the Baum-Connes conjecture for $D_{\\infty}$",
        "Almost sharp variational estimates for discrete truncated operators of\n  Carleson type",
        "Markov fractions and the slopes of the exceptional bundles on $\\mathbb\n  P^2$",
        "Large-time estimates for the Dirichlet heat equation in exterior domains",
        "The Impact of Digitalisation and Sustainability on Inclusiveness:\n  Inclusive Growth Determinants",
        "A study of the Antlion Random Walk",
        "Signatures of modified gravity from the gravitational Aharonov-Bohm\n  effect",
        "Restricted type estimates on the Bergman projection of some singular\n  domains",
        "Sparse Orthogonal Matching Pursuit-based Parameter Estimation for\n  Integrated Sensing and Communications",
        "Explainable AI-Driven Neural Activity Analysis in Parkinsonian Rats\n  under Electrical Stimulation",
        "Benchmarking a magnon-scattering reservoir with modal and temporal\n  multiplexing"
      ],
      "abstract":[
        "Iterative phase retrieval algorithms are widely used in digital optics for\ntheir efficiency and simplicity. Conventionally, these algorithms do not\nconsider aberrations as they assume an ideal, aberration-free optical system.\nHere, we propose modified iterative phase retrieval algorithms that take into\naccount the space-invariant and space-variant point spread function of the\noptical system.",
        "An epidemic Susceptible-Vaccinated-Infected-Removed-Susceptible (SVIRS) model\nis presented on a weighted-undirected network with graph Laplacian diffusion.\nDisease-free equilibrium always exists while the existence and uniqueness of\nendemic equilibrium have been shown. When the basic reproduction number is\nbelow unity, the disease-free equilibrium is asymptotically globally stable.\nThe endemic equilibrium is asymptotically globally stable if the basic\nreproduction number is above unity. Numerical analysis is illustrated with a\nroad graph of the state of Minnesota. The effect of all important model\nparameters has been discussed.",
        "This tutorial focuses on the fundamental architectures of Variational\nAutoencoders (VAE) and Generative Adversarial Networks (GAN), disregarding\ntheir numerous variations, to highlight their core principles. Both VAE and GAN\nutilize simple distributions, such as Gaussians, as a basis and leverage the\npowerful nonlinear transformation capabilities of neural networks to\napproximate arbitrarily complex distributions. The theoretical basis lies in\nthat a linear combination of multiple Gaussians can almost approximate any\nprobability distribution, while neural networks enable further refinement\nthrough nonlinear transformations. Both methods approximate complex data\ndistributions implicitly. This implicit approximation is crucial because\ndirectly modeling high-dimensional distributions explicitly is often\nintractable. However, the choice of a simple latent prior, while\ncomputationally convenient, introduces limitations. In VAEs, the fixed Gaussian\nprior forces the posterior distribution to align with it, potentially leading\nto loss of information and reduced expressiveness. This restriction affects\nboth the interpretability of the model and the quality of generated samples.",
        "We present the non-perturbative computation of the entropy density in QCD for\ntemperatures ranging from 3 GeV up to the electro-weak scale, using $N_f=3$\nflavours of massless O$(a)$-improved Wilson fermions. We adopt a new strategy\ndesigned to be computationally efficient and based on formulating thermal QCD\nin a moving reference frame, where the fields satisfy shifted boundary\nconditions in the temporal direction and periodic boundary conditions along the\nspatial ones. In this setup the entropy density can be computed as the\nderivative of the free-energy density with respect to the shift parameter. For\neach physical temperature, we perform Monte Carlo simulations at four values of\nthe lattice spacing in order to extrapolate the numerical data of the entropy\ndensity to the continuum limit. We achieve a final accuracy of approximatively\n$0.5$-$1.0\\%$ and our results are compared with predictions from\nhigh-temperature perturbation theory.",
        "In this work, we introduce a Distributed Quantum Long Short-Term Memory\n(QLSTM) framework that leverages modular quantum computing to address\nscalability challenges on Noisy Intermediate-Scale Quantum (NISQ) devices. By\nembedding variational quantum circuits into LSTM cells, the QLSTM captures\nlong-range temporal dependencies, while a distributed architecture partitions\nthe underlying Variational Quantum Circuits (VQCs) into smaller, manageable\nsubcircuits that can be executed on a network of quantum processing units. We\nassess the proposed framework using nontrivial benchmark problems such as\ndamped harmonic oscillators and Nonlinear Autoregressive Moving Average\nsequences. Our results demonstrate that the distributed QLSTM achieves stable\nconvergence and improved training dynamics compared to classical approaches.\nThis work underscores the potential of modular, distributed quantum computing\narchitectures for large-scale sequence modelling, providing a foundation for\nthe future integration of hybrid quantum-classical solutions into advanced\nQuantum High-performance computing (HPC) ecosystems.",
        "A graph $G$ is a \\emph{cover} of a graph $F$ if there exists an onto mapping\n$\\pi : V(G) \\to V(F)$, called a (\\emph{covering}) \\emph{projection}, such that\n$\\pi$ maps the neighbours of any vertex $v$ in $G$ bijectively onto the\nneighbours of $\\pi(v)$ in $F$. This paper is the first attempt to study the\nconnection between domination parameters and graph covers. We focus on the\ndomination number, the total domination number, and the connected domination\nnumber. We prove tight upper bounds for the domination parameters of $G$.\nMoreover, we prove lower bounds for the domination parameters of $G$. Finally,\nwe propose a conjecture on the lower bound for the domination number of $G$ and\nprovide evidence to support the conjecture.",
        "In this paper, we will construct H\\\"ormander's $L^2$-estimate of the operator\n$d$ on a flat vector bundle over a $p$-convex Riemannian manifold and discuss\nsome geometric applications of it. In particular, we will generalize the\nclassical Pr\\'ekopa's theorem in convex analysis.",
        "Based on high-throughput density functional theory calculations, we have\nfound 49 ferromag-netic cases in FexN1-x (0<x<1) compounds, focusing especially\non permanent magnet and giant magnetocaloric effect applications. It is found\nthat 15 compounds are potential permanent mag-nets with a magneto-crystalline\nanisotropy energy more than 1 MJ\/m3, filling in the gap of appli-cation\nspectrum between high-performance and widely used permanents. Among the\npotential permanent magnets, Fe2N can be classified as a hard magnet while the\nother 14 compounds can be classified as semi-hard magnets. According to the\ncalculations of magnetic deformation proxy, 40 compounds are identified as\npotential giant magnetocaloric effect candidates. We suspect that Fe-N\ncompounds provide fine opportunities for applications in both rare-earth free\npermanent magnets and magnetocaloric effect.",
        "Quasi-periodic eruptions (QPEs) are a recently identified class of X-ray\ntransient associated with tidal disruption events by supermassive black holes,\nand for which there are multiple possible explanations. In this paper we\npresent a simple model which requires the black hole be spinning, be misaligned\nwith the accretion flow (both conditions of which are almost certainly met) and\nthat the accretion rate is a few times the Eddington limit. We speculate that\nthe resulting Lense-Thirring torques force the disc and entrained outflows to\nprecess, leading to increased X-ray flux when the wind-cone is oriented at\nlower inclinations to the observer. We test the range of parameters for which\nthis model could explain the period and brightness of the QPE events discovered\nthus far, and make qualitative comparisons between the observed X-ray spectra\nand lightcurves to those extracted from GR-RMHD simulations. Overall, we find\nsome areas of promising concordance, and identify challenges related to the\ndetails of current simulations.",
        "We discover a new, non-radial example of a manifold whose heat kernel\ndecreases monotonically along all minimal geodesics. We also classify the flat\ntori with this monotonicity property. Furthermore, we show that for a generic\nmetric on any smooth manifold the monotonicity property fails at large times.\nThis answers a recent question of Alonso-Or\\'an, Chamizo, Mart\\'inez, and Mas.",
        "In this paper, we propose several mathematical models for 3D surface\nreconstruction and volume estimation from a set of scattered cloud data. Three\nmeshless methods including the interpolation-based method by RBF, PDE-based\napproach by Kansa's method and the Method of Fundamental Solutions are employed\nand compared. For the optimal recovery of the surfaces, the selection of free\nparameters in related PDE models are further studied and analyzed. Besides,\nseveral criteria like distance are employed in above methods instead of the\nclassical parameter lambda determination strategy, which leads to a more\nreliable reconstruction performance. Finally, the volume estimation of 3D\nirregular objects is proposed based on the optimal reconstructed geometric\nmodels via proposed meshless methods. Numerous numerical examples are presented\nto demonstrate the effectiveness of the proposed surface reconstruction methods\nand the volume estimation strategy.",
        "As the global economic environment becomes increasingly unstable, enhancing\nfinancial flexibility to cope with risks has become the consensus of many\ncompanies. At the same time, environmental, social, and governance (ESG)\nperformance may be one of the effective ways. We studied the impact of a firm's\nESG performance on its financial flexibility with a sample of companies listed\non the Hong Kong stock market from 2018 to 2022. The empirical results show\nthat good environmental, social and governance performance can significantly\nimprove a firm's financial flexibility. In addition, this paper also finds that\nthe influence of ESG performance on financial flexibility is weak for\nstate-owned enterprises due to the influence of governance structure and market\ncharacteristics. Finally, the further analysis shows that there is a mediating\nrole played by financing constraints in this process. This study can provide\nbackground information for state-owned enterprises' governance, information\ndisclosure, and corporate operations. It also has guiding significance for\nrelevant investors, management and officials.",
        "Classifiers trained solely on labeled source data may yield misleading\nresults when applied to unlabeled target data drawn from a different\ndistribution. Transfer learning can rectify this by transferring knowledge from\nsource to target data, but its effectiveness frequently relies on stringent\nassumptions, such as label shift. In this paper, we introduce a novel General\nConditional Shift (GCS) assumption, which encompasses label shift as a special\nscenario. Under GCS, we demonstrate that both the target distribution and the\nshift function are identifiable. To estimate the conditional probabilities\n${\\bm\\eta}_P$ for source data, we propose leveraging deep neural networks\n(DNNs). Subsequent to transferring the DNN estimator, we estimate the target\nlabel distribution ${\\bm\\pi}_Q$ utilizing a pseudo-maximum likelihood approach.\nUltimately, by incorporating these estimates and circumventing the need to\nestimate the shift function, we construct our proposed Bayes classifier. We\nestablish concentration bounds for our estimators of both ${\\bm\\eta}_P$ and\n${\\bm\\pi}_Q$ in terms of the intrinsic dimension of ${\\bm\\eta}_P$ . Notably,\nour DNN-based classifier achieves the optimal minimax rate, up to a logarithmic\nfactor. A key advantage of our method is its capacity to effectively combat the\ncurse of dimensionality when ${\\bm\\eta}_P$ exhibits a low-dimensional\nstructure. Numerical simulations, along with an analysis of an Alzheimer's\ndisease dataset, underscore its exceptional performance.",
        "Epidemic models are invaluable tools to understand and implement strategies\nto control the spread of infectious diseases, as well as to inform public\nhealth policies and resource allocation. However, current modeling approaches\nhave limitations that reduce their practical utility, such as the exclusion of\nhuman behavioral change in response to the epidemic or ignoring the presence of\nundetected infectious individuals in the population. These limitations became\nparticularly evident during the COVID-19 pandemic, underscoring the need for\nmore accurate and informative models. Motivated by these challenges, we develop\na novel Bayesian epidemic modeling framework to better capture the complexities\nof disease spread by incorporating behavioral responses and undetected\ninfections. In particular, our framework makes three contributions: 1)\nleveraging additional data on hospitalizations and deaths in modeling the\ndisease dynamics, 2) accounting data uncertainty arising from the large\npresence of asymptomatic and undetected infections, and 3) allowing the\npopulation behavioral change to be dynamically influenced by multiple data\nsources (cases and deaths). We thoroughly investigate the properties of the\nproposed model via simulation, and illustrate its utility on COVID-19 data from\nMontreal and Miami.",
        "In recent work on time-series prediction, Transformers and even large\nlanguage models have garnered significant attention due to their strong\ncapabilities in sequence modeling. However, in practical deployments,\ntime-series prediction often requires operation in resource-constrained\nenvironments, such as edge devices, which are unable to handle the\ncomputational overhead of large models. To address such scenarios, some\nlightweight models have been proposed, but they exhibit poor performance on\nnon-stationary sequences. In this paper, we propose $\\textit{SWIFT}$, a\nlightweight model that is not only powerful, but also efficient in deployment\nand inference for Long-term Time Series Forecasting (LTSF). Our model is based\non three key points: (i) Utilizing wavelet transform to perform lossless\ndownsampling of time series. (ii) Achieving cross-band information fusion with\na learnable filter. (iii) Using only one shared linear layer or one shallow MLP\nfor sub-series' mapping. We conduct comprehensive experiments, and the results\nshow that $\\textit{SWIFT}$ achieves state-of-the-art (SOTA) performance on\nmultiple datasets, offering a promising method for edge computing and\ndeployment in this task. Moreover, it is noteworthy that the number of\nparameters in $\\textit{SWIFT-Linear}$ is only 25\\% of what it would be with a\nsingle-layer linear model for time-domain prediction. Our code is available at\nhttps:\/\/github.com\/LancelotXWX\/SWIFT.",
        "We study contests in which two groups compete to win (or not to win) a\ngroup-specific public-good\/bad prize. Each player in the groups can exert two\ntypes of effort: one to help her own group win the prize, and one to sabotage\nher own group's chances of winning it. The players in the groups choose their\neffort levels simultaneously and independently. We introduce a specific form of\ncontest success function that determines each group's probability of winning\nthe prize, taking into account players' sabotage activities. We show that two\ntypes of purestrategy Nash equilibrium occur, depending on parameter values:\none without sabotage activities and one with sabotage activities. In the first\ntype, only the highest-valuation player in each group expends positive effort,\nwhereas, in the second type, only the lowest-valuation player in each group\nexpends positive effort.",
        "A prominent report claimed substantial support for two introductions of\nSARS-CoV-2 into humans using a calculation that combined phylodynamic\ninferences and epidemic models. Inspection of the calculation identifies an\nimbalance in the hypothesis testing framework that confounds this result; the\nsingle-introduction model was tested against more stringent conditions than the\ntwo-introduction model. Here, I show that when the two-introduction model is\ntested against the same conditions, the support disappears.",
        "Direct-ink writing leverages the rheological complexity of yield-stress\nfluids to construct complex geometries, particularly those with large gaps\nacross internal structures. However, extensional rheological properties have\nrarely been considered in work that studies rheology-printability correlations.\nHere, we test our hypothesis that extensional properties correlate with\ndrawability, a key indicator of printability that signifies speed robustness,\nprinting resolution, and gap-spanning performance. We formulated cementitious\nsuspensions using hydroxyapatite (HAp) particles, independently tuning them for\nyield stress and extensibility, two crucial rheological properties, and\ntest-printed. To enhance extensibility, we incorporated hydroxypropyl\nmethylcellulose as a polymeric modifier, but this enhancement may decrease as\nyield stress increases, presenting a challenge in materials design. We\nmodulated particle interactions to achieve a wide range of yield stress and\nextensibility, allowing for rigorous testing of our hypothesis. This approach\ncreated inks with high extensibility and high yield stress, generally\nconsidered mutually exclusive properties. We evaluated correlations between\ndrawability and key rheological properties, finding the strongest positive\ncorrelation with extensional failure strains (strain-to-break) rather than\nyield stress. We establish a bijective property-manufacturing relationship\n(one-on-one mapping of shear yield stress to buildability and extensional\nstrain-to-break to drawability) by combining our findings on drawability with\nprevious studies on buildability. This relationship provides a comprehensive\nframework for designing high-performance inks that can be self-supporting,\ncapable of high-speed printing, and allow gap-spanning features.",
        "We make an exposition of the proof of the Baum-Connes conjecture for the\ninfinite dihedral group following the ideas of Higson and Kasparov.",
        "We establish $r$-variational estimates for discrete truncated Carleson-type\noperators on $\\ell^p$ for $1<p<\\infty$. Notably, these estimates are sharp and\nenhance the results obtained by Krause and Roos (J. Eur. Math. Soc. 2022, J.\nFunct. Anal. 2023), up to a logarithmic loss related to the scale. On the other\nhand, as $r$ approaches infinity, the consequences align with the estimates\nproved by Krause and Roos. Moreover, for the case of quadratic phases, we\nremove this logarithmic loss with respect to the scale, at the cost of\nincreasing $p$ slightly.",
        "We show that the Markov fractions introduced recently by Boris Springborn are\nprecisely the slopes of the exceptional vector bundles on $\\mathbb P^2$ studied\nin 1980s by Dr\\`ezet and Le Potier and by Rudakov. In particular, we provide a\nsimpler proof of Rudakov's result claiming that the ranks of the exceptional\nbundles on $\\mathbb P^2$ are Markov numbers.",
        "We give large-time asymptotic estimates, both in uniform and $L^1$ norms, for\nsolutions of the Dirichlet heat equation in the complement of a bounded open\nset of $\\mathbb{R}^d$ satisfying certain technical assumptions. We always\nassume that the initial datum has suitable finite moments (often, finite first\nmoment). All estimates include an explicit rate of approach to the asymptotic\nprofiles at the different scales natural to the problem, in analogy with the\nGaussian behaviour of the heat equation in the full space. As a consequence we\nobtain by an approximation procedure the asymptotic profile, with rates, for\nthe Dirichlet heat kernel in these exterior domains. The estimates on the rates\nare new even when the domain is the complement of the unit ball in\n$\\mathbb{R}^d$, except for previous results by Uchiyama in dimension 2, which\nwe are able to improve in some scales. We obtain that the heat kernel behaves\nasymptotically as the heat kernel in the full space, with a factor that takes\ninto account the shape of the domain through a harmonic profile, and a second\nfactor which accounts for the loss of mass through the boundary. The main ideas\nwe use come from entropy methods in PDE and probability, whose application\nseems to be new in the context of diffusion problems in exterior domains.",
        "Inclusiveness and economic development have been slowed by the pandemics and\nmilitary conflicts. This study investigates the main determinants of\ninclusiveness at the European level. A multi-method approach is used, with\nPrincipal Component Analysis (PCA) applied to create the Inclusiveness Index\nand Generalised Method of Moments (GMM) analysis used to investigate the\ndeterminants of inclusiveness. The data comprises a range of 22 years, from\n2000 to 2021, for 32 European countries. The determinants of inclusiveness and\ntheir effects were identified. First, economic growth, industrial upgrading,\nelectricity consumption, digitalisation, and the quantitative aspect of\ngovernance, all have a positive impact on inclusive growth in Europe. Second,\nthe level of CO2 emissions and inflation have a negative impact on\ninclusiveness. Tomorrow's inclusive and sustainable growth must include\ninvestments in renewable energy, digital infrastructure, inequality policies,\nsustainable governance, human capital, and inflation management. These findings\ncan help decision makers design inclusive growth policies.",
        "This paper treats a new type of random walk referred to as an Antlion Random\nWalk (ARW), which is motivated by mathematical modeling of the decision-making\nprocess using chaotic semiconductor lasers with memory parameters. We discuss\nthe dependency of the property of the probability distribution of ARWs on the\nmemory parameter $\\alpha$ and discuss uniqueness of them in contrast to the\nconventional, simple RWs through similarity to the normal distribution.",
        "To date, no observational confirmation of dark matter particles has been\nfound. In this paper, we put forward an alternative approach to inferring\nevidence for dark matter through modified gravity, without invoking fundamental\ndark matter particles. Specifically, we explore the possibility of extracting\nsignatures of Kaluza-Klein gravity through the gravitational Aharonov-Bohm\neffect. Kaluza-Klein theory has recently been proposed as an alternative to the\ndark sector, and predicts a tower of particles, including spin-0 and spin-1\ngravitons alongside the usual spin-2 gravitons, which can gravitationally\ncouple to matter. We thus analyze a quantum system in free fall around a\ngravitating body in the presence of a modified Yukawa-like gravitational\npotential, and determine the gravitational phase induced by the additional\ndegrees of freedom introduced by the Kaluza-Klein model. Our results reveal\nthat, in addition to the usual result from General Relativity, the quantum wave\nfunction of the system exhibits an additional effect: a splitting of the energy\nlevels with a new quantum number due to the extra vector gravitational degrees\nof freedom. The energy splitting difference between general relativity and\nKaluza-Klein gravity is found to be of the order of meV for an atomic system\nand eV for a nuclear system. Similar values also arise in generic modified\ngravity models and can be feasibly tested in the future. Numerical estimates\nfor the graviton mass are also provided, and potential imprints on\ngravitational waves are mentioned.",
        "We obtain (weighted) restricted type estimates for the Bergman projection\noperator on monomial polyhedra, a class of domains generalizing the Hartogs\ntriangle. From these estimates, we recapture $L^p$ boundedness results of the\nBergman projection on these domains. On some monomial polyhedra, we also\ndiscover that the Bergman projection could fail to be of weak type $(q_*,q_*)$\nwhere $q_*$ is the right endpoint of the interval of $L^p$-regularity of the\ndomain.",
        "Accurate parameter estimation such as angle of arrival (AOA) is essential to\nenhance the performance of integrated sensing and communication (ISAC) in\nmmWave multiple-input multiple-output (MIMO) systems. This work presents a\nsensing-aided communication channel estimation mechanism, where the sensing\nchannel shares the same AOA with the uplink communication channel. First, we\npropose a novel orthogonal matching pursuit (OMP)-based method for coarsely\nestimating the AOA in a sensing channel, offering improved accuracy compared to\nconventional methods that rely on rotational invariance techniques. Next, we\nrefine the coarse estimates obtained in the first step by modifying the\nSpace-Alternating Generalized Expectation Maximization algorithm for fine\nparameter estimation. Through simulations and mathematical analysis, we\ndemonstrate that scenarios with shared AOA achieve a better Cramer-Rao lower\nbound (CRLB) than those without sharing. This finding highlights the potential\nof leveraging joint sensing and communication channels to enhance parameter\nestimation accuracy, particularly in channel or location estimation\napplications.",
        "Parkinson's disease (PD) is a neurodegenerative disorder characterized by\nmotor dysfunction and abnormal neural oscillations. These symptoms can be\nmodulated through electrical stimulation. Traditional neural activity analysis\nin PD has typically relied on statistical methods, which often introduce bias\nowing to the need for expert-driven feature extraction. To address this\nlimitation, we explore an explainable artificial intelligence (XAI) approach to\nanalyze neural activity in Parkinsonian rats receiving electrical stimulation.\nElectrocorticogram (ECoG) signals were collected before and after electrical\nstimulation using graphene-based electrodes that enable less-invasive\nmonitoring and stimulation in PD. EEGNet, a convolutional neural network,\nclassified these ECoG signals into pre- and post-stimulation states. We applied\nlayer-wise relevance propagation, an XAI technique, to identify key neural\ninputs contributing to the model's decisions, incorporating the spatial\nelectrode information matched to the cortex map. The XAI analysis highlighted\narea-specific importance in beta and gamma frequency bands, which could not be\ndetected through mean comparison analyses relying on feature extraction. These\nfindings demonstrate the potential of XAI in analyzing neural dynamics in\nneurodegenerative disorders such as PD, suggesting that the integration of\ngraphene-based electrodes with advanced deep learning models offers a promising\nsolution for real-time PD monitoring and therapy.",
        "Physical reservoir computing has emerged as a powerful framework for\nexploiting the inherent nonlinear dynamics of physical systems to perform\ncomputational tasks. Recently, we presented the magnon-scattering reservoir,\nwhose internal nodes are given by the fundamental wave-like excitations of\nferromagnets called magnons. These excitations can be geometrically-quantized\nand, in response to an external stimulus, show transient nonlinear scattering\ndynamics that can be harnessed to perform memory and nonlinear transformation\ntasks. Here, we test a magnon-scattering reservoir in a single magnetic disk in\nthe vortex state towards two key performance indicators for physical reservoir\ncomputing, the short-term memory and parity-check tasks. Using time-resolved\nBrillouin-light-scattering microscopy, we measure the evolution of the\nreservoir's spectral response to an input sequence consisting of random binary\ninputs encoded in microwave pulses with two distinct frequencies. Two different\noutput spaces of the reservoir are defined, one based on the time-averaged\nfrequency spectra and another based on temporal multiplexing. Our results\ndemonstrate that the memory and nonlinear transformation capability do not\ndepend on the chosen read-out scheme as long as the dimension of the output\nspace is large enough to capture all nonlinear features provided by the\nmagnon-magnon interactions. This further shows that solely the nonlinear\nmagnons in the physical system, not the read-out, determine the reservoir's\ncapacity."
      ]
    }
  }
]