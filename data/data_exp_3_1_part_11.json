[
  {
    "id":2411.04775,
    "research_type":"applied",
    "start_id":"b7",
    "start_title":"A Data\u2013Driven Approximation of the Koopman Operator: Extending Dynamic Mode Decomposition",
    "start_abstract":"The Koopman operator is a linear but infinite dimensional operator that\ngoverns the evolution of scalar observables defined on the state space of an\nautonomous dynamical system, and is a powerful tool for the analysis and\ndecomposition of nonlinear dynamical systems. In this manuscript, we present a\ndata driven method for approximating the leading eigenvalues, eigenfunctions,\nand modes of the Koopman operator. The method requires a data set of snapshot\npairs and a dictionary of scalar observables, but does not require explicit\ngoverning equations or interaction with a \"black box\" integrator. We will show\nthat this approach is, in effect, an extension of Dynamic Mode Decomposition\n(DMD), which has been used to approximate the Koopman eigenvalues and modes.\nFurthermore, if the data provided to the method are generated by a Markov\nprocess instead of a deterministic dynamical system, the algorithm approximates\nthe eigenfunctions of the Kolmogorov backward equation, which could be\nconsidered as the \"stochastic Koopman operator\" [1]. Finally, four illustrative\nexamples are presented: two that highlight the quantitative performance of the\nmethod when presented with either deterministic or stochastic data, and two\nthat show potential applications of the Koopman eigenfunctions.",
    "start_categories":[
      "Mathematical Analysis"
    ],
    "start_fields":[

    ],
    "target_paper":{
      "id":[
        "b21",
        "b14"
      ],
      "title":[
        "Towards Scalable Koopman Operator Learning: Convergence Rates and A Distributed Learning Algorithm",
        "VAMPnets for deep learning of molecular kinetics"
      ],
      "abstract":[
        "We propose an alternating optimization algorithm to the nonconvex Koopman operator learning problem for nonlinear dynamic systems. show that proposed will converge a critical point with rate O(1\/T) and $O\\left( {\\frac{1}{{\\log T}}} \\right)$ constant diminishing rates, respectively, under some mild conditions. To cope high dimensional dynamical systems, we present first-ever distributed algorithm. has same convergence properties as centralized learning, in absence of optimal tracker, so long basis functions satisfy set state-based decomposition Numerical experiments are provided complement our theoretical results.",
        "Abstract There is an increasing demand for computing the relevant structures, equilibria, and long-timescale kinetics of biomolecular processes, such as protein-drug binding, from high-throughput molecular dynamics simulations. Current methods employ transformation simulated coordinates into structural features, dimension reduction, clustering dimension-reduced data, estimation a Markov state model or related interconversion rates between structures. This handcrafted approach demands substantial amount modeling expertise, poor decisions at any step will lead to large errors. Here we variational processes (VAMP) develop deep learning framework using neural networks, dubbed VAMPnets. A VAMPnet encodes entire mapping states, thus combining whole data processing pipeline in single end-to-end framework. Our method performs equally better than state-of-the-art provides easily interpretable few-state kinetic models."
      ],
      "categories":[
        "eess.SP",
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "The complexities of falling freely",
        "On Construction, Properties and Simulation of Haar-Based Multifractional\n  Processes",
        "Characterization of Residual Charge Images in LSST Camera e2v CCDs",
        "A Bias-Correction Decentralized Stochastic Gradient Algorithm with\n  Momentum Acceleration",
        "CAMEF: Causal-Augmented Multi-Modality Event-Driven Financial\n  Forecasting by Integrating Time Series Patterns and Salient Macroeconomic\n  Announcements",
        "Robust Support Vector Machines for Imbalanced and Noisy Data via Benders\n  Decomposition",
        "Recent progress in high-temperature superconducting undulators",
        "Steady State Classification of Allee Effect System",
        "Hall Coefficient of the Intercalated Graphite CaC$_6$ in the Uniaxial\n  CDW Ground State",
        "ARFlow: Autogressive Flow with Hybrid Linear Attention",
        "Neighborhoods, connectivity, and diameter of the nilpotent graph of a\n  finite group",
        "The day-ahead scenario generation method for new energy based on an\n  improved conditional generative diffusion model",
        "A Jacobian-free Newton-Krylov method for cell-centred finite volume\n  solid mechanics",
        "The defocusing Calogero--Moser derivative nonlinear Schr{\\\"o}dinger\n  equation with a nonvanishing condition at infinity",
        "MemorySAM: Memorize Modalities and Semantics with Segment Anything Model\n  2 for Multi-modal Semantic Segmentation",
        "Lifetime measurement of the 5s5p 1P1 state in strontium",
        "Frequency-resolved time lags due to X-ray disk reprocessing in AGN",
        "Regularity of edge ideals of powers of graphs",
        "Symmetries of the q-deformed real projective line",
        "Functoriality of Coulomb branches",
        "Exploring Large Language Models for Translating Romanian Computational\n  Problems into English",
        "On the Conditional Phase Distribution of the TWDP Multipath Fading\n  Process",
        "Free products and rescalings involving non-separable abelian von Neumann\n  algebras",
        "The effect of a band gap gradient on the radiative losses in the open\n  circuit voltage of solar cells",
        "Andromeda XXXV: The Faintest Dwarf Satellite of the Andromeda Galaxy",
        "READ: Reinforcement-based Adversarial Learning for Text Classification\n  with Limited Labeled Data",
        "Heating of a semi-infinite Hooke chain",
        "Pullback measure attractors for non-autonomous stochastic\n  FitzHugh-Nagumo system with distribution dependence on unbounded domains"
      ],
      "abstract":[
        "Suppose you drop a coin from 10 feet above the ground. How long does it take\nto reach the ground? This routine exercise is well-known to every AP physics\nand calculus student: the answer is given by a formula that assumes constant\nacceleration due to gravity. But what if you ask the same question in the more\nrealistic scenario of non-constant acceleration following an inverse square\nlaw? In this article, we explain the analysis of this realistic scenario using\nfreshman-level calculus and examine some implications. As a bonus, we also\nanswer the following intriguing question: Suppose the Earth were to\ninstantaneously collapse to a mathematical point at its center. How long would\nit take for us surface dwellers to fall to the center?",
        "Multifractional processes extend the concept of fractional Brownian motion by\nreplacing the constant Hurst parameter with a time-varying Hurst function. This\nextension allows for modulation of the roughness of sample paths over time. The\npaper introduces a new class of multifractional processes, the Gaussian\nHaar-based multifractional processes (GHBMP), which is based on the Haar\nwavelet series representations. The resulting processes cover a significantly\nbroader set of Hurst functions compared to the existing literature, enhancing\ntheir suitability for both practical applications and theoretical studies. The\ntheoretical properties of these processes are investigated. Simulation studies\nconducted for various Hurst functions validate the proposed model and\ndemonstrate its applicability, even for Hurst functions exhibiting\ndiscontinuous behaviour.",
        "LSST Camera CCDs produced by the manufacturer e2v exhibit strong and novel\nresidual charge images when exposed to bright sources. These manifest in images\nfollowing bright exposures both in the same pixel areas as the bright source,\nand in the pixels trailing between the source and the serial register. Both of\nthese pose systematic challenges to the Rubin Observatory Legacy Survey of\nSpace and Time instrument signature removal. The latter trail region is\nespecially impactful as it affects a much larger pixel area in a less well\ndefined position. In our study of this effect at UC Davis, we imaged bright\nspots to characterize these residual charge effects. We find a strong\ndependence of the residual charge on the parallel clocking scheme, including\nthe relative levels of the clocking voltages, and the timing of gate phase\ntransition during the parallel transfer. Our study points to independent causes\nof residual charge in the bright spot region and trail region. We propose\npotential causes in both regions and suggest methodologies for minimizing\nresidual charge. We consider the trade-offs to these methods including\ndecreasing the camera's full well and dynamic range at the high end. Some of\nthese results and suggestions have been reviewed by the camera commissioning\nteam and may result in changes made to the clocking voltage scheme on the LSST\nCamera.",
        "Distributed stochastic optimization algorithms can simultaneously process\nlarge-scale datasets, significantly accelerating model training. However, their\neffectiveness is often hindered by the sparsity of distributed networks and\ndata heterogeneity. In this paper, we propose a momentum-accelerated\ndistributed stochastic gradient algorithm, termed Exact-Diffusion with Momentum\n(EDM), which mitigates the bias from data heterogeneity and incorporates\nmomentum techniques commonly used in deep learning to enhance convergence rate.\nOur theoretical analysis demonstrates that the EDM algorithm converges\nsub-linearly to the neighborhood of the optimal solution, the radius of which\nis irrespective of data heterogeneity, when applied to non-convex objective\nfunctions; under the Polyak-Lojasiewicz condition, which is a weaker assumption\nthan strong convexity, it converges linearly to the target region. Our analysis\ntechniques employed to handle momentum in complex distributed parameter update\nstructures yield a sufficiently tight convergence upper bound, offering a new\nperspective for the theoretical analysis of other momentum-based distributed\nalgorithms.",
        "Accurately forecasting the impact of macroeconomic events is critical for\ninvestors and policymakers. Salient events like monetary policy decisions and\nemployment reports often trigger market movements by shaping expectations of\neconomic growth and risk, thereby establishing causal relationships between\nevents and market behavior. Existing forecasting methods typically focus either\non textual analysis or time-series modeling, but fail to capture the\nmulti-modal nature of financial markets and the causal relationship between\nevents and price movements. To address these gaps, we propose CAMEF\n(Causal-Augmented Multi-Modality Event-Driven Financial Forecasting), a\nmulti-modality framework that effectively integrates textual and time-series\ndata with a causal learning mechanism and an LLM-based counterfactual event\naugmentation technique for causal-enhanced financial forecasting. Our\ncontributions include: (1) a multi-modal framework that captures causal\nrelationships between policy texts and historical price data; (2) a new\nfinancial dataset with six types of macroeconomic releases from 2008 to April\n2024, and high-frequency real trading data for five key U.S. financial assets;\nand (3) an LLM-based counterfactual event augmentation strategy. We compare\nCAMEF to state-of-the-art transformer-based time-series and multi-modal\nbaselines, and perform ablation studies to validate the effectiveness of the\ncausal learning mechanism and event types.",
        "This study introduces a novel formulation to enhance Support Vector Machines\n(SVMs) in handling class imbalance and noise. Unlike the conventional Soft\nMargin SVM, which penalizes the magnitude of constraint violations, the\nproposed model quantifies the number of violations and aims to minimize their\nfrequency. To achieve this, a binary variable is incorporated into the\nobjective function of the primal SVM formulation, replacing the traditional\nslack variable. Furthermore, each misclassified sample is assigned a priority\nand an associated constraint. The resulting formulation is a mixed-integer\nprogramming model, efficiently solved using Benders decomposition. The proposed\nmodel's performance was benchmarked against existing models, including Soft\nMargin SVM, weighted SVM, and NuSVC. Two primary hypotheses were examined: 1)\nThe proposed model improves the F1-score for the minority class in imbalanced\nclassification tasks. 2) The proposed model enhances classification accuracy in\nnoisy datasets. These hypotheses were evaluated using a Wilcoxon test across\nmultiple publicly available datasets from the OpenML repository. The results\nsupported both hypotheses (\\( p < 0.05 \\)). In addition, the proposed model\nexhibited several interesting properties, such as improved robustness to noise,\na decision boundary shift favoring the minority class, a reduced number of\nsupport vectors, and decreased prediction time. The open-source Python\nimplementation of the proposed SVM model is available.",
        "Considerable effort has been devoted to the development of superconducting\nundulators (SCUs) intended for particle accelerator-based light sources,\nincluding synchrotrons and free electron laser (FEL) facilities. Recently, a\nhigh-temperature superconducting (HTS) undulator prototype, consisting of\nstaggered-array Re-Ba-Cu-O bulks, achieved an on-axis sinusoidal magnetic field\nprofile with a peak amplitude B$_0$ of 2.1 T and a period length of 10 mm,\nresulting in a deflection parameter K = 1.96. Such a short period HTS undulator\nnot only enables the generation of higher-energy photons, but also supports the\nconstruction of economically feasible and compact FELs with shorter linear\naccelerators (LINACs). This article provides a comprehensive review of recent\nadvances in the staggered-array bulk HTS undulator as well as other types of\nHTS undulators. Furthermore, it offers insights into the development of\nengineering HTS undulator prototypes designed for deployment in synchrotron and\nfree electron laser (FEL) facilities. We conclude by discussing opportunities\nfor and the challenges facing the use of HTS undulators in practical\napplications.",
        "In this paper, we consider the steady state classification problem of the\nAllee effect system for multiple tribes. First, we reduce the high-dimensional\nmodel into several two-dimensional and three-dimensional algebraic systems such\nthat we can prove a comprehensive formula of the border polynomial for\narbitrary dimension. Then, we propose an efficient algorithm for classifying\nthe generic parameters according to the number of steady states, and we\nsuccessfully complete the computation for up to the seven-dimensional Allee\neffect system.",
        "We evaluate the Hall coefficient characterising magnetotransport in an\nintercalated graphite CaC$_6$ with the Fermi surface reconstructed by an\nuniaxial charge density wave from closed pockets to open sheets. As the typical\norder parameter, corresponding to the pseudo-gap in electronic spectrum and\nconsequently to spacing between electron trajectories in reciprocal space, is\nof the order of $10^2$K, magnetic breakdown in strong experimentally achievable\nfields of the order of 10T is inevitable. The classical expressions for the\ncomponents of the magnetoconductivity tensor are strongly modified by magnetic\nfield-assisted over-gap tunneling causing quantum interference. Due to magnetic\nbreakdown, all magnetoconductivity components undergo strong quantum\noscillations reflected in the Hall coefficient. In their nature, these are\ndifferent than standard Shubnikov de Haas oscillations which would not appear\nin a system with an open Fermi surface.",
        "Flow models are effective at progressively generating realistic images, but\nthey generally struggle to capture long-range dependencies during the\ngeneration process as they compress all the information from previous time\nsteps into a single corrupted image. To address this limitation, we propose\nintegrating autoregressive modeling -- known for its excellence in modeling\ncomplex, high-dimensional joint probability distributions -- into flow models.\nDuring training, at each step, we construct causally-ordered sequences by\nsampling multiple images from the same semantic category and applying different\nlevels of noise, where images with higher noise levels serve as causal\npredecessors to those with lower noise levels. This design enables the model to\nlearn broader category-level variations while maintaining proper causal\nrelationships in the flow process. During generation, the model\nautoregressively conditions the previously generated images from earlier\ndenoising steps, forming a contextual and coherent generation trajectory.\nAdditionally, we design a customized hybrid linear attention mechanism tailored\nto our modeling approach to enhance computational efficiency. Our approach,\ntermed ARFlow, under 400k training steps, achieves 14.08 FID scores on ImageNet\nat 128 * 128 without classifier-free guidance, reaching 4.34 FID with\nclassifier-free guidance 1.5, significantly outperforming the previous\nflow-based model SiT's 9.17 FID. Extensive ablation studies demonstrate the\neffectiveness of our modeling strategy and chunk-wise attention design.",
        "The nilpotent graph of a group $G$ is the simple and undirected graph whose\nvertices are the elements of $G$ and two distinct vertices are adjacent if they\ngenerate a nilpotent subgroup of $G$. Here we discuss some topological\nproperties of the nilpotent graph of a finite group $G$. Indeed, we\ncharacterize finite solvable groups whose closed neighborhoods are nilpotent\nsubgroups. Moreover, we study the connectivity of the graph $\\Gamma(G)$\nobtained removing all universal vertices from the nilpotent graph of $G$. Some\nupper bounds to the diameter of $\\Gamma(G)$ are provided when $G$ belongs to\nsome classes of groups.",
        "In the context of the rising share of new energy generation, accurately\ngenerating new energy output scenarios is crucial for day-ahead power system\nscheduling. Deep learning-based scenario generation methods can address this\nneed, but their black-box nature raises concerns about interpretability. To\ntackle this issue, this paper introduces a method for day-ahead new energy\nscenario generation based on an improved conditional generative diffusion\nmodel. This method is built on the theoretical framework of Markov chains and\nvariational inference. It first transforms historical data into pure noise\nthrough a diffusion process, then uses conditional information to guide the\ndenoising process, ultimately generating scenarios that satisfy the conditional\ndistribution. Additionally, the noise table is improved to a cosine form,\nenhancing the quality of the generated scenarios. When applied to actual wind\nand solar output data, the results demonstrate that this method effectively\ngenerates new energy output scenarios with good adaptability.",
        "This study investigates the efficacy of Jacobian-free Newton-Krylov methods\nin finite-volume solid mechanics. Traditional Newton-based approaches require\nexplicit Jacobian matrix formation and storage, which can be computationally\nexpensive and memory-intensive. In contrast, Jacobian-free Newton-Krylov\nmethods approximate the Jacobian's action using finite differences, combined\nwith Krylov subspace solvers such as the generalised minimal residual method\n(GMRES), enabling seamless integration into existing segregated finite-volume\nframeworks without major code refactoring. This work proposes and benchmarks\nthe performance of a compact-stencil Jacobian-free Newton-Krylov method against\na conventional segregated approach on a suite of test cases, encompassing\nvarying geometric dimensions, nonlinearities, dynamic responses, and material\nbehaviours. Key metrics, including computational cost, memory efficiency, and\nrobustness, are evaluated, along with the influence of preconditioning\nstrategies and stabilisation scaling. Results show that the proposed\nJacobian-free Newton-Krylov method outperforms the segregated approach in all\nlinear and nonlinear elastic cases, achieving order-of-magnitude speedups in\nmany instances; however, divergence is observed in elastoplastic cases,\nhighlighting areas for further development. It is found that preconditioning\nchoice impacts performance: a LU direct solver is fastest in small to\nmoderately-sized cases, while a multigrid method is more effective for larger\nproblems. The findings demonstrate that Jacobian-free Newton-Krylov methods are\npromising for advancing finite-volume solid mechanics simulations, particularly\nfor existing segregated frameworks where minimal modifications enable their\nadoption. The described implementations are available in the solids4foam\ntoolbox for OpenFOAM, inviting the community to explore, extend, and compare\nthese procedures.",
        "We consider the defocusing Calogero--Moser derivative nonlinear\nSchr{\\\"o}dinger equation\\begin{align*}i \\partial_{t} u+\\partial_{x}^2 u-2\\Pi\nD\\left(|u|^{2}\\right)u=0, \\quad (t,x ) \\in \\mathbb{R} \\times\n\\mathbb{R}\\end{align*}posed on $E := \\left\\{u \\in L^{\\infty}(\\mathbb{R}): u'\n\\in L^{2}(\\mathbb{R}), u'' \\in L^{2}(\\mathbb{R}), |u|^{2}-1 \\in\nL^{2}(\\mathbb{R})\\right\\}$. We prove the global well-posedness of this equation\nin $E$. Moreover, we give an explicit formula for the chiral solution to this\nequation.",
        "Research has focused on Multi-Modal Semantic Segmentation (MMSS), where\npixel-wise predictions are derived from multiple visual modalities captured by\ndiverse sensors. Recently, the large vision model, Segment Anything Model 2\n(SAM2), has shown strong zero-shot segmentation performance on both images and\nvideos. When extending SAM2 to MMSS, two issues arise: 1. How can SAM2 be\nadapted to multi-modal data? 2. How can SAM2 better understand semantics?\nInspired by cross-frame correlation in videos, we propose to treat multi-modal\ndata as a sequence of frames representing the same scene. Our key idea is to\n''memorize'' the modality-agnostic information and 'memorize' the semantics\nrelated to the targeted scene. To achieve this, we apply SAM2's memory\nmechanisms across multi-modal data to capture modality-agnostic features.\nMeanwhile, to memorize the semantic knowledge, we propose a training-only\nSemantic Prototype Memory Module (SPMM) to store category-level prototypes\nacross training for facilitating SAM2's transition from instance to semantic\nsegmentation. A prototypical adaptation loss is imposed between global and\nlocal prototypes iteratively to align and refine SAM2's semantic understanding.\nExtensive experimental results demonstrate that our proposed MemorySAM\noutperforms SoTA methods by large margins on both synthetic and real-world\nbenchmarks (65.38% on DELIVER, 52.88% on MCubeS). Source code will be made\npublicly available.",
        "We present a direct lifetime measurement of the $5s5p~^1P_1$ state of\nstrontium using time-correlated single-photon counting of laser induced\nfluorescence in a hot atomic beam. To achieve fast switch-off times and a high\nsignal-to-noise ratio, we excite the strontium atoms with a femtosecond pulsed\nlaser at $\\approx$461 nm and collect the fluorescence onto a hybrid\nsingle-photon detector. Analysis of the measured exponential decay gives a\nlifetime of the $^1P_1$ state of $\\tau = (5.216 \\pm 0.006_{stat} \\pm\n0.013_{sys})$ ns, where all the systematic effects have been thoroughly\nconsidered.",
        "Over the last years, a number of broadband reverberation mapping campaigns\nhave been conducted to explore the short-term UV and optical variability of\nnearby AGN. Despite the extensive data collected, the origin of the observed\nvariability is still debated in the literature. Frequency-resolved time lags\noffer a promising approach to distinguish between different scenarios, as they\nprobe variability on different time scales. In this study, we present the\nexpected frequency-resolved lags resulting from X-ray reprocessing in the\naccretion disk. The predicted lags are found to feature a general shape that\nresembles that of observational measurements, while exhibiting strong\ndependence on various physical parameters. Additionally, we compare our model\npredictions to observational data for the case of NGC 5548, concluding that the\nX-ray illumination of the disk can effectively account for the observed\nfrequency-resolved lags and power spectra in a self-consistent way. To date,\nX-ray disk reprocessing is the only physical model that has successfully\nreproduced the observed multi-wavelength variability, in both amplitude and\ntime delays, across a range of temporal frequencies.",
        "We prove that the regularity of edge ideals of powers of forests is weakly\ndecreasing. We then compute the regularity of edge ideals of powers of cycles.",
        "We generalize in two steps the quantized action of the modular group on\n$q$-deformed real numbers introduced by Morier-Genoud and Ovsienko. First, we\nlet the projective general linear group $PGL_2(\\mathbb{Z})$ act on $q$-real\nnumbers via a $q$-deformed action. The quantized matrices we get have\ncombinatorial interpretations. Then we consider an extension of the group\n$PGL_2(\\mathbb{Z})$ by the $2$-elements cyclic group, and define a quantized\naction of this extension on $q$-real numbers. We deduce from these actions some\nunderlying relations between $q$-real numbers, and between left and right\nversions of $q$-deformed rational numbers. In particular we investigate the\ncase of some algebraic numbers of degree $4$ and $6$. We also prove that the\nway of quantizing real numbers defined by Morier-Genoud and Ovsienko is an\ninjective process.",
        "We prove that the affine closure of the cotangent bundle of the parabolic\nbase affine space for GL_n or SL_n is a Coulomb branch, which confirms a\nconjecture of Bourget-Dancer-Grimminger-Hanany-Zhong. In particular, we show\nthat the ring of functions on the cotangent bundle of the parabolic base affine\nspace of GL_n or SL_n is finitely generated.\n  We prove this by showing that, if we are given a map $H \\to G$ of complex\nreductive groups and a representation of $G$ satisfying an assumption we call\ngluable, then the Coulomb branch for the induced representation of $H$ is\nobtained from the corresponding Coulomb branch for $G$ by a certain Hamiltonian\nreduction procedure. In particular, we show that the Coulomb branch associated\nto any quiver with no loops can be obtained from Coulomb branches associated to\nquivers with exactly two vertices using this procedure.",
        "Recent studies have suggested that large language models (LLMs) underperform\non mathematical and computer science tasks when these problems are translated\nfrom Romanian into English, compared to their original Romanian format.\nAccurate translation is critical for applications ranging from automatic\ntranslations in programming competitions to the creation of high-quality\neducational materials, as well as minimizing errors or fraud in human\ntranslations. This study shows that robust large language models (LLMs) can\nmaintain or even enhance their performance in translating less common languages\nwhen given well-structured prompts. Our findings suggest that LLMs, with\nappropriate supervision, can be reliably used for the automatic translation of\nIOI (International Olympiad in Informatics)-style tasks. We evaluate several\ntranslation methods across multiple LLMs, including OpenRoLLM, Llama 3.1 8B,\nLlama 3.2 3B and GPT-4o, assessing their translation accuracy and performance\nstability through repeated runs. Additionally, we augment the OJI (Romanian\nCounty-Level Informatics Olympiad) Romanian dataset with accurate English\ntranslations, enhancing its utility for future LLM training and evaluation.\nThrough detailed syntactic and semantic analyses, we confirm that with human\noversight, LLMs can serve as a viable solution for multilingual\nproblem-solving. We also compare the translation quality of LLMs against human\ntranslators, as evaluated by a certified expert, underscoring the potential of\nLLMs in realworld scenarios.",
        "In this paper, the conditional phase distribution of the two-wave with\ndiffuse power (TWDP) process is derived as a closed-form and as an\ninfinite-series expression. For the obtained infinite series expression, a\ntruncation analysis is performed and the truncated expression is used to\nexamine the influence of different channel conditions on the behavior of the\nTWDP phase. All the results are verified through Monte Carlo simulations.",
        "For a self-symmetric tracial von Neumann algebra $A$, we study rescalings of\n$A^{*n} * L\\mathbb{F}_r$ for $n \\in \\mathbb{N}$ and $r \\in (1, \\infty]$ and use\nthem to obtain an interpolation $\\mathcal{F}_{s,r}(A)$ for all real numbers\n$s>0$ and $1-s < r \\leq \\infty$. We get formulas for their free products, and\nfree products with finite-dimensional or hyperfinite von Neumann algebras. In\nparticular, for any such $A$, we can compute compressions $(A^{*n})^t$ for\n$0<t<1$, and the Murray-von Neumann fundamental group of $A^{*\\infty}$. When\n$A$ is also non-separable and abelian, this answers two questions in Section\n4.3 of recent work of Boutonnet-Drimbe-Ioana-Popa.",
        "The radiative open circuit voltage loss in a solar cell occurs because the\nabsorptance spectrum near the band gap shows gradual increase rather than sharp\nstep function like transition. This broadening effect has been attributed to\nband gap fluctuations and or to Urbach tails. In this report, we use modelling\nbased on Planck s generalized law to distinguish between these two effects. Our\nresults demonstrate that Urbach tails have only a minimal effect on the\nabsorptance edge broadening and clarify that even an ideal direct semiconductor\nwith no band gap fluctuations shows broadening at the absorptance onset.\nFurthermore, state of the art inorganic thin film solar cells often incorporate\na band gap gradient across their thickness, which can further contribute to\nabsorptance broadening. Using Cu(In,Ga)Se2 (CIGSe) absorbers as a case study,\nwe perform a comprehensive analysis of voltage losses through absolute\nphotoluminescence and electroluminescence spectroscopy, combined with\nphotospectrometry and high-spatial-resolution cathodoluminescence measurements.\nWe find that the loss analysis based on the combination of radiative,\ngeneration and non-radiative losses is complete. Samples with a graded band gap\nprofile show more pronounced broadening of the absorptance onset and up to 16\nmV higher radiative losses compared to the samples with uniform band gap. There\nis indication, that band gap-graded samples also have larger lateral band gap\ninhomogeneity.",
        "We present the discovery of Andromeda XXXV, the faintest Andromeda satellite\ngalaxy discovered to date, identified as an overdensity of stars in the\nPan-Andromeda Archaeological Survey and confirmed via Hubble Space Telescope\nimaging. Located at a heliocentric distance of $927^{+76}_{-63}$ kpc and\n$158^{+57}_{-45}$ kpc from Andromeda, Andromeda XXXV is an extended ($r_h =\n53\\,^{+13}_{-11}$ pc), elliptical ($\\epsilon = 0.4\\, \\pm 0.2$), metal-poor\n($[\\text{Fe}\/\\text{H}]\\sim-1.9$) system, and the least luminous ($M_V=-5.2 \\pm\n0.3$) of Andromeda's dwarf satellites discovered so far. Andromeda XXXV's\nproperties are consistent with the known population of dwarf galaxies around\nthe Local Group, bearing close structural resemblance to the Canes Venatici II\nand Hydra II Milky Way (MW) dwarf satellite galaxies. Its stellar population,\ncharacterized by a red horizontal branch or a red clump feature, mirrors that\nof other Andromeda satellite galaxies in showing evidence for a spread in age\nand metallicity, with no signs of younger stellar generations. This\nage-metallicity spread is not observed in MW satellites of comparable stellar\nmass, and highlights the persistent differences between the satellite systems\nof Andromeda and the MW, extending even into the ultrafaint regime.",
        "Pre-trained transformer models such as BERT have shown massive gains across\nmany text classification tasks. However, these models usually need enormous\nlabeled data to achieve impressive performances. Obtaining labeled data is\noften expensive and time-consuming, whereas collecting unlabeled data using\nsome heuristics is relatively much cheaper for any task. Therefore, this paper\nproposes a method that encapsulates reinforcement learning-based text\ngeneration and semi-supervised adversarial learning approaches in a novel way\nto improve the model's performance. Our method READ, Reinforcement-based\nAdversarial learning, utilizes an unlabeled dataset to generate diverse\nsynthetic text through reinforcement learning, improving the model's\ngeneralization capability using adversarial learning. Our experimental results\nshow that READ outperforms the existing state-of-art methods on multiple\ndatasets.",
        "We consider unsteady ballistic heat transport in a semi-infinite Hooke chain\nwith a free end and an arbitrary heat source. An analytical description of the\nevolution of the kinetic temperature is proposed in both discrete (exact) and\ncontinuum (approximate) formulations. The continualization of the discrete\nsolution for kinetic temperature is performed through a large-time asymptotic\nestimate of the fundamental solution of the dynamical problem for the instantly\nperturbed conservative semi-infinite chain at the fronts of the incident and\nreflected thermal waves. By analyzing the continuum solution, we observe that\nany instantaneous heat supply (i.e., a heat pulse) results in the\nanti-localization of the reflected thermal wave. We demonstrate that sudden\npoint heat supply leads to a transition to a non-equilibrium steady state,\nwhich, unexpectedly, may exist even in the non-dissipative case. The results of\nthis paper are expected to provide insight into the continuum description of\nnanoscale heat transport.",
        "This paper is primarily focused on the asymptotic dynamics of a\nnon-autonomous stochastic FitzHugh-Nagumo system with distribution dependence,\nspecifically on unbounded domains $\\mathbb{R}^{n}$. Initially, we establish the\nwell-posedness of solutions for the FitzHugh-Nagumo system with distribution\ndependence by utilizing the Banach fixed-point theorem. Subsequently, we\ndemonstrate the existence and uniqueness of pullback measure attractors for\nthis system through the application of splitting techniques, tail-end estimates\nand Vitali's theorem."
      ]
    }
  },
  {
    "id":2411.04775,
    "research_type":"applied",
    "start_id":"b21",
    "start_title":"Towards Scalable Koopman Operator Learning: Convergence Rates and A Distributed Learning Algorithm",
    "start_abstract":"We propose an alternating optimization algorithm to the nonconvex Koopman operator learning problem for nonlinear dynamic systems. show that proposed will converge a critical point with rate O(1\/T) and $O\\left( {\\frac{1}{{\\log T}}} \\right)$ constant diminishing rates, respectively, under some mild conditions. To cope high dimensional dynamical systems, we present first-ever distributed algorithm. has same convergence properties as centralized learning, in absence of optimal tracker, so long basis functions satisfy set state-based decomposition Numerical experiments are provided complement our theoretical results.",
    "start_categories":[
      "eess.SP",
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b7"
      ],
      "title":[
        "A Data\u2013Driven Approximation of the Koopman Operator: Extending Dynamic Mode Decomposition"
      ],
      "abstract":[
        "The Koopman operator is a linear but infinite dimensional operator that\ngoverns the evolution of scalar observables defined on the state space of an\nautonomous dynamical system, and is a powerful tool for the analysis and\ndecomposition of nonlinear dynamical systems. In this manuscript, we present a\ndata driven method for approximating the leading eigenvalues, eigenfunctions,\nand modes of the Koopman operator. The method requires a data set of snapshot\npairs and a dictionary of scalar observables, but does not require explicit\ngoverning equations or interaction with a \"black box\" integrator. We will show\nthat this approach is, in effect, an extension of Dynamic Mode Decomposition\n(DMD), which has been used to approximate the Koopman eigenvalues and modes.\nFurthermore, if the data provided to the method are generated by a Markov\nprocess instead of a deterministic dynamical system, the algorithm approximates\nthe eigenfunctions of the Kolmogorov backward equation, which could be\nconsidered as the \"stochastic Koopman operator\" [1]. Finally, four illustrative\nexamples are presented: two that highlight the quantitative performance of the\nmethod when presented with either deterministic or stochastic data, and two\nthat show potential applications of the Koopman eigenfunctions."
      ],
      "categories":[
        "Mathematical Analysis"
      ]
    },
    "list":{
      "title":[
        "Decay of resolvent kernels and Schr\\\"odinger eigenstates for L\\'evy\n  operators",
        "Maximizing nanoparticle light absorption: size, geometry, and a prospect\n  for metal alloys",
        "On reflected isotropic stable processes",
        "Global Contact-Rich Planning with Sparsity-Rich Semidefinite Relaxations",
        "Geometrically simple counterexamples to a local-global principle for\n  quadratic twists",
        "Physics-Informed Super-Resolution Diffusion for 6D Phase Space\n  Diagnostics",
        "A Catalog of Stellar and Dust Properties for 500,000 Stars in the\n  Southwest Bar of the Small Magellanic Cloud",
        "Invariance of three-dimensional Bessel bridges in terms of time reversal",
        "Numerical simulation of a fine-tunable F\\\"oppl-von K\\'arm\\'an model for\n  foldable and bilayer plates",
        "Modeling Cost-Associated Cooperation: A Dilemma of Species Interaction\n  Unveiling New Aspects of Fear Effect",
        "Journey from the Wilson exact RG towards the Wegner-Morris Fokker-Planck\n  RG and the Carosso field-coarsening via Langevin stochastic processes",
        "The two filter formula reconsidered: Smoothing in partially observed\n  Gauss--Markov models without information parametrization",
        "Continuous Selection of Unitaries in II$_1$ Factors",
        "Electric-field sensing with driven-dissipative time crystals in\n  room-temperature Rydberg vapor",
        "A Graph-Based Iterative Strategy for Solving the All-Line Transmission\n  Switching Problem",
        "Classification of simple quandles of small order",
        "Calibration and Option Pricing with Stochastic Volatility and Double\n  Exponential Jumps",
        "Three-Dimensional to Layered Halide Perovskites: A Parameter-Free Hybrid\n  Functional Method for Predicting Electronic Band Gaps",
        "Twisted torus knots with Horadam parameters",
        "Baseline filtering and peak reconstruction for haloscope-like axion\n  searches",
        "Chiral Symmetry in Dense Matter with Meson Condensation",
        "Higgs Thermal Nonequilibrium in Primordial QGP",
        "A Study of the Provincial Road Networks of Canada as Complex Networks",
        "Uncertainty quantification for stationary and time-dependent PDEs\n  subject to Gevrey regular random domain deformations",
        "Improving ex ante accuracy assessment in predicting house price\n  dispersion: evidence from the USA",
        "Global Well-Posedness of a Nonlinear Fokker-Planck Type Model of Grain\n  Growth",
        "Structure formation in the local Universe and the cosmological constant",
        "PAC Learnability of Scenario Decision-Making Algorithms: Necessary and\n  Sufficient Conditions",
        "Super-Eddington Accretion in Quasars"
      ],
      "abstract":[
        "We study the spatial decay behaviour of resolvent kernels for a large class\nof non-local L\\'evy operators and bound states of the corresponding\nSchr\\\"odinger operators. Our findings naturally lead us to proving results for\nL\\'evy measures, which have subexponential or exponential decay, respectively.\nThis leads to sharp transitions in the the decay rates of the resolvent\nkernels. We obtain estimates that allow us to describe and understand the\nintricate decay behaviour of the resolvent kernels and the bound states in\neither regime, extending findings by Carmona, Masters and Simon for fractional\nLaplacians (the subexponential regime) and classical relativistic operators\n(the exponential regime). Our proofs are mainly based on methods from the\ntheory of operator semigroups.",
        "In this work we show how to maximize absorption of plasmonic nanoparticles in\nterms of size, geometry and material. For that reason the interaction of\nnanoparticles with light was decomposed into different effects. We determined\nthat the main effect dictating the optimal amount of optical losses is\nradiation damping, and how it depends on nanoparticle size and geometry. Based\non this, we find that for many combinations of sizes and geometries losses in\npure metals are far from optimal. To overcome the aforementioned issue,\nalloying is presented as straightforward and flexible way of modulating the\noptical losses. Furthermore, strategies for tuning the optical losses to values\nabove, between, and even below those in pure plasmonic metals are developed in\nterms of selecting the right alloy composition. In some cases, alloys showed a\nmultifold increase in absorption when compared to pure plasmonic metals. The\nphysical reasons governing such changes are elucidated based on the electronic\nstructure changes during alloying of different metals, which enables\ngeneralization of the results to other systems. Besides increasing absorption,\nelectronic structure changes can also be utilized for channeling the absorbed\nenergy to suit different purposes, such as hot carrier generation for\nphotocatalysis or solar energy harvesting. Overall, these results establish\nalloying as a powerful tool for designing nanostructures for applications that\nutilize light absorption.",
        "We build two types of isotropic stable processes reflected in a strongly\nconvex bounded domain $\\mathcal{D}$. In both cases, when the process tries to\njump across the boundary, it is stopped at the unique point where\n$\\partial\\mathcal{D}$ intersects the line segment defined by the attempted\njump. It then leaves the boundary either continuously (for the first type) or\nby a power-law distributed jump (for the second type). The construction of\nthese processes is done via an It\\^o synthesis: we concatenate their excursions\nin the domain, which are obtained by translating, rotating and stopping the\nexcursions of some stable processes reflected in the half-space. The key\ningredient in this procedure is the construction of the boundary processes,\ni.e. the processes time-changed by their local time on the boundary, which\nsolve stochastic differential equations driven by some Poisson measures of\nexcursions. The well-posedness of these boundary processes relies on delicate\nestimates involving some geometric inequalities and the laws of the undershoot\nand overshoot of the excursion when it leaves the domain. After having\nconstructed the processes, we show that they are Markov and Feller, we study\ntheir infinitesimal generator and write down the reflected fractional heat\nequations satisfied by their time-marginals.",
        "We show that contact-rich motion planning is also sparsity-rich when viewed\nas polynomial optimization (POP). We can exploit not only the correlative and\nterm sparsity patterns that are general to all POPs, but also specialized\nsparsity patterns from the robot kinematic structure and the separability of\ncontact modes. Such sparsity enables the design of high-order but sparse\nsemidefinite programming (SDPs) relaxations--building upon Lasserre's moment\nand sums of squares hierarchy--that (i) can be solved in seconds by\noff-the-shelf SDP solvers, and (ii) compute near globally optimal solutions to\nthe nonconvex contact-rich planning problems with small certified\nsuboptimality. Through extensive experiments both in simulation (Push Bot, Push\nBox, Push Box with Obstacles, and Planar Hand) and real world (Push T), we\ndemonstrate the power of using convex SDP relaxations to generate global\ncontact-rich motion plans. As a contribution of independent interest, we\nrelease the Sparse Polynomial Optimization Toolbox (SPOT)--implemented in C++\nwith interfaces to both Python and Matlab--that automates sparsity exploitation\nfor robotics and beyond.",
        "Two abelian varieties $A$ and $B$ over a number field $K$ are said to be\nstrongly locally quadratic twists if they are quadratic twists at every\ncompletion of $K$. While it was known that this does not imply that $A$ and $B$\nare quadratic twists over $K$, the only known counterexamples (necessarily of\ndimension $\\geq 4$) are not geometrically simple. We show that, for every prime\n$p\\equiv 13 \\pmod{24}$, there exists a pair of geometrically simple abelian\nvarieties of dimension $p-1$ over $\\mathbb{Q}$ that are strongly locally\nquadratic twists but not quadratic twists. The proof is based on Galois\ncohomology computations and class field theory.",
        "Adaptive physics-informed super-resolution diffusion is developed for\nnon-invasive virtual diagnostics of the 6D phase space density of charged\nparticle beams. An adaptive variational autoencoder (VAE) embeds initial beam\ncondition images and scalar measurements to a low-dimensional latent space from\nwhich a 326 pixel 6D tensor representation of the beam's 6D phase space density\nis generated. Projecting from a 6D tensor generates physically consistent 2D\nprojections. Physics-guided super-resolution diffusion transforms\nlow-resolution images of the 6D density to high resolution 256x256 pixel\nimages. Un-supervised adaptive latent space tuning enables tracking of\ntime-varying beams without knowledge of time-varying initial conditions. The\nmethod is demonstrated with experimental data and multi-particle simulations at\nthe HiRES UED. The general approach is applicable to a wide range of complex\ndynamic systems evolving in high-dimensional phase space. The method is shown\nto be robust to distribution shift without re-training.",
        "We present a catalog of individual stellar and dust extinction properties\nalong close to 500,000 sight lines in the southwest bar of the Small Magellanic\nCloud (SMC). The catalog is based on multiband Hubble Space Telescope\nphotometric data spanning near-ultraviolet to near-infrared wavelengths from\nthe Small Magellanic Cloud Investigation of Dust and Gas Evolution survey\n(SMIDGE) covering a 100 x 200 pc area. We use the probabilistic technique of\nthe Bayesian Extinction And Stellar Tool (BEAST) to model the spectral energy\ndistributions of individual stars in SMIDGE and include the effects of\nobservational uncertainties in the data. We compare BEAST-derived dust\nextinction properties with tracers of the interstellar medium, such as the\nemission from the 12CO (2-1) transition (I(CO)), the dust mass surface density\n({\\Sigma}dust) from far-IR emission, the H I column density (N(HI)) from the\n21cm transition, and the mass fraction of polycyclic aromatic hydrocarbons\n(PAHs; qPAH, derived from IR emission). We find that the dust extinction (A(V\n)) in the SMIDGE field is strongly correlated with {\\Sigma}dust and I(CO), and\nless so with N(HI) and qPAH, and suggest potential explanations. Our extinction\nmeasurements are also sensitive to the presence of the 2175 {\\AA} bump in the\nextinction curve toward UV bright stars. While most do not show evidence for\nthe bump, we identify ~200 lines of sight that are 2175 {\\AA} bump candidates.\nFurthermore, we find distinct structures in the dust extinction-distance\ndistributions that provide insights into the 3D geometry of the SMC.",
        "Given $a,b\\ge 0$ and $t>0$, let $\\rho =\\{ \\rho _{s}\\} _{0\\le s\\le t}$ be a\nthree-dimensional Bessel bridge from $a$ to $b$ over $[0,t]$. In this paper,\nbased on a conditional identity in law between Brownian bridges stemming from\nPitman's theorem, we show in particular that the process given by\n\\begin{align*}\n  \\rho _{s}+\\Bigl| b-a+\n  \\min _{0\\le u\\le s}\\rho _{u}-\\min _{s\\le u\\le t}\\rho _{u}\n  \\Bigr|\n  -\\Bigl|\n  \\min _{0\\le u\\le s}\\rho _{u}-\\min _{s\\le u\\le t}\\rho _{u}\n  \\Bigr| ,\\quad 0\\le s\\le t, \\end{align*} has the same law as the time reversal\n$\\{ \\rho _{t-s}\\} _{0\\le s\\le t}$ of $\\rho $. As an immediate application,\nletting $R=\\{ R_{s}\\} _{s\\ge 0}$ be a three-dimensional Bessel process starting\nfrom $a$, we obtain the following time-reversal and time-inversion results on\n$R$: $\\{ R_{t-s}\\} _{0\\le s\\le t}$ is identical in law with the process given\nby \\begin{align*}\n  R_{s}+R_{t}-2\\min _{s\\le u\\le t}R_{u},\\quad 0\\le s\\le t, \\end{align*} when\n$a=0$, and $\\{ sR_{1\/s}\\} _{s>0}$ is identical in law with the process given by\n\\begin{align*}\n  R_{s}-2(1+s)\\min _{0\\le u\\le s}\\frac{R_{u}}{1+u}+a(1+s),\\quad s>0,\n\\end{align*} for every $a\\ge 0$.",
        "A numerical scheme is proposed to identify low energy configurations of a\nF\\\"oppl-von K\\'arm\\'an model for bilayer plates. The dependency of the\ncorresponding elastic energy on the in-plane displacement $u$ and the\nout-of-plane deflection $w$ leads to a practical minimization of the functional\nvia a decoupled gradient flow. In particular, the energies of the resulting\niterates are shown to be monotonically decreasing. The discretization of the\nmodel relies on $P1$ finite elements for the horizontal part $u$ and utilizes\nthe discrete Kirchhoff triangle for the vertical component $w$. The model\nallows for analysing various different problem settings via numerical\nsimulation: (i) stable low-energy configurations are detected dependent on a\nspecified prestrain described by elastic material properties, (ii) curvature\ninversions of spherical and cylindrical configurations are investigated, (iii)\nelastic responses of foldable cardboards for different spontaneous curvatures\nand crease geometries are compared.",
        "With limited resources, competition is widespread, yet cooperation persists\nacross taxa, from microorganisms to large mammals. Recent observations reveal\ncontingent factors often drive cooperative interactions, with the intensity\nheterogeneously distributed within species. While cooperation has beneficial\noutcomes, it may also incur significant costs, largely depending on species\ndensity. This creates a dilemma that is pivotal in shaping sustainable\ncooperation strategies. Understanding how cooperation intensity governs the\ncost-benefit balance, and whether an optimal strategy exists for species\nsurvival, is a fundamental question in ecological research, and the focus of\nthis study. We develop a novel mathematical model within the Lotka-Volterra\nframework to explore the dynamics of cost-associated partial cooperation, which\nremains relatively unexplored in ODE model-based studies. Our findings\ndemonstrate that partial cooperation benefits ecosystems up to a certain\nintensity, beyond which costs become dominant, leading to system collapse via\nheteroclinic bifurcation. This outcome captures the cost-cooperation dilemma,\nproviding insights for adopting sustainable strategies and resource management\nfor species survival. We propose a novel mathematical approach to detect and\ntrack heteroclinic orbits in predator-prey systems. Moreover, we show that\nintroducing fear of predation can protect the regime shift, even with a type-I\nfunctional response, challenging traditional ecological views. Although fear is\nknown to resolve the \"paradox of enrichment,\" our results suggest that certain\nlevels of partial cooperation can reestablish this dynamic even at higher fear\nintensity. Finally, we validate the system's dynamical robustness across\nfunctional responses through structural sensitivity analysis.",
        "Within the Wilson RG of 'incomplete integration' as a function of the RG-time\n$t$, the non-linear differential RG flow for the energy $E_t[\\phi(.)]$\ntranslates for the probability distribution $P_t[\\phi(.)] \\sim e^{-\nE_t[\\phi(.)]} $ into the linear Fokker-Planck RG flow associated to independent\nnon-identical Ornstein-Uhlenbeck processes for the Fourier modes. The\ncorresponding Langevin stochastic differential equation for the real-space\nfield $\\phi_t(\\vec x)$ can be then interpreted within the Carosso perspective\nas genuine infinitesimal coarsening-transformations that are the analog of\nspin-blocking, and whose irreversible character is essential to overcome the\nparadox of the naive description of the Wegner-Morris RG flow as a mere\ninfinitesimal change of variables in the partition function integral. This\ninterpretation suggests to consider new RG-schemes, in particular the Carosso\nRG where the Langevin SDE corresponds to the well known stochastic heat\nequation or the Edwards-Wilkinson dynamics. We stress the advantages of this\nstochastic formulation of exact RG flows. While statistical field theory is\nusually written in infinite space, we focus here on the formulation on a large\nvolume $L^d$ with periodic boundary conditions, in order to distinguish between\nextensive and intensives observables while keeping the translation-invariance.\nSince the empirical magnetization $m_e \\equiv \\frac{1}{L^d} \\int_{L^d} d^d \\vec\nx \\ \\phi(\\vec x) $ is an intensive variable corresponding to the zero-momentum\nFourier coefficient of the field, its probability distribution $p_L(m_e)$ can\nbe obtained from the gradual integration over all the other Fourier\ncoefficients associated to non-vanishing-momenta via exact differential RG, in\norder to obtain the large deviation properties with respect to the volume\n$L^d$.",
        "In this article, the two filter formula is re-examined in the setting of\npartially observed Gauss--Markov models. It is traditionally formulated as a\nfilter running backward in time, where the Gaussian density is parametrized in\n``information form''. However, the quantity in the backward recursion is\nstrictly speaking not a distribution, but a likelihood. Taking this observation\nseriously, a recursion over log-quadratic likelihoods is formulated instead,\nwhich obviates the need for ``information'' parametrization. In particular, it\ngreatly simplifies the square-root formulation of the algorithm. Furthermore,\nformulae are given for producing the forward Markov representation of the a\nposteriori distribution over paths from the proposed likelihood representation.",
        "We prove continuous-valued analogues of the basic fact that Murray-von\nNeumann subequivalence of projections in II$_1$ factors is completely\ndetermined by tracial evaluations. We moreover use this result to solve the\nso-called trace problem in the case of factorial trivial $W^\\ast$-bundles whose\nbase space has covering dimension at most 1. Our arguments are based on\napplications of a continuous selection theorem due to Michael to von Neumann\nalgebras.",
        "Mode competition in nonequilibrium Rydberg gases enables the exploration of\nemergent many-body phases. This work leverages this emergent phase for electric\nfield detection at room temperature. Sensitive frequency-resolved electric\nfield measurements at very low-frequencies (VLF) are of central importance in a\nwide range of applications where deep-penetration is required in\ncommunications, navigation and imaging or surveying. The long wavelengths on\norder of 10-100 km (3-30 kHz) limit the efficiency, sensitivity, and bandwidth\nof compact classical detectors that are constrained by the Chu limit.\nRydberg-atom electrometers are an attractive approach for microwave\nelectric-field sensors but have reduced sensitivity at lower-frequencies. Very\nrecent efforts to advance the standard Rydberg-atoms approach is based on DC\nelectric-field (E-field) Stark shifting and have resulted in sensitivities\nbetween 67.9-2.2 uVcm-1Hz-1\/2 (0.1-10 kHz) by fine optimization of the DC\nE-field. A major challenge in these approaches is the need for embedded\nelectrodes or plates due to DC E-field Stark screening effect, which can\nperturb coupling of VLF signals when injected from external sources. In this\narticle, it is demonstrated that state-of-art sensitivity (~1.6-2.3\nuVcm-1Hz-1\/2) can instead be achieved using limit-cycle oscillations in\ndriven-dissipative Rydberg atoms by using a magnetic field (B-field) to develop\nmode-competition between nearby Rydberg states. The mode-competition between\nnearby Rydberg-states develop an effective transition centered at the\noscillation frequency capable of supporting external VLF E-field coupling in\nthe ~10-15kHz regime without the requirement for fine optimization of the\nB-field magnitude.",
        "The transmission switching problem aims to determine the optimal network\ntopology that minimizes the operating costs of a power system. This problem is\ntypically formulated as a mixed-integer optimization model, which involves\nbig-M constants that lead to weak relaxations and significant computational\nchallenges, particularly when all lines are switchable. In this paper, we\npropose a two-fold approach: first, using graph theory to derive tighter big-M\nvalues by solving a relaxed longest path problem; second, introducing an\niterative algorithm that incorporates a heuristic version of the switching\nproblem to efficiently generate low-cost feasible solutions, thereby\naccelerating the search for optimal solutions in the integer optimization\nsolver. Numerical results on the 118-bus network show that the proposed\nmethodology significantly reduces the computational burden compared to\nconventional approaches.",
        "In this article, we define quasiprimitive quandles and describe them with the\nhelp of quasiprimitive permutation groups. As a consequence, we enumerate\nfinite non-affine simple quandles up to order $4096$.",
        "This work examines a stochastic volatility model with double-exponential\njumps in the context of option pricing. The model has been considered in\nprevious research articles, but no thorough analysis has been conducted to\nstudy its quality of calibration and pricing capabilities thus far. We provide\nevidence that this model outperforms challenger models possessing similar\nfeatures (stochastic volatility and jumps), especially in the fit of the short\nterm implied volatility smile, and that it is particularly tractable for the\npricing of exotic options from different generations. The article utilizes\nFourier pricing techniques (the PROJ method and its refinements) for different\ntypes of claims and several generations of exotics (Asian options, cliquets,\nbarrier options, and options on realized variance), and all source codes are\nmade publicly available to facilitate adoption and future research. The results\nindicate that this model is highly promising, thanks to the asymmetry of the\njumps distribution allowing it to capture richer dynamics than a normal jump\nsize distribution. The parameters all have meaningful econometrics\ninterpretations that are important for adoption by risk-managers.",
        "Accurately predicting electronic band gaps in halide perovskites using ab\ninitio density functional theory (DFT) is essential for their application in\noptoelectronic devices. Standard hybrid functionals such as HSE and PBE0 can\novercome the limitations of DFT with reasonable computational cost but are\nknown to underestimate the measured band gaps for layered halide perovskites.\nIn this study, we assess the performance of the doubly screened\ndielectric-dependent hybrid (DSH) functional for predicting band gaps in\nthree-dimensional (3D) and layered hybrid perovskites. We show that the DSH\nfunctional, which employs material-dependent mixing parameters derived from\nmacroscopic dielectric constants, provides accurate band gap predictions for 3D\nhalide perovskites when structural local disorder is considered. For layered\nhybrid perovskites, DSH functional based on average dielectric constants\noverestimates the band gaps. To improve the predictions and stay in a\nparameter-free ab initio workflow, we propose to use the calculated dielectric\nconstant of the respective 3D perovskites. We find that the DSH functionals\nusing the dielectric constants of the 3D perovskite accurately predict\nexperimental gaps, with the lowest mean absolute errors compared to HSE and\nPBE0 for layered perovskites with various organic spacers, as well as for\nmultilayered $BA_2MA_{n-1}Pb_{n}I_{3n-1}$ with n = 2, 3. Notably, the HSE\nfunctional systematically underestimates the band gaps in layered perovskites.\nWe attribute the root of this failure to the absence of non-local long-range\ndielectric screening, a critical factor for halide perovskites. The\ncomputational framework introduced here provides an efficient parameter-free ab\ninitio methodology for predicting the electronic properties of 3D and layered\nhalide perovskites and their heterostructures, aiding in developing advanced\noptoelectronic devices.",
        "Sangyop Lee has done much work to determine the knot types of twisted torus\nknots, including classifying the twisted torus knots which are the unknot.\nAmong the unknotted twisted torus knots are those of the form $(F_{n+2}, F_n,\nF_{n+1}, -1)$, where $F_i$ is the $i$th Fibonacci number. Here, we consider\ntwisted torus knots with parameters that are defined recursively, similarly to\nthe Fibonacci sequence. We call these \\textit{Horadam parameters}, after the\ngeneralization of the Fibonacci sequence introduced by A.F. Horadam. Here, we\nprovide families of twisted torus knots that generalize Lee's work with Horadam\nparameters. Additionally, we provide lists of primitive\/primitive and\nprimitive\/Seifert twisted torus knots and connect these lists to the Horadam\ntwisted torus knots.",
        "Axions are well-motivated dark matter particles. Many experiments are looking\nfor their experimental evidence. For haloscopes, the problem reduces to the\nidentification of a peak above a noisy baseline. Its modeling, however, may\nproblematic. State-of-the-art analysis rely on the Savitzky-Golay (SG)\nfiltering, which is intrinsically affected by any possible over fluctuation,\nleading to biased results. In this paper we study the efficiency that different\nextensions of SG can provide in the peak reconstruction in a standard\nhaloscope-like experiment. We show that, once the correlations among bins are\ntaken into account, there is no appreciable difference. The standard SG remains\nthe advisable choice because of its numerical efficiency.",
        "Kaon condensation in hyperon-mixed matter [($Y$+$K$) phase], which may be\nrealized in neutron stars, is discussed on the basis of chiral symmetry. With\nthe use of the effective chiral Lagrangian for kaon--baryon and kaon--kaon\ninteractions; coupled with the relativistic mean field theory and universal\nthree-baryon repulsive interaction, we clarify the effects of the $s$-wave\nkaon--baryon scalar interaction simulated by the kaon--baryon sigma terms and\nvector interaction (Tomozawa--Weinberg term) on kaon properties in\nhyperon-mixed matter, the onset density of kaon condensation, and the equation\nof state with the ($Y$+$K$) phase. In particular, the quark condensates in the\n($Y$+$K$) phase are obtained, and their relevance to chiral symmetry\nrestoration is discussed.",
        "In this work we investigate the chemical and kinetic nonequilibrium dynamics\nof the Higgs boson during the primordial Universe QGP (quark-gluon plasma)\nepoch $130\\mathrm{\\,GeV}>T>10\\mathrm{\\,GeV}$. We show that the Higgs bosons is\nalways out of chemical abundance equilibrium with a fugacity $\\Upsilon_h =\n0.69$ due to virtual decay channels. Additionally, Higgs momentum distribution\nis found to be ``cold'' for $T<40$\\,GeV, since the scattering rate drops below\nthe production rate.",
        "In this paper, we report on a study of the road networks of the provinces of\nCanada as complex networks. A number of statistical features have been analyzed\nand compared with two random models. In addition, we have also studied the\nresilience of these road networks under random failures and targeted attacks.",
        "We study uncertainty quantification for partial differential equations\nsubject to domain uncertainty. We parameterize the random domain using the\nmodel recently considered by Chernov and Le (2024) as well as Harbrecht,\nSchmidlin, and Schwab (2024) in which the input random field is assumed to\nbelong to a Gevrey smoothness class. This approach has the advantage of being\nsubstantially more general than models which assume a particular parametric\nrepresentation of the input random field such as a Karhunen-Loeve series\nexpansion. We consider both the Poisson equation as well as the heat equation\nand design randomly shifted lattice quasi-Monte Carlo (QMC) cubature rules for\nthe computation of the expected solution under domain uncertainty. We show that\nthese QMC rules exhibit dimension-independent, essentially linear cubature\nconvergence rates in this framework. In addition, we complete the error\nanalysis by taking into account the approximation errors incurred by dimension\ntruncation of the random input field and finite element discretization.\nNumerical experiments are presented to confirm the theoretical rates.",
        "The study focuses on improving the ex ante prediction accuracy assessment in\nthe case of forecasting various house price dispersion measures in the USA. It\naddresses a critical gap in real estate market forecasting by proposing a novel\nmethod for assessing ex ante prediction accuracy under unanticipated shocks.\nThe proposal is based on a parametric bootstrap approach under a misspecified\nmodel, allowing for the simulation of future values and estimation of\nprediction errors in case of unexpected price changes. The study highlights the\nlimitations of the traditional approach that fails to account for unforeseen\nmarket events and provides a more in-depth understanding of how prediction\naccuracy changes under unexpected scenarios. The proposed methods offers\nvaluable insights for real estate market management by enabling more robust\nrisk assessment and decision-making in the face of unexpected market\nfluctuations. Real data application is based on longitudinal U.S. data on real\nestate transactions.",
        "Most technologically useful materials spanning multiple length scales are\npolycrystalline. Polycrystalline microstructures are composed of a myriad of\nsmall crystals or grains with different lattice orientations which are\nseparated by interfaces or grain boundaries. The changes in the grain and grain\nboundary structure of polycrystals highly influence the materials properties\nincluding, but not limited to, electrical, mechanical, and thermal. Thus, an\nunderstanding of how microstructures evolve is essential for the engineering of\nnew materials. In this paper, we consider a recently introduced nonlinear\nFokker-Planck-type system and establish a global well-posedness result for it.\nSuch systems under specific energy laws emerge in the modeling of the grain\nboundary dynamics in polycrystals.",
        "The structure formation in the local Universe is considered within the\nweak-field modification of General Relativity involving the cosmological\nconstant. This approach enables to describe the dynamics of groups and clusters\nof galaxies, to explain the discrepancy in the observational properties of the\nlocal (late) and the global (early) Universe, i.e. the Hubble tension as a\nresult of two flows, local and global ones, with non-equal Hubble parameters.\nThe kinetic analysis with the modified gravitational potential involving the\ncosmological constant is shown to predict semi-periodical structure of\nfilaments in the local universe. In the local scale this complements the\nZeldovich pancake theory of evolution of the primordial density perturbations\nand of structure formation in the cosmological scale. The role of the\ncosmological constant is outlined in rescaling of the physical constants from\none aeon to another within the Conformal Cyclic Cosmology.",
        "We study the PAC property of scenario decision-making algorithms, that is,\nthe ability to make a decision that has an arbitrarily low risk of violating an\nunknown safety constraint, provided sufficiently many realizations (called\nscenarios) of the safety constraint are sampled. Sufficient conditions for\nscenario decision-making algorithms to be PAC are available in the literature,\nsuch as finiteness of the VC dimension of its associated classifier and\nexistence of a compression scheme. We study the question of whether these\nsufficient conditions are also necessary. We show with counterexamples that\nthis is not the case in general. This contrasts with binary classification\nlearning, for which the analogous conditions are sufficient and necessary.\nPopular scenario decision-making algorithms, such as scenario optimization,\nenjoy additional properties, such as stability and consistency. We show that\neven under these additional assumptions the above conclusions hold. Finally, we\nderive a necessary condition for scenario decision-making algorithms to be PAC,\ninspired by the VC dimension and the so-called no-free-lunch theorem.",
        "This review provides an observational perspective on the fundamental\nproperties of super-Eddington accretion onto supermassive black holes in\nquasars. It begins by outlining the selection criteria, particularly focusing\non optical and UV broad-line intensity ratios, used to identify a population of\nunobscured super-Eddington candidates. Several defining features place these\ncandidates at the extreme end of the Population A in main sequence of quasars:\namong them are the highest observed singly-ionized iron emission, extreme\noutflow velocities in UV resonance lines, and unusually high metal abundances.\nThese key properties reflect the coexistence of a virialized sub-system within\nthe broad-line region alongside powerful outflows, with the observed gas\nenrichment likely driven by nuclear or circumnuclear star formation. The most\ncompelling evidence for the occurrence of super-Eddington accretion onto\nsupermassive black holes comes from recent observations of massive black holes\nat early cosmic epochs. These black holes require rapid growth rates that are\nonly achievable through radiatively inefficient super-Eddington accretion.\nFurthermore, extreme Eddington ratios, close to or slightly exceeding unity,\nare consistent with the saturation of radiative output per unit mass predicted\nby accretion disk theory for super-Eddington accretion rates. The extreme\nproperties of super-Eddington candidates suggest that these quasars could make\nthem stable and well-defined cosmological distance indicators, leveraging the\ncorrelation between broad-line width and luminosity expected in virialized\nsystems. Finally, several analogies with accretion processes around\nstellar-mass black holes, particularly in the high\/soft state, are explored to\nprovide additional insight into the mechanisms driving super-Eddington\naccretion."
      ]
    }
  },
  {
    "id":2411.04775,
    "research_type":"applied",
    "start_id":"b14",
    "start_title":"VAMPnets for deep learning of molecular kinetics",
    "start_abstract":"Abstract There is an increasing demand for computing the relevant structures, equilibria, and long-timescale kinetics of biomolecular processes, such as protein-drug binding, from high-throughput molecular dynamics simulations. Current methods employ transformation simulated coordinates into structural features, dimension reduction, clustering dimension-reduced data, estimation a Markov state model or related interconversion rates between structures. This handcrafted approach demands substantial amount modeling expertise, poor decisions at any step will lead to large errors. Here we variational processes (VAMP) develop deep learning framework using neural networks, dubbed VAMPnets. A VAMPnet encodes entire mapping states, thus combining whole data processing pipeline in single end-to-end framework. Our method performs equally better than state-of-the-art provides easily interpretable few-state kinetic models.",
    "start_categories":[
      "eess.SP",
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b7"
      ],
      "title":[
        "A Data\u2013Driven Approximation of the Koopman Operator: Extending Dynamic Mode Decomposition"
      ],
      "abstract":[
        "The Koopman operator is a linear but infinite dimensional operator that\ngoverns the evolution of scalar observables defined on the state space of an\nautonomous dynamical system, and is a powerful tool for the analysis and\ndecomposition of nonlinear dynamical systems. In this manuscript, we present a\ndata driven method for approximating the leading eigenvalues, eigenfunctions,\nand modes of the Koopman operator. The method requires a data set of snapshot\npairs and a dictionary of scalar observables, but does not require explicit\ngoverning equations or interaction with a \"black box\" integrator. We will show\nthat this approach is, in effect, an extension of Dynamic Mode Decomposition\n(DMD), which has been used to approximate the Koopman eigenvalues and modes.\nFurthermore, if the data provided to the method are generated by a Markov\nprocess instead of a deterministic dynamical system, the algorithm approximates\nthe eigenfunctions of the Kolmogorov backward equation, which could be\nconsidered as the \"stochastic Koopman operator\" [1]. Finally, four illustrative\nexamples are presented: two that highlight the quantitative performance of the\nmethod when presented with either deterministic or stochastic data, and two\nthat show potential applications of the Koopman eigenfunctions."
      ],
      "categories":[
        "Mathematical Analysis"
      ]
    },
    "list":{
      "title":[
        "Decay of resolvent kernels and Schr\\\"odinger eigenstates for L\\'evy\n  operators",
        "Maximizing nanoparticle light absorption: size, geometry, and a prospect\n  for metal alloys",
        "On reflected isotropic stable processes",
        "Global Contact-Rich Planning with Sparsity-Rich Semidefinite Relaxations",
        "Geometrically simple counterexamples to a local-global principle for\n  quadratic twists",
        "Physics-Informed Super-Resolution Diffusion for 6D Phase Space\n  Diagnostics",
        "A Catalog of Stellar and Dust Properties for 500,000 Stars in the\n  Southwest Bar of the Small Magellanic Cloud",
        "Invariance of three-dimensional Bessel bridges in terms of time reversal",
        "Numerical simulation of a fine-tunable F\\\"oppl-von K\\'arm\\'an model for\n  foldable and bilayer plates",
        "Modeling Cost-Associated Cooperation: A Dilemma of Species Interaction\n  Unveiling New Aspects of Fear Effect",
        "Journey from the Wilson exact RG towards the Wegner-Morris Fokker-Planck\n  RG and the Carosso field-coarsening via Langevin stochastic processes",
        "The two filter formula reconsidered: Smoothing in partially observed\n  Gauss--Markov models without information parametrization",
        "Continuous Selection of Unitaries in II$_1$ Factors",
        "Electric-field sensing with driven-dissipative time crystals in\n  room-temperature Rydberg vapor",
        "A Graph-Based Iterative Strategy for Solving the All-Line Transmission\n  Switching Problem",
        "Classification of simple quandles of small order",
        "Calibration and Option Pricing with Stochastic Volatility and Double\n  Exponential Jumps",
        "Three-Dimensional to Layered Halide Perovskites: A Parameter-Free Hybrid\n  Functional Method for Predicting Electronic Band Gaps",
        "Twisted torus knots with Horadam parameters",
        "Baseline filtering and peak reconstruction for haloscope-like axion\n  searches",
        "Chiral Symmetry in Dense Matter with Meson Condensation",
        "Higgs Thermal Nonequilibrium in Primordial QGP",
        "A Study of the Provincial Road Networks of Canada as Complex Networks",
        "Uncertainty quantification for stationary and time-dependent PDEs\n  subject to Gevrey regular random domain deformations",
        "Improving ex ante accuracy assessment in predicting house price\n  dispersion: evidence from the USA",
        "Global Well-Posedness of a Nonlinear Fokker-Planck Type Model of Grain\n  Growth",
        "Structure formation in the local Universe and the cosmological constant",
        "PAC Learnability of Scenario Decision-Making Algorithms: Necessary and\n  Sufficient Conditions",
        "Super-Eddington Accretion in Quasars"
      ],
      "abstract":[
        "We study the spatial decay behaviour of resolvent kernels for a large class\nof non-local L\\'evy operators and bound states of the corresponding\nSchr\\\"odinger operators. Our findings naturally lead us to proving results for\nL\\'evy measures, which have subexponential or exponential decay, respectively.\nThis leads to sharp transitions in the the decay rates of the resolvent\nkernels. We obtain estimates that allow us to describe and understand the\nintricate decay behaviour of the resolvent kernels and the bound states in\neither regime, extending findings by Carmona, Masters and Simon for fractional\nLaplacians (the subexponential regime) and classical relativistic operators\n(the exponential regime). Our proofs are mainly based on methods from the\ntheory of operator semigroups.",
        "In this work we show how to maximize absorption of plasmonic nanoparticles in\nterms of size, geometry and material. For that reason the interaction of\nnanoparticles with light was decomposed into different effects. We determined\nthat the main effect dictating the optimal amount of optical losses is\nradiation damping, and how it depends on nanoparticle size and geometry. Based\non this, we find that for many combinations of sizes and geometries losses in\npure metals are far from optimal. To overcome the aforementioned issue,\nalloying is presented as straightforward and flexible way of modulating the\noptical losses. Furthermore, strategies for tuning the optical losses to values\nabove, between, and even below those in pure plasmonic metals are developed in\nterms of selecting the right alloy composition. In some cases, alloys showed a\nmultifold increase in absorption when compared to pure plasmonic metals. The\nphysical reasons governing such changes are elucidated based on the electronic\nstructure changes during alloying of different metals, which enables\ngeneralization of the results to other systems. Besides increasing absorption,\nelectronic structure changes can also be utilized for channeling the absorbed\nenergy to suit different purposes, such as hot carrier generation for\nphotocatalysis or solar energy harvesting. Overall, these results establish\nalloying as a powerful tool for designing nanostructures for applications that\nutilize light absorption.",
        "We build two types of isotropic stable processes reflected in a strongly\nconvex bounded domain $\\mathcal{D}$. In both cases, when the process tries to\njump across the boundary, it is stopped at the unique point where\n$\\partial\\mathcal{D}$ intersects the line segment defined by the attempted\njump. It then leaves the boundary either continuously (for the first type) or\nby a power-law distributed jump (for the second type). The construction of\nthese processes is done via an It\\^o synthesis: we concatenate their excursions\nin the domain, which are obtained by translating, rotating and stopping the\nexcursions of some stable processes reflected in the half-space. The key\ningredient in this procedure is the construction of the boundary processes,\ni.e. the processes time-changed by their local time on the boundary, which\nsolve stochastic differential equations driven by some Poisson measures of\nexcursions. The well-posedness of these boundary processes relies on delicate\nestimates involving some geometric inequalities and the laws of the undershoot\nand overshoot of the excursion when it leaves the domain. After having\nconstructed the processes, we show that they are Markov and Feller, we study\ntheir infinitesimal generator and write down the reflected fractional heat\nequations satisfied by their time-marginals.",
        "We show that contact-rich motion planning is also sparsity-rich when viewed\nas polynomial optimization (POP). We can exploit not only the correlative and\nterm sparsity patterns that are general to all POPs, but also specialized\nsparsity patterns from the robot kinematic structure and the separability of\ncontact modes. Such sparsity enables the design of high-order but sparse\nsemidefinite programming (SDPs) relaxations--building upon Lasserre's moment\nand sums of squares hierarchy--that (i) can be solved in seconds by\noff-the-shelf SDP solvers, and (ii) compute near globally optimal solutions to\nthe nonconvex contact-rich planning problems with small certified\nsuboptimality. Through extensive experiments both in simulation (Push Bot, Push\nBox, Push Box with Obstacles, and Planar Hand) and real world (Push T), we\ndemonstrate the power of using convex SDP relaxations to generate global\ncontact-rich motion plans. As a contribution of independent interest, we\nrelease the Sparse Polynomial Optimization Toolbox (SPOT)--implemented in C++\nwith interfaces to both Python and Matlab--that automates sparsity exploitation\nfor robotics and beyond.",
        "Two abelian varieties $A$ and $B$ over a number field $K$ are said to be\nstrongly locally quadratic twists if they are quadratic twists at every\ncompletion of $K$. While it was known that this does not imply that $A$ and $B$\nare quadratic twists over $K$, the only known counterexamples (necessarily of\ndimension $\\geq 4$) are not geometrically simple. We show that, for every prime\n$p\\equiv 13 \\pmod{24}$, there exists a pair of geometrically simple abelian\nvarieties of dimension $p-1$ over $\\mathbb{Q}$ that are strongly locally\nquadratic twists but not quadratic twists. The proof is based on Galois\ncohomology computations and class field theory.",
        "Adaptive physics-informed super-resolution diffusion is developed for\nnon-invasive virtual diagnostics of the 6D phase space density of charged\nparticle beams. An adaptive variational autoencoder (VAE) embeds initial beam\ncondition images and scalar measurements to a low-dimensional latent space from\nwhich a 326 pixel 6D tensor representation of the beam's 6D phase space density\nis generated. Projecting from a 6D tensor generates physically consistent 2D\nprojections. Physics-guided super-resolution diffusion transforms\nlow-resolution images of the 6D density to high resolution 256x256 pixel\nimages. Un-supervised adaptive latent space tuning enables tracking of\ntime-varying beams without knowledge of time-varying initial conditions. The\nmethod is demonstrated with experimental data and multi-particle simulations at\nthe HiRES UED. The general approach is applicable to a wide range of complex\ndynamic systems evolving in high-dimensional phase space. The method is shown\nto be robust to distribution shift without re-training.",
        "We present a catalog of individual stellar and dust extinction properties\nalong close to 500,000 sight lines in the southwest bar of the Small Magellanic\nCloud (SMC). The catalog is based on multiband Hubble Space Telescope\nphotometric data spanning near-ultraviolet to near-infrared wavelengths from\nthe Small Magellanic Cloud Investigation of Dust and Gas Evolution survey\n(SMIDGE) covering a 100 x 200 pc area. We use the probabilistic technique of\nthe Bayesian Extinction And Stellar Tool (BEAST) to model the spectral energy\ndistributions of individual stars in SMIDGE and include the effects of\nobservational uncertainties in the data. We compare BEAST-derived dust\nextinction properties with tracers of the interstellar medium, such as the\nemission from the 12CO (2-1) transition (I(CO)), the dust mass surface density\n({\\Sigma}dust) from far-IR emission, the H I column density (N(HI)) from the\n21cm transition, and the mass fraction of polycyclic aromatic hydrocarbons\n(PAHs; qPAH, derived from IR emission). We find that the dust extinction (A(V\n)) in the SMIDGE field is strongly correlated with {\\Sigma}dust and I(CO), and\nless so with N(HI) and qPAH, and suggest potential explanations. Our extinction\nmeasurements are also sensitive to the presence of the 2175 {\\AA} bump in the\nextinction curve toward UV bright stars. While most do not show evidence for\nthe bump, we identify ~200 lines of sight that are 2175 {\\AA} bump candidates.\nFurthermore, we find distinct structures in the dust extinction-distance\ndistributions that provide insights into the 3D geometry of the SMC.",
        "Given $a,b\\ge 0$ and $t>0$, let $\\rho =\\{ \\rho _{s}\\} _{0\\le s\\le t}$ be a\nthree-dimensional Bessel bridge from $a$ to $b$ over $[0,t]$. In this paper,\nbased on a conditional identity in law between Brownian bridges stemming from\nPitman's theorem, we show in particular that the process given by\n\\begin{align*}\n  \\rho _{s}+\\Bigl| b-a+\n  \\min _{0\\le u\\le s}\\rho _{u}-\\min _{s\\le u\\le t}\\rho _{u}\n  \\Bigr|\n  -\\Bigl|\n  \\min _{0\\le u\\le s}\\rho _{u}-\\min _{s\\le u\\le t}\\rho _{u}\n  \\Bigr| ,\\quad 0\\le s\\le t, \\end{align*} has the same law as the time reversal\n$\\{ \\rho _{t-s}\\} _{0\\le s\\le t}$ of $\\rho $. As an immediate application,\nletting $R=\\{ R_{s}\\} _{s\\ge 0}$ be a three-dimensional Bessel process starting\nfrom $a$, we obtain the following time-reversal and time-inversion results on\n$R$: $\\{ R_{t-s}\\} _{0\\le s\\le t}$ is identical in law with the process given\nby \\begin{align*}\n  R_{s}+R_{t}-2\\min _{s\\le u\\le t}R_{u},\\quad 0\\le s\\le t, \\end{align*} when\n$a=0$, and $\\{ sR_{1\/s}\\} _{s>0}$ is identical in law with the process given by\n\\begin{align*}\n  R_{s}-2(1+s)\\min _{0\\le u\\le s}\\frac{R_{u}}{1+u}+a(1+s),\\quad s>0,\n\\end{align*} for every $a\\ge 0$.",
        "A numerical scheme is proposed to identify low energy configurations of a\nF\\\"oppl-von K\\'arm\\'an model for bilayer plates. The dependency of the\ncorresponding elastic energy on the in-plane displacement $u$ and the\nout-of-plane deflection $w$ leads to a practical minimization of the functional\nvia a decoupled gradient flow. In particular, the energies of the resulting\niterates are shown to be monotonically decreasing. The discretization of the\nmodel relies on $P1$ finite elements for the horizontal part $u$ and utilizes\nthe discrete Kirchhoff triangle for the vertical component $w$. The model\nallows for analysing various different problem settings via numerical\nsimulation: (i) stable low-energy configurations are detected dependent on a\nspecified prestrain described by elastic material properties, (ii) curvature\ninversions of spherical and cylindrical configurations are investigated, (iii)\nelastic responses of foldable cardboards for different spontaneous curvatures\nand crease geometries are compared.",
        "With limited resources, competition is widespread, yet cooperation persists\nacross taxa, from microorganisms to large mammals. Recent observations reveal\ncontingent factors often drive cooperative interactions, with the intensity\nheterogeneously distributed within species. While cooperation has beneficial\noutcomes, it may also incur significant costs, largely depending on species\ndensity. This creates a dilemma that is pivotal in shaping sustainable\ncooperation strategies. Understanding how cooperation intensity governs the\ncost-benefit balance, and whether an optimal strategy exists for species\nsurvival, is a fundamental question in ecological research, and the focus of\nthis study. We develop a novel mathematical model within the Lotka-Volterra\nframework to explore the dynamics of cost-associated partial cooperation, which\nremains relatively unexplored in ODE model-based studies. Our findings\ndemonstrate that partial cooperation benefits ecosystems up to a certain\nintensity, beyond which costs become dominant, leading to system collapse via\nheteroclinic bifurcation. This outcome captures the cost-cooperation dilemma,\nproviding insights for adopting sustainable strategies and resource management\nfor species survival. We propose a novel mathematical approach to detect and\ntrack heteroclinic orbits in predator-prey systems. Moreover, we show that\nintroducing fear of predation can protect the regime shift, even with a type-I\nfunctional response, challenging traditional ecological views. Although fear is\nknown to resolve the \"paradox of enrichment,\" our results suggest that certain\nlevels of partial cooperation can reestablish this dynamic even at higher fear\nintensity. Finally, we validate the system's dynamical robustness across\nfunctional responses through structural sensitivity analysis.",
        "Within the Wilson RG of 'incomplete integration' as a function of the RG-time\n$t$, the non-linear differential RG flow for the energy $E_t[\\phi(.)]$\ntranslates for the probability distribution $P_t[\\phi(.)] \\sim e^{-\nE_t[\\phi(.)]} $ into the linear Fokker-Planck RG flow associated to independent\nnon-identical Ornstein-Uhlenbeck processes for the Fourier modes. The\ncorresponding Langevin stochastic differential equation for the real-space\nfield $\\phi_t(\\vec x)$ can be then interpreted within the Carosso perspective\nas genuine infinitesimal coarsening-transformations that are the analog of\nspin-blocking, and whose irreversible character is essential to overcome the\nparadox of the naive description of the Wegner-Morris RG flow as a mere\ninfinitesimal change of variables in the partition function integral. This\ninterpretation suggests to consider new RG-schemes, in particular the Carosso\nRG where the Langevin SDE corresponds to the well known stochastic heat\nequation or the Edwards-Wilkinson dynamics. We stress the advantages of this\nstochastic formulation of exact RG flows. While statistical field theory is\nusually written in infinite space, we focus here on the formulation on a large\nvolume $L^d$ with periodic boundary conditions, in order to distinguish between\nextensive and intensives observables while keeping the translation-invariance.\nSince the empirical magnetization $m_e \\equiv \\frac{1}{L^d} \\int_{L^d} d^d \\vec\nx \\ \\phi(\\vec x) $ is an intensive variable corresponding to the zero-momentum\nFourier coefficient of the field, its probability distribution $p_L(m_e)$ can\nbe obtained from the gradual integration over all the other Fourier\ncoefficients associated to non-vanishing-momenta via exact differential RG, in\norder to obtain the large deviation properties with respect to the volume\n$L^d$.",
        "In this article, the two filter formula is re-examined in the setting of\npartially observed Gauss--Markov models. It is traditionally formulated as a\nfilter running backward in time, where the Gaussian density is parametrized in\n``information form''. However, the quantity in the backward recursion is\nstrictly speaking not a distribution, but a likelihood. Taking this observation\nseriously, a recursion over log-quadratic likelihoods is formulated instead,\nwhich obviates the need for ``information'' parametrization. In particular, it\ngreatly simplifies the square-root formulation of the algorithm. Furthermore,\nformulae are given for producing the forward Markov representation of the a\nposteriori distribution over paths from the proposed likelihood representation.",
        "We prove continuous-valued analogues of the basic fact that Murray-von\nNeumann subequivalence of projections in II$_1$ factors is completely\ndetermined by tracial evaluations. We moreover use this result to solve the\nso-called trace problem in the case of factorial trivial $W^\\ast$-bundles whose\nbase space has covering dimension at most 1. Our arguments are based on\napplications of a continuous selection theorem due to Michael to von Neumann\nalgebras.",
        "Mode competition in nonequilibrium Rydberg gases enables the exploration of\nemergent many-body phases. This work leverages this emergent phase for electric\nfield detection at room temperature. Sensitive frequency-resolved electric\nfield measurements at very low-frequencies (VLF) are of central importance in a\nwide range of applications where deep-penetration is required in\ncommunications, navigation and imaging or surveying. The long wavelengths on\norder of 10-100 km (3-30 kHz) limit the efficiency, sensitivity, and bandwidth\nof compact classical detectors that are constrained by the Chu limit.\nRydberg-atom electrometers are an attractive approach for microwave\nelectric-field sensors but have reduced sensitivity at lower-frequencies. Very\nrecent efforts to advance the standard Rydberg-atoms approach is based on DC\nelectric-field (E-field) Stark shifting and have resulted in sensitivities\nbetween 67.9-2.2 uVcm-1Hz-1\/2 (0.1-10 kHz) by fine optimization of the DC\nE-field. A major challenge in these approaches is the need for embedded\nelectrodes or plates due to DC E-field Stark screening effect, which can\nperturb coupling of VLF signals when injected from external sources. In this\narticle, it is demonstrated that state-of-art sensitivity (~1.6-2.3\nuVcm-1Hz-1\/2) can instead be achieved using limit-cycle oscillations in\ndriven-dissipative Rydberg atoms by using a magnetic field (B-field) to develop\nmode-competition between nearby Rydberg states. The mode-competition between\nnearby Rydberg-states develop an effective transition centered at the\noscillation frequency capable of supporting external VLF E-field coupling in\nthe ~10-15kHz regime without the requirement for fine optimization of the\nB-field magnitude.",
        "The transmission switching problem aims to determine the optimal network\ntopology that minimizes the operating costs of a power system. This problem is\ntypically formulated as a mixed-integer optimization model, which involves\nbig-M constants that lead to weak relaxations and significant computational\nchallenges, particularly when all lines are switchable. In this paper, we\npropose a two-fold approach: first, using graph theory to derive tighter big-M\nvalues by solving a relaxed longest path problem; second, introducing an\niterative algorithm that incorporates a heuristic version of the switching\nproblem to efficiently generate low-cost feasible solutions, thereby\naccelerating the search for optimal solutions in the integer optimization\nsolver. Numerical results on the 118-bus network show that the proposed\nmethodology significantly reduces the computational burden compared to\nconventional approaches.",
        "In this article, we define quasiprimitive quandles and describe them with the\nhelp of quasiprimitive permutation groups. As a consequence, we enumerate\nfinite non-affine simple quandles up to order $4096$.",
        "This work examines a stochastic volatility model with double-exponential\njumps in the context of option pricing. The model has been considered in\nprevious research articles, but no thorough analysis has been conducted to\nstudy its quality of calibration and pricing capabilities thus far. We provide\nevidence that this model outperforms challenger models possessing similar\nfeatures (stochastic volatility and jumps), especially in the fit of the short\nterm implied volatility smile, and that it is particularly tractable for the\npricing of exotic options from different generations. The article utilizes\nFourier pricing techniques (the PROJ method and its refinements) for different\ntypes of claims and several generations of exotics (Asian options, cliquets,\nbarrier options, and options on realized variance), and all source codes are\nmade publicly available to facilitate adoption and future research. The results\nindicate that this model is highly promising, thanks to the asymmetry of the\njumps distribution allowing it to capture richer dynamics than a normal jump\nsize distribution. The parameters all have meaningful econometrics\ninterpretations that are important for adoption by risk-managers.",
        "Accurately predicting electronic band gaps in halide perovskites using ab\ninitio density functional theory (DFT) is essential for their application in\noptoelectronic devices. Standard hybrid functionals such as HSE and PBE0 can\novercome the limitations of DFT with reasonable computational cost but are\nknown to underestimate the measured band gaps for layered halide perovskites.\nIn this study, we assess the performance of the doubly screened\ndielectric-dependent hybrid (DSH) functional for predicting band gaps in\nthree-dimensional (3D) and layered hybrid perovskites. We show that the DSH\nfunctional, which employs material-dependent mixing parameters derived from\nmacroscopic dielectric constants, provides accurate band gap predictions for 3D\nhalide perovskites when structural local disorder is considered. For layered\nhybrid perovskites, DSH functional based on average dielectric constants\noverestimates the band gaps. To improve the predictions and stay in a\nparameter-free ab initio workflow, we propose to use the calculated dielectric\nconstant of the respective 3D perovskites. We find that the DSH functionals\nusing the dielectric constants of the 3D perovskite accurately predict\nexperimental gaps, with the lowest mean absolute errors compared to HSE and\nPBE0 for layered perovskites with various organic spacers, as well as for\nmultilayered $BA_2MA_{n-1}Pb_{n}I_{3n-1}$ with n = 2, 3. Notably, the HSE\nfunctional systematically underestimates the band gaps in layered perovskites.\nWe attribute the root of this failure to the absence of non-local long-range\ndielectric screening, a critical factor for halide perovskites. The\ncomputational framework introduced here provides an efficient parameter-free ab\ninitio methodology for predicting the electronic properties of 3D and layered\nhalide perovskites and their heterostructures, aiding in developing advanced\noptoelectronic devices.",
        "Sangyop Lee has done much work to determine the knot types of twisted torus\nknots, including classifying the twisted torus knots which are the unknot.\nAmong the unknotted twisted torus knots are those of the form $(F_{n+2}, F_n,\nF_{n+1}, -1)$, where $F_i$ is the $i$th Fibonacci number. Here, we consider\ntwisted torus knots with parameters that are defined recursively, similarly to\nthe Fibonacci sequence. We call these \\textit{Horadam parameters}, after the\ngeneralization of the Fibonacci sequence introduced by A.F. Horadam. Here, we\nprovide families of twisted torus knots that generalize Lee's work with Horadam\nparameters. Additionally, we provide lists of primitive\/primitive and\nprimitive\/Seifert twisted torus knots and connect these lists to the Horadam\ntwisted torus knots.",
        "Axions are well-motivated dark matter particles. Many experiments are looking\nfor their experimental evidence. For haloscopes, the problem reduces to the\nidentification of a peak above a noisy baseline. Its modeling, however, may\nproblematic. State-of-the-art analysis rely on the Savitzky-Golay (SG)\nfiltering, which is intrinsically affected by any possible over fluctuation,\nleading to biased results. In this paper we study the efficiency that different\nextensions of SG can provide in the peak reconstruction in a standard\nhaloscope-like experiment. We show that, once the correlations among bins are\ntaken into account, there is no appreciable difference. The standard SG remains\nthe advisable choice because of its numerical efficiency.",
        "Kaon condensation in hyperon-mixed matter [($Y$+$K$) phase], which may be\nrealized in neutron stars, is discussed on the basis of chiral symmetry. With\nthe use of the effective chiral Lagrangian for kaon--baryon and kaon--kaon\ninteractions; coupled with the relativistic mean field theory and universal\nthree-baryon repulsive interaction, we clarify the effects of the $s$-wave\nkaon--baryon scalar interaction simulated by the kaon--baryon sigma terms and\nvector interaction (Tomozawa--Weinberg term) on kaon properties in\nhyperon-mixed matter, the onset density of kaon condensation, and the equation\nof state with the ($Y$+$K$) phase. In particular, the quark condensates in the\n($Y$+$K$) phase are obtained, and their relevance to chiral symmetry\nrestoration is discussed.",
        "In this work we investigate the chemical and kinetic nonequilibrium dynamics\nof the Higgs boson during the primordial Universe QGP (quark-gluon plasma)\nepoch $130\\mathrm{\\,GeV}>T>10\\mathrm{\\,GeV}$. We show that the Higgs bosons is\nalways out of chemical abundance equilibrium with a fugacity $\\Upsilon_h =\n0.69$ due to virtual decay channels. Additionally, Higgs momentum distribution\nis found to be ``cold'' for $T<40$\\,GeV, since the scattering rate drops below\nthe production rate.",
        "In this paper, we report on a study of the road networks of the provinces of\nCanada as complex networks. A number of statistical features have been analyzed\nand compared with two random models. In addition, we have also studied the\nresilience of these road networks under random failures and targeted attacks.",
        "We study uncertainty quantification for partial differential equations\nsubject to domain uncertainty. We parameterize the random domain using the\nmodel recently considered by Chernov and Le (2024) as well as Harbrecht,\nSchmidlin, and Schwab (2024) in which the input random field is assumed to\nbelong to a Gevrey smoothness class. This approach has the advantage of being\nsubstantially more general than models which assume a particular parametric\nrepresentation of the input random field such as a Karhunen-Loeve series\nexpansion. We consider both the Poisson equation as well as the heat equation\nand design randomly shifted lattice quasi-Monte Carlo (QMC) cubature rules for\nthe computation of the expected solution under domain uncertainty. We show that\nthese QMC rules exhibit dimension-independent, essentially linear cubature\nconvergence rates in this framework. In addition, we complete the error\nanalysis by taking into account the approximation errors incurred by dimension\ntruncation of the random input field and finite element discretization.\nNumerical experiments are presented to confirm the theoretical rates.",
        "The study focuses on improving the ex ante prediction accuracy assessment in\nthe case of forecasting various house price dispersion measures in the USA. It\naddresses a critical gap in real estate market forecasting by proposing a novel\nmethod for assessing ex ante prediction accuracy under unanticipated shocks.\nThe proposal is based on a parametric bootstrap approach under a misspecified\nmodel, allowing for the simulation of future values and estimation of\nprediction errors in case of unexpected price changes. The study highlights the\nlimitations of the traditional approach that fails to account for unforeseen\nmarket events and provides a more in-depth understanding of how prediction\naccuracy changes under unexpected scenarios. The proposed methods offers\nvaluable insights for real estate market management by enabling more robust\nrisk assessment and decision-making in the face of unexpected market\nfluctuations. Real data application is based on longitudinal U.S. data on real\nestate transactions.",
        "Most technologically useful materials spanning multiple length scales are\npolycrystalline. Polycrystalline microstructures are composed of a myriad of\nsmall crystals or grains with different lattice orientations which are\nseparated by interfaces or grain boundaries. The changes in the grain and grain\nboundary structure of polycrystals highly influence the materials properties\nincluding, but not limited to, electrical, mechanical, and thermal. Thus, an\nunderstanding of how microstructures evolve is essential for the engineering of\nnew materials. In this paper, we consider a recently introduced nonlinear\nFokker-Planck-type system and establish a global well-posedness result for it.\nSuch systems under specific energy laws emerge in the modeling of the grain\nboundary dynamics in polycrystals.",
        "The structure formation in the local Universe is considered within the\nweak-field modification of General Relativity involving the cosmological\nconstant. This approach enables to describe the dynamics of groups and clusters\nof galaxies, to explain the discrepancy in the observational properties of the\nlocal (late) and the global (early) Universe, i.e. the Hubble tension as a\nresult of two flows, local and global ones, with non-equal Hubble parameters.\nThe kinetic analysis with the modified gravitational potential involving the\ncosmological constant is shown to predict semi-periodical structure of\nfilaments in the local universe. In the local scale this complements the\nZeldovich pancake theory of evolution of the primordial density perturbations\nand of structure formation in the cosmological scale. The role of the\ncosmological constant is outlined in rescaling of the physical constants from\none aeon to another within the Conformal Cyclic Cosmology.",
        "We study the PAC property of scenario decision-making algorithms, that is,\nthe ability to make a decision that has an arbitrarily low risk of violating an\nunknown safety constraint, provided sufficiently many realizations (called\nscenarios) of the safety constraint are sampled. Sufficient conditions for\nscenario decision-making algorithms to be PAC are available in the literature,\nsuch as finiteness of the VC dimension of its associated classifier and\nexistence of a compression scheme. We study the question of whether these\nsufficient conditions are also necessary. We show with counterexamples that\nthis is not the case in general. This contrasts with binary classification\nlearning, for which the analogous conditions are sufficient and necessary.\nPopular scenario decision-making algorithms, such as scenario optimization,\nenjoy additional properties, such as stability and consistency. We show that\neven under these additional assumptions the above conclusions hold. Finally, we\nderive a necessary condition for scenario decision-making algorithms to be PAC,\ninspired by the VC dimension and the so-called no-free-lunch theorem.",
        "This review provides an observational perspective on the fundamental\nproperties of super-Eddington accretion onto supermassive black holes in\nquasars. It begins by outlining the selection criteria, particularly focusing\non optical and UV broad-line intensity ratios, used to identify a population of\nunobscured super-Eddington candidates. Several defining features place these\ncandidates at the extreme end of the Population A in main sequence of quasars:\namong them are the highest observed singly-ionized iron emission, extreme\noutflow velocities in UV resonance lines, and unusually high metal abundances.\nThese key properties reflect the coexistence of a virialized sub-system within\nthe broad-line region alongside powerful outflows, with the observed gas\nenrichment likely driven by nuclear or circumnuclear star formation. The most\ncompelling evidence for the occurrence of super-Eddington accretion onto\nsupermassive black holes comes from recent observations of massive black holes\nat early cosmic epochs. These black holes require rapid growth rates that are\nonly achievable through radiatively inefficient super-Eddington accretion.\nFurthermore, extreme Eddington ratios, close to or slightly exceeding unity,\nare consistent with the saturation of radiative output per unit mass predicted\nby accretion disk theory for super-Eddington accretion rates. The extreme\nproperties of super-Eddington candidates suggest that these quasars could make\nthem stable and well-defined cosmological distance indicators, leveraging the\ncorrelation between broad-line width and luminosity expected in virialized\nsystems. Finally, several analogies with accretion processes around\nstellar-mass black holes, particularly in the high\/soft state, are explored to\nprovide additional insight into the mechanisms driving super-Eddington\naccretion."
      ]
    }
  },
  {
    "id":2411.17971,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"Multiscale modeling and simulation of brain blood flow",
    "start_abstract":"The aim of this work is to present an overview recent advances in multi-scale modeling brain blood flow. In particular, we some approaches that enable the silico study and multi-physics phenomena cerebral vasculature. We discuss formulation continuum atomistic approaches, a consistent framework for their concurrent coupling, list challenges one needs overcome achieving seamless scalable integration heterogeneous numerical solvers. effectiveness proposed demonstrated realistic case involving thrombus formation process taking place on wall patient-specific aneurysm. This highlights ability algorithms resolve important biophysical processes span several spatial temporal scales, potentially yielding new insight into key aspects flow health disease. Finally, open questions emerging topics future research.",
    "start_categories":[
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b5"
      ],
      "title":[
        "Learning Reduced-Order Models for Cardiovascular Simulations with Graph Neural Networks"
      ],
      "abstract":[
        "Reduced-order models based on physics are a popular choice in cardiovascular modeling due to their efficiency, but they may experience loss in accuracy when working with anatomies that contain numerous junctions or pathological conditions. We develop one-dimensional reduced-order models that simulate blood flow dynamics using a graph neural network trained on three-dimensional hemodynamic simulation data. Given the initial condition of the system, the network iteratively predicts the pressure and flow rate at the vessel centerline nodes. Our numerical results demonstrate the accuracy and generalizability of our method in physiological geometries comprising a variety of anatomies and boundary conditions. Our findings demonstrate that our approach can achieve errors below 3% for pressure and flow rate, provided there is adequate training data. As a result, our method exhibits superior performance compared to physics-based one-dimensional models while maintaining high efficiency at inference time."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "Behaviour of Newton Polygon over polynomial composition",
        "Paying Attention to Facts: Quantifying the Knowledge Capacity of\n  Attention Layers",
        "Optimizing Fire Safety: Reducing False Alarms Using Advanced Machine\n  Learning Techniques",
        "$\\ell_{p}$ has nontrivial Euclidean distortion growth when $2<p<4$",
        "Avoiding spurious sharpness minimization broadens applicability of SAM",
        "Disparities in LLM Reasoning Accuracy and Explanations: A Case Study on\n  African American English",
        "MuJoCo Playground",
        "Adaptive Mesh Refinement for Variational Inequalities",
        "Efficient and Universal Neural-Network Decoder for Stabilizer-Based\n  Quantum Error Correction",
        "A View of the Certainty-Equivalence Method for PAC RL as an Application\n  of the Trajectory Tree Method",
        "The BAD Paradox: A Critical Assessment of the Belin\/Ambr\\'osio Deviation\n  Model",
        "Agentic AI Systems Applied to tasks in Financial Services: Modeling and\n  model risk management crews",
        "AoI-Sensitive Data Forwarding with Distributed Beamforming in\n  UAV-Assisted IoT",
        "Balanced Rate-Distortion Optimization in Learned Image Compression",
        "GEMA-Score: Granular Explainable Multi-Agent Score for Radiology Report\n  Evaluation",
        "Vectorial Kato inequality for $p$-harmonic maps with optimal constant",
        "All-order solution of ladders and rainbows in Minimal Subtraction",
        "Sequential One-Sided Hypothesis Testing of Markov Chains",
        "Advancing C-C Coupling of Electrocatalytic CO2 Reduction Reaction for\n  C2+ Products",
        "Making Them a Malicious Database: Exploiting Query Code to Jailbreak\n  Aligned Large Language Models",
        "Formation Control of Multi-agent System with Local Interaction and\n  Artificial Potential Field",
        "Dynamic Bragg microcavities in collisions of unipolar light pulses of\n  unusual shape in two- and three-level medium",
        "Ordinal Exponentiation in Homotopy Type Theory",
        "Hamiltonian Heat Baths, Coarse-Graining and Irreversibility: A\n  Microscopic Dynamical Entropy from Classical Mechanics",
        "RemiHaven: Integrating \"In-Town\" and \"Out-of-Town\" Peers to Provide\n  Personalized Reminiscence Support for Older Drifters",
        "Sim-to-Real Transfer for Mobile Robots with Reinforcement Learning: from\n  NVIDIA Isaac Sim to Gazebo and Real ROS 2 Robots",
        "Derived deformation functors, Koszul duality, and Maurer-Cartan spaces",
        "Leveraging Registers in Vision Transformers for Robust Adaptation",
        "TherAIssist: Assisting Art Therapy Homework and Client-Practitioner\n  Collaboration through Human-AI Interaction"
      ],
      "abstract":[
        "In this paper, we study the structure of Newton polygons for compositions of\npolynomials over the rationals. We establish sufficient conditions under which\nthe successive vertices of the Newton polygon of the composition $ g(f^n(x)) $\nwith respect to a prime $ p $ can be explicitly described in terms of the\nNewton polygon of the polynomial $ g(x) $. Our results provide deeper insights\ninto how the Newton polygon of a polynomial evolves under iteration and\ncomposition, with applications to the study of dynamical irreducibility,\neventual stability, non-monogenity of tower of number fields, etc.",
        "In this paper, we investigate the ability of single-layer attention-only\ntransformers (i.e. attention layers) to memorize facts contained in databases\nfrom a linear-algebraic perspective. We associate with each database a\n3-tensor, propose the rank of this tensor as a measure of the size of the\ndatabase, and provide bounds on the rank in terms of properties of the\ndatabase. We also define a 3-tensor corresponding to an attention layer, and\nempirically demonstrate the relationship between its rank and database rank on\na dataset of toy models and random databases. By highlighting the roles played\nby the value-output and query-key weights, and the effects of argmax and\nsoftmax on rank, our results shed light on the `additive motif' of factual\nrecall in transformers, while also suggesting a way of increasing layer\ncapacity without increasing the number of parameters.",
        "Fire safety practices are important to reduce the extent of destruction\ncaused by fire. While smoke alarms help save lives, firefighters struggle with\nthe increasing number of false alarms. This paper presents a precise and\nefficient Weighted ensemble model for decreasing false alarms. It estimates the\ndensity, computes weights according to the high and low-density regions,\nforwards the high region weights to KNN and low region weights to XGBoost and\ncombines the predictions. The proposed model is effective at reducing response\ntime, increasing fire safety, and minimizing the damage that fires cause. A\nspecifically designed dataset for smoke detection is utilized to test the\nproposed model. In addition, a variety of ML models, such as Logistic\nRegression (LR), Decision Tree (DT), Random Forest (RF), Nai:ve Bayes (NB),\nK-Nearest Neighbour (KNN), Support Vector Machine (SVM), Extreme Gradient\nBoosting (XGBoost), Adaptive Boosting (ADAB), have also been utilized. To\nmaximize the use of the smoke detection dataset, all the algorithms utilize the\nSMOTE re-sampling technique. After evaluating the assessment criteria, this\npaper presents a concise summary of the comprehensive findings obtained by\ncomparing the outcomes of all models.",
        "We prove that if $2 < p < 4$, then every $n$-point subset of $\\ell_{p}$\nembeds into a Hilbert space with distortion $o(\\log n)$, i.e., with distortion\nthat is asymptotically smaller than what Bourgain's embedding theorem provides\nfor arbitrary $n$-point metric spaces. This has been previously unknown for any\n$2<p<\\infty$. We also prove that for every $2<p<\\infty$ the largest possible\nseparation modulus of an $n$-point subset of $\\ell_{p}$ is bounded from above\nand from below by positive constant multiples of $\\sqrt{\\log n}$ which may\ndepend only on $p$.",
        "Curvature regularization techniques like Sharpness Aware Minimization (SAM)\nhave shown great promise in improving generalization on vision tasks. However,\nwe find that SAM performs poorly in domains like natural language processing\n(NLP), often degrading performance -- even with twice the compute budget. We\ninvestigate the discrepancy across domains and find that in the NLP setting,\nSAM is dominated by regularization of the logit statistics -- instead of\nimproving the geometry of the function itself. We use this observation to\ndevelop an alternative algorithm we call Functional-SAM, which regularizes\ncurvature only through modification of the statistics of the overall function\nimplemented by the neural network, and avoids spurious minimization through\nlogit manipulation. Furthermore, we argue that preconditioning the SAM\nperturbation also prevents spurious minimization, and when combined with\nFunctional-SAM, it gives further improvements. Our proposed algorithms show\nimproved performance over AdamW and SAM baselines when trained for an equal\nnumber of steps, in both fixed-length and Chinchilla-style training settings,\nat various model scales (including billion-parameter scale). On the whole, our\nwork highlights the importance of more precise characterizations of sharpness\nin broadening the applicability of curvature regularization to large language\nmodels (LLMs).",
        "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nreasoning tasks, leading to their widespread deployment. However, recent\nstudies have highlighted concerning biases in these models, particularly in\ntheir handling of dialectal variations like African American English (AAE). In\nthis work, we systematically investigate dialectal disparities in LLM reasoning\ntasks. We develop an experimental framework comparing LLM performance given\nStandard American English (SAE) and AAE prompts, combining LLM-based dialect\nconversion with established linguistic analyses. We find that LLMs consistently\nproduce less accurate responses and simpler reasoning chains and explanations\nfor AAE inputs compared to equivalent SAE questions, with disparities most\npronounced in social science and humanities domains. These findings highlight\nsystematic differences in how LLMs process and reason about different language\nvarieties, raising important questions about the development and deployment of\nthese systems in our multilingual and multidialectal world. Our code repository\nis publicly available at https:\/\/github.com\/Runtaozhou\/dialect_bias_eval.",
        "We introduce MuJoCo Playground, a fully open-source framework for robot\nlearning built with MJX, with the express goal of streamlining simulation,\ntraining, and sim-to-real transfer onto robots. With a simple \"pip install\nplayground\", researchers can train policies in minutes on a single GPU.\nPlayground supports diverse robotic platforms, including quadrupeds, humanoids,\ndexterous hands, and robotic arms, enabling zero-shot sim-to-real transfer from\nboth state and pixel inputs. This is achieved through an integrated stack\ncomprising a physics engine, batch renderer, and training environments. Along\nwith video results, the entire framework is freely available at\nplayground.mujoco.org",
        "Variational inequalities play a pivotal role in a wide array of scientific\nand engineering applications. This project presents two techniques for adaptive\nmesh refinement (AMR) in the context of variational inequalities, with a\nspecific focus on the classical obstacle problem.\n  We propose two distinct AMR strategies: Variable Coefficient Elliptic\nSmoothing (VCES) and Unstructured Dilation Operator (UDO). VCES uses a nodal\nactive set indicator function as the initial iterate to a time-dependent heat\nequation problem. Solving a single step of this problem has the effect of\nsmoothing the indicator about the free boundary. We threshold this smoothed\nindicator function to identify elements near the free boundary. Key parameters\nsuch as timestep and threshold values significantly influence the efficacy of\nthis method.\n  The second strategy, UDO, focuses on the discrete identification of elements\nadjacent to the free boundary, employing a graph-based approach to mark\nneighboring elements for refinement. This technique resembles the dilation\nmorphological operation in image processing, but tailored for unstructured\nmeshes.\n  We also examine the theory of variational inequalities, the convergence\nbehavior of finite element solutions, and implementation in the Firedrake\nfinite element library. Convergence analysis reveals that accurate free\nboundary estimation is pivotal for solver performance. Numerical experiments\ndemonstrate the effectiveness of the proposed methods in dynamically enhancing\nmesh resolution around free boundaries, thereby improving the convergence rates\nand computational efficiency of variational inequality solvers. Our approach\nintegrates seamlessly with existing Firedrake numerical solvers, and it is\npromising for solving more complex free boundary problems.",
        "Quantum error correction is crucial for large-scale quantum computing, but\nthe absence of efficient decoders for new codes like quantum low-density\nparity-check (QLDPC) codes has hindered progress. Here we introduce a universal\ndecoder based on linear attention sequence modeling and graph neural network\nthat operates directly on any stabilizer code's graph structure. Our numerical\nexperiments demonstrate that this decoder outperforms specialized algorithms in\nboth accuracy and speed across diverse stabilizer codes, including surface\ncodes, color codes, and QLDPC codes. The decoder maintains linear time scaling\nwith syndrome measurements and requires no structural modifications between\ndifferent codes. For the Bivariate Bicycle code with distance 12, our approach\nachieves a 39.4% lower logical error rate than previous best decoders while\nrequiring only ~1% of the decoding time. These results provide a practical,\nuniversal solution for quantum error correction, eliminating the need for\ncode-specific decoders.",
        "Reinforcement learning (RL) enables an agent interacting with an unknown MDP\n$M$ to optimise its behaviour by observing transitions sampled from $M$. A\nnatural entity that emerges in the agent's reasoning is $\\widehat{M}$, the\nmaximum likelihood estimate of $M$ based on the observed transitions. The\nwell-known \\textit{certainty-equivalence} method (CEM) dictates that the agent\nupdate its behaviour to $\\widehat{\\pi}$, which is an optimal policy for\n$\\widehat{M}$. Not only is CEM intuitive, it has been shown to enjoy\nminimax-optimal sample complexity in some regions of the parameter space for\nPAC RL with a generative model~\\citep{Agarwal2020GenModel}.\n  A seemingly unrelated algorithm is the ``trajectory tree method''\n(TTM)~\\citep{Kearns+MN:1999}, originally developed for efficient decision-time\nplanning in large POMDPs. This paper presents a theoretical investigation that\nstems from the surprising finding that CEM may indeed be viewed as an\napplication of TTM. The qualitative benefits of this view are (1) new and\nsimple proofs of sample complexity upper bounds for CEM, in fact under a (2)\nweaker assumption on the rewards than is prevalent in the current literature.\nOur analysis applies to both non-stationary and stationary MDPs.\nQuantitatively, we obtain (3) improvements in the sample-complexity upper\nbounds for CEM both for non-stationary and stationary MDPs, in the regime that\nthe ``mistake probability'' $\\delta$ is small. Additionally, we show (4) a\nlower bound on the sample complexity for finite-horizon MDPs, which establishes\nthe minimax-optimality of our upper bound for non-stationary MDPs in the\nsmall-$\\delta$ regime.",
        "The Belin\/Ambr\\'osio Deviation (BAD) model is a widely used diagnostic tool\nfor detecting keratoconus and corneal ectasia. The input to the model is a set\nof z-score normalized $D$ indices that represent physical characteristics of\nthe cornea. Paradoxically, the output of the model, Total Deviation Value\n($D_{\\text{final}}$), is reported in standard deviations from the mean, but\n$D_{\\text{final}}$ does not behave like a z-score normalized value. Although\nthresholds like $D_{\\text{final}} \\ge 1.6$ for \"suspicious\" and\n$D_{\\text{final}} \\ge 3.0$ for \"abnormal\" are commonly cited, there is little\nexplanation on how to interpret values outside of those thresholds or to\nunderstand how they relate to physical characteristics of the cornea. This\nstudy explores the reasons for $D_{\\text{final}}$'s apparent inconsistency\nthrough a meta-analysis of published data and a more detailed statistical\nanalysis of over 1,600 Pentacam exams. The results reveal that systematic bias\nin the BAD regression model, multicollinearity among predictors, and\ninconsistencies in normative datasets contribute to the non-zero mean of\n$D_{\\text{final}}$, complicating its clinical interpretation. These findings\nhighlight critical limitations in the model's design and underscore the need\nfor recalibration to enhance its transparency and diagnostic reliability.",
        "The advent of large language models has ushered in a new era of agentic\nsystems, where artificial intelligence programs exhibit remarkable autonomous\ndecision-making capabilities across diverse domains. This paper explores\nagentic system workflows in the financial services industry. In particular, we\nbuild agentic crews that can effectively collaborate to perform complex\nmodeling and model risk management (MRM) tasks. The modeling crew consists of a\nmanager and multiple agents who perform specific tasks such as exploratory data\nanalysis, feature engineering, model selection, hyperparameter tuning, model\ntraining, model evaluation, and writing documentation. The MRM crew consists of\na manager along with specialized agents who perform tasks such as checking\ncompliance of modeling documentation, model replication, conceptual soundness,\nanalysis of outcomes, and writing documentation. We demonstrate the\neffectiveness and robustness of modeling and MRM crews by presenting a series\nof numerical examples applied to credit card fraud detection, credit card\napproval, and portfolio credit risk modeling datasets.",
        "This paper proposes a UAV-assisted forwarding system based on distributed\nbeamforming to enhance age of information (AoI) in Internet of Things (IoT).\nSpecifically, UAVs collect and relay data between sensor nodes (SNs) and the\nremote base station (BS). However, flight delays increase the AoI and degrade\nthe network performance. To mitigate this, we adopt distributed beamforming to\nextend the communication range, reduce the flight frequency and ensure the\ncontinuous data relay and efficient energy utilization. Then, we formulate an\noptimization problem to minimize AoI and UAV energy consumption, by jointly\noptimizing the UAV trajectories and communication schedules. The problem is\nnon-convex and with high dynamic, and thus we propose a deep reinforcement\nlearning (DRL)-based algorithm to solve the problem, thereby enhancing the\nstability and accelerate convergence speed. Simulation results show that the\nproposed algorithm effectively addresses the problem and outperforms other\nbenchmark algorithms.",
        "Learned image compression (LIC) using deep learning architectures has seen\nsignificant advancements, yet standard rate-distortion (R-D) optimization often\nencounters imbalanced updates due to diverse gradients of the rate and\ndistortion objectives. This imbalance can lead to suboptimal optimization,\nwhere one objective dominates, thereby reducing overall compression efficiency.\nTo address this challenge, we reformulate R-D optimization as a multi-objective\noptimization (MOO) problem and introduce two balanced R-D optimization\nstrategies that adaptively adjust gradient updates to achieve more equitable\nimprovements in both rate and distortion. The first proposed strategy utilizes\na coarse-to-fine gradient descent approach along standard R-D optimization\ntrajectories, making it particularly suitable for training LIC models from\nscratch. The second proposed strategy analytically addresses the reformulated\noptimization as a quadratic programming problem with an equality constraint,\nwhich is ideal for fine-tuning existing models. Experimental results\ndemonstrate that both proposed methods enhance the R-D performance of LIC\nmodels, achieving around a 2\\% BD-Rate reduction with acceptable additional\ntraining cost, leading to a more balanced and efficient optimization process.\nCode will be available at https:\/\/gitlab.com\/viper-purdue\/Balanced-RD.",
        "Automatic medical report generation supports clinical diagnosis, reduces the\nworkload of radiologists, and holds the promise of improving diagnosis\nconsistency. However, existing evaluation metrics primarily assess the accuracy\nof key medical information coverage in generated reports compared to\nhuman-written reports, while overlooking crucial details such as the location\nand certainty of reported abnormalities. These limitations hinder the\ncomprehensive assessment of the reliability of generated reports and pose risks\nin their selection for clinical use. Therefore, we propose a Granular\nExplainable Multi-Agent Score (GEMA-Score) in this paper, which conducts both\nobjective quantification and subjective evaluation through a large language\nmodel-based multi-agent workflow. Our GEMA-Score parses structured reports and\nemploys NER-F1 calculations through interactive exchanges of information among\nagents to assess disease diagnosis, location, severity, and uncertainty.\nAdditionally, an LLM-based scoring agent evaluates completeness, readability,\nand clinical terminology while providing explanatory feedback. Extensive\nexperiments validate that GEMA-Score achieves the highest correlation with\nhuman expert evaluations on a public dataset, demonstrating its effectiveness\nin clinical scoring (Kendall coefficient = 0.70 for Rexval dataset and Kendall\ncoefficient = 0.54 for RadEvalX dataset). The anonymous project demo is\navailable at: https:\/\/github.com\/Zhenxuan-Zhang\/GEMA_score.",
        "We derive the sharp vectorial Kato inequality for $p$-harmonic mappings.\nSurprisingly, the optimal constant differs from the one obtained for scalar\nvalued $p$-harmonic functions by Chang, Chen, and Wei. As an application we\ndemonstrate how this inequality can be used in the study of regularity of\n$p$-harmonic maps. Furthermore, in the case of $p$-harmonic maps from $B^3$ to\n$\\mathbb{S}^3$, we enhance the known range of $p$ values for which regularity\nis achieved. Specifically, we establish that for $p \\in [2, 2.642]$, minimizing\n$p$-harmonic maps must be regular.",
        "In dimensional regularization with $D=D_0-2\\epsilon$, the minimal subtraction\n(MS) scheme is characterized by counterterms that only consist of singular\nterms in $\\epsilon$. We develop a general method to compute the infinite sums\nof massless ladder or rainbow Feynman integrals in MS at $D_0$. Our method is\nbased on relating the MS-solution to a kinematic solution at a\ncoupling-dependent renormalization point. If the $\\epsilon$-dependent Mellin\ntransform of the kernel diagram of the insertions can be computed in closed\nform, we typically obtain a closed expression for the all-order solution in MS.\nAs examples, we consider Yukawa theory and $\\phi^4$ theory in $D_0=4$, and\n$\\phi^3$ theory in $D_0=6$.",
        "We study the problem of sequentially testing whether a given stochastic\nprocess is generated by a known Markov chain. Formally, given access to a\nstream of random variables, we want to quickly determine whether this sequence\nis a trajectory of a Markov chain with a known transition matrix $P$ (null\nhypothesis) or not (composite alternative hypothesis). This problem naturally\narises in many engineering problems.\n  The main technical challenge is to develop a sequential testing scheme that\nadapts its sample size to the unknown alternative. Indeed, if we knew the\nalternative distribution (that is, the transition matrix) $Q$, a natural\napproach would be to use a generalization of Wald's sequential probability\nratio test (SPRT). Building on this intuition, we propose and analyze a family\nof one-sided SPRT-type tests for our problem that use a data-driven estimator\n$\\hat{Q}$. In particular, we show that if the deployed estimator admits a\nworst-case regret guarantee scaling as $\\mathcal{O}\\left( \\log{t} \\right)$,\nthen the performance of our test asymptotically matches that of SPRT in the\nsimple hypothesis testing case. In other words, our test automatically adapts\nto the unknown hardness of the problem, without any prior information. We end\nwith a discussion of known Markov chain estimators with $\\mathcal{O}\\left(\n\\log{t} \\right)$ regret.",
        "The production of multicarbon (C2+) products through electrocatalytic CO2\nreduction reaction (CO2RR) is crucial to addressing global environmental\nchallenges and advancing sustainable energy solutions. However, efficiently\nproducing these high-value chemicals via C-C coupling reactions is a\nsignificant challenge. This requires catalysts with optimized surface\nconfigurations and electronic properties capable of breaking the scaling\nrelations among various intermediates. In this report, we introduce the\nfundamentals of electrocatalytic CO2RR and the mechanism of C-C coupling. We\nexamine the effects of catalytic surface interactions with key intermediates\nand reaction pathways, and discuss emerging strategies for enhancing C-C\ncoupling reactions toward C2+ products. Despite varieties of these strategies,\nwe summarize direct clues for the proper design of the catalyst for the\nelectrocatalytic CO2RR towards C2+ products, aiming to provide valuable\ninsights to broad readers in the field.",
        "Recent advances in large language models (LLMs) have demonstrated remarkable\npotential in the field of natural language processing. Unfortunately, LLMs face\nsignificant security and ethical risks. Although techniques such as safety\nalignment are developed for defense, prior researches reveal the possibility of\nbypassing such defenses through well-designed jailbreak attacks. In this paper,\nwe propose QueryAttack, a novel framework to examine the generalizability of\nsafety alignment. By treating LLMs as knowledge databases, we translate\nmalicious queries in natural language into structured non-natural query\nlanguage to bypass the safety alignment mechanisms of LLMs. We conduct\nextensive experiments on mainstream LLMs, and the results show that QueryAttack\nnot only can achieve high attack success rates (ASRs), but also can jailbreak\nvarious defense methods. Furthermore, we tailor a defense method against\nQueryAttack, which can reduce ASR by up to 64% on GPT-4-1106. Our code is\navailable at https:\/\/github.com\/horizonsinzqs\/QueryAttack.",
        "A novel local interaction control method (LICM) is proposed in this paper to\nrealize the formation control of multi-agent system (MAS). A local interaction\nleader follower (LILF) structure is provided by coupling the advantages of\ninformation consensus and leader follower frame, the agents can obtain the\nstate information of the leader by interacting with their neighbours, which\nwill reduce the communication overhead of the system and the dependence on a\nsingle node of the topology. In addition, the artificial potential field (APF)\nmethod is introduced to achieve obstacle avoidance and collision avoidance\nbetween agents. Inspired by the stress response of animals, a stress response\nmechanism-artificial potential field (SRM-APF) is proposed, which will be\ntriggered when the local minimum problem of APF occurs. Ultimately, the\nsimulation experiments of three formation shapes, including triangular\nformation, square formation and hexagonal formation, validate the effectiveness\nof the proposed method.",
        "Unipolar light pulses with a non-zero electric area due to the unidirectional\naction on charged particles can be used for the ultrafast control of the\nproperties of quantum systems. To control atomic properties in an efficient\nway, it is necessary to vary the temporal shape of the pulses used. This has\nled to the problem of obtaining pulses of an unusual shape, such as a\nrectangular one. A number of new phenomena, not possible with conventional\nmulti-cycle pulses, were discovered by analyzing the interaction of such\nunipolar pulses with matter. These include the formation of dynamic\nmicrocavities at each resonant transition of a multilevel medium when such\npulses collide with the medium. In this work, we compare the behavior of\ndynamic microcavities in a two-level and a three-level medium when unipolar\npulses of unusual shape (rectangular) are collided with the medium. We do this\non the basis of the numerical solution of the system for the density matrix of\nthe medium and the wave equation for the electric field. Medium parameters\ncorrespond to atomic hydrogen. It is shown that for rectangular pulses in a\nthree-level medium, the dynamics of the cavities can be very different from the\ntwo-level model, as opposed to pulses of other shapes (e.g. Gaussian shape).\nWhen the third level of the medium is taken into account, the self-induced\ntransparency-like regime disappears. Differences in the dynamics of resonators\nin a three-level medium are revealed when the pulses behave like 2{\\pi} pulses\nof self-induced transparency.",
        "While ordinals have traditionally been studied mostly in classical\nframeworks, constructive ordinal theory has seen significant progress in recent\nyears. However, a general constructive treatment of ordinal exponentiation has\nthus far been missing. We present two seemingly different definitions of\nconstructive ordinal exponentiation in the setting of homotopy type theory. The\nfirst is abstract, uses suprema of ordinals, and is solely motivated by the\nexpected equations. The second is more concrete, based on decreasing lists, and\ncan be seen as a constructive version of a classical construction by\nSierpi\\'{n}ski based on functions with finite support. We show that our two\napproaches are equivalent (whenever it makes sense to ask the question), and\nuse this equivalence to prove algebraic laws and decidability properties of the\nexponential. All our results are formalized in the proof assistant Agda.",
        "The Hamiltonian evolution of an isolated classical system is reversible, yet\nthe second law of thermodynamics states that its entropy can only increase.\nThis has confounded attempts to identify a `Microscopic Dynamical Entropy'\n(MDE), by which we mean an entropy computable from the system's evolving\nphase-space density $\\rho(t)$, that equates {\\em quantitatively} to its\nthermodynamic entropy $S^{\\rm th}(t)$, both within and beyond equilibrium.\nSpecifically, under Hamiltonian dynamics the Gibbs entropy of $\\rho$ is\nconserved in time; those of coarse-grained approximants to $\\rho$ show a second\nlaw but remain quantitatively unrelated to heat flow. Moreover coarse-graining\ngenerally destroys the Hamiltonian evolution, giving paradoxical predictions\nwhen $\\rho(t)$ exactly rewinds, as it does after velocity-reversal. Here we\nderive the MDE for an isolated system XY in which subsystem Y acts as a heat\nbath for subsystem X. We allow $\\rho_{XY}(t)$ to evolve without\ncoarse-graining, but compute its entropy by disregarding the detailed structure\nof $\\rho_{Y|X}$. The Gibbs entropy of the resulting phase-space density\n$\\tilde\\rho_{XY}(t)$ comprises the MDE for the purposes of both classical and\nstochastic thermodynamics. The MDE obeys the second law whenever $\\rho_X$\nevolves independently of the details of Y, yet correctly rewinds after\nvelocity-reversal of the full XY system.",
        "With increasing social mobility and an aging society, more older adults in\nChina are migrating to new cities, known as \"older drifters.\" Due to fewer\nsocial connections and cultural adaptation challenges, they face negative\nemotions such as loneliness and depression. While reminiscence-based\ninterventions have been used to improve older adults' psychological well-being,\nchallenges such as the lack of tangible materials and limited social resources\nconstrain the feasibility of traditional reminiscence approaches for older\ndrifters. To address this challenge, we designed RemiHaven, a personalized\nreminiscence support tool based on a two-phase formative study. It integrates\n\"In-Town\" and \"Out-of-Town\" peer agents to enhance personalization, engagement,\nand emotional resonance in the reminiscence process, powered by Multimodal\nLarge Language Models (MLLMs). Our evaluations show RemiHaven's strengths in\nsupporting reminiscence while identifying potential challenges. We conclude by\noffering insights for the future design of reminiscence support tools for older\nmigrants.",
        "Unprecedented agility and dexterous manipulation have been demonstrated with\ncontrollers based on deep reinforcement learning (RL), with a significant\nimpact on legged and humanoid robots. Modern tooling and simulation platforms,\nsuch as NVIDIA Isaac Sim, have been enabling such advances. This article\nfocuses on demonstrating the applications of Isaac in local planning and\nobstacle avoidance as one of the most fundamental ways in which a mobile robot\ninteracts with its environments. Although there is extensive research on\nproprioception-based RL policies, the article highlights less standardized and\nreproducible approaches to exteroception. At the same time, the article aims to\nprovide a base framework for end-to-end local navigation policies and how a\ncustom robot can be trained in such simulation environment. We benchmark\nend-to-end policies with the state-of-the-art Nav2, navigation stack in Robot\nOperating System (ROS). We also cover the sim-to-real transfer process by\ndemonstrating zero-shot transferability of policies trained in the Isaac\nsimulator to real-world robots. This is further evidenced by the tests with\ndifferent simulated robots, which show the generalization of the learned\npolicy. Finally, the benchmarks demonstrate comparable performance to Nav2,\nopening the door to quick deployment of state-of-the-art end-to-end local\nplanners for custom robot platforms, but importantly furthering the\npossibilities by expanding the state and action spaces or task definitions for\nmore complex missions. Overall, with this article we introduce the most\nimportant steps, and aspects to consider, in deploying RL policies for local\npath planning and obstacle avoidance with Isaac Sim training, Gazebo testing,\nand ROS 2 for real-time inference in real robots. The code is available at\nhttps:\/\/github.com\/sahars93\/RL-Navigation.",
        "We summarise the chain of comparisons showing Hinich's derived Maurer-Cartan\nfunctor gives an equivalence between differential graded Lie algebras and\nderived Schlessinger functors on Artinian differential graded-commutative\nalgebras. We include some motivating deformation problems and analogues for\nmore general Koszul dual pairs of operads.",
        "Vision Transformers (ViTs) have shown success across a variety of tasks due\nto their ability to capture global image representations. Recent studies have\nidentified the existence of high-norm tokens in ViTs, which can interfere with\nunsupervised object discovery. To address this, the use of \"registers\" which\nare additional tokens that isolate high norm patch tokens while capturing\nglobal image-level information has been proposed. While registers have been\nstudied extensively for object discovery, their generalization properties\nparticularly in out-of-distribution (OOD) scenarios, remains underexplored. In\nthis paper, we examine the utility of register token embeddings in providing\nadditional features for improving generalization and anomaly rejection. To that\nend, we propose a simple method that combines the special CLS token embedding\ncommonly employed in ViTs with the average-pooled register embeddings to create\nfeature representations which are subsequently used for training a downstream\nclassifier. We find that this enhances OOD generalization and anomaly\nrejection, while maintaining in-distribution (ID) performance. Extensive\nexperiments across multiple ViT backbones trained with and without registers\nreveal consistent improvements of 2-4\\% in top-1 OOD accuracy and a 2-3\\%\nreduction in false positive rates for anomaly detection. Importantly, these\ngains are achieved without additional computational overhead.",
        "Art therapy homework is essential for fostering clients' reflection on daily\nexperiences between sessions. However, current practices present challenges:\nclients often lack guidance for completing tasks that combine art-making and\nverbal expression, while therapists find it difficult to track and tailor\nhomework. How HCI systems might support art therapy homework remains\nunderexplored. To address this, we present TherAIssist, comprising a\nclient-facing application leveraging human-AI co-creative art-making and\nconversational agents to facilitate homework, and a therapist-facing\napplication enabling customization of homework agents and AI-compiled homework\nhistory. A 30-day field study with 24 clients and 5 therapists showed how\nTherAIssist supported clients' homework and reflection in their everyday\nsettings. Results also revealed how therapists infused their practice\nprinciples and personal touch into the agents to offer tailored homework, and\nhow AI-compiled homework history became a meaningful resource for in-session\ninteractions. Implications for designing human-AI systems to facilitate\nasynchronous client-practitioner collaboration are discussed."
      ]
    }
  },
  {
    "id":2411.17971,
    "research_type":"applied",
    "start_id":"b5",
    "start_title":"Learning Reduced-Order Models for Cardiovascular Simulations with Graph Neural Networks",
    "start_abstract":"Reduced-order models based on physics are a popular choice in cardiovascular modeling due to their efficiency, but they may experience loss in accuracy when working with anatomies that contain numerous junctions or pathological conditions. We develop one-dimensional reduced-order models that simulate blood flow dynamics using a graph neural network trained on three-dimensional hemodynamic simulation data. Given the initial condition of the system, the network iteratively predicts the pressure and flow rate at the vessel centerline nodes. Our numerical results demonstrate the accuracy and generalizability of our method in physiological geometries comprising a variety of anatomies and boundary conditions. Our findings demonstrate that our approach can achieve errors below 3% for pressure and flow rate, provided there is adequate training data. As a result, our method exhibits superior performance compared to physics-based one-dimensional models while maintaining high efficiency at inference time.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Multiscale modeling and simulation of brain blood flow"
      ],
      "abstract":[
        "The aim of this work is to present an overview recent advances in multi-scale modeling brain blood flow. In particular, we some approaches that enable the silico study and multi-physics phenomena cerebral vasculature. We discuss formulation continuum atomistic approaches, a consistent framework for their concurrent coupling, list challenges one needs overcome achieving seamless scalable integration heterogeneous numerical solvers. effectiveness proposed demonstrated realistic case involving thrombus formation process taking place on wall patient-specific aneurysm. This highlights ability algorithms resolve important biophysical processes span several spatial temporal scales, potentially yielding new insight into key aspects flow health disease. Finally, open questions emerging topics future research."
      ],
      "categories":[
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "Fundamental Trade-off Between Computation and Communication in Private\n  Coded Distributed Computing",
        "Reinforcement-Learning Portfolio Allocation with Dynamic Embedding of\n  Market Information",
        "Adaptive Moment Estimation Optimization Algorithm Using Projection\n  Gradient for Deep Learning",
        "Completely Integrable Foliations: Singular Locus, Invariant Curves and\n  Topological Counterparts",
        "Enhanced Derivative-Free Optimization Using Adaptive Correlation-Induced\n  Finite Difference Estimators",
        "Efficient Framework for Solving Plasma Waves with Arbitrary\n  Distributions",
        "Scalable skewed Bayesian inference for latent Gaussian models",
        "Signal-to-noise ratio aware minimax analysis of sparse linear regression",
        "Unveiling the Dynamics and Genesis of Small-scale Fine Structure Loops\n  in the Lower Solar Atmosphere",
        "Data mining the functional architecture of the brain's circuitry",
        "LuxNAS: A Coherent Photonic Neural Network Powered by Neural\n  Architecture Search",
        "Single spin asymmetry in forward $pA$ collisions from Pomeron-Odderon\n  interference",
        "Keldysh field theory approach to direct electric and thermoelectric\n  currents in quantum dots coupled to superconducting leads",
        "Quantum-enhanced quickest change detection of transmission loss",
        "Alpha-element abundance patterns in star-forming regions of the local\n  Universe",
        "Edge spectrum for truncated $\\mathbb{Z}_2$-insulators",
        "Sub-kHz single-frequency pulsed semiconductor laser based on NPRO\n  injection locking",
        "Enhancing Olfactory Perception Through Large Language Models:\n  Integrating Sensory Data for Advanced Odor Recognition",
        "On Lorentzian-Euclidean black holes and Lorentzian to Riemannian metric\n  transitions",
        "Network topology of the Euro Area interbank market",
        "High-Dimensional Bayesian Optimization Using Both Random and Supervised\n  Embeddings",
        "Observe Gamma-Rays and Neutrinos Associated with Ultra-High Energy\n  Cosmic Rays",
        "Role extraction by matrix equations and generalized random walks",
        "Inheritance of shadowing for dynamical semigroups",
        "Image Reconstruction from an Elastically Distorted Scan",
        "Investigating Solar Wind Outflows from Open-Closed Magnetic Field\n  Structures Using Coordinated Solar Orbiter and Hinode Observations",
        "Reed-Muller Codes on CQ Channels via a New Correlation Bound for Quantum\n  Observables",
        "Transitions to Intermittent Chaos in Quorum Sensing Dynamics",
        "SIAC Accuracy Enhancement of Stochastic Galerkin Solutions for Wave\n  Equations with Uncertain Coefficients"
      ],
      "abstract":[
        "Distributed computing enables scalable machine learning by distributing tasks\nacross multiple nodes, but ensuring privacy in such systems remains a\nchallenge. This paper introduces a private coded distributed computing model\nthat integrates privacy constraints to keep task assignments hidden. By\nleveraging placement delivery arrays (PDAs), we design an extended PDA\nframework to characterize achievable computation and communication loads under\nprivacy constraints. By constructing two classes of extended PDAs, we explore\nthe trade-offs between computation and communication, showing that although\nprivacy increases communication overhead, it can be significantly alleviated\nthrough optimized PDA-based coded strategies.",
        "We develop a portfolio allocation framework that leverages deep learning\ntechniques to address challenges arising from high-dimensional, non-stationary,\nand low-signal-to-noise market information. Our approach includes a dynamic\nembedding method that reduces the non-stationary, high-dimensional state space\ninto a lower-dimensional representation. We design a reinforcement learning\n(RL) framework that integrates generative autoencoders and online meta-learning\nto dynamically embed market information, enabling the RL agent to focus on the\nmost impactful parts of the state space for portfolio allocation decisions.\nEmpirical analysis based on the top 500 U.S. stocks demonstrates that our\nframework outperforms common portfolio benchmarks and the predict-then-optimize\n(PTO) approach using machine learning, particularly during periods of market\nstress. Traditional factor models do not fully explain this superior\nperformance. The framework's ability to time volatility reduces its market\nexposure during turbulent times. Ablation studies confirm the robustness of\nthis performance across various reinforcement learning algorithms.\nAdditionally, the embedding and meta-learning techniques effectively manage the\ncomplexities of high-dimensional, noisy, and non-stationary financial data,\nenhancing both portfolio performance and risk management.",
        "Training deep neural networks is challenging. To accelerate training and\nenhance performance, we propose PadamP, a novel optimization algorithm. PadamP\nis derived by applying the adaptive estimation of the p-th power of the\nsecond-order moments under scale invariance, enhancing projection adaptability\nby modifying the projection discrimination condition. It is integrated into\nAdam-type algorithms, accelerating training, boosting performance, and\nimproving generalization in deep learning. Combining projected gradient\nbenefits with adaptive moment estimation, PadamP tackles unconstrained\nnon-convex problems. Convergence for the non-convex case is analyzed, focusing\non the decoupling of first-order moment estimation coefficients and\nsecond-order moment estimation coefficients. Unlike prior work relying on , our\nproof generalizes the convergence theorem, enhancing practicality. Experiments\nusing VGG-16 and ResNet-18 on CIFAR-10 and CIFAR-100 show PadamP's\neffectiveness, with notable performance on CIFAR-10\/100, especially for VGG-16.\nThe results demonstrate that PadamP outperforms existing algorithms in terms of\nconvergence speed and generalization ability, making it a valuable addition to\nthe field of deep learning optimization.",
        "We study codimension $q \\geq 2$ holomorphic foliations defined in a\nneighborhood of a point $P$ of a complex manifold that are completely\nintegrable, i.e. with $q$ independent meromorphic first integrals. We show that\neither $P$ is a regular point, a non-isolated singularity or there are\ninfinitely many invariant analytic varieties through $P$ of the same dimension\nas the foliation, the so called separatrices. Moreover, we see that this\nphenomenon is of topological nature.\n  Indeed, we introduce topological counterparts of completely integrable local\nholomorphic foliations and tools, specially the concept of total holonomy\ngroup, to build holomorphic first integrals if they have isolated separatrices.\nAs a result, we provide a topological characterization of completely integrable\nnon-degenerated elementary isolated singularities of vector fields with an\nisolated separatrix.",
        "Gradient-based methods are well-suited for derivative-free optimization\n(DFO), where finite-difference (FD) estimates are commonly used as gradient\nsurrogates. Traditional stochastic approximation methods, such as\nKiefer-Wolfowitz (KW) and simultaneous perturbation stochastic approximation\n(SPSA), typically utilize only two samples per iteration, resulting in\nimprecise gradient estimates and necessitating diminishing step sizes for\nconvergence. In this paper, we first explore an efficient FD estimate, referred\nto as correlation-induced FD estimate, which is a batch-based estimate. Then,\nwe propose an adaptive sampling strategy that dynamically determines the batch\nsize at each iteration. By combining these two components, we develop an\nalgorithm designed to enhance DFO in terms of both gradient estimation\nefficiency and sample efficiency. Furthermore, we establish the consistency of\nour proposed algorithm and demonstrate that, despite using a batch of samples\nper iteration, it achieves the same convergence rate as the KW and SPSA\nmethods. Additionally, we propose a novel stochastic line search technique to\nadaptively tune the step size in practice. Finally, comprehensive numerical\nexperiments confirm the superior empirical performance of the proposed\nalgorithm.",
        "Plasma, which constitutes 99\\% of the visible matter in the universe, is\ncharacterized by a wide range of waves and instabilities that play a pivotal\nrole in space physics, astrophysics, laser-plasma interactions, fusion\nresearch, and laboratory experiments. The linear physics of these phenomena is\ndescribed by kinetic dispersion relations (KDR). However, solving KDRs for\narbitrary velocity distributions remains a significant challenge, particularly\nfor non-Maxwellian distributions frequently observed in various plasma\nenvironments. This work introduces a novel, efficient, and unified numerical\nframework to address this challenge. The proposed method rapidly and accurately\nyields all significant solutions of KDRs for nearly arbitrary velocity\ndistributions, supporting both unstable and damped modes across all frequencies\nand wavevectors. The approach expands plasma species' velocity distribution\nfunctions using a series of carefully chosen orthogonal basis functions and\nemploys a highly accurate rational approximation to transform the problem into\nan equivalent matrix eigenvalue problem, eliminating the need for initial\nguesses. The efficiency and versatility of this framework are demonstrated,\nenabling simplified studies of plasma waves with arbitrary distributions. This\nadvancement paves the way for uncovering new physics in natural plasma\nenvironments, such as spacecraft observations in space plasmas, and\napplications like wave heating in fusion research.",
        "Approximate Bayesian inference for the class of latent Gaussian models can be\nachieved efficiently with integrated nested Laplace approximations (INLA).\nBased on recent reformulations in the INLA methodology, we propose a further\nextension that is necessary in some cases like heavy-tailed likelihoods or\nbinary regression with imbalanced data. This extension formulates a skewed\nversion of the Laplace method such that some marginals are skewed and some are\nkept Gaussian while the dependence is maintained with the Gaussian copula from\nthe Laplace method. Our approach is formulated to be scalable in model and data\nsize, using a variational inferential framework enveloped in INLA. We\nillustrate the necessity and performance using simulated cases, as well as a\ncase study of a rare disease where class imbalance is naturally present.",
        "We consider parameter estimation under sparse linear regression -- an\nextensively studied problem in high-dimensional statistics and compressed\nsensing. While the minimax framework has been one of the most fundamental\napproaches for studying statistical optimality in this problem, we identify two\nimportant issues that the existing minimax analyses face: (i) The\nsignal-to-noise ratio appears to have no effect on the minimax optimality,\nwhile it shows a major impact in numerical simulations. (ii) Estimators such as\nbest subset selection and Lasso are shown to be minimax optimal, yet they\nexhibit significantly different performances in simulations. In this paper, we\ntackle the two issues by employing a minimax framework that accounts for\nvariations in the signal-to-noise ratio (SNR), termed the SNR-aware minimax\nframework. We adopt a delicate higher-order asymptotic analysis technique to\nobtain the SNR-aware minimax risk. Our theoretical findings determine three\ndistinct SNR regimes: low-SNR, medium-SNR, and high-SNR, wherein minimax\noptimal estimators exhibit markedly different behaviors. The new theory not\nonly offers much better elaborations for empirical results, but also brings new\ninsights to the estimation of sparse signals in noisy data.",
        "Recent high-resolution solar observations have unveiled the presence of\nsmall-scale loop-like structures in the lower solar atmosphere, often referred\nto as unresolved fine structures, low-lying loops, and miniature hot loops.\nThese structures undergo rapid changes within minutes, and their formation\nmechanism has remained elusive. In this study, we conducted a comprehensive\nanalysis of two small loops utilizing data from the Interface Region Imaging\nSpectrograph (IRIS), the Goode Solar Telescope (GST) at Big Bear Solar\nObservatory, and the Atmospheric Imaging Assembly (AIA) and the Helioseismic\nMagnetic Imager (HMI) onboard the Solar Dynamics Observatory (SDO), aiming to\nelucidate the underlying process behind their formation. The GST observations\nrevealed that these loops, with lengths of $\\sim$3.5 Mm and heights of $\\sim$1\nMm, manifest as bright emission structures in H$\\alpha$ wing images,\nparticularly prominent in the red wing. IRIS observations showcased these loops\nin 1330 angstrom slit-jaw images, with TR and chromospheric line spectra\nexhibiting significant enhancement and broadening above the loops, indicative\nof plasmoid-mediated reconnection during their formation. Additionally, we\nobserved upward-erupting jets above these loops across various passbands.\nFurthermore, differential emission measurement analysis reveals an enhanced\nemission measure at the location of these loops, suggesting the presence of\nplasma exceeding 1 MK. Based on our observations, we propose that these loops\nand associated jets align with the minifilament eruption model. Our findings\nsuggest a unified mechanism governing the formation of small-scale loops and\njets akin to larger-scale X-ray jets.",
        "The brain is a highly complex organ consisting of a myriad of subsystems that\nflexibly interact and adapt over time and context to enable perception,\ncognition, and behavior. Understanding the multi-scale nature of the brain,\ni.e., how circuit- and moleclular-level interactions build up the fundamental\ncomponents of brain function, holds incredible potential for developing\ninterventions for neurodegenerative and psychiatric diseases, as well as open\nnew understanding into our very nature. Historically technological limitations\nhave forced systems neuroscience to be local in anatomy (localized, small\nneural populations in single brain areas), in behavior (studying single tasks),\nin time (focusing on specific stages of learning or development), and in\nmodality (focusing on imaging single biological quantities). New developments\nin neural recording technology and behavioral monitoring now provide the data\nneeded to break free of local neuroscience to global neuroscience: i.e.,\nunderstanding how the brain's many subsystem interact, adapt, and change across\nthe multitude of behaviors animals and humans must perform to thrive.\nSpecifically, while we have much knowledge of the anatomical architecture of\nthe brain (i.e., the hardware), we finally are approaching the data needed to\nfind the functional architecture and discover the fundamental properties of the\nsoftware that runs on the hardware. We must take this opportunity to bridge\nbetween the vast amounts of data to discover this functional architecture which\nwill face numerous challenges from low-level data alignment up to high level\nquestions of interpretable mathematical models of behavior that can synthesize\nthe myriad of datasets together.",
        "We demonstrate a novel coherent photonic neural network using tunable\nphase-change-material-based couplers and neural architecture search. Compared\nto the MZI-based Clements network, our results indicate 85% reduction in the\nnetwork footprint while maintaining the accuracy.",
        "Working in the hybrid framework of the high energy $pA$ collisions we\nidentify a new contribution to transverse single spin asymmetry (SSA). The\nphase necessary for the SSA is provided by the Pomeron-Odderon interference in\nthe dense nuclear target. The complete formula for the $pA \\to h X$ polarized\ncross section also contains the transversity distribution for the polarized\nprojectile as well as the real part of the twist-3 fragmentation function. We\nnumerically estimate the asymmetry $A_N$ and its nuclear dependence. Based on a\nmodel computation we find that $A_N$ can be a percent level in the forward and\nlow-$P_{h\\perp}$ region. For large nuclei we find significant suppression, with\n$A_N \\propto A^{-7\/6}$ parametrically. As a notable feature we find a node of\n$A_N$ as a function of the $P_{h\\perp}$ around the values of the initial\nsaturation scale that could be used to test this mechanism experimentally.",
        "We study the transport properties of a quantum dot contacted to two\nsuperconducting reservoirs by means of the Keldysh field theory approach. We\ndetermine the direct current occurring at equilibrium and the electric and\nthermoelectric currents triggered when the system is driven out of equilibrium\nby a voltage or a temperature bias, also for a normal-quantum\ndot-superconductor junction. In particular, we derive and present for the first\ntime the explicit expression of the thermoelectric current in a\nsuperconductor-quantum dot-superconductor junction for any values of the\ntemperature difference between the superconducting leads. We show that in the\nlinear response regime, in addition to the Josephson current, a weakly\nphase-dependent thermoelectric contribution occurs, providing that\nelectron-hole symmetry is broken. Far from linearity, instead, other\ncontributions arise which lead to thermoelectric effects, dominant at weak\ncoupling, also in the presence of particle-hole symmetry.",
        "A sudden increase of loss in an optical communications channel can be caused\nby a malicious wiretapper, or for a benign reason such as inclement weather in\na free-space channel or an unintentional bend in an optical fiber. We show that\nadding a small amount of squeezing to bright phase-modulated coherent-state\npulses can dramatically increase the homodyne detection receiver's sensitivity\nto change detection in channel loss, without affecting the communications rate.\nWe further show that augmenting blocks of $n$ pulses of a coherent-state\ncodeword with weak continuous-variable entanglement generated by splitting\nsqueezed vacuum pulses in a temporal $n$-mode equal splitter progressively\nenhances this change-detection sensitivity as $n$ increases; the aforesaid\nsqueezed-light augmentation being the $n=1$ special case. For $n$ high enough,\nan arbitrarily small amount of quantum-augmented photons per pulse diminishes\nthe change-detection latency by the inverse of the pre-detection channel loss.\nThis superadditivity-like phenomenon in the entanglement-augmented relative\nentropy rate, which quantifies the latency of change-point detection, may find\nother uses. We discuss the quantum limit of quickest change detection and a\nreceiver that achieves it, tradeoffs between continuous and discrete-variable\nquantum augmentation, and the broad problem of joint classical-and-quantum\ncommunications and channel-change-detection that our study opens up.",
        "(Abridged) We reassess the alpha-element abundance ratios (Ne\/O, S\/O, Ar\/O)\nwith respect to metallicity in ~1000 spectra of Galactic and extragalactic HII\nregions and star-forming galaxies (SFGs) of the local Universe. Using the DEep\nSpectra of Ionised REgions Database (DESIRED) Extended project (DESIRED-E),\nwhich includes spectra with direct electron temperature determinations, we\nhomogeneously derive physical conditions and chemical abundances for all\nobjects. Various ionisation correction factor (ICF) schemes are analyzed for\nNe, S, and Ar to identify the most reliable abundance estimates. Our findings\nindicate that the ICF scheme by Izotov et al. (2006) better reproduces the\nNe\/O, S\/O, and Ar\/O trends. Ne\/O ratios in HII regions display large dispersion\nand no clear dependence on O\/H, suggesting that current ICF(Ne) schemes fail\nfor these objects. However, SFGs show consistent linear relations with slightly\npositive slopes for log(Ne\/O) vs. 12+log(O\/H) or 12+log(Ne\/H), likely\ninfluenced by metallicity-dependent O dust depletion and ICF effects. The\nlog(S\/O) vs. 12+log(O\/H) distribution is largely constant, especially for HII\nregions or combined samples (SFGs + HII regions). Conversely, log(S\/O) vs.\n12+log(S\/H) shows a tight linear fit with a positive slope, flattening at\n12+log(S\/H) < 6.0, suggesting S contributions from SNe Ia. For log(Ar\/O) vs.\n12+log(O\/H), similar trends emerge for HII regions and SFGs, independent of\nionisation degree or ICF(Ar). A slight log(Ar\/O) decrease with increasing\n12+log(O\/H) contrasts with log(Ar\/O) vs. 12+log(Ar\/H), which shows a small\npositive slope, indicating a possible minor Ar contribution from SNe Ia.",
        "Fermionic time-reversal-invariant insulators in two dimensions -- class AII\nin the Kitaev table -- come in two different topological phases. These are\ncharacterized by a $\\mathbb{Z}_2$-index: the Fu-Kane-Mele index. We prove that\nif two such insulators with different indices occupy regions containing\narbitrarily large balls, then the spectrum of the resulting operator fills the\nbulk spectral gap. Our argument follows a proof by contradiction developed in\nan earlier work by two of the authors for quantum Hall systems. It boils down\nto showing that the $\\mathbb{Z}_2$-index can be computed only from bulk\ninformation in sufficiently large balls. This is achieved via a result of\nindependent interest: a local trace formula for the $\\mathbb{Z}_2$-index.",
        "We report a single-frequency, narrow-linewidth semiconductor pulsed laser\nbased on pump current modulation and optical injection locking technique. A\nmonolithic non-planar ring oscillator laser is employed as the seed source to\nguarantee the single-frequency narrow-linewidth performance. Simultaneously,\npulse operation is achieved by directly modulating the pump current of the\nsemiconductor laser. The single-frequency pulsed laser (SFPL) has achieved a\npulse repetition rate of 50 kHz-1 MHz, a pulse duration ranging from 120 ns to\na quasi-continuous state, and a peak power of 160 mW. Moreover, the SFPL has\nreached a pulsed laser linewidth as narrow as 905 Hz, optical spectrum\nsignal-to-noise ratio of better than 65 dB at a center wavelength of 1064.45\nnm. Such extremely narrow-linewidth, repetition-rate and pulse-width tunable\nSFPL has great potential for applications in coherent LIDAR, metrology, remote\nsensing, and nonlinear frequency conversion.",
        "The integration of biological principles into artificial olfactory systems\nhas led to significant advancements in odor detection and classification.\nInspired by the intricate mechanisms of natural olfaction, researchers are\ndeveloping sophisticated systems that mimic the functionality of biological\nolfactory pathways. These systems utilize high-density chemoresistive sensor\narrays (HCSA) combined with advanced computational techniques, such as\nFPGA-accelerated glomerular convergence circuits (FGCC) and hierarchical graph\nneural networks (HGNN). This bioinspired approach enables real-time adaptive\nresponses to volatile organic compounds (VOCs), enhancing the accuracy and\nefficiency of odor identification. At the core of these innovations is the\nmultiparametric sigmoidal sensor activation (MPSA), which quantifies VOCs by\nleveraging the diverse responses of sensor arrays. The implementation of\nlateral inhibition via programmable synaptic crossbars (LIPSC) further refines\nodor processing by mimicking neural interactions found in biological systems.\nAdditionally, temporal self-organizing maps (TSOM) facilitate dynamic\nclustering of odor patterns, allowing for a nuanced understanding of complex\nodor environments. A novel aspect of this research lies in the Grassmannian\nmanifold embedding (GME) of odor profiles, which provides a mathematical\nframework for representing and analyzing the multidimensional nature of odors.\nCoupled with Hamiltonian Monte Carlo-optimized feedback (HMC-FB), this system\neffectively compensates for drift in sensor readings, ensuring consistent\nperformance over time. By bridging the gap between biological inspiration and\ntechnological innovation, these artificial olfactory systems are poised to\nrevolutionize applications ranging from environmental monitoring to food safety\nand healthcare diagnostics.",
        "In recent papers on spacetimes with a signature-changing metric, the concept\nof a Lorentzian-Euclidean black hole and new elements for Lorentzian-Riemannian\nsignature change have been introduced. A Lorentzian-Euclidean black hole is a\nsignature-changing modification of the Schwarzschild spacetime satisfying the\nvacuum Einstein equations in a weak sense. Here the event horizon serves as a\nboundary beyond which time becomes imaginary. We demonstrate that the proper\ntime needed to reach the horizon remains finite, consistently with the\nclassical Schwarzschild solution. About Lorentzian to Riemannian metric\ntransitions, we stress that the hypersurface where the metric signature changes\nis naturally a spacelike hypersurface which might be identified with the future\nor past causal boundary of the Lorentzian sector. Moreover, a number of\ngeometric interpretations appear, as the degeneracy of the metric corresponds\nto the collapse of the causal cones into a line, the degeneracy of the dual\nmetric corresponds to collapsing into a hyperplane, and additional geometric\nstructures on the transition hypersurface (Galilean and dual Galilean) might be\nexplored.",
        "The rapidly increasing availability of large amounts of granular financial\ndata, paired with the advances of big data related technologies induces the\nneed of suitable analytics that can represent and extract meaningful\ninformation from such data. In this paper we propose a multi-layer network\napproach to distill the Euro Area (EA) banking system in different distinct\nlayers. Each layer of the network represents a specific type of financial\nrelationship between banks, based on various sources of EA granular data\ncollections. The resulting multi-layer network allows one to describe, analyze\nand compare the topology and structure of EA banks from different perspectives,\neventually yielding a more complete picture of the financial market. This\ngranular information representation has the potential to enable researchers and\npractitioners to better apprehend financial system dynamics as well as to\nsupport financial policies to manage and monitor financial risk from a more\nholistic point of view.",
        "Bayesian optimization (BO) is one of the most powerful strategies to solve\ncomputationally expensive-to-evaluate blackbox optimization problems. However,\nBO methods are conventionally used for optimization problems of small dimension\nbecause of the curse of dimensionality. In this paper, a high-dimensionnal\noptimization method incorporating linear embedding subspaces of small dimension\nis proposed to efficiently perform the optimization. An adaptive learning\nstrategy for these linear embeddings is carried out in conjunction with the\noptimization. The resulting BO method, named efficient global optimization\ncoupled with random and supervised embedding (EGORSE), combines in an adaptive\nway both random and supervised linear embeddings. EGORSE has been compared to\nstate-of-the-art algorithms and tested on academic examples with a number of\ndesign variables ranging from 10 to 600. The obtained results show the high\npotential of EGORSE to solve high-dimensional blackbox optimization problems,\nin terms of both CPU time and the limited number of calls to the expensive\nblackbox simulation.",
        "IceCube measures a diffuse neutrino flux comparable to the Waxman-Bahcall\nbound, which suggests the possibility that the ultra-high energy cosmic rays\n(UHECRs) have a common origin with diffuse high energy neutrinos. We propose\nhigh energy gamma-ray and\/or neutrino observations toward the arrival\ndirections of UHECRs to search for the sources and test this possibility. We\ncalculate the detection probability of gamma-ray\/neutrino sources, and find\nthat the average probability per UHECR of >10 EeV is $\\sim$10% if the\nsensitivity of the gamma-ray or neutrino telescope is $\\sim$10$^{-12}$ erg\ncm$^{-2}$s$^{-1}$ and the source number density is $\\sim$10$^{-5}$ Mpc$^{-3}$.\nFuture gamma-ray and neutrino observations toward UHECRs, e.g., by LHAASO-WCDA,\nCTA, IceCube\/Gen2, are encouraged to constrain the density of UHECR sources or\neven identify the sources of UHECRs.",
        "The nodes in a network can be grouped into 'roles' based on similar\nconnection patterns. This is usually achieved by defining a pairwise node\nsimilarity matrix and then clustering rows and columns of this matrix. This\npaper presents a new similarity matrix for solving role extraction problems in\ndirected networks, which is defined as the solution of a matrix equation and\ncomputes node similarities based on random walks that can proceed along the\nlink direction and in the opposite direction. The resulting node similarity\nmeasure performs remarkably in role extraction tasks on directed networks with\nheterogeneous node degree distributions.",
        "We extend the single-perturbation approach (developed in our earlier\npublications for the case of a single map) to the analysis of the shadowing\nproperty for semigroups of endomorphisms. Our approach allows to give a\nconstructive representation for a true trajectory which shadows a given\npseudo-trajectory. One of the main motivations is the question of inheritance:\ndoes the presence of shadowing for all generators of a semigroup imply\nshadowing for the semigroup and vice versa. Somewhat surprisingly, the answer\nto these questions is generally negative. Moreover, the situation with\nshadowing turns out to be quite different in a semigroup and in a\nnon-autonomous system, despite the fact that the latter can be represented as a\nsingle branch of the former.",
        "We consider the problem of inverting the artifacts associated with scanning a\npage from an open book, i.e. \"xeroxing.\" The process typically leads to a\nnon-uniform combination of distortion, blurring and darkening owing to the fact\nthat the page is bound to a stiff spine that causes the sheet of paper to be\nbent inhomogeneously. Complementing purely data-driven approaches, we use\nknowledge about the geometry and elasticity of the curved sheet to pose and\nsolve a minimal physically consistent inverse problem to reconstruct the image.\nOur results rely on 3 dimensionless parameters, all of which can be measured\nfor a scanner, and show that we can improve on the data-driven approaches. More\nbroadly, our results might serve as a \"textbook\" example and a tutorial of how\nknowledge of generative mechanisms can speed up the solution of inverse\nproblems.",
        "ESA\/NASA's Solar Orbiter (SO) allows us to study the solar corona at closer\ndistances and from different perspectives, which helps us to gain significant\ninsights into the origins of the solar wind. In this work, we present the\nanalysis of solar wind outflows from two locations: a narrow open-field\ncorridor and a small, mid-latitude coronal hole. These outflows were observed\noff-limb by the Metis coronagraph onboard SO and on-disk by the Extreme\nUltraviolet Imaging Spectrometer (EIS) onboard Hinode. Magnetic field\nextrapolations suggest that the upflow regions seen in EIS were the sources of\nthe outflowing solar wind observed with Metis. We find that the plasma\nassociated with the narrow open-field corridor has higher electron densities\nand lower outflow velocities compared to the coronal hole plasma in the middle\ncorona, even though the plasma properties of the two source regions in the low\ncorona are found to be relatively similar. The speed of solar wind from the\nopen-field corridor also shows no correlation with the magnetic field expansion\nfactor, unlike the coronal hole. These pronounced differences at higher\naltitudes may arise from the dynamic nature of the low-middle corona, in which\nreconnection can readily occur and may play an important role in driving solar\nwind variability.",
        "The question of whether Reed-Muller (RM) codes achieve capacity on binary\nmemoryless symmetric (BMS) channels has drawn attention since it was resolved\npositively for the binary erasure channel by Kudekar et al. in 2016. In 2021,\nReeves and Pfister extended this to prove the bit-error probability vanishes on\nBMS channels when the code rate is less than capacity. In 2023, Abbe and Sandon\nimproved this to show the block-error probability also goes to zero. These\nresults analyze decoding functions using symmetry and the nested structure of\nRM codes. In this work, we focus on binary-input symmetric classical-quantum\n(BSCQ) channels and the Holevo capacity. For a BSCQ, we consider observables\nthat estimate the channel input in the sense of minimizing the mean-squared\nerror (MSE). Using the orthogonal decomposition of these observables under a\nweighted inner product, we establish a recursive relation for the minimum MSE\nestimate of a single bit in the RM code. Our results show that any set of\n$2^{o(\\sqrt{\\log N})}$ bits can be decoded with a high probability when the\ncode rate is less than the Holevo capacity.",
        "This study analyses the dynamical consequences of heterogeneous temporal\ndelays within a quorum sensing-inspired (QS-inspired) system, specifically\naddressing the differential response kinetics of two subpopulations to\nsignalling molecules. A nonlinear delay differential equation (DDE) model,\npredicated upon an activator-inhibitor framework, is formulated to represent\nthe interspecies interactions. Key analytical techniques, including the\nderivation of the pseudo-characteristic polynomial and the determination of\nHopf bifurcation criteria, are employed to investigate the stability properties\nof steady-state solutions. The analysis reveals the critical role of multiple,\ndissimilar delays in modulating system dynamics and inducing bifurcations.\nNumerical simulations, conducted in conjunction with analytical results, reveal\nthe emergence of periodic self-sustained oscillations and intermittent chaotic\nbehaviour. These observations emphasise the intricate relationship between\ntemporal heterogeneity and the stability landscape of systems exhibiting\nQS-inspired dynamics. This interplay highlights the capacity for temporal\nvariations to induce complex dynamical transitions within such systems. These\nfindings assist to the comprehension of temporal dynamics within these and\nrelated systems, and may contribute to the development of strategies aimed at\nmodulating intercellular communication and engineering synthetic biological\nsystems with temporal control.",
        "This article establishes the usefulness of the Smoothness-Increasing\nAccuracy-Increasing (SIAC) filter for reducing the errors in the mean and\nvariance for a wave equation with uncertain coefficients solved via generalized\npolynomial chaos (gPC) whose coefficients are approximated using discontinuous\nGalerkin (DG-gPC). Theoretical error estimates that utilize information in the\nnegative-order norm are established. While the gPC approximation leads to order\nof accuracy of $m-1\/2$ for a sufficiently smooth solution (smoothness of $m$ in\nrandom space), the approximated coefficients solved via DG improves from order\n$k+1$ to $2k+1$ for a solution of smoothness $2k+2$ in physical space. Our\nnumerical examples verify the performance of the filter for improving the\nquality of the approximation and reducing the numerical error and significantly\neliminating the noise from the spatial approximation of the mean and variance.\nFurther, we illustrate how the errors are effected by both the choice of\nsmoothness of the kernel and number of function translates in the kernel.\nHence, this article opens the applicability of SIAC filters to other hyperbolic\nproblems with uncertainty, and other stochastic equations."
      ]
    }
  },
  {
    "id":2411.18141,
    "research_type":"applied",
    "start_id":"b18",
    "start_title":"Quantum machine learning in chemistry and materials",
    "start_abstract":"Within the past few years, we have witnessed the rising of quantum machine learning (QML) models which infer electronic properties of molecules and materials, rather than solving approximations to the electronic Schr\u00f6dinger equation. The increasing availability of large quantum mechanics reference datasets has enabled these developments. We review the basic theories and key ingredients of popular QML models such as choice of regressor, data of varying trustworthiness, the role of the representation, and the effect of training set selection. Throughout we emphasize the indispensable role of learning curves when it comes to the comparative assessment of different QML models.",
    "start_categories":[
      "cs.ET"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b26"
      ],
      "title":[
        "Durban's water wars, sewage spills, fish kills and blue flag beaches. Durban's Climate Gamble"
      ],
      "abstract":[
        "Water is one of the primary barometers of climate change: A rise in sea-levels, flooding, and extreme storms combined with general water stress and more severe and frequent droughts will escalate crises in municipal infrastructure, requiring continual upgrades for water purification, stormwater drainage, and sewage treatment, all of which will dramatically raise the price of water at the retail level. In South Africa, the dry western side will be most adversely affected by droughts (threatening the production of rooibos tea and Cape wines). According to the Academy of Science in South Africa (ASSAf), Durban is also at great risk and will experience higher temperatures and heat stress, volatile rainfall, up to 160 million cubic metres less water each year by 2100, a sea-level rise of up to a metre by 2100 across Durban\u2019s 100 km of developed coastline, lower biodiversity, higher disease levels (especially malaria and cholera), declining agricultural output (a one degree Celsius rise leaves the surrounding region unreliable for the staple maize production), and other economic stresses (ASSAf 2011: 27). Tourism, one of Durban\u2019s main economic engines, will be irreparably harmed. Swimmers and surfers think of Durban\u2019s beachfront as one of the world\u2019s finest in any urban context. After apartheid-era rules that prohibited black people from using the best beaches were lifted at the end of the 1980s, the area stretching from the Blue Lagoon\u2019s Umgeni River to South Beach\u2019s uShaka Marine World\u2013including the immensely popular North Beach area near the main restaurant strip\u2013represented one of South Africa\u2019s most impressive, open and democratic public spaces."
      ],
      "categories":[
        "cond-mat.mtrl-sci"
      ]
    },
    "list":{
      "title":[
        "Universality for catalytic equations and fully parked trees",
        "A Production Routing Problem with Mobile Inventories",
        "Wikipedia Contributions in the Wake of ChatGPT",
        "Gaussian credible intervals in Bayesian nonparametric estimation of the\n  unseen",
        "Identifying rich clubs in spatiotemporal interaction networks",
        "First-principle based Floquet engineering of solids in the velocity\n  gauge",
        "A Peanut-hull-PLA based 3D printing filament with antimicrobial effect",
        "Non-reciprocity and multibody interactions in acoustically levitated\n  particle systems: A three body problem",
        "On a reaction-diffusion virus model with general boundary conditions in\n  heterogeneous environments",
        "Schmid-Higgs Mode in the Presence of Pair-Breaking Interactions",
        "Thom polynomials for singularities of maps",
        "$\\texttt{PrecisionLauricella}$: package for numerical computation of\n  Lauricella functions depending on a parameter",
        "Unidentified Aerial Phenomena. Characterization of Dark UAPs",
        "Accumulation of Charge on an Extremal Black Hole's Event Horizon",
        "Expression of special stretched $9j$ coefficients in terms of $_5F_4$\n  hypergeometric series",
        "Generalization Bounds for Equivariant Networks on Markov Data",
        "Meaningful, Useful and Legitimate Information in the Use of Index\n  Numbers for Decision Making",
        "Thermal Conduction and Thermal-Driven Winds in Magnetized Viscous\n  Accretion Disk Dynamics",
        "Optimizing Bidding Curves for Renewable Energy in Two-Settlement\n  Electricity Markets",
        "Three-loop chiral effective potential in the Wess-Zumino model",
        "A splitting theorem for manifolds with spectral nonnegative Ricci\n  curvature and mean-convex boundary",
        "Scalable solution chemical synthesis and comprehensive analysis of\n  Bi2Te3 and Sb2Te3",
        "Observation of the dimer-singlet phase in the one-dimensional S = 1\/2\n  Heisenberg antiferromagnet Cu(Ampy)ClBr (Ampy= C6H8N2 =\n  2-(Aminomethyl)pyridine)",
        "ZnO@C\/PVDF Electrospun Membrane as Piezoelectric Nanogenerator for\n  Wearable Applications",
        "Restoring thermalization in long-range quantum magnets with staggered\n  magnetic fields",
        "Functional equation arising in behavioral sciences: solvability and\n  collocation scheme in H\\\"older spaces",
        "LZMidi: Compression-Based Symbolic Music Generation",
        "Detection and control of electronic orbital magnetism by spin waves in\n  honeycomb ferromagnets",
        "Oscillation-eliminating central DG schemes for hyperbolic conservation\n  laws"
      ],
      "abstract":[
        "We show that critical parking trees conditioned to be fully parked converge\nin the scaling limits towards the Brownian growth-fragmentation tree, a\nself-similar Markov tree different from Aldous' Brownian tree recently\nintroduced and studied by Bertoin, Curien and Riera. As a by-product of our\nstudy, we prove that positive non-linear polynomial equations involving a\ncatalytic variable display a universal polynomial exponent $5\/2$ at their\nsingularity, confirming a conjecture by Chapuy, Schaeffer and Drmota & Hainzl.\nCompared to previous analytical works on the subject, our approach is\nprobabilistic and exploits an underlying random walk hidden in the random tree\nmodel.",
        "Hydrogen is an energy vector, and one possible way to reduce CO 2 emissions.\nThis paper focuses on a hydrogen transport problem where mobile storage units\nare moved by trucks between sources to be refilled and destinations to meet\ndemands, involving swap operations upon arrival. This contrasts with existing\nliterature where inventories remain stationary. The objective is to optimize\ndaily routing and refilling schedules of the mobile storages. We model the\nproblem as a flow problem on a time-expanded graph, where each node of the\ngraph is indexed by a time-interval and a location and then, we give an\nequivalent Mixed Integer Linear Programming (MILP) formulation of the problem.\nFor small to medium-sized instances, this formulation can be efficiently solved\nusing standard MILP solvers. However, for larger instances, the computational\ncomplexity increases significantly due to the highly combinatorial nature of\nthe refilling process at the sources. To address this challenge, we propose a\ntwo-step heuristic that enhances.",
        "How has Wikipedia activity changed for articles with content similar to\nChatGPT following its introduction? We estimate the impact using\ndifferences-in-differences models, with dissimilar Wikipedia articles as a\nbaseline for comparison, to examine how changes in voluntary knowledge\ncontributions and information-seeking behavior differ by article content. Our\nanalysis reveals that newly created, popular articles whose content overlaps\nwith ChatGPT 3.5 saw a greater decline in editing and viewership after the\nNovember 2022 launch of ChatGPT than dissimilar articles did. These findings\nindicate heterogeneous substitution effects, where users selectively engage\nless with existing platforms when AI provides comparable content. This points\nto potential uneven impacts on the future of human-driven online knowledge\ncontributions.",
        "The unseen-species problem assumes $n\\geq1$ samples from a population of\nindividuals belonging to different species, possibly infinite, and calls for\nestimating the number $K_{n,m}$ of hitherto unseen species that would be\nobserved if $m\\geq1$ new samples were collected from the same population. This\nis a long-standing problem in statistics, which has gained renewed relevance in\nbiological and physical sciences, particularly in settings with large values of\n$n$ and $m$. In this paper, we adopt a Bayesian nonparametric approach to the\nunseen-species problem under the Pitman-Yor prior, and propose a novel\nmethodology to derive large $m$ asymptotic credible intervals for $K_{n,m}$,\nfor any $n\\geq1$. By leveraging a Gaussian central limit theorem for the\nposterior distribution of $K_{n,m}$, our method improves upon competitors in\ntwo key aspects: firstly, it enables the full parameterization of the\nPitman-Yor prior, including the Dirichlet prior; secondly, it avoids the need\nof Monte Carlo sampling, enhancing computational efficiency. We validate the\nproposed method on synthetic and real data, demonstrating that it improves the\nempirical performance of competitors by significantly narrowing the gap between\nasymptotic and exact credible intervals for any $m\\geq1$.",
        "Spatial networks are widely used in various fields to represent and analyze\ninteractions or relationships between locations or spatially distributed\nentities.There is a network science concept known as the 'rich club'\nphenomenon, which describes the tendency of 'rich' nodes to form densely\ninterconnected sub-networks. Although there are established methods to quantify\ntopological, weighted, and temporal rich clubs individually, there is limited\nresearch on measuring the rich club effect in spatially-weighted temporal\nnetworks, which could be particularly useful for studying dynamic spatial\ninteraction networks. To address this gap, we introduce the spatially-weighted\ntemporal rich club (WTRC), a metric that quantifies the strength and\nconsistency of connections between rich nodes in a spatiotemporal network.\nAdditionally, we present a unified rich club framework that distinguishes the\nWTRC effect from other rich club effects, providing a way to measure\ntopological, weighted, and temporal rich club effects together. Through two\ncase studies of human mobility networks at different spatial scales, we\ndemonstrate how the WTRC is able to identify significant weighted temporal rich\nclub effects, whereas the unweighted equivalent in the same network either\nfails to detect a rich club effect or inaccurately estimates its significance.\nIn each case study, we explore the spatial layout and temporal variations\nrevealed by the WTRC analysis, showcasing its particular value in studying\nspatiotemporal interaction networks. This research offers new insights into the\nstudy of spatiotemporal networks, with critical implications for applications\nsuch as transportation, redistricting, and epidemiology.",
        "We introduce a practical and accurate strategy to capture light-matter\ninteractions using the Floquet formalism in the velocity gauge in combination\nwith realistic first-principle models of solids. The velocity gauge, defined by\nthe linear coupling to the vector potential, is a standard method to capture\nthe light-matter interaction in solids. However, its use with first-principle\nmodels has been limited by the challenging fact that it requires a large number\nof bands for convergence and its incompatibility with non-local pseudopotential\nplane wave methods. To improve its convergence properties, we explicitly take\ninto account the truncation of Hilbert space in the construction of the Floquet\nHamiltonian in the velocity gauge. To avoid the incompatibility with the\npseudopotentials, we base our computations on generalized tight-binding\nHamiltonians derived from first-principles through maximally-localized Wannier\nfunctions. We exemplify the approach by computing the optical absorption\nspectra of laser-dressed trans-polyacetylene chain using realistic electronic\nstructure. We show that, by proceeding in this way, Floquet consideration\ninvolving the truncated Hilbert spaces reproduces the full basis calculations\nwith only a few bands and with significantly reduced computation time. The\nstrategy has been implemented in FloqticS, a general code for the Floquet\nengineering of the optical properties of materials. Overall, this work\nintroduces a useful theoretical tool to realize Floquet engineering of\nrealistic solids in the velocity gauge.",
        "Peanut hulls, also known as Arachis hypogaea L. particles (AHL), are an\nabundant biomass source with a long shelf life. In this study, we incorporate\npeanut hull powder into PLA polymer, imparting recyclability, biodegradability,\nand biocompatibility, along with the antimicrobial properties of AHL particles.\nIn particular, we treat AHL particles as a reinforcement for PLA polymer to\nproduce 3D printing filament compatible with the fused filament fabrication\n(FFF) 3D printing method. We provide a step-by-step method for preparing AHL\nparticles, incorporating them into PLA, and ultimately forming high-quality\nfilaments. We assess the quality of the filaments in terms of extruded\ndimensions, mechanical strength, and elastic modulus, along with physical\nproperties such as porosity and melt flow index. We evaluate the printability\nand wettability of the filaments as well. Notably, and unlike other\nbiomass-based reinforcements in PLA, AHL preserves the filament's strength and\nenhances its elastic modulus. 3D-printed components fabricated using our\nPLA-AHL filaments successfully retain their antimicrobial properties and\nexhibit increased overall hardness. However, this comes at the expense of\nforming more microvoids and a rougher surface, making the material more prone\nto fracture and leading to a slight reduction in fracture toughness with\nincreasing AHL mass fraction.",
        "In active fluids and active solids the constituents individually generate\nmovement by each extracting energy from their environment or from their own\nsource. Non-reciprocal interactions among these active constituents then enable\nnovel collective behavior that often can be strikingly counterintuitive.\nHowever, non-reciprocity in these cases typically requires that the interacting\nbodies have different physical properties or it needs to be programmed\nexplicitly into all pairwise interactions. Here we show that collective\nactivity in a driven system can emerge spontaneously through multibody\nnonreciprocal forces, even if all bodies are individually non-active and have\nidentical properties. We demonstrate this with as few as three identical\nspheres, acoustically levitated in air, which exhibit collective activity as\nthey interact through non-pairwise forces: similar to the classic gravitational\nthree-body problem, the interaction between two spheres depends sensitively on\nthe relative position of the third sphere. Non-reciprocity arises naturally\nfrom both near-field sound scattering and microstreaming forces among the\nspheres. The underdamped dynamics in air furthermore make it possible to go\nbeyond collective center-of-mass propulsion or rotation and observe internal,\nengine-like reconfigurations that follow limit cycles. These findings open up\nnew possibilities for self-assembly, where now multibody interactions not only\ndetermine the resulting structure but also drive the spontaneously emerging\ndynamics.",
        "To describe the propagation of West Nile virus and\/or Zika virus, in this\npaper, we propose and study a time-periodic reaction-diffusion model with\ngeneral boundary conditions in heterogeneous environments and with four\nunknowns: susceptible host, infectious host, susceptible vector and infectious\nvector. We can prove that such problem has a positive time periodic solution if\nand only if host and vector persist and the basic reproduction ratio is greater\nthan one, and moreover the positive time periodic solution is unique and\nglobally asymptotically stable when it exists.",
        "Collective modes in superconductors provided the first realization of the\nHiggs mechanism. The transverse Goldstone mode acquires a gap (i.e. a mass)\nwhen it hybridizes with the electromagnetic gauge field. The longitudinal\nSchmid-Higgs mode, on the other hand, is always massive. In conventional BCS\ntheory, its gap is exactly $2\\Delta$, coinciding with the excitation threshold\nfor quasiparticles. Being situated right at the edge of the continuum spectrum\nit gives rise to peculiar dynamics for the Schmid-Higgs mode. For instance,\nwhen suddenly excited at $t=0$, it exhibits algebraically decaying oscillations\nof the form $\\sim \\sin(2\\Delta t)\/{t}^{1\/2}$. In this study, we explore the\nbehavior of Schmid-Higgs oscillations in the presence of pair-breaking\nmechanisms, such as magnetic impurities or in-plane magnetic fields. These\nprocesses suppress the quasiparticle excitation threshold down to\n$2\\varepsilon_g < 2\\Delta$, potentially placing the longitudinal mode within\nthe continuum spectrum. Despite this, we show that the algebraically decaying\noscillations persist, taking the form $\\sim \\sin(2\\varepsilon_g t)\/t^2$. The\nSchmid-Higgs mode becomes truly overdamped and exponentially decaying only in\nthe gapless superconductors with $\\varepsilon_g=0$.",
        "This is a gentle introduction to a general theory of universal polynomials\nassociated to classification of map-germs, called Thom polynomials. The theory\nwas originated by Ren\\'e Thom in the 1950s and has since been evolved in\nvarious aspects by many authors. In a nutshell, this is about intersection\ntheory on certain moduli spaces, say `classifying spaces of\nmono\/multi-singularities of maps', which provides consistent and deep insights\ninto both classical and modern enumerative geometry with many potential\napplications.",
        "We introduce the $\\texttt{PrecisionLauricella}$ package, a computational tool\ndeveloped in Wolfram Mathematica for high-precision numerical evaluations of\nLauricella functions with indices linearly dependent on a parameter,\n$\\varepsilon$. The package leverages a method based on analytical continuation\nvia Frobenius generalized power series, providing an efficient and accurate\nalternative to conventional approaches relying on multi-dimensional series\nexpansions or Mellin--Barnes representations. This one-dimensional approach is\nparticularly advantageous for high-precision calculations and facilitates\nfurther optimization through $\\varepsilon$-dependent reconstruction from\nevaluations at specific numerical values, enabling efficient parallelization.\nThe underlying mathematical framework for this method has been detailed in our\nprevious work, while the current paper focuses on the design, implementation,\nand practical applications of the $\\texttt{PrecisionLauricella}$ package.",
        "We use high-tech observations of Unidentified Aerial Phenomena (UAP) class\nobjects to evaluate their characteristics. We present data in three cases. (1)\nMulti-side daytime observations of UAPs over Kiev. (2) Night observations of a\ngroup of objects in the vicinity of the Moon. (3) UAP observations in the\ncombat zone in Ukraine. Dark UAPs in the visible wavelength range are observed\nonly during the day. At night they can only be seen in the infrared wavelength\nrange. We note large sizes of UAPs, from three to six kilometers.They exhibit\nlarge velocities, from 2.5 Mach and much larger. They have low albedo, from\nthree percent and below, that is, they actually exhibit features of a\ncompletely black body.",
        "We numerically analyze the behavior of a charged scalar field on a fixed\nextremal Reissner-Nordstr\\\"om background. We find an extension of the Aretakis\ninstability characterized by an accumulation of charge on the extremal event\nhorizon. In particular, when the charge coupling to the scalar field is\nsufficiently large, the charge density on the horizon asymptotes to a nonzero\nconstant at late times. By constructing monochromatic initial data at the onset\nof charged superradiance, we give evidence supporting the claim that this\ninstability is connected to the presence of a nearly zero-damped mode.\nThroughout this work, we employ a numerical integration scheme in compactified\ndouble-null coordinates, which allows us to capture the asymptotic behavior of\nthe matter at the boundaries of the spacetime.",
        "The Clebsch-Gordan coefficients or Wigner $3j$ symbols are known to be\nproportional to a $_3F_2(1)$ hypergeometric series, and Racah $6j$ coefficients\nto a $_4F_3(1)$. In general, however, non-trivial $9j$ symbols can not be\nexpressed as a $_5F_4$. In this letter, we show, using the Dougall-Ramanujan\nidentity, that special stretched $9j$ symbols can be reformulated as $_5F_4(1)$\nhypergeometric series.",
        "Equivariant neural networks play a pivotal role in analyzing datasets with\nsymmetry properties, particularly in complex data structures. However,\nintegrating equivariance with Markov properties presents notable challenges due\nto the inherent dependencies within such data. Previous research has primarily\nconcentrated on establishing generalization bounds under the assumption of\nindependently and identically distributed data, frequently neglecting the\ninfluence of Markov dependencies. In this study, we investigate the impact of\nMarkov properties on generalization performance alongside the role of\nequivariance within this context. We begin by applying a new McDiarmid's\ninequality to derive a generalization bound for neural networks trained on\nMarkov datasets, using Rademacher complexity as a central measure of model\ncapacity. Subsequently, we utilize group theory to compute the covering number\nunder equivariant constraints, enabling us to obtain an upper bound on the\nRademacher complexity based on this covering number. This bound provides\npractical insights into selecting low-dimensional irreducible representations,\nenhancing generalization performance for fixed-width equivariant neural\nnetworks.",
        "Often information relevant to a decision is summarized in an index number.\nThis paper explores conditions under which conclusions using index numbers are\nrelevant to the decision that needs to be made. Specifically it explores the\nidea that a statement using scales of measurement is meaningful in the sense\nthat its truth or falsity does not depend on an arbitrary choice of parameters;\nthe concept that a conclusion using index numbers is useful for the specific\ndecision that needs to be made; and the notion that such a conclusion is\nlegitimate in the sense that it is collected and used in a way that satisfies\ncultural, historical, organizational and legal constraints. While\nmeaningfulness is a precisely defined concept, usefulness and legitimacy are\nnot, and the paper explores properties of these concepts that lay the\ngroundwork for making them more precise. Many examples involving two well-known\nand widely-used index numbers, body mass indices and air pollution indices, are\nused to explore the properties of and interrelationships among meaningfulness,\nusefulness, and legitimacy.",
        "This paper investigates the effects of saturated thermal conduction (TC) and\nthermal-driven winds (TDWs) on magnetized advection-dominated accretion onto a\nrotating black hole (BH). We incorporate dissipative processes in the\nmagnetized accretion flow and expect the accretion disk to be threaded by\npredominantly toroidal and turbulent magnetic fields. We solve the\nmagnetohydrodynamics equations and construct a self-consistent steady model of\nthe magnetized accretion flow surrounding a rotating BH, which includes TC and\nTDWs. We seek global accretion solutions spanning from the BH horizon to a\nlarge distance and analyze the solution's characteristics as a function of\ndissipation parameters. Accretion solutions with multiple critical points may\nexhibit shock waves if they meet the standing shock criteria. We found steady,\nglobal transonic, and shocked accretion solutions around the rotating BH. In\nparticular, the wind parameter ($m$) and the saturated conduction parameter\n($\\Phi_{\\rm s}$) significantly influence the dynamical behavior of shocks. The\nshock location moves away from the BH horizon as $\\Phi_{\\rm s}$ and $m$\nincrease, assuming fixed conditions at the disk's outer edge. Our formalism\nexplains the declining phase of BH outbursts, characterized by a monotonic\ndecrease in QPO frequency as the burst decays. Based on our findings, we\nconclude that the combined effect of $\\Phi_{\\rm s}$ and $m$ parameters\nsubstantially alters the steady shock specific energy vs angular momentum\nparameter space and also modifies the corresponding post-shock luminosity vs\nQPO frequency parameter space. We propose, based on our theoretical model, that\nthe $\\Phi_{\\rm s}$ and $m$ parameters may significantly influence the evolution\nof the BH outbursts.",
        "Coordination of day-ahead and real-time electricity markets is imperative for\ncost-effective electricity supply and also to provide efficient incentives for\nthe energy transition. Although stochastic market designs feature the\nleast-cost coordination, they are incompatible with current deterministic\nmarkets. This paper proposes a new approach for compatible coordination in\ntwo-settlement markets based on benchmark bidding curves for variable renewable\nenergy. These curves are optimized based on a bilevel optimization problem,\nanticipating per-scenario responses of deterministic market-clearing problems\nand ultimately minimizing the expected cost across day-ahead and real-time\nmarkets. Although the general bilevel model is challenging to solve, we\ntheoretically prove that a single-segment bidding curve with a zero bidding\nprice is sufficient to achieve system optimality if the marginal cost of\nvariable renewable energy is zero, thus addressing the computational challenge.\nIn practice, variable renewable energy producers can be allowed to bid\nmulti-segment curves with non-zero prices. We test the bilevel framework for\nboth single- and multiple-segment bidding curves under the assumption of fixed\nbidding prices. We leverage duality theory and McCormick envelopes to derive\nthe linear programming approximation of the bilevel problem, which scales to\npractical systems such as a 1576-bus NYISO system. We benchmark the proposed\ncoordination and find absolute dominance over the baseline solution, which\nassumes that renewables agnostically bid their expected forecasts. We also\ndemonstrate that our proposed scheme provides a good approximation of the\nleast-cost, yet unattainable in practice, stochastic market outcome.",
        "We calculate the three-loop contribution to the chiral effective potential in\nthe massless Wess-Zumino model. It is shown that while the non-renormalisation\ntheorem forbids divergent contributions to the chiral potential, in the\nmassless case the finite corrections survive. There are only three three-loop\nsupergraphs that give rise to a superfield effective action in the pure chiral\nsector. Two of them are UV finite while the third requires one-loop counterterm\ncorresponding to the chiral field renormalisation.",
        "We prove a splitting theorem for a smooth noncompact manifold with (possibly\nnoncompact) boundary. We show that if a noncompact manifold of dimension $n\\geq\n2$ has $\\lambda_1(-\\alpha\\Delta+\\operatorname{Ric})\\geq 0$ for some\n$\\alpha<\\frac{4}{n-1}$ and mean-convex boundary, then it is either isometric to\n$\\Sigma\\times \\mathbb{R}_{\\geq 0}$ for a closed manifold $\\Sigma$ with\nnonnegative Ricci curvature or it has no interior ends.",
        "Thermoelectric (TE) materials can directly convert heat into electrical\nenergy. However, they sustain costly production procedures and batch-to-batch\nperformance variations. Therefore, developing scalable synthetic techniques for\nlarge-scale and reproducible quality TE materials is critical for advancing TE\ntechnology. This study developed a facile, high throughput, solution-chemical\nsynthetic technique. Microwave-assisted thermolysis process, providing\nenergy-efficient volumetric heating, was used for the synthesis of bismuth and\nantimony telluride (Bi2Te3, Sb2Te3). As-made materials were characterized using\nvarious techniques, including XRPD, SEM, TEM, XAS, and XPS. Detailed\ninvestigation of the local atomic structure of the synthesized Bi2Te3 and\nSb2Te3 powder samples was conducted through synchrotron radiation XAS\nexperiments. The sintered TE materials exhibited low thermal conductivity,\nachieving the highest TE figure-of-merit values of 0.7 (573 K) and 0.9 (523 K)\nfor n-type Bi2Te3 and p-type Sb2Te3, respectively, shifted significantly to the\nhigh-temperature region when compared to earlier reports, highlighting their\npotential for power generation applications. The scalable, energyand\ntime-efficient synthetic method developed, along with the demonstration of its\npotential for TE materials, opens the door for a wider application of these\nmaterials with minimal environmental impact.",
        "Spin-1\/2 Heisenberg antiferromagnetic frustrated spin chain systems display\nexotic ground states with unconventional excitations and distinct quantum phase\ntransitions as the ratio of next-nearest-neighbor to nearest-neighbor coupling\nis tuned. We present a comprehensive investigation of the structural, magnetic,\nand thermodynamics properties of the spin-1\/2 compound, Cu(Ampy)ClBr (Ampy=\nC6H8N2 = 2-(Aminomethyl)pyridine) via x-ray diffraction, magnetization,\nspecific heat, 1H nuclear magnetic resonance (NMR), electron spin resonance\n(ESR), and muon spin relaxation (muSR) techniques. The crystal structure\nfeatures an anisotropic triangular chain lattice of magnetic Cu2+ ions. Our\nbulk and local probe experiments detect neither long-range magnetic ordering\nnor spin freezing down to 0.06 K despite the presence of moderate\nantiferromagnetic interaction between Cu2+ spins as reflected by a Curie-Weiss\ntemperature of about -9 K from the bulk susceptibility data. A broad maximum is\nobserved at about 9 K in magnetic susceptibility and specific heat data,\nindicating the onset of short-range spin correlations. At low temperatures, the\nzero-field magnetic specific heat and the 1H NMR spin-lattice relaxation rate\nfollow an exponential temperature dependence, indicating the presence of gapped\nmagnetic excitations. Furthermore, persistent spin dynamics down to 0.088 K\nobserved by zero-field muSR evidences lack of any static magnetism. We\nattribute these experimental results to the stabilization of a dimer-singlet\nphase in the presence of a next-near neighbor interaction and of a randomness\nin the exchange coupling driven by Cl\/Br mixing.",
        "The rapid growth of wearable technology demands sustainable, flexible, and\nlightweight energy sources for various applications ranging from health\nmonitoring to electronic textiles. Although wearable devices based on the\npiezoelectric effect are widespread, achieving simultaneous breathability,\nwaterproof, and enhanced piezoelectric performance remains challenging. Herein,\nthis study aims to develop a piezoelectric nanogenerator (PENG) using ZnO\nnanofillers in two morphologies (nanoparticles and nanorods), with a carbon\ncoating (ZnO@C) core-cell structure to enhance piezoelectric performance.\nElectrospinning technique was employed to fabricate a lightweight, breathable,\nand water-resistant ZnO@C\/PVDF membrane, enabling in situ electrical poling and\nmechanical stretching to enhance electroactive \\b{eta}-phase formation and thus\nimprove piezoelectric performance. A maximum power density of 384.83 {\\mu}W\/cm3\nwas obtained at RL = 104 k{\\Omega}, with a maximum Vout = 19.9 V for ZnO@C\nnanorod-incorporated PVDF samples. The results demonstrate that ZnO@C nanorods\nexhibit superior voltage output due to their larger surface-to-volume ratio,\nleading to enhanced interaction with PVDF chains compared to nanoparticles. The\nfabricated membrane showed promising results with a water vapor transmission\nrate (WVTR) of ~0.5 kg\/m2\/day, indicating excellent breathability, and a water\ncontact angle of ~116{\\deg}, demonstrating significant waterproofness. These\nfindings highlight the potential of the ZnO@C\/PVDF electrospun membrane as an\neffective piezoelectric nanogenerator and energy harvester for wearable\napplications.",
        "Quantum systems with strong long-range interactions are thought to resist\nthermalization because of their discrete energy spectra. We show that applying\na staggered magnetic field to a strong long-range Heisenberg antiferromagnet\nrestores thermalization for a large class of initial states by breaking\npermutational symmetry. Using self-consistent mean-field theory and exact\ndiagonalization, we reveal that the energy spectrum, while composed of discrete\nsubspaces, collectively forms a dense spectrum. The equilibration time is\nindependent of system size and depends only on the fluctuations in the initial\nstate. For initial states at low to intermediate energies, the long-time\naverage aligns with the microcanonical ensemble. However, for states in the\nmiddle of the spectrum the long-time average depends on the initial state due\nto quantum scar-like eigenstates localized at unstable points in classical\nphase space. Our results can be readily tested on a range of experimental\nplatforms, including Rydberg atoms or optical cavities.",
        "We consider a generalization of a functional equation that models the\nlearning process in various animal species. The equation can be considered\nnonlocal, as it is built with a convex combination of the unknown function\nevaluated at mixed arguments. This makes the equation contain two terms with\nvanishing delays. We prove the existence and uniqueness of the solution in the\nH\\\"older space which is a natural function space to consider. In the second\npart of the paper, we devise an efficient numerical collocation method used to\nfind an approximation to the main problem. We prove the convergence of the\nscheme and, in passing, several properties of the linear interpolation operator\nacting on the H\\\"older space. Numerical simulations verify that the order of\nconvergence of the method (measured in the supremum norm) is equal to the order\nof H\\\"older continuity.",
        "Recent advances in symbolic music generation primarily rely on deep learning\nmodels such as Transformers, GANs, and diffusion models. While these approaches\nachieve high-quality results, they require substantial computational resources,\nlimiting their scalability. We introduce LZMidi, a lightweight symbolic music\ngeneration framework based on a Lempel-Ziv (LZ78)-induced sequential\nprobability assignment (SPA). By leveraging the discrete and sequential\nstructure of MIDI data, our approach enables efficient music generation on\nstandard CPUs with minimal training and inference costs. Theoretically, we\nestablish universal convergence guarantees for our approach, underscoring its\nreliability and robustness. Compared to state-of-the-art diffusion models,\nLZMidi achieves competitive Frechet Audio Distance (FAD), Wasserstein Distance\n(WD), and Kullback-Leibler (KL) scores, while significantly reducing\ncomputational overhead - up to 30x faster training and 300x faster generation.\nOur results position LZMidi as a significant advancement in compression-based\nlearning, highlighting how universal compression techniques can efficiently\nmodel and generate structured sequential data, such as symbolic music, with\npractical scalability and theoretical rigor.",
        "Exploring and manipulating the orbital degrees of freedom in solids has\nbecome a fascinating research topic in modern magnetism. Here, we demonstrate\nthat spin waves can provide a way to control electronic orbital magnetism by\nthe mechanism of scalar spin chirality, allowing for experimental detection\nusing techniques such as the magneto-optical Kerr effect and scanning\ntransmission electron microscopy. By applying linear spin wave theory, we\nuncover that electronic magnon-driven orbital magnetization is extremely\nsensitive to the character of the magnonic excitations. Furthermore, we show\nthat both the induced electronic orbital magnetism and the Nernst transport\nproperties of the orbital angular momentum can be regulated by the strength of\nthe Dzyaloshinskii-Moriya interaction, Kitaev interaction, as well as the\ndirection and magnitude of the external magnetic field. We argue that\nmagnon-mediated electronic orbital magnetism presents an emergent variable\nwhich has to be taken into account when considering the physics of coupling\nmagnonic excitiations to phonons and light.",
        "This paper proposes and analyzes a class of essentially non-oscillatory\ncentral discontinuous Galerkin (CDG) methods for general hyperbolic\nconservation laws. First, we introduce a novel compact, non-oscillatory\nstabilization mechanism that effectively suppresses spurious oscillations while\npreserving the high-order accuracy of CDG methods. Unlike existing\nlimiter-based approaches that rely on large stencils or problem-specific\nparameters for oscillation control, our dual damping mechanism is inspired by\nCDG-based numerical dissipation and leverages overlapping solutions within the\nCDG framework, significantly enhancing stability while maintaining compactness.\nOur approach is free of problem-dependent parameters and complex characteristic\ndecomposition, making it efficient and robust. Second, we provide a rigorous\nstability and optimal error analysis for fully discrete Runge-Kutta (RK) CDG\nschemes, addressing a gap in the theoretical understanding of these methods.\nSpecifically, we establish the approximate skew-symmetry and weak boundedness\nof the CDG discretization. These results enable us to rigorously analyze the\nfully discrete error estimates for our oscillation-eliminating CDG (OECDG)\nmethod, a challenging task due to its nonlinear nature, even for linear\nadvection equations. Building on this framework, we reformulate nonlinear\noscillation-eliminating CDG schemes as linear RK CDG schemes with a nonlinear\nsource term, extending error estimates beyond the linear case to schemes with\nnonlinear oscillation control. While existing error analyses for DG or CDG\nschemes have largely been restricted to linear cases without nonlinear\noscillation-control techniques, our analysis represents an important\ntheoretical advancement. Experiments validate the theoretical findings and\ndemonstrate the effectiveness of the OECDG method."
      ]
    }
  },
  {
    "id":2411.18141,
    "research_type":"applied",
    "start_id":"b26",
    "start_title":"Durban's water wars, sewage spills, fish kills and blue flag beaches. Durban's Climate Gamble",
    "start_abstract":"Water is one of the primary barometers of climate change: A rise in sea-levels, flooding, and extreme storms combined with general water stress and more severe and frequent droughts will escalate crises in municipal infrastructure, requiring continual upgrades for water purification, stormwater drainage, and sewage treatment, all of which will dramatically raise the price of water at the retail level. In South Africa, the dry western side will be most adversely affected by droughts (threatening the production of rooibos tea and Cape wines). According to the Academy of Science in South Africa (ASSAf), Durban is also at great risk and will experience higher temperatures and heat stress, volatile rainfall, up to 160 million cubic metres less water each year by 2100, a sea-level rise of up to a metre by 2100 across Durban\u2019s 100 km of developed coastline, lower biodiversity, higher disease levels (especially malaria and cholera), declining agricultural output (a one degree Celsius rise leaves the surrounding region unreliable for the staple maize production), and other economic stresses (ASSAf 2011: 27). Tourism, one of Durban\u2019s main economic engines, will be irreparably harmed. Swimmers and surfers think of Durban\u2019s beachfront as one of the world\u2019s finest in any urban context. After apartheid-era rules that prohibited black people from using the best beaches were lifted at the end of the 1980s, the area stretching from the Blue Lagoon\u2019s Umgeni River to South Beach\u2019s uShaka Marine World\u2013including the immensely popular North Beach area near the main restaurant strip\u2013represented one of South Africa\u2019s most impressive, open and democratic public spaces.",
    "start_categories":[
      "cond-mat.mtrl-sci"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b18"
      ],
      "title":[
        "Quantum machine learning in chemistry and materials"
      ],
      "abstract":[
        "Within the past few years, we have witnessed the rising of quantum machine learning (QML) models which infer electronic properties of molecules and materials, rather than solving approximations to the electronic Schr\u00f6dinger equation. The increasing availability of large quantum mechanics reference datasets has enabled these developments. We review the basic theories and key ingredients of popular QML models such as choice of regressor, data of varying trustworthiness, the role of the representation, and the effect of training set selection. Throughout we emphasize the indispensable role of learning curves when it comes to the comparative assessment of different QML models."
      ],
      "categories":[
        "cs.ET"
      ]
    },
    "list":{
      "title":[
        "6KSFx Synth Dataset",
        "Optimized detection of cyber-attacks on IoT networks via hybrid deep\n  learning models",
        "A Bot-based Approach to Manage Codes of Conduct in Open-Source Projects",
        "Superficial Self-Improved Reasoners Benefit from Model Merging",
        "SpeHeatal: A Cluster-Enhanced Segmentation Method for Sperm Morphology\n  Analysis",
        "Global well-posedness of the defocusing nonlinear wave equation outside\n  of a ball with radial data for $3<p<5$",
        "Improving Discriminator Guidance in Diffusion Models",
        "From Occasional to Steady: Habit Formation Insights From a Comprehensive\n  Fitness Study",
        "HEATS: A Hierarchical Framework for Efficient Autonomous Target Search\n  with Mobile Manipulators",
        "Cooperative Bearing-Only Target Pursuit via Multiagent Reinforcement\n  Learning: Design and Experiment",
        "Learning to reset in target search problems",
        "Analysis and Optimization of Robustness in Multiplex Flow Networks\n  Against Cascading Failures",
        "Adapting Beyond the Depth Limit: Counter Strategies in Large Imperfect\n  Information Games",
        "New Dataset and Methods for Fine-Grained Compositional Referring\n  Expression Comprehension via Specialist-MLLM Collaboration",
        "FUNU: Boosting Machine Unlearning Efficiency by Filtering Unnecessary\n  Unlearning",
        "Finite Sample Identification of Partially Observed Bilinear Dynamical\n  Systems",
        "Data collaboration for causal inference from limited medical testing and\n  medication data",
        "Knudsen boundary layer equations with incoming boundary condition: full\n  range of cutoff collision kernels and Mach numbers of the far field",
        "Fast computation of the TGOSPA metric for multiple target tracking via\n  unbalanced optimal transport",
        "FlexDuo: A Pluggable System for Enabling Full-Duplex Capabilities in\n  Speech Dialogue Systems",
        "FeNeC: Enhancing Continual Learning via Feature Clustering with\n  Neighbor- or Logit-Based Classification",
        "Training Medical Large Vision-Language Models with Abnormal-Aware\n  Feedback",
        "The Pitfalls of Imitation Learning when Actions are Continuous",
        "A four-term exact sequence of fundamental groups of orbit configuration\n  spaces",
        "Exact Schwinger functions for a class of bounded interactions in $d\\geq\n  2$",
        "A threshold for Poisson behavior of non-stationary product measures",
        "ColorDynamic: Generalizable, Scalable, Real-time, End-to-end Local\n  Planner for Unstructured and Dynamic Environments",
        "On Two Parameter Time-Changed Poisson Random Fields with Drifts",
        "Efficient Knowledge Feeding to Language Models: A Novel Integrated\n  Encoder-Decoder Architecture"
      ],
      "abstract":[
        "Procedural audio, often referred to as \"digital Foley\", generates sound from\nscratch using computational processes. It represents an innovative approach to\nsound-effects creation. However, the development and adoption of procedural\naudio has been constrained by a lack of publicly available datasets and models,\nwhich hinders evaluation and optimization. To address this important gap, this\npaper presents a dataset of 6000 synthetic audio samples specifically designed\nto advance research and development in sound synthesis within 30 sound\ncategories. By offering a description of the diverse synthesis methods used in\neach sound category and supporting the creation of robust evaluation\nframeworks, this dataset not only highlights the potential of procedural audio,\nbut also provides a resource for researchers, audio developers, and sound\ndesigners. This contribution can accelerate the progress of procedural audio,\nopening up new possibilities in digital sound design.",
        "The rapid expansion of Internet of Things (IoT) devices has increased the\nrisk of cyber-attacks, making effective detection essential for securing IoT\nnetworks. This work introduces a novel approach combining Self-Organizing Maps\n(SOMs), Deep Belief Networks (DBNs), and Autoencoders to detect known and\npreviously unseen attack patterns. A comprehensive evaluation using simulated\nand real-world traffic data is conducted, with models optimized via Particle\nSwarm Optimization (PSO). The system achieves an accuracy of up to 99.99% and\nMatthews Correlation Coefficient (MCC) values exceeding 99.50%. Experiments on\nNSL-KDD, UNSW-NB15, and CICIoT2023 confirm the model's strong performance\nacross diverse attack types. These findings suggest that the proposed method\nenhances IoT security by identifying emerging threats and adapting to evolving\nattack strategies.",
        "The development of Open-Source Software (OSS) projects relies on the\ncollaborative work of contributors, generally scattered around the world. To\nenable this collaboration, OSS projects are hosted on social-coding platforms\nlike GitHub, which provide the infrastructure to host the code as well as the\nsupport for enabling the participation of the community. The potentially rich\nand diverse mixture of contributors in OSS projects makes their management not\nonly a technical challenge, where automation tools and bots are usually\ndeployed, but also a social one. To this aim, OSS projects have been\nincreasingly deploying a declaration of their code of conduct, which defines\nrules to ensure a respectful and inclusive participatory environment in the\ncommunity, being the Contributor Covenant the main model to follow. However,\nthe broad adoption and enforcement of codes of conduct in OSS projects is still\nlimited. In particular, the definition, deployment, and enforcement of codes of\nconduct is a very challenging task. In this paper, we propose an approach to\neffectively manage codes of conduct in OSS projects based on the Contributor\nCovenant proposal. Our solution has been implemented as a bot-based solution\nwhere bots help in the definition of codes of conduct, the monitoring of OSS\nprojects, and the enforcement of ethical rules.",
        "As scaled language models (LMs) approach human-level reasoning capabilities,\nself-improvement emerges as a solution to synthesizing high-quality data\ncorpus. While previous research has identified model collapse as a risk in\nself-improvement, where model outputs become increasingly deterministic, we\ndiscover a more fundamental challenge: the superficial self-improved reasoners\nphenomenon. In particular, our analysis reveals that even when LMs show\nimproved in-domain (ID) reasoning accuracy, they actually compromise their\ngeneralized reasoning capabilities on out-of-domain (OOD) tasks due to\nmemorization rather than genuine. Through a systematic investigation of LM\narchitecture, we discover that during self-improvement, LM weight updates are\nconcentrated in less reasoning-critical layers, leading to superficial\nlearning. To address this, we propose Iterative Model Merging (IMM), a method\nthat strategically combines weights from original and self-improved models to\npreserve generalization while incorporating genuine reasoning improvements. Our\napproach effectively mitigates both LM collapse and superficial learning,\nmoving towards more stable self-improving systems.",
        "The accurate assessment of sperm morphology is crucial in andrological\ndiagnostics, where the segmentation of sperm images presents significant\nchallenges. Existing approaches frequently rely on large annotated datasets and\noften struggle with the segmentation of overlapping sperm and the presence of\ndye impurities. To address these challenges, this paper first analyzes the\nissue of overlapping sperm tails from a geometric perspective and introduces a\nnovel clustering algorithm, Con2Dis, which effectively segments overlapping\ntails by considering three essential factors: CONnectivity, CONformity, and\nDIStance. Building on this foundation, we propose an unsupervised method,\nSpeHeatal, designed for the comprehensive segmentation of the SPErm HEAd and\nTAiL. SpeHeatal employs the Segment Anything Model(SAM) to generate masks for\nsperm heads while filtering out dye impurities, utilizes Con2Dis to segment\ntails, and then applies a tailored mask splicing technique to produce complete\nsperm masks. Experimental results underscore the superior performance of\nSpeHeatal, particularly in handling images with overlapping sperm.",
        "We continue the study of the Dirichlet boundary value problem of nonlinear\nwave equation with radial data in the exterior $\\Omega = \\mathbb{R}^3\\backslash\n\\bar{B}(0,1)$. We combine the distorted Fourier truncation method in\n\\cite{Bourgain98:FTM}, the global-in-time (endpoint) Strichartz estimates in\n\\cite{XuYang:NLW} with the energy method in \\cite{GallPlan03:NLW} to prove the\nglobal well-posedness of the radial solution to the defocusing,\nenergy-subcriticial nonlinear wave equation outside of a ball in $\\left(\\dot\nH^{s}_{D}(\\Omega) \\cap L^{p+1}(\\Omega) \\right)\\times \\dot H^{s-1}_{D}(\\Omega)$\nwith $1-\\frac{(p+3)(1-s_c)}{4(2p-3)}<s<1$, $s_c=\\frac{3}{2}-\\frac{2}{p-1} $,\nwhich extends the result for the cubic nonlinearity in \\cite{XuYang:NLW} to the\ncase $3<p<5$. Except from the argument in \\cite{XuYang:NLW}, another new\ningredient is that we need make use of the radial Sobolev inequality to deal\nwith the super-conformal nonlinearity in addition to the Sobolev inequality.",
        "Discriminator Guidance has become a popular method for efficiently refining\npre-trained Score-Matching Diffusion models. However, in this paper, we\ndemonstrate that the standard implementation of this technique does not\nnecessarily lead to a distribution closer to the real data distribution.\nSpecifically, we show that training the discriminator using Cross-Entropy loss,\nas commonly done, can in fact increase the Kullback-Leibler divergence between\nthe model and target distributions, particularly when the discriminator\noverfits. To address this, we propose a theoretically sound training objective\nfor discriminator guidance that properly minimizes the KL divergence. We\nanalyze its properties and demonstrate empirically across multiple datasets\nthat our proposed method consistently improves over the conventional method by\nproducing samples of higher quality.",
        "Exercising regularly is widely recognized as a cornerstone of health, yet the\nchallenge of sustaining consistent exercise habits persists. Understanding the\nfactors that influence the formation of these habits is crucial for developing\neffective interventions. This study utilizes data from Mars Athletic Club,\nT\\\"urkiye's largest sports chain, to investigate the dynamics of gym attendance\nand habit formation. The general problem addressed by this study is identifying\nthe critical periods and factors that contribute to the successful\nestablishment of consistent exercise routines among gym-goers. Here we show\nthat there are specific periods during which gym attendance is most crucial for\nhabit formation. By developing a survival metric based on gym attendance\npatterns, we pinpoint these critical periods and segment members into distinct\nclusters based on their visit patterns. Our analysis reveals significant\ndifferences in how various subgroups respond to interventions, such as group\nclasses, personal trainer sessions, and visiting different clubs. Using causal\ninference analysis, we demonstrate that personalized guidance and social\ndynamics are key drivers of sustained long-term engagement. By systematically\nexamining these variables and considering the specific characteristics of\ndifferent clusters, our research demonstrates the importance of a tailored,\nmulti-dimensional approach to promoting exercise habits, which integrates\nsocial dynamics, personalized guidance, and strategic interventions to sustain\nlong-term engagement.",
        "Utilizing robots for autonomous target search in complex and unknown\nenvironments can greatly improve the efficiency of search and rescue missions.\nHowever, existing methods have shown inadequate performance due to hardware\nplatform limitations, inefficient viewpoint selection strategies, and\nconservative motion planning. In this work, we propose HEATS, which enhances\nthe search capability of mobile manipulators in complex and unknown\nenvironments. We design a target viewpoint planner tailored to the strengths of\nmobile manipulators, ensuring efficient and comprehensive viewpoint planning.\nSupported by this, a whole-body motion planner integrates global path search\nwith local IPC optimization, enabling the mobile manipulator to safely and\nagilely visit target viewpoints, significantly improving search performance. We\npresent extensive simulated and real-world tests, in which our method\ndemonstrates reduced search time, higher target search completeness, and lower\nmovement cost compared to classic and state-of-the-art approaches. Our method\nwill be open-sourced for community benefit.",
        "This paper addresses the multi-robot pursuit problem for an unknown target,\nencompassing both target state estimation and pursuit control. First, in state\nestimation, we focus on using only bearing information, as it is readily\navailable from vision sensors and effective for small, distant targets.\nChallenges such as instability due to the nonlinearity of bearing measurements\nand singularities in the two-angle representation are addressed through a\nproposed uniform bearing-only information filter. This filter integrates\nmultiple 3D bearing measurements, provides a concise formulation, and enhances\nstability and resilience to target loss caused by limited field of view (FoV).\nSecond, in target pursuit control within complex environments, where challenges\nsuch as heterogeneity and limited FoV arise, conventional methods like\ndifferential games or Voronoi partitioning often prove inadequate. To address\nthese limitations, we propose a novel multiagent reinforcement learning (MARL)\nframework, enabling multiple heterogeneous vehicles to search, localize, and\nfollow a target while effectively handling those challenges. Third, to bridge\nthe sim-to-real gap, we propose two key techniques: incorporating adjustable\nlow-level control gains in training to replicate the dynamics of real-world\nautonomous ground vehicles (AGVs), and proposing spectral-normalized RL\nalgorithms to enhance policy smoothness and robustness. Finally, we demonstrate\nthe successful zero-shot transfer of the MARL controllers to AGVs, validating\nthe effectiveness and practical feasibility of our approach. The accompanying\nvideo is available at https:\/\/youtu.be\/HO7FJyZiJ3E.",
        "Target search problems are central to a wide range of fields, from biological\nforaging to the optimization algorithms. Recently, the ability to reset the\nsearch has been shown to significantly improve the searcher's efficiency.\nHowever, the optimal resetting strategy depends on the specific properties of\nthe search problem and can often be challenging to determine. In this work, we\npropose a reinforcement learning (RL)-based framework to train agents capable\nof optimizing their search efficiency in environments by learning how to reset.\nFirst, we validate the approach in a well-established benchmark: the Brownian\nsearch with resetting. There, RL agents consistently recover strategies closely\nresembling the sharp resetting distribution, known to be optimal in this\nscenario. We then extend the framework by allowing agents to control not only\nwhen to reset, but also their spatial dynamics through turning actions. In this\nmore complex setting, the agents discover strategies that adapt both resetting\nand turning to the properties of the environment, outperforming the proposed\nbenchmarks. These results demonstrate how reinforcement learning can serve both\nas an optimization tool and a mechanism for uncovering new, interpretable\nstrategies in stochastic search processes with resetting.",
        "Networked systems are susceptible to cascading failures, where the failure of\nan initial set of nodes propagates through the network, often leading to\nsystem-wide failures. In this work, we propose a multiplex flow network model\nto study robustness against cascading failures triggered by random failures.\nThe model is inspired by systems where nodes carry or support multiple types of\nflows, and failures result in the redistribution of flows within the same layer\nrather than between layers. To represent different types of interdependencies\nbetween the layers of the multiplex network, we define two cases of failure\nconditions: layer-independent overload and layer-influenced overload. We\nprovide recursive equations and their solutions to calculate the steady-state\nfraction of surviving nodes, validate them through a set of simulation\nexperiments, and discuss optimal load-capacity allocation strategies. Our\nresults demonstrate that allocating the total excess capacity to each layer\nproportional to the mean effective load in the layer and distributing that\nexcess capacity equally among the nodes within the layer ensures maximum\nrobustness. The proposed framework for different failure conditions allows us\nto analyze the two overload conditions presented and can be extended to explore\nmore complex interdependent relationships.",
        "We study the problem of adapting to a known sub-rational opponent during\nonline play while remaining robust to rational opponents. We focus on large\nimperfect-information (zero-sum) games, which makes it impossible to inspect\nthe whole game tree at once and necessitates the use of depth-limited search.\nHowever, all existing methods assume rational play beyond the depth-limit,\nwhich only allows them to adapt a very limited portion of the opponent's\nbehaviour. We propose an algorithm Adapting Beyond Depth-limit (ABD) that uses\na strategy-portfolio approach - which we refer to as matrix-valued states - for\ndepth-limited search. This allows the algorithm to fully utilise all\ninformation about the opponent model, making it the first robust-adaptation\nmethod to be able to do so in large imperfect-information games. As an\nadditional benefit, the use of matrix-valued states makes the algorithm simpler\nthan traditional methods based on optimal value functions. Our experimental\nresults in poker and battleship show that ABD yields more than a twofold\nincrease in utility when facing opponents who make mistakes beyond the depth\nlimit and also delivers significant improvements in utility and safety against\nrandomly generated opponents.",
        "Referring Expression Comprehension (REC) is a foundational cross-modal task\nthat evaluates the interplay of language understanding, image comprehension,\nand language-to-image grounding. To advance this field, we introduce a new REC\ndataset with two key features. First, it is designed with controllable\ndifficulty levels, requiring fine-grained reasoning across object categories,\nattributes, and relationships. Second, it incorporates negative text and images\ngenerated through fine-grained editing, explicitly testing a model's ability to\nreject non-existent targets, an often-overlooked yet critical challenge in\nexisting datasets. To address fine-grained compositional REC, we propose novel\nmethods based on a Specialist-MLLM collaboration framework, leveraging the\ncomplementary strengths of them: Specialist Models handle simpler tasks\nefficiently, while MLLMs are better suited for complex reasoning. Based on this\nsynergy, we introduce two collaborative strategies. The first, Slow-Fast\nAdaptation (SFA), employs a routing mechanism to adaptively delegate simple\ntasks to Specialist Models and complex tasks to MLLMs. Additionally, common\nerror patterns in both models are mitigated through a target-refocus strategy.\nThe second, Candidate Region Selection (CRS), generates multiple bounding box\ncandidates based on Specialist Model and uses the advanced reasoning\ncapabilities of MLLMs to identify the correct target. Extensive experiments on\nour dataset and other challenging compositional benchmarks validate the\neffectiveness of our approaches. The SFA strategy achieves a trade-off between\nlocalization accuracy and efficiency, and the CRS strategy greatly boosts the\nperformance of both Specialist Models and MLLMs. We aim for this work to offer\nvaluable insights into solving complex real-world tasks by strategically\ncombining existing tools for maximum effectiveness, rather than reinventing\nthem.",
        "Machine unlearning is an emerging field that selectively removes specific\ndata samples from a trained model. This capability is crucial for addressing\nprivacy concerns, complying with data protection regulations, and correcting\nerrors or biases introduced by certain data. Unlike traditional machine\nlearning, where models are typically static once trained, machine unlearning\nfacilitates dynamic updates that enable the model to ``forget'' information\nwithout requiring complete retraining from scratch. There are various machine\nunlearning methods, some of which are more time-efficient when data removal\nrequests are fewer.\n  To decrease the execution time of such machine unlearning methods, we aim to\nreduce the size of data removal requests based on the fundamental assumption\nthat the removal of certain data would not result in a distinguishable\nretrained model. We first propose the concept of unnecessary unlearning, which\nindicates that the model would not alter noticeably after removing some data\npoints. Subsequently, we review existing solutions that can be used to solve\nour problem. We highlight their limitations in adaptability to different\nunlearning scenarios and their reliance on manually selected parameters. We\nconsequently put forward FUNU, a method to identify data points that lead to\nunnecessary unlearning. FUNU circumvents the limitations of existing solutions.\nThe idea is to discover data points within the removal requests that have\nsimilar neighbors in the remaining dataset. We utilize a reference model to set\nparameters for finding neighbors, inspired from the area of model memorization.\nWe provide a theoretical analysis of the privacy guarantee offered by FUNU and\nconduct extensive experiments to validate its efficacy.",
        "We consider the problem of learning a realization of a partially observed\nbilinear dynamical system (BLDS) from noisy input-output data. Given a single\ntrajectory of input-output samples, we provide a finite time analysis for\nlearning the system's Markov-like parameters, from which a balanced realization\nof the bilinear system can be obtained. Our bilinear system identification\nalgorithm learns the system's Markov-like parameters by regressing the outputs\nto highly correlated, nonlinear, and heavy-tailed covariates. Moreover, the\nstability of BLDS depends on the sequence of inputs used to excite the system.\nThese properties, unique to partially observed bilinear dynamical systems, pose\nsignificant challenges to the analysis of our algorithm for learning the\nunknown dynamics. We address these challenges and provide high probability\nerror bounds on our identification algorithm under a uniform stability\nassumption. Our analysis provides insights into system theoretic quantities\nthat affect learning accuracy and sample complexity. Lastly, we perform\nnumerical experiments with synthetic data to reinforce these insights.",
        "Observational studies enable causal inferences when randomized controlled\ntrials (RCTs) are not feasible. However, integrating sensitive medical data\nacross multiple institutions introduces significant privacy challenges. The\ndata collaboration quasi-experiment (DC-QE) framework addresses these concerns\nby sharing \"intermediate representations\" -- dimensionality-reduced data\nderived from raw data -- instead of the raw data. While the DC-QE can estimate\ntreatment effects, its application to medical data remains unexplored. This\nstudy applied the DC-QE framework to medical data from a single institution to\nsimulate distributed data environments under independent and identically\ndistributed (IID) and non-IID conditions. We propose a novel method for\ngenerating intermediate representations within the DC-QE framework.\nExperimental results demonstrated that DC-QE consistently outperformed\nindividual analyses across various accuracy metrics, closely approximating the\nperformance of centralized analysis. The proposed method further improved\nperformance, particularly under non-IID conditions. These outcomes highlight\nthe potential of the DC-QE framework as a robust approach for\nprivacy-preserving causal inferences in healthcare. Broader adoption of this\nframework and increased use of intermediate representations could grant\nresearchers access to larger, more diverse datasets while safeguarding patient\nconfidentiality. This approach may ultimately aid in identifying previously\nunrecognized causal relationships, support drug repurposing efforts, and\nenhance therapeutic interventions for rare diseases.",
        "This paper establishes tahe existence and uniqueness of the nonlinear Knudsen\nlayer equation with incoming boundary conditions. It is well-known that the\nsolvability conditions of the problem vary with the Mach number of the far\nMaxwellian $\\mathcal{M}^\\infty$. We consider full ranges of cutoff collision\nkernels (i.e., $- 3 < \\gamma \\leq 1$) and all the Mach numbers of the far field\nin the $L^\\infty_{x,v}$ framework. Additionally, the solution exhibits\nexponential decay $\\exp \\{- c x^\\frac{2}{3 - \\gamma} - c |v|^2 \\}$ for some $c\n> 0$. To address the general angular cutoff collision kernel, we introduce a\n$(x,v)$-mixed weight $\\sigma$. The proof is essentially bsed on adding an\nartificial damping term.",
        "In multiple target tracking, it is important to be able to evaluate the\nperformance of different tracking algorithms. The trajectory generalized\noptimal sub-pattern assignment metric (TGOSPA) is a recently proposed metric\nfor such evaluations. The TGOSPA metric is computed as the solution to an\noptimization problem, but for large tracking scenarios, solving this problem\nbecomes computationally demanding. In this paper, we present an approximation\nalgorithm for evaluating the TGOSPA metric, based on casting the TGOSPA problem\nas an unbalanced multimarginal optimal transport problem. Following recent\nadvances in computational optimal transport, we introduce an entropy\nregularization and derive an iterative scheme for solving the Lagrangian dual\nof the regularized problem. Numerical results suggest that our proposed\nalgorithm is more computationally efficient than the alternative of computing\nthe exact metric using a linear programming solver, while still providing an\nadequate approximation of the metric.",
        "Full-Duplex Speech Dialogue Systems (Full-Duplex SDS) have significantly\nenhanced the naturalness of human-machine interaction by enabling real-time\nbidirectional communication. However, existing approaches face challenges such\nas difficulties in independent module optimization and contextual noise\ninterference due to highly coupled architectural designs and oversimplified\nbinary state modeling. This paper proposes FlexDuo, a flexible full-duplex\ncontrol module that decouples duplex control from spoken dialogue systems\nthrough a plug-and-play architectural design. Furthermore, inspired by human\ninformation-filtering mechanisms in conversations, we introduce an explicit\nIdle state. On one hand, the Idle state filters redundant noise and irrelevant\naudio to enhance dialogue quality. On the other hand, it establishes a semantic\nintegrity-based buffering mechanism, reducing the risk of mutual interruptions\nwhile ensuring accurate response transitions. Experimental results on the\nFisher corpus demonstrate that FlexDuo reduces the false interruption rate by\n24.9% and improves response accuracy by 7.6% compared to integrated full-duplex\ndialogue system baselines. It also outperforms voice activity detection (VAD)\ncontrolled baseline systems in both Chinese and English dialogue quality. The\nproposed modular architecture and state-based dialogue model provide a novel\ntechnical pathway for building flexible and efficient duplex dialogue systems.",
        "The ability of deep learning models to learn continuously is essential for\nadapting to new data categories and evolving data distributions. In recent\nyears, approaches leveraging frozen feature extractors after an initial\nlearning phase have been extensively studied. Many of these methods estimate\nper-class covariance matrices and prototypes based on backbone-derived feature\nrepresentations. Within this paradigm, we introduce FeNeC (Feature Neighborhood\nClassifier) and FeNeC-Log, its variant based on the log-likelihood function.\nOur approach generalizes the existing concept by incorporating data clustering\nto capture greater intra-class variability. Utilizing the Mahalanobis distance,\nour models classify samples either through a nearest neighbor approach or\ntrainable logit values assigned to consecutive classes. Our proposition may be\nreduced to the existing approaches in a special case while extending them with\nthe ability of more flexible adaptation to data. We demonstrate that two FeNeC\nvariants achieve competitive performance in scenarios where task identities are\nunknown and establish state-of-the-art results on several benchmarks.",
        "Existing Medical Large Vision-Language Models (Med-LVLMs), which encapsulate\nextensive medical knowledge, demonstrate excellent capabilities in\nunderstanding medical images and responding to human queries based on these\nimages. However, there remain challenges in visual localization in medical\nimages, which is crucial for abnormality detection and interpretation. To\naddress these issues, we propose a novel UMed-LVLM designed with Unveiling\nMedical abnormalities. Specifically, we collect a Medical Abnormalities\nUnveiling (MAU) dataset and propose a two-stage training method for UMed-LVLM\ntraining. To collect MAU dataset, we propose a prompt method utilizing the\nGPT-4V to generate diagnoses based on identified abnormal areas in medical\nimages. Moreover, the two-stage training method includes Abnormal-Aware\nInstruction Tuning and Abnormal-Aware Rewarding, comprising Abnormal\nLocalization Rewarding and Vision Relevance Rewarding. Experimental results\ndemonstrate that our UMed-LVLM surpasses existing Med-LVLMs in identifying and\nunderstanding medical abnormality. In addition, this work shows that enhancing\nthe abnormality detection capabilities of Med-LVLMs significantly improves\ntheir understanding of medical images and generalization capability.",
        "We study the problem of imitating an expert demonstrator in a discrete-time,\ncontinuous state-and-action control system. We show that, even if the dynamics\nare stable (i.e. contracting exponentially quickly), and the expert is smooth\nand deterministic, any smooth, deterministic imitator policy necessarily\nsuffers error on execution that is exponentially larger, as a function of\nproblem horizon, than the error under the distribution of expert training data.\nOur negative result applies to both behavior cloning and offline-RL algorithms,\nunless they produce highly \"improper\" imitator policies--those which are\nnon-smooth, non-Markovian, or which exhibit highly state-dependent\nstochasticity--or unless the expert trajectory distribution is sufficiently\n\"spread.\" We provide experimental evidence of the benefits of these more\ncomplex policy parameterizations, explicating the benefits of today's popular\npolicy parameterizations in robot learning (e.g. action-chunking and Diffusion\nPolicies). We also establish a host of complementary negative and positive\nresults for imitation in control systems.",
        "We deduce that the fundamental groups of the orbit configuration spaces of an\neffective and properly discontinuous action of a discrete group on a connected\naspherical 2-manifold, with isolated fixed points, fit into a four-term exact\nsequence. This comes as a consequence of the four-term exact sequence of\norbifold braid groups ([16], [11] and [17]). The proof relates these two exact\nsequences and also draws a new consequence (Corollary 2.3) on the later one.",
        "We consider a scalar Euclidean QFT with interaction given by a bounded,\nmeasurable function $V$ such that $V^{\\pm}:=\\lim_{w\\to \\pm\\infty}V(w)$ exist.\nWe find a field renormalization such that all the $n$-point connected Schwinger\nfunctions for $n\\neq 2$ exist non-perturbatively in the UV limit. They coincide\nwith the tree-level one-particle irreducible Schwinger functions of the\n$\\mathrm{erf}(\\phi\/\\sqrt{2})$ interaction with a coupling constant $\\frac{1}{2}\n(V^+ - V^-)$. By a slight modification of our construction we can change this\ncoupling constant to $\\frac{1}{2} (V_+ - V_-)$, where $V_{\\pm}:= \\lim_{w\\to\n0^{\\pm}} V(w)$. Thereby non-Gaussianity of these latter theories is governed by\na discontinuity of $V$ at zero. The open problem of controlling also the\ntwo-point function of these QFTs is discussed.",
        "Let $\\gamma_{n}= O (\\log^{-c}n)$ and let $\\nu$ be the infinite product\nmeasure whose $n$-th marginal is Bernoulli$(1\/2+\\gamma_{n})$. We show that\n$c=1\/2$ is the threshold, above which $\\nu$-almost every point is simply\nPoisson generic in the sense of Peres-Weiss, and below which this can fail.\nThis provides a range in which $\\nu$ is singular with respect to the uniform\nproduct measure, but $\\nu$-almost every point is simply Poisson generic.",
        "Deep Reinforcement Learning (DRL) has demonstrated potential in addressing\nrobotic local planning problems, yet its efficacy remains constrained in highly\nunstructured and dynamic environments. To address these challenges, this study\nproposes the ColorDynamic framework. First, an end-to-end DRL formulation is\nestablished, which maps raw sensor data directly to control commands, thereby\nensuring compatibility with unstructured environments. Under this formulation,\na novel network, Transqer, is introduced. The Transqer enables online DRL\nlearning from temporal transitions, substantially enhancing decision-making in\ndynamic scenarios. To facilitate scalable training of Transqer with diverse\ndata, an efficient simulation platform E-Sparrow, along with a data\naugmentation technique leveraging symmetric invariance, are developed.\nComparative evaluations against state-of-the-art methods, alongside assessments\nof generalizability, scalability, and real-time performance, were conducted to\nvalidate the effectiveness of ColorDynamic. Results indicate that our approach\nachieves a success rate exceeding 90% while exhibiting real-time capacity\n(1.2-1.3 ms per planning). Additionally, ablation studies were performed to\ncorroborate the contributions of individual components. Building on this, the\nOkayPlan-ColorDynamic (OPCD) navigation system is presented, with simulated and\nreal-world experiments demonstrating its superiority and applicability in\ncomplex scenarios. The codebase and experimental demonstrations have been\nopen-sourced on our website to facilitate reproducibility and further research.",
        "We study the composition of bivariate L\\'evy process with bivariate inverse\nsubordinator. The explicit expressions for its dispersion and auto correlation\nmatrices are obtained. Also, the time-changed two parameter L\\'evy processes\nwith rectangular increments are studied. We introduce some time-changed\nvariants of the Poisson random field in plane with and without drift, and\nderive the associated fractional differential equations for their\ndistributions. Later, we consider some time-changed L\\'evy processes where the\ntime-changing components are two parameter Poisson random fields with drifts.\nMoreover, two parameter coordinatewise semigroup operators associated with some\nof the introduced processes are discussed.",
        "This paper introduces a novel approach to efficiently feeding knowledge to\nlanguage models (LLMs) during prediction by integrating retrieval and\ngeneration processes within a unified framework. While the Retrieval-Augmented\nGeneration (RAG) model addresses gaps in LLMs' training data and knowledge\nlimits, it is hindered by token limit restrictions and dependency on the\nretrieval system's accuracy. Our proposed architecture incorporates in-context\nvectors (ICV) to overcome these challenges. ICV recasts in-context learning by\nusing latent embeddings of LLMs to create a vector that captures essential task\ninformation. This vector is then used to shift the latent states of the LLM,\nenhancing the generation process without adding demonstration examples to the\nprompt. ICV directly integrates information into the model, enabling it to\nprocess this information more effectively. Our extensive experimental\nevaluation demonstrates that ICV outperforms standard in-context learning and\nfine-tuning across question-answering, information retrieval, and other tasks.\nThis approach mitigates the limitations of current RAG models and offers a more\nrobust solution for handling extensive and diverse datasets. Despite leveraging\na fraction of the parameters, our ICV-enhanced model achieves competitive\nperformance against models like LLaMA-3, Gemma, and Phi-3, significantly\nreducing computational costs and memory requirements. ICV reduces prompt\nlength, is easy to control, surpasses token limitations, and is computationally\nefficient compared to fine-tuning."
      ]
    }
  },
  {
    "id":2411.16349,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Cerebral aneurysms. New Engl. J. Medicine",
    "start_abstract":"Saccular intracranial aneurysms cause substantial morbidity and mortality. Recently, major changes have occurred in the way we think about and treat this disease. This review discusses the percutaneous endovascular treatment of intracranial aneurysms as compared with surgical intervention. The technological advances and supporting research contributing to this important change in practice patterns are reviewed.",
    "start_categories":[
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b18"
      ],
      "title":[
        "Data-driven science and engineering: machine learning, dynamical systems, and control"
      ],
      "abstract":[
        "\"Data-driven science and engineering: machine learning, dynamical systems, control.\" Contemporary Physics, 60(4), p. 320"
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "Effects of oxidation and impurities in lithium surfaces on the emitting\n  wall plasma sheath",
        "Conservative, pressure-equilibrium-preserving discontinuous Galerkin\n  method for compressible, multicomponent flows",
        "MAISTEP -- a new grid-based machine learning tool for inferring stellar\n  parameters I. Ages of giant-planet host stars",
        "Moment-based Characterization of Spatially Distributed Sources in SAR\n  Tomography",
        "Infinite-temperature thermostats by energy localization in a\n  nonequilibrium setup",
        "A Systematic Evaluation of Generative Models on Tabular Transportation\n  Data",
        "Grey system model on time scales",
        "Jailbreaking Generative AI: Empowering Novices to Conduct Phishing\n  Attacks",
        "A Resilient and Energy-Efficient Smart Metering Infrastructure Utilizing\n  a Self-Organizing UAV Swarm",
        "Single-molecule phosphorescence and intersystem crossing in a coupled\n  exciton-plasmon system",
        "SEAlign: Alignment Training for Software Engineering Agent",
        "Discrete-time weak approximation of a Black-Scholes model with drift and\n  volatility Markov switching",
        "Long-term evolution of Sco X-1: implications for the current spin\n  frequency and ellipticity of the neutron star",
        "Modular and Integrated AI Control Framework across Fiber and Wireless\n  Networks for 6G",
        "Antiferromagnetic and bond-order-wave phases in the half-filled\n  two-dimensional optical Su-Schrieffer-Heeger-Hubbard model",
        "Polarization-controlled strong light-matter interaction with templated\n  molecular aggregates",
        "Performant LLM Agentic Framework for Conversational AI",
        "Efficient Langevin sampling with position-dependent diffusion",
        "Counting Black Hole Microstates at Future Null Infinity",
        "Issues with Neural Tangent Kernel Approach to Neural Networks",
        "Charged Black Holes in the Kalb-Ramond Background with Lorentz\n  Violation: Null Geodesics and Optical Appearance of a Thin Accretion Disk",
        "Electronic band structures of topological kagome materials",
        "Layered Topological Antiferromagnetic Metal at Room Temperature --\n  YbMn$_2$Ge$_2$",
        "Rigid and flexible Wasserstein spaces",
        "Cascading Bandits Robust to Adversarial Corruptions",
        "Uniqueness of Transonic Shock Solutions for Two-Dimensional Steady\n  Compressible Euler Flows in an Expanding Nozzle",
        "Kinematically induced dipole anisotropy in line-emitting galaxy number\n  counts and line intensity maps",
        "Digging into the Interior of Hot Cores with ALMA (DIHCA). V. Deuterium\n  Fractionation of Methanol",
        "Leptogenesis in five-dimensional asymptotic Grand Unification models"
      ],
      "abstract":[
        "Use of lithium as a surface coating in fusion devices improves plasma\nperformance, but the change in wall properties affects the secondary electron\nemission properties of the material. Lithium oxidizes easily, which drives the\nemission yield well above unity. We present here simulations demonstrating the\nchange in sheath structure from monotonic to the nonmonotonic space-charge\nlimited sheath using an energy-dependent data-driven emission model which\nself-consistently captures both secondary emission and backscattering\npopulations. Increased secondary electron emission from the material has\nramifications for the degradation and erosion of the wall. Results shows that\nthe oxidation leads to an increased electron flux into the wall, and a reduced\nion flux. The net transfer of energy to the surface is significantly greater\nfor the oxidized case than for the pure lithium case. High reflection rates of\nlow-energy backscattered particles leads to a high re-emission rate at the\nwall.",
        "This paper concerns preservation of velocity and pressure equilibria in\nsmooth, compressible, multicomponent flows in the inviscid limit. First, we\nderive the velocity-equilibrium and pressure-equilibrium conditions of a\nstandard discontinuous Galerkin method that discretizes the conservative form\nof the compressible, multicomponent Euler equations. We show that under certain\nconstraints on the numerical flux, the scheme is\nvelocity-equilibrium-preserving. However, standard discontinuous Galerkin\nschemes are not pressure-equilibrium-preserving. Therefore, we introduce a\ndiscontinuous Galerkin method that discretizes the pressure-evolution equation\nin place of the total-energy conservation equation. Semidiscrete conservation\nof total energy, which would otherwise be lost, is restored via the correction\nterms of [Abgrall, J. Comput. Phys., 372, 2018, pp. 640-666] and [Abgrall et\nal., J. Comput. Phys., 453, 2022, 110955]. Since the addition of the correction\nterms prevents exact preservation of pressure and velocity equilibria, we\npropose modifications that then lead to a velocity-equilibrium-preserving,\npressure-equilibrium-preserving, and (semidiscretely) energy-conservative\ndiscontinuous Galerkin scheme, although there are certain tradeoffs. Additional\nextensions are also introduced. We apply the developed scheme to smooth,\ninterfacial flows involving mixtures of thermally perfect gases initially in\npressure and velocity equilibria to demonstrate its performance in one, two,\nand three spatial dimensions.",
        "Our understanding of exoplanet demographics partly depends on their\ncorresponding host star parameters. With the majority of exoplanet-host stars\nhaving only atmospheric constraints available, robust inference of their\nparameters is susceptible to the approach used. The goal of this work is to\ndevelop a grid-based machine learning tool capable of determining the stellar\nradius, mass, and age using only atmospheric constraints and to analyse the age\ndistribution of stars hosting giant planets. Our machine learning approach\ninvolves combining four tree-based machine learning algorithms (Random Forest,\nExtra Trees, Extreme Gradient Boosting, and CatBoost) trained on a grid of\nstellar models to infer stellar radius, mass, and age using Teff, [Fe\/H], and\nluminosities. We perform a detailed statistical analysis to compare the\ninferences of our tool with those based on seismic data from the APOKASC and\nLEGACY samples. Finally, we apply our tool to determine the ages of stars\nhosting giant planets. Comparing the stellar parameter inferences from our\nmachine learning tool with those from the APOKASC and LEGACY, we find a bias\n(and a scatter) of -0.5\\% (5\\%) and -0.2\\% (2\\%) in radius, 6\\% (5\\%) per cent\nand -2\\% (3\\%) in mass, and -9\\% (16\\%) and 7\\% (23\\%) in age, respectively.\nTherefore, our machine learning predictions are commensurate with seismic\ninferences. When applying our model to a sample of stars hosting Jupiter-mass\nplanets, we find the average age estimates for the hosts of Hot Jupiters, Warm\nJupiters, and Cold Jupiters to be 1.98, 2.98, and 3.51 Gyr, respectively. These\nstatistical ages of the host stars confirm previous predictions - based on\nstellar model ages for a relatively small number of hosts, as well as on the\naverage age-velocity dispersion relation - that stars hosting Hot Jupiters are\nstatistically younger than those hosting Warm and Cold Jupiters.",
        "This paper presents a non-parametric method for 3-D imaging of natural\nvolumes using Synthetic Aperture Radar tomography. This array processing-based\ntechnique aims at characterizing a spatially distributed density of incoherent\nsources, whose shape is imprecisely known. The proposed technique estimates the\nmoments of the reflectivity density using a low-complexity covariance matching\napproach, and retrieves the mean location, dispersion, and power of the\ndistributed source. Numerical simulations of realistic tomographic scenarios\nshow that the proposed model-free scheme achieves better accuracy than slightly\nmisspecified maximum likelihood estimators, derived from approximately known\ndistribution shapes.",
        "Some lattice models having two conservation laws may display an equilibrium\nphase transition from a homogeneous (positive temperature - PT) to a condensed\n(negative temperature) phase, where a finite fraction of the energy is\nlocalized in a few sites. We study one such stochastic model in an\nout-of-equilibrium setup, where the ends of the lattice chain are attached to\ntwo PT baths. We show that localized peaks may spontaneously emerge, acting as\ninfinite-temperature heat baths. The number $N_b$ of peaks is expected to grow\nin time $t$ as $N_b \\sim \\sqrt{\\ln t}$, as a consequence of an effective\nfreezing of the dynamics. Asymptotically, the chain spontaneously subdivides\ninto three intervals: the two external ones lying inside the PT region; the\nmiddle one characterized by peaks superposed to a background lying along the\ninfinite-temperature line. In the thermodynamic limit, the Onsager formalism\nallows determining the shape of the whole profile.",
        "The sharing of large-scale transportation data is beneficial for\ntransportation planning and policymaking. However, it also raises significant\nsecurity and privacy concerns, as the data may include identifiable personal\ninformation, such as individuals' home locations. To address these concerns,\nsynthetic data generation based on real transportation data offers a promising\nsolution that allows privacy protection while potentially preserving data\nutility. Although there are various synthetic data generation techniques, they\nare often not tailored to the unique characteristics of transportation data,\nsuch as the inherent structure of transportation networks formed by all trips\nin the datasets. In this paper, we use New York City taxi data as a case study\nto conduct a systematic evaluation of the performance of widely used tabular\ndata generative models. In addition to traditional metrics such as distribution\nsimilarity, coverage, and privacy preservation, we propose a novel graph-based\nmetric tailored specifically for transportation data. This metric evaluates the\nsimilarity between real and synthetic transportation networks, providing\npotentially deeper insights into their structural and functional alignment. We\nalso introduced an improved privacy metric to address the limitations of the\ncommonly-used one. Our experimental results reveal that existing tabular data\ngenerative models often fail to perform as consistently as claimed in the\nliterature, particularly when applied to transportation data use cases.\nFurthermore, our novel graph metric reveals a significant gap between synthetic\nand real data. This work underscores the potential need to develop generative\nmodels specifically tailored to take advantage of the unique characteristics of\nemerging domains, such as transportation.",
        "The Grey System Theory (GST) is a powerful mathematical framework employed\nfor modeling systems with uncertain or incomplete information. This paper\nproposes an integration of the GST with time scales, a generalized approach\nthat encompasses both discrete and continuous time models. The proposed model,\ncalled the Grey System Model on Time Scales (GST-T), offers a robust solution\nfor analyzing hybrid systems where events occur on varying time domains.",
        "The rapid advancements in generative AI models, such as ChatGPT, have\nintroduced both significant benefits and new risks within the cybersecurity\nlandscape. This paper investigates the potential misuse of the latest AI model,\nChatGPT-4o Mini, in facilitating social engineering attacks, with a particular\nfocus on phishing, one of the most pressing cybersecurity threats today. While\nexisting literature primarily addresses the technical aspects, such as\njailbreaking techniques, none have fully explored the free and straightforward\nexecution of a comprehensive phishing campaign by novice users using ChatGPT-4o\nMini. In this study, we examine the vulnerabilities of AI-driven chatbot\nservices in 2025, specifically how methods like jailbreaking and reverse\npsychology can bypass ethical safeguards, allowing ChatGPT to generate phishing\ncontent, suggest hacking tools, and assist in carrying out phishing attacks.\nOur findings underscore the alarming ease with which even inexperienced users\ncan execute sophisticated phishing campaigns, emphasizing the urgent need for\nstronger cybersecurity measures and heightened user awareness in the age of AI.",
        "The smart metering infrastructure may become one of the key elements in\nefficiently managing energy in smart cities. At the same time, traditional\nmeasurement record collection is performed by manual methods, which raises\ncost, safety, and accuracy issues. This paper proposes an innovative SMI\narchitecture based on an unmanned aerial vehicle swarm organizing itself for\nthe autonomous data collection in smart metering infrastructure with\nscalability and cost-effectiveness while minimizing risks. We design an\narchitecture-based comprehensive system with various phases of operation,\ncommunication protocols, and robust failure-handling mechanisms to ensure\nreliable operations. We further perform extensive simulations in maintenance of\nprecise formations during flight, efficient data collection from smart meters,\nand adaptation to various failure scenarios. Importantly, we analyze the energy\nconsumption of the proposed system in both drone flight operations and network\ncommunication. We now propose a battery sizing strategy and provide an estimate\nof the operational lifetime of the swarm, underlining the feasibility and\npracticality of our approach. Our results show that UAV swarms have great\npotential to revolutionize smart metering and to bring a further brick to\ngreener and more resilient smart cities.",
        "Scanning the sharp metal tip of a scanning tunneling microscope (STM) over a\nmolecule allows tuning the coupling between the tip plasmon and a molecular\nfluorescence emitter. This allows access to local variations of fluorescence\nfield enhancement and wavelength shifts, which are central parameters for\ncharacterizing the plasmon-exciton coupling. Performing the same for\nphosphorescence with molecular scale resolution remains a significant\nchallenge. In this study, we present the first investigation of phosphorescence\nfrom isolated Pt-Phthalocyanine molecules by analyzing tip-enhanced emission\nspectra in both current-induced and laser-induced phosphorescence. The latter\ndirectly monitors singlet-to-triplet state intersystem crossing of a molecule\nbelow the tip. The study paves the way to a detailed understanding of triplet\nexcitation pathways and their potential control at sub-molecular length scales.\nAdditionally, the coupling of organic phosphors to plasmonic structures is a\npromising route for the improving light-emitting diodes.",
        "Recent advances in code generation models have demonstrated impressive\ncapabilities in automating software development tasks, yet these models still\nstruggle in real-world software engineering scenarios. Although current\ntraining methods, particularly post-training, excel at solving competitive\nprogramming problems, they fail to adequately prepare models for the\ncomplexities of practical software development. This misalignment raises the\ncritical question: Are existing alignment training methods well suited for\nreal-world software engineering tasks? In this study, we identify this issue\nand propose SEAlign, a novel alignment framework designed to bridge the gap\nbetween code generation models and real-world software development tasks.\nSEAlign leverages the unique characteristics of software engineering processes,\nincluding high-quality workflow steps, to enhance model capabilities. Our\nframework further employs Monte Carlo Tree Search for fine-grained alignment in\nmulti-step decision processes, followed by preference optimization on critical\nactions to ensure models meet real-world requirements. We evaluate SEAlign on\nthree standard agentic benchmarks for real-world software engineering,\nincluding HumanEvalFix, SWE-Bench-Lite, and SWE-Bench-Verified. Experimental\nresults demonstrate state-of-the-art performance with minimal training\noverhead. In addition, we develop an agent-based software development platform\nusing SEAlign, which successfully automates the creation of several small\napplications. Human evaluations of these applications highlight significant\nimprovements in both task performance and user experience. Our findings\nunderscore the potential of SEAlign to accelerate the adoption of large code\nmodels in real-world software development. We believe that this research makes\na meaningful step towards fully automated software engineering.",
        "We consider a continuous-time financial market with an asset whose price is\nmodeled by a linear stochastic differential equation with drift and volatility\nswitching driven by a uniformly ergodic jump Markov process with a countable\nstate space (in fact, this is a Black-Scholes model with Markov switching). We\nconstruct a multiplicative scheme of series of discrete-time markets with\ndiscrete-time Markov switching. First, we establish that the discrete-time\nswitching Markov chains weakly converge to the limit continuous-time Markov\nprocess. Second, having this in hand, we apply conditioning on Markov chains\nand prove that the discrete-time market models themselves weakly converge to\nthe Black-Scholes model with Markov switching. The convergence is proved under\nvery general assumptions both on the discrete-time net profits and on a\ngenerator of a continuous-time Markov switching process.",
        "Sco X-1 is the brightest observed extra-solar X-ray source, which is a\nneutron star (NS) low-mass X-ray binary (LMXB), and is thought to have a strong\npotential for continuous gravitational waves (CW) detection due to its high\naccretion rate and relative proximity. Here, we compute the long-term evolution\nof its parameters, particularly the NS spin frequency ($\\nu$) and the surface\nmagnetic field ($B$), to probe its nature and its potential for CW detection.\nWe find that Sco X-1 is an unusually young ($\\sim7\\times10^6$ yr) LMXB and\nconstrain the current NS mass to $\\sim 1.4-1.6~{\\rm M}_\\odot$. Our computations\nreveal a rapid $B$ decay, with the maximum current value of $\\sim\n1.8\\times10^8$ G, which can be useful to constrain the decay models. Note that\nthe maximum current $\\nu$ value is $\\sim 550$ Hz, implying that, unlike what is\ngenerally believed, a CW emission is not required to explain the current source\nproperties. However, $\\nu$ will exceed an observed cut-off frequency of $\\sim\n730$ Hz, and perhaps even the NS break-up frequency, in the future, without a\nCW emission. The minimum NS mass quadrupole moment ($Q$) to avoid this is $\\sim\n(2-3)\\times10^{37}$ g cm$^2$, corresponding to a CW strain of $\\sim 10^{-26}$.\nOur estimation of current $\\nu$ values can improve the CW search sensitivity.",
        "The rapid evolution of communication networks towards 6G increasingly\nincorporates advanced AI-driven controls across various network segments to\nachieve intelligent, zero-touch operation. This paper proposes a comprehensive\nand modular framework for AI controllers, designed to be highly flexible and\nadaptable for use across both fiber optical and radio networks. Building on the\nprinciples established by the O-RAN Alliance for near-Real-Time RAN Intelligent\nControllers (near-RT RICs), our framework extends this AI-driven control into\nthe optical domain. Our approach addresses the critical need for a unified AI\ncontrol framework across diverse network transport technologies and domains,\nenabling the development of intelligent, automated, and scalable 6G networks.",
        "Electron-phonon ($e$-ph) interactions arise in many strongly correlated\nquantum materials from the modulation of the nearest-neighbor hopping\nintegrals, as in the celebrated Su-Schrieffer-Heeger (SSH) model. Nevertheless,\nrelatively few non-perturbative studies of correlated SSH models have been\nconducted in dimensions greater than one, and those that have been done have\nprimarily focused on bond models, where generalized displacements independently\nmodulate each hopping integral. We conducted a sign-problem free determinant\nquantum Monte Carlo study of the optical SSH-Hubbard model on a two-dimensional\nsquare lattice, where site-centered phonon modes simultaneously modulate pairs\nof nearest-neighbor hopping integrals. We report the model's low-temperature\nphase diagram in the challenging adiabatic regime ($\\Omega\/E_\\mathrm{F} \\sim\n1\/8$). It exhibits insulating antiferromagnetic Mott and bond-order-wave (BOW)\nphases with a narrow region of coexistence between them. We also find that a\ncritical $e$-ph coupling is required to stabilize the BOW phase in the small\n$U$ limit. Lastly, in stark contrast to recent findings for the model's bond\nvariant, we find no evidence for a long-range antiferromagnetism in the pure\n$(U\/t=0)$ optical SSH model.",
        "We demonstrate strong light-matter interaction for a layer of templated\nmerocyanine molecules in a planar microcavity. Using a single layer of graphene\nnanoribbons as a templating layer, we obtain an aligned layer of aggregated\nmolecules. The molecular layer displays anisotropic optical properties\nresembling those of a biaxial crystal. The anisotropic excitonic component in\nthe cavity results in strongly polarization-dependent light-matter interaction\nand in increased Rabi-energies. The increased light-matter interaction is\npossibly due to reduced molecular disorder in the templated molecular layer.\nThis conclusion is supported by an analysis based on a multi-oscillator model.\nWe further use photoluminescence microspectroscopy to demonstrate that the\nlight-matter coupling is spatially homogeneous. Our study introduces molecular\ntemplating to strong light-matter studies. The reduced disorder of the system\nas a consequence of templating is highly beneficial for engineering\nlight-matter interaction.",
        "The rise of Agentic applications and automation in the Voice AI industry has\nled to an increased reliance on Large Language Models (LLMs) to navigate\ngraph-based logic workflows composed of nodes and edges. However, existing\nmethods face challenges such as alignment errors in complex workflows and\nhallucinations caused by excessive context size. To address these limitations,\nwe introduce the Performant Agentic Framework (PAF), a novel system that\nassists LLMs in selecting appropriate nodes and executing actions in order when\ntraversing complex graphs. PAF combines LLM-based reasoning with a\nmathematically grounded vector scoring mechanism, achieving both higher\naccuracy and reduced latency. Our approach dynamically balances strict\nadherence to predefined paths with flexible node jumps to handle various user\ninputs efficiently. Experiments demonstrate that PAF significantly outperforms\nbaseline methods, paving the way for scalable, real-time Conversational AI\nsystems in complex business environments.",
        "We introduce a numerical method for Brownian dynamics with position dependent\ndiffusion tensor which is second order accurate for sampling the invariant\nmeasure while requiring only one force evaluation per timestep. Analysis of the\nsampling bias is performed using the algebraic framework of exotic aromatic\nButcher-series. Numerical experiments confirm the theoretical order of\nconvergence and illustrate the efficiency of the new method.",
        "We propose a black hole microstate counting method based on the canonical\nquantization of the asymptotic symmetries of a two-dimensional dilaton-gravity\nsystem at future null infinity. This dilaton-gravity is obtained from the\ns-wave reduction of an $N$-dimensional Einstein gravity over a generic\nasymptotically flat black hole solution. We show that a Cardy-type formula\nleads to the Bekenstein-Hawking entropy. In this scenario, the quantized bulk\ndegrees of freedom act as a source for the dual field theory operators living\nat future null infinity that are soft gravitons of the higher dimensional\nsystem, up to some gauge fixing.",
        "Neural tangent kernels (NTKs) have been proposed to study the behavior of\ntrained neural networks from the perspective of Gaussian processes. An\nimportant result in this body of work is the theorem of equivalence between a\ntrained neural network and kernel regression with the corresponding NTK. This\ntheorem allows for an interpretation of neural networks as special cases of\nkernel regression. However, does this theorem of equivalence hold in practice?\n  In this paper, we revisit the derivation of the NTK rigorously and conduct\nnumerical experiments to evaluate this equivalence theorem. We observe that\nadding a layer to a neural network and the corresponding updated NTK do not\nyield matching changes in the predictor error. Furthermore, we observe that\nkernel regression with a Gaussian process kernel in the literature that does\nnot account for neural network training produces prediction errors very close\nto that of kernel regression with NTKs. These observations suggest the\nequivalence theorem does not hold well in practice and puts into question\nwhether neural tangent kernels adequately address the training process of\nneural networks.",
        "In this paper, we investigate the optical appearance of a charged black hole\nin the Kalb-Ramond background, incorporating a Lorentz-violating parameter\n$l=0.01$. By analyzing the null geodesics, we derive the photon sphere, event\nhorizon, effective potential, and critical impact parameters. We then employ a\nray-tracing technique to study the trajectories of photons surrounding a thin\naccretion disk. Three different emission models are considered to explore the\nobserved intensity profiles of direct rings, lensing rings, and photon sphere.\nBy comparing these results with those of the standard Reissner-Nordstr\\\"om\nblack hole ($l=0$) and the Kalb-Ramond black hole with different values of\nLorentz-violating parameter (specifically, $l=0.05$ and $l=0.1$ respectively),\nwe find that the Lorentz symmetry breaking will lead to a decrease in the radii\nof the photon sphere, the event horizon, and the innermost stable circular\norbit. Consequently, this makes the detection of these black holes more\nchallenging.",
        "The kagome lattice has garnered significant attention due to its ability to\nhost quantum spin Fermi liquid states. Recently, the combination of unique\nlattice geometry, electron-electron correlations, and adjustable magnetism in\nsolid kagome materials has led to the discovery of numerous fascinating quantum\nproperties. These include unconventional superconductivity, charge and spin\ndensity waves (CDW\/SDW), pair density waves (PDW), and Chern insulator phases.\nThese emergent states are closely associated with the distinctive\ncharacteristics of the kagome lattice's electronic structure, such as van Hove\nsingularities, Dirac fermions, and flat bands, which can exhibit exotic\nquasi-particle excitations under different symmetries and magnetic conditions.\nRecently, various quantum kagome materials have been developed, typically\nconsisting of kagome layers stacked along the $z$-axis with atoms either\nfilling the geometric centers of the kagome lattice or embedded between the\nlayers. In this topical review, we begin by introducing the fundamental\nproperties of several kagome materials. To gain an in-depth understanding of\nthe relationship between topology and correlation, we then discuss the complex\nphenomena observed in these systems. These include the simplest kagome metal\n$T_3X$, kagome intercalation metal $TX$, and the ternary compounds $AT_6X_6$\nand $RT_3X_5$ ($A$ = Li, Mg, Ca, or rare earth; $T$ = V, Cr, Mn, Fe, Co, Ni;\n$X$ = Sn, Ge; $R$ = K, Rb, Cs). Finally, we provide a perspective on future\nexperimental work in this field.",
        "Metallic antiferromagnets are essential for efficient spintronic applications\ndue to their fast switching and high mobility, yet room-temperature metallic\nantiferromagnets are rare. Here, we investigate YbMn$_2$Ge$_2$, a room\ntemperature antiferromagnet, and establish it as an exfoliable layered metal\nwith altermagnetic surface states. Using multi-orbital Hubbard model\ncalculations, we reveal that its robust metallic AFM ordering is stabilized by\nelectronic correlations and a partially nested Fermi surface. Furthermore, we\nshow that YbMn$_2$Ge$_2$ hosts symmetry-protected topological Dirac crossings,\nconnecting unique even-order spin-polarized surface states with parabolic and\ninverted Mexican-hat-like dispersion. Our findings position YbMn$_2$Ge$_2$ as a\npromising platform for exploring the interplay of correlation, topology, and\nsurface altermagnetism of layered antiferromagnets.",
        "In this paper, we study isometries of $p$-Wasserstein spaces. In our first\nresult, for every complete and separable metric space $X$ and for every\n$p\\geq1$, we construct a metric space $Y$ such that $X$ embeds isometrically\ninto $Y$, and the $p$-Wasserstein space over $Y$ admits mass-splitting\nisometries. Our second result is about embeddings into rigid constructions. We\nshow that any complete and separable metric space $X$ can be embedded\nisometrically into a metric space $Y$ such that the $1$-Wasserstein space is\nisometrically rigid.",
        "Online learning to rank sequentially recommends a small list of items to\nusers from a large candidate set and receives the users' click feedback. In\nmany real-world scenarios, users browse the recommended list in order and click\nthe first attractive item without checking the rest. Such behaviors are usually\nformulated as the cascade model. Many recent works study algorithms for\ncascading bandits, an online learning to rank framework in the cascade model.\nHowever, the performance of existing methods may drop significantly if part of\nthe user feedback is adversarially corrupted (e.g., click fraud). In this work,\nwe study how to resist adversarial corruptions in cascading bandits. We first\nformulate the ``\\textit{Cascading Bandits with Adversarial Corruptions}\" (CBAC)\nproblem, which assumes that there is an adaptive adversary that may manipulate\nthe user feedback. Then we propose two robust algorithms for this problem,\nwhich assume the corruption level is known and agnostic, respectively. We show\nthat both algorithms can achieve logarithmic regret when the algorithm is not\nunder attack, and the regret increases linearly with the corruption level. The\nexperimental results also verify the robustness of our methods.",
        "In this paper, we are trying to show the uniqueness of transonic shock\nsolutions in an expanding nozzle under certain conditions and assumptions on\nthe boundary data and the shock solution. The idea is to compare two transonic\nshock solutions and show that they should coincide if the perturbation of the\nnozzle is sufficiently small. To this end, a condition on the pressure of the\nflow across the shock front is proposed, such that a priori estimates for the\nsubsonic flow behind the shock front could be established without the\nassumption that it is a small perturbation of the unperturbed uniform subsonic\nstate. With the help of these estimates, the uniqueness of the position of the\nintersection point between the shock front and the nozzle boundary could be\nfurther established by demonstrating the monotonicity of the solvability\ncondition for the elliptic sub-problem of the subsonic flow behind the shock\nfront. Then, via contraction arguments, two transonic shock solutions could be\nverified to coincide as the perturbation is small, which leads to the\nuniqueness of the transonic shock solution.",
        "The motion of the solar system against an isotropic radiation background,\nsuch as the cosmic microwave background, induces a dipole anisotropy in the\nbackground due to the Doppler effect. Flux-limited observation of the continuum\nradiation from galaxies also has been studied extensively to show a dipole\nanisotropy due to the Doppler effect and the aberration effect. We show that a\nsimilar dipole anisotropy exists in spectral-line intensity maps, represented\nas either galaxy number counts or the diffuse intensity maps. The amplitude of\nthese dipole anisotropies is determined by not only the solar velocity against\nthe large-scale structures but also the temporal evolution of the monopole\n(sky-average) component. Measuring the dipole at multiple frequencies, which\nhave mutually independent origins due to their occurrence from multiple\nredshifts, can provide a very accurate measure of the solar velocity thanks to\nthe redundant information. We find that such a measurement can even constrain\nastrophysical parameters in the nearby universe. We explore the potential for\ndipole measurement of existing and upcoming surveys, and conclude that the\nspectral number count of galaxies through SPHEREx will be optimal for the first\nmeasurement of the dipole anisotropy in the spectral-line galaxy distribution.\nLIM surveys with reasonable accuracy are also found to be promising. We also\ndiscuss whether these experiments might reveal a peculiar nature of our local\nuniverse, that seems to call for a non-standard cosmology other than the simple\nLambdaCDM model as suggested by recent measures of the baryon acoustic\noscillation signatures and the Alcock-Paczynski tests.",
        "We have observed the $^{13}$CH$_3$OH $5_1-4_1$ A$^+$, $^{13}$CH$_3$OH\n$14_1-13_2$ A$^-$, and CH$_2$DOH $8_{2,6}-8_{1,7}$ $e_0$ lines toward 24\nhigh-mass star-forming regions by using Atacama Large Millimeter\/submillimeter\nArray (ALMA) with an angular resolution of about 0$^{\\prime\\prime}$.3. This\nresolution corresponds to a linear scale of 400-1600 au, allowing us to resolve\nindividual cores properly. We detected the $^{13}$CH$_3$OH and CH$_2$DOH\nemission near the continuum peaks in many of these regions. From the two\n$^{13}$CH$_3$OH lines, we calculated the temperature toward the $^{13}$CH$_3$OH\npeaks, and confirm that the emission traces hot ($>$100 K) regions. The\n$N$(CH$_2$DOH)\/$N$($^{12}$CH$_3$OH) ratio in the observed high-mass\nstar-forming regions is found to be lower than that in low-mass star-forming\nregions. We have found no correlation between the\n$N$(CH$_2$DOH)\/$N$($^{13}$CH$_3$OH) or $N$(CH$_2$DOH)\/$N$($^{12}$CH$_3$OH)\nratios and either temperatures or distance to the sources, and have also found\na source-to-source variation in these ratios. Our model calculations predict\nthat the $N$(CH$_2$DOH)\/$N$($^{12}$CH$_3$OH) ratio in hot cores depends on the\nduration of the cold phase; the shorter the cold phase, the lower the deuterium\nfractionation in the hot cores. We have suggested that the lower\n$N$(CH$_2$DOH)\/$N$($^{12}$CH$_3$OH) ratio in high-mass star-forming regions\ncompared to that in low-mass star-forming regions is due to the shorter\nduration of the cold phase and that the diversity in the\n$N$(CH$_2$DOH)\/$N$($^{12}$CH$_3$OH) ratio in high-mass star-forming regions is\ndue to the diversity in the length of the cold prestellar phase, and not the\ntime that the objects have been in the hot core phase.",
        "Asymptotic grand unification models can be constructed in five dimensions\ncompactified on an orbifold. We demonstrate that the parameter space of such\nmodels admit solutions that naturally achieve the baryon asymmetry via\nleptogenesis, these solutions indicating that the scale of the extra dimensions\nis much higher than the TeV scale."
      ]
    }
  },
  {
    "id":2411.16349,
    "research_type":"applied",
    "start_id":"b18",
    "start_title":"Data-driven science and engineering: machine learning, dynamical systems, and control",
    "start_abstract":"\"Data-driven science and engineering: machine learning, dynamical systems, control.\" Contemporary Physics, 60(4), p. 320",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Cerebral aneurysms. New Engl. J. Medicine"
      ],
      "abstract":[
        "Saccular intracranial aneurysms cause substantial morbidity and mortality. Recently, major changes have occurred in the way we think about and treat this disease. This review discusses the percutaneous endovascular treatment of intracranial aneurysms as compared with surgical intervention. The technological advances and supporting research contributing to this important change in practice patterns are reviewed."
      ],
      "categories":[
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "On the hierarchy of plate models for a singularly perturbed multi-well\n  nonlinear elastic energy",
        "Some topological genera and Jacobi forms",
        "A Fresh Perspective on Water Dynamics in Aqueous Salt Solutions",
        "Probing topological matter and fermion dynamics on a neutral-atom\n  quantum computer",
        "Irregularities of distribution and Fourier transforms of\n  multi-dimensional convex bodies",
        "Photonic heat amplifiers based on Anderson insulators",
        "Frequency-noise-insensitive universal control of Kerr-cat qubits",
        "Bilateral Bailey pairs and Rogers-Ramanujan type identities",
        "Entropy Inequalities Constrain Holographic Erasure Correction",
        "From Data to Combinatorial Multivector field Through an\n  Optimization-Based Framework",
        "Model Predictive and Reinforcement Learning Methods for Active Flow\n  Control of an Airfoil with Dual-point Excitation of Plasma Actuators",
        "Bursty acceleration and 3D trajectories of electrons in a solar flare",
        "Test fields and naked singularities: is the second law the cosmic\n  censor?",
        "Lonely passenger problem: the more buses there are, the more lonely\n  passengers there will be",
        "Universal self-gravitating skyrmions",
        "The Deligne Complex for the $B_3$ Artin Group",
        "Odd and even derivations, transposed Poisson superalgebra and 3-Lie\n  superalgebra",
        "A Linear Quantum Coupler for Clean Bosonic Control",
        "Arbitrary control of the flow of light using pseudomagnetic fields in\n  photonic crystals at telecommunication wavelengths",
        "On the decay instability of electron acoustic waves",
        "Prescribed Chern scalar curvature flow on compact Hermitian manifolds\n  with negative Gauduchon degree",
        "The fundamental representation of pricing adjustments",
        "Bayesian mixture modeling using a mixture of finite mixtures with\n  normalized inverse Gaussian weights",
        "Damping Tuning Considering Random Disturbances Adopting Distributionally\n  Robust Optimization",
        "Category-Selective Neurons in Deep Networks: Comparing Purely Visual and\n  Visual-Language Models",
        "Exact Parent Hamiltonians for All Landau Level States in a Half-flux\n  Lattice",
        "Evolution of the near-core rotation frequency of 2,497 intermediate-mass\n  stars from their dominant gravito-inertial mode",
        "A note on uniform continuity of monotone functions",
        "Two in context learning tasks with complex functions"
      ],
      "abstract":[
        "In the celebrated work of Friesecke, James and M\\\"uller '06 the authors\nderive a hierarchy of models for plates by carefully analyzing the\n$\\Gamma$-convergence of the rescaled nonlinear elastic energy. The key\ningredient of their proofs is the rigidity estimate proved in an earlier work\nof theirs. Here we consider the case in which the elastic energy has a\nmulti-well structure: this type of functional arises, for example, in the study\nof solid-solid phase transitions. Since the rigidity estimate fails in the case\nof compatible wells, we follow Alicandro, Dal Maso, Lazzaroni and Palombaro '18\nand add a regularization term to the energy that penalizes jumps from one well\nto another, leading to good compactness properties. In this setting we recover\nthe full hierarchy of plate models with an explicit dependence on the wells.\nFinally, we study the convergence of energy minimizers with suitable external\nforces and full Neumann boundary conditions. To do so, we adapt the definition\nof optimal rotations introduced by Maor, Mora '21.",
        "We revisit and elucidate the $\\widehat{A}$-genus, Hirzebruch's $L$-genus and\nWitten's $W$-genus, cobordism invariants of special classes of manifolds. After\nslight modification, involving Hecke's trick, we find that the\n$\\widehat{A}$-genus and $L$-genus arise directly from Jacobi's theta function.\nIn this way, for every $k\\geq 0,$ we obtain exact formulas for the quasimodular\nexpressions of $\\widehat{A}_k$ and $L_k$ as ``traces'' of partition Eisenstein\nseries \\[\\widehat{\\mathcal{A}}_k(\\tau)=\n\\operatorname{Tr}_k(\\phi_{\\widehat{A}};\\tau)\\ \\ \\ \\ \\ \\ {\\text {\\rm and}}\\ \\ \\\n\\ \\ \\ \\mathcal{L}_k(\\tau)= \\operatorname{Tr}_k(\\phi_L;\\tau). \\] Surprisingly,\nRamanujan defined twists of the $\\widehat{\\mathcal{A}}_k(\\tau)$ in his ``lost\nnotebook'' in his study of derivatives of theta functions, decades before Borel\nand Hirzebruch rediscovered them in the context of spin manifolds. In addition,\nwe show that the nonholomorphic $G_2^{\\star}$-completion of the characteristic\nseries of the Witten genus is the Jacobi theta function avatar of the\n$\\widehat{A}$-genus.",
        "Molecular dynamics in pure water and aqueous salt solutions remain\nincompletely understood, partly due to the apparent contradictions between\nresults from different spectroscopic techniques. In this work, we demonstrate,\nby detailed comparison of light scattering and dielectric spectroscopy data for\npure water and aqueous lithium chloride solutions, that these apparent\ncontradictions can be resolved by accounting for orientational\ncross-correlations of neighboring molecules. Remarkably, a single structural\nrelaxation mode with largely temperature- and concentration-independent shape\ncan be identified in all spectra, from room temperature down to the deeply\nsupercooled regime. These results provide a new perspective for the study of\nmolecular dynamics in aqueous salt solutions.",
        "Quantum simulations of many-body systems are among the most promising\napplications of quantum computers. In particular, models based on\nstrongly-correlated fermions are central to our understanding of quantum\nchemistry and materials problems, and can lead to exotic, topological phases of\nmatter. However, due to the non-local nature of fermions, such models are\nchallenging to simulate with qubit devices. Here we realize a digital quantum\nsimulation architecture for two-dimensional fermionic systems based on\nreconfigurable atom arrays. We utilize a fermion-to-qubit mapping based on\nKitaev's model on a honeycomb lattice, in which fermionic statistics are\nencoded using long-range entangled states. We prepare these states efficiently\nusing measurement and feedforward, realize subsequent fermionic evolution\nthrough Floquet engineering with tunable entangling gates interspersed with\natom rearrangement, and improve results with built-in error detection.\nLeveraging this fermion description of the Kitaev spin model, we efficiently\nprepare topological states across its complex phase diagram and verify the\nnon-Abelian spin liquid phase by evaluating an odd Chern number. We further\nexplore this two-dimensional fermion system by realizing tunable dynamics and\ndirectly probing fermion exchange statistics. Finally, we simulate strong\ninteractions and study dynamics of the Fermi-Hubbard model on a square lattice.\nThese results pave the way for digital quantum simulations of complex fermionic\nsystems for materials science, chemistry, and high-energy physics.",
        "W. Schmidt, H. Montgomery, and J. Beck proved a result on irregularities of\ndistribution with respect to $d$-dimensional balls. In this paper, we extend\ntheir result to any $d$-dimensional convex body with a smooth boundary and\nfinite order of contact. As an intermediate step, we prove a geometric\ninequality for the Fourier transform of the characteristic function of a convex\nbody.",
        "A photonic heat amplifier (PHA) designed for cryogenic operations is\nintroduced and analyzed. This device comprises two Anderson insulator\nreservoirs connected by lossless lines, allowing them to exchange heat through\nphotonic modes. This configuration enables negative differential thermal\nconductance (NDTC), which can be harnessed to amplify thermal signals. To\nachieve this, we maintain one reservoir at a high temperature, serving as the\nsource terminal of a thermal transistor. Concurrently, in the other one, we\nestablish tunnel contacts to metallic reservoirs, which function as the gate\nand drain terminals. With this arrangement, it is possible to control the heat\nflux exchange between the source and drain by adjusting the gate temperature.\nWe present two distinct parameter choices that yield different performances:\nthe first emphasizes modulating the source-drain heat current, while the second\nfocuses on the temperature modulation of the colder Anderson insulator. Lastly,\nwe present a potential design variation in which all electronic reservoirs are\nthermally connected through only photonic modes, allowing interactions between\ndistant elements. The proposal of the PHA addresses the lack of thermal\ntransistors and amplifiers in the mK range while being compatible with the rich\ntoolbox of circuit quantum electrodynamics. It can be adapted to various\napplications, including sensing and developing thermal circuits and control\ndevices at sub-Kelvin temperatures, which are relevant to quantum technologies.",
        "We theoretically study the influence of frequency uncertainties on the\noperation of a Kerr-cat qubit. As the mean photon number increases, Kerr-cat\nqubits provide an increasing level of protection against phase errors induced\nby unknown frequency shifts during idling and X rotations. However, realizing\nrotations about the other principal axes (e.g., Y and Z axes) while preserving\nrobustness is nontrivial. To address this challenge, we propose a universal set\nof gate schemes which circumvents the tradeoff between protection and\ncontrollability in Kerr-cat qubits and retains robustness to unknown frequency\nshifts to at least first order. Assuming an effective Kerr oscillator model, we\ntheoretically and numerically analyze the robustness of elementary gates on\nKerr-cat qubits, with special focus on gates along nontrivial rotation axes. An\nappealing application of this qubit design would include tunable\nsuperconducting platforms, where the induced protection against frequency noise\nwould allow for a more flexible choice of operating point and thus the\npotential mitigation of the impact of spurious two-level systems.",
        "Rogers-Ramanujan type identities occur in various branches of mathematics and\nphysics. As a classic and powerful tool to deal with Rogers-Ramanujan type\nidentities, the theory of Bailey's lemma has been extensively studied and\ngeneralized. In this paper, we found a bilateral Bailey pair that naturally\narises from the q-binomial theorem. By applying the bilateral versions of\nBailey lemmas, Bailey chains and Bailey lattices, we derive a number of\nRogers-Ramanujan type identities, which unify many known identities as special\ncases. Further combined with the bilateral Bailey chains due to Berkovich,\nMcCoy and Schilling and the bilateral Bailey lattices due to Jouhet et al., we\nalso obtain identities on Appell-Lerch series and identities of Andrews-Gordon\ntype. Moreover, by applying Andrews and Warnaar's bilateral Bailey lemmas, we\nderive identities on Hecke-type series.",
        "We interpret holographic entropy inequalities in terms of erasure correction.\nThe non-saturation of an inequality is a necessary condition for certain\nschemes of holographic erasure correction, manifested in the bulk as non-empty\noverlaps of corresponding entanglement wedges.",
        "This paper extends and generalizes previous works on constructing\ncombinatorial multivector fields from continuous systems (see [10]) and the\nconstruction of combinatorial vector fields from data (see [2]) by introducing\nan optimization based framework for the construction of combinatorial\nmultivector fields from finite vector field data. We address key challenges in\nconvexity, computational complexity and resolution, providing theoretical\nguarantees and practical methodologies for generating combinatorial\nrepresentation of the dynamics of our data.",
        "This paper presents an analysis of Model Predictive Control (MPC) and\nReinforcement Learning (RL) approaches for active flow control over a NACA\n(National Advisory Committee for Aeronautics) 4412 airfoil around the static\nstall condition at a Reynolds number of 4*10^5. The Reynolds Averaged\nNavier-Stokes (RANS) equations with the Scale-Adaptive Simulation (SAS)\nturbulence model are utilized for the numerical simulations. The dielectric\nbarrier discharge (DBD) plasma actuators were employed in dual excitation mode\nfor flow separation control. The study systematically evaluates adaptive MPC,\ntemporal difference reinforcement learning (TDRL), and deep Q-learning (DQL)\nbased on optimizing the excitation frequency and expediting the time to\nidentify stable conditions. Moreover, an integrated approach that combines\nsignal processing with DQL is examined. The results demonstrate that while MPC\nand RL significantly improve flow control, RL approaches offer superior\nadaptability and performance. In optimal conditions, a lift coefficient of\naround 1.619 was achieved within less than 2.5 seconds with an excitation\nfrequency of 100 or 200 Hz. This research highlights that RL-based approaches\ncould perform better in flow control applications than MPC.",
        "During a solar flare, electrons are accelerated to non-thermal energies as a\nresult of magnetic reconnection. These electrons then propagate upwards and\ndownwards from the energy release site along magnetic field lines and produce\nradio and X-ray emission. On 11 November 2022, an M5.1 solar flare was observed\nby the Spectrometer\/Telescope for Imaging X-rays (STIX) on board Solar Orbiter\ntogether with various ground- and space-based radio instruments. The flare was\nassociated with several fine hard X-ray (HXR) structures and a complex set of\nmetric radio bursts (type III, J, and narrowband). By studying the evolution of\nX-ray, extreme ultraviolet, and radio sources, we aim to study the trajectories\nof the flare-accelerated electrons in the lower solar atmosphere and low\ncorona. We used observations from the STIX on board Solar Orbiter to study the\nevolution of X-ray sources. Using radio imaging from the Nan\\c{c}ay Radio\nheliograph (NRH) and the Newkirk density model, we constructed 3D trajectories\nof 14 radio bursts. Imaging of the HXR fine structures shows several sources at\ndifferent times. The STIX and NRH imaging shows correlated changes in the\nlocation of the HXR and radio source at the highest frequency during the most\nintense impulsive period. Imaging and 3D trajectories of all the bursts show\nthat electrons are getting accelerated at different locations and along several\ndistinct field lines. The longitude and latitude extent of the trajectories are\n~30 arcsec and ~ 152 arcsec. We find that the electrons producing HXR and radio\nemission have similar acceleration origins. Importantly, our study supports the\nscenario that the flare acceleration process is temporally and spatially\nfragmentary, and during each of these small-scale processes, the electron beams\nare injected into a very fibrous environment and produce complex HXR and radio\nemission.",
        "It has been claimed that a Kerr-Newman black hole can generically be overspun\nby neutral test fields, and it has been argued that even when backreactions are\ntaken into account, the black hole can still be destroyed. In this paper, we\nrevisit the weak cosmic censorship conjecture for a Kerr-Newman black hole with\na test scalar field and a neutrino field, and point out that the assumption in\nprevious work regarding the energy and angular momentum of the test fields\nabsorbed by the black hole violates the second law of black hole\nthermodynamics. By solving the test scalar field and neutrino field near the\nevent horizon explicitly, we demonstrate that an extremal Kerr-Newman black\nhole cannot be overspun by a test scalar field but can be destroyed by a\nneutrino field. Our results indicate that the condition required to overspin an\nextremal Kerr-Newman black hole coincides with the condition needed to violate\nthe second law of black hole thermodynamics. Furthermore, we observe that such\na violation of the second law might inevitably result in a breakdown of the\nweak cosmic censorship conjecture.",
        "Empty buses are standing at a bus station. $n$ passengers arrive, and they\neach board a bus completely at random (meaning that they choose uniformly and\nindependently). Then all buses depart. We show that the more buses there are,\nthe more likely it is that someone (i.e. at least one passenger) travels alone\n(while $n$ is fixed). More generally, we show that the number of lonely\npassengers increases with the number of buses, in the sense of stochastic\ndominance. This problem turned out to be surprisingly difficult, with no short\nsolution known to the author so far, despite the efforts of many experts. Some\nof the results can also be formulated as properties of Stirling numbers of the\nsecond kind.",
        "The self-gravitating skyrmion is an exact solution of the Einstein\n$SU(2)$-Skyrme model describing a topological soliton with baryon number $B=1$,\nliving in a $4$-dimensional space-time in the presence of a cosmological\nconstant. Here we show that, using the maximal embedding Ansatz of $SU(2)$ into\n$SU(N)$ in the Euler angles parametrization, this solution can be generalized\nto include arbitrary values of the flavor number and, consequently, allowing\nhigher values of the topological charge. Also, we show that higher-order\ncorrections in the 't Hooft expansion can be considered while still preserving\nthe analytical nature of the solutions. Finally we will show that from the\ngravitational solutions it is possible to construct skyrmions in flat\nspace-time at a finite volume.",
        "We show that the piecewise Euclidean Moussong metric on the Deligne complex\nof the Artin group of type $B_3$ is $\\mathrm{CAT}(0)$. We do this by\nestablishing a criteria for a complex made of $B_3$ simplices to be\n$\\mathrm{CAT}(1)$ in terms of embedded edge paths, which in particular applies\nto the spherical Deligne complex of type $B_3$. This provides one more step to\nshowing that the Moussong metric is $\\mathrm{CAT}(0)$ for any 3-dimensional\nArtin group.",
        "One important example of a transposed Poisson algebra can be constructed by\nmeans of a commutative algebra and its derivation. This approach can be\nextended to superalgebras, that is, one can construct a transposed Poisson\nsuperalgebra given a commutative superalgebra and its even derivation. In this\npaper we show that including odd derivations in the framework of this approach\nrequires introducing a new notion. It is a super vector space with two\noperations that satisfy the compatibility condition of transposed Poisson\nsuperalgebra. The first operation is determined by a left supermodule over\ncommutative superalgebra and the second is a Jordan bracket. Then it is proved\nthat the super vector space generated by an odd derivation of a commutative\nsuperalgebra satisfies all the requirements of introduced notion. We also show\nhow to construct a 3-Lie superalgebra if we are given a transposed Poisson\nsuperalgebra and its even derivation.",
        "Quantum computing with superconducting circuits relies on high-fidelity\ndriven nonlinear processes. An ideal quantum nonlinearity would selectively\nactivate desired coherent processes at high strength, without activating\nparasitic mixing products or introducing additional decoherence. The wide\nbandwidth of the Josephson nonlinearity makes this difficult, with undesired\ndrive-induced transitions and decoherence limiting qubit readout, gates,\ncouplers and amplifiers. Significant strides have been recently made into\nbuilding better `quantum mixers', with promise being shown by Kerr-free\nthree-wave mixers that suppress driven frequency shifts, and balanced quantum\nmixers that explicitly forbid a significant fraction of parasitic processes. We\npropose a novel mixer that combines both these strengths, with engineered\nselection rules that make it essentially linear (not just Kerr-free) when idle,\nand activate clean parametric processes even when driven at high strength.\nFurther, its ideal Hamiltonian is simple to analyze analytically, and we show\nthat this ideal behavior is first-order insensitive to dominant experimental\nimperfections. We expect this mixer to allow significant advances in high-Q\ncontrol, readout, and amplification.",
        "In photonics, the idea of controlling light in a similar way that magnetic\nfields control electrons has always been attractive. It can be realized by\nsynthesizing pseudomagnetic fields (PMFs) in photonic crystals (PhCs). Previous\nworks mainly focus on the Landau levels and the robust transport of the chiral\nstates. More versatile control over light using complex nonuniform PMFs such as\nthe flexible splitting and routing of light has been elusive, which hinders\ntheir application in practical photonic integrated circuits. Here we propose an\nuniversal and systematic methodology to design nonuniform PMFs and arbitrarily\ncontrol the flow of light in silicon PhCs at telecommunication wavelengths. As\nproofs of concept, a low-loss S-bend and a highly efficient 50:50 power\nsplitter based on PMFs are experimentally demonstrated. A high-speed data\ntransmission experiment is performed on these devices to prove their\napplicability in real communication systems. The proposed method offers a new\nparadigm for the exploration of fundamental physics and the development of\nnovel nanophotonic devices.",
        "Electron acoustic waves (EAWs) are nonlinear plasma modes characterized by\nelectron trapping, which suppresses the usual Landau damping. Despite being\npredicted in the 1990s, their excitation and decay mechanisms remain a subject\nof active research. This study investigates the nonlinear dynamics of EAWs,\nfocusing on their excitation, decay instability, and the role of vortex merging\nin phase space. Using Eulerian Vlasov-Poisson simulations, we reproduce the\nexcitation of stable EAWs via an external resonant driving force and explore\ntheir decay under the effect of a low-amplitude perturbation. The study\nidentifies a distinct 2 to 1 decay process, where an EAW with a shorter\nwavelength merges into a longer-wavelength mode, driven by vortex dynamics in\nphase space. We find that the instability is triggered by a small fraction of\nparticles capable of transitioning between potential wells, facilitating energy\nexchange between two adjacent phase space holes and vortex merging. Our\nsimulations highlight the chaotic nature of particle trajectories in the\nvicinity of the separatrix between trapped and free phase space regions, which\nsignificantly contributes to the instability growth. Additionally, we analyze\nthe influence of the perturbation amplitude on the growth rate of the\ninstability, shedding light on the critical role of phase-space dynamics in the\ndecay process. These findings offer a deeper understanding of the nonlinear\nbehavior of plasma waves and suggest future directions for studying plasma wave\nstability in more complex systems, as the decay mechanism discussed here is\nlikely to be universal in plasmas with closed separatrices in phase space,\nunderscoring its significance in nonlinear plasma dynamics.",
        "In this paper, we present a unified flow approach to prescribed Chern scalar\ncurvature problem on compact Hermitian manifolds with negative Gauduchon\ndegree. When the conformal class of its Hermitian metric contains a balanced\nmetric, we give some sufficient conditions on the candidate curvature function\n$f$ which guaranties the convergence of the flow to a conformal Hermitian\nmetric whose Chern scalar curvature is $f$.",
        "This article consolidates and extends past work on derivative pricing\nadjustments, including XVA, by providing an encapsulating representation of the\nadjustment between any two derivative pricing functions, within an Ito\nSDE\/parabolic PDE framework. We give examples of this representation\nencapsulating others from the past 20 years, ranging from a well known option\npricing adjustment introduced by Gatheral, to the collection of\nsemi-replication XVA originating from Burgard & Kjaer. To highlight extensions,\nwe discuss certain meta-adjustments beyond XVA, designed to help signal and\nmitigate XVA model risk.",
        "In Bayesian inference for mixture models with an unknown number of\ncomponents, a finite mixture model is usually employed that assumes prior\ndistributions for mixing weights and the number of components. This model is\ncalled a mixture of finite mixtures (MFM). As a prior distribution for the\nweights, a (symmetric) Dirichlet distribution is widely used for conjugacy and\ncomputational simplicity, while the selection of the concentration parameter\ninfluences the estimate of the number of components. In this paper, we focus on\nestimating the number of components. As a robust alternative Dirichlet weights,\nwe present a method based on a mixture of finite mixtures with normalized\ninverse Gaussian weights. The motivation is similar to the use of normalized\ninverse Gaussian processes instead of Dirichlet processes for infinite mixture\nmodeling. Introducing latent variables, the posterior computation is carried\nout using block Gibbs sampling without using the reversible jump algorithm. The\nperformance of the proposed method is illustrated through some numerical\nexperiments and real data examples, including clustering, density estimation,\nand community detection.",
        "In scenarios where high penetration of renewable energy sources (RES) is\nconnected to the grid over long distances, the output of RES exhibits\nsignificant fluctuations, making it difficult to accurately characterize. The\nintermittency and uncertainty of these fluctuations pose challenges to the\nstability of the power system. This paper proposes a distributionally robust\ndamping optimization control framework (DRDOC) to address the uncertainty in\nthe true distribution of random disturbances caused by RES. First, the\ninstallation location of damping controllers and key control parameters are\ndetermined through Sobol sensitivity indices and participation factors. Next, a\nnonlinear relationship between damping and random disturbances is established\nwith Polynomial Chaos Expansion (PCE). The uncertainty in the distribution of\ndisturbances is captured by ambiguity sets. The DRDOC is formulated as a convex\noptimization problem, which is further simplified for efficient computation.\nFinally, the optimal control parameters are derived through convex optimization\ntechniques. Simulation results demonstrate the effectiveness and distribution\nrobustness of the proposed DRDOC.",
        "Category-selective regions in the human brain, such as the fusiform face area\n(FFA), extrastriate body area (EBA), parahippocampal place area (PPA), and\nvisual word form area (VWFA), play a crucial role in high-level visual\nprocessing. Here, we investigate whether artificial neural networks (ANNs)\nexhibit similar category-selective neurons and how these neurons vary across\nmodel layers and between purely visual and vision-language models. Inspired by\nfMRI functional localizer experiments, we presented images from different\ncategories (faces, bodies, scenes, words, scrambled scenes, and scrambled\nwords) to deep networks and identified category-selective neurons using\nstatistical criteria. Comparing ResNet and the structurally controlled\nResNet-based CLIP model, we found that both models contain category-selective\nneurons, with their proportion increasing across layers, mirroring category\nselectivity in higher-level visual brain regions. However, CLIP exhibited a\nhigher proportion but lower specificity of category-selective neurons compared\nto ResNet. Additionally, CLIP's category-selective neurons were more evenly\ndistributed across feature maps and demonstrated greater representational\nconsistency across layers. These findings suggest that language learning\nincreases the number of category-selective neurons while reducing their\nselectivity strength, reshaping visual representations in deep networks. Our\nstudy provides insights into how ANNs mirror biological vision and how\nmultimodal learning influences category-selective representations.",
        "Realizing topological flat bands with tailored single-particle Hilbert spaces\nis a critical step toward exploring many-body phases, such as those featuring\nanyonic excitations. One prominent example is the Kapit-Mueller model, a\nvariant of the Harper-Hofstadter model that stabilizes lattice analogs of the\nlowest Landau level states. The Kapit-Mueller model is constructed based on the\nPoisson summation rule, an exact lattice sum rule for coherent states. In this\nwork, we consider higher Landau-level generalizations of the Poisson summation\nrule, from which we derive families of parent Hamiltonians on a half-flux\nlattice which have exact flat bands whose flatband wavefunctions are lattice\nversion of higher Landau level states. Focusing on generic Bravais lattices\nwith only translation and inversion symmetries, we discuss how these symmetries\nenforced gaplessness and singular points for odd Landau level series, and how\nto achieve fully gapped parent Hamiltonians by mixing even and odd series. Our\nmodel points to a large class of tight-binding models with suitable energetic\nand quantum geometries that are potentially useful for realizing non-Abelian\nfractionalized states when interactions are included. The model exhibits fast\ndecay hopping amplitudes, making it potentially realizable with neutral atoms\nin optical lattices.",
        "We combined Gaia DR3 and TESS photometric light curves to estimate the\ninternal physical properties of 2,497 gravity-mode pulsators. We relied on\nasteroseismic properties of Kepler $\\gamma\\,$Dor and SPB stars to derive the\nnear-core rotation frequency, $f_{\\rm rot}$, of the Gaia-discovered pulsators\nfrom their dominant prograde dipole gravito-inertial pulsation mode. We offer a\nrecipe based on linear regression to deduce $f_{\\rm rot}$ from the dominant\ngravito-inertial mode frequency. It is applicable to prograde dipole modes with\nan amplitude above 4mmag and occurring in the sub-inertial regime. By applying\nit to the 2,497 pulsators, we have increased the sample of intermediate-mass\ndwarfs with such an asteroseismic observable by a factor of 4. We used the\nestimate of $f_{\\rm rot}$ to deduce spin parameters between 2 and 6, while the\nsample's near-core rotation rates range from 0.7% to 25% of the critical\nKeplerian rate. We used $f_{\\rm rot}$, along with the Gaia effective\ntemperature and luminosity to deduce the (convective core) mass, radius, and\nevolutionary stage from grid modelling based on rotating stellar models. We\nderived a decline of $f_{\\rm rot}$ with a factor of 2 during the main-sequence\nevolution for this population of field stars, which covers a mass range from\n1.3M$_\\odot$ to 7M$_\\odot$. We found observational evidence for an increase in\nthe radial order of excited gravity modes as the stars evolve. For 969\npulsators, we derived an upper limit of the radial differential rotation\nbetween the convective core boundary and the surface from Gaia's vbroad\nmeasurement and found values up to 5.4. Our recipe for the near-core rotation\nfrequency from the dominant gravito-inertial mode detected in the independent\nGaia and TESS light curves is easy to use, facilitates applications to large\nsamples, and allows to map their angular momentum and evolutionary stage in the\nMilky Way.",
        "We prove that it is consistent with ZFC that for every non-decreasing\nfunction $f:[0,1]\\to [0,1]$, each subset of $[0,1]$ of cardinality $\\mathfrak\nc$ contains a set of cardinality $\\mathfrak c$ on which $f$ is uniformly\ncontinuous. We show that this statement follows from the assumptions that\n$\\mathfrak d^* < \\mathfrak c$ and $\\mathfrak c$ is regular, where $\\mathfrak\nd^*\\leq \\mathfrak d$ is the smallest cardinality $\\kappa$ such that any two\ndisjoint countable dense sets in the Cantor set can be separated by sets each\nof which is an intersection of at most $\\kappa$-many open sets in the Cantor\nset. We establish also that $\\mathfrak d^*=\\min\\{\\mathfrak u, \\mathfrak\nd\\}=\\min\\{\\mathfrak r, \\mathfrak d\\}$, thus giving an alternative proof of the\nlatter equality established by J. Aubrey in 2004.",
        "We examine two in context learning (ICL) tasks with mathematical functions in\nseveral train and test settings for transformer models. Our study generalizes\nwork on linear functions by showing that small transformers, even models with\nattention layers only, can approximate arbitrary polynomial functions and hence\ncontinuous functions under certain conditions. Our models also can approximate\npreviously unseen classes of polynomial functions, as well as the zeros of\ncomplex functions. Our models perform far better on this task than LLMs like\nGPT4 and involve complex reasoning when provided with suitable training data\nand methods. Our models also have important limitations; they fail to\ngeneralize outside of training distributions and so don't learn class forms of\nfunctions. We explain why this is so."
      ]
    }
  },
  {
    "id":2411.19,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Addressing disparities in the global epidemiology of stroke",
    "start_abstract":"Stroke is the second leading cause of death and the third leading cause of disability worldwide. Though the burden of stroke worldwide seems to have declined in the past three decades, much of this effect reflects decreases in high-income countries (HICs). By contrast, the burden of stroke has grown rapidly in low-income and middle-income countries (LMICs), where epidemiological, socioeconomic and demographic shifts have increased the incidence of stroke and other non-communicable diseases. Furthermore, even in HICs, disparities in stroke epidemiology exist along racial, ethnic, socioeconomic and geographical lines. In this Review, we highlight the under-acknowledged disparities in the burden of stroke. We emphasize the shifting global landscape of stroke risk factors, critical gaps in stroke service delivery, and the need for a more granular analysis of the burden of stroke within and between LMICs and HICs to guide context-appropriate capacity-building. Finally, we review strategies for addressing key inequalities in stroke epidemiology, including improvements in epidemiological surveillance and context-specific research efforts in under-resourced regions, development of the global workforce of stroke care providers, expansion of access to preventive and treatment services through mobile and telehealth platforms, and scaling up of evidence-based strategies and policies that target local, national, regional and global stroke disparities.",
    "start_categories":[
      "q-bio.NC"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b7"
      ],
      "title":[
        "Artificial intelligence applications in stroke"
      ],
      "abstract":[
        "Management of stroke highly depends on information from imaging studies. Noncontrast computed tomography (CT) and magnetic resonance imaging (MRI) can both be used to distinguish between ischemic and hemorrhagic stroke, which is difficult based on clinical features. Hypodensity on CT and DWI hyperintensity on MRI identifies irreversibly damaged tissue, although the sensitivity of MRI is higher in the acute setting. Angiographic and perfusion imaging sequences can identify a large vessel occlusion and, along with perfusion imaging, can select patients for endovascular therapy. The FLAIR-DWI mismatch yields information about patients with unknown time of onset (including wake-up strokes). Stroke imaging also gives insight into prognosis, with current methods aiming to give a picture of the short-term consequences of successful reperfusion or continued large vessel occlusion. One important caveat about stroke imaging is that it must be done quickly, as faster treatment leads to better outcomes.1 However, most steps in the stroke imaging triage pathway require the presence of human radiologists and neurologists, and this is often the time-limiting step. The expertise required for these tasks may not be available at all sites or at all times. Therefore, there is interest in automated methods for stroke imaging evaluation. Artificial intelligence (AI) is a broad term reflecting the use of computers to perform tasks that humans may find difficult, often in ways that are hard to pinpoint. For example, although humans find high-level computation difficult, calculator technology is not considered AI because we know how to break this down into discrete steps and feel we understand it. However, facial recognition is a task that humans perform well, but an algorithm to identify faces is usually considered AI since we cannot articulate precisely how this is done. Machine learning (ML) is a subset of AI in which algorithms learn from the data itself without explicit programming. ML methods reflect a broad range of statistical techniques ranging from linear regression to more complex methods such as support vector machines and decision trees. ML methods can be further broken into supervised and unsupervised learning, which differ from one another in that the former requires access to gold standard labels although the latter attempts to find the answers implicitly in the data itself. While ML methods have grown more popular over recent years, the advent of a specific supervised ML method based on architectures resembling human neural networks over the past decade has led to a quantum leap in performance.2 This method, called deep learning (DL) because of many multiple internal layers, can be considered a transformative technology. Compared with previous methods that required humans to identify image features, a deep neural network trained on a dataset with known outputs can learn the best features for organizing the data. In this review, we will discuss ML methods applied to stroke imaging with an emphasis on DL applications. We refer to Figure for a graphical overview of the applications discussed in this review."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Complex Brillouin Zone for Localised Modes in Hermitian and\n  Non-Hermitian Problems",
        "Explicit adaptive time stepping for the Cahn-Hilliard equation by\n  exponential Krylov subspace and Chebyshev polynomial methods",
        "On reflected isotropic stable processes",
        "Quantum Quandaries: Unraveling Encoding Vulnerabilities in Quantum\n  Neural Networks",
        "Evaluating Compression and Nanoindentation in FCC Nickel: A Methodology\n  for Interatomic Potential Selection",
        "Integrated Sensing and Communication System Based on Radio Frequency\n  Resonance Beam",
        "Activity of low mass stars in the light of spot signature in the Fourier\n  domain",
        "Impact of structural distortions on the correlated electronic structure\n  of orbital-selective Mott insulating Na$_3$Co$_2$SbO$_6$ under strains",
        "Influence of nanoparticulates and microgrooves on the secondary electron\n  yield and electrical resistance of laser-treated copper surfaces",
        "Accelerated DC loadflow solver for topology optimization",
        "Predicting Spin-Dependent Coulomb Interaction Based on the Yang-Mills\n  Equations",
        "Experimental realization of a quantum heat engine based on\n  dissipation-engineered superconducting circuits",
        "Range-Only Dynamic Output Feedback Controller for Safe and Secure Target\n  Circumnavigation",
        "Quantum Diffie-Hellman key exchange",
        "Criteria for ion acceleration in laboratory magnetized\n  quasi-perpendicular collisionless shocks: when are 2D simulations enough?",
        "Repulsive interatomic potentials calculated at three levels of theory",
        "Language Model Re-rankers are Steered by Lexical Similarities",
        "Parametric instability of ultracold Bose gases with long-range\n  interaction and quantum fluctuations trapped in optical lattices",
        "A heuristic for the deployment of collecting routes for urban recycle\n  stations (eco-points)",
        "AF-KAN: Activation Function-Based Kolmogorov-Arnold Networks for\n  Efficient Representation Learning",
        "On sparsity of integral points in orbits and correspondences with big\n  pullbacks under iterates",
        "Adaptive Score Alignment Learning for Continual Perceptual Quality\n  Assessment of 360-Degree Videos in Virtual Reality",
        "Towards Multi-Stakeholder Evaluation of ML Models: A Crowdsourcing Study\n  on Metric Preferences in Job-matching System",
        "Primordial Black Hole Formation via Inverted Bubble Collapse",
        "Integrating Language Models for Enhanced Network State Monitoring in\n  DRL-Based SFC Provisioning",
        "On the examples of Egyptian fractions in Liber Abaci",
        "New Fashion Products Performance Forecasting: A Survey on Evolutions,\n  Models and Emerging Trends",
        "A Taxonomy for Evaluating Generalist Robot Policies",
        "Skill and spatial mismatches for sustainable development in Brazil"
      ],
      "abstract":[
        "We develop a mathematical and numerical framework for studying evanescent\nwaves in subwavelength band gap materials. By establishing a link between the\ncomplex Brillouin zone and various Hermitian and non-Hermitian phenomena,\nincluding defect localisation in band gap materials and the non-Hermitian skin\neffect, we provide a unified perspective on these systems. In two-dimensional\nstructures, we develop analytical techniques and numerical methods to study\nsingularities of the complex band structure. This way, we demonstrate that gap\nfunctions effectively predict the decay rates of defect states. Furthermore, we\npresent an analysis of the Floquet transform with respect to complex\nquasimomenta. Based on this, we show that evanescent waves may undergo a phase\ntransition, where local oscillations drastically depend on the location of\ncorresponding frequency inside the band gap.",
        "The Cahn-Hilliard equation has been widely employed within various\nmathematical models in physics, chemistry and engineering. Explicit stabilized\ntime stepping methods can be attractive for time integration of the\nCahn-Hilliard equation, especially on parallel and hybrid supercomputers. In\nthis paper, we propose an exponential time integration method for the\nCahn-Hilliard equation and describe its efficient Krylov subspace based\nimplementation. We compare the method to a Chebyshev polynomial local iteration\nmodified (LIM) time stepping scheme. Both methods are explicit (i.e., do not\ninvolve linear system solution) and tested with both constant and adaptively\nchosen time steps.",
        "We build two types of isotropic stable processes reflected in a strongly\nconvex bounded domain $\\mathcal{D}$. In both cases, when the process tries to\njump across the boundary, it is stopped at the unique point where\n$\\partial\\mathcal{D}$ intersects the line segment defined by the attempted\njump. It then leaves the boundary either continuously (for the first type) or\nby a power-law distributed jump (for the second type). The construction of\nthese processes is done via an It\\^o synthesis: we concatenate their excursions\nin the domain, which are obtained by translating, rotating and stopping the\nexcursions of some stable processes reflected in the half-space. The key\ningredient in this procedure is the construction of the boundary processes,\ni.e. the processes time-changed by their local time on the boundary, which\nsolve stochastic differential equations driven by some Poisson measures of\nexcursions. The well-posedness of these boundary processes relies on delicate\nestimates involving some geometric inequalities and the laws of the undershoot\nand overshoot of the excursion when it leaves the domain. After having\nconstructed the processes, we show that they are Markov and Feller, we study\ntheir infinitesimal generator and write down the reflected fractional heat\nequations satisfied by their time-marginals.",
        "Quantum computing (QC) has the potential to revolutionize fields like machine\nlearning, security, and healthcare. Quantum machine learning (QML) has emerged\nas a promising area, enhancing learning algorithms using quantum computers.\nHowever, QML models are lucrative targets due to their high training costs and\nextensive training times. The scarcity of quantum resources and long wait times\nfurther exacerbate the challenge. Additionally, QML providers may rely on third\nparty quantum clouds for hosting models, exposing them and their training data\nto potential threats. As QML as a Service (QMLaaS) becomes more prevalent,\nreliance on third party quantum clouds poses a significant security risk. This\nwork demonstrates that adversaries in quantum cloud environments can exploit\nwhite box access to QML models to infer the users encoding scheme by analyzing\ncircuit transpilation artifacts. The extracted data can be reused for training\nclone models or sold for profit. We validate the proposed attack through\nsimulations, achieving high accuracy in distinguishing between encoding\nschemes. We report that 95% of the time, the encoding can be predicted\ncorrectly. To mitigate this threat, we propose a transient obfuscation layer\nthat masks encoding fingerprints using randomized rotations and entanglement,\nreducing adversarial detection to near random chance 42% , with a depth\noverhead of 8.5% for a 5 layer QNN design.",
        "We performed molecular dynamics simulations to investigate the mechanical\nresponse of face-centered cubic (FCC) nickel under uniaxial compression and\nnanoindentation using traditional interatomic potentials, including the\nEmbedded Atom Method (EAM) and Modified Embedded Atom Method (MEAM). By\ncalculating the generalized stacking fault energy (GSFE), we analyzed the\ndissociated slip paths responsible for stacking fault formation and partial\nShockley dislocations during mechanical loading. Our findings highlight the\ncritical importance of selecting appropriate interatomic potentials to model\ncompression and nanoindentation tests accurately, aligning simulations with\nexperimental observations. We propose a practical methodology for identifying\nempirical interatomic potentials suitable for mechanical testing of\nsingle-element materials. This approach establishes a benchmark for FCC nickel\nsimulations and provides a basis for extending these methods to more complex\nNi-based alloys, facilitating comparisons with experimental results such as\nthose from electron microscopy.",
        "To address the challenge of complex beam control in traditional\nmultiple-input multiple-output (MIMO) systems, research has proposed\nestablishing adaptive beam alignment by utilizing retro-directive antenna (RDA)\narrays to create echo resonance between the base station (BS) and user\nequipment (UE), thereby reducing system computational load. However, in\nconventional resonant beam systems (RBS), the use of the same frequency for\nuplink and downlink inevitably leads to echo interference issues. Therefore,\nthis paper proposes an innovative design for an resonance beam-based integrated\nsensing and communication (RB-ISAC) system to achieve efficient passive sensing\nand bidirectional communication. In this system, the UE does not actively\ntransmit signals but instead relies on a passive phase conjugation and\nfrequency conversion structure to separate the uplink and downlink carrier\nfrequencies. Additionally, through effective compensation for signal\npropagation loss, resonance is achieved after multiple iterations, at which\npoint the beam's field distribution becomes a low-diffraction-loss,\nhigh-focusing pattern, automatically aligning the transmitter with the\nreceiver. This enables high-precision passive positioning while facilitating\nuplink and downlink communication. Simulation results demonstrate that the\nproposed system can achieve resonance after multiple iterations, and can\nsupport uplink and downlink communication within a range of 5 meters while\nachieving passive direction of arrival (DOA) estimation with an error of less\nthan 2 degrees.",
        "Context. Magnetic fields exhibit a wide variety of behaviours in low mass\nstars and further characterization is required to understand these\nobservations. Stellar photometry from space missions such as MOST, CoRoT,\nKepler, and, in future PLATO, provide thousands of highly precise light curves\n(LC) that can shed new light upon stellar activity, in particular through the\nsignature of transiting spots. Aims. We study the impact of star spots on light\ncurves in the Fourier domain, reducing the degeneracies encountered by direct\nspot modelling in the temporal domain, and use this new formulation to explore\nthe spot properties from the available data. Methods. We propose a model of LC\npower spectra at low frequency based on a description of spot transits that\nallows us to retrieve information about the amplitude of their photometric\nimpact $\\mathcal{H}$, and about the spot mean lifetime over the observation\n$\\tau_{\\rm life}$ when the power spectrum exibits rotation peaks. We first\nvalidate this method with simulated LCs and then apply it to the Kepler data to\nextract global trends over a set of more than 37 755 stars. Results. This\nanalysis leads to a classification of the sample into \"peakless\" or \"with\npeaks\" spectra, and enables the identification of different activity regimes\nbased on $\\mathcal{H}$ and $\\tau_{\\rm life}$ for different ranges of Rossby\nnumber. More specifically, we observe an intense regime of activity between Ro\n= 0.7 and Ro = 1, for stars with masses under 1$M_\\odot$. Conclusions. This new\nsystematic method can be used to provide new observational constraints on\nstellar activity (and possibly a link with stellar magnetism) when applied to\nlarge photometric datasets, such as those from the future PLATO mission.",
        "Na$_{3}$Co$_{2}$SbO$_6$ is a promising candidate to realize the Kitaev spin\nliquid phase since the large Kitaev spin exchange interaction is tunable via\nthe change in electronic structure, such as the trigonal crystal field\nsplitting ($\\Delta_{TCF}$). Here, we show that the uncorrelated electronic\nstructure of Na$_{3}$Co$_{2}$SbO$_6$ is rather insensitive to the strain effect\ndue to the low crystal symmetry accompanied by oxygen displacements and the\npresence of Sb $s$ orbitals. This suggests that the Kitaev spin-exchange\ninteraction obtained from perturbation theory also does not depend much on the\nstrain effect. Using density functional theory plus dynamical mean field\ntheory, we find that the correlated electronic structure of\nNa$_{3}$Co$_{2}$SbO$_6$ is an orbital selective Mott insulating state where the\ntrigonal $a_{1g}$ orbital is insulating due to correlation-assisted\nhybridization, while other $d$ orbitals behave as typical Mott insulators,\nresulting in tunability of $\\Delta_{TCF}$ under the strain effect effectively.\nOur results show that the local Co-site symmetry and dynamical correlation\neffects will play an important role in engineering the novel magnetic phase in\nthis and related materials.",
        "Laser surface structuring has proven to be an effective technique for\nachieving a copper surface with secondary electron yield (SEY) values close to\nor below unity. However, the attributes that minimize SEY, such as moderately\ndeep grooves and redeposited nanoparticles, may lead to undesirable\nconsequences, including increased radio frequency surface resistance. This\ninvestigation systematically examined data about different cleaning procedures\ndesigned to eliminate redeposited adsorbed particulates. Various analysis\ntechniques were used iteratively after each consecutive cleaning step,\nproviding insights into the evolving surface characteristics. The collected\nexperimental results identified distinct impacts of microgrooves, groove\norientation, and associated particulates on secondary electron yield and\nsurface resistance. Exposing the crests while retaining high particulate\ncoverage in the grooves leads to reduced SEY values and surface resistance,\nsuggesting that the tips of the grooves exert a more significant influence on\nsurface current density than the groove depth. At the same time, nanoparticles\nin the grooves have a more significant impact on SEY values than the exposed\ntips at the surface.",
        "We present a massively parallel solver that accelerates DC loadflow\ncomputations for power grid topology optimization tasks. Our approach leverages\nlow-rank updates of the Power Transfer Distribution Factors (PTDFs) to\nrepresent substation splits, line outages, and reconfigurations without ever\nrefactorizing the system. Furthermore, we implement the core routines on\nGraphics Processing Units (GPUs), thereby exploiting their high-throughput\narchitecture for linear algebra. A two-level decomposition separates changes in\nbranch topology from changes in nodal injections, enabling additional speed-ups\nby an in-the-loop brute force search over injection variations at minimal\nadditional cost. We demonstrate billion-loadflow-per-second performance on\npower grids of varying sizes in workload settings which are typical for\ngradient-free topology optimization such as Reinforcement Learning or Quality\nDiversity methods. While adopting the DC approximation sacrifices some accuracy\nand prohibits the computation of voltage magnitudes, we show that this\nsacrifice unlocks new scales of computational feasibility, offering a powerful\ntool for large-scale grid planning and operational topology optimization.",
        "The standard Coulomb interaction is one of four fundamental interactions in\nNature. It is interesting to know how will the standard Coulomb interaction be\nmodified when it meets spin. Since the standard Coulomb potential is a simple\nbut fundamental solution of Maxwell's equations, hence Maxwell's equations can\npredict the existence of the standard Coulomb potential. The Yang-Mills\nequations are the natural generalizations of Maxwell's equations from the\nAbelian potentials to the non-Abelian ones, thus based on the Yang-Mills\nequations, one can predict the reasonable form of spin-dependent Coulomb\npotential, which naturally reduces the standard Coulomb potential if without\nconsidering the spin. Our work sheds a new light to how to couple spin with\nfundamental interactions.",
        "Quantum heat engines (QHEs) have attracted long-standing scientific interest,\nespecially inspired by considerations of the interplay between heat and work\nwith the quantization of energy levels, quantum superposition, and\nentanglement. Operating QHEs calls for effective control of the thermal\nreservoirs and the eigenenergies of the quantum working medium of the engine.\nAlthough superconducting circuits enable accurate engineering of controlled\nquantum systems, beneficial in quantum computing, this framework has not yet\nbeen employed to experimentally realize a cyclic QHE. Here, we experimentally\ndemonstrate a quantum heat engine based on superconducting circuits, using a\nsingle-junction quantum-circuit refrigerator (QCR) as a two-way tunable heat\nreservoir coupled to a flux-tunable transmon qubit acting as the working medium\nof the engine. We implement a quantum Otto cycle by a tailored drive on the QCR\nto sequentially induce cooling and heating, interleaved with flux ramps that\ncontrol the qubit frequency. Utilizing single-shot qubit readout, we monitor\nthe evolution of the qubit state during several cycles of the heat engine and\nmeasure positive output powers and efficiencies that agree with our simulations\nof the quantum evolution. Our results verify theoretical models on the\nthermodynamics of quantum heat engines and advance the control of\ndissipation-engineered thermal environments. These proof-of-concept results\npave the way for explorations on possible advantages of QHEs with respect to\nclassical heat engines.",
        "The safety and security of robotic systems are paramount when navigating\naround a hostile target. This paper addresses the problem of circumnavigating\nan unknown target by a unicycle robot while ensuring it maintains a desired\nsafe distance and remains within the sensing region around the target\nthroughout its motion. The proposed control design methodology is based on the\nconstruction of a joint Lyapunov function that incorporates: (i) a quadratic\npotential function characterizing the desired target-circumnavigation\nobjective, and (ii) a barrier Lyapunov function-based potential term to enforce\nsafety and sensing constraints on the robot's motion. A notable feature of the\nproposed control design is its reliance exclusively on local range measurements\nbetween the robot and the target, realized using a dynamic output feedback\ncontroller that treats the range as the only observable output for feedback.\nUsing the Lyapunov stability theory, we show that the desired equilibrium of\nthe closed-loop system is asymptotically stable, and the prescribed safety and\nsecurity constraints are met under the proposed controllers. We also obtain\nrestrictive bounds on the post-design signals and provide both simulation and\nexperimental results to validate the theoretical contributions.",
        "The Diffie-Hellman key exchange plays a crucial role in conventional\ncryptography, as it allows two legitimate users to establish a common, usually\nephemeral, secret key. Its security relies on the discrete-logarithm problem,\nwhich is considered to be a mathematical one-way function, while the final key\nis formed by random independent actions of the two users. In the present work\nwe investigate the extension of Diffie-Hellman key exchange to the quantum\nsetting, where the two legitimate users exchange independent random quantum\nstates. The proposed protocol relies on the bijective mapping of integers onto\na set of symmetric coherent states, and we investigate the regime of parameters\nfor which the map behaves as a quantum one-way function. Its security is\nanalyzed in the framework of minimum-error-discrimination and\nphoton-number-splitting attacks, while its performance and the challenges in a\npossible realization are also discussed.",
        "The study of collisionless shocks and their role in cosmic ray acceleration\nhas gained importance through observations and simulations, driving interest in\nreproducing these conditions in laboratory experiments using high-power lasers.\nIn this work, we examine the role of three-dimensional (3D) effects in ion\nacceleration in quasi-perpendicular shocks under laboratory-relevant\nconditions. Using hybrid particle-in-cell simulations (kinetic ions and fluid\nelectrons), we explore how the Alfv\\'enic and sonic Mach numbers, along with\nplasma beta, influence ion energization, unlocked only in 3D, and establish\nscaling criteria for when conducting 3D simulations is necessary. Our results\nshow that efficient ion acceleration requires Alfv\\'enic Mach numbers $\\geq 25$\nand sonic Mach numbers $\\geq 13$, with plasma-$\\beta \\leq 5$. We theoretically\nfound that, while 2D simulations suffice for current laboratory-accessible\nshock conditions, 3D effects become crucial for shock velocities exceeding 1000\nkm\/s and experiments sustaining the shock for at least 10 ns. We surveyed\nprevious laboratory experiments on collisionless shocks and found that 3D\neffects are unimportant under those conditions, implying that 1D and 2D\nsimulations should be enough to model the accelerated ion spectra. However, we\ndo find that the same experiments are realistically close to accessing the\nregime relevant to 3D effects, an exciting prospect for future laboratory\nefforts. We propose modifications to past experimental configurations to\noptimize and control 3D effects on ion acceleration. These proposed experiments\ncould be used to benchmark plasma astrophysics kinetic codes and\/or employed as\ncontrollable sources of energetic particles.",
        "The high-energy repulsive interaction between nuclei at distances much\nsmaller than the equilibrium bond length is the key quantity determining the\nnuclear stopping power and atom scattering in keV and MeV radiation events.\nThis interaction is traditionally modeled within orbital-free density\nfunctional theory with frozen atomic electron densities, following the\nZiegler-Biersack-Littmark (ZBL) model. In this work, we calculate atom pair\nspecific repulsive interatomic potentials with the ZBL model, and compare them\nto two kinds of quantum chemical calculations - second-order M{\\o}ller-Plesset\nperturbation theory in flexible Gaussian basis sets as well as density\nfunctional theory with numerical atomic orbital basis sets - which go well\nbeyond the limitations in the ZBL model, allowing the density to relax in the\ncalculations. We show that the repulsive interatomic potentials predicted by\nthe two quantum chemical models agree within $\\sim$ 1% for potential energies\nabove 30 eV, while the ZBL pair-specific potentials and universal ZBL\npotentials differ much more from either of these calculations. We provide new\npair-specific fits of the screening functions in terms of 3 exponentials to the\ncalculations for all pairs $Z_1$-$Z_2$ for $1 \\leq Z_i \\leq 92$, and show that\nthey agree within $\\sim 2$% with the raw data. We use the new potentials to\nsimulate ion implantation depth profiles in single crystalline Si and show very\ngood agreement with experiment. However, we also show that under channeling\nconditions, the attractive part of the potential can affect the depth profiles.\nThe full data sets of all the calculated interatomic potentials as well as\nanalytic fits to the data are shared as open access.",
        "Language model (LM) re-rankers are used to refine retrieval results for\nretrieval-augmented generation (RAG). They are more expensive than lexical\nmatching methods like BM25 but assumed to better process semantic information.\nTo understand whether LM re-rankers always live up to this assumption, we\nevaluate 6 different LM re-rankers on the NQ, LitQA2 and DRUID datasets. Our\nresults show that LM re-rankers struggle to outperform a simple BM25 re-ranker\non DRUID. Leveraging a novel separation metric based on BM25 scores, we explain\nand identify re-ranker errors stemming from lexical dissimilarities. We also\ninvestigate different methods to improve LM re-ranker performance and find\nthese methods mainly useful for NQ. Taken together, our work identifies and\nexplains weaknesses of LM re-rankers and points to the need for more\nadversarial and realistic datasets for their evaluation.",
        "We investigate the dynamical instabilities of an ultracold Bose-Bose mixture\nwith long-range dipole-dipole interactions, trapped in deep optical lattices\nand subject to periodically varying contact interaction. The effect of\nbeyond-mean-field corrections due to quantum fluctuations is considered. In the\ntight-binding regime, we employ Wannier functions to derive a discrete\nnonlinear Schr\\\"odinger equation that captures the dynamics of the system.\nUsing linear stability analysis coupled to multiple scale expansion, we\nsystematically study the modulational and parametric instabilities of the\nsystem, identifying the conditions under which these instabilities emerge. The\ncorresponding instability domains are found and examined. The roles of\nlong-range interactions and quantum fluctuations are highlighted, demonstrating\ntheir significant impact on the stability and dynamics of the lattice system.\nOur analytical predictions are validated through direct numerical simulations,\nwhich confirm the instability diagrams through the effective onset of\ninstabilities and reveal the intricate interplay between interaction strength,\nquantum fluctuation, and the long-range nature of the dipole-dipole forces.\nThis work provides insights into the control and manipulation of ultracold\nquantum gases in optical lattices, which have potential applications in quantum\nsimulation and condensed matter physics.",
        "The rapid and constant increase in urban population has led to a drastic rise\nin urban solid waste production with worrying consequences for the environment\nand society. In many cities, an efficient waste management combined with a\nsuitable design of vehicle routes (VR) can lead to benefits in the\nenvironmental, economic, and social impacts. The general population is becoming\nincreasingly aware of the need for the separation of the various categories of\nmunicipal solid waste. The numerous materials collected include glass, PET or\nbatteries, and electric components, which are sorted at the eco-points. The\nmanagement of eco-points gives rise to several problems that can be formulated\nanalytically. The location and number of eco-point containers, the\ndetermination of the fleet size for picking up the collected waste, and the\ndesign of itineraries are all intertwined, and present computationally\ndifficult problems, and therefore must be solved in a sequential way. In this\npaper, a mathematical model has been formulated, based on the Bin Packing (BP)\nand VR schemes, for the deployment of routes of mobile containers in the\nselective collection of urban solid waste. A heuristic algorithm has also been\ndeveloped, which considers two different configurations of the containers to\nsolve the proposed mathematical programming model. The results obtained from\nthe numerical simulations show the validation of the proposed methodology\ncarried out for the benchmark of the Sioux Falls network and the specific real\ncase study.",
        "Kolmogorov-Arnold Networks (KANs) have inspired numerous works exploring\ntheir applications across a wide range of scientific problems, with the\npotential to replace Multilayer Perceptrons (MLPs). While many KANs are\ndesigned using basis and polynomial functions, such as B-splines, ReLU-KAN\nutilizes a combination of ReLU functions to mimic the structure of B-splines\nand take advantage of ReLU's speed. However, ReLU-KAN is not built for multiple\ninputs, and its limitations stem from ReLU's handling of negative values, which\ncan restrict feature extraction. To address these issues, we introduce\nActivation Function-Based Kolmogorov-Arnold Networks (AF-KAN), expanding\nReLU-KAN with various activations and their function combinations. This novel\nKAN also incorporates parameter reduction methods, primarily attention\nmechanisms and data normalization, to enhance performance on image\nclassification datasets. We explore different activation functions, function\ncombinations, grid sizes, and spline orders to validate the effectiveness of\nAF-KAN and determine its optimal configuration. In the experiments, AF-KAN\nsignificantly outperforms MLP, ReLU-KAN, and other KANs with the same parameter\ncount. It also remains competitive even when using fewer than 6 to 10 times the\nparameters while maintaining the same network structure. However, AF-KAN\nrequires a longer training time and consumes more FLOPs. The repository for\nthis work is available at https:\/\/github.com\/hoangthangta\/All-KAN.",
        "We prove new unconditional results of sparsity of integral points on orbits\nunder many maps and correspondences in arbitrary dimensions, generalizing\ntheorems of Yasufuku(2015) and others. The main ingredients are new diophantine\napproximation tools and recent constructions for correspondences due to Ingram\n(2011).",
        "Virtual Reality Video Quality Assessment (VR-VQA) aims to evaluate the\nperceptual quality of 360-degree videos, which is crucial for ensuring a\ndistortion-free user experience. Traditional VR-VQA methods trained on static\ndatasets with limited distortion diversity struggle to balance correlation and\nprecision. This becomes particularly critical when generalizing to diverse VR\ncontent and continually adapting to dynamic and evolving video distribution\nvariations. To address these challenges, we propose a novel approach for\nassessing the perceptual quality of VR videos, Adaptive Score Alignment\nLearning (ASAL). ASAL integrates correlation loss with error loss to enhance\nalignment with human subjective ratings and precision in predicting perceptual\nquality. In particular, ASAL can naturally adapt to continually changing\ndistributions through a feature space smoothing process that enhances\ngeneralization to unseen content. To further improve continual adaptation to\ndynamic VR environments, we extend ASAL with adaptive memory replay as a novel\nContinul Learning (CL) framework. Unlike traditional CL models, ASAL utilizes\nkey frame extraction and feature adaptation to address the unique challenges of\nnon-stationary variations with both the computation and storage restrictions of\nVR devices. We establish a comprehensive benchmark for VR-VQA and its CL\ncounterpart, introducing new data splits and evaluation metrics. Our\nexperiments demonstrate that ASAL outperforms recent strong baseline models,\nachieving overall correlation gains of up to 4.78\\% in the static joint\ntraining setting and 12.19\\% in the dynamic CL setting on various datasets.\nThis validates the effectiveness of ASAL in addressing the inherent challenges\nof VR-VQA.Our code is available at https:\/\/github.com\/ZhouKanglei\/ASAL_CVQA.",
        "While machine learning (ML) technology affects diverse stakeholders, there is\nno one-size-fits-all metric to evaluate the quality of outputs, including\nperformance and fairness. Using predetermined metrics without soliciting\nstakeholder opinions is problematic because it leads to an unfair disregard for\nstakeholders in the ML pipeline. In this study, to establish practical ways to\nincorporate diverse stakeholder opinions into the selection of metrics for ML,\nwe investigate participants' preferences for different metrics by using\ncrowdsourcing. We ask 837 participants to choose a better model from two\nhypothetical ML models in a hypothetical job-matching system twenty times and\ncalculate their utility values for seven metrics. To examine the participants'\nfeedback in detail, we divide them into five clusters based on their utility\nvalues and analyze the tendencies of each cluster, including their preferences\nfor metrics and common attributes. Based on the results, we discuss the points\nthat should be considered when selecting appropriate metrics and evaluating ML\nmodels with multiple stakeholders.",
        "We propose a novel mechanism of primordial black hole (PBH) formation through\ninverted bubble collapse. In this scenario, bubbles nucleate sparsely in an\nincomplete first-order phase transition, followed by a bulk phase transition in\nthe rest of the universe that inverts these pre-existing bubbles into false\nvacuum regions. These spherically symmetric false-vacuum bubbles subsequently\ncollapse to form PBHs. Unlike conventional PBH formation mechanisms associated\nwith domain wall collapse or bubble coalescence, our inverted bubble collapse\nmechanism naturally ensures spherical collapse. We demonstrate that, when\napplied to the singlet extension of the Standard Model, this mechanism can\nproduce highly monochromatic PBHs with masses up to ${\\cal\nO}(10^{-7}\\,\\text{-}\\,10^{-5}) M_\\odot$, which potentially explain the\nmicrolensing events observed in the OGLE and Subaru HSC data.",
        "Efficient Service Function Chain (SFC) provisioning and Virtual Network\nFunction (VNF) placement are critical for enhancing network performance in\nmodern architectures such as Software-Defined Networking (SDN) and Network\nFunction Virtualization (NFV). While Deep Reinforcement Learning (DRL) aids\ndecision-making in dynamic network environments, its reliance on structured\ninputs and predefined rules limits adaptability in unforeseen scenarios.\nAdditionally, incorrect actions by a DRL agent may require numerous training\niterations to correct, potentially reinforcing suboptimal policies and\ndegrading performance. This paper integrates DRL with Language Models (LMs),\nspecifically Bidirectional Encoder Representations from Transformers (BERT) and\nDistilBERT, to enhance network management. By feeding final VNF allocations\nfrom DRL into the LM, the system can process and respond to queries related to\nSFCs, DCs, and VNFs, enabling real-time insights into resource utilization,\nbottleneck detection, and future demand planning. The LMs are fine-tuned to our\ndomain-specific dataset using Low-Rank Adaptation (LoRA). Results show that\nBERT outperforms DistilBERT with a lower test loss (0.28 compared to 0.36) and\nhigher confidence (0.83 compared to 0.74), though BERT requires approximately\n46% more processing time.",
        "The focus of this note is to formulate the algorithms and give the examples\nused by Fibonacci in Liber Abaci to expand any fraction into a sum of unit\nfractions. The description in Liber Abaci is all verbal and the examples are\nnumbers which may lead to different algorithmic descriptions with the same\ninput and results. An additional complication is that the manuscripts that\nexist are copies of an older manuscript and in the process new errors are\nintroduced. Additional errors may also be introduced in the transcript and the\ntranslation. Fibonacci introduces seven categories each with several numerical\nexamples. We give a precise description of the computational procedure using\nstandard mathematical notation.",
        "The fast fashion industry's insatiable demand for new styles and rapid\nproduction cycles has led to a significant environmental burden.\nOverproduction, excessive waste, and harmful chemicals have contributed to the\nnegative environmental impact of the industry. To mitigate these issues, a\nparadigm shift that prioritizes sustainability and efficiency is urgently\nneeded. Integrating learning-based predictive analytics into the fashion\nindustry represents a significant opportunity to address environmental\nchallenges and drive sustainable practices. By forecasting fashion trends and\noptimizing production, brands can reduce their ecological footprint while\nremaining competitive in a rapidly changing market. However, one of the key\nchallenges in forecasting fashion sales is the dynamic nature of consumer\npreferences. Fashion is acyclical, with trends constantly evolving and\nresurfacing. In addition, cultural changes and unexpected events can disrupt\nestablished patterns. This problem is also known as New Fashion Products\nPerformance Forecasting (NFPPF), and it has recently gained more and more\ninterest in the global research landscape. Given its multidisciplinary nature,\nthe field of NFPPF has been approached from many different angles. This\ncomprehensive survey wishes to provide an up-to-date overview that focuses on\nlearning-based NFPPF strategies. The survey is based on the Preferred Reporting\nItems for Systematic Reviews and Meta-Analyses (PRISMA) methodological flow,\nallowing for a systematic and complete literature review. In particular, we\npropose the first taxonomy that covers the learning panorama for NFPPF,\nexamining in detail the different methodologies used to increase the amount of\nmultimodal information, as well as the state-of-the-art available datasets.\nFinally, we discuss the challenges and future directions.",
        "Machine learning for robotics promises to unlock generalization to novel\ntasks and environments. Guided by this promise, many recent works have focused\non scaling up robot data collection and developing larger, more expressive\npolicies to achieve this. But how do we measure progress towards this goal of\npolicy generalization in practice? Evaluating and quantifying generalization is\nthe Wild West of modern robotics, with each work proposing and measuring\ndifferent types of generalization in their own, often difficult to reproduce,\nsettings. In this work, our goal is (1) to outline the forms of generalization\nwe believe are important in robot manipulation in a comprehensive and\nfine-grained manner, and (2) to provide reproducible guidelines for measuring\nthese notions of generalization. We first propose STAR-Gen, a taxonomy of\ngeneralization for robot manipulation structured around visual, semantic, and\nbehavioral generalization. We discuss how our taxonomy encompasses most prior\nnotions of generalization in robotics. Next, we instantiate STAR-Gen with a\nconcrete real-world benchmark based on the widely-used Bridge V2 dataset. We\nevaluate a variety of state-of-the-art models on this benchmark to demonstrate\nthe utility of our taxonomy in practice. Our taxonomy of generalization can\nyield many interesting insights into existing models: for example, we observe\nthat current vision-language-action models struggle with various types of\nsemantic generalization, despite the promise of pre-training on internet-scale\nlanguage datasets. We believe STAR-Gen and our guidelines can improve the\ndissemination and evaluation of progress towards generalization in robotics,\nwhich we hope will guide model design and future data collection efforts. We\nprovide videos and demos at our website stargen-taxonomy.github.io.",
        "Structural change is necessary for all countries transitioning to a more\nenvironmentally sustainable economy, but what are the likely impacts on\nworkers? Studies often find that green transition scenarios result in net\npositive job creation numbers overall but rarely provide insights into the more\ngranular dynamics of the labour market. This paper combines a dynamic labour\nmarket simulation model with development scenarios focused on agriculture and\ngreen manufacturing. We study how, within the context of a green transition,\nproductivity shifts in different sectors and regions, with differing\nenvironmental impacts, may affect and be constrained by the labour market in\nBrazil. By accounting for labour market frictions associated with skill and\nspatial mismatches, we find that productivity shocks, if not well managed, can\nexacerbate inequality. Agricultural workers tend to be the most negatively\naffected as they are less occupationally and geographically mobile. Our results\nhighlight the importance of well-targeted labour market policies to ensure the\ngreen transition is just and equitable."
      ]
    }
  },
  {
    "id":2411.19,
    "research_type":"applied",
    "start_id":"b7",
    "start_title":"Artificial intelligence applications in stroke",
    "start_abstract":"Management of stroke highly depends on information from imaging studies. Noncontrast computed tomography (CT) and magnetic resonance imaging (MRI) can both be used to distinguish between ischemic and hemorrhagic stroke, which is difficult based on clinical features. Hypodensity on CT and DWI hyperintensity on MRI identifies irreversibly damaged tissue, although the sensitivity of MRI is higher in the acute setting. Angiographic and perfusion imaging sequences can identify a large vessel occlusion and, along with perfusion imaging, can select patients for endovascular therapy. The FLAIR-DWI mismatch yields information about patients with unknown time of onset (including wake-up strokes). Stroke imaging also gives insight into prognosis, with current methods aiming to give a picture of the short-term consequences of successful reperfusion or continued large vessel occlusion. One important caveat about stroke imaging is that it must be done quickly, as faster treatment leads to better outcomes.1 However, most steps in the stroke imaging triage pathway require the presence of human radiologists and neurologists, and this is often the time-limiting step. The expertise required for these tasks may not be available at all sites or at all times. Therefore, there is interest in automated methods for stroke imaging evaluation. Artificial intelligence (AI) is a broad term reflecting the use of computers to perform tasks that humans may find difficult, often in ways that are hard to pinpoint. For example, although humans find high-level computation difficult, calculator technology is not considered AI because we know how to break this down into discrete steps and feel we understand it. However, facial recognition is a task that humans perform well, but an algorithm to identify faces is usually considered AI since we cannot articulate precisely how this is done. Machine learning (ML) is a subset of AI in which algorithms learn from the data itself without explicit programming. ML methods reflect a broad range of statistical techniques ranging from linear regression to more complex methods such as support vector machines and decision trees. ML methods can be further broken into supervised and unsupervised learning, which differ from one another in that the former requires access to gold standard labels although the latter attempts to find the answers implicitly in the data itself. While ML methods have grown more popular over recent years, the advent of a specific supervised ML method based on architectures resembling human neural networks over the past decade has led to a quantum leap in performance.2 This method, called deep learning (DL) because of many multiple internal layers, can be considered a transformative technology. Compared with previous methods that required humans to identify image features, a deep neural network trained on a dataset with known outputs can learn the best features for organizing the data. In this review, we will discuss ML methods applied to stroke imaging with an emphasis on DL applications. We refer to Figure for a graphical overview of the applications discussed in this review.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Addressing disparities in the global epidemiology of stroke"
      ],
      "abstract":[
        "Stroke is the second leading cause of death and the third leading cause of disability worldwide. Though the burden of stroke worldwide seems to have declined in the past three decades, much of this effect reflects decreases in high-income countries (HICs). By contrast, the burden of stroke has grown rapidly in low-income and middle-income countries (LMICs), where epidemiological, socioeconomic and demographic shifts have increased the incidence of stroke and other non-communicable diseases. Furthermore, even in HICs, disparities in stroke epidemiology exist along racial, ethnic, socioeconomic and geographical lines. In this Review, we highlight the under-acknowledged disparities in the burden of stroke. We emphasize the shifting global landscape of stroke risk factors, critical gaps in stroke service delivery, and the need for a more granular analysis of the burden of stroke within and between LMICs and HICs to guide context-appropriate capacity-building. Finally, we review strategies for addressing key inequalities in stroke epidemiology, including improvements in epidemiological surveillance and context-specific research efforts in under-resourced regions, development of the global workforce of stroke care providers, expansion of access to preventive and treatment services through mobile and telehealth platforms, and scaling up of evidence-based strategies and policies that target local, national, regional and global stroke disparities."
      ],
      "categories":[
        "q-bio.NC"
      ]
    },
    "list":{
      "title":[
        "Multidimensional moment problem and Stieltjes transform",
        "Algebraic solution of the Jacobi inverse problem and explicit addition\n  laws",
        "Yang-Mills-Utiyama Theory and Graviweak Correspondence",
        "Measuring Top Yukawa Coupling through $2\\rightarrow 3$ VBS at Muon\n  Collider",
        "Phonons in Electron Crystals with Berry Curvature",
        "Counting principal ideals of small norm in the simplest cubic fields",
        "Hybrid Quantum Neural Networks with Amplitude Encoding: Advancing\n  Recovery Rate Predictions",
        "A Virgo Environmental Survey Tracing Ionised Gas Emission (VESTIGE)\n  XVII. Statistical properties of individual HII regions in unperturbed systems",
        "Birth of magnetized low-mass protostars and circumstellar disks",
        "The soccer model, stochastic ordering and martingale transport",
        "MNE: overparametrized neural evolution with applications to diffusion\n  processes and sampling",
        "Recurrent Neural Networks for Dynamic VWAP Execution: Adaptive Trading\n  Strategies with Temporal Kolmogorov-Arnold Networks",
        "On generalized Tur{\\'a}n problems with bounded matching number and\n  circumference",
        "The sliding tile puzzle, roots to polynomials, and $\\textbf{P}$ vs.\n  $\\textbf{NP}$ complexity",
        "Multi-constraint Graph Partitioning Problems Via Recursive Bipartition\n  Algorithm Based on Subspace Minimization Conjugate Gradient Method",
        "Single-qubit probes for temperature estimation in the presence of\n  collective baths",
        "Orthogonal Delay-Doppler Division Multiplexing Modulation with\n  Hierarchical Mode-Based Index Modulation",
        "Symplectic-Amoeba formulation of the non-Bloch band theory for\n  one-dimensional two-band systems",
        "$K_2$-regularity and normality",
        "Ideal MHD. Part II: Rigidity from infinity for ideal Alfv\\'en waves in\n  3D thin domains",
        "Structure of operator algebras for matrix orthogonal polynomials",
        "Algorithm to generate hierarchical structure of desiccation crack\n  patterns",
        "Gravitomagnetic tidal response of relativistic stars in partially\n  screened scalar-tensor theories",
        "Aspherical 4-manifolds with elementary amenable fundamental group",
        "Higher order div-curl type estimates for elliptic linear differential\n  operators on localizable Hardy spaces",
        "Quantifying the imaginarity via different distance measures",
        "Ultracategories as colax algebras for a pseudo-monad on CAT",
        "Shoot-through layers in upright proton arcs unlock advantages in plan\n  quality and range verification",
        "Consensus About Classical Reality in a Quantum Universe"
      ],
      "abstract":[
        "The truncated multidimensional moment problem is studied in terms of the\nStieltjes transform as the interpolation problem. A step-by-step algorithm is\nconstructed for the multidimensional moment problem and the set of solutions is\nfound in terms of continued fractions.",
        "We formulate a solution to the Algebraic version of the Inverse Jacobi\nproblem. Using this solution we produce explicit addition laws on any algebraic\ncurve generalizing the law suggested by Leykin [2] in the case of (n, s)\ncurves. This gives a positive answer to a question asked by T. Shaska whether\naddition laws appearing in [2] can be produced in a coordinate free manner.",
        "This report provides a geometrical Yang-Mills theory, including gravity. The\ntheory treats the space-time symmetry of the local Lorentz group in the same\nmanner as the internal gauge symmetry. We extend this general relativistic\nYang-Mills theory in the Minkowski space-time to a broader space, including\nEuclidean and Minkowskian spaces. In the extended space, we can transport\ntopological achievements from the Euclidean Yang-Mills theory into the\nLorentzian Yang-Mills theory. This extension also provides a relation between\nspace-time and internal symmetry. The perspective provided by the extended\ntheory suggests the novel relation between gravity and the weak force, leading\nus to the graviweak correspondence.",
        "We study the measurement of top Yukawa coupling through $2\\rightarrow 3$ VBS\nat future muon colliders, focusing on the lepton and semi-lepton channels of\n$\\nu\\nu tth\/z$. First, analyzing the partonic amplitudes of $W_LW_L\\rightarrow\nt\\bar t h\/Z_L$ and simulating the full processes of $\\nu\\nu tth\/z$ without\ndecaying, we find they are highly sensitive to the anomalous top Yukawa $\\delta\ny_t$. This sensitivity is enhanced by selecting helicities of the final $t\\bar\nt$ and $Z$ to be $t_L\\bar t_L+t_R\\bar t_R$ and $Z_L$, which serves as the\ndefault and core setting of our analysis. We then obtain the limits on $\\delta\ny_t$ with this setting, giving $[-1.0\\%, 1.1\\%]$ for $\\nu\\nu tth$ only and\n$[-0.36\\%, 0.92\\%]$ for $\\nu\\nu tth$ and $\\nu\\nu ttz$ combined at $30$ TeV and\n$1\\sigma$. Second, we proceed to analyze the processes after decaying and with\nbackground processes. To enhance the sensitivity to $\\delta y_t$, our settings\ninclude selecting the helicities of the final particles, as well as applying\nsuitable cuts. However, we don't do bin-by-bin analysis. We obtain the limits\non $\\delta y_t$ for those channels at $10\/30$ TeV and $1\\sigma\/2 \\sigma$. The\nbest limit is from the semi-lepton channel of $\\nu\\nu tth$. With spin tagging\nefficiency at $\\epsilon_s=0.9$, it gives $[-1.6\\% , 1.8\\%]$ at $1\\sigma$ and $\n[-2.4\\%, 2.7\\% ]$ at $2\\sigma$ at $30$ TeV; $[-7.0\\%, 6.7\\%]$ at $1\\sigma$ and\n$[-9.8\\%, 9.8\\%]$ at $2\\sigma$ at $10$ TeV.",
        "Recent advances in 2D materials featuring nonzero Berry curvature have\ninspired extensions of the Wigner crystallization paradigm. This paper derives\na low-energy effective theory for such quantum crystals, including the\nanomalous Hall crystal (AHC) with nonzero Chern number. First we show that the\nlow frequency dispersion of phonons in AHC, despite the presence of Berry\ncurvature, resembles that of the zero field (rather than finite magnetic field)\nWigner crystal due to the commutation of translation generators. We explain how\nkey parameters of the phonon theory such as elastic constants and effective\nmass can be extracted from microscopic models, and apply them to two families\nof models: the recently introduced $\\lambda$-jellium model and a model of\nrhombohedral multilayer graphene (RMG). In the $\\lambda$-jellium model, we\nexplore the energy landscape as crystal geometry shifts, revealing that AHC can\nbecome \"soft\" under certain conditions. This causes transitions in lattice\ngeometry, although the quantized Hall response remains unchanged. Surprisingly,\nthe Berry curvature seems to enhance the effective mass, leading to a reduction\nin phonon speed. For the AHC in RMG, we obtain estimates of phonon speed and\nshear stiffness. We also identify a previously overlooked \"kineo-elastic\" term\nin the phonon effective action that is present in the symmetry setting of RMG,\nand leads to dramatic differences in phonon speeds in opposite directions. We\nnumerically confirm these predictions of the effective actions by\ntime-dependent Hartree-Fock calculations. Our work points to the wealth of new\nphenomena that can arise when electron crystallization occurs in the presence\nof band geometry and topology.",
        "We estimate the number of principal ideals $ I $ of norm $ \\mathrm{N}(I) \\leq\nx $ in the family of the simplest cubic fields. The advantage of our result is\nthat it provides the correct order of magnitude for arbitrary $ x \\geq 1 $,\neven when $ x $ is significantly smaller than the discriminant. In particular,\nit shows that there exist surprisingly many principal ideals of small norm.",
        "Recovery rate prediction plays a pivotal role in bond investment strategies,\nenhancing risk assessment, optimizing portfolio allocation, improving pricing\naccuracy, and supporting effective credit risk management. However, forecasting\nfaces challenges like high-dimensional features, small sample sizes, and\noverfitting. We propose a hybrid Quantum Machine Learning model incorporating\nParameterized Quantum Circuits (PQC) within a neural network framework. PQCs\ninherently preserve unitarity, avoiding computationally costly orthogonality\nconstraints, while amplitude encoding enables exponential data compression,\nreducing qubit requirements logarithmically. Applied to a global dataset of\n1,725 observations (1996-2023), our method achieved superior accuracy (RMSE\n0.228) compared to classical neural networks (0.246) and quantum models with\nangle encoding (0.242), with efficient computation times. This work highlights\nthe potential of hybrid quantum-classical architectures in advancing recovery\nrate forecasting.",
        "The Virgo Environmental Survey Tracing Ionised Gas Emission (VESTIGE) is a\nblind narrow-band Halpha+[NII] imaging survey of the Virgo cluster carried out\nwith MegaCam at the CFHT telescope. The survey provides deep narrow-band images\nfor 385 galaxies hosting star forming HII regions. We identify individual HII\nregions and measure their main physical properties such as Halpha luminosity,\nequivalent diameter, and electron density with the purpose of deriving standard\nrelations as reference for future local and high-z studies of HII regions in\nstar forming systems in different environments. For this purpose we use a\ncomplete sample of ~ 13.000 HII regions of luminosity L(Halpha)>= 10^37 erg\ns^-1 to derive the main statistical properties of HII regions in unperturbed\nsystems, identified as those galaxies with a normal HI gas content (64\nobjects). These are the composite Halpha luminosity function, equivalent\ndiameter and electron density distribution, and luminosity-size relation. We\nalso derive the main scaling relations between several parameters\nrepresentative of the HII regions properties (total number, luminosity of the\nfirst ranked regions, fraction of the diffuse component, best fit parameters of\nthe Schechter luminosity function measured for individual galaxies) and those\ncharacterising the properties of the host galaxies (stellar mass, star\nformation rate and specific star formation rate, stellar mass and star\nformation rate surface density, metallicity, molecular-to-atomic gas ratio,\ntotal gas-to-dust mass ratio). We briefly discuss the results of this analysis\nand their implications in the study of the star formation process in galaxy\ndiscs.",
        "Although protostars and disks are often studied separately owing to numerical\nand observational challenges, breakthroughs in recent years have highlighted\nthe need to study both objects in concert. The role of magnetic fields in this\nregard must be investigated. We aim to describe the birth of the protostar and\nthat of its disk, as well as their early joint evolution following the second\ncollapse. We wish to study the structure of the nascent star-disk system, while\nfocusing on the innermost sub-AU region. We carry out high resolution 3D RMHD\nsimulations, describing the collapse of dense cloud cores to stellar densities.\nThe calculations reach $\\approx 2.3$ yr after protostellar birth. Our\nsimulations are also compared to their hydro counterpart to better isolate the\nrole of magnetic fields. When accounting for ambipolar diffusion, the\nefficiency of magnetic braking is drastically reduced and the nascent protostar\nreaches breakup velocity, thus forming a rotationally supported disk. The\ndiffusion of the magnetic field also allows for the implantation of a $\\sim\n\\mathrm{kG}$ field in the protostar, which is thereafter maintained. The\nmagnetic field is mainly toroidal in the star-disk system, although a notable\nvertical component threads it. We also show that the nascent disk is prone to\nthe MRI, although our resolution is inadequate to capture the mechanism. We\nnote a sensitivity of the disk's properties with regards to the angular\nmomentum inherited prior to the second collapse, as well as the magnetic field\nstrength. These calculations carry multiple implications on several issues in\nstellar formation theory, and offer perspectives for future modeling of the\nsystem. Should the fossil field hypothesis to explain the origins of magnetic\nfields in young stellar objects hold, we show that a $\\sim \\mathrm{kG}$ field\nstrength may be implanted and maintained in the protostar at birth.",
        "Tournaments are competitions between a number of teams, the outcome of which\ndetermines the relative strength or rank of each team. In many cases, the\nstrength of a team in the tournament is given by a score. Perhaps, the most\nstriking mathematical result on the tournament is Moon's theorem, which\nprovides a necessary and sufficient condition for a feasible score sequence via\nmajorization. To give a probabilistic interpretation of Moon's result, Aldous\nand Kolesnik introduced the soccer model,the existence of which gives a short\nproof of Moon's theorem. However, the existence proof of Aldous and Kolesnik is\nnonconstructive, leading to the question of a ``canonical'' construction of the\nsoccer model. The purpose of this paper is to provide explicit constructions of\nthe soccer model with an additional stochastic ordering constraint, which can\nbe formulated by martingale transport. Two solutions are given: one is by\nsolving an entropy optimization problem via Sinkhorn's algorithm, and the other\nrelies on the idea of shadow couplings. It turns out that both constructions\nyield the property of strong stochastic transitivity. The nontransitive\nsituations of the soccer model are also considered.",
        "We propose a framework for solving evolution equations within parametric\nfunction classes, especially ones that are specified by neural networks. We\ncall this framework the minimal neural evolution (MNE) because it is motivated\nby the goal of seeking the smallest instantaneous change in the neural network\nparameters that is compatible with exact solution of the evolution equation at\na set of evolving collocation points. Formally, the MNE is quite similar to the\nrecently introduced Neural Galerkin framework, but a difference in perspective\nmotivates an alternative sketching procedure that effectively reduces the\nlinear systems solved within the integrator to a size that is interpretable as\nan effective rank of the evolving neural tangent kernel, while maintaining a\nsmooth evolution equation for the neural network parameters. We focus\nspecifically on the application of this framework to diffusion processes, where\nthe score function allows us to define intuitive dynamics for the collocation\npoints. These can in turn be propagated jointly with the neural network\nparameters using a high-order adaptive integrator. In particular, we\ndemonstrate how the Ornstein-Uhlenbeck diffusion process can be used for the\ntask of sampling from a probability distribution given a formula for the\ndensity but no training data. This framework extends naturally to allow for\nconditional sampling and marginalization, and we show how to systematically\nremove the sampling bias due to parametric approximation error. We validate the\nefficiency, systematic improvability, and scalability of our approach on\nillustrative examples in low and high spatial dimensions.",
        "The execution of Volume Weighted Average Price (VWAP) orders remains a\ncritical challenge in modern financial markets, particularly as trading volumes\nand market complexity continue to increase. In my previous work\narXiv:2502.13722, I introduced a novel deep learning approach that demonstrated\nsignificant improvements over traditional VWAP execution methods by directly\noptimizing the execution problem rather than relying on volume curve\npredictions. However, that model was static because it employed the fully\nlinear approach described in arXiv:2410.21448, which is not designed for\ndynamic adjustment. This paper extends that foundation by developing a dynamic\nneural VWAP framework that adapts to evolving market conditions in real time.\nWe introduce two key innovations: first, the integration of recurrent neural\nnetworks to capture complex temporal dependencies in market dynamics, and\nsecond, a sophisticated dynamic adjustment mechanism that continuously\noptimizes execution decisions based on market feedback. The empirical analysis,\nconducted across five major cryptocurrency markets, demonstrates that this\ndynamic approach achieves substantial improvements over both traditional\nmethods and our previous static implementation, with execution performance\ngains of 10 to 15% in liquid markets and consistent outperformance across\nvarying conditions. These results suggest that adaptive neural architectures\ncan effectively address the challenges of modern VWAP execution while\nmaintaining computational efficiency suitable for practical deployment.",
        "Let \\( \\mathcal{F} \\) be a family of graphs. The generalized Tur\\'an number\n\\( \\operatorname{ex}(n, K_r, \\mathcal{F}) \\) is the maximum number of $K_r$ in\nan \\( n \\)-vertex graph that does not contain any member of \\( \\mathcal{F} \\)\nas a subgraph. Recently, Alon and Frankl initiated the study of Tur\\'an\nproblems with bounded matching number. In this paper, we determine the\ngeneralized Tur\\'an number of \\( C_{\\geq k} \\) with bounded matching number.",
        "This work explores the relationship between solution space and time\ncomplexity in the context of the $\\textbf{P}$ vs. $\\textbf{NP}$ problem,\nparticularly through the lens of the sliding tile puzzle and root finding\nalgorithms. We focus on the trade-off between finding a solution and verifying\nit, highlighting how understanding the structure of the solution space can\ninform the complexity of these problems. By examining the relationship between\nthe number of possible configurations and the time complexity required to\ntraverse this space we demonstrate that the minimal time to verify a solution\nis often smaller than the time required to discover it. Our results suggest\nthat the efficiency of solving $\\textbf{NP}$-complete problems is not only\ndetermined by the ability to find solutions but also by how effectively we can\nnavigate and characterize the solution space. This study contributes to the\nongoing discourse on computational complexity, particularly in understanding\nthe interplay between solution space size, algorithm design, and the inherent\nchallenges of finding versus verifying solutions.",
        "The graph partitioning problem is a well-known NP-hard problem. In this\npaper, we formulate a 0-1 quadratic integer programming model for the graph\npartitioning problem with vertex weight constraints and fixed vertex\nconstraints, and propose a recursive bipartition algorithm based on the\nsubspace minimization conjugate gradient method. To alleviate the difficulty of\nsolving the model, the constrained problem is transformed into an unconstrained\noptimization problem using equilibrium terms, elimination methods, and\ntrigonometric properties, and solved via an accelerated subspace minimization\nconjugate gradient algorithm. Initial feasible partitions are generated using a\nhyperplane rounding algorithm, followed by heuristic refinement strategies,\nincluding one-neighborhood and two-interchange adjustments, to iteratively\nimprove the results. Numerical experiments on knapsack-constrained graph\npartitioning and industrial examples demonstrate the effectiveness and\nfeasibility of the proposed algorithm.",
        "We study the performance of single-qubit probes for temperature estimation in\nthe presence of collective baths. We consider a system of two qubits, each\nlocally dissipating into its own bath while being coupled to a common bath. In\nthis setup, we investigate different scenarios for temperature estimation of\nboth the common and local baths. First, we explore how the precision of a\nsingle-qubit probe for the temperature of the common bath may be enhanced by\nthe collective effects generated by the bath itself, if the second qubit is in\nresonance with the probe. We also analyze how the presence of additional local\nbaths on each qubit may jeopardize, or improve, this result. Next, we show that\none qubit may serve as a probe to measure the temperature of the local bath,\naffecting the other qubit by exploiting their interaction mediated by the\ncommon bath. This approach enables remote temperature sensing without directly\ncoupling the probe to the target qubit or its local environment, thereby\nminimizing potential disturbances and practical challenges. However, in the\nabsence of a direct qubit-qubit coupling, this protocol works only for very\nhigh temperatures of the local bath whose temperature we aim at estimating.\nThis being said, remote temperature sensing works for broader temperature\nregimes in the presence of a direct coupling between the qubits. Furthermore,\nwe also investigate the impact of dephasing and the dynamics of quantum\ncorrelations in the model for remote temperature sensing.",
        "The orthogonal time frequency space with index modulation (OTFS-IM) offers\nflexible tradeoffs between spectral efficiency (SE) and bit error rate (BER) in\ndoubly selective fading channels. While OTFS-IM schemes demonstrated such\npotential, a persistent challenge lies in the detection complexity. To address\nthis problem, we propose the hierarchical mode-based index modulation (HMIM).\nHMIM introduces a novel approach to modulate information bits by IM patterns,\nsignificantly simplifying the complexity of maximum a posteriori (MAP)\nestimation with Gaussian noise. Further, we incorporate HMIM with the recently\nproposed orthogonal delay-Doppler division multiplexing (ODDM) modulation,\nnamely ODDM-HMIM, to exploit the full diversity of the delay-Doppler (DD)\nchannel. The BER performance of ODDM-HMIM is analyzed considering a maximum\nlikelihood (ML) detector. Our numerical results reveal that, with the same SE,\nHMIM can outperform conventional IM in terms of both BER and computational\ncomplexity. In addition, we propose a successive interference\ncancellation-based minimum mean square error (SIC-MMSE) detector for ODDM-HMIM,\nwhich enables low-complexity detection with large frame sizes.",
        "The non-Hermitian skin effect is a topological phenomenon, resulting in the\ncondensation of bulk modes near the boundaries. Due to the localization of bulk\nmodes at the edges, boundary effects remain significant even in the\nthermodynamic limit. This makes conventional Bloch band theory inapplicable and\nhinders the accurate computation of the spectrum. The Amoeba formulation\naddresses this problem by determining the potential from which the spectrum can\nbe derived using the generalized Szeg\\\"o's limit theorem, reducing the problem\nto an optimization of the Ronkin function. While this theory provides novel\ninsights into non-Hermitian physics, challenges arise from the multiband nature\nand symmetry-protected degeneracies, even in one-dimensional cases. In this\nwork, we investigate one-dimensional two-band class AII$^\\dagger$ systems,\nwhere Kramers pairs invalidate the conventional Amoeba formalism. We find that\nthese challenges can be overcome by optimizing the band-resolved Ronkin\nfunctions, which is achieved by extrapolating the total Ronkin function.\nFinally, we propose a generalized Szeg\\\"o's limit theorem for class\nAII$^\\dagger$ and numerically demonstrate that our approach correctly computes\nthe potential and localization length.",
        "We take a fresh look at the relationship between $K$-regularity and\nregularity of schemes, proving two results in this direction. First, we show\nthat $K_2$-regular affine algebras over fields of characteristic zero are\nnormal. Second, we improve on Vorst's $K$-regularity bound in the case of local\ncomplete intersections; this is related to recent work on higher du Bois\nsingularities.",
        "This paper concerns the rigidity from infinity for Alfv\\'en waves governed by\nideal incompressible magnetohydrodynamic equations subjected to strong\nbackground magnetic fields along the $x_1$-axis in 3D thin domains\n$\\Omega_\\delta=\\mathbb{R}^2\\times(-\\delta,\\delta)$ with $\\delta\\in(0,1]$ and\nslip boundary conditions. We show that in any thin domain $\\Omega_\\delta$,\nAlfv\\'en waves must vanish identically if their scattering fields vanish at\ninfinities. As an application, the rigidity of Alfv\\'en waves in\n$\\Omega_{\\delta}$, propagating along the horizontal direction, can be\napproximated by the rigidity of Alfv\\'en waves in $\\mathbb{R}^2$ when $\\delta$\nis sufficiently small. Our proof relies on the uniform (with respect to\n$\\delta$) weighted energy estimates with a position parameter in weights to\ntrack the center of Alfv\\'en waves. The key issues in the analysis include\ndealing with the nonlinear nature of Alfv\\'en waves and the geometry of thin\ndomains.",
        "In this paper, we study the structure of the differential operator algebra \\(\n\\mathcal{D}(W) \\) and its associated eigenvalue algebra \\( \\Lambda(W) \\) for\nmatrix-valued orthogonal polynomials. While \\( \\Lambda(W) \\) is isomorphic to\n\\( \\mathcal{D}(W) \\), its simpler framework allows us to efficiently derive\nstrong results about \\( \\mathcal{D}(W) \\) and its center \\( \\mathcal{Z}(W) \\).\nWe analyze the behavior of the center under Darboux transformations,\nestablishing explicit relationships between the centers of Darboux-equivalent\nweights. These results are illustrated through the study of both reducible and\nirreducible matrix weights, including a detailed analysis of an irreducible\nJacobi-type weight.",
        "We propose an algorithm generating planar networks which structure resembles\na hierarchical structure of desiccation crack patterns.",
        "In scalar-tensor theories beyond Horndeski, the Vainshtein screening\nmechanism is only partially effective inside astrophysical bodies. We\ninvestigate the potential to detect this partial breaking of Vainshtein\nscreening through the tidal response of fluid bodies. Specifically, we\ncalculate the gravitomagnetic tidal Love numbers and analyze how deviations\nfrom general relativity depend on parameters governing the breaking of\nVainshtein screening in the weak-gravity regime. For fixed parameter values,\nthe relative deviations increase with higher multipoles and larger compactness.\nHowever, we demonstrate that these parameters alone are insufficient to fully\ncharacterize the tidal response of relativistic bodies in scalar-tensor\ntheories beyond Horndeski.",
        "We classify the possible elementary amenable fundamental groups of compact\naspherical 4-manifolds with boundary and conclude that they are either\npolycyclic or solvable Baumslag- Solitar. Since these groups are good and\nsatisfy the Farrell-Jones Conjecture, one concludes that such manifolds satisfy\ntopological rigidity: a homotopy equivalence which is a homeomorphism on the\nboundary is homotopic, relative to the boundary, to a homeomorphism. We\nclassify the closed 3-manifolds which arise as the boundary of an compact\naspherical 4-manifold with elementary amenable fundamental group, generalizing\nresults of Freedman and Quinn in the cases of trivial and infinite cyclic\nfundamental groups. Moreover, two such 4-manifolds are homeomorphic if and only\nif their \"enhanced\" peripheral group systems are equivalent, and each such\nmanifold is the boundary connected sum of a compact aspherical 4-manifold with\nprime boundary and a contractible 4-manifold.",
        "In this work, we present higher order div-curl type estimates in the sense of\nCoifman, Lions, Meyer & Semmes in the local setup of elliptic linear\ndifferential operators with smooth coefficients on localizable Hardy spaces.\nOur version implies and extends results obtained for first order operators\nassociated to elliptic systems and complexes of vector fields. As tools, with\nown interest, we develop a new smooth atomic decomposition on localizable\nHardy-Sobolev spaces and a Poincar\\'e type inequality.",
        "The recently introduced resource theory of imaginarity facilitates a\nsystematic investigation into the role of complex numbers in quantum mechanics\nand quantum information theory. In this work, we propose well-defined measures\nof imaginarity using various distance metrics, drawing inspiration from recent\nadvancements in quantum entanglement and coherence. Specifically, we focus on\nquantitatively evaluating imaginarity through measures such as Tsallis relative\n$\\alpha$-entropy, Sandwiched R\\'{e}nyi relative entropy, and Tsallis relative\noperator entropy. Additionally, we analyze the decay rates of these measures.\nOur findings reveal that the Tsallis relative $\\alpha$-entropy of imaginarity\nexhibits higher decay rate under quantum channels compared to other measures.\nFinally, we examine the ordering of single-qubit states under these imaginarity\nmeasures, demonstrating that the order remains invariant under the bit-flip\nchannel for specific parameter ranges. This study enhances our understanding of\nimaginarity as a quantum resource and its potential applications in quantum\ninformation theory.",
        "We show a result inspired by a conjecture by Shulman claiming that\nultracategories as defined by Lurie are normal colax algebras for a certain\npseudo-monad on the category of categories CAT. Such definition allows us to\nregard left and right ultrafunctors as defined by Lurie as instances of\nlax\/colax algebras morphisms",
        "Background and purpose: Upright proton therapy with compact delivery systems\nhas the potential to reduce costs for treatments but could also lead to\nbroadening of the beam penumbra. This study aims at combining upright static\nproton arcs with additional layers of shoot-through (ST) protons to sharpen the\nbeam penumbra and improve plan quality for such systems. An additional\nadvantage of the method is that it provides a straightforward approach for\nrange verification.\n  Methods: We examined various treatment plans for a virtual phantom: 3-beam\nIMPT, static arc (Arc) with\/without ST (Arc+ST), and with\/without collimation\n(+Coll). In the virtual phantom three different targets were utilized to study\nthe effect on conformity index (CI), homogeneity index (HI), robustness and\nmean dose to the phantom volume. The phantom study was complemented with a\nhead-and-neck (H&N) patient case with a similar set of plans. A range\nverification concept that determines residual ranges of the ST protons was\nstudied in simulated scenarios for the H&N case.\n  Results: The Arc+ST plans show superior CI, HI and target robustness compared\nto the Arc+Coll plans. For the Arc plans without ST, the collimated plans\nperform better than the uncollimated plans. For Arc+ST, on the other hand,\ncollimation has little impact on CI, HI and robustness. However, a small\nincrease in the mean dose to the phantom volume is seen without collimation.\nFor the H&N case, similar improvements for Arc+ST can be seen with only a\nmarginal increase of the mean dose to the patient volume. The range\nverification simulation shows that the method is suitable to detect range\nerrors.\n  Conclusions: Combining proton arcs and ST layers can enhance compact upright\nproton solutions by improving plan quality. It is also tailored for the\ninclusion of a fast and straightforward residual range verification method.",
        "Quantum Darwinism recognizes that decoherence imprints redundant records of\npreferred quasi-classical pointer states on the environment. These redundant\nrecords are then accessed by observers. We show how redundancy enables and even\nimplies consensus between observers who use fragments of that decohering\nenvironment to acquire information about systems of interest. We quantify\nconsensus using information-theoretic measures that employ mutual information\nto assess the correlation between the records available to observers from\ndistinct -- hence, independently accessible -- fragments of the environment. We\nprove that when these fragments have enough information about a system,\nobservers that access them will attribute the same pointer state to that\nsystem. Thus, those who know enough about the system agree about what they\nknow. We then test proposed measures of consensus in a solvable model of\ndecoherence as well as in numerical simulations of a many-body system. These\nresults provide detailed understanding of how our classical everyday world\narises from within the fundamentally quantum Universe we inhabit."
      ]
    }
  },
  {
    "id":2411.1057,
    "research_type":"applied",
    "start_id":"b13",
    "start_title":"Using deep autoencoders to identify abnormal brain structural patterns in neuropsychiatric disorders: A large\u2010scale multi\u2010sample study",
    "start_abstract":"Machine learning is becoming an increasingly popular approach for investigating spatially distributed and subtle neuroanatomical alterations in brain\u2010based disorders. However, some machine learning models have been criticized for requiring a large number of cases in each experimental group, and for resembling a \u201cblack box\u201d that provides little or no insight into the nature of the data. In this article, we propose an alternative conceptual and practical approach for investigating brain\u2010based disorders which aim to overcome these limitations. We used an artificial neural network known as \u201cdeep autoencoder\u201d to create a normative model using structural magnetic resonance imaging data from 1,113 healthy people. We then used this model to estimate total and regional neuroanatomical deviation in individual patients with schizophrenia and autism spectrum disorder using two independent data sets (n =\u2009263). We report that the model was able to generate different values of total neuroanatomical deviation for each disease under investigation relative to their control group (p <\u2009.005). Furthermore, the model revealed distinct patterns of neuroanatomical deviations for the two diseases, consistent with the existing neuroimaging literature. We conclude that the deep autoencoder provides a flexible and promising framework for assessing total and regional neuroanatomical deviations in neuropsychiatric populations.",
    "start_categories":[
      "cs.NE"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b14"
      ],
      "title":[
        "Using normative modelling to detect disease progression in mild cognitive impairment and Alzheimer\u2019s disease in a cross-sectional multi-cohort study"
      ],
      "abstract":[
        "Normative modelling is an emerging method for quantifying how individuals deviate from the healthy populational pattern. Several machine learning models have been implemented to develop normative models to investigate brain disorders, including regression, support vector machines and Gaussian process models. With the advance of deep learning technology, the use of deep neural networks has also been proposed. In this study, we assessed normative models based on deep autoencoders using structural neuroimaging data from patients with Alzheimer\u2019s disease (n\u2009=\u2009206) and mild cognitive impairment (n\u2009=\u2009354). We first trained the autoencoder on an independent dataset (UK Biobank dataset) with 11,034 healthy controls. Then, we estimated how each patient deviated from this norm and established which brain regions were associated to this deviation. Finally, we compared the performance of our normative model against traditional classifiers. As expected, we found that patients exhibited deviations according to the severity of their clinical condition. The model identified medial temporal regions, including the hippocampus, and the ventricular system as critical regions for the calculation of the deviation score. Overall, the normative model had comparable cross-cohort generalizability to traditional classifiers. To promote open science, we are making all scripts and the trained models available to the wider research community."
      ],
      "categories":[
        "q-bio.NC"
      ]
    },
    "list":{
      "title":[
        "What Drives Cluster Cool-Core Transformations? A Population Level\n  Analysis of TNG-Cluster",
        "A First Look at \"Continuous Spin\" Gravity -- Time Delay Signatures",
        "Unified model of the Hall effect from insulator to overdoped compounds\n  in cuprate superconductors",
        "Every group is the automorphism group of a graph with arbitrarily large\n  genus",
        "Equivalence principles in Weyl transverse gravity",
        "Doping dependence of the magnetic ground state in the frustrated magnets\n  Ba$_2$$M$Te$_{1-x}$W$_{x}$O$_6$ ($M$ = Mn, Co)",
        "Constrained differential operators, Sobolev inequalities, and Riesz\n  potentials",
        "Semi-Parametric Batched Global Multi-Armed Bandits with Covariates",
        "Topological Data Analysis of Abelian Magnetic Monopoles in Gauge\n  Theories",
        "Two-sided bounds on the point-wise spatial decay of ground states in the\n  renormalized Nelson model with confining potentials",
        "Elusive properties of countably infinite graphs",
        "Quantum stick-slip motion in nanoscaled friction",
        "Constructing Fundamentals for the Theory of Proportions and Symbolic\n  Allusions Applied Interdisciplinarily",
        "Observation of the $W$-annihilation process $D_s^+ \\to \\omega\\rho^+$ and\n  measurement of $D_s^+ \\to \\phi\\rho^+$ in $D^+_s\\to \\pi^+\\pi^+\\pi^-\\pi^0\\pi^0$\n  decays",
        "PAH Feature Ratios Around Stellar Clusters and Associations in 19 Nearby\n  Galaxies",
        "Exact collective occupancies of the Moshinsky model in two-dimensional\n  geometry",
        "Semilinear Dynamic Programming: Analysis, Algorithms, and Certainty\n  Equivalence Properties",
        "On the test properties of the Frobenius endomorphism",
        "Possible Explanation of $F_2^n\/F_2^p$ at Large $x$ Using Quantum\n  Statistical Mechanics",
        "Detection and estimation of vertex-wise latent position shifts across\n  networks",
        "Disproving some theorems in Sharma and Chauhan et al. (2018, 2021)",
        "Field Aligned Currents and Aurora During the Terrestrial Alfven Wing\n  State",
        "Linear sigma model with quarks and Polyakov loop in rotation: phase\n  diagrams, Tolman-Ehrenfest law and mechanical properties",
        "On the query complexity of sampling from non-log-concave distributions",
        "Laser induced Compton Scattering to Dark Matter in Effective Field\n  Theory",
        "Tomographic measurement data of states that never existed",
        "Geometric Interpretations of the $k$-Nearest Neighbour Distributions",
        "Complementary signatures of $\\alpha-$attractor inflation in CMB and\n  cosmic string Gravitational Waves",
        "Lattice calculation of the $D_s\\mapsto X \\ell \\bar{\\nu}_\\ell$ inclusive\n  decay rate: an overview"
      ],
      "abstract":[
        "In this study, we examine the frequency and physical drivers of\ntransformations from cool-core (CC) to non-cool-core (NCC) clusters, and vice\nversa, in a sample of 352 massive galaxy clusters (M_vir = 10^14-15.3 M_sun)\nfrom the TNG-Cluster magnetohydrodynamical cosmological simulation of galaxies.\nBy identifying transformations based on the evolution of central entropy and\nfocusing on z<2.5, we find that clusters frequently undergo such events,\ndepending on their assembly and supermassive black hole histories. On average,\nclusters experience 2 to 3 transformations. Transformations can occur in both\ndirections and can be temporary, but those to higher entropy cores, i.e. in the\ndirection from CC to NCC states, are the vast majority. CC phases are shorter\nthan NCC phases, and thus overall the TNG-Cluster population forms with\nlow-entropy cores and moves towards NCC states with time. We study the role\nthat mergers play in driving transformations, and find that mergers within\n~1Gyr prior to a transformation toward higher (but not lower) entropy cores\noccur statistically more often than in a random control sample. Most\nimportantly, we find examples of mergers associated with CC disruption\nregardless of their mass ratio or angular momentum. However, past merger\nactivity is not a good predictor for z=0 CC status, at least based on core\nentropy, even though clusters undergoing more mergers eventually have the\nhighest core entropy values at z=0. We consider the interplay between AGN\nfeedback and evolving cluster core thermodynamics. We find that core\ntransformations are accompanied by an increase in AGN activity, whereby\nfrequent and repeated (kinetic) energy injections from the central SMBHs can\nproduce a collective, long-term impact on central entropy, ultimately heating\ncluster cores. Whether such fast-paced periods of AGN activity are triggered by\nmergers is plausible, but not necessary.",
        "We consider the possibility that gravity is mediated by \"continuous spin\"\nparticles, i.e., massless particles whose invariant spin scale $\\rho_g$ is\nnon-zero. In this case, the primary helicity-2 modes of gravitational radiation\non a Minkowski background mix with a tower of integer-helicity partner modes\nunder boosts, with $\\rho_g$ controlling the degree of mixing. We develop a\nformalism for coupling spinless matter to continuous spin gravity at linearized\nlevel. Using this formalism, we calculate the time delay signatures induced by\ngravitational waves in an idealized laser interferometer detector. The\nfractional deviation from general relativity predictions is $O(\\rho_g\/\\omega)$\nfor gravitational wave frequencies $\\omega >\\rho_g$, and the effects of waves\nwith $\\omega \\lesssim \\rho_g$ are damped. The precision and low frequency\nranges of gravitational wave detectors suggest potential sensitivity to spin\nscales at or below $\\sim 10^{-14}$ eV at ground-based laser interferometers and\n$\\sim 10^{-24}$ eV at pulsar timing arrays, motivating further analysis of\nobservable signatures.",
        "Measurements of the Hall coefficient in La$_{2-x}$Sr$_x$CuO$_4$, ranging from\nthe undoped ($x = p = 0$) Mott insulator to overdoped compounds, exhibit a\ntemperature dependence that offers insights into their electronic structure. We\ninterpret these results using a model based on the theory of phase-separation\n(PS) dynamics, which begins at half-filled ($n = 1$) and at a temperature\n$T_{\\rm PS}(p)$, near the pseudogap temperature $T^*(p)$. The $n = 1$ holes\nhave low mobility and provide the modulations of the charge density waves\n(CDW). As doping increases from $p = 0$, these modulations guide the additional\np holes to occupy alternating CDW domains. This charge inhomogeneity may\nfacilitate the formation of localized superconducting amplitudes below the\ncritical onset temperature $T_{\\rm c}^{\\rm max}(p)$. Using thermal activation\nexpressions, along with quantum tunnelling between the charge domains, we\nsuccessfully reproduce all Hall coefficient measurements $R_{\\rm H}(p,T)$ and\nhighlight the relevant energies of cuprates. The calculations confirm three\nsignificant electronic features: the phase-separating role of the pseudogap\ntemperature, the superconducting state achieved through phase coherence,\n  and the two types of charge carriers whose energies and mobilities become\ncomparable at $p \\approx 0.19$, where\n  $T^*(p) \\approx T_{\\rm c}^{\\rm max}(p)$. This results in a crossover from $n\n= p$ to $n = 1 + p$. These findings, along with the\n  $R_{\\rm H}(p,T)$ calculations from insulating to overdoped compounds,\nunderscore the critical role of the electronic\n  phase separation in the properties of cuprates.",
        "We prove that, to every abstract group $G$, we can associate a sequence of\ngraphs $\\Gamma_n$ such that the automorphism group of $\\Gamma_n$ is isomorphic\nto $G$ and the genus of $\\Gamma_n$ is an unbounded function of $n$.",
        "There exist two consistent theories of massless, self-interacting gravitons,\nwhich differ by their local symmetries: general relativity and Weyl transverse\ngravity. We show that these two theories are also the only two metric\ndescriptions of gravity in 4 spacetime dimensions which obey the equivalence\nprinciple for test gravitational physics. We further analyse how the weaker\nformulations of the equivalence principle are realised in Weyl transverse\ngravity (and its generalisations). The analysis sheds light on the behaviour of\nmatter fields in this theory.",
        "Theoretically, the relative change of the Heisenberg-type nearest-neighbor\ncoupling $J_1$ and next-nearest-neighbor coupling $J_2$ in the\nface-centered-cubic lattice can give rise to three main antiferromagnetic\norderings of type-I, type-II, and type-III. However, it is difficult to tune\nthe $J_2\/J_1$ ratio in real materials. Here, we report studies on the influence\nof Te$^{6+}$ and W$^{6+}$ ions replacement to the magnetic interactions and the\nmagnetic ground states in the double-perovskite compounds\nBa$_2$$M$Te$_{1-x}$W$_{x}$O$_6$ ($M$ = Mn, Co). For\nBa$_2$MnTe$_{1-x}$W$_{x}$O$_6$, the W$^{6+}$ doping on Te$^{6+}$ site is\nsuccessful in $0.02 \\leq x \\leq 0.9$ with short-range orders of the type-I\n($0.02 \\leq x \\leq 0.08$) and type-II ($0.1 \\leq x \\leq 0.9$). In\nBa$_2$CoTe$_{1-x}$W${_x}$O$_6$, x-ray diffraction measurements reveal two\ncrystal structures, including the trigonal phase ($0 \\leq x \\leq 0.1$) and the\ncubic phase ($0.5 \\leq x \\leq 1$), between which is a miscibility gap. Two\nmagnetic transitions are identified in the trigonal phase due to two magnetic\nsubsystems, and the type-II magnetic order is observed in the cubic phase.\nMagnetic phase diagrams of Ba$_2M$Te$_{1-x}$W$_{x}$O$_6$ ($M$ = Mn, Co) are\nestablished. Our work shows that the magnetic interactions and ground states of\nBa$_2$$M$Te$_{1-x}$W$_x$O$_6$ can be tuned effectively by the replacement of\nTe$^{6+}$ by W$^{6+}$ ions.",
        "Inequalities for Riesz potentials are well-known to be equivalent to Sobolev\ninequalities of the same order for domain norms \"far\" from $L^1$, but to be\nweaker otherwise. Recent contributions by Van Schaftingen, by Hernandez,\nRai\\c{t}\\u{a} and Spector, and by Stolyarov proved that this gap can be filled\nin Riesz potential inequalities for vector-valued functions in $L^1$ fulfilling\na co-canceling differential condition. This work demonstrates that such a\nproperty is not just peculiar to the space $L^1$. Indeed, under the same\ndifferential constraint, a Riesz potential inequality is shown to hold for any\ndomain and target rearrangement-invariant norms that render a Sobolev\ninequality of the same order true. This is based on a new interpolation\ninequality, which, via a kind of duality argument, yields a parallel property\nof Sobolev inequalities for any linear homogeneous elliptic canceling\ndifferential operator. Specifically, Sobolev inequalities involving the full\ngradient of a certain order share the same rearrangement-invariant domain and\ntarget spaces as their analogs for any other homogeneous elliptic canceling\ndifferential operator of equal order. As a consequence, Riesz potential\ninequalities under the co-canceling constraint and Sobolev inequalities for\nhomogeneous elliptic canceling differential operators are offered for general\nfamilies of rearrangement-invariant spaces, such as the Orlicz spaces and the\nLorentz-Zygmund spaces. Especially relevant instances of inequalities for\ndomain spaces neighboring $L^1$ are singled out.",
        "The multi-armed bandits (MAB) framework is a widely used approach for\nsequential decision-making, where a decision-maker selects an arm in each round\nwith the goal of maximizing long-term rewards. Moreover, in many practical\napplications, such as personalized medicine and recommendation systems,\nfeedback is provided in batches, contextual information is available at the\ntime of decision-making, and rewards from different arms are related rather\nthan independent. We propose a novel semi-parametric framework for batched\nbandits with covariates and a shared parameter across arms, leveraging the\nsingle-index regression (SIR) model to capture relationships between arm\nrewards while balancing interpretability and flexibility. Our algorithm,\nBatched single-Index Dynamic binning and Successive arm elimination (BIDS),\nemploys a batched successive arm elimination strategy with a dynamic binning\nmechanism guided by the single-index direction. We consider two settings: one\nwhere a pilot direction is available and another where the direction is\nestimated from data, deriving theoretical regret bounds for both cases. When a\npilot direction is available with sufficient accuracy, our approach achieves\nminimax-optimal rates (with $d = 1$) for nonparametric batched bandits,\ncircumventing the curse of dimensionality. Extensive experiments on simulated\nand real-world datasets demonstrate the effectiveness of our algorithm compared\nto the nonparametric batched bandit method introduced by\n\\cite{jiang2024batched}.",
        "Motivated by recent literature on the possible existence of a second\nhigher-temperature phase transition in Quantum Chromodynamics, we revisit the\nproposal that colour confinement is related to the dynamics of magnetic\nmonopoles using methods of Topological Data Analysis, which provides a\nmathematically rigorous characterisation of topological properties of\nquantities defined on a lattice. After introducing persistent homology, one of\nthe main tools in Topological Data Analysis, we shall discuss how this concept\ncan be used to quantitatively analyse the behaviour of monopoles across the\ndeconfinement phase transition. Our approach is first demonstrated for Compact\n$U(1)$ Lattice Gauge Theory, which is known to have a zero-temperature\ndeconfinement phase transition driven by the restoration of the symmetry\nassociated with the conservation of the magnetic charge. For this system, we\nperform a finite-size scaling analysis of observables capturing the homology of\nmagnetic current loops, showing that the expected value of the deconfinement\ncritical coupling is reproduced by our analysis. We then extend our method to\n$SU(3)$ gauge theory, in which Abelian magnetic monopoles are identified after\nprojection in the Maximal Abelian Gauge. A finite-size scaling of our\nhomological observables of Abelian magnetic current loops at temporal size $N_t\n= 4$ provides the expected value of the critical coupling with an accuracy that\nis generally higher than that obtained with conventional thermodynamic\napproaches at comparable statistics, hinting towards the relevance of\ntopological properties of monopole currents for confinement.",
        "We study the renormalized Nelson model for a scalar matter particle in a\ncontinuous confining potential interacting with a possibly massless quantized\nradiation field. When the radiation field is massless we impose a mild infrared\nregularization ensuring that the Nelson Hamiltonian has a non-degenerate ground\nstate in all considered cases. Employing Feynman-Kac representations, we derive\nlower bounds on the point-wise spatial decay of the partial Fock space norms of\nground state eigenvectors. Here the exponential rate function governing the\ndecay is given by the Agmon distance familiar from the analysis of\nSchr\\\"{o}dinger operators. For a large class of confining potentials, our lower\nbounds on the decay of ground state eigenvectors match asymptotically with the\nupper bounds implied by previous work of the present authors.",
        "A graph property is elusive (or evasive) if any algorithm testing it by\nasking questions of the form ''Is there an edge between vertices x and y?''\nmust, in the worst case, examine all pairs of vertices. Elusiveness for\ninfinite vertex sets has been first studied by Csern\\'ak and Soukup, who proved\nthat the long-standing Aanderaa-Karp-Rosenberg Conjecture -- which states that\nevery nontrivial monotone graph property is elusive -- fails for infinite\nvertex sets. We extend their work by giving a closer look to the case when the\nvertex set is countably infinite and the ''algorithm'' terminates after\ninfinitely many steps. Among others, we prove that connectedness is elusive,\nwhich strengthens a result of Csern\\'ak and Soukup. We give counterexamples to\nthe infinite version of the Aanderaa-Karp-Rosenberg Conjecture even if the\n''algorithm'' is required to terminate after infinitely many steps, which\nstrengthens results of Csern\\'ak and Soukup.",
        "Friction in atomistic systems is usually described by the classical\nPrandtl-Tomlinson model suitable for capturing the dragging force of a\nnanoparticle in a periodic potential. Here we consider the quantum mechanical\nversion of this model in which the dissipation is facilitated by releasing heat\nto an external bath reservoir. The time evolution of the system is captured\nwith the Liouville-von Neumann equation through the density matrix of the\nsystem in the Markov approximation. We examine several kinetic and dissipative\nproperties of the nanoparticle by delineating classical vs quantum mechanical\neffects. We find that that the Landau-Zener tunneling is a key factor in the\noverall reduction of the frictional dissipation when compared to the classical\nmotion in which such tunneling is absent. This in-depth study analyzes the\ninterplay between velocity, strength of interaction, and temperature to control\nthe frictional process and provide useful guidelines for experimental data\ninterpretation.",
        "The Theory of Proportions and Symbolic Allusions applied Interdisciplinary\n(TPASAI) is a framework that integrates mathematics, linguistics, psychology,\nand game theory to uncover hidden patterns and proportions in reality. Its\ncentral idea is that numerical encoding of symbols, dates, and language can\nreveal recurring structures and connections that reflect universal principles.\nBy applying fractal analysis, the theory identifies patterns across different\nscales, offering a unifying perspective on the structure of the world. One key\naspect of TPASAI is symbolic analysis, which allows for the reinterpretation of\ntraumatic experiences in psychotherapy. For example, assigning numerical values\nto elements like fingers, dates, or words can help individuals uncover\nmeaningful associations between personal experiences and collective symbols.\nThis approach encourages cognitive flexibility and provides a therapeutic\navenue for recontextualizing emotions. The theory also incorporates principles\nof game theory, which frame reality as a system of symbolic \"codes\" governed by\nrules that can be understood and strategically used. This perspective is\nespecially useful for psychological conditions like obsessive-compulsive\ndisorder (OCD), enabling patients to approach their obsessions as decipherable\npatterns rather than rigid constraints. TPASAI has practical applications in\npsychology, education, and technology. In education, it aids in teaching\nmathematical and linguistic concepts by exploring connections between symbolic\nrepresentations and real-world events. In technology, the methodology can be\nemployed in ciphering and natural language processing. The innovation of TPASAI\nlies in its ability to merge the structured rigor of mathematics with the\ninterpretative flexibility of symbolic analysis, offering a deeper\nunderstanding of events and relationships.",
        "We present the first amplitude analysis and branching fraction measurement of\nthe decay $D^+_s\\to \\pi^+\\pi^+\\pi^-\\pi^0\\pi^0$, using $e^+e^-$ collision data\ncollected with the BESIII detector at center-of-mass energies between 4.128 and\n4.226 GeV corresponding to an integrated luminosity of 7.33 fb$^{-1}$, and\nreport the first observation of the pure $W$-annihilation decay $D_s^+ \\to\n\\omega\\rho^+$ with a branching fraction of $(0.99\\pm0.08_{\\rm stat}\\pm0.07_{\\rm\nsyst})\\%$. In comparison to the low significance of the $\\mathcal{D}$ wave in\nthe decay $D_s^+ \\to \\phi\\rho^+$, the dominance of the $\\mathcal{D}$ wave over\nthe $\\mathcal{S}$ and $\\mathcal{P}$ waves, with a fraction of\n$(51.85\\pm7.28_{\\rm stat}\\pm7.90_{\\rm syst})\\%$ observed in the decay, provides\ncrucial information for the``polarization puzzle\", as well as for the\nunderstanding of charm meson decays. The branching fraction of $D^+_s\\to\n\\pi^+\\pi^+\\pi^-\\pi^0\\pi^0$ is measured to be $(4.41\\pm0.15_{\\rm\nstat}\\pm0.13_{\\rm syst})\\%$. Moreover, the branching fraction of $D_s^+ \\to\n\\phi\\rho^+$ is measured to be $(3.98\\pm0.33_{\\rm stat}\\pm0.21_{\\rm syst})\\%$,\nand the $R_{\\phi}= {\\mathcal{B}(\\phi\\to\\pi^+\\pi^-\\pi^0)}\/{\\mathcal{B}(\\phi\\to\nK^+K^-)}$ is determined to be $(0.222\\pm0.019_{\\rm stat}\\pm0.016_{\\rm syst}$),\nwhich is consistent with the previous measurement based on charm meson decays,\nbut deviates from the results from $e^+e^-$ annihilation and $K$-$N$ scattering\nexperiments by more than 3$\\sigma$.",
        "We present a comparison of observed polycyclic aromatic hydrocarbon (PAH)\nfeature ratios in 19 nearby galaxies with a grid of theoretical expectations\nfor near- and mid-infrared dust emission. The PAH feature ratios are drawn from\nCycle 1 JWST observations and are measured for 7224 stellar clusters and 29176\nstellar associations for which we have robust ages and mass estimates from HST\nfive-band photometry. Though there are galaxy-to-galaxy variations, the\nobserved PAH feature ratios largely agree with the theoretical models,\nparticularly those that are skewed toward more ionized and larger PAH size\ndistributions. For each galaxy we also extract PAH feature ratios for 200\npc-wide circular regions in the diffuse interstellar medium, which serve as a\nnon-cluster\/association control sample. Compared to what we find for stellar\nclusters and associations, the 3.3um\/7.7um and 3.3um\/11.3um ratios from the\ndiffuse interstellar medium are $\\sim 0.10-0.15$ dex smaller. When the observed\nPAH feature ratios are compared to the radiation field hardness as probed by\nthe [OIII]\/H$\\beta$ ratio, we find anti-correlations for nearly all galaxies in\nthe sample. These results together suggest that the PAH feature ratios are\ndriven by the shape intensity of the radiation field, and that the smallest\nPAHs -- observed via JWST F335M imaging -- are increasingly 'processed' or\ndestroyed in regions with the most intense and hard radiation fields.",
        "In this paper, we investigate the ground state of $N$ bosonic atoms confined\nin a two-dimensional isotropic harmonic trap, where the atoms interact via a\nharmonic potential. We derive an exact diagonal representation of the\nfirst-order reduced density matrix in polar coordinates, in which the angular\ncomponents of the natural orbitals are eigenstates of the angular momentum\noperator. Furthermore, we present an exact expression for the collective\noccupancy of the natural orbitals with angular momentum $l$, quantifying the\nfraction of particles carrying that angular momentum. The present study\nexplores how the dependence of collective occupancy relies on angular momentum\n$l$ and the control parameters of the system. Building on these findings, we\nexamine boson fragmentation into components with different $l$ and reveal a\nunique feature of the system: the natural orbitals contributing to the\ncorrelations are uniformly distributed across all significant $l$ components.",
        "We consider a broad class of dynamic programming (DP) problems that involve a\npartially linear structure and some positivity properties in their system\nequation and cost function. We address deterministic and stochastic problems,\npossibly with Markov jump parameters. We focus primarily on infinite horizon\nproblems and prove that under our assumptions, the optimal cost function is\nlinear, and that an optimal policy can be computed efficiently with standard DP\nalgorithms. Moreover, we show that forms of certainty equivalence hold for our\nstochastic problems, in analogy with the classical linear quadratic optimal\ncontrol problems.",
        "In this paper, we prove two theorems concerning the test properties of the\nFrobenius endomorphism over commutative Noetherian local rings of prime\ncharacteristic $p$. Our first theorem generalizes a result of Funk-Marley on\nthe vanishing of Ext and Tor modules, while our second theorem generalizes one\nof our previous results on maximal Cohen-Macaulay tensor products. In these\nearlier results, we replace $^{e}R$ with a more general module $^{e}M$, where\n$R$ is a Cohen-Macaulay ring, $M$ is a Cohen-Macaulay $R$-module with full\nsupport, and $^{e}M$ is the module viewed as an $R$-module via the $e$-th\niteration of the Frobenius endomorphism. We also provide examples and present\napplications of our results, yielding new characterizations of the regularity\nof local rings.",
        "The ratio of the neutron to proton structure functions, $F^n_2(x)\/F^p_2(x)$,\nis expected to approach 1\/4 as $x \\to 1$, based on the assumption that $d (x) \/\nu (x)$ vanishes as $x \\to 1$. This expectation is in striking disagreement with\na recent measurement by the Marathon experiment of the scattering of electrons\noff the mirror nuclei $^3$H and $^3$He, showing that $F^n_2(x)\/F^p_2(x)$ is\nlarger than 1\/4 for $ x \\rightarrow 1$. We have examined the consequences of\nthe Pauli Exclusion Principle for the parton distributions in the nucleon when\nthe partons are described by quantum statistical mechanics. We find that the\nrecent experimental result on the $F^n_2(x)\/F^p_2(x)$ over the broad range of\n$x$ can be well described by the quantum statistical approach.",
        "Pairwise network comparison is essential for various applications, including\nneuroscience, disease research, and dynamic network analysis. While existing\nliterature primarily focuses on comparing entire network structures, we address\na vertex-wise comparison problem where two random networks share the same set\nof vertices but allow for structural variations in some vertices, enabling a\nmore detailed and flexible analysis of network differences. In our framework,\nsome vertices retain their latent positions between networks, while others\nundergo shifts. To identify the shifted and unshifted vertices and estimate\ntheir latent position shifts, we propose a method that first derives vertex\nembeddings in a low-rank Euclidean space for each network, then aligns these\nestimated vertex latent positions into a common space to resolve potential\nnon-identifiability, and finally tests whether each vertex is shifted or not\nand estimates the vertex shifts. Our theoretical results establish the test\nstatistic for the algorithms, guide parameter selection, and provide\nperformance guarantees. Simulation studies and real data applications,\nincluding a case-control study in disease research and dynamic network\nanalysis, demonstrate that the proposed algorithms are both computationally\nefficient and effective in extracting meaningful insights from network\ncomparisons.",
        "The main objective of this work is to show, through counterexamples, that\nsome of the theorems presented in the papers of Sharma \\textit{et al.} (2018)\nand Chauhan \\textit{et al.} ( 2021) are incorrect. Although they used these\ntheorems to establish a sufficient condition for a multi-twisted (MT) code to\nbe linear complementary dual (LCD), we show that this condition itself remains\nvalid. We further improve this condition by removing the restrictions on the\nshift constants and relaxing the required coprimality condition. We show that\ncompared to the previous condition, the modified condition is able to identify\nmore LCD MT codes. Furthermore, without the need for a normalized set of\ngenerators, we develop a formula to determine the dimension of any\n$\\rho$-generator MT code.",
        "When sub-Alfvenic (Alfven Mach number M_ < 1) plasmas impact Earth, the\nmagnetosphere develops Alfven wings. A Multiscale Atmosphere Geospace\nEnvironment (MAGE) global simulation of the April 2023 geomagnetic storm,\nvalidated against Active Magnetosphere and Planetary Electrodynamics Response\nExperiment (AMPERE), reveals the mechanism of field-aligned current (FAC)\ngeneration and auroral precipitation for the terrestrial Alfven wings.\nSimulation and observations show northern hemisphere planetward flowing auroral\nelectrons (negative FAC) are predominantly at magnetic local times (MLTs) 8-12.\nJust before the wings formed, solar wind conditions were similar and M_A ~ 1.4,\nyet the same FAC system extended from 9-18 MLT. Flow vorticity drives FACs at\nthe boundary of the Alfven wings and unshocked solar wind. The Alfven wing\nshape presents a different obstacle to the solar wind compared to typical lobe\nfluxes, producing the unique FAC and auroral patterns. New insights about\nAlfven wing FACs will help to understand auroral features for exoplanets inside\ntheir host star's Alfven zone.",
        "We study the effect of rotation on the confining and chiral properties of QCD\nusing the Polyakov-enhanced linear sigma model coupled to quarks. Working in\nthe homogeneous approximation, we obtain the phase diagram at finite\ntemperature, baryon density and angular frequency, taking into account the\ncausality constraint enforced by the spectral boundary conditions at a\ncylindrical surface. We explicitly address various limits with respect to\nsystem size, angular frequency and chemical potential. We demonstrate that, in\nthis model, the critical temperatures of both transitions diminish in response\nto the increasing rotation, being in contradiction with the first-principle\nlattice results. In the limit of large volume, the thermodynamics of the model\nis consistent with the Tolman-Ehrenfest law. We also compute the mechanical\ncharacteristics of rotating plasma such as the moment of inertia and the $K_4$\nshape coefficient.",
        "We study the problem of sampling from a $d$-dimensional distribution with\ndensity $p(x)\\propto e^{-f(x)}$, which does not necessarily satisfy good\nisoperimetric conditions.\n  Specifically, we show that for any $L,M$ satisfying $LM\\ge d\\ge 5$,\n$\\epsilon\\in \\left(0,\\frac{1}{32}\\right)$, and any algorithm with query\naccesses to the value of $f(x)$ and $\\nabla f(x)$, there exists an\n$L$-log-smooth distribution with second moment at most $M$ such that the\nalgorithm requires $\\left(\\frac{LM}{d\\epsilon}\\right)^{\\Omega(d)}$ queries to\ncompute a sample whose distribution is within $\\epsilon$ in total variation\ndistance to the target distribution. We complement the lower bound with an\nalgorithm requiring $\\left(\\frac{LM}{d\\epsilon}\\right)^{\\mathcal O(d)}$\nqueries, thereby characterizing the tight (up to the constant in the exponent)\nquery complexity for sampling from the family of non-log-concave distributions.\n  Our results are in sharp contrast with the recent work of Huang et al.\n(COLT'24), where an algorithm with quasi-polynomial query complexity was\nproposed for sampling from a non-log-concave distribution when\n$M=\\mathtt{poly}(d)$. Their algorithm works under the stronger condition that\nall distributions along the trajectory of the Ornstein-Uhlenbeck process,\nstarting from the target distribution, are $\\mathcal O(1)$-log-smooth. We\ninvestigate this condition and prove that it is strictly stronger than\nrequiring the target distribution to be $\\mathcal O(1)$-log-smooth.\nAdditionally, we study this condition in the context of mixtures of Gaussians.\n  Finally, we place our results within the broader theme of ``sampling versus\noptimization'', as studied in Ma et al. (PNAS'19). We show that for a wide\nrange of parameters, sampling is strictly easier than optimization by a\nsuper-exponential factor in the dimension $d$.",
        "The detection of light dark matter (DM) is a longstanding challenge in\nterrestrial experiments. High-intensity facility of an intense electromagnetic\nfield may provide a plausible strategy to study strong-field particle physics\nand search for light DM. In this work, we propose to search for light DM\nparticle through the nonlinear Compton scattering in the presence of a\nhigh-intense laser field. An ultra-relativistic electron beam collides with an\nintense laser pulse of a number of optical photons and then decays to a pair of\nDM particles. We take into account the Dirac-type fermionic DM in leptophilic\nscenario and the DM-electron interactions in the framework of effective field\ntheory. The decay rates of electron to a DM pair are calculated for effective\nDM operators of different bilinear products. We show the sensitivities of laser\ninduced Compton scattering to the effective cutoff scale for DM lighter than 1\nMeV and compare with direct detection experiments.",
        "Microscopic Schr{\\\"o}dinger cat states are generated from quantum correlated\nfields using a probabilistic heralding photon subtraction event. Subsequent\nquantum state tomography provides complete information about the state with\ntypical photon numbers of the order of one. Another approach strives for a\nlarger number of quantum-correlated photons by conditioning the measurement\nanalysis on events with exactly this number of photons. Here, we present a new\napproach to derive measurement data of quantum correlated states with average\nquantum-correlated photon numbers significantly larger than one. We produce an\nensemble of a heralded, photon-subtracted squeezed vacuum state of light. We\nsplit the states at a balanced beam splitter and simultaneously measure a pair\nof orthogonal field quadratures at the outputs using tomographic `Q-function\nhomodyne detection' (QHD). The final act is probabilistic two-copy data\npost-processing aiming for data from a new state with larger photon number.\nEvaluating the final tomographic data as that of a grown microscopic\nSchr{\\\"o}dinger cat state shows that the probabilistic post-processing\nincreased the photon number of $|\\alpha_0|^2 \\approx 1.2$ to $|\\alpha_2|^2\n\\approx 6.8$. Our concept for obtaining tomographic measurement data of\nmesoscopic non-classical states that never existed might be a turning point in\nmeasurement-based quantum technology.",
        "The $k$-Nearest Neighbour Cumulative Distribution Functions are measures of\nclustering for discrete datasets that are fast and efficient to compute. They\nare significantly more informative than the 2-point correlation function. Their\nconnection to $N$-point correlation functions, void probability functions and\nCounts-in-Cells is known. However, the connections between the CDFs and other\ngeometric and topological spatial summary statistics are yet to be fully\nexplored in the literature. This understanding will be crucial to find\noptimally informative summary statistics to analyse data from stage 4\ncosmological surveys. We explore quantitatively the geometric interpretations\nof the $k$NN CDF summary statistics. We establish an equivalence between the\n1NN CDF at radius $r$ and the volume of spheres with the same radius around the\ndata points. We show that higher $k$NN CDFs are equivalent to the volumes of\nintersections of $\\ge k$ spheres around the data points. We present similar\ngeometric interpretations for the $k$NN cross-correlation joint CDFs. We\nfurther show that the volume, or the CDFs, have information about the angles\nand arc lengths created at the intersections of spheres around the data points,\nwhich can be accessed through the derivatives of the CDF. We show this\ninformation is very similar to that captured by Germ Grain Minkowski\nFunctionals. Using a Fisher analysis we compare the information content and\nconstraining power of various data vectors constructed from the $k$NN CDFs and\nMinkowski Functionals. We find that the CDFs and their derivatives and the\nMinkowski Functionals have nearly identical information content. However, $k$NN\nCDFs are computationally orders of magnitude faster to evaluate. Finally, we\nfind that there is information in the full shape of the CDFs, and therefore\ncaution against using the values of the CDF only at sparsely sampled radii.",
        "When cosmic strings are formed during inflation, they regrow to reach a\nscaling regime, leaving distinct imprints on the stochastic gravitational wave\nbackground (SGWB). Such signatures, associated with specific primordial\nfeatures, can be detected by upcoming gravitational wave observatories, such as\nthe LISA and Einstein Telescope (ET). Our analysis explores scenarios in which\ncosmic strings form either before or during inflation. We examine how the\nnumber of e-folds experienced by cosmic strings during inflation correlates\nwith the predictions of inflationary models observable in cosmic microwave\nbackground (CMB) measurements. This correlation provides a testable link\nbetween inflationary physics and the associated gravitational wave signals in a\ncomplementary manner. Focusing on $\\alpha$-attractor models of inflation, with\nthe Polynomial $\\alpha$-attractor serving as an illustrative example, we find\nconstraints, for instance, on the spectral index $n_s$ to $0.962 \\lesssim n_s\n\\lesssim 0.972$ for polynomial exponent $n=1$, $0.956 \\lesssim n_s \\lesssim\n0.968$ for $n=2$, $0.954 \\lesssim n_s \\lesssim 0.965$ for $n=3$, and $0.963\n\\lesssim n_s \\lesssim 0.964$ for $n=4$, which along with the GW signals from\nLISA, are capable of detecting local cosmic strings that have experienced $\\sim\n34 - 47$ e-folds of inflation consistent with current Planck data and are also\ntestable in upcoming CMB experiments such as LiteBIRD and CMB-S4.",
        "In this talks, on behalf of our collaboration we present an overview of our\nfirst-principle lattice QCD calculation of the $D_s\\mapsto X \\ell\n\\bar{\\nu}_\\ell$ inclusive decay rate. Here we introduce the theoretical\nbackground and focus on the methodological aspects of the calculation. A\ndetailed discussion of our results, including the comparison with the\ncorresponding experimental measurements will soon be presented in a forthcoming\npublication."
      ]
    }
  },
  {
    "id":2411.1057,
    "research_type":"applied",
    "start_id":"b14",
    "start_title":"Using normative modelling to detect disease progression in mild cognitive impairment and Alzheimer\u2019s disease in a cross-sectional multi-cohort study",
    "start_abstract":"Normative modelling is an emerging method for quantifying how individuals deviate from the healthy populational pattern. Several machine learning models have been implemented to develop normative models to investigate brain disorders, including regression, support vector machines and Gaussian process models. With the advance of deep learning technology, the use of deep neural networks has also been proposed. In this study, we assessed normative models based on deep autoencoders using structural neuroimaging data from patients with Alzheimer\u2019s disease (n\u2009=\u2009206) and mild cognitive impairment (n\u2009=\u2009354). We first trained the autoencoder on an independent dataset (UK Biobank dataset) with 11,034 healthy controls. Then, we estimated how each patient deviated from this norm and established which brain regions were associated to this deviation. Finally, we compared the performance of our normative model against traditional classifiers. As expected, we found that patients exhibited deviations according to the severity of their clinical condition. The model identified medial temporal regions, including the hippocampus, and the ventricular system as critical regions for the calculation of the deviation score. Overall, the normative model had comparable cross-cohort generalizability to traditional classifiers. To promote open science, we are making all scripts and the trained models available to the wider research community.",
    "start_categories":[
      "q-bio.NC"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b13"
      ],
      "title":[
        "Using deep autoencoders to identify abnormal brain structural patterns in neuropsychiatric disorders: A large\u2010scale multi\u2010sample study"
      ],
      "abstract":[
        "Machine learning is becoming an increasingly popular approach for investigating spatially distributed and subtle neuroanatomical alterations in brain\u2010based disorders. However, some machine learning models have been criticized for requiring a large number of cases in each experimental group, and for resembling a \u201cblack box\u201d that provides little or no insight into the nature of the data. In this article, we propose an alternative conceptual and practical approach for investigating brain\u2010based disorders which aim to overcome these limitations. We used an artificial neural network known as \u201cdeep autoencoder\u201d to create a normative model using structural magnetic resonance imaging data from 1,113 healthy people. We then used this model to estimate total and regional neuroanatomical deviation in individual patients with schizophrenia and autism spectrum disorder using two independent data sets (n =\u2009263). We report that the model was able to generate different values of total neuroanatomical deviation for each disease under investigation relative to their control group (p <\u2009.005). Furthermore, the model revealed distinct patterns of neuroanatomical deviations for the two diseases, consistent with the existing neuroimaging literature. We conclude that the deep autoencoder provides a flexible and promising framework for assessing total and regional neuroanatomical deviations in neuropsychiatric populations."
      ],
      "categories":[
        "cs.NE"
      ]
    },
    "list":{
      "title":[
        "Performance of Practical Quantum Oblivious Key Distribution",
        "SCE: Scalable Consistency Ensembles Make Blackbox Large Language Model\n  Generation More Reliable",
        "Heterogenous Macro-Finance Model: A Mean-field Game Approach",
        "PBM-VFL: Vertical Federated Learning with Feature and Sample Privacy",
        "Psy-Insight: Explainable Multi-turn Bilingual Dataset for Mental Health\n  Counseling",
        "Logistic regression models: practical induced prior specification",
        "Perturbative solutions for compact objects in (2+1)-dimensional\n  Bopp-Podolsky electrodynamics",
        "How to compute the volume in low dimension?",
        "Deeply Optimizing the SAT Solver for the IC3 Algorithm",
        "Remarks on log pluricanonical representations",
        "Dedicated Inference Engine and Binary-Weight Neural Networks for\n  Lightweight Instance Segmentation",
        "Run-and-tumble particles with 1D Coulomb interaction: the active jellium\n  model and the non-reciprocal self-gravitating gas",
        "Revealing the Implicit Noise-based Imprint of Generative Models",
        "Rationality and categorical properties of the moduli of instanton\n  bundles on the projective 3-space",
        "A Spiral Bicycle Track that Can Be Traced by a Unicycle",
        "Silent Branding Attack: Trigger-free Data Poisoning Attack on\n  Text-to-Image Diffusion Models",
        "A Unified Model of Text and Citations for Topic-Specific Citation\n  Networks",
        "Backtracking for Safety",
        "Improving robot understanding using conversational AI: demonstration and\n  feasibility study",
        "Genus formulas for families of modular curves",
        "DPF-Net: Physical Imaging Model Embedded Data-Driven Underwater Image\n  Enhancement",
        "Learning Orientation Field for OSM-Guided Autonomous Navigation",
        "Improving Multi-Label Contrastive Learning by Leveraging Label\n  Distribution",
        "The class of Aronszajn lines under epimorphisms",
        "Indoor Fusion Positioning Based on \"IMU-Ultrasonic-UWB\" and Factor Graph\n  Optimization Method",
        "Robust Watermarks Leak: Channel-Aware Feature Extraction Enables\n  Adversarial Watermark Manipulation",
        "Impact of Lead Time on Aggregate EV Flexibility for Congestion\n  Management Services",
        "Task-Agnostic Attacks Against Vision Foundation Models",
        "Single-Letter Characterization of the Mismatched Distortion-Rate\n  Function"
      ],
      "abstract":[
        "Motivated by the applications of secure multiparty computation as a\nprivacy-protecting data analysis tool, and identifying oblivious transfer as\none of its main practical enablers, we propose a practical realization of\nrandomized quantum oblivious transfer. By using only symmetric cryptography\nprimitives to implement commitments, we construct computationally-secure\nrandomized oblivious transfer without the need for public-key cryptography or\nassumptions imposing limitations on the adversarial devices. We show that the\nprotocol is secure under an indistinguishability-based notion of security and\ndemonstrate an experimental implementation to test its real-world performance.\nIts security and performance are then compared to both quantum and classical\nalternatives, showing potential advantages over existing solutions based on the\nnoisy storage model and public-key cryptography.",
        "Large language models (LLMs) have demonstrated remarkable performance, yet\ntheir diverse strengths and weaknesses prevent any single LLM from achieving\ndominance across all tasks. Ensembling multiple LLMs is a promising approach to\ngenerate reliable responses but conventional ensembling frameworks suffer from\nhigh computational overheads. This work introduces Scalable Consistency\nEnsemble (SCE), an efficient framework for ensembling LLMs by prompting\nconsistent outputs. The SCE framework systematically evaluates and integrates\noutputs to produce a cohesive result through two core components: SCE-CHECK, a\nmechanism that gauges the consistency between response pairs via semantic\nequivalence; and SCE-FUSION, which adeptly merges the highest-ranked consistent\nresponses from SCE-CHECK, to optimize collective strengths and mitigating\npotential weaknesses. To improve the scalability with multiple inference\nqueries, we further propose ``{You Only Prompt Once}'' (YOPO), a novel\ntechnique that reduces the inference complexity of pairwise comparison from\nquadratic to constant time. We perform extensive empirical evaluations on\ndiverse benchmark datasets to demonstrate \\methodName's effectiveness. Notably,\nthe \\saccheckcomponent outperforms conventional baselines with enhanced\nperformance and a significant reduction in computational overhead.",
        "We investigate the full dynamics of capital allocation and wealth\ndistribution of heterogeneous agents in a frictional economy during booms and\nbusts using tools from mean-field games. Two groups in our models, namely the\nexpert and the household, are interconnected within and between their classes\nthrough the law of capital processes and are bound by financial constraints.\nSuch a mean-field interaction explains why experts accumulate a lot of capital\nin the good times and reverse their behavior quickly in the bad times even in\nthe absence of aggregate macro-shocks. When common noises from the market are\ninvolved, financial friction amplifies the mean-field effect and leads to\ncapital fire sales by experts. In addition, the implicit interlink between and\nwithin heterogeneous groups demonstrates the slow economic recovery and\ncharacterizes the deviating and fear-of-missing-out (FOMO) behaviors of\nhouseholds compared to their counterparts. Our model also gives a fairly\nexplicit representation of the equilibrium solution without exploiting\ncomplicated numerical approaches.",
        "We present Poisson Binomial Mechanism Vertical Federated Learning (PBM-VFL),\na communication-efficient Vertical Federated Learning algorithm with\nDifferential Privacy guarantees. PBM-VFL combines Secure Multi-Party\nComputation with the recently introduced Poisson Binomial Mechanism to protect\nparties' private datasets during model training. We define the novel concept of\nfeature privacy and analyze end-to-end feature and sample privacy of our\nalgorithm. We compare sample privacy loss in VFL with privacy loss in HFL. We\nalso provide the first theoretical characterization of the relationship between\nprivacy budget, convergence error, and communication cost in\ndifferentially-private VFL. Finally, we empirically show that our model\nperforms well with high levels of privacy.",
        "The in-context learning capabilities of large language models (LLMs) show\ngreat potential in mental health support. However, the lack of counseling\ndatasets, particularly in Chinese corpora, restricts their application in this\nfield. To address this, we constructed Psy-Insight, the first mental\nhealth-oriented explainable multi-task bilingual dataset. We collected\nface-to-face multi-turn counseling dialogues, which are annotated with\nmulti-task labels and conversation process explanations. Our annotations\ninclude psychotherapy, emotion, strategy, and topic labels, as well as\nturn-level reasoning and session-level guidance. Psy-Insight is not only\nsuitable for tasks such as label recognition but also meets the need for\ntraining LLMs to act as empathetic counselors through logical reasoning.\nExperiments show that training LLMs on Psy-Insight enables the models to not\nonly mimic the conversation style but also understand the underlying strategies\nand reasoning of counseling.",
        "Bayesian inference for statistical models with a hierarchical structure is\noften characterized by specification of priors for parameters at different\nlevels of the hierarchy. When higher level parameters are functions of the\nlower level parameters, specifying a prior on the lower level parameters leads\nto induced priors on the higher level parameters. However, what are deemed\nuninformative priors for lower level parameters can induce strikingly non-vague\npriors for higher level parameters. Depending on the sample size and specific\nmodel parameterization, these priors can then have unintended effects on the\nposterior distribution of the higher level parameters.\n  Here we focus on Bayesian inference for the Bernoulli distribution parameter\n$\\theta$ which is modeled as a function of covariates via a logistic\nregression, where the coefficients are the lower level parameters for which\npriors are specified. A specific area of interest and application is the\nmodeling of survival probabilities in capture-recapture data and occupancy and\ndetection probabilities in presence-absence data. In particular we propose\nalternative priors for the coefficients that yield specific induced priors for\n$\\theta$. We address three induced prior cases. The simplest is when the\ninduced prior for $\\theta$ is Uniform(0,1). The second case is when the induced\nprior for $\\theta$ is an arbitrary Beta($\\alpha$, $\\beta$) distribution. The\nthird case is one where the intercept in the logistic model is to be treated\ndistinct from the partial slope coefficients; e.g., $E[\\theta]$ equals a\nspecified value on (0,1) when all covariates equal 0. Simulation studies were\ncarried out to evaluate performance of these priors and the methods were\napplied to a real presence\/absence data set and occupancy modelling.",
        "We investigate the space-time geometry generated by compact objects in\n(2+1)-dimensional Bopp-Podolsky electrodynamics. Inspired by previous studies\nwhere the Bopp-Podolsky field acts as a source for spherically symmetric\nsolutions, we revisit this question within the lower-dimensional (2+1)\nframework. Using a perturbative approach, we derive a charged BTZ-like black\nhole solution and compute corrections up to second order in a perturbative\nexpansion valid far from the horizon. Our analysis suggests that the\nnear-horizon and inner structure of the solution remain unaltered, indicating\nthat no new non-black hole objects emerge in this regime. In particular, we do\nnot find evidence of wormhole solutions in the (2+1)-dimensional version of\nthis theory.",
        "Estimating the volume of a convex body is a canonical problem in theoretical\ncomputer science. Its study has led to major advances in randomized algorithms,\nMarkov chain theory, and computational geometry. In particular, determining the\nquery complexity of volume estimation to a membership oracle has been a\nlongstanding open question. Most of the previous work focuses on the\nhigh-dimensional limit. In this work, we tightly characterize the\ndeterministic, randomized and quantum query complexity of this problem in the\nhigh-precision limit, i.e., when the dimension is constant.",
        "The IC3 algorithm, also known as PDR, is a SAT-based model checking algorithm\nthat has significantly influenced the field in recent years due to its\nefficiency, scalability, and completeness. It utilizes SAT solvers to solve a\nseries of SAT queries associated with relative induction. In this paper, we\nintroduce several optimizations for the SAT solver in IC3 based on our\nobservations of the unique characteristics of these SAT queries. By observing\nthat SAT queries do not necessarily require decisions on all variables, we\ncompute a subset of variables that need to be decided before each solving\nprocess while ensuring that the result remains unaffected. Additionally, noting\nthat the overhead of binary heap operations in VSIDS is non-negligible, we\nreplace the binary heap with buckets to achieve constant-time operations.\nFurthermore, we support temporary clauses without the need to allocate a new\nactivation variable for each solving process, thereby eliminating the need to\nreset solvers. We developed a novel lightweight CDCL SAT solver, GipSAT, which\nintegrates these optimizations. A comprehensive evaluation highlights the\nperformance improvements achieved by GipSAT. Specifically, the GipSAT-based IC3\ndemonstrates an average speedup of 3.61 times in solving time compared to the\nIC3 implementation based on MiniSat.",
        "We show the finiteness of log pluricanonical representations under the\nassumption of the existence of a good minimal model.",
        "Reducing computational costs is an important issue for development of\nembedded systems. Binary-weight Neural Networks (BNNs), in which weights are\nbinarized and activations are quantized, are employed to reduce computational\ncosts of various kinds of applications. In this paper, a design methodology of\nhardware architecture for inference engines is proposed to handle modern BNNs\nwith two operation modes. Multiply-Accumulate (MAC) operations can be\nsimplified by replacing multiply operations with bitwise operations. The\nproposed method can effectively reduce the gate count of inference engines by\nremoving a part of computational costs from the hardware system. The\narchitecture of MAC operations can calculate the inference results of BNNs\nefficiently with only 52% of hardware costs compared with the related works. To\nshow that the inference engine can handle practical applications, two\nlightweight networks which combine the backbones of SegNeXt and the decoder of\nSparseInst for instance segmentation are also proposed. The output results of\nthe lightweight networks are computed using only bitwise operations and add\noperations. The proposed inference engine has lower hardware costs than related\nworks. The experimental results show that the proposed inference engine can\nhandle the proposed instance-segmentation networks and achieves higher accuracy\nthan YOLACT on the \"Person\" category although the model size is 77.7$\\times$\nsmaller compared with YOLACT.",
        "Recently we studied $N$ run-and-tumble particles in one dimension - which\nswitch with rate $\\gamma$ between driving velocities $\\pm v_0$ - interacting\nvia the long range 1D Coulomb potential (also called rank interaction), both in\nthe attractive and in the repulsive case, with and without a confining\npotential. We extend this study in two directions. First we consider the same\nsystem, but inside a harmonic confining potential, which we call \"active\njellium\". We obtain a parametric representation of the particle density in the\nstationary state at large $N$, which we analyze in detail. Contrary to the\nlinear potential, there is always a steady-state where the density has a\nbounded support. However, we find that the model still exhibits transitions\nbetween phases with different behaviors of the density at the edges, ranging\nfrom a continuous decay to a jump, or even a shock (i.e. a cluster of\nparticles, which manifests as a delta peak in the density). Notably, the\ninteractions forbid a divergent density at the edges, which may occur in the\nnon-interacting case. In the second part, we consider a non-reciprocal version\nof the rank interaction: the $+$ particles (of velocity $+v_0$) are attracted\ntowards the $-$ particles (of velocity $-v_0$) with a constant force $b\/N$,\nwhile the $-$ particles are repelled by the $+$ particles with a force of same\namplitude. In order for a stationary state to exist we add a linear confining\npotential. We derive an explicit expression for the stationary density at large\n$N$, which exhibits an explicit breaking of the mirror symmetry with respect to\n$x=0$. This again shows the existence of several phases, which differ by the\npresence or absence of a shock at $x=0$, with one phase even exhibiting a\nvanishing density on the whole region $x>0$. Our analytical results are\ncomplemented by numerical simulations for finite $N$.",
        "With the rapid advancement of vision generation models, the potential\nsecurity risks stemming from synthetic visual content have garnered increasing\nattention, posing significant challenges for AI-generated image detection.\nExisting methods suffer from inadequate generalization capabilities, resulting\nin unsatisfactory performance on emerging generative models. To address this\nissue, this paper presents a novel framework that leverages noise-based\nmodel-specific imprint for the detection task. Specifically, we propose a novel\nnoise-based imprint simulator to capture intrinsic patterns imprinted in images\ngenerated by different models. By aggregating imprints from various generative\nmodels, imprints of future models can be extrapolated to expand training data,\nthereby enhancing generalization and robustness. Furthermore, we design a new\npipeline that pioneers the use of noise patterns, derived from a noise-based\nimprint extractor, alongside other visual features for AI-generated image\ndetection, resulting in a significant improvement in performance. Our approach\nachieves state-of-the-art performance across three public benchmarks including\nGenImage, Synthbuster and Chameleon.",
        "We prove the rationality and irreducibility of the moduli space of\nmathematical instanton vector bundles of arbitrary rank and charge on $\\mathbb\nP^3$. In particular, the result applies to the rank-2 case. This problem was\nfirst studied by Barth, Hartshorne, Hirschowitz-Narasimhan in the late 1970s.\nWe also show that the mathematical instantons of variable rank and charge form\na monoidal category. The proof is based on an in-depth analysis of the\nBarth-Hulek monad-construction and on a detailed description of the moduli\nspace of (framed and unframed) stable bundles on Hirzebruch surfaces.",
        "A unibike curve is a track that can be made by either a bicycle or a\nunicycle. More precisely, the end of a unit tangent vector at any point on a\nunibike curve lies on the curve (so the bike's front wheel always lies on the\ntrack made by the rear wheel). David Finn found such a curve in 2002, but it\nloops around itself in an extremely complicated way with many twists and\nself-intersections. Starting with the polar square root curve r = sqrt[t\/(2\npi)] and iterating a simple construction involving a differential equation\napparently leads in the limit to a unibike curve having a spiral shape. The\niteration gets each curve as a rear track of its predecessor. Solving hundreds\nof differential equations numerically, where each depends on the preceding one,\nleads to error buildup, but with some care one can get a curve having unibike\nerror less than 10^-7. The evidence is strong for the conjecture that the limit\nof the iteration exists and is a unibike curve.",
        "Text-to-image diffusion models have achieved remarkable success in generating\nhigh-quality contents from text prompts. However, their reliance on publicly\navailable data and the growing trend of data sharing for fine-tuning make these\nmodels particularly vulnerable to data poisoning attacks. In this work, we\nintroduce the Silent Branding Attack, a novel data poisoning method that\nmanipulates text-to-image diffusion models to generate images containing\nspecific brand logos or symbols without any text triggers. We find that when\ncertain visual patterns are repeatedly in the training data, the model learns\nto reproduce them naturally in its outputs, even without prompt mentions.\nLeveraging this, we develop an automated data poisoning algorithm that\nunobtrusively injects logos into original images, ensuring they blend naturally\nand remain undetected. Models trained on this poisoned dataset generate images\ncontaining logos without degrading image quality or text alignment. We\nexperimentally validate our silent branding attack across two realistic\nsettings on large-scale high-quality image datasets and style personalization\ndatasets, achieving high success rates even without a specific text trigger.\nHuman evaluation and quantitative metrics including logo detection show that\nour method can stealthily embed logos.",
        "Social scientists analyze citation networks to study how documents influence\nsubsequent work across various domains such as judicial politics and\ninternational relations. However, conventional approaches that summarize\ndocument attributes in citation networks often overlook the diverse semantic\ncontexts in which citations occur. This paper develops the paragraph-citation\ntopic model (PCTM), which analyzes citation networks and document texts\njointly. The PCTM extends conventional topic models by assigning topics to\nparagraphs of citing documents, allowing citations to share topics with their\nembedding paragraphs. Our empirical analysis of U.S. Supreme Court opinions in\nthe privacy issue domain, which includes cases on reproductive rights,\ndemonstrates that citations within individual documents frequently span\nmultiple substantive areas, and citations to individual documents show\nconsiderable topical diversity.",
        "Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, but ensuring their safety and alignment with human values\nremains crucial. Current safety alignment methods, such as supervised\nfine-tuning and reinforcement learning-based approaches, can exhibit\nvulnerabilities to adversarial attacks and often result in shallow safety\nalignment, primarily focusing on preventing harmful content in the initial\ntokens of the generated output. While methods like resetting can help recover\nfrom unsafe generations by discarding previous tokens and restarting the\ngeneration process, they are not well-suited for addressing nuanced safety\nviolations like toxicity that may arise within otherwise benign and lengthy\ngenerations. In this paper, we propose a novel backtracking method designed to\naddress these limitations. Our method allows the model to revert to a safer\ngeneration state, not necessarily at the beginning, when safety violations\noccur during generation. This approach enables targeted correction of\nproblematic segments without discarding the entire generated text, thereby\npreserving efficiency. We demonstrate that our method dramatically reduces\ntoxicity appearing through the generation process with minimal impact to\nefficiency.",
        "Explanations constitute an important aspect of successful human robot\ninteractions and can enhance robot understanding. To improve the understanding\nof the robot, we have developed four levels of explanation (LOE) based on two\nquestions: what needs to be explained, and why the robot has made a particular\ndecision. The understandable robot requires a communicative action when there\nis disparity between the human s mental model of the robot and the robots state\nof mind. This communicative action was generated by utilizing a conversational\nAI platform to generate explanations. An adaptive dialog was implemented for\ntransition from one LOE to another. Here, we demonstrate the adaptive dialog in\na collaborative task with errors and provide results of a feasibility study\nwith users.",
        "For each open subgroup $H\\leq \\operatorname{GL}_2(\\widehat{\\mathbb{Z}})$,\nthere is a modular curve $X_H$, defined as a quotient of the full modular curve\n$X(N)$, where $N$ is the level of $H$. The genus formula of a modular curve is\nwell known for $X_0(N)$, $X_1(N)$, $X(N)$, $X_{\\mathrm{sp}}(N)$,\n$X_{\\mathrm{ns}}(N)$, and $X_{S_4}(p)$ for $p$ prime. We explicitly work out\nthe invariants of the genus formulas for $X_{\\mathrm{sp}}^+(N)$,\n$X_{\\mathrm{ns}}^+(N)$, and $X_{\\text{arith},1}(M,MN)$. In Table $1$, we\nprovide the invariants of the genus formulas for all of the modular curves\nlisted.",
        "Due to the complex interplay of light absorption and scattering in the\nunderwater environment, underwater images experience significant degradation.\nThis research presents a two-stage underwater image enhancement network called\nthe Data-Driven and Physical Parameters Fusion Network (DPF-Net), which\nharnesses the robustness of physical imaging models alongside the generality\nand efficiency of data-driven methods. We first train a physical parameter\nestimate module using synthetic datasets to guarantee the trustworthiness of\nthe physical parameters, rather than solely learning the fitting relationship\nbetween raw and reference images by the application of the imaging equation, as\nis common in prior studies. This module is subsequently trained in conjunction\nwith an enhancement network, where the estimated physical parameters are\nintegrated into a data-driven model within the embedding space. To maintain the\nuniformity of the restoration process amid underwater imaging degradation, we\npropose a physics-based degradation consistency loss. Additionally, we suggest\nan innovative weak reference loss term utilizing the entire dataset, which\nalleviates our model's reliance on the quality of individual reference images.\nOur proposed DPF-Net demonstrates superior performance compared to other\nbenchmark methods across multiple test sets, achieving state-of-the-art\nresults. The source code and pre-trained models are available on the project\nhome page: https:\/\/github.com\/OUCVisionGroup\/DPF-Net.",
        "OpenStreetMap (OSM) has gained popularity recently in autonomous navigation\ndue to its public accessibility, lower maintenance costs, and broader\ngeographical coverage. However, existing methods often struggle with noisy OSM\ndata and incomplete sensor observations, leading to inaccuracies in trajectory\nplanning. These challenges are particularly evident in complex driving\nscenarios, such as at intersections or facing occlusions. To address these\nchallenges, we propose a robust and explainable two-stage framework to learn an\nOrientation Field (OrField) for robot navigation by integrating LiDAR scans and\nOSM routes. In the first stage, we introduce the novel representation, OrField,\nwhich can provide orientations for each grid on the map, reasoning jointly from\nnoisy LiDAR scans and OSM routes. To generate a robust OrField, we train a deep\nneural network by encoding a versatile initial OrField and output an optimized\nOrField. Based on OrField, we propose two trajectory planners for OSM-guided\nrobot navigation, called Field-RRT* and Field-Bezier, respectively, in the\nsecond stage by improving the Rapidly Exploring Random Tree (RRT) algorithm and\nBezier curve to estimate the trajectories. Thanks to the robustness of OrField\nwhich captures both global and local information, Field-RRT* and Field-Bezier\ncan generate accurate and reliable trajectories even in challenging conditions.\nWe validate our approach through experiments on the SemanticKITTI dataset and\nour own campus dataset. The results demonstrate the effectiveness of our\nmethod, achieving superior performance in complex and noisy conditions. Our\ncode for network training and real-world deployment is available at\nhttps:\/\/github.com\/IMRL\/OriField.",
        "In multi-label learning, leveraging contrastive learning to learn better\nrepresentations faces a key challenge: selecting positive and negative samples\nand effectively utilizing label information. Previous studies selected positive\nand negative samples based on the overlap between labels and used them for\nlabel-wise loss balancing. However, these methods suffer from a complex\nselection process and fail to account for the varying importance of different\nlabels. To address these problems, we propose a novel method that improves\nmulti-label contrastive learning through label distribution. Specifically, when\nselecting positive and negative samples, we only need to consider whether there\nis an intersection between labels. To model the relationships between labels,\nwe introduce two methods to recover label distributions from logical labels,\nbased on Radial Basis Function (RBF) and contrastive loss, respectively. We\nevaluate our method on nine widely used multi-label datasets, including image\nand vector datasets. The results demonstrate that our method outperforms\nstate-of-the-art methods in six evaluation metrics.",
        "A linear order $A$ is called strongly surjective if for every non empty\nsuborder $B \\preceq A$, there is an epimorphism from $A$ onto $B$ (denoted by\n$B \\trianglelefteq A$). We show, answering some questions of D\\'aniel T.\nSoukup, that under $\\mathsf{MA}_{\\aleph_{1}}$ there is a strongly surjective\nCountryman line. We also study the general structure of the class of Aronszajn\nlines under $\\trianglelefteq$, and compare it with the well known embeddability\nrelation $\\preceq$. Under $\\mathsf{PFA}$, the class of Aronszajn lines and the\nclass of countable linear orders enjoy similar nice properties when viewed\nunder the embeddability relation; both are well-quasi-ordered and have a finite\nbasis. We show that this analogy does not extend perfectly to the\n$\\trianglelefteq$ relation; while it is known that the countable linear orders\nare still well-quasi-ordered under $\\trianglelefteq$, we show that already in\n$\\mathsf{ZFC}$ the class of Aronszajn lines has an infinite antichain, and\nunder $\\mathsf{MA}_{\\aleph_{1}}$ an infinite decreasing chain as well. We show\nthat some of the analogy survives by proving that under $\\mathsf{PFA}$, for\nsome carefully constructed Countryman line $C$, $C$ and $C^{\\star}$ form a\n$\\trianglelefteq$-basis for the class of Aronszajn lines. Finally we show that\nthis does not extend to all uncountable linear orders by proving that there is\nnever a finite $\\trianglelefteq$-basis for the uncountable real orders.",
        "This paper presents a high-precision positioning system that integrates\nultra-wideband (UWB) time difference of arrival (TDoA) measurements, inertial\nmeasurement unit (IMU) data, and ultrasonic sensors through factor graph\noptimization. To overcome the shortcomings of standalone UWB systems in\nnon-line-of-sight (NLOS) scenarios and the inherent drift associated with\ninertial navigation, we developed a novel hybrid fusion framework. First, a\ndynamic covariance estimation mechanism is incorporated, which automatically\nadjusts measurement weights based on real-time channel conditions. Then, a\ntightly-coupled sensor fusion architecture is employed, utilizing IMU\npre-integration theory for temporal synchronization. Finally, a sliding-window\nfactor graph optimization backend is utilized, incorporating NLOS mitigation\nconstraints. Experimental results in complex indoor environments show a 38\\%\nimprovement in positioning accuracy compared to conventional Kalman\nfilter-based approaches, achieving a 12.3 cm root mean square (RMS) error under\ndynamic motion conditions. The system maintains robust performance even with\nintermittent UWB signal availability, down to a 40\\% packet reception rate,\neffectively suppressing IMU drift through multi-modal constraint fusion. This\nwork offers a practical solution for applications that require reliable indoor\npositioning in GPS-denied environments.",
        "Watermarking plays a key role in the provenance and detection of AI-generated\ncontent. While existing methods prioritize robustness against real-world\ndistortions (e.g., JPEG compression and noise addition), we reveal a\nfundamental tradeoff: such robust watermarks inherently improve the redundancy\nof detectable patterns encoded into images, creating exploitable information\nleakage. To leverage this, we propose an attack framework that extracts leakage\nof watermark patterns through multi-channel feature learning using a\npre-trained vision model. Unlike prior works requiring massive data or detector\naccess, our method achieves both forgery and detection evasion with a single\nwatermarked image. Extensive experiments demonstrate that our method achieves a\n60\\% success rate gain in detection evasion and 51\\% improvement in forgery\naccuracy compared to state-of-the-art methods while maintaining visual\nfidelity. Our work exposes the robustness-stealthiness paradox: current\n\"robust\" watermarks sacrifice security for distortion resistance, providing\ninsights for future watermark design.",
        "Increased electrification of energy end-usage can lead to network congestion\nduring periods of high consumption. Flexibility of loads, such as aggregate\nsmart charging of Electric Vehicles (EVs), is increasingly leveraged to manage\ngrid congestion through various market-based mechanisms. Under such an\narrangement, this paper quantifies the effect of lead time on the aggregate\nflexibility of EV fleets. Simulations using real-world charging transactions\nspanning over different categories of charging stations are performed for two\nflexibility products (redispatch and capacity limitations) when offered along\nwith different business-as-usual (BAU) schedules. Results show that the\nvariation of tradable flexibility depends mainly on the BAU schedules, the\nduration of the requested flexibility, and its start time. Further, the\nimplication of these flexibility products on the average energy costs and\nemissions is also studied for different cases. Simulations show that\nbidirectional (V2G) charging outperforms unidirectional smart charging in all\ncases.",
        "The study of security in machine learning mainly focuses on downstream\ntask-specific attacks, where the adversarial example is obtained by optimizing\na loss function specific to the downstream task. At the same time, it has\nbecome standard practice for machine learning practitioners to adopt publicly\navailable pre-trained vision foundation models, effectively sharing a common\nbackbone architecture across a multitude of applications such as\nclassification, segmentation, depth estimation, retrieval, question-answering\nand more. The study of attacks on such foundation models and their impact to\nmultiple downstream tasks remains vastly unexplored. This work proposes a\ngeneral framework that forges task-agnostic adversarial examples by maximally\ndisrupting the feature representation obtained with foundation models. We\nextensively evaluate the security of the feature representations obtained by\npopular vision foundation models by measuring the impact of this attack on\nmultiple downstream tasks and its transferability between models.",
        "The mismatched distortion-rate problem has remained open since its\nformulation by Lapidoth in 1997. In this paper, we characterize the mismatched\ndistortion-rate function. Our single-letter solution highlights the adequate\nconditional distributions for the encoder and the decoder. The achievability\nresult relies on a time-sharing argument that allows to convexify the upper\nbound of Lapidoth. We show that it is sufficient to consider two regimes, one\nwith a large rate and another one with a small rate. Our main contribution is\nthe converse proof. Suppose that the encoder selects a single-letter\nconditional distribution distinct from the one in the solution, we construct an\nencoding strategy that leads to the same expected cost for both encoder and\ndecoder. This ensures that the encoder cannot gain by changing the\nsingle-letter conditional distribution. This argument relies on a careful\nidentification of the sequence of auxiliary random variables. By building on\nCaratheodory's Theorem we show that the cardinality of the auxiliary random\nvariables is equal to the one of the source alphabet plus three."
      ]
    }
  },
  {
    "id":2411.14474,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"Convolutional recurrent neural networks for music classification",
    "start_abstract":"We introduce a convolutional recurrent neural network (CRNN) for music tagging. CRNNs take advantage of networks (CNNs) local feature extraction and temporal summarisation the extracted features. compare CRNN with three CNN structures that have been used tagging while controlling number parameters respect to their performance training time per sample. Overall, we found show strong parameter time, indicating effectiveness its hybrid structure in summarisation.",
    "start_categories":[
      "cs.NE"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b15"
      ],
      "title":[
        "A deep representation for invariance and music classification"
      ],
      "abstract":[
        "Representations in the auditory cortex might be based on mechanisms similar to visual ventral stream; modules for building invariance transformations and multiple layers compositionality selectivity. In this paper we propose use of such computational extracting invariant discriminative audio representations. Building a theory hierarchical architectures, novel, mid-level representation acoustical signals, using empirical distributions projections set templates their transformations. Under assumption that, by construction, dictionary is composed from classes, samples orbit variance-inducing signal (such as shift scale), resulting signature theoretically guaranteed unique, stable deformations. Modules projection pooling can then constitute deep networks, learning composite We present main theoretical aspects framework unsupervised representations, empirically evaluated music genre classification."
      ],
      "categories":[
        "physics.class-ph"
      ]
    },
    "list":{
      "title":[
        "NeuroPMD: Neural Fields for Density Estimation on Product Manifolds",
        "Superstructure reflexions in tilted perovskites Part 1",
        "Qubit operations using a modular optical system engineered with\n  PyOpticL: a code-to-CAD optical layout tool",
        "Low-energy neutron cross-talk between organic scintillator detectors",
        "Serrin's overdetermined problems on epigraphs",
        "Symbolic Computations of the Two-Colored Diagrams for Central\n  Configurations of the Planar N-vortex Problem",
        "Local perfect chirality at reflection-zeros away from exceptional points\n  in optical whispering gallery microcavity",
        "The Lehmer complex of a Bruhat interval",
        "Theoretical determination of Gilbert damping in reduced dimensions",
        "Pricing American Parisian Options under General Time-Inhomogeneous\n  Markov Models",
        "Algorithms for 2-Solvable Difference Equations",
        "On a formula of the $q$-series $_{2k+4}\\phi_{2k+3}$ and its applications",
        "Estimating invasive rodent abundance using removal data and hierarchical\n  models",
        "The nexus between disease surveillance, adaptive human behavior and\n  epidemic containment",
        "Orbital Angular Momentum Experimental Bound on the Maximum Predictive\n  Power of Physical Theories in Multi-Dimensional Systems",
        "A multi-component phase-field model for T1 precipitates in Al-Cu-Li\n  alloys",
        "A numerical analysis of Araki-Uhlmann relative entropy in Quantum Field\n  Theory",
        "The Ladder and Readout Cables of Intermediate Silicon Strip Detector for\n  sPHENIX",
        "Consistent Solutions of the Radiation Diffusion Equation in Spherical\n  and Cylindrical Geometries",
        "Demystifying the fusion mechanism in heavy-ion collisions: A\n  six-dimensional Langevin dissipative dynamics approach",
        "Exclusive photoproduction of $\\chi_{c}\\gamma$ pairs",
        "Optimal Response for Hyperbolic Systems by the fast adjoint response\n  method",
        "A trailing lognormal approximation of the Lyman-$\\alpha$ forest:\n  comparison with full hydrodynamic simulations at $2.2\\leq z\\leq 2.7$",
        "Choice Sets and Smart Card Data In Public Transport Route Choice Models:\n  Generated vs. Empirical Sets",
        "Nonlinearity and Quantumness in Thermodynamics: From Principles to\n  Technologies",
        "Computing ternary liquid phase diagrams: Fe-Cu-Ni",
        "Electroweak Scalar Effects Beyond Dimension-6 in SMEFT",
        "A Constant Rate Quantum Computer on a Line",
        "Chemical distance in the Poisson Boolean model with regularly varying\n  diameters"
      ],
      "abstract":[
        "We propose a novel deep neural network methodology for density estimation on\nproduct Riemannian manifold domains. In our approach, the network directly\nparameterizes the unknown density function and is trained using a penalized\nmaximum likelihood framework, with a penalty term formed using manifold\ndifferential operators. The network architecture and estimation algorithm are\ncarefully designed to handle the challenges of high-dimensional product\nmanifold domains, effectively mitigating the curse of dimensionality that\nlimits traditional kernel and basis expansion estimators, as well as overcoming\nthe convergence issues encountered by non-specialized neural network methods.\nExtensive simulations and a real-world application to brain structural\nconnectivity data highlight the clear advantages of our method over the\ncompeting alternatives.",
        "The superstructure spots that appear in diffraction patterns of tilted\nperovskites are well documented and easily calculated using crystallographic\nsoftware. Here, by considering a distortion mode as a perturbation of the\nprototype perovskite structure, we show how the structure factor equation\nyields Boolean conditions for the presence of first order superstructure\nreflexions. A subsequent article describes conditions for second order\nreflexions, which appear only in structures with mixed in-phase and anti-phase\noxygen octahedral tilting. This approach may have some advantages for the\nanalysis of electron diffraction patterns of perovskites.",
        "Complex optical design is hindered by conventional piecewise setup, which\nprevents modularization and therefore abstraction of subsystems at the circuit\nlevel. This limits multiple fields that require complex optics systems,\nincluding quantum computing with atoms and trapped ions, because their optical\nsystems are not scalable. We present an open-source Python library for optical\nlayout (PyOpticL) which uses beam-path simulation and dynamic beam-path routing\nfor quick and easy optical layout by placing optical elements along the beam\npath without a priori specification, enabling dynamic layouts with automatic\nrouting and connectivity. We use PyOpticL to create modular drop-in optical\nbaseplates for common optical subsystems used in atomic and molecular optics\n(AMO) experiments including laser sources, frequency and intensity modulation,\nand locking to an atomic reference for stabilization. We demonstrate this\nminimal working example of a dynamic full laser system for strontium trapped\nions by using it for laser cooling, qubit state detection, and 99.9% fidelity\nsingle-qubit gates with 3D printed baseplates. This enables a new paradigm of\ndesign abstraction layers for engineering optical systems leveraging modular\nbaseplates, as they can be used for any wavelength in the system and enables\nscaling up the underlying optical systems for quantum computers. This new\nopen-source hardware and software code-to-CAD library seeks to foster\nopen-source collaborative hardware and systems design across numerous fields of\nresearch including AMO physics and quantum computing with neutral atoms and\ntrapped ions.",
        "A series of measurements have been performed with low-energy monoenergetic\nneutrons to characterise cross-talk between two organic scintillator detectors.\nCross-talk time-of-flight spectra and probabilities were determined for neutron\nenergies from 1.4 to 15.5 MeV and effective scattering angles ranging from\n$\\sim$50{\\deg} to $\\sim$100{\\deg}. Monte-Carlo simulations incorporating both\nthe active and inactive materials making up the detectors showed reasonable\nagreement with the measurements. Whilst the time-of-flight spectra were very\nwell reproduced, the cross-talk probabilities were only in approximate\nagreement with the measurements, with the most significant discrepancies\n($\\sim$40 %) occurring at the lowest energies. The neutron interaction\nprocesses producing cross-talk at the energies explored here are discussed in\nthe light of these results.",
        "In this work we establish some rigidity results for Serrin's overdetermined\nproblem \\begin{equation*}\n  \\left\\{\n  \\begin{array}{cll}\n  - \\Delta u=f(u) & \\text{in}& \\Omega,\\newline\n  u > 0& \\text{in} & \\Omega,\\newline\n  u=0 & \\text{on} & \\partial \\Omega,\\newline\n  \\dfrac{\\partial u}{\\partial \\eta} = \\mathfrak{c} = const. & \\text{on} &\n\\partial \\Omega,\n  \\end{array}\n  \\right. \\end{equation*}\n  when $\\Omega \\subset \\mathbb{R}^N$ is an epigraph (not necessarily globally\nLipschitz-continuous) and $u$ is a classical solution, possibly unbounded. In\nbroad terms, our main results prove that $\\Omega$ must be an affine half-space\nand $u$ must be one-dimensional, provided the epigraph is bounded from below.\nThese results hold when $f$ is of Allen-Cahn type and $ N \\geq 2$ or,\nalternatively, when $f$ is locally Lipschitz-continuous (with no restriction on\nthe sign of $f(0)$) and $ N \\leq 3$. These results partially answer a question\nraised by Berestycki, Caffarelli and Nirenberg in [1]. Finally, when $f(0) <0$,\nwe also prove a new monotonicity result, valid in any dimension $ N \\geq 2$.",
        "We apply the singular sequence method to investigate the finiteness problem\nfor stationary configurations of the planar N-vortex problem. The initial step\nof the singular sequence method involves identifying all two-colored diagrams.\nThese diagrams represent potential scenarios where finiteness may fail. We\ndevelop a symbolic computation algorithm to determine all two-colored diagrams\nfor central configurations of the planar N-vortex problem.",
        "Recently, a local and imperfect chirality of the resonant eigenmode at the\nexceptional point (EP) has been reported in the optical whispering gallery\nmicrocavity system perturbed by two strong nanoscatterers [Phys. Rev. A 108,\nL041501 (2023)]. Here, we discover a local perfect chirality of the resonant\neigenmode away from the EP in the parameter space of the strongly perturbed\nmicrocavity system. By considering the multiple scattering process of the\nazimuthally propagating modes (APMs) at the nanoscatterers with a\nfirst-principles-based model, the local perfect chirality is predicted to\nresult from the unidirectional reflectionlessness, i.e., the reflection-zero\n(R-zero) of the APMs at the two nanoscatterers. Numerical results and model\npredictions consistently show that the structural parameters of the R-zero\ntypically deviate from those of the EP, which means that the pair of split\nresonant eigenmodes at the R-zero have different complex resonance frequencies\nand electromagnetic fields. In general, only one of the pair of split\neigenmodes exhibits a local perfect chirality within the local azimuthal range\ndivided by the two nanoscatterers. With the decrease of the two nanoscatterers'\nsizes or their relative azimuthal angle, the R-zero tends to coincide with the\nEP.",
        "We introduce Lehmer codes, with immersions in the Bruhat order, for several\nfinite Coxeter groups, including all the classical Weyl groups. This allows to\nassociate to each lower Bruhat interval of these groups a multicomplex whose\nf-polynomial is the Poincar\\'e polynomial of the interval. Via a general\nconstruction, we prove that these polynomials are h-polynomials of\nvertex-decomposable simplicial complexes. Moreover we provide a classification,\nin terms of unimodal permutations, of Poincar\\'e polynomials of smooth Schubert\nvarieties in flag manifolds.",
        "An ab initio scheme based on the linear response theory of exchange torque\ncorrelation is presented to calculate intrinsic Gilbert damping parameters in\nmagnets of reduced dimensions. The method implemented into the real-space\nKorringa-Kohn-Rostoker (RS-KKR) Greens' function framework enables to obtain\ndiagonal elements of the atomic-site-dependent on-site and non-local Gilbert\ndamping tensor. Going from the 3D bulk and surfaces of iron and cobalt\nferromagnets addressed in our previous work [Phys. Rev. B 109, 094417 (2024)],\nin the present paper monolayers of Fe and Co on (001)- and (111)-oriented Cu,\nAg, and Au substrates are studied, and particularly the substrate-dependent\ntrends are compared. Furthermore, the Gilbert damping parameters are calculated\nfor Fe and Co adatoms and dimers on (001)-oriented substrates. It is\ninvestigated how the damping parameter of single adatoms depends on their\nvertical position. This dependence is quantified in relation to the adatoms'\ndensity of states at the Fermi energy showing a non-monotonic behavior. By\nrotating the spin moment of the adatoms and collinear magnetic dimers, an\nanisotropic behavior of the damping is revealed. Finally, a significant, three-\nto ten-times increase of the on-site Gilbert damping is found in\nantiferromagnetic dimers in comparison to the ferromagnetic ones, whilst the\ninter-site damping is even more enhanced.",
        "This paper develops general approaches for pricing various types of\nAmerican-style Parisian options (down-in\/-out, perpetual\/finite-maturity) with\ngeneral payoff functions based on continuous-time Markov chain (CTMC)\napproximation under general 1D time-inhomogeneous Markov models. For the\ndown-in types, by conditioning on the Parisian stopping time, we reduce the\npricing problem to that of a series of vanilla American options with different\nmaturities and their prices integrated with the distribution function of the\nParisian stopping time yield the American Parisian down-in option price. This\nfacilitates an efficient application of CTMC approximation to obtain the\napproximate option price by calculating the required quantities. For the\nperpetual down-in cases under time-homogeneous models, significant\ncomputational cost can be reduced. The down-out cases are more complicated, for\nwhich we use the state augmentation approach to record the excursion duration\nand then the approximate option price is obtained by solving a series of\nvariational inequalities recursively with the Lemke's pivoting method. We show\nthe convergence of CTMC approximation for all the types of American Parisian\noptions under general time-inhomogeneous Markov models, and the accuracy and\nefficiency of our algorithms are confirmed with extensive numerical\nexperiments.",
        "Our paper \"Solving Third Order Linear Difference Equations in Terms of Second\nOrder Equations\" gave two algorithms for solving difference equations in terms\nof lower order equations: an algorithm for absolute factorization, and an\nalgorithm for solving third order equations in terms of second order. Here we\nimprove the efficiency for absolute factorization, and extend the other\nalgorithm to order four.",
        "In this paper we apply a formula of the very-well poised $_{2k+4}\\phi_{2k+3}$\nto write a $k$-tuple sum of $q$-series as a linear combination of terms wherein\neach term is a product of expressions of the form $\\frac{1}{(qy,\nqy^{-1};q)_\\infty}$. As an application, we shall express a variety of sums and\ndouble sums of $q$-series as linear combinations of infinite products. Our\nformulas are motivated by their connection to overpartition pairs.",
        "Invasive rodents pose significant ecological, economic, and public health\nchallenges. Robust methods are needed for estimating population abundance to\nguide effective management. Traditional methods such as capture-recapture are\noften impractical for invasive species due to ethical and logistical\nconstraints. Here, I showcase the application of hierarchical multinomial\nN-mixture models for estimating the abundance of invasive rodents using removal\ndata. First, I perform a simulation study which demonstrates minimal bias in\nabundance estimates across a range of sampling scenarios. Second, I analyze\nremoval data for two invasive rodent species, namely coypus (Myocastor coypus)\nin France and muskrats (Ondatra zibethicus) in the Netherlands. Using\nhierarchical multinomial N-mixture models, I examine the effects of temperature\non abundance while accounting for imperfect and time-varying capture\nprobabilities. I also show how to accommodate spatial variability using random\neffects, and quantify uncertainty in parameter estimates. Overall, I hope to\ndemonstrate the flexibility and utility of hierarchical models in invasive\nspecies management.",
        "Epidemics exhibit interconnected processes that operate at multiple time and\norganizational scales, a hallmark of complex adaptive systems. Modern\nepidemiological modeling frameworks incorporate feedback between\nindividual-level behavioral choices and centralized interventions. Nonetheless,\nthe realistic operational course for disease detection, planning, and response\nis often overlooked. Disease detection is a dynamic challenge, shaped by the\ninterplay between surveillance efforts and transmission characteristics. It\nserves as a tipping point that triggers emergency declarations, information\ndissemination, adaptive behavioral responses, and the deployment of public\nhealth interventions. Evaluating the impact of disease surveillance systems as\ntriggers for adaptive behavior and public health interventions is key to\ndesigning effective control policies.\n  We examine the multiple behavioral and epidemiological dynamics generated by\nthe feedback between disease surveillance and the intertwined dynamics of\ninformation and disease propagation. Specifically, we study the intertwined\ndynamics between: $(i)$ disease surveillance triggering health emergency\ndeclarations, $(ii)$ risk information dissemination producing decentralized\nbehavioral responses, and $(iii)$ centralized interventions. Our results show\nthat robust surveillance systems that quickly detect a disease outbreak can\ntrigger an early response from the population, leading to large epidemic sizes.\nThe key result is that the response scenarios that minimize the final epidemic\nsize are determined by the trade-off between the risk information dissemination\nand disease transmission, with the triggering effect of surveillance mediating\nthis trade-off. Finally, our results confirm that behavioral adaptation can\ncreate a hysteresis-like effect on the final epidemic size.",
        "The completeness of quantum mechanics in predictive power is a central\nquestion in its foundational study. While most investigations focus on\ntwo-dimensional systems, high-dimensional systems are more general and widely\napplicable. Building on the non-extensibility theorem by Colbeck and Renner\n[Phys. Rev. Lett. 101, 050403 (2008)], which established that no higher theory\ncan enhance the predictive power of quantum mechanics for two-dimensional\nsystems, we extend this result to arbitrarily dimensional systems. We connect\nmaximum potential predictive power achievable by any alternative theory to\nexperimentally observable correlations, and establish optimal experimental\nbounds across varying dimensions by exploiting two-photon orbital angular\nmomentum entangled states with entanglement concentration. These bounds falsify\na broader class of alternative theories, including Bell's and Leggett's models,\nand those that remain theoretically ambiguous or experimentally unverified. Our\nfindings not only deepen the foundational understanding of quantum mechanics\nbut also hold significant potential for high-dimensional quantum cryptography.",
        "In this study, the role of elastic and interfacial energies in the shape\nevolution of T1 precipitates in Al-Cu-Li alloys is investigated using\nphase-field modeling. We employ a formulation considering the stoichiometric\nnature of the precipitate phase explicitly, including coupled equation systems\nfor various order parameters. Inputs such as elastic properties are derived\nfrom DFT calculations, while chemical potentials are obtained from CALPHAD\ndatabases. This methodology provides a framework that is consistent with the\nderived chemical potentials to study the interplay of thermodynamic, kinetic,\nand elastic effects on T1 precipitate evolution in Al-Cu-Li alloys. It is shown\nthat diffusion-controlled lengthening and interface-controlled thickening are\nimportant mechanisms to describe the growth of T1 precipitates. Furthermore,\nthe study illustrates that the precipitate shape is significantly influenced by\nthe anisotropy in interfacial energy and linear reaction rate, however, elastic\neffects only play a minor role.",
        "We numerically investigate the Araki-Uhlmann relative entropy in Quantum\nField Theory, focusing on a free massive scalar field in 1+1-dimensional\nMinkowski spacetime. Using Tomita-Takesaki modular theory, we analyze the\nrelative entropy between a coherent state and the vacuum state, with several\ntypes of test functions localized in the right Rindler wedge. Our results\nconfirm that relative entropy decreases with increasing mass and grows with the\nsize of the spacetime region, aligning with theoretical expectations.",
        "A new silicon-strip-type detector was developed for precise charged-particle\ntracking in the central rapidity region of heavy ion collisions. A new detector\nand collaboration at the Relativistic Heavy Ion Collider at Brookhaven National\nLaboratory is sPHENIX, which is a major upgrade of the PHENIX detector. The\nintermediate tracker (INTT) is part of the advanced tracking system of the\nsPHENIX detector complex together with a CMOS monolithic-active-pixel-sensor\nbased silicon-pixel vertex detector, a time-projection chamber, and a\nmicromegas-based detector. The INTT detector is barrel shaped and comprises 56\nsilicon ladders. Two different types of strip sensors of 78~$\\mu m$ pitch and\n320~$\\mu m$ thick are mounted on each half of a silicon ladder. Each strip\nsensor is segmented into 8$\\times$2 and 5$\\times$2 blocks with lengths of 16\nand 20 mm. Strips are read out with a silicon strip-readout (FPHX) chip. In\norder to transmit massive data from the FPHX to the down stream readout\nelectronics card (ROC), a series of long and high speed readout cables were\ndeveloped. This document focuses on the silicon ladder, the readout cables, and\nthe ROC of the INTT. The radiation hardness is studied for some parts of the\nINTT devices in the last part of this document, since the INTT employed some\nmaterials from the technology frontier of the industry whose radiation hardness\nis not necessarily well known.",
        "We have extended the radiation diffusion model of Hammer and Rosen to\ndiverging spherical and cylindrical geometries. The effect of curvilinear\ngeometry on the supersonic, expanding wavefront increases as the internal\nradius of a spherical or cylindrical shell approaches zero. Small spherical\ngeometries are important for modeling systems at the size scale of ICF\ncapsules, at these scales existing quasi-analytic models for planar geometry\nsignificantly disagree with the results of simulation. With this method, the\nbenefits of rapid iteration can be applied to common spherical systems at much\nsmaller length scales. We present comparisons between numerical diffusion\nsolutions and the analytic model to give ranges of applicability for the model.",
        "We present an in-depth investigation of heavy-ion fusion dynamics using a\nsix-dimensional Langevin framework that enables unrestricted motion of the\nasymmetry parameter. The stochastic formalism naturally incorporates friction\neffects and energy fluctuations, providing a detailed understanding of the\nfusion process. The dynamics transition into the overdamped regime,\nfacilitating rapid neck stabilization while effectively capturing the interplay\nbetween shape and rotational degrees of freedom. This approach achieves\nexcellent agreement with experimental spin distributions and fusion\ncross-sections, establishing a robust foundation for forthcoming studies on the\nsynthesis of superheavy elements and the exploration of the enigmatic fusion\nhindrance mechanism.",
        "In this manuscript we study the exclusive photoproduction of $\\chi_{c}\\gamma$\npairs in the collinear factorization framework. We found the coefficient\nfunctions for all possible spins and helicity projections of $\\chi_{c}$ mesons\nand final-state photons in the leading order in the strong coupling\n$\\alpha_{s}$. In our analysis we focused on the contribution of the leading\ntwist chiral even GPDs, and found that for all spin states of $\\chi_{c}$ the\nsuggested process is determined by the behavior of the gluon GPDs $H_{g}$ in\nthe ERBL kinematics. Using the phenomenological parametrizations of the GPDs\nfrom the literature, we estimated numerically the photoproduction\ncross-sections and the expected counting rates in the kinematics of\nmiddle-energy photoproduction experiments that could be realized at the\nElectron-Ion Collider. We observed that the $\\chi_{c1}$, $\\chi_{c2}$ mesons are\nproduced predominantly with the same polarization as the incoming photon, and\nthe expected counting rates of $\\chi_{c1}\\gamma$ and $\\chi_{c2}\\gamma$ pairs\nare sufficiently large for their experimental study. We also analyzed the\nangular distribution of $\\chi_{c}$ mesons in electroproduction experiments, and\nnoticed that for some helicity components there are sizable angular\nasymmetries, which can be used as complementary observables for experimental\nstudy. Finally, we estimated the role of this process as a potential background\nto $\\chi_{c}$ photoproduction, which has been recently suggested as a tool for\nstudies of odderons. We found that the contribution of $\\chi_{c}\\gamma$ (with\nundetected photon) is on par with expected contributions of odderons and\nPrimakoff mechanisms in the kinematics of small momentum transfer\n$|t|\\lesssim1$ GeV$^{2}$, but becomes negligible at larger $|t|$.",
        "In a uniformly hyperbolic system, we consider the problem of finding the\noptimal infinitesimal perturbation to apply to the system, from a certain set\n$P$ of feasible ones, to maximally increase the expectation of a given\nobservation function. We perturb the system both by composing with a\ndiffeomorphism near the identity or by adding a deterministic perturbation to\nthe dynamics. In both cases, using the fast adjoint response formula, we show\nthat the linear response operator, which associates the response of the\nexpectation to the perturbation on the dynamics, is bounded in terms of the\n$C^{1,\\alpha}$ norm of the perturbation. Under the assumption that $P$ is a\nstrictly convex, closed subset of a Hilbert space $\\cH$ that can be\ncontinuously mapped in the space of $C^3$ vector fields on our phase space, we\nshow that there is a unique optimal perturbation in $P$ that maximizes the\nincrease of the given observation function. Furthermore since the response\noperator is represented by a certain element $v$ of $\\cH$, when the feasible\nset $P$ is the unit ball of $\\cH$, the optimal perturbation is $v\/||v||_{\\cH}$.\nWe also show how to compute the Fourier expansion $v$ in different cases. Our\napproach can work even on high dimensional systems. We demonstrate our method\non numerical examples in dimensions 2, 3, and 21.",
        "Lyman-$\\alpha$(Ly$\\alpha$) forest in the spectra of distant quasars encodes\nthe information of the underlying cosmic density field at smallest scales. The\nmodelling of the upcoming large and high-fidelity forest data using\ncosmological hydrodynamical simulations is computationally challenging and\ntherefore, requires accurate semi-analytical techniques. One such approach is\nbased on the assumption that baryonic density fields in the intergalactic\nmedium (IGM) follow lognormal distribution. Keeping this in mind, we extend our\nearlier work to improve the lognormal model of the Ly$\\alpha$ forest in\nrecovering the parameters characterizing IGM state, particularly the hydrogen\nphotoionization rate ($\\Gamma_{12}$), between $2.2 \\leq z \\leq 2.7$, by\nsimulating the model spectra at a slightly lower redshift than the Sherwood\nsmooth particle hydrodynamical simulations (SPH) data. The recovery of thermal\nparameters, namely, the mean-density IGM temperature ($T_0$) and the slope of\nthe temperature-density relation ($\\gamma$) is also alleviated. These\nparameters are estimated through a Markov Chain Monte Carlo (MCMC) technique,\nusing the mean and power spectrum of the transmitted flux. We find that the\nusual lognormal distribution of IGM densities tend to over-predict the number\nof Ly$\\alpha$ absorbers seen in SPH simulation. A lognormal model simulated at\na lower redshift than SPH data can address this limitation to a certain extent.\nWe show that with such a \"trailing\" model of lognormal distribution, values of\n$\\Gamma_{12}$ are recovered at $\\lesssim 1-\\sigma$. We argue that this model\ncan be useful for constraining cosmological parameters.",
        "This study evaluates path sets generation for route choice models in\nmultimodal public transportation networks, using both conventional (network\nalgorithms) and empirical (smart card data driven) methods. While the empirical\napproach can present limitations with a short observation period, it improves\nsubstantially with more data, offering a computational efficiency advantage\nover conventional methods. In such approach, while incorporating real-world\ndelays increased travel time variability, it still aligned with planned travel\ntimes, and relaxing access\/egress assumptions further enhanced coverage. Work\nis undergoing on the evaluation of the impact of different choice sets in bias\nand efficiency of route choice parameter estimates.",
        "The impact of quantum mechanics on thermodynamics, particularly on the\nprinciples and designs of heat machines (HM), has been limited by the\nincompatibility of quantum coherent evolution with the dissipative, open-system\nnature of all existing HM and their basic structure, which has not been\nradically changed since Carnot. We have recently proposed a paradigm change\nwhereby conventional HM functionality is replaced by that of few-mode coherent,\nclosed systems with nonlinear, e.g. cross-Kerr, inter-mode couplings. These\ncouplings allow us to coherently filter incident thermal noise, transforming it\ninto a resource of work and information. Current technological advances enable\nheat engines, noise sensors or microscopes based on such designs to operate\nwith thermal noise sources of few photons. This paradigm shift opens a path\ntowards radically new understanding and exploitation of the relation between\ncoherent, quantum or classical, evolution and thermodynamic behavior.",
        "We compute the phase separation of the immiscible liquid alloy Fe-Cu-Ni. Our\ncomputational approach uses a virtual semigrand canonical Widom approach to\ndetermine differences in excess chemical potentials between different species.\nUsing an embedded atom potential for Fe-Cu-Ni, we simulate liquid states over a\nrange of compositions and temperatures. This raw data is then fit to\nRedlich-Kister polynomials for the Gibbs free energy with a simple temperature\ndependence. Using the analytic form, we can determine the phase diagram for the\nternary liquid, compute the miscibility gap and spinodal decomposition as a\nfunction of temperature for this EAM potential. In addition, we compute density\nas a function of composition and temperature, and predict pair correlation\nfunctions. We use static structure factors to estimate the second derivative of\nthe Gibbs free energy (the $S^0$ method) and compare with our fit Gibbs free\nenergy. Finally, using a nonequilibrium Hamiltonian integration method, we\nseparately compute absolute Gibbs free energies for the pure liquid states;\nthis shows that our endpoints are accurate to within 1 meV for our ternary\nGibbs free energy, as well as the absolute Gibbs free energy for the ternary\nliquid.",
        "The Standard Model Effective Field Theory (SMEFT) provides a robust framework\nfor probing deviations in the couplings of Standard Model particles from their\ntheoretical predictions. This framework relies on an expansion in\nhigher-dimensional operators, often truncated at dimension-six. In this work,\nwe compute the effective dimension-eight operators generated by integrating out\nheavy scalar fields at one-loop order in the Green's basis within two extended\nscalar sector models: the Two Higgs Doublet Model and the Complex Triplet\nScalar Model. We also investigate the impact of heavy scalar fields on the\nfermion sector, deriving the fermionic effective operators up to dimension\neight for these models, and detail how contributions can be mapped onto\nnon-redundant bases. To assess the importance of higher-order contributions in\nthe SMEFT expansion, we analyze the dimension-eight effects for electroweak\nprecision observables at the next frontier of precision lepton machines such as\nGigaZ.",
        "We prove by construction that the Bravyi-Poulin-Terhal bound on the spatial\ndensity of stabilizer codes does not generalize to stabilizer circuits. To do\nso, we construct a fault tolerant quantum computer with a coding rate above 5%\nand quasi-polylog time overhead, out of a line of qubits with nearest-neighbor\nconnectivity, and prove it has a threshold. The construction is based on\nmodifications to the tower of Hamming codes of Yamasaki and Koashi (Nature\nPhysics, 2024), with operators measured using a variant of Shor's measurement\ngadget.",
        "We study the Poisson Boolean model with convex bodies which are\nrotation-invariant distributed. We assume that the convex bodies have regularly\nvarying diameters with indices $-\\alpha_1\\geq \\dots\\geq-\\alpha_d$ where\n$\\alpha_k >0$ for all $k\\in\\{1,\\dots,d\\}.$ It is known that a sufficient\ncondition for the robustness of the model, i.e. the union of the convex bodies\nhas an unbounded connected component no matter what the intensity of the\nunderlying Poisson process is, is that there exists some $k\\in\\{1,\\dots,d\\}$\nsuch that $\\alpha_k<\\min\\{2k,d\\}$. To avoid that this connected component\ncovers all of $\\mathbb{R}^d$ almost surely we also require $\\alpha_k> k$ for\nall $k\\in\\{1,\\dots,d\\}$. We show that under these assumptions, the chemical\ndistance of two far apart vertices $\\mathbf{x}$ and $\\mathbf{y}$ behaves like\n$c\\log\\log|x-y|$ as $|x-y|\\rightarrow \\infty$, with an explicit and very\nsurprising constant $c$ that depends only on the model parameters. We\nfurthermore show that if there exists $k$ such that $\\alpha_k\\leq k$, the\nchemical distance is smaller than $c\\log\\log|x-y|$ for all $c>0$ and that if\n$\\alpha_k\\geq\\min\\{2k,d\\}$ for all $k$, it is bigger than $c\\log\\log|x-y|$ for\nall $c>0$."
      ]
    }
  },
  {
    "id":2411.14474,
    "research_type":"applied",
    "start_id":"b15",
    "start_title":"A deep representation for invariance and music classification",
    "start_abstract":"Representations in the auditory cortex might be based on mechanisms similar to visual ventral stream; modules for building invariance transformations and multiple layers compositionality selectivity. In this paper we propose use of such computational extracting invariant discriminative audio representations. Building a theory hierarchical architectures, novel, mid-level representation acoustical signals, using empirical distributions projections set templates their transformations. Under assumption that, by construction, dictionary is composed from classes, samples orbit variance-inducing signal (such as shift scale), resulting signature theoretically guaranteed unique, stable deformations. Modules projection pooling can then constitute deep networks, learning composite We present main theoretical aspects framework unsupervised representations, empirically evaluated music genre classification.",
    "start_categories":[
      "physics.class-ph"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "Convolutional recurrent neural networks for music classification"
      ],
      "abstract":[
        "We introduce a convolutional recurrent neural network (CRNN) for music tagging. CRNNs take advantage of networks (CNNs) local feature extraction and temporal summarisation the extracted features. compare CRNN with three CNN structures that have been used tagging while controlling number parameters respect to their performance training time per sample. Overall, we found show strong parameter time, indicating effectiveness its hybrid structure in summarisation."
      ],
      "categories":[
        "cs.NE"
      ]
    },
    "list":{
      "title":[
        "A shape-optimization approach for inverse diffusion problems using a\n  single boundary measurement",
        "Hydrated Cable Bacteria Exhibit Protonic Conductivity Over Long\n  Distances",
        "Torsion models for tensor-triangulated categories",
        "RiLoCo: An ISAC-oriented AI Solution to Build RIS-empowered Networks",
        "Localization of critical points in annular conical sets via the method\n  of Nehari manifold",
        "Temporal Preference Optimization for Long-Form Video Understanding",
        "Detecting APT Malware Command and Control over HTTP(S) Using Contextual\n  Summaries",
        "PCSI -- The Platform for Content-Structure Inference",
        "Gen-A: Generalizing Ambisonics Neural Encoding to Unseen Microphone\n  Arrays",
        "Scalable intensity-based photonic matrix-vector multiplication processor\n  using single-wavelength time-division-multiplexed signals",
        "Time Warp: The Gap Between Developers' Ideal vs Actual Workweeks in an\n  AI-Driven Era",
        "Algorithms and Hardness Results for the $(k,\\ell)$-Cover Problem",
        "A Feature-Level Ensemble Model for COVID-19 Identification in CXR Images\n  using Choquet Integral and Differential Evolution Optimization",
        "Spread them Apart: Towards Robust Watermarking of Generated Content",
        "Uncertainty Expression for Human-Robot Task Communication",
        "Spatial Context-Driven Positive Pair Sampling for Enhanced\n  Histopathology Image Classification",
        "Resurrecting saturated LLM benchmarks with adversarial encoding",
        "Evolving Performance Practices in Beethoven's Cello Sonatas: Tempo,\n  Portamento, and Historical Interpretation of the First Movements",
        "IM360: Textured Mesh Reconstruction for Large-scale Indoor Mapping with\n  360$^\\circ$ Cameras",
        "A Binary Classification Social Network Dataset for Graph Machine\n  Learning",
        "IPCGRL: Language-Instructed Reinforcement Learning for Procedural Level\n  Generation",
        "$k$-SVD with Gradient Descent",
        "Efficient and Privacy-Preserved Link Prediction via Condensed Graphs",
        "DeepSolution: Boosting Complex Engineering Solution Design via\n  Tree-based Exploration and Bi-point Thinking",
        "Supervised Learning with Evolving Tasks and Performance Guarantees",
        "On Statistical Estimation of Edge-Reinforced Random Walks",
        "CoME: An Unlearning-based Approach to Conflict-free Model Editing",
        "Semantically Cohesive Word Grouping in Indian Languages",
        "PropNet: a White-Box and Human-Like Network for Sentence Representation"
      ],
      "abstract":[
        "This paper explores the reconstruction of a space-dependent parameter in\ninverse diffusion problems, proposing a shape-optimization-based approach. The\nmain objective is to recover the absorption coefficient from a single boundary\nmeasurement. While conventional gradient-based methods rely on the Fr\\'{e}chet\nderivative of a cost functional with respect to the unknown parameter, we also\nutilize its shape derivative with respect to the unknown boundary interface for\nrecovery. This non-conventional approach addresses the problem of parameter\nrecovery from a single measurement, which represents the key innovation of this\nwork. Numerical experiments confirm the effectiveness of the proposed method,\neven for intricate and non-convex boundary interfaces.",
        "This study presents the direct measurement of proton transport along\nfilamentous Desulfobulbaceae, or cable bacteria. Cable bacteria are filamentous\nmulticellular microorganisms that have garnered much interest due to their\nability to serve as electrical conduits, transferring electrons over several\nmillimeters. Our results indicate that cable bacteria can also function as\nprotonic conduits because they contain proton wires that transport protons at\ndistances greater than 100 um. We find that protonic conductivity ({\\sigma}P)\nalong cable bacteria varies between samples and is measured as high as 114 +\/-\n28 uS cm^-1 at 25-degrees C and 70-percent relative humidity (RH). For cable\nbacteria, the protonic conductance (GP) and {\\sigma}P are dependent upon the\nRH, increasing by as much as 26-fold between 60-percent and 80-percent RH. This\nobservation implies that proton transport occurs via the Grotthuss mechanism\nalong water associated with cable bacteria, forming proton wires. In order to\ndetermine {\\sigma}P and GP along cable bacteria, we implemented a protocol\nusing a modified transfer-printing technique to deposit either palladium\ninterdigitated protodes (IDP), palladium transfer length method (TLM) protodes,\nor gold interdigitated electrodes(IDE) on top of cable bacteria. Due to the\nrelatively mild nature of the transfer-printing technique, this method should\nbe applicable to a broad array of biological samples and curved materials. The\nobservation of protonic conductivity in cable bacteria presents possibilities\nfor investigating the importance of long-distance proton transport in microbial\necosystems and to potentially build biotic or biomimetic scaffolds to interface\nwith materials via proton-mediated gateways or channels.",
        "Given a rigidly-compactly generated tensor-triangulated category whose Balmer\nspectrum is finite dimensional and Noetherian, we construct a torsion model for\nit, which is equivalent to the original tensor-triangulated category. The\ntorsion model is determined in an adelic fashion by objects with singleton\nsupports. This categorifies the Cousin complex from algebra, and the process of\nreconstructing a spectrum from its monochromatic layers in chromatic stable\nhomotopy theory. This model is inspired by work of the second author in\nrational equivariant stable homotopy theory, and extends previous work of the\nauthors from the one-dimensional setting.",
        "The advance towards 6G networks comes with the promise of unprecedented\nperformance in sensing and communication capabilities. The feat of achieving\nthose, while satisfying the ever-growing demands placed on wireless networks,\npromises revolutionary advancements in sensing and communication technologies.\nAs 6G aims to cater to the growing demands of wireless network users, the\nimplementation of intelligent and efficient solutions becomes essential. In\nparticular, reconfigurable intelligent surfaces (RISs), also known as Smart\nSurfaces, are envisioned as a transformative technology for future 6G networks.\nThe performance of RISs when used to augment existing devices is nevertheless\nlargely affected by their precise location. Suboptimal deployments are also\ncostly to correct, negating their low-cost benefits. This paper investigates\nthe topic of optimal RISs diffusion, taking into account the improvement they\nprovide both for the sensing and communication capabilities of the\ninfrastructure while working with other antennas and sensors. We develop a\ncombined metric that takes into account the properties and location of the\nindividual devices to compute the performance of the entire infrastructure. We\nthen use it as a foundation to build a reinforcement learning architecture that\nsolves the RIS deployment problem. Since our metric measures the surface where\ngiven localization thresholds are achieved and the communication coverage of\nthe area of interest, the novel framework we provide is able to seamlessly\nbalance sensing and communication, showing its performance gain against\nreference solutions, where it achieves simultaneously almost the reference\nperformance for communication and the reference performance for localization.",
        "Using the Nehari manifold method, we establish sufficient conditions such\nthat a smooth functional attains a ground state within an annular domain of a\nclosed cone. The localization we obtain immediately allows for multiplicity\nwhen applied to disjoint conical sets. To illustrate our results, we consider a\ntwo-point boundary value problem and obtain a solution within a shell of a\nclosed cone, defined in terms of a Harnack inequality with respect to the\nenergy norm. The conditions imposed on the nonlinear term naturally extend\nthose from classical examples in the literature which were derived using the\nmethod of Nehari manifold on the entire domain.",
        "Despite significant advancements in video large multimodal models\n(video-LMMs), achieving effective temporal grounding in long-form videos\nremains a challenge for existing models. To address this limitation, we propose\nTemporal Preference Optimization (TPO), a novel post-training framework\ndesigned to enhance the temporal grounding capabilities of video-LMMs through\npreference learning. TPO adopts a self-training approach that enables models to\ndifferentiate between well-grounded and less accurate temporal responses by\nleveraging curated preference datasets at two granularities: localized temporal\ngrounding, which focuses on specific video segments, and comprehensive temporal\ngrounding, which captures extended temporal dependencies across entire video\nsequences. By optimizing on these preference datasets, TPO significantly\nenhances temporal understanding while reducing reliance on manually annotated\ndata. Extensive experiments on three long-form video understanding\nbenchmarks--LongVideoBench, MLVU, and Video-MME--demonstrate the effectiveness\nof TPO across two state-of-the-art video-LMMs. Notably, LLaVA-Video-TPO\nestablishes itself as the leading 7B model on the Video-MME benchmark,\nunderscoring the potential of TPO as a scalable and efficient solution for\nadvancing temporal reasoning in long-form video understanding. Project page:\nhttps:\/\/ruili33.github.io\/tpo_website.",
        "Advanced Persistent Threats (APTs) are among the most sophisticated threats\nfacing critical organizations worldwide. APTs employ specific tactics,\ntechniques, and procedures (TTPs) which make them difficult to detect in\ncomparison to frequent and aggressive attacks. In fact, current network\nintrusion detection systems struggle to detect APTs communications, allowing\nsuch threats to persist unnoticed on victims' machines for months or even\nyears. In this paper, we present EarlyCrow, an approach to detect APT malware\ncommand and control over HTTP(S) using contextual summaries.\n  The design of EarlyCrow is informed by a novel threat model focused on TTPs\npresent in traffic generated by tools recently used as part of APT campaigns.\nThe threat model highlights the importance of the context around the malicious\nconnections, and suggests traffic attributes which help APT detection.\nEarlyCrow defines a novel multipurpose network flow format called PairFlow,\nwhich is leveraged to build the contextual summary of a PCAP capture,\nrepresenting key behavioral, statistical and protocol information relevant to\nAPT TTPs. We evaluate the effectiveness of EarlyCrow on unseen APTs obtaining a\nheadline macro average F1-score of 93.02% with FPR of $0.74%.",
        "The Platform for Content-Structure Inference (PCSI, pronounced \"pixie\")\nfacilitates the sharing of information about the process of converting Web\nresources into structured content objects that conform to a predefined format.\nPCSI records encode methods for deriving structured content from classes of\nURLs, and report the results of applying particular methods to particular URLs.\nThe methods are scripts written in Hex, a variant of Awk with facilities for\ntraversing the HTML DOM.",
        "Using deep neural networks (DNNs) for encoding of microphone array (MA)\nsignals to the Ambisonics spatial audio format can surpass certain limitations\nof established conventional methods, but existing DNN-based methods need to be\ntrained separately for each MA. This paper proposes a DNN-based method for\nAmbisonics encoding that can generalize to arbitrary MA geometries unseen\nduring training. The method takes as inputs the MA geometry and MA signals and\nuses a multi-level encoder consisting of separate paths for geometry and signal\ndata, where geometry features inform the signal encoder at each level. The\nmethod is validated in simulated anechoic and reverberant conditions with one\nand two sources. The results indicate improvement over conventional encoding\nacross the whole frequency range for dry scenes, while for reverberant scenes\nthe improvement is frequency-dependent.",
        "Photonic integrated circuits provide a compact platform for ultrafast and\nenergy-efficient matrix-vector multiplications (MVMs) in the optical domain.\nRecently, schemes based on time-division multiplexing (TDM) have been proposed\nas scalable approaches for realizing large-scale photonic MVM processors.\nHowever, existing demonstrations rely on coherent detection or multiple\nwavelengths, both of which complicate their operations. In this work, we\ndemonstrate a scalable TDM-based photonic MVM processor that uses only\nsingle-wavelength intensity-modulated optical signals, thereby avoiding\ncoherent detection and enabling simplified operations. A 32-channel processor\nis fabricated on a Si-on-insulator (SOI) platform and used to experimentally\nperform convolution operations in a convolutional neural network (CNN) for\nhandwritten digit recognition, achieving a classification accuracy of 93.47%\nfor 1500 images.",
        "Software developers balance a variety of different tasks in a workweek, yet\nthe allocation of time often differs from what they consider ideal. Identifying\nand addressing these deviations is crucial for organizations aiming to enhance\nthe productivity and well-being of the developers. In this paper, we present\nthe findings from a survey of 484 software developers at Microsoft, which aims\nto identify the key differences between how developers would like to allocate\ntheir time during an ideal workweek versus their actual workweek. Our analysis\nreveals significant deviations between a developer's ideal workweek and their\nactual workweek, with a clear correlation: as the gap between these two\nworkweeks widens, we observe a decline in both productivity and satisfaction.\nBy examining these deviations in specific activities, we assess their direct\nimpact on the developers' satisfaction and productivity. Additionally, given\nthe growing adoption of AI tools in software engineering, both in the industry\nand academia, we identify specific tasks and areas that could be strong\ncandidates for automation. In this paper, we make three key contributions: 1)\nWe quantify the impact of workweek deviations on developer productivity and\nsatisfaction 2) We identify individual tasks that disproportionately affect\nsatisfaction and productivity 3) We provide actual data-driven insights to\nguide future AI automation efforts in software engineering, aligning them with\nthe developers' requirements and ideal workflows for maximizing their\nproductivity and satisfaction.",
        "A connected graph has a $(k,\\ell)$-cover if each of its edges is contained in\nat least $\\ell$ cliques of order $k$. Motivated by recent advances in extremal\ncombinatorics and the literature on edge modification problems, we study the\nalgorithmic version of the $(k,\\ell)$-cover problem. Given a connected graph\n$G$, the $(k, \\ell)$-cover problem is to identify the smallest subset of\nnon-edges of $G$ such that their addition to $G$ results in a graph with a $(k,\n\\ell)$-cover. For every constant $k\\geq3$, we show that the $(k,1)$-cover\nproblem is $\\mathbb{NP}$-complete for general graphs. Moreover, we show that\nfor every constant $k\\geq 3$, the $(k,1)$-cover problem admits no\npolynomial-time constant-factor approximation algorithm unless\n$\\mathbb{P}=\\mathbb{NP}$. However, we show that the $(3,1)$-cover problem can\nbe solved in polynomial time when the input graph is chordal. For the class of\ntrees and general values of $k$, we show that the $(k,1)$-cover problem is\n$\\mathbb{NP}$-hard even for spiders. However, we show that for every $k\\geq4$,\nthe $(3,k-2)$-cover and the $(k,1)$-cover problems are constant-factor\napproximable when the input graph is a tree.",
        "The COVID-19 pandemic has profoundly impacted billions globally. It\nchallenges public health and healthcare systems due to its rapid spread and\nsevere respiratory effects. An effective strategy to mitigate the COVID-19\npandemic involves integrating testing to identify infected individuals. While\nRT-PCR is considered the gold standard for diagnosing COVID-19, it has some\nlimitations such as the risk of false negatives. To address this problem, this\npaper introduces a novel Deep Learning Diagnosis System that integrates\npre-trained Deep Convolutional Neural Networks (DCNNs) within an ensemble\nlearning framework to achieve precise identification of COVID-19 cases from\nChest X-ray (CXR) images. We combine feature vectors from the final hidden\nlayers of pre-trained DCNNs using the Choquet integral to capture interactions\nbetween different DCNNs that a linear approach cannot. We employed\nSugeno-$\\lambda$ measure theory to derive fuzzy measures for subsets of\nnetworks to enable aggregation. We utilized Differential Evolution to estimate\nfuzzy densities. We developed a TensorFlow-based layer for Choquet operation to\nfacilitate efficient aggregation, due to the intricacies involved in\naggregating feature vectors. Experimental results on the COVIDx dataset show\nthat our ensemble model achieved 98\\% accuracy in three-class classification\nand 99.50\\% in binary classification, outperforming its components-DenseNet-201\n(97\\% for three-class, 98.75\\% for binary), Inception-v3 (96.25\\% for\nthree-class, 98.50\\% for binary), and Xception (94.50\\% for three-class, 98\\%\nfor binary)-and surpassing many previous methods.",
        "Generative models that can produce realistic images have improved\nsignificantly in recent years. The quality of the generated content has\nincreased drastically, so sometimes it is very difficult to distinguish between\nthe real images and the generated ones. Such an improvement comes at a price of\nethical concerns about the usage of the generative models: the users of\ngenerative models can improperly claim ownership of the generated content\nprotected by a license. In this paper, we propose an approach to embed\nwatermarks into the generated content to allow future detection of the\ngenerated content and identification of the user who generated it. The\nwatermark is embedded during the inference of the model, so the proposed\napproach does not require the retraining of the latter. We prove that\nwatermarks embedded are guaranteed to be robust against additive perturbations\nof a bounded magnitude. We apply our method to watermark diffusion models and\nshow that it matches state-of-the-art watermarking schemes in terms of\nrobustness to different types of synthetic watermark removal attacks.",
        "An underlying assumption of many existing approaches to human-robot task\ncommunication is that the robot possesses a sufficient amount of environmental\ndomain knowledge, including the locations of task-critical objects. This\nassumption is unrealistic if the locations of known objects change or have not\nyet been discovered by the robot. In this work, our key insight is that in many\nscenarios, robot end users possess more scene insight than the robot and need\nways to express it. Presently, there is a lack of research on how solutions for\ncollecting end-user scene insight should be designed. We thereby created an\nUncertainty Expression System (UES) to investigate how best to elicit end-user\nscene insight. The UES allows end users to convey their knowledge of object\nuncertainty using either: (1) a precision interface that allows meticulous\nexpression of scene insight; (2) a painting interface by which users create a\nheat map of possible object locations; and (3) a ranking interface by which end\nusers express object locations via an ordered list. We then conducted a user\nstudy to compare the effectiveness of these approaches based on the accuracy of\nscene insight conveyed to the robot, the efficiency at which end users are able\nto express this scene insight, and both usability and task load. Results\nindicate that the rank interface is more user friendly and efficient than the\nprecision interface, and that the paint interface is the least accurate.",
        "Deep learning has demonstrated great promise in cancer classification from\nwhole-slide images (WSIs) but remains constrained by the need for extensive\nannotations. Annotation-free methods, such as multiple instance learning (MIL)\nand self-supervised learning (SSL), have emerged to address this challenge;\nhowever, current SSL techniques often depend on synthetic augmentations or\ntemporal context, which may not adequately capture the intricate spatial\nrelationships inherent to histopathology. In this work, we introduce a novel\nspatial context-driven positive pair sampling strategy for SSL that leverages\nthe natural coherence of adjacent patches in WSIs. By constructing biologically\nrelevant positive pairs from spatially proximate patches, our approach\nharnesses inherent spatial coherence to enhance patch-level representations,\nultimately boosting slide-level classification performance. Experiments on\nmultiple datasets reveal that our strategy improves classification accuracy by\n5\\% to 10\\% over the standard method, paving the way for more clinically\nrelevant AI models in cancer diagnosis. The code is available at\nhttps:\/\/anonymous.4open.science\/r\/contextual-pairs-E72F\/.",
        "Recent work showed that small changes in benchmark questions can reduce LLMs'\nreasoning and recall. We explore two such changes: pairing questions and adding\nmore answer options, on three benchmarks: WMDP-bio, GPQA, and MMLU variants. We\nfind that for more capable models, these predictably reduce performance,\nessentially heightening the performance ceiling of a benchmark and unsaturating\nit again. We suggest this approach can resurrect old benchmarks.",
        "This paper examines the evolving performance practices of Ludwig van\nBeethoven's cello sonatas, with a particular focus on tempo and portamento\nbetween 1930 and 2012. It integrates analyses of 22 historical recordings,\nadvancements in recording technology to shed light on changes in interpretative\napproaches. By comparing Beethoven's metronome markings, as understood through\ncontemporaries such as Czerny and Moscheles, with their application in modern\nperformances, my research highlights notable deviations. These differences\nprove the challenges performers face in reconciling historical tempos with the\ndemands of contemporary performance practice. My study pays special attention\nto the diminishing use of audible portamento in the latter half of the 20th\ncentury, contrasted with a gradual increase in tempo after 1970. This\ndevelopment is linked to broader cultural and pedagogical shifts, including the\nadoption of fingering techniques that reduce hand shifts, thereby facilitating\ngreater technical precision at faster tempos. Nonetheless, my study identifies\nthe persistence of 'silent portamento' as an expressive device, allowing\nperformers to retain stylistic expression without compromising rhythmic\nintegrity. My paper offers valuable insights for performers and scholars alike,\nadvocating a critical reassessment of Beethoven's tempo markings and the\nnuanced application of portamento in modern performance practice.",
        "We present a novel 3D reconstruction pipeline for 360$^\\circ$ cameras for 3D\nmapping and rendering of indoor environments. Traditional Structure-from-Motion\n(SfM) methods may not work well in large-scale indoor scenes due to the\nprevalence of textureless and repetitive regions. To overcome these challenges,\nour approach (IM360) leverages the wide field of view of omnidirectional images\nand integrates the spherical camera model into every core component of the SfM\npipeline. In order to develop a comprehensive 3D reconstruction solution, we\nintegrate a neural implicit surface reconstruction technique to generate\nhigh-quality surfaces from sparse input data. Additionally, we utilize a\nmesh-based neural rendering approach to refine texture maps and accurately\ncapture view-dependent properties by combining diffuse and specular components.\nWe evaluate our pipeline on large-scale indoor scenes from the Matterport3D and\nStanford2D3D datasets. In practice, IM360 demonstrate superior performance in\nterms of textured mesh reconstruction over SOTA. We observe accuracy\nimprovements in terms of camera localization and registration as well as\nrendering high frequency details.",
        "Social networks have a vast range of applications with graphs. The available\nbenchmark datasets are citation, co-occurrence, e-commerce networks, etc, with\nclasses ranging from 3 to 15. However, there is no benchmark classification\nsocial network dataset for graph machine learning. This paper fills the gap and\npresents the Binary Classification Social Network Dataset (\\textit{BiSND}),\ndesigned for graph machine learning applications to predict binary classes. We\npresent the BiSND in \\textit{tabular and graph} formats to verify its\nrobustness across classical and advanced machine learning. We employ a diverse\nset of classifiers, including four traditional machine learning algorithms\n(Decision Trees, K-Nearest Neighbour, Random Forest, XGBoost), one Deep Neural\nNetwork (multi-layer perceptrons), one Graph Neural Network (Graph\nConvolutional Network), and three state-of-the-art Graph Contrastive Learning\nmethods (BGRL, GRACE, DAENS). Our findings reveal that BiSND is suitable for\nclassification tasks, with F1-scores ranging from 67.66 to 70.15, indicating\npromising avenues for future enhancements.",
        "Recent research has highlighted the significance of natural language in\nenhancing the controllability of generative models. While various efforts have\nbeen made to leverage natural language for content generation, research on deep\nreinforcement learning (DRL) agents utilizing text-based instructions for\nprocedural content generation remains limited. In this paper, we propose\nIPCGRL, an instruction-based procedural content generation method via\nreinforcement learning, which incorporates a sentence embedding model. IPCGRL\nfine-tunes task-specific embedding representations to effectively compress\ngame-level conditions. We evaluate IPCGRL in a two-dimensional level generation\ntask and compare its performance with a general-purpose embedding method. The\nresults indicate that IPCGRL achieves up to a 21.4% improvement in\ncontrollability and a 17.2% improvement in generalizability for unseen\ninstructions. Furthermore, the proposed method extends the modality of\nconditional input, enabling a more flexible and expressive interaction\nframework for procedural content generation.",
        "We show that a gradient-descent with a simple, universal rule for step-size\nselection provably finds $k$-SVD, i.e., the $k\\geq 1$ largest singular values\nand corresponding vectors, of any matrix, despite nonconvexity. There has been\nsubstantial progress towards this in the past few years where existing results\nare able to establish such guarantees for the \\emph{exact-parameterized} and\n\\emph{over-parameterized} settings, with choice of oracle-provided step size.\nBut guarantees for generic setting with a step size selection that does not\nrequire oracle-provided information has remained a challenge. We overcome this\nchallenge and establish that gradient descent with an appealingly simple\nadaptive step size (akin to preconditioning) and random initialization enjoys\nglobal linear convergence for generic setting. Our convergence analysis reveals\nthat the gradient method has an attracting region, and within this attracting\nregion, the method behaves like Heron's method (a.k.a. the Babylonian method).\nEmpirically, we validate the theoretical results. The emergence of modern\ncompute infrastructure for iterative optimization coupled with this work is\nlikely to provide means to solve $k$-SVD for very large matrices.",
        "Link prediction is crucial for uncovering hidden connections within complex\nnetworks, enabling applications such as identifying potential customers and\nproducts. However, this research faces significant challenges, including\nconcerns about data privacy, as well as high computational and storage costs,\nespecially when dealing with large-scale networks. Condensed graphs, which are\nmuch smaller than the original graphs while retaining essential information,\nhas become an effective solution to both maintain data utility and preserve\nprivacy. Existing methods, however, initialize synthetic graphs through random\nnode selection without considering node connectivity, and are mainly designed\nfor node classification tasks. As a result, their potential for\nprivacy-preserving link prediction remains largely unexplored. We introduce\nHyDRO\\textsuperscript{+}, a graph condensation method guided by algebraic\nJaccard similarity, which leverages local connectivity information to optimize\ncondensed graph structures. Extensive experiments on four real-world networks\nshow that our method outperforms state-of-the-art methods and even the original\nnetworks in balancing link prediction accuracy and privacy preservation.\nMoreover, our method achieves nearly 20* faster training and reduces storage\nrequirements by 452*, as demonstrated on the Computers dataset, compared to\nlink prediction on the original networks. This work represents the first\nattempt to leverage condensed graphs for privacy-preserving link prediction\ninformation sharing in real-world complex networks. It offers a promising\npathway for preserving link prediction information while safeguarding privacy,\nadvancing the use of graph condensation in large-scale networks with privacy\nconcerns.",
        "Designing solutions for complex engineering challenges is crucial in human\nproduction activities. However, previous research in the retrieval-augmented\ngeneration (RAG) field has not sufficiently addressed tasks related to the\ndesign of complex engineering solutions. To fill this gap, we introduce a new\nbenchmark, SolutionBench, to evaluate a system's ability to generate complete\nand feasible solutions for engineering problems with multiple complex\nconstraints. To further advance the design of complex engineering solutions, we\npropose a novel system, SolutionRAG, that leverages the tree-based exploration\nand bi-point thinking mechanism to generate reliable solutions. Extensive\nexperimental results demonstrate that SolutionRAG achieves state-of-the-art\n(SOTA) performance on the SolutionBench, highlighting its potential to enhance\nthe automation and reliability of complex engineering solution design in\nreal-world applications.",
        "Multiple supervised learning scenarios are composed by a sequence of\nclassification tasks. For instance, multi-task learning and continual learning\naim to learn a sequence of tasks that is either fixed or grows over time.\nExisting techniques for learning tasks that are in a sequence are tailored to\nspecific scenarios, lacking adaptability to others. In addition, most of\nexisting techniques consider situations in which the order of the tasks in the\nsequence is not relevant. However, it is common that tasks in a sequence are\nevolving in the sense that consecutive tasks often have a higher similarity.\nThis paper presents a learning methodology that is applicable to multiple\nsupervised learning scenarios and adapts to evolving tasks. Differently from\nexisting techniques, we provide computable tight performance guarantees and\nanalytically characterize the increase in the effective sample size.\nExperiments on benchmark datasets show the performance improvement of the\nproposed methodology in multiple scenarios and the reliability of the presented\nperformance guarantees.",
        "Reinforced random walks (RRWs), including vertex-reinforced random walks\n(VRRWs) and edge-reinforced random walks (ERRWs), model random walks where the\ntransition probabilities evolve based on prior visitation history~\\cite{mgr,\nfmk, tarres, volkov}. These models have found applications in various areas,\nsuch as network representation learning~\\cite{xzzs}, reinforced\nPageRank~\\cite{gly}, and modeling animal behaviors~\\cite{smouse}, among others.\nHowever, statistical estimation of the parameters governing RRWs remains\nunderexplored. This work focuses on estimating the initial edge weights of\nERRWs using observed trajectory data. Leveraging the connections between an\nERRW and a random walk in a random environment (RWRE)~\\cite{mr, mr2}, as given\nby the so-called \"magic formula\", we propose an estimator based on the\ngeneralized method of moments. To analyze the sample complexity of our\nestimator, we exploit the hyperbolic Gaussian structure embedded in the random\nenvironment to bound the fluctuations of the underlying random edge\nconductances.",
        "Large language models (LLMs) often retain outdated or incorrect information\nfrom pre-training, which undermines their reliability. While model editing\nmethods have been developed to address such errors without full re-training,\nthey frequently suffer from knowledge conflicts, where outdated information\ninterferes with new knowledge. In this work, we propose Conflict-free Model\nEditing (CoME), a novel framework that enhances the accuracy of knowledge\nupdates in LLMs by selectively removing outdated knowledge. CoME leverages\nunlearning to mitigate knowledge interference, allowing new information to be\nintegrated without compromising relevant linguistic features. Through\nexperiments on GPT-J and LLaMA-3 using Counterfact and ZsRE datasets, we\ndemonstrate that CoME improves both editing accuracy and model reliability when\napplied to existing editing methods. Our results highlight that the targeted\nremoval of outdated knowledge is crucial for enhancing model editing\neffectiveness and maintaining the model's generative performance.",
        "Indian languages are inflectional and agglutinative and typically follow\nclause-free word order. The structure of sentences across most major Indian\nlanguages are similar when their dependency parse trees are considered. While\nsome differences in the parsing structure occur due to peculiarities of a\nlanguage or its preferred natural way of conveying meaning, several apparent\ndifferences are simply due to the granularity of representation of the smallest\nsemantic unit of processing in a sentence. The semantic unit is typically a\nword, typographically separated by whitespaces. A single whitespace-separated\nword in one language may correspond to a group of words in another. Hence,\ngrouping of words based on semantics helps unify the parsing structure of\nparallel sentences across languages and, in the process, morphology. In this\nwork, we propose word grouping as a major preprocessing step for any\ncomputational or linguistic processing of sentences for Indian languages. Among\nIndian languages, since Hindi is one of the least agglutinative, we expect it\nto benefit the most from word-grouping. Hence, in this paper, we focus on Hindi\nto study the effects of grouping. We perform quantitative assessment of our\nproposal with an intrinsic method that perturbs sentences by shuffling words as\nwell as an extrinsic evaluation that verifies the importance of word grouping\nfor the task of Machine Translation (MT) using decomposed prompting. We also\nqualitatively analyze certain aspects of the syntactic structure of sentences.\nOur experiments and analyses show that the proposed grouping technique brings\nuniformity in the syntactic structures, as well as aids underlying NLP tasks.",
        "Transformer-based embedding methods have dominated the field of sentence\nrepresentation in recent years. Although they have achieved remarkable\nperformance on NLP missions, such as semantic textual similarity (STS) tasks,\ntheir black-box nature and large-data-driven training style have raised\nconcerns, including issues related to bias, trust, and safety. Many efforts\nhave been made to improve the interpretability of embedding models, but these\nproblems have not been fundamentally resolved. To achieve inherent\ninterpretability, we propose a purely white-box and human-like sentence\nrepresentation network, PropNet. Inspired by findings from cognitive science,\nPropNet constructs a hierarchical network based on the propositions contained\nin a sentence. While experiments indicate that PropNet has a significant gap\ncompared to state-of-the-art (SOTA) embedding models in STS tasks, case studies\nreveal substantial room for improvement. Additionally, PropNet enables us to\nanalyze and understand the human cognitive processes underlying STS benchmarks."
      ]
    }
  },
  {
    "id":2411.14975,
    "research_type":"applied",
    "start_id":"b15",
    "start_title":"HiCervix: An Extensive Hierarchical Dataset and Benchmark for Cervical Cytology Classification",
    "start_abstract":"Cervical cytology is a critical screening strategy for early detection of pre-cancerous and cancerous cervical lesions. The challenge lies in accurately classifying various cell types. Existing automated methods are primarily trained on databases covering narrow range coarse-grained types, which fail to provide comprehensive detailed performance analysis that represents real-world cytopathology conditions. To overcome these limitations, we introduce HiCervix, the most extensive, multi-center dataset currently available public. HiCervix includes 40,229 cells from 4,496 whole slide images, categorized into 29 annotated classes. These classes organized within three-level hierarchical tree capture fine-grained subtype information. exploit semantic correlation inherent this tree, propose HierSwin, vision transformer-based classification network. HierSwin serves as benchmark feature learning both coarse-level fine-level cancer tasks. In our experiments, demonstrated remarkable performance, achieving 92.08% accuracy 82.93% averaged across all three levels. When compared board-certified cytopathologists, achieved high (0.8293 versus 0.7359 accuracy), highlighting its potential clinical applications. This newly released dataset, along with method, poised make substantial impact advancement deep algorithms rapid greatly improve prevention patient outcomes settings.",
    "start_categories":[
      "q-bio.CB"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b17"
      ],
      "title":[
        "LoRA: Low-Rank Adaptation of Large Language Models"
      ],
      "abstract":[
        "An important paradigm of natural language processing consists large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances fine-tuned each with is prohibitively expensive. We propose Low-Rank Adaptation, LoRA, freezes the pre-trained weights injects trainable rank decomposition matrices into layer Transformer architecture, greatly reducing number parameters for downstream tasks. Compared Adam, LoRA can reduce by 10,000 times GPU memory requirement 3 times. performs on-par better than fine-tuning in quality RoBERTa, DeBERTa, GPT-2, GPT-3, despite having fewer a higher training throughput, and, unlike adapters, no additional inference latency. also provide empirical investigation rank-deficiency adaptation, sheds light efficacy LoRA. release package that facilitates integration PyTorch models our implementations checkpoints GPT-2 at https:\/\/github.com\/microsoft\/LoRA."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Probing electric field tunable multiband superconductivity in\n  alternating twisted quadralayer graphene",
        "ExtremeAIGC: Benchmarking LMM Vulnerability to AI-Generated Extremist\n  Content",
        "Online Meta-learning for AutoML in Real-time (OnMAR)",
        "Correlated vibration-solvent and Duschinsky effects on optical\n  spectroscopy",
        "Rise of the Community Champions: From Reviewer Crunch to Community Power",
        "On the diagonals of rational functions: the minimal number of variables\n  (unabridged version)",
        "Magnetic moments in the Poynting theorem, Maxwell equations, Dirac\n  equation, and QED",
        "Analysis and Extension of Noisy-target Training for Unsupervised Target\n  Signal Enhancement",
        "Modified Dai-Liao Spectral Conjugate Gradient Method with Application to\n  Signal Processing",
        "AI-Driven Solutions for Falcon Disease Classification: Concatenated\n  ConvNeXt cum EfficientNet AI Model Approach",
        "Memory-dependent abstractions of stochastic systems through the lens of\n  transfer operators",
        "FairDeFace: Evaluating the Fairness and Adversarial Robustness of Face\n  Obfuscation Methods",
        "Invariance properties of the solution operator for measure-valued\n  semilinear transport equations",
        "An Interactive Framework for Implementing Privacy-Preserving Federated\n  Learning: Experiments on Large Language Models",
        "Extraction of HI gas with bulk motions in the disk of galaxies",
        "Improved $L^p$ bounds for the strong spherical maximal operator",
        "The Gas-to-Dust Ratio Investigation in the Massive Star-Forming region\n  M17",
        "Open-Source Factor Graph Optimization Package for GNSS: Examples and\n  Applications",
        "Identifying Likely-Reputable Blockchain Projects on Ethereum",
        "A Distributional Perspective on Word Learning in Neural Language Models",
        "Experience-replay Innovative Dynamics",
        "Decoding-based Regression",
        "Weakly-Constrained 4D Var for Downscaling with Uncertainty using\n  Data-Driven Surrogate Models",
        "Coherence of a hole spin flopping-mode qubit in a circuit quantum\n  electrodynamics environment",
        "What Influences the Field Goal Attempts of Professional Players?\n  Analysis of Basketball Shot Charts via Log Gaussian Cox Processes with\n  Spatially Varying Coefficients",
        "Hybrid Human-Machine Perception via Adaptive LiDAR for Advanced Driver\n  Assistance Systems",
        "Development and validation of a high-fidelity full-spectrum Monte Carlo\n  model for the Swiss airborne gamma-ray spectrometry system",
        "Does Chain-of-Thought Reasoning Help Mobile GUI Agent? An Empirical\n  Study",
        "Development of an uncertainty-aware equation of state for gold"
      ],
      "abstract":[
        "Alternating twisted multilayer graphene presents a compelling multiband\nsystem for exploring superconductivity. Here we investigate robust\nsuperconductivity in alternating twisted quadralayer graphene, elucidating\ncarrier contributions from both flat and dispersive bands. The\nsuperconductivity is robust, with a strong electrical field tunability, a\nmaximum BKT transition temperature of 1.6 K, and high critical magnetic fields\nbeyond the Pauli limit. We disentangle the carrier density of Dirac bands and\nflat bands from the Landau fan diagram. Moreover, we could estimate the\nflatband Fermi velocity from the obtained high critical current near half\nfilling when superconductivity is killed at finite magnetic fields, and further\nquantify the superfluid stiffness from the low critical current in the\nsuperconducting regime. Our results exhibit the electric field tunable coupling\nstrength within the superconducting phase, revealing unconventional properties\nwith vanishing Fermi velocity and large superfluid stiffness. These phenomena,\nattributed to substantial quantum metric contributions, offer new insights into\nthe mechanisms underlying unconventional superconductivity in moire systems.",
        "Large Multimodal Models (LMMs) are increasingly vulnerable to AI-generated\nextremist content, including photorealistic images and text, which can be used\nto bypass safety mechanisms and generate harmful outputs. However, existing\ndatasets for evaluating LMM robustness offer limited exploration of extremist\ncontent, often lacking AI-generated images, diverse image generation models,\nand comprehensive coverage of historical events, which hinders a complete\nassessment of model vulnerabilities. To fill this gap, we introduce\nExtremeAIGC, a benchmark dataset and evaluation framework designed to assess\nLMM vulnerabilities against such content. ExtremeAIGC simulates real-world\nevents and malicious use cases by curating diverse text- and image-based\nexamples crafted using state-of-the-art image generation techniques. Our study\nreveals alarming weaknesses in LMMs, demonstrating that even cutting-edge\nsafety measures fail to prevent the generation of extremist material. We\nsystematically quantify the success rates of various attack strategies,\nexposing critical gaps in current defenses and emphasizing the need for more\nrobust mitigation strategies.",
        "Automated machine learning (AutoML) is a research area focusing on using\noptimisation techniques to design machine learning (ML) algorithms, alleviating\nthe need for a human to perform manual algorithm design. Real-time AutoML\nenables the design process to happen while the ML algorithm is being applied to\na task. Real-time AutoML is an emerging research area, as such existing\nreal-time AutoML techniques need improvement with respect to the quality of\ndesigns and time taken to create designs. To address these issues, this study\nproposes an Online Meta-learning for AutoML in Real-time (OnMAR) approach.\nMeta-learning gathers information about the optimisation process undertaken by\nthe ML algorithm in the form of meta-features. Meta-features are used in\nconjunction with a meta-learner to optimise the optimisation process. The OnMAR\napproach uses a meta-learner to predict the accuracy of an ML design. If the\naccuracy predicted by the meta-learner is sufficient, the design is used, and\nif the predicted accuracy is low, an optimisation technique creates a new\ndesign. A genetic algorithm (GA) is the optimisation technique used as part of\nthe OnMAR approach. Different meta-learners (k-nearest neighbours, random\nforest and XGBoost) are tested. The OnMAR approach is model-agnostic (i.e. not\nspecific to a single real-time AutoML application) and therefore evaluated on\nthree different real-time AutoML applications, namely: composing an image\nclustering algorithm, configuring the hyper-parameters of a convolutional\nneural network, and configuring a video classification pipeline. The OnMAR\napproach is effective, matching or outperforming existing real-time AutoML\napproaches, with the added benefit of a faster runtime.",
        "Understanding the role of vibrations in optical spectroscopies is essential\nfor the precise interpretation of spectroscopic behavior, especially in systems\nwith complex solvation effects. This workstudies the correlated Duschinsky and\nsolvent effects on the optical spectra using the extended\ndissipaton-equation-of-motion (ext-DEOM) approach, which is an exact and\nnon-Markovian, nonperturbative approach for nonlinear environmental couplings.\nIn the paper, the environment (bath) is composed of the solvent and\nintramolecular vibrational modes whose Duschinsky rotations constitute the\nquardratic couplings to the electronic states. To apply the ext-DEOM, one key\nstep is to obtain the bath coupling descriptors, which is elaborated. As an\naccurate description of solvated molecular systems, the simulating results\ndemonstrate how the above factors affect the position and shape of spectral\nbands.",
        "Academic publishing is facing a crisis driven by exponential growth in\nsubmissions and an overwhelmed peer review system, leading to inconsistent\ndecisions and a severe reviewer shortage. This paper introduces Panvas, a\nplatform that reimagines academic publishing as a continuous, community-driven\nprocess. Panvas addresses these systemic failures with a novel combination of\neconomic incentives (paid reviews) and rich interaction mechanisms\n(multi-dimensional ratings, threaded discussions, and expert-led reviews). By\nmoving beyond the traditional accept\/reject paradigm and integrating paper\nhosting with code\/data repositories and social networking, Panvas fosters a\nmeritocratic environment for scholarly communication and presents a radical\nrethinking of how we evaluate and disseminate scientific knowledge. We present\nthe system design, development roadmap, and a user study plan to evaluate its\neffectiveness.",
        "From some observations on the linear differential operators occurring in the\nLattice Green function of the d-dimensional face centred and simple cubic\nlattices, and on the linear differential operators occurring in the n-particle\ncontributions\n  to the magnetic susceptibility of the square Ising model, we forward some\nconjectures on the diagonals of rational functions. These conjectures are also\nin agreement with exact results we obtain for many Calabi-Yau operators, and\nmany other examples related, or not related to physics.\n  Consider a globally bounded power series which is the diagonal of rational\nfunctions of a certain number of variables, annihilated by an irreducible\nminimal order linear differential operator homomorphic to its adjoint. Among\nthe logarithmic formal series solutions, at the origin, of this operator, call\nn the highest power of the logarithm. We conjecture that this diagonal series\ncan be represented as a diagonal of a rational function with a minimal number\nof variables N_v related to this highest power n by the relation N_v = n +2.\n  Since the operator is homomorphic to its adjoint, its differential Galois\ngroup is symplectic or orthogonal. We also conjecture that the symplectic or\northogonal character of the differential Galois group is related to the parity\nof the highest power n, namely symplectic for n odd and orthogonal for n even.\n  We also sketch the case where the denominator of the rational function is not\nirreducible and is the product of, for instance, two polynomials. The analysis\nof the linear differential operators annihilating the diagonal of rational\nfunction where the denominator is the product of two polynomials, sheds some\nlight on the emergence of such mixture of direct sums and products of factors.\n  The conjecture N_v = n +2 still holds for such reducible linear differential\noperators.",
        "The role of magnetic moments in electrodynamics is examined in this work. The\neffects are described in the context of conventional quantum electrodynamics\nexpressed in terms of the electromagnetic fields or in the context of an\nextended Poynting theorem and extended Maxwell equations. These extensions take\ninto account the energetics of interaction of magnetic moments with\ninhomogeneous magnetic fields. We show how magnetic moment effects are included\nin either version of electrodynamics and that these apparently different\nformulations can give consistent results. In either case, we express the\ninteractions in terms of electromagnetic fields only, avoiding use of a vector\npotential.",
        "Deep neural network-based target signal enhancement (TSE) is usually trained\nin a supervised manner using clean target signals. However, collecting clean\ntarget signals is costly and such signals are not always available. Thus, it is\ndesirable to develop an unsupervised method that does not rely on clean target\nsignals. Among various studies on unsupervised TSE methods, Noisy-target\nTraining (NyTT) has been established as a fundamental method. NyTT simply\nreplaces clean target signals with noisy ones in the typical supervised\ntraining, and it has been experimentally shown to achieve TSE. Despite its\neffectiveness and simplicity, its mechanism and detailed behavior are still\nunclear. In this paper, to advance NyTT and, thus, unsupervised methods as a\nwhole, we analyze NyTT from various perspectives. We experimentally demonstrate\nthe mechanism of NyTT, the desirable conditions, and the effectiveness of\nutilizing noisy signals in situations where a small number of clean target\nsignals are available. Furthermore, we propose an improved version of NyTT\nbased on its properties and explore its capabilities in the dereverberation and\ndeclipping tasks, beyond the denoising task.",
        "In this article, we present a modified variant of the Dai-Liao spectral\nconjugate gradient method, developed through an analysis of eigenvalues and\ninspired by a modified secant condition. We show that the proposed method is\nglobally convergent for general nonlinear functions under standard assumptions.\nBy incorporating the new secant condition and a quasi-Newton direction, we\nintroduce updated spectral parameters. These changes ensure that the resulting\nsearch direction satisfies the sufficient descent property without relying on\nany line search. Numerical experiments show that the proposed algorithm\nperforms better than several existing methods in terms of convergence speed and\ncomputational efficiency. Its effectiveness is further demonstrated through an\napplication in signal processing.",
        "Falconry, an ancient practice of training and hunting with falcons,\nemphasizes the need for vigilant health monitoring to ensure the well-being of\nthese highly valued birds, especially during hunting activities. This research\npaper introduces a cutting-edge approach, which leverages the power of\nConcatenated ConvNeXt and EfficientNet AI models for falcon disease\nclassification. Focused on distinguishing 'Normal,' 'Liver,' and\n'Aspergillosis' cases, the study employs a comprehensive dataset for model\ntraining and evaluation, utilizing metrics such as accuracy, precision, recall,\nand f1-score. Through rigorous experimentation and evaluation, we demonstrate\nthe superior performance of the concatenated AI model compared to traditional\nmethods and standalone architectures. This novel approach contributes to\naccurate falcon disease classification, laying the groundwork for further\nadvancements in avian veterinary AI applications.",
        "With the increasing ubiquity of safety-critical autonomous systems operating\nin uncertain environments, there is a need for mathematical methods for formal\nverification of stochastic models. Towards formally verifying properties of\nstochastic systems, methods based on discrete, finite Markov approximations --\nabstractions -- thereof have surged in recent years. These are found in\ncontexts where: either a) one only has partial, discrete observations of the\nunderlying continuous stochastic process, or b) the original system is too\ncomplex to analyze, so one partitions the continuous state-space of the\noriginal system to construct a handleable, finite-state model thereof. In both\ncases, the abstraction is an approximation of the discrete stochastic process\nthat arises precisely from the discretization of the underlying continuous\nprocess. The fact that the abstraction is Markov and the discrete process is\nnot (even though the original one is) leads to approximation errors. Towards\naccounting for non-Markovianity, we introduce memory-dependent abstractions for\nstochastic systems, capturing dynamics with memory effects. Our contribution is\ntwofold. First, we provide a formalism for memory-dependent abstractions based\non transfer operators. Second, we quantify the approximation error by upper\nbounding the total variation distance between the true continuous state\ndistribution and its discrete approximation.",
        "The lack of a common platform and benchmark datasets for evaluating face\nobfuscation methods has been a challenge, with every method being tested using\narbitrary experiments, datasets, and metrics. While prior work has demonstrated\nthat face recognition systems exhibit bias against some demographic groups,\nthere exists a substantial gap in our understanding regarding the fairness of\nface obfuscation methods. Providing fair face obfuscation methods can ensure\nequitable protection across diverse demographic groups, especially since they\ncan be used to preserve the privacy of vulnerable populations. To address these\ngaps, this paper introduces a comprehensive framework, named FairDeFace,\ndesigned to assess the adversarial robustness and fairness of face obfuscation\nmethods. The framework introduces a set of modules encompassing data\nbenchmarks, face detection and recognition algorithms, adversarial models,\nutility detection models, and fairness metrics. FairDeFace serves as a\nversatile platform where any face obfuscation method can be integrated,\nallowing for rigorous testing and comparison with other state-of-the-art\nmethods. In its current implementation, FairDeFace incorporates 6 attacks, and\nseveral privacy, utility and fairness metrics. Using FairDeFace, and by\nconducting more than 500 experiments, we evaluated and compared the adversarial\nrobustness of seven face obfuscation methods. This extensive analysis led to\nmany interesting findings both in terms of the degree of robustness of existing\nmethods and their biases against some gender or racial groups. FairDeFace also\nuses visualization of focused areas for both obfuscation and verification\nattacks to show not only which areas are mostly changed in the obfuscation\nprocess for some demographics, but also why they failed through focus area\ncomparison of obfuscation and verification.",
        "We provide conditions under which we prove for measure-valued transport\nequations with non-linear reaction term in the space of finite signed Radon\nmeasures, that positivity is preserved, as well as absolute continuity with\nrespect to Lebesgue measure, if the initial condition has that property.\nMoreover, if the initial condition has $L^p$ regular density, then the solution\nhas the same property.",
        "Federated learning (FL) enhances privacy by keeping user data on local\ndevices. However, emerging attacks have demonstrated that the updates shared by\nusers during training can reveal significant information about their data. This\nhas greatly thwart the adoption of FL methods for training robust AI models in\nsensitive applications. Differential Privacy (DP) is considered the gold\nstandard for safeguarding user data. However, DP guarantees are highly\nconservative, providing worst-case privacy guarantees. This can result in\noverestimating privacy needs, which may compromise the model's accuracy.\nAdditionally, interpretations of these privacy guarantees have proven to be\nchallenging in different contexts. This is further exacerbated when other\nfactors, such as the number of training iterations, data distribution, and\nspecific application requirements, can add further complexity to this problem.\nIn this work, we proposed a framework that integrates a human entity as a\nprivacy practitioner to determine an optimal trade-off between the model's\nprivacy and utility. Our framework is the first to address the variable memory\nrequirement of existing DP methods in FL settings, where resource-limited\ndevices (e.g., cell phones) can participate. To support such settings, we adopt\na recent DP method with fixed memory usage to ensure scalable private FL. We\nevaluated our proposed framework by fine-tuning a BERT-based LLM model using\nthe GLUE dataset (a common approach in literature), leveraging the new\naccountant, and employing diverse data partitioning strategies to mimic\nreal-world conditions. As a result, we achieved stable memory usage, with an\naverage accuracy reduction of 1.33% for $\\epsilon = 10$ and 1.9% for $\\epsilon\n= 6$, when compared to the state-of-the-art DP accountant which does not\nsupport fixed memory usage.",
        "We propose a new method for extracting bulk motion gases in the disk of a\ngalaxy from HI data cubes, offering improvements over classical techniques like\nmoment analysis and line profile fitting. Our approach decomposes the\nline-of-sight velocity profiles into multiple Gaussian components, which are\nthen classified into (underlying and dominant) bulk and non-bulk motion gases\nbased on criteria such as HI surface density, velocity dispersion, kinetic\nenergy, and rotation velocity. A 2D tilted-ring analysis is employed to refine\nthe kinematical parametres of the galaxy disk, ensuring robust extraction of\nthe bulk motion gases. We demonstrate the effectiveness of this method using\nthe HI data cubes of NGC 4559 from the WSRT-HALOGAS survey, distinguishing\nbetween bulk and non-bulk gas components. From this, we find that approximately\n50% of the HI gas in NGC 4559 is classified as non-bulk, possibly linked to\nprocesses such as stellar feedback. This work provides a robust framework for\nanalysing HI kinematics of galaxies from high sensitivity HI observations of\ngalaxies like MeerKAT-MHONGOOSE and FAST-FEASTS and allows us to best exploit\nthe kinematic information of the complex gas dynamics within galaxy disks.",
        "We study the $L^p$ mapping properties of the strong spherical maximal\nfunction, which is a multiparameter generalisation of Stein's spherical maximal\nfunction. We show that this operator is bounded on $L^p$ for $p > 2$ in all\ndimensions $n \\geq 3$. This matches the conjectured sharp range $p>(n+1)\/(n-1)$\nwhen $n=3$. For $n=2$ the analogous estimate was recently proved by Chen, Guo\nand Yang.\n  Our result builds upon and improves an earlier bound of Lee, Lee and Oh. The\nmain novelty is an estimate in discretised incidence geometry that bounds the\nvolume of the intersection of thin neighbourhoods of axis-parallel ellipsoids.\nThis estimate is then interpolated with the Fourier analytic $L^p$-Sobolev\nestimates of Lee, Lee and Oh.",
        "M17 is a well-known massive star-forming region, and its Gas-to-Dust Ratio\n(GDR) may vary significantly compared to the other areas. The mass of gas can\nbe traced by the ${\\rm CO}$ emission observed in the \\emph{Milky Way Imaging\nScroll Painting (MWISP) project}. The dust mass can be traced by analyzing the\ninterstellar extinction magnitude obtained from the \\emph{United Kingdom\nInfrared Telescope (UKIRT)}. We computed the ratio ${W({\\rm CO})\/A_V}$: for\n${A_V \\le }$ 10 mag, ${{ W(^{12}{\\rm CO})\/ A_V}= (6.27 \\pm 0.19)}$ ${\\mathrm{{K\n\\cdot km\/s} \\cdot mag^{-1}}}$ and ${{ W(^{13}{\\rm CO})\/ A_V} = (0.75 \\pm\n0.72)}$ ${ \\mathrm{{K \\cdot km\/s} \\cdot mag^{-1}}}$; whereas for ${{A_V} \\ge\n10}$ mag, ${{ W(^{12}{\\rm CO})\/ A_V} = (15.8 \\pm 0.06) }$ ${\\mathrm{{K \\cdot\nkm\/s} \\cdot mag^{-1}}}$ and ${{ W(^{13}{\\rm CO})\/ A_V} = (3.11 \\pm 0.25)}$ ${\n\\mathrm{{K \\cdot km\/s} \\cdot mag^{-1}}}$. Then, we converted the ${W({\\rm\nCO})\/A_V}$ into ${N(\\rm H)\/A_V}$. Using the WD01 model, we derived the GDR: for\n${A_V \\le }$ 10 mag, the GDRs were ${118 \\pm 9}$ for ${^{12}{\\rm CO}}$ and ${83\n\\pm 62}$ for ${^{13}{\\rm CO}}$, comparable to those of the Milky Way; however,\nfor ${A_V \\ge }$ 10 mag, the GDRs increased significantly to ${296 \\pm 3}$ for\n${^{12}{\\rm CO}}$ and ${387 \\pm 40}$ for ${^{13}{\\rm CO}}$, approximately three\ntimes higher than those of the Milky Way. In the discussion, we compared the\nresults of this work with previous studies and provided a detailed discussion\nof the influence of massive stars and other factors on GDR.",
        "State estimation methods using factor graph optimization (FGO) have garnered\nsignificant attention in global navigation satellite system (GNSS) research.\nFGO exhibits superior estimation accuracy compared with traditional state\nestimation methods that rely on least-squares or Kalman filters. However, only\na few FGO libraries are specialized for GNSS observations. This paper\nintroduces an open-source GNSS FGO package named gtsam\\_gnss, which has a\nsimple structure and can be easily applied to GNSS research and development.\nThis package separates the preprocessing of GNSS observations from factor\noptimization. Moreover, it describes the error function of the GNSS factor in a\nstraightforward manner, allowing for general-purpose inputs. This design\nfacilitates the transition from ordinary least-squares-based positioning to FGO\nand supports user-specific GNSS research. In addition, gtsam\\_gnss includes\nanalytical examples involving various factors using GNSS data in real urban\nenvironments. This paper presents three application examples: the use of a\nrobust error model, estimation of integer ambiguity in the carrier phase, and\ncombination of GNSS and inertial measurements from smartphones. The proposed\nframework demonstrates excellent state estimation performance across all use\ncases.",
        "Identifying reputable Ethereum projects remains a critical challenge within\nthe expanding blockchain ecosystem. The ability to distinguish between\nlegitimate initiatives and potentially fraudulent schemes is non-trivial. This\nwork presents a systematic approach that integrates multiple data sources with\nadvanced analytics to evaluate credibility, transparency, and overall\ntrustworthiness. The methodology applies machine learning techniques to analyse\ntransaction histories on the Ethereum blockchain.\n  The study classifies accounts based on a dataset comprising 2,179 entities\nlinked to illicit activities and 3,977 associated with reputable projects.\nUsing the LightGBM algorithm, the approach achieves an average accuracy of\n0.984 and an average AUC of 0.999, validated through 10-fold cross-validation.\nKey influential factors include time differences between transactions and\nreceived_tnx.\n  The proposed methodology provides a robust mechanism for identifying\nreputable Ethereum projects, fostering a more secure and transparent investment\nenvironment. By equipping stakeholders with data-driven insights, this research\nenables more informed decision-making, risk mitigation, and the promotion of\nlegitimate blockchain initiatives. Furthermore, it lays the foundation for\nfuture advancements in trust assessment methodologies, contributing to the\ncontinued development and maturity of the Ethereum ecosystem.",
        "Language models (LMs) are increasingly being studied as models of human\nlanguage learners. Due to the nascency of the field, it is not well-established\nwhether LMs exhibit similar learning dynamics to humans, and there are few\ndirect comparisons between learning trajectories in humans and models. Word\nlearning trajectories for children are relatively well-documented, and recent\nwork has tried to extend these investigations to language models. However,\nthere are no widely agreed-upon metrics for word learning in language models.\nWe take a distributional approach to this problem, defining lexical knowledge\nin terms of properties of the learned distribution for a target word. We argue\nthat distributional signatures studied in prior work fail to capture key\ndistributional information. Thus, we propose an array of signatures that\nimprove on earlier approaches by capturing knowledge of both where the target\nword can and cannot occur as well as gradient preferences about the word's\nappropriateness. We obtain learning trajectories for a selection of small\nlanguage models we train from scratch, study the relationship between different\ndistributional signatures, compare how well they align with human word learning\ntrajectories and interpretable lexical features, and address basic\nmethodological questions about estimating these distributional signatures. Our\nmetrics largely capture complementary information, suggesting that it is\nimportant not to rely on a single metric. However, across all metrics, language\nmodels' learning trajectories fail to correlate with those of children.",
        "Despite its groundbreaking success, multi-agent reinforcement learning (MARL)\nstill suffers from instability and nonstationarity. Replicator dynamics, the\nmost well-known model from evolutionary game theory (EGT), provide a\ntheoretical framework for the convergence of the trajectories to Nash\nequilibria and, as a result, have been used to ensure formal guarantees for\nMARL algorithms in stable game settings. However, they exhibit the opposite\nbehavior in other settings, which poses the problem of finding alternatives to\nensure convergence. In contrast, innovative dynamics, such as the Brown-von\nNeumann-Nash (BNN) or Smith, result in periodic trajectories with the potential\nto approximate Nash equilibria. Yet, no MARL algorithms based on these dynamics\nhave been proposed. In response to this challenge, we develop a novel\nexperience replay-based MARL algorithm that incorporates revision protocols as\ntunable hyperparameters. We demonstrate, by appropriately adjusting the\nrevision protocols, that the behavior of our algorithm mirrors the trajectories\nresulting from these dynamics. Importantly, our contribution provides a\nframework capable of extending the theoretical guarantees of MARL algorithms\nbeyond replicator dynamics. Finally, we corroborate our theoretical findings\nwith empirical results.",
        "Language models have recently been shown capable of performing regression\ntasks wherein numeric predictions are represented as decoded strings. In this\nwork, we provide theoretical grounds for this capability and furthermore\ninvestigate the utility of causal auto-regressive sequence models when they are\napplied to any feature representation. We find that, despite being trained in\nthe usual way - for next-token prediction via cross-entropy loss -\ndecoding-based regression is as performant as traditional approaches for\ntabular regression tasks, while being flexible enough to capture arbitrary\ndistributions, such as in the task of density estimation.",
        "Dynamic downscaling typically involves using numerical weather prediction\n(NWP) solvers to refine coarse data to higher spatial resolutions. Data-driven\nmodels such as FourCastNet have emerged as a promising alternative to the\ntraditional NWP models for forecasting. Once these models are trained, they are\ncapable of delivering forecasts in a few seconds, thousands of times faster\ncompared to classical NWP models. However, as the lead times, and, therefore,\ntheir forecast window, increase, these models show instability in that they\ntend to diverge from reality. In this paper, we propose to use data\nassimilation approaches to stabilize them when used for downscaling tasks. Data\nassimilation uses information from three different sources, namely an imperfect\ncomputational model based on partial differential equations (PDE), from noisy\nobservations, and from an uncertainty-reflecting prior. In this work, when\ncarrying out dynamic downscaling, we replace the computationally expensive\nPDE-based NWP models with FourCastNet in a ``weak-constrained 4DVar framework\"\nthat accounts for the implied model errors. We demonstrate the efficacy of this\napproach for a hurricane-tracking problem; moreover, the 4DVar framework\nnaturally allows the expression and quantification of uncertainty. We\ndemonstrate, using ERA5 data, that our approach performs better than the\nensemble Kalman filter (EnKF) and the unstabilized FourCastNet model, both in\nterms of forecast accuracy and forecast uncertainty.",
        "The entanglement of microwave photons and spin qubits in silicon represents a\npivotal step forward for quantum information processing utilizing semiconductor\nquantum dots. Such hybrid spin circuit quantum electrodynamics (cQED) has been\nachieved by granting a substantial electric dipole moment to a spin by\nde-localizing it in a double quantum dot under spin-orbit interaction, thereby\nforming a flopping-mode (FM) spin qubit. Despite its promise, the coherence\nproperties demonstrated to date remain insufficient to envision FM spin qubits\nas practical single qubits. Here, we present a FM hole spin qubit in a silicon\nnanowire coupled to a high-impedance niobium nitride microwave resonator for\nreadout. We report Rabi frequencies exceeding 100 MHz and coherence times in\nthe microsecond range, resulting in a high single gate quality factor of 380.\nThis establishes FM spin qubits as fast and reliable qubits. Moreover, using\nthe large frequency tunability of the FM qubit, we reveal for the first time\nthat photonic effects predominantly limit coherence, with radiative decay being\nthe main relaxation channel and photon shot-noise inducing dephasing. These\nresults highlight that optimized microwave engineering can unlock the potential\nof FM spin qubits in hybrid cQED architectures, offering a scalable and robust\nplatform for fast and coherent spin qubits with strong coupling to microwave\nphotons.",
        "Basketball shot charts provide valuable information regarding local patterns\nof in-game performance to coaches, players, sports analysts, and statisticians.\nThe spatial patterns of where shots were attempted and whether the shots were\nsuccessful suggest options for offensive and defensive strategies as well as\nhistorical summaries of performance against particular teams and players. The\ndata represent a marked spatio-temporal point process with locations\nrepresenting locations of attempted shots and an associated mark representing\nthe shot's outcome (made\/missed). Here, we develop a Bayesian log Gaussian Cox\nprocess model allowing joint analysis of the spatial pattern of locations and\noutcomes of shots across multiple games. We build a hierarchical model for the\nlog intensity function using Gaussian processes, and allow spatially varying\neffects for various game-specific covariates. We aim to model the spatial\nrelative risk under different covariate values. For inference via posterior\nsimulation, we design a Markov chain Monte Carlo (MCMC) algorithm based on a\nkernel convolution approach. We illustrate the proposed method using extensive\nsimulation studies. A case study analyzing the shot data of NBA legends Stephen\nCurry, LeBron James, and Michael Jordan highlights the effectiveness of our\napproach in real-world scenarios and provides practical insights into\noptimizing shooting strategies by examining how different playing conditions,\ngame locations, and opposing team strengths impact shooting efficiency.",
        "Accurate environmental perception is critical for advanced driver assistance\nsystems (ADAS). Light detection and ranging (LiDAR) systems play a crucial role\nin ADAS; they can reliably detect obstacles and help ensure traffic safety.\nExisting research on LiDAR sensing has demonstrated that adapting the LiDAR's\nresolution and range based on environmental characteristics can improve machine\nperception. However, current adaptive LiDAR approaches for ADAS have not\nexplored the possibility of combining the perception abilities of the vehicle\nand the human driver, which can potentially further enhance the detection\nperformance. In this paper, we propose a novel system that adapts LiDAR\ncharacteristics to human driver's visual perception to enhance LiDAR sensing\noutside human's field of view. We develop a proof-of-concept prototype of the\nsystem in the virtual environment CARLA. Our system integrates real-time data\non the driver's gaze to identify regions in the environment that the driver is\nmonitoring. This allows the system to optimize LiDAR resources by dynamically\nincreasing the LiDAR's range and resolution in peripheral areas that the driver\nmay not be attending to. Our simulations show that this gaze-aware LiDAR\nenhances detection performance compared to a baseline standalone LiDAR,\nparticularly in challenging environmental conditions like fog. Our hybrid\nhuman-machine sensing approach potentially offers improved safety and\nsituational awareness in real-time driving scenarios for ADAS applications.",
        "Airborne Gamma-Ray Spectrometry (AGRS) is a critical tool for radiological\nemergency response, enabling the rapid identification and quantification of\nhazardous terrestrial radionuclides over large areas. However, existing\ncalibration methods are limited to a few gamma-ray sources, excluding most\nradionuclides released in severe nuclear accidents and nuclear weapon\ndetonations, compromising effective response and risk assessment. Here, we\npresent a high-fidelity Monte Carlo model that overcomes these limitations,\noffering full-spectrum calibration for any gamma-ray source. Unlike previous\napproaches, our model integrates a detailed mass model of the aircraft and a\ncalibrated non-proportional scintillation model, enabling accurate\nevent-by-event predictions of the spectrometer's response to arbitrarily\ncomplex gamma-ray fields. Validation in near-, mid-, and far-field scenarios\ndemonstrates that the model not only addresses major deficiencies of previous\napproaches but also achieves the accuracy required to supersede empirical\ncalibration methods. This advancement enables high-fidelity spectral signature\ngeneration for any gamma-ray source, reduces calibration time and costs,\nminimizes reliance on high-intensity sources, and eliminates related\nradioactive waste. The approach presented here is a critical step toward\nintegrating advanced full-spectrum data reduction methods for AGRS, unlocking\nnew capabilities beyond emergency response, such as atmospheric cosmic-ray flux\nquantification for geophysics and trace-level airborne radionuclide\nidentification for nuclear security.",
        "Reasoning capabilities have significantly improved the performance of\nvision-language models (VLMs) in domains such as mathematical problem-solving,\ncoding, and visual question-answering. However, their impact on real-world\napplications remains unclear. This paper presents the first empirical study on\nthe effectiveness of reasoning-enabled VLMs in mobile GUI agents, a domain that\nrequires interpreting complex screen layouts, understanding user instructions,\nand executing multi-turn interactions. We evaluate two pairs of commercial\nmodels--Gemini 2.0 Flash and Claude 3.7 Sonnet--comparing their base and\nreasoning-enhanced versions across two static benchmarks (ScreenSpot and\nAndroidControl) and one interactive environment (AndroidWorld). We surprisingly\nfind the Claude 3.7 Sonnet reasoning model achieves state-of-the-art\nperformance on AndroidWorld. However, reasoning VLMs generally offer marginal\nimprovements over non-reasoning models on static benchmarks and even degrade\nperformance in some agent setups. Notably, reasoning and non-reasoning VLMs\nfail on different sets of tasks, suggesting that reasoning does have an impact,\nbut its benefits and drawbacks counterbalance each other. We attribute these\ninconsistencies to the limitations of benchmarks and VLMs. Based on the\nfindings, we provide insights for further enhancing mobile GUI agents in terms\nof benchmarks, VLMs, and their adaptability in dynamically invoking reasoning\nVLMs. The experimental data are publicly available at\nhttps:\/\/github.com\/LlamaTouch\/VLM-Reasoning-Traces.",
        "This study introduces a framework that employs Gaussian Processes (GPs) to\ndevelop high-fidelity equation of state (EOS) tables, essential for modeling\nmaterial properties across varying temperatures and pressures. GPs offer a\nrobust predictive modeling approach and are especially adept at handling\nuncertainties systematically. By integrating Error-in-Variables (EIV) into the\nGP model, we adeptly navigate uncertainties in both input parameters (like\ntemperature and density) and output variables (including pressure and other\nthermodynamic properties). Our methodology is demonstrated using\nfirst-principles density functional theory (DFT) data for gold, observing its\nproperties over maximum density compression (up to 100 g\/cc) and extreme\ntemperatures within the warm dense matter region (reaching 300 eV).\nFurthermore, we assess the resilience of our uncertainty propagation within the\nresultant EOS tables under various conditions, including data scarcity and the\nintrinsic noise of experiments and simulations."
      ]
    }
  },
  {
    "id":2411.14975,
    "research_type":"applied",
    "start_id":"b17",
    "start_title":"LoRA: Low-Rank Adaptation of Large Language Models",
    "start_abstract":"An important paradigm of natural language processing consists large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances fine-tuned each with is prohibitively expensive. We propose Low-Rank Adaptation, LoRA, freezes the pre-trained weights injects trainable rank decomposition matrices into layer Transformer architecture, greatly reducing number parameters for downstream tasks. Compared Adam, LoRA can reduce by 10,000 times GPU memory requirement 3 times. performs on-par better than fine-tuning in quality RoBERTa, DeBERTa, GPT-2, GPT-3, despite having fewer a higher training throughput, and, unlike adapters, no additional inference latency. also provide empirical investigation rank-deficiency adaptation, sheds light efficacy LoRA. release package that facilitates integration PyTorch models our implementations checkpoints GPT-2 at https:\/\/github.com\/microsoft\/LoRA.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b15"
      ],
      "title":[
        "HiCervix: An Extensive Hierarchical Dataset and Benchmark for Cervical Cytology Classification"
      ],
      "abstract":[
        "Cervical cytology is a critical screening strategy for early detection of pre-cancerous and cancerous cervical lesions. The challenge lies in accurately classifying various cell types. Existing automated methods are primarily trained on databases covering narrow range coarse-grained types, which fail to provide comprehensive detailed performance analysis that represents real-world cytopathology conditions. To overcome these limitations, we introduce HiCervix, the most extensive, multi-center dataset currently available public. HiCervix includes 40,229 cells from 4,496 whole slide images, categorized into 29 annotated classes. These classes organized within three-level hierarchical tree capture fine-grained subtype information. exploit semantic correlation inherent this tree, propose HierSwin, vision transformer-based classification network. HierSwin serves as benchmark feature learning both coarse-level fine-level cancer tasks. In our experiments, demonstrated remarkable performance, achieving 92.08% accuracy 82.93% averaged across all three levels. When compared board-certified cytopathologists, achieved high (0.8293 versus 0.7359 accuracy), highlighting its potential clinical applications. This newly released dataset, along with method, poised make substantial impact advancement deep algorithms rapid greatly improve prevention patient outcomes settings."
      ],
      "categories":[
        "q-bio.CB"
      ]
    },
    "list":{
      "title":[
        "Nanostructured thin films of indium oxide nanocrystals confined in\n  alumina matrixes",
        "Steady-state coherence in multipartite quantum systems: its connection\n  with thermodynamic quantities and impact on quantum thermal machines",
        "Solving Superconformal Ward Identities in Mellin Space",
        "Footprint in fitting $B\\to D$ vector form factor and determination for\n  $D$-meson leading-twist LCDA",
        "SU(4) gate design via unitary process tomography: its application to\n  cross-resonance based superconducting quantum devices",
        "Constrained mean-field control with singular control: Existence,\n  stochastic maximum principle and constrained FBSDE",
        "Jet rates in Higgs boson decay at third order in QCD",
        "On (in)consistency of M-estimators under contamination",
        "Score Matching Riemannian Diffusion Means",
        "Lower bound on the radii of circular orbits in the extremal Kerr\n  black-hole spacetime",
        "Cartan Quantum Metrology",
        "GPU Accelerated Image Quality Assessment-Based Software for Transient\n  Detection",
        "Multiaccuracy and Multicalibration via Proxy Groups",
        "Federated Variational Inference for Bayesian Mixture Models",
        "On the surjectivity of $\\mathfrak{p}$-adic Galois representations\n  attached to Drinfeld modules of rank $2$",
        "COSMOS-Web: The emergence of the Hubble Sequence",
        "Metastability and Ostwald Step Rule in the Crystallisation of Diamond\n  and Graphite from Molten Carbon",
        "Radii of light nuclei from the Jacobi No-Core Shell Model",
        "It's Not All Black and White: Degree of Truthfulness for Risk-Avoiding\n  Agents",
        "Beyond Fishing: The Value of Maritime Cultural Heritage in Germany",
        "Rolling Forward: Enhancing LightGCN with Causal Graph Convolution for\n  Credit Bond Recommendation",
        "Electromagnetic System Conceptual Design for a Negative Triangularity\n  Tokamak",
        "Long-distance genuine multipartite Entanglement between Magnetic Defects\n  in Spin Chains",
        "Radiation sputtering of hydrocarbon ices at Europa-relevant temperatures",
        "Central Velocity Dispersion being the Primary Driver of Abundance\n  Patterns in Quenched Galaxies",
        "Adaptive Observation Cost Control for Variational Quantum Eigensolvers",
        "Online Conformal Probabilistic Numerics via Adaptive Edge-Cloud\n  Offloading",
        "Study of $1^{--}$ P wave charmoniumlike and bottomoniumlike tetraquark\n  spectroscopy",
        "Holographic QCD phase diagram for a rotating plasma in the Hawking-Page\n  approach"
      ],
      "abstract":[
        "Nanocrystals of indium oxide (In$_2$O$_3$) with sizes below 10 nm were\nprepared in alumina matrixes by using a co-pulverization method. The used\nsubstrates such as borosilicate glasses or (100) silicon as well as the\nsubstrate temperatures during the deposition process were modified and their\neffects characterized on the structural and physical properties of\nalumina-In$_2$O$_3$ films. Complementary investigation methods including X-ray\ndiffraction, optical transmittance in the range 250-1100 nm and transmission\nelectron microscopy were used to analyze the nanostructured films. The\ncrystalline order, morphology and optical responses were monitored as function\nof the deposition parameters and the post-synthesis annealing. The optimal\nconditions were found and allow realizing suitable nanostructured films with a\nmajor crystalline order of cubic phase for the In$_2$O$_3$ nanocrystals. The\noptical properties of the films were analyzed and the key parameters such as\ndirect and indirect band gaps were evaluated as function of the synthesis\nconditions and the crystalline quality of the films.",
        "Understanding how coherence of quantum systems affects thermodynamic\nquantities, such as work and heat, is essential for harnessing quantumness\neffectively in thermal quantum technologies. Here, we study the unique\ncontributions of quantum coherence among different subsystems of a multipartite\nsystem, specifically in non-equilibrium steady states, to work and heat\ncurrents. Our system comprises two coupled ensembles, each consisting of $N$\nparticles, interacting with two baths of different temperatures, respectively.\nThe particles in an ensemble interact with their bath either simultaneously or\nsequentially, leading to non-local dissipation and enabling the decomposition\nof work and heat currents into local and non-local components.We find that the\nnon-local heat current, as well as both the local and non-local work\ncurrents,are linked to the system quantum coherence. We provide explicit\nexpressions of coherence-related quantities that determine the work currents\nunder various intrasystem interactions.Our scheme is versatile, capable of\nfunctioning as a refrigerator, an engine, and an accelerator, with its\nperformance being highly sensitive to the configuration settings. These\nfindings establish a connection between thermodynamic quantities and quantum\ncoherence, supplying valuable insights for the design of quantum thermal\nmachines.",
        "We study four-point correlators in superconformal theories in various\ndimensions. We develop an efficient method to solve the superconformal Ward\nidentities in Mellin space. For 4d $\\mathcal{N}=4$ SYM and the 6d\n$\\mathcal{N}=(2,0)$ theory, our method reproduces the known solutions. As novel\napplications of this method, we also derive solutions in 3d $\\mathcal{N} = 8$\nABJM, and in 4d $\\mathcal{N} = 4$ SYM with line defects.",
        "In this paper, we fit the $B\\to D$ vector transition form factor (TFF) by\nusing the data measured by BABAR and Belle Collaborations within Monte Carlo\n(MC) method. Meanwhile, the $B\\to D$ TFF is also calculated by using the QCD\nlight-cone sum rules approach (LCSRs) within right-handed chiral current\ncorrelation function. In which, the $D$-meson leading-twist light-cone\ndistribution amplitude (LCDA) serves as crucial input parameter is\nreconstructed with light-cone harmonic oscillator model where its longitudinal\nbehavior primarily determined by the model-free parameter $B_{2;D}$. After\nmatching the TFF with two scenarios from MC and LCSRs, we have $B_{2;D}=0.17$.\nThen, we present the curve of $D$-meson leading-twist LCDA in comparison with\nother theoretical approaches. Subsequently, the $B\\to D$ TFF $f_{+}^{BD}(q^2)$\nat the large recoil region is $f_{+}^{BD}(0)=0.625^{+0.087}_{-0.113}$, which is\ncompared in detail with theoretical estimates and experimental measurements.\nFurthermore, we calculate the decay width and branching ratio of the\nCabibbo-favored semileptonic decays $B\\to D\\ell \\bar{\\nu}_{\\ell}$, which lead\nto the results $\\mathcal{B}(B^0\\to D^-\\ell ^+\\nu _{\\ell})\n=(1.96_{-0.55}^{+0.51})\\times 10^{-2}$ and $\\mathcal{B}(B^+\\to \\bar{D}^0\\ell\n^+\\nu _{\\ell}) =(2.12_{-0.59}^{+0.55})\\times 10^{-2}$. Finally, we predict the\nCKM matrix element with two scenarios $|V_{cb}|_{\\rm\nSR}=42.97_{-2.57}^{+2.42}\\times 10^{-3}$ and $|V_{cb} |_{\\rm\nMC}=42.82_{-1.29}^{+1.07}\\times 10^{-3}$ from $B^0\\to D^-\\ell^+\\nu_{\\ell}$,\n$|V_{cb}|_{\\rm SR}=41.93_{-1.05}^{+1.03}\\times 10^{-3}$ and $|V_{cb} |_{\\rm\nMC}=41.82_{-0.25}^{+0.23}\\times 10^{-3}$ from $B^+\\to\n\\bar{D}^0\\ell^+\\nu_{\\ell}$ which are in good agreement with theoretical and\nexperimental predictions.",
        "We present a novel approach for implementing pulse-efficient SU(4) gates on\ncross resonance (CR)-based superconducting quantum devices. Our method\nintroduces a parameterized unitary derived from the CR-Hamiltonian propagator,\nwhich accounts for static-$ZZ$ interactions. Leveraging the Weyl chamber's\ngeometric structure, we successfully realize a continuous 2-qubit basis gate,\n$R_{ZZ}(\\theta)$, as an echo-free pulse schedule on the IBM Quantum device\nibm_kawasaki. We evaluate the average fidelity and gate time of various SU(4)\ngates generated using the $R_{ZZ}(\\theta)$ to confirm the advantages of our\nimplementation.",
        "This paper studies some mean-field control (MFC) problems with singular\ncontrol under general dynamic state-control-law constraints. We first propose a\ncustomized relaxed control formulation to cope with the dynamic mixed\nconstraints and establish the existence of an optimal control using some\ncompactification arguments in the proper canonical spaces to accommodate the\nsingular control. To characterize the optimal pair of regular and singular\ncontrols, we treat the controlled McKean-Vlasov process as an\ninfinite-dimensional equality constraint and recast the MFC problem as an\noptimization problem on canonical spaces with constraints on Banach space,\nallowing us to derive the stochastic maximum principle (SMP) and a constrained\nBSDE using a novel Lagrange multipliers method. In addition, we further\ninvestigate the uniqueness and the stability result of the solution to the\nconstrained FBSDE associated to the constrained MFC problem with singular\ncontrol.",
        "We compute the production rates for two, three, four and five jets in the\nhadronic decay of a Higgs boson in its two dominant decay modes to bottom\nquarks and gluons to third order in the QCD coupling constant. The five-, four-\nand three-jet rates are obtained from a next-to-next-to-leading order (NNLO)\ncalculation of Higgs decay to three jets, while the two-jet rate is inferred at\nnext-to-next-to-next-to-leading order (N$^3$LO) from the inclusive decay rate.\nOur results show distinct differences in the dependence of the jet rates on the\njet resolution parameter between the two decay modes, supporting the aim of\ndiscriminating different Higgs boson decay channels via classic QCD\nobservables.",
        "We consider robust location-scale estimators under contamination. We show\nthat commonly used robust estimators such as the median and the Huber estimator\nare inconsistent under asymmetric contamination, while the Tukey estimator is\nconsistent. In order to make nuisance parameter free inference based on the\nTukey estimator a consistent scale estimator is required. However, standard\nrobust scale estimators such as the interquartile range and the median absolute\ndeviation are inconsistent under contamination.",
        "Estimating means on Riemannian manifolds is generally computationally\nexpensive because the Riemannian distance function is not known in closed-form\nfor most manifolds. To overcome this, we show that Riemannian diffusion means\ncan be efficiently estimated using score matching with the gradient of Brownian\nmotion transition densities using the same principle as in Riemannian diffusion\nmodels. Empirically, we show that this is more efficient than Monte Carlo\nsimulation while retaining accuracy and is also applicable to learned\nmanifolds. Our method, furthermore, extends to computing the Fr\\'echet mean and\nthe logarithmic map for general Riemannian manifolds. We illustrate the\napplicability of the estimation of diffusion mean by efficiently extending\nEuclidean algorithms to general Riemannian manifolds with a Riemannian\n$k$-means algorithm and maximum likelihood Riemannian regression.",
        "It is often stated in the physics literature that maximally-spinning Kerr\nblack-hole spacetimes are characterized by near-horizon co-rotating circular\ngeodesics of radius $r_{\\text{circular}}$ with the property\n$r_{\\text{circular}}\\to r^+_{\\text{H}}$, where $r_{\\text{H}}$ is the horizon\nradius of the extremal black hole. Based on the famous Thorne hoop conjecture,\nin the present compact paper we provide evidence for the existence of a\nnon-trivial lower bound\n${{r_{\\text{circular}}-r_{\\text{H}}}\\over{r_{\\text{H}}}}\\gtrsim (\\mu\/M)^{1\/2}$\non the radii of circular orbits in the extremal Kerr black-hole spacetime,\nwhere $\\mu\/M$ is the dimensionless mass ratio which characterizes the composed\nblack-hole-orbiting-particle system.",
        "We address the characterization of two-qubit gates, focusing on bounds to\nprecision in the joint estimation of the three parameters that define their\nCartan decomposition. We derive the optimal probe states that jointly maximize\nprecision, minimize sloppiness, and eliminate quantum incompatibility.\nAdditionally, we analyze the properties of the set of optimal probes and\nevaluate their robustness against noise.",
        "Fast imaging localises celestial transients using source finders in the image\ndomain. The need for high computational throughput in this process is driven by\nnext-generation telescopes such as Square Kilometre Array (SKA), which, upon\ncompletion, will be the world's largest aperture synthesis radio telescope. It\nwill collect data at unprecedented velocity and volume. Due to the vast amounts\nof data the SKA will produce, current source finders based on source extraction\nmay be inefficient in a wide-field search. In this paper, we focus on the\nsoftware development of GPU-accelerated transient finders based on Image\nQuality Assessment (IQA) methods -- Low-Information Similarity Index (LISI) and\naugmented LISI (augLISI). We accelerate the algorithms using GPUs, achieving\nkernel time of approximately 0.1 milliseconds for transient finding in\n2048X2048 images.",
        "As the use of predictive machine learning algorithms increases in high-stakes\ndecision-making, it is imperative that these algorithms are fair across\nsensitive groups. Unfortunately, measuring and enforcing fairness in real-world\napplications can be challenging due to missing or incomplete sensitive group\ndata. Proxy-sensitive attributes have been proposed as a practical and\neffective solution in these settings, but only for parity-based fairness\nnotions. Knowing how to evaluate and control for fairness with missing\nsensitive group data for newer and more flexible frameworks, such as\nmultiaccuracy and multicalibration, remains unexplored. In this work, we\naddress this gap by demonstrating that in the absence of sensitive group data,\nproxy-sensitive attributes can provably be used to derive actionable upper\nbounds on the true multiaccuracy and multicalibration, providing insights into\na model's potential worst-case fairness violations. Additionally, we show that\nadjusting models to satisfy multiaccuracy and multicalibration across\nproxy-sensitive attributes can significantly mitigate these violations for the\ntrue, but unknown, sensitive groups. Through several experiments on real-world\ndatasets, we illustrate that approximate multiaccuracy and multicalibration can\nbe achieved even when sensitive group information is incomplete or unavailable.",
        "We present a federated learning approach for Bayesian model-based clustering\nof large-scale binary and categorical datasets. We introduce a principled\n'divide and conquer' inference procedure using variational inference with local\nmerge and delete moves within batches of the data in parallel, followed by\n'global' merge moves across batches to find global clustering structures. We\nshow that these merge moves require only summaries of the data in each batch,\nenabling federated learning across local nodes without requiring the full\ndataset to be shared. Empirical results on simulated and benchmark datasets\ndemonstrate that our method performs well in comparison to existing clustering\nalgorithms. We validate the practical utility of the method by applying it to\nlarge scale electronic health record (EHR) data.",
        "Let $\\mathbb{F}_{q}$ be the finite field with $q\\geq 5$ elements and\n$A:=\\mathbb{F}_{q}[T]$. For a class of $\\mathfrak{p} \\in \\mathrm{Spec}(A)\n\\setminus \\{(0)\\}$, but fixed, we produce infinitely many Drinfeld $A$-modules\nof rank $2$, for which the associated $\\mathfrak{p}$-adic Galois representation\nis surjective. This result is a variant of the work of~[Ray24] for\n$\\mathfrak{p}=(T)$. We also show that for a class of $\\mathfrak{l}=(l) \\in\n\\mathrm{Spec}(A)$, where $l$ is a monic polynomial, the $\\mathfrak{p}$-adic\nGalois representation, attached to the Drinfeld $A$-module\n$\\varphi_{T}=T+g_{1}\\tau-l^{q-1}\\tau^2$ with $g_{1} \\in A \\setminus\n\\mathfrak{l}$, is surjective for all $\\mathfrak{p} \\in\n\\mathrm{Spec}(A)\\setminus\\{(0)\\}$. This result generalizes the work of [Zyw11]\nfrom $\\mathfrak{l}=(T), g_1=1$.",
        "Leveraging the wide area coverage of the COSMOS-Web survey, we quantify the\nabundance of different morphological types from $z\\sim 7$ with unprecedented\nstatistics and establish robust constraints on the epoch of emergence of the\nHubble sequence. We measure the global (spheroids, disk-dominated,\nbulge-dominated, peculiar) and resolved (stellar bars) morphologies for about\n400,000 galaxies down to F150W=27 using deep learning, representing a\ntwo-orders-of-magnitude increase over previous studies. We then provide\nreference Stellar Mass Functions (SMFs) of different morphologies between\n$z\\sim 0.2$ and $z\\sim 7$ and best-fit parameters to inform models of galaxy\nformation. All catalogs and data are made publicly available. (a)At redshift z\n> 4.5, the massive galaxy population ($\\log M_*\/M_\\odot>10$) is dominated by\ndisturbed morphologies (~70%) -- even in the optical rest frame -- and very\ncompact objects (~30%) with effective radii smaller than ~500pc. This confirms\nthat a significant fraction of the star formation at cosmic dawn occurs in very\ndense regions, although the stellar mass for these systems could be\noverestimated.(b)Galaxies with Hubble-type morphologies -- including bulge and\ndisk-dominated galaxies -- arose rapidly around $z\\sim 4$ and dominate the\nmorphological diversity of massive galaxies as early as $z\\sim 3$. (c)Using\nstellar bars as a proxy, we speculate that stellar disks in massive galaxies\nmight have been common (>50%) among the star-forming population since cosmic\nnoon ($z\\sim2$-2.5) and formed as early as $z\\sim 7$ (d)Massive quenched\ngalaxies are predominantly bulge-dominated from z~4 onward, suggesting that\nmorphological transformations briefly precede or are simultaneous to quenching\nmechanisms at the high-mass end. (e) Low-mass ($\\log M_*\/M_\\odot<10$) quenched\ngalaxies are typically disk-dominated, pointing to different quenching routes\nin the two ends of the stellar mass spectrum from cosmic dawn.",
        "The crystallisation of carbon from the melt under extreme conditions is\nhighly relevant to earth and planetary science, materials manufacturing, and\nnuclear fusion research. The thermodynamic conditions near the\ngraphite-diamond-liquid (GDL) triple point are especially of interest for\ngeological and technological applications, but high-pressure flash heating\nexperiments aiming to resolve this region of the phase diagram of carbon\nexhibit large discrepancies. Experimental challenges are often related to the\npersistence of metastable crystalline or glassy phases, superheated crystals,\nor supercooled liquids. A deeper understanding of the crystallisation kinetics\nof diamond and graphite is crucial for effectively interpreting the outcomes of\nthese experiments. Here, we reveal the microscopic mechanisms of diamond and\ngraphite nucleation from liquid carbon through molecular simulations with\nfirst-principles machine learning potentials. Our simulations accurately\nreproduce the experimental phase diagram of carbon in the region around the GDL\ntriple point and show that liquid carbon crystallises spontaneously upon\ncooling at constant pressure. Surprisingly, metastable graphite crystallises in\nthe domain of diamond thermodynamic stability at pressures above the triple\npoint. Furthermore, whereas diamond crystallises through a classical nucleation\npathway, graphite follows a two-step process in which low-density fluctuations\nforego ordering. Calculations of the nucleation rates of the two competing\nphases confirm this result and reveal a manifestation of Ostwald's step rule\nwhere the strong metastability of graphite hinders the transformation to the\nstable diamond phase. Our results provide a new key to interpreting melting and\nrecrystallisation experiments and shed light on nucleation kinetics in\npolymorphic materials with deep metastable states.",
        "Accurately determining the size of the atomic nucleus with realistic nuclear\nforces is a long outstanding issue of nuclear physics. The no-core shell model\n(NCSM), one of the powerful ab initio methods for nuclear structure, can\nachieve accurate energies of light nuclei. The extraction of converged radii is\nmore difficult. In this work, we present a novel method to effectively extract\nthe radius of light nuclei by restoring the long-range behavior of densities\nfrom NCSM calculations. The correct large distance asymptotic of two-body\nrelative densities are deduced based on the NCSM densities in limited basis\nsize. The resulting radii using the corrected densities show a nice\nconvergence. The root-mean-square matter and charge radii of $^{4,6,8}$He and\n$^{6,7,8}$Li can be accurately obtained based on Jacobi-NCSM calculations with\nthe high-precision chiral two-nucleon and three-nucleon forces combined with\nthis new method. Our method can be straightforwardly extended to other ab\ninitio calculations, potentially providing a better description of nuclear\nsizes with realistic nuclear forces.",
        "The classic notion of truthfulness requires that no agent has a profitable\nmanipulation -- an untruthful report that, for some combination of reports of\nthe other agents, increases her utility. This strong notion implicitly assumes\nthat the manipulating agent either knows what all other agents are going to\nreport, or is willing to take the risk and act as-if she knows their reports.\n  Without knowledge of the others' reports, most manipulations are risky --\nthey might decrease the manipulator's utility for some other combinations of\nreports by the other agents. Accordingly, a recent paper (Bu, Song and Tao,\n``On the existence of truthful fair cake cutting mechanisms'', Artificial\nIntelligence 319 (2023), 103904) suggests a relaxed notion, which we refer to\nas risk-avoiding truthfulness (RAT), which requires only that no agent can gain\nfrom a safe manipulation -- one that is sometimes beneficial and never harmful.\n  Truthfulness and RAT are two extremes: the former considers manipulators with\ncomplete knowledge of others, whereas the latter considers manipulators with no\nknowledge at all. In reality, agents often know about some -- but not all -- of\nthe other agents. This paper introduces the RAT-degree of a mechanism, defined\nas the smallest number of agents whose reports, if known, may allow another\nagent to safely manipulate, or $n$ if there is no such number. This notion\ninterpolates between classic truthfulness (degree $n$) and RAT (degree at least\n$1$): a mechanism with a higher RAT-degree is harder to manipulate safely.\n  To illustrate the generality and applicability of this concept, we analyze\nthe RAT-degree of prominent mechanisms across various social choice settings,\nincluding auctions, indivisible goods allocations, cake-cutting, voting, and\nstable matchings.",
        "The importance of maritime heritage in providing benefits such as a sense of\nplace and identity has been widely discussed. However, there remains a lack of\ncomprehensive quantitative analysis, particularly regarding monetary valuation\nand its impact on people's preferences. In this study, I present the results of\na choice experiment that assesses the value of the maritime cultural heritage\nassociated with shrimp fishing through seafood consumption preferences in\nGermany. Additionally, I investigate people's attitudes toward cultural\nheritage and examine how these attitudes affect their stated preferences. I\nfind that these attitudes are significantly stronger in towns where local\nfishermen led a prominent awareness campaign on fishing culture during the\nstudy period. Moreover, I observe a positive willingness to pay for a cultural\nheritage attribute in shrimp dishes, which varies depending on individuals'\nattitudes toward cultural heritage.",
        "Graph Neural Networks have significantly advanced research in recommender\nsystems over the past few years. These methods typically capture global\ninterests using aggregated past interactions and rely on static embeddings of\nusers and items over extended periods of time. While effective in some domains,\nthese methods fall short in many real-world scenarios, especially in finance,\nwhere user interests and item popularity evolve rapidly over time. To address\nthese challenges, we introduce a novel extension to Light Graph Convolutional\nNetwork (LightGCN) designed to learn temporal node embeddings that capture\ndynamic interests. Our approach employs causal convolution to maintain a\nforward-looking model architecture. By preserving the chronological order of\nuser-item interactions and introducing a dynamic update mechanism for\nembeddings through a sliding window, the proposed model generates well-timed\nand contextually relevant recommendations. Extensive experiments on a\nreal-world dataset from BNP Paribas demonstrate that our approach significantly\nenhances the performance of LightGCN while maintaining the simplicity and\nefficiency of its architecture. Our findings provide new insights into\ndesigning graph-based recommender systems in time-sensitive applications,\nparticularly for financial product recommendations.",
        "Negative triangularity (NT) tokamak configurations have several key benefits\nincluding sufficient core confinement, improved power handling, and reduced\nedge pressure gradients that allow for edge-localized mode (ELM) free\noperation. We present the design of a compact NT device for testing\nsophisticated simulation and control software, with the aim of demonstrating NT\ncontrollability and informing power plant operation. The TokaMaker code is used\nto develop the basic electromagnetic system of the $R_0$ = 1 m, $a$ = 0.27 m,\n$B_t$ = 3 T, $I_p$ = 0.75 MA tokamak. The proposed design utilizes eight\npoloidal field coils with maximum currents of 1 MA to achieve a wide range of\nplasma geometries with $-0.7 < \\delta < -0.3$ and $1.5 < \\kappa < 1.9$.\nScenarios with strong negative triangularity and high elongation are\nparticularly susceptible to vertical instability, necessitating the inclusion\nof high-field side and\/or low-field side passive stabilizing plates which\ntogether reduce vertical instability growth rates by $\\approx$75%. Upper limits\nfor the forces on poloidal and toroidal field coils are predicted and\nmechanical loads on passive structures during current quench events are\nassessed. The 3 T on-axis toroidal field is achieved with 16 demountable copper\ntoroidal field coils, allowing for easy maintenance of the vacuum vessel and\npoloidal field coils. This pre-conceptual design study demonstrates that the\nkey capabilities required of a dedicated NT tokamak experiment can be realized\nwith existing copper magnet technologies.",
        "We investigate the emergence and properties of long-distance genuine\nmultipartite entanglement, induced via three localized magnetic defects, in a\none-dimensional transverse-field XX spin-$1\/2$ chain. Using both analytical and\nnumerical techniques, we determine the conditions for the existence of bound\nstates localized at the defects. We find that the reduced density matrix (RDM)\nof the defects exhibits long-distance genuine multipartite entanglement (GME)\nacross the whole range of the Hamiltonian parameter space, including regions\nwhere the two-qubit concurrence is zero. We quantify the entanglement by using\nnumerical lower bounds for the GME concurrence, as well as by analytically\nderiving the GME concurrence in regions where the RDM is of rank two. Our work\nprovides insights into generating multipartite entanglement in many-body\nquantum systems via local control techniques.",
        "The surfaces of some icy moons, such as Jupiter's moon Europa, are heavily\nbombarded by energetic particles that can alter the surface materials and\naffect the composition of its exosphere. Detection of CO2 on Europa's surface\nindicate that Europa's interior may be transporting freshly exposed\ncarbon-containing material to the surface. It is unknown whether this CO2 is a\nproduct of radiation of carbon-containing precursors or whether it is present\nin the initial deposits. Regardless, further radiolysis by high-energy\nelectrons or ions can sputter CO2 (and organic fragments if present) into\nEuropa's exosphere. In this study, we investigate the radiation sputtering of\nCO2 and organic fragments from hydrocarbon water ice mixtures at different\nEuropa-relevant surface temperatures to identify how its sputtering products\nevolve over time. This study shows that the sputtering of hydrocarbon water ice\nleads to the production of mostly CO2, CO, and fragmented hydrocarbons. The\nonset of sputtered hydrocarbons is immediate, and quickly reaches a steady\nstate, whereas CO2 and CO are formed more gradually. It is found that higher\ntemperatures cause more sputtering, and that there are some notable differences\nin the distribution of species that are sputtered at different temperatures,\nindicating local heterogeneity of sputtering yields depending on the surface\ntemperature.",
        "The element abundances of galaxies provide crucial insights into their\nformation and evolution. Using high-resolution IFU data from the MaNGA survey,\nwe analyze the central spectra (0-0.5 $R_{\\rm e}$) of 1,185 quenched galaxies\n($z = 0.012-0.15$) to study their element abundances and stellar populations.\nWe employ the full-spectrum fitting code {\\tt alf} to derive stellar ages and\nelement abundances from synthetic spectra and empirical libraries. Our key\nfindings are: (1) Central velocity dispersion ($\\sigma_*$) is the most\neffective parameter correlating with (relative) element abundances, especially\n[Na\/Fe], [Mg\/Fe], [C\/Fe], and [N\/Fe], outperforming $M_\\ast$ and $M_\\ast\/R_{\\rm\ne}$. (2) When binned by $\\sigma_*$, the relative abundances of Na, Mg, C, and N\nremain stable across different formation times ($T_{\\rm form}$), suggesting\nthese elements are primarily influenced by the burstiness of star formation\n(traced by $\\sigma_*$) rather than prolonged evolutionary processes. (3) Fe and\nCa show little variation with $\\sigma_*$, indicating weaker sensitivity to\n$\\sigma_*$-driven processes. However, $T_{\\rm form}$ has a global influence on\nall elements, contributing to their overall chemical evolution, albeit\nsecondary to $\\sigma_*$ for most elements. These results support the primary\nrole of $\\sigma_*$ in shaping the abundance patterns, likely stemming from the\nconnection between central massive black holes and possibly dark matter halos,\nwhich influences the burstiness of star formation histories.",
        "The objective to be minimized in the variational quantum eigensolver (VQE)\nhas a restricted form, which allows a specialized sequential minimal\noptimization (SMO) that requires only a few observations in each iteration.\nHowever, the SMO iteration is still costly due to the observation noise -- one\nobservation at a point typically requires averaging over hundreds to thousands\nof repeated quantum measurement shots for achieving a reasonable noise level.\nIn this paper, we propose an adaptive cost control method, named subspace in\nconfident region (SubsCoRe), for SMO. SubsCoRe uses the Gaussian process (GP)\nsurrogate, and requires it to have low uncertainty over the subspace being\nupdated, so that optimization in each iteration is performed with guaranteed\naccuracy. The adaptive cost control is performed by first setting the required\naccuracy according to the progress of the optimization, and then choosing the\nminimum number of measurement shots and their distribution such that the\nrequired accuracy is satisfied. We demonstrate that SubsCoRe significantly\nimproves the efficiency of SMO, and outperforms the state-of-the-art methods.",
        "Consider an edge computing setting in which a user submits queries for the\nsolution of a linear system to an edge processor, which is subject to\ntime-varying computing availability. The edge processor applies a probabilistic\nlinear solver (PLS) so as to be able to respond to the user's query within the\nallotted time and computing budget. Feedback to the user is in the form of an\nuncertainty set. Due to model misspecification, the uncertainty set obtained\nvia a direct application of PLS does not come with coverage guarantees with\nrespect to the true solution of the linear system. This work introduces a new\nmethod to calibrate the uncertainty sets produced by PLS with the aim of\nguaranteeing long-term coverage requirements. The proposed method, referred to\nas online conformal prediction-PLS (OCP-PLS), assumes sporadic feedback from\ncloud to edge. This enables the online calibration of uncertainty thresholds\nvia online conformal prediction (OCP), an online optimization method previously\nstudied in the context of prediction models. The validity of OCP-PLS is\nverified via experiments that bring insights into trade-offs between coverage,\nprediction set size, and cloud usage.",
        "The masses of $1^{--}$ P-wave charmonium-like and bottomonium-like tetraquark\nstates are calculated in a constituent quark model (CQM) where the Cornell-like\npotential and Breit-Fermi interaction are employed. All model parameters were\nimported from previous work, and predetermined by studying the low-lying\nconventional S- and P-wave light, charmed, bottom, charmonium, and bottomonium\nmeson mass spectra. The decay widths of $1^{--}$ P-wave tetraquark states are\ncalculated for possible two-body strong decay channels within the rearrangement\nmechanism, including $\\omega \\chi_{cJ}$, $\\eta J\/\\psi$, and $\\rho J\/\\psi$ for\ncharmonium-like tetraquarks, and $\\omega \\chi_{bJ}$ for bottomonium-like\ntetraquarks. The tetraquark theoretical results are compared with the selected\nexotic states, also known as Y states, and tentative assignments are suggested.\nThis study suggests that $\\psi(4230)$, $\\psi(4360)$, $\\psi(4660)$, and\n$\\Upsilon$(10753) may be P-wave tetraquark states and that multiple states\nmight exist around 4.23 GeV and 4.36 GeV.",
        "We investigate the combined effect of rotation and finite chemical potential\nin the confinement\/deconfinement transition of strongly interacting matter. The\nholographic description consists of a five-dimensional geometry that contains a\nblack hole (BH) in the deconfined (plasma) phase. The geometry is equipped with\nsome cut-off that introduces an infrared energy scale. We consider two\npossibilities: the so-called hard wall and soft wall AdS\/QCD models. The\ntransition between the plasma and hadronic phases is represented\nholographically as a Hawking-Page transition between geometries with and\nwithout a black hole. The gravitational dual of the rotating plasma at finite\ndensity is given by a Reissner-Nordstr\\\"om (RN) charged anti-de Sitter (AdS) BH\nwith non-zero angular momentum. This analysis provides the critical temperature\nof deconfinement as a function of the quark chemical potential and the plasma\nrotational velocity. For the particular case of very low temperatures and high\ndensities, it is found that the critical value of the chemical potential for\nthe transition to occur at $ T \\to 0 $ does not depend on the plasma vorticity,\nsince the effects of rotation and quark density on the critical temperature are\nshown to be independent."
      ]
    }
  },
  {
    "id":2411.15331,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"Improvement of quantitative structure\u2013activity relationship (QSAR) tools for predicting Ames mutagenicity: outcomes of the Ames\/QSAR International Challenge Project",
    "start_abstract":"The International Conference on Harmonization (ICH) M7 guideline allows the use of in silico approaches for predicting Ames mutagenicity initial assessment impurities pharmaceuticals. This is first international that addresses quantitative structure\u2013activity relationship (QSAR) models lieu actual toxicological studies human health assessment. Therefore, QSAR now require higher predictive power identifying mutagenic chemicals. To increase models, larger experimental datasets from reliable sources are required. Division Genetics and Mutagenesis, National Institute Health Sciences (DGM\/NIHS) Japan recently established a unique proprietary database containing 12140 new chemicals have not been previously used developing models. DGM\/NIHS provided this to vendors validate improve their tools. Ames\/QSAR Challenge Project was initiated 2014 with 12 testing 17 tools against these compounds three phases. We present final results. All were considerably improved by participation project. Most achieved >50% sensitivity (positive prediction among all positives) (accuracy) as high 80%, almost equivalent inter-laboratory reproducibility tests. further tools, accumulation additional test data required well re-evaluation some previous Indeed, Ames-positive or Ames-negative may incorrectly classified because methodological weakness, resulting false-positive false-negative predictions These incorrect hamper source noise development It thus essential establish large benchmark consisting only well-validated results build more accurate",
    "start_categories":[
      "q-bio.GN"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Mutagenpred-gcnns: a graph convolutional neural network-based classification model for mutagenicity prediction with data-driven molecular fingerprints"
      ],
      "abstract":[
        "An important task in the early stage of drug discovery is the identification of mutagenic compounds. Mutagenicity prediction models that can interpret relationships between toxicological endpoints and compound structures are especially favorable. In this research, we used an advanced graph convolutional neural network (GCNN) architecture to identify the molecular representation and develop predictive models based on these representations. The predictive model based on features extracted by GCNNs can not only predict the mutagenicity of compounds but also identify the structure alerts in compounds. In fivefold cross-validation and external validation, the highest area under the curve was 0.8782 and 0.8382, respectively; the highest accuracy (Q) was 80.98% and 76.63%, respectively; the highest sensitivity was 83.27% and 78.92%, respectively; and the highest specificity was 78.83% and 76.32%, respectively. Additionally, our model also identified some toxicophores, such as aromatic nitro, three-membered heterocycles, quinones, and nitrogen and sulfur mustard. These results indicate that GCNNs could learn the features of mutagens effectively. In summary, we developed a mutagenicity classification model with high predictive performance and interpretability based on a data-driven molecular representation trained through GCNNs."
      ],
      "categories":[
        "cs.NE"
      ]
    },
    "list":{
      "title":[
        "Credit Risk Identification in Supply Chains Using Generative Adversarial\n  Networks",
        "Optimization Landscapes Learned: Proxy Networks Boost Convergence in\n  Physics-based Inverse Problems",
        "Estimation-Aware Trajectory Optimization with Set-Valued Measurement\n  Uncertainties",
        "How do recollimation-induced instabilities shape the propagation of\n  hydrodynamic relativistic jets?",
        "Structural Perturbation in Large Language Model Representations through\n  Recursive Symbolic Regeneration",
        "Understanding and Evaluating Hallucinations in 3D Visual Language Models",
        "Synthetic Clarification and Correction Dialogues about Data-Centric\n  Tasks -- A Teacher-Student Approach",
        "Comparative Analysis of Control Strategies for Position Regulation in DC\n  Servo Motors",
        "ATLAS Navigator: Active Task-driven LAnguage-embedded Gaussian Splatting",
        "Two-stage Deep Denoising with Self-guided Noise Attention for Multimodal\n  Medical Images",
        "On Choquard-Kirchhoff Type Critical Multiphase Problem",
        "Privacy-Preserving Hybrid Ensemble Model for Network Anomaly Detection:\n  Balancing Security and Data Protection",
        "Primordial Origin of Methane on Eris and Makemake Supported by D\/H\n  Ratios",
        "Quarkonia and Deconfined Quark-Gluon Matter in Heavy-Ion Collisions",
        "Quantum-Inspired Fidelity-based Divergence",
        "ST-FlowNet: An Efficient Spiking Neural Network for Event-Based Optical\n  Flow Estimation",
        "Towards Efficient Parametric State Estimation in Circulating Fuel\n  Reactors with Shallow Recurrent Decoder Networks",
        "Exploring and Evaluating Multimodal Knowledge Reasoning Consistency of\n  Multimodal Large Language Models",
        "exHarmony: Authorship and Citations for Benchmarking the Reviewer\n  Assignment Problem",
        "Rotational effects on the small-scale dynamo",
        "Curvature perturbations from vacuum transition during inflation",
        "Hierarchical RL-MPC for Demand Response Scheduling",
        "An exponential integrator multicontinuum homogenization method for\n  fractional diffusion problem with multiscale coefficients",
        "Foundation Models for Spatio-Temporal Data Science: A Tutorial and\n  Survey",
        "Matrix weighted inequalities for fractional type integrals associated to\n  operators with new classes of weights",
        "Vexed by VEX tools: Consistency evaluation of container vulnerability\n  scanners",
        "Drone Detection and Tracking with YOLO and a Rule-based Method",
        "Asymptotic approximations for convection onset with Ekman pumping at low\n  wavenumbers",
        "Optimizing 2D+1 Packing in Constrained Environments Using Deep\n  Reinforcement Learning"
      ],
      "abstract":[
        "Credit risk management within supply chains has emerged as a critical\nresearch area due to its significant implications for operational stability and\nfinancial sustainability. The intricate interdependencies among supply chain\nparticipants mean that credit risks can propagate across networks, with impacts\nvarying by industry. This study explores the application of Generative\nAdversarial Networks (GANs) to enhance credit risk identification in supply\nchains. GANs enable the generation of synthetic credit risk scenarios,\naddressing challenges related to data scarcity and imbalanced datasets. By\nleveraging GAN-generated data, the model improves predictive accuracy while\neffectively capturing dynamic and temporal dependencies in supply chain data.\nThe research focuses on three representative industries-manufacturing (steel),\ndistribution (pharmaceuticals), and services (e-commerce) to assess\nindustry-specific credit risk contagion. Experimental results demonstrate that\nthe GAN-based model outperforms traditional methods, including logistic\nregression, decision trees, and neural networks, achieving superior accuracy,\nrecall, and F1 scores. The findings underscore the potential of GANs in\nproactive risk management, offering robust tools for mitigating financial\ndisruptions in supply chains. Future research could expand the model by\nincorporating external market factors and supplier relationships to further\nenhance predictive capabilities. Keywords- Generative Adversarial Networks\n(GANs); Supply Chain Risk; Credit Risk Identification; Machine Learning; Data\nAugmentation",
        "Solving inverse problems in physics is central to understanding complex\nsystems and advancing technologies in various fields. Iterative optimization\nalgorithms, commonly used to solve these problems, often encounter local\nminima, chaos, or regions with zero gradients. This is due to their\noverreliance on local information and highly chaotic inverse loss landscapes\ngoverned by underlying partial differential equations (PDEs). In this work, we\nshow that deep neural networks successfully replicate such complex loss\nlandscapes through spatio-temporal trajectory inputs. They also offer the\npotential to control the underlying complexity of these chaotic loss landscapes\nduring training through various regularization methods. We show that optimizing\non network-smoothened loss landscapes leads to improved convergence in\npredicting optimum inverse parameters over conventional momentum-based\noptimizers such as BFGS on multiple challenging problems.",
        "In this paper, we present an optimization-based framework for generating\nestimation-aware trajectories in scenarios where measurement (output)\nuncertainties are state-dependent and set-valued. The framework leverages the\nconcept of regularity for set-valued output maps. Specifically, we demonstrate\nthat, for output-regular maps, one can utilize a set-valued observability\nmeasure that is concave with respect to finite-horizon state trajectories. By\nmaximizing this measure, optimized estimation-aware trajectories can be\ndesigned for a broad class of systems, including those with locally linearized\ndynamics. To illustrate the effectiveness of the proposed approach, we provide\na representative example in the context of trajectory planning for vision-based\nestimation. We present an estimation-aware trajectory for an uncooperative\ntarget-tracking problem that uses a machine learning (ML)-based estimation\nmodule on an ego-satellite.",
        "Recollimation is a phenomenon of particular importance in the dynamic\nevolution of jets and in the emission of high-energy radiation. Additionally,\nthe full comprehension of this phenomenon provides insights into fundamental\nproperties of jets in the vicinity of the Active Galactic Nucleus (AGN).\nThree-dimensional (magneto-)hydrodynamic simulations revealed that the jet\nconditions at recollimation favor the growth of strong instabilities,\nchallenging the traditional view-supported from two-dimensional simulations-of\nconfined jets undergoing a series of recollimation and reflection shocks. To\ninvestigate the stability of relativistic jets in AGNs at recollimation sites,\nwe perform a set of long duration three-dimensional relativistic hydrodynamic\nsimulations with the state-of-the-art PLUTO code, to focus on the development\nof hydrodynamical instabilities. We explore the non-linear growth of the\ninstabilities and their effects on the physical jet properties as a function of\nthe initial jet parameters: jet Lorentz factor, temperature, opening angle and\njet-environment density-contrast. The parameter space is designed to describe\nlow-power, weakly magnetized jets at small distances from the core (around the\nparsec scale). All collimating jets we simulated develop instabilities.\nRecollimation instabilities decelerate the jet, heat it, entrain external\nmaterial, and move the recollimation point to shorter distances from the core.\nThis is true for both conical and cylindrical jets. The instabilities, that are\nfirst triggered by the centrifugal instability, appear to be less disruptive in\nthe case of narrower, denser, more relativistic, and warmer jets. These results\nprovide valuable insights into the complex processes governing AGN jets and\ncould be used to model the properties of low-power, weakly magnetized jetted\nAGNs.",
        "Symbolic perturbations offer a novel approach for influencing neural\nrepresentations without requiring direct modification of model parameters. The\nrecursive regeneration of symbolic structures introduces structured variations\nin latent embeddings, leading to controlled shifts in attention dynamics and\nlexical diversity across sequential generations. A comparative analysis with\nconventional fine-tuning techniques reveals that structural modifications at\nthe symbolic level induce distinct variations in contextual sensitivity while\nmaintaining overall model fluency and coherence. Shifts in attention weight\ndistributions highlight the role of symbolic modifications in adjusting token\ndependencies, influencing response variability, and refining long-form text\ngeneration. Experimental findings suggest that symbolic perturbations can\nenhance adaptability in domain-specific applications, allowing modifications in\nmodel behavior without retraining. Evaluations of semantic drift indicate that\nrecursive regeneration alters long-range token dependencies, affecting topic\ncoherence across extended text sequences. Results from lexical variability\nassessments further support the conclusion that symbolic-level modifications\nintroduce interpretable variations in generated responses, potentially enabling\nmore controlled stylistic adjustments in automated text generation.",
        "Recently, 3D-LLMs, which combine point-cloud encoders with large models, have\nbeen proposed to tackle complex tasks in embodied intelligence and scene\nunderstanding. In addition to showing promising results on 3D tasks, we found\nthat they are significantly affected by hallucinations. For instance, they may\ngenerate objects that do not exist in the scene or produce incorrect\nrelationships between objects. To investigate this issue, this work presents\nthe first systematic study of hallucinations in 3D-LLMs. We begin by quickly\nevaluating hallucinations in several representative 3D-LLMs and reveal that\nthey are all significantly affected by hallucinations. We then define\nhallucinations in 3D scenes and, through a detailed analysis of datasets,\nuncover the underlying causes of these hallucinations. We find three main\ncauses: (1) Uneven frequency distribution of objects in the dataset. (2) Strong\ncorrelations between objects. (3) Limited diversity in object attributes.\nAdditionally, we propose new evaluation metrics for hallucinations, including\nRandom Point Cloud Pair and Opposite Question Evaluations, to assess whether\nthe model generates responses based on visual information and aligns it with\nthe text's meaning.",
        "Real dialogues with AI assistants for solving data-centric tasks often follow\ndynamic, unpredictable paths due to imperfect information provided by the user\nor in the data, which must be caught and handled. Developing datasets which\ncapture such user-AI interactions is difficult and time-consuming. In this\nwork, we develop a novel framework for synthetically generating controlled,\nmulti-turn conversations between a user and AI assistant for the task of\ntable-based question answering, which can be generated from an existing dataset\nwith fully specified table QA examples for any target domain. Each conversation\naims to solve a table-based reasoning question through collaborative effort,\nmodeling one of two real-world scenarios: (1) an AI-initiated clarification, or\n(2) a user-initiated correction. Critically, we employ a strong teacher LLM to\nverify the correctness of our synthetic conversations, ensuring high quality.\nWe demonstrate synthetic datasets generated from TAT-QA and WikiTableQuestions\nas benchmarks of frontier LLMs. We find that even larger models struggle to\neffectively issuing clarification questions and accurately integrate user\nfeedback for corrections.",
        "A servomotor is a closed-loop system designed for precise movement control,\nutilizing position feedback to achieve accurate final positions. Due to the\nability to deliver higher power output and operate at enhanced speeds, DC servo\nmotors are considered ideal for applications requiring precision and\nperformance. This research aims to design, simulate, and compare various\ncontrol strategies for precise position control in DC servo motors (DSM). The\ncontrollers evaluated in this study include proportional (P),\nproportional-integral (PI), proportional-integral-derivative (PID),\nstate-feedback controllers (SFC), and state-feedback controllers augmented with\nintegral action (SFCIA). The performance of these controllers was evaluated\nusing MATLAB simulations, characterized by overshoot, settling time,\nsteady-state error, rise time, and peak time. The results indicate that the\nstate-feedback controller with integral action (SFCIA) surpasses other control\nstrategies by achieving zero steady-state error, minimal overshoot, the\nshortest settling time, and optimized rise and peak times. These findings\nhighlight the effectiveness of SFCIA for tasks requiring high levels of\nstability, precision, and dynamic performance.",
        "We address the challenge of task-oriented navigation in unstructured and\nunknown environments, where robots must incrementally build and reason on rich,\nmetric-semantic maps in real time. Since tasks may require clarification or\nre-specification, it is necessary for the information in the map to be rich\nenough to enable generalization across a wide range of tasks. To effectively\nexecute tasks specified in natural language, we propose a hierarchical\nrepresentation built on language-embedded Gaussian splatting that enables both\nsparse semantic planning that lends itself to online operation and dense\ngeometric representation for collision-free navigation. We validate the\neffectiveness of our method through real-world robot experiments conducted in\nboth cluttered indoor and kilometer-scale outdoor environments, with a\ncompetitive ratio of about 60% against privileged baselines. Experiment videos\nand more details can be found on our project page: https:\/\/atlasnav.github.io",
        "Medical image denoising is considered among the most challenging vision\ntasks. Despite the real-world implications, existing denoising methods have\nnotable drawbacks as they often generate visual artifacts when applied to\nheterogeneous medical images. This study addresses the limitation of the\ncontemporary denoising methods with an artificial intelligence (AI)-driven\ntwo-stage learning strategy. The proposed method learns to estimate the\nresidual noise from the noisy images. Later, it incorporates a novel noise\nattention mechanism to correlate estimated residual noise with noisy inputs to\nperform denoising in a course-to-refine manner. This study also proposes to\nleverage a multi-modal learning strategy to generalize the denoising among\nmedical image modalities and multiple noise patterns for widespread\napplications. The practicability of the proposed method has been evaluated with\ndense experiments. The experimental results demonstrated that the proposed\nmethod achieved state-of-the-art performance by significantly outperforming the\nexisting medical image denoising methods in quantitative and qualitative\ncomparisons. Overall, it illustrates a performance gain of 7.64 in Peak\nSignal-to-Noise Ratio (PSNR), 0.1021 in Structural Similarity Index (SSIM),\n0.80 in DeltaE ($\\Delta E$), 0.1855 in Visual Information Fidelity Pixel-wise\n(VIFP), and 18.54 in Mean Squared Error (MSE) metrics.",
        "In this paper, we obtain the existence of weak solutions to the\nChoquard-Kirchhoff type critical multiphase problem: \\begin{equation*}\n\\left\\{\\begin{array}{cc}\n  &-M(\\varphi_{\\h}(\\lvert{\\nabla u}\\rvert))div(\\lvert{\\nabla\nu}\\rvert^{p(x)-2}\\nabla u+a_1(x)\\lvert{\\nabla u}\\rvert^{q(x)-2}\\nabla\nu+a_2(x)\\lvert{\\nabla u}\\rvert^{r(x)-2}\\nabla u)\n  & =\\lambda g(x)\\lvert{u}\\rvert^{\\gamma(x)-2}u+\\theta B(x,u)+\\kappa\n\\left(\\int_{\\q}\\frac{F(y,u(y))}{\\lvert{x-y}\\rvert^{d(x,y)}}\\, dy\\right) f(x,u)\n\\ \\text{in} \\ \\Omega,\n  & u=0 \\ \\text{on} \\ {\\partial \\Omega}. \\end{array}\\right. \\end{equation*}\n  The term $B(x,u)$ on the right-hand side generalizes the critical growth. We\nobtain existence and multiplicity results by establishing certain embedding\nresults and concentration compactness principle along with the\nHardy-Littlewood-Sobolev type inequality for the Musielak Orlicz Sobolev space\n$ W^{1,\\mathcal{T}}(\\q)$.",
        "Privacy-preserving network anomaly detection has become an essential area of\nresearch due to growing concerns over the protection of sensitive data.\nTraditional anomaly detection models often prioritize accuracy while neglecting\nthe critical aspect of privacy. In this work, we propose a hybrid ensemble\nmodel that incorporates privacy-preserving techniques to address both detection\naccuracy and data protection. Our model combines the strengths of several\nmachine learning algorithms, including K-Nearest Neighbors (KNN), Support\nVector Machines (SVM), XGBoost, and Artificial Neural Networks (ANN), to create\na robust system capable of identifying network anomalies while ensuring\nprivacy. The proposed approach integrates advanced preprocessing techniques\nthat enhance data quality and address the challenges of small sample sizes and\nimbalanced datasets. By embedding privacy measures into the model design, our\nsolution offers a significant advancement over existing methods, ensuring both\nenhanced detection performance and strong privacy safeguards.",
        "Deuterium, a heavy isotope of hydrogen, is a key tracer of the formation of\nthe Solar System. Recent JWST observations have expanded the dataset of D\/H\nratios in methane on the KBOs Eris and Makemake, providing new insights into\ntheir origins. This study examines the elevated D\/H ratios in methane on these\nKBOs in the context of protosolar nebula dynamics and chemistry, and proposes a\nprimordial origin for the methane, in contrast to previous hypotheses\nsuggesting abiotic production by internal heating. A time-dependent disk model\ncoupled with a deuterium chemistry module was used to simulate the isotopic\nexchange between methane and hydrogen. Observational constraints, including the\nD\/H ratio measured in methane in comet 67P\/Churyumov-Gerasimenko, were used to\nrefine the primordial D\/H abundance. The simulations show that the observed D\/H\nratios in methane on Eris and Makemake are consistent with a primordial origin.\nThe results suggest that methane on these KBOs likely originates from the\nprotosolar nebula, similar to cometary methane, and was sequestered in solid\nform -- either as pure condensates or clathrates -- within their building\nblocks prior to accretion. These results provide a { simple} explanation for\nthe high D\/H ratios in methane on Eris and Makemake, without the need to invoke\ninternal production mechanisms.",
        "In this report, we present an experimental overview of quarkonium results\nobtained in nucleus-nucleus heavy-ion collisions, with a focus on the data\ncollected at the LHC. We discuss the current understanding of charmonium and\nbottomonium behavior in the deconfined medium produced in such collisions,\ncomparing the various observables now accessible to state-of-the-art\ntheoretical models. We also discuss the open points and how future heavy-ion\nexperiments aim to clarify these aspects.",
        "Kullback--Leibler (KL) divergence is a fundamental measure of the\ndissimilarity between two probability distributions, but it can become unstable\nin high-dimensional settings due to its sensitivity to mismatches in\ndistributional support. To address robustness limitations, we propose a novel\nQuantum-Inspired Fidelity-based Divergence (QIF), leveraging quantum\ninformation principles yet efficiently computable on classical hardware.\nCompared to KL divergence, QIF demonstrates improved numerical stability under\npartial or near-disjoint support conditions, thereby reducing the need for\nextensive regularization in specific scenarios. Moreover, QIF admits\nwell-defined theoretical bounds and continuous similarity measures. Building on\nthis, we introduce a novel regularization method, QR-Drop, which utilizes QIF\nto improve generalization in machine learning models. Empirical results show\nthat QR-Drop effectively mitigates overfitting and outperforms state-of-the-art\nmethods.",
        "Spiking Neural Networks (SNNs) have emerged as a promising tool for\nevent-based optical flow estimation tasks due to their ability to leverage\nspatio-temporal information and low-power capabilities. However, the\nperformance of SNN models is often constrained, limiting their application in\nreal-world scenarios. In this work, we address this gap by proposing a novel\nneural network architecture, ST-FlowNet, specifically tailored for optical flow\nestimation from event-based data. The ST-FlowNet architecture integrates\nConvGRU modules to facilitate cross-modal feature augmentation and temporal\nalignment of the predicted optical flow, improving the network's ability to\ncapture complex motion dynamics. Additionally, to overcome the challenges\nassociated with training SNNs, we introduce a novel approach to derive SNN\nmodels from pre-trained artificial neural networks (ANNs) through ANN-to-SNN\nconversion or our proposed BISNN method. Notably, the BISNN method alleviates\nthe complexities involved in biological parameter selection, further enhancing\nthe robustness of SNNs in optical flow estimation tasks. Extensive evaluations\non three benchmark event-based datasets demonstrate that the SNN-based\nST-FlowNet model outperforms state-of-the-art methods, delivering superior\nperformance in accurate optical flow estimation across a diverse range of\ndynamic visual scenes. Furthermore, the inherent energy efficiency of SNN\nmodels is highlighted, establishing a compelling advantage for their practical\ndeployment. Overall, our work presents a novel framework for optical flow\nestimation using SNNs and event-based data, contributing to the advancement of\nneuromorphic vision applications.",
        "The recent developments in data-driven methods have paved the way to new\nmethodologies to provide accurate state reconstruction of engineering systems;\nnuclear reactors represent particularly challenging applications for this task\ndue to the complexity of the strongly coupled physics involved and the\nextremely harsh and hostile environments, especially for new technologies such\nas Generation-IV reactors. Data-driven techniques can combine different sources\nof information, including computational proxy models and local noisy\nmeasurements on the system, to robustly estimate the state. This work leverages\nthe novel Shallow Recurrent Decoder architecture to infer the entire state\nvector (including neutron fluxes, precursors concentrations, temperature,\npressure and velocity) of a reactor from three out-of-core time-series neutron\nflux measurements alone. In particular, this work extends the standard\narchitecture to treat parametric time-series data, ensuring the possibility of\ninvestigating different accidental scenarios and showing the capabilities of\nthis approach to provide an accurate state estimation in various operating\nconditions. This paper considers as a test case the Molten Salt Fast Reactor\n(MSFR), a Generation-IV reactor concept, characterised by strong coupling\nbetween the neutronics and the thermal hydraulics due to the liquid nature of\nthe fuel. The promising results of this work are further strengthened by the\npossibility of quantifying the uncertainty associated with the state\nestimation, due to the considerably low training cost. The accurate\nreconstruction of every characteristic field in real-time makes this approach\nsuitable for monitoring and control purposes in the framework of a reactor\ndigital twin.",
        "In recent years, multimodal large language models (MLLMs) have achieved\nsignificant breakthroughs, enhancing understanding across text and vision.\nHowever, current MLLMs still face challenges in effectively integrating\nknowledge across these modalities during multimodal knowledge reasoning,\nleading to inconsistencies in reasoning outcomes. To systematically explore\nthis issue, we propose four evaluation tasks and construct a new dataset. We\nconduct a series of experiments on this dataset to analyze and compare the\nextent of consistency degradation in multimodal knowledge reasoning within\nMLLMs. Based on the experimental results, we identify factors contributing to\nthe observed degradation in consistency. Our research provides new insights\ninto the challenges of multimodal knowledge reasoning and offers valuable\nguidance for future efforts aimed at improving MLLMs.",
        "The peer review process is crucial for ensuring the quality and reliability\nof scholarly work, yet assigning suitable reviewers remains a significant\nchallenge. Traditional manual methods are labor-intensive and often\nineffective, leading to nonconstructive or biased reviews. This paper\nintroduces the exHarmony (eHarmony but for connecting experts to manuscripts)\nbenchmark, designed to address these challenges by re-imagining the Reviewer\nAssignment Problem (RAP) as a retrieval task. Utilizing the extensive data from\nOpenAlex, we propose a novel approach that considers a host of signals from the\nauthors, most similar experts, and the citation relations as potential\nindicators for a suitable reviewer for a manuscript. This approach allows us to\ndevelop a standard benchmark dataset for evaluating the reviewer assignment\nproblem without needing explicit labels. We benchmark various methods,\nincluding traditional lexical matching, static neural embeddings, and\ncontextualized neural embeddings, and introduce evaluation metrics that assess\nboth relevance and diversity in the context of RAP. Our results indicate that\nwhile traditional methods perform reasonably well, contextualized embeddings\ntrained on scholarly literature show the best performance. The findings\nunderscore the importance of further research to enhance the diversity and\neffectiveness of reviewer assignments.",
        "Using direct numerical simulations of forced rotating turbulence, we study\nthe effect of rotation on the growth rate and the saturation level of the\nsmall-scale dynamo. For slow rotation rates, increasing the rotation rate\nreduces both the growth rate and the saturation level. Once the rotation rate\ncrosses a threshold, large-scale vortices are formed which enhance the growth\nrate and the saturation level. Below this threshold, the suppression of the\nsmall-scale dynamo with increasing rotation is explained by the fact that at\nscales close to, but smaller than, the forcing scale, rotating turbulence is\none-dimensionalized, with the velocity component along the rotation axis being\nlarger than the other two components. This is due to the rotational\ndestabilization of vortices produced by the forcing function. While the\nrotational effect on the growth rate becomes small at high Re, the ratio of the\nsteady-state magnetic to kinetic energies remains suppressed by up to 35% as\ncompared to the non-rotating case.",
        "We demonstrate that in the presence of a light scalar spectator field, vacuum\ntransitions taking place during inflation can produce large, potentially\ndetectable non-Gaussian signatures in the primordial curvature perturbation.\nSuch transitions are common in theories with multiple scalar fields when the\npotential has several minima. Our computation proceeds by numerically finding\nthe instanton solution that describes quantum tunnelling between vacuum states\nin a de Sitter background, calculating its dependence on the spectator field\nand, thereby, its effect on the expansion of space. For a scenario with Higgs\ninflation, we obtain the non-Gaussianity parameter $f_\\mathrm{NL} \\sim O(10)$\nand study its parameter dependence.",
        "This paper presents a hierarchical framework for demand response optimization\nin air separation units (ASUs) that combines reinforcement learning (RL) with\nlinear model predictive control (LMPC). We investigate two control\narchitectures: a direct RL approach and a control-informed methodology where an\nRL agent provides setpoints to a lower-level LMPC. The proposed RL-LMPC\nframework demonstrates improved sample efficiency during training and better\nconstraint satisfaction compared to direct RL control. Using an industrial ASU\ncase study, we show that our approach successfully manages operational\nconstraints while optimizing electricity costs under time-varying pricing.\nResults indicate that the RL-LMPC architecture achieves comparable economic\nperformance to direct RL while providing better robustness and requiring fewer\ntraining samples to converge. The framework offers a practical solution for\nimplementing flexible operation strategies in process industries, bridging the\ngap between data-driven methods and traditional control approaches.",
        "In this paper, we present a robust and fully discretized method for solving\nthe time fractional diffusion equation with high-contrast multiscale\ncoefficients. We establish the homogenized equation in a coarse mesh using a\nmulticontinuum approach and employ the exponential integrator method for time\ndiscretization. The multicontinuum upscaled model captures the physical\ncharacteristics of the solution for the high-contrast multiscale problem,\nincluding averages and gradient effects in each continuum at the coarse scale.\nWe use the exponential integration method to address the nonlocality induced by\nthe time fractional derivative and the stiffness from the multiscale\ncoefficients in the semi-discretized problem. Convergence analysis of the\nnumerical scheme is provided, along with illustrative numerical examples. Our\nresults demonstrate the accuracy, efficiency, and improved stability for\nvarying order of fractional derivatives.",
        "Spatio-Temporal (ST) data science, which includes sensing, managing, and\nmining large-scale data across space and time, is fundamental to understanding\ncomplex systems in domains such as urban computing, climate science, and\nintelligent transportation. Traditional deep learning approaches have\nsignificantly advanced this field, particularly in the stage of ST data mining.\nHowever, these models remain task-specific and often require extensive labeled\ndata. Inspired by the success of Foundation Models (FM), especially large\nlanguage models, researchers have begun exploring the concept of\nSpatio-Temporal Foundation Models (STFMs) to enhance adaptability and\ngeneralization across diverse ST tasks. Unlike prior architectures, STFMs\nempower the entire workflow of ST data science, ranging from data sensing,\nmanagement, to mining, thereby offering a more holistic and scalable approach.\nDespite rapid progress, a systematic study of STFMs for ST data science remains\nlacking. This survey aims to provide a comprehensive review of STFMs,\ncategorizing existing methodologies and identifying key research directions to\nadvance ST general intelligence.",
        "Let $e^{-tL}$ be a analytic semigroup generated by $-L$, where $L$ is a\nnon-negative self-adjoint operator on $L^2(\\mathbb{R}^d)$. Assume that the\nkernels of $e^{-tL}$, denoted by $p_t(x,y)$, only satisfy the upper bound: for\nall $N>0$, there are constants $c,C>0$ such that \\begin{align}\\label{upper\nbound}\n|p_t(x,y)|\\leq\\frac{C}{t^{d\/2}}e^{-\\frac{|x-y|^2}{ct}}\\Big(1+\\frac{\\sqrt{t}}{\\rho(x)}+\n\\frac{\\sqrt{t}}{\\rho(y)}\\Big)^{-N} \\end{align} holds for all\n$x,y\\in\\mathbb{R}^d$ and $t>0$. We first establish the quantitative matrix\nweighted inequalities for fractional type integrals associated to $L$ with new\nclasses of matrix weights, which are nontrivial extension of the results\nestablished by Li, Rahm and Wick [23]. Next, we give new two-weight bump\nconditions with Young functions satisfying wider conditions for fractional type\nintegrals associated to $L$, which cover the result obtained by Cruz-Uribe,\nIsralowitz and Moen [6]. We point out that the new classes of matrix weights\nand bump conditions are larger and weaker than the classical ones given in [17]\nand [6], respectively. As applications, our results can be applied to settings\nof magnetic Schr\\\"{o}dinger operator, Laguerre operators, etc.",
        "This paper presents a study that analyzed state-of-the-art vulnerability\nscanning tools applied to containers. We have focused the work on tools\nfollowing the Vulnerability Exploitability eXchange (VEX) format, which has\nbeen introduced to complement Software Bills of Material (SBOM) with security\nadvisories of known vulnerabilities. Being able to get an accurate\nunderstanding of vulnerabilities found in the dependencies of third-party\nsoftware is critical for secure software development and risk analysis.\nAccepting the overwhelming challenge of estimating the precise accuracy and\nprecision of a vulnerability scanner, we have in this study instead set out to\nexplore how consistently different tools perform. By doing this, we aim to\nassess the maturity of the VEX tool field as a whole (rather than any\nparticular tool). We have used the Jaccard and Tversky indices to produce\nsimilarity scores of tool performance for several different datasets created\nfrom container images. Overall, our results show a low level of consistency\namong the tools, thus indicating a low level of maturity in VEX tool space. We\nhave performed a number of experiments to find and explanation to our results,\nbut largely they are inconclusive and further research is needed to understand\nthe underlying causalities of our findings.",
        "Drones or unmanned aerial vehicles are traditionally used for military\nmissions, warfare, and espionage. However, the usage of drones has\nsignificantly increased due to multiple industrial applications involving\nsecurity and inspection, transportation, research purposes, and recreational\ndrone flying. Such an increased volume of drone activity in public spaces\nrequires regulatory actions for purposes of privacy protection and safety.\nHence, detection of illegal drone activities such as boundary encroachment\nbecomes a necessity. Such detection tasks are usually automated and performed\nby deep learning models which are trained on annotated image datasets. This\npaper builds on a previous work and extends an already published open source\ndataset. A description and analysis of the entire dataset is provided. The\ndataset is used to train the YOLOv7 deep learning model and some of its minor\nvariants and the results are provided. Since the detection models are based on\na single image input, a simple cross-correlation based tracker is used to\nreduce detection drops and improve tracking performance in videos. Finally, the\nentire drone detection system is summarized.",
        "Ekman pumping is a phenomenon induced by no-slip boundary conditions in\nrotating fluids. In the context of Rayleigh-B\\'enard convection, Ekman pumping\ncauses a significant change in the linear stability of the system compared to\nwhen it is not present (that is, stress-free). Motivated by numerical solutions\nto the marginal stability problem of the incompressible Navier-Stokes (iNSE)\nsystem, we seek analytical asymptotic solutions which describe the departure of\nthe no-slip solution from the stress-free. The substitution of normal modes\ninto a reduced asymptotic model yields a linear system for which we explore\nanalytical solutions for various scalings of wavenumber. We find very good\nagreement between the analytical asymptotic solutions and the numerical\nsolutions to the iNSE linear stability problem with no-slip boundary\nconditions.",
        "This paper proposes a novel approach based on deep reinforcement learning\n(DRL) for the 2D+1 packing problem with spatial constraints. This problem is an\nextension of the traditional 2D packing problem, incorporating an additional\nconstraint on the height dimension. Therefore, a simulator using the OpenAI Gym\nframework has been developed to efficiently simulate the packing of rectangular\npieces onto two boards with height constraints. Furthermore, the simulator\nsupports multidiscrete actions, enabling the selection of a position on either\nboard and the type of piece to place. Finally, two DRL-based methods (Proximal\nPolicy Optimization -- PPO and the Advantage Actor-Critic -- A2C) have been\nemployed to learn a packing strategy and demonstrate its performance compared\nto a well-known heuristic baseline (MaxRect-BL). In the experiments carried\nout, the PPO-based approach proved to be a good solution for solving complex\npackaging problems and highlighted its potential to optimize resource\nutilization in various industrial applications, such as the manufacturing of\naerospace composites."
      ]
    }
  },
  {
    "id":2411.15331,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"Mutagenpred-gcnns: a graph convolutional neural network-based classification model for mutagenicity prediction with data-driven molecular fingerprints",
    "start_abstract":"An important task in the early stage of drug discovery is the identification of mutagenic compounds. Mutagenicity prediction models that can interpret relationships between toxicological endpoints and compound structures are especially favorable. In this research, we used an advanced graph convolutional neural network (GCNN) architecture to identify the molecular representation and develop predictive models based on these representations. The predictive model based on features extracted by GCNNs can not only predict the mutagenicity of compounds but also identify the structure alerts in compounds. In fivefold cross-validation and external validation, the highest area under the curve was 0.8782 and 0.8382, respectively; the highest accuracy (Q) was 80.98% and 76.63%, respectively; the highest sensitivity was 83.27% and 78.92%, respectively; and the highest specificity was 78.83% and 76.32%, respectively. Additionally, our model also identified some toxicophores, such as aromatic nitro, three-membered heterocycles, quinones, and nitrogen and sulfur mustard. These results indicate that GCNNs could learn the features of mutagens effectively. In summary, we developed a mutagenicity classification model with high predictive performance and interpretability based on a data-driven molecular representation trained through GCNNs.",
    "start_categories":[
      "cs.NE"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "Improvement of quantitative structure\u2013activity relationship (QSAR) tools for predicting Ames mutagenicity: outcomes of the Ames\/QSAR International Challenge Project"
      ],
      "abstract":[
        "The International Conference on Harmonization (ICH) M7 guideline allows the use of in silico approaches for predicting Ames mutagenicity initial assessment impurities pharmaceuticals. This is first international that addresses quantitative structure\u2013activity relationship (QSAR) models lieu actual toxicological studies human health assessment. Therefore, QSAR now require higher predictive power identifying mutagenic chemicals. To increase models, larger experimental datasets from reliable sources are required. Division Genetics and Mutagenesis, National Institute Health Sciences (DGM\/NIHS) Japan recently established a unique proprietary database containing 12140 new chemicals have not been previously used developing models. DGM\/NIHS provided this to vendors validate improve their tools. Ames\/QSAR Challenge Project was initiated 2014 with 12 testing 17 tools against these compounds three phases. We present final results. All were considerably improved by participation project. Most achieved >50% sensitivity (positive prediction among all positives) (accuracy) as high 80%, almost equivalent inter-laboratory reproducibility tests. further tools, accumulation additional test data required well re-evaluation some previous Indeed, Ames-positive or Ames-negative may incorrectly classified because methodological weakness, resulting false-positive false-negative predictions These incorrect hamper source noise development It thus essential establish large benchmark consisting only well-validated results build more accurate"
      ],
      "categories":[
        "q-bio.GN"
      ]
    },
    "list":{
      "title":[
        "The Relativity of Causal Knowledge",
        "Meta-Learning-Based People Counting and Localization Models Employing\n  CSI from Commodity WiFi NICs",
        "Resonance fluorescence spectra of a driven Kerr nonlinear resonator",
        "Semadeni derivative of Banach spaces and functions on nonmetrizable\n  rectangles",
        "Minimum numbers of Dehn colors of knots and $\\mathcal{R}$-palette graphs",
        "The Third Generation of Nanogenerators: The Irreplaceable Potential\n  Source Enabled by the Flexoelectric Nanogenerator",
        "Simplicial effects and weakly associative partial groups",
        "Vectorial Kato inequality for $p$-harmonic maps with optimal constant",
        "Uniform-in-time error estimate of random batch method with replacement\n  for the Cucker-Smale model",
        "Electrical Control of the Exchange Bias Effect at\n  Ferromagnet-Altermagnet Junctions",
        "Hybrid Quantum-Classical Optimisation of Traveling Salesperson Problem",
        "Limited attention and models of choice: A behavioral equivalence",
        "A Pathwise Coordinate Descent Algorithm for LASSO Penalized Quantile\n  Regression",
        "Precompactness in bivariate metric semigroup-valued bounded variation\n  spaces",
        "Thomas-Wigner rotation via Clifford algebras",
        "Analysing the flux stability of stellar calibrator candidates with TESS",
        "Balanced cross-Kerr coupling for superconducting qubit readout",
        "Nano-topographical changes in latent fingerprint due to degradation over\n  time studied by Atomic force microscopy -- option to set a timeline?",
        "On singular problems associated with mixed operators under mixed\n  boundary conditions",
        "On total transitivity of graphs",
        "Stochastic Stefan problem on moving hypersurfaces: an approach by a new\n  framework of nonhomogeneous monotonicity",
        "Weak-emission-line quasars: A new clue from their optical variability",
        "On the global solvability of the generalised Navier-Stokes system in\n  critical Besov spaces",
        "Role of the Short-Range Dynamics in the Formation of $D^{(*)}\\bar\n  D^{(*)}\/B^{(*)}\\bar B^{(*)} $ Hadronic Molecules",
        "Orbits on a product of two flags and a line and the Bruhat Order, I",
        "Extended circular nim",
        "A Dimension-Reduced Multivariate Spatial Model for Extreme Events:\n  Balancing Flexibility and Scalability",
        "Cavity quantum electrodynamics of photonic temporal crystals",
        "Effect of interface on magnetic exchange coupling in Co\/Ru\/Co trilayer:\n  from ab-initio simulations to micromagnetics"
      ],
      "abstract":[
        "Recent advances in artificial intelligence reveal the limits of purely\npredictive systems and call for a shift toward causal and collaborative\nreasoning. Drawing inspiration from the revolution of Grothendieck in\nmathematics, we introduce the relativity of causal knowledge, which posits\nstructural causal models (SCMs) are inherently imperfect, subjective\nrepresentations embedded within networks of relationships. By leveraging\ncategory theory, we arrange SCMs into a functor category and show that their\nobservational and interventional probability measures naturally form convex\nstructures. This result allows us to encode non-intervened SCMs with convex\nspaces of probability measures. Next, using sheaf theory, we construct the\nnetwork sheaf and cosheaf of causal knowledge. These structures enable the\ntransfer of causal knowledge across the network while incorporating\ninterventional consistency and the perspective of the subjects, ultimately\nleading to the formal, mathematical definition of relative causal knowledge.",
        "In this paper, we consider people counting and localization systems\nexploiting channel state information (CSI) measured from commodity WiFi network\ninterface cards (NICs). While CSI has useful information of amplitude and phase\nto describe signal propagation in a measurement environment of interest, CSI\nmeasurement suffers from offsets due to various uncertainties. Moreover, an\nuncontrollable external environment where other WiFi devices communicate each\nother induces interfering signals, resulting in erroneous CSI captured at a\nreceiver. In this paper, preprocessing of CSI is first proposed for offset\nremoval, and it guarantees low-latency operation without any filtering process.\nAfterwards, we design people counting and localization models based on\npre-training. To be adaptive to different measurement environments,\nmeta-learning-based people counting and localization models are also proposed.\nNumerical results show that the proposed meta-learning-based people counting\nand localization models can achieve high sensing accuracy, compared to other\nlearning schemes that follow simple training and test procedures.",
        "Resonance fluorescence spectra of a driven Kerr nonlinear resonator is\ninvestigated both theoretically and experimentally. When the Kerr nonlinear\nresonator is driven strongly such that the induced Rabi frequency is comparable\nto or larger than the Kerr nonlinearity, the system cannot be approximated as a\ntwo-level system. We theoretically derive characteristic features in the\nfluorescence spectra such as the decrease of the center-peak intensity and the\nasymmetric sideband peaks in the presence of finite dephasing. Those features\nare consistently explained by the population of the initial dressed state and\nits transition matrix element to the final dressed state of the transition\ncorresponding to each peak. Finally, we experimentally measure the resonance\nfluorescence spectra of a driven superconducting Kerr nonlinear resonator and\nfind a quantitative agreement with our theory.",
        "We study Banach spaces $C(K)$ of real-valued continuous functions from the\nfinite product of compact lines. It turns out that the topological character of\nthese compact lines can be used to distinguish whether two spaces of continuous\nfunctions on products are isomorphic or embeddable to each other. In\nparticular, for compact lines $K_1, \\dots, K_n, L_1, \\dots, L_k$ of uncountable\ncharacter and $k \\neq n$, we claim that Banach spaces $C(\\prod_{i=1}^n K_i)$\nand $C(\\prod_{j=1}^k L_j)$ are not isomorphic.",
        "In this paper, we consider minimum numbers of colors of knots for Dehn\ncolorings. In particular, we will show that for any odd prime number $p$ and\nany Dehn $p$-colorable knot $K$, the minimum number of colors for $K$ is at\nleast $\\lfloor \\log_2 p \\rfloor +2$. Moreover, we will define the $\\R$-palette\ngraph for a set of colors. The $\\R$-palette graphs are quite useful to give\ncandidates of sets of colors which might realize a nontrivially Dehn\n$p$-colored diagram. In Appendix, we also prove that for Dehn $5$-colorable\nknot, the minimum number of colors is $4$.",
        "The electroneutrality assumption has long been adopted by scholars; however,\nthis assumption may lead to an oversight of certain physical effects. Using\nderivations from a discontinuous medium, we have obtained an expression for the\npotential and energy of a many-body unipolar charge system, which corresponds\nwell to its counterpart in a continuous medium. The compressed form of this\nexpression suggests that compressing a macroscale charged body to the nanoscale\ncan yield an enormous electric potential and energy, thereby establishing a\nconcrete research framework for third-generation nanogenerators. This effect\nmay serve as a crucial reference for understanding anomalous spatial\nelectromagnetic distributions and divergent energy fields.",
        "In this paper, we introduce a new category of simplicial effects that extends\nthe categories of effect algebras and their multi-object counterpart, effect\nalgebroids. Our approach is based on relaxing the associativity condition\nsatisfied by effect algebras and, more generally, partial monoids. Within this\nframework, simplicial effects and weakly associative partial groups arise as\ntwo extreme cases in the category of weak partial monoids. Our motivation is to\ncapture simplicial structures from the theory of simplicial distributions and\nmeasurements that behave like effects.",
        "We derive the sharp vectorial Kato inequality for $p$-harmonic mappings.\nSurprisingly, the optimal constant differs from the one obtained for scalar\nvalued $p$-harmonic functions by Chang, Chen, and Wei. As an application we\ndemonstrate how this inequality can be used in the study of regularity of\n$p$-harmonic maps. Furthermore, in the case of $p$-harmonic maps from $B^3$ to\n$\\mathbb{S}^3$, we enhance the known range of $p$ values for which regularity\nis achieved. Specifically, we establish that for $p \\in [2, 2.642]$, minimizing\n$p$-harmonic maps must be regular.",
        "The Random Batch Method (RBM), proposed by Jin et al. in 2020, is an\nefficient algorithm for simulating interacting particle systems. The\nuniform-in-time error estimates of the RBM without replacement have been\nobtained for various interacting particle systems, while the analysis of the\nRBM with replacement is just considered in (Cai et al., 2024) recently for the\nfirst-order systems governed by Langevin dynamics. In this work, we present the\nerror estimate for the RBM with replacement applied to a second-order system\nknown as the Cucker-Smale model. By introducing a crucial auxiliary system and\nleveraging the intrinsic characteristics of the Cucker-Smale model, we derive\nan estimate that is uniform in both time and particle numbers. Additionally, we\nprovide numerical simulations to validate the analytical results.",
        "This work analyzes the behavior of the interface between a ferromagnetic\nmaterial and an alter-magnet. We use a well-established line of arguments based\non electronic mean-field calculations to show that new surface phenomena that\nlead to altermagnetic materials induce an exchange bias effect on the nearby\nferromagnet. We reveal the physical mechanisms behind this phenomenon that lead\nto quantitative control over its strength. Interestingly, we predict exotic\nelectric-field-induced phenomena. This is an analogy to the relationship\nbetween exchange bias and the injection of spin currents in\nspin-transfer-dominated scenarios, which has been reported earlier in the\ntraditional antiferromagnetic\/ferromagnetic junction.",
        "The Traveling Salesperson Problem (TSP) is a fundamental NP-hard optimisation\nchallenge with widespread applications in logistics, operations research, and\nnetwork design. While classical algorithms effectively solve small to\nmedium-sized instances, they struggle with scalability due to exponential\ncomplexity. In this work, we present a hybrid quantum-classical approach that\nleverages IBM's Qiskit Runtime to integrate quantum optimisation techniques\nwith classical machine learning methods, specifically K-Means clustering and\nRandom Forest classifiers. These machine learning components aid in problem\ndecomposition and noise mitigation, improving the quality of quantum solutions.\nExperimental results for TSP instances ranging from 4 to 8 cities reveal that\nthe quantum-only approach produces solutions up to 21.7% worse than the\nclassical baseline, while the hybrid method reduces this cost increase to 11.3%\nfor 8 cities. This demonstrates that hybrid approaches improve solution quality\ncompared to purely quantum methods but remain suboptimal compared to classical\nsolvers. Despite current hardware limitations, these results highlight the\npotential of quantum-enhanced methods for combinatorial optimisation, paving\nthe way for future advancements in scalable quantum computing frameworks.",
        "We show that many models of choice can be alternatively represented as\nspecial cases of choice with limited attention (Masatlioglu, Nakajima, and\nOzbay, 2012), and the properties of the unobserved attention filters that\nexplain the observed choices are singled out. Moreover, for each specification,\nwe infer information about the DM's attention and preference from irrational\nfeatures of choice data.",
        "$\\ell_1$ penalized quantile regression is used in many fields as an\nalternative to penalized least squares regressions for high-dimensional data\nanalysis. Existing algorithms for penalized quantile regression either use\nlinear programming, which does not scale well in high dimension, or an\napproximate coordinate descent (CD) which does not solve for exact\ncoordinatewise minimum of the nonsmooth loss function. Further, neither\napproaches build fast, pathwise algorithms commonly used in high-dimensional\nstatistics to leverage sparsity structure of the problem in large-scale data\nsets. To avoid the computational challenges associated with the nonsmooth\nquantile loss, some recent works have even advocated using smooth\napproximations to the exact problem. In this work, we develop a fast, pathwise\ncoordinate descent algorithm to compute exact $\\ell_1$ penalized quantile\nregression estimates for high-dimensional data. We derive an easy-to-compute\nexact solution for the coordinatewise nonsmooth loss minimization, which, to\nthe best of our knowledge, has not been reported in the literature. We also\nemploy a random perturbation strategy to help the algorithm avoid getting stuck\nalong the regularization path. In simulated data sets, we show that our\nalgorithm runs substantially faster than existing alternatives based on\napproximate CD and linear program, while retaining the same level of estimation\naccuracy.",
        "In this paper, we show that if a set in bivariate metric semigroups-valued\nbounded variation spaces is pointwise totally bounded and joint equivariated\nthen it is precompact. These spaces include bounded Jordan variation spaces,\nbounded Wiener variation spaces, bounded Waterman variation spaces, bounded\nRiesz variation spaces and bounded Korenblum variation spaces. To do so, we\nintroduce the concept of equimetric set.",
        "We derive Macfarlane's formula for the Thomas-Wigner angle of rotation using\nClifford-algebra methods. The presentation is pedagogical and elementary,\nsuitable for students with some basic knowledge of special relativity; no prior\nknowledge of Clifford algebras is required.",
        "The ESA space mission Ariel requires bright sources that are stable at the\nlevel of 100ppm over 6 hours in order to accurately measure exoplanet\natmospheres through transmission spectroscopy. To ensure this, in-flight\ninstrument calibration can be performed by observing stellar calibrators. In\nthis study, a stellar calibrator candidate list distributed over the sky is\ncreated and a flux variability analysis is performed to identify the best\nstellar calibrators for transit spectroscopy of exoplanet atmospheres with\nAriel. A starting candidate sample of 1937 solar-type stars is created using\nthe all-sky surveys Two Micron All Sky Survey and Gaia. Using stellar light\ncurves from the Transit Exoplanet Survey Satellite (TESS), the flux variability\nof each star is characterised by computing its Lomb-Scargle periodogram and\nreduced chi-squared. This enables the elimination of stars with detectable\nvariability from the sample. Approximately 22.2% of stars from the starting\nsample pass the selection as potential calibrators. These do not all\nnecessarily meet Ariel's stability requirement, although some will. No\ncorrelation between flux stability and stellar properties is found, as long as\nthe correct value ranges for the parameters are chosen, like a surface\ntemperature between 5000 and 6300K. The only exception is stellar magnitude:\nNoise in TESS data increases as stars get dimmer, so, a high percentage of\nfaint stars passes the selection since their variability is more likely hidden\nwithin the inherent TESS noise. Contrarily, stars brighter than 5mag cannot be\nused as calibrators. A list of 430 promising bright calibration targets\ndistributed over the sky has been selected. These can potentially be used as\nstellar calibrators for the Ariel mission. Targets from this list will have to\nbe further studied to determine which ones possess a flux stability better than\n100ppm over 6 hours.",
        "Dispersive readout, the standard method for measuring superconducting qubits,\nis limited by multiphoton qubit-resonator processes arising even at moderate\ndrive powers. These processes degrade performance, causing dispersive readout\nto lag behind single- and two-qubit gates in both speed and fidelity. In this\nwork, we propose a novel readout method, termed \"junction readout\". Junction\nreadout leverages the nonperturbative cross-Kerr interaction resulting from\ncoupling a qubit and a resonator via a Josephson junction. Furthermore, by\nadding a capacitive coupling in parallel to the junction, Purcell decay can be\nsuppressed without the need for a Purcell filter. We also show that junction\nreadout is more robust against deleterious multiphoton processes, and offers\ngreater flexibility for resonator frequency allocation. Crucially, junction\nreadout achieves superior performance compared to dispersive readout while\nmaintaining similar hardware overhead. Numerical simulations show that junction\nreadout can achieve fidelities exceeding $99.99\\%$ in under $30$ ns, making it\na promising alternative for superconducting qubit readout with current\nhardware.",
        "Latent fingerprints, if present, are crucial in identifying the suspect who\nwas at the crime scene. If there are many latent fingerprints or the suspect is\nfrom the same household, crime investigators may have difficulty identifying\nwhose latent fingerprints are time-related to the crime. Here, we report\nchanges in the nanoscale topography of latent fingerprints, which may serve as\na timeline and could help estimate when the latent fingerprint was imprinted.\nOn the latent fingerprint of an adolescent, we observed a change in\nnano-topography over time, specifically the formation of nano-chain structures\nin space between the imprinted papillary ridges. We consequently compared this\nobservation with the decomposition of the latent fingerprints of a child and\nadult. We observed a significant difference in the time change in\nnano-topography of latent fingerprints of a child, adolescent, and young adult.\nThe nano-topographical changes of latent fingerprints were studied by atomic\nforce microscopy over 70 days. In the case of child's and adolescent's latent\nfingerprints, the first nano-chains were observed already 24 hours after\nimprinting of the latent fingerprint, and the number of nano-chains increased\nsteadily up to 21 days, then we observed that another organic material covered\nthe nano-chains, and they started slowly deteriorating; nevertheless, the\nnano-chains were still present on the 70th day.",
        "In this paper, we study the following singular problem associated with mixed\noperators (the combination of the classical Laplace operator and the fractional\nLaplace operator) under mixed boundary conditions \\begin{equation*} \\label{1}\n  \\left\\{\n  \\begin{aligned}\n  \\mathcal{L}u &= g(u), \\quad u > 0 \\quad \\text{in} \\quad \\Omega,\n  u &= 0 \\quad \\text{in} \\quad U^c,\n  \\mathcal{N}_s(u) &= 0 \\quad \\text{in} \\quad \\mathcal{N},\n  \\frac{\\partial u}{\\partial \\nu} &= 0 \\quad \\text{in} \\quad \\partial \\Omega\n\\cap \\overline{\\mathcal{N}},\n  \\end{aligned}\n  \\right.\n  \\tag{$P_\\lambda$} \\end{equation*}\n  where $U= (\\Omega \\cup {\\mathcal{N}} \\cup\n(\\partial\\Omega\\cap\\overline{\\mathcal{N}}))$, $\\Omega \\subseteq \\mathbb{R}^N$\nis a non empty open set, $\\mathcal{D}$, $\\mathcal{N}$ are open subsets of\n$\\mathbb{R}^N\\setminus{\\bar{\\Omega }}$ such that ${\\mathcal{D}} \\cup\n{\\mathcal{N}}= \\mathbb{R}^N\\setminus{\\bar{\\Omega}}$, $\\mathcal{D} \\cap\n{\\mathcal{N}}= \\emptyset $ and $\\Omega\\cup \\mathcal{N}$ is a bounded set with\nsmooth boundary, $\\lambda >0$ is a real parameter and\n  $\\mathcal{L}= -\\Delta+(-\\Delta)^{s},~ \\text{for}~s \\in (0, 1).$\n  Here $g(u)=u^{-q}$ or $g(u)= \\lambda u^{-q}+ u^p$ with $0<q<1<p\\leq 2^*-1$.\nWe study $(P_\\lambda)$ to derive the existence of weak solutions along with its\n$L^\\infty$-regularity. Moreover, some Sobolev-type variational inequalities\nassociated with these weak solutions are established.",
        "Let $G=(V, E)$ be a graph where $V$ and $E$ are the vertex and edge sets,\nrespectively. For two disjoint subsets $A$ and $B$ of $V$, we say $A$\n\\emph{dominates} $B$ if every vertex of $B$ is adjacent to at least one vertex\nof $A$. A vertex partition $\\pi = \\{V_1, V_2, \\ldots, V_k\\}$ of $G$ is called a\n\\emph{transitive partition} of size $k$ if $V_i$ dominates $V_j$ for all $1\\leq\ni<j\\leq k$. In this article, we study a variation of transitive partition,\nnamely \\emph{total transitive partition}. The total transitivity $Tr_t(G)$ is\nequal to the maximum order of a vertex partition $\\pi = \\{V_1, V_2, \\ldots,\nV_k\\}$ of $G$ obtained by repeatedly removing a total dominating set from $G$,\nuntil no vertices remain. Thus, $V_1$ is a total dominating set of $G$, $V_2$\nis a total dominating set of the graph $G_1=G-V_1$, and, in general, $V_{i+1}$\nis a total dominating set in the graph $G_i=G-\\bigcup_{j=1}^i V_i$. A vertex\npartition of order $Tr_t(G)$ is called $Tr_t$-partition. The \\textsc{Maximum\nTotal Transitivity Problem} is to find a total transitive partition of a given\ngraph with the maximum number of parts. First, we characterize split graphs\nwith total transitivity equal to $1$ and $\\omega(G)-1$. Moreover, for the split\ngraph $G$ and $1\\leq p\\leq \\omega(G)-1$, we give some necessary conditions for\n$Tr_t(G)=p$. Furthermore, we show that the decision version of this problem is\nNP-complete for bipartite graphs. On the positive side, we design a\npolynomial-time algorithm to solve \\textsc{Maximum Total Transitivity Problem}\nin trees.",
        "The purpose of this paper is to establish the well-posedness of the\nstochastic Stefan problem on moving hypersurfaces. Through a specially designed\ntransformation, it turns out we need to solve stochastic partial differential\nequations on a fixed hypersurface with a new kind of nonhomogeneous\nmonotonicity involving a family of time-dependent operators. This new class of\nSPDEs is of independent interest and can also be applied to solve many other\ninteresting models such as the stochastic $p$-Laplacian equations, stochastic\nAllen-Cahn equation and stochastic heat equations on time-dependent domains or\nhypersurfaces. (Monotone) Operator-valued calculus and geometric analysis of\nmoving hypersurfaces play important roles in the study. Moreover, a forthcoming\nresult on the well-posedness of stochastic 2D Navier-Stokes equation on moving\ndomains is also based on our framework.",
        "Weak-emission-line QSOs (WLQs) are an enigmatic subclass of the QSO\npopulation, as their optical\/UV spectra are marked by abnormally weak (or\nabsent) emission lines. To obtain much-needed additional clues to the origin of\nthis and other known peculiarities of WLQs, we have determined the 'ensemble'\noptical variability characteristics for a large, well-defined sample of 76\nradio-quiet WLQs and also for a matched control sample comprising 603 normal\nradio-quiet QSOs. This analysis was done using their light-curves recorded in\nthe $g$ and $r$ bands, under the ZTF survey during 2018-2024, with a typical\ncadence of 3 days. We find that, compared to normal QSOs, WLQs exhibit\nsystematically milder optical variability on month\/year-like time scales (by a\nfactor of $\\sim$ 1.76$\\pm$0.05 in amplitude). We have independently verified\nthis by carrying out an equivalent analysis of the V-band light-curves acquired\nunder the CRTS during 2007- 2014, with a typical cadence of 10 days. This new\nobservational differentiator between WLQs and normal QSOs may provide clues to\nunderstanding the intriguing nature of WLQs. It is proposed that the clumpiness\nof the torus material flowing into the central engine may play a key role in\nexplaining the observed differences between the WLQs and normal QSOs.",
        "This paper is devoted to the global solvability of the Navier-Stokes system\nwith fractional Laplacian $(-\\Delta)^{\\alpha}$ in $\\mathbb{R}^{n}$ for\n$n\\geq2$, where the convective term has the form $(|u|^{m-1}u)\\cdot\\nabla u$\nfor $m\\geq1$. By establishing the estimates for the difference\n$|u_{1}|^{m-1}u_{1}-|u_{2}|^{m-1}u_{2}$ in homogeneous Besov spaces, and\nemploying the maximal regularity property of $(-\\Delta)^{\\alpha}$ in Lorentz\nspaces, we prove global existence and uniqueness of the strong solution of the\nNavier-Stokes in critical Besov spaces for both $m=1$ and $m>1$",
        "We investigate potential hadronic molecular states in the $D^{(*)}\\bar\nD^{(*)}$ and $B^{(*)}\\bar B^{*}$ systems using light meson exchange\ninteractions. Our analysis focuses on coupled-channel systems with spin-parity\nquantum numbers $J^{PC}=0^{++}$, $1^{+\\pm}$ and $2^{++}$, examining how the\n$\\delta(r)$ potential affects states near threshold. Using coupled-channel\nanalysis, we reproduce the $X(3872)$ mass with a given cutoff for the\n$(I)J^{PC}=(0)1^{++}$ state, finding a minimal impact from the $\\delta(r)$\nterm. At this cutoff, both the $(0)0^{++}$ state near the $D\\bar D$ threshold\nand the $(0)1^{+-}$ state near the $D\\bar D^*$ threshold show less sensitivity\nto the $\\delta(r)$ term compared to the three states-$(0)0^{++}$, $(0)1^{+-}$,\nand $(0)2^{++}$-near the $D^*\\bar D^*$ threshold. As anticipated, the\n$B^{(*)}\\bar B^{*}$ systems exhibit similar behavior but with stronger binding\ndue to their larger reduced mass. These findings suggest promising directions\nfor future experimental searches, particularly in the isoscalar sector, which\ncould substantially advance our understanding of exotic tetraquark states.",
        "Let $G=GL(n)$ be the $n\\times n$ complex general linear group and let\n$\\mathcal{B}_{n}$ be its flag variety. The standard Borel subgroup $B$ of upper\ntriangular matrices acts on the product $\\mathcal{B}_{n}\\times\n\\mathbb{P}^{n-1}$ with finitely many orbits. In this paper, we study the\n$B$-orbits on the subvarieties $\\mathcal{B}_{n}\\times \\mathcal{O}_{i}$, where\n$\\mathcal{O}_{i}$ is the $B$-orbit on $\\mathbb{P}^{n-1}$ containing the line\nthrough the origin in the direction of the $i$-th standard basis vector of\n$\\mathbb{C}^{n}$. For each $i=1,\\dots, n$, we construct a bijection between\n$B$-orbits on $\\mathcal{B}_{n}\\times\\mathcal{O}_{i}$ and certain pairs of\nSchubert cells in $\\mathcal{B}_{n}\\times\\mathcal{B}_{n}$. We also show that\nthis bijection can be used to understand the Richardson-Springer monoid action\non such $B$-orbits in terms of the classical monoid action of the symmetric\ngroup on itself. We also develop combinatorial models of these orbits and use\nthese models to compute exponential generating functions for the sequences\n$\\{|B\\backslash(\\mathcal{B}_{n}\\times\\mathcal{O}_{i})|\\}_{n\\geq 1}$ and\n$\\{|B\\backslash (\\mathcal{B}_{n}\\times \\mathbb{P}^{n-1})|\\}_{n\\geq 1}$. In the\nsequel to this paper, we use the results of this paper to construct a\ncorrespondence between $B$-orbits on $\\mathcal{B}_{n}\\times\\mathbb{P}^{n-1}$\nand a collection of $B$-orbits on the flag variety $\\mathcal{B}_{n+1}$ of\n$GL(n+1)$ and show that this correspondence respects closures relations and\npreserves monoid actions. As a consequence both closure relations and monoid\nactions for all $B$-orbits on $\\mathcal{B}_{n}\\times\\mathbb{P}^{n-1}$ can be\nunderstood via the Bruhat order by using our results in [CE].",
        "Circular nim $CN(m, k)$ is a variant of nim, in which there are $m$ piles of\ntokens arranged in a circle and each player, in their turn, chooses at most $k$\nconsecutive piles in the circle and removes an arbitrary number of tokens from\neach pile. The player must remove at least one token in total. For some cases\nof $m$ and $k$, closed formulas to determine which player has a winning\nstrategy have been found. Almost all cases are still open problems. In this\npaper, we consider a variant of circular nim, extended circular nim. In\nextended circular nim $ECN(m_S, k)$, there are $m$ piles of tokes arranged in a\ncircle. $S$ is a set of positive integers less than or equal to half of $m$.\nEach player, in their turn, chooses at most $k$ piles selected every $s$-th\npile in a circle for an $s \\in S$. We find some closed formulas to determine\nwhich player has a winning strategy for the cases where the number of piles is\nno more than eight.",
        "Modeling extreme precipitation and temperature is vital for understanding the\nimpacts of climate change, as hazards like intense rainfall and record-breaking\ntemperatures can result in severe consequences, including floods, droughts, and\nwildfires. Gaining insight into the spatial variation and interactions between\nthese extremes is critical for effective risk management, early warning\nsystems, and informed policy-making. However, challenges such as the rarity of\nextreme events, spatial dependencies, and complex cross-variable interactions\nhinder accurate modeling. We introduce a novel framework for modeling spatial\nextremes, building upon spatial generalized extreme value (GEV) models. Our\napproach incorporates a dimension-reduced latent spatial process to improve\nscalability and flexibility, particularly in capturing asymmetry in\ncross-covariance structures. This Joint Latent Spatial GEV model (JLS-GEV)\novercomes key limitations of existing methods by providing a more flexible\nframework for inter-variable dependencies. In addition to addressing event\nrarity, spatial dependence and cross-variable interactions, JLS-GEV supports\nnonstationary spatial behaviors and independently collected data sources, while\nmaintaining practical fitting times through dimension reduction. We validate\nJLS-GEV through extensive simulation studies, demonstrating its superior\nperformance in capturing spatial extremes compared to baseline modeling\napproaches. Application to real-world data on extreme precipitation and\ntemperature in the southeastern United States highlights its practical utility.\nWhile primarily motivated by environmental challenges, this framework is\nbroadly applicable to interdisciplinary studies of spatial extremes in\ninterdependent natural processes.",
        "Photonic temporal crystals host a variety of intriguing phenomena, from wave\namplification and mixing to exotic band structures, all stemming from the\ntime-periodic modulation of optical properties. While these features have been\nwell described classically, their quantum manifestation has remained elusive.\nHere, we introduce a quantum electrodynamical model of PTCs that reveals a\ndeeper connection between classical and quantum pictures: the classical\nmomentum gap arises from a localization-delocalization quantum phase transition\nin a Floquet-photonic synthetic lattice. Leveraging an effective Hamiltonian\nperspective, we pinpoint the critical momenta and highlight how classical\nexponential field growth manifests itself as wave-packet acceleration in the\nquantum synthetic space. Remarkably, when a two-level atom is embedded in such\na cavity, its Rabi oscillations undergo irreversible decay to a half-and-half\nmixed state-a previously unobserved phenomenon driven by photonic\ndelocalization within the momentum gap, even with just a single frequency mode.\nOur findings establish photonic temporal crystals as versatile platforms for\nstudying nonequilibrium quantum photonics and suggest new avenues for\ncontrolling light matter interactions through time domain engineering.",
        "Interfaces play a substantial role for the functional properties of\nstructured magnetic materials and magnetic multilayers. Modeling the functional\nbehavior of magnetic materials requires the treatment of the relevant phenomena\nat the device level. Properties predicted from the electronic structure and\nspin dynamics at the atomistic level have to be properly transferred into a\ncontinuum level treatment. In this work we show how Co\/Ru\/Co three layers can\nbe simulated with the continuum theory of micromagnetism, with interface\ncoupling energies and bulk intrinsic properties properly derived from the\nresults of \\emph{ab initio} and spin dynamics simulations at different\ntemperatures."
      ]
    }
  },
  {
    "id":2411.03341,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"Highly multiplexed imaging of tumor tissues with subcellular resolution by mass cytometry",
    "start_abstract":"Mass cytometry enables high-dimensional, single-cell analysis of cell type and state. In mass cytometry, rare earth metals are used as reporters on antibodies. Analysis of metal abundances using the mass cytometer allows determination of marker expression in individual cells. Mass cytometry has previously been applied only to cell suspensions. To gain spatial information, we have coupled immunohistochemical and immunocytochemical methods with high-resolution laser ablation to CyTOF mass cytometry. This approach enables the simultaneous imaging of 32 proteins and protein modifications at subcellular resolution; with the availability of additional isotopes, measurement of over 100 markers will be possible. We applied imaging mass cytometry to human breast cancer samples, allowing delineation of cell subpopulations and cell-cell interactions and highlighting tumor heterogeneity. Imaging mass cytometry complements existing imaging approaches. It will enable basic studies of tissue heterogeneity and function and support the transition of medicine toward individualized molecularly targeted diagnosis and therapies.",
    "start_categories":[
      "q-bio.QM"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b11"
      ],
      "title":[
        "A ConvNet for the 2020s"
      ],
      "abstract":[
        "The \"Roaring 20s\" of visual recognition began with the introduction Vision Transformers (ViTs), which quickly superseded ConvNets as state-of-the-art image classification model. A vanilla ViT, on other hand, faces difficulties when applied to general computer vision tasks such object detection and semantic segmentation. It is hierarchical (e.g., Swin Transformers) that reintroduced several ConvNet priors, making practically viable a generic backbone demonstrating remarkable performance wide variety tasks. However, effectiveness hybrid approaches still largely credited intrinsic superiority Transformers, rather than inherent inductive biases convolutions. In this work, we reexamine design spaces test limits what pure can achieve. We gradually \"modernize\" standard ResNet toward Transformer, discover key components contribute difference along way. outcome exploration family models dubbed ConvNeXt. Constructed entirely from modules, ConvNeXts compete favorably in terms accuracy scalability, achieving 87.8% ImageNet top-1 outperforming COCO ADE20K segmentation, while maintaining simplicity efficiency ConvNets."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "Intrinsic Donaldson-Thomas theory. I. Component lattices of stacks",
        "Ferromagnetic Resonance in a Magnetically Dilute Percolating\n  Ferromagnet: An Experimental and Theoretical Study",
        "Towards Enterprise-Ready Computer Using Generalist Agent",
        "Maps from Grassmannians of 2-planes to projective spaces",
        "Approaching the Limits to EFL Writing Enhancement with AI-generated Text\n  and Diverse Learners",
        "Sensitivity of Double Deeply Virtual Compton Scattering observables to\n  Generalized Parton Distributions",
        "The EnviroMapper Toolkit: an Input Physicalisation that Captures the\n  Situated Experience of Environmental Comfort in Offices",
        "VIRGOS: Secure Graph Convolutional Network on Vertically Split Data from\n  Sparse Matrix Decomposition",
        "Shifting Power: Leveraging LLMs to Simulate Human Aversion in ABMs of\n  Bilateral Financial Exchanges, A bond market study",
        "Post-Quantum Stealth Address Protocols",
        "MinD the gap: Membrane proteins form 3D patterns in a suspension of\n  liposomes",
        "Causes and Strategies in Multiagent Systems",
        "Hybrid MIMO in the Upper Mid-Band: Architectures, Processing, and Energy\n  Efficiency",
        "Unit Edge-Length Rectilinear Drawings with Crossings and Rectangular\n  Faces",
        "DBRouting: Routing End User Queries to Databases for Answerability",
        "Towards Unified Structured Light Optimization",
        "AI\/ML Based Detection and Categorization of Covert Communication in IPv6\n  Network",
        "DreamInsert: Zero-Shot Image-to-Video Object Insertion from A Single\n  Image",
        "The Role of Legal Frameworks in Shaping Ethical Artificial Intelligence\n  Use in Corporate Governance",
        "A Survey of Theory of Mind in Large Language Models: Evaluations,\n  Representations, and Safety Risks",
        "Quantum interference with time-frequency modes and multiple-photons\n  generated by a silicon nitride microresonator",
        "Split-Aperture Phased Array Radar Resource Management for Tracking Tasks",
        "The Battling Influencers Game: Nash Equilibria Structure of a Potential\n  Game and Implications to Value Alignment",
        "Hierarchical Neuro-Symbolic Decision Transformer",
        "Simultaneous Energy Harvesting and Bearing Fault Detection using\n  Piezoelectric Cantilevers",
        "Data Fusion for Full-Range Response Reconstruction via Diffusion Models",
        "A Non-Monotone Line-Search Method for Minimizing Functions with Spurious\n  Local Minima",
        "Adaptive finite element approximation for quasi-static crack growth",
        "A Fresh Perspective on Water Dynamics in Aqueous Salt Solutions"
      ],
      "abstract":[
        "This is the first paper in a series on intrinsic Donaldson-Thomas theory, a\ngeneralization of Donaldson-Thomas theory from the linear case, or the case of\nmoduli stacks of objects in $3$-Calabi-Yau abelian categories, to the\nnon-linear case of general $(-1)$-shifted symplectic stacks. This is done by\ndeveloping a new framework for studying the enumerative geometry of general\nalgebraic stacks, and we expect that this framework can also be applied to\nextending other types of enumerative theories for linear stacks to the\nnon-linear case.\n  In this paper, we establish the foundations of our framework. We introduce\nthe component lattice of an algebraic stack, which is the key combinatorial\nobject in our theory. It generalizes and globalizes the cocharacter lattice and\nthe Weyl group of an algebraic group, and is defined as the set of connected\ncomponents of the stack of graded points of the original stack.\n  We prove several results on the structure of graded and filtered points of a\nstack using the component lattice. The first is the constancy theorem, which\nstates that there is a wall-and-chamber structure on the component lattice,\nsuch that the isomorphism types of connected components of the stacks of graded\nand filtered points stay constant within each chamber. The second is the\nfiniteness theorem, providing a criterion for the finiteness of the number of\npossible isomorphism types of these components. The third is the associativity\ntheorem, generalizing the structure of Hall algebras from linear stacks to\ngeneral stacks, involving a notion of Hall categories.\n  Finally, we discuss some applications of these results outside\nDonaldson-Thomas theory, including a construction of stacks of real-weighted\nfiltrations, and a generalization of the semistable reduction theorem to\nreal-weighted filtrations.",
        "Ferromagnetic resonance (FMR) serves as a powerful probe of magnetization\ndynamics and anisotropy in percolating ferromagnets, where short-range\ninteractions govern long-range magnetic order. We apply this approach to\nGa$_{1-x}$Mn$_x$N ($x \\simeq 8$), a dilute ferromagnetic semiconductor,\ncombining FMR and superconducting quantum interference device magnetometry. Our\nresults confirm the percolative nature of ferromagnetism in (Ga,Mn)N, with a\nCurie temperature $T_{\\mathrm{C}} = 12$ K, and reveal that despite magnetic\ndilution, key features of conventional ferromagnets are retained. FMR\nmeasurements establish a robust uniaxial anisotropy, dictated by Mn$^{3+}$\nsingle-ion anisotropy, with an easy-plane character at low Mn content. While\nexcessive line broadening suppresses FMR signals below 9 K, they persist up to\n70 K, indicating the presence of non-percolating ferromagnetic clusters well\nabove $T_{\\mathrm{C}}$. The temperature dependence of the FMR intensity follows\nthat of the magnetization, underscoring the stability of these clusters.\nAnalysis of the FMR linewidth provides insights into relaxation processes,\nrevealing large Gilbert damping due to the low magnetization of the system.\nStrikingly, atomistic spin model simulations reproduce the experimentally\nobserved resonance fields, anisotropy trends, and linewidth evolution with\nremarkable accuracy. This agreement underscores the predictive power of our\nmodeling approach in describing percolating ferromagnets. This study advances\nthe understanding of percolating ferromagnetic systems, demonstrating that FMR\nis a key technique for probing their unique dynamic and anisotropic properties.\nOur findings contribute to the broader exploration of dilute ferromagnets and\nprovide new insights into percolating ferromagnetic systems, which will be\nrelevant for spintronic opportunities.",
        "This paper presents our ongoing work toward developing an enterprise-ready\nComputer Using Generalist Agent (CUGA) system. Our research highlights the\nevolutionary nature of building agentic systems suitable for enterprise\nenvironments. By integrating state-of-the-art agentic AI techniques with a\nsystematic approach to iterative evaluation, analysis, and refinement, we have\nachieved rapid and cost-effective performance gains, notably reaching a new\nstate-of-the-art performance on the WebArena benchmark. We detail our\ndevelopment roadmap, the methodology and tools that facilitated rapid learning\nfrom failures and continuous system refinement, and discuss key lessons learned\nand future challenges for enterprise adoption.",
        "Using quaternions and octonions, we construct some maps from the Grassmannian\nof 2-dimensional planes of $\\mathbb{R}^n$, $\\mathrm{Gr}_2(\\mathbb{R}^n)$, to\nthe projective space $\\mathbb{R}\\mathrm{P}^k$, for certain values of $n$ and\n$k$. All of our maps induce an isomorphism at the level of fundamental groups,\nand two of them are shown to be submersions. As an application, we obtain new\nestimates of the Lusternik-Schnirelmann category of\n$\\mathrm{Gr}_2(\\mathbb{R}^n)$ for specific values of $n$.",
        "Generative artificial intelligence (AI) chatbots, such as ChatGPT, are\nreshaping how English as a foreign language (EFL) students write since students\ncan compose texts by integrating their own words with AI-generated text. This\nstudy investigated how 59 Hong Kong secondary school students with varying\nlevels of academic achievement interacted with AI-generated text to compose a\nfeature article, exploring whether any interaction patterns benefited the\noverall quality of the article. Through content analysis, multiple linear\nregression and cluster analysis, we found the overall number of words --\nwhether AI- or human-generated -- is the main predictor of writing quality.\nHowever, the impact varies by students' competence to write independently, for\ninstance, by using their own words accurately and coherently to compose a text,\nand to follow specific interaction patterns with AI-generated text. Therefore,\nalthough composing texts with human words and AI-generated text may become\nprevalent in EFL writing classrooms, without educators' careful attention to\nEFL writing pedagogy and AI literacy, high-achieving students stand to benefit\nmore from using AI-generated text than low-achieving students.",
        "Double Deeply Virtual Compton Scattering (DDVCS) is a promising channel for\nGeneralized Parton Distribution (GPD) studies as it is a generalization of the\nDeeply Virtual Compton Scattering (DVCS) and Timelike Compton Scattering (TCS)\nprocesses. Contrary to DVCS and TCS, the GPD phase space accessed through DDVCS\nis not constrained by on-shell conditions on the incoming and outgoing photons\nthus allowing unrestricted GPD extraction from experimental observables.\nConsidering polarized electron and positron beams directed to a polarized\nproton target, we study the sensitivity of the DDVCS cross-section asymmetries\nto the chiral-even proton GPDs from different model predictions. The\nfeasibility of such measurements is further investigated in the context of the\nCLAS and SoLID spectrometers at the Thomas Jefferson National Accelerator\nFacility and the future Electron-Ion Collider at the Brookhaven National\nLaboratory.",
        "The environmental comfort in offices is traditionally captured by surveying\nan entire workforce simultaneously, which yet fails to capture the situatedness\nof the different personal experiences. To address this limitation, we developed\nthe EnviroMapper Toolkit, a data physicalisation toolkit that allows individual\noffice workers to record their personal experiences of environmental comfort by\nmapping the actual moments and locations these occurred. By analysing two\nin-the-wild studies in existing open-plan office environments (N=14), we\ndemonstrate how this toolkit acts like a situated input visualisation that can\nbe interpreted by domain experts who were not present during its construction.\nThis study therefore offers four key contributions: (1) the iterative design\nprocess of the physicalisation toolkit; (2) its preliminary deployment in two\nreal-world office contexts; (3) the decoding of the resulting artefacts by\ndomain experts; and (4) design considerations to support future input\nphysicalisation and visualisation constructions that capture and synthesise\ndata from multiple individuals.",
        "Securely computing graph convolutional networks (GCNs) is critical for\napplying their analytical capabilities to privacy-sensitive data like\nsocial\/credit networks. Multiplying a sparse yet large adjacency matrix of a\ngraph in GCN--a core operation in training\/inference--poses a performance\nbottleneck in secure GCNs. Consider a GCN with $|V|$ nodes and $|E|$ edges; it\nincurs a large $O(|V|^2)$ communication overhead. Modeling bipartite graphs and\nleveraging the monotonicity of non-zero entry locations, we propose a co-design\nharmonizing secure multi-party computation (MPC) with matrix sparsity. Our\nsparse matrix decomposition transforms an arbitrary sparse matrix into a\nproduct of structured matrices. Specialized MPC protocols for oblivious\npermutation and selection multiplication are then tailored, enabling our secure\nsparse matrix multiplication ($(SM)^2$) protocol, optimized for secure\nmultiplication of these structured matrices. Together, these techniques take\n$O(|E|)$ communication in constant rounds. Supported by $(SM)^2$, we present\nVirgos, a secure 2-party framework that is communication-efficient and\nmemory-friendly on standard vertically-partitioned graph datasets. Performance\nof Virgos has been empirically validated across diverse network conditions.",
        "Bilateral markets, such as those for government bonds, involve decentralized\nand opaque transactions between market makers (MMs) and clients, posing\nsignificant challenges for traditional modeling approaches. To address these\ncomplexities, we introduce TRIBE an agent-based model augmented with a large\nlanguage model (LLM) to simulate human-like decision-making in trading\nenvironments. TRIBE leverages publicly available data and stylized facts to\ncapture realistic trading dynamics, integrating human biases like risk aversion\nand ambiguity sensitivity into the decision-making processes of agents. Our\nresearch yields three key contributions: first, we demonstrate that integrating\nLLMs into agent-based models to enhance client agency is feasible and enriches\nthe simulation of agent behaviors in complex markets; second, we find that even\nslight trade aversion encoded within the LLM leads to a complete cessation of\ntrading activity, highlighting the sensitivity of market dynamics to agents'\nrisk profiles; third, we show that incorporating human-like variability shifts\npower dynamics towards clients and can disproportionately affect the entire\nsystem, often resulting in systemic agent collapse across simulations. These\nfindings underscore the emergent properties that arise when introducing\nstochastic, human-like decision processes, revealing new system behaviors that\nenhance the realism and complexity of artificial societies.",
        "The Stealth Address Protocol (SAP) allows users to receive assets through\nstealth addresses that are unlinkable to their stealth meta-addresses. The most\nwidely used SAP, Dual-Key SAP (DKSAP), and the most performant SAP, Elliptic\nCurve Pairing Dual-Key SAP (ECPDKSAP), are based on elliptic curve\ncryptography, which is vulnerable to quantum attacks. These protocols depend on\nthe elliptic curve discrete logarithm problem, which could be efficiently\nsolved on a sufficiently powerful quantum computer using the Shor algorithm. In\nthis paper three novel post-quantum SAPs based on lattice-based cryptography\nare presented: LWE SAP, Ring-LWE SAP and Module-LWE SAP. These protocols\nleverage Learning With Errors (LWE) problem to ensure quantum-resistant\nprivacy. Among them, Module-LWE SAP, which is based on the Kyber key\nencapsulation mechanism, achieves the best performance and outperforms ECPDKSAP\nby approximately 66.8% in the scan time of the ephemeral public key registry.",
        "The self-organization of pattern-forming systems depends not only on the\nchemical but also physical properties of their components. In this work, we\nfragmented and dispersed the MinDE protein system's lipid substrate into\ndiffusive sub-micrometer-sized liposomes, and report that the ATP-fueled\nprotein-protein interactions continue to drive spatially extended patterns at\nscales well separated from those of the requisite liposomes, despite the\ncomplete loss of membrane continuity. The patterns form in three-dimensions\nbecause the membrane is dispersed in a volume. By varying protein\nconcentration, liposome size distribution, and density, we observed and\ncharacterized rich 3D dynamical patterns at steady state, including traveling\nwaves, dynamical spirals and a mixed phase where both patterns coexist.\nSimulations and linear stability analysis of a coarse-grained model reveal that\nthe dispersed membranes's physical properties effectively rescale two key\nfactors that govern pattern formation and wavelength selection:\nprotein-membrane binding rates and diffusion. This work highlights the\nrobustness of pattern formation in membrane-bulk systems despite membrane\nfragmentation. It suggests that biological protein systems have the potential\nto serve as adaptable templates for out-of-equilibrium self-organization in 3D,\nbeyond in vivo biological contexts.",
        "Causality plays an important role in daily processes, human reasoning, and\nartificial intelligence. There has however not been much research on causality\nin multi-agent strategic settings. In this work, we introduce a systematic way\nto build a multi-agent system model, represented as a concurrent game\nstructure, for a given structural causal model. In the obtained so-called\ncausal concurrent game structure, transitions correspond to interventions on\nagent variables of the given causal model. The Halpern and Pearl framework of\ncausality is used to determine the effects of a certain value for an agent\nvariable on other variables. The causal concurrent game structure allows us to\nanalyse and reason about causal effects of agents' strategic decisions. We\nformally investigate the relation between causal concurrent game structures and\nthe original structural causal models.",
        "As 6G networks evolve, the upper mid-band spectrum (7 GHz to 24 GHz), or\nfrequency range 3 (FR3), is emerging as a promising balance between the\ncoverage offered by sub-6 GHz bands and the high-capacity of millimeter wave\n(mmWave) frequencies. This paper explores the structure of FR3 hybrid MIMO\nsystems and proposes two architectural classes: Frequency Integrated (FI) and\nFrequency Partitioned (FP). FI architectures enhance spectral efficiency by\nexploiting multiple sub-bands parallelism, while FP architectures dynamically\nallocate sub-band access according to specific application requirements.\nAdditionally, two approaches, fully digital (FD) and hybrid analog-digital\n(HAD), are considered, comparing shared (SRF) versus dedicated RF (DRF) chain\nconfigurations. Herein signal processing solutions are investigated,\nparticularly for an uplink multi-user scenario with power control optimization.\n  Results demonstrate that SRF and DRF architectures achieve comparable\nspectral efficiency; however, SRF structures consume nearly half the power of\nDRF in the considered setup. While FD architectures provide higher spectral\nefficiency, they do so at the cost of increased power consumption compared to\nHAD. Additionally, FI architectures show slightly greater power consumption\ncompared to FP; however, they provide a significant benefit in spectral\nefficiency (over 4 x), emphasizing an important trade-off in FR3 engineering.",
        "Unit edge-length drawings, rectilinear drawings (where each edge is either a\nhorizontal or a vertical segment), and rectangular face drawings are among the\nmost studied subjects in Graph Drawing. However, most of the literature on\nthese topics refers to planar graphs and planar drawings. In this paper we\nstudy drawings with all the above nice properties but that can have edge\ncrossings; we call them Unit Edge length Rectilinear drawings with Rectangular\nFaces (UER-RF drawings). We consider crossings as dummy vertices and apply the\nunit edge-length convention to the edge segments connecting any two (real or\ndummy) vertices. Note that UER-RF drawings are grid drawings (vertices are\nplaced at distinct integer coordinates), which is another classical requirement\nof graph visualizations. We present several efficient and easily implementable\nalgorithms for recognizing graphs that admit UER-RF drawings and for\nconstructing such drawings if they exist. We consider restrictions on the\ndegree of the vertices or on the size of the faces. For each type of\nrestriction, we consider both the general unconstrained setting and a setting\nin which either the external boundary of the drawing is fixed or the rotation\nsystem of the graph is fixed as part of the input.",
        "Enterprise level data is often distributed across multiple sources and\nidentifying the correct set-of data-sources with relevant information for a\nknowledge request is a fundamental challenge. In this work, we define the novel\ntask of routing an end-user query to the appropriate data-source, where the\ndata-sources are databases. We synthesize datasets by extending existing\ndatasets designed for NL-to-SQL semantic parsing. We create baselines on these\ndatasets by using open-source LLMs, using both pre-trained and task specific\nembeddings fine-tuned using the training data. With these baselines we\ndemonstrate that open-source LLMs perform better than embedding based approach,\nbut suffer from token length limitations. Embedding based approaches benefit\nfrom task specific fine-tuning, more so when there is availability of data in\nterms of database specific questions for training. We further find that the\ntask becomes more difficult (i) with an increase in the number of data-sources,\n(ii) having data-sources closer in terms of their domains,(iii) having\ndatabases without external domain knowledge required to interpret its entities\nand (iv) with ambiguous and complex queries requiring more fine-grained\nunderstanding of the data-sources or logical reasoning for routing to an\nappropriate source. This calls for the need for developing more sophisticated\nsolutions to better address the task.",
        "Structured light (SL) 3D reconstruction captures the precise surface shape of\nobjects, providing high-accuracy 3D data essential for industrial inspection\nand robotic vision systems. However, current research on optimizing projection\npatterns in SL 3D reconstruction faces two main limitations: each scene\nrequires separate training of calibration parameters, and optimization is\nrestricted to specific types of SL, which restricts their application range. To\ntackle these limitations, we present a unified framework for SL optimization,\nadaptable to diverse lighting conditions, object types, and different types of\nSL. Our framework quickly determines the optimal projection pattern using only\na single projected image. Key contributions include a novel global matching\nmethod for projectors, enabling precise projector-camera alignment with just\none projected image, and a new projection compensation model with a photometric\nadjustment module to reduce artifacts from out-of-gamut clipping. Experimental\nresults show our method achieves superior decoding accuracy across various\nobjects, SL patterns, and lighting conditions, significantly outperforming\nprevious methods.",
        "The flexibility and complexity of IPv6 extension headers allow attackers to\ncreate covert channels or bypass security mechanisms, leading to potential data\nbreaches or system compromises. The mature development of machine learning has\nbecome the primary detection technology option used to mitigate covert\ncommunication threats. However, the complexity of detecting covert\ncommunication, evolving injection techniques, and scarcity of data make\nbuilding machine-learning models challenging. In previous related research,\nmachine learning has shown good performance in detecting covert communications,\nbut oversimplified attack scenario assumptions cannot represent the complexity\nof modern covert technologies and make it easier for machine learning models to\ndetect covert communications. To bridge this gap, in this study, we analyzed\nthe packet structure and network traffic behavior of IPv6, used encryption\nalgorithms, and performed covert communication injection without changing\nnetwork packet behavior to get closer to real attack scenarios. In addition to\nanalyzing and injecting methods for covert communications, this study also uses\ncomprehensive machine learning techniques to train the model proposed in this\nstudy to detect threats, including traditional decision trees such as random\nforests and gradient boosting, as well as complex neural network architectures\nsuch as CNNs and LSTMs, to achieve detection accuracy of over 90\\%. This study\ndetails the methods used for dataset augmentation and the comparative\nperformance of the applied models, reinforcing insights into the adaptability\nand resilience of the machine learning application in IPv6 covert\ncommunication. In addition, we also proposed a Generative AI-assisted\ninterpretation concept based on prompt engineering as a preliminary study of\nthe role of Generative AI agents in covert communication.",
        "Recent developments in generative diffusion models have turned many dreams\ninto realities. For video object insertion, existing methods typically require\nadditional information, such as a reference video or a 3D asset of the object,\nto generate the synthetic motion. However, inserting an object from a single\nreference photo into a target background video remains an uncharted area due to\nthe lack of unseen motion information. We propose DreamInsert, which achieves\nImage-to-Video Object Insertion in a training-free manner for the first time.\nBy incorporating the trajectory of the object into consideration, DreamInsert\ncan predict the unseen object movement, fuse it harmoniously with the\nbackground video, and generate the desired video seamlessly. More\nsignificantly, DreamInsert is both simple and effective, achieving zero-shot\ninsertion without end-to-end training or additional fine-tuning on\nwell-designed image-video data pairs. We demonstrated the effectiveness of\nDreamInsert through a variety of experiments. Leveraging this capability, we\npresent the first results for Image-to-Video object insertion in a\ntraining-free manner, paving exciting new directions for future content\ncreation and synthesis. The code will be released soon.",
        "This article examines the evolving role of legal frameworks in shaping\nethical artificial intelligence (AI) use in corporate governance. As AI systems\nbecome increasingly prevalent in business operations and decision-making, there\nis a growing need for robust governance structures to ensure their responsible\ndevelopment and deployment. Through analysis of recent legislative initiatives,\nindustry standards, and scholarly perspectives, this paper explores key legal\nand regulatory approaches aimed at promoting transparency, accountability, and\nfairness in corporate AI applications. It evaluates the strengths and\nlimitations of current frameworks, identifies emerging best practices, and\noffers recommendations for developing more comprehensive and effective AI\ngovernance regimes. The findings highlight the importance of adaptable,\nprinciple-based regulations coupled with sector-specific guidance to address\nthe unique challenges posed by AI technologies in the corporate sphere.",
        "Theory of Mind (ToM), the ability to attribute mental states to others and\npredict their behaviour, is fundamental to social intelligence. In this paper,\nwe survey studies evaluating behavioural and representational ToM in Large\nLanguage Models (LLMs), identify important safety risks from advanced LLM ToM\ncapabilities, and suggest several research directions for effective evaluation\nand mitigation of these risks.",
        "We demonstrate bipartite gaussian boson sampling with squeezed light in 6\nmixed time-frequency modes. Non-degenerate two-mode squeezing is generated in\ntwo time-bins from a silicon nitride microresonator with simultaneous high\nspectral purity (>0.86(3)) and indistinguishability (0.985(2)). An unbalanced\ninterferometer embedding electro-optic modulators, which is stabilized by\nexploiting the continuous energy-time entanglement of the generated photon\npairs, controls time and frequency-bin modes. We measure 144 collision-free\nevents with 4 photons at the output, achieving a fidelity >0.98 with the\ntheoretical probability distribution. We use this result to identify the\nsimilarity between families of isomorphic graphs with 6 vertices, and present\nan approach for the realization of universal operations on time-frequency\nmodes.",
        "The next generation of radar systems will include advanced digital front-end\ntechnology in the apertures allowing for spatially subdividing radar tasks over\nthe array, the so-called split-aperture phased array (SAPA) concept. The goal\nof this paper is to introduce radar resource management for the SAPA concept\nand to demonstrate the added benefit of the SAPA concept for active tracking\ntasks. To do so, the radar resource management problem is formulated and solved\nby employing the quality of service based resource allocation model (Q-RAM)\nframework. As active tracking tasks may be scheduled simultaneously, the\nresource allocation of tasks becomes dependent on the other tasks. The solution\nto the resource allocation problem is obtained by introducing the adaptive fast\ntraversal algorithm combined with a three dimensional strip packing algorithm\nto handle task dependencies. It will be demonstrated by a simulation example\nthat the SAPA concept can significantly increase the number of active tracks of\na multifunction radar system compared to scheduling tasks sequentially.",
        "When multiple influencers attempt to compete for a receiver's attention,\ntheir influencing strategies must account for the presence of one another. We\nintroduce the Battling Influencers Game (BIG), a multi-player simultaneous-move\ngeneral-sum game, to provide a game-theoretic characterization of this social\nphenomenon. We prove that BIG is a potential game, that it has either one or an\ninfinite number of pure Nash equilibria (NEs), and these pure NEs can be found\nby convex optimization. Interestingly, we also prove that at any pure NE, all\n(except at most one) influencers must exaggerate their actions to the maximum\nextent. In other words, it is rational for the influencers to be non-truthful\nand extreme because they anticipate other influencers to cancel out part of\ntheir influence. We discuss the implications of BIG to value alignment.",
        "We present a hierarchical neuro-symbolic control framework that couples\nclassical symbolic planning with transformer-based policies to address complex,\nlong-horizon decision-making tasks. At the high level, a symbolic planner\nconstructs an interpretable sequence of operators based on logical\npropositions, ensuring systematic adherence to global constraints and goals. At\nthe low level, each symbolic operator is translated into a sub-goal token that\nconditions a decision transformer to generate a fine-grained sequence of\nactions in uncertain, high-dimensional environments. We provide theoretical\nanalysis showing how approximation errors from both the symbolic planner and\nthe neural execution layer accumulate. Empirical evaluations in grid-worlds\nwith multiple keys, locked doors, and item-collection tasks show that our\nhierarchical approach outperforms purely end-to-end neural approach in success\nrates and policy efficiency.",
        "Bearings are critical components in industrial machinery, yet their\nvulnerability to faults often leads to costly breakdowns. Conventional fault\ndetection methods depend on continuous, high-frequency vibration sensing,\ndigitising, and wireless transmission to the cloud-an approach that\nsignificantly drains the limited energy reserves of battery-powered sensors,\naccelerating their depletion and increasing maintenance costs. This work\nproposes a fundamentally different approach: rather than using instantaneous\nvibration data, we employ piezoelectric energy harvesters (PEHs) tuned to\nspecific frequencies and leverage the cumulative harvested energy over time as\nthe key diagnostic feature. By directly utilising the energy generated from the\nmachinery's vibrations, we eliminate the need for frequent analog-to-digital\nconversions and data transmission, thereby reducing energy consumption at the\nsensor node and extending its operational lifetime. To validate this approach,\nwe use a numerical PEH model and publicly available acceleration datasets,\nexamining various PEH designs with different natural frequencies. We also\nconsider the influence of the classification algorithm, the number of devices,\nand the observation window duration. The results demonstrate that the harvested\nenergy reliably indicates bearing faults across a range of conditions and\nseverities. By converting vibration energy into both a power source and a\ndiagnostic feature, our solution offers a more sustainable, low-maintenance\nstrategy for fault detection in smart machinery.",
        "Accurately capturing the full-range response of structures is crucial in\nstructural health monitoring (SHM) for ensuring safety and operational\nintegrity. However, limited sensor deployment due to cost, accessibility, or\nscale often hinders comprehensive monitoring. This paper presents a novel data\nfusion framework utilizing diffusion models to reconstruct the full-range\nstructural response from sparse and heterogeneous sensor measurements. We\nincorporate Diffusion Posterior Sampling (DPS) into the reconstruction\nframework, using sensor measurements as probabilistic constraints to guide the\nsampling process. A lightweight neural network serves as the surrogate forward\nmodel within the DPS algorithm, which maps full-range structural responses to\nlocal sensor data. This approach enables flexibility in sensor configurations\nwhile reducing computational costs. The proposed framework is validated on a\nsteel plate shear wall exhibiting nonlinear responses. Comparative experiments\nare conducted with three forward models. Among these, the neural network\nsurrogate model achieves a desirable reconstruction accuracy, with a weighted\nmean absolute percentage error (WMAPE) as low as 1.57%, while also\ndemonstrating superior adaptability and computational efficiency. Additional\nexperiments explore the impact of sensor placement strategies and noise levels.\nResults show that even under sparse measurements or high noise conditions, the\nWMAPE remains capped at 15%, demonstrating the robustness in challenging\nscenarios. The proposed framework shows new possibilities for probabilistic\nmodeling and decision-making in SHM, offering a novel data fusion approach for\nfull-range monitoring of structures.",
        "In this paper, we propose a new non-monotone line-search method for smooth\nunconstrained optimization problems with objective functions that have many\nnon-global local minimizers. The method is based on a relaxed Armijo condition\nthat allows a controllable increase in the objective function between\nconsecutive iterations. This property helps the iterates escape from nearby\nlocal minimizers in the early iterations. For objective functions with\nLipschitz continuous gradients, we derive worst-case complexity estimates on\nthe number of iterations needed for the method to find approximate stationary\npoints. Numerical results are presented, showing that the new method can\nsignificantly outperform other non-monotone methods on functions with spurious\nlocal minima.",
        "We provide an adaptive finite element approximation for a model of\nquasi-static crack growth in dimension two. The discrete setting consists of\nintegral functionals that are defined on continuous, piecewise affine\nfunctions, where the triangulation is a part of the unknown of the problem and\nadaptive in each minimization step. The limit passage is conducted\nsimultaneously in the vanishing mesh size and discretized time step, and\nresults in an evolution for the continuum Griffith model of brittle fracture\nwith isotropic surface energy [FriedrichSolombrino16] which is characterized by\nan irreversibility condition, a global stability, and an energy balance. Our\nresult corresponds to an evolutionary counterpart of the static\nGamma-convergence result in [BabadjianBonhomme23] for which, as a byproduct, we\nprovide an alternative proof.",
        "Molecular dynamics in pure water and aqueous salt solutions remain\nincompletely understood, partly due to the apparent contradictions between\nresults from different spectroscopic techniques. In this work, we demonstrate,\nby detailed comparison of light scattering and dielectric spectroscopy data for\npure water and aqueous lithium chloride solutions, that these apparent\ncontradictions can be resolved by accounting for orientational\ncross-correlations of neighboring molecules. Remarkably, a single structural\nrelaxation mode with largely temperature- and concentration-independent shape\ncan be identified in all spectra, from room temperature down to the deeply\nsupercooled regime. These results provide a new perspective for the study of\nmolecular dynamics in aqueous salt solutions."
      ]
    }
  }
]