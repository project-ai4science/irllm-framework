[
  {
    "id":2411.18141,
    "research_type":"applied",
    "start_id":"b18",
    "start_title":"Quantum machine learning in chemistry and materials",
    "start_abstract":"Within the past few years, we have witnessed the rising of quantum machine learning (QML) models which infer electronic properties of molecules and materials, rather than solving approximations to the electronic Schr\u00f6dinger equation. The increasing availability of large quantum mechanics reference datasets has enabled these developments. We review the basic theories and key ingredients of popular QML models such as choice of regressor, data of varying trustworthiness, the role of the representation, and the effect of training set selection. Throughout we emphasize the indispensable role of learning curves when it comes to the comparative assessment of different QML models.",
    "start_categories":[
      "cs.ET"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b26"
      ],
      "title":[
        "Durban's water wars, sewage spills, fish kills and blue flag beaches. Durban's Climate Gamble"
      ],
      "abstract":[
        "Water is one of the primary barometers of climate change: A rise in sea-levels, flooding, and extreme storms combined with general water stress and more severe and frequent droughts will escalate crises in municipal infrastructure, requiring continual upgrades for water purification, stormwater drainage, and sewage treatment, all of which will dramatically raise the price of water at the retail level. In South Africa, the dry western side will be most adversely affected by droughts (threatening the production of rooibos tea and Cape wines). According to the Academy of Science in South Africa (ASSAf), Durban is also at great risk and will experience higher temperatures and heat stress, volatile rainfall, up to 160 million cubic metres less water each year by 2100, a sea-level rise of up to a metre by 2100 across Durban\u2019s 100 km of developed coastline, lower biodiversity, higher disease levels (especially malaria and cholera), declining agricultural output (a one degree Celsius rise leaves the surrounding region unreliable for the staple maize production), and other economic stresses (ASSAf 2011: 27). Tourism, one of Durban\u2019s main economic engines, will be irreparably harmed. Swimmers and surfers think of Durban\u2019s beachfront as one of the world\u2019s finest in any urban context. After apartheid-era rules that prohibited black people from using the best beaches were lifted at the end of the 1980s, the area stretching from the Blue Lagoon\u2019s Umgeni River to South Beach\u2019s uShaka Marine World\u2013including the immensely popular North Beach area near the main restaurant strip\u2013represented one of South Africa\u2019s most impressive, open and democratic public spaces."
      ],
      "categories":[
        "cond-mat.mtrl-sci"
      ]
    },
    "list":{
      "title":[
        "Effects of altermagnetic order, strain and doping on the optical and\n  vibrational properties of RuO$_2$",
        "Three-Dimensional to Layered Halide Perovskites: A Parameter-Free Hybrid\n  Functional Method for Predicting Electronic Band Gaps",
        "Optimizing Lead-Free Chalcogenide Perovskites for High-Efficiency\n  Photovoltaics via Alloying Strategies",
        "Unraveling phase transformation with phononic hyperbolicity using\n  off-resonant terahertz light",
        "Galvanic molecular intercalation",
        "Quantum simulations of defects near the (0001) surface of\n  $\\alpha$-Al$_2$O$_3$",
        "Si-compatible topological and infrared materials: the promise of Low-Sn\n  GeSn digital alloys",
        "Modeling solute-grain boundary interactions in a bcc Ti-Mo alloy using\n  density functional theory",
        "Giant non-reciprocal band structure effect in a multiferroic material",
        "Computational Study of Magnetic Behaviour in Ni-Adsorbed Nb2C-OF MXene\n  using Density Functional Theory",
        "Uncertainty Quantification for Misspecified Machine Learned Interatomic\n  Potentials",
        "Sliding ferroelectric control of unconventional magnetism in stacked\n  bilayers",
        "Resolving the sodiation process in hard carbon anodes with nanostructure\n  specific X-ray imaging",
        "Universality for catalytic equations and fully parked trees",
        "A Production Routing Problem with Mobile Inventories",
        "Wikipedia Contributions in the Wake of ChatGPT",
        "Gaussian credible intervals in Bayesian nonparametric estimation of the\n  unseen",
        "Identifying rich clubs in spatiotemporal interaction networks",
        "First-principle based Floquet engineering of solids in the velocity\n  gauge",
        "A Peanut-hull-PLA based 3D printing filament with antimicrobial effect",
        "Non-reciprocity and multibody interactions in acoustically levitated\n  particle systems: A three body problem",
        "On a reaction-diffusion virus model with general boundary conditions in\n  heterogeneous environments",
        "Schmid-Higgs Mode in the Presence of Pair-Breaking Interactions",
        "Thom polynomials for singularities of maps",
        "$\\texttt{PrecisionLauricella}$: package for numerical computation of\n  Lauricella functions depending on a parameter",
        "Unidentified Aerial Phenomena. Characterization of Dark UAPs",
        "Accumulation of Charge on an Extremal Black Hole's Event Horizon",
        "Expression of special stretched $9j$ coefficients in terms of $_5F_4$\n  hypergeometric series"
      ],
      "abstract":[
        "RuO$_2$, one of the most widely studied transition metal oxides, was recently\npredicted to host a novel form of collinear magnetic order referred to as\naltermagnetism. In this study we combine experiment (reflectance,\ntransmittance, ellipsometry and Raman measurements) and first-principles\ncalculations to elucidate the potential role of altermagnetic order, strain and\ndoping on the optical and vibrational properties of RuO$_2$ grown on TiO$_2$\n(001), (101) and (110) substrates. The combination of experiment and theory in\nthis study surprisingly indicates RuO$_2$ is in fact best described if one\nassumes the nonmagnetic state. Calculations of the altermagnetic state leads to\npoor agreement with the measured optical and vibrational properties of RuO$_2$.",
        "Accurately predicting electronic band gaps in halide perovskites using ab\ninitio density functional theory (DFT) is essential for their application in\noptoelectronic devices. Standard hybrid functionals such as HSE and PBE0 can\novercome the limitations of DFT with reasonable computational cost but are\nknown to underestimate the measured band gaps for layered halide perovskites.\nIn this study, we assess the performance of the doubly screened\ndielectric-dependent hybrid (DSH) functional for predicting band gaps in\nthree-dimensional (3D) and layered hybrid perovskites. We show that the DSH\nfunctional, which employs material-dependent mixing parameters derived from\nmacroscopic dielectric constants, provides accurate band gap predictions for 3D\nhalide perovskites when structural local disorder is considered. For layered\nhybrid perovskites, DSH functional based on average dielectric constants\noverestimates the band gaps. To improve the predictions and stay in a\nparameter-free ab initio workflow, we propose to use the calculated dielectric\nconstant of the respective 3D perovskites. We find that the DSH functionals\nusing the dielectric constants of the 3D perovskite accurately predict\nexperimental gaps, with the lowest mean absolute errors compared to HSE and\nPBE0 for layered perovskites with various organic spacers, as well as for\nmultilayered $BA_2MA_{n-1}Pb_{n}I_{3n-1}$ with n = 2, 3. Notably, the HSE\nfunctional systematically underestimates the band gaps in layered perovskites.\nWe attribute the root of this failure to the absence of non-local long-range\ndielectric screening, a critical factor for halide perovskites. The\ncomputational framework introduced here provides an efficient parameter-free ab\ninitio methodology for predicting the electronic properties of 3D and layered\nhalide perovskites and their heterostructures, aiding in developing advanced\noptoelectronic devices.",
        "Lead-free chalcogenide perovskites are emerging as game-changers in the race\nfor sustainable, high-performance photovoltaics. These materials offer a\nperfect trifecta: non-toxic elemental composition, exceptional phase stability,\nand outstanding optoelectronic properties. However, unlocking their full\npotential for solar cell applications requires advanced strategies to fine-tune\ntheir electronic and optical behavior. In this study, we take CaHfS$_{3}$-a\npromising but underexplored candidate-and revolutionize its performance by\nintroducing targeted substitutions: Ti at the cation site and Se at the anion\nsite. Using cutting-edge computational techniques, including density functional\ntheory, GW calculations, and the Bethe-Salpeter equation (BSE), we reveal how\nthese substitutions transform the material's properties. Our findings highlight\nthat alloyed compounds such as CaHfS$_{3-x}$Se$_{x}$ and\nCaHf$_{1-y}$Ti$_{y}$X$_{3}$ (X = S, Se) are not only phase-stable but also\nfeature adjustable direct G$_{0}$W$_{0}$@PBE bandgaps (1.29-2.67 eV), reduced\nexciton binding energies, and significantly improved polaron mobility. These\nmodifications enable better light absorption, reduced electron-hole\nrecombination, longer exciton lifetimes, and enhanced quantum yield.\nImpressively, the alloyed perovskites, specifically, for the Ti-rich Se-based\nperovskites, achieve a spectroscopic-limited maximum efficiency of up to\n28.06%, outperforming traditional lead-based halide perovskites. Our results\ndemonstrate that strategic alloying is a powerful tool to supercharge the\noptoelectronic properties of lead-free chalcogenide perovskites, positioning\nthem as strong contenders for next-generation photovoltaic technologies.",
        "Noncontacting and nondestructive control of geometric phase in conventional\nsemiconductors plays a pivotal role in various applications. In the current\nwork, we present a theoretical and computational investigation on terahertz\n(THz) light-induced phase transformation of conventional binary semiconducting\ncompounds among different structures including rock-salt, zinc-blende,\nwurtzite, and hexagonal phases. Using MgS and MgSe as prototypical examples, we\nperform anharmonic phonon mediated calculations and reveal large contrasting\nlattice contributed dielectric susceptibility in the THz regime. We then\nconstruct a THz-induced phase diagram under intermediate temperature and reveal\nrock-salt to hexagonal and then wurtzite structure transformations with\nincreasing light intensity. This does not require a high temperature\nenvironment as observed in traditional experiments. The low energy barrier\nsuggests that the phase transition kinetics can be fast, and the stable room\ntemperature phonon dispersions guarantee their non-volatile nature.\nFurthermore, we disclose the phononic hyperbolicity with strong anisotropic THz\nsusceptibility components, which serves as a natural hyperbolic material with\nnegative refractive index. Our work suggests the potential to realize\nmetastable hidden phases using noninvasive THz irradiation, which expands the\nconventional pressure-temperature ($P-T$) phase diagram by adding light as an\nadditional control factor.",
        "The intercalation of molecular species between the layers of van der Waals\n(vdW) materials has recently emerged as a powerful approach to combine the\nremarkable electronic and magnetic properties of vdW materials with the\nchemical flexibility of organic molecules. However, the full transformative\npotential of molecular intercalation remains underexplored, largely due to the\nlack of simple, broadly applicable methods that preserve high crystalline\nquality down to the few-layer limit. Here, we introduce a simple galvanic\napproach to intercalate different molecules into various vdW materials under\nambient conditions, leveraging the low reduction potential of selected metals\nto enable a spontaneous molecular insertion. We employ our method, which is\nparticularly well-suited for the in-situ intercalation of few-layer-thick\ncrystals, to intercalate nine vdW materials, including magnets and\nsuperconductors, with molecules ranging from conventional alkylammonium ions to\nmetallorganic and bio-inspired chiral cations. Notably, intercalation leads to\na molecule-dependent enhancement of the superconducting transition in 2H-TaS2,\nreaching a critical temperature of 4.7 K, higher than TaS2 monolayers.\nAdditionally, RuCl3 exhibits an unprecedented transition from antiferromagnetic\nto ferrimagnetic ordering upon intercalation with cobaltocenium. These results\nestablish our approach as a versatile technique for engineering atomically thin\nquantum materials and heterostructures, unlocking the transformative effects of\nmolecular intercalation.",
        "Defects in materials are ubiquitous and one of their adverse effects in\n$\\alpha$-Al$_2$O$_3$ is the initiation of corrosion. While this process starts\nnear the surface, the defects involved and their electronic structure need to\nbe elucidated with high accuracy. Since point defects are confined to a small\nspatial region, defect embedding theory allows the definition of an active\nspace, comprising of the defect electronic states, that is coupled to the\nenvironment of the host material. The active space Hamiltonian is of small\nrank, enabling access to its electronic properties using a high-level or even\nexact quantum theory. In this paper we use these techniques and\nfirst-principles simulations to compute the structural and electronic\nproperties of near-surface vacancies for the (0001) surface of\n$\\alpha$-Al$_2$O$_3$, and investigate the influence of defects and hydration on\nthe initiation and propagation of corrosion. We report the defect electronic\nstructure for strongly localized ground and excited states of the surface O\nvacancy and compare results obtained using full configuration interaction and a\nvariational quantum eigensolver on a quantum computer. Error mitigation\ntechniques are explored and shown to reduce the error due to the hardware noise\nto the point where the quantum result agrees with the exact solution within\nchemical accuracy.",
        "Recently, GeSn alloys have attracted much interest for direct-gap infrared\nphotonics and as potential topological materials which are compatible with the\nsemiconductor industry. However, for photonics, the high-Sn content required\nleads to low detectivity, associated with poor material quality, and the (>35%)\nSn required for topological properties have been out of reach experimentally.\nHere, we demonstrate that by patterning the Sn distribution within Ge, the\nelectronic properties have a far greater tunability than is possible with the\nrandom alloy. For the GeSn \\delta-digital alloy (DA) formed by confining Sn\natoms in atomic layer(s) along the [111] direction of Ge, we show that ~10% Sn\ncan lead to a triple-point semimetal. These findings are understood in terms of\nSn ordering causing spatial separation of Sn and Ge band edges, leading to band\ninversion. This mechanism can also lead to a weak topological insulator, Weyl\nsemimetal, and enables tunable direct bandgaps down to 2 meV, covering the\nentire infrared range. Our findings are generally applicable to other\nsemiconductors DAs and point to a new class of currently unexplored topological\nsystems accessible by epitaxy and establish the promise of low-Sn GeSn DAs for\napplication as infrared laser diodes and photodetectors in Si photonic\nintegrated circuits and infrared image sensors.",
        "Solute segregation in alloys is a key phenomenon which affects various\nmaterial characteristics such as embrittlement, grain growth and precipitation\nkinetics. In this work, the segregation energies of Y, Zr, and Nb to a\n\\textgreek{S}5 grain boundary in a bcc Ti-25 at \\% Mo alloy were determined\nusing density functional theory (DFT) calculations. A systematic approach was\nlaid out by computing the solution energy distributions in the bulk alloy using\nWarren-Cowley short-range order parameters to find a representative bulk-solute\nreference energy. Additionally, different scenarios were considered when a\nsolute atom replaces different sites in terms of their local Ti-Mo chemistry at\nthe GB plane to calculate the distribution of segregation energies. The solute\nsegregation to a Mo site at the GB plane is preferred rather than to a Ti site.\nFurther analysis shows that these segregation energy trends can be rationalized\nbased on a primarily elastic interaction. Thus the segregation energies scale\nwith the solute size such that Y has the largest segregation energies followed\nby Zr and Nb.",
        "Multiferroic materials, characterized by the coexistence of ferroelectricity\nand ferromagnetism, may unveil band structures suggestive of complex phenomena\nand new functionalities. In this Letter, we analyze the band structure of EuO\nin its multiferroic phase. Using density functional theory calculations and\ndetailed symmetry analysis, we reveal a previously overlooked non-reciprocal\nband structure effect, where the electronic energy bands exhibit asymmetry\nalong opposite directions with respect to the special points in the Brillouin\nzone. This effect, which is enabled by spin-orbit coupling, is giant for the\ntop valence Eu $4f$ bands, and can be switched by external electric or magnetic\nfields. Furthermore, this results in an enhanced bulk photovoltaic effect.\nSpecifically, our predictions indicate the emergence of a large injection\ncurrent response to linearly polarized light, resulting in a photoconductivity\nvalue several orders of magnitude higher than that reported in any other oxide\nmaterial. Ultimately, this non-reciprocal band structure effect and the\nassociated large bulk photovoltaic response may be general phenomena emerging\nnot just in EuO but also in other multiferroics or magnetoelectrics,\npotentially providing new cross-functionalities.",
        "Magnetic 2D materials have achieved significantly consideration owing to\ntheir encouraging applications. A variation of these 2D materials by occurrence\nof defects, by the transition-metal doping or adsorption or by the surface\nfunctionalization can initiate both the spin-polarization and magnetic\nproperties in these materials. Density functional theory (DFT) is used to\ndetermine the electric, magnetic properties along with the electronic\nstructures and stability of synthesized two-dimensional materials. This work\ndescribes the magnetic properties of Ni-ad-Nb2C-OF MXene. The study focuses on\nthe computational approach based first principal calculation providing insight\nonto the magnetic properties of adsorbed compound and comparing it with\npristine Nb2C-OF MXene. The pristine Nb2C-OF and Ni-ad-Nb2C-OF structures are\nsimulated and optimized using Wien2k software. Using exchange-correlational\nfunctionals; spin-GGA and spin-GGA+U (for Nickel U= 6eV), Ni-ad-Nb2C-OF\nelectronic band structure is found to be metallic having magnetic moment\ncalculated +1.01516{\\mu}_\\b{eta} showing its non-superconducting and\nferromagnetic behaviour. Owing to this magnetic nature, this 2D compound can be\nused for new upcoming applications such as spintronics and nano magnetic data\nstorage devices.",
        "The use of high-dimensional regression techniques from machine learning has\nsignificantly improved the quantitative accuracy of interatomic potentials.\nAtomic simulations can now plausibly target quantitative predictions in a\nvariety of settings, which has brought renewed interest in robust means to\nquantify uncertainties on simulation results. In many practical settings,\nencompassing both classical and a large class of machine learning potentials,\nthe dominant form of uncertainty is currently not due to lack of training data\nbut to misspecification, namely the inability of any one choice of model\nparameters to exactly match all ab initio training data. However, Bayesian\ninference, the most common formal tool used to quantify uncertainty, is known\nto ignore misspecification and thus significantly underestimates parameter\nuncertainties. Here, we employ a recent misspecification-aware regression\ntechnique to quantify parameter uncertainties, which is then propagated to a\nbroad range of phase and defect properties in tungsten via brute force\nresampling or implicit differentiation. The propagated misspecification\nuncertainties robustly envelope errors to direct \\textit{ab initio} calculation\nof material properties outside of the training dataset, an essential\nrequirement for any quantitative multi-scale modeling scheme. Finally, we\ndemonstrate application to recent foundational machine learning interatomic\npotentials, accurately predicting and bounding errors in MACE-MPA-0 energy\npredictions across the diverse materials project database. Perspectives for the\napproach in multiscale simulation workflows are discussed.",
        "The control of unconventional magnetism, which displays an antiferromagnetic\nconfiguration with ferromagnetism-like properties, has drawn intense attention\nfor advancing antiferromagnetic spintronics. Here, through symmetry analysis,\nwe propose a general stacking rule, characterized by a connection operator\nlinking two stacked bilayers, for controlling unconventional magnetism via\nsliding ferroelectricity. Such rule enables the simultaneous switching of both\nelectric polarization and nonrelativistic spin splitting or anomalous Hall\neffect in altermagnets, a class of collinear unconventional magnets. By\ncomprehensively surveying the 80 layer groups, we identify all the stacking\norders that allow for such two types of simultaneous switching. Combined with\nfirst-principles calculations, we demonstrate the sliding ferroelectric control\nof spin polarization and anomalous Hall effect in the altermagnetic AgF2\nbilayer. Our work provides a symmetry strategy for achieving ferroelectric\ncontrol of unconventional magnetism in bilayer systems and opens avenues for\nexploring new types of magnetoelectric coupling.",
        "Hard carbons show significant promise as anode materials for sodium-ion\nbatteries. However, monitoring the sodiation process in the hard carbon\nelectrode during cycling and understanding the sodiation mechanism remain\nchallenging. This article reports on operando 2D scanning small- and wide-angle\nX-ray scattering (SWAXS) and ex situ 3D SAXS tomography of hard carbon\nelectrodes during the sodiation process. Structural changes are monitored with\nspatial and temporal resolution during the electrochemical process and shows\nthat sodiation through micropore filling is the more dominating mechanism in\nthe later stages of sodiation, i.e. in the plateau region of the voltage\nprofile, while intercalation occurs continuously. Spatial inhomogeneities are\nresolved over the electrode and reveal an increased level of inhomogeneity at\nhigher degree of sodiation with regions of different degrees of micropore\nfilling. Resolving the processes spatially enables us to correlate plating,\nstarting from the interface between the electrode and the current collector, to\na higher degree of micropore filling. The work demonstrates how SWAXS imaging\ncan contribute to understanding the sodiation of hard carbon anodes, not only\nby spatially resolved analysis, but also as a method to decouple contributions\nfrom different components in a cell, enabling more accurate scattering analysis\nin in situ environments.",
        "We show that critical parking trees conditioned to be fully parked converge\nin the scaling limits towards the Brownian growth-fragmentation tree, a\nself-similar Markov tree different from Aldous' Brownian tree recently\nintroduced and studied by Bertoin, Curien and Riera. As a by-product of our\nstudy, we prove that positive non-linear polynomial equations involving a\ncatalytic variable display a universal polynomial exponent $5\/2$ at their\nsingularity, confirming a conjecture by Chapuy, Schaeffer and Drmota & Hainzl.\nCompared to previous analytical works on the subject, our approach is\nprobabilistic and exploits an underlying random walk hidden in the random tree\nmodel.",
        "Hydrogen is an energy vector, and one possible way to reduce CO 2 emissions.\nThis paper focuses on a hydrogen transport problem where mobile storage units\nare moved by trucks between sources to be refilled and destinations to meet\ndemands, involving swap operations upon arrival. This contrasts with existing\nliterature where inventories remain stationary. The objective is to optimize\ndaily routing and refilling schedules of the mobile storages. We model the\nproblem as a flow problem on a time-expanded graph, where each node of the\ngraph is indexed by a time-interval and a location and then, we give an\nequivalent Mixed Integer Linear Programming (MILP) formulation of the problem.\nFor small to medium-sized instances, this formulation can be efficiently solved\nusing standard MILP solvers. However, for larger instances, the computational\ncomplexity increases significantly due to the highly combinatorial nature of\nthe refilling process at the sources. To address this challenge, we propose a\ntwo-step heuristic that enhances.",
        "How has Wikipedia activity changed for articles with content similar to\nChatGPT following its introduction? We estimate the impact using\ndifferences-in-differences models, with dissimilar Wikipedia articles as a\nbaseline for comparison, to examine how changes in voluntary knowledge\ncontributions and information-seeking behavior differ by article content. Our\nanalysis reveals that newly created, popular articles whose content overlaps\nwith ChatGPT 3.5 saw a greater decline in editing and viewership after the\nNovember 2022 launch of ChatGPT than dissimilar articles did. These findings\nindicate heterogeneous substitution effects, where users selectively engage\nless with existing platforms when AI provides comparable content. This points\nto potential uneven impacts on the future of human-driven online knowledge\ncontributions.",
        "The unseen-species problem assumes $n\\geq1$ samples from a population of\nindividuals belonging to different species, possibly infinite, and calls for\nestimating the number $K_{n,m}$ of hitherto unseen species that would be\nobserved if $m\\geq1$ new samples were collected from the same population. This\nis a long-standing problem in statistics, which has gained renewed relevance in\nbiological and physical sciences, particularly in settings with large values of\n$n$ and $m$. In this paper, we adopt a Bayesian nonparametric approach to the\nunseen-species problem under the Pitman-Yor prior, and propose a novel\nmethodology to derive large $m$ asymptotic credible intervals for $K_{n,m}$,\nfor any $n\\geq1$. By leveraging a Gaussian central limit theorem for the\nposterior distribution of $K_{n,m}$, our method improves upon competitors in\ntwo key aspects: firstly, it enables the full parameterization of the\nPitman-Yor prior, including the Dirichlet prior; secondly, it avoids the need\nof Monte Carlo sampling, enhancing computational efficiency. We validate the\nproposed method on synthetic and real data, demonstrating that it improves the\nempirical performance of competitors by significantly narrowing the gap between\nasymptotic and exact credible intervals for any $m\\geq1$.",
        "Spatial networks are widely used in various fields to represent and analyze\ninteractions or relationships between locations or spatially distributed\nentities.There is a network science concept known as the 'rich club'\nphenomenon, which describes the tendency of 'rich' nodes to form densely\ninterconnected sub-networks. Although there are established methods to quantify\ntopological, weighted, and temporal rich clubs individually, there is limited\nresearch on measuring the rich club effect in spatially-weighted temporal\nnetworks, which could be particularly useful for studying dynamic spatial\ninteraction networks. To address this gap, we introduce the spatially-weighted\ntemporal rich club (WTRC), a metric that quantifies the strength and\nconsistency of connections between rich nodes in a spatiotemporal network.\nAdditionally, we present a unified rich club framework that distinguishes the\nWTRC effect from other rich club effects, providing a way to measure\ntopological, weighted, and temporal rich club effects together. Through two\ncase studies of human mobility networks at different spatial scales, we\ndemonstrate how the WTRC is able to identify significant weighted temporal rich\nclub effects, whereas the unweighted equivalent in the same network either\nfails to detect a rich club effect or inaccurately estimates its significance.\nIn each case study, we explore the spatial layout and temporal variations\nrevealed by the WTRC analysis, showcasing its particular value in studying\nspatiotemporal interaction networks. This research offers new insights into the\nstudy of spatiotemporal networks, with critical implications for applications\nsuch as transportation, redistricting, and epidemiology.",
        "We introduce a practical and accurate strategy to capture light-matter\ninteractions using the Floquet formalism in the velocity gauge in combination\nwith realistic first-principle models of solids. The velocity gauge, defined by\nthe linear coupling to the vector potential, is a standard method to capture\nthe light-matter interaction in solids. However, its use with first-principle\nmodels has been limited by the challenging fact that it requires a large number\nof bands for convergence and its incompatibility with non-local pseudopotential\nplane wave methods. To improve its convergence properties, we explicitly take\ninto account the truncation of Hilbert space in the construction of the Floquet\nHamiltonian in the velocity gauge. To avoid the incompatibility with the\npseudopotentials, we base our computations on generalized tight-binding\nHamiltonians derived from first-principles through maximally-localized Wannier\nfunctions. We exemplify the approach by computing the optical absorption\nspectra of laser-dressed trans-polyacetylene chain using realistic electronic\nstructure. We show that, by proceeding in this way, Floquet consideration\ninvolving the truncated Hilbert spaces reproduces the full basis calculations\nwith only a few bands and with significantly reduced computation time. The\nstrategy has been implemented in FloqticS, a general code for the Floquet\nengineering of the optical properties of materials. Overall, this work\nintroduces a useful theoretical tool to realize Floquet engineering of\nrealistic solids in the velocity gauge.",
        "Peanut hulls, also known as Arachis hypogaea L. particles (AHL), are an\nabundant biomass source with a long shelf life. In this study, we incorporate\npeanut hull powder into PLA polymer, imparting recyclability, biodegradability,\nand biocompatibility, along with the antimicrobial properties of AHL particles.\nIn particular, we treat AHL particles as a reinforcement for PLA polymer to\nproduce 3D printing filament compatible with the fused filament fabrication\n(FFF) 3D printing method. We provide a step-by-step method for preparing AHL\nparticles, incorporating them into PLA, and ultimately forming high-quality\nfilaments. We assess the quality of the filaments in terms of extruded\ndimensions, mechanical strength, and elastic modulus, along with physical\nproperties such as porosity and melt flow index. We evaluate the printability\nand wettability of the filaments as well. Notably, and unlike other\nbiomass-based reinforcements in PLA, AHL preserves the filament's strength and\nenhances its elastic modulus. 3D-printed components fabricated using our\nPLA-AHL filaments successfully retain their antimicrobial properties and\nexhibit increased overall hardness. However, this comes at the expense of\nforming more microvoids and a rougher surface, making the material more prone\nto fracture and leading to a slight reduction in fracture toughness with\nincreasing AHL mass fraction.",
        "In active fluids and active solids the constituents individually generate\nmovement by each extracting energy from their environment or from their own\nsource. Non-reciprocal interactions among these active constituents then enable\nnovel collective behavior that often can be strikingly counterintuitive.\nHowever, non-reciprocity in these cases typically requires that the interacting\nbodies have different physical properties or it needs to be programmed\nexplicitly into all pairwise interactions. Here we show that collective\nactivity in a driven system can emerge spontaneously through multibody\nnonreciprocal forces, even if all bodies are individually non-active and have\nidentical properties. We demonstrate this with as few as three identical\nspheres, acoustically levitated in air, which exhibit collective activity as\nthey interact through non-pairwise forces: similar to the classic gravitational\nthree-body problem, the interaction between two spheres depends sensitively on\nthe relative position of the third sphere. Non-reciprocity arises naturally\nfrom both near-field sound scattering and microstreaming forces among the\nspheres. The underdamped dynamics in air furthermore make it possible to go\nbeyond collective center-of-mass propulsion or rotation and observe internal,\nengine-like reconfigurations that follow limit cycles. These findings open up\nnew possibilities for self-assembly, where now multibody interactions not only\ndetermine the resulting structure but also drive the spontaneously emerging\ndynamics.",
        "To describe the propagation of West Nile virus and\/or Zika virus, in this\npaper, we propose and study a time-periodic reaction-diffusion model with\ngeneral boundary conditions in heterogeneous environments and with four\nunknowns: susceptible host, infectious host, susceptible vector and infectious\nvector. We can prove that such problem has a positive time periodic solution if\nand only if host and vector persist and the basic reproduction ratio is greater\nthan one, and moreover the positive time periodic solution is unique and\nglobally asymptotically stable when it exists.",
        "Collective modes in superconductors provided the first realization of the\nHiggs mechanism. The transverse Goldstone mode acquires a gap (i.e. a mass)\nwhen it hybridizes with the electromagnetic gauge field. The longitudinal\nSchmid-Higgs mode, on the other hand, is always massive. In conventional BCS\ntheory, its gap is exactly $2\\Delta$, coinciding with the excitation threshold\nfor quasiparticles. Being situated right at the edge of the continuum spectrum\nit gives rise to peculiar dynamics for the Schmid-Higgs mode. For instance,\nwhen suddenly excited at $t=0$, it exhibits algebraically decaying oscillations\nof the form $\\sim \\sin(2\\Delta t)\/{t}^{1\/2}$. In this study, we explore the\nbehavior of Schmid-Higgs oscillations in the presence of pair-breaking\nmechanisms, such as magnetic impurities or in-plane magnetic fields. These\nprocesses suppress the quasiparticle excitation threshold down to\n$2\\varepsilon_g < 2\\Delta$, potentially placing the longitudinal mode within\nthe continuum spectrum. Despite this, we show that the algebraically decaying\noscillations persist, taking the form $\\sim \\sin(2\\varepsilon_g t)\/t^2$. The\nSchmid-Higgs mode becomes truly overdamped and exponentially decaying only in\nthe gapless superconductors with $\\varepsilon_g=0$.",
        "This is a gentle introduction to a general theory of universal polynomials\nassociated to classification of map-germs, called Thom polynomials. The theory\nwas originated by Ren\\'e Thom in the 1950s and has since been evolved in\nvarious aspects by many authors. In a nutshell, this is about intersection\ntheory on certain moduli spaces, say `classifying spaces of\nmono\/multi-singularities of maps', which provides consistent and deep insights\ninto both classical and modern enumerative geometry with many potential\napplications.",
        "We introduce the $\\texttt{PrecisionLauricella}$ package, a computational tool\ndeveloped in Wolfram Mathematica for high-precision numerical evaluations of\nLauricella functions with indices linearly dependent on a parameter,\n$\\varepsilon$. The package leverages a method based on analytical continuation\nvia Frobenius generalized power series, providing an efficient and accurate\nalternative to conventional approaches relying on multi-dimensional series\nexpansions or Mellin--Barnes representations. This one-dimensional approach is\nparticularly advantageous for high-precision calculations and facilitates\nfurther optimization through $\\varepsilon$-dependent reconstruction from\nevaluations at specific numerical values, enabling efficient parallelization.\nThe underlying mathematical framework for this method has been detailed in our\nprevious work, while the current paper focuses on the design, implementation,\nand practical applications of the $\\texttt{PrecisionLauricella}$ package.",
        "We use high-tech observations of Unidentified Aerial Phenomena (UAP) class\nobjects to evaluate their characteristics. We present data in three cases. (1)\nMulti-side daytime observations of UAPs over Kiev. (2) Night observations of a\ngroup of objects in the vicinity of the Moon. (3) UAP observations in the\ncombat zone in Ukraine. Dark UAPs in the visible wavelength range are observed\nonly during the day. At night they can only be seen in the infrared wavelength\nrange. We note large sizes of UAPs, from three to six kilometers.They exhibit\nlarge velocities, from 2.5 Mach and much larger. They have low albedo, from\nthree percent and below, that is, they actually exhibit features of a\ncompletely black body.",
        "We numerically analyze the behavior of a charged scalar field on a fixed\nextremal Reissner-Nordstr\\\"om background. We find an extension of the Aretakis\ninstability characterized by an accumulation of charge on the extremal event\nhorizon. In particular, when the charge coupling to the scalar field is\nsufficiently large, the charge density on the horizon asymptotes to a nonzero\nconstant at late times. By constructing monochromatic initial data at the onset\nof charged superradiance, we give evidence supporting the claim that this\ninstability is connected to the presence of a nearly zero-damped mode.\nThroughout this work, we employ a numerical integration scheme in compactified\ndouble-null coordinates, which allows us to capture the asymptotic behavior of\nthe matter at the boundaries of the spacetime.",
        "The Clebsch-Gordan coefficients or Wigner $3j$ symbols are known to be\nproportional to a $_3F_2(1)$ hypergeometric series, and Racah $6j$ coefficients\nto a $_4F_3(1)$. In general, however, non-trivial $9j$ symbols can not be\nexpressed as a $_5F_4$. In this letter, we show, using the Dougall-Ramanujan\nidentity, that special stretched $9j$ symbols can be reformulated as $_5F_4(1)$\nhypergeometric series."
      ]
    }
  },
  {
    "id":2411.18141,
    "research_type":"applied",
    "start_id":"b26",
    "start_title":"Durban's water wars, sewage spills, fish kills and blue flag beaches. Durban's Climate Gamble",
    "start_abstract":"Water is one of the primary barometers of climate change: A rise in sea-levels, flooding, and extreme storms combined with general water stress and more severe and frequent droughts will escalate crises in municipal infrastructure, requiring continual upgrades for water purification, stormwater drainage, and sewage treatment, all of which will dramatically raise the price of water at the retail level. In South Africa, the dry western side will be most adversely affected by droughts (threatening the production of rooibos tea and Cape wines). According to the Academy of Science in South Africa (ASSAf), Durban is also at great risk and will experience higher temperatures and heat stress, volatile rainfall, up to 160 million cubic metres less water each year by 2100, a sea-level rise of up to a metre by 2100 across Durban\u2019s 100 km of developed coastline, lower biodiversity, higher disease levels (especially malaria and cholera), declining agricultural output (a one degree Celsius rise leaves the surrounding region unreliable for the staple maize production), and other economic stresses (ASSAf 2011: 27). Tourism, one of Durban\u2019s main economic engines, will be irreparably harmed. Swimmers and surfers think of Durban\u2019s beachfront as one of the world\u2019s finest in any urban context. After apartheid-era rules that prohibited black people from using the best beaches were lifted at the end of the 1980s, the area stretching from the Blue Lagoon\u2019s Umgeni River to South Beach\u2019s uShaka Marine World\u2013including the immensely popular North Beach area near the main restaurant strip\u2013represented one of South Africa\u2019s most impressive, open and democratic public spaces.",
    "start_categories":[
      "cond-mat.mtrl-sci"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b18"
      ],
      "title":[
        "Quantum machine learning in chemistry and materials"
      ],
      "abstract":[
        "Within the past few years, we have witnessed the rising of quantum machine learning (QML) models which infer electronic properties of molecules and materials, rather than solving approximations to the electronic Schr\u00f6dinger equation. The increasing availability of large quantum mechanics reference datasets has enabled these developments. We review the basic theories and key ingredients of popular QML models such as choice of regressor, data of varying trustworthiness, the role of the representation, and the effect of training set selection. Throughout we emphasize the indispensable role of learning curves when it comes to the comparative assessment of different QML models."
      ],
      "categories":[
        "cs.ET"
      ]
    },
    "list":{
      "title":[
        "TAP-CAM: A Tunable Approximate Matching Engine based on Ferroelectric\n  Content Addressable Memory",
        "Limitations in Parallel Ising Machine Networks: Theory and Practice",
        "Statistical QoS Provisioning for Underwater Magnetic Induction\n  Communication",
        "Improving VANET Simulation Channel Model in an Urban Environment via\n  Calibration Using Real-World Communication Data",
        "Mapping Spiking Neural Networks to Heterogeneous Crossbar Architectures\n  using Integer Linear Programming",
        "Characterization and Mitigation of ADC Noise by Reference Tuning in\n  RRAM-Based Compute-In-Memory",
        "Monolithic 3D FPGAs Utilizing Back-End-of-Line Configuration Memories",
        "Blind Eye: Motion and Obstacle Detection Leveraging Wi-Fi",
        "Consensus ranking by quantum annealing",
        "Energy-Efficient Cryogenic Neuromorphic Network with Superconducting\n  Memristor",
        "Solving Boolean satisfiability problems with resistive content\n  addressable memories",
        "Modular Mechanism Design Optimization in Large-Scale Systems with\n  Manufacturing Cost Considerations",
        "Microdroplet-Based Communications with Frequency Shift Keying Modulation",
        "6KSFx Synth Dataset",
        "Optimized detection of cyber-attacks on IoT networks via hybrid deep\n  learning models",
        "A Bot-based Approach to Manage Codes of Conduct in Open-Source Projects",
        "Superficial Self-Improved Reasoners Benefit from Model Merging",
        "SpeHeatal: A Cluster-Enhanced Segmentation Method for Sperm Morphology\n  Analysis",
        "Global well-posedness of the defocusing nonlinear wave equation outside\n  of a ball with radial data for $3<p<5$",
        "Improving Discriminator Guidance in Diffusion Models",
        "From Occasional to Steady: Habit Formation Insights From a Comprehensive\n  Fitness Study",
        "HEATS: A Hierarchical Framework for Efficient Autonomous Target Search\n  with Mobile Manipulators",
        "Cooperative Bearing-Only Target Pursuit via Multiagent Reinforcement\n  Learning: Design and Experiment",
        "Learning to reset in target search problems",
        "Analysis and Optimization of Robustness in Multiplex Flow Networks\n  Against Cascading Failures",
        "Adapting Beyond the Depth Limit: Counter Strategies in Large Imperfect\n  Information Games",
        "New Dataset and Methods for Fine-Grained Compositional Referring\n  Expression Comprehension via Specialist-MLLM Collaboration",
        "FUNU: Boosting Machine Unlearning Efficiency by Filtering Unnecessary\n  Unlearning"
      ],
      "abstract":[
        "Pattern search is crucial in numerous analytic applications for retrieving\ndata entries akin to the query. Content Addressable Memories (CAMs), an\nin-memory computing fabric, directly compare input queries with stored entries\nthrough embedded comparison logic, facilitating fast parallel pattern search in\nmemory. While conventional CAM designs offer exact match functionality, they\nare inadequate for meeting the approximate search needs of emerging\ndata-intensive applications. Some recent CAM designs propose approximate\nmatching functions, but they face limitations such as excessively large cell\narea or the inability to precisely control the degree of approximation. In this\npaper, we propose TAP-CAM, a novel ferroelectric field effect transistor\n(FeFET) based ternary CAM (TCAM) capable of both exact and tunable approximate\nmatching. TAP-CAM employs a compact 2FeFET-2R cell structure as the entry\nstorage unit, and similarities in Hamming distances between input queries and\nstored entries are measured using an evaluation transistor associated with the\nmatchline of CAM array. The operation, robustness and performance of the\nproposed design at array level have been discussed and evaluated, respectively.\nWe conduct a case study of K-nearest neighbor (KNN) search to benchmark the\nproposed TAP-CAM at application level. Results demonstrate that compared to 16T\nCMOS CAM with exact match functionality, TAP-CAM achieves a 16.95x energy\nimprovement, along with a 3.06% accuracy enhancement. Compared to 2FeFET TCAM\nwith approximate match functionality, TAP-CAM achieves a 6.78x energy\nimprovement.",
        "Analog Ising machines (IMs) occupy an increasingly prominent area of computer\narchitecture research, offering high-quality and low latency\/energy solutions\nto intractable computing tasks. However, IMs have a fixed capacity, with little\nto no utility in out-of-capacity problems. Previous works have proposed\nparallel, multi-IM architectures to circumvent this limitation. In this work we\ntheoretically and numerically investigate tradeoffs in parallel IM networks to\nguide researchers in this burgeoning field. We propose formal models of\nparallel IM excution models, then provide theoretical guarantees for\nprobabilistic convergence. Numerical experiments illustrate our findings and\nprovide empirical insight into high and low synchronization frequency regimes.\nWe also provide practical heuristics for parameter\/model selection, informed by\nour theoretical and numerical findings.",
        "Magnetic induction (MI) communication, with stable channel conditions and\nsmall antenna size, is considered as a promising solution for underwater\ncommunication network. However, the narrowband nature of the MI link can cause\nsignificant delays in the network. To comprehensively ensure the timeliness and\neffectiveness of the MI network, in this paper we introduce a statistical\nquality of service (QoS) framework for MI communication, aiming to maximize the\nachievable rate while provisioning delay and queue-length requirements.\nSpecifically, we employ effective capacity theory to model underwater MI\ncommunication. Based on convex optimization theory, we propose a current\ncontrol strategy that maximizes the effective capacity under the constraints of\nlimited channel capacity and limited power. Simulations demonstrate that the\ncurrent control strategy proposed for MI communication differs significantly\nfrom that in the conventional statistical QoS provisioning framework. In\naddition, compared to other current control strategies, the proposed strategy\nsubstantially improves the achievable rate under various delay QoS\nrequirements.",
        "Wireless communication channels in Vehicular Ad-hoc NETworks (VANETs) suffer\nfrom packet losses, which severely influences the performance of their\napplications. There are several reasons for this loss, including but not\nlimited to signal interference with itself after being reflected from the\nground and other objects, the doppler effect caused by the speed of the\nvehicle, and buildings and other vehicles blocking the signal. As a result,\nVANET simulators must be calibrated in order to mimic the behavior of\nreal-world vehicular communication channels effectively. In this paper, we\ncalibrated an OMNET++(Objective Modular Network Testbed in C++)\/Veins simulator\nfor VANET's dedicated short-range communications (DSRC) protocol using the\nfield data from the urban testbed in Downtown Chattanooga, TN. Channel\npropagation models, as well as physical layer parameters, were calibrated using\na Genetic Algorithm (GA). The performance of the calibrated simulator was\nimproved significantly in comparison with the default settings in Veins. The\nfinal results were compared to the real-world data collected from the testbed\nand performance shows that the final calibrated channel model performs better\nthan uncalibrated models in simulating the packet delivery pattern of DSRC\nchannels.",
        "Advances in novel hardware devices and architectures allow Spiking Neural\nNetwork evaluation using ultra-low power, mixed-signal, memristor crossbar\narrays. As individual network sizes quickly scale beyond the dimensional\ncapabilities of single crossbars, networks must be mapped onto multiple\ncrossbars. Crossbar sizes within modern Memristor Crossbar Architectures are\ndetermined predominately not by device technology but by network topology;\nmore, smaller crossbars consume less area thanks to the high structural\nsparsity found in larger, brain-inspired SNNs. Motivated by continuing\nincreases in SNN sparsity due to improvements in training methods, we propose\nutilizing heterogeneous crossbar sizes to further reduce area consumption. This\napproach was previously unachievable as prior compiler studies only explored\nsolutions targeting homogeneous MCAs. Our work improves on the state-of-the-art\nby providing Integer Linear Programming formulations supporting arbitrarily\nheterogeneous architectures. By modeling axonal interactions between neurons\nour methods produce better mappings while removing inhibitive a priori\nknowledge requirements. We first show a 16.7-27.6% reduction in area\nconsumption for square-crossbar homogeneous architectures. Then, we demonstrate\n66.9-72.7% further reduction when using a reasonable configuration of\nheterogeneous crossbar dimensions. Next, we present a new optimization\nformulation capable of minimizing the number of inter-crossbar routes. When\napplied to solutions already near-optimal in area an 11.9-26.4% routing\nreduction is observed without impacting area consumption. Finally, we present a\nprofile-guided optimization capable of minimizing the number of runtime spikes\nbetween crossbars. Compared to the best-area-then-route optimized solutions we\nobserve a further 0.5-14.8% inter-crossbar spike reduction while requiring 1-3\norders of magnitude less solver time.",
        "With the escalating demand for power-efficient neural network architectures,\nnon-volatile compute-in-memory designs have garnered significant attention.\nHowever, owing to the nature of analog computation, susceptibility to noise\nremains a critical concern. This study confronts this challenge by introducing\na detailed model that incorporates noise factors arising from both ADCs and\nRRAM devices. The experimental data is derived from a 40nm foundry RRAM\ntest-chip, wherein different reference voltage configurations are applied, each\ntailored to its respective module. The mean and standard deviation values of\nHRS and LRS cells are derived through a randomized vector, forming the\nfoundation for noise simulation within our analytical framework. Additionally,\nthe study examines the read-disturb effects, shedding light on the potential\nfor accuracy deterioration in neural networks due to extended exposure to\nhigh-voltage stress. This phenomenon is mitigated through the proposed\nlow-voltage read mode. Leveraging our derived comprehensive fault model from\nthe RRAM test-chip, we evaluate CIM noise impact on both supervised learning\n(time-independent) and reinforcement learning (time-dependent) tasks, and\ndemonstrate the effectiveness of reference tuning to mitigate noise impacts.",
        "This work presents a novel monolithic 3D (M3D) FPGA architecture that\nleverages stackable back-end-of-line (BEOL) transistors to implement\nconfiguration memory and pass gates, significantly improving area, latency, and\npower efficiency. By integrating n-type (W-doped In_2O_3) and p-type (SnO)\namorphous oxide semiconductor (AOS) transistors in the BEOL, Si SRAM\nconfiguration bits are substituted with a less leaky equivalent that can be\nprogrammed at logic-compatible voltages. BEOL-compatible AOS transistors are\ncurrently under extensive research and development in the device community,\nwith investment by leading foundries, from which reported data is used to\ndevelop robust physics-based models in TCAD that enable circuit design. The use\nof AOS pass gates reduces the overhead of reconfigurable circuits by mapping\nFPGA switch block (SB) and connection block (CB) matrices above configurable\nlogic blocks (CLBs), thereby increasing the proximity of logic elements and\nreducing latency. By interfacing with the latest Verilog-to-Routing (VTR)\nsuite, an AOS-based M3D FPGA design implemented in 7 nm technology is\ndemonstrated with 3.4x lower area-time squared product (AT^2), 27% lower\ncritical path latency, and 26% lower reconfigurable routing block power on\nbenchmarks including hyperdimensional computing and large language models\n(LLMs).",
        "Wireless Fidelity or Wi-Fi, has completely transfigured wireless networking\nby offering a smooth connection to the internet and networks, particularly when\ndealing with enclosed environments. As with the majority of wireless\ntechnology, it functions through radio communication. This makes it possible\nfor Wi-Fi to operate effectively close to an Access Point. However, a device's\nability to receive Wi-Fi signals can vary greatly. These discrepancies arise\nbecause of impediments or motions between the device and the access point. We\nhave creatively used these variances as unique opportunities for applications\nthat can be used to detect movement in confined areas. As this approach makes\nuse of the current wireless infrastructure, no additional hardware is required.\nThese applications could potentially be leveraged to enable sophisticated\nrobots or enhance security systems.",
        "Consensus ranking is a technique used to derive a single ranking that best\nrepresents the preferences of multiple individuals or systems. It aims to\naggregate different rankings into one that minimizes overall disagreement or\ndistance from each of the individual rankings. Kemeny ranking aggregation, in\nparticular, is a widely used method in decision-making and social choice, with\napplications ranging from search engines to music recommendation systems. It\nseeks to determine a consensus ranking of a set of candidates based on the\npreferences of a group of individuals. However, existing quantum annealing\nalgorithms face challenges in efficiently processing large datasets with many\ncandidates. In this paper, we propose a method to improve the performance of\nquantum annealing for Kemeny rank aggregation. Our approach identifies the\npairwise preference matrix that represents the solution list and subsequently\nreconstructs the ranking using classical methods. This method already yields\nbetter results than existing approaches. Furthermore, we present a range of\nenhancements that significantly improve the proposed method's performance,\nthereby increasing the number of candidates that can be effectively handled.\nFinally, we evaluate the efficiency of our approach by comparing its\nperformance and execution time with that of KwikSort, a well-known approximate\nalgorithm.",
        "Cryogenic neuromorphic systems, inspired by the brains unparalleled\nefficiency, present a promising paradigm for next generation computing\narchitectures.This work introduces a fully integrated neuromorphic framework\nthat combines superconducting memristor(SM) based spiking neurons and synapse\ntopologies to achieve a low power neuromorphic network with non volatile\nsynaptic strength.This neurosynaptic framework is validated by implementing the\ncart pole control task, a dynamic decision making problem requiring real time\ncomputation.Through detailed simulations, we demonstrate the network's ability\nto execute this task with an average fitness of 5965 timesteps across 1000\nrandomized test episodes, with 40 percent achieving the target fitness of\n15,000 timesteps (0.02s per timestep).The system achieves 23 distinct spiking\nrates across neurons, ensuring efficient information encoding.Our findings\nestablish the potential of SM based cryogenic neuromorphic systems to address\nthe energy and scalability limitations of traditional computing, paving the way\nfor biologically inspired, ultra low power computational frameworks.",
        "Solving optimization problems is a highly demanding workload requiring\nhigh-performance computing systems. Optimization solvers are usually difficult\nto parallelize in conventional digital architectures, particularly when\nstochastic decisions are involved. Recently, analog computing architectures for\naccelerating stochastic optimization solvers have been presented, but they were\nlimited to academic problems in quadratic polynomial format. Here we present\nKLIMA, a k-Local In-Memory Accelerator with resistive Content Addressable\nMemories (CAMs) and Dot-Product Engines (DPEs) to accelerate the solution of\nhigh-order industry-relevant optimization problems, in particular Boolean\nSatisfiability. By co-designing the optimization heuristics and circuit\narchitecture we improve the speed and energy to solution up to 182x compared to\nthe digital state of the art.",
        "Modular design maximizes utility by using standardized components in\nlarge-scale systems. From a manufacturing perspective, it supports green\ntechnology by reducing material waste and improving reusability. Industrially,\nit offers economic benefits through economies of scale, making it a practical\ndesign strategy. Typically, modularization selects a representative design from\npredefined candidates to meet all performance requirements. However, achieving\neffective modularization in mechanical mechanisms presents challenges. First,\nmechanisms depend on geometric relationships for functional motion, and varying\nloads lead to different optimal parameters, complicating representative design\nselection. Second, the chosen design often exceeds optimal parameters, causing\nover-specification and performance deviations, which worsen as scale increases.\nTo address this, we propose a modular mechanism design framework using\nsurrogate-based optimization. This approach finds optimal designs for\nlarge-scale systems and partitions them into groups, each assigned an optimized\ndesign. This multi-objective optimization (MOO) problem balances economies of\nscale and performance consistency. Unlike conventional methods based on\npredefined candidates and simple grouping, our framework optimizes design\nvariables flexibly for modularization. Additionally, we analyze manufacturing\ncost parameters to develop a decision support system for selecting optimal\nstrategies in diverse design scenarios. This enhances maintainability, improves\ninterchangeability, and fosters environmentally sustainable manufacturing.",
        "Droplet-based communications has been investigated as a more robust\nalternative to diffusion-based molecular communications (MC), yet most existing\ndemonstrations employ large \"plug-like\" droplets or simple T-junction designs\nfor droplet generation, restricting modulation strategies and achievable data\nrates. Here, we report a microfluidic communication system that encodes\ninformation via the generation rate of sub-100 $\\mu$m water-in-oil\nmicrodroplets using a microfabricated flow focusing architecture. By precisely\ntuning the flow rate of the dispersed-phase (water) via a pressure-regulated\nflow controller, we implement frequency shift keying modulation with four\nsymbols (4-FSK). A high-speed optical detection and video processing setup\nserves as the receiver, tracking system response in the microfluidic channel\nacross different symbol durations (20 s and 12 s) and quantifying error\nperformance. Despite the miniaturized device and channel architecture, our\nexperiments demonstrate programmable and reliable data transmission with\nminimal symbol errors. Beyond water-in-oil systems, the same encoding\nprinciples can be extended to other compartmentalized carriers (e.g., giant\nunilamellar vesicles, polymersomes) that can also be synthesized via flow\nfocusing techniques, paving the way for biocompatible, robust, and\nhigh-capacity communication in intrabody networks and the emerging Internet of\nBio-Nano Things.",
        "Procedural audio, often referred to as \"digital Foley\", generates sound from\nscratch using computational processes. It represents an innovative approach to\nsound-effects creation. However, the development and adoption of procedural\naudio has been constrained by a lack of publicly available datasets and models,\nwhich hinders evaluation and optimization. To address this important gap, this\npaper presents a dataset of 6000 synthetic audio samples specifically designed\nto advance research and development in sound synthesis within 30 sound\ncategories. By offering a description of the diverse synthesis methods used in\neach sound category and supporting the creation of robust evaluation\nframeworks, this dataset not only highlights the potential of procedural audio,\nbut also provides a resource for researchers, audio developers, and sound\ndesigners. This contribution can accelerate the progress of procedural audio,\nopening up new possibilities in digital sound design.",
        "The rapid expansion of Internet of Things (IoT) devices has increased the\nrisk of cyber-attacks, making effective detection essential for securing IoT\nnetworks. This work introduces a novel approach combining Self-Organizing Maps\n(SOMs), Deep Belief Networks (DBNs), and Autoencoders to detect known and\npreviously unseen attack patterns. A comprehensive evaluation using simulated\nand real-world traffic data is conducted, with models optimized via Particle\nSwarm Optimization (PSO). The system achieves an accuracy of up to 99.99% and\nMatthews Correlation Coefficient (MCC) values exceeding 99.50%. Experiments on\nNSL-KDD, UNSW-NB15, and CICIoT2023 confirm the model's strong performance\nacross diverse attack types. These findings suggest that the proposed method\nenhances IoT security by identifying emerging threats and adapting to evolving\nattack strategies.",
        "The development of Open-Source Software (OSS) projects relies on the\ncollaborative work of contributors, generally scattered around the world. To\nenable this collaboration, OSS projects are hosted on social-coding platforms\nlike GitHub, which provide the infrastructure to host the code as well as the\nsupport for enabling the participation of the community. The potentially rich\nand diverse mixture of contributors in OSS projects makes their management not\nonly a technical challenge, where automation tools and bots are usually\ndeployed, but also a social one. To this aim, OSS projects have been\nincreasingly deploying a declaration of their code of conduct, which defines\nrules to ensure a respectful and inclusive participatory environment in the\ncommunity, being the Contributor Covenant the main model to follow. However,\nthe broad adoption and enforcement of codes of conduct in OSS projects is still\nlimited. In particular, the definition, deployment, and enforcement of codes of\nconduct is a very challenging task. In this paper, we propose an approach to\neffectively manage codes of conduct in OSS projects based on the Contributor\nCovenant proposal. Our solution has been implemented as a bot-based solution\nwhere bots help in the definition of codes of conduct, the monitoring of OSS\nprojects, and the enforcement of ethical rules.",
        "As scaled language models (LMs) approach human-level reasoning capabilities,\nself-improvement emerges as a solution to synthesizing high-quality data\ncorpus. While previous research has identified model collapse as a risk in\nself-improvement, where model outputs become increasingly deterministic, we\ndiscover a more fundamental challenge: the superficial self-improved reasoners\nphenomenon. In particular, our analysis reveals that even when LMs show\nimproved in-domain (ID) reasoning accuracy, they actually compromise their\ngeneralized reasoning capabilities on out-of-domain (OOD) tasks due to\nmemorization rather than genuine. Through a systematic investigation of LM\narchitecture, we discover that during self-improvement, LM weight updates are\nconcentrated in less reasoning-critical layers, leading to superficial\nlearning. To address this, we propose Iterative Model Merging (IMM), a method\nthat strategically combines weights from original and self-improved models to\npreserve generalization while incorporating genuine reasoning improvements. Our\napproach effectively mitigates both LM collapse and superficial learning,\nmoving towards more stable self-improving systems.",
        "The accurate assessment of sperm morphology is crucial in andrological\ndiagnostics, where the segmentation of sperm images presents significant\nchallenges. Existing approaches frequently rely on large annotated datasets and\noften struggle with the segmentation of overlapping sperm and the presence of\ndye impurities. To address these challenges, this paper first analyzes the\nissue of overlapping sperm tails from a geometric perspective and introduces a\nnovel clustering algorithm, Con2Dis, which effectively segments overlapping\ntails by considering three essential factors: CONnectivity, CONformity, and\nDIStance. Building on this foundation, we propose an unsupervised method,\nSpeHeatal, designed for the comprehensive segmentation of the SPErm HEAd and\nTAiL. SpeHeatal employs the Segment Anything Model(SAM) to generate masks for\nsperm heads while filtering out dye impurities, utilizes Con2Dis to segment\ntails, and then applies a tailored mask splicing technique to produce complete\nsperm masks. Experimental results underscore the superior performance of\nSpeHeatal, particularly in handling images with overlapping sperm.",
        "We continue the study of the Dirichlet boundary value problem of nonlinear\nwave equation with radial data in the exterior $\\Omega = \\mathbb{R}^3\\backslash\n\\bar{B}(0,1)$. We combine the distorted Fourier truncation method in\n\\cite{Bourgain98:FTM}, the global-in-time (endpoint) Strichartz estimates in\n\\cite{XuYang:NLW} with the energy method in \\cite{GallPlan03:NLW} to prove the\nglobal well-posedness of the radial solution to the defocusing,\nenergy-subcriticial nonlinear wave equation outside of a ball in $\\left(\\dot\nH^{s}_{D}(\\Omega) \\cap L^{p+1}(\\Omega) \\right)\\times \\dot H^{s-1}_{D}(\\Omega)$\nwith $1-\\frac{(p+3)(1-s_c)}{4(2p-3)}<s<1$, $s_c=\\frac{3}{2}-\\frac{2}{p-1} $,\nwhich extends the result for the cubic nonlinearity in \\cite{XuYang:NLW} to the\ncase $3<p<5$. Except from the argument in \\cite{XuYang:NLW}, another new\ningredient is that we need make use of the radial Sobolev inequality to deal\nwith the super-conformal nonlinearity in addition to the Sobolev inequality.",
        "Discriminator Guidance has become a popular method for efficiently refining\npre-trained Score-Matching Diffusion models. However, in this paper, we\ndemonstrate that the standard implementation of this technique does not\nnecessarily lead to a distribution closer to the real data distribution.\nSpecifically, we show that training the discriminator using Cross-Entropy loss,\nas commonly done, can in fact increase the Kullback-Leibler divergence between\nthe model and target distributions, particularly when the discriminator\noverfits. To address this, we propose a theoretically sound training objective\nfor discriminator guidance that properly minimizes the KL divergence. We\nanalyze its properties and demonstrate empirically across multiple datasets\nthat our proposed method consistently improves over the conventional method by\nproducing samples of higher quality.",
        "Exercising regularly is widely recognized as a cornerstone of health, yet the\nchallenge of sustaining consistent exercise habits persists. Understanding the\nfactors that influence the formation of these habits is crucial for developing\neffective interventions. This study utilizes data from Mars Athletic Club,\nT\\\"urkiye's largest sports chain, to investigate the dynamics of gym attendance\nand habit formation. The general problem addressed by this study is identifying\nthe critical periods and factors that contribute to the successful\nestablishment of consistent exercise routines among gym-goers. Here we show\nthat there are specific periods during which gym attendance is most crucial for\nhabit formation. By developing a survival metric based on gym attendance\npatterns, we pinpoint these critical periods and segment members into distinct\nclusters based on their visit patterns. Our analysis reveals significant\ndifferences in how various subgroups respond to interventions, such as group\nclasses, personal trainer sessions, and visiting different clubs. Using causal\ninference analysis, we demonstrate that personalized guidance and social\ndynamics are key drivers of sustained long-term engagement. By systematically\nexamining these variables and considering the specific characteristics of\ndifferent clusters, our research demonstrates the importance of a tailored,\nmulti-dimensional approach to promoting exercise habits, which integrates\nsocial dynamics, personalized guidance, and strategic interventions to sustain\nlong-term engagement.",
        "Utilizing robots for autonomous target search in complex and unknown\nenvironments can greatly improve the efficiency of search and rescue missions.\nHowever, existing methods have shown inadequate performance due to hardware\nplatform limitations, inefficient viewpoint selection strategies, and\nconservative motion planning. In this work, we propose HEATS, which enhances\nthe search capability of mobile manipulators in complex and unknown\nenvironments. We design a target viewpoint planner tailored to the strengths of\nmobile manipulators, ensuring efficient and comprehensive viewpoint planning.\nSupported by this, a whole-body motion planner integrates global path search\nwith local IPC optimization, enabling the mobile manipulator to safely and\nagilely visit target viewpoints, significantly improving search performance. We\npresent extensive simulated and real-world tests, in which our method\ndemonstrates reduced search time, higher target search completeness, and lower\nmovement cost compared to classic and state-of-the-art approaches. Our method\nwill be open-sourced for community benefit.",
        "This paper addresses the multi-robot pursuit problem for an unknown target,\nencompassing both target state estimation and pursuit control. First, in state\nestimation, we focus on using only bearing information, as it is readily\navailable from vision sensors and effective for small, distant targets.\nChallenges such as instability due to the nonlinearity of bearing measurements\nand singularities in the two-angle representation are addressed through a\nproposed uniform bearing-only information filter. This filter integrates\nmultiple 3D bearing measurements, provides a concise formulation, and enhances\nstability and resilience to target loss caused by limited field of view (FoV).\nSecond, in target pursuit control within complex environments, where challenges\nsuch as heterogeneity and limited FoV arise, conventional methods like\ndifferential games or Voronoi partitioning often prove inadequate. To address\nthese limitations, we propose a novel multiagent reinforcement learning (MARL)\nframework, enabling multiple heterogeneous vehicles to search, localize, and\nfollow a target while effectively handling those challenges. Third, to bridge\nthe sim-to-real gap, we propose two key techniques: incorporating adjustable\nlow-level control gains in training to replicate the dynamics of real-world\nautonomous ground vehicles (AGVs), and proposing spectral-normalized RL\nalgorithms to enhance policy smoothness and robustness. Finally, we demonstrate\nthe successful zero-shot transfer of the MARL controllers to AGVs, validating\nthe effectiveness and practical feasibility of our approach. The accompanying\nvideo is available at https:\/\/youtu.be\/HO7FJyZiJ3E.",
        "Target search problems are central to a wide range of fields, from biological\nforaging to the optimization algorithms. Recently, the ability to reset the\nsearch has been shown to significantly improve the searcher's efficiency.\nHowever, the optimal resetting strategy depends on the specific properties of\nthe search problem and can often be challenging to determine. In this work, we\npropose a reinforcement learning (RL)-based framework to train agents capable\nof optimizing their search efficiency in environments by learning how to reset.\nFirst, we validate the approach in a well-established benchmark: the Brownian\nsearch with resetting. There, RL agents consistently recover strategies closely\nresembling the sharp resetting distribution, known to be optimal in this\nscenario. We then extend the framework by allowing agents to control not only\nwhen to reset, but also their spatial dynamics through turning actions. In this\nmore complex setting, the agents discover strategies that adapt both resetting\nand turning to the properties of the environment, outperforming the proposed\nbenchmarks. These results demonstrate how reinforcement learning can serve both\nas an optimization tool and a mechanism for uncovering new, interpretable\nstrategies in stochastic search processes with resetting.",
        "Networked systems are susceptible to cascading failures, where the failure of\nan initial set of nodes propagates through the network, often leading to\nsystem-wide failures. In this work, we propose a multiplex flow network model\nto study robustness against cascading failures triggered by random failures.\nThe model is inspired by systems where nodes carry or support multiple types of\nflows, and failures result in the redistribution of flows within the same layer\nrather than between layers. To represent different types of interdependencies\nbetween the layers of the multiplex network, we define two cases of failure\nconditions: layer-independent overload and layer-influenced overload. We\nprovide recursive equations and their solutions to calculate the steady-state\nfraction of surviving nodes, validate them through a set of simulation\nexperiments, and discuss optimal load-capacity allocation strategies. Our\nresults demonstrate that allocating the total excess capacity to each layer\nproportional to the mean effective load in the layer and distributing that\nexcess capacity equally among the nodes within the layer ensures maximum\nrobustness. The proposed framework for different failure conditions allows us\nto analyze the two overload conditions presented and can be extended to explore\nmore complex interdependent relationships.",
        "We study the problem of adapting to a known sub-rational opponent during\nonline play while remaining robust to rational opponents. We focus on large\nimperfect-information (zero-sum) games, which makes it impossible to inspect\nthe whole game tree at once and necessitates the use of depth-limited search.\nHowever, all existing methods assume rational play beyond the depth-limit,\nwhich only allows them to adapt a very limited portion of the opponent's\nbehaviour. We propose an algorithm Adapting Beyond Depth-limit (ABD) that uses\na strategy-portfolio approach - which we refer to as matrix-valued states - for\ndepth-limited search. This allows the algorithm to fully utilise all\ninformation about the opponent model, making it the first robust-adaptation\nmethod to be able to do so in large imperfect-information games. As an\nadditional benefit, the use of matrix-valued states makes the algorithm simpler\nthan traditional methods based on optimal value functions. Our experimental\nresults in poker and battleship show that ABD yields more than a twofold\nincrease in utility when facing opponents who make mistakes beyond the depth\nlimit and also delivers significant improvements in utility and safety against\nrandomly generated opponents.",
        "Referring Expression Comprehension (REC) is a foundational cross-modal task\nthat evaluates the interplay of language understanding, image comprehension,\nand language-to-image grounding. To advance this field, we introduce a new REC\ndataset with two key features. First, it is designed with controllable\ndifficulty levels, requiring fine-grained reasoning across object categories,\nattributes, and relationships. Second, it incorporates negative text and images\ngenerated through fine-grained editing, explicitly testing a model's ability to\nreject non-existent targets, an often-overlooked yet critical challenge in\nexisting datasets. To address fine-grained compositional REC, we propose novel\nmethods based on a Specialist-MLLM collaboration framework, leveraging the\ncomplementary strengths of them: Specialist Models handle simpler tasks\nefficiently, while MLLMs are better suited for complex reasoning. Based on this\nsynergy, we introduce two collaborative strategies. The first, Slow-Fast\nAdaptation (SFA), employs a routing mechanism to adaptively delegate simple\ntasks to Specialist Models and complex tasks to MLLMs. Additionally, common\nerror patterns in both models are mitigated through a target-refocus strategy.\nThe second, Candidate Region Selection (CRS), generates multiple bounding box\ncandidates based on Specialist Model and uses the advanced reasoning\ncapabilities of MLLMs to identify the correct target. Extensive experiments on\nour dataset and other challenging compositional benchmarks validate the\neffectiveness of our approaches. The SFA strategy achieves a trade-off between\nlocalization accuracy and efficiency, and the CRS strategy greatly boosts the\nperformance of both Specialist Models and MLLMs. We aim for this work to offer\nvaluable insights into solving complex real-world tasks by strategically\ncombining existing tools for maximum effectiveness, rather than reinventing\nthem.",
        "Machine unlearning is an emerging field that selectively removes specific\ndata samples from a trained model. This capability is crucial for addressing\nprivacy concerns, complying with data protection regulations, and correcting\nerrors or biases introduced by certain data. Unlike traditional machine\nlearning, where models are typically static once trained, machine unlearning\nfacilitates dynamic updates that enable the model to ``forget'' information\nwithout requiring complete retraining from scratch. There are various machine\nunlearning methods, some of which are more time-efficient when data removal\nrequests are fewer.\n  To decrease the execution time of such machine unlearning methods, we aim to\nreduce the size of data removal requests based on the fundamental assumption\nthat the removal of certain data would not result in a distinguishable\nretrained model. We first propose the concept of unnecessary unlearning, which\nindicates that the model would not alter noticeably after removing some data\npoints. Subsequently, we review existing solutions that can be used to solve\nour problem. We highlight their limitations in adaptability to different\nunlearning scenarios and their reliance on manually selected parameters. We\nconsequently put forward FUNU, a method to identify data points that lead to\nunnecessary unlearning. FUNU circumvents the limitations of existing solutions.\nThe idea is to discover data points within the removal requests that have\nsimilar neighbors in the remaining dataset. We utilize a reference model to set\nparameters for finding neighbors, inspired from the area of model memorization.\nWe provide a theoretical analysis of the privacy guarantee offered by FUNU and\nconduct extensive experiments to validate its efficacy."
      ]
    }
  },
  {
    "id":2411.16349,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Cerebral aneurysms. New Engl. J. Medicine",
    "start_abstract":"Saccular intracranial aneurysms cause substantial morbidity and mortality. Recently, major changes have occurred in the way we think about and treat this disease. This review discusses the percutaneous endovascular treatment of intracranial aneurysms as compared with surgical intervention. The technological advances and supporting research contributing to this important change in practice patterns are reviewed.",
    "start_categories":[
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b18"
      ],
      "title":[
        "Data-driven science and engineering: machine learning, dynamical systems, and control"
      ],
      "abstract":[
        "\"Data-driven science and engineering: machine learning, dynamical systems, control.\" Contemporary Physics, 60(4), p. 320"
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "T\\'yr-the-Pruner: Unlocking Accurate 50% Structural Pruning for LLMs via\n  Global Sparsity Distribution Optimization",
        "Artificial Intelligence in Reactor Physics: Current Status and Future\n  Prospects",
        "Fuzzy Information Entropy and Region Biased Matrix Factorization for Web\n  Service QoS Prediction",
        "Exploit Gradient Skewness to Circumvent Byzantine Defenses for Federated\n  Learning",
        "Probabilistic neural operators for functional uncertainty quantification",
        "A Flexible Fairness Framework with Surrogate Loss Reweighting for\n  Addressing Sociodemographic Disparities",
        "Controlling Neural Collapse Enhances Out-of-Distribution Detection and\n  Transfer Learning",
        "Online-BLS: An Accurate and Efficient Online Broad Learning System for\n  Data Stream Classification",
        "GeneralizeFormer: Layer-Adaptive Model Generation across Test-Time\n  Distribution Shifts",
        "Pairwise Elimination with Instance-Dependent Guarantees for Bandits with\n  Cost Subsidy",
        "AdaGC: Improving Training Stability for Large Language Model Pretraining",
        "HypeRL: Parameter-Informed Reinforcement Learning for Parametric PDEs",
        "Tighten The Lasso: A Convex Hull Volume-based Anomaly Detection Method",
        "Effects of oxidation and impurities in lithium surfaces on the emitting\n  wall plasma sheath",
        "Conservative, pressure-equilibrium-preserving discontinuous Galerkin\n  method for compressible, multicomponent flows",
        "MAISTEP -- a new grid-based machine learning tool for inferring stellar\n  parameters I. Ages of giant-planet host stars",
        "Moment-based Characterization of Spatially Distributed Sources in SAR\n  Tomography",
        "Infinite-temperature thermostats by energy localization in a\n  nonequilibrium setup",
        "A Systematic Evaluation of Generative Models on Tabular Transportation\n  Data",
        "Grey system model on time scales",
        "Jailbreaking Generative AI: Empowering Novices to Conduct Phishing\n  Attacks",
        "A Resilient and Energy-Efficient Smart Metering Infrastructure Utilizing\n  a Self-Organizing UAV Swarm",
        "Single-molecule phosphorescence and intersystem crossing in a coupled\n  exciton-plasmon system",
        "SEAlign: Alignment Training for Software Engineering Agent",
        "Discrete-time weak approximation of a Black-Scholes model with drift and\n  volatility Markov switching",
        "Long-term evolution of Sco X-1: implications for the current spin\n  frequency and ellipticity of the neutron star",
        "Modular and Integrated AI Control Framework across Fiber and Wireless\n  Networks for 6G",
        "Antiferromagnetic and bond-order-wave phases in the half-filled\n  two-dimensional optical Su-Schrieffer-Heeger-Hubbard model"
      ],
      "abstract":[
        "Structural pruning enhances hardware-agnostic inference efficiency for large\nlanguage models (LLMs) but often struggles to maintain performance. Local\npruning performs efficient layer-by-layer compression but ignores global\ntopology. Global pruning has the potential to find the optimal solution\nalthough resource-intensive. However, existing methods tend to rank structural\nsaliency uniformly, ignoring inter-structure dependencies and failing to\nachieve end-to-end optimization. To address these limitations, we propose\nT\\'yr-the-Pruner, an efficient end-to-end search-based global structural\npruning framework. This framework constructs a supernet by repeatedly applying\nlocal pruning across a range of sparsity ratios to each layer in an LLM, with\nthe core goal of determining the optimal sparsity distribution under a target\noverall sparsity ratio. Concretely, we introduce an effective local pruning and\nan expectation error accumulation approach to improve supernet construction.\nFurthermore, we employ an iterative prune-and-search strategy with\ncoarse-to-fine sparsity granularity to ensure efficient search convergence.\nExperimental results show that T\\'yr-the-Pruner achieves state-of-the-art\nstructural pruning, retaining 97% of the dense model's performance while\nremoving a challenging 50% of Llama-3.1-70B's parameters.",
        "Reactor physics is the study of neutron properties, focusing on using models\nto examine the interactions between neutrons and materials in nuclear reactors.\nArtificial intelligence (AI) has made significant contributions to reactor\nphysics, e.g., in operational simulations, safety design, real-time monitoring,\ncore management and maintenance. This paper presents a comprehensive review of\nAI approaches in reactor physics, especially considering the category of\nMachine Learning (ML), with the aim of describing the application scenarios,\nfrontier topics, unsolved challenges and future research directions. From\nequation solving and state parameter prediction to nuclear industry\napplications, this paper provides a step-by-step overview of ML methods applied\nto steady-state, transient and combustion problems. Most literature works\nachieve industry-demanded models by enhancing the efficiency of deterministic\nmethods or correcting uncertainty methods, which leads to successful\napplications. However, research on ML methods in reactor physics is somewhat\nfragmented, and the ability to generalize models needs to be strengthened.\nProgress is still possible, especially in addressing theoretical challenges and\nenhancing industrial applications such as building surrogate models and digital\ntwins.",
        "Nowadays, there are many similar services available on the internet, making\nQuality of Service (QoS) a key concern for users. Since collecting QoS values\nfor all services through user invocations is impractical, predicting QoS values\nis a more feasible approach. Matrix factorization is considered an effective\nprediction method. However, most existing matrix factorization algorithms focus\non capturing global similarities between users and services, overlooking the\nlocal similarities between users and their similar neighbors, as well as the\nnon-interactive effects between users and services. This paper proposes a\nmatrix factorization approach based on user information entropy and region\nbias, which utilizes a similarity measurement method based on fuzzy information\nentropy to identify similar neighbors of users. Simultaneously, it integrates\nthe region bias between each user and service linearly into matrix\nfactorization to capture the non-interactive features between users and\nservices. This method demonstrates improved predictive performance in more\nrealistic and complex network environments. Additionally, numerous experiments\nare conducted on real-world QoS datasets. The experimental results show that\nthe proposed method outperforms some of the state-of-the-art methods in the\nfield at matrix densities ranging from 5% to 20%.",
        "Federated Learning (FL) is notorious for its vulnerability to Byzantine\nattacks. Most current Byzantine defenses share a common inductive bias: among\nall the gradients, the densely distributed ones are more likely to be honest.\nHowever, such a bias is a poison to Byzantine robustness due to a newly\ndiscovered phenomenon in this paper - gradient skew. We discover that a group\nof densely distributed honest gradients skew away from the optimal gradient\n(the average of honest gradients) due to heterogeneous data. This gradient skew\nphenomenon allows Byzantine gradients to hide within the densely distributed\nskewed gradients. As a result, Byzantine defenses are confused into believing\nthat Byzantine gradients are honest. Motivated by this observation, we propose\na novel skew-aware attack called STRIKE: first, we search for the skewed\ngradients; then, we construct Byzantine gradients within the skewed gradients.\nExperiments on three benchmark datasets validate the effectiveness of our\nattack",
        "Neural operators aim to approximate the solution operator of a system of\ndifferential equations purely from data. They have shown immense success in\nmodeling complex dynamical systems across various domains. However, the\noccurrence of uncertainties inherent in both model and data has so far rarely\nbeen taken into account\\textemdash{}a critical limitation in complex, chaotic\nsystems such as weather forecasting. In this paper, we introduce the\nprobabilistic neural operator (PNO), a framework for learning probability\ndistributions over the output function space of neural operators. PNO extends\nneural operators with generative modeling based on strictly proper scoring\nrules, integrating uncertainty information directly into the training process.\nWe provide a theoretical justification for the approach and demonstrate\nimproved performance in quantifying uncertainty across different domains and\nwith respect to different baselines. Furthermore, PNO requires minimal\nadjustment to existing architectures, shows improved performance for most\nprobabilistic prediction tasks, and leads to well-calibrated predictive\ndistributions and adequate uncertainty representations even for long dynamical\ntrajectories. Implementing our approach into large-scale models for physical\napplications can lead to improvements in corresponding uncertainty\nquantification and extreme event identification, ultimately leading to a deeper\nunderstanding of the prediction of such surrogate models.",
        "This paper presents a new algorithmic fairness framework called\n$\\boldsymbol{\\alpha}$-$\\boldsymbol{\\beta}$ Fair Machine Learning\n($\\boldsymbol{\\alpha}$-$\\boldsymbol{\\beta}$ FML), designed to optimize fairness\nlevels across sociodemographic attributes. Our framework employs a new family\nof surrogate loss functions, paired with loss reweighting techniques, allowing\nprecise control over fairness-accuracy trade-offs through tunable\nhyperparameters $\\boldsymbol{\\alpha}$ and $\\boldsymbol{\\beta}$. To efficiently\nsolve the learning objective, we propose Parallel Stochastic Gradient Descent\nwith Surrogate Loss (P-SGD-S) and establish convergence guarantees for both\nconvex and nonconvex loss functions. Experimental results demonstrate that our\nframework improves overall accuracy while reducing fairness violations,\noffering a smooth trade-off between standard empirical risk minimization and\nstrict minimax fairness. Results across multiple datasets confirm its\nadaptability, ensuring fairness improvements without excessive performance\ndegradation.",
        "Out-of-distribution (OOD) detection and OOD generalization are widely studied\nin Deep Neural Networks (DNNs), yet their relationship remains poorly\nunderstood. We empirically show that the degree of Neural Collapse (NC) in a\nnetwork layer is inversely related with these objectives: stronger NC improves\nOOD detection but degrades generalization, while weaker NC enhances\ngeneralization at the cost of detection. This trade-off suggests that a single\nfeature space cannot simultaneously achieve both tasks. To address this, we\ndevelop a theoretical framework linking NC to OOD detection and generalization.\nWe show that entropy regularization mitigates NC to improve generalization,\nwhile a fixed Simplex Equiangular Tight Frame (ETF) projector enforces NC for\nbetter detection. Based on these insights, we propose a method to control NC at\ndifferent DNN layers. In experiments, our method excels at both tasks across\nOOD datasets and DNN architectures.",
        "The state-of-the-art online learning models generally conduct a single online\ngradient descent when a new sample arrives and thus suffer from suboptimal\nmodel weights. To this end, we introduce an online broad learning system\nframework with closed-form solutions for each online update. Different from\nemploying existing incremental broad learning algorithms for online learning\ntasks, which tend to incur degraded accuracy and expensive online update\noverhead, we design an effective weight estimation algorithm and an efficient\nonline updating strategy to remedy the above two deficiencies, respectively.\nSpecifically, an effective weight estimation algorithm is first developed by\nreplacing notorious matrix inverse operations with Cholesky decomposition and\nforward-backward substitution to improve model accuracy. Second, we devise an\nefficient online updating strategy that dramatically reduces online update\ntime. Theoretical analysis exhibits the splendid error bound and low time\ncomplexity of our model. The most popular test-then-training evaluation\nexperiments on various real-world datasets prove its superiority and\nefficiency. Furthermore, our framework is naturally extended to data stream\nscenarios with concept drift and exceeds state-of-the-art baselines.",
        "We consider the problem of test-time domain generalization, where a model is\ntrained on several source domains and adjusted on target domains never seen\nduring training. Different from the common methods that fine-tune the model or\nadjust the classifier parameters online, we propose to generate multiple layer\nparameters on the fly during inference by a lightweight meta-learned\ntransformer, which we call \\textit{GeneralizeFormer}. The layer-wise parameters\nare generated per target batch without fine-tuning or online adjustment. By\ndoing so, our method is more effective in dynamic scenarios with multiple\ntarget distributions and also avoids forgetting valuable source distribution\ncharacteristics. Moreover, by considering layer-wise gradients, the proposed\nmethod adapts itself to various distribution shifts. To reduce the\ncomputational and time cost, we fix the convolutional parameters while only\ngenerating parameters of the Batch Normalization layers and the linear\nclassifier. Experiments on six widely used domain generalization datasets\ndemonstrate the benefits and abilities of the proposed method to efficiently\nhandle various distribution shifts, generalize in dynamic scenarios, and avoid\nforgetting.",
        "Multi-armed bandits (MAB) are commonly used in sequential online\ndecision-making when the reward of each decision is an unknown random variable.\nIn practice however, the typical goal of maximizing total reward may be less\nimportant than minimizing the total cost of the decisions taken, subject to a\nreward constraint. For example, we may seek to make decisions that have at\nleast the reward of a reference ``default'' decision, with as low a cost as\npossible. This problem was recently introduced in the Multi-Armed Bandits with\nCost Subsidy (MAB-CS) framework. MAB-CS is broadly applicable to problem\ndomains where a primary metric (cost) is constrained by a secondary metric\n(reward), and the rewards are unknown. In our work, we address variants of\nMAB-CS including ones with reward constrained by the reward of a known\nreference arm or by the subsidized best reward. We introduce the\nPairwise-Elimination (PE) algorithm for the known reference arm variant and\ngeneralize PE to PE-CS for the subsidized best reward variant. Our\ninstance-dependent analysis of PE and PE-CS reveals that both algorithms have\nan order-wise logarithmic upper bound on Cost and Quality Regret, making our\npolicies the first with such a guarantee. Moreover, by comparing our upper and\nlower bound results we establish that PE is order-optimal for all known\nreference arm problem instances. Finally, experiments are conducted using the\nMovieLens 25M and Goodreads datasets for both PE and PE-CS revealing the\neffectiveness of PE and the superior balance between performance and\nreliability offered by PE-CS compared to baselines from the literature.",
        "Large Language Models (LLMs) face increasing loss spikes during scaling,\nundermining training stability and final performance. While gradient clipping\nmitigates this issue, traditional global approaches poorly handle\nparameter-specific gradient variations and decaying gradient norms. We propose\n**AdaGC**, an adaptive gradient clipping framework that automatically adjusts\nlocal thresholds per parameter through exponential moving average of gradient\nnorms. Theoretical analysis proves AdaGC's convergence under non-convex\nconditions. Extensive experiments demonstrate significant improvements: On\nLlama-2 7B\/13B, AdaGC completely eliminates loss spikes while reducing WikiText\nperplexity by 3.5% (+0.14pp LAMBADA accuracy) for 7B and achieving 0.65% lower\ntraining loss with 1.47% reduced validation perplexity for 13B compared to\nglobal clipping. For CLIP ViT-Base, AdaGC converges 25% faster than StableAdamW\nwith full spike elimination. The method shows universal effectiveness across\narchitectures (Llama-2 7B\/13B) and modalities (CLIP), with successful\nintegration into diverse optimizers like AdamW and Lion. Source code will be\nreleased on GitHub.",
        "In this work, we devise a new, general-purpose reinforcement learning\nstrategy for the optimal control of parametric partial differential equations\n(PDEs). Such problems frequently arise in applied sciences and engineering and\nentail a significant complexity when control and\/or state variables are\ndistributed in high-dimensional space or depend on varying parameters.\nTraditional numerical methods, relying on either iterative minimization\nalgorithms or dynamic programming, while reliable, often become computationally\ninfeasible. Indeed, in either way, the optimal control problem must be solved\nfor each instance of the parameters, and this is out of reach when dealing with\nhigh-dimensional time-dependent and parametric PDEs. In this paper, we propose\nHypeRL, a deep reinforcement learning (DRL) framework to overcome the\nlimitations shown by traditional methods. HypeRL aims at approximating the\noptimal control policy directly. Specifically, we employ an actor-critic DRL\napproach to learn an optimal feedback control strategy that can generalize\nacross the range of variation of the parameters. To effectively learn such\noptimal control laws, encoding the parameter information into the DRL policy\nand value function neural networks (NNs) is essential. To do so, HypeRL uses\ntwo additional NNs, often called hypernetworks, to learn the weights and biases\nof the value function and the policy NNs. We validate the proposed approach on\ntwo PDE-constrained optimal control benchmarks, namely a 1D\nKuramoto-Sivashinsky equation and a 2D Navier-Stokes equations, by showing that\nthe knowledge of the PDE parameters and how this information is encoded, i.e.,\nvia a hypernetwork, is an essential ingredient for learning parameter-dependent\ncontrol policies that can generalize effectively to unseen scenarios and for\nimproving the sample efficiency of such policies.",
        "The rapid advancements in data-driven methodologies have underscored the\ncritical importance of ensuring data quality. Consequently, detecting\nout-of-distribution (OOD) data has emerged as an essential task to maintain the\nreliability and robustness of data-driven models, in general, and machine and\ndeep learning models, in particular. In this study, we leveraged the convex\nhull property of a dataset and the fact that anomalies highly contribute to the\nincrease of the CH's volume to propose a novel anomaly detection algorithm. Our\nalgorithm computes the CH's volume as an increasing number of data points are\nremoved from the dataset to define a decision line between OOD and\nin-distribution data points. We compared the proposed algorithm to seven widely\nused anomaly detection algorithms over ten datasets, showing comparable results\nfor state-of-the-art (SOTA) algorithms. Moreover, we show that with a\ncomputationally cheap and simple check, one can detect datasets that are\nwell-suited for the proposed algorithm which outperforms the SOTA anomaly\ndetection algorithms.",
        "Use of lithium as a surface coating in fusion devices improves plasma\nperformance, but the change in wall properties affects the secondary electron\nemission properties of the material. Lithium oxidizes easily, which drives the\nemission yield well above unity. We present here simulations demonstrating the\nchange in sheath structure from monotonic to the nonmonotonic space-charge\nlimited sheath using an energy-dependent data-driven emission model which\nself-consistently captures both secondary emission and backscattering\npopulations. Increased secondary electron emission from the material has\nramifications for the degradation and erosion of the wall. Results shows that\nthe oxidation leads to an increased electron flux into the wall, and a reduced\nion flux. The net transfer of energy to the surface is significantly greater\nfor the oxidized case than for the pure lithium case. High reflection rates of\nlow-energy backscattered particles leads to a high re-emission rate at the\nwall.",
        "This paper concerns preservation of velocity and pressure equilibria in\nsmooth, compressible, multicomponent flows in the inviscid limit. First, we\nderive the velocity-equilibrium and pressure-equilibrium conditions of a\nstandard discontinuous Galerkin method that discretizes the conservative form\nof the compressible, multicomponent Euler equations. We show that under certain\nconstraints on the numerical flux, the scheme is\nvelocity-equilibrium-preserving. However, standard discontinuous Galerkin\nschemes are not pressure-equilibrium-preserving. Therefore, we introduce a\ndiscontinuous Galerkin method that discretizes the pressure-evolution equation\nin place of the total-energy conservation equation. Semidiscrete conservation\nof total energy, which would otherwise be lost, is restored via the correction\nterms of [Abgrall, J. Comput. Phys., 372, 2018, pp. 640-666] and [Abgrall et\nal., J. Comput. Phys., 453, 2022, 110955]. Since the addition of the correction\nterms prevents exact preservation of pressure and velocity equilibria, we\npropose modifications that then lead to a velocity-equilibrium-preserving,\npressure-equilibrium-preserving, and (semidiscretely) energy-conservative\ndiscontinuous Galerkin scheme, although there are certain tradeoffs. Additional\nextensions are also introduced. We apply the developed scheme to smooth,\ninterfacial flows involving mixtures of thermally perfect gases initially in\npressure and velocity equilibria to demonstrate its performance in one, two,\nand three spatial dimensions.",
        "Our understanding of exoplanet demographics partly depends on their\ncorresponding host star parameters. With the majority of exoplanet-host stars\nhaving only atmospheric constraints available, robust inference of their\nparameters is susceptible to the approach used. The goal of this work is to\ndevelop a grid-based machine learning tool capable of determining the stellar\nradius, mass, and age using only atmospheric constraints and to analyse the age\ndistribution of stars hosting giant planets. Our machine learning approach\ninvolves combining four tree-based machine learning algorithms (Random Forest,\nExtra Trees, Extreme Gradient Boosting, and CatBoost) trained on a grid of\nstellar models to infer stellar radius, mass, and age using Teff, [Fe\/H], and\nluminosities. We perform a detailed statistical analysis to compare the\ninferences of our tool with those based on seismic data from the APOKASC and\nLEGACY samples. Finally, we apply our tool to determine the ages of stars\nhosting giant planets. Comparing the stellar parameter inferences from our\nmachine learning tool with those from the APOKASC and LEGACY, we find a bias\n(and a scatter) of -0.5\\% (5\\%) and -0.2\\% (2\\%) in radius, 6\\% (5\\%) per cent\nand -2\\% (3\\%) in mass, and -9\\% (16\\%) and 7\\% (23\\%) in age, respectively.\nTherefore, our machine learning predictions are commensurate with seismic\ninferences. When applying our model to a sample of stars hosting Jupiter-mass\nplanets, we find the average age estimates for the hosts of Hot Jupiters, Warm\nJupiters, and Cold Jupiters to be 1.98, 2.98, and 3.51 Gyr, respectively. These\nstatistical ages of the host stars confirm previous predictions - based on\nstellar model ages for a relatively small number of hosts, as well as on the\naverage age-velocity dispersion relation - that stars hosting Hot Jupiters are\nstatistically younger than those hosting Warm and Cold Jupiters.",
        "This paper presents a non-parametric method for 3-D imaging of natural\nvolumes using Synthetic Aperture Radar tomography. This array processing-based\ntechnique aims at characterizing a spatially distributed density of incoherent\nsources, whose shape is imprecisely known. The proposed technique estimates the\nmoments of the reflectivity density using a low-complexity covariance matching\napproach, and retrieves the mean location, dispersion, and power of the\ndistributed source. Numerical simulations of realistic tomographic scenarios\nshow that the proposed model-free scheme achieves better accuracy than slightly\nmisspecified maximum likelihood estimators, derived from approximately known\ndistribution shapes.",
        "Some lattice models having two conservation laws may display an equilibrium\nphase transition from a homogeneous (positive temperature - PT) to a condensed\n(negative temperature) phase, where a finite fraction of the energy is\nlocalized in a few sites. We study one such stochastic model in an\nout-of-equilibrium setup, where the ends of the lattice chain are attached to\ntwo PT baths. We show that localized peaks may spontaneously emerge, acting as\ninfinite-temperature heat baths. The number $N_b$ of peaks is expected to grow\nin time $t$ as $N_b \\sim \\sqrt{\\ln t}$, as a consequence of an effective\nfreezing of the dynamics. Asymptotically, the chain spontaneously subdivides\ninto three intervals: the two external ones lying inside the PT region; the\nmiddle one characterized by peaks superposed to a background lying along the\ninfinite-temperature line. In the thermodynamic limit, the Onsager formalism\nallows determining the shape of the whole profile.",
        "The sharing of large-scale transportation data is beneficial for\ntransportation planning and policymaking. However, it also raises significant\nsecurity and privacy concerns, as the data may include identifiable personal\ninformation, such as individuals' home locations. To address these concerns,\nsynthetic data generation based on real transportation data offers a promising\nsolution that allows privacy protection while potentially preserving data\nutility. Although there are various synthetic data generation techniques, they\nare often not tailored to the unique characteristics of transportation data,\nsuch as the inherent structure of transportation networks formed by all trips\nin the datasets. In this paper, we use New York City taxi data as a case study\nto conduct a systematic evaluation of the performance of widely used tabular\ndata generative models. In addition to traditional metrics such as distribution\nsimilarity, coverage, and privacy preservation, we propose a novel graph-based\nmetric tailored specifically for transportation data. This metric evaluates the\nsimilarity between real and synthetic transportation networks, providing\npotentially deeper insights into their structural and functional alignment. We\nalso introduced an improved privacy metric to address the limitations of the\ncommonly-used one. Our experimental results reveal that existing tabular data\ngenerative models often fail to perform as consistently as claimed in the\nliterature, particularly when applied to transportation data use cases.\nFurthermore, our novel graph metric reveals a significant gap between synthetic\nand real data. This work underscores the potential need to develop generative\nmodels specifically tailored to take advantage of the unique characteristics of\nemerging domains, such as transportation.",
        "The Grey System Theory (GST) is a powerful mathematical framework employed\nfor modeling systems with uncertain or incomplete information. This paper\nproposes an integration of the GST with time scales, a generalized approach\nthat encompasses both discrete and continuous time models. The proposed model,\ncalled the Grey System Model on Time Scales (GST-T), offers a robust solution\nfor analyzing hybrid systems where events occur on varying time domains.",
        "The rapid advancements in generative AI models, such as ChatGPT, have\nintroduced both significant benefits and new risks within the cybersecurity\nlandscape. This paper investigates the potential misuse of the latest AI model,\nChatGPT-4o Mini, in facilitating social engineering attacks, with a particular\nfocus on phishing, one of the most pressing cybersecurity threats today. While\nexisting literature primarily addresses the technical aspects, such as\njailbreaking techniques, none have fully explored the free and straightforward\nexecution of a comprehensive phishing campaign by novice users using ChatGPT-4o\nMini. In this study, we examine the vulnerabilities of AI-driven chatbot\nservices in 2025, specifically how methods like jailbreaking and reverse\npsychology can bypass ethical safeguards, allowing ChatGPT to generate phishing\ncontent, suggest hacking tools, and assist in carrying out phishing attacks.\nOur findings underscore the alarming ease with which even inexperienced users\ncan execute sophisticated phishing campaigns, emphasizing the urgent need for\nstronger cybersecurity measures and heightened user awareness in the age of AI.",
        "The smart metering infrastructure may become one of the key elements in\nefficiently managing energy in smart cities. At the same time, traditional\nmeasurement record collection is performed by manual methods, which raises\ncost, safety, and accuracy issues. This paper proposes an innovative SMI\narchitecture based on an unmanned aerial vehicle swarm organizing itself for\nthe autonomous data collection in smart metering infrastructure with\nscalability and cost-effectiveness while minimizing risks. We design an\narchitecture-based comprehensive system with various phases of operation,\ncommunication protocols, and robust failure-handling mechanisms to ensure\nreliable operations. We further perform extensive simulations in maintenance of\nprecise formations during flight, efficient data collection from smart meters,\nand adaptation to various failure scenarios. Importantly, we analyze the energy\nconsumption of the proposed system in both drone flight operations and network\ncommunication. We now propose a battery sizing strategy and provide an estimate\nof the operational lifetime of the swarm, underlining the feasibility and\npracticality of our approach. Our results show that UAV swarms have great\npotential to revolutionize smart metering and to bring a further brick to\ngreener and more resilient smart cities.",
        "Scanning the sharp metal tip of a scanning tunneling microscope (STM) over a\nmolecule allows tuning the coupling between the tip plasmon and a molecular\nfluorescence emitter. This allows access to local variations of fluorescence\nfield enhancement and wavelength shifts, which are central parameters for\ncharacterizing the plasmon-exciton coupling. Performing the same for\nphosphorescence with molecular scale resolution remains a significant\nchallenge. In this study, we present the first investigation of phosphorescence\nfrom isolated Pt-Phthalocyanine molecules by analyzing tip-enhanced emission\nspectra in both current-induced and laser-induced phosphorescence. The latter\ndirectly monitors singlet-to-triplet state intersystem crossing of a molecule\nbelow the tip. The study paves the way to a detailed understanding of triplet\nexcitation pathways and their potential control at sub-molecular length scales.\nAdditionally, the coupling of organic phosphors to plasmonic structures is a\npromising route for the improving light-emitting diodes.",
        "Recent advances in code generation models have demonstrated impressive\ncapabilities in automating software development tasks, yet these models still\nstruggle in real-world software engineering scenarios. Although current\ntraining methods, particularly post-training, excel at solving competitive\nprogramming problems, they fail to adequately prepare models for the\ncomplexities of practical software development. This misalignment raises the\ncritical question: Are existing alignment training methods well suited for\nreal-world software engineering tasks? In this study, we identify this issue\nand propose SEAlign, a novel alignment framework designed to bridge the gap\nbetween code generation models and real-world software development tasks.\nSEAlign leverages the unique characteristics of software engineering processes,\nincluding high-quality workflow steps, to enhance model capabilities. Our\nframework further employs Monte Carlo Tree Search for fine-grained alignment in\nmulti-step decision processes, followed by preference optimization on critical\nactions to ensure models meet real-world requirements. We evaluate SEAlign on\nthree standard agentic benchmarks for real-world software engineering,\nincluding HumanEvalFix, SWE-Bench-Lite, and SWE-Bench-Verified. Experimental\nresults demonstrate state-of-the-art performance with minimal training\noverhead. In addition, we develop an agent-based software development platform\nusing SEAlign, which successfully automates the creation of several small\napplications. Human evaluations of these applications highlight significant\nimprovements in both task performance and user experience. Our findings\nunderscore the potential of SEAlign to accelerate the adoption of large code\nmodels in real-world software development. We believe that this research makes\na meaningful step towards fully automated software engineering.",
        "We consider a continuous-time financial market with an asset whose price is\nmodeled by a linear stochastic differential equation with drift and volatility\nswitching driven by a uniformly ergodic jump Markov process with a countable\nstate space (in fact, this is a Black-Scholes model with Markov switching). We\nconstruct a multiplicative scheme of series of discrete-time markets with\ndiscrete-time Markov switching. First, we establish that the discrete-time\nswitching Markov chains weakly converge to the limit continuous-time Markov\nprocess. Second, having this in hand, we apply conditioning on Markov chains\nand prove that the discrete-time market models themselves weakly converge to\nthe Black-Scholes model with Markov switching. The convergence is proved under\nvery general assumptions both on the discrete-time net profits and on a\ngenerator of a continuous-time Markov switching process.",
        "Sco X-1 is the brightest observed extra-solar X-ray source, which is a\nneutron star (NS) low-mass X-ray binary (LMXB), and is thought to have a strong\npotential for continuous gravitational waves (CW) detection due to its high\naccretion rate and relative proximity. Here, we compute the long-term evolution\nof its parameters, particularly the NS spin frequency ($\\nu$) and the surface\nmagnetic field ($B$), to probe its nature and its potential for CW detection.\nWe find that Sco X-1 is an unusually young ($\\sim7\\times10^6$ yr) LMXB and\nconstrain the current NS mass to $\\sim 1.4-1.6~{\\rm M}_\\odot$. Our computations\nreveal a rapid $B$ decay, with the maximum current value of $\\sim\n1.8\\times10^8$ G, which can be useful to constrain the decay models. Note that\nthe maximum current $\\nu$ value is $\\sim 550$ Hz, implying that, unlike what is\ngenerally believed, a CW emission is not required to explain the current source\nproperties. However, $\\nu$ will exceed an observed cut-off frequency of $\\sim\n730$ Hz, and perhaps even the NS break-up frequency, in the future, without a\nCW emission. The minimum NS mass quadrupole moment ($Q$) to avoid this is $\\sim\n(2-3)\\times10^{37}$ g cm$^2$, corresponding to a CW strain of $\\sim 10^{-26}$.\nOur estimation of current $\\nu$ values can improve the CW search sensitivity.",
        "The rapid evolution of communication networks towards 6G increasingly\nincorporates advanced AI-driven controls across various network segments to\nachieve intelligent, zero-touch operation. This paper proposes a comprehensive\nand modular framework for AI controllers, designed to be highly flexible and\nadaptable for use across both fiber optical and radio networks. Building on the\nprinciples established by the O-RAN Alliance for near-Real-Time RAN Intelligent\nControllers (near-RT RICs), our framework extends this AI-driven control into\nthe optical domain. Our approach addresses the critical need for a unified AI\ncontrol framework across diverse network transport technologies and domains,\nenabling the development of intelligent, automated, and scalable 6G networks.",
        "Electron-phonon ($e$-ph) interactions arise in many strongly correlated\nquantum materials from the modulation of the nearest-neighbor hopping\nintegrals, as in the celebrated Su-Schrieffer-Heeger (SSH) model. Nevertheless,\nrelatively few non-perturbative studies of correlated SSH models have been\nconducted in dimensions greater than one, and those that have been done have\nprimarily focused on bond models, where generalized displacements independently\nmodulate each hopping integral. We conducted a sign-problem free determinant\nquantum Monte Carlo study of the optical SSH-Hubbard model on a two-dimensional\nsquare lattice, where site-centered phonon modes simultaneously modulate pairs\nof nearest-neighbor hopping integrals. We report the model's low-temperature\nphase diagram in the challenging adiabatic regime ($\\Omega\/E_\\mathrm{F} \\sim\n1\/8$). It exhibits insulating antiferromagnetic Mott and bond-order-wave (BOW)\nphases with a narrow region of coexistence between them. We also find that a\ncritical $e$-ph coupling is required to stabilize the BOW phase in the small\n$U$ limit. Lastly, in stark contrast to recent findings for the model's bond\nvariant, we find no evidence for a long-range antiferromagnetism in the pure\n$(U\/t=0)$ optical SSH model."
      ]
    }
  },
  {
    "id":2411.16349,
    "research_type":"applied",
    "start_id":"b18",
    "start_title":"Data-driven science and engineering: machine learning, dynamical systems, and control",
    "start_abstract":"\"Data-driven science and engineering: machine learning, dynamical systems, control.\" Contemporary Physics, 60(4), p. 320",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Cerebral aneurysms. New Engl. J. Medicine"
      ],
      "abstract":[
        "Saccular intracranial aneurysms cause substantial morbidity and mortality. Recently, major changes have occurred in the way we think about and treat this disease. This review discusses the percutaneous endovascular treatment of intracranial aneurysms as compared with surgical intervention. The technological advances and supporting research contributing to this important change in practice patterns are reviewed."
      ],
      "categories":[
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "Tumor microenvironment (Part I): Tissue integrity in a rat model of\n  peripheral neural cancer",
        "Evolvable Soma Theory of Ageing: Insights from Computer Simulations",
        "Effect of a new type of healthy and live food supplement on osteoporosis\n  blood parameters and induced rheumatoid arthritis in Wistar rats",
        "An LLM-Powered Clinical Calculator Chatbot Backed by Verifiable Clinical\n  Calculators and their Metadata",
        "Photodynamic, UV-curable and fibre-forming polyvinyl alcohol derivative\n  with broad processability and staining-free antibacterial capability",
        "Bridging high resolution sub-cellular imaging with physiologically\n  relevant engineered tissues",
        "Simplified model of immunotherapy for glioblastoma multiforme: cancer\n  stem cells hypothesis perspective",
        "Unveiling sex dimorphism in the healthy cardiac anatomy: fundamental\n  differences between male and female heart shapes",
        "Integrating anatomy and electrophysiology in the healthy human heart:\n  Insights from biventricular statistical shape analysis using universal\n  coordinates",
        "Geometric immunosuppression in CAR-T cell treatment: Insights from\n  mathematical modeling",
        "Practical parameter identifiability of respiratory mechanics in the\n  extremely preterm infant",
        "Advanced 3D-Printed Multiphasic Scaffold with Optimal PRP Dosage for\n  Chondrogenesis of BM-MSCs in Osteochondral Tissue Engineering",
        "A tumor-immune model of chronic myeloid leukemia with optimal\n  immunotherapeutic protocols",
        "On the hierarchy of plate models for a singularly perturbed multi-well\n  nonlinear elastic energy",
        "Some topological genera and Jacobi forms",
        "A Fresh Perspective on Water Dynamics in Aqueous Salt Solutions",
        "Probing topological matter and fermion dynamics on a neutral-atom\n  quantum computer",
        "Irregularities of distribution and Fourier transforms of\n  multi-dimensional convex bodies",
        "Photonic heat amplifiers based on Anderson insulators",
        "Frequency-noise-insensitive universal control of Kerr-cat qubits",
        "Bilateral Bailey pairs and Rogers-Ramanujan type identities",
        "Entropy Inequalities Constrain Holographic Erasure Correction",
        "From Data to Combinatorial Multivector field Through an\n  Optimization-Based Framework",
        "Model Predictive and Reinforcement Learning Methods for Active Flow\n  Control of an Airfoil with Dual-point Excitation of Plasma Actuators",
        "Bursty acceleration and 3D trajectories of electrons in a solar flare",
        "Test fields and naked singularities: is the second law the cosmic\n  censor?",
        "Lonely passenger problem: the more buses there are, the more lonely\n  passengers there will be",
        "Universal self-gravitating skyrmions"
      ],
      "abstract":[
        "ICAM-1 (intercellular adhesion molecule 1) and MPZ (myelin protein zero) are\nthought to be a factor in the integrity of nerve tissues. In this report, we\nattempted to trace the expression of ICAM-1, responsible for cell-to-cell\nadhesion, and of MPZ, the main constituent of myelin sheath, in malignant\ntissues of the sciatic nerve (SN) in inbred male Copenhagen rats. AT-1 Cells\n(anaplastic tumor 1) were injected in the perineurial sheath, and tissues of\nthe SNs were collected after 7, 14 and 21 days and compared to a sham-operated\ngroup of rats (n = 6 each). Tissues were sectioned and histologically examined,\nunder light microscope, and stained for measuring the immunoreactivity of\nICAM-1 and MPZ under laser scanning microscope. The cancer model was\nestablished, and the tumor growth was confirmed. ICAM-1 showed severe\ndecreases, proportional to the growing anaplastic cells, as compared to the\nsham group. MPZ revealed, however, a distinct defensive pattern before\nsubstantially decreasing in a comparison with sham. These results support the\nnotion that malignancies damage peripheral nerves and cause severe axonal\ninjury and loss of neuronal integrity, and clearly define the role of ICAM-1\nand MPZ in safeguarding the nerve tissues.",
        "Biological evolution continuously refines the design of species, resulting in\nhighly optimised organisms over hundreds of millennia. Intuitively, we expect\nthat random changes-evolution's primary mechanism-are more likely to be harmful\nthan beneficial, leading to widespread detrimental effects in evolving species.\nThe Evolvable Soma Theory of Ageing (ESTA) suggests that ageing is the\ncumulative result of these harmful effects, which predominantly cause bodily\ndamage, while a few may lead to beneficial adaptations that evolution can\nexploit. While the disposable soma theory views ageing as a consequence of\nlimited evolutionary pressure, ESTA posits that ageing is essentially evolution\nin action. In this study, we gather evidence supporting this theory through\ncomputer simulations. We conduct experiments using a platform where genes are\nlinked to onset values that determine when they are expressed. Three scenarios\nare tested: one with single-point fitness evaluation, constant mutation rate\nand fixed gene onsets; one with single-point fitness evaluation,\nonset-dependent mutation rate and fixed gene onsets; and one with spread\nfitness evaluation, onset-dependent mutation rate and evolvable gene onsets.\nThe last scenario, which embodies the evolvable soma hypothesis, demonstrates\nsuperior performance in both algorithmic efficiency and biological plausibility\ncompared to the others.",
        "Summary Osteoporosis is a skeletal disorder, characterized by a decrease in\nbone strength and puts the individual at risk for fracture. On the other hand,\nrheumatoid arthritis is a systemic disease of unknown etiology that causes\ninflammation of the joints of the organs. Purpose Due to the destructive\neffects of these diseases and its increasing prevalence and lack of appropriate\nmedication for treatment, the present study aimed to evaluate the therapeutic\neffect of a new type of healthy and live food supplement on rheumatoid\narthritis and induced osteoporosis in rats. Methods In this research, healthy\nand live food powder were synthesized by a new and green route. This organic\nbiomaterial was named NBS. The NBS food supplement had various vitamins, macro\nand micro molecules, and ingredients. The new healthy and nutritious diet\nshowed that the use of this supplement led to the return of the parameters to\nnormal levels. Results The concentration of 12.5 mg\/ kg showed the least\ntherapeutic effect and 50 mg\/ kg had the highest therapeutic effect for\nosteoporosis. The results of blood parameters involved in inflammation in both\nhealthy and patient groups showed that the use of complete adjuvant induction\ncauses joint inflammation. In the study of the interaction of the\nconcentrations, it was observed that the concentration of 50 mg\/ kg had the\nhighest therapeutic effect against the disease in the studied mice. Conclusion\nThe results showed that the new healthy and viable supplement restores the\nblood osteoporotic and rheumatoid factors of the mice to normal.",
        "Clinical calculators are widely used, and large language models (LLMs) make\nit possible to engage them using natural language. We demonstrate a\npurpose-built chatbot that leverages software implementations of verifiable\nclinical calculators via LLM tools and metadata about these calculators via\nretrieval augmented generation (RAG). We compare the chatbot's response\naccuracy to an unassisted off-the-shelf LLM on four natural language\nconversation workloads. Our chatbot achieves 100% accuracy on queries\ninterrogating calculator metadata content and shows a significant increase in\nclinical calculation accuracy vs. the off-the-shelf LLM when prompted with\ncomplete sentences (86.4% vs. 61.8%) or with medical shorthand (79.2% vs.\n62.0%). It eliminates calculation errors when prompted with complete sentences\n(0% vs. 16.8%) and greatly reduces them when prompted with medical shorthand\n(2.4% vs. 18%). While our chatbot is not ready for clinical use, these results\nshow progress in minimizing incorrect calculation results.",
        "Antimicrobial photodynamic therapy (APDT) is a promising antibiotic-free\nstrategy for broad-spectrum infection control in chronic wounds, minimising\nbacterial resistance risks. However, rapid photosensitiser diffusion, tissue\nstaining, side toxicity, and short-lived antimicrobial effects present\nsignificant clinical limitations for integrating APDT into wound dressings. To\naddress these challenges, we present the design of a bespoke polyvinyl alcohol\n(PVA) derivative conjugated with both phenothiazine and methacrylate\nfunctionalities, enabling staining-free antibacterial photodynamic effects,\ncellular tolerability and processability into various wound dressing formats,\nincluding films, textile fibres and nanoscale coatings. Tosylation of PVA is\nleveraged for the covalent coupling of toluidine blue, as confirmed by UV-Vis\nspectroscopy and the minimal release of TB in vitro. UV-induced network\nformation is exploited to accomplish cast films and nanoscale integrated wound\ndressing coatings. UV curing is also successfully coupled with an in-house wet\nspinning process to realise individual, water-insoluble fibres as the building\nblocks of fibrous wound dressings. A fluorometric assay supports the generation\nof reactive oxygen species when the UV-cured samples are exposed to work, but\nnot UV, light, yielding a mean log10 reduction of up to 2.13 in S. aureus, and\nthe complete eradication of P. aeruginosa. Direct and extract cytotoxicity\ntests with UV-cured films and fibres demonstrate the viability of L929\nfibroblasts following 60-min light irradiation and 72-hour cell culture. The\nbespoke molecular architecture, broad processability and cellular tolerability\nof this PVA derivative are highly attractive aiming to integrate durable\nstaining-free photodynamic capability in a wide range of healthcare\ntechnologies, from chronic wound dressings up to minimally invasive localised\ntherapy.",
        "While high-resolution microscopic techniques are crucial for studying\ncellular structures in cell biology, obtaining such images from thick 3D\nengineered tissues remains challenging. In this review, we explore advancements\nin fluorescence microscopy, alongside the use of various fluorescent probes and\nmaterial processing techniques to address these challenges. We navigate through\nthe diverse array of imaging options available in tissue engineering field,\nfrom wide field to super-resolution microscopy, so researchers can make more\ninformed decisions based on the specific tissue and cellular structures of\ninterest. Finally, we provide some recent examples of how traditional\nlimitations on obtaining high-resolution images on sub-cellular architecture\nwithin 3D tissues have been overcome by combining imaging advancements with\ninnovative tissue engineering approaches.",
        "Despite ongoing efforts in cancer research, a fully effective treatment for\nglioblastoma multiforme (GBM) is still unknown. Since adoptive cell transfer\nimmunotherapy is one of the potential cure candidates, efforts have been made\nto assess its effectiveness using mathematical modeling. In this paper, we\nconsider a model of GBM immunotherapy proposed by Abernathy and Burke (2016),\nwhich also takes into account the dynamics of cancer stem cells, i.e., the type\nof cancer cells that are hypothesized to be largely responsible for cancer\nrecurrence. We modify the initial ODE system by applying simplifying\nassumptions and analyze the existence and stability of steady states of the\nobtained simplified model depending on the treatment levels.",
        "Sex-based differences in cardiovascular disease are well documented, yet the\nprecise nature and extent of these discrepancies in cardiac anatomy remain\nincompletely understood. Traditional scaling models often fail to capture the\ninterplay of age, blood pressure, and body size, prompting a more nuanced\ninvestigation. Here, we employ statistical shape modeling in a healthy subset\n(n=456) of the UK Biobank to explore sex-specific variations in biventricular\nanatomy. We reconstruct 3D meshes and perform multivariate analyses of shape\ncoefficients, controlling for age, blood pressure, and various body size\nmetrics. Our findings reveal that sex alone explains at least 25 percent of\nmorphological variability, with strong discrimination between men and women\n(AUC=0.96-0.71) persisting even after correction for confounders. Notably, the\nmost discriminative modes highlight pronounced differences in cardiac chamber\nvolumes, the anterior-posterior width of the right ventricle, and the relative\npositioning of the cardiac chambers. These results underscore that sex has a\nfundamental influence on cardiac morphology, which may have important clinical\nimplications for differing cardiac structural assessments in men and women.\nFuture work should investigate how these anatomical differences manifest in\nvarious cardiovascular conditions, ultimately paving the way for more precise\nrisk stratification and personalized therapeutic strategies for both men and\nwomen.",
        "A cardiac digital twin is a virtual replica of a patient-specific heart,\nmimicking its anatomy and physiology. A crucial step of building a cardiac\ndigital twin is anatomical twinning, where the computational mesh of the\ndigital twin is tailored to the patient-specific cardiac anatomy. In a number\nof studies, the effect of anatomical variation on clinically relevant\nfunctional measurements like electrocardiograms (ECGs) is investigated, using\ncomputational simulations. While such a simulation environment provides\nresearchers with a carefully controlled ground truth, the impact of anatomical\ndifferences on functional measurements in real-world patients remains\nunderstudied. In this study, we develop a biventricular statistical shape model\nand use it to quantify the effect of biventricular anatomy on ECG-derived and\ndemographic features, providing novel insights for the development of digital\ntwins of cardiac electrophysiology. To this end, a dataset comprising\nhigh-resolution cardiac CT scans from 271 healthy individuals, including\nathletes, is utilized. Furthermore, a novel, universal, ventricular\ncoordinate-based method is developed to establish lightweight shape\ncorrespondence. The performance of the shape model is rigorously established,\nfocusing on its dimensionality reduction capabilities and the training data\nrequirements. Additionally, a comprehensive synthetic cohort is made available,\nfeaturing ready-to-use biventricular meshes with fiber structures and\nanatomical region annotations. These meshes are well-suited for\nelectrophysiological simulations.",
        "Chimeric antigen receptor T (CAR-T) cell therapy has emerged as a promising\ntreatment for hematological malignancies, offering a targeted approach to\ncancer treatment. Understanding the complexities of CAR-T cell therapy within\nsolid tumors poses challenges due to the intricate interactions within the\ntumor microenvironment. Mathematical modeling may serve as a valuable tool to\nunravel the dynamics of CAR-T cell therapy and improve its effectiveness in\nsolid tumors. This study aimed to investigate the impact of spatial aspects in\nCAR-T therapy of solid tumors, utilizing cellular automata for modeling\npurposes. Our main objective was to deepen our understanding of treatment\neffects by analyzing scenarios with different spatial distributions and varying\nthe initial quantities of tumor and CAR-T cells. Tumor geometry significantly\ninfluenced treatment efficacy in-silico, with notable differences observed\nbetween tumors with block-like arrangements and those with sparse cell\ndistributions, leading to the concept of immune suppression due to geometrical\neffects. This research delves into the intricate relationship between spatial\ndynamics and the effectiveness of CAR-T therapy in solid tumors, highlighting\nthe relevance of tumor geometry in the outcome of cellular immunotherapy\ntreatments. Our results provide a basis for improving the efficacy of CAR-T\ncell treatments by combining them with other ones reducing the density of\ncompact tumor areas and thus opening access ways for tumor killing T-cells.",
        "The complexity of mathematical models describing respiratory mechanics has\ngrown in recent years, however, parameter identifiability of such models has\nonly been studied in the last decade in the context of observable data. This\nstudy investigates parameter identifiability of a nonlinear respiratory\nmechanics model tuned to the physiology of an extremely preterm infant, using\nglobal Morris screening, local deterministic sensitivity analysis, and singular\nvalue decomposition-based subset selection. The model predicts airflow and\ndynamic pulmonary volumes and pressures under varying levels of continuous\npositive airway pressure, and a range of parameters characterizing both\nsurfactant-treated and surfactant-deficient lung. Sensitivity analyses\nindicated eleven parameters influence model outputs over the range of\ncontinuous positive airway pressure and lung health scenarios. The model was\nadapted to data from a spontaneously breathing 1 kg infant using gradient-based\noptimization to estimate the parameter subset characterizing the patient's\nstate of health.",
        "In osteochondral tissue engineering (OCTE), simultaneously regenerating\nsubchondral bone and cartilage tissue presents a significant challenge.\nMultiphasic scaffolds were created and manufactured using 3D printing to\naddress this issue. Excellent interfacial mechanical properties and\nbiocompatibility enhance the growth and chondrogenic differentiation of bone\nmarrow mesenchymal stem cells (BM-MSCs). The subchondral bone bottom layer is\nmimicked by incorporating varying concentrations of graphene oxide (GO) (0%,\n1%, and 2% w\/v) into a bioink composed of alginate (Alg) and gelatin (Gel).\nBased on evaluations of mechanical and biocompatibility properties, 1% GO is\nselected for further studies. Subsequently, the GO concentration is kept\nconstant while varying the platelet-rich plasma (PRP) dosage in the multiphasic\nscaffolds. Different PRP dosages (0%, 1%, 2%, and 3% w\/v) are integrated into\nthe Alg-Gel bioink to simulate cartilage tissues. Results indicate that\n3D-printed scaffolds containing 1% or 2% PRP exhibit favorable biomechanical\nproperties, with no significant differences observed. However, BM-MSCs exposed\nto 2% PRP demonstrate enhanced adhesion, growth, and viability. Additionally,\nreal-time PCR and Alcian blue staining confirm increased chondrogenic\nexpression and glycosaminoglycans (GAGs) synthesis. This work highlights the\npromising potential of 3D-printed multiphasic frameworks in the development of\nOCTE.",
        "The interactions between tumor cells and the immune system play a crucial\nrole in cancer evolution. In this study, we explore how these interactions\ninfluence cancer progression by modeling the relationships among naive T cells,\neffector T cells, and chronic myeloid leukemia cells. We examine the existence\nof equilibria, the asymptotic stability of the positive steady state, and the\nglobal stability of the tumor-free equilibrium. Additionally, we develop a\npartial differential equation to describe the conditions under which the\nconcentration of cancer cells reaches a level that allows for effective control\nof cancer evolution. Finally, we apply our proposed model to investigate\noptimal treatment strategies that aim to minimize both the concentration of\ncancer cells at the end of treatment and the accumulation of tumor burden, as\nwell as the cost associated with treatment during the intervention period. Our\nstudy reveals an optimal therapeutic protocol using optimal control theory. We\nperform numerical simulations to illustrate our theoretical results and to\nexplore the dynamic behavior of the system and optimal therapeutic protocols.\nThe simulations indicate that the optimal treatment strategy can be more\neffective than a constant treatment approach, even when applying the same\ntreatment interval and total drug input.",
        "In the celebrated work of Friesecke, James and M\\\"uller '06 the authors\nderive a hierarchy of models for plates by carefully analyzing the\n$\\Gamma$-convergence of the rescaled nonlinear elastic energy. The key\ningredient of their proofs is the rigidity estimate proved in an earlier work\nof theirs. Here we consider the case in which the elastic energy has a\nmulti-well structure: this type of functional arises, for example, in the study\nof solid-solid phase transitions. Since the rigidity estimate fails in the case\nof compatible wells, we follow Alicandro, Dal Maso, Lazzaroni and Palombaro '18\nand add a regularization term to the energy that penalizes jumps from one well\nto another, leading to good compactness properties. In this setting we recover\nthe full hierarchy of plate models with an explicit dependence on the wells.\nFinally, we study the convergence of energy minimizers with suitable external\nforces and full Neumann boundary conditions. To do so, we adapt the definition\nof optimal rotations introduced by Maor, Mora '21.",
        "We revisit and elucidate the $\\widehat{A}$-genus, Hirzebruch's $L$-genus and\nWitten's $W$-genus, cobordism invariants of special classes of manifolds. After\nslight modification, involving Hecke's trick, we find that the\n$\\widehat{A}$-genus and $L$-genus arise directly from Jacobi's theta function.\nIn this way, for every $k\\geq 0,$ we obtain exact formulas for the quasimodular\nexpressions of $\\widehat{A}_k$ and $L_k$ as ``traces'' of partition Eisenstein\nseries \\[\\widehat{\\mathcal{A}}_k(\\tau)=\n\\operatorname{Tr}_k(\\phi_{\\widehat{A}};\\tau)\\ \\ \\ \\ \\ \\ {\\text {\\rm and}}\\ \\ \\\n\\ \\ \\ \\mathcal{L}_k(\\tau)= \\operatorname{Tr}_k(\\phi_L;\\tau). \\] Surprisingly,\nRamanujan defined twists of the $\\widehat{\\mathcal{A}}_k(\\tau)$ in his ``lost\nnotebook'' in his study of derivatives of theta functions, decades before Borel\nand Hirzebruch rediscovered them in the context of spin manifolds. In addition,\nwe show that the nonholomorphic $G_2^{\\star}$-completion of the characteristic\nseries of the Witten genus is the Jacobi theta function avatar of the\n$\\widehat{A}$-genus.",
        "Molecular dynamics in pure water and aqueous salt solutions remain\nincompletely understood, partly due to the apparent contradictions between\nresults from different spectroscopic techniques. In this work, we demonstrate,\nby detailed comparison of light scattering and dielectric spectroscopy data for\npure water and aqueous lithium chloride solutions, that these apparent\ncontradictions can be resolved by accounting for orientational\ncross-correlations of neighboring molecules. Remarkably, a single structural\nrelaxation mode with largely temperature- and concentration-independent shape\ncan be identified in all spectra, from room temperature down to the deeply\nsupercooled regime. These results provide a new perspective for the study of\nmolecular dynamics in aqueous salt solutions.",
        "Quantum simulations of many-body systems are among the most promising\napplications of quantum computers. In particular, models based on\nstrongly-correlated fermions are central to our understanding of quantum\nchemistry and materials problems, and can lead to exotic, topological phases of\nmatter. However, due to the non-local nature of fermions, such models are\nchallenging to simulate with qubit devices. Here we realize a digital quantum\nsimulation architecture for two-dimensional fermionic systems based on\nreconfigurable atom arrays. We utilize a fermion-to-qubit mapping based on\nKitaev's model on a honeycomb lattice, in which fermionic statistics are\nencoded using long-range entangled states. We prepare these states efficiently\nusing measurement and feedforward, realize subsequent fermionic evolution\nthrough Floquet engineering with tunable entangling gates interspersed with\natom rearrangement, and improve results with built-in error detection.\nLeveraging this fermion description of the Kitaev spin model, we efficiently\nprepare topological states across its complex phase diagram and verify the\nnon-Abelian spin liquid phase by evaluating an odd Chern number. We further\nexplore this two-dimensional fermion system by realizing tunable dynamics and\ndirectly probing fermion exchange statistics. Finally, we simulate strong\ninteractions and study dynamics of the Fermi-Hubbard model on a square lattice.\nThese results pave the way for digital quantum simulations of complex fermionic\nsystems for materials science, chemistry, and high-energy physics.",
        "W. Schmidt, H. Montgomery, and J. Beck proved a result on irregularities of\ndistribution with respect to $d$-dimensional balls. In this paper, we extend\ntheir result to any $d$-dimensional convex body with a smooth boundary and\nfinite order of contact. As an intermediate step, we prove a geometric\ninequality for the Fourier transform of the characteristic function of a convex\nbody.",
        "A photonic heat amplifier (PHA) designed for cryogenic operations is\nintroduced and analyzed. This device comprises two Anderson insulator\nreservoirs connected by lossless lines, allowing them to exchange heat through\nphotonic modes. This configuration enables negative differential thermal\nconductance (NDTC), which can be harnessed to amplify thermal signals. To\nachieve this, we maintain one reservoir at a high temperature, serving as the\nsource terminal of a thermal transistor. Concurrently, in the other one, we\nestablish tunnel contacts to metallic reservoirs, which function as the gate\nand drain terminals. With this arrangement, it is possible to control the heat\nflux exchange between the source and drain by adjusting the gate temperature.\nWe present two distinct parameter choices that yield different performances:\nthe first emphasizes modulating the source-drain heat current, while the second\nfocuses on the temperature modulation of the colder Anderson insulator. Lastly,\nwe present a potential design variation in which all electronic reservoirs are\nthermally connected through only photonic modes, allowing interactions between\ndistant elements. The proposal of the PHA addresses the lack of thermal\ntransistors and amplifiers in the mK range while being compatible with the rich\ntoolbox of circuit quantum electrodynamics. It can be adapted to various\napplications, including sensing and developing thermal circuits and control\ndevices at sub-Kelvin temperatures, which are relevant to quantum technologies.",
        "We theoretically study the influence of frequency uncertainties on the\noperation of a Kerr-cat qubit. As the mean photon number increases, Kerr-cat\nqubits provide an increasing level of protection against phase errors induced\nby unknown frequency shifts during idling and X rotations. However, realizing\nrotations about the other principal axes (e.g., Y and Z axes) while preserving\nrobustness is nontrivial. To address this challenge, we propose a universal set\nof gate schemes which circumvents the tradeoff between protection and\ncontrollability in Kerr-cat qubits and retains robustness to unknown frequency\nshifts to at least first order. Assuming an effective Kerr oscillator model, we\ntheoretically and numerically analyze the robustness of elementary gates on\nKerr-cat qubits, with special focus on gates along nontrivial rotation axes. An\nappealing application of this qubit design would include tunable\nsuperconducting platforms, where the induced protection against frequency noise\nwould allow for a more flexible choice of operating point and thus the\npotential mitigation of the impact of spurious two-level systems.",
        "Rogers-Ramanujan type identities occur in various branches of mathematics and\nphysics. As a classic and powerful tool to deal with Rogers-Ramanujan type\nidentities, the theory of Bailey's lemma has been extensively studied and\ngeneralized. In this paper, we found a bilateral Bailey pair that naturally\narises from the q-binomial theorem. By applying the bilateral versions of\nBailey lemmas, Bailey chains and Bailey lattices, we derive a number of\nRogers-Ramanujan type identities, which unify many known identities as special\ncases. Further combined with the bilateral Bailey chains due to Berkovich,\nMcCoy and Schilling and the bilateral Bailey lattices due to Jouhet et al., we\nalso obtain identities on Appell-Lerch series and identities of Andrews-Gordon\ntype. Moreover, by applying Andrews and Warnaar's bilateral Bailey lemmas, we\nderive identities on Hecke-type series.",
        "We interpret holographic entropy inequalities in terms of erasure correction.\nThe non-saturation of an inequality is a necessary condition for certain\nschemes of holographic erasure correction, manifested in the bulk as non-empty\noverlaps of corresponding entanglement wedges.",
        "This paper extends and generalizes previous works on constructing\ncombinatorial multivector fields from continuous systems (see [10]) and the\nconstruction of combinatorial vector fields from data (see [2]) by introducing\nan optimization based framework for the construction of combinatorial\nmultivector fields from finite vector field data. We address key challenges in\nconvexity, computational complexity and resolution, providing theoretical\nguarantees and practical methodologies for generating combinatorial\nrepresentation of the dynamics of our data.",
        "This paper presents an analysis of Model Predictive Control (MPC) and\nReinforcement Learning (RL) approaches for active flow control over a NACA\n(National Advisory Committee for Aeronautics) 4412 airfoil around the static\nstall condition at a Reynolds number of 4*10^5. The Reynolds Averaged\nNavier-Stokes (RANS) equations with the Scale-Adaptive Simulation (SAS)\nturbulence model are utilized for the numerical simulations. The dielectric\nbarrier discharge (DBD) plasma actuators were employed in dual excitation mode\nfor flow separation control. The study systematically evaluates adaptive MPC,\ntemporal difference reinforcement learning (TDRL), and deep Q-learning (DQL)\nbased on optimizing the excitation frequency and expediting the time to\nidentify stable conditions. Moreover, an integrated approach that combines\nsignal processing with DQL is examined. The results demonstrate that while MPC\nand RL significantly improve flow control, RL approaches offer superior\nadaptability and performance. In optimal conditions, a lift coefficient of\naround 1.619 was achieved within less than 2.5 seconds with an excitation\nfrequency of 100 or 200 Hz. This research highlights that RL-based approaches\ncould perform better in flow control applications than MPC.",
        "During a solar flare, electrons are accelerated to non-thermal energies as a\nresult of magnetic reconnection. These electrons then propagate upwards and\ndownwards from the energy release site along magnetic field lines and produce\nradio and X-ray emission. On 11 November 2022, an M5.1 solar flare was observed\nby the Spectrometer\/Telescope for Imaging X-rays (STIX) on board Solar Orbiter\ntogether with various ground- and space-based radio instruments. The flare was\nassociated with several fine hard X-ray (HXR) structures and a complex set of\nmetric radio bursts (type III, J, and narrowband). By studying the evolution of\nX-ray, extreme ultraviolet, and radio sources, we aim to study the trajectories\nof the flare-accelerated electrons in the lower solar atmosphere and low\ncorona. We used observations from the STIX on board Solar Orbiter to study the\nevolution of X-ray sources. Using radio imaging from the Nan\\c{c}ay Radio\nheliograph (NRH) and the Newkirk density model, we constructed 3D trajectories\nof 14 radio bursts. Imaging of the HXR fine structures shows several sources at\ndifferent times. The STIX and NRH imaging shows correlated changes in the\nlocation of the HXR and radio source at the highest frequency during the most\nintense impulsive period. Imaging and 3D trajectories of all the bursts show\nthat electrons are getting accelerated at different locations and along several\ndistinct field lines. The longitude and latitude extent of the trajectories are\n~30 arcsec and ~ 152 arcsec. We find that the electrons producing HXR and radio\nemission have similar acceleration origins. Importantly, our study supports the\nscenario that the flare acceleration process is temporally and spatially\nfragmentary, and during each of these small-scale processes, the electron beams\nare injected into a very fibrous environment and produce complex HXR and radio\nemission.",
        "It has been claimed that a Kerr-Newman black hole can generically be overspun\nby neutral test fields, and it has been argued that even when backreactions are\ntaken into account, the black hole can still be destroyed. In this paper, we\nrevisit the weak cosmic censorship conjecture for a Kerr-Newman black hole with\na test scalar field and a neutrino field, and point out that the assumption in\nprevious work regarding the energy and angular momentum of the test fields\nabsorbed by the black hole violates the second law of black hole\nthermodynamics. By solving the test scalar field and neutrino field near the\nevent horizon explicitly, we demonstrate that an extremal Kerr-Newman black\nhole cannot be overspun by a test scalar field but can be destroyed by a\nneutrino field. Our results indicate that the condition required to overspin an\nextremal Kerr-Newman black hole coincides with the condition needed to violate\nthe second law of black hole thermodynamics. Furthermore, we observe that such\na violation of the second law might inevitably result in a breakdown of the\nweak cosmic censorship conjecture.",
        "Empty buses are standing at a bus station. $n$ passengers arrive, and they\neach board a bus completely at random (meaning that they choose uniformly and\nindependently). Then all buses depart. We show that the more buses there are,\nthe more likely it is that someone (i.e. at least one passenger) travels alone\n(while $n$ is fixed). More generally, we show that the number of lonely\npassengers increases with the number of buses, in the sense of stochastic\ndominance. This problem turned out to be surprisingly difficult, with no short\nsolution known to the author so far, despite the efforts of many experts. Some\nof the results can also be formulated as properties of Stirling numbers of the\nsecond kind.",
        "The self-gravitating skyrmion is an exact solution of the Einstein\n$SU(2)$-Skyrme model describing a topological soliton with baryon number $B=1$,\nliving in a $4$-dimensional space-time in the presence of a cosmological\nconstant. Here we show that, using the maximal embedding Ansatz of $SU(2)$ into\n$SU(N)$ in the Euler angles parametrization, this solution can be generalized\nto include arbitrary values of the flavor number and, consequently, allowing\nhigher values of the topological charge. Also, we show that higher-order\ncorrections in the 't Hooft expansion can be considered while still preserving\nthe analytical nature of the solutions. Finally we will show that from the\ngravitational solutions it is possible to construct skyrmions in flat\nspace-time at a finite volume."
      ]
    }
  },
  {
    "id":2411.19,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Addressing disparities in the global epidemiology of stroke",
    "start_abstract":"Stroke is the second leading cause of death and the third leading cause of disability worldwide. Though the burden of stroke worldwide seems to have declined in the past three decades, much of this effect reflects decreases in high-income countries (HICs). By contrast, the burden of stroke has grown rapidly in low-income and middle-income countries (LMICs), where epidemiological, socioeconomic and demographic shifts have increased the incidence of stroke and other non-communicable diseases. Furthermore, even in HICs, disparities in stroke epidemiology exist along racial, ethnic, socioeconomic and geographical lines. In this Review, we highlight the under-acknowledged disparities in the burden of stroke. We emphasize the shifting global landscape of stroke risk factors, critical gaps in stroke service delivery, and the need for a more granular analysis of the burden of stroke within and between LMICs and HICs to guide context-appropriate capacity-building. Finally, we review strategies for addressing key inequalities in stroke epidemiology, including improvements in epidemiological surveillance and context-specific research efforts in under-resourced regions, development of the global workforce of stroke care providers, expansion of access to preventive and treatment services through mobile and telehealth platforms, and scaling up of evidence-based strategies and policies that target local, national, regional and global stroke disparities.",
    "start_categories":[
      "q-bio.NC"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b7"
      ],
      "title":[
        "Artificial intelligence applications in stroke"
      ],
      "abstract":[
        "Management of stroke highly depends on information from imaging studies. Noncontrast computed tomography (CT) and magnetic resonance imaging (MRI) can both be used to distinguish between ischemic and hemorrhagic stroke, which is difficult based on clinical features. Hypodensity on CT and DWI hyperintensity on MRI identifies irreversibly damaged tissue, although the sensitivity of MRI is higher in the acute setting. Angiographic and perfusion imaging sequences can identify a large vessel occlusion and, along with perfusion imaging, can select patients for endovascular therapy. The FLAIR-DWI mismatch yields information about patients with unknown time of onset (including wake-up strokes). Stroke imaging also gives insight into prognosis, with current methods aiming to give a picture of the short-term consequences of successful reperfusion or continued large vessel occlusion. One important caveat about stroke imaging is that it must be done quickly, as faster treatment leads to better outcomes.1 However, most steps in the stroke imaging triage pathway require the presence of human radiologists and neurologists, and this is often the time-limiting step. The expertise required for these tasks may not be available at all sites or at all times. Therefore, there is interest in automated methods for stroke imaging evaluation. Artificial intelligence (AI) is a broad term reflecting the use of computers to perform tasks that humans may find difficult, often in ways that are hard to pinpoint. For example, although humans find high-level computation difficult, calculator technology is not considered AI because we know how to break this down into discrete steps and feel we understand it. However, facial recognition is a task that humans perform well, but an algorithm to identify faces is usually considered AI since we cannot articulate precisely how this is done. Machine learning (ML) is a subset of AI in which algorithms learn from the data itself without explicit programming. ML methods reflect a broad range of statistical techniques ranging from linear regression to more complex methods such as support vector machines and decision trees. ML methods can be further broken into supervised and unsupervised learning, which differ from one another in that the former requires access to gold standard labels although the latter attempts to find the answers implicitly in the data itself. While ML methods have grown more popular over recent years, the advent of a specific supervised ML method based on architectures resembling human neural networks over the past decade has led to a quantum leap in performance.2 This method, called deep learning (DL) because of many multiple internal layers, can be considered a transformative technology. Compared with previous methods that required humans to identify image features, a deep neural network trained on a dataset with known outputs can learn the best features for organizing the data. In this review, we will discuss ML methods applied to stroke imaging with an emphasis on DL applications. We refer to Figure for a graphical overview of the applications discussed in this review."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Agency Is Frame-Dependent",
        "AlgoPilot: Fully Autonomous Program Synthesis Without Human-Written\n  Programs",
        "MACI: Multi-Agent Collaborative Intelligence for Adaptive Reasoning and\n  Temporal Planning",
        "Why Do Multi-Agent LLM Systems Fail?",
        "From System 1 to System 2: A Survey of Reasoning Large Language Models",
        "Deconstructing Long Chain-of-Thought: A Structured Reasoning\n  Optimization Framework for Long CoT Distillation",
        "STRIVE: Structured Reasoning for Self-Improvement in Claim Verification",
        "Psychometric-Based Evaluation for Theorem Proving with Large Language\n  Models",
        "ADAM-1: AI and Bioinformatics for Alzheimer's Detection and\n  Microbiome-Clinical Data Integrations",
        "AI-based Identity Fraud Detection: A Systematic Review",
        "DeclareAligner: A Leap Towards Efficient Optimal Alignments for\n  Declarative Process Model Conformance Checking",
        "Intention Recognition in Real-Time Interactive Navigation Maps",
        "Enhancing the Product Quality of the Injection Process Using eXplainable\n  Artificial Intelligence",
        "Complex Brillouin Zone for Localised Modes in Hermitian and\n  Non-Hermitian Problems",
        "Explicit adaptive time stepping for the Cahn-Hilliard equation by\n  exponential Krylov subspace and Chebyshev polynomial methods",
        "On reflected isotropic stable processes",
        "Quantum Quandaries: Unraveling Encoding Vulnerabilities in Quantum\n  Neural Networks",
        "Evaluating Compression and Nanoindentation in FCC Nickel: A Methodology\n  for Interatomic Potential Selection",
        "Integrated Sensing and Communication System Based on Radio Frequency\n  Resonance Beam",
        "Activity of low mass stars in the light of spot signature in the Fourier\n  domain",
        "Impact of structural distortions on the correlated electronic structure\n  of orbital-selective Mott insulating Na$_3$Co$_2$SbO$_6$ under strains",
        "Influence of nanoparticulates and microgrooves on the secondary electron\n  yield and electrical resistance of laser-treated copper surfaces",
        "Accelerated DC loadflow solver for topology optimization",
        "Predicting Spin-Dependent Coulomb Interaction Based on the Yang-Mills\n  Equations",
        "Experimental realization of a quantum heat engine based on\n  dissipation-engineered superconducting circuits",
        "Range-Only Dynamic Output Feedback Controller for Safe and Secure Target\n  Circumnavigation",
        "Quantum Diffie-Hellman key exchange",
        "Criteria for ion acceleration in laboratory magnetized\n  quasi-perpendicular collisionless shocks: when are 2D simulations enough?"
      ],
      "abstract":[
        "Agency is a system's capacity to steer outcomes toward a goal, and is a\ncentral topic of study across biology, philosophy, cognitive science, and\nartificial intelligence. Determining if a system exhibits agency is a\nnotoriously difficult question: Dennett (1989), for instance, highlights the\npuzzle of determining which principles can decide whether a rock, a thermostat,\nor a robot each possess agency. We here address this puzzle from the viewpoint\nof reinforcement learning by arguing that agency is fundamentally\nframe-dependent: Any measurement of a system's agency must be made relative to\na reference frame. We support this claim by presenting a philosophical argument\nthat each of the essential properties of agency proposed by Barandiaran et al.\n(2009) and Moreno (2018) are themselves frame-dependent. We conclude that any\nbasic science of agency requires frame-dependence, and discuss the implications\nof this claim for reinforcement learning.",
        "Program synthesis has traditionally relied on human-provided specifications,\nexamples, or prior knowledge to generate functional algorithms. Existing\nmethods either emulate human-written algorithms or solve specific tasks without\ngenerating reusable programmatic logic, limiting their ability to create novel\nalgorithms. We introduce AlgoPilot, a groundbreaking approach for fully\nautomated program synthesis without human-written programs or trajectories.\nAlgoPilot leverages reinforcement learning (RL) guided by a Trajectory Language\nModel (TLM) to synthesize algorithms from scratch. The TLM, trained on\ntrajectories generated by random Python functions, serves as a soft constraint\nduring the RL process, aligning generated sequences with patterns likely to\nrepresent valid algorithms. Using sorting as a test case, AlgoPilot\ndemonstrates its ability to generate trajectories that are interpretable as\nclassical algorithms, such as Bubble Sort, while operating without prior\nalgorithmic knowledge. This work establishes a new paradigm for algorithm\ndiscovery and lays the groundwork for future advancements in autonomous program\nsynthesis.",
        "Artificial intelligence requires deliberate reasoning, temporal awareness,\nand effective constraint management, capabilities traditional LLMs often lack\ndue to their reliance on pattern matching, limited self-verification, and\ninconsistent constraint handling. We introduce Multi-Agent Collaborative\nIntelligence (MACI), a framework comprising three key components: 1) a\nmeta-planner (MP) that identifies, formulates, and refines all roles and\nconstraints of a task (e.g., wedding planning) while generating a dependency\ngraph, with common-sense augmentation to ensure realistic and practical\nconstraints; 2) a collection of agents to facilitate planning and address\ntask-specific requirements; and 3) a run-time monitor that manages plan\nadjustments as needed. By decoupling planning from validation, maintaining\nminimal agent context, and integrating common-sense reasoning, MACI overcomes\nthe aforementioned limitations and demonstrates robust performance in two\nscheduling problems.",
        "Despite growing enthusiasm for Multi-Agent Systems (MAS), where multiple LLM\nagents collaborate to accomplish tasks, their performance gains across popular\nbenchmarks remain minimal compared to single-agent frameworks. This gap\nhighlights the need to analyze the challenges hindering MAS effectiveness.\n  In this paper, we present the first comprehensive study of MAS challenges. We\nanalyze five popular MAS frameworks across over 150 tasks, involving six expert\nhuman annotators. We identify 14 unique failure modes and propose a\ncomprehensive taxonomy applicable to various MAS frameworks. This taxonomy\nemerges iteratively from agreements among three expert annotators per study,\nachieving a Cohen's Kappa score of 0.88. These fine-grained failure modes are\norganized into 3 categories, (i) specification and system design failures, (ii)\ninter-agent misalignment, and (iii) task verification and termination. To\nsupport scalable evaluation, we integrate MASFT with LLM-as-a-Judge. We also\nexplore if identified failures could be easily prevented by proposing two\ninterventions: improved specification of agent roles and enhanced orchestration\nstrategies. Our findings reveal that identified failures require more complex\nsolutions, highlighting a clear roadmap for future research. We open-source our\ndataset and LLM annotator.",
        "Achieving human-level intelligence requires refining the transition from the\nfast, intuitive System 1 to the slower, more deliberate System 2 reasoning.\nWhile System 1 excels in quick, heuristic decisions, System 2 relies on logical\nreasoning for more accurate judgments and reduced biases. Foundational Large\nLanguage Models (LLMs) excel at fast decision-making but lack the depth for\ncomplex reasoning, as they have not yet fully embraced the step-by-step\nanalysis characteristic of true System 2 thinking. Recently, reasoning LLMs\nlike OpenAI's o1\/o3 and DeepSeek's R1 have demonstrated expert-level\nperformance in fields such as mathematics and coding, closely mimicking the\ndeliberate reasoning of System 2 and showcasing human-like cognitive abilities.\nThis survey begins with a brief overview of the progress in foundational LLMs\nand the early development of System 2 technologies, exploring how their\ncombination has paved the way for reasoning LLMs. Next, we discuss how to\nconstruct reasoning LLMs, analyzing their features, the core methods enabling\nadvanced reasoning, and the evolution of various reasoning LLMs. Additionally,\nwe provide an overview of reasoning benchmarks, offering an in-depth comparison\nof the performance of representative reasoning LLMs. Finally, we explore\npromising directions for advancing reasoning LLMs and maintain a real-time\n\\href{https:\/\/github.com\/zzli2022\/Awesome-Slow-Reason-System}{GitHub\nRepository} to track the latest developments. We hope this survey will serve as\na valuable resource to inspire innovation and drive progress in this rapidly\nevolving field.",
        "Recent advancements in large language models (LLMs) have demonstrated\nremarkable reasoning capabilities through long chain-of-thought (CoT)\nreasoning. The R1 distillation scheme has emerged as a promising approach for\ntraining cost-effective models with enhanced reasoning abilities. However, the\nunderlying mechanisms driving its effectiveness remain unclear. This study\nexamines the universality of distillation data and identifies key components\nthat enable the efficient transfer of long-chain reasoning capabilities in LLM\ndistillation. Our findings reveal that the effectiveness of long CoT reasoning\ndistillation from teacher models like Qwen-QwQ degrades significantly on\nnonhomologous models, challenging the assumed universality of current\ndistillation methods. To gain deeper insights into the structure and patterns\nof long CoT reasoning, we propose DLCoT (Deconstructing Long Chain-of-Thought),\na distillation data enhancement framework. DLCoT consists of three key steps:\n(1) data segmentation to decompose complex long CoT structures, (2)\nsimplification by eliminating unsolvable and redundant solutions, and (3)\noptimization of intermediate error states. Our approach significantly improves\nmodel performance and token efficiency, facilitating the development of\nhigh-performance LLMs.",
        "Claim verification is the task of determining whether a claim is supported or\nrefuted by evidence. Self-improvement methods, where reasoning chains are\ngenerated and those leading to correct results are selected for training, have\nsucceeded in tasks like mathematical problem solving. However, in claim\nverification, this approach struggles. Low-quality reasoning chains may falsely\nmatch binary truth labels, introducing faulty reasoning into the\nself-improvement process and ultimately degrading performance. To address this,\nwe propose STRIVE: Structured Reasoning for Self-Improved Verification. Our\nmethod introduces a structured reasoning design with Claim Decomposition,\nEntity Analysis, and Evidence Grounding Verification. These components improve\nreasoning quality, reduce errors, and provide additional supervision signals\nfor self-improvement. STRIVE begins with a warm-up phase, where the base model\nis fine-tuned on a small number of annotated examples to learn the structured\nreasoning design. It is then applied to generate reasoning chains for all\ntraining examples, selecting only those that are correct and structurally sound\nfor subsequent self-improvement training. We demonstrate that STRIVE achieves\nsignificant improvements over baseline models, with a 31.4% performance gain\nover the base model and 20.7% over Chain of Thought on the HOVER datasets,\nhighlighting its effectiveness.",
        "Large language models (LLMs) for formal theorem proving have become a\nprominent research focus. At present, the proving ability of these LLMs is\nmainly evaluated through proof pass rates on datasets such as miniF2F. However,\nthis evaluation method overlooks the varying importance of theorems. As a\nresult, it fails to highlight the real performance disparities between LLMs and\nleads to high evaluation costs. This study proposes a psychometric-based\nevaluation method for theorem proving with LLMs, comprising two main\ncomponents: Dataset Annotation and Adaptive Evaluation. First, we propose a\nmetric calculation method to annotate the dataset with difficulty and\ndiscrimination metrics. Specifically, we annotate each theorem in the miniF2F\ndataset and grade them into varying difficulty levels according to the\nperformance of LLMs, resulting in an enhanced dataset: miniF2F-Graded.\nExperimental results show that the difficulty grading in miniF2F-Graded better\nreflects the theorem difficulty perceived by LLMs. Secondly, we design an\nadaptive evaluation method to dynamically select the most suitable theorems for\ntesting based on the annotated metrics and the real-time performance of LLMs.\nWe apply this method to evaluate 10 LLMs. The results show that our method\nfinely highlights the performance disparities between LLMs. It also reduces\nevaluation costs by using only 23% of the theorems in the dataset.",
        "The Alzheimer's Disease Analysis Model Generation 1 (ADAM) is a multi-agent\nlarge language model (LLM) framework designed to integrate and analyze\nmulti-modal data, including microbiome profiles, clinical datasets, and\nexternal knowledge bases, to enhance the understanding and detection of\nAlzheimer's disease (AD). By leveraging retrieval-augmented generation (RAG)\ntechniques along with its multi-agent architecture, ADAM-1 synthesizes insights\nfrom diverse data sources and contextualizes findings using literature-driven\nevidence. Comparative evaluation against XGBoost revealed similar mean F1\nscores but significantly reduced variance for ADAM-1, highlighting its\nrobustness and consistency, particularly in small laboratory datasets. While\ncurrently tailored for binary classification tasks, future iterations aim to\nincorporate additional data modalities, such as neuroimaging and biomarkers, to\nbroaden the scalability and applicability for Alzheimer's research and\ndiagnostics.",
        "With the rapid development of digital services, a large volume of personally\nidentifiable information (PII) is stored online and is subject to cyberattacks\nsuch as Identity fraud. Most recently, the use of Artificial Intelligence (AI)\nenabled deep fake technologies has significantly increased the complexity of\nidentity fraud. Fraudsters may use these technologies to create highly\nsophisticated counterfeit personal identification documents, photos and videos.\nThese advancements in the identity fraud landscape pose challenges for identity\nfraud detection and society at large. There is a pressing need to review and\nunderstand identity fraud detection methods, their limitations and potential\nsolutions. This research aims to address this important need by using the\nwell-known systematic literature review method. This paper reviewed a selected\nset of 43 papers across 4 major academic literature databases. In particular,\nthe review results highlight the two types of identity fraud prevention and\ndetection methods, in-depth and open challenges. The results were also\nconsolidated into a taxonomy of AI-based identity fraud detection and\nprevention methods including key insights and trends. Overall, this paper\nprovides a foundational knowledge base to researchers and practitioners for\nfurther research and development in this important area of digital identity\nfraud.",
        "In many engineering applications, processes must be followed precisely,\nmaking conformance checking between event logs and declarative process models\ncrucial for ensuring adherence to desired behaviors. This is a critical area\nwhere Artificial Intelligence (AI) plays a pivotal role in driving effective\nprocess improvement. However, computing optimal alignments poses significant\ncomputational challenges due to the vast search space inherent in these models.\nConsequently, existing approaches often struggle with scalability and\nefficiency, limiting their applicability in real-world settings. This paper\nintroduces DeclareAligner, a novel algorithm that uses the A* search algorithm,\nan established AI pathfinding technique, to tackle the problem from a fresh\nperspective leveraging the flexibility of declarative models. Key features of\nDeclareAligner include only performing actions that actively contribute to\nfixing constraint violations, utilizing a tailored heuristic to navigate\ntowards optimal solutions, and employing early pruning to eliminate\nunproductive branches, while also streamlining the process through\npreprocessing and consolidating multiple fixes into unified actions. The\nproposed method is evaluated using 8,054 synthetic and real-life alignment\nproblems, demonstrating its ability to efficiently compute optimal alignments\nby significantly outperforming the current state of the art. By enabling\nprocess analysts to more effectively identify and understand conformance\nissues, DeclareAligner has the potential to drive meaningful process\nimprovement and management.",
        "In this demonstration, we develop IntentRec4Maps, a system to recognise\nusers' intentions in interactive maps for real-world navigation. IntentRec4Maps\nuses the Google Maps Platform as the real-world interactive map, and a very\neffective approach for recognising users' intentions in real-time. We showcase\nthe recognition process of IntentRec4Maps using two different Path-Planners and\na Large Language Model (LLM).\n  GitHub: https:\/\/github.com\/PeijieZ\/IntentRec4Maps",
        "The injection molding process is a traditional technique for making products\nin various industries such as electronics and automobiles via solidifying\nliquid resin into certain molds. Although the process is not related to\ncreating the main part of engines or semiconductors, this manufacturing\nmethodology sets the final form of the products. Re-cently, research has\ncontinued to reduce the defect rate of the injection molding process. This\nstudy proposes an optimal injection molding process control system to reduce\nthe defect rate of injection molding products with XAI (eXplainable Artificial\nIntelligence) ap-proaches. Boosting algorithms (XGBoost and LightGBM) are used\nas tree-based classifiers for predicting whether each product is normal or\ndefective. The main features to control the process for improving the product\nare extracted by SHapley Additive exPlanations, while the individual\nconditional expectation analyzes the optimal control range of these extracted\nfeatures. To validate the methodology presented in this work, the actual\ninjection molding AI manufacturing dataset provided by KAMP (Korea AI\nManufacturing Platform) is employed for the case study. The results reveal that\nthe defect rate decreases from 1.00% (Original defect rate) to 0.21% with\nXGBoost and 0.13% with LightGBM, respectively.",
        "We develop a mathematical and numerical framework for studying evanescent\nwaves in subwavelength band gap materials. By establishing a link between the\ncomplex Brillouin zone and various Hermitian and non-Hermitian phenomena,\nincluding defect localisation in band gap materials and the non-Hermitian skin\neffect, we provide a unified perspective on these systems. In two-dimensional\nstructures, we develop analytical techniques and numerical methods to study\nsingularities of the complex band structure. This way, we demonstrate that gap\nfunctions effectively predict the decay rates of defect states. Furthermore, we\npresent an analysis of the Floquet transform with respect to complex\nquasimomenta. Based on this, we show that evanescent waves may undergo a phase\ntransition, where local oscillations drastically depend on the location of\ncorresponding frequency inside the band gap.",
        "The Cahn-Hilliard equation has been widely employed within various\nmathematical models in physics, chemistry and engineering. Explicit stabilized\ntime stepping methods can be attractive for time integration of the\nCahn-Hilliard equation, especially on parallel and hybrid supercomputers. In\nthis paper, we propose an exponential time integration method for the\nCahn-Hilliard equation and describe its efficient Krylov subspace based\nimplementation. We compare the method to a Chebyshev polynomial local iteration\nmodified (LIM) time stepping scheme. Both methods are explicit (i.e., do not\ninvolve linear system solution) and tested with both constant and adaptively\nchosen time steps.",
        "We build two types of isotropic stable processes reflected in a strongly\nconvex bounded domain $\\mathcal{D}$. In both cases, when the process tries to\njump across the boundary, it is stopped at the unique point where\n$\\partial\\mathcal{D}$ intersects the line segment defined by the attempted\njump. It then leaves the boundary either continuously (for the first type) or\nby a power-law distributed jump (for the second type). The construction of\nthese processes is done via an It\\^o synthesis: we concatenate their excursions\nin the domain, which are obtained by translating, rotating and stopping the\nexcursions of some stable processes reflected in the half-space. The key\ningredient in this procedure is the construction of the boundary processes,\ni.e. the processes time-changed by their local time on the boundary, which\nsolve stochastic differential equations driven by some Poisson measures of\nexcursions. The well-posedness of these boundary processes relies on delicate\nestimates involving some geometric inequalities and the laws of the undershoot\nand overshoot of the excursion when it leaves the domain. After having\nconstructed the processes, we show that they are Markov and Feller, we study\ntheir infinitesimal generator and write down the reflected fractional heat\nequations satisfied by their time-marginals.",
        "Quantum computing (QC) has the potential to revolutionize fields like machine\nlearning, security, and healthcare. Quantum machine learning (QML) has emerged\nas a promising area, enhancing learning algorithms using quantum computers.\nHowever, QML models are lucrative targets due to their high training costs and\nextensive training times. The scarcity of quantum resources and long wait times\nfurther exacerbate the challenge. Additionally, QML providers may rely on third\nparty quantum clouds for hosting models, exposing them and their training data\nto potential threats. As QML as a Service (QMLaaS) becomes more prevalent,\nreliance on third party quantum clouds poses a significant security risk. This\nwork demonstrates that adversaries in quantum cloud environments can exploit\nwhite box access to QML models to infer the users encoding scheme by analyzing\ncircuit transpilation artifacts. The extracted data can be reused for training\nclone models or sold for profit. We validate the proposed attack through\nsimulations, achieving high accuracy in distinguishing between encoding\nschemes. We report that 95% of the time, the encoding can be predicted\ncorrectly. To mitigate this threat, we propose a transient obfuscation layer\nthat masks encoding fingerprints using randomized rotations and entanglement,\nreducing adversarial detection to near random chance 42% , with a depth\noverhead of 8.5% for a 5 layer QNN design.",
        "We performed molecular dynamics simulations to investigate the mechanical\nresponse of face-centered cubic (FCC) nickel under uniaxial compression and\nnanoindentation using traditional interatomic potentials, including the\nEmbedded Atom Method (EAM) and Modified Embedded Atom Method (MEAM). By\ncalculating the generalized stacking fault energy (GSFE), we analyzed the\ndissociated slip paths responsible for stacking fault formation and partial\nShockley dislocations during mechanical loading. Our findings highlight the\ncritical importance of selecting appropriate interatomic potentials to model\ncompression and nanoindentation tests accurately, aligning simulations with\nexperimental observations. We propose a practical methodology for identifying\nempirical interatomic potentials suitable for mechanical testing of\nsingle-element materials. This approach establishes a benchmark for FCC nickel\nsimulations and provides a basis for extending these methods to more complex\nNi-based alloys, facilitating comparisons with experimental results such as\nthose from electron microscopy.",
        "To address the challenge of complex beam control in traditional\nmultiple-input multiple-output (MIMO) systems, research has proposed\nestablishing adaptive beam alignment by utilizing retro-directive antenna (RDA)\narrays to create echo resonance between the base station (BS) and user\nequipment (UE), thereby reducing system computational load. However, in\nconventional resonant beam systems (RBS), the use of the same frequency for\nuplink and downlink inevitably leads to echo interference issues. Therefore,\nthis paper proposes an innovative design for an resonance beam-based integrated\nsensing and communication (RB-ISAC) system to achieve efficient passive sensing\nand bidirectional communication. In this system, the UE does not actively\ntransmit signals but instead relies on a passive phase conjugation and\nfrequency conversion structure to separate the uplink and downlink carrier\nfrequencies. Additionally, through effective compensation for signal\npropagation loss, resonance is achieved after multiple iterations, at which\npoint the beam's field distribution becomes a low-diffraction-loss,\nhigh-focusing pattern, automatically aligning the transmitter with the\nreceiver. This enables high-precision passive positioning while facilitating\nuplink and downlink communication. Simulation results demonstrate that the\nproposed system can achieve resonance after multiple iterations, and can\nsupport uplink and downlink communication within a range of 5 meters while\nachieving passive direction of arrival (DOA) estimation with an error of less\nthan 2 degrees.",
        "Context. Magnetic fields exhibit a wide variety of behaviours in low mass\nstars and further characterization is required to understand these\nobservations. Stellar photometry from space missions such as MOST, CoRoT,\nKepler, and, in future PLATO, provide thousands of highly precise light curves\n(LC) that can shed new light upon stellar activity, in particular through the\nsignature of transiting spots. Aims. We study the impact of star spots on light\ncurves in the Fourier domain, reducing the degeneracies encountered by direct\nspot modelling in the temporal domain, and use this new formulation to explore\nthe spot properties from the available data. Methods. We propose a model of LC\npower spectra at low frequency based on a description of spot transits that\nallows us to retrieve information about the amplitude of their photometric\nimpact $\\mathcal{H}$, and about the spot mean lifetime over the observation\n$\\tau_{\\rm life}$ when the power spectrum exibits rotation peaks. We first\nvalidate this method with simulated LCs and then apply it to the Kepler data to\nextract global trends over a set of more than 37 755 stars. Results. This\nanalysis leads to a classification of the sample into \"peakless\" or \"with\npeaks\" spectra, and enables the identification of different activity regimes\nbased on $\\mathcal{H}$ and $\\tau_{\\rm life}$ for different ranges of Rossby\nnumber. More specifically, we observe an intense regime of activity between Ro\n= 0.7 and Ro = 1, for stars with masses under 1$M_\\odot$. Conclusions. This new\nsystematic method can be used to provide new observational constraints on\nstellar activity (and possibly a link with stellar magnetism) when applied to\nlarge photometric datasets, such as those from the future PLATO mission.",
        "Na$_{3}$Co$_{2}$SbO$_6$ is a promising candidate to realize the Kitaev spin\nliquid phase since the large Kitaev spin exchange interaction is tunable via\nthe change in electronic structure, such as the trigonal crystal field\nsplitting ($\\Delta_{TCF}$). Here, we show that the uncorrelated electronic\nstructure of Na$_{3}$Co$_{2}$SbO$_6$ is rather insensitive to the strain effect\ndue to the low crystal symmetry accompanied by oxygen displacements and the\npresence of Sb $s$ orbitals. This suggests that the Kitaev spin-exchange\ninteraction obtained from perturbation theory also does not depend much on the\nstrain effect. Using density functional theory plus dynamical mean field\ntheory, we find that the correlated electronic structure of\nNa$_{3}$Co$_{2}$SbO$_6$ is an orbital selective Mott insulating state where the\ntrigonal $a_{1g}$ orbital is insulating due to correlation-assisted\nhybridization, while other $d$ orbitals behave as typical Mott insulators,\nresulting in tunability of $\\Delta_{TCF}$ under the strain effect effectively.\nOur results show that the local Co-site symmetry and dynamical correlation\neffects will play an important role in engineering the novel magnetic phase in\nthis and related materials.",
        "Laser surface structuring has proven to be an effective technique for\nachieving a copper surface with secondary electron yield (SEY) values close to\nor below unity. However, the attributes that minimize SEY, such as moderately\ndeep grooves and redeposited nanoparticles, may lead to undesirable\nconsequences, including increased radio frequency surface resistance. This\ninvestigation systematically examined data about different cleaning procedures\ndesigned to eliminate redeposited adsorbed particulates. Various analysis\ntechniques were used iteratively after each consecutive cleaning step,\nproviding insights into the evolving surface characteristics. The collected\nexperimental results identified distinct impacts of microgrooves, groove\norientation, and associated particulates on secondary electron yield and\nsurface resistance. Exposing the crests while retaining high particulate\ncoverage in the grooves leads to reduced SEY values and surface resistance,\nsuggesting that the tips of the grooves exert a more significant influence on\nsurface current density than the groove depth. At the same time, nanoparticles\nin the grooves have a more significant impact on SEY values than the exposed\ntips at the surface.",
        "We present a massively parallel solver that accelerates DC loadflow\ncomputations for power grid topology optimization tasks. Our approach leverages\nlow-rank updates of the Power Transfer Distribution Factors (PTDFs) to\nrepresent substation splits, line outages, and reconfigurations without ever\nrefactorizing the system. Furthermore, we implement the core routines on\nGraphics Processing Units (GPUs), thereby exploiting their high-throughput\narchitecture for linear algebra. A two-level decomposition separates changes in\nbranch topology from changes in nodal injections, enabling additional speed-ups\nby an in-the-loop brute force search over injection variations at minimal\nadditional cost. We demonstrate billion-loadflow-per-second performance on\npower grids of varying sizes in workload settings which are typical for\ngradient-free topology optimization such as Reinforcement Learning or Quality\nDiversity methods. While adopting the DC approximation sacrifices some accuracy\nand prohibits the computation of voltage magnitudes, we show that this\nsacrifice unlocks new scales of computational feasibility, offering a powerful\ntool for large-scale grid planning and operational topology optimization.",
        "The standard Coulomb interaction is one of four fundamental interactions in\nNature. It is interesting to know how will the standard Coulomb interaction be\nmodified when it meets spin. Since the standard Coulomb potential is a simple\nbut fundamental solution of Maxwell's equations, hence Maxwell's equations can\npredict the existence of the standard Coulomb potential. The Yang-Mills\nequations are the natural generalizations of Maxwell's equations from the\nAbelian potentials to the non-Abelian ones, thus based on the Yang-Mills\nequations, one can predict the reasonable form of spin-dependent Coulomb\npotential, which naturally reduces the standard Coulomb potential if without\nconsidering the spin. Our work sheds a new light to how to couple spin with\nfundamental interactions.",
        "Quantum heat engines (QHEs) have attracted long-standing scientific interest,\nespecially inspired by considerations of the interplay between heat and work\nwith the quantization of energy levels, quantum superposition, and\nentanglement. Operating QHEs calls for effective control of the thermal\nreservoirs and the eigenenergies of the quantum working medium of the engine.\nAlthough superconducting circuits enable accurate engineering of controlled\nquantum systems, beneficial in quantum computing, this framework has not yet\nbeen employed to experimentally realize a cyclic QHE. Here, we experimentally\ndemonstrate a quantum heat engine based on superconducting circuits, using a\nsingle-junction quantum-circuit refrigerator (QCR) as a two-way tunable heat\nreservoir coupled to a flux-tunable transmon qubit acting as the working medium\nof the engine. We implement a quantum Otto cycle by a tailored drive on the QCR\nto sequentially induce cooling and heating, interleaved with flux ramps that\ncontrol the qubit frequency. Utilizing single-shot qubit readout, we monitor\nthe evolution of the qubit state during several cycles of the heat engine and\nmeasure positive output powers and efficiencies that agree with our simulations\nof the quantum evolution. Our results verify theoretical models on the\nthermodynamics of quantum heat engines and advance the control of\ndissipation-engineered thermal environments. These proof-of-concept results\npave the way for explorations on possible advantages of QHEs with respect to\nclassical heat engines.",
        "The safety and security of robotic systems are paramount when navigating\naround a hostile target. This paper addresses the problem of circumnavigating\nan unknown target by a unicycle robot while ensuring it maintains a desired\nsafe distance and remains within the sensing region around the target\nthroughout its motion. The proposed control design methodology is based on the\nconstruction of a joint Lyapunov function that incorporates: (i) a quadratic\npotential function characterizing the desired target-circumnavigation\nobjective, and (ii) a barrier Lyapunov function-based potential term to enforce\nsafety and sensing constraints on the robot's motion. A notable feature of the\nproposed control design is its reliance exclusively on local range measurements\nbetween the robot and the target, realized using a dynamic output feedback\ncontroller that treats the range as the only observable output for feedback.\nUsing the Lyapunov stability theory, we show that the desired equilibrium of\nthe closed-loop system is asymptotically stable, and the prescribed safety and\nsecurity constraints are met under the proposed controllers. We also obtain\nrestrictive bounds on the post-design signals and provide both simulation and\nexperimental results to validate the theoretical contributions.",
        "The Diffie-Hellman key exchange plays a crucial role in conventional\ncryptography, as it allows two legitimate users to establish a common, usually\nephemeral, secret key. Its security relies on the discrete-logarithm problem,\nwhich is considered to be a mathematical one-way function, while the final key\nis formed by random independent actions of the two users. In the present work\nwe investigate the extension of Diffie-Hellman key exchange to the quantum\nsetting, where the two legitimate users exchange independent random quantum\nstates. The proposed protocol relies on the bijective mapping of integers onto\na set of symmetric coherent states, and we investigate the regime of parameters\nfor which the map behaves as a quantum one-way function. Its security is\nanalyzed in the framework of minimum-error-discrimination and\nphoton-number-splitting attacks, while its performance and the challenges in a\npossible realization are also discussed.",
        "The study of collisionless shocks and their role in cosmic ray acceleration\nhas gained importance through observations and simulations, driving interest in\nreproducing these conditions in laboratory experiments using high-power lasers.\nIn this work, we examine the role of three-dimensional (3D) effects in ion\nacceleration in quasi-perpendicular shocks under laboratory-relevant\nconditions. Using hybrid particle-in-cell simulations (kinetic ions and fluid\nelectrons), we explore how the Alfv\\'enic and sonic Mach numbers, along with\nplasma beta, influence ion energization, unlocked only in 3D, and establish\nscaling criteria for when conducting 3D simulations is necessary. Our results\nshow that efficient ion acceleration requires Alfv\\'enic Mach numbers $\\geq 25$\nand sonic Mach numbers $\\geq 13$, with plasma-$\\beta \\leq 5$. We theoretically\nfound that, while 2D simulations suffice for current laboratory-accessible\nshock conditions, 3D effects become crucial for shock velocities exceeding 1000\nkm\/s and experiments sustaining the shock for at least 10 ns. We surveyed\nprevious laboratory experiments on collisionless shocks and found that 3D\neffects are unimportant under those conditions, implying that 1D and 2D\nsimulations should be enough to model the accelerated ion spectra. However, we\ndo find that the same experiments are realistically close to accessing the\nregime relevant to 3D effects, an exciting prospect for future laboratory\nefforts. We propose modifications to past experimental configurations to\noptimize and control 3D effects on ion acceleration. These proposed experiments\ncould be used to benchmark plasma astrophysics kinetic codes and\/or employed as\ncontrollable sources of energetic particles."
      ]
    }
  },
  {
    "id":2411.19,
    "research_type":"applied",
    "start_id":"b7",
    "start_title":"Artificial intelligence applications in stroke",
    "start_abstract":"Management of stroke highly depends on information from imaging studies. Noncontrast computed tomography (CT) and magnetic resonance imaging (MRI) can both be used to distinguish between ischemic and hemorrhagic stroke, which is difficult based on clinical features. Hypodensity on CT and DWI hyperintensity on MRI identifies irreversibly damaged tissue, although the sensitivity of MRI is higher in the acute setting. Angiographic and perfusion imaging sequences can identify a large vessel occlusion and, along with perfusion imaging, can select patients for endovascular therapy. The FLAIR-DWI mismatch yields information about patients with unknown time of onset (including wake-up strokes). Stroke imaging also gives insight into prognosis, with current methods aiming to give a picture of the short-term consequences of successful reperfusion or continued large vessel occlusion. One important caveat about stroke imaging is that it must be done quickly, as faster treatment leads to better outcomes.1 However, most steps in the stroke imaging triage pathway require the presence of human radiologists and neurologists, and this is often the time-limiting step. The expertise required for these tasks may not be available at all sites or at all times. Therefore, there is interest in automated methods for stroke imaging evaluation. Artificial intelligence (AI) is a broad term reflecting the use of computers to perform tasks that humans may find difficult, often in ways that are hard to pinpoint. For example, although humans find high-level computation difficult, calculator technology is not considered AI because we know how to break this down into discrete steps and feel we understand it. However, facial recognition is a task that humans perform well, but an algorithm to identify faces is usually considered AI since we cannot articulate precisely how this is done. Machine learning (ML) is a subset of AI in which algorithms learn from the data itself without explicit programming. ML methods reflect a broad range of statistical techniques ranging from linear regression to more complex methods such as support vector machines and decision trees. ML methods can be further broken into supervised and unsupervised learning, which differ from one another in that the former requires access to gold standard labels although the latter attempts to find the answers implicitly in the data itself. While ML methods have grown more popular over recent years, the advent of a specific supervised ML method based on architectures resembling human neural networks over the past decade has led to a quantum leap in performance.2 This method, called deep learning (DL) because of many multiple internal layers, can be considered a transformative technology. Compared with previous methods that required humans to identify image features, a deep neural network trained on a dataset with known outputs can learn the best features for organizing the data. In this review, we will discuss ML methods applied to stroke imaging with an emphasis on DL applications. We refer to Figure for a graphical overview of the applications discussed in this review.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Addressing disparities in the global epidemiology of stroke"
      ],
      "abstract":[
        "Stroke is the second leading cause of death and the third leading cause of disability worldwide. Though the burden of stroke worldwide seems to have declined in the past three decades, much of this effect reflects decreases in high-income countries (HICs). By contrast, the burden of stroke has grown rapidly in low-income and middle-income countries (LMICs), where epidemiological, socioeconomic and demographic shifts have increased the incidence of stroke and other non-communicable diseases. Furthermore, even in HICs, disparities in stroke epidemiology exist along racial, ethnic, socioeconomic and geographical lines. In this Review, we highlight the under-acknowledged disparities in the burden of stroke. We emphasize the shifting global landscape of stroke risk factors, critical gaps in stroke service delivery, and the need for a more granular analysis of the burden of stroke within and between LMICs and HICs to guide context-appropriate capacity-building. Finally, we review strategies for addressing key inequalities in stroke epidemiology, including improvements in epidemiological surveillance and context-specific research efforts in under-resourced regions, development of the global workforce of stroke care providers, expansion of access to preventive and treatment services through mobile and telehealth platforms, and scaling up of evidence-based strategies and policies that target local, national, regional and global stroke disparities."
      ],
      "categories":[
        "q-bio.NC"
      ]
    },
    "list":{
      "title":[
        "Neuronal Correlates of Semantic Event Classes during Presentation of\n  Complex Naturalistic Stimuli: Anatomical Patterns, Context-Sensitivity, and\n  Potential Impact on shared Human-Robot Ontologies",
        "Stiff-sloppy analysis of brain networks to reveal individual differences\n  in task performance",
        "The Amplitude Modulation Structure of Japanese Infant- and\n  Child-Directed Speech: Longitudinal Data Reveal Universal Acoustic Physical\n  Structures Underpinning Moraic Timing",
        "Dynamic Markov Blanket Detection for Macroscopic Physics Discovery",
        "Neuro-oscillatory models of cortical speech processing",
        "$^{18}$F-FDG brain PET hypometabolism in post-SARS-CoV-2 infection:\n  substrate for persistent\/delayed disorders?",
        "Electrophysiological Investigation of Insect Pain Threshold",
        "Reasoning within and between collective action problems",
        "From Thought to Action: How a Hierarchy of Neural Dynamics Supports\n  Language Production",
        "A Turing Test for Artificial Nets devoted to model Human Vision",
        "Automated generation of epilepsy surgery resection masks; The RAMPS\n  pipeline",
        "Spaces and sequences in the hippocampus: a homological perspective",
        "Towards an integrative approach to the study of brain-environment\n  interactions in human and non-human primate",
        "Multidimensional moment problem and Stieltjes transform",
        "Algebraic solution of the Jacobi inverse problem and explicit addition\n  laws",
        "Yang-Mills-Utiyama Theory and Graviweak Correspondence",
        "Measuring Top Yukawa Coupling through $2\\rightarrow 3$ VBS at Muon\n  Collider",
        "Phonons in Electron Crystals with Berry Curvature",
        "Counting principal ideals of small norm in the simplest cubic fields",
        "Hybrid Quantum Neural Networks with Amplitude Encoding: Advancing\n  Recovery Rate Predictions",
        "A Virgo Environmental Survey Tracing Ionised Gas Emission (VESTIGE)\n  XVII. Statistical properties of individual HII regions in unperturbed systems",
        "Birth of magnetized low-mass protostars and circumstellar disks",
        "The soccer model, stochastic ordering and martingale transport",
        "MNE: overparametrized neural evolution with applications to diffusion\n  processes and sampling",
        "Recurrent Neural Networks for Dynamic VWAP Execution: Adaptive Trading\n  Strategies with Temporal Kolmogorov-Arnold Networks",
        "On generalized Tur{\\'a}n problems with bounded matching number and\n  circumference",
        "The sliding tile puzzle, roots to polynomials, and $\\textbf{P}$ vs.\n  $\\textbf{NP}$ complexity",
        "Multi-constraint Graph Partitioning Problems Via Recursive Bipartition\n  Algorithm Based on Subspace Minimization Conjugate Gradient Method"
      ],
      "abstract":[
        "The present study forms part of a research project that aims to develop\ncognition-enabled robotic agents with environmental interaction capabilities\nclose to human proficiency. This approach is based on human-derived neuronal\ndata in combination with a shared ontology to enable robots to learn from human\nexperiences. To gain further insight into the relation between human neuronal\nactivity patterns and ontological classes, we introduced General Linear Model\n(GLM) analyses on fMRI data of participants who were presented with complex\nnaturalistic video stimuli comparable to the robot tasks. We modeled four event\nclasses (pick, place, fetch and deliver) attached to different environmental\nand object-related context and employed a Representational Similarity Analysis\n(RSA) on associated brain activity patterns as a starting point for an\nautomatic hierarchical clustering. Based on the default values for the\nHemodynamic Response Function (HRF), the activity patterns were reliably\ngrouped according to their parent classes of object interaction and navigation.\nAlthough fetch and deliver events were also distinguished by neuronal patterns,\npick and place events demonstrated higher ambiguity with respect to neuronal\nactivation patterns. Introducing a shorter HRF time-to-peak leads to a more\nreliable grouping of all four semantic classes, despite contextual factors.\nThese data might give novel insights into the neuronal representation of\ncomplex stimuli and may enable further research in ontology validation in\ncognition-enabled robotics.",
        "Understanding how brain networks recruit resources during cognitive tasks is\nkey to explaining individual differences in task performance. Brain network\nparameters-including activity levels of regions and their connectivity-reflect\nthe integration and segregation of functional subnetworks underlying task\nprocessing. However, the complexity and high dimensionality of these parameters\npose a significant barrier to identifying functionally relevant individual\ndifferences. Here, we introduce stiff-sloppy analysis as a framework for\nuncovering the stiff parameter combinations that critically influence\ntask-state brain dynamics, exemplified by working memory. Using the pairwise\nmaximum entropy model (PMEM) calibrated to fMRI data and Fisher Information\nMatrix (FIM) analysis, we reveal that the stiff dimensions of the model\nparameters capture the most relevant integration and segregation processes of\nthe default mode network and the working memory network. Individual differences\nalong these stiff neural dimensions consistently correlate with working memory\nperformance. Notably, stiff parameters robustly predicted working memory\nperformance, even when the less sensitive (\"sloppy\") parameters were excluded.\nThis study establishes stiff-sloppy analysis as a powerful approach to identify\ncognition-related brain networks, bridging neural dynamics and behavior and\noffering new avenues for personalized neuroscience including therapeutic\ninnovation.",
        "Infant-directed speech (IDS) is highly rhythmic, and in European languages\nIDS is dominated by patterns of amplitude modulation (AM) at ~2Hz (reflecting\nprosody) and ~5Hz (reflecting individual syllables). The rhythm structure of\nspoken Japanese is thought to differ from European stress-timed and\nsyllable-timed languages, depending on moraic units. Morae comprise any onset\nphoneme and vowel phonemes within a syllable, PA-N-DA. Arguably, initial speech\nencoding via cortical tracking by infants is likely to utilize\nlanguage-universal physical acoustic structures in speech rather than\nlanguage-specific structures like morae, since the infant brain must be\nprepared to acquire any human language. Here a language-blind computational\nmodel of linguistic rhythm based on features of the amplitude envelope is used\nto compute these physical acoustic stimulus characteristics for Japanese. Using\n~18,000 samples of natural IDS and child-directed speech (CDS) recorded\nlongitudinally from 6 parents while speaking to their children over the ages\n0-5 years, we find that the temporal modulation patterns that characterise the\namplitude envelope of Japanese are highly similar to those found for\nstress-timed and syllable-timed European languages. However, the AM band\ncorresponding to the syllabic level in CDS\/IDS in European languages (~2-12Hz,\ntheta-rate cortical tracking) was elongated in Japanese (2.5-17Hz). Further,\nthe phase synchronization ratios between the two slowest AM bands were as\nlikely to be 1:3 as 1:2, which differs from European languages where 1:2 ratios\nare dominant. Accordingly, the language-universal amplitude-driven physical\nacoustic structures important for cortical speech tracking flexibly accommodate\nlanguage-specific differences in core rhythmic units.",
        "The free energy principle (FEP), along with the associated constructs of\nMarkov blankets and ontological potentials, have recently been presented as the\ncore components of a generalized modeling method capable of mathematically\ndescribing arbitrary objects that persist in random dynamical systems; that is,\na mathematical theory of ``every'' ``thing''. Here, we leverage the FEP to\ndevelop a mathematical physics approach to the identification of objects,\nobject types, and the macroscopic, object-type-specific rules that govern their\nbehavior. We take a generative modeling approach and use variational Bayesian\nexpectation maximization to develop a dynamic Markov blanket detection\nalgorithm that is capable of identifying and classifying macroscopic objects,\ngiven partial observation of microscopic dynamics. This unsupervised algorithm\nuses Bayesian attention to explicitly label observable microscopic elements\naccording to their current role in a given system, as either the internal or\nboundary elements of a given macroscopic object; and it identifies macroscopic\nphysical laws that govern how the object interacts with its environment.\nBecause these labels are dynamic or evolve over time, the algorithm is capable\nof identifying complex objects that travel through fixed media or exchange\nmatter with their environment. This approach leads directly to a flexible class\nof structured, unsupervised algorithms that sensibly partition complex\nmany-particle or many-component systems into collections of interacting\nmacroscopic subsystems, namely, ``objects'' or ``things''. We derive a few\nexamples of this kind of macroscopic physics discovery algorithm and\ndemonstrate its utility with simple numerical experiments, in which the\nalgorithm correctly labels the components of Newton's cradle, a burning fuse,\nthe Lorenz attractor, and a simulated cell.",
        "In this review, we examine computational models that explore the role of\nneural oscillations in speech perception, spanning from early auditory\nprocessing to higher cognitive stages. We focus on models that use rhythmic\nbrain activities, such as gamma, theta, and delta oscillations, to encode\nphonemes, segment speech into syllables and words, and integrate linguistic\nelements to infer meaning. We analyze the mechanisms underlying these models,\ntheir biological plausibility, and their potential applications in processing\nand understanding speech in real time, a computational feature that is achieved\nby the human brain but not yet implemented in speech recognition models.\nReal-time processing enables dynamic adaptation to incoming speech, allowing\nsystems to handle the rapid and continuous flow of auditory information\nrequired for effective communication, interactive applications, and accurate\nspeech recognition in a variety of real-world settings. While significant\nprogress has been made in modeling the neural basis of speech perception,\nchallenges remain, particularly in accounting for the complexity of semantic\nprocessing and the integration of contextual influences. Moreover, the high\ncomputational demands of biologically realistic models pose practical\ndifficulties for their implementation and analysis. Despite these limitations,\nthese models provide valuable insights into the neural mechanisms of speech\nperception. We conclude by identifying current limitations, proposing future\nresearch directions, and suggesting how these models can be further developed\nto achieve a more comprehensive understanding of speech processing in the human\nbrain.",
        "Purpose: Several brain complications of SARS-CoV-2 infection have been\nreported. It has been moreover speculated that this neurotropism could\npotentially cause a delayed outbreak of neuropsychiatric and neurodegenerative\ndiseases of neuroinflammatory origin. A propagation mechanism has been proposed\nacross the cribriform plate of the ethmoid bone, from the nose to the olfactory\nepithelium, and possibly afterward to other limbic structures, and deeper parts\nof the brain including the brainstem. Methods: Review of clinical examination,\nand whole-brain voxel-based analysis of $^{18}$F-FDG PET metabolism in\ncomparison with healthy subjects (p voxel<0.001, p-cluster<0.05, uncorrected),\nof two patients with confirmed diagnosis of SARS-CoV-2 explored at the\npost-viral stage of the disease. Results: Hypometabolism of the\nolfactory\/rectus gyrus was found on the two patients, especially one with\n4-week prolonged anosmia. Additional hypometabolisms were found within\namygdala, hippocampus, parahippocampus, cingulate cortex, pre-\/post-central\ngyrus, thalamus\/hypothalamus, cerebellum, pons, and medulla in the other\npatient who complained of delayed onset of a painful syndrome. Conclusion:\nThese preliminary findings reinforce the hypotheses of SARS-CoV-2 neurotropism\nthrough the olfactory bulb and the possible extension of this impairment to\nother brain structures. $^{18}$F-FDG PET hypometabolism could constitute a\ncerebral quantitative biomarker of this involvement. Post-viral cohort studies\nare required to specify the exact relationship between such hypometabolisms and\nthe possible persistent disorders, especially involving cognitive or emotion\ndisturbances, residual respiratory symptoms, or painful complaints.",
        "The question of whether insects experience pain has long been debated in\nneuroscience and animal behavior research. Increasing evidence suggests that\ninsects possess the ability to detect and respond to noxious stimuli,\nexhibiting behaviors indicative of pain perception. This study investigates the\nrelationship between pain stimuli and physiological responses in crickets\n(Gryllidae), focusing on heart rate (ECG) and brain wave (EEG) patterns. We\napplied a range of mechanical, chemical, thermal, and electrical stimuli to\ncrickets, recording ECG and EEG data while employing a deep learning-based\nmodel to classify pain levels. Our findings revealed significant heart rate\nchanges and EEG fluctuations in response to various stimuli, with the highest\nintensity stimuli inducing marked physiological stress. The AI-based analysis,\nutilizing AlexNet for EEG signal classification, achieved 90% accuracy in\ndistinguishing between resting, low-pain, and high-pain states. While no social\nsharing of pain was observed through ECG measurements, these results contribute\nto the growing body of evidence supporting insect nociception and offer new\ninsights into their physiological responses to external stressors. This\nresearch advances the understanding of insect pain mechanisms and demonstrates\nthe potential for AI-driven analysis in entomological studies.",
        "Understanding cooperation in social systems is challenging because the\never-changing rules that govern societies interact with individual actions,\nresulting in intricate collective outcomes. In virtual-world experiments, we\nallowed people to make changes in the systems that they are making decisions\nwithin and investigated how they weigh the influence of different rules in\ndecision-making. When choosing between worlds differing in more than one rule,\na naive heuristics model predicted participants decisions as well, and in some\ncases better, than game earnings (utility) or by the subjective quality of\nsingle rules. In contrast, when a subset of engaged participants made\ninstantaneous (within-world) decisions, their behavior aligned very closely\nwith objective utility and not with the heuristics model. Findings suggest\nthat, whereas choices between rules may deviate from rational benchmarks, the\nfrequency of real time cooperation decisions to provide feedback can be a\nreliable indicator of the objective utility of these rules.",
        "Humans effortlessly communicate their thoughts through intricate sequences of\nmotor actions. Yet, the neural processes that coordinate language production\nremain largely unknown, in part because speech artifacts limit the use of\nneuroimaging. To elucidate the unfolding of language production in the brain,\nwe investigate with magnetoencephalography (MEG) and electroencephalography\n(EEG) the neurophysiological activity of 35 skilled typists, while they typed\nsentences on a keyboard. This approach confirms the hierarchical predictions of\nlinguistic theories: the neural activity preceding the production of each word\nis marked by the sequential rise and fall of context-, word-, syllable-, and\nletter-level representations. Remarkably, each of these neural representations\nis maintained over long time periods within each level of the language\nhierarchy. This phenomenon results in a superposition of successive\nrepresentations that is supported by a hierarchy of dynamic neural codes.\nOverall, these findings provide a precise computational breakdown of the neural\ndynamics that coordinate the production of language in the human brain.",
        "In this 2022 work we argued that, despite claims about successful modeling of\nthe visual brain using artificial nets, the problem is far from being solved\n(even for low-level vision). Examples of open issues include: where should we\nread from ANNs in order to reproduce human behavior?, this ad-hoc read-out is\nconsidered part of the brain model or not?, should we use artificial\npsychophysics or artificial physiology?, in the case of ANNs, artificial\nexperiments should literally match the experiments done with humans?. There is\na clear need of rigorous procedures for experimental tests for ANNs devoted to\nmodel the visual brain, and more generally, to understand ANNs devoted to\ngeneric vision tasks. Following our experience in using low-level facts from\nQuantitative Visual Neuroscience in computer vision, in this work we presented\nthe idea of developing a low-level dataset compiling the basic spatio-temporal\nand chromatic facts that are known to happen in the retina-V1 pathway, and they\nare not currently available in existing databases such as BrainScore. In our\nresults we checked the behavior of three recently proposed models with similar\narchitecture: (1) A parametric model tuned via Maximum Differentiation [Malo &\nSimoncelli SPIE 15, Martinez et al. PLOS 18, Martinez et al. Front. Neurosci.\n19], (2) A non-parametric model called PerceptNet tuned to maximize the\ncorrelation with human opinion on subjective distortions [Hepburn et al. IEEE\nICIP 19], and (3) A model with the same encoder as PerceptNet, but tuned for\nimage segmentation (published as Hernandez-Camara et al. Patt.Recogn.Lett. 23).\nResults on 10 compelling psycho\/physio visual facts show that the first model\nis the one with closer behavior to the humans in terms of receptive fields, but\nmore interestingly, on the nonlinear behavior when facing complex\nspatio-chromatic patterns of a range of luminances and contrasts.",
        "MRI-based delineation of brain tissue removed by epilepsy surgery can be\nchallenging due to post-operative brain shift. In consequence, most studies use\nmanual approaches which are prohibitively time-consuming for large sample\nsizes, require expertise, and can be prone to errors.\n  We propose RAMPS (Resections And Masks in Preoperative Space), an automated\npipeline to generate a 3D resection mask of pre-operative tissue. Our pipeline\nleverages existing software including FreeSurfer, SynthStrip, Sythnseg and ANTS\nto generate a mask in the same space as the patient's pre-operative T1 weighted\nMRI. We compare our automated masks against manually drawn masks and two other\nexisting pipelines (Epic-CHOP and ResectVol).\n  Comparing to manual masks (N=87), RAMPS achieved a median(IQR) dice\nsimilarity of 0.86(0.078) in temporal lobe resections, and 0.72(0.32) in\nextratemporal resections. In comparison to other pipelines, RAMPS had higher\ndice similarities (N=62) (RAMPS:0.86, Epic-CHOP: 0.72, ResectVol: 0.72).\n  We release a user-friendly, easy to use pipeline, RAMPS, open source for\naccurate delineation of resected tissue.",
        "Topological techniques have become a popular tool for studying information\nflows in neural networks. In particular, simplicial homology theory is used to\nanalyze how cognitive representations of space emerge from large conglomerates\nof independent neuronal contributions. Meanwhile, a growing number of studies\nsuggest that many cognitive functions are sustained by serial patterns of\nactivity. Here, we investigate stashes of such patterns using path homology\ntheory -- an impartial, universal approach that does not require a priori\nassumptions about the sequences' nature, functionality, underlying mechanisms,\nor other contexts. We focus on the hippocampus -- a key enabler of learning and\nmemory in mammalian brains -- and quantify the ordinal arrangement of its\nactivity similarly to how its topology has previously been studied in terms of\nsimplicial homologies. The results reveal that the vast majority of sequences\nproduced during spatial navigation are structurally equivalent to one another.\nOnly a few classes of distinct sequences form an ordinal schema of serial\nactivity that remains stable as the pool of sequences consolidates.\nImportantly, the structure of both maps is upheld by combinations of short\nsequences, suggesting that brief activity motifs dominate physiological\ncomputations. This ordinal organization emerges and stabilizes on timescales\ncharacteristic of spatial learning, displaying similar dynamics. Yet, the\nordinal maps generally do not reflect topological affinities -- spatial and\nsequential analyses address qualitatively different aspects of spike flows,\nrepresenting two complementary formats of information processing.",
        "By retracing my scientific journey that began 20 years ago, I highlight in\nthis thesis the need to consider the organization of the brain, which is\ncertainly globally hierarchical, but also highly distributed and mixed in the\nneuronal response of its different areas (by focusing on the sensorimotor\ncortex-basal ganglia network). I also emphasize the importance of adopting\nbehavioral paradigms that reflect as much as possible the characteristics of\nthe scenarios encountered in the real life of animals. And finally, I mention\nthe importance of \"disintegrating\" the way data analysis is traditionally\ncarried out, and of taking into account the dynamic nature of behavior, for\nexample by favoring the study of behavioral variables and so-called \"latent\"\nneuronal activities. I conclude this thesis by presenting the vision of my\nideal laboratory in a perspective of 5 to 10 years from today. This laboratory\nwould have two experimental devices, a first, classic, for carrying out tests\nin a constrained and therefore unnatural environment, the data of which would\nbe compared to those from a second device called \"naturalistic\" in which the\nanimals could potentially express their entire behavioral repertoire. The tasks\ntested would be characterized by strong ecological principles, such as in those\nsimulating the properties of foraging, and the data would make it possible to\ntest hypotheses based on the progressive addition of ecological components in\nthese tasks, all this in order to maintain control over the interpretability of\nthese data which are complex by nature.",
        "The truncated multidimensional moment problem is studied in terms of the\nStieltjes transform as the interpolation problem. A step-by-step algorithm is\nconstructed for the multidimensional moment problem and the set of solutions is\nfound in terms of continued fractions.",
        "We formulate a solution to the Algebraic version of the Inverse Jacobi\nproblem. Using this solution we produce explicit addition laws on any algebraic\ncurve generalizing the law suggested by Leykin [2] in the case of (n, s)\ncurves. This gives a positive answer to a question asked by T. Shaska whether\naddition laws appearing in [2] can be produced in a coordinate free manner.",
        "This report provides a geometrical Yang-Mills theory, including gravity. The\ntheory treats the space-time symmetry of the local Lorentz group in the same\nmanner as the internal gauge symmetry. We extend this general relativistic\nYang-Mills theory in the Minkowski space-time to a broader space, including\nEuclidean and Minkowskian spaces. In the extended space, we can transport\ntopological achievements from the Euclidean Yang-Mills theory into the\nLorentzian Yang-Mills theory. This extension also provides a relation between\nspace-time and internal symmetry. The perspective provided by the extended\ntheory suggests the novel relation between gravity and the weak force, leading\nus to the graviweak correspondence.",
        "We study the measurement of top Yukawa coupling through $2\\rightarrow 3$ VBS\nat future muon colliders, focusing on the lepton and semi-lepton channels of\n$\\nu\\nu tth\/z$. First, analyzing the partonic amplitudes of $W_LW_L\\rightarrow\nt\\bar t h\/Z_L$ and simulating the full processes of $\\nu\\nu tth\/z$ without\ndecaying, we find they are highly sensitive to the anomalous top Yukawa $\\delta\ny_t$. This sensitivity is enhanced by selecting helicities of the final $t\\bar\nt$ and $Z$ to be $t_L\\bar t_L+t_R\\bar t_R$ and $Z_L$, which serves as the\ndefault and core setting of our analysis. We then obtain the limits on $\\delta\ny_t$ with this setting, giving $[-1.0\\%, 1.1\\%]$ for $\\nu\\nu tth$ only and\n$[-0.36\\%, 0.92\\%]$ for $\\nu\\nu tth$ and $\\nu\\nu ttz$ combined at $30$ TeV and\n$1\\sigma$. Second, we proceed to analyze the processes after decaying and with\nbackground processes. To enhance the sensitivity to $\\delta y_t$, our settings\ninclude selecting the helicities of the final particles, as well as applying\nsuitable cuts. However, we don't do bin-by-bin analysis. We obtain the limits\non $\\delta y_t$ for those channels at $10\/30$ TeV and $1\\sigma\/2 \\sigma$. The\nbest limit is from the semi-lepton channel of $\\nu\\nu tth$. With spin tagging\nefficiency at $\\epsilon_s=0.9$, it gives $[-1.6\\% , 1.8\\%]$ at $1\\sigma$ and $\n[-2.4\\%, 2.7\\% ]$ at $2\\sigma$ at $30$ TeV; $[-7.0\\%, 6.7\\%]$ at $1\\sigma$ and\n$[-9.8\\%, 9.8\\%]$ at $2\\sigma$ at $10$ TeV.",
        "Recent advances in 2D materials featuring nonzero Berry curvature have\ninspired extensions of the Wigner crystallization paradigm. This paper derives\na low-energy effective theory for such quantum crystals, including the\nanomalous Hall crystal (AHC) with nonzero Chern number. First we show that the\nlow frequency dispersion of phonons in AHC, despite the presence of Berry\ncurvature, resembles that of the zero field (rather than finite magnetic field)\nWigner crystal due to the commutation of translation generators. We explain how\nkey parameters of the phonon theory such as elastic constants and effective\nmass can be extracted from microscopic models, and apply them to two families\nof models: the recently introduced $\\lambda$-jellium model and a model of\nrhombohedral multilayer graphene (RMG). In the $\\lambda$-jellium model, we\nexplore the energy landscape as crystal geometry shifts, revealing that AHC can\nbecome \"soft\" under certain conditions. This causes transitions in lattice\ngeometry, although the quantized Hall response remains unchanged. Surprisingly,\nthe Berry curvature seems to enhance the effective mass, leading to a reduction\nin phonon speed. For the AHC in RMG, we obtain estimates of phonon speed and\nshear stiffness. We also identify a previously overlooked \"kineo-elastic\" term\nin the phonon effective action that is present in the symmetry setting of RMG,\nand leads to dramatic differences in phonon speeds in opposite directions. We\nnumerically confirm these predictions of the effective actions by\ntime-dependent Hartree-Fock calculations. Our work points to the wealth of new\nphenomena that can arise when electron crystallization occurs in the presence\nof band geometry and topology.",
        "We estimate the number of principal ideals $ I $ of norm $ \\mathrm{N}(I) \\leq\nx $ in the family of the simplest cubic fields. The advantage of our result is\nthat it provides the correct order of magnitude for arbitrary $ x \\geq 1 $,\neven when $ x $ is significantly smaller than the discriminant. In particular,\nit shows that there exist surprisingly many principal ideals of small norm.",
        "Recovery rate prediction plays a pivotal role in bond investment strategies,\nenhancing risk assessment, optimizing portfolio allocation, improving pricing\naccuracy, and supporting effective credit risk management. However, forecasting\nfaces challenges like high-dimensional features, small sample sizes, and\noverfitting. We propose a hybrid Quantum Machine Learning model incorporating\nParameterized Quantum Circuits (PQC) within a neural network framework. PQCs\ninherently preserve unitarity, avoiding computationally costly orthogonality\nconstraints, while amplitude encoding enables exponential data compression,\nreducing qubit requirements logarithmically. Applied to a global dataset of\n1,725 observations (1996-2023), our method achieved superior accuracy (RMSE\n0.228) compared to classical neural networks (0.246) and quantum models with\nangle encoding (0.242), with efficient computation times. This work highlights\nthe potential of hybrid quantum-classical architectures in advancing recovery\nrate forecasting.",
        "The Virgo Environmental Survey Tracing Ionised Gas Emission (VESTIGE) is a\nblind narrow-band Halpha+[NII] imaging survey of the Virgo cluster carried out\nwith MegaCam at the CFHT telescope. The survey provides deep narrow-band images\nfor 385 galaxies hosting star forming HII regions. We identify individual HII\nregions and measure their main physical properties such as Halpha luminosity,\nequivalent diameter, and electron density with the purpose of deriving standard\nrelations as reference for future local and high-z studies of HII regions in\nstar forming systems in different environments. For this purpose we use a\ncomplete sample of ~ 13.000 HII regions of luminosity L(Halpha)>= 10^37 erg\ns^-1 to derive the main statistical properties of HII regions in unperturbed\nsystems, identified as those galaxies with a normal HI gas content (64\nobjects). These are the composite Halpha luminosity function, equivalent\ndiameter and electron density distribution, and luminosity-size relation. We\nalso derive the main scaling relations between several parameters\nrepresentative of the HII regions properties (total number, luminosity of the\nfirst ranked regions, fraction of the diffuse component, best fit parameters of\nthe Schechter luminosity function measured for individual galaxies) and those\ncharacterising the properties of the host galaxies (stellar mass, star\nformation rate and specific star formation rate, stellar mass and star\nformation rate surface density, metallicity, molecular-to-atomic gas ratio,\ntotal gas-to-dust mass ratio). We briefly discuss the results of this analysis\nand their implications in the study of the star formation process in galaxy\ndiscs.",
        "Although protostars and disks are often studied separately owing to numerical\nand observational challenges, breakthroughs in recent years have highlighted\nthe need to study both objects in concert. The role of magnetic fields in this\nregard must be investigated. We aim to describe the birth of the protostar and\nthat of its disk, as well as their early joint evolution following the second\ncollapse. We wish to study the structure of the nascent star-disk system, while\nfocusing on the innermost sub-AU region. We carry out high resolution 3D RMHD\nsimulations, describing the collapse of dense cloud cores to stellar densities.\nThe calculations reach $\\approx 2.3$ yr after protostellar birth. Our\nsimulations are also compared to their hydro counterpart to better isolate the\nrole of magnetic fields. When accounting for ambipolar diffusion, the\nefficiency of magnetic braking is drastically reduced and the nascent protostar\nreaches breakup velocity, thus forming a rotationally supported disk. The\ndiffusion of the magnetic field also allows for the implantation of a $\\sim\n\\mathrm{kG}$ field in the protostar, which is thereafter maintained. The\nmagnetic field is mainly toroidal in the star-disk system, although a notable\nvertical component threads it. We also show that the nascent disk is prone to\nthe MRI, although our resolution is inadequate to capture the mechanism. We\nnote a sensitivity of the disk's properties with regards to the angular\nmomentum inherited prior to the second collapse, as well as the magnetic field\nstrength. These calculations carry multiple implications on several issues in\nstellar formation theory, and offer perspectives for future modeling of the\nsystem. Should the fossil field hypothesis to explain the origins of magnetic\nfields in young stellar objects hold, we show that a $\\sim \\mathrm{kG}$ field\nstrength may be implanted and maintained in the protostar at birth.",
        "Tournaments are competitions between a number of teams, the outcome of which\ndetermines the relative strength or rank of each team. In many cases, the\nstrength of a team in the tournament is given by a score. Perhaps, the most\nstriking mathematical result on the tournament is Moon's theorem, which\nprovides a necessary and sufficient condition for a feasible score sequence via\nmajorization. To give a probabilistic interpretation of Moon's result, Aldous\nand Kolesnik introduced the soccer model,the existence of which gives a short\nproof of Moon's theorem. However, the existence proof of Aldous and Kolesnik is\nnonconstructive, leading to the question of a ``canonical'' construction of the\nsoccer model. The purpose of this paper is to provide explicit constructions of\nthe soccer model with an additional stochastic ordering constraint, which can\nbe formulated by martingale transport. Two solutions are given: one is by\nsolving an entropy optimization problem via Sinkhorn's algorithm, and the other\nrelies on the idea of shadow couplings. It turns out that both constructions\nyield the property of strong stochastic transitivity. The nontransitive\nsituations of the soccer model are also considered.",
        "We propose a framework for solving evolution equations within parametric\nfunction classes, especially ones that are specified by neural networks. We\ncall this framework the minimal neural evolution (MNE) because it is motivated\nby the goal of seeking the smallest instantaneous change in the neural network\nparameters that is compatible with exact solution of the evolution equation at\na set of evolving collocation points. Formally, the MNE is quite similar to the\nrecently introduced Neural Galerkin framework, but a difference in perspective\nmotivates an alternative sketching procedure that effectively reduces the\nlinear systems solved within the integrator to a size that is interpretable as\nan effective rank of the evolving neural tangent kernel, while maintaining a\nsmooth evolution equation for the neural network parameters. We focus\nspecifically on the application of this framework to diffusion processes, where\nthe score function allows us to define intuitive dynamics for the collocation\npoints. These can in turn be propagated jointly with the neural network\nparameters using a high-order adaptive integrator. In particular, we\ndemonstrate how the Ornstein-Uhlenbeck diffusion process can be used for the\ntask of sampling from a probability distribution given a formula for the\ndensity but no training data. This framework extends naturally to allow for\nconditional sampling and marginalization, and we show how to systematically\nremove the sampling bias due to parametric approximation error. We validate the\nefficiency, systematic improvability, and scalability of our approach on\nillustrative examples in low and high spatial dimensions.",
        "The execution of Volume Weighted Average Price (VWAP) orders remains a\ncritical challenge in modern financial markets, particularly as trading volumes\nand market complexity continue to increase. In my previous work\narXiv:2502.13722, I introduced a novel deep learning approach that demonstrated\nsignificant improvements over traditional VWAP execution methods by directly\noptimizing the execution problem rather than relying on volume curve\npredictions. However, that model was static because it employed the fully\nlinear approach described in arXiv:2410.21448, which is not designed for\ndynamic adjustment. This paper extends that foundation by developing a dynamic\nneural VWAP framework that adapts to evolving market conditions in real time.\nWe introduce two key innovations: first, the integration of recurrent neural\nnetworks to capture complex temporal dependencies in market dynamics, and\nsecond, a sophisticated dynamic adjustment mechanism that continuously\noptimizes execution decisions based on market feedback. The empirical analysis,\nconducted across five major cryptocurrency markets, demonstrates that this\ndynamic approach achieves substantial improvements over both traditional\nmethods and our previous static implementation, with execution performance\ngains of 10 to 15% in liquid markets and consistent outperformance across\nvarying conditions. These results suggest that adaptive neural architectures\ncan effectively address the challenges of modern VWAP execution while\nmaintaining computational efficiency suitable for practical deployment.",
        "Let \\( \\mathcal{F} \\) be a family of graphs. The generalized Tur\\'an number\n\\( \\operatorname{ex}(n, K_r, \\mathcal{F}) \\) is the maximum number of $K_r$ in\nan \\( n \\)-vertex graph that does not contain any member of \\( \\mathcal{F} \\)\nas a subgraph. Recently, Alon and Frankl initiated the study of Tur\\'an\nproblems with bounded matching number. In this paper, we determine the\ngeneralized Tur\\'an number of \\( C_{\\geq k} \\) with bounded matching number.",
        "This work explores the relationship between solution space and time\ncomplexity in the context of the $\\textbf{P}$ vs. $\\textbf{NP}$ problem,\nparticularly through the lens of the sliding tile puzzle and root finding\nalgorithms. We focus on the trade-off between finding a solution and verifying\nit, highlighting how understanding the structure of the solution space can\ninform the complexity of these problems. By examining the relationship between\nthe number of possible configurations and the time complexity required to\ntraverse this space we demonstrate that the minimal time to verify a solution\nis often smaller than the time required to discover it. Our results suggest\nthat the efficiency of solving $\\textbf{NP}$-complete problems is not only\ndetermined by the ability to find solutions but also by how effectively we can\nnavigate and characterize the solution space. This study contributes to the\nongoing discourse on computational complexity, particularly in understanding\nthe interplay between solution space size, algorithm design, and the inherent\nchallenges of finding versus verifying solutions.",
        "The graph partitioning problem is a well-known NP-hard problem. In this\npaper, we formulate a 0-1 quadratic integer programming model for the graph\npartitioning problem with vertex weight constraints and fixed vertex\nconstraints, and propose a recursive bipartition algorithm based on the\nsubspace minimization conjugate gradient method. To alleviate the difficulty of\nsolving the model, the constrained problem is transformed into an unconstrained\noptimization problem using equilibrium terms, elimination methods, and\ntrigonometric properties, and solved via an accelerated subspace minimization\nconjugate gradient algorithm. Initial feasible partitions are generated using a\nhyperplane rounding algorithm, followed by heuristic refinement strategies,\nincluding one-neighborhood and two-interchange adjustments, to iteratively\nimprove the results. Numerical experiments on knapsack-constrained graph\npartitioning and industrial examples demonstrate the effectiveness and\nfeasibility of the proposed algorithm."
      ]
    }
  },
  {
    "id":2411.1057,
    "research_type":"applied",
    "start_id":"b13",
    "start_title":"Using deep autoencoders to identify abnormal brain structural patterns in neuropsychiatric disorders: A large\u2010scale multi\u2010sample study",
    "start_abstract":"Machine learning is becoming an increasingly popular approach for investigating spatially distributed and subtle neuroanatomical alterations in brain\u2010based disorders. However, some machine learning models have been criticized for requiring a large number of cases in each experimental group, and for resembling a \u201cblack box\u201d that provides little or no insight into the nature of the data. In this article, we propose an alternative conceptual and practical approach for investigating brain\u2010based disorders which aim to overcome these limitations. We used an artificial neural network known as \u201cdeep autoencoder\u201d to create a normative model using structural magnetic resonance imaging data from 1,113 healthy people. We then used this model to estimate total and regional neuroanatomical deviation in individual patients with schizophrenia and autism spectrum disorder using two independent data sets (n =\u2009263). We report that the model was able to generate different values of total neuroanatomical deviation for each disease under investigation relative to their control group (p <\u2009.005). Furthermore, the model revealed distinct patterns of neuroanatomical deviations for the two diseases, consistent with the existing neuroimaging literature. We conclude that the deep autoencoder provides a flexible and promising framework for assessing total and regional neuroanatomical deviations in neuropsychiatric populations.",
    "start_categories":[
      "cs.NE"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b14"
      ],
      "title":[
        "Using normative modelling to detect disease progression in mild cognitive impairment and Alzheimer\u2019s disease in a cross-sectional multi-cohort study"
      ],
      "abstract":[
        "Normative modelling is an emerging method for quantifying how individuals deviate from the healthy populational pattern. Several machine learning models have been implemented to develop normative models to investigate brain disorders, including regression, support vector machines and Gaussian process models. With the advance of deep learning technology, the use of deep neural networks has also been proposed. In this study, we assessed normative models based on deep autoencoders using structural neuroimaging data from patients with Alzheimer\u2019s disease (n\u2009=\u2009206) and mild cognitive impairment (n\u2009=\u2009354). We first trained the autoencoder on an independent dataset (UK Biobank dataset) with 11,034 healthy controls. Then, we estimated how each patient deviated from this norm and established which brain regions were associated to this deviation. Finally, we compared the performance of our normative model against traditional classifiers. As expected, we found that patients exhibited deviations according to the severity of their clinical condition. The model identified medial temporal regions, including the hippocampus, and the ventricular system as critical regions for the calculation of the deviation score. Overall, the normative model had comparable cross-cohort generalizability to traditional classifiers. To promote open science, we are making all scripts and the trained models available to the wider research community."
      ],
      "categories":[
        "q-bio.NC"
      ]
    },
    "list":{
      "title":[
        "Increased GM-WM in a prefrontal network and decreased GM in the insula\n  and the precuneus are associated with reappraisal usage: A data fusion\n  approach",
        "A Turing Test for Artificial Nets devoted to model Human Vision",
        "Neuronal Correlates of Semantic Event Classes during Presentation of\n  Complex Naturalistic Stimuli: Anatomical Patterns, Context-Sensitivity, and\n  Potential Impact on shared Human-Robot Ontologies",
        "Automatic target validation based on neuroscientific literature mining\n  for tractography",
        "Evolution of diverse (and advanced) cognitive abilities through adaptive\n  fine-tuning of learning and chunking mechanisms",
        "Language-specific Tonal Features Drive Speaker-Listener Neural\n  Synchronization",
        "Predictability of temporal network dynamics in normal ageing and brain\n  pathology",
        "Structuring the Environment Nudges Participants Toward Hierarchical Over\n  Shortest Path Planning",
        "A comprehensive and reliable protocol for manual segmentation of the\n  human claustrum using high-resolution MRI",
        "Latent computing by biological neural networks: A dynamical systems\n  framework",
        "Neuro-oscillatory models of cortical speech processing",
        "How constraints on editing affects cultural evolution",
        "The Role of Affective States in Computational Psychiatry",
        "What Drives Cluster Cool-Core Transformations? A Population Level\n  Analysis of TNG-Cluster",
        "A First Look at \"Continuous Spin\" Gravity -- Time Delay Signatures",
        "Unified model of the Hall effect from insulator to overdoped compounds\n  in cuprate superconductors",
        "Every group is the automorphism group of a graph with arbitrarily large\n  genus",
        "Equivalence principles in Weyl transverse gravity",
        "Doping dependence of the magnetic ground state in the frustrated magnets\n  Ba$_2$$M$Te$_{1-x}$W$_{x}$O$_6$ ($M$ = Mn, Co)",
        "Constrained differential operators, Sobolev inequalities, and Riesz\n  potentials",
        "Semi-Parametric Batched Global Multi-Armed Bandits with Covariates",
        "Topological Data Analysis of Abelian Magnetic Monopoles in Gauge\n  Theories",
        "Two-sided bounds on the point-wise spatial decay of ground states in the\n  renormalized Nelson model with confining potentials",
        "Elusive properties of countably infinite graphs",
        "Quantum stick-slip motion in nanoscaled friction",
        "Constructing Fundamentals for the Theory of Proportions and Symbolic\n  Allusions Applied Interdisciplinarily",
        "Observation of the $W$-annihilation process $D_s^+ \\to \\omega\\rho^+$ and\n  measurement of $D_s^+ \\to \\phi\\rho^+$ in $D^+_s\\to \\pi^+\\pi^+\\pi^-\\pi^0\\pi^0$\n  decays",
        "PAH Feature Ratios Around Stellar Clusters and Associations in 19 Nearby\n  Galaxies"
      ],
      "abstract":[
        "Emotion regulation plays a crucial role in mental health, and difficulties in\nregulating emotions can contribute to psychological disorders. While\nreappraisal and suppression are well-studied strategies, the combined\ncontributions of gray matter (GM) and white matter (WM) to these strategies\nremain unclear due to methodological limitations in previous studies. To\naddress this, we applied a data fusion approach using Parallel Independent\nComponent Analysis (Parallel ICA) to GM and WM MRI images from 165 individuals.\nParallel ICA identified two networks associated with reappraisal usage. Network\n1 included a large lateral and medial prefrontal cortical network, overlapping\nwith the default mode network (DMN) and adjacent WM regions. Higher reappraisal\nfrequency was associated with greater GM-WM density within this network, and\nthis network was negatively correlated with perceived stress. Network 2\nincluded the insula, precuneus, sub-gyral, and lingual gyri in its GM portion,\nshowing a negative association with reappraisal usage. The WM portion, adjacent\nto regions of the central executive network (CEN), was positively associated\nwith reappraisal usage. Regarding suppression, no significant network was\nassociated with this strategy. This study provides new insights into individual\ndifferences in reappraisal use, showing a positive association between\nreappraisal frequency and increased gray and white matter concentration in a\nlarge frontal network, including regions of the frontal DMN and the CEN.\nConversely, subcortical areas exhibited reduced gray and white concentration.",
        "In this 2022 work we argued that, despite claims about successful modeling of\nthe visual brain using artificial nets, the problem is far from being solved\n(even for low-level vision). Examples of open issues include: where should we\nread from ANNs in order to reproduce human behavior?, this ad-hoc read-out is\nconsidered part of the brain model or not?, should we use artificial\npsychophysics or artificial physiology?, in the case of ANNs, artificial\nexperiments should literally match the experiments done with humans?. There is\na clear need of rigorous procedures for experimental tests for ANNs devoted to\nmodel the visual brain, and more generally, to understand ANNs devoted to\ngeneric vision tasks. Following our experience in using low-level facts from\nQuantitative Visual Neuroscience in computer vision, in this work we presented\nthe idea of developing a low-level dataset compiling the basic spatio-temporal\nand chromatic facts that are known to happen in the retina-V1 pathway, and they\nare not currently available in existing databases such as BrainScore. In our\nresults we checked the behavior of three recently proposed models with similar\narchitecture: (1) A parametric model tuned via Maximum Differentiation [Malo &\nSimoncelli SPIE 15, Martinez et al. PLOS 18, Martinez et al. Front. Neurosci.\n19], (2) A non-parametric model called PerceptNet tuned to maximize the\ncorrelation with human opinion on subjective distortions [Hepburn et al. IEEE\nICIP 19], and (3) A model with the same encoder as PerceptNet, but tuned for\nimage segmentation (published as Hernandez-Camara et al. Patt.Recogn.Lett. 23).\nResults on 10 compelling psycho\/physio visual facts show that the first model\nis the one with closer behavior to the humans in terms of receptive fields, but\nmore interestingly, on the nonlinear behavior when facing complex\nspatio-chromatic patterns of a range of luminances and contrasts.",
        "The present study forms part of a research project that aims to develop\ncognition-enabled robotic agents with environmental interaction capabilities\nclose to human proficiency. This approach is based on human-derived neuronal\ndata in combination with a shared ontology to enable robots to learn from human\nexperiences. To gain further insight into the relation between human neuronal\nactivity patterns and ontological classes, we introduced General Linear Model\n(GLM) analyses on fMRI data of participants who were presented with complex\nnaturalistic video stimuli comparable to the robot tasks. We modeled four event\nclasses (pick, place, fetch and deliver) attached to different environmental\nand object-related context and employed a Representational Similarity Analysis\n(RSA) on associated brain activity patterns as a starting point for an\nautomatic hierarchical clustering. Based on the default values for the\nHemodynamic Response Function (HRF), the activity patterns were reliably\ngrouped according to their parent classes of object interaction and navigation.\nAlthough fetch and deliver events were also distinguished by neuronal patterns,\npick and place events demonstrated higher ambiguity with respect to neuronal\nactivation patterns. Introducing a shorter HRF time-to-peak leads to a more\nreliable grouping of all four semantic classes, despite contextual factors.\nThese data might give novel insights into the neuronal representation of\ncomplex stimuli and may enable further research in ontology validation in\ncognition-enabled robotics.",
        "Target identification for tractography studies requires solid anatomical\nknowledge validated by an extensive literature review across species for each\nseed structure to be studied. Manual literature review to identify targets for\na given seed region is tedious and potentially subjective. Therefore,\ncomplementary approaches would be useful. We propose to use text-mining models\nto automatically suggest potential targets from the neuroscientific literature,\nfull-text articles and abstracts, so that they can be used for anatomical\nconnection studies and more specifically for tractography. We applied\ntext-mining models to three structures: two well-studied structures, since\nvalidated deep brain stimulation targets, the internal globus pallidus and the\nsubthalamic nucleus and, the nucleus accumbens, an exploratory target for\ntreating psychiatric disorders. We performed a systematic review of the\nliterature to document the projections of the three selected structures and\ncompared it with the targets proposed by text-mining models, both in rat and\nprimate (including human). We ran probabilistic tractography on the nucleus\naccumbens and compared the output with the results of the text-mining models\nand literature review. Overall, text-mining the literature could find three\ntimes as many targets as two man-weeks of curation could. The overall\nefficiency of the text-mining against literature review in our study was 98%\nrecall (at 36% precision), meaning that over all the targets for the three\nselected seeds, only one target has been missed by text-mining. We demonstrate\nthat connectivity for a structure of interest can be extracted from a very\nlarge amount of publications and abstracts. We believe this tool will be useful\nin helping the neuroscience community to facilitate connectivity studies of\nparticular brain regions. The text mining tools used for the study are part of\nthe HBP Neuroinformatics Platform, publicly available at\nhttp:\/\/connectivity-brainer.rhcloud.com",
        "The evolution of cognition is frequently discussed as the evolution of\ncognitive abilities or the evolution of some neuronal structures in the brain.\nHowever, since such traits or abilities are often highly complex, understanding\ntheir evolution requires explaining how they could have gradually evolved\nthrough selection acting on heritable variations in simpler cognitive\nmechanisms. With this in mind, making use of a previously proposed theory, here\nwe show how the evolution of cognitive abilities can be captured by the\nfine-tuning of basic learning mechanisms and, in particular, chunking\nmechanisms. We use the term chunking broadly for all types of non-elemental\nlearning, claiming that the process by which elements are combined into chunks\nand associated with other chunks, or elements, is critical for what the brain\ncan do, and that it must be fine-tuned to ecological conditions. We discuss the\nrelevance of this approach to studies in animal cognition, using examples from\nanimal foraging and decision-making, problem solving, and cognitive\nflexibility. Finally, we explain how even the apparent human-animal gap in\nsequence learning ability can be explained in terms of different fine-tunings\nof a similar chunking process.",
        "Verbal communication transmits information across diverse linguistic levels,\nwith neural synchronization (NS) between speakers and listeners emerging as a\nputative mechanism underlying successful exchange. However, the specific speech\nfeatures driving this synchronization and how language-specific versus\nuniversal characteristics facilitate information transfer remain poorly\nunderstood. We developed a novel content-based interbrain encoding model to\ndisentangle the contributions of acoustic and linguistic features to\nspeaker-listener NS during Mandarin storytelling and listening, as measured via\nmagnetoencephalography (MEG). Results revealed robust NS throughout\nfrontotemporal-parietal networks with systematic time lags between speech\nproduction and perception. Crucially, suprasegmental lexical tone features\n(tone categories, pitch height, and pitch contour), essential for lexical\nmeaning in Mandarin, contributed more significantly to NS than either acoustic\nelements or universal segmental units (consonants and vowels). These tonal\nfeatures generated distinctive spatiotemporal NS patterns, creating\nlanguage-specific neural \"communication channels\" that facilitated efficient\nrepresentation sharing between interlocutors. Furthermore, the strength and\npatterns of NS driven by these language-specific features predicted\ncommunication success. These findings demonstrate the neural mechanisms\nunderlying shared representations during verbal exchange and highlight how\nlanguage-specific features can shape neural coupling to optimize information\ntransfer during human communication.",
        "Spontaneous brain activity generically displays transient spatiotemporal\ncoherent structures, which can selectively be affected in various neurological\nand psychiatric pathologies. Here we model the full brain's\nelectroencephalographic activity as a high-dimensional functional network\nperforming a trajectory in a latent graph phase space. This approach allows us\nto investigate the orbital stability of brain's activity and in particular its\nshort-term predictability. We do this by constructing a non-parametric\nstatistic quantifying the expansion of initially close functional network\ntrajectories. We apply the method to cohorts of healthy ageing individuals, and\npatients previously diagnosed with Parkinson's or Alzheimer's disease. Results\nnot only characterise brain dynamics from a new angle, but further show that\nfunctional network predictability varies in a marked scale-dependent way across\nhealthy controls and patient groups. The path towards both pathologies is\nmarkedly different. Furthermore, healthy ageing's predictability appears to\nstrongly differ from that of Parkinson's disease, but much less from that of\npatients with Alzheimer's disease.",
        "Effective planning is crucial for navigating complex environments and\nachieving goals efficiently. In this study, we investigated how environmental\nstructure influences the selection of planning strategies. Participants\nnavigated a space station to collect colored spheres, with environments either\nstructured (spheres grouped by color) or unstructured (spheres scattered\nrandomly). We tested three types of plans: hierarchical (grouping spheres by\ncolor), shortest path (minimizing travel distance), and neutral (none of the\nabove). By manipulating environmental structure, we were able to nudge\nparticipants toward a preference for hierarchical planning in structured\nenvironments, while shortest path plans were favored in unstructured\nenvironments. A mismatch between self-reported preferences and actual choices\nindicated that participants often adopted implicit strategies, unaware of their\ndecision-making processes. These findings highlight the powerful effect of\nenvironmental cues on planning and suggest that even subtle changes in\nstructure can guide the selection of planning strategies.",
        "The claustrum is a thin gray matter structure in each brain hemisphere,\ncharacterized by exceptionally high connectivity with nearly all brain regions.\nDespite extensive animal studies on its anatomy and function and growing\nevidence of claustral deficits in neuropsychiatric disorders, its specific\nroles in normal and abnormal human brain function remain largely unknown. This\nis primarily due to its thin and complex morphology, which limits accurate\nanatomical delineation and neural activity isolation in conventional in vivo\nneuroimaging. To facilitate future neuroimaging studies, we developed a\ncomprehensive and reliable manual segmentation protocol based on a\ncellular-resolution brain atlas and high-resolution (0.7^3 mm) MRI data. The\nprotocols involve detailed guidelines to delineate the entire claustrum,\nincluding the inferior parts that have not been clearly described in earlier\nMRI studies. Additionally, we propose a geometric method to parcellate the\nclaustrum into three subregions (the dorsal, ventral, and temporal claustrum)\nalong the superior-to-inferior axis. The mean bilateral claustrum volume in 10\nyoung adults was 3307.5 mm^3, approximately 0.21% of total intracranial volume.\nOur segmentation protocol demonstrated high inter- and intra-rater reliability\n(ICC > 0.89, DSC > 0.85), confirming its replicability. This comprehensive and\nreliable claustrum segmentation protocols will provide a cornerstone for future\nneuroimaging studies of systematic, large-scale investigations of the anatomy\nand the functions of the human claustrum in normal and pathological\npopulations.",
        "Although individual neurons and neural populations exhibit the phenomenon of\nrepresentational drift, perceptual and behavioral outputs of many neural\ncircuits can remain stable across time scales over which representational drift\nis substantial. These observations motivate a dynamical systems framework for\nneural network activity that focuses on the concept of \\emph{latent processing\nunits,} core elements for robust coding and computation embedded in collective\nneural dynamics. Our theoretical treatment of these latent processing units\nyields five key attributes of computing through neural network dynamics. First,\nneural computations that are low-dimensional can nevertheless generate\nhigh-dimensional neural dynamics. Second, the manifolds defined by neural\ndynamical trajectories exhibit an inherent coding redundancy as a direct\nconsequence of the universal computing capabilities of the underlying dynamical\nsystem. Third, linear readouts or decoders of neural population activity can\nsuffice to optimally subserve downstream circuits controlling behavioral\noutputs. Fourth, whereas recordings from thousands of neurons may suffice for\nnear optimal decoding from instantaneous neural activity patterns, experimental\naccess to millions of neurons may be necessary to predict neural ensemble\ndynamical trajectories across timescales of seconds. Fifth, despite the\nvariable activity of single cells, neural networks can maintain stable\nrepresentations of the variables computed by the latent processing units,\nthereby making computations robust to representational drift. Overall, our\nframework for latent computation provides an analytic description and\nempirically testable predictions regarding how large systems of neurons perform\nrobust computations via their collective dynamics.",
        "In this review, we examine computational models that explore the role of\nneural oscillations in speech perception, spanning from early auditory\nprocessing to higher cognitive stages. We focus on models that use rhythmic\nbrain activities, such as gamma, theta, and delta oscillations, to encode\nphonemes, segment speech into syllables and words, and integrate linguistic\nelements to infer meaning. We analyze the mechanisms underlying these models,\ntheir biological plausibility, and their potential applications in processing\nand understanding speech in real time, a computational feature that is achieved\nby the human brain but not yet implemented in speech recognition models.\nReal-time processing enables dynamic adaptation to incoming speech, allowing\nsystems to handle the rapid and continuous flow of auditory information\nrequired for effective communication, interactive applications, and accurate\nspeech recognition in a variety of real-world settings. While significant\nprogress has been made in modeling the neural basis of speech perception,\nchallenges remain, particularly in accounting for the complexity of semantic\nprocessing and the integration of contextual influences. Moreover, the high\ncomputational demands of biologically realistic models pose practical\ndifficulties for their implementation and analysis. Despite these limitations,\nthese models provide valuable insights into the neural mechanisms of speech\nperception. We conclude by identifying current limitations, proposing future\nresearch directions, and suggesting how these models can be further developed\nto achieve a more comprehensive understanding of speech processing in the human\nbrain.",
        "When is it beneficial to constrain creativity? Creativity thrives with\nfreedom, but when people collaborate to create artifacts, there is tension\nbetween giving individuals freedom to revise, and protecting prior\nachievements. To test how imposing constraints may affect collective\ncreativity, we performed cultural evolution experiments where participants\ncollaborated to create melodies and images in chains. With melodies, we found\nthat limiting step size (number of musical notes that can be changed) improved\npleasantness ratings for created tunes. Similar results were observed in\ncohorts of musicians, and with different selection regimes. In contrast,\nlimiting step size in creating images consistently reduced pleasantness. These\nconflicting findings suggest that in domains such as music, where artifacts can\nbe easily damaged, and where evolutionary outcomes are hard to foresee,\ncollective creativity may benefit from imposing small step sizes. We discuss\nparallels with search algorithms and the evolution of conservative birdsong\ncultures.",
        "Studying psychiatric illness has often been limited by difficulties in\nconnecting symptoms and behavior to neurobiology. Computational psychiatry\napproaches promise to bridge this gap by providing formal accounts of the\nlatent information processing changes that underlie the development and\nmaintenance of psychiatric phenomena. Models based on these theories generate\nindividual-level parameter estimates which can then be tested for relationships\nto neurobiology. In this review, we explore computational modelling approaches\nto one key aspect of health and illness: affect. We discuss strengths and\nlimitations of key approaches to modelling affect, with a focus on\nreinforcement learning, active inference, the hierarchical gaussian filter, and\ndrift-diffusion models. We find that, in this literature, affect is an\nimportant source of modulation in decision making, and has a bidirectional\ninfluence on how individuals infer both internal and external states.\nHighlighting the potential role of affect in information processing changes\nunderlying symptom development, we extend an existing model of psychosis, where\naffective changes are influenced by increasing cortical noise and consequent\nincreases in either perceived environmental instability or expected noise in\nsensory input, becoming part of a self-reinforcing process generating\nnegatively valenced, over-weighted priors underlying positive symptom\ndevelopment. We then provide testable predictions from this model at\ncomputational, neurobiological, and phenomenological levels of description.",
        "In this study, we examine the frequency and physical drivers of\ntransformations from cool-core (CC) to non-cool-core (NCC) clusters, and vice\nversa, in a sample of 352 massive galaxy clusters (M_vir = 10^14-15.3 M_sun)\nfrom the TNG-Cluster magnetohydrodynamical cosmological simulation of galaxies.\nBy identifying transformations based on the evolution of central entropy and\nfocusing on z<2.5, we find that clusters frequently undergo such events,\ndepending on their assembly and supermassive black hole histories. On average,\nclusters experience 2 to 3 transformations. Transformations can occur in both\ndirections and can be temporary, but those to higher entropy cores, i.e. in the\ndirection from CC to NCC states, are the vast majority. CC phases are shorter\nthan NCC phases, and thus overall the TNG-Cluster population forms with\nlow-entropy cores and moves towards NCC states with time. We study the role\nthat mergers play in driving transformations, and find that mergers within\n~1Gyr prior to a transformation toward higher (but not lower) entropy cores\noccur statistically more often than in a random control sample. Most\nimportantly, we find examples of mergers associated with CC disruption\nregardless of their mass ratio or angular momentum. However, past merger\nactivity is not a good predictor for z=0 CC status, at least based on core\nentropy, even though clusters undergoing more mergers eventually have the\nhighest core entropy values at z=0. We consider the interplay between AGN\nfeedback and evolving cluster core thermodynamics. We find that core\ntransformations are accompanied by an increase in AGN activity, whereby\nfrequent and repeated (kinetic) energy injections from the central SMBHs can\nproduce a collective, long-term impact on central entropy, ultimately heating\ncluster cores. Whether such fast-paced periods of AGN activity are triggered by\nmergers is plausible, but not necessary.",
        "We consider the possibility that gravity is mediated by \"continuous spin\"\nparticles, i.e., massless particles whose invariant spin scale $\\rho_g$ is\nnon-zero. In this case, the primary helicity-2 modes of gravitational radiation\non a Minkowski background mix with a tower of integer-helicity partner modes\nunder boosts, with $\\rho_g$ controlling the degree of mixing. We develop a\nformalism for coupling spinless matter to continuous spin gravity at linearized\nlevel. Using this formalism, we calculate the time delay signatures induced by\ngravitational waves in an idealized laser interferometer detector. The\nfractional deviation from general relativity predictions is $O(\\rho_g\/\\omega)$\nfor gravitational wave frequencies $\\omega >\\rho_g$, and the effects of waves\nwith $\\omega \\lesssim \\rho_g$ are damped. The precision and low frequency\nranges of gravitational wave detectors suggest potential sensitivity to spin\nscales at or below $\\sim 10^{-14}$ eV at ground-based laser interferometers and\n$\\sim 10^{-24}$ eV at pulsar timing arrays, motivating further analysis of\nobservable signatures.",
        "Measurements of the Hall coefficient in La$_{2-x}$Sr$_x$CuO$_4$, ranging from\nthe undoped ($x = p = 0$) Mott insulator to overdoped compounds, exhibit a\ntemperature dependence that offers insights into their electronic structure. We\ninterpret these results using a model based on the theory of phase-separation\n(PS) dynamics, which begins at half-filled ($n = 1$) and at a temperature\n$T_{\\rm PS}(p)$, near the pseudogap temperature $T^*(p)$. The $n = 1$ holes\nhave low mobility and provide the modulations of the charge density waves\n(CDW). As doping increases from $p = 0$, these modulations guide the additional\np holes to occupy alternating CDW domains. This charge inhomogeneity may\nfacilitate the formation of localized superconducting amplitudes below the\ncritical onset temperature $T_{\\rm c}^{\\rm max}(p)$. Using thermal activation\nexpressions, along with quantum tunnelling between the charge domains, we\nsuccessfully reproduce all Hall coefficient measurements $R_{\\rm H}(p,T)$ and\nhighlight the relevant energies of cuprates. The calculations confirm three\nsignificant electronic features: the phase-separating role of the pseudogap\ntemperature, the superconducting state achieved through phase coherence,\n  and the two types of charge carriers whose energies and mobilities become\ncomparable at $p \\approx 0.19$, where\n  $T^*(p) \\approx T_{\\rm c}^{\\rm max}(p)$. This results in a crossover from $n\n= p$ to $n = 1 + p$. These findings, along with the\n  $R_{\\rm H}(p,T)$ calculations from insulating to overdoped compounds,\nunderscore the critical role of the electronic\n  phase separation in the properties of cuprates.",
        "We prove that, to every abstract group $G$, we can associate a sequence of\ngraphs $\\Gamma_n$ such that the automorphism group of $\\Gamma_n$ is isomorphic\nto $G$ and the genus of $\\Gamma_n$ is an unbounded function of $n$.",
        "There exist two consistent theories of massless, self-interacting gravitons,\nwhich differ by their local symmetries: general relativity and Weyl transverse\ngravity. We show that these two theories are also the only two metric\ndescriptions of gravity in 4 spacetime dimensions which obey the equivalence\nprinciple for test gravitational physics. We further analyse how the weaker\nformulations of the equivalence principle are realised in Weyl transverse\ngravity (and its generalisations). The analysis sheds light on the behaviour of\nmatter fields in this theory.",
        "Theoretically, the relative change of the Heisenberg-type nearest-neighbor\ncoupling $J_1$ and next-nearest-neighbor coupling $J_2$ in the\nface-centered-cubic lattice can give rise to three main antiferromagnetic\norderings of type-I, type-II, and type-III. However, it is difficult to tune\nthe $J_2\/J_1$ ratio in real materials. Here, we report studies on the influence\nof Te$^{6+}$ and W$^{6+}$ ions replacement to the magnetic interactions and the\nmagnetic ground states in the double-perovskite compounds\nBa$_2$$M$Te$_{1-x}$W$_{x}$O$_6$ ($M$ = Mn, Co). For\nBa$_2$MnTe$_{1-x}$W$_{x}$O$_6$, the W$^{6+}$ doping on Te$^{6+}$ site is\nsuccessful in $0.02 \\leq x \\leq 0.9$ with short-range orders of the type-I\n($0.02 \\leq x \\leq 0.08$) and type-II ($0.1 \\leq x \\leq 0.9$). In\nBa$_2$CoTe$_{1-x}$W${_x}$O$_6$, x-ray diffraction measurements reveal two\ncrystal structures, including the trigonal phase ($0 \\leq x \\leq 0.1$) and the\ncubic phase ($0.5 \\leq x \\leq 1$), between which is a miscibility gap. Two\nmagnetic transitions are identified in the trigonal phase due to two magnetic\nsubsystems, and the type-II magnetic order is observed in the cubic phase.\nMagnetic phase diagrams of Ba$_2M$Te$_{1-x}$W$_{x}$O$_6$ ($M$ = Mn, Co) are\nestablished. Our work shows that the magnetic interactions and ground states of\nBa$_2$$M$Te$_{1-x}$W$_x$O$_6$ can be tuned effectively by the replacement of\nTe$^{6+}$ by W$^{6+}$ ions.",
        "Inequalities for Riesz potentials are well-known to be equivalent to Sobolev\ninequalities of the same order for domain norms \"far\" from $L^1$, but to be\nweaker otherwise. Recent contributions by Van Schaftingen, by Hernandez,\nRai\\c{t}\\u{a} and Spector, and by Stolyarov proved that this gap can be filled\nin Riesz potential inequalities for vector-valued functions in $L^1$ fulfilling\na co-canceling differential condition. This work demonstrates that such a\nproperty is not just peculiar to the space $L^1$. Indeed, under the same\ndifferential constraint, a Riesz potential inequality is shown to hold for any\ndomain and target rearrangement-invariant norms that render a Sobolev\ninequality of the same order true. This is based on a new interpolation\ninequality, which, via a kind of duality argument, yields a parallel property\nof Sobolev inequalities for any linear homogeneous elliptic canceling\ndifferential operator. Specifically, Sobolev inequalities involving the full\ngradient of a certain order share the same rearrangement-invariant domain and\ntarget spaces as their analogs for any other homogeneous elliptic canceling\ndifferential operator of equal order. As a consequence, Riesz potential\ninequalities under the co-canceling constraint and Sobolev inequalities for\nhomogeneous elliptic canceling differential operators are offered for general\nfamilies of rearrangement-invariant spaces, such as the Orlicz spaces and the\nLorentz-Zygmund spaces. Especially relevant instances of inequalities for\ndomain spaces neighboring $L^1$ are singled out.",
        "The multi-armed bandits (MAB) framework is a widely used approach for\nsequential decision-making, where a decision-maker selects an arm in each round\nwith the goal of maximizing long-term rewards. Moreover, in many practical\napplications, such as personalized medicine and recommendation systems,\nfeedback is provided in batches, contextual information is available at the\ntime of decision-making, and rewards from different arms are related rather\nthan independent. We propose a novel semi-parametric framework for batched\nbandits with covariates and a shared parameter across arms, leveraging the\nsingle-index regression (SIR) model to capture relationships between arm\nrewards while balancing interpretability and flexibility. Our algorithm,\nBatched single-Index Dynamic binning and Successive arm elimination (BIDS),\nemploys a batched successive arm elimination strategy with a dynamic binning\nmechanism guided by the single-index direction. We consider two settings: one\nwhere a pilot direction is available and another where the direction is\nestimated from data, deriving theoretical regret bounds for both cases. When a\npilot direction is available with sufficient accuracy, our approach achieves\nminimax-optimal rates (with $d = 1$) for nonparametric batched bandits,\ncircumventing the curse of dimensionality. Extensive experiments on simulated\nand real-world datasets demonstrate the effectiveness of our algorithm compared\nto the nonparametric batched bandit method introduced by\n\\cite{jiang2024batched}.",
        "Motivated by recent literature on the possible existence of a second\nhigher-temperature phase transition in Quantum Chromodynamics, we revisit the\nproposal that colour confinement is related to the dynamics of magnetic\nmonopoles using methods of Topological Data Analysis, which provides a\nmathematically rigorous characterisation of topological properties of\nquantities defined on a lattice. After introducing persistent homology, one of\nthe main tools in Topological Data Analysis, we shall discuss how this concept\ncan be used to quantitatively analyse the behaviour of monopoles across the\ndeconfinement phase transition. Our approach is first demonstrated for Compact\n$U(1)$ Lattice Gauge Theory, which is known to have a zero-temperature\ndeconfinement phase transition driven by the restoration of the symmetry\nassociated with the conservation of the magnetic charge. For this system, we\nperform a finite-size scaling analysis of observables capturing the homology of\nmagnetic current loops, showing that the expected value of the deconfinement\ncritical coupling is reproduced by our analysis. We then extend our method to\n$SU(3)$ gauge theory, in which Abelian magnetic monopoles are identified after\nprojection in the Maximal Abelian Gauge. A finite-size scaling of our\nhomological observables of Abelian magnetic current loops at temporal size $N_t\n= 4$ provides the expected value of the critical coupling with an accuracy that\nis generally higher than that obtained with conventional thermodynamic\napproaches at comparable statistics, hinting towards the relevance of\ntopological properties of monopole currents for confinement.",
        "We study the renormalized Nelson model for a scalar matter particle in a\ncontinuous confining potential interacting with a possibly massless quantized\nradiation field. When the radiation field is massless we impose a mild infrared\nregularization ensuring that the Nelson Hamiltonian has a non-degenerate ground\nstate in all considered cases. Employing Feynman-Kac representations, we derive\nlower bounds on the point-wise spatial decay of the partial Fock space norms of\nground state eigenvectors. Here the exponential rate function governing the\ndecay is given by the Agmon distance familiar from the analysis of\nSchr\\\"{o}dinger operators. For a large class of confining potentials, our lower\nbounds on the decay of ground state eigenvectors match asymptotically with the\nupper bounds implied by previous work of the present authors.",
        "A graph property is elusive (or evasive) if any algorithm testing it by\nasking questions of the form ''Is there an edge between vertices x and y?''\nmust, in the worst case, examine all pairs of vertices. Elusiveness for\ninfinite vertex sets has been first studied by Csern\\'ak and Soukup, who proved\nthat the long-standing Aanderaa-Karp-Rosenberg Conjecture -- which states that\nevery nontrivial monotone graph property is elusive -- fails for infinite\nvertex sets. We extend their work by giving a closer look to the case when the\nvertex set is countably infinite and the ''algorithm'' terminates after\ninfinitely many steps. Among others, we prove that connectedness is elusive,\nwhich strengthens a result of Csern\\'ak and Soukup. We give counterexamples to\nthe infinite version of the Aanderaa-Karp-Rosenberg Conjecture even if the\n''algorithm'' is required to terminate after infinitely many steps, which\nstrengthens results of Csern\\'ak and Soukup.",
        "Friction in atomistic systems is usually described by the classical\nPrandtl-Tomlinson model suitable for capturing the dragging force of a\nnanoparticle in a periodic potential. Here we consider the quantum mechanical\nversion of this model in which the dissipation is facilitated by releasing heat\nto an external bath reservoir. The time evolution of the system is captured\nwith the Liouville-von Neumann equation through the density matrix of the\nsystem in the Markov approximation. We examine several kinetic and dissipative\nproperties of the nanoparticle by delineating classical vs quantum mechanical\neffects. We find that that the Landau-Zener tunneling is a key factor in the\noverall reduction of the frictional dissipation when compared to the classical\nmotion in which such tunneling is absent. This in-depth study analyzes the\ninterplay between velocity, strength of interaction, and temperature to control\nthe frictional process and provide useful guidelines for experimental data\ninterpretation.",
        "The Theory of Proportions and Symbolic Allusions applied Interdisciplinary\n(TPASAI) is a framework that integrates mathematics, linguistics, psychology,\nand game theory to uncover hidden patterns and proportions in reality. Its\ncentral idea is that numerical encoding of symbols, dates, and language can\nreveal recurring structures and connections that reflect universal principles.\nBy applying fractal analysis, the theory identifies patterns across different\nscales, offering a unifying perspective on the structure of the world. One key\naspect of TPASAI is symbolic analysis, which allows for the reinterpretation of\ntraumatic experiences in psychotherapy. For example, assigning numerical values\nto elements like fingers, dates, or words can help individuals uncover\nmeaningful associations between personal experiences and collective symbols.\nThis approach encourages cognitive flexibility and provides a therapeutic\navenue for recontextualizing emotions. The theory also incorporates principles\nof game theory, which frame reality as a system of symbolic \"codes\" governed by\nrules that can be understood and strategically used. This perspective is\nespecially useful for psychological conditions like obsessive-compulsive\ndisorder (OCD), enabling patients to approach their obsessions as decipherable\npatterns rather than rigid constraints. TPASAI has practical applications in\npsychology, education, and technology. In education, it aids in teaching\nmathematical and linguistic concepts by exploring connections between symbolic\nrepresentations and real-world events. In technology, the methodology can be\nemployed in ciphering and natural language processing. The innovation of TPASAI\nlies in its ability to merge the structured rigor of mathematics with the\ninterpretative flexibility of symbolic analysis, offering a deeper\nunderstanding of events and relationships.",
        "We present the first amplitude analysis and branching fraction measurement of\nthe decay $D^+_s\\to \\pi^+\\pi^+\\pi^-\\pi^0\\pi^0$, using $e^+e^-$ collision data\ncollected with the BESIII detector at center-of-mass energies between 4.128 and\n4.226 GeV corresponding to an integrated luminosity of 7.33 fb$^{-1}$, and\nreport the first observation of the pure $W$-annihilation decay $D_s^+ \\to\n\\omega\\rho^+$ with a branching fraction of $(0.99\\pm0.08_{\\rm stat}\\pm0.07_{\\rm\nsyst})\\%$. In comparison to the low significance of the $\\mathcal{D}$ wave in\nthe decay $D_s^+ \\to \\phi\\rho^+$, the dominance of the $\\mathcal{D}$ wave over\nthe $\\mathcal{S}$ and $\\mathcal{P}$ waves, with a fraction of\n$(51.85\\pm7.28_{\\rm stat}\\pm7.90_{\\rm syst})\\%$ observed in the decay, provides\ncrucial information for the``polarization puzzle\", as well as for the\nunderstanding of charm meson decays. The branching fraction of $D^+_s\\to\n\\pi^+\\pi^+\\pi^-\\pi^0\\pi^0$ is measured to be $(4.41\\pm0.15_{\\rm\nstat}\\pm0.13_{\\rm syst})\\%$. Moreover, the branching fraction of $D_s^+ \\to\n\\phi\\rho^+$ is measured to be $(3.98\\pm0.33_{\\rm stat}\\pm0.21_{\\rm syst})\\%$,\nand the $R_{\\phi}= {\\mathcal{B}(\\phi\\to\\pi^+\\pi^-\\pi^0)}\/{\\mathcal{B}(\\phi\\to\nK^+K^-)}$ is determined to be $(0.222\\pm0.019_{\\rm stat}\\pm0.016_{\\rm syst}$),\nwhich is consistent with the previous measurement based on charm meson decays,\nbut deviates from the results from $e^+e^-$ annihilation and $K$-$N$ scattering\nexperiments by more than 3$\\sigma$.",
        "We present a comparison of observed polycyclic aromatic hydrocarbon (PAH)\nfeature ratios in 19 nearby galaxies with a grid of theoretical expectations\nfor near- and mid-infrared dust emission. The PAH feature ratios are drawn from\nCycle 1 JWST observations and are measured for 7224 stellar clusters and 29176\nstellar associations for which we have robust ages and mass estimates from HST\nfive-band photometry. Though there are galaxy-to-galaxy variations, the\nobserved PAH feature ratios largely agree with the theoretical models,\nparticularly those that are skewed toward more ionized and larger PAH size\ndistributions. For each galaxy we also extract PAH feature ratios for 200\npc-wide circular regions in the diffuse interstellar medium, which serve as a\nnon-cluster\/association control sample. Compared to what we find for stellar\nclusters and associations, the 3.3um\/7.7um and 3.3um\/11.3um ratios from the\ndiffuse interstellar medium are $\\sim 0.10-0.15$ dex smaller. When the observed\nPAH feature ratios are compared to the radiation field hardness as probed by\nthe [OIII]\/H$\\beta$ ratio, we find anti-correlations for nearly all galaxies in\nthe sample. These results together suggest that the PAH feature ratios are\ndriven by the shape intensity of the radiation field, and that the smallest\nPAHs -- observed via JWST F335M imaging -- are increasingly 'processed' or\ndestroyed in regions with the most intense and hard radiation fields."
      ]
    }
  },
  {
    "id":2411.1057,
    "research_type":"applied",
    "start_id":"b14",
    "start_title":"Using normative modelling to detect disease progression in mild cognitive impairment and Alzheimer\u2019s disease in a cross-sectional multi-cohort study",
    "start_abstract":"Normative modelling is an emerging method for quantifying how individuals deviate from the healthy populational pattern. Several machine learning models have been implemented to develop normative models to investigate brain disorders, including regression, support vector machines and Gaussian process models. With the advance of deep learning technology, the use of deep neural networks has also been proposed. In this study, we assessed normative models based on deep autoencoders using structural neuroimaging data from patients with Alzheimer\u2019s disease (n\u2009=\u2009206) and mild cognitive impairment (n\u2009=\u2009354). We first trained the autoencoder on an independent dataset (UK Biobank dataset) with 11,034 healthy controls. Then, we estimated how each patient deviated from this norm and established which brain regions were associated to this deviation. Finally, we compared the performance of our normative model against traditional classifiers. As expected, we found that patients exhibited deviations according to the severity of their clinical condition. The model identified medial temporal regions, including the hippocampus, and the ventricular system as critical regions for the calculation of the deviation score. Overall, the normative model had comparable cross-cohort generalizability to traditional classifiers. To promote open science, we are making all scripts and the trained models available to the wider research community.",
    "start_categories":[
      "q-bio.NC"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b13"
      ],
      "title":[
        "Using deep autoencoders to identify abnormal brain structural patterns in neuropsychiatric disorders: A large\u2010scale multi\u2010sample study"
      ],
      "abstract":[
        "Machine learning is becoming an increasingly popular approach for investigating spatially distributed and subtle neuroanatomical alterations in brain\u2010based disorders. However, some machine learning models have been criticized for requiring a large number of cases in each experimental group, and for resembling a \u201cblack box\u201d that provides little or no insight into the nature of the data. In this article, we propose an alternative conceptual and practical approach for investigating brain\u2010based disorders which aim to overcome these limitations. We used an artificial neural network known as \u201cdeep autoencoder\u201d to create a normative model using structural magnetic resonance imaging data from 1,113 healthy people. We then used this model to estimate total and regional neuroanatomical deviation in individual patients with schizophrenia and autism spectrum disorder using two independent data sets (n =\u2009263). We report that the model was able to generate different values of total neuroanatomical deviation for each disease under investigation relative to their control group (p <\u2009.005). Furthermore, the model revealed distinct patterns of neuroanatomical deviations for the two diseases, consistent with the existing neuroimaging literature. We conclude that the deep autoencoder provides a flexible and promising framework for assessing total and regional neuroanatomical deviations in neuropsychiatric populations."
      ],
      "categories":[
        "cs.NE"
      ]
    },
    "list":{
      "title":[
        "An Enhancement of Cuckoo Search Algorithm for Optimal Earthquake\n  Evacuation Space Allocation in Intramuros, Manila City",
        "A Digital Machine Learning Algorithm Simulating Spiking Neural Network\n  CoLaNET",
        "Continuous signal sparse encoding using analog neuromorphic variability",
        "QGAIC: Quantum Inspired Genetic Algorithm for Image Classification",
        "Comprehensive Benchmarking Environment for Worker Flexibility in\n  Flexible Job Shop Scheduling Problems",
        "A Hybrid Tabu Scatter Search Algorithm for Simulation-Based Optimization\n  of Multi-Objective Runway Operations Scheduling",
        "A Weight Adaptation Trigger Mechanism in Decomposition-based\n  Evolutionary Multi-Objective Optimisation",
        "Was Tournament Selection All We Ever Needed? A Critical Reflection on\n  Lexicase Selection",
        "A Runtime Analysis of the Multi-Valued Compact Genetic Algorithm on\n  Generalized LeadingOnes",
        "Cancermorphic Computing Toward Multilevel Machine Intelligence",
        "UAV-assisted Joint Mobile Edge Computing and Data Collection via\n  Matching-enabled Deep Reinforcement Learning",
        "Extract-QD Framework: A Generic Approach for Quality-Diversity in Noisy,\n  Stochastic or Uncertain Domains",
        "Quantum Simplicial Neural Networks",
        "Performance of Practical Quantum Oblivious Key Distribution",
        "SCE: Scalable Consistency Ensembles Make Blackbox Large Language Model\n  Generation More Reliable",
        "Heterogenous Macro-Finance Model: A Mean-field Game Approach",
        "PBM-VFL: Vertical Federated Learning with Feature and Sample Privacy",
        "Psy-Insight: Explainable Multi-turn Bilingual Dataset for Mental Health\n  Counseling",
        "Logistic regression models: practical induced prior specification",
        "Perturbative solutions for compact objects in (2+1)-dimensional\n  Bopp-Podolsky electrodynamics",
        "How to compute the volume in low dimension?",
        "Deeply Optimizing the SAT Solver for the IC3 Algorithm",
        "Remarks on log pluricanonical representations",
        "Dedicated Inference Engine and Binary-Weight Neural Networks for\n  Lightweight Instance Segmentation",
        "Run-and-tumble particles with 1D Coulomb interaction: the active jellium\n  model and the non-reciprocal self-gravitating gas",
        "Revealing the Implicit Noise-based Imprint of Generative Models",
        "Rationality and categorical properties of the moduli of instanton\n  bundles on the projective 3-space",
        "A Spiral Bicycle Track that Can Be Traced by a Unicycle"
      ],
      "abstract":[
        "The Cuckoo Search Algorithm (CSA), while effective in solving complex\noptimization problems, faces limitations in random population initialization\nand reliance on fixed parameters. Random initialization of the population often\nresults in clustered solutions, resulting in uneven exploration of the search\nspace and hindering effective global optimization. Furthermore, the use of\nfixed values for discovery rate and step size creates a trade-off between\nsolution accuracy and convergence speed. To address these limitations, an\nEnhanced Cuckoo Search Algorithm (ECSA) is proposed. This algorithm utilizes\nthe Sobol Sequence to generate a more uniformly distributed initial population\nand incorporates Cosine Annealing with Warm Restarts to dynamically adjust the\nparameters. The performance of the algorithms was evaluated on 13 benchmark\nfunctions (7 unimodal, 6 multimodal). Statistical analyses were conducted to\ndetermine the significance and consistency of the results. The ECSA outperforms\nthe CSA in 11 out of 13 benchmark functions with a mean fitness improvement of\n30% across all functions, achieving 35% for unimodal functions and 24% for\nmultimodal functions. The enhanced algorithm demonstrated increased convergence\nefficiency, indicating its superiority to the CSA in solving a variety of\noptimization problems. The ECSA is subsequently applied to optimize earthquake\nevacuation space allocation in Intramuros, Manila.",
        "During last several years, our research team worked on development of a\nspiking neural network (SNN) architecture, which could be used in the wide\nrange of supervised learning classification tasks. It should work under the\ncondition, that all participating signals (the classified object description,\ncorrect class label and SNN decision) should have spiking nature. As a result,\nthe CoLaNET (columnar layered network) SNN architecture was invented. The\ndistinctive feature of this architecture is a combination of prototypical\nnetwork structures corresponding to different classes and significantly\ndistinctive instances of one class (=columns) and functionally differing\npopulations of neurons inside columns (=layers). The other distinctive feature\nis a novel combination of anti-Hebbian and dopamine-modulated plasticity. While\nCoLaNET is relatively simple, it includes several hyperparameters. Their choice\nfor particular classification tasks is not trivial. Besides that, specific\nfeatures of the data classified (e.g. classification of separate pictures like\nin MNIST dataset vs. classifying objects in a continuous video stream) require\ncertain modifications of CoLaNET structure. To solve these problems, the deep\nmathematical exploration of CoLaNET should be carried out. However, SNNs, being\nstochastic discrete systems, are usually very hard for exact mathematical\nanalysis. To make it easier, I developed a continuous numeric (non-spiking)\nmachine learning algorithm which approximates CoLaNET behavior with\nsatisfactory accuracy. It is described in the paper. At present, it is being\nstudied by exact analytic methods. We hope that the results of this study could\nbe applied to direct calculation of CoLaNET hyperparameters and optimization of\nits structure.",
        "Achieving fast and reliable temporal signal encoding is crucial for\nlow-power, always-on systems. While current spike-based encoding algorithms\nrely on complex networks or precise timing references, simple and robust\nencoding models can be obtained by leveraging the intrinsic properties of\nanalog hardware substrates. We propose an encoding framework inspired by\nbiological principles that leverages intrinsic neuronal variability to robustly\nencode continuous stimuli into spatio-temporal patterns, using at most one\nspike per neuron. The encoder has low model complexity, relying on a shallow\nnetwork of heterogeneous neurons. It relies on an internal time reference,\nallowing for continuous processing. Moreover, stimulus parameters can be\nlinearly decoded from the spiking patterns, granting fast information\nretrieval. Our approach, validated on both analog neuromorphic hardware and\nsimulation, demonstrates high robustness to noise, spike jitter, and reduced\nheterogeneity. Consistently with biological observations, we observed the\nspontaneous emergence of patterns with stereotyped spiking order. The proposed\nencoding scheme facilitates fast, robust and continuous information processing,\nmaking it well-suited for low-power, low-latency processing of temporal data on\nanalog neuromorphic substrates.",
        "This study uses two meta-heuristics methodologies to introduce two novel\nquantum-inspired meta heuristic approaches: quantum-inspired genetic algorithm\n(QIGA1) and quantum-inspired genetic algorithm with dynamic approach (QIGA2).\nThe two suggested methods combine a classical and quantum genetic algorithm\napproach. Both approaches use The correlation coefficient as an assessment\nfunction to identify the best (optimal) values for binary image. In quantum\ncomputing, they use simple ideas like qubits and state superposition. Due to\nthese characteristics, parallelism which uses the time discreteness of quantum\nmechanical systems, is exhibited. For five distinct MNIST datasets, the\nperformance of all participating algorithms has been assessed by comparing the\nsuggested approaches first with their traditional approach counterparts and\nthen with the proposed methods QIGA1 and QIGA2. Each method's ideal threshold\nvalue, associated fitness value (best and average), loss, and accuracy for each\nMNIST dataset have all been published. The outcomes demonstrate the superior\nefficiency of the suggested approaches over their traditional equivalents.",
        "In Production Scheduling, the Flexible Job Shop Scheduling Problem (FJSSP)\naims to optimize a sequence of operations and assign each to an eligible\nmachine with varying processing times. For integration of the workforce, each\nmachine also requires a worker to be present to process an operation which\nadditionally affects the processing times. The resulting problem is called\nFlexible Job Shop Scheduling Problem with Worker Flexibility (FJSSP-W). The\nFJSSP has been approached with various problem representations, including Mixed\nInteger Linear Programming (MILP), Constrained Programming (CP), and\nSimulation-based Optimization (SBO). In the latter area in particular, there\nexists a large number of specialized Evolutionary Algorithms (EA) like Particle\nSwarm Optimization (PSO) or Genetic Algorithms (GA). Yet, the solvers are often\ndeveloped for single use cases only, and validated on a few selected test\ninstances, let alone compared with results from solvers using other problem\nrepresentations. While suitable approaches do also exist, the design of the\nFJSSP-W instances is not standardized and the algorithms are hardly comparable.\nThis calls for a systematic benchmarking environment that provides a\ncomprehensive set of FJSSP(-W) instances and supports targeted algorithm\ndevelopment. It will facilitate the comparison of algorithmic performance in\nthe face of different problem characteristics. The present paper presents a\ncollection of 402 commonly accepted FJSSP instances and proposes an approach to\nextend these with worker flexibility. In addition, we present a detailed\nprocedure for the evaluation of scheduling algorithms on these problem sets and\nprovide suitable model representations for this purpose. We provide complexity\ncharacteristics for all presented instances as well as baseline results of\ncommon commercial solvers to facilitate the validation of new algorithmic\ndevelopments.",
        "This dissertation addresses the growing challenge of air traffic flow\nmanagement by proposing a simulation-based optimization (SbO) approach for\nmulti-objective runway operations scheduling. The goal is to optimize airport\ncapacity utilization while minimizing delays, fuel consumption, and\nenvironmental impacts. Given the NP-Hard complexity of the problem, traditional\nanalytical methods often rely on oversimplifications and fail to account for\nreal-world uncertainties, limiting their practical applicability. The proposed\nSbO framework integrates a discrete-event simulation model to handle stochastic\nconditions and a hybrid Tabu-Scatter Search algorithm to identify\nPareto-optimal solutions, explicitly incorporating uncertainty and fairness\namong aircraft as key objectives. Computational experiments using real-world\ndata from a major U.S. airport demonstrate the approach's effectiveness and\ntractability, outperforming traditional methods such as First-Come-First-Served\n(FCFS) and deterministic approaches while maintaining schedule fairness. The\nalgorithm's ability to generate trade-off solutions between competing\nobjectives makes it a promising decision support tool for air traffic\ncontrollers managing complex runway operations.",
        "Decomposition-based multi-objective evolutionary algorithms (MOEAs) are\nwidely used for solving multi-objective optimisation problems. However, their\neffectiveness depends on the consistency between the problems Pareto front\nshape and the weight distribution. Decomposition-based MOEAs, with uniformly\ndistributed weights (in a simplex), perform well on problems with a regular\n(simplex-like) Pareto front, but not on those with an irregular Pareto front.\nPrevious studies have focused on adapting the weights to approximate the\nirregular Pareto front during the evolutionary process. However, these\nadaptations can actually harm the performance on the regular Pareto front via\nchanging the weights during the search process that are eventually the best fit\nfor the Pareto front. In this paper, we propose an algorithm called the weight\nadaptation trigger mechanism for decomposition-based MOEAs (ATM-MOEA\/D) to\ntackle this issue. ATM-MOEA\/D uses an archive to gradually approximate the\nshape of the Pareto front during the search. When the algorithm detects\nevolution stagnation (meaning the population no longer improves significantly),\nit compares the distribution of the population with that of the archive to\ndistinguish between regular and irregular Pareto fronts. Only when an irregular\nPareto front is identified, the weights are adapted. Our experimental results\nshow that the proposed algorithm not only performs generally better than seven\nstate-of-the-art weight-adapting methods on irregular Pareto fronts but also is\nable to achieve the same results as fixed-weight methods like MOEA\/D on regular\nPareto fronts.",
        "The success of lexicase selection has led to various extensions, including\nits combination with down-sampling, which further increased performance.\nHowever, recent work found that down-sampling also leads to significant\nimprovements in the performance of tournament selection. This raises the\nquestion of whether tournament selection combined with down-sampling is the\nbetter choice, given its faster running times. To address this question, we run\na set of experiments comparing epsilon-lexicase and tournament selection with\ndifferent down-sampling techniques on synthetic problems of varying noise\nlevels and problem sizes as well as real-world symbolic regression problems.\nOverall, we find that down-sampling improves generalization and performance\neven when compared over the same number of generations. This means that\ndown-sampling is beneficial even with way fewer fitness evaluations.\nAdditionally, down-sampling successfully reduces code growth. We observe that\npopulation diversity increases for tournament selection when combined with\ndown-sampling. Further, we find that tournament selection and epsilon-lexicase\nselection with down-sampling perform similar, while tournament selection is\nsignificantly faster. We conclude that tournament selection should be further\nanalyzed and improved in future work instead of only focusing on the\nimprovement of lexicase variants.",
        "In the literature on runtime analyses of estimation of distribution\nalgorithms (EDAs), researchers have recently explored univariate EDAs for\nmulti-valued decision variables. Particularly, Jedidia et al. gave the first\nruntime analysis of the multi-valued UMDA on the r-valued LeadingOnes\n(r-LeadingOnes) functions and Adak et al. gave the first runtime analysis of\nthe multi-valued cGA (r-cGA) on the r-valued OneMax function. We utilize their\nframework to conduct an analysis of the multi-valued cGA on the r-valued\nLeadingOnes function. Even for the binary case, a runtime analysis of the\nclassical cGA on LeadingOnes was not yet available. In this work, we show that\nthe runtime of the r-cGA on r-LeadingOnes is O(n^2r^2 log^3 n log^2 r) with\nhigh probability.",
        "Despite their potential to address crucial bottlenecks in computing\narchitectures and contribute to the pool of biological inspiration for\nengineering, pathological biological mechanisms remain absent from\ncomputational theory. We hereby introduce the concept of cancer-inspired\ncomputing as a paradigm drawing from the adaptive, resilient, and evolutionary\nstrategies of cancer, for designing computational systems capable of thriving\nin dynamic, adversarial or resource-constrained environments. Unlike known\nbioinspired approaches (e.g., evolutionary and neuromorphic architectures),\ncancer-inspired computing looks at emulating the uniqueness of cancer cells\nsurvival tactics, such as somatic mutation, metastasis, angiogenesis and immune\nevasion, as parallels to desirable features in computing architectures, for\nexample decentralized propagation and resource optimization, to impact areas\nlike fault tolerance and cybersecurity. While the chaotic growth of cancer is\ncurrently viewed as uncontrollable in biology, randomness-based algorithms are\nalready being successfully demonstrated in enhancing the capabilities of other\ncomputing architectures, for example chaos computing integration. This vision\nfocuses on the concepts of multilevel intelligence and context-driven mutation,\nand their potential to simultaneously overcome plasticity-limited neuromorphic\napproaches and the randomness of chaotic approaches. The introduction of this\nconcept aims to generate interdisciplinary discussion to explore the potential\nof cancer-inspired mechanisms toward powerful and resilient artificial systems.",
        "Unmanned aerial vehicle (UAV)-assisted mobile edge computing (MEC) and data\ncollection (DC) have been popular research issues. Different from existing\nworks that consider MEC and DC scenarios separately, this paper investigates a\nmulti-UAV-assisted joint MEC-DC system. Specifically, we formulate a joint\noptimization problem to minimize the MEC latency and maximize the collected\ndata volume. This problem can be classified as a non-convex mixed integer\nprogramming problem that exhibits long-term optimization and dynamics. Thus, we\npropose a deep reinforcement learning-based approach that jointly optimizes the\nUAV movement, user transmit power, and user association in real time to solve\nthe problem efficiently. Specifically, we reformulate the optimization problem\ninto an action space-reduced Markov decision process (MDP) and optimize the\nuser association by using a two-phase matching-based association (TMA)\nstrategy. Subsequently, we propose a soft actor-critic (SAC)-based approach\nthat integrates the proposed TMA strategy (SAC-TMA) to solve the formulated\njoint optimization problem collaboratively. Simulation results demonstrate that\nthe proposed SAC-TMA is able to coordinate the two subsystems and can\neffectively reduce the system latency and improve the data collection volume\ncompared with other benchmark algorithms.",
        "Quality-Diversity (QD) has demonstrated potential in discovering collections\nof diverse solutions to optimisation problems. Originally designed for\ndeterministic environments, QD has been extended to noisy, stochastic, or\nuncertain domains through various Uncertain-QD (UQD) methods. However, the\nlarge number of UQD methods, each with unique constraints, makes selecting the\nmost suitable one challenging. To remedy this situation, we present two\ncontributions: first, the Extract-QD Framework (EQD Framework), and second,\nExtract-ME (EME), a new method derived from it. The EQD Framework unifies\nexisting approaches within a modular view, and facilitates developing novel\nmethods by interchanging modules. We use it to derive EME, a novel method that\nconsistently outperforms or matches the best existing methods on standard\nbenchmarks, while previous methods show varying performance. In a second\nexperiment, we show how our EQD Framework can be used to augment existing QD\nalgorithms and in particular the well-established\nPolicy-Gradient-Assisted-MAP-Elites method, and demonstrate improved\nperformance in uncertain domains at no additional evaluation cost. For any new\nuncertain task, our contributions now provide EME as a reliable \"first guess\"\nmethod, and the EQD Framework as a tool for developing task-specific\napproaches. Together, these contributions aim to lower the cost of adopting UQD\ninsights in QD applications.",
        "Graph Neural Networks (GNNs) excel at learning from graph-structured data but\nare limited to modeling pairwise interactions, insufficient for capturing\nhigher-order relationships present in many real-world systems. Topological Deep\nLearning (TDL) has allowed for systematic modeling of hierarchical higher-order\ninteractions by relying on combinatorial topological spaces such as simplicial\ncomplexes. In parallel, Quantum Neural Networks (QNNs) have been introduced to\nleverage quantum mechanics for enhanced computational and learning power. In\nthis work, we present the first Quantum Topological Deep Learning Model:\nQuantum Simplicial Networks (QSNs), being QNNs operating on simplicial\ncomplexes. QSNs are a stack of Quantum Simplicial Layers, which are inspired by\nthe Ising model to encode higher-order structures into quantum states.\nExperiments on synthetic classification tasks show that QSNs can outperform\nclassical simplicial TDL models in accuracy and efficiency, demonstrating the\npotential of combining quantum computing with TDL for processing data on\ncombinatorial topological spaces.",
        "Motivated by the applications of secure multiparty computation as a\nprivacy-protecting data analysis tool, and identifying oblivious transfer as\none of its main practical enablers, we propose a practical realization of\nrandomized quantum oblivious transfer. By using only symmetric cryptography\nprimitives to implement commitments, we construct computationally-secure\nrandomized oblivious transfer without the need for public-key cryptography or\nassumptions imposing limitations on the adversarial devices. We show that the\nprotocol is secure under an indistinguishability-based notion of security and\ndemonstrate an experimental implementation to test its real-world performance.\nIts security and performance are then compared to both quantum and classical\nalternatives, showing potential advantages over existing solutions based on the\nnoisy storage model and public-key cryptography.",
        "Large language models (LLMs) have demonstrated remarkable performance, yet\ntheir diverse strengths and weaknesses prevent any single LLM from achieving\ndominance across all tasks. Ensembling multiple LLMs is a promising approach to\ngenerate reliable responses but conventional ensembling frameworks suffer from\nhigh computational overheads. This work introduces Scalable Consistency\nEnsemble (SCE), an efficient framework for ensembling LLMs by prompting\nconsistent outputs. The SCE framework systematically evaluates and integrates\noutputs to produce a cohesive result through two core components: SCE-CHECK, a\nmechanism that gauges the consistency between response pairs via semantic\nequivalence; and SCE-FUSION, which adeptly merges the highest-ranked consistent\nresponses from SCE-CHECK, to optimize collective strengths and mitigating\npotential weaknesses. To improve the scalability with multiple inference\nqueries, we further propose ``{You Only Prompt Once}'' (YOPO), a novel\ntechnique that reduces the inference complexity of pairwise comparison from\nquadratic to constant time. We perform extensive empirical evaluations on\ndiverse benchmark datasets to demonstrate \\methodName's effectiveness. Notably,\nthe \\saccheckcomponent outperforms conventional baselines with enhanced\nperformance and a significant reduction in computational overhead.",
        "We investigate the full dynamics of capital allocation and wealth\ndistribution of heterogeneous agents in a frictional economy during booms and\nbusts using tools from mean-field games. Two groups in our models, namely the\nexpert and the household, are interconnected within and between their classes\nthrough the law of capital processes and are bound by financial constraints.\nSuch a mean-field interaction explains why experts accumulate a lot of capital\nin the good times and reverse their behavior quickly in the bad times even in\nthe absence of aggregate macro-shocks. When common noises from the market are\ninvolved, financial friction amplifies the mean-field effect and leads to\ncapital fire sales by experts. In addition, the implicit interlink between and\nwithin heterogeneous groups demonstrates the slow economic recovery and\ncharacterizes the deviating and fear-of-missing-out (FOMO) behaviors of\nhouseholds compared to their counterparts. Our model also gives a fairly\nexplicit representation of the equilibrium solution without exploiting\ncomplicated numerical approaches.",
        "We present Poisson Binomial Mechanism Vertical Federated Learning (PBM-VFL),\na communication-efficient Vertical Federated Learning algorithm with\nDifferential Privacy guarantees. PBM-VFL combines Secure Multi-Party\nComputation with the recently introduced Poisson Binomial Mechanism to protect\nparties' private datasets during model training. We define the novel concept of\nfeature privacy and analyze end-to-end feature and sample privacy of our\nalgorithm. We compare sample privacy loss in VFL with privacy loss in HFL. We\nalso provide the first theoretical characterization of the relationship between\nprivacy budget, convergence error, and communication cost in\ndifferentially-private VFL. Finally, we empirically show that our model\nperforms well with high levels of privacy.",
        "The in-context learning capabilities of large language models (LLMs) show\ngreat potential in mental health support. However, the lack of counseling\ndatasets, particularly in Chinese corpora, restricts their application in this\nfield. To address this, we constructed Psy-Insight, the first mental\nhealth-oriented explainable multi-task bilingual dataset. We collected\nface-to-face multi-turn counseling dialogues, which are annotated with\nmulti-task labels and conversation process explanations. Our annotations\ninclude psychotherapy, emotion, strategy, and topic labels, as well as\nturn-level reasoning and session-level guidance. Psy-Insight is not only\nsuitable for tasks such as label recognition but also meets the need for\ntraining LLMs to act as empathetic counselors through logical reasoning.\nExperiments show that training LLMs on Psy-Insight enables the models to not\nonly mimic the conversation style but also understand the underlying strategies\nand reasoning of counseling.",
        "Bayesian inference for statistical models with a hierarchical structure is\noften characterized by specification of priors for parameters at different\nlevels of the hierarchy. When higher level parameters are functions of the\nlower level parameters, specifying a prior on the lower level parameters leads\nto induced priors on the higher level parameters. However, what are deemed\nuninformative priors for lower level parameters can induce strikingly non-vague\npriors for higher level parameters. Depending on the sample size and specific\nmodel parameterization, these priors can then have unintended effects on the\nposterior distribution of the higher level parameters.\n  Here we focus on Bayesian inference for the Bernoulli distribution parameter\n$\\theta$ which is modeled as a function of covariates via a logistic\nregression, where the coefficients are the lower level parameters for which\npriors are specified. A specific area of interest and application is the\nmodeling of survival probabilities in capture-recapture data and occupancy and\ndetection probabilities in presence-absence data. In particular we propose\nalternative priors for the coefficients that yield specific induced priors for\n$\\theta$. We address three induced prior cases. The simplest is when the\ninduced prior for $\\theta$ is Uniform(0,1). The second case is when the induced\nprior for $\\theta$ is an arbitrary Beta($\\alpha$, $\\beta$) distribution. The\nthird case is one where the intercept in the logistic model is to be treated\ndistinct from the partial slope coefficients; e.g., $E[\\theta]$ equals a\nspecified value on (0,1) when all covariates equal 0. Simulation studies were\ncarried out to evaluate performance of these priors and the methods were\napplied to a real presence\/absence data set and occupancy modelling.",
        "We investigate the space-time geometry generated by compact objects in\n(2+1)-dimensional Bopp-Podolsky electrodynamics. Inspired by previous studies\nwhere the Bopp-Podolsky field acts as a source for spherically symmetric\nsolutions, we revisit this question within the lower-dimensional (2+1)\nframework. Using a perturbative approach, we derive a charged BTZ-like black\nhole solution and compute corrections up to second order in a perturbative\nexpansion valid far from the horizon. Our analysis suggests that the\nnear-horizon and inner structure of the solution remain unaltered, indicating\nthat no new non-black hole objects emerge in this regime. In particular, we do\nnot find evidence of wormhole solutions in the (2+1)-dimensional version of\nthis theory.",
        "Estimating the volume of a convex body is a canonical problem in theoretical\ncomputer science. Its study has led to major advances in randomized algorithms,\nMarkov chain theory, and computational geometry. In particular, determining the\nquery complexity of volume estimation to a membership oracle has been a\nlongstanding open question. Most of the previous work focuses on the\nhigh-dimensional limit. In this work, we tightly characterize the\ndeterministic, randomized and quantum query complexity of this problem in the\nhigh-precision limit, i.e., when the dimension is constant.",
        "The IC3 algorithm, also known as PDR, is a SAT-based model checking algorithm\nthat has significantly influenced the field in recent years due to its\nefficiency, scalability, and completeness. It utilizes SAT solvers to solve a\nseries of SAT queries associated with relative induction. In this paper, we\nintroduce several optimizations for the SAT solver in IC3 based on our\nobservations of the unique characteristics of these SAT queries. By observing\nthat SAT queries do not necessarily require decisions on all variables, we\ncompute a subset of variables that need to be decided before each solving\nprocess while ensuring that the result remains unaffected. Additionally, noting\nthat the overhead of binary heap operations in VSIDS is non-negligible, we\nreplace the binary heap with buckets to achieve constant-time operations.\nFurthermore, we support temporary clauses without the need to allocate a new\nactivation variable for each solving process, thereby eliminating the need to\nreset solvers. We developed a novel lightweight CDCL SAT solver, GipSAT, which\nintegrates these optimizations. A comprehensive evaluation highlights the\nperformance improvements achieved by GipSAT. Specifically, the GipSAT-based IC3\ndemonstrates an average speedup of 3.61 times in solving time compared to the\nIC3 implementation based on MiniSat.",
        "We show the finiteness of log pluricanonical representations under the\nassumption of the existence of a good minimal model.",
        "Reducing computational costs is an important issue for development of\nembedded systems. Binary-weight Neural Networks (BNNs), in which weights are\nbinarized and activations are quantized, are employed to reduce computational\ncosts of various kinds of applications. In this paper, a design methodology of\nhardware architecture for inference engines is proposed to handle modern BNNs\nwith two operation modes. Multiply-Accumulate (MAC) operations can be\nsimplified by replacing multiply operations with bitwise operations. The\nproposed method can effectively reduce the gate count of inference engines by\nremoving a part of computational costs from the hardware system. The\narchitecture of MAC operations can calculate the inference results of BNNs\nefficiently with only 52% of hardware costs compared with the related works. To\nshow that the inference engine can handle practical applications, two\nlightweight networks which combine the backbones of SegNeXt and the decoder of\nSparseInst for instance segmentation are also proposed. The output results of\nthe lightweight networks are computed using only bitwise operations and add\noperations. The proposed inference engine has lower hardware costs than related\nworks. The experimental results show that the proposed inference engine can\nhandle the proposed instance-segmentation networks and achieves higher accuracy\nthan YOLACT on the \"Person\" category although the model size is 77.7$\\times$\nsmaller compared with YOLACT.",
        "Recently we studied $N$ run-and-tumble particles in one dimension - which\nswitch with rate $\\gamma$ between driving velocities $\\pm v_0$ - interacting\nvia the long range 1D Coulomb potential (also called rank interaction), both in\nthe attractive and in the repulsive case, with and without a confining\npotential. We extend this study in two directions. First we consider the same\nsystem, but inside a harmonic confining potential, which we call \"active\njellium\". We obtain a parametric representation of the particle density in the\nstationary state at large $N$, which we analyze in detail. Contrary to the\nlinear potential, there is always a steady-state where the density has a\nbounded support. However, we find that the model still exhibits transitions\nbetween phases with different behaviors of the density at the edges, ranging\nfrom a continuous decay to a jump, or even a shock (i.e. a cluster of\nparticles, which manifests as a delta peak in the density). Notably, the\ninteractions forbid a divergent density at the edges, which may occur in the\nnon-interacting case. In the second part, we consider a non-reciprocal version\nof the rank interaction: the $+$ particles (of velocity $+v_0$) are attracted\ntowards the $-$ particles (of velocity $-v_0$) with a constant force $b\/N$,\nwhile the $-$ particles are repelled by the $+$ particles with a force of same\namplitude. In order for a stationary state to exist we add a linear confining\npotential. We derive an explicit expression for the stationary density at large\n$N$, which exhibits an explicit breaking of the mirror symmetry with respect to\n$x=0$. This again shows the existence of several phases, which differ by the\npresence or absence of a shock at $x=0$, with one phase even exhibiting a\nvanishing density on the whole region $x>0$. Our analytical results are\ncomplemented by numerical simulations for finite $N$.",
        "With the rapid advancement of vision generation models, the potential\nsecurity risks stemming from synthetic visual content have garnered increasing\nattention, posing significant challenges for AI-generated image detection.\nExisting methods suffer from inadequate generalization capabilities, resulting\nin unsatisfactory performance on emerging generative models. To address this\nissue, this paper presents a novel framework that leverages noise-based\nmodel-specific imprint for the detection task. Specifically, we propose a novel\nnoise-based imprint simulator to capture intrinsic patterns imprinted in images\ngenerated by different models. By aggregating imprints from various generative\nmodels, imprints of future models can be extrapolated to expand training data,\nthereby enhancing generalization and robustness. Furthermore, we design a new\npipeline that pioneers the use of noise patterns, derived from a noise-based\nimprint extractor, alongside other visual features for AI-generated image\ndetection, resulting in a significant improvement in performance. Our approach\nachieves state-of-the-art performance across three public benchmarks including\nGenImage, Synthbuster and Chameleon.",
        "We prove the rationality and irreducibility of the moduli space of\nmathematical instanton vector bundles of arbitrary rank and charge on $\\mathbb\nP^3$. In particular, the result applies to the rank-2 case. This problem was\nfirst studied by Barth, Hartshorne, Hirschowitz-Narasimhan in the late 1970s.\nWe also show that the mathematical instantons of variable rank and charge form\na monoidal category. The proof is based on an in-depth analysis of the\nBarth-Hulek monad-construction and on a detailed description of the moduli\nspace of (framed and unframed) stable bundles on Hirzebruch surfaces.",
        "A unibike curve is a track that can be made by either a bicycle or a\nunicycle. More precisely, the end of a unit tangent vector at any point on a\nunibike curve lies on the curve (so the bike's front wheel always lies on the\ntrack made by the rear wheel). David Finn found such a curve in 2002, but it\nloops around itself in an extremely complicated way with many twists and\nself-intersections. Starting with the polar square root curve r = sqrt[t\/(2\npi)] and iterating a simple construction involving a differential equation\napparently leads in the limit to a unibike curve having a spiral shape. The\niteration gets each curve as a rear track of its predecessor. Solving hundreds\nof differential equations numerically, where each depends on the preceding one,\nleads to error buildup, but with some care one can get a curve having unibike\nerror less than 10^-7. The evidence is strong for the conjecture that the limit\nof the iteration exists and is a unibike curve."
      ]
    }
  },
  {
    "id":2411.14474,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"Convolutional recurrent neural networks for music classification",
    "start_abstract":"We introduce a convolutional recurrent neural network (CRNN) for music tagging. CRNNs take advantage of networks (CNNs) local feature extraction and temporal summarisation the extracted features. compare CRNN with three CNN structures that have been used tagging while controlling number parameters respect to their performance training time per sample. Overall, we found show strong parameter time, indicating effectiveness its hybrid structure in summarisation.",
    "start_categories":[
      "cs.NE"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b15"
      ],
      "title":[
        "A deep representation for invariance and music classification"
      ],
      "abstract":[
        "Representations in the auditory cortex might be based on mechanisms similar to visual ventral stream; modules for building invariance transformations and multiple layers compositionality selectivity. In this paper we propose use of such computational extracting invariant discriminative audio representations. Building a theory hierarchical architectures, novel, mid-level representation acoustical signals, using empirical distributions projections set templates their transformations. Under assumption that, by construction, dictionary is composed from classes, samples orbit variance-inducing signal (such as shift scale), resulting signature theoretically guaranteed unique, stable deformations. Modules projection pooling can then constitute deep networks, learning composite We present main theoretical aspects framework unsupervised representations, empirically evaluated music genre classification."
      ],
      "categories":[
        "physics.class-ph"
      ]
    },
    "list":{
      "title":[
        "Sliding with Friction and The Brachistochrone Problem",
        "Concepts of Electric Circuit Analysis Revised: Analysis without Current\n  Direction",
        "Canonical Energy-Momentum Tensor of Abelian Fields",
        "Inflation and instabilities of a spherical magnetoelastic balloon",
        "On minimizing cyclists' ascent times: Part II",
        "A geometric derivation of Noether's theorem",
        "The magnetic scalar potential and demagnetization vector for a prism",
        "Experimental Realization of Special-Unitary Operations in Classical\n  Mechanics by Non-Adiabatic Evolutions",
        "Magnetic levitation at low rotation frequencies using an on-axis\n  magnetic field",
        "A lower bound for the energy decay rate in piezoelectricity",
        "The shooting methods to solve 3D nonlinear strings assemblies",
        "About the Keplerization of motion in any central force field",
        "Driven damped harmonic oscillator revisited: energy resonance",
        "NeuroPMD: Neural Fields for Density Estimation on Product Manifolds",
        "Superstructure reflexions in tilted perovskites Part 1",
        "Qubit operations using a modular optical system engineered with\n  PyOpticL: a code-to-CAD optical layout tool",
        "Low-energy neutron cross-talk between organic scintillator detectors",
        "Serrin's overdetermined problems on epigraphs",
        "Symbolic Computations of the Two-Colored Diagrams for Central\n  Configurations of the Planar N-vortex Problem",
        "Local perfect chirality at reflection-zeros away from exceptional points\n  in optical whispering gallery microcavity",
        "The Lehmer complex of a Bruhat interval",
        "Theoretical determination of Gilbert damping in reduced dimensions",
        "Pricing American Parisian Options under General Time-Inhomogeneous\n  Markov Models",
        "Algorithms for 2-Solvable Difference Equations",
        "On a formula of the $q$-series $_{2k+4}\\phi_{2k+3}$ and its applications",
        "Estimating invasive rodent abundance using removal data and hierarchical\n  models",
        "The nexus between disease surveillance, adaptive human behavior and\n  epidemic containment",
        "Orbital Angular Momentum Experimental Bound on the Maximum Predictive\n  Power of Physical Theories in Multi-Dimensional Systems"
      ],
      "abstract":[
        "We analyze the motion of a particle in the gravity field along a family of\ndifferentiable curves taking into account the Coulomb friction forces. A\nparametric equation of the optimal curves is given that generalizes the cycloid\none in this case. The results of numerical calculations in the Mathcad program\nshow that the found curve minimize the descent time for a given friction\ncoefficient and can claim to be a brachistochrone with Coulomb friction.",
        "The electric current as the flux of current density -- a signed scalar, not a\nvector -- is inconsistent with the concept of the current direction, commonly\ninvoked in the electric circuit analyses within, for example, Kirchhoff's\ncurrent law, the resistance rule, and Lenz's law. This paper presents an\napproach to the circuit analysis that is not predicated upon the ``current\ndirection,'' and thus avoids such inconsistencies. The approach is summarized\nin five rules and the application of these rules is demonstrated by analyzing\nan RLC series circuit.",
        "In this tutorial, we provide the natural derivation of symmetrical,\ngauge-invariant canonical energy-momentum tensor for the abelian gauge field,\ni.e., the electromagnetic field.",
        "This study explores the instabilities during the axisymmetric inflation of an\ninitially spherical magnetoelastic balloon, modeled as a magnetizable Ogden\nmaterial, under combined internal pressure and a non-uniform magnetic field\ngenerated by current-carrying coils. The nonlinear interplay of geometric and\nmaterial effects leads to governing equations sensitive to bifurcations and\ninstabilities. A coordinate singularity at the poles of the balloon is\nidentified within the system of governing differential equations, which is\nresolved through an appropriate choice of field variables and L'H\\^opital's\nrule. Stability analysis reveals that as inflation progresses, axisymmetry is\nbroken through a supercritical pitchfork bifurcation, resulting in a\npear-shaped equilibrium. This symmetry is later restored through a reverse\nsubcritical pitchfork bifurcation, forming an isolated loop of pear-shaped\nsolutions containing stable and unstable branches in the case of a\nsix-parameter Ogden material model (SPOM). The onset of symmetry-breaking\nbifurcations is influenced by material parameters and magnetic field intensity,\nwith critical values beyond which such bifurcations are suppressed. Both\nsymmetry-preserving and pear-shaped configurations are stable under small\nasymmetric perturbations in both magnetic and non-magnetic cases. Snap-through\ntransitions between pear-shaped and axisymmetric configurations are also\nobserved.",
        "We formulate an optimization of a bicycle ascent time under the constraints\nof the average, maximum, and minimum powers. In contrast to the first part of\nthis study, we do not restrict the departure to flying starts with an initial\nspeed determined by the model and its optimization. We allow for various\ninitial speeds, from a standstill to a launched start. We accomplish this by\ngeneralizing the discontinuous piecewise constant speed model to a continuous\npiecewise linear speed model. Regardless of the initial speed, steepness or\nprofile of the ascent the optimal strategy tends to a constant ground speed, in\nagreement with the conclusion of the previous, more restricted, formulation.\nThis new formulation allows us to compare various initial-speed strategies and,\nhence, has a direct application to competitive cycling. Notably, in timetrials\ncomposed of flat and steep sections, it helps one decide whether or not to\nchange bicycle, which requires stopping and restarting, from one that is more\nappropriate for flats to one that is more appropriate for uphills.",
        "Noether's theorem is a cornerstone of analytical mechanics, making the link\nbetween symmetries and conserved quantities. In this article, I propose a\nsimple, geometric derivation of this theorem that circumvents the usual\ndifficulties that a student of this field usually encounters. The derivation is\nbased on the direct use of the differential form $pdq -Hdt$, where $p$ is the\nmomentum and $H$ the Hamiltonian, integrated over a simple curve.",
        "We analytically solve Poisson's equation for the magnetic scalar potential\ngenerated by a magnetized prism and determine a closed-form solution for the\nmagnetic scalar potential given only in terms of arctan and natural logarithmic\nfunctions. We furthermore show that this can be written as a demagnetization\nvector, containing all the geometric information of the prism, multiplied with\nthe magnetization, analogous to the way demagnetization tensors are written. We\nvalidate the derived analytical expression for the magnetic scalar potential by\ncomparing with a finite element simulation and show that these agree perfectly.\nWe finally extend the concept of the demagnetization tensor, which contains the\ngeometric information for the source generating the potential, to gravitational\nobjects.",
        "Artificial classical wave systems such as wave crystals and metamaterials\nhave demonstrated promising capabilities in simulating a wide range of quantum\nmechanical phenomena. Yet some gaps between quantum and classical worlds are\ngenerally considered fundamental and difficult to bridge. Dynamics obeying\nspecial unitary groups, e.g., electronic spins described by SU(2), color\nsymmetries of fundamental particles described by SU(3), are such examples. In\nthis work, we present the experimental realization of universal SU(2) and SU(3)\ndynamic operations in classical mechanical oscillator systems with temporally\nmodulated coupling terms. Our approach relies on the sequential execution of\nnon-adiabatic holonomic evolutions, which are typically used in constructing\nquantum-logic gates. The method is swift and purely geometric and can be\nextended to realize more sophisticated dynamic operations. Our results open a\nnew way for studying and simulating quantum phenomena in classical systems.",
        "Magnetic levitation by rotation is a simply yet astonishing phenomenon where\na permanent magnet can be levitated by placing it in the vicinity of another\npermanent magnet that rotates sufficiently fast. The few previous works on this\nnovel type of magnetic levitation all required magnets rotating on the order of\n200 Hz. Here we investigate the influence of applying an on-axis, static\nmagnetic field and show that this can lower the needed rotation frequency to\nbelow 50 Hz. We explain this by a detailed analysis of the force producing\nlevitation, which is a superposition of a repelling force caused by the\noff-axis (rotating) magnetic field and an attractive force due to the on-axis\nfield. We study this force and resulting levitation experimentally,\nanalytically and numerically for three different rotor magnet configurations,\nshowing that the levitation distance and frequency range can be accurately\npredicted from both the numerical and analytical models.",
        "This paper establishes a lower bound for the energy decay rate in\npiezoelectric cylinders. The bound incorporates material properties and\ngeometric factors, including the cross-section's Poincar\\'e-Wirtinger and Korn\nconstants. A detailed analysis of a circular cross-section cylinder yields a\nprecise numerical lower bound, illustrating the practical application of this\nresult.",
        "This article presents an alternative approach to finite elements for modeling\nand analyzing 3D static mooring lines using string theory and the shooting\nmethod (SM) to solve two-point boundary value problems (TPBVPs) for 3D\nnonlinear static string equations with various boundary condition (BC) types\nrelevant to offshore slender system assemblies.The two-point boundary value\nproblem for nonlinear extensible elastic strings was formulated by\nincorporating arbitrary 3D external distributed loads that are not restricted\nto gravity alone. The TPBVP was reformulated based on the formalism of the\nshooting method. A multi-body\/multi-shooting approach is proposed to handle\nmulti-material segments and line assemblies. A formulation of the boundary\nconditions that allows the modeling of Dirichlet, Robin, and mixed boundary\nconditions representing the displacement, force, and combined\nforce\/displacement constraints is presented.Four validation cases are\npresented, comparing the results to analytical solutions: (1) a single catenary\nsegment with ball-prismatic joint boundary conditions under several imposed\nforces, (2) a review of all possible boundary conditions for strings, including\nspring-based BCs, (3) the Velaria problem with nonlinear radial distributed\nload, and (4) a three-segment hanging configuration with different material\nproperties connected by a buoy.The results demonstrate an accuracy under 10-9\nin terms of absolute errors for both positions and tensions along the entire\nlength of the mooring lines. The proposed method also provides error control\nthrough adaptive step integration. It demonstrates high accuracy in modeling\ncomplex 3D kinematics and configurations for mooring lines, while limiting the\niterative problem size.The proposed method provides an efficient alternative to\ndiscretization-based techniques for analyzing static configurations of string\nkinematics slender systems with various end constraints, such as mooring lines\nand hawsers assemblies in offshore engineering, while maintaining simplicity in\napproach and implementation.",
        "The method of keplerization of one-body motion in any central force field,\nintroduced by Martinusi and Gurfil in 2012, is reviewed and reformulated into a\ngeneral homogenization method which applies to any kind of bounded motion. It\nis also shown how this extended method provides a proof of the existence of a\ndynamical symmetry group and how it can be used to extend that group to a\nglobal symmetry group, for any such system",
        "We derive the exact expression for the resonant frequency of the\ntime-averaged steady-state energy and show that this frequency is excellently\napproximated by the arithmetic mean of the amplitude and velocity resonant\nfrequencies. In addition, we argue that the frequency of the amplitude\nresonance can be regarded as the energy resonance frequency, since it provides\nthe maximal peak values of instantaneous energy.",
        "We propose a novel deep neural network methodology for density estimation on\nproduct Riemannian manifold domains. In our approach, the network directly\nparameterizes the unknown density function and is trained using a penalized\nmaximum likelihood framework, with a penalty term formed using manifold\ndifferential operators. The network architecture and estimation algorithm are\ncarefully designed to handle the challenges of high-dimensional product\nmanifold domains, effectively mitigating the curse of dimensionality that\nlimits traditional kernel and basis expansion estimators, as well as overcoming\nthe convergence issues encountered by non-specialized neural network methods.\nExtensive simulations and a real-world application to brain structural\nconnectivity data highlight the clear advantages of our method over the\ncompeting alternatives.",
        "The superstructure spots that appear in diffraction patterns of tilted\nperovskites are well documented and easily calculated using crystallographic\nsoftware. Here, by considering a distortion mode as a perturbation of the\nprototype perovskite structure, we show how the structure factor equation\nyields Boolean conditions for the presence of first order superstructure\nreflexions. A subsequent article describes conditions for second order\nreflexions, which appear only in structures with mixed in-phase and anti-phase\noxygen octahedral tilting. This approach may have some advantages for the\nanalysis of electron diffraction patterns of perovskites.",
        "Complex optical design is hindered by conventional piecewise setup, which\nprevents modularization and therefore abstraction of subsystems at the circuit\nlevel. This limits multiple fields that require complex optics systems,\nincluding quantum computing with atoms and trapped ions, because their optical\nsystems are not scalable. We present an open-source Python library for optical\nlayout (PyOpticL) which uses beam-path simulation and dynamic beam-path routing\nfor quick and easy optical layout by placing optical elements along the beam\npath without a priori specification, enabling dynamic layouts with automatic\nrouting and connectivity. We use PyOpticL to create modular drop-in optical\nbaseplates for common optical subsystems used in atomic and molecular optics\n(AMO) experiments including laser sources, frequency and intensity modulation,\nand locking to an atomic reference for stabilization. We demonstrate this\nminimal working example of a dynamic full laser system for strontium trapped\nions by using it for laser cooling, qubit state detection, and 99.9% fidelity\nsingle-qubit gates with 3D printed baseplates. This enables a new paradigm of\ndesign abstraction layers for engineering optical systems leveraging modular\nbaseplates, as they can be used for any wavelength in the system and enables\nscaling up the underlying optical systems for quantum computers. This new\nopen-source hardware and software code-to-CAD library seeks to foster\nopen-source collaborative hardware and systems design across numerous fields of\nresearch including AMO physics and quantum computing with neutral atoms and\ntrapped ions.",
        "A series of measurements have been performed with low-energy monoenergetic\nneutrons to characterise cross-talk between two organic scintillator detectors.\nCross-talk time-of-flight spectra and probabilities were determined for neutron\nenergies from 1.4 to 15.5 MeV and effective scattering angles ranging from\n$\\sim$50{\\deg} to $\\sim$100{\\deg}. Monte-Carlo simulations incorporating both\nthe active and inactive materials making up the detectors showed reasonable\nagreement with the measurements. Whilst the time-of-flight spectra were very\nwell reproduced, the cross-talk probabilities were only in approximate\nagreement with the measurements, with the most significant discrepancies\n($\\sim$40 %) occurring at the lowest energies. The neutron interaction\nprocesses producing cross-talk at the energies explored here are discussed in\nthe light of these results.",
        "In this work we establish some rigidity results for Serrin's overdetermined\nproblem \\begin{equation*}\n  \\left\\{\n  \\begin{array}{cll}\n  - \\Delta u=f(u) & \\text{in}& \\Omega,\\newline\n  u > 0& \\text{in} & \\Omega,\\newline\n  u=0 & \\text{on} & \\partial \\Omega,\\newline\n  \\dfrac{\\partial u}{\\partial \\eta} = \\mathfrak{c} = const. & \\text{on} &\n\\partial \\Omega,\n  \\end{array}\n  \\right. \\end{equation*}\n  when $\\Omega \\subset \\mathbb{R}^N$ is an epigraph (not necessarily globally\nLipschitz-continuous) and $u$ is a classical solution, possibly unbounded. In\nbroad terms, our main results prove that $\\Omega$ must be an affine half-space\nand $u$ must be one-dimensional, provided the epigraph is bounded from below.\nThese results hold when $f$ is of Allen-Cahn type and $ N \\geq 2$ or,\nalternatively, when $f$ is locally Lipschitz-continuous (with no restriction on\nthe sign of $f(0)$) and $ N \\leq 3$. These results partially answer a question\nraised by Berestycki, Caffarelli and Nirenberg in [1]. Finally, when $f(0) <0$,\nwe also prove a new monotonicity result, valid in any dimension $ N \\geq 2$.",
        "We apply the singular sequence method to investigate the finiteness problem\nfor stationary configurations of the planar N-vortex problem. The initial step\nof the singular sequence method involves identifying all two-colored diagrams.\nThese diagrams represent potential scenarios where finiteness may fail. We\ndevelop a symbolic computation algorithm to determine all two-colored diagrams\nfor central configurations of the planar N-vortex problem.",
        "Recently, a local and imperfect chirality of the resonant eigenmode at the\nexceptional point (EP) has been reported in the optical whispering gallery\nmicrocavity system perturbed by two strong nanoscatterers [Phys. Rev. A 108,\nL041501 (2023)]. Here, we discover a local perfect chirality of the resonant\neigenmode away from the EP in the parameter space of the strongly perturbed\nmicrocavity system. By considering the multiple scattering process of the\nazimuthally propagating modes (APMs) at the nanoscatterers with a\nfirst-principles-based model, the local perfect chirality is predicted to\nresult from the unidirectional reflectionlessness, i.e., the reflection-zero\n(R-zero) of the APMs at the two nanoscatterers. Numerical results and model\npredictions consistently show that the structural parameters of the R-zero\ntypically deviate from those of the EP, which means that the pair of split\nresonant eigenmodes at the R-zero have different complex resonance frequencies\nand electromagnetic fields. In general, only one of the pair of split\neigenmodes exhibits a local perfect chirality within the local azimuthal range\ndivided by the two nanoscatterers. With the decrease of the two nanoscatterers'\nsizes or their relative azimuthal angle, the R-zero tends to coincide with the\nEP.",
        "We introduce Lehmer codes, with immersions in the Bruhat order, for several\nfinite Coxeter groups, including all the classical Weyl groups. This allows to\nassociate to each lower Bruhat interval of these groups a multicomplex whose\nf-polynomial is the Poincar\\'e polynomial of the interval. Via a general\nconstruction, we prove that these polynomials are h-polynomials of\nvertex-decomposable simplicial complexes. Moreover we provide a classification,\nin terms of unimodal permutations, of Poincar\\'e polynomials of smooth Schubert\nvarieties in flag manifolds.",
        "An ab initio scheme based on the linear response theory of exchange torque\ncorrelation is presented to calculate intrinsic Gilbert damping parameters in\nmagnets of reduced dimensions. The method implemented into the real-space\nKorringa-Kohn-Rostoker (RS-KKR) Greens' function framework enables to obtain\ndiagonal elements of the atomic-site-dependent on-site and non-local Gilbert\ndamping tensor. Going from the 3D bulk and surfaces of iron and cobalt\nferromagnets addressed in our previous work [Phys. Rev. B 109, 094417 (2024)],\nin the present paper monolayers of Fe and Co on (001)- and (111)-oriented Cu,\nAg, and Au substrates are studied, and particularly the substrate-dependent\ntrends are compared. Furthermore, the Gilbert damping parameters are calculated\nfor Fe and Co adatoms and dimers on (001)-oriented substrates. It is\ninvestigated how the damping parameter of single adatoms depends on their\nvertical position. This dependence is quantified in relation to the adatoms'\ndensity of states at the Fermi energy showing a non-monotonic behavior. By\nrotating the spin moment of the adatoms and collinear magnetic dimers, an\nanisotropic behavior of the damping is revealed. Finally, a significant, three-\nto ten-times increase of the on-site Gilbert damping is found in\nantiferromagnetic dimers in comparison to the ferromagnetic ones, whilst the\ninter-site damping is even more enhanced.",
        "This paper develops general approaches for pricing various types of\nAmerican-style Parisian options (down-in\/-out, perpetual\/finite-maturity) with\ngeneral payoff functions based on continuous-time Markov chain (CTMC)\napproximation under general 1D time-inhomogeneous Markov models. For the\ndown-in types, by conditioning on the Parisian stopping time, we reduce the\npricing problem to that of a series of vanilla American options with different\nmaturities and their prices integrated with the distribution function of the\nParisian stopping time yield the American Parisian down-in option price. This\nfacilitates an efficient application of CTMC approximation to obtain the\napproximate option price by calculating the required quantities. For the\nperpetual down-in cases under time-homogeneous models, significant\ncomputational cost can be reduced. The down-out cases are more complicated, for\nwhich we use the state augmentation approach to record the excursion duration\nand then the approximate option price is obtained by solving a series of\nvariational inequalities recursively with the Lemke's pivoting method. We show\nthe convergence of CTMC approximation for all the types of American Parisian\noptions under general time-inhomogeneous Markov models, and the accuracy and\nefficiency of our algorithms are confirmed with extensive numerical\nexperiments.",
        "Our paper \"Solving Third Order Linear Difference Equations in Terms of Second\nOrder Equations\" gave two algorithms for solving difference equations in terms\nof lower order equations: an algorithm for absolute factorization, and an\nalgorithm for solving third order equations in terms of second order. Here we\nimprove the efficiency for absolute factorization, and extend the other\nalgorithm to order four.",
        "In this paper we apply a formula of the very-well poised $_{2k+4}\\phi_{2k+3}$\nto write a $k$-tuple sum of $q$-series as a linear combination of terms wherein\neach term is a product of expressions of the form $\\frac{1}{(qy,\nqy^{-1};q)_\\infty}$. As an application, we shall express a variety of sums and\ndouble sums of $q$-series as linear combinations of infinite products. Our\nformulas are motivated by their connection to overpartition pairs.",
        "Invasive rodents pose significant ecological, economic, and public health\nchallenges. Robust methods are needed for estimating population abundance to\nguide effective management. Traditional methods such as capture-recapture are\noften impractical for invasive species due to ethical and logistical\nconstraints. Here, I showcase the application of hierarchical multinomial\nN-mixture models for estimating the abundance of invasive rodents using removal\ndata. First, I perform a simulation study which demonstrates minimal bias in\nabundance estimates across a range of sampling scenarios. Second, I analyze\nremoval data for two invasive rodent species, namely coypus (Myocastor coypus)\nin France and muskrats (Ondatra zibethicus) in the Netherlands. Using\nhierarchical multinomial N-mixture models, I examine the effects of temperature\non abundance while accounting for imperfect and time-varying capture\nprobabilities. I also show how to accommodate spatial variability using random\neffects, and quantify uncertainty in parameter estimates. Overall, I hope to\ndemonstrate the flexibility and utility of hierarchical models in invasive\nspecies management.",
        "Epidemics exhibit interconnected processes that operate at multiple time and\norganizational scales, a hallmark of complex adaptive systems. Modern\nepidemiological modeling frameworks incorporate feedback between\nindividual-level behavioral choices and centralized interventions. Nonetheless,\nthe realistic operational course for disease detection, planning, and response\nis often overlooked. Disease detection is a dynamic challenge, shaped by the\ninterplay between surveillance efforts and transmission characteristics. It\nserves as a tipping point that triggers emergency declarations, information\ndissemination, adaptive behavioral responses, and the deployment of public\nhealth interventions. Evaluating the impact of disease surveillance systems as\ntriggers for adaptive behavior and public health interventions is key to\ndesigning effective control policies.\n  We examine the multiple behavioral and epidemiological dynamics generated by\nthe feedback between disease surveillance and the intertwined dynamics of\ninformation and disease propagation. Specifically, we study the intertwined\ndynamics between: $(i)$ disease surveillance triggering health emergency\ndeclarations, $(ii)$ risk information dissemination producing decentralized\nbehavioral responses, and $(iii)$ centralized interventions. Our results show\nthat robust surveillance systems that quickly detect a disease outbreak can\ntrigger an early response from the population, leading to large epidemic sizes.\nThe key result is that the response scenarios that minimize the final epidemic\nsize are determined by the trade-off between the risk information dissemination\nand disease transmission, with the triggering effect of surveillance mediating\nthis trade-off. Finally, our results confirm that behavioral adaptation can\ncreate a hysteresis-like effect on the final epidemic size.",
        "The completeness of quantum mechanics in predictive power is a central\nquestion in its foundational study. While most investigations focus on\ntwo-dimensional systems, high-dimensional systems are more general and widely\napplicable. Building on the non-extensibility theorem by Colbeck and Renner\n[Phys. Rev. Lett. 101, 050403 (2008)], which established that no higher theory\ncan enhance the predictive power of quantum mechanics for two-dimensional\nsystems, we extend this result to arbitrarily dimensional systems. We connect\nmaximum potential predictive power achievable by any alternative theory to\nexperimentally observable correlations, and establish optimal experimental\nbounds across varying dimensions by exploiting two-photon orbital angular\nmomentum entangled states with entanglement concentration. These bounds falsify\na broader class of alternative theories, including Bell's and Leggett's models,\nand those that remain theoretically ambiguous or experimentally unverified. Our\nfindings not only deepen the foundational understanding of quantum mechanics\nbut also hold significant potential for high-dimensional quantum cryptography."
      ]
    }
  },
  {
    "id":2411.14474,
    "research_type":"applied",
    "start_id":"b15",
    "start_title":"A deep representation for invariance and music classification",
    "start_abstract":"Representations in the auditory cortex might be based on mechanisms similar to visual ventral stream; modules for building invariance transformations and multiple layers compositionality selectivity. In this paper we propose use of such computational extracting invariant discriminative audio representations. Building a theory hierarchical architectures, novel, mid-level representation acoustical signals, using empirical distributions projections set templates their transformations. Under assumption that, by construction, dictionary is composed from classes, samples orbit variance-inducing signal (such as shift scale), resulting signature theoretically guaranteed unique, stable deformations. Modules projection pooling can then constitute deep networks, learning composite We present main theoretical aspects framework unsupervised representations, empirically evaluated music genre classification.",
    "start_categories":[
      "physics.class-ph"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "Convolutional recurrent neural networks for music classification"
      ],
      "abstract":[
        "We introduce a convolutional recurrent neural network (CRNN) for music tagging. CRNNs take advantage of networks (CNNs) local feature extraction and temporal summarisation the extracted features. compare CRNN with three CNN structures that have been used tagging while controlling number parameters respect to their performance training time per sample. Overall, we found show strong parameter time, indicating effectiveness its hybrid structure in summarisation."
      ],
      "categories":[
        "cs.NE"
      ]
    },
    "list":{
      "title":[
        "Continuous signal sparse encoding using analog neuromorphic variability",
        "Neuromorphic Digital-Twin-based Controller for Indoor Multi-UAV Systems\n  Deployment",
        "Transformer Semantic Genetic Programming for Symbolic Regression",
        "Spiking Neural Networks for Temporal Processing: Status Quo and Future\n  Prospects",
        "Toward Relative Positional Encoding in Spiking Transformers",
        "A Digital Machine Learning Algorithm Simulating Spiking Neural Network\n  CoLaNET",
        "Brain in the Dark: Design Principles for Neuromimetic Inference under\n  the Free Energy Principle",
        "Neuromorphic Circuits with Spiking Astrocytes for Increased Energy\n  Efficiency, Fault Tolerance, and Memory Capacitance",
        "EvoRL: A GPU-accelerated Framework for Evolutionary Reinforcement\n  Learning",
        "Multiple-gain Estimation for Running Time of Evolutionary Combinatorial\n  Optimization",
        "A Hybrid Tabu Scatter Search Algorithm for Simulation-Based Optimization\n  of Multi-Objective Runway Operations Scheduling",
        "Comprehensive Benchmarking Environment for Worker Flexibility in\n  Flexible Job Shop Scheduling Problems",
        "Warm Starting of CMA-ES for Contextual Optimization Problems",
        "A shape-optimization approach for inverse diffusion problems using a\n  single boundary measurement",
        "Hydrated Cable Bacteria Exhibit Protonic Conductivity Over Long\n  Distances",
        "Torsion models for tensor-triangulated categories",
        "RiLoCo: An ISAC-oriented AI Solution to Build RIS-empowered Networks",
        "Localization of critical points in annular conical sets via the method\n  of Nehari manifold",
        "Temporal Preference Optimization for Long-Form Video Understanding",
        "Detecting APT Malware Command and Control over HTTP(S) Using Contextual\n  Summaries",
        "PCSI -- The Platform for Content-Structure Inference",
        "Gen-A: Generalizing Ambisonics Neural Encoding to Unseen Microphone\n  Arrays",
        "Scalable intensity-based photonic matrix-vector multiplication processor\n  using single-wavelength time-division-multiplexed signals",
        "Time Warp: The Gap Between Developers' Ideal vs Actual Workweeks in an\n  AI-Driven Era",
        "Algorithms and Hardness Results for the $(k,\\ell)$-Cover Problem",
        "A Feature-Level Ensemble Model for COVID-19 Identification in CXR Images\n  using Choquet Integral and Differential Evolution Optimization",
        "Spread them Apart: Towards Robust Watermarking of Generated Content",
        "Uncertainty Expression for Human-Robot Task Communication"
      ],
      "abstract":[
        "Achieving fast and reliable temporal signal encoding is crucial for\nlow-power, always-on systems. While current spike-based encoding algorithms\nrely on complex networks or precise timing references, simple and robust\nencoding models can be obtained by leveraging the intrinsic properties of\nanalog hardware substrates. We propose an encoding framework inspired by\nbiological principles that leverages intrinsic neuronal variability to robustly\nencode continuous stimuli into spatio-temporal patterns, using at most one\nspike per neuron. The encoder has low model complexity, relying on a shallow\nnetwork of heterogeneous neurons. It relies on an internal time reference,\nallowing for continuous processing. Moreover, stimulus parameters can be\nlinearly decoded from the spiking patterns, granting fast information\nretrieval. Our approach, validated on both analog neuromorphic hardware and\nsimulation, demonstrates high robustness to noise, spike jitter, and reduced\nheterogeneity. Consistently with biological observations, we observed the\nspontaneous emergence of patterns with stereotyped spiking order. The proposed\nencoding scheme facilitates fast, robust and continuous information processing,\nmaking it well-suited for low-power, low-latency processing of temporal data on\nanalog neuromorphic substrates.",
        "Presented study introduces a novel distributed cloud-edge framework for\nautonomous multi-UAV systems that combines the computational efficiency of\nneuromorphic computing with nature-inspired control strategies. The proposed\narchitecture equips each UAV with an individual Spiking Neural Network (SNN)\nthat learns to reproduce optimal control signals generated by a cloud-based\ncontroller, enabling robust operation even during communication interruptions.\nBy integrating spike coding with nature-inspired control principles inspired by\nTilapia fish territorial behavior, our system achieves sophisticated formation\ncontrol and obstacle avoidance in complex urban environments. The distributed\narchitecture leverages cloud computing for complex calculations while\nmaintaining local autonomy through edge-based SNNs, significantly reducing\nenergy consumption and computational overhead compared to traditional\ncentralized approaches. Our framework addresses critical limitations of\nconventional methods, including the dependency on pre-modeled environments,\ncomputational intensity of traditional methods, and local minima issues in\npotential field approaches. Simulation results demonstrate the system's\neffectiveness across two different scenarios. First, the indoor deployment of a\nmulti-UAV system made-up of 15 UAVs. Then the collision-free formation control\nof a moving UAV flock including 6 UAVs considering the obstacle avoidance.\nOwing to the sparsity of spiking patterns, and the event-based nature of SNNs\nin average for the whole group of UAVs, the framework achieves almost 90%\nreduction in computational burden compared to traditional von Neumann\narchitectures implementing traditional artificial neural networks.",
        "In standard genetic programming (stdGP), solutions are varied by modifying\ntheir syntax, with uncertain effects on their semantics. Geometric-semantic\ngenetic programming (GSGP), a popular variant of GP, effectively searches the\nsemantic solution space using variation operations based on linear\ncombinations, although it results in significantly larger solutions. This paper\npresents Transformer Semantic Genetic Programming (TSGP), a novel and flexible\nsemantic approach that uses a generative transformer model as search operator.\nThe transformer is trained on synthetic test problems and learns semantic\nsimilarities between solutions. Once the model is trained, it can be used to\ncreate offspring solutions with high semantic similarity also for unseen and\nunknown problems. Experiments on several symbolic regression problems show that\nTSGP generates solutions with comparable or even significantly better\nprediction quality than stdGP, SLIM_GSGP, DSR, and DAE-GP. Like SLIM_GSGP, TSGP\nis able to create new solutions that are semantically similar without creating\nsolutions of large size. An analysis of the search dynamic reveals that the\nsolutions generated by TSGP are semantically more similar than the solutions\ngenerated by the benchmark approaches allowing a better exploration of the\nsemantic solution space.",
        "Temporal processing is fundamental for both biological and artificial\nintelligence systems, as it enables the comprehension of dynamic environments\nand facilitates timely responses. Spiking Neural Networks (SNNs) excel in\nhandling such data with high efficiency, owing to their rich neuronal dynamics\nand sparse activity patterns. Given the recent surge in the development of\nSNNs, there is an urgent need for a comprehensive evaluation of their temporal\nprocessing capabilities. In this paper, we first conduct an in-depth assessment\nof commonly used neuromorphic benchmarks, revealing critical limitations in\ntheir ability to evaluate the temporal processing capabilities of SNNs. To\nbridge this gap, we further introduce a benchmark suite consisting of three\ntemporal processing tasks characterized by rich temporal dynamics across\nmultiple timescales. Utilizing this benchmark suite, we perform a thorough\nevaluation of recently introduced SNN approaches to elucidate the current\nstatus of SNNs in temporal processing. Our findings indicate significant\nadvancements in recently developed spiking neuron models and neural\narchitectures regarding their temporal processing capabilities, while also\nhighlighting a performance gap in handling long-range dependencies when\ncompared to state-of-the-art non-spiking models. Finally, we discuss the key\nchallenges and outline potential avenues for future research.",
        "Spiking neural networks (SNNs) are bio-inspired networks that model how\nneurons in the brain communicate through discrete spikes, which have great\npotential in various tasks due to their energy efficiency and temporal\nprocessing capabilities. SNNs with self-attention mechanisms (Spiking\nTransformers) have recently shown great advancements in various tasks such as\nsequential modeling and image classifications. However, integrating positional\ninformation, which is essential for capturing sequential relationships in data,\nremains a challenge in Spiking Transformers. In this paper, we introduce an\napproximate method for relative positional encoding (RPE) in Spiking\nTransformers, leveraging Gray Code as the foundation for our approach. We\nprovide comprehensive proof of the method's effectiveness in partially\ncapturing relative positional information for sequential tasks. Additionally,\nwe extend our RPE approach by adapting it to a two-dimensional form suitable\nfor image patch processing. We evaluate the proposed RPE methods on several\ntasks, including time series forecasting, text classification, and patch-based\nimage classification. Our experimental results demonstrate that the\nincorporation of RPE significantly enhances performance by effectively\ncapturing relative positional information.",
        "During last several years, our research team worked on development of a\nspiking neural network (SNN) architecture, which could be used in the wide\nrange of supervised learning classification tasks. It should work under the\ncondition, that all participating signals (the classified object description,\ncorrect class label and SNN decision) should have spiking nature. As a result,\nthe CoLaNET (columnar layered network) SNN architecture was invented. The\ndistinctive feature of this architecture is a combination of prototypical\nnetwork structures corresponding to different classes and significantly\ndistinctive instances of one class (=columns) and functionally differing\npopulations of neurons inside columns (=layers). The other distinctive feature\nis a novel combination of anti-Hebbian and dopamine-modulated plasticity. While\nCoLaNET is relatively simple, it includes several hyperparameters. Their choice\nfor particular classification tasks is not trivial. Besides that, specific\nfeatures of the data classified (e.g. classification of separate pictures like\nin MNIST dataset vs. classifying objects in a continuous video stream) require\ncertain modifications of CoLaNET structure. To solve these problems, the deep\nmathematical exploration of CoLaNET should be carried out. However, SNNs, being\nstochastic discrete systems, are usually very hard for exact mathematical\nanalysis. To make it easier, I developed a continuous numeric (non-spiking)\nmachine learning algorithm which approximates CoLaNET behavior with\nsatisfactory accuracy. It is described in the paper. At present, it is being\nstudied by exact analytic methods. We hope that the results of this study could\nbe applied to direct calculation of CoLaNET hyperparameters and optimization of\nits structure.",
        "Deep learning has revolutionised artificial intelligence (AI) by enabling\nautomatic feature extraction and function approximation from raw data. However,\nit faces challenges such as a lack of out-of-distribution generalisation,\ncatastrophic forgetting and poor interpretability. In contrast, biological\nneural networks, such as those in the human brain, do not suffer from these\nissues, inspiring AI researchers to explore neuromimetic deep learning, which\naims to replicate brain mechanisms within AI models. A foundational theory for\nthis approach is the Free Energy Principle (FEP), which despite its potential,\nis often considered too complex to understand and implement in AI as it\nrequires an interdisciplinary understanding across a variety of fields. This\npaper seeks to demystify the FEP and provide a comprehensive framework for\ndesigning neuromimetic models with human-like perception capabilities. We\npresent a roadmap for implementing these models and a Pytorch code repository\nfor applying FEP in a predictive coding network.",
        "In the rapidly advancing field of neuromorphic computing, integrating\nbiologically-inspired models like the Leaky Integrate-and-Fire Astrocyte (LIFA)\ninto spiking neural networks (SNNs) enhances system robustness and performance.\nThis paper introduces the LIFA model in SNNs, addressing energy efficiency,\nmemory management, routing mechanisms, and fault tolerance. Our core\narchitecture consists of neurons, synapses, and astrocyte circuits, with each\nastrocyte supporting multiple neurons for self-repair. This clustered model\nimproves fault tolerance and operational efficiency, especially under adverse\nconditions. We developed a routing methodology to map the LIFA model onto a\nfault-tolerant, many-core design, optimizing network functionality and\nefficiency. Our model features a fault tolerance rate of 81.10\\% and a\nresilience improvement rate of 18.90\\%, significantly surpassing other\nimplementations. The results validate our approach in memory management,\nhighlighting its potential as a robust solution for advanced neuromorphic\ncomputing applications. The integration of astrocytes represents a significant\nadvancement, setting the stage for more resilient and adaptable neuromorphic\nsystems.",
        "Evolutionary Reinforcement Learning (EvoRL) has emerged as a promising\napproach to overcoming the limitations of traditional reinforcement learning\n(RL) by integrating the Evolutionary Computation (EC) paradigm with RL.\nHowever, the population-based nature of EC significantly increases\ncomputational costs, thereby restricting the exploration of algorithmic design\nchoices and scalability in large-scale settings. To address this challenge, we\nintroduce $\\texttt{$\\textbf{EvoRL}$}$, the first end-to-end EvoRL framework\noptimized for GPU acceleration. The framework executes the entire training\npipeline on accelerators, including environment simulations and EC processes,\nleveraging hierarchical parallelism through vectorization and compilation\ntechniques to achieve superior speed and scalability. This design enables the\nefficient training of large populations on a single machine. In addition to its\nperformance-oriented design, $\\texttt{$\\textbf{EvoRL}$}$ offers a comprehensive\nplatform for EvoRL research, encompassing implementations of traditional RL\nalgorithms (e.g., A2C, PPO, DDPG, TD3, SAC), Evolutionary Algorithms (e.g.,\nCMA-ES, OpenES, ARS), and hybrid EvoRL paradigms such as Evolutionary-guided RL\n(e.g., ERL, CEM-RL) and Population-Based AutoRL (e.g., PBT). The framework's\nmodular architecture and user-friendly interface allow researchers to\nseamlessly integrate new components, customize algorithms, and conduct fair\nbenchmarking and ablation studies. The project is open-source and available at:\nhttps:\/\/github.com\/EMI-Group\/evorl.",
        "The running-time analysis of evolutionary combinatorial optimization is a\nfundamental topic in evolutionary computation. Its current research mainly\nfocuses on specific algorithms for simplified problems due to the challenge\nposed by fluctuating fitness values. This paper proposes a multiple-gain model\nto estimate the fitness trend of population during iterations. The proposed\nmodel is an improved version of the average gain model, which is the approach\nto estimate the running time of evolutionary algorithms for numerical\noptimization. The improvement yields novel results of evolutionary\ncombinatorial optimization, including a briefer proof for the time complexity\nupper bound in the case of (1+1) EA for the Onemax problem, two tighter time\ncomplexity upper bounds than the known results in the case of (1+$\\lambda$) EA\nfor the knapsack problem with favorably correlated weights and a closed-form\nexpression of time complexity upper bound in the case of (1+$\\lambda$) EA for\ngeneral $k$-MAX-SAT problems. The results indicate that the practical running\ntime aligns with the theoretical results, verifying that the multiple-gain\nmodel is more general for running-time analysis of evolutionary combinatorial\noptimization than state-of-the-art methods.",
        "This dissertation addresses the growing challenge of air traffic flow\nmanagement by proposing a simulation-based optimization (SbO) approach for\nmulti-objective runway operations scheduling. The goal is to optimize airport\ncapacity utilization while minimizing delays, fuel consumption, and\nenvironmental impacts. Given the NP-Hard complexity of the problem, traditional\nanalytical methods often rely on oversimplifications and fail to account for\nreal-world uncertainties, limiting their practical applicability. The proposed\nSbO framework integrates a discrete-event simulation model to handle stochastic\nconditions and a hybrid Tabu-Scatter Search algorithm to identify\nPareto-optimal solutions, explicitly incorporating uncertainty and fairness\namong aircraft as key objectives. Computational experiments using real-world\ndata from a major U.S. airport demonstrate the approach's effectiveness and\ntractability, outperforming traditional methods such as First-Come-First-Served\n(FCFS) and deterministic approaches while maintaining schedule fairness. The\nalgorithm's ability to generate trade-off solutions between competing\nobjectives makes it a promising decision support tool for air traffic\ncontrollers managing complex runway operations.",
        "In Production Scheduling, the Flexible Job Shop Scheduling Problem (FJSSP)\naims to optimize a sequence of operations and assign each to an eligible\nmachine with varying processing times. For integration of the workforce, each\nmachine also requires a worker to be present to process an operation which\nadditionally affects the processing times. The resulting problem is called\nFlexible Job Shop Scheduling Problem with Worker Flexibility (FJSSP-W). The\nFJSSP has been approached with various problem representations, including Mixed\nInteger Linear Programming (MILP), Constrained Programming (CP), and\nSimulation-based Optimization (SBO). In the latter area in particular, there\nexists a large number of specialized Evolutionary Algorithms (EA) like Particle\nSwarm Optimization (PSO) or Genetic Algorithms (GA). Yet, the solvers are often\ndeveloped for single use cases only, and validated on a few selected test\ninstances, let alone compared with results from solvers using other problem\nrepresentations. While suitable approaches do also exist, the design of the\nFJSSP-W instances is not standardized and the algorithms are hardly comparable.\nThis calls for a systematic benchmarking environment that provides a\ncomprehensive set of FJSSP(-W) instances and supports targeted algorithm\ndevelopment. It will facilitate the comparison of algorithmic performance in\nthe face of different problem characteristics. The present paper presents a\ncollection of 402 commonly accepted FJSSP instances and proposes an approach to\nextend these with worker flexibility. In addition, we present a detailed\nprocedure for the evaluation of scheduling algorithms on these problem sets and\nprovide suitable model representations for this purpose. We provide complexity\ncharacteristics for all presented instances as well as baseline results of\ncommon commercial solvers to facilitate the validation of new algorithmic\ndevelopments.",
        "Several practical applications of evolutionary computation possess objective\nfunctions that receive the design variables and externally given parameters.\nSuch problems are termed contextual optimization problems. These problems\nrequire finding the optimal solutions corresponding to the given context\nvectors. Existing contextual optimization methods train a policy model to\npredict the optimal solution from context vectors. However, the performance of\nsuch models is limited by their representation ability. By contrast, warm\nstarting methods have been used to initialize evolutionary algorithms on a\ngiven problem using the optimization results on similar problems. Because warm\nstarting methods do not consider the context vectors, their performances can be\nimproved on contextual optimization problems. Herein, we propose a covariance\nmatrix adaptation evolution strategy with contextual warm starting (CMA-ES-CWS)\nto efficiently optimize the contextual optimization problem with a given\ncontext vector. The CMA-ES-CWS utilizes the optimization results of past\ncontext vectors to train the multivariate Gaussian process regression.\nSubsequently, the CMA-ES-CWS performs warm starting for a given context vector\nby initializing the search distribution using posterior distribution of the\nGaussian process regression. The results of the numerical simulation suggest\nthat CMA-ES-CWS outperforms the existing contextual optimization and warm\nstarting methods.",
        "This paper explores the reconstruction of a space-dependent parameter in\ninverse diffusion problems, proposing a shape-optimization-based approach. The\nmain objective is to recover the absorption coefficient from a single boundary\nmeasurement. While conventional gradient-based methods rely on the Fr\\'{e}chet\nderivative of a cost functional with respect to the unknown parameter, we also\nutilize its shape derivative with respect to the unknown boundary interface for\nrecovery. This non-conventional approach addresses the problem of parameter\nrecovery from a single measurement, which represents the key innovation of this\nwork. Numerical experiments confirm the effectiveness of the proposed method,\neven for intricate and non-convex boundary interfaces.",
        "This study presents the direct measurement of proton transport along\nfilamentous Desulfobulbaceae, or cable bacteria. Cable bacteria are filamentous\nmulticellular microorganisms that have garnered much interest due to their\nability to serve as electrical conduits, transferring electrons over several\nmillimeters. Our results indicate that cable bacteria can also function as\nprotonic conduits because they contain proton wires that transport protons at\ndistances greater than 100 um. We find that protonic conductivity ({\\sigma}P)\nalong cable bacteria varies between samples and is measured as high as 114 +\/-\n28 uS cm^-1 at 25-degrees C and 70-percent relative humidity (RH). For cable\nbacteria, the protonic conductance (GP) and {\\sigma}P are dependent upon the\nRH, increasing by as much as 26-fold between 60-percent and 80-percent RH. This\nobservation implies that proton transport occurs via the Grotthuss mechanism\nalong water associated with cable bacteria, forming proton wires. In order to\ndetermine {\\sigma}P and GP along cable bacteria, we implemented a protocol\nusing a modified transfer-printing technique to deposit either palladium\ninterdigitated protodes (IDP), palladium transfer length method (TLM) protodes,\nor gold interdigitated electrodes(IDE) on top of cable bacteria. Due to the\nrelatively mild nature of the transfer-printing technique, this method should\nbe applicable to a broad array of biological samples and curved materials. The\nobservation of protonic conductivity in cable bacteria presents possibilities\nfor investigating the importance of long-distance proton transport in microbial\necosystems and to potentially build biotic or biomimetic scaffolds to interface\nwith materials via proton-mediated gateways or channels.",
        "Given a rigidly-compactly generated tensor-triangulated category whose Balmer\nspectrum is finite dimensional and Noetherian, we construct a torsion model for\nit, which is equivalent to the original tensor-triangulated category. The\ntorsion model is determined in an adelic fashion by objects with singleton\nsupports. This categorifies the Cousin complex from algebra, and the process of\nreconstructing a spectrum from its monochromatic layers in chromatic stable\nhomotopy theory. This model is inspired by work of the second author in\nrational equivariant stable homotopy theory, and extends previous work of the\nauthors from the one-dimensional setting.",
        "The advance towards 6G networks comes with the promise of unprecedented\nperformance in sensing and communication capabilities. The feat of achieving\nthose, while satisfying the ever-growing demands placed on wireless networks,\npromises revolutionary advancements in sensing and communication technologies.\nAs 6G aims to cater to the growing demands of wireless network users, the\nimplementation of intelligent and efficient solutions becomes essential. In\nparticular, reconfigurable intelligent surfaces (RISs), also known as Smart\nSurfaces, are envisioned as a transformative technology for future 6G networks.\nThe performance of RISs when used to augment existing devices is nevertheless\nlargely affected by their precise location. Suboptimal deployments are also\ncostly to correct, negating their low-cost benefits. This paper investigates\nthe topic of optimal RISs diffusion, taking into account the improvement they\nprovide both for the sensing and communication capabilities of the\ninfrastructure while working with other antennas and sensors. We develop a\ncombined metric that takes into account the properties and location of the\nindividual devices to compute the performance of the entire infrastructure. We\nthen use it as a foundation to build a reinforcement learning architecture that\nsolves the RIS deployment problem. Since our metric measures the surface where\ngiven localization thresholds are achieved and the communication coverage of\nthe area of interest, the novel framework we provide is able to seamlessly\nbalance sensing and communication, showing its performance gain against\nreference solutions, where it achieves simultaneously almost the reference\nperformance for communication and the reference performance for localization.",
        "Using the Nehari manifold method, we establish sufficient conditions such\nthat a smooth functional attains a ground state within an annular domain of a\nclosed cone. The localization we obtain immediately allows for multiplicity\nwhen applied to disjoint conical sets. To illustrate our results, we consider a\ntwo-point boundary value problem and obtain a solution within a shell of a\nclosed cone, defined in terms of a Harnack inequality with respect to the\nenergy norm. The conditions imposed on the nonlinear term naturally extend\nthose from classical examples in the literature which were derived using the\nmethod of Nehari manifold on the entire domain.",
        "Despite significant advancements in video large multimodal models\n(video-LMMs), achieving effective temporal grounding in long-form videos\nremains a challenge for existing models. To address this limitation, we propose\nTemporal Preference Optimization (TPO), a novel post-training framework\ndesigned to enhance the temporal grounding capabilities of video-LMMs through\npreference learning. TPO adopts a self-training approach that enables models to\ndifferentiate between well-grounded and less accurate temporal responses by\nleveraging curated preference datasets at two granularities: localized temporal\ngrounding, which focuses on specific video segments, and comprehensive temporal\ngrounding, which captures extended temporal dependencies across entire video\nsequences. By optimizing on these preference datasets, TPO significantly\nenhances temporal understanding while reducing reliance on manually annotated\ndata. Extensive experiments on three long-form video understanding\nbenchmarks--LongVideoBench, MLVU, and Video-MME--demonstrate the effectiveness\nof TPO across two state-of-the-art video-LMMs. Notably, LLaVA-Video-TPO\nestablishes itself as the leading 7B model on the Video-MME benchmark,\nunderscoring the potential of TPO as a scalable and efficient solution for\nadvancing temporal reasoning in long-form video understanding. Project page:\nhttps:\/\/ruili33.github.io\/tpo_website.",
        "Advanced Persistent Threats (APTs) are among the most sophisticated threats\nfacing critical organizations worldwide. APTs employ specific tactics,\ntechniques, and procedures (TTPs) which make them difficult to detect in\ncomparison to frequent and aggressive attacks. In fact, current network\nintrusion detection systems struggle to detect APTs communications, allowing\nsuch threats to persist unnoticed on victims' machines for months or even\nyears. In this paper, we present EarlyCrow, an approach to detect APT malware\ncommand and control over HTTP(S) using contextual summaries.\n  The design of EarlyCrow is informed by a novel threat model focused on TTPs\npresent in traffic generated by tools recently used as part of APT campaigns.\nThe threat model highlights the importance of the context around the malicious\nconnections, and suggests traffic attributes which help APT detection.\nEarlyCrow defines a novel multipurpose network flow format called PairFlow,\nwhich is leveraged to build the contextual summary of a PCAP capture,\nrepresenting key behavioral, statistical and protocol information relevant to\nAPT TTPs. We evaluate the effectiveness of EarlyCrow on unseen APTs obtaining a\nheadline macro average F1-score of 93.02% with FPR of $0.74%.",
        "The Platform for Content-Structure Inference (PCSI, pronounced \"pixie\")\nfacilitates the sharing of information about the process of converting Web\nresources into structured content objects that conform to a predefined format.\nPCSI records encode methods for deriving structured content from classes of\nURLs, and report the results of applying particular methods to particular URLs.\nThe methods are scripts written in Hex, a variant of Awk with facilities for\ntraversing the HTML DOM.",
        "Using deep neural networks (DNNs) for encoding of microphone array (MA)\nsignals to the Ambisonics spatial audio format can surpass certain limitations\nof established conventional methods, but existing DNN-based methods need to be\ntrained separately for each MA. This paper proposes a DNN-based method for\nAmbisonics encoding that can generalize to arbitrary MA geometries unseen\nduring training. The method takes as inputs the MA geometry and MA signals and\nuses a multi-level encoder consisting of separate paths for geometry and signal\ndata, where geometry features inform the signal encoder at each level. The\nmethod is validated in simulated anechoic and reverberant conditions with one\nand two sources. The results indicate improvement over conventional encoding\nacross the whole frequency range for dry scenes, while for reverberant scenes\nthe improvement is frequency-dependent.",
        "Photonic integrated circuits provide a compact platform for ultrafast and\nenergy-efficient matrix-vector multiplications (MVMs) in the optical domain.\nRecently, schemes based on time-division multiplexing (TDM) have been proposed\nas scalable approaches for realizing large-scale photonic MVM processors.\nHowever, existing demonstrations rely on coherent detection or multiple\nwavelengths, both of which complicate their operations. In this work, we\ndemonstrate a scalable TDM-based photonic MVM processor that uses only\nsingle-wavelength intensity-modulated optical signals, thereby avoiding\ncoherent detection and enabling simplified operations. A 32-channel processor\nis fabricated on a Si-on-insulator (SOI) platform and used to experimentally\nperform convolution operations in a convolutional neural network (CNN) for\nhandwritten digit recognition, achieving a classification accuracy of 93.47%\nfor 1500 images.",
        "Software developers balance a variety of different tasks in a workweek, yet\nthe allocation of time often differs from what they consider ideal. Identifying\nand addressing these deviations is crucial for organizations aiming to enhance\nthe productivity and well-being of the developers. In this paper, we present\nthe findings from a survey of 484 software developers at Microsoft, which aims\nto identify the key differences between how developers would like to allocate\ntheir time during an ideal workweek versus their actual workweek. Our analysis\nreveals significant deviations between a developer's ideal workweek and their\nactual workweek, with a clear correlation: as the gap between these two\nworkweeks widens, we observe a decline in both productivity and satisfaction.\nBy examining these deviations in specific activities, we assess their direct\nimpact on the developers' satisfaction and productivity. Additionally, given\nthe growing adoption of AI tools in software engineering, both in the industry\nand academia, we identify specific tasks and areas that could be strong\ncandidates for automation. In this paper, we make three key contributions: 1)\nWe quantify the impact of workweek deviations on developer productivity and\nsatisfaction 2) We identify individual tasks that disproportionately affect\nsatisfaction and productivity 3) We provide actual data-driven insights to\nguide future AI automation efforts in software engineering, aligning them with\nthe developers' requirements and ideal workflows for maximizing their\nproductivity and satisfaction.",
        "A connected graph has a $(k,\\ell)$-cover if each of its edges is contained in\nat least $\\ell$ cliques of order $k$. Motivated by recent advances in extremal\ncombinatorics and the literature on edge modification problems, we study the\nalgorithmic version of the $(k,\\ell)$-cover problem. Given a connected graph\n$G$, the $(k, \\ell)$-cover problem is to identify the smallest subset of\nnon-edges of $G$ such that their addition to $G$ results in a graph with a $(k,\n\\ell)$-cover. For every constant $k\\geq3$, we show that the $(k,1)$-cover\nproblem is $\\mathbb{NP}$-complete for general graphs. Moreover, we show that\nfor every constant $k\\geq 3$, the $(k,1)$-cover problem admits no\npolynomial-time constant-factor approximation algorithm unless\n$\\mathbb{P}=\\mathbb{NP}$. However, we show that the $(3,1)$-cover problem can\nbe solved in polynomial time when the input graph is chordal. For the class of\ntrees and general values of $k$, we show that the $(k,1)$-cover problem is\n$\\mathbb{NP}$-hard even for spiders. However, we show that for every $k\\geq4$,\nthe $(3,k-2)$-cover and the $(k,1)$-cover problems are constant-factor\napproximable when the input graph is a tree.",
        "The COVID-19 pandemic has profoundly impacted billions globally. It\nchallenges public health and healthcare systems due to its rapid spread and\nsevere respiratory effects. An effective strategy to mitigate the COVID-19\npandemic involves integrating testing to identify infected individuals. While\nRT-PCR is considered the gold standard for diagnosing COVID-19, it has some\nlimitations such as the risk of false negatives. To address this problem, this\npaper introduces a novel Deep Learning Diagnosis System that integrates\npre-trained Deep Convolutional Neural Networks (DCNNs) within an ensemble\nlearning framework to achieve precise identification of COVID-19 cases from\nChest X-ray (CXR) images. We combine feature vectors from the final hidden\nlayers of pre-trained DCNNs using the Choquet integral to capture interactions\nbetween different DCNNs that a linear approach cannot. We employed\nSugeno-$\\lambda$ measure theory to derive fuzzy measures for subsets of\nnetworks to enable aggregation. We utilized Differential Evolution to estimate\nfuzzy densities. We developed a TensorFlow-based layer for Choquet operation to\nfacilitate efficient aggregation, due to the intricacies involved in\naggregating feature vectors. Experimental results on the COVIDx dataset show\nthat our ensemble model achieved 98\\% accuracy in three-class classification\nand 99.50\\% in binary classification, outperforming its components-DenseNet-201\n(97\\% for three-class, 98.75\\% for binary), Inception-v3 (96.25\\% for\nthree-class, 98.50\\% for binary), and Xception (94.50\\% for three-class, 98\\%\nfor binary)-and surpassing many previous methods.",
        "Generative models that can produce realistic images have improved\nsignificantly in recent years. The quality of the generated content has\nincreased drastically, so sometimes it is very difficult to distinguish between\nthe real images and the generated ones. Such an improvement comes at a price of\nethical concerns about the usage of the generative models: the users of\ngenerative models can improperly claim ownership of the generated content\nprotected by a license. In this paper, we propose an approach to embed\nwatermarks into the generated content to allow future detection of the\ngenerated content and identification of the user who generated it. The\nwatermark is embedded during the inference of the model, so the proposed\napproach does not require the retraining of the latter. We prove that\nwatermarks embedded are guaranteed to be robust against additive perturbations\nof a bounded magnitude. We apply our method to watermark diffusion models and\nshow that it matches state-of-the-art watermarking schemes in terms of\nrobustness to different types of synthetic watermark removal attacks.",
        "An underlying assumption of many existing approaches to human-robot task\ncommunication is that the robot possesses a sufficient amount of environmental\ndomain knowledge, including the locations of task-critical objects. This\nassumption is unrealistic if the locations of known objects change or have not\nyet been discovered by the robot. In this work, our key insight is that in many\nscenarios, robot end users possess more scene insight than the robot and need\nways to express it. Presently, there is a lack of research on how solutions for\ncollecting end-user scene insight should be designed. We thereby created an\nUncertainty Expression System (UES) to investigate how best to elicit end-user\nscene insight. The UES allows end users to convey their knowledge of object\nuncertainty using either: (1) a precision interface that allows meticulous\nexpression of scene insight; (2) a painting interface by which users create a\nheat map of possible object locations; and (3) a ranking interface by which end\nusers express object locations via an ordered list. We then conducted a user\nstudy to compare the effectiveness of these approaches based on the accuracy of\nscene insight conveyed to the robot, the efficiency at which end users are able\nto express this scene insight, and both usability and task load. Results\nindicate that the rank interface is more user friendly and efficient than the\nprecision interface, and that the paint interface is the least accurate."
      ]
    }
  },
  {
    "id":2411.14975,
    "research_type":"applied",
    "start_id":"b15",
    "start_title":"HiCervix: An Extensive Hierarchical Dataset and Benchmark for Cervical Cytology Classification",
    "start_abstract":"Cervical cytology is a critical screening strategy for early detection of pre-cancerous and cancerous cervical lesions. The challenge lies in accurately classifying various cell types. Existing automated methods are primarily trained on databases covering narrow range coarse-grained types, which fail to provide comprehensive detailed performance analysis that represents real-world cytopathology conditions. To overcome these limitations, we introduce HiCervix, the most extensive, multi-center dataset currently available public. HiCervix includes 40,229 cells from 4,496 whole slide images, categorized into 29 annotated classes. These classes organized within three-level hierarchical tree capture fine-grained subtype information. exploit semantic correlation inherent this tree, propose HierSwin, vision transformer-based classification network. HierSwin serves as benchmark feature learning both coarse-level fine-level cancer tasks. In our experiments, demonstrated remarkable performance, achieving 92.08% accuracy 82.93% averaged across all three levels. When compared board-certified cytopathologists, achieved high (0.8293 versus 0.7359 accuracy), highlighting its potential clinical applications. This newly released dataset, along with method, poised make substantial impact advancement deep algorithms rapid greatly improve prevention patient outcomes settings.",
    "start_categories":[
      "q-bio.CB"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b17"
      ],
      "title":[
        "LoRA: Low-Rank Adaptation of Large Language Models"
      ],
      "abstract":[
        "An important paradigm of natural language processing consists large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances fine-tuned each with is prohibitively expensive. We propose Low-Rank Adaptation, LoRA, freezes the pre-trained weights injects trainable rank decomposition matrices into layer Transformer architecture, greatly reducing number parameters for downstream tasks. Compared Adam, LoRA can reduce by 10,000 times GPU memory requirement 3 times. performs on-par better than fine-tuning in quality RoBERTa, DeBERTa, GPT-2, GPT-3, despite having fewer a higher training throughput, and, unlike adapters, no additional inference latency. also provide empirical investigation rank-deficiency adaptation, sheds light efficacy LoRA. release package that facilitates integration PyTorch models our implementations checkpoints GPT-2 at https:\/\/github.com\/microsoft\/LoRA."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Small Models Struggle to Learn from Strong Reasoners",
        "Modeling Arbitrarily Applicable Relational Responding with the\n  Non-Axiomatic Reasoning System: A Machine Psychology Approach",
        "ProMRVL-CAD: Proactive Dialogue System with Multi-Round Vision-Language\n  Interactions for Computer-Aided Diagnosis",
        "Empirical Evaluation of the Implicit Hitting Set Approach for Weighted\n  CSPs",
        "Equilibrate RLHF: Towards Balancing Helpfulness-Safety Trade-off in\n  Large Language Models",
        "Offline Critic-Guided Diffusion Policy for Multi-User Delay-Constrained\n  Scheduling",
        "Predicting Team Performance from Communications in Simulated\n  Search-and-Rescue",
        "On Sequential Fault-Intolerant Process Planning",
        "Unveiling the Potential of Text in High-Dimensional Time Series\n  Forecasting",
        "Prognostics and Health Management of Wafer Chemical-Mechanical Polishing\n  System using Autoencoder",
        "MathMistake Checker: A Comprehensive Demonstration for Step-by-Step Math\n  Problem Mistake Finding by Prompt-Guided LLMs",
        "Opus: A Workflow Intention Framework for Complex Workflow Generation",
        "PEA: Enhancing LLM Performance on Computational-Reasoning Tasks",
        "Probing electric field tunable multiband superconductivity in\n  alternating twisted quadralayer graphene",
        "ExtremeAIGC: Benchmarking LMM Vulnerability to AI-Generated Extremist\n  Content",
        "Online Meta-learning for AutoML in Real-time (OnMAR)",
        "Correlated vibration-solvent and Duschinsky effects on optical\n  spectroscopy",
        "Rise of the Community Champions: From Reviewer Crunch to Community Power",
        "On the diagonals of rational functions: the minimal number of variables\n  (unabridged version)",
        "Magnetic moments in the Poynting theorem, Maxwell equations, Dirac\n  equation, and QED",
        "Analysis and Extension of Noisy-target Training for Unsupervised Target\n  Signal Enhancement",
        "Modified Dai-Liao Spectral Conjugate Gradient Method with Application to\n  Signal Processing",
        "AI-Driven Solutions for Falcon Disease Classification: Concatenated\n  ConvNeXt cum EfficientNet AI Model Approach",
        "Memory-dependent abstractions of stochastic systems through the lens of\n  transfer operators",
        "FairDeFace: Evaluating the Fairness and Adversarial Robustness of Face\n  Obfuscation Methods",
        "Invariance properties of the solution operator for measure-valued\n  semilinear transport equations",
        "An Interactive Framework for Implementing Privacy-Preserving Federated\n  Learning: Experiments on Large Language Models",
        "Extraction of HI gas with bulk motions in the disk of galaxies"
      ],
      "abstract":[
        "Large language models (LLMs) excel in complex reasoning tasks, and distilling\ntheir reasoning capabilities into smaller models has shown promise. However, we\nuncover an interesting phenomenon, which we term the Small Model Learnability\nGap: small models ($\\leq$3B parameters) do not consistently benefit from long\nchain-of-thought (CoT) reasoning or distillation from larger models. Instead,\nthey perform better when fine-tuned on shorter, simpler reasoning chains that\nbetter align with their intrinsic learning capacity. To address this, we\npropose Mix Distillation, a simple yet effective strategy that balances\nreasoning complexity by combining long and short CoT examples or reasoning from\nboth larger and smaller models. Our experiments demonstrate that Mix\nDistillation significantly improves small model reasoning performance compared\nto training on either data alone. These findings highlight the limitations of\ndirect strong model distillation and underscore the importance of adapting\nreasoning complexity for effective reasoning capability transfer.",
        "Arbitrarily Applicable Relational Responding (AARR) is a cornerstone of human\nlanguage and reasoning, referring to the learned ability to relate symbols in\nflexible, context-dependent ways. In this paper, we present a novel theoretical\napproach for modeling AARR within an artificial intelligence framework using\nthe Non-Axiomatic Reasoning System (NARS). NARS is an adaptive reasoning system\ndesigned for learning under uncertainty. By integrating principles from\nRelational Frame Theory - the behavioral psychology account of AARR - with the\nreasoning mechanisms of NARS, we conceptually demonstrate how key properties of\nAARR (mutual entailment, combinatorial entailment, and transformation of\nstimulus functions) can emerge from the inference rules and memory structures\nof NARS. Two theoretical experiments illustrate this approach: one modeling\nstimulus equivalence and transfer of function, and another modeling complex\nrelational networks involving opposition frames. In both cases, the system\nlogically demonstrates the derivation of untrained relations and\ncontext-sensitive transformations of stimulus significance, mirroring\nestablished human cognitive phenomena. These results suggest that AARR - long\nconsidered uniquely human - can be conceptually captured by suitably designed\nAI systems, highlighting the value of integrating behavioral science insights\ninto artificial general intelligence (AGI) research.",
        "Recent advancements in large language models (LLMs) have demonstrated\nextraordinary comprehension capabilities with remarkable breakthroughs on\nvarious vision-language tasks. However, the application of LLMs in generating\nreliable medical diagnostic reports remains in the early stages. Currently,\nmedical LLMs typically feature a passive interaction model where doctors\nrespond to patient queries with little or no involvement in analyzing medical\nimages. In contrast, some ChatBots simply respond to predefined queries based\non visual inputs, lacking interactive dialogue or consideration of medical\nhistory. As such, there is a gap between LLM-generated patient-ChatBot\ninteractions and those occurring in actual patient-doctor consultations. To\nbridge this gap, we develop an LLM-based dialogue system, namely proactive\nmulti-round vision-language interactions for computer-aided diagnosis\n(ProMRVL-CAD), to generate patient-friendly disease diagnostic reports. The\nproposed ProMRVL-CAD system allows proactive dialogue to provide patients with\nconstant and reliable medical access via an integration of knowledge graph into\na recommendation system. Specifically, we devise two generators: a Proactive\nQuestion Generator (Pro-Q Gen) to generate proactive questions that guide the\ndiagnostic procedure and a Multi-Vision Patient-Text Diagnostic Report\nGenerator (MVP-DR Gen) to produce high-quality diagnostic reports. Evaluating\ntwo real-world publicly available datasets, MIMIC-CXR and IU-Xray, our model\nhas better quality in generating medical reports. We further demonstrate the\nperformance of ProMRVL achieves robust under the scenarios with low image\nquality. Moreover, we have created a synthetic medical dialogue dataset that\nsimulates proactive diagnostic interactions between patients and doctors,\nserving as a valuable resource for training LLM.",
        "SAT technology has proven to be surprisingly effective in a large variety of\ndomains. However, for the Weighted CSP problem dedicated algorithms have always\nbeen superior. One approach not well-studied so far is the use of SAT in\nconjunction with the Implicit Hitting Set approach. In this work, we explore\nsome alternatives to the existing algorithm of reference. The alternatives,\nmostly borrowed from related boolean frameworks, consider trade-offs for the\ntwo main components of the IHS approach: the computation of low-cost hitting\nvectors, and their transformation into high-cost cores. For each one, we\npropose 4 levels of intensity. Since we also test the usefulness of cost\nfunction merging, our experiments consider 32 different implementations. Our\nempirical study shows that for WCSP it is not easy to identify the best\nalternative. Nevertheless, the cost-function merging encoding and extracting\nmaximal cores seems to be a robust approach.",
        "Fine-tuning large language models (LLMs) based on human preferences, commonly\nachieved through reinforcement learning from human feedback (RLHF), has been\neffective in improving their performance. However, maintaining LLM safety\nthroughout the fine-tuning process remains a significant challenge, as\nresolving conflicts between safety and helpfulness can be non-trivial.\nTypically, the safety alignment of LLM is trained on data with safety-related\ncategories. However, our experiments find that naively increasing the scale of\nsafety training data usually leads the LLMs to an ``overly safe'' state rather\nthan a ``truly safe'' state, boosting the refusal rate through extensive\nsafety-aligned data without genuinely understanding the requirements for safe\nresponses. Such an approach can inadvertently diminish the models' helpfulness.\nTo understand the phenomenon, we first investigate the role of safety data by\ncategorizing them into three different groups, and observe that each group\nbehaves differently as training data scales up. To boost the balance between\nsafety and helpfulness, we propose an Equilibrate RLHF framework including a\nFine-grained Data-centric (FDC) approach that achieves better safety alignment\neven with fewer training data, and an Adaptive Message-wise Alignment (AMA)\napproach, which selectively highlight the key segments through a gradient\nmasking strategy. Extensive experimental results demonstrate that our approach\nsignificantly enhances the safety alignment of LLMs while balancing safety and\nhelpfulness.",
        "Effective multi-user delay-constrained scheduling is crucial in various\nreal-world applications, such as instant messaging, live streaming, and data\ncenter management. In these scenarios, schedulers must make real-time decisions\nto satisfy both delay and resource constraints without prior knowledge of\nsystem dynamics, which are often time-varying and challenging to estimate.\nCurrent learning-based methods typically require interactions with actual\nsystems during the training stage, which can be difficult or impractical, as it\nis capable of significantly degrading system performance and incurring\nsubstantial service costs. To address these challenges, we propose a novel\noffline reinforcement learning-based algorithm, named \\underline{S}cheduling By\n\\underline{O}ffline Learning with \\underline{C}ritic Guidance and\n\\underline{D}iffusion Generation (SOCD), to learn efficient scheduling policies\npurely from pre-collected \\emph{offline data}. SOCD innovatively employs a\ndiffusion-based policy network, complemented by a sampling-free critic network\nfor policy guidance. By integrating the Lagrangian multiplier optimization into\nthe offline reinforcement learning, SOCD effectively trains high-quality\nconstraint-aware policies exclusively from available datasets, eliminating the\nneed for online interactions with the system. Experimental results demonstrate\nthat SOCD is resilient to various system dynamics, including partially\nobservable and large-scale environments, and delivers superior performance\ncompared to existing methods.",
        "Understanding how individual traits influence team performance is valuable,\nbut these traits are not always directly observable. Prior research has\ninferred traits like trust from behavioral data. We analyze conversational data\nto identify team traits and their correlation with teaming outcomes. Using\ntranscripts from a Minecraft-based search-and-rescue experiment, we apply topic\nmodeling and clustering to uncover key interaction patterns. Our findings show\nthat variations in teaming outcomes can be explained through these inferences,\nwith different levels of predictive power derived from individual traits and\nteam dynamics.",
        "We propose and study a planning problem we call Sequential Fault-Intolerant\nProcess Planning (SFIPP). SFIPP captures a reward structure common in many\nsequential multi-stage decision problems where the planning is deemed\nsuccessful only if all stages succeed. Such reward structures are different\nfrom classic additive reward structures and arise in important applications\nsuch as drug\/material discovery, security, and quality-critical product design.\nWe design provably tight online algorithms for settings in which we need to\npick between different actions with unknown success chances at each stage. We\ndo so both for the foundational case in which the behavior of actions is\ndeterministic, and the case of probabilistic action outcomes, where we\neffectively balance exploration for learning and exploitation for planning\nthrough the usage of multi-armed bandit algorithms. In our empirical\nevaluations, we demonstrate that the specialized algorithms we develop, which\nleverage additional information about the structure of the SFIPP instance,\noutperform our more general algorithm.",
        "Time series forecasting has traditionally focused on univariate and\nmultivariate numerical data, often overlooking the benefits of incorporating\nmultimodal information, particularly textual data. In this paper, we propose a\nnovel framework that integrates time series models with Large Language Models\nto improve high-dimensional time series forecasting. Inspired by multimodal\nmodels, our method combines time series and textual data in the dual-tower\nstructure. This fusion of information creates a comprehensive representation,\nwhich is then processed through a linear layer to generate the final forecast.\nExtensive experiments demonstrate that incorporating text enhances\nhigh-dimensional time series forecasting performance. This work paves the way\nfor further research in multimodal time series forecasting.",
        "The Prognostics and Health Management Data Challenge (PHM) 2016 tracks the\nhealth state of components of a semiconductor wafer polishing process. The\nultimate goal is to develop an ability to predict the measurement on the wafer\nsurface wear through monitoring the components health state. This translates to\ncost saving in large scale production. The PHM dataset contains many time\nseries measurements not utilized by traditional physics based approach. On the\nother hand task, applying a data driven approach such as deep learning to the\nPHM dataset is non-trivial. The main issue with supervised deep learning is\nthat class label is not available to the PHM dataset. Second, the feature space\ntrained by an unsupervised deep learner is not specifically targeted at the\npredictive ability or regression. In this work, we propose using the\nautoencoder based clustering whereby the feature space trained is found to be\nmore suitable for performing regression. This is due to having a more compact\ndistribution of samples respective to their nearest cluster means. We justify\nour claims by comparing the performance of our proposed method on the PHM\ndataset with several baselines such as the autoencoder as well as\nstate-of-the-art approaches.",
        "We propose a novel system, MathMistake Checker, designed to automate\nstep-by-step mistake finding in mathematical problems with lengthy answers\nthrough a two-stage process. The system aims to simplify grading, increase\nefficiency, and enhance learning experiences from a pedagogical perspective. It\nintegrates advanced technologies, including computer vision and the\nchain-of-thought capabilities of the latest large language models (LLMs). Our\nsystem supports open-ended grading without reference answers and promotes\npersonalized learning by providing targeted feedback. We demonstrate its\neffectiveness across various types of math problems, such as calculation and\nword problems.",
        "This paper introduces Workflow Intention, a novel framework for identifying\nand encoding process objectives within complex business environments. Workflow\nIntention is the alignment of Input, Process and Output elements defining a\nWorkflow's transformation objective interpreted from Workflow Signal inside\nBusiness Artefacts. It specifies how Input is processed to achieve desired\nOutput, incorporating quality standards, business rules, compliance\nrequirements and constraints. We adopt an end-to-end Business Artefact Encoder\nand Workflow Signal interpretation methodology involving four steps:\nModality-Specific Encoding, Intra-Modality Attention, Inter-Modality Fusion\nAttention then Intention Decoding. We provide training procedures and critical\nloss function definitions. In this paper we introduce the concepts of Workflow\nSignal and Workflow Intention, where Workflow Signal decomposed into Input,\nProcess and Output elements is interpreted from Business Artefacts, and\nWorkflow Intention is a complete triple of these elements. We introduce a\nmathematical framework for representing Workflow Signal as a vector and\nWorkflow Intention as a tensor, formalizing properties of these objects.\nFinally, we propose a modular, scalable, trainable, attention-based multimodal\ngenerative system to resolve Workflow Intention from Business Artefacts.",
        "Large Language Models (LLMs) have exhibited remarkable capabilities across\ndiverse domains, prompting investigations into their potential as generic\nreasoning engines. While recent studies have explored inference-time\ncomputation to enhance model performance on complex problems, current research\nlacks a formal framework to characterize the complexity of reasoning tasks.\nThis study introduces the Predicate-Enumeration-Aggregation (PEA) framework, a\nformal approach to describe and solve a class of important reasoning tasks\ntermed computational reasoning problems. The PEA framework decomposes these\nproblems into predicate and enumeration components, using LLMs to synthesize\nprograms based on specified predicates, enumeration, and aggregation rules.\nThese synthesized programs are then executed to obtain solutions to the\ncomputational tasks. We demonstrate the framework's efficacy on benchmark tasks\nincluding Boolean satisfiability problems, game of $24$, and planning problems.\nEmpirical evaluation reveals that PEA substantially enhances the performance of\nunderlying models on benchmark computational problems, yielding an average\naccuracy improvement of approximately $50\\%$, coupled with increased\nefficiency.",
        "Alternating twisted multilayer graphene presents a compelling multiband\nsystem for exploring superconductivity. Here we investigate robust\nsuperconductivity in alternating twisted quadralayer graphene, elucidating\ncarrier contributions from both flat and dispersive bands. The\nsuperconductivity is robust, with a strong electrical field tunability, a\nmaximum BKT transition temperature of 1.6 K, and high critical magnetic fields\nbeyond the Pauli limit. We disentangle the carrier density of Dirac bands and\nflat bands from the Landau fan diagram. Moreover, we could estimate the\nflatband Fermi velocity from the obtained high critical current near half\nfilling when superconductivity is killed at finite magnetic fields, and further\nquantify the superfluid stiffness from the low critical current in the\nsuperconducting regime. Our results exhibit the electric field tunable coupling\nstrength within the superconducting phase, revealing unconventional properties\nwith vanishing Fermi velocity and large superfluid stiffness. These phenomena,\nattributed to substantial quantum metric contributions, offer new insights into\nthe mechanisms underlying unconventional superconductivity in moire systems.",
        "Large Multimodal Models (LMMs) are increasingly vulnerable to AI-generated\nextremist content, including photorealistic images and text, which can be used\nto bypass safety mechanisms and generate harmful outputs. However, existing\ndatasets for evaluating LMM robustness offer limited exploration of extremist\ncontent, often lacking AI-generated images, diverse image generation models,\nand comprehensive coverage of historical events, which hinders a complete\nassessment of model vulnerabilities. To fill this gap, we introduce\nExtremeAIGC, a benchmark dataset and evaluation framework designed to assess\nLMM vulnerabilities against such content. ExtremeAIGC simulates real-world\nevents and malicious use cases by curating diverse text- and image-based\nexamples crafted using state-of-the-art image generation techniques. Our study\nreveals alarming weaknesses in LMMs, demonstrating that even cutting-edge\nsafety measures fail to prevent the generation of extremist material. We\nsystematically quantify the success rates of various attack strategies,\nexposing critical gaps in current defenses and emphasizing the need for more\nrobust mitigation strategies.",
        "Automated machine learning (AutoML) is a research area focusing on using\noptimisation techniques to design machine learning (ML) algorithms, alleviating\nthe need for a human to perform manual algorithm design. Real-time AutoML\nenables the design process to happen while the ML algorithm is being applied to\na task. Real-time AutoML is an emerging research area, as such existing\nreal-time AutoML techniques need improvement with respect to the quality of\ndesigns and time taken to create designs. To address these issues, this study\nproposes an Online Meta-learning for AutoML in Real-time (OnMAR) approach.\nMeta-learning gathers information about the optimisation process undertaken by\nthe ML algorithm in the form of meta-features. Meta-features are used in\nconjunction with a meta-learner to optimise the optimisation process. The OnMAR\napproach uses a meta-learner to predict the accuracy of an ML design. If the\naccuracy predicted by the meta-learner is sufficient, the design is used, and\nif the predicted accuracy is low, an optimisation technique creates a new\ndesign. A genetic algorithm (GA) is the optimisation technique used as part of\nthe OnMAR approach. Different meta-learners (k-nearest neighbours, random\nforest and XGBoost) are tested. The OnMAR approach is model-agnostic (i.e. not\nspecific to a single real-time AutoML application) and therefore evaluated on\nthree different real-time AutoML applications, namely: composing an image\nclustering algorithm, configuring the hyper-parameters of a convolutional\nneural network, and configuring a video classification pipeline. The OnMAR\napproach is effective, matching or outperforming existing real-time AutoML\napproaches, with the added benefit of a faster runtime.",
        "Understanding the role of vibrations in optical spectroscopies is essential\nfor the precise interpretation of spectroscopic behavior, especially in systems\nwith complex solvation effects. This workstudies the correlated Duschinsky and\nsolvent effects on the optical spectra using the extended\ndissipaton-equation-of-motion (ext-DEOM) approach, which is an exact and\nnon-Markovian, nonperturbative approach for nonlinear environmental couplings.\nIn the paper, the environment (bath) is composed of the solvent and\nintramolecular vibrational modes whose Duschinsky rotations constitute the\nquardratic couplings to the electronic states. To apply the ext-DEOM, one key\nstep is to obtain the bath coupling descriptors, which is elaborated. As an\naccurate description of solvated molecular systems, the simulating results\ndemonstrate how the above factors affect the position and shape of spectral\nbands.",
        "Academic publishing is facing a crisis driven by exponential growth in\nsubmissions and an overwhelmed peer review system, leading to inconsistent\ndecisions and a severe reviewer shortage. This paper introduces Panvas, a\nplatform that reimagines academic publishing as a continuous, community-driven\nprocess. Panvas addresses these systemic failures with a novel combination of\neconomic incentives (paid reviews) and rich interaction mechanisms\n(multi-dimensional ratings, threaded discussions, and expert-led reviews). By\nmoving beyond the traditional accept\/reject paradigm and integrating paper\nhosting with code\/data repositories and social networking, Panvas fosters a\nmeritocratic environment for scholarly communication and presents a radical\nrethinking of how we evaluate and disseminate scientific knowledge. We present\nthe system design, development roadmap, and a user study plan to evaluate its\neffectiveness.",
        "From some observations on the linear differential operators occurring in the\nLattice Green function of the d-dimensional face centred and simple cubic\nlattices, and on the linear differential operators occurring in the n-particle\ncontributions\n  to the magnetic susceptibility of the square Ising model, we forward some\nconjectures on the diagonals of rational functions. These conjectures are also\nin agreement with exact results we obtain for many Calabi-Yau operators, and\nmany other examples related, or not related to physics.\n  Consider a globally bounded power series which is the diagonal of rational\nfunctions of a certain number of variables, annihilated by an irreducible\nminimal order linear differential operator homomorphic to its adjoint. Among\nthe logarithmic formal series solutions, at the origin, of this operator, call\nn the highest power of the logarithm. We conjecture that this diagonal series\ncan be represented as a diagonal of a rational function with a minimal number\nof variables N_v related to this highest power n by the relation N_v = n +2.\n  Since the operator is homomorphic to its adjoint, its differential Galois\ngroup is symplectic or orthogonal. We also conjecture that the symplectic or\northogonal character of the differential Galois group is related to the parity\nof the highest power n, namely symplectic for n odd and orthogonal for n even.\n  We also sketch the case where the denominator of the rational function is not\nirreducible and is the product of, for instance, two polynomials. The analysis\nof the linear differential operators annihilating the diagonal of rational\nfunction where the denominator is the product of two polynomials, sheds some\nlight on the emergence of such mixture of direct sums and products of factors.\n  The conjecture N_v = n +2 still holds for such reducible linear differential\noperators.",
        "The role of magnetic moments in electrodynamics is examined in this work. The\neffects are described in the context of conventional quantum electrodynamics\nexpressed in terms of the electromagnetic fields or in the context of an\nextended Poynting theorem and extended Maxwell equations. These extensions take\ninto account the energetics of interaction of magnetic moments with\ninhomogeneous magnetic fields. We show how magnetic moment effects are included\nin either version of electrodynamics and that these apparently different\nformulations can give consistent results. In either case, we express the\ninteractions in terms of electromagnetic fields only, avoiding use of a vector\npotential.",
        "Deep neural network-based target signal enhancement (TSE) is usually trained\nin a supervised manner using clean target signals. However, collecting clean\ntarget signals is costly and such signals are not always available. Thus, it is\ndesirable to develop an unsupervised method that does not rely on clean target\nsignals. Among various studies on unsupervised TSE methods, Noisy-target\nTraining (NyTT) has been established as a fundamental method. NyTT simply\nreplaces clean target signals with noisy ones in the typical supervised\ntraining, and it has been experimentally shown to achieve TSE. Despite its\neffectiveness and simplicity, its mechanism and detailed behavior are still\nunclear. In this paper, to advance NyTT and, thus, unsupervised methods as a\nwhole, we analyze NyTT from various perspectives. We experimentally demonstrate\nthe mechanism of NyTT, the desirable conditions, and the effectiveness of\nutilizing noisy signals in situations where a small number of clean target\nsignals are available. Furthermore, we propose an improved version of NyTT\nbased on its properties and explore its capabilities in the dereverberation and\ndeclipping tasks, beyond the denoising task.",
        "In this article, we present a modified variant of the Dai-Liao spectral\nconjugate gradient method, developed through an analysis of eigenvalues and\ninspired by a modified secant condition. We show that the proposed method is\nglobally convergent for general nonlinear functions under standard assumptions.\nBy incorporating the new secant condition and a quasi-Newton direction, we\nintroduce updated spectral parameters. These changes ensure that the resulting\nsearch direction satisfies the sufficient descent property without relying on\nany line search. Numerical experiments show that the proposed algorithm\nperforms better than several existing methods in terms of convergence speed and\ncomputational efficiency. Its effectiveness is further demonstrated through an\napplication in signal processing.",
        "Falconry, an ancient practice of training and hunting with falcons,\nemphasizes the need for vigilant health monitoring to ensure the well-being of\nthese highly valued birds, especially during hunting activities. This research\npaper introduces a cutting-edge approach, which leverages the power of\nConcatenated ConvNeXt and EfficientNet AI models for falcon disease\nclassification. Focused on distinguishing 'Normal,' 'Liver,' and\n'Aspergillosis' cases, the study employs a comprehensive dataset for model\ntraining and evaluation, utilizing metrics such as accuracy, precision, recall,\nand f1-score. Through rigorous experimentation and evaluation, we demonstrate\nthe superior performance of the concatenated AI model compared to traditional\nmethods and standalone architectures. This novel approach contributes to\naccurate falcon disease classification, laying the groundwork for further\nadvancements in avian veterinary AI applications.",
        "With the increasing ubiquity of safety-critical autonomous systems operating\nin uncertain environments, there is a need for mathematical methods for formal\nverification of stochastic models. Towards formally verifying properties of\nstochastic systems, methods based on discrete, finite Markov approximations --\nabstractions -- thereof have surged in recent years. These are found in\ncontexts where: either a) one only has partial, discrete observations of the\nunderlying continuous stochastic process, or b) the original system is too\ncomplex to analyze, so one partitions the continuous state-space of the\noriginal system to construct a handleable, finite-state model thereof. In both\ncases, the abstraction is an approximation of the discrete stochastic process\nthat arises precisely from the discretization of the underlying continuous\nprocess. The fact that the abstraction is Markov and the discrete process is\nnot (even though the original one is) leads to approximation errors. Towards\naccounting for non-Markovianity, we introduce memory-dependent abstractions for\nstochastic systems, capturing dynamics with memory effects. Our contribution is\ntwofold. First, we provide a formalism for memory-dependent abstractions based\non transfer operators. Second, we quantify the approximation error by upper\nbounding the total variation distance between the true continuous state\ndistribution and its discrete approximation.",
        "The lack of a common platform and benchmark datasets for evaluating face\nobfuscation methods has been a challenge, with every method being tested using\narbitrary experiments, datasets, and metrics. While prior work has demonstrated\nthat face recognition systems exhibit bias against some demographic groups,\nthere exists a substantial gap in our understanding regarding the fairness of\nface obfuscation methods. Providing fair face obfuscation methods can ensure\nequitable protection across diverse demographic groups, especially since they\ncan be used to preserve the privacy of vulnerable populations. To address these\ngaps, this paper introduces a comprehensive framework, named FairDeFace,\ndesigned to assess the adversarial robustness and fairness of face obfuscation\nmethods. The framework introduces a set of modules encompassing data\nbenchmarks, face detection and recognition algorithms, adversarial models,\nutility detection models, and fairness metrics. FairDeFace serves as a\nversatile platform where any face obfuscation method can be integrated,\nallowing for rigorous testing and comparison with other state-of-the-art\nmethods. In its current implementation, FairDeFace incorporates 6 attacks, and\nseveral privacy, utility and fairness metrics. Using FairDeFace, and by\nconducting more than 500 experiments, we evaluated and compared the adversarial\nrobustness of seven face obfuscation methods. This extensive analysis led to\nmany interesting findings both in terms of the degree of robustness of existing\nmethods and their biases against some gender or racial groups. FairDeFace also\nuses visualization of focused areas for both obfuscation and verification\nattacks to show not only which areas are mostly changed in the obfuscation\nprocess for some demographics, but also why they failed through focus area\ncomparison of obfuscation and verification.",
        "We provide conditions under which we prove for measure-valued transport\nequations with non-linear reaction term in the space of finite signed Radon\nmeasures, that positivity is preserved, as well as absolute continuity with\nrespect to Lebesgue measure, if the initial condition has that property.\nMoreover, if the initial condition has $L^p$ regular density, then the solution\nhas the same property.",
        "Federated learning (FL) enhances privacy by keeping user data on local\ndevices. However, emerging attacks have demonstrated that the updates shared by\nusers during training can reveal significant information about their data. This\nhas greatly thwart the adoption of FL methods for training robust AI models in\nsensitive applications. Differential Privacy (DP) is considered the gold\nstandard for safeguarding user data. However, DP guarantees are highly\nconservative, providing worst-case privacy guarantees. This can result in\noverestimating privacy needs, which may compromise the model's accuracy.\nAdditionally, interpretations of these privacy guarantees have proven to be\nchallenging in different contexts. This is further exacerbated when other\nfactors, such as the number of training iterations, data distribution, and\nspecific application requirements, can add further complexity to this problem.\nIn this work, we proposed a framework that integrates a human entity as a\nprivacy practitioner to determine an optimal trade-off between the model's\nprivacy and utility. Our framework is the first to address the variable memory\nrequirement of existing DP methods in FL settings, where resource-limited\ndevices (e.g., cell phones) can participate. To support such settings, we adopt\na recent DP method with fixed memory usage to ensure scalable private FL. We\nevaluated our proposed framework by fine-tuning a BERT-based LLM model using\nthe GLUE dataset (a common approach in literature), leveraging the new\naccountant, and employing diverse data partitioning strategies to mimic\nreal-world conditions. As a result, we achieved stable memory usage, with an\naverage accuracy reduction of 1.33% for $\\epsilon = 10$ and 1.9% for $\\epsilon\n= 6$, when compared to the state-of-the-art DP accountant which does not\nsupport fixed memory usage.",
        "We propose a new method for extracting bulk motion gases in the disk of a\ngalaxy from HI data cubes, offering improvements over classical techniques like\nmoment analysis and line profile fitting. Our approach decomposes the\nline-of-sight velocity profiles into multiple Gaussian components, which are\nthen classified into (underlying and dominant) bulk and non-bulk motion gases\nbased on criteria such as HI surface density, velocity dispersion, kinetic\nenergy, and rotation velocity. A 2D tilted-ring analysis is employed to refine\nthe kinematical parametres of the galaxy disk, ensuring robust extraction of\nthe bulk motion gases. We demonstrate the effectiveness of this method using\nthe HI data cubes of NGC 4559 from the WSRT-HALOGAS survey, distinguishing\nbetween bulk and non-bulk gas components. From this, we find that approximately\n50% of the HI gas in NGC 4559 is classified as non-bulk, possibly linked to\nprocesses such as stellar feedback. This work provides a robust framework for\nanalysing HI kinematics of galaxies from high sensitivity HI observations of\ngalaxies like MeerKAT-MHONGOOSE and FAST-FEASTS and allows us to best exploit\nthe kinematic information of the complex gas dynamics within galaxy disks."
      ]
    }
  },
  {
    "id":2411.14975,
    "research_type":"applied",
    "start_id":"b17",
    "start_title":"LoRA: Low-Rank Adaptation of Large Language Models",
    "start_abstract":"An important paradigm of natural language processing consists large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances fine-tuned each with is prohibitively expensive. We propose Low-Rank Adaptation, LoRA, freezes the pre-trained weights injects trainable rank decomposition matrices into layer Transformer architecture, greatly reducing number parameters for downstream tasks. Compared Adam, LoRA can reduce by 10,000 times GPU memory requirement 3 times. performs on-par better than fine-tuning in quality RoBERTa, DeBERTa, GPT-2, GPT-3, despite having fewer a higher training throughput, and, unlike adapters, no additional inference latency. also provide empirical investigation rank-deficiency adaptation, sheds light efficacy LoRA. release package that facilitates integration PyTorch models our implementations checkpoints GPT-2 at https:\/\/github.com\/microsoft\/LoRA.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b15"
      ],
      "title":[
        "HiCervix: An Extensive Hierarchical Dataset and Benchmark for Cervical Cytology Classification"
      ],
      "abstract":[
        "Cervical cytology is a critical screening strategy for early detection of pre-cancerous and cancerous cervical lesions. The challenge lies in accurately classifying various cell types. Existing automated methods are primarily trained on databases covering narrow range coarse-grained types, which fail to provide comprehensive detailed performance analysis that represents real-world cytopathology conditions. To overcome these limitations, we introduce HiCervix, the most extensive, multi-center dataset currently available public. HiCervix includes 40,229 cells from 4,496 whole slide images, categorized into 29 annotated classes. These classes organized within three-level hierarchical tree capture fine-grained subtype information. exploit semantic correlation inherent this tree, propose HierSwin, vision transformer-based classification network. HierSwin serves as benchmark feature learning both coarse-level fine-level cancer tasks. In our experiments, demonstrated remarkable performance, achieving 92.08% accuracy 82.93% averaged across all three levels. When compared board-certified cytopathologists, achieved high (0.8293 versus 0.7359 accuracy), highlighting its potential clinical applications. This newly released dataset, along with method, poised make substantial impact advancement deep algorithms rapid greatly improve prevention patient outcomes settings."
      ],
      "categories":[
        "q-bio.CB"
      ]
    },
    "list":{
      "title":[
        "Intercellular contact is sufficient to drive Fibroblast to Myofibroblast\n  transitions",
        "Heuristics based on Adjacency Graph Packing for DCJ Distance Considering\n  Intergenic Regions",
        "Tumor-associated CD19$^+$ macrophages induce immunosuppressive\n  microenvironment in hepatocellular carcinoma",
        "TopoLa: A Universal Framework to Enhance Cell Representations for\n  Single-cell and Spatial Omics through Topology-encoded Latent Hyperbolic\n  Geometry",
        "Remodeling Peptide-MHC-TCR Triad Binding as Sequence Fusion for\n  Immunogenicity Prediction",
        "The effect of environmental factors in biofilm and phage interactions in\n  Agent Based Model",
        "A spatially resolved and lipid-structured model for macrophage\n  populations in early human atherosclerotic lesions",
        "Multicellular self-organization in Escherichia coli",
        "Structural and Practical Identifiability of Phenomenological Growth\n  Models for Epidemic Forecasting",
        "A technical review of multi-omics data integration methods: from\n  classical statistical to deep generative approaches",
        "Trait-structured chemotaxis: Exploring ligand-receptor dynamics and\n  travelling wave properties in a Keller-Segel model",
        "Long-term follow-up of DYT1 dystonia patients treated by deep brain\n  stimulation: an open-label study",
        "Stochastic Time to Extinction of an SIQS Epidemic Model with Quiescence",
        "Nanostructured thin films of indium oxide nanocrystals confined in\n  alumina matrixes",
        "Steady-state coherence in multipartite quantum systems: its connection\n  with thermodynamic quantities and impact on quantum thermal machines",
        "Solving Superconformal Ward Identities in Mellin Space",
        "Footprint in fitting $B\\to D$ vector form factor and determination for\n  $D$-meson leading-twist LCDA",
        "SU(4) gate design via unitary process tomography: its application to\n  cross-resonance based superconducting quantum devices",
        "Constrained mean-field control with singular control: Existence,\n  stochastic maximum principle and constrained FBSDE",
        "Jet rates in Higgs boson decay at third order in QCD",
        "On (in)consistency of M-estimators under contamination",
        "Score Matching Riemannian Diffusion Means",
        "Lower bound on the radii of circular orbits in the extremal Kerr\n  black-hole spacetime",
        "Cartan Quantum Metrology",
        "GPU Accelerated Image Quality Assessment-Based Software for Transient\n  Detection",
        "Multiaccuracy and Multicalibration via Proxy Groups",
        "Federated Variational Inference for Bayesian Mixture Models",
        "On the surjectivity of $\\mathfrak{p}$-adic Galois representations\n  attached to Drinfeld modules of rank $2$"
      ],
      "abstract":[
        "Fibroblast cells play a key role in maintaining the extracellular matrix.\nDuring wound healing, fibroblasts differentiate into highly contractile\nmyofibroblasts, which secrete extracellular matrix proteins like collagen to\nfacilitate tissue repair. Under normal conditions, myofibroblasts undergo\nprogrammed cell death after healing to prevent excessive scar formation.\nHowever, in diseases like fibrosis, the myofibroblasts remain active even after\nthe wound is closed, resulting in excessive collagen buildup and a stiff,\nfibrotic matrix. The reasons for the persistence of myofibroblasts in fibrosis\nare not well understood. Here we show the existence of a mechanism where direct\nphysical contact between a fibroblast and a myofibroblast is sufficient for\nfibroblasts to transition into myofibroblasts. We show that\nfibroblast-myofibroblast transition can occur even in the absence of known\nbiochemical cues such as growth factor activation or mechanical cues from a\nstiff, fibrotic matrix. Further, we show that contact-based\nfibroblast-myofibroblast activation can be blocked by G{\\alpha}q\/11\/14\ninhibitor FR9003591, which inhibits the formation of myofibroblasts. These\nfindings offer new insights into the persistence of fibrosis despite\ntherapeutic interventions and suggest a potential strategy to target\nfibroblast-to-myofibroblast transition in fibrosis.",
        "In this work, we explore heuristics for the Adjacency Graph Packing problem,\nwhich can be applied to the Double Cut and Join (DCJ) Distance Problem. The DCJ\nis a rearrangement operation and the distance problem considering it is a well\nestablished method for genome comparison. Our heuristics will use the structure\ncalled adjacency graph adapted to include information about intergenic regions,\nmultiple copies of genes in the genomes, and multiple circular or linear\nchromosomes. The only required property from the genomes is that it must be\npossible to turn one into the other with DCJ operations. We propose one greedy\nheuristic and one heuristic based on Genetic Algorithms. Our experimental tests\nin artificial genomes show that the use of heuristics is capable of finding\ngood results that are superior to a simpler random strategy.",
        "Tumor-associated macrophages are a key component that contributes to the\nimmunosuppressive microenvironment in human cancers. However, therapeutic\ntargeting of macrophages has been a challenge in clinic due to the limited\nunderstanding of their heterogeneous subpopulations and distinct functions.\nHere, we identify a unique and clinically relevant CD19$^+$ subpopulation of\nmacrophages that is enriched in many types of cancer, particularly in\nhepatocellular carcinoma (HCC). The CD19$^+$ macrophages exhibit increased\nlevels of PD-L1 and CD73, enhanced mitochondrial oxidation, and compromised\nphagocytosis, indicating their immunosuppressive functions. Targeting CD19$^+$\nmacrophages with anti-CD19 chimeric antigen receptor T (CAR-T) cells inhibited\nHCC tumor growth. We identify PAX5 as a primary driver of up-regulated\nmitochondrial biogenesis in CD19$^+$ macrophages, which depletes cytoplasmic\nCa$^{2+}$, leading to lysosomal deficiency and consequent accumulation of CD73\nand PD-L1. Inhibiting CD73 or mitochondrial oxidation enhanced the efficacy of\nimmune checkpoint blockade therapy in treating HCC, suggesting great promise\nfor CD19$^+$ macrophage-targeting therapeutics.",
        "Recent advances in cellular research demonstrate that scRNA-seq characterizes\ncellular heterogeneity, while spatial transcriptomics reveals the spatial\ndistribution of gene expression. Cell representation is the fundamental issue\nin the two fields. Here, we propose Topology-encoded Latent Hyperbolic Geometry\n(TopoLa), a computational framework enhancing cell representations by capturing\nfine-grained intercellular topological relationships. The framework introduces\na new metric, TopoLa distance (TLd), which quantifies the geometric distance\nbetween cells within latent hyperbolic space, capturing the network's\ntopological structure more effectively. With this framework, the cell\nrepresentation can be enhanced considerably by performing convolution on its\nneighboring cells. Performance evaluation across seven biological tasks,\nincluding scRNA-seq data clustering and spatial transcriptomics domain\nidentification, shows that TopoLa significantly improves the performance of\nseveral state-of-the-art models. These results underscore the generalizability\nand robustness of TopoLa, establishing it as a valuable tool for advancing both\nbiological discovery and computational methodologies.",
        "The complex nature of tripartite peptide-MHC-TCR interactions is a critical\nyet underexplored area in immunogenicity prediction. Traditional studies on\nTCR-antigen binding have not fully addressed the complex dependencies in triad\nbinding. In this paper, we propose new modeling approaches for these tripartite\ninteractions, utilizing sequence information from MHCs, peptides, and TCRs. Our\nmethods adhere to native sequence forms and align with biological processes to\nenhance prediction accuracy. By incorporating representation learning\ntechniques, we introduce a fusion mechanism to integrate the three sequences\neffectively. Empirical experiments show that our models outperform traditional\nmethods, achieving a 2.8 to 13.3 percent improvement in prediction accuracy\nacross existing benchmarks. We further validate our approach with extensive\nablation studies, demonstrating the effectiveness of the proposed model\ncomponents. The model implementation, code, and supplementary materials,\nincluding a manuscript with colored hyperlinks and a technical appendix for\ndigital viewing, will be open-sourced upon publication.",
        "As antibiotic resistance continues to pose a significant threat to public\nhealth, alternative treatments are urgently needed. Phage therapy, which\nutilizes bacteriophages to specifically target bacterial pathogens, has emerged\nas a promising solution. Given that bacteria often exist in biofilms -complex\nmicro-communities that complicate treatment strategies, there is a clear need\nfor models that account for spatial dynamics. This study aims to employ\nmathematical and statistical methodologies to identify optimal treatment\nstrategies involving phage-antibiotic combinations. We developed an agent-based\nmodel to analyze how environmental factors (e.g. temperature, pH, and resource\navailability) influence bacteria-phage interactions during therapy, focusing on\nboth healthy and immunocompromised patients. Utilizing \\textit{Escherichia\ncoli} as a case study, we observed that bacterial cells exhibit mutations that\nenhance their adaptability to varying environmental conditions and treatment\napproaches. Our findings suggest that the effectiveness of therapies targeting\npathogenic and mutated bacterial cells can be significantly improved through\nstrategic control of application timing and dosing. Additionally, we\ninvestigated the impact of biofilm structure on the efficacy of phage therapy,\nunderscoring its importance in developing targeted treatment strategies.",
        "Atherosclerosis is a chronic inflammatory disease of the artery wall. The\nearly stages of atherosclerosis are driven by interactions between lipids and\nmonocyte-derived-macrophages (MDMs). The mechanisms that govern the spatial\ndistribution of lipids and MDMs in the lesion remain poorly understood. In this\npaper, we develop a spatially-resolved and lipid-structured model for early\natherosclerosis. The model development and analysis are guided by images of\nhuman coronary lesions by Nakashima et al. 2007. Consistent with their\nfindings, the model predicts that lipid initially accumulates deep in the\nintima due to a spatially non-uniform LDL retention capacity. The model also\nqualitatively reproduces the global internal maxima in the Nakashima images\nonly when the MDM mobility is sufficiently sensitive to lipid content, and MDM\nlifespan sufficiently insensitive. Introducing lipid content-dependence to MDM\nmobility and mean lifespan produced minimal impact on model behaviour at early\ntimes, but strongly impacted lesion composition at steady state. Increases to\nthe sensitivity of MDM lifespan to lipid content yield lesions with fewer MDMs,\nless total lesion lipid content and reduced mean MDM infiltration depth.\nIncreases to the sensitivity of MDM mobility to lipid content also reduces the\nMDM infiltration depth, but increases the proportion of lipid-laden MDMs. We\nfind that MDM lipid content increases with spatial depth, regardless of blood\nLDL and HDL content. These results shed light on the mechanisms that drive\nspatial variation in the composition of early atherosclerotic lesions, and the\nrole of macrophage lipid content in disease progression.",
        "Escherichia coli has long been a trusty companion, maintaining health in our\nguts and advancing biological knowledge in the laboratory. In light of recent\nfindings, we discuss multicellular self-organization in E. coli and develop\ngeneral ideas for multicellularity, including the necessity for multicellular\ndynamics and interpretation by dynamic graphs, applicable to both unicellular\nand multicellular organisms. In this context, we next discuss the documented\nbehaviors of E. coli self-organization (rosette formation, multicellular\nextension, and attached dormancy) and two potential behaviors (internal\ncommunication and mating). Finally, by comparing the dynamic graphs for\ndifferent communities, we develop principles relevant to the theory of\nmulticellularity.",
        "Phenomenological models are highly effective tools for forecasting disease\ndynamics using real world data, particularly in scenarios where detailed\nknowledge of disease mechanisms is limited. However, their reliability depends\non the model parameters' structural and practical identifiability. In this\nstudy, we systematically analyze the identifiability of six commonly used\ngrowth models in epidemiology: the generalized growth model (GGM), the\ngeneralized logistic model (GLM), the Richards model, the generalized Richards\nmodel (GRM), the Gompertz model, and a modified SEIR model with inhomogeneous\nmixing. To address challenges posed by non integer power exponents in these\nmodels, we reformulate them by introducing additional state variables. This\nenables rigorous structural identifiability analysis using the\nStructuralIdentifiability.jl package in JULIA. We validate the structural\nidentifiability results by performing parameter estimation and forecasting\nusing the GrowthPredict MATLAB toolbox. This toolbox is designed to fit and\nforecast time series trajectories based on phenomenological growth models. We\napplied it to three epidemiological datasets: weekly incidence data for\nmonkeypox, COVID 19, and Ebola. Additionally, we assess practical\nidentifiability through Monte Carlo simulations to evaluate parameter\nestimation robustness under varying levels of observational noise. Our results\ndemonstrate the structural and practical identifiability of the models,\nemphasizing how noise affects parameter estimation accuracy. These findings\nprovide valuable insights into the utility and limitations of phenomenological\nmodels for epidemic data analysis, highlighting their adaptability to real\nworld challenges and their role in guiding public health decision making.",
        "The rapid advancement of high-throughput sequencing and other assay\ntechnologies has resulted in the generation of large and complex multi-omics\ndatasets, offering unprecedented opportunities for advancing precision medicine\nstrategies. However, multi-omics data integration presents significant\nchallenges due to the high dimensionality, heterogeneity, experimental gaps,\nand frequency of missing values across data types. Computational methods have\nbeen developed to address these issues, employing statistical and machine\nlearning approaches to uncover complex biological patterns and provide deeper\ninsights into our understanding of disease mechanisms. Here, we comprehensively\nreview state-of-the-art multi-omics data integration methods with a focus on\ndeep generative models, particularly variational autoencoders (VAEs) that have\nbeen widely used for data imputation and augmentation, joint embedding\ncreation, and batch effect correction. We explore the technical aspects of loss\nfunctions and regularisation techniques including adversarial training,\ndisentanglement and contrastive learning. Moreover, we discuss recent\nadvancements in foundation models and the integration of emerging data\nmodalities, while describing the current limitations and outlining future\ndirections for enhancing multi-modal methodologies in biomedical research.",
        "A novel trait-structured Keller-Segel model that explores the dynamics of a\nmigrating cell population guided by chemotaxis in response to an external\nligand concentration is derived and analysed. Unlike traditional Keller-Segel\nmodels, this framework introduces an explicit representation of ligand-receptor\nbindings on the cell membrane, where the percentage of occupied receptors\nconstitutes the trait that influences cellular phenotype. The model posits that\nthe cell's phenotypic state directly modulates its capacity for chemotaxis and\nproliferation, governed by a trade-off due to a finite energy budget: cells\nhighly proficient in chemotaxis exhibit lower proliferation rates, while more\nproliferative cells show diminished chemotactic abilities. The model is derived\nfrom the principles of a biased random walk, resulting in a system of two\nnon-local partial differential equations, describing the densities of both\ncells and ligands. Using a Hopf-Cole transformation, we derive an equation that\ncharacterises the distribution of cellular traits within travelling wave\nsolutions for the total cell density, allowing us to uncover the monotonicity\nproperties of these waves. Numerical investigations are conducted to examine\nthe model's behaviour across various biological scenarios, providing insights\ninto the complex interplay between chemotaxis, proliferation, and phenotypic\ndiversity in migrating cell populations.",
        "Long-term efficacy of internal globus pallidus (GPi) deep-brain stimulation\n(DBS) in DYT1 dystonia and disease progression under DBS was studied.\nTwenty-six patients of this open-label study were divided into two groups: (A)\nwith single bilateral GPi lead, (B) with a second bilateral GPi lead implanted\nowning to subsequent worsening of symptomatology. Dystonia was assessed with\nthe Burke Scale. Appearance of new symptoms and distribution according to body\nregion were recorded. In the whole cohort, significant decreases in motor and\ndisability subscores (P < 0.0001) were observed at 1 year and maintained up to\n10 years. Group B showed worsening of the symptoms. At 1 year, there were no\nsignificant differences between Groups A (without subsequent worsening) and B;\nat 5 years, a significant difference was found for motor and disability scores.\nWithin Group B, four patients exhibited additional improvement after the second\nDBS surgery. In the 26 patients, significant difference (P = 0.001) was found\nbetween the number of body regions affected by dystonia preoperatively and over\nthe whole follow-up. DBS efficacy in DYT1 dystonia can be maintained up to 10\nyears (two patients). New symptoms appear with long-term follow-up and may\nimprove with additional leads in a subgroup of patients.",
        "Parasite quiescence is the ability for the pathogen to be inactive, with\nrespect to metabolism and infectiousness, for some amount of time and then\nbecome active (infectious) again. The population is thus composed of an\ninactive proportion, and an active part in which evolution and reproduction\ntakes place. In this paper, we investigate the effect of parasite quiescence on\nthe time to extinction of infectious disease epidemics. We build a\nSusceptible-Infected-Quiescent-Susceptible (SIQS) epidemiological model.\nHereby, host individuals infected by a quiescent parasite strain cannot\nrecover, but are not infectious. We particularly focus on stochastic effects.\nWe show that the quiescent state does not affect the reproduction number, but\nfor a wide range of parameters the model behaves as an SIS model at a slower\ntime scale, given by the fraction of time infected individuals are within the I\nstate (and not in the Q state). This finding, proven using a time scale\nargument and singular perturbation theory for Markov processes, is illustrated\nand validated by numerical experiments based on the quasi-steady state\ndistribution. We find here that the result even holds without a distinct time\nscale separation. Our results highlight the influence of quiescence as a\nbet-hedging strategy against disease stochastic extinction, and are relevant\nfor predicting infectious disease dynamics in small populations.",
        "Nanocrystals of indium oxide (In$_2$O$_3$) with sizes below 10 nm were\nprepared in alumina matrixes by using a co-pulverization method. The used\nsubstrates such as borosilicate glasses or (100) silicon as well as the\nsubstrate temperatures during the deposition process were modified and their\neffects characterized on the structural and physical properties of\nalumina-In$_2$O$_3$ films. Complementary investigation methods including X-ray\ndiffraction, optical transmittance in the range 250-1100 nm and transmission\nelectron microscopy were used to analyze the nanostructured films. The\ncrystalline order, morphology and optical responses were monitored as function\nof the deposition parameters and the post-synthesis annealing. The optimal\nconditions were found and allow realizing suitable nanostructured films with a\nmajor crystalline order of cubic phase for the In$_2$O$_3$ nanocrystals. The\noptical properties of the films were analyzed and the key parameters such as\ndirect and indirect band gaps were evaluated as function of the synthesis\nconditions and the crystalline quality of the films.",
        "Understanding how coherence of quantum systems affects thermodynamic\nquantities, such as work and heat, is essential for harnessing quantumness\neffectively in thermal quantum technologies. Here, we study the unique\ncontributions of quantum coherence among different subsystems of a multipartite\nsystem, specifically in non-equilibrium steady states, to work and heat\ncurrents. Our system comprises two coupled ensembles, each consisting of $N$\nparticles, interacting with two baths of different temperatures, respectively.\nThe particles in an ensemble interact with their bath either simultaneously or\nsequentially, leading to non-local dissipation and enabling the decomposition\nof work and heat currents into local and non-local components.We find that the\nnon-local heat current, as well as both the local and non-local work\ncurrents,are linked to the system quantum coherence. We provide explicit\nexpressions of coherence-related quantities that determine the work currents\nunder various intrasystem interactions.Our scheme is versatile, capable of\nfunctioning as a refrigerator, an engine, and an accelerator, with its\nperformance being highly sensitive to the configuration settings. These\nfindings establish a connection between thermodynamic quantities and quantum\ncoherence, supplying valuable insights for the design of quantum thermal\nmachines.",
        "We study four-point correlators in superconformal theories in various\ndimensions. We develop an efficient method to solve the superconformal Ward\nidentities in Mellin space. For 4d $\\mathcal{N}=4$ SYM and the 6d\n$\\mathcal{N}=(2,0)$ theory, our method reproduces the known solutions. As novel\napplications of this method, we also derive solutions in 3d $\\mathcal{N} = 8$\nABJM, and in 4d $\\mathcal{N} = 4$ SYM with line defects.",
        "In this paper, we fit the $B\\to D$ vector transition form factor (TFF) by\nusing the data measured by BABAR and Belle Collaborations within Monte Carlo\n(MC) method. Meanwhile, the $B\\to D$ TFF is also calculated by using the QCD\nlight-cone sum rules approach (LCSRs) within right-handed chiral current\ncorrelation function. In which, the $D$-meson leading-twist light-cone\ndistribution amplitude (LCDA) serves as crucial input parameter is\nreconstructed with light-cone harmonic oscillator model where its longitudinal\nbehavior primarily determined by the model-free parameter $B_{2;D}$. After\nmatching the TFF with two scenarios from MC and LCSRs, we have $B_{2;D}=0.17$.\nThen, we present the curve of $D$-meson leading-twist LCDA in comparison with\nother theoretical approaches. Subsequently, the $B\\to D$ TFF $f_{+}^{BD}(q^2)$\nat the large recoil region is $f_{+}^{BD}(0)=0.625^{+0.087}_{-0.113}$, which is\ncompared in detail with theoretical estimates and experimental measurements.\nFurthermore, we calculate the decay width and branching ratio of the\nCabibbo-favored semileptonic decays $B\\to D\\ell \\bar{\\nu}_{\\ell}$, which lead\nto the results $\\mathcal{B}(B^0\\to D^-\\ell ^+\\nu _{\\ell})\n=(1.96_{-0.55}^{+0.51})\\times 10^{-2}$ and $\\mathcal{B}(B^+\\to \\bar{D}^0\\ell\n^+\\nu _{\\ell}) =(2.12_{-0.59}^{+0.55})\\times 10^{-2}$. Finally, we predict the\nCKM matrix element with two scenarios $|V_{cb}|_{\\rm\nSR}=42.97_{-2.57}^{+2.42}\\times 10^{-3}$ and $|V_{cb} |_{\\rm\nMC}=42.82_{-1.29}^{+1.07}\\times 10^{-3}$ from $B^0\\to D^-\\ell^+\\nu_{\\ell}$,\n$|V_{cb}|_{\\rm SR}=41.93_{-1.05}^{+1.03}\\times 10^{-3}$ and $|V_{cb} |_{\\rm\nMC}=41.82_{-0.25}^{+0.23}\\times 10^{-3}$ from $B^+\\to\n\\bar{D}^0\\ell^+\\nu_{\\ell}$ which are in good agreement with theoretical and\nexperimental predictions.",
        "We present a novel approach for implementing pulse-efficient SU(4) gates on\ncross resonance (CR)-based superconducting quantum devices. Our method\nintroduces a parameterized unitary derived from the CR-Hamiltonian propagator,\nwhich accounts for static-$ZZ$ interactions. Leveraging the Weyl chamber's\ngeometric structure, we successfully realize a continuous 2-qubit basis gate,\n$R_{ZZ}(\\theta)$, as an echo-free pulse schedule on the IBM Quantum device\nibm_kawasaki. We evaluate the average fidelity and gate time of various SU(4)\ngates generated using the $R_{ZZ}(\\theta)$ to confirm the advantages of our\nimplementation.",
        "This paper studies some mean-field control (MFC) problems with singular\ncontrol under general dynamic state-control-law constraints. We first propose a\ncustomized relaxed control formulation to cope with the dynamic mixed\nconstraints and establish the existence of an optimal control using some\ncompactification arguments in the proper canonical spaces to accommodate the\nsingular control. To characterize the optimal pair of regular and singular\ncontrols, we treat the controlled McKean-Vlasov process as an\ninfinite-dimensional equality constraint and recast the MFC problem as an\noptimization problem on canonical spaces with constraints on Banach space,\nallowing us to derive the stochastic maximum principle (SMP) and a constrained\nBSDE using a novel Lagrange multipliers method. In addition, we further\ninvestigate the uniqueness and the stability result of the solution to the\nconstrained FBSDE associated to the constrained MFC problem with singular\ncontrol.",
        "We compute the production rates for two, three, four and five jets in the\nhadronic decay of a Higgs boson in its two dominant decay modes to bottom\nquarks and gluons to third order in the QCD coupling constant. The five-, four-\nand three-jet rates are obtained from a next-to-next-to-leading order (NNLO)\ncalculation of Higgs decay to three jets, while the two-jet rate is inferred at\nnext-to-next-to-next-to-leading order (N$^3$LO) from the inclusive decay rate.\nOur results show distinct differences in the dependence of the jet rates on the\njet resolution parameter between the two decay modes, supporting the aim of\ndiscriminating different Higgs boson decay channels via classic QCD\nobservables.",
        "We consider robust location-scale estimators under contamination. We show\nthat commonly used robust estimators such as the median and the Huber estimator\nare inconsistent under asymmetric contamination, while the Tukey estimator is\nconsistent. In order to make nuisance parameter free inference based on the\nTukey estimator a consistent scale estimator is required. However, standard\nrobust scale estimators such as the interquartile range and the median absolute\ndeviation are inconsistent under contamination.",
        "Estimating means on Riemannian manifolds is generally computationally\nexpensive because the Riemannian distance function is not known in closed-form\nfor most manifolds. To overcome this, we show that Riemannian diffusion means\ncan be efficiently estimated using score matching with the gradient of Brownian\nmotion transition densities using the same principle as in Riemannian diffusion\nmodels. Empirically, we show that this is more efficient than Monte Carlo\nsimulation while retaining accuracy and is also applicable to learned\nmanifolds. Our method, furthermore, extends to computing the Fr\\'echet mean and\nthe logarithmic map for general Riemannian manifolds. We illustrate the\napplicability of the estimation of diffusion mean by efficiently extending\nEuclidean algorithms to general Riemannian manifolds with a Riemannian\n$k$-means algorithm and maximum likelihood Riemannian regression.",
        "It is often stated in the physics literature that maximally-spinning Kerr\nblack-hole spacetimes are characterized by near-horizon co-rotating circular\ngeodesics of radius $r_{\\text{circular}}$ with the property\n$r_{\\text{circular}}\\to r^+_{\\text{H}}$, where $r_{\\text{H}}$ is the horizon\nradius of the extremal black hole. Based on the famous Thorne hoop conjecture,\nin the present compact paper we provide evidence for the existence of a\nnon-trivial lower bound\n${{r_{\\text{circular}}-r_{\\text{H}}}\\over{r_{\\text{H}}}}\\gtrsim (\\mu\/M)^{1\/2}$\non the radii of circular orbits in the extremal Kerr black-hole spacetime,\nwhere $\\mu\/M$ is the dimensionless mass ratio which characterizes the composed\nblack-hole-orbiting-particle system.",
        "We address the characterization of two-qubit gates, focusing on bounds to\nprecision in the joint estimation of the three parameters that define their\nCartan decomposition. We derive the optimal probe states that jointly maximize\nprecision, minimize sloppiness, and eliminate quantum incompatibility.\nAdditionally, we analyze the properties of the set of optimal probes and\nevaluate their robustness against noise.",
        "Fast imaging localises celestial transients using source finders in the image\ndomain. The need for high computational throughput in this process is driven by\nnext-generation telescopes such as Square Kilometre Array (SKA), which, upon\ncompletion, will be the world's largest aperture synthesis radio telescope. It\nwill collect data at unprecedented velocity and volume. Due to the vast amounts\nof data the SKA will produce, current source finders based on source extraction\nmay be inefficient in a wide-field search. In this paper, we focus on the\nsoftware development of GPU-accelerated transient finders based on Image\nQuality Assessment (IQA) methods -- Low-Information Similarity Index (LISI) and\naugmented LISI (augLISI). We accelerate the algorithms using GPUs, achieving\nkernel time of approximately 0.1 milliseconds for transient finding in\n2048X2048 images.",
        "As the use of predictive machine learning algorithms increases in high-stakes\ndecision-making, it is imperative that these algorithms are fair across\nsensitive groups. Unfortunately, measuring and enforcing fairness in real-world\napplications can be challenging due to missing or incomplete sensitive group\ndata. Proxy-sensitive attributes have been proposed as a practical and\neffective solution in these settings, but only for parity-based fairness\nnotions. Knowing how to evaluate and control for fairness with missing\nsensitive group data for newer and more flexible frameworks, such as\nmultiaccuracy and multicalibration, remains unexplored. In this work, we\naddress this gap by demonstrating that in the absence of sensitive group data,\nproxy-sensitive attributes can provably be used to derive actionable upper\nbounds on the true multiaccuracy and multicalibration, providing insights into\na model's potential worst-case fairness violations. Additionally, we show that\nadjusting models to satisfy multiaccuracy and multicalibration across\nproxy-sensitive attributes can significantly mitigate these violations for the\ntrue, but unknown, sensitive groups. Through several experiments on real-world\ndatasets, we illustrate that approximate multiaccuracy and multicalibration can\nbe achieved even when sensitive group information is incomplete or unavailable.",
        "We present a federated learning approach for Bayesian model-based clustering\nof large-scale binary and categorical datasets. We introduce a principled\n'divide and conquer' inference procedure using variational inference with local\nmerge and delete moves within batches of the data in parallel, followed by\n'global' merge moves across batches to find global clustering structures. We\nshow that these merge moves require only summaries of the data in each batch,\nenabling federated learning across local nodes without requiring the full\ndataset to be shared. Empirical results on simulated and benchmark datasets\ndemonstrate that our method performs well in comparison to existing clustering\nalgorithms. We validate the practical utility of the method by applying it to\nlarge scale electronic health record (EHR) data.",
        "Let $\\mathbb{F}_{q}$ be the finite field with $q\\geq 5$ elements and\n$A:=\\mathbb{F}_{q}[T]$. For a class of $\\mathfrak{p} \\in \\mathrm{Spec}(A)\n\\setminus \\{(0)\\}$, but fixed, we produce infinitely many Drinfeld $A$-modules\nof rank $2$, for which the associated $\\mathfrak{p}$-adic Galois representation\nis surjective. This result is a variant of the work of~[Ray24] for\n$\\mathfrak{p}=(T)$. We also show that for a class of $\\mathfrak{l}=(l) \\in\n\\mathrm{Spec}(A)$, where $l$ is a monic polynomial, the $\\mathfrak{p}$-adic\nGalois representation, attached to the Drinfeld $A$-module\n$\\varphi_{T}=T+g_{1}\\tau-l^{q-1}\\tau^2$ with $g_{1} \\in A \\setminus\n\\mathfrak{l}$, is surjective for all $\\mathfrak{p} \\in\n\\mathrm{Spec}(A)\\setminus\\{(0)\\}$. This result generalizes the work of [Zyw11]\nfrom $\\mathfrak{l}=(T), g_1=1$."
      ]
    }
  },
  {
    "id":2411.15331,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"Improvement of quantitative structure\u2013activity relationship (QSAR) tools for predicting Ames mutagenicity: outcomes of the Ames\/QSAR International Challenge Project",
    "start_abstract":"The International Conference on Harmonization (ICH) M7 guideline allows the use of in silico approaches for predicting Ames mutagenicity initial assessment impurities pharmaceuticals. This is first international that addresses quantitative structure\u2013activity relationship (QSAR) models lieu actual toxicological studies human health assessment. Therefore, QSAR now require higher predictive power identifying mutagenic chemicals. To increase models, larger experimental datasets from reliable sources are required. Division Genetics and Mutagenesis, National Institute Health Sciences (DGM\/NIHS) Japan recently established a unique proprietary database containing 12140 new chemicals have not been previously used developing models. DGM\/NIHS provided this to vendors validate improve their tools. Ames\/QSAR Challenge Project was initiated 2014 with 12 testing 17 tools against these compounds three phases. We present final results. All were considerably improved by participation project. Most achieved >50% sensitivity (positive prediction among all positives) (accuracy) as high 80%, almost equivalent inter-laboratory reproducibility tests. further tools, accumulation additional test data required well re-evaluation some previous Indeed, Ames-positive or Ames-negative may incorrectly classified because methodological weakness, resulting false-positive false-negative predictions These incorrect hamper source noise development It thus essential establish large benchmark consisting only well-validated results build more accurate",
    "start_categories":[
      "q-bio.GN"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Mutagenpred-gcnns: a graph convolutional neural network-based classification model for mutagenicity prediction with data-driven molecular fingerprints"
      ],
      "abstract":[
        "An important task in the early stage of drug discovery is the identification of mutagenic compounds. Mutagenicity prediction models that can interpret relationships between toxicological endpoints and compound structures are especially favorable. In this research, we used an advanced graph convolutional neural network (GCNN) architecture to identify the molecular representation and develop predictive models based on these representations. The predictive model based on features extracted by GCNNs can not only predict the mutagenicity of compounds but also identify the structure alerts in compounds. In fivefold cross-validation and external validation, the highest area under the curve was 0.8782 and 0.8382, respectively; the highest accuracy (Q) was 80.98% and 76.63%, respectively; the highest sensitivity was 83.27% and 78.92%, respectively; and the highest specificity was 78.83% and 76.32%, respectively. Additionally, our model also identified some toxicophores, such as aromatic nitro, three-membered heterocycles, quinones, and nitrogen and sulfur mustard. These results indicate that GCNNs could learn the features of mutagens effectively. In summary, we developed a mutagenicity classification model with high predictive performance and interpretability based on a data-driven molecular representation trained through GCNNs."
      ],
      "categories":[
        "cs.NE"
      ]
    },
    "list":{
      "title":[
        "Transformer Semantic Genetic Programming for Symbolic Regression",
        "Range and Angle Estimation with Spiking Neural Resonators for FMCW Radar",
        "Neuromorphic Digital-Twin-based Controller for Indoor Multi-UAV Systems\n  Deployment",
        "Large Language Model-Based Benchmarking Experiment Settings for\n  Evolutionary Multi-Objective Optimization",
        "Genetic AI: Evolutionary Simulation for Data Analysis",
        "Fig Tree-Wasp Symbiotic Coevolutionary Optimization Algorithm",
        "Abnormal Mutations: Evolution Strategies Don't Require Gaussianity",
        "Brain in the Dark: Design Principles for Neuromimetic Inference under\n  the Free Energy Principle",
        "The working principles of model-based GAs fall within the PAC framework:\n  A mathematical theory of problem decomposition",
        "Warm Starting of CMA-ES for Contextual Optimization Problems",
        "Multiple-gain Estimation for Running Time of Evolutionary Combinatorial\n  Optimization",
        "Hybrid Firefly Algorithm and Sperm Swarm Optimization Algorithm using\n  Newton-Raphson Method (HFASSON) and its application in CR-VANET",
        "Spiking World Model with Multi-Compartment Neurons for Model-based\n  Reinforcement Learning",
        "Credit Risk Identification in Supply Chains Using Generative Adversarial\n  Networks",
        "Optimization Landscapes Learned: Proxy Networks Boost Convergence in\n  Physics-based Inverse Problems",
        "Estimation-Aware Trajectory Optimization with Set-Valued Measurement\n  Uncertainties",
        "How do recollimation-induced instabilities shape the propagation of\n  hydrodynamic relativistic jets?",
        "Structural Perturbation in Large Language Model Representations through\n  Recursive Symbolic Regeneration",
        "Understanding and Evaluating Hallucinations in 3D Visual Language Models",
        "Synthetic Clarification and Correction Dialogues about Data-Centric\n  Tasks -- A Teacher-Student Approach",
        "Comparative Analysis of Control Strategies for Position Regulation in DC\n  Servo Motors",
        "ATLAS Navigator: Active Task-driven LAnguage-embedded Gaussian Splatting",
        "Two-stage Deep Denoising with Self-guided Noise Attention for Multimodal\n  Medical Images",
        "On Choquard-Kirchhoff Type Critical Multiphase Problem",
        "Privacy-Preserving Hybrid Ensemble Model for Network Anomaly Detection:\n  Balancing Security and Data Protection",
        "Primordial Origin of Methane on Eris and Makemake Supported by D\/H\n  Ratios",
        "Quarkonia and Deconfined Quark-Gluon Matter in Heavy-Ion Collisions",
        "Quantum-Inspired Fidelity-based Divergence"
      ],
      "abstract":[
        "In standard genetic programming (stdGP), solutions are varied by modifying\ntheir syntax, with uncertain effects on their semantics. Geometric-semantic\ngenetic programming (GSGP), a popular variant of GP, effectively searches the\nsemantic solution space using variation operations based on linear\ncombinations, although it results in significantly larger solutions. This paper\npresents Transformer Semantic Genetic Programming (TSGP), a novel and flexible\nsemantic approach that uses a generative transformer model as search operator.\nThe transformer is trained on synthetic test problems and learns semantic\nsimilarities between solutions. Once the model is trained, it can be used to\ncreate offspring solutions with high semantic similarity also for unseen and\nunknown problems. Experiments on several symbolic regression problems show that\nTSGP generates solutions with comparable or even significantly better\nprediction quality than stdGP, SLIM_GSGP, DSR, and DAE-GP. Like SLIM_GSGP, TSGP\nis able to create new solutions that are semantically similar without creating\nsolutions of large size. An analysis of the search dynamic reveals that the\nsolutions generated by TSGP are semantically more similar than the solutions\ngenerated by the benchmark approaches allowing a better exploration of the\nsemantic solution space.",
        "Automotive radar systems face the challenge of managing high sampling rates\nand large data bandwidth while complying with stringent real-time and energy\nefficiency requirements. The growing complexity of autonomous vehicles further\nintensifies these requirements. Neuromorphic computing offers promising\nsolutions because of its inherent energy efficiency and parallel processing\ncapacity. This research presents a novel spiking neuron model for signal\nprocessing of frequency-modulated continuous wave (FMCW) radars that\noutperforms the state-of-the-art spectrum analysis algorithms in latency and\ndata bandwidth. These spiking neural resonators are based on the\nresonate-and-fire neuron model and optimized to dynamically process raw radar\ndata while simultaneously emitting an output in the form of spikes. We designed\nthe first neuromorphic neural network consisting of these spiking neural\nresonators that estimates range and angle from FMCW radar data. We evaluated\nthe range-angle maps on simulated datasets covering multiple scenarios and\ncompared the results with a state-of-the-art pipeline for radar processing. The\nproposed neuron model significantly reduces the processing latency compared to\ntraditional frequency analysis algorithms, such as the Fourier transformation\n(FT), which needs to sample and store entire data frames before processing. The\nevaluations demonstrate that these spiking neural resonators achieve\nstate-of-the-art detection accuracy while emitting spikes simultaneously to\nprocessing and transmitting only 0.02 % of the data compared to a float-32 FT.\nThe results showcase the potential for neuromorphic signal processing for FMCW\nradar systems and pave the way for designing neuromorphic radar sensors.",
        "Presented study introduces a novel distributed cloud-edge framework for\nautonomous multi-UAV systems that combines the computational efficiency of\nneuromorphic computing with nature-inspired control strategies. The proposed\narchitecture equips each UAV with an individual Spiking Neural Network (SNN)\nthat learns to reproduce optimal control signals generated by a cloud-based\ncontroller, enabling robust operation even during communication interruptions.\nBy integrating spike coding with nature-inspired control principles inspired by\nTilapia fish territorial behavior, our system achieves sophisticated formation\ncontrol and obstacle avoidance in complex urban environments. The distributed\narchitecture leverages cloud computing for complex calculations while\nmaintaining local autonomy through edge-based SNNs, significantly reducing\nenergy consumption and computational overhead compared to traditional\ncentralized approaches. Our framework addresses critical limitations of\nconventional methods, including the dependency on pre-modeled environments,\ncomputational intensity of traditional methods, and local minima issues in\npotential field approaches. Simulation results demonstrate the system's\neffectiveness across two different scenarios. First, the indoor deployment of a\nmulti-UAV system made-up of 15 UAVs. Then the collision-free formation control\nof a moving UAV flock including 6 UAVs considering the obstacle avoidance.\nOwing to the sparsity of spiking patterns, and the event-based nature of SNNs\nin average for the whole group of UAVs, the framework achieves almost 90%\nreduction in computational burden compared to traditional von Neumann\narchitectures implementing traditional artificial neural networks.",
        "When we manually design an evolutionary optimization algorithm, we implicitly\nor explicitly assume a set of target optimization problems. In the case of\nautomated algorithm design, target optimization problems are usually explicitly\nshown. Recently, the use of large language models (LLMs) for the design of\nevolutionary multi-objective optimization (EMO) algorithms have been examined\nin some studies. In those studies, target multi-objective problems are not\nalways explicitly shown. It is well known in the EMO community that the\nperformance evaluation results of EMO algorithms depend on not only test\nproblems but also many other factors such as performance indicators, reference\npoint, termination condition, and population size. Thus, it is likely that the\ndesigned EMO algorithms by LLMs depends on those factors. In this paper, we try\nto examine the implicit assumption about the performance comparison of EMO\nalgorithms in LLMs. For this purpose, we ask LLMs to design a benchmarking\nexperiment of EMO algorithms. Our experiments show that LLMs often suggest\nclassical benchmark settings: Performance examination of NSGA-II, MOEA\/D and\nNSGA-III on ZDT, DTLZ and WFG by HV and IGD under the standard parameter\nspecifications.",
        "We introduce Genetic AI, a novel method for data analysis by evolutionary\nsimulations. The method can be applied to data of any domain and allows for a\ndata-less training of AI models. Without employing predefined rules or training\ndata, Genetic AI first converts the input data into genes and organisms. In a\nsimulation from first principles, these genes and organisms compete for\nfitness, where their behavior is governed by universal evolutionary strategies.\nInvestigating evolutionary stable equilibriums, Genetic AI helps understanding\ncorrelations and symmetries in general input data. Several numerical\nexperiments demonstrate the dynamics of exemplary systems.",
        "The nature inspired algorithms are becoming popular due to their simplicity\nand wider applicability. In the recent past several such algorithms have been\ndeveloped. They are mainly bio-inspired, swarm based, physics based and\nsocio-inspired; however, the domain based on symbiotic relation between\ncreatures is still to be explored. A novel metaheuristic optimization algorithm\nreferred to as Fig Tree-Wasp Symbiotic Coevolutionary (FWSC) algorithm is\nproposed. It models the symbiotic coevolutionary relationship between fig trees\nand wasps. More specifically, the mating of wasps, pollinating the figs,\nsearching for new trees for pollination and wind effect drifting of wasps are\nmodeled in the algorithm. These phenomena help in balancing the two important\naspects of exploring the search space efficiently as well as exploit the\npromising regions. The algorithm is successfully tested on a variety of test\nproblems. The results are compared with existing methods and algorithms. The\nWilcoxon Signed Rank Test and Friedman Test are applied for the statistical\nvalidation of the algorithm performance. The algorithm is also further applied\nto solve the real-world engineering problems. The performance of the FWSC\nunderscored that the algorithm can be applied to wider variety of real-world\nproblems.",
        "The mutation process in evolution strategies has been interlinked with the\nnormal distribution since its inception. Many lines of reasoning have been\ngiven for this strong dependency, ranging from maximum entropy arguments to the\nneed for isotropy. However, some theoretical results suggest that other\ndistributions might lead to similar local convergence properties. This paper\nempirically shows that a wide range of evolutionary strategies, from the\n(1+1)-ES to CMA-ES, show comparable optimization performance when using a\nmutation distribution other than the standard Gaussian. Replacing it with,\ne.g., uniformly distributed mutations, does not deteriorate the performance of\nES, when using the default adaptation mechanism for the strategy parameters. We\nobserve that these results hold not only for the sphere model but also for a\nwider range of benchmark problems.",
        "Deep learning has revolutionised artificial intelligence (AI) by enabling\nautomatic feature extraction and function approximation from raw data. However,\nit faces challenges such as a lack of out-of-distribution generalisation,\ncatastrophic forgetting and poor interpretability. In contrast, biological\nneural networks, such as those in the human brain, do not suffer from these\nissues, inspiring AI researchers to explore neuromimetic deep learning, which\naims to replicate brain mechanisms within AI models. A foundational theory for\nthis approach is the Free Energy Principle (FEP), which despite its potential,\nis often considered too complex to understand and implement in AI as it\nrequires an interdisciplinary understanding across a variety of fields. This\npaper seeks to demystify the FEP and provide a comprehensive framework for\ndesigning neuromimetic models with human-like perception capabilities. We\npresent a roadmap for implementing these models and a Pytorch code repository\nfor applying FEP in a predictive coding network.",
        "The concepts of linkage, building blocks, and problem decomposition have long\nexisted in the genetic algorithm (GA) field and have guided the development of\nmodel-based GAs for decades. However, their definitions are usually vague,\nmaking it difficult to develop theoretical support. This paper provides an\nalgorithm-independent definition to describe the concept of linkage. With this\ndefinition, the paper proves that any problems with a bounded degree of linkage\nare decomposable and that proper problem decomposition is possible via linkage\nlearning. The way of decomposition given in this paper also offers a new\nperspective on nearly decomposable problems with bounded difficulty and\nbuilding blocks from the theoretical aspect. Finally, this paper relates\nproblem decomposition to PAC learning and proves that the global optima of\nthese problems and the minimum decomposition blocks are PAC learnable under\ncertain conditions.",
        "Several practical applications of evolutionary computation possess objective\nfunctions that receive the design variables and externally given parameters.\nSuch problems are termed contextual optimization problems. These problems\nrequire finding the optimal solutions corresponding to the given context\nvectors. Existing contextual optimization methods train a policy model to\npredict the optimal solution from context vectors. However, the performance of\nsuch models is limited by their representation ability. By contrast, warm\nstarting methods have been used to initialize evolutionary algorithms on a\ngiven problem using the optimization results on similar problems. Because warm\nstarting methods do not consider the context vectors, their performances can be\nimproved on contextual optimization problems. Herein, we propose a covariance\nmatrix adaptation evolution strategy with contextual warm starting (CMA-ES-CWS)\nto efficiently optimize the contextual optimization problem with a given\ncontext vector. The CMA-ES-CWS utilizes the optimization results of past\ncontext vectors to train the multivariate Gaussian process regression.\nSubsequently, the CMA-ES-CWS performs warm starting for a given context vector\nby initializing the search distribution using posterior distribution of the\nGaussian process regression. The results of the numerical simulation suggest\nthat CMA-ES-CWS outperforms the existing contextual optimization and warm\nstarting methods.",
        "The running-time analysis of evolutionary combinatorial optimization is a\nfundamental topic in evolutionary computation. Its current research mainly\nfocuses on specific algorithms for simplified problems due to the challenge\nposed by fluctuating fitness values. This paper proposes a multiple-gain model\nto estimate the fitness trend of population during iterations. The proposed\nmodel is an improved version of the average gain model, which is the approach\nto estimate the running time of evolutionary algorithms for numerical\noptimization. The improvement yields novel results of evolutionary\ncombinatorial optimization, including a briefer proof for the time complexity\nupper bound in the case of (1+1) EA for the Onemax problem, two tighter time\ncomplexity upper bounds than the known results in the case of (1+$\\lambda$) EA\nfor the knapsack problem with favorably correlated weights and a closed-form\nexpression of time complexity upper bound in the case of (1+$\\lambda$) EA for\ngeneral $k$-MAX-SAT problems. The results indicate that the practical running\ntime aligns with the theoretical results, verifying that the multiple-gain\nmodel is more general for running-time analysis of evolutionary combinatorial\noptimization than state-of-the-art methods.",
        "This paper proposes a new hybrid algorithm, combining FA, SSO, and the N-R\nmethod to accelerate convergence towards global optima, named the Hybrid\nFirefly Algorithm and Sperm Swarm Optimization with Newton-Raphson (HFASSON).\nThe performance of HFASSON is evaluated using 23 benchmark functions from the\nCEC 2017 suite, tested in 30, 50, and 100 dimensions. A statistical comparison\nis performed to assess the effectiveness of HFASSON against FA, SSO, HFASSO,\nand five hybrid algorithms: Water Cycle Moth Flame Optimization (WCMFO), Hybrid\nParticle Swarm Optimization and Genetic Algorithm (HPSOGA), Hybrid Sperm Swarm\nOptimization and Gravitational Search Algorithm (HSSOGSA), Grey Wolf and Cuckoo\nSearch Algorithm (GWOCS), and Hybrid Firefly Genetic Algorithm (FAGA). Results\nfrom the Friedman rank test show the superior performance of HFASSON.\nAdditionally, HFASSON is applied to Cognitive Radio Vehicular Ad-hoc Networks\n(CR-VANET), outperforming basic CR-VANET in spectrum utilization. These\nfindings demonstrate HFASSON's efficiency in wireless network applications.",
        "Brain-inspired spiking neural networks (SNNs) have garnered significant\nresearch attention in algorithm design and perception applications. However,\ntheir potential in the decision-making domain, particularly in model-based\nreinforcement learning, remains underexplored. The difficulty lies in the need\nfor spiking neurons with long-term temporal memory capabilities, as well as\nnetwork optimization that can integrate and learn information for accurate\npredictions. The dynamic dendritic information integration mechanism of\nbiological neurons brings us valuable insights for addressing these challenges.\nIn this study, we propose a multi-compartment neuron model capable of\nnonlinearly integrating information from multiple dendritic sources to\ndynamically process long sequential inputs. Based on this model, we construct a\nSpiking World Model (Spiking-WM), to enable model-based deep reinforcement\nlearning (DRL) with SNNs. We evaluated our model using the DeepMind Control\nSuite, demonstrating that Spiking-WM outperforms existing SNN-based models and\nachieves performance comparable to artificial neural network (ANN)-based world\nmodels employing Gated Recurrent Units (GRUs). Furthermore, we assess the\nlong-term memory capabilities of the proposed model in speech datasets,\nincluding SHD, TIMIT, and LibriSpeech 100h, showing that our multi-compartment\nneuron model surpasses other SNN-based architectures in processing long\nsequences. Our findings underscore the critical role of dendritic information\nintegration in shaping neuronal function, emphasizing the importance of\ncooperative dendritic processing in enhancing neural computation.",
        "Credit risk management within supply chains has emerged as a critical\nresearch area due to its significant implications for operational stability and\nfinancial sustainability. The intricate interdependencies among supply chain\nparticipants mean that credit risks can propagate across networks, with impacts\nvarying by industry. This study explores the application of Generative\nAdversarial Networks (GANs) to enhance credit risk identification in supply\nchains. GANs enable the generation of synthetic credit risk scenarios,\naddressing challenges related to data scarcity and imbalanced datasets. By\nleveraging GAN-generated data, the model improves predictive accuracy while\neffectively capturing dynamic and temporal dependencies in supply chain data.\nThe research focuses on three representative industries-manufacturing (steel),\ndistribution (pharmaceuticals), and services (e-commerce) to assess\nindustry-specific credit risk contagion. Experimental results demonstrate that\nthe GAN-based model outperforms traditional methods, including logistic\nregression, decision trees, and neural networks, achieving superior accuracy,\nrecall, and F1 scores. The findings underscore the potential of GANs in\nproactive risk management, offering robust tools for mitigating financial\ndisruptions in supply chains. Future research could expand the model by\nincorporating external market factors and supplier relationships to further\nenhance predictive capabilities. Keywords- Generative Adversarial Networks\n(GANs); Supply Chain Risk; Credit Risk Identification; Machine Learning; Data\nAugmentation",
        "Solving inverse problems in physics is central to understanding complex\nsystems and advancing technologies in various fields. Iterative optimization\nalgorithms, commonly used to solve these problems, often encounter local\nminima, chaos, or regions with zero gradients. This is due to their\noverreliance on local information and highly chaotic inverse loss landscapes\ngoverned by underlying partial differential equations (PDEs). In this work, we\nshow that deep neural networks successfully replicate such complex loss\nlandscapes through spatio-temporal trajectory inputs. They also offer the\npotential to control the underlying complexity of these chaotic loss landscapes\nduring training through various regularization methods. We show that optimizing\non network-smoothened loss landscapes leads to improved convergence in\npredicting optimum inverse parameters over conventional momentum-based\noptimizers such as BFGS on multiple challenging problems.",
        "In this paper, we present an optimization-based framework for generating\nestimation-aware trajectories in scenarios where measurement (output)\nuncertainties are state-dependent and set-valued. The framework leverages the\nconcept of regularity for set-valued output maps. Specifically, we demonstrate\nthat, for output-regular maps, one can utilize a set-valued observability\nmeasure that is concave with respect to finite-horizon state trajectories. By\nmaximizing this measure, optimized estimation-aware trajectories can be\ndesigned for a broad class of systems, including those with locally linearized\ndynamics. To illustrate the effectiveness of the proposed approach, we provide\na representative example in the context of trajectory planning for vision-based\nestimation. We present an estimation-aware trajectory for an uncooperative\ntarget-tracking problem that uses a machine learning (ML)-based estimation\nmodule on an ego-satellite.",
        "Recollimation is a phenomenon of particular importance in the dynamic\nevolution of jets and in the emission of high-energy radiation. Additionally,\nthe full comprehension of this phenomenon provides insights into fundamental\nproperties of jets in the vicinity of the Active Galactic Nucleus (AGN).\nThree-dimensional (magneto-)hydrodynamic simulations revealed that the jet\nconditions at recollimation favor the growth of strong instabilities,\nchallenging the traditional view-supported from two-dimensional simulations-of\nconfined jets undergoing a series of recollimation and reflection shocks. To\ninvestigate the stability of relativistic jets in AGNs at recollimation sites,\nwe perform a set of long duration three-dimensional relativistic hydrodynamic\nsimulations with the state-of-the-art PLUTO code, to focus on the development\nof hydrodynamical instabilities. We explore the non-linear growth of the\ninstabilities and their effects on the physical jet properties as a function of\nthe initial jet parameters: jet Lorentz factor, temperature, opening angle and\njet-environment density-contrast. The parameter space is designed to describe\nlow-power, weakly magnetized jets at small distances from the core (around the\nparsec scale). All collimating jets we simulated develop instabilities.\nRecollimation instabilities decelerate the jet, heat it, entrain external\nmaterial, and move the recollimation point to shorter distances from the core.\nThis is true for both conical and cylindrical jets. The instabilities, that are\nfirst triggered by the centrifugal instability, appear to be less disruptive in\nthe case of narrower, denser, more relativistic, and warmer jets. These results\nprovide valuable insights into the complex processes governing AGN jets and\ncould be used to model the properties of low-power, weakly magnetized jetted\nAGNs.",
        "Symbolic perturbations offer a novel approach for influencing neural\nrepresentations without requiring direct modification of model parameters. The\nrecursive regeneration of symbolic structures introduces structured variations\nin latent embeddings, leading to controlled shifts in attention dynamics and\nlexical diversity across sequential generations. A comparative analysis with\nconventional fine-tuning techniques reveals that structural modifications at\nthe symbolic level induce distinct variations in contextual sensitivity while\nmaintaining overall model fluency and coherence. Shifts in attention weight\ndistributions highlight the role of symbolic modifications in adjusting token\ndependencies, influencing response variability, and refining long-form text\ngeneration. Experimental findings suggest that symbolic perturbations can\nenhance adaptability in domain-specific applications, allowing modifications in\nmodel behavior without retraining. Evaluations of semantic drift indicate that\nrecursive regeneration alters long-range token dependencies, affecting topic\ncoherence across extended text sequences. Results from lexical variability\nassessments further support the conclusion that symbolic-level modifications\nintroduce interpretable variations in generated responses, potentially enabling\nmore controlled stylistic adjustments in automated text generation.",
        "Recently, 3D-LLMs, which combine point-cloud encoders with large models, have\nbeen proposed to tackle complex tasks in embodied intelligence and scene\nunderstanding. In addition to showing promising results on 3D tasks, we found\nthat they are significantly affected by hallucinations. For instance, they may\ngenerate objects that do not exist in the scene or produce incorrect\nrelationships between objects. To investigate this issue, this work presents\nthe first systematic study of hallucinations in 3D-LLMs. We begin by quickly\nevaluating hallucinations in several representative 3D-LLMs and reveal that\nthey are all significantly affected by hallucinations. We then define\nhallucinations in 3D scenes and, through a detailed analysis of datasets,\nuncover the underlying causes of these hallucinations. We find three main\ncauses: (1) Uneven frequency distribution of objects in the dataset. (2) Strong\ncorrelations between objects. (3) Limited diversity in object attributes.\nAdditionally, we propose new evaluation metrics for hallucinations, including\nRandom Point Cloud Pair and Opposite Question Evaluations, to assess whether\nthe model generates responses based on visual information and aligns it with\nthe text's meaning.",
        "Real dialogues with AI assistants for solving data-centric tasks often follow\ndynamic, unpredictable paths due to imperfect information provided by the user\nor in the data, which must be caught and handled. Developing datasets which\ncapture such user-AI interactions is difficult and time-consuming. In this\nwork, we develop a novel framework for synthetically generating controlled,\nmulti-turn conversations between a user and AI assistant for the task of\ntable-based question answering, which can be generated from an existing dataset\nwith fully specified table QA examples for any target domain. Each conversation\naims to solve a table-based reasoning question through collaborative effort,\nmodeling one of two real-world scenarios: (1) an AI-initiated clarification, or\n(2) a user-initiated correction. Critically, we employ a strong teacher LLM to\nverify the correctness of our synthetic conversations, ensuring high quality.\nWe demonstrate synthetic datasets generated from TAT-QA and WikiTableQuestions\nas benchmarks of frontier LLMs. We find that even larger models struggle to\neffectively issuing clarification questions and accurately integrate user\nfeedback for corrections.",
        "A servomotor is a closed-loop system designed for precise movement control,\nutilizing position feedback to achieve accurate final positions. Due to the\nability to deliver higher power output and operate at enhanced speeds, DC servo\nmotors are considered ideal for applications requiring precision and\nperformance. This research aims to design, simulate, and compare various\ncontrol strategies for precise position control in DC servo motors (DSM). The\ncontrollers evaluated in this study include proportional (P),\nproportional-integral (PI), proportional-integral-derivative (PID),\nstate-feedback controllers (SFC), and state-feedback controllers augmented with\nintegral action (SFCIA). The performance of these controllers was evaluated\nusing MATLAB simulations, characterized by overshoot, settling time,\nsteady-state error, rise time, and peak time. The results indicate that the\nstate-feedback controller with integral action (SFCIA) surpasses other control\nstrategies by achieving zero steady-state error, minimal overshoot, the\nshortest settling time, and optimized rise and peak times. These findings\nhighlight the effectiveness of SFCIA for tasks requiring high levels of\nstability, precision, and dynamic performance.",
        "We address the challenge of task-oriented navigation in unstructured and\nunknown environments, where robots must incrementally build and reason on rich,\nmetric-semantic maps in real time. Since tasks may require clarification or\nre-specification, it is necessary for the information in the map to be rich\nenough to enable generalization across a wide range of tasks. To effectively\nexecute tasks specified in natural language, we propose a hierarchical\nrepresentation built on language-embedded Gaussian splatting that enables both\nsparse semantic planning that lends itself to online operation and dense\ngeometric representation for collision-free navigation. We validate the\neffectiveness of our method through real-world robot experiments conducted in\nboth cluttered indoor and kilometer-scale outdoor environments, with a\ncompetitive ratio of about 60% against privileged baselines. Experiment videos\nand more details can be found on our project page: https:\/\/atlasnav.github.io",
        "Medical image denoising is considered among the most challenging vision\ntasks. Despite the real-world implications, existing denoising methods have\nnotable drawbacks as they often generate visual artifacts when applied to\nheterogeneous medical images. This study addresses the limitation of the\ncontemporary denoising methods with an artificial intelligence (AI)-driven\ntwo-stage learning strategy. The proposed method learns to estimate the\nresidual noise from the noisy images. Later, it incorporates a novel noise\nattention mechanism to correlate estimated residual noise with noisy inputs to\nperform denoising in a course-to-refine manner. This study also proposes to\nleverage a multi-modal learning strategy to generalize the denoising among\nmedical image modalities and multiple noise patterns for widespread\napplications. The practicability of the proposed method has been evaluated with\ndense experiments. The experimental results demonstrated that the proposed\nmethod achieved state-of-the-art performance by significantly outperforming the\nexisting medical image denoising methods in quantitative and qualitative\ncomparisons. Overall, it illustrates a performance gain of 7.64 in Peak\nSignal-to-Noise Ratio (PSNR), 0.1021 in Structural Similarity Index (SSIM),\n0.80 in DeltaE ($\\Delta E$), 0.1855 in Visual Information Fidelity Pixel-wise\n(VIFP), and 18.54 in Mean Squared Error (MSE) metrics.",
        "In this paper, we obtain the existence of weak solutions to the\nChoquard-Kirchhoff type critical multiphase problem: \\begin{equation*}\n\\left\\{\\begin{array}{cc}\n  &-M(\\varphi_{\\h}(\\lvert{\\nabla u}\\rvert))div(\\lvert{\\nabla\nu}\\rvert^{p(x)-2}\\nabla u+a_1(x)\\lvert{\\nabla u}\\rvert^{q(x)-2}\\nabla\nu+a_2(x)\\lvert{\\nabla u}\\rvert^{r(x)-2}\\nabla u)\n  & =\\lambda g(x)\\lvert{u}\\rvert^{\\gamma(x)-2}u+\\theta B(x,u)+\\kappa\n\\left(\\int_{\\q}\\frac{F(y,u(y))}{\\lvert{x-y}\\rvert^{d(x,y)}}\\, dy\\right) f(x,u)\n\\ \\text{in} \\ \\Omega,\n  & u=0 \\ \\text{on} \\ {\\partial \\Omega}. \\end{array}\\right. \\end{equation*}\n  The term $B(x,u)$ on the right-hand side generalizes the critical growth. We\nobtain existence and multiplicity results by establishing certain embedding\nresults and concentration compactness principle along with the\nHardy-Littlewood-Sobolev type inequality for the Musielak Orlicz Sobolev space\n$ W^{1,\\mathcal{T}}(\\q)$.",
        "Privacy-preserving network anomaly detection has become an essential area of\nresearch due to growing concerns over the protection of sensitive data.\nTraditional anomaly detection models often prioritize accuracy while neglecting\nthe critical aspect of privacy. In this work, we propose a hybrid ensemble\nmodel that incorporates privacy-preserving techniques to address both detection\naccuracy and data protection. Our model combines the strengths of several\nmachine learning algorithms, including K-Nearest Neighbors (KNN), Support\nVector Machines (SVM), XGBoost, and Artificial Neural Networks (ANN), to create\na robust system capable of identifying network anomalies while ensuring\nprivacy. The proposed approach integrates advanced preprocessing techniques\nthat enhance data quality and address the challenges of small sample sizes and\nimbalanced datasets. By embedding privacy measures into the model design, our\nsolution offers a significant advancement over existing methods, ensuring both\nenhanced detection performance and strong privacy safeguards.",
        "Deuterium, a heavy isotope of hydrogen, is a key tracer of the formation of\nthe Solar System. Recent JWST observations have expanded the dataset of D\/H\nratios in methane on the KBOs Eris and Makemake, providing new insights into\ntheir origins. This study examines the elevated D\/H ratios in methane on these\nKBOs in the context of protosolar nebula dynamics and chemistry, and proposes a\nprimordial origin for the methane, in contrast to previous hypotheses\nsuggesting abiotic production by internal heating. A time-dependent disk model\ncoupled with a deuterium chemistry module was used to simulate the isotopic\nexchange between methane and hydrogen. Observational constraints, including the\nD\/H ratio measured in methane in comet 67P\/Churyumov-Gerasimenko, were used to\nrefine the primordial D\/H abundance. The simulations show that the observed D\/H\nratios in methane on Eris and Makemake are consistent with a primordial origin.\nThe results suggest that methane on these KBOs likely originates from the\nprotosolar nebula, similar to cometary methane, and was sequestered in solid\nform -- either as pure condensates or clathrates -- within their building\nblocks prior to accretion. These results provide a { simple} explanation for\nthe high D\/H ratios in methane on Eris and Makemake, without the need to invoke\ninternal production mechanisms.",
        "In this report, we present an experimental overview of quarkonium results\nobtained in nucleus-nucleus heavy-ion collisions, with a focus on the data\ncollected at the LHC. We discuss the current understanding of charmonium and\nbottomonium behavior in the deconfined medium produced in such collisions,\ncomparing the various observables now accessible to state-of-the-art\ntheoretical models. We also discuss the open points and how future heavy-ion\nexperiments aim to clarify these aspects.",
        "Kullback--Leibler (KL) divergence is a fundamental measure of the\ndissimilarity between two probability distributions, but it can become unstable\nin high-dimensional settings due to its sensitivity to mismatches in\ndistributional support. To address robustness limitations, we propose a novel\nQuantum-Inspired Fidelity-based Divergence (QIF), leveraging quantum\ninformation principles yet efficiently computable on classical hardware.\nCompared to KL divergence, QIF demonstrates improved numerical stability under\npartial or near-disjoint support conditions, thereby reducing the need for\nextensive regularization in specific scenarios. Moreover, QIF admits\nwell-defined theoretical bounds and continuous similarity measures. Building on\nthis, we introduce a novel regularization method, QR-Drop, which utilizes QIF\nto improve generalization in machine learning models. Empirical results show\nthat QR-Drop effectively mitigates overfitting and outperforms state-of-the-art\nmethods."
      ]
    }
  },
  {
    "id":2411.15331,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"Mutagenpred-gcnns: a graph convolutional neural network-based classification model for mutagenicity prediction with data-driven molecular fingerprints",
    "start_abstract":"An important task in the early stage of drug discovery is the identification of mutagenic compounds. Mutagenicity prediction models that can interpret relationships between toxicological endpoints and compound structures are especially favorable. In this research, we used an advanced graph convolutional neural network (GCNN) architecture to identify the molecular representation and develop predictive models based on these representations. The predictive model based on features extracted by GCNNs can not only predict the mutagenicity of compounds but also identify the structure alerts in compounds. In fivefold cross-validation and external validation, the highest area under the curve was 0.8782 and 0.8382, respectively; the highest accuracy (Q) was 80.98% and 76.63%, respectively; the highest sensitivity was 83.27% and 78.92%, respectively; and the highest specificity was 78.83% and 76.32%, respectively. Additionally, our model also identified some toxicophores, such as aromatic nitro, three-membered heterocycles, quinones, and nitrogen and sulfur mustard. These results indicate that GCNNs could learn the features of mutagens effectively. In summary, we developed a mutagenicity classification model with high predictive performance and interpretability based on a data-driven molecular representation trained through GCNNs.",
    "start_categories":[
      "cs.NE"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "Improvement of quantitative structure\u2013activity relationship (QSAR) tools for predicting Ames mutagenicity: outcomes of the Ames\/QSAR International Challenge Project"
      ],
      "abstract":[
        "The International Conference on Harmonization (ICH) M7 guideline allows the use of in silico approaches for predicting Ames mutagenicity initial assessment impurities pharmaceuticals. This is first international that addresses quantitative structure\u2013activity relationship (QSAR) models lieu actual toxicological studies human health assessment. Therefore, QSAR now require higher predictive power identifying mutagenic chemicals. To increase models, larger experimental datasets from reliable sources are required. Division Genetics and Mutagenesis, National Institute Health Sciences (DGM\/NIHS) Japan recently established a unique proprietary database containing 12140 new chemicals have not been previously used developing models. DGM\/NIHS provided this to vendors validate improve their tools. Ames\/QSAR Challenge Project was initiated 2014 with 12 testing 17 tools against these compounds three phases. We present final results. All were considerably improved by participation project. Most achieved >50% sensitivity (positive prediction among all positives) (accuracy) as high 80%, almost equivalent inter-laboratory reproducibility tests. further tools, accumulation additional test data required well re-evaluation some previous Indeed, Ames-positive or Ames-negative may incorrectly classified because methodological weakness, resulting false-positive false-negative predictions These incorrect hamper source noise development It thus essential establish large benchmark consisting only well-validated results build more accurate"
      ],
      "categories":[
        "q-bio.GN"
      ]
    },
    "list":{
      "title":[
        "Learnable Group Transform: Enhancing Genotype-to-Phenotype Prediction\n  for Rice Breeding with Small, Structured Datasets",
        "Origin of $\\alpha$-satellite repeat arrays from mitochondrial molecular\n  fossils -- sequential insertion, expansion, and evolution in the nuclear\n  genome",
        "TopoLa: A Universal Framework to Enhance Cell Representations for\n  Single-cell and Spatial Omics through Topology-encoded Latent Hyperbolic\n  Geometry",
        "GiantHunter: Accurate detection of giant virus in metagenomic data using\n  reinforcement-learning and Monte Carlo tree search",
        "Deep Learning-based Feature Discovery for Decoding Phenotypic Plasticity\n  in Pediatric High-Grade Gliomas Single-Cell Transcriptomics",
        "Bizard: A Community-Driven Platform for Accelerating and Enhancing\n  Biomedical Data Visualization",
        "Application of Single-cell Deep Learning in Elucidating the Mapping\n  Relationship Between Visceral and Body Surface Inflammatory Patterns",
        "Eukaryotes evade information storage-replication rate trade-off with\n  endosymbiont assistance leading to larger genomes",
        "Curated loci prime editing (cliPE) for accessible multiplexed assays of\n  variant effect (MAVEs)",
        "Genomic Analysis of Date Palm Fruit Size Traits and Identification of\n  Candidate Genes through GWAS",
        "Find Central Dogma Again: Leveraging Multilingual Transfer in Large\n  Language Models",
        "Transcriptome signature for the identification of bevacizumab responders\n  in ovarian cancer",
        "CoverM: Read alignment statistics for metagenomics",
        "The Relativity of Causal Knowledge",
        "Meta-Learning-Based People Counting and Localization Models Employing\n  CSI from Commodity WiFi NICs",
        "Resonance fluorescence spectra of a driven Kerr nonlinear resonator",
        "Semadeni derivative of Banach spaces and functions on nonmetrizable\n  rectangles",
        "Minimum numbers of Dehn colors of knots and $\\mathcal{R}$-palette graphs",
        "The Third Generation of Nanogenerators: The Irreplaceable Potential\n  Source Enabled by the Flexoelectric Nanogenerator",
        "Simplicial effects and weakly associative partial groups",
        "Vectorial Kato inequality for $p$-harmonic maps with optimal constant",
        "Uniform-in-time error estimate of random batch method with replacement\n  for the Cucker-Smale model",
        "Electrical Control of the Exchange Bias Effect at\n  Ferromagnet-Altermagnet Junctions",
        "Hybrid Quantum-Classical Optimisation of Traveling Salesperson Problem",
        "Limited attention and models of choice: A behavioral equivalence",
        "A Pathwise Coordinate Descent Algorithm for LASSO Penalized Quantile\n  Regression",
        "Precompactness in bivariate metric semigroup-valued bounded variation\n  spaces",
        "Thomas-Wigner rotation via Clifford algebras"
      ],
      "abstract":[
        "Genotype-to-Phenotype (G2P) prediction plays a pivotal role in crop breeding,\nenabling the identification of superior genotypes based on genomic data. Rice\n(Oryza sativa), one of the most important staple crops, faces challenges in\nimproving yield and resilience due to the complex genetic architecture of\nagronomic traits and the limited sample size in breeding datasets. Current G2P\nprediction methods, such as GWAS and linear models, often fail to capture\ncomplex non-linear relationships between genotypes and phenotypes, leading to\nsuboptimal prediction accuracy. Additionally, population stratification and\noverfitting are significant obstacles when models are applied to small datasets\nwith diverse genetic backgrounds. This study introduces the Learnable Group\nTransform (LGT) method, which aims to overcome these challenges by combining\nthe advantages of traditional linear models with advanced machine learning\ntechniques. LGT utilizes a group-based transformation of genotype data to\ncapture spatial relationships and genetic structures across diverse rice\npopulations, offering flexibility to generalize even with limited data. Through\nextensive experiments on the Rice529 dataset, a panel of 529 rice accessions,\nLGT demonstrated substantial improvements in prediction accuracy for multiple\nagronomic traits, including yield and plant height, compared to\nstate-of-the-art baselines such as linear models and recent deep learning\napproaches. Notably, LGT achieved an R^2 improvement of up to 15\\% for yield\nprediction, significantly reducing error and demonstrating its ability to\nextract meaningful signals from high-dimensional, noisy genomic data. These\nresults highlight the potential of LGT as a powerful tool for genomic\nprediction in rice breeding, offering a promising solution for accelerating the\nidentification of high-yielding and resilient rice varieties.",
        "Alpha satellite DNA is large tandem arrays of 150-400 bp units, and its\norigin remains an evolutionary mystery. In this research, we identified 1,545\nalpha-satellite-like (SatL) repeat units in the nuclear genome of jewel wasp\nNasonia vitripennis. Among them, thirty-nine copies of SatL were organized in\ntwo palindromic arrays in mitochondria, resulting in a 50% increase in the\ngenome size. Strikingly, genomic neighborhood analyses of 1,516 nuclear SatL\nrepeats revealed that they are located in NuMT (nuclear mitochondrial DNA)\nregions, and SatL phylogeny matched perfectly with mitochondrial genes and NuMT\npseudogenes. These results support that SatL arrays originated from ten\nindependent mitochondria insertion events into the nuclear genome within the\nlast 500,000 years, after divergence from its sister species N. giraulti.\nDramatic repeat GC-percent elevation (from 33.9% to 50.4%) is a hallmark of\nrapid SatL sequence evolution in mitochondria due to GC-biased gene conversion\nfacilitated by the palindromic sequence pairing of the two mitochondrial SatL\narrays. The nuclear SatL repeat arrays underwent substantial copy number\nexpansion, from 12-15 (SatL1) to over 400 copies (SatL4). The oldest SatL4B\narray consists of four types of repeat units derived from deletions in the\nAT-rich region of ancestral repeats, and complex high-order structures have\nevolved through duplications. We also discovered similar repeat insertions into\nthe nuclear genome of Muscidifurax, suggesting this mechanism can be common in\ninsects. This is the first report of the mitochondrial origin of nuclear\nsatellite sequences, and our findings shed new light on the origin and\nevolution of satellite DNA.",
        "Recent advances in cellular research demonstrate that scRNA-seq characterizes\ncellular heterogeneity, while spatial transcriptomics reveals the spatial\ndistribution of gene expression. Cell representation is the fundamental issue\nin the two fields. Here, we propose Topology-encoded Latent Hyperbolic Geometry\n(TopoLa), a computational framework enhancing cell representations by capturing\nfine-grained intercellular topological relationships. The framework introduces\na new metric, TopoLa distance (TLd), which quantifies the geometric distance\nbetween cells within latent hyperbolic space, capturing the network's\ntopological structure more effectively. With this framework, the cell\nrepresentation can be enhanced considerably by performing convolution on its\nneighboring cells. Performance evaluation across seven biological tasks,\nincluding scRNA-seq data clustering and spatial transcriptomics domain\nidentification, shows that TopoLa significantly improves the performance of\nseveral state-of-the-art models. These results underscore the generalizability\nand robustness of TopoLa, establishing it as a valuable tool for advancing both\nbiological discovery and computational methodologies.",
        "Motivation: Nucleocytoplasmic large DNA viruses (NCLDVs) are notable for\ntheir large genomes and extensive gene repertoires, which contribute to their\nwidespread environmental presence and critical roles in processes such as host\nmetabolic reprogramming and nutrient cycling. Metagenomic sequencing has\nemerged as a powerful tool for uncovering novel NCLDVs in environmental\nsamples. However, identifying NCLDV sequences in metagenomic data remains\nchallenging due to their high genomic diversity, limited reference genomes, and\nshared regions with other microbes. Existing alignment-based and machine\nlearning methods struggle with achieving optimal trade-offs between sensitivity\nand precision. Results: In this work, we present GiantHunter, a reinforcement\nlearning-based tool for identifying NCLDVs from metagenomic data. By employing\na Monte Carlo tree search strategy, GiantHunter dynamically selects\nrepresentative non-NCLDV sequences as the negative training data, enabling the\nmodel to establish a robust decision boundary. Benchmarking on rigorously\ndesigned experiments shows that GiantHunter achieves high precision while\nmaintaining competitive sensitivity, improving the F1-score by 10% and reducing\ncomputational cost by 90% compared to the second-best method. To demonstrate\nits real-world utility, we applied GiantHunter to 60 metagenomic datasets\ncollected from six cities along the Yangtze River, located both upstream and\ndownstream of the Three Gorges Dam. The results reveal significant differences\nin NCLDV diversity correlated with proximity to the dam, likely influenced by\nreduced flow velocity caused by the dam. These findings highlight the potential\nof GiantSeeker to advance our understanding of NCLDVs and their ecological\nroles in diverse environments.",
        "By use of complex network dynamics and graph-based machine learning, we\nidentified critical determinants of lineage-specific plasticity across the\nsingle-cell transcriptomics of pediatric high-grade glioma (pHGGs) subtypes:\nIDHWT glioblastoma and K27M-mutant glioma. Our study identified network\ninteractions regulating glioma morphogenesis via the tumor-immune\nmicroenvironment, including neurodevelopmental programs, calcium dynamics, iron\nmetabolism, metabolic reprogramming, and feedback loops between MAPK\/ERK and\nWNT signaling. These relationships highlight the emergence of a hybrid spectrum\nof cellular states navigating a disrupted neuro-differentiation hierarchy. We\nidentified transition genes such as DKK3, NOTCH2, GATAD1, GFAP, and SEZ6L in\nIDHWT glioblastoma, and H3F3A, ANXA6, HES6\/7, SIRT2, FXYD6, PTPRZ1, MEIS1,\nCXXC5, and NDUFAB1 in K27M subtypes. We also identified MTRNR2L1, GAPDH, IGF2,\nFKBP variants, and FXYD7 as transition genes that influence cell fate\ndecision-making across both subsystems. Our findings suggest pHGGs are\ndevelopmentally trapped in states exhibiting maladaptive behaviors, and hybrid\ncellular identities. In effect, tumor heterogeneity (metastability) and\nplasticity emerge as stress-response patterns to immune-inflammatory\nmicroenvironments and oxidative stress. Furthermore, we show that pHGGs are\nsteered by developmental trajectories from radial glia predominantly favoring\nneocortical cell fates, in telencephalon and prefrontal cortex (PFC)\ndifferentiation. By addressing underlying patterning processes and plasticity\nnetworks as therapeutic vulnerabilities, our findings provide precision\nmedicine strategies aimed at modulating glioma cell fates and overcoming\ntherapeutic resistance. We suggest transition therapy toward neuronal-like\nlineage differentiation as a potential therapy to help stabilize pHGG\nplasticity and aggressivity.",
        "Bizard is a novel visualization code repository designed to simplify data\nanalysis in biomedical research. It integrates diverse visualization codes,\nfacilitating the selection and customization of optimal visualization methods\nfor specific research needs. The platform offers a user-friendly interface with\nadvanced browsing and filtering mechanisms, comprehensive tutorials, and\ninteractive forums to enhance knowledge exchange and innovation. Bizard's\ncollaborative model encourages continuous refinement and expansion of its\nfunctionalities, making it an indispensable tool for advancing biomedical data\nvisualization and analytical methodologies. By leveraging Bizard's resources,\nresearchers can enhance data visualization skills, drive methodological\nadvancements, and improve data interpretation standards, ultimately fostering\nthe development of precision medicine and personalized therapeutic\ninterventions.Bizard can be accessed from http:\/\/genaimed.org\/Bizard\/.",
        "As a system of integrated homeostasis, life is susceptible to disruptions by\nvisceral inflammation, which can disturb internal environment equilibrium. The\nrole of body-spread subcutaneous fascia (scFascia) in this process is poorly\nunderstood. In the rat model of Salmonella-induced dysentery, scRNA-seq of\nscFascia and deep-learning analysis revealed Warburg-like metabolic\nreprogramming in macrophages (MPs) with reduced citrate cycle activity.\nCd34+\/Pdgfra+ telocytes (CPTCs) regulated MPs differentiation and proliferation\nvia Wnt\/Fgf signal, suggesting a pathological crosstalk pattern in the\nscFascia, herein termed the fascia-visceral inflammatory crosstalk pattern\n(FVICP). PySCENIC analysis indicated increased activity transcription factors\nFosl1, Nfkb2, and Atf4, modulated by CPTCs signaling to MPs, downregulating\naerobic respiration and upregulating cell cycle, DNA replication, and\ntranscription. This study highlights scFascia's role in immunomodulation and\nmetabolic reprogramming during visceral inflammation, underscoring its function\nin systemic homeostasis.",
        "Genome length varies widely among organisms, from compact genomes of\nprokaryotes to vast and complex genomes of eukaryotes. In this study, we\ntheoretically identify the evolutionary pressures that may have driven this\ndivergence in genome length. We use a parameter-free model to study genome\nlength evolution under selection pressure to minimize replication time and\nmaximize information storage capacity. We show that prokaryotes tend to reduce\ngenome length, constrained by a single replication origin, while eukaryotes\nexpand their genomes by incorporating multiple replication origins. We propose\na connection between genome length and cellular energetics, suggesting that\nendosymbiotic organelles, mitochondria and chloroplasts, evolutionarily\nregulate the number of replication origins, thereby influencing genome length\nin eukaryotes. We show that the above two selection pressures also lead to\nstrict equalization of the number of purines and their corresponding\nbase-pairing pyrimidines within a single DNA strand, known as Chagraff's second\nparity rule, a hitherto unexplained observation in genomes of nearly all known\nspecies. This arises from the symmetrization of replichore length, another\nobservation that has been shown to hold across species, which our model\nreproduces. The model also reproduces other experimentally observed phenomena,\nsuch as a general preference for deletions over insertions, and elongation and\nhigh variance of genome lengths under reduced selection pressure for\nreplication rate, termed the C-value paradox. We highlight the possibility of\nregulation of the firing of latent replication origins in response to cues from\nthe extracellular environment leading to the regulation of cell cycle rates in\nmulticellular eukaryotes.",
        "Multiplexed assays of variant effect (MAVEs) perform simultaneous\ncharacterization of many variants. Prime editing has been recently adopted for\nintroducing many variants in their native genomic contexts. However, robust\nprotocols and standards are limited, preventing widespread uptake. Herein, we\ndescribe curated loci prime editing (cliPE) which is an accessible, low-cost\nexperimental pipeline to perform MAVEs using prime editing of a target gene, as\nwell as a companion Shiny app (pegRNA Designer) to rapidly and easily design\nuser-specific MAVE libraries.",
        "The commercial value of economically significant fruits, including date palm\nfruit (dates), is influenced by various factors, such as biochemical\ncomposition and morphological features like size, shape, and visual appearance,\nwhich are key determinants of their quality and market value. Dates are\ntypically consumed at the dry stage (Tamar), during which they exhibit a wide\nrange of physical characteristics, such as color, length, weight, and skin\nappearance. Understanding the genetic basis of these traits is crucial for\nimproving crop quality and breeding new cultivars. In this study, we integrated\na genome dataset from highly diverse date cultivars with phenotypes of dry\nfruit such as length, width, area, and weight, identifying multiple significant\ngenetic loci (SNPs) associated with these traits. We also identified candidate\ngenes located near the associated SNPs that are involved in biological\nprocesses such as cell differentiation, proliferation, growth, and the\nregulation of signalling pathways for growth regulators like auxin and abscisic\nacid, as observed in other plants. Gene expression analysis reveals that many\nof these genes are highly expressed in the early stage of fruit development\nwhen the fruit attains its maximum size and weight. These findings will enhance\nour understanding of genetic determinants of fruit size particularly at the\ncommercially important Tamar stage.",
        "In recent years, large language models (LLMs) have achieved state-of-the-art\nresults in various biological sequence analysis tasks, such as sequence\nclassification, structure prediction, and function prediction. Similar to\nadvancements in AI for other scientific fields, deeper research into biological\nLLMs has begun to focus on using these models to rediscover important existing\nbiological laws or uncover entirely new patterns in biological sequences. This\nstudy leverages GPT-like LLMs to utilize language transfer capabilities to\nrediscover the genetic code rules of the central dogma. In our experimental\ndesign, we transformed the central dogma into a binary classification problem\nof aligning DNA sequences with protein sequences, where positive examples are\nmatching DNA and protein sequences, and negative examples are non-matching\npairs. We first trained a GPT-2 model from scratch using a dataset comprising\nprotein sequences, DNA sequences, and sequences from languages such as English\nand Chinese. Subsequently, we fine-tuned the model using the natural language\nsentences similarity judgment dataset from PAWS-X. When tested on a dataset for\nDNA and protein sequence alignment judgment, the fine-tuned model achieved a\nclassification accuracy of 81%. The study also analyzed factors contributing to\nthis zero-shot capability, including model training stability and types of\ntraining data. This research demonstrates that LLMs can, through the transfer\nof natural language capabilities and solely relying on the analysis of\nsequences themselves, rediscover the central dogma without prior knowledge of\nit. This study bridges natural language and genetic language, opening a new\ndoor for AI-driven biological research.",
        "The standard of care for ovarian cancer comprises cytoreductive surgery,\nfollowed by adjuvant platinum-based chemotherapy plus taxane therapy and\nmaintenance therapy with the antiangiogenic compound bevacizumab and\/or a PARP\ninhibitor. Nevertheless, there is currently no clear clinical indication for\nthe use of bevacizumab, highlighting the urgent need for biomarkers to assess\nthe response to bevacizumab. In the present study, based on a novel RNA-seq\ndataset (n=181) and a previously published microarray-based dataset (n=377), we\nhave identified an expression signature potentially associated with benefit\nfrom bevacizumab addition and assumed to reflect cancer stemness acquisition\ndriven by activation of CTCFL. Patients with this signature demonstrated\nimproved overall survival when bevacizumab was added to standard chemotherapy\nin both novel (HR=0.41(0.23-0.74), adj.p-value=7.70e-03) and previously\npublished cohorts (HR=0.51(0.34-0.75), adj.p-value=3.25e-03), while no\nsignificant differences in survival explained by treatment were observed in\npatients negative for this signature. In addition to the CTCFL signature, we\nfound several other reproducible expression signatures which may also represent\nbiomarker candidates not related to established molecular subtypes of ovarian\ncancer and require further validation studies based on additional RNA-seq data.",
        "Genome-centric analysis of metagenomic samples is a powerful method for\nunderstanding the function of microbial communities. Calculating read coverage\nis a central part of analysis, enabling differential coverage binning for\nrecovery of genomes and estimation of microbial community composition. Coverage\nis determined by processing read alignments to reference sequences of either\ncontigs or genomes. Per-reference coverage is typically calculated in an ad-hoc\nmanner, with each software package providing its own implementation and\nspecific definition of coverage. Here we present a unified software package\nCoverM which calculates several coverage statistics for contigs and genomes in\nan ergonomic and flexible manner. It uses 'Mosdepth arrays' for computational\nefficiency and avoids unnecessary I\/O overhead by calculating coverage\nstatistics from streamed read alignment results. CoverM is free software\navailable at https:\/\/github.com\/wwood\/coverm. CoverM is implemented in Rust,\nwith Python (https:\/\/github.com\/apcamargo\/pycoverm) and Julia\n(https:\/\/github.com\/JuliaBinaryWrappers\/CoverM_jll.jl) interfaces.",
        "Recent advances in artificial intelligence reveal the limits of purely\npredictive systems and call for a shift toward causal and collaborative\nreasoning. Drawing inspiration from the revolution of Grothendieck in\nmathematics, we introduce the relativity of causal knowledge, which posits\nstructural causal models (SCMs) are inherently imperfect, subjective\nrepresentations embedded within networks of relationships. By leveraging\ncategory theory, we arrange SCMs into a functor category and show that their\nobservational and interventional probability measures naturally form convex\nstructures. This result allows us to encode non-intervened SCMs with convex\nspaces of probability measures. Next, using sheaf theory, we construct the\nnetwork sheaf and cosheaf of causal knowledge. These structures enable the\ntransfer of causal knowledge across the network while incorporating\ninterventional consistency and the perspective of the subjects, ultimately\nleading to the formal, mathematical definition of relative causal knowledge.",
        "In this paper, we consider people counting and localization systems\nexploiting channel state information (CSI) measured from commodity WiFi network\ninterface cards (NICs). While CSI has useful information of amplitude and phase\nto describe signal propagation in a measurement environment of interest, CSI\nmeasurement suffers from offsets due to various uncertainties. Moreover, an\nuncontrollable external environment where other WiFi devices communicate each\nother induces interfering signals, resulting in erroneous CSI captured at a\nreceiver. In this paper, preprocessing of CSI is first proposed for offset\nremoval, and it guarantees low-latency operation without any filtering process.\nAfterwards, we design people counting and localization models based on\npre-training. To be adaptive to different measurement environments,\nmeta-learning-based people counting and localization models are also proposed.\nNumerical results show that the proposed meta-learning-based people counting\nand localization models can achieve high sensing accuracy, compared to other\nlearning schemes that follow simple training and test procedures.",
        "Resonance fluorescence spectra of a driven Kerr nonlinear resonator is\ninvestigated both theoretically and experimentally. When the Kerr nonlinear\nresonator is driven strongly such that the induced Rabi frequency is comparable\nto or larger than the Kerr nonlinearity, the system cannot be approximated as a\ntwo-level system. We theoretically derive characteristic features in the\nfluorescence spectra such as the decrease of the center-peak intensity and the\nasymmetric sideband peaks in the presence of finite dephasing. Those features\nare consistently explained by the population of the initial dressed state and\nits transition matrix element to the final dressed state of the transition\ncorresponding to each peak. Finally, we experimentally measure the resonance\nfluorescence spectra of a driven superconducting Kerr nonlinear resonator and\nfind a quantitative agreement with our theory.",
        "We study Banach spaces $C(K)$ of real-valued continuous functions from the\nfinite product of compact lines. It turns out that the topological character of\nthese compact lines can be used to distinguish whether two spaces of continuous\nfunctions on products are isomorphic or embeddable to each other. In\nparticular, for compact lines $K_1, \\dots, K_n, L_1, \\dots, L_k$ of uncountable\ncharacter and $k \\neq n$, we claim that Banach spaces $C(\\prod_{i=1}^n K_i)$\nand $C(\\prod_{j=1}^k L_j)$ are not isomorphic.",
        "In this paper, we consider minimum numbers of colors of knots for Dehn\ncolorings. In particular, we will show that for any odd prime number $p$ and\nany Dehn $p$-colorable knot $K$, the minimum number of colors for $K$ is at\nleast $\\lfloor \\log_2 p \\rfloor +2$. Moreover, we will define the $\\R$-palette\ngraph for a set of colors. The $\\R$-palette graphs are quite useful to give\ncandidates of sets of colors which might realize a nontrivially Dehn\n$p$-colored diagram. In Appendix, we also prove that for Dehn $5$-colorable\nknot, the minimum number of colors is $4$.",
        "The electroneutrality assumption has long been adopted by scholars; however,\nthis assumption may lead to an oversight of certain physical effects. Using\nderivations from a discontinuous medium, we have obtained an expression for the\npotential and energy of a many-body unipolar charge system, which corresponds\nwell to its counterpart in a continuous medium. The compressed form of this\nexpression suggests that compressing a macroscale charged body to the nanoscale\ncan yield an enormous electric potential and energy, thereby establishing a\nconcrete research framework for third-generation nanogenerators. This effect\nmay serve as a crucial reference for understanding anomalous spatial\nelectromagnetic distributions and divergent energy fields.",
        "In this paper, we introduce a new category of simplicial effects that extends\nthe categories of effect algebras and their multi-object counterpart, effect\nalgebroids. Our approach is based on relaxing the associativity condition\nsatisfied by effect algebras and, more generally, partial monoids. Within this\nframework, simplicial effects and weakly associative partial groups arise as\ntwo extreme cases in the category of weak partial monoids. Our motivation is to\ncapture simplicial structures from the theory of simplicial distributions and\nmeasurements that behave like effects.",
        "We derive the sharp vectorial Kato inequality for $p$-harmonic mappings.\nSurprisingly, the optimal constant differs from the one obtained for scalar\nvalued $p$-harmonic functions by Chang, Chen, and Wei. As an application we\ndemonstrate how this inequality can be used in the study of regularity of\n$p$-harmonic maps. Furthermore, in the case of $p$-harmonic maps from $B^3$ to\n$\\mathbb{S}^3$, we enhance the known range of $p$ values for which regularity\nis achieved. Specifically, we establish that for $p \\in [2, 2.642]$, minimizing\n$p$-harmonic maps must be regular.",
        "The Random Batch Method (RBM), proposed by Jin et al. in 2020, is an\nefficient algorithm for simulating interacting particle systems. The\nuniform-in-time error estimates of the RBM without replacement have been\nobtained for various interacting particle systems, while the analysis of the\nRBM with replacement is just considered in (Cai et al., 2024) recently for the\nfirst-order systems governed by Langevin dynamics. In this work, we present the\nerror estimate for the RBM with replacement applied to a second-order system\nknown as the Cucker-Smale model. By introducing a crucial auxiliary system and\nleveraging the intrinsic characteristics of the Cucker-Smale model, we derive\nan estimate that is uniform in both time and particle numbers. Additionally, we\nprovide numerical simulations to validate the analytical results.",
        "This work analyzes the behavior of the interface between a ferromagnetic\nmaterial and an alter-magnet. We use a well-established line of arguments based\non electronic mean-field calculations to show that new surface phenomena that\nlead to altermagnetic materials induce an exchange bias effect on the nearby\nferromagnet. We reveal the physical mechanisms behind this phenomenon that lead\nto quantitative control over its strength. Interestingly, we predict exotic\nelectric-field-induced phenomena. This is an analogy to the relationship\nbetween exchange bias and the injection of spin currents in\nspin-transfer-dominated scenarios, which has been reported earlier in the\ntraditional antiferromagnetic\/ferromagnetic junction.",
        "The Traveling Salesperson Problem (TSP) is a fundamental NP-hard optimisation\nchallenge with widespread applications in logistics, operations research, and\nnetwork design. While classical algorithms effectively solve small to\nmedium-sized instances, they struggle with scalability due to exponential\ncomplexity. In this work, we present a hybrid quantum-classical approach that\nleverages IBM's Qiskit Runtime to integrate quantum optimisation techniques\nwith classical machine learning methods, specifically K-Means clustering and\nRandom Forest classifiers. These machine learning components aid in problem\ndecomposition and noise mitigation, improving the quality of quantum solutions.\nExperimental results for TSP instances ranging from 4 to 8 cities reveal that\nthe quantum-only approach produces solutions up to 21.7% worse than the\nclassical baseline, while the hybrid method reduces this cost increase to 11.3%\nfor 8 cities. This demonstrates that hybrid approaches improve solution quality\ncompared to purely quantum methods but remain suboptimal compared to classical\nsolvers. Despite current hardware limitations, these results highlight the\npotential of quantum-enhanced methods for combinatorial optimisation, paving\nthe way for future advancements in scalable quantum computing frameworks.",
        "We show that many models of choice can be alternatively represented as\nspecial cases of choice with limited attention (Masatlioglu, Nakajima, and\nOzbay, 2012), and the properties of the unobserved attention filters that\nexplain the observed choices are singled out. Moreover, for each specification,\nwe infer information about the DM's attention and preference from irrational\nfeatures of choice data.",
        "$\\ell_1$ penalized quantile regression is used in many fields as an\nalternative to penalized least squares regressions for high-dimensional data\nanalysis. Existing algorithms for penalized quantile regression either use\nlinear programming, which does not scale well in high dimension, or an\napproximate coordinate descent (CD) which does not solve for exact\ncoordinatewise minimum of the nonsmooth loss function. Further, neither\napproaches build fast, pathwise algorithms commonly used in high-dimensional\nstatistics to leverage sparsity structure of the problem in large-scale data\nsets. To avoid the computational challenges associated with the nonsmooth\nquantile loss, some recent works have even advocated using smooth\napproximations to the exact problem. In this work, we develop a fast, pathwise\ncoordinate descent algorithm to compute exact $\\ell_1$ penalized quantile\nregression estimates for high-dimensional data. We derive an easy-to-compute\nexact solution for the coordinatewise nonsmooth loss minimization, which, to\nthe best of our knowledge, has not been reported in the literature. We also\nemploy a random perturbation strategy to help the algorithm avoid getting stuck\nalong the regularization path. In simulated data sets, we show that our\nalgorithm runs substantially faster than existing alternatives based on\napproximate CD and linear program, while retaining the same level of estimation\naccuracy.",
        "In this paper, we show that if a set in bivariate metric semigroups-valued\nbounded variation spaces is pointwise totally bounded and joint equivariated\nthen it is precompact. These spaces include bounded Jordan variation spaces,\nbounded Wiener variation spaces, bounded Waterman variation spaces, bounded\nRiesz variation spaces and bounded Korenblum variation spaces. To do so, we\nintroduce the concept of equimetric set.",
        "We derive Macfarlane's formula for the Thomas-Wigner angle of rotation using\nClifford-algebra methods. The presentation is pedagogical and elementary,\nsuitable for students with some basic knowledge of special relativity; no prior\nknowledge of Clifford algebras is required."
      ]
    }
  },
  {
    "id":2411.03341,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"Highly multiplexed imaging of tumor tissues with subcellular resolution by mass cytometry",
    "start_abstract":"Mass cytometry enables high-dimensional, single-cell analysis of cell type and state. In mass cytometry, rare earth metals are used as reporters on antibodies. Analysis of metal abundances using the mass cytometer allows determination of marker expression in individual cells. Mass cytometry has previously been applied only to cell suspensions. To gain spatial information, we have coupled immunohistochemical and immunocytochemical methods with high-resolution laser ablation to CyTOF mass cytometry. This approach enables the simultaneous imaging of 32 proteins and protein modifications at subcellular resolution; with the availability of additional isotopes, measurement of over 100 markers will be possible. We applied imaging mass cytometry to human breast cancer samples, allowing delineation of cell subpopulations and cell-cell interactions and highlighting tumor heterogeneity. Imaging mass cytometry complements existing imaging approaches. It will enable basic studies of tissue heterogeneity and function and support the transition of medicine toward individualized molecularly targeted diagnosis and therapies.",
    "start_categories":[
      "q-bio.QM"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b11"
      ],
      "title":[
        "A ConvNet for the 2020s"
      ],
      "abstract":[
        "The \"Roaring 20s\" of visual recognition began with the introduction Vision Transformers (ViTs), which quickly superseded ConvNets as state-of-the-art image classification model. A vanilla ViT, on other hand, faces difficulties when applied to general computer vision tasks such object detection and semantic segmentation. It is hierarchical (e.g., Swin Transformers) that reintroduced several ConvNet priors, making practically viable a generic backbone demonstrating remarkable performance wide variety tasks. However, effectiveness hybrid approaches still largely credited intrinsic superiority Transformers, rather than inherent inductive biases convolutions. In this work, we reexamine design spaces test limits what pure can achieve. We gradually \"modernize\" standard ResNet toward Transformer, discover key components contribute difference along way. outcome exploration family models dubbed ConvNeXt. Constructed entirely from modules, ConvNeXts compete favorably in terms accuracy scalability, achieving 87.8% ImageNet top-1 outperforming COCO ADE20K segmentation, while maintaining simplicity efficiency ConvNets."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "Hierarchical Alignment-enhanced Adaptive Grounding Network for\n  Generalized Referring Expression Comprehension",
        "Can Generative Geospatial Diffusion Models Excel as Discriminative\n  Geospatial Foundation Models?",
        "UltraFusion: Ultra High Dynamic Imaging using Exposure Fusion",
        "Learning Few-Step Diffusion Models by Trajectory Distribution Matching",
        "Geometric Mean Improves Loss For Few-Shot Learning",
        "SurgicalVLM-Agent: Towards an Interactive AI Co-Pilot for Pituitary\n  Surgery",
        "The Devil is in the Details: Simple Remedies for Image-to-LiDAR\n  Representation Learning",
        "V2X-DG: Domain Generalization for Vehicle-to-Everything Cooperative\n  Perception",
        "Pointmap Association and Piecewise-Plane Constraint for Consistent and\n  Compact 3D Gaussian Segmentation Field",
        "Sequence models for continuous cell cycle stage prediction from\n  brightfield images",
        "Benchmarking Multimodal Models for Fine-Grained Image Analysis: A\n  Comparative Study Across Diverse Visual Features",
        "End-to-End HOI Reconstruction Transformer with Graph-based Encoding",
        "Towards a Unified Copernicus Foundation Model for Earth Vision",
        "Intrinsic Donaldson-Thomas theory. I. Component lattices of stacks",
        "Ferromagnetic Resonance in a Magnetically Dilute Percolating\n  Ferromagnet: An Experimental and Theoretical Study",
        "Towards Enterprise-Ready Computer Using Generalist Agent",
        "Maps from Grassmannians of 2-planes to projective spaces",
        "Approaching the Limits to EFL Writing Enhancement with AI-generated Text\n  and Diverse Learners",
        "Sensitivity of Double Deeply Virtual Compton Scattering observables to\n  Generalized Parton Distributions",
        "The EnviroMapper Toolkit: an Input Physicalisation that Captures the\n  Situated Experience of Environmental Comfort in Offices",
        "VIRGOS: Secure Graph Convolutional Network on Vertically Split Data from\n  Sparse Matrix Decomposition",
        "Shifting Power: Leveraging LLMs to Simulate Human Aversion in ABMs of\n  Bilateral Financial Exchanges, A bond market study",
        "Post-Quantum Stealth Address Protocols",
        "MinD the gap: Membrane proteins form 3D patterns in a suspension of\n  liposomes",
        "Causes and Strategies in Multiagent Systems",
        "Hybrid MIMO in the Upper Mid-Band: Architectures, Processing, and Energy\n  Efficiency",
        "Unit Edge-Length Rectilinear Drawings with Crossings and Rectangular\n  Faces",
        "DBRouting: Routing End User Queries to Databases for Answerability"
      ],
      "abstract":[
        "In this work, we address the challenging task of Generalized Referring\nExpression Comprehension (GREC). Compared to the classic Referring Expression\nComprehension (REC) that focuses on single-target expressions, GREC extends the\nscope to a more practical setting by further encompassing no-target and\nmulti-target expressions. Existing REC methods face challenges in handling the\ncomplex cases encountered in GREC, primarily due to their fixed output and\nlimitations in multi-modal representations. To address these issues, we propose\na Hierarchical Alignment-enhanced Adaptive Grounding Network (HieA2G) for GREC,\nwhich can flexibly deal with various types of referring expressions. First, a\nHierarchical Multi-modal Semantic Alignment (HMSA) module is proposed to\nincorporate three levels of alignments, including word-object, phrase-object,\nand text-image alignment. It enables hierarchical cross-modal interactions\nacross multiple levels to achieve comprehensive and robust multi-modal\nunderstanding, greatly enhancing grounding ability for complex cases. Then, to\naddress the varying number of target objects in GREC, we introduce an Adaptive\nGrounding Counter (AGC) to dynamically determine the number of output targets.\nAdditionally, an auxiliary contrastive loss is employed in AGC to enhance\nobject-counting ability by pulling in multi-modal features with the same\ncounting and pushing away those with different counting. Extensive experimental\nresults show that HieA2G achieves new state-of-the-art performance on the\nchallenging GREC task and also the other 4 tasks, including REC, Phrase\nGrounding, Referring Expression Segmentation (RES), and Generalized Referring\nExpression Segmentation (GRES), demonstrating the remarkable superiority and\ngeneralizability of the proposed HieA2G.",
        "Self-supervised learning (SSL) has revolutionized representation learning in\nRemote Sensing (RS), advancing Geospatial Foundation Models (GFMs) to leverage\nvast unlabeled satellite imagery for diverse downstream tasks. Currently, GFMs\nprimarily focus on discriminative objectives, such as contrastive learning or\nmasked image modeling, owing to their proven success in learning transferable\nrepresentations. However, generative diffusion models--which demonstrate the\npotential to capture multi-grained semantics essential for RS tasks during\nimage generation--remain underexplored for discriminative applications. This\nprompts the question: can generative diffusion models also excel and serve as\nGFMs with sufficient discriminative power? In this work, we answer this\nquestion with SatDiFuser, a framework that transforms a diffusion-based\ngenerative geospatial foundation model into a powerful pretraining tool for\ndiscriminative RS. By systematically analyzing multi-stage, noise-dependent\ndiffusion features, we develop three fusion strategies to effectively leverage\nthese diverse representations. Extensive experiments on remote sensing\nbenchmarks show that SatDiFuser outperforms state-of-the-art GFMs, achieving\ngains of up to +5.7% mIoU in semantic segmentation and +7.9% F1-score in\nclassification, demonstrating the capacity of diffusion-based generative\nfoundation models to rival or exceed discriminative GFMs. Code will be\nreleased.",
        "Capturing high dynamic range (HDR) scenes is one of the most important issues\nin camera design. Majority of cameras use exposure fusion technique, which\nfuses images captured by different exposure levels, to increase dynamic range.\nHowever, this approach can only handle images with limited exposure difference,\nnormally 3-4 stops. When applying to very high dynamic scenes where a large\nexposure difference is required, this approach often fails due to incorrect\nalignment or inconsistent lighting between inputs, or tone mapping artifacts.\nIn this work, we propose UltraFusion, the first exposure fusion technique that\ncan merge input with 9 stops differences. The key idea is that we model the\nexposure fusion as a guided inpainting problem, where the under-exposed image\nis used as a guidance to fill the missing information of over-exposed highlight\nin the over-exposed region. Using under-exposed image as a soft guidance,\ninstead of a hard constrain, our model is robust to potential alignment issue\nor lighting variations. Moreover, utilizing the image prior of the generative\nmodel, our model also generates natural tone mapping, even for very\nhigh-dynamic range scene. Our approach outperforms HDR-Transformer on latest\nHDR benchmarks. Moreover, to test its performance in ultra high dynamic range\nscene, we capture a new real-world exposure fusion benchmark, UltraFusion\nDataset, with exposure difference up to 9 stops, and experiments show that\n\\model~can generate beautiful and high-quality fusion results under various\nscenarios. An online demo is provided at\nhttps:\/\/openimaginglab.github.io\/UltraFusion\/.",
        "Accelerating diffusion model sampling is crucial for efficient AIGC\ndeployment. While diffusion distillation methods -- based on distribution\nmatching and trajectory matching -- reduce sampling to as few as one step, they\nfall short on complex tasks like text-to-image generation. Few-step generation\noffers a better balance between speed and quality, but existing approaches face\na persistent trade-off: distribution matching lacks flexibility for multi-step\nsampling, while trajectory matching often yields suboptimal image quality. To\nbridge this gap, we propose learning few-step diffusion models by Trajectory\nDistribution Matching (TDM), a unified distillation paradigm that combines the\nstrengths of distribution and trajectory matching. Our method introduces a\ndata-free score distillation objective, aligning the student's trajectory with\nthe teacher's at the distribution level. Further, we develop a\nsampling-steps-aware objective that decouples learning targets across different\nsteps, enabling more adjustable sampling. This approach supports both\ndeterministic sampling for superior image quality and flexible multi-step\nadaptation, achieving state-of-the-art performance with remarkable efficiency.\nOur model, TDM, outperforms existing methods on various backbones, such as SDXL\nand PixArt-$\\alpha$, delivering superior quality and significantly reduced\ntraining costs. In particular, our method distills PixArt-$\\alpha$ into a\n4-step generator that outperforms its teacher on real user preference at 1024\nresolution. This is accomplished with 500 iterations and 2 A800 hours -- a mere\n0.01% of the teacher's training cost. In addition, our proposed TDM can be\nextended to accelerate text-to-video diffusion. Notably, TDM can outperform its\nteacher model (CogVideoX-2B) by using only 4 NFE on VBench, improving the total\nscore from 80.91 to 81.65. Project page: https:\/\/tdm-t2x.github.io\/",
        "Few-shot learning (FSL) is a challenging task in machine learning, demanding\na model to render discriminative classification by using only a few labeled\nsamples. In the literature of FSL, deep models are trained in a manner of\nmetric learning to provide metric in a feature space which is well\ngeneralizable to classify samples of novel classes; in the space, even a few\namount of labeled training examples can construct an effective classifier. In\nthis paper, we propose a novel FSL loss based on \\emph{geometric mean} to embed\ndiscriminative metric into deep features. In contrast to the other losses such\nas utilizing arithmetic mean in softmax-based formulation, the proposed method\nleverages geometric mean to aggregate pair-wise relationships among samples for\nenhancing discriminative metric across class categories. The proposed loss is\nnot only formulated in a simple form but also is thoroughly analyzed in\ntheoretical ways to reveal its favorable characteristics which are favorable\nfor learning feature metric in FSL. In the experiments on few-shot image\nclassification tasks, the method produces competitive performance in comparison\nto the other losses.",
        "Image-guided surgery demands adaptive, real-time decision support, yet static\nAI models struggle with structured task planning and providing interactive\nguidance. Large vision-language models (VLMs) offer a promising solution by\nenabling dynamic task planning and predictive decision support. We introduce\nSurgicalVLM-Agent, an AI co-pilot for image-guided pituitary surgery, capable\nof conversation, planning, and task execution. The agent dynamically processes\nsurgeon queries and plans the tasks such as MRI tumor segmentation, endoscope\nanatomy segmentation, overlaying preoperative imaging with intraoperative\nviews, instrument tracking, and surgical visual question answering (VQA). To\nenable structured task planning, we develop the PitAgent dataset, a surgical\ncontext-aware dataset covering segmentation, overlaying, instrument\nlocalization, tool tracking, tool-tissue interactions, phase identification,\nand surgical activity recognition. Additionally, we propose FFT-GaLore, a fast\nFourier transform (FFT)-based gradient projection technique for efficient\nlow-rank adaptation, optimizing fine-tuning for LLaMA 3.2 in surgical\nenvironments. We validate SurgicalVLM-Agent by assessing task planning and\nprompt generation on our PitAgent dataset and evaluating zero-shot VQA using a\npublic pituitary dataset. Results demonstrate state-of-the-art performance in\ntask planning and query interpretation, with highly semantically meaningful VQA\nresponses, advancing AI-driven surgical assistance.",
        "LiDAR is a crucial sensor in autonomous driving, commonly used alongside\ncameras. By exploiting this camera-LiDAR setup and recent advances in image\nrepresentation learning, prior studies have shown the promising potential of\nimage-to-LiDAR distillation. These prior arts focus on the designs of their own\nlosses to effectively distill the pre-trained 2D image representations into a\n3D model. However, the other parts of the designs have been surprisingly\nunexplored. We find that fundamental design elements, e.g., the LiDAR\ncoordinate system, quantization according to the existing input interface, and\ndata utilization, are more critical than developing loss functions, which have\nbeen overlooked in prior works. In this work, we show that simple fixes to\nthese designs notably outperform existing methods by 16% in 3D semantic\nsegmentation on the nuScenes dataset and 13% in 3D object detection on the\nKITTI dataset in downstream task performance. We focus on overlooked design\nchoices along the spatial and temporal axes. Spatially, prior work has used\ncylindrical coordinate and voxel sizes without considering their side effects\nyielded with a commonly deployed sparse convolution layer input interface,\nleading to spatial quantization errors in 3D models. Temporally, existing work\nhas avoided cumbersome data curation by discarding unsynced data, limiting the\nuse to only the small portion of data that is temporally synced across sensors.\nWe analyze these effects and propose simple solutions for each overlooked\naspect.",
        "LiDAR-based Vehicle-to-Everything (V2X) cooperative perception has\ndemonstrated its impact on the safety and effectiveness of autonomous driving.\nSince current cooperative perception algorithms are trained and tested on the\nsame dataset, the generalization ability of cooperative perception systems\nremains underexplored. This paper is the first work to study the Domain\nGeneralization problem of LiDAR-based V2X cooperative perception (V2X-DG) for\n3D detection based on four widely-used open source datasets: OPV2V, V2XSet,\nV2V4Real and DAIR-V2X. Our research seeks to sustain high performance not only\nwithin the source domain but also across other unseen domains, achieved solely\nthrough training on source domain. To this end, we propose Cooperative Mixup\nAugmentation based Generalization (CMAG) to improve the model generalization\ncapability by simulating the unseen cooperation, which is designed compactly\nfor the domain gaps in cooperative perception. Furthermore, we propose a\nconstraint for the regularization of the robust generalized feature\nrepresentation learning: Cooperation Feature Consistency (CFC), which aligns\nthe intermediately fused features of the generalized cooperation by CMAG and\nthe early fused features of the original cooperation in source domain.\nExtensive experiments demonstrate that our approach achieves significant\nperformance gains when generalizing to other unseen datasets while it also\nmaintains strong performance on the source dataset.",
        "Achieving a consistent and compact 3D segmentation field is crucial for\nmaintaining semantic coherence across views and accurately representing scene\nstructures. Previous 3D scene segmentation methods rely on video segmentation\nmodels to address inconsistencies across views, but the absence of spatial\ninformation often leads to object misassociation when object temporarily\ndisappear and reappear. Furthermore, in the process of 3D scene reconstruction,\nsegmentation and optimization are often treated as separate tasks. As a result,\noptimization typically lacks awareness of semantic category information, which\ncan result in floaters with ambiguous segmentation. To address these\nchallenges, we introduce CCGS, a method designed to achieve both view\nconsistent 2D segmentation and a compact 3D Gaussian segmentation field. CCGS\nincorporates pointmap association and a piecewise-plane constraint. First, we\nestablish pixel correspondence between adjacent images by minimizing the\nEuclidean distance between their pointmaps. We then redefine object mask\noverlap accordingly. The Hungarian algorithm is employed to optimize mask\nassociation by minimizing the total matching cost, while allowing for partial\nmatches. To further enhance compactness, the piecewise-plane constraint\nrestricts point displacement within local planes during optimization, thereby\npreserving structural integrity. Experimental results on ScanNet and Replica\ndatasets demonstrate that CCGS outperforms existing methods in both 2D panoptic\nsegmentation and 3D Gaussian segmentation.",
        "Understanding cell cycle dynamics is crucial for studying biological\nprocesses such as growth, development and disease progression. While\nfluorescent protein reporters like the Fucci system allow live monitoring of\ncell cycle phases, they require genetic engineering and occupy additional\nfluorescence channels, limiting broader applicability in complex experiments.\nIn this study, we conduct a comprehensive evaluation of deep learning methods\nfor predicting continuous Fucci signals using non-fluorescence brightfield\nimaging, a widely available label-free modality. To that end, we generated a\nlarge dataset of 1.3 M images of dividing RPE1 cells with full cell cycle\ntrajectories to quantitatively compare the predictive performance of distinct\nmodel categories including single time-frame models, causal state space models\nand bidirectional transformer models. We show that both causal and\ntransformer-based models significantly outperform single- and fixed frame\napproaches, enabling the prediction of visually imperceptible transitions like\nG1\/S within 1h resolution. Our findings underscore the importance of sequence\nmodels for accurate predictions of cell cycle dynamics and highlight their\npotential for label-free imaging.",
        "This article introduces a benchmark designed to evaluate the capabilities of\nmultimodal models in analyzing and interpreting images. The benchmark focuses\non seven key visual aspects: main object, additional objects, background,\ndetail, dominant colors, style, and viewpoint. A dataset of 14,580 images,\ngenerated from diverse text prompts, was used to assess the performance of\nseven leading multimodal models. These models were evaluated on their ability\nto accurately identify and describe each visual aspect, providing insights into\ntheir strengths and weaknesses for comprehensive image understanding. The\nfindings of this benchmark have significant implications for the development\nand selection of multimodal models for various image analysis tasks.",
        "With the diversification of human-object interaction (HOI) applications and\nthe success of capturing human meshes, HOI reconstruction has gained widespread\nattention. Existing mainstream HOI reconstruction methods often rely on\nexplicitly modeling interactions between humans and objects. However, such a\nway leads to a natural conflict between 3D mesh reconstruction, which\nemphasizes global structure, and fine-grained contact reconstruction, which\nfocuses on local details. To address the limitations of explicit modeling, we\npropose the End-to-End HOI Reconstruction Transformer with Graph-based Encoding\n(HOI-TG). It implicitly learns the interaction between humans and objects by\nleveraging self-attention mechanisms. Within the transformer architecture, we\ndevise graph residual blocks to aggregate the topology among vertices of\ndifferent spatial structures. This dual focus effectively balances global and\nlocal representations. Without bells and whistles, HOI-TG achieves\nstate-of-the-art performance on BEHAVE and InterCap datasets. Particularly on\nthe challenging InterCap dataset, our method improves the reconstruction\nresults for human and object meshes by 8.9% and 8.6%, respectively.",
        "Advances in Earth observation (EO) foundation models have unlocked the\npotential of big satellite data to learn generic representations from space,\nbenefiting a wide range of downstream applications crucial to our planet.\nHowever, most existing efforts remain limited to fixed spectral sensors, focus\nsolely on the Earth's surface, and overlook valuable metadata beyond imagery.\nIn this work, we take a step towards next-generation EO foundation models with\nthree key components: 1) Copernicus-Pretrain, a massive-scale pretraining\ndataset that integrates 18.7M aligned images from all major Copernicus Sentinel\nmissions, spanning from the Earth's surface to its atmosphere; 2)\nCopernicus-FM, a unified foundation model capable of processing any spectral or\nnon-spectral sensor modality using extended dynamic hypernetworks and flexible\nmetadata encoding; and 3) Copernicus-Bench, a systematic evaluation benchmark\nwith 15 hierarchical downstream tasks ranging from preprocessing to specialized\napplications for each Sentinel mission. Our dataset, model, and benchmark\ngreatly improve the scalability, versatility, and multimodal adaptability of EO\nfoundation models, while also creating new opportunities to connect EO,\nweather, and climate research. Codes, datasets and models are available at\nhttps:\/\/github.com\/zhu-xlab\/Copernicus-FM.",
        "This is the first paper in a series on intrinsic Donaldson-Thomas theory, a\ngeneralization of Donaldson-Thomas theory from the linear case, or the case of\nmoduli stacks of objects in $3$-Calabi-Yau abelian categories, to the\nnon-linear case of general $(-1)$-shifted symplectic stacks. This is done by\ndeveloping a new framework for studying the enumerative geometry of general\nalgebraic stacks, and we expect that this framework can also be applied to\nextending other types of enumerative theories for linear stacks to the\nnon-linear case.\n  In this paper, we establish the foundations of our framework. We introduce\nthe component lattice of an algebraic stack, which is the key combinatorial\nobject in our theory. It generalizes and globalizes the cocharacter lattice and\nthe Weyl group of an algebraic group, and is defined as the set of connected\ncomponents of the stack of graded points of the original stack.\n  We prove several results on the structure of graded and filtered points of a\nstack using the component lattice. The first is the constancy theorem, which\nstates that there is a wall-and-chamber structure on the component lattice,\nsuch that the isomorphism types of connected components of the stacks of graded\nand filtered points stay constant within each chamber. The second is the\nfiniteness theorem, providing a criterion for the finiteness of the number of\npossible isomorphism types of these components. The third is the associativity\ntheorem, generalizing the structure of Hall algebras from linear stacks to\ngeneral stacks, involving a notion of Hall categories.\n  Finally, we discuss some applications of these results outside\nDonaldson-Thomas theory, including a construction of stacks of real-weighted\nfiltrations, and a generalization of the semistable reduction theorem to\nreal-weighted filtrations.",
        "Ferromagnetic resonance (FMR) serves as a powerful probe of magnetization\ndynamics and anisotropy in percolating ferromagnets, where short-range\ninteractions govern long-range magnetic order. We apply this approach to\nGa$_{1-x}$Mn$_x$N ($x \\simeq 8$), a dilute ferromagnetic semiconductor,\ncombining FMR and superconducting quantum interference device magnetometry. Our\nresults confirm the percolative nature of ferromagnetism in (Ga,Mn)N, with a\nCurie temperature $T_{\\mathrm{C}} = 12$ K, and reveal that despite magnetic\ndilution, key features of conventional ferromagnets are retained. FMR\nmeasurements establish a robust uniaxial anisotropy, dictated by Mn$^{3+}$\nsingle-ion anisotropy, with an easy-plane character at low Mn content. While\nexcessive line broadening suppresses FMR signals below 9 K, they persist up to\n70 K, indicating the presence of non-percolating ferromagnetic clusters well\nabove $T_{\\mathrm{C}}$. The temperature dependence of the FMR intensity follows\nthat of the magnetization, underscoring the stability of these clusters.\nAnalysis of the FMR linewidth provides insights into relaxation processes,\nrevealing large Gilbert damping due to the low magnetization of the system.\nStrikingly, atomistic spin model simulations reproduce the experimentally\nobserved resonance fields, anisotropy trends, and linewidth evolution with\nremarkable accuracy. This agreement underscores the predictive power of our\nmodeling approach in describing percolating ferromagnets. This study advances\nthe understanding of percolating ferromagnetic systems, demonstrating that FMR\nis a key technique for probing their unique dynamic and anisotropic properties.\nOur findings contribute to the broader exploration of dilute ferromagnets and\nprovide new insights into percolating ferromagnetic systems, which will be\nrelevant for spintronic opportunities.",
        "This paper presents our ongoing work toward developing an enterprise-ready\nComputer Using Generalist Agent (CUGA) system. Our research highlights the\nevolutionary nature of building agentic systems suitable for enterprise\nenvironments. By integrating state-of-the-art agentic AI techniques with a\nsystematic approach to iterative evaluation, analysis, and refinement, we have\nachieved rapid and cost-effective performance gains, notably reaching a new\nstate-of-the-art performance on the WebArena benchmark. We detail our\ndevelopment roadmap, the methodology and tools that facilitated rapid learning\nfrom failures and continuous system refinement, and discuss key lessons learned\nand future challenges for enterprise adoption.",
        "Using quaternions and octonions, we construct some maps from the Grassmannian\nof 2-dimensional planes of $\\mathbb{R}^n$, $\\mathrm{Gr}_2(\\mathbb{R}^n)$, to\nthe projective space $\\mathbb{R}\\mathrm{P}^k$, for certain values of $n$ and\n$k$. All of our maps induce an isomorphism at the level of fundamental groups,\nand two of them are shown to be submersions. As an application, we obtain new\nestimates of the Lusternik-Schnirelmann category of\n$\\mathrm{Gr}_2(\\mathbb{R}^n)$ for specific values of $n$.",
        "Generative artificial intelligence (AI) chatbots, such as ChatGPT, are\nreshaping how English as a foreign language (EFL) students write since students\ncan compose texts by integrating their own words with AI-generated text. This\nstudy investigated how 59 Hong Kong secondary school students with varying\nlevels of academic achievement interacted with AI-generated text to compose a\nfeature article, exploring whether any interaction patterns benefited the\noverall quality of the article. Through content analysis, multiple linear\nregression and cluster analysis, we found the overall number of words --\nwhether AI- or human-generated -- is the main predictor of writing quality.\nHowever, the impact varies by students' competence to write independently, for\ninstance, by using their own words accurately and coherently to compose a text,\nand to follow specific interaction patterns with AI-generated text. Therefore,\nalthough composing texts with human words and AI-generated text may become\nprevalent in EFL writing classrooms, without educators' careful attention to\nEFL writing pedagogy and AI literacy, high-achieving students stand to benefit\nmore from using AI-generated text than low-achieving students.",
        "Double Deeply Virtual Compton Scattering (DDVCS) is a promising channel for\nGeneralized Parton Distribution (GPD) studies as it is a generalization of the\nDeeply Virtual Compton Scattering (DVCS) and Timelike Compton Scattering (TCS)\nprocesses. Contrary to DVCS and TCS, the GPD phase space accessed through DDVCS\nis not constrained by on-shell conditions on the incoming and outgoing photons\nthus allowing unrestricted GPD extraction from experimental observables.\nConsidering polarized electron and positron beams directed to a polarized\nproton target, we study the sensitivity of the DDVCS cross-section asymmetries\nto the chiral-even proton GPDs from different model predictions. The\nfeasibility of such measurements is further investigated in the context of the\nCLAS and SoLID spectrometers at the Thomas Jefferson National Accelerator\nFacility and the future Electron-Ion Collider at the Brookhaven National\nLaboratory.",
        "The environmental comfort in offices is traditionally captured by surveying\nan entire workforce simultaneously, which yet fails to capture the situatedness\nof the different personal experiences. To address this limitation, we developed\nthe EnviroMapper Toolkit, a data physicalisation toolkit that allows individual\noffice workers to record their personal experiences of environmental comfort by\nmapping the actual moments and locations these occurred. By analysing two\nin-the-wild studies in existing open-plan office environments (N=14), we\ndemonstrate how this toolkit acts like a situated input visualisation that can\nbe interpreted by domain experts who were not present during its construction.\nThis study therefore offers four key contributions: (1) the iterative design\nprocess of the physicalisation toolkit; (2) its preliminary deployment in two\nreal-world office contexts; (3) the decoding of the resulting artefacts by\ndomain experts; and (4) design considerations to support future input\nphysicalisation and visualisation constructions that capture and synthesise\ndata from multiple individuals.",
        "Securely computing graph convolutional networks (GCNs) is critical for\napplying their analytical capabilities to privacy-sensitive data like\nsocial\/credit networks. Multiplying a sparse yet large adjacency matrix of a\ngraph in GCN--a core operation in training\/inference--poses a performance\nbottleneck in secure GCNs. Consider a GCN with $|V|$ nodes and $|E|$ edges; it\nincurs a large $O(|V|^2)$ communication overhead. Modeling bipartite graphs and\nleveraging the monotonicity of non-zero entry locations, we propose a co-design\nharmonizing secure multi-party computation (MPC) with matrix sparsity. Our\nsparse matrix decomposition transforms an arbitrary sparse matrix into a\nproduct of structured matrices. Specialized MPC protocols for oblivious\npermutation and selection multiplication are then tailored, enabling our secure\nsparse matrix multiplication ($(SM)^2$) protocol, optimized for secure\nmultiplication of these structured matrices. Together, these techniques take\n$O(|E|)$ communication in constant rounds. Supported by $(SM)^2$, we present\nVirgos, a secure 2-party framework that is communication-efficient and\nmemory-friendly on standard vertically-partitioned graph datasets. Performance\nof Virgos has been empirically validated across diverse network conditions.",
        "Bilateral markets, such as those for government bonds, involve decentralized\nand opaque transactions between market makers (MMs) and clients, posing\nsignificant challenges for traditional modeling approaches. To address these\ncomplexities, we introduce TRIBE an agent-based model augmented with a large\nlanguage model (LLM) to simulate human-like decision-making in trading\nenvironments. TRIBE leverages publicly available data and stylized facts to\ncapture realistic trading dynamics, integrating human biases like risk aversion\nand ambiguity sensitivity into the decision-making processes of agents. Our\nresearch yields three key contributions: first, we demonstrate that integrating\nLLMs into agent-based models to enhance client agency is feasible and enriches\nthe simulation of agent behaviors in complex markets; second, we find that even\nslight trade aversion encoded within the LLM leads to a complete cessation of\ntrading activity, highlighting the sensitivity of market dynamics to agents'\nrisk profiles; third, we show that incorporating human-like variability shifts\npower dynamics towards clients and can disproportionately affect the entire\nsystem, often resulting in systemic agent collapse across simulations. These\nfindings underscore the emergent properties that arise when introducing\nstochastic, human-like decision processes, revealing new system behaviors that\nenhance the realism and complexity of artificial societies.",
        "The Stealth Address Protocol (SAP) allows users to receive assets through\nstealth addresses that are unlinkable to their stealth meta-addresses. The most\nwidely used SAP, Dual-Key SAP (DKSAP), and the most performant SAP, Elliptic\nCurve Pairing Dual-Key SAP (ECPDKSAP), are based on elliptic curve\ncryptography, which is vulnerable to quantum attacks. These protocols depend on\nthe elliptic curve discrete logarithm problem, which could be efficiently\nsolved on a sufficiently powerful quantum computer using the Shor algorithm. In\nthis paper three novel post-quantum SAPs based on lattice-based cryptography\nare presented: LWE SAP, Ring-LWE SAP and Module-LWE SAP. These protocols\nleverage Learning With Errors (LWE) problem to ensure quantum-resistant\nprivacy. Among them, Module-LWE SAP, which is based on the Kyber key\nencapsulation mechanism, achieves the best performance and outperforms ECPDKSAP\nby approximately 66.8% in the scan time of the ephemeral public key registry.",
        "The self-organization of pattern-forming systems depends not only on the\nchemical but also physical properties of their components. In this work, we\nfragmented and dispersed the MinDE protein system's lipid substrate into\ndiffusive sub-micrometer-sized liposomes, and report that the ATP-fueled\nprotein-protein interactions continue to drive spatially extended patterns at\nscales well separated from those of the requisite liposomes, despite the\ncomplete loss of membrane continuity. The patterns form in three-dimensions\nbecause the membrane is dispersed in a volume. By varying protein\nconcentration, liposome size distribution, and density, we observed and\ncharacterized rich 3D dynamical patterns at steady state, including traveling\nwaves, dynamical spirals and a mixed phase where both patterns coexist.\nSimulations and linear stability analysis of a coarse-grained model reveal that\nthe dispersed membranes's physical properties effectively rescale two key\nfactors that govern pattern formation and wavelength selection:\nprotein-membrane binding rates and diffusion. This work highlights the\nrobustness of pattern formation in membrane-bulk systems despite membrane\nfragmentation. It suggests that biological protein systems have the potential\nto serve as adaptable templates for out-of-equilibrium self-organization in 3D,\nbeyond in vivo biological contexts.",
        "Causality plays an important role in daily processes, human reasoning, and\nartificial intelligence. There has however not been much research on causality\nin multi-agent strategic settings. In this work, we introduce a systematic way\nto build a multi-agent system model, represented as a concurrent game\nstructure, for a given structural causal model. In the obtained so-called\ncausal concurrent game structure, transitions correspond to interventions on\nagent variables of the given causal model. The Halpern and Pearl framework of\ncausality is used to determine the effects of a certain value for an agent\nvariable on other variables. The causal concurrent game structure allows us to\nanalyse and reason about causal effects of agents' strategic decisions. We\nformally investigate the relation between causal concurrent game structures and\nthe original structural causal models.",
        "As 6G networks evolve, the upper mid-band spectrum (7 GHz to 24 GHz), or\nfrequency range 3 (FR3), is emerging as a promising balance between the\ncoverage offered by sub-6 GHz bands and the high-capacity of millimeter wave\n(mmWave) frequencies. This paper explores the structure of FR3 hybrid MIMO\nsystems and proposes two architectural classes: Frequency Integrated (FI) and\nFrequency Partitioned (FP). FI architectures enhance spectral efficiency by\nexploiting multiple sub-bands parallelism, while FP architectures dynamically\nallocate sub-band access according to specific application requirements.\nAdditionally, two approaches, fully digital (FD) and hybrid analog-digital\n(HAD), are considered, comparing shared (SRF) versus dedicated RF (DRF) chain\nconfigurations. Herein signal processing solutions are investigated,\nparticularly for an uplink multi-user scenario with power control optimization.\n  Results demonstrate that SRF and DRF architectures achieve comparable\nspectral efficiency; however, SRF structures consume nearly half the power of\nDRF in the considered setup. While FD architectures provide higher spectral\nefficiency, they do so at the cost of increased power consumption compared to\nHAD. Additionally, FI architectures show slightly greater power consumption\ncompared to FP; however, they provide a significant benefit in spectral\nefficiency (over 4 x), emphasizing an important trade-off in FR3 engineering.",
        "Unit edge-length drawings, rectilinear drawings (where each edge is either a\nhorizontal or a vertical segment), and rectangular face drawings are among the\nmost studied subjects in Graph Drawing. However, most of the literature on\nthese topics refers to planar graphs and planar drawings. In this paper we\nstudy drawings with all the above nice properties but that can have edge\ncrossings; we call them Unit Edge length Rectilinear drawings with Rectangular\nFaces (UER-RF drawings). We consider crossings as dummy vertices and apply the\nunit edge-length convention to the edge segments connecting any two (real or\ndummy) vertices. Note that UER-RF drawings are grid drawings (vertices are\nplaced at distinct integer coordinates), which is another classical requirement\nof graph visualizations. We present several efficient and easily implementable\nalgorithms for recognizing graphs that admit UER-RF drawings and for\nconstructing such drawings if they exist. We consider restrictions on the\ndegree of the vertices or on the size of the faces. For each type of\nrestriction, we consider both the general unconstrained setting and a setting\nin which either the external boundary of the drawing is fixed or the rotation\nsystem of the graph is fixed as part of the input.",
        "Enterprise level data is often distributed across multiple sources and\nidentifying the correct set-of data-sources with relevant information for a\nknowledge request is a fundamental challenge. In this work, we define the novel\ntask of routing an end-user query to the appropriate data-source, where the\ndata-sources are databases. We synthesize datasets by extending existing\ndatasets designed for NL-to-SQL semantic parsing. We create baselines on these\ndatasets by using open-source LLMs, using both pre-trained and task specific\nembeddings fine-tuned using the training data. With these baselines we\ndemonstrate that open-source LLMs perform better than embedding based approach,\nbut suffer from token length limitations. Embedding based approaches benefit\nfrom task specific fine-tuning, more so when there is availability of data in\nterms of database specific questions for training. We further find that the\ntask becomes more difficult (i) with an increase in the number of data-sources,\n(ii) having data-sources closer in terms of their domains,(iii) having\ndatabases without external domain knowledge required to interpret its entities\nand (iv) with ambiguous and complex queries requiring more fine-grained\nunderstanding of the data-sources or logical reasoning for routing to an\nappropriate source. This calls for the need for developing more sophisticated\nsolutions to better address the task."
      ]
    }
  },
  {
    "id":2411.03341,
    "research_type":"applied",
    "start_id":"b11",
    "start_title":"A ConvNet for the 2020s",
    "start_abstract":"The \"Roaring 20s\" of visual recognition began with the introduction Vision Transformers (ViTs), which quickly superseded ConvNets as state-of-the-art image classification model. A vanilla ViT, on other hand, faces difficulties when applied to general computer vision tasks such object detection and semantic segmentation. It is hierarchical (e.g., Swin Transformers) that reintroduced several ConvNet priors, making practically viable a generic backbone demonstrating remarkable performance wide variety tasks. However, effectiveness hybrid approaches still largely credited intrinsic superiority Transformers, rather than inherent inductive biases convolutions. In this work, we reexamine design spaces test limits what pure can achieve. We gradually \"modernize\" standard ResNet toward Transformer, discover key components contribute difference along way. outcome exploration family models dubbed ConvNeXt. Constructed entirely from modules, ConvNeXts compete favorably in terms accuracy scalability, achieving 87.8% ImageNet top-1 outperforming COCO ADE20K segmentation, while maintaining simplicity efficiency ConvNets.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "Highly multiplexed imaging of tumor tissues with subcellular resolution by mass cytometry"
      ],
      "abstract":[
        "Mass cytometry enables high-dimensional, single-cell analysis of cell type and state. In mass cytometry, rare earth metals are used as reporters on antibodies. Analysis of metal abundances using the mass cytometer allows determination of marker expression in individual cells. Mass cytometry has previously been applied only to cell suspensions. To gain spatial information, we have coupled immunohistochemical and immunocytochemical methods with high-resolution laser ablation to CyTOF mass cytometry. This approach enables the simultaneous imaging of 32 proteins and protein modifications at subcellular resolution; with the availability of additional isotopes, measurement of over 100 markers will be possible. We applied imaging mass cytometry to human breast cancer samples, allowing delineation of cell subpopulations and cell-cell interactions and highlighting tumor heterogeneity. Imaging mass cytometry complements existing imaging approaches. It will enable basic studies of tissue heterogeneity and function and support the transition of medicine toward individualized molecularly targeted diagnosis and therapies."
      ],
      "categories":[
        "q-bio.QM"
      ]
    },
    "list":{
      "title":[
        "Petri Net Modeling of Root Hair Response to Phosphate Starvation in\n  Arabidopsis Thaliana",
        "Fixed-budget simulation method for growing cell populations",
        "A technical review of multi-omics data integration methods: from\n  classical statistical to deep generative approaches",
        "MorphoITH: A Framework for Deconvolving Intra-Tumor Heterogeneity Using\n  Tissue Morphology",
        "Hierarchical Functional Group Ranking via IUPAC Name Analysis for Drug\n  Discovery: A Case Study on TDP1 Inhibitors",
        "Towards Precision Oncology: Predicting Mortality and Relapse-Free\n  Survival in Head and Neck Cancer Using Clinical Data",
        "Optimal compound downselection to promote diversity and parallel\n  chemistry",
        "Automating Care by Self-maintainability for Full Laboratory Automation",
        "The three-dimensional impulse-response model: Modeling the training\n  process in accordance with energy system-specific adaptation",
        "Uncertainty and sensitivity analysis of hair growth duration in human\n  scalp follicles under normal and alopecic conditions",
        "VenusMutHub: A systematic evaluation of protein mutation effect\n  predictors on small-scale experimental data",
        "Spatially-Structured Models of Viral Dynamics: A Scoping Review",
        "A Systematic Computational Framework for Practical Identifiability\n  Analysis in Mathematical Models Arising from Biology",
        "From Site Response to Site-city Interaction: a Case Study in the Tokyo\n  Area",
        "Well-to-Tank Carbon Intensity Variability of Fossil Marine Fuels: A\n  Country-Level Assessment",
        "The IDEA detector concept for FCC-ee",
        "A simple magnetic field stabilization technique for atomic Bose-Einstein\n  condensate experiments",
        "Terahertz Magnon Excitations and Switching in Non-Collinear\n  Antiferromagnets",
        "Strong Long-Wave Infrared Optical Response in a Topological\n  Semiconductor with a Mexican Hat Band Structure",
        "Diffusion Approximation for Slow-Fast SDEs with State-Dependent\n  Switching",
        "Direct Nucleation of Hierarchical Nanostructures on Plasmonic Fiber\n  Optics Enables Enhanced SERS Performance",
        "Periodic Variability of the Central Stars of Planetary Nebulae Surveyed\n  through the Zwicky Transient Facility",
        "Further results on relative, divergence measures based on extropy and\n  their applications",
        "On recurrence and entropy in hyperspace of continua in dimension one",
        "Emerging Excess Consistent with a Narrow Resonance at 152 GeV in\n  High-Energy Proton-Proton Collisions",
        "Fibonacci-Modulation-Induced Multiple Topological Anderson Insulators",
        "The route of shear to Ising superconductivity in bilayer graphene",
        "Detection of Physiological Data Tampering Attacks with Quantum Machine\n  Learning"
      ],
      "abstract":[
        "Limited availability of inorganic phosphate (Pi) in soil is an important\nconstraint to plant growth. In order to understand better the underlying\nmechanism of plant response to Pi, the response to phosphate starvation in\nArabidopsis thaliana was investigated through use of Petri Nets, a formal\nlanguage suitable for bio-modeling. A. thaliana displays a range of responses\nto deal with Pi starvation, but special attention was paid to root hair\nelongation in this study. A central player in the root hair pathway is the\ntranscription factor ROOT HAIR DEFECTIVE 6-LIKE 4 (RSL4), which has been found\nto be upregulated during the Pi stress. A Petri Net was created which could\nsimulate the gene regulatory networks responsible for the increase in root hair\nlength, as well as the resulting increase in root hair length. Notably,\ndiscrepancies between the model and the literature suggested an important role\nfor RSL2 in regulating RSL4. In the future, the net designed in the current\nstudy could be used as a platform to develop hypotheses about the interaction\nbetween RSL2 and RSL4.",
        "Investigating the dynamics of growing cell populations is crucial for\nunraveling key biological mechanisms in living organisms, with many important\napplications in therapeutics and biochemical engineering. Classical agent-based\nsimulation algorithms are often inefficient for these systems because they\ntrack each individual cell, making them impractical for fast (or even\nexponentially) growing cell populations. To address this challenge, we\nintroduce a novel stochastic simulation approach based on a Feynman-Kac-like\nrepresentation of the population dynamics. This method, named the\nFeynman-Kac-inspired Gillespie's Stochastic Simulation Algorithm (FKG-SSA),\nalways employs a fixed number of independently simulated cells for Monte Carlo\ncomputation of the system, resulting in a constant computational complexity\nregardless of the population size. Furthermore, we theoretically show the\nstatistical consistency of the proposed method, indicating its accuracy and\nreliability. Finally, a couple of biologically relevant numerical examples are\npresented to illustrate the approach. Overall, the proposed FKG-SSA effectively\naddresses the challenge of simulating growing cell populations, providing a\nsolid foundation for better analysis of these systems.",
        "The rapid advancement of high-throughput sequencing and other assay\ntechnologies has resulted in the generation of large and complex multi-omics\ndatasets, offering unprecedented opportunities for advancing precision medicine\nstrategies. However, multi-omics data integration presents significant\nchallenges due to the high dimensionality, heterogeneity, experimental gaps,\nand frequency of missing values across data types. Computational methods have\nbeen developed to address these issues, employing statistical and machine\nlearning approaches to uncover complex biological patterns and provide deeper\ninsights into our understanding of disease mechanisms. Here, we comprehensively\nreview state-of-the-art multi-omics data integration methods with a focus on\ndeep generative models, particularly variational autoencoders (VAEs) that have\nbeen widely used for data imputation and augmentation, joint embedding\ncreation, and batch effect correction. We explore the technical aspects of loss\nfunctions and regularisation techniques including adversarial training,\ndisentanglement and contrastive learning. Moreover, we discuss recent\nadvancements in foundation models and the integration of emerging data\nmodalities, while describing the current limitations and outlining future\ndirections for enhancing multi-modal methodologies in biomedical research.",
        "The ability of tumors to evolve and adapt by developing subclones in\ndifferent genetic and epigenetic states is a major challenge in oncology.\nTraditional tools like multi-regional sequencing used to study tumor evolution\nand the resultant intra-tumor heterogeneity (ITH) are often impractical because\nof their resource-intensiveness and limited scalability. Here, we present\nMorphoITH, a novel framework that leverages histopathology slides to deconvolve\nmolecular ITH through tissue morphology. MorphoITH integrates a self-supervised\ndeep learning similarity measure to capture phenotypic variation across\nmultiple dimensions (cytology, architecture, and microenvironment) with\nrigorous methods to eliminate spurious sources of variation. Using a prototype\nof ITH, clear cell renal cell carcinoma (ccRCC), we show that MorphoITH\ncaptures clinically-significant biological features, such as vascular\narchitecture and nuclear grades. Furthermore, we find that MorphoITH recognizes\ndifferential biological states corresponding to subclonal changes in key driver\ngenes (BAP1\/PBRM1\/SETD2). Finally, by applying MorphoITH to a multi-regional\nsequencing experiment, we postulate evolutionary trajectories that largely\nrecapitulate genetic evolution. In summary, MorphoITH provides a scalable\nphenotypic lens that bridges the gap between histopathology and genomics,\nadvancing precision oncology.",
        "The article proposes a computational approach that can generate a descending\norder of the IUPAC-notated functional groups based on their importance for a\ngiven case study. Thus, a reduced list of functional groups could be obtained\nfrom which drug discovery can be successfully initiated. The approach,\napplicable to any study case with sufficient data, was demonstrated using a\nPubChem bioassay focused on TDP1 inhibitors. The Scikit Learn interpretation of\nthe Random Forest Classifier (RFC) algorithm was employed. The machine learning\n(ML) model RFC obtained 70.9% accuracy, 73.1% precision, 66.1% recall, 69.4% F1\nand 70.8% receiver-operating characteristic (ROC). In addition to the main\nstudy, the CID_SID ML model was developed, which, using only the PubChem\ncompound and substance identifiers (CIDs and SIDs) data, can predict with 85.2%\naccuracy, 94.2% precision, 75% precision, F1 of 83.5% F1 and 85.2% ROC whether\na compound is a TDP1 inhibitor.",
        "Head and neck squamous cell carcinoma (HNSCC) presents significant challenges\nin clinical oncology due to its heterogeneity and high mortality rates. This\nstudy aims to leverage clinical data and machine learning (ML) principles to\npredict key outcomes for HNSCC patients: mortality, and relapse-free survival.\nUtilizing data sourced from the Cancer Imaging Archive, an extensive pipeline\nwas implemented to ensure robust model training and evaluation. Ensemble and\nindividual classifiers, including XGBoost, Random Forest, and Support Vectors,\nwere employed to develop predictive models. The study identified key clinical\nfeatures influencing HNSCC mortality outcomes and achieved predictive accuracy\nand ROC-AUC values exceeding 90\\% across tasks. Support Vector Machine strongly\nexcelled in relapse-free survival, with an recall value of 0.99 and precision\nof 0.97. Key clinical features including loco-regional control, smoking and\ntreatment type, were identified as critical predictors of patient outcomes.\nThis study underscores the medical impact of using ML-driven insights to refine\nprognostic accuracy and optimize personalized treatment strategies in HNSCC.",
        "Early stage drug discovery and molecular design projects often follow\niterative design-make-test cycles. The selection of which compounds to\nsynthesize from all possible candidate compounds is a complex decision inherent\nto these design cycles that must weigh multiple factors. We build upon the\nalgorithmic downselection framework SPARROW that considers synthetic cost,\nsynthetic feasibility, and compound utility, extending it to address additional\ncritical factors related to the risk of synthesis failure, molecular diversity,\nand parallel chemistry capabilities. These design considerations further align\nalgorithmic compound selection with the true complexity of this decision-making\nprocess, allowing SPARROW to capture a broader set of principles typically\nreliant on expert chemist intuition. The application of these formulations to\nan exemplary case study highlights SPARROW's ability to promote the selection\nof diverse batches of compounds whose syntheses are amenable to parallel\nchemistry.",
        "The automation of experiments in life sciences and chemistry has\nsignificantly advanced with the development of various instruments and AI\ntechnologies. However, achieving full laboratory automation, where experiments\nconceived by scientists are seamlessly executed in automated laboratories,\nremains a challenge. We identify the lack of automation in planning and\noperational tasks--critical human-managed processes collectively termed\n\"care\"--as a major barrier. Automating care is the key enabler for full\nlaboratory automation. To address this, we propose the concept of\nself-maintainability (SeM): the ability of a laboratory system to autonomously\nadapt to internal and external disturbances, maintaining operational readiness\nakin to living cells. A SeM-enabled laboratory features autonomous recognition\nof its state, dynamic resource and information management, and adaptive\nresponses to unexpected conditions. This shifts the planning and execution of\nexperimental workflows, including scheduling and reagent allocation, from\nhumans to the system. We present a conceptual framework for implementing\nSeM-enabled laboratories, comprising three modules--Requirement manager,\nLabware manager, and Device manager--and a Central manager. SeM not only\nenables scientists to execute envisioned experiments seamlessly but also\nprovides developers with a design concept that drives the technological\ninnovations needed for full automation.",
        "Athletic training is characterized by physiological systems responding to\nrepeated exercise-induced stress, resulting in gradual alterations in the\nfunctional properties of these systems. The adaptive response leading to\nimproved performance follows a remarkably predictable pattern that may be\ndescribed by a systems model provided that training load can be accurately\nquantified and that the constants defining the training-performance\nrelationship are known. While various impulse-response models have been\nproposed, they are inherently limited in reducing training stress (the impulse)\ninto a single metric, assuming that the adaptive responses are independent of\nthe type of training performed. This is despite ample evidence of markedly\ndiverse acute and chronic responses to exercise of different intensities and\ndurations. Herein, we propose an alternative, three-dimensional\nimpulse-response model that uses three training load metrics as inputs and\nthree performance metrics as outputs. These metrics, represented by a\nthree-parameter critical power model, reflect the stress imposed on each of the\nthree energy systems: the alactic (phosphocreatine\/immediate) system; the\nlactic (glycolytic) system; and the aerobic (oxidative) system. The purpose of\nthis article is to outline the scientific rationale and the practical\nimplementation of the three-dimensional impulse-response model.",
        "Hair follicles constantly cycle through phases of growth, regression and\nrest, as matrix keratinocytes (MKs), the cells producing hair fibers,\nproliferate, and then undergo spontaneous apoptosis. Damage to MKs and\nperturbations in their normal dynamics result in a shortened growth phase,\nleading to hair loss. Two common factors causing such disruption are hormonal\nimbalance and attacks by the immune system. Androgenetic alopecia (AGA) is hair\nloss caused by high sensitivity to androgens, and alopecia areata (AA) is hair\nloss caused by an autoimmune reaction against MKs. In this study, we inform a\nmathematical model for the human hair cycle with experimental data for the\nlengths of hair cycle phases available from male control subjects and subjects\nwith AGA. We also, connect a mathematical model for AA with estimates for the\nduration of hair cycle phases obtained from the literature. Subsequently, with\neach model we perform parameter screening, uncertainty quantification and\nglobal sensitivity analysis and compare the results within and between the\ncontrol and AGA subject groups as well as among AA, control and AGA conditions.\nThe findings reveal that in AGA subjects there is greater uncertainty\nassociated with the duration of hair growth than in control subjects and that,\ncompared to control and AGA conditions, in AA it is more certain that longer\nhair growth phase could not be expected. The comparison of results also\nindicates that in AA lower proliferation of MKs and weaker communication of the\ndermal papilla with MKs via signaling molecules could be expected than in\nnormal and AGA conditions, and in AA stronger inhibition of MK proliferation by\nregulatory molecules could be expected than in AGA. Finally, the global\nsensitivity analysis highlights the process of MK apoptosis as highly impactful\nfor the length of hair growth only in the AA case, but not for control and AGA\nconditions.",
        "In protein engineering, while computational models are increasingly used to\npredict mutation effects, their evaluations primarily rely on high-throughput\ndeep mutational scanning (DMS) experiments that use surrogate readouts, which\nmay not adequately capture the complex biochemical properties of interest. Many\nproteins and their functions cannot be assessed through high-throughput methods\ndue to technical limitations or the nature of the desired properties, and this\nis particularly true for the real industrial application scenario. Therefore,\nthe desired testing datasets, will be small-size (~10-100) experimental data\nfor each protein, and involve as many proteins as possible and as many\nproperties as possible, which is, however, lacking. Here, we present\nVenusMutHub, a comprehensive benchmark study using 905 small-scale experimental\ndatasets curated from published literature and public databases, spanning 527\nproteins across diverse functional properties including stability, activity,\nbinding affinity, and selectivity. These datasets feature direct biochemical\nmeasurements rather than surrogate readouts, providing a more rigorous\nassessment of model performance in predicting mutations that affect specific\nmolecular functions. We evaluate 23 computational models across various\nmethodological paradigms, such as sequence-based, structure-informed and\nevolutionary approaches. This benchmark provides practical guidance for\nselecting appropriate prediction methods in protein engineering applications\nwhere accurate prediction of specific functional properties is crucial.",
        "There is growing recognition in both the experimental and modelling\nliterature of the importance of spatial structure to the dynamics of viral\ninfections in tissues. Aided by the evolution of computing power and motivated\nby recent biological insights, there has been an explosion of new,\nspatially-explicit models for within-host viral dynamics in recent years. This\ndevelopment has only been accelerated in the wake of the COVID-19 pandemic.\nSpatially-structured models offer improved biological realism and can account\nfor dynamics which cannot be well-described by conventional, mean-field\napproaches. However, despite their growing popularity, spatially-structured\nmodels of viral dynamics are underused in biological applications. One major\nobstacle to the wider application of such models is the huge variety in\napproaches taken, with little consensus as to which features should be included\nand how they should be implemented for a given biological context. Previous\nreviews of the field have focused on specific modelling frameworks or on models\nfor particular viral species. Here, we instead apply a scoping review approach\nto the literature of spatially-structured viral dynamics models as a whole to\nprovide an exhaustive update of the state of the field. Our analysis is\nstructured along two axes, methodology and viral species, in order to examine\nthe breadth of techniques used and the requirements of different biological\napplications. We then discuss the contributions of mathematical and\ncomputational modelling to our understanding of key spatially-structured\naspects of viral dynamics, and suggest key themes for future model development\nto improve robustness and biological utility.",
        "Practical identifiability is a critical concern in data-driven modeling of\nmathematical systems. In this paper, we propose a novel framework for practical\nidentifiability analysis to evaluate parameter identifiability in mathematical\nmodels of biological systems. Starting with a rigorous mathematical definition\nof practical identifiability, we demonstrate its equivalence to the\ninvertibility of the Fisher Information Matrix. Our framework establishes the\nrelationship between practical identifiability and coordinate identifiability,\nintroducing a novel metric that simplifies and accelerates the evaluation of\nparameter identifiability compared to the profile likelihood method.\nAdditionally, we introduce new regularization terms to address non-identifiable\nparameters, enabling uncertainty quantification and improving model\nreliability. To guide experimental design, we present an optimal data\ncollection algorithm that ensures all model parameters are practically\nidentifiable. Applications to Hill functions, neural networks, and dynamic\nbiological models demonstrate the feasibility and efficiency of the proposed\ncomputational framework in uncovering critical biological processes and\nidentifying key observable variables.",
        "Considering the purpose of the session relating early engineering\ndevelopments in site response and soil-structure interaction, this paper\nfocuses on the development of studies regarding site-city interaction following\nthe striking site response observations obtained in Mexico City during the 1985\nGuerrero-Michoacan event, The first part presents an overview of the\ninvestigations on multiple structure-soil-structure interaction, starting with\nMexico-city like environments with dense urbanization on soft soils, which\nlater evolved with the concept of metamaterials. Up to now, such investigations\nhave been largely relying on numerical simulations in 2D and 3D media, coupling\nsoft surface soil layers and simplified building models, including also some\ntheoretical developments using various mechanical concepts. They also relied on\na number of laboratory experiments on reduced-scale mock-ups with diverse\nvibratory sources (shaking table, acoustic devices). The latest studies coupled\nfull-scale experiments on mechanical analogs such as forests or wind turbine\nfarms involving sets of resonators with similar frequencies, and numerical\nsimulation to investigate their impact on the propagation of surface (Rayleigh)\nwaves. Almost all such studies converge in predicting lower ground motion\namplitude for sites located within the ''urbanized'' area, but none of them can\nbe considered a ''groundtruth'' proof for a real earthquake in a real city. The\nsecond part thus takes advantage of the long duration of strong motion\nobservations in the Kanto area thanks to the KiK-net, K-NET and JMA\n(Shin-dokei) networks, to investigate the possible changes in site response\nwith time. The first results obtained with the event-specific site terms\nderived from Generalized Inversion Techniques (Nakano et al., 2015) indicate a\nsystematic reduction of the low frequency (0.2 -1 Hz) site amplification, in\nthe central-south Tokyo area. As this frequency band corresponds both to the\nsite frequency (very thick deposits) and to the high-rise buildings, the\ndiscussion focuses on the possible relation with the extensive construction in\nsome areas of downtown Tokyo over the last 2 decades.",
        "The transition toward a low-carbon maritime transportation requires\nunderstanding lifecycle carbon intensity (CI) of marine fuels. While\nwell-to-tank emissions significantly contribute to total greenhouse gas\nemissions, many studies lack global perspective in accounting for upstream\noperations, transportation, refining, and distribution. This study evaluates\nwell-to-tank CI of High Sulphur Fuel Oil (HSFO) and well-to-refinery exit CI of\nLiquefied Petroleum Gas (LPG) worldwide at asset level. HSFO represents\ntraditional marine fuel, while LPG serves as potential transition fuel due to\nlower tank-to-wake emissions and compatibility with low-carbon fuels. Using\nOPGEE and PRELIM tools with R-based geospatial methods, we derive country-level\nCI values for 72 countries (HSFO) and 74 countries (LPG), covering 98% of\nglobal production. Results show significant variation in climate impacts\nglobally. HSFO upstream CI ranges 1-22.7 gCO2e\/MJ, refining CI 1.2-12.6\ngCO2e\/MJ, with global volume-weighted-average well-to-tank CI of 12.4 gCO2e\/MJ.\nUpstream and refining account for 55% and 32% of HSFO well-to-tank CI, with\nlarge exporters and intensive refining practices showing higher emissions. For\nLPG, upstream CI ranges 0.9-22.7 gCO2e\/MJ, refining CI 2.8-13.9 gCO2e\/MJ, with\nvolume-weighted-average well-to-refinery CI of 15.6 gCO2e\/MJ. Refining\ncomprises 49% of LPG well-to-refinery CI, while upstream and transport\nrepresent 44% and 6%. Major players include China, United States and Russia.\nThese findings reveal significant CI variability across countries and supply\nchains, offering opportunities for targeted emission reduction policies.",
        "A detector concept, named IDEA, optimized for the physics and running\nconditions at the FCC-ee is presented. After discussing the expected running\nconditions and the main physics drivers, a detailed description of the\nindividual sub-detectors is given. These include: a very light tracking system\nwith a powerful vertex detector inside a large drift chamber surrounded by a\nsilicon wrapper, a high resolution dual readout crystal electromagnetic\ncalorimeter, an HTS based superconducting solenoid, a dual readout fiber\ncalorimeter and three layers of muon chambers embedded in the magnet flux\nreturn yoke. Some examples of the expected detector performance, based on fast\nand full simulation, are also given.",
        "We demonstrate a simple magnetic field stabilization technique in a\nBose-Einstein condensate experiment. Our technique is based on the precise\nmeasurement of the current fluctuations in the main magnetic field coils and\namounts to their compensation using an auxiliary coil. It has the advantage of\nsimplicity as compensation is done using a low inductance coil that can be\nstraightforwardly driven at the relevant frequencies (1 kHz). The performances\nof the different components (power supply, current transducer, electronics...)\nare precisely characterized. In addition, for optimal stability the ambient\nmagnetic field is also measured and compensated. The magnetic field stability\naround 57 G is measured by Ramsey spectroscopy of magnetic field sensitive\nradiofrequency transition between two spin states of potassium 39 and the\nshot-to-shot fluctuations are reduced to 64(7) $\\mu$G rms, i.e. at the 1 x 10\n-6 level. In the context of our experiment, this result opens interesting\nprospects for the study of three-body interactions in Bose-Einstein condensate\npotassium spin mixtures.",
        "We investigate how spatiotemporal spin polarized current can lead to\nterahertz frequency excitations in non-collinear antiferromagnets. By solving\nthe Landau-Lifshitz-Gilbert equation numerically for non-collinear\nantiferromagnet, we show that the magnon frequency spectrum exhibits standing\nspin wave modes and depends on the thickness of Mn$_3$Ge in heterostructure\nFe|Au|Mn$_3$Ge. Also, we analyze the switching process of ground state as a\nfunction of a spin current. We show a switching phase diagram, which contains\nswitching and non-switching regions. Our work suggests non-collinear\nantiferromagnets as an efficient platform for terahertz magnonics and ultrafast\nmemory devices.",
        "Light sources and photodetectors operating in the far- to mid-infrared\n(FIR\/MIR) band ($8$-$12~\\rm \\mu m$, $0.1$-$0.15~\\rm eV$) remain relatively\npoorly developed compared to their counterparts operating in the visible and\nnear-infrared ranges, despite extensive application potential for thermal\nimaging, standoff sensing, and other technologies. This is attributable in part\nto the lack of narrow-gap materials ($<0.1~\\rm eV$) with high optical gain and\nabsorption. In this work, a narrow-gap semiconductor, $\\rm Pb_{0.7}Sn_{0.3}Se$,\nis demonstrated to exhibit an optical response $>10\\times$ larger than that of\n$\\rm Hg_{x}Cd_{1-x}Te$ (MCT), the dominant material for FIR\/MIR photodetectors.\nA previous theoretical investigation indicated that chalcogen $p$ and metal $d$\nband inversion in this material creates a Mexican hat band structure (MHBS),\nwhich results in a dramatic increase in the joint density of states at the\noptical transition edge compared to typical semiconductors. This prediction is\nexperimentally validated here using single-crystal specimens of $\\rm\nPb_{0.7}Sn_{0.3}Se$ measured using temperature-dependent spectroscopic\nellipsometry over a wavelength range of $1.7$-$20~\\rm \\mu m$ ($0.73$-$0.062~\\rm\neV$). These measurements demonstrate a large enhancement in extinction\ncoefficient and refractive index characteristic of a MHBS in the vicinity of\nthe absorption edge, in agreement with theoretical predictions. The realization\nof topological semiconductors with a MHBS is expected to lead to\nhigh-efficiency detectors operating in the FIR\/MID range.",
        "In this paper, we study the diffusion approximation for slow-fast stochastic\ndifferential equations with state-dependent switching, where the slow component\n$X^{\\varepsilon}$ is the solution of a stochastic differential equation with\nadditional homogenization term, while the fast component $\\alpha^{\\varepsilon}$\nis a switching process. We first prove the weak convergence of\n$\\{X^\\varepsilon\\}_{0<\\varepsilon\\leq 1}$ to $\\bar{X}$ in the space of\ncontinuous functions, as $\\varepsilon\\rightarrow 0$. Using the martingale\nproblem approach and Poisson equation associated with a Markov chain, we\nidentify this weak limiting process as the unique solution $\\bar{X}$ of a new\nstochastic differential equation, which has new drift and diffusion terms that\ndiffer from those in the original equation. Next, we prove the order $1\/2$ of\nweak convergence of $X^{\\varepsilon}_t$ to $\\bar{X}_t$ by applying suitable\ntest functions $\\phi$, for any $t\\in [0, T]$. Additionally, we provide an\nexample to illustrate that the order we achieve is optimal.",
        "We present an innovative fabrication method to achieve bottom-up in situ\nsurface-overstructured Au nanoislands (NIs) with tunable grades of surface\ncoverage, elongation, and branching, directly on micro-optical fibers for\nsensing applications. These all-in-gold hierarchical nanostructures consist of\nNIs coated with surface protrusions of various morphologies. They are created\nin solution using a selective seeded growth approach, whereby additional gold\ngrowth is achieved over Au NIs formerly developed on the fiber facet by a\nsolid-state dewetting approach. The morphology of nanosized surface-NI\noverstructuring can be adjusted from multi-dot-decorated Au NIs to\nmulti-arm-decorated Au NIs. This engineering of optical fibers allows for\nimproved remote surface-enhanced Raman spectroscopy (SERS) molecular detection.\nBy combining solid-state dewetting and wet-chemical approaches, we achieve\nstable in-contact deposition of surface-overstructured NIs with the optical\nfiber solid substrate, alongside precise control over branching morphology and\nanisotropy extent. The fiber optic probes engineered by surface-overstructured\nNIs exhibit outstanding sensing performance in an instant and through-fiber\ndetection scheme, achieving a remarkable detection limit at 10-7 M for the R6G\naqueous solution. These engineered probes demonstrate an improved detection\nlimit by one order of magnitude and enhanced peak prominence compared to\ndevices solely decorated with pristine NIs.",
        "A consensus has been reached in recent years that binarity plays an important\nrole in the formation and evolution of a significant fraction of planetary\nnebulae (PNe). Utilizing the archived photometric data from the Zwicky\nTransient Facility survey, we conducted a comprehensive data mining in search\nfor brightness variations in a large sample of Galactic PNe. This effort leads\nto identification of 39 PNe, whose central stars exhibit periodic variation in\nlight curves. Among these objects, 20 are known binary central stars of PNe,\nwhile the remaining 19 are new discoveries. Additionally, we identified 14 PNe\nwith central stars displaying anomalous variation in light curves, as well as\neight variables based on the high-cadence photometric data. Among the new\ndiscoveries of periodicity, we found compelling evidence of binary systems at\nthe centres of two archetypal quadrupolar PNe. We also report on very peculiar\nbrightness variation observed in the central core of the compact PN NGC6833.\nSeveral PNe in our sample deserve follow-up observations, both high-dispersion\nspectroscopy and high-precision photometry, to reveal the true nature of their\ncentral binarity or even multiplicity.",
        "This study explores information measures based on extropy, introducing\ndynamic relative extropy measures for residual and past lifetimes, and\ninvestigating their various properties. Furthermore, the study analyzes the\nrelationships between extropy-based divergence with dynamic relative extropy\nand other extropy measures. A nonparametric estimator for relative extropy is\ndeveloped, and its performance is assessed through numerical simulation\nstudies. The practical applicability of the relative extropy is demonstrated\nthrough some real-life data sets.",
        "We show that if $G$ is a topological graph, and $f$ is continuous map, then\nthe induced map $\\tilde{f}$ acting on the hyperspace $C(G)$ of all connected\nsubsets of $G$ by natural formula $\\tilde{f}(C)=f(C)$ carries the same entropy\nas $f$.\n  This is well known that it does not hold on the larger hyperspace of all\ncompact subsets. Also negative examples were given for the hyperspace $C(X)$ on\nsome continua $X$, including dendrites.\n  Our work extends previous positive results obtained first for much simpler\ncase of compact interval by completely different tools.",
        "The Higgs boson discovery at the Large Hadron Collider (LHC) at CERN\nconfirmed the existence of the last missing particle of the Standard Model\n(SM). The existence of new fundamental constituents of matter beyond the SM is\nof great importance for our understanding of Nature. In this context, indirect\n(non-resonant) indications for new scalar bosons were found in the data from\nthe first run of the LHC, taken between 2010 and 2012 at CERN: an excess in the\ninvariant mass of muon-electron pairs, consistent with a new Higgs boson ($S$)\nwith a mass of $150\\pm5$ GeV. Other processes with multiple leptons in the\nfinal state, moderate missing energy, and possibly (bottom quark) jets exhibit\ndeviations from the SM predictions. These anomalies can be explained within a\nsimplified model in which a new heavy Higgs boson $H$ decays into two lighter\nHiggses $S$. This lighter Higgs $S$ subsequently decays to $W$ bosons, bottom\nquarks and has also an invisible decay mode.\n  Here, we demonstrate that using this model we can identify narrow excesses in\ndi-photon and $Z$-photon spectra around 152 GeV. By incorporating the latest\nmeasurements of di-photons in association with leptons, we obtain a combined\nglobal significance of $5.4\\sigma$. This represents the highest significance\never reported for an excess consistent with a narrow resonance beyond the SM\n(BSM) in high-energy proton-proton collision data at the LHC. Such findings\nhave the potential to usher in a new era in particle physics - the BSM epoch -\noffering crucial insights into unresolved puzzles of nature.",
        "We uncover the emergence of multiple topological Anderson insulators (TAIs)\nin a 1D spin-orbit coupled (SOC) chain driven by Fibonacci modulation,\ntransforming a trivial band structure into a cascade of topologically\nnontrivial phases. This intriguing phenomenon is marked by the appearance of\nzero-energy modes and transitions in the $\\mathcal{Z}_2$ topological quantum\nnumber. Strikingly, as the SOC amplitude decreases, the number of TAI phases\ngrows, a behavior intricately linked to the fractal structure of the energy\nspectrum induced by Fibonacci modulation. Unlike conventional TAI phases, which\nexhibit fully localized eigenstates, the wave functions in the\nFibonacci-modulated TAI phases exhibit multifractal behavior. Furthermore, this\nmodel can be experimentally realized in a Bose-Einstein condensate along the\nmomentum lattice, where its topological transitions and multifractal properties\ncan be probed through quench dynamics. Our findings open new avenues for\nexploring exotic disorder-induced topological phases and their intricate\nmultifractal nature.",
        "We show that the sheared graphene bilayers can be tuned to have flat\nlow-energy bands for sufficiently large size of their moir\\'e supercell. In\nthis regime, the interacting system becomes prone to develop broken-symmetry\nphases, with valley symmetry breaking as the dominant pattern. The strong\nsignal of symmetry breaking favors the onset of a pairing instability in which\nthe electrons with opposite spin projection in the Cooper pairs live in\ndifferent valleys. The Fermi lines become distorted due to the repulsive\nCoulomb interaction, which makes the screening highly anisotropic, leading\neasily to attraction in some of the interaction channels. We also show that the\nsheared graphene bilayers offer the possibility to realize the combined\nbreakdown of parity and valley symmetry, making them very suitable to study the\ninterplay between correlations and topology in a two-dimensional electron\nsystem.",
        "The widespread use of cloud-based medical devices and wearable sensors has\nmade physiological data susceptible to tampering. These attacks can compromise\nthe reliability of healthcare systems which can be critical and\nlife-threatening. Detection of such data tampering is of immediate need.\nMachine learning has been used to detect anomalies in datasets but the\nperformance of Quantum Machine Learning (QML) is still yet to be evaluated for\nphysiological sensor data. Thus, our study compares the effectiveness of QML\nfor detecting physiological data tampering, focusing on two types of white-box\nattacks: data poisoning and adversarial perturbation. The results show that QML\nmodels are better at identifying label-flipping attacks, achieving accuracy\nrates of 75%-95% depending on the data and attack severity. This superior\nperformance is due to the ability of quantum algorithms to handle complex and\nhigh-dimensional data. However, both QML and classical models struggle to\ndetect more sophisticated adversarial perturbation attacks, which subtly alter\ndata without changing its statistical properties. Although QML performed poorly\nagainst this attack with around 45%-65% accuracy, it still outperformed\nclassical algorithms in some cases."
      ]
    }
  },
  {
    "id":2411.05443,
    "research_type":"applied",
    "start_id":"b9",
    "start_title":"Topological Methods for the Analysis of High Dimensional Data Sets and 3D Object Recognition. The Eurographics Association",
    "start_abstract":"We present a computational method for extracting simple descriptions of high dimensional data sets in the form of simplicial complexes. Our method, called Mapper, is based on the idea of partial clustering of the data guided by a set of functions defined on the data. The proposed method is not dependent on any particular clustering algorithm, i.e. any clustering algorithm may be used with Mapper. We implement this method and present a few sample applications in which simple descriptions of the data present important information about its structure.",
    "start_categories":[
      "cs.DC"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b14"
      ],
      "title":[
        "A Global Geometric Framework for Nonlinear Dimensionality Reduction"
      ],
      "abstract":[
        "Scientists working with large volumes of high-dimensional data, such as global climate patterns, stellar spectra, or human gene distributions, regularly confront the problem dimensionality reduction: finding meaningful low-dimensional structures hidden in their observations. The brain confronts same everyday perception, extracting from its sensory inputs\u201430,000 auditory nerve fibers 106 optic fibers\u2014a manageably small number perceptually relevant features. Here we describe an approach to solving reduction problems that uses easily measured local metric information learn underlying geometry a data set. Unlike classical techniques principal component analysis (PCA) and multidimensional scaling (MDS), our is capable discovering nonlinear degrees freedom underlie complex natural observations, handwriting images face under different viewing conditions. In contrast previous algorithms for reduction, ours efficiently computes globally optimal solution, and, important class manifolds, guaranteed converge asymptotically true structure."
      ],
      "categories":[
        "math.ST"
      ]
    },
    "list":{
      "title":[
        "Partial domination of middle graphs",
        "Thermodynamical limits for models of car-sharing systems: the Autolib\n  example",
        "Regular rigid Korovin orbits",
        "Hazard Rate for Associated Data in Deconvolution Problems: Asymptotic\n  Normality",
        "Multi-View Clustering Meets High-Dimensional Mixed Data: A Fusion\n  Regularized Method",
        "Asymptotics of solutions to the porous medium equation near conical\n  singularities",
        "Second Quantization and Evolution Operators in infinite dimension",
        "Constructions of Covering Sequences and Arrays",
        "Simultaneous analysis of approximate leave-one-out cross-validation and\n  mean-field inference",
        "On a non-local area-preserving curve flow",
        "Quasisymmetric mappings in b-metric spaces",
        "CoHAs of Torsion Sheaves on Weighted Projective Curves",
        "The feasibility of multi-graph alignment: a Bayesian approach",
        "Efficient inference of rankings from multi-body comparisons",
        "GWSkyNet-Multi II: an updated deep learning model for rapid\n  classification of gravitational-wave events",
        "On the components of random geometric graphs in the dense limit",
        "Infinitely many saturated travelling waves for epidemic models with\n  distributed-contacts",
        "Precision Higgs Constraints in U(1) Extensions of the Standard Model\n  with a Light Z'-Boson",
        "Chance-Constrained Covariance Steering for Discrete-Time Markov Jump\n  Linear Systems",
        "Noncommutative Novikov bialgebras and differential antisymmetric\n  infinitesimal bialgebras with weight",
        "The second-order intrinsic Wiedemann-Franz law",
        "A model calculation of the CKM matrix",
        "A Unified Blockwise Measurement Design for Learning Quantum Channels and\n  Lindbladians via Low-Rank Matrix Sensing",
        "Generalized Recurrence Criteria for Classes of Open Quantum Walks",
        "Survival Concept-Based Learning Models",
        "Is a phonon excitation of a superfluid Bose gas a Goldstone boson?",
        "Multiple Horn problems for planar networks and invertible matrices",
        "Torsion in Magnitude homology theories"
      ],
      "abstract":[
        "For any graph $G=(V,E)$, a subset $S\\subseteq V$ is called {\\it an isolating\nset} of $G$ if $V\\setminus N_G[S]$ is an independent set of $G$, where\n$N_G[S]=S\\cup N_G(S)$, and {\\it the isolation number} of $G$, denoted by\n$\\iota(G)$, is the size of a smallest isolating set of $G$. In this article, we\nshow that the isolation number of the middle graph of $G$ is equal to the size\nof a smallest maximal matching of $G$.",
        "We analyze mean-field equations obtained for models motivated by a large\nstation-based car-sharing system in France called Autolib. The main focus is on\na version where users reserve a parking space when they take a car. In a first\nmodel, the reservation of parking spaces is effective for all users (see [4])\nand capacity constraints are ignored. The model is carried out in\nthermodynamical limit, that is when the number $N$ of stations and the number\nof cars $M_N$ tend to infinity, with $U = \\lim_{N\\to\\infty} M_N\/N$. This limit\nis described by Kolmogorov equations of a two-dimensional time-inhomogeneous\nMarkov process depicting the numbers of reservations and cars at a station. It\nsatisfies a non-linear differential system. We prove analytically that this\nsystem has a unique solution, which converges, as $t\\to\\infty$, to an\nequilibrium point exponentially fast. Moreover, this equilibrium point\ncorresponds to the stationary distribution of a two queue tandem (reservations,\ncars), which is here always ergodic. The intensity factor of each queue has an\nexplicit form obtained from an intrinsic mass conservation relationship. Two\nrelated models with capacity constraints are briefly presented in the last\nsection: the simplest one with no reservation leads to a one-dimensional\nproblem; the second one corresponds to our first model with finite total\ncapacity $K$.",
        "An example of an infinite regular feebly compact quasitopological group is\npresented such that all continuous real-valued functions on the group are\nconstant. The example is based on the use of Korovin orbits in $X^G$, where $X$\nis a special regular countably compact space constructed by S.Bardyla and\nL.Zdomskyy and $G$ is an abstract Abelian group of an appropriate cardinality.\nAlso, we study the interplay between the separation properties of the space $X$\nand Korovin orbits in $X^G$. We show in particular that if $X$ contains two\nnonempty disjoint open subsets, then every Korovin orbit in $X^G$ is Hausdorff.",
        "In reliability theory and survival analysis, observed data are often weakly\ndependent and subject to additive measurement errors. Such contamination arises\nwhen the underlying data are neither independent nor strongly mixed but instead\nexhibit association. This paper focuses on estimating the hazard rate by\ndeconvolving the density function and constructing an estimator of the\ndistribution function. We assume that the data originate from a strictly\nstationary sequence satisfying association conditions. Under appropriate\nsmoothness assumptions on the error distribution, we establish the\nquadratic-mean convergence and asymptotic normality of the proposed estimators.\nThe finite-sample performance of both the hazard rate and distribution function\nestimators is evaluated through a simulation study. We conclude with a\ndiscussion of open problems and potential future research directions.",
        "Multi-view clustering leverages consistent and complementary information\nacross multiple views to provide more comprehensive insights than analysis of\nsingle-view data. However, the heterogeneity and redundancy of high-dimensional\nmixed multi-view data pose significant challenges to the existing clustering\ntechniques. In this paper, we propose a novel multi-view fusion regularized\nclustering method with adaptive group sparsity, enabling reliable clustering\nwhile effectively capturing local features. Technically, for multi-view data\nwith mixed features exhibiting different distributions, different losses or\ndivergence metrics are considered with a collective fusion penalty to obtain\ncommon groups. Moreover, the non-convex group sparsity consisting of\ninter-group sparsity and intra-group sparsity is utilized to screen informative\nfeatures, thereby enhancing the robustness. Furthermore, we develop an\neffective proximal alternating direction method of multipliers (ADMM) and each\nsubproblem admits a closed-form solution. It is rigorously proven that this\nalgorithm globally converges to a Karush-Kuhn-Tucker (KKT) point, while\nestablishing the equivalence between local minimum points and KKT points within\na certain region. Extensive numerical experiments on both simulated and real\ndata validate the superior performance of the presented method in clustering\naccuracy and feature selection.",
        "We show that, on a manifold with conical singularities, the geometry of the\ncross-section is reflected in the solutions to the porous medium equation near\nthe conic points: We prove that the asymptotics of the solutions near the\nconical points are determined by the spectrum of the Laplacian on the\ncross-section. The key to this result is a precise description of the maximal\ndomain of the cone Laplacian.",
        "In an infinite dimensional separable Hilbert space $X$, we study compactness\nproperties and the hypercobtractivity of the realizations of Ornstein-Uhlenbeck\nevolution operators $P_{s,t}$ in the spaces $L^p(X,\\gamma_t)$,\n$\\{\\gamma_t\\}_{t\\in\\R}$ being a suitable evolution system of measures for\n$P_{s,t}$. Moreover we study the asymptotic behavior of $P_{s,t}$. All these\nresults are produced thank to a representation of $P_{s,t}$ through the second\nquantization operator. Among the examples we consider the transition evolution\noperator associated to a non-autonomous stochastic parabolic PDE.",
        "An $(n,R)$-covering sequence is a cyclic sequence whose consecutive\n$n$-tuples form a code of length $n$ and covering radius $R$. Using several\nconstruction methods improvements of the upper bounds on the length of such\nsequences for $n \\leq 20$ and $1 \\leq R \\leq 3$, are obtained. The definition\nis generalized in two directions. An $(n,m,R)$-covering sequence code is a set\nof cyclic sequences of length $m$ whose consecutive $n$-tuples form a code of\nlength~$n$ and covering radius $R$. The definition is also generalized to\narrays in which the $m \\times n$ sub-matrices form a covering code with\ncovering radius $R$. We prove that asymptotically there are covering sequences\nthat attain the sphere-covering bound up to a constant factor.",
        "Approximate Leave-One-Out Cross-Validation (ALO-CV) is a method that has been\nproposed to estimate the generalization error of a regularized estimator in the\nhigh-dimensional regime where dimension and sample size are of the same order,\nthe so called ``proportional regime''. A new analysis is developed to derive\nthe consistency of ALO-CV for non-differentiable regularizer under Gaussian\ncovariates and strong-convexity of the regularizer. Using a conditioning\nargument, the difference between the ALO-CV weights and their counterparts in\nmean-field inference is shown to be small. Combined with upper bounds between\nthe mean-field inference estimate and the leave-one-out quantity, this provides\na proof that ALO-CV approximates the leave-one-out quantity as well up to\nnegligible error terms. Linear models with square loss, robust linear\nregression and single-index models are explicitly treated.",
        "In this paper, we study a new area-preserving curvature flow for closed\nconvex planar curves. This flow will decrease the length of the evolving curve\nand make the curve more and more circular during the evolution process. And\nfinally, the curve converges to a finite circle in $C^{\\infty}$ sense as time\ngoes to infinity.",
        "Considering quasisymmetric mappings between b-metric spaces we have found a\nnew estimation for the ratio of diameters of two subsets which are images of\ntwo bounded subsets. This result generalizes the well-known\nTukia-V\\\"{a}is\\\"{a}l\\\"{a} inequality. The condition under which the image of a\nb-metric space under quasisymmetry is also a b-metric space is established.\nMoreover, the latter question is investigated for additive metric spaces.",
        "We describe the cohomological Hall algebra of torsion sheaves on a weighted\nprojective line with weights $(2, \\dots, 2)$ in terms of generators and\nrelations.",
        "We establish thresholds for the feasibility of random multi-graph alignment\nin two models. In the Gaussian model, we demonstrate an \"all-or-nothing\"\nphenomenon: above a critical threshold, exact alignment is achievable with high\nprobability, while below it, even partial alignment is statistically\nimpossible. In the sparse Erd\\H{o}s-R\\'enyi model, we rigorously identify a\nthreshold below which no meaningful partial alignment is possible and\nconjecture that above this threshold, partial alignment can be achieved. To\nprove these results, we develop a general Bayesian estimation framework over\nmetric spaces, which provides insight into a broader class of high-dimensional\nstatistical problems.",
        "Many of the existing approaches to assess and predict the performance of\nplayers, teams or products in competitive contests rely on the assumption that\ncomparisons occur between pairs of such entities. There are, however, several\nreal contests where more than two entities are part of each comparison, e.g.,\nsports tournaments,multiplayer board and card games, and preference surveys.\nThe Plackett-Luce (PL) model provides a principled approach to infer the\nranking of entities involved in such contests characterized by multi-body\ncomparisons. Unfortunately, traditional algorithms used to compute PL rankings\nsuffer from slow convergence limiting the application of the PL model to\nrelatively small-scale systems. We present here an alternative implementation\nthat allows for significant speed-ups and validate its efficiency in both\nsynthetic and real-world sets of data. Further, we perform systematic\ncross-validation tests concerning the ability of the PL model to predict\nunobserved comparisons. We find that a PL model trained on a set composed of\nmulti-body comparisons is more predictive than a PL model trained on a set of\nprojected pairwise comparisons derived from the very same training set,\nemphasizing the need of properly accounting for the true multi-body nature of\nreal-world systems whenever such an information is available.",
        "Multi-messenger observations of gravitational waves and electromagnetic\nemission from compact object mergers offer unique insights into the structure\nof neutron stars, the formation of heavy elements, and the expansion rate of\nthe Universe. With the LIGO-Virgo-KAGRA (LVK) gravitational-wave detectors\ncurrently in their fourth observing run (O4), it is an exciting time for\ndetecting these mergers. However, assessing whether to follow up a candidate\ngravitational-wave event given limited telescope time and resources is\nchallenging; the candidate can be a false alert due to detector glitches, or\nmay not have any detectable electromagnetic counterpart even if it is real.\nGWSkyNet-Multi is a deep learning model developed to facilitate follow-up\ndecisions by providing real-time classification of candidate events, using\nlocalization information released in LVK rapid public alerts. Here we introduce\nGWSkyNet-Multi II, an updated model targeted towards providing more robust and\ninformative predictions during O4 and beyond. Specifically, the model now\nprovides normalized probability scores and associated uncertainties for each of\nthe four corresponding source categories released by the LVK: glitch, binary\nblack hole, neutron star-black hole, and binary neutron star. Informed by\nexplainability studies of the original model, the updated model architecture is\nalso significantly simplified, including replacing input images with intuitive\nsummary values, making it more interpretable. For significant O4 event alerts\nissued between May 2023 and December 2024, GWSkyNet-Multi II produces a\nprediction that is consistent with the updated LVK classification for 93% of\nevents. The updated model can be used by the community to help make\ntime-critical follow-up decisions.",
        "Consider the geometric graph on $n$ independent uniform random points in a\nconnected compact region $A$ of ${\\bf R}^d, d \\geq 2$ with $C^2$ boundary, or\nin the unit square, with distance parameter $r_n$. Let $K_n$ be the number of\ncomponents of this graph, and $R_n$ the number of vertices not in the giant\ncomponent. Let $S_n$ be the number of isolated vertices. We show that if $r_n$\nis chosen so that $nr_n^d$ tends to infinity but slowly enough that ${\\bf\nE}[S_n]$ also tends to infinity, then $K_n$, $R_n$ and $S_n$ are all asymptotic\nto $\\mu_n$ in probability as $n \\to \\infty$ where (with $|A|$, $\\theta_d$ and\n$|\\partial A|$ denoting the volume of $A$, of the unit $d$-ball, and the\nperimeter of $A$ respectively) $\\mu_n := ne^{-\\pi n r_n^d\/|A|}$ if $d=2$ and\n$\\mu_n := ne^{-\\theta_d n r_n^d\/|A|} + \\theta_{d-1}^{-1} |\\partial A| r_n^{1-d}\ne^{- \\theta_d n r_n^d\/(2|A|)}$ if $d\\geq 3$. We also give variance asymptotics\nand central limit theorems for $K_n$ and $R_n$ in this limiting regime when $d\n\\geq 3$, and for Poisson input with $d \\geq 2$. We extend these results\n(substituting ${\\bf E}[S_n]$ for $\\mu_n$) to a class of non-uniform\ndistributions on $A$.",
        "We consider an epidemic model with distributed-contacts. When the contact\nkernel concentrates, one formally reaches a very degenerate Fisher-KPP equation\nwith a diffusion term that is not in divergence form. We make an exhaustive\nstudy of its travelling waves. For every admissible speed, there exists not\nonly a non-saturated (smooth) wave but also infinitely many saturated (sharp)\nones. Furthermore their tails may differ from what is usually expected. These\nresults are thus in sharp contrast with their counterparts on related models.",
        "Anomaly free $U(1)$ extensions of the standard model (SM) predict a new\nneutral gauge boson $Z'$. When the $Z'$ obtains its mass from the spontaneous\nbreaking of the new $U(1)$ symmetry by a new complex scalar field, the model\nalso predicts a second real scalar $s$ and the searches for the new scalar and\nthe new gauge boson become intertwined. We present the computation of\nproduction cross sections and decay widths of such a scalar $s$ in models with\na light $Z'$ boson, when the decay $h\\to Z' Z'$ may have a sizeable branching\nratio. We show how Higgs signal strength measurement in this channel can\nprovide stricter exclusion bounds on the parameters of the model than those\nobtained from the total signal strength for Higgs boson production.",
        "In this paper, we propose a novel convex optimization framework to solve the\noptimal covariance steering problem for discrete-time Markov Jump Linear\nSystems (MJLS) with chance constraints. We derive the analytical expressions\nfor the mean and covariance trajectories of time-varying discrete-time MJLS and\nshow that they cannot be separated even without chance constraints, unlike the\nsingle-mode dynamics case. To solve the covariance steering problem, we propose\na two-step convex optimization framework, which optimizes the mean and\ncovariance subproblems sequentially. Further, we use Gaussian approximations to\nincorporate chance constraints and propose an iterative optimization framework\nto solve the chance-constrained covariance steering problem. Both problems are\noriginally nonconvex, and we derive convex relaxations which are proved to be\nlossless at optimality using the Karush-Kuhn-Tucker (KKT) conditions. Numerical\nsimulations demonstrate the proposed method by achieving target covariances\nwhile respecting chance constraints under Gaussian noise and Markovian jump\ndynamics.",
        "This paper first develops a bialgebra theory for a noncommutative Novikov\nalgebra, called a noncommutative Novikov bialgebra, which is further\ncharacterized by matched pairs and Manin triples of noncommutative Novikov\nalgebras. The classical Yang-Baxter type equation, $\\mathcal{O}$-operators, and\nnoncommutative pre-Novikov algebras are introduced to study noncommutative\nNovikov bialgebra. As an application, noncommutative pre-Novikov algebras are\nobtained from differential dendriform algebras. Next, to generalize Gelfand's\nclassical construction of a Novikov algebra from a commutative differential\nalgebra to the bialgebra context in the noncommutative case, we establish\nantisymmetric infinitesimal (ASI) bialgebras for (noncommutative) differential\nalgebras, and obtain the condition under which a differential ASI bialgebra\ninduces a noncommutative Novikov bialgebra.",
        "In recent years, the nonlinear anomalous thermal Hall effect has attracted\nsubstantial attention. In this paper, we carry out a theoretical exploration of\nthe intrinsic anomalous thermal Hall and Nernst effect that is induced by the\nthermal Berry connection polarizability. This effect is independent of the\nrelaxation time and can be present in antiferromagnets possessing PT symmetry.\nAdditionally, we put forward a second-order intrinsic Wiedemann-Franz law,\nwhich represents the ratio of the second-order intrinsic thermal conductivity\ncoefficient to the second-order intrinsic electrical conductivity coefficient .\nWhen analyzed within a four-band PT symmetric Dirac model, we observe that the\nsecond-order intrinsic thermal conductivity coefficient is linearly\nproportional to the second-order intrinsic electrical conductivity coefficient\n, and the second-order intrinsic Wiedemann-Franz law is characterized by the\nchemical potential $\\mu$ in the low-temperature regime. These findings provide\nsignificant implications for experimental verification.",
        "We propose a strategy to compute the CKM matrix based on the conjecture,\nrecently put forward in the literature, according to which elementary particle\nmasses are not generated like in the standard Higgs scenario, but emerge from a\nnon-perturbative mechanism triggered by the presence in the fundamental\nLagrangian of ``irrelevant'' chiral breaking operators of the Wilson type of\ndimension $d\\geq 6$ scaled by $d-4$ powers of the UV cutoff. Non-perturbatively\ngenerated quark masses have the form $m_q\\sim C_q(\\alpha) \\Lambda_{RGI}$ where\n$\\Lambda_{RGI}$ is the RGI scale of the theory and $C_q(\\alpha)$ is a function\nof the gauge couplings. For the (elementary) fermion $q$ the $C_q(\\alpha)$\nleading behaviour is $C_q(\\alpha)={\\mbox{O}}(\\alpha^{1+(d_q-4)\/2})$. The\ndependence of the gauge coupling power behaviour from the dimension $d_q$ of\nthe Wilson-like operators associated with the fermion $q$ can be exploited to\nconstruct hierarchically organized up and down ''proto-mass matrices'' for\n''proto-flavours'', the diagonalization of which yields flavoured quarks with\ndefinite masses and a first principle construction of the CKM matrix.",
        "Quantum superoperator learning is a pivotal task in quantum information\nscience, enabling accurate reconstruction of unknown quantum operations from\nmeasurement data. We propose a robust approach based on the matrix sensing\ntechniques for quantum superoperator learning that extends beyond the positive\nsemidefinite case, encompassing both quantum channels and Lindbladians. We\nfirst introduce a randomized measurement design using a near-optimal number of\nmeasurements. By leveraging the restricted isometry property (RIP), we provide\ntheoretical guarantees for the identifiability and recovery of low-rank\nsuperoperators in the presence of noise. Additionally, we propose a blockwise\nmeasurement design that restricts the tomography to the sub-blocks,\nsignificantly enhancing performance while maintaining a comparable scale of\nmeasurements. We also provide a performance guarantee for this setup. Our\napproach employs alternating least squares (ALS) with acceleration for\noptimization in matrix sensing. Numerical experiments validate the efficiency\nand scalability of the proposed methods.",
        "In this paper, we study the recurrence of open quantum walks (OQWs) induced\nby finite-dimensional coins $(L,B,R)$. The focus is on homogeneous OQWs with a\nset of vertices $\\mathbb{Z}$, the set of integers. We present three distinct\nrecurrence criteria, each adapted to different types of coins. The first\ncriterion was developed for a class of Lazy OQWs in any finite dimension, where\nthe presented criterion is associated with an auxiliary map and its only\ninvariant state, resulting in the first recurrence criterion for Lazy OQWs. The\nsecond one is restricted to Lazy OQWs of dimension 2, where we provide a\ncomplete characterization of the recurrence for this lower dimension. Finally,\nwe present a general criterion for finite-dimensional coins in the non-lazy\ncase $(B=0)$, which generalizes many of the previously known results. This new\ncriterion holds for irreducible and reducible OQWs through a decomposition of\nthe Hilbert space where our quantum states act.",
        "Concept-based learning enhances prediction accuracy and interpretability by\nleveraging high-level, human-understandable concepts. However, existing CBL\nframeworks do not address survival analysis tasks, which involve predicting\nevent times in the presence of censored data -- a common scenario in fields\nlike medicine and reliability analysis. To bridge this gap, we propose two\nnovel models: SurvCBM (Survival Concept-based Bottleneck Model) and SurvRCM\n(Survival Regularized Concept-based Model), which integrate concept-based\nlearning with survival analysis to handle censored event time data. The models\nemploy the Cox proportional hazards model and the Beran estimator. SurvCBM is\nbased on the architecture of the well-known concept bottleneck model, offering\ninterpretable predictions through concept-based explanations. SurvRCM uses\nconcepts as regularization to enhance accuracy. Both models are trained\nend-to-end and provide interpretable predictions in terms of concepts. Two\ninterpretability approaches are proposed: one leveraging the linear\nrelationship in the Cox model and another using an instance-based explanation\nframework with the Beran estimator. Numerical experiments demonstrate that\nSurvCBM outperforms SurvRCM and traditional survival models, underscoring the\nimportance and advantages of incorporating concept information. The code for\nthe proposed algorithms is publicly available.",
        "It is generally accepted that phonons in a superfluid Bose gas are Goldstone\nbosons. This is justified by spontaneous symmetry breaking (SSB), which is\nusually defined as follows: the Hamiltonian of the system is invariant under\nthe $U(1)$ transformation $\\hat{\\Psi}(\\mathbf{r},t)\\rightarrow e^{i\\alpha}%\n\\hat{\\Psi}(\\mathbf{r},t)$, whereas the order parameter $\\Psi(\\mathbf{r},t)$ is\nnot. However, the strict definition of SSB is different: the Hamiltonian and\nthe boundary conditions are invariant under a symmetry transformation, while\nthe \\emph{ground state} is not. Based on the latter criterion, we study a\nfinite system of spinless, weakly interacting bosons using three approaches:\nthe standard Bogoliubov method, the particle-number-conserving Bogoliubov\nmethod, and the approach based on the exact ground-state wave function. Our\nresults show that the answer to the question in the title is \\textquotedblleft\nno\\textquotedblright. Thus, phonons in a real-world (finite) superfluid Bose\ngas are similar to sound in a classical gas: they are not Goldstone bosons, but\nquantised collective vibrational modes arising from the interaction between\natoms. In the case of an infinite Bose gas, however, the picture becomes\nparadoxical: the ground state can be regarded as either infinitely degenerate\nor non-degenerate, making the phonon both similar to a Goldstone boson and\ndifferent from it.",
        "The multiplicative multiple Horn problem is asking to determine possible\nsingular values of the combinations $AB, BC$ and $ABC$ for a triple of\ninvertible matrices $A,B,C$ with given singular values. There are similar\nproblems for eigenvalues of sums of Hermitian matrices (the additive problem),\nand for maximal weights of multi-paths in concatenations of planar networks\n(the tropical problem).\n  For the planar network multiple Horn problem, we establish necessary\nconditions, and we conjecture that for large enough networks they are also\nsufficient. These conditions are given by the trace equalities and rhombus\ninequalities (familiar from the hive description of the classical Horn\nproblem), and by the new set of tetrahedron equalities. Furthermore, if one\nimposes Gelfand-Zeitlin conditions on weights of planar networks, tetrahedron\nequalities turn into the octahedron recurrence from the theory of crystals. We\ngive a geometric interpretation of our results in terms of positive varieties\nwith potential. In this approach, rhombus inequalities follow from the\ninequality $\\Phi^t \\leqslant 0$ for the tropicalized potential, and tetrahedron\nequalities are obtained as tropicalization of certain Pl\\\"ucker relations.\n  For the multiplicative problem, we introduce a scaling parameter $s$, and we\nshow that for $s$ large enough (corresponding to exponentially large\/small\nsingular values) the Duistermaat-Heckman measure associated to the\nmultiplicative problem concentrates in a small neighborhood of the octahedron\nrecurrence locus.",
        "In this article, we analyze the structure and relationships between magnitude\nhomology and Eulerian magnitude homology of finite graphs. Building on the work\nof Kaneta and Yoshinaga, Sazdanovic and Summers, and Asao and Izumihara, we\nprovide two proofs of the existence of torsion in Eulerian magnitude homology,\noffer insights into the types and orders of torsion, and present explicit\ncomputations for various classes of graphs."
      ]
    }
  },
  {
    "id":2411.05443,
    "research_type":"applied",
    "start_id":"b14",
    "start_title":"A Global Geometric Framework for Nonlinear Dimensionality Reduction",
    "start_abstract":"Scientists working with large volumes of high-dimensional data, such as global climate patterns, stellar spectra, or human gene distributions, regularly confront the problem dimensionality reduction: finding meaningful low-dimensional structures hidden in their observations. The brain confronts same everyday perception, extracting from its sensory inputs\u201430,000 auditory nerve fibers 106 optic fibers\u2014a manageably small number perceptually relevant features. Here we describe an approach to solving reduction problems that uses easily measured local metric information learn underlying geometry a data set. Unlike classical techniques principal component analysis (PCA) and multidimensional scaling (MDS), our is capable discovering nonlinear degrees freedom underlie complex natural observations, handwriting images face under different viewing conditions. In contrast previous algorithms for reduction, ours efficiently computes globally optimal solution, and, important class manifolds, guaranteed converge asymptotically true structure.",
    "start_categories":[
      "math.ST"
    ],
    "start_fields":[
      "Mathematics and Statistics"
    ],
    "target_paper":{
      "id":[
        "b9"
      ],
      "title":[
        "Topological Methods for the Analysis of High Dimensional Data Sets and 3D Object Recognition. The Eurographics Association"
      ],
      "abstract":[
        "We present a computational method for extracting simple descriptions of high dimensional data sets in the form of simplicial complexes. Our method, called Mapper, is based on the idea of partial clustering of the data guided by a set of functions defined on the data. The proposed method is not dependent on any particular clustering algorithm, i.e. any clustering algorithm may be used with Mapper. We implement this method and present a few sample applications in which simple descriptions of the data present important information about its structure."
      ],
      "categories":[
        "cs.DC"
      ]
    },
    "list":{
      "title":[
        "ED-DAO: Energy Donation Algorithms based on Decentralized Autonomous\n  Organization",
        "Access Specification-Aware Software Transactional Memory Techniques for\n  Efficient Execution of Smart Contract Transactions",
        "GMB-ECC: Guided Measuring and Benchmarking of the Edge Cloud Continuum",
        "Jenga: Effective Memory Management for Serving LLM with Heterogeneity",
        "Byzantine-Tolerant Consensus in GPU-Inspired Shared Memory",
        "Performance Models for a Two-tiered Storage System",
        "Light Virtualization: a proof-of-concept for hardware-based\n  virtualization",
        "NM-SpMM: Accelerating Matrix Multiplication Using N:M Sparsity with\n  GPGPU",
        "Atomic Smart Contract Interoperability with High Efficiency via\n  Cross-Chain Integrated Execution",
        "MonadBFT: Fast, Responsive, Fork-Resistant Streamlined Consensus",
        "Understanding the Communication Needs of Asynchronous Many-Task Systems\n  -- A Case Study of HPX+LCI",
        "Closing a Source Complexity Gap between Chapel and HPX",
        "Enhancing Memory Efficiency in Large Language Model Training Through\n  Chronos-aware Pipeline Parallelism",
        "The effect of accretion on scalar superradiant instability",
        "Scaling of the elastic proton-proton cross-section",
        "Feedback-enhanced squeezing or cooling of fluctuations in a parametric\n  resonator",
        "Ising superconductivity in noncentrosymmetric bulk NbSe2",
        "Investigating and Improving Counter-Stereotypical Action Relation in\n  Text-to-Image Diffusion Models",
        "LIFT-GS: Cross-Scene Render-Supervised Distillation for 3D Language\n  Grounding",
        "Selection Function of Clusters in Dark Energy Survey Year 3 Data from\n  Cross-Matching with South Pole Telescope Detections",
        "The Complexity of Local Stoquastic Hamiltonians on 2D Lattices",
        "Neural cyberattacks applied to the vision under realistic visual stimuli",
        "Proof-Producing Translation of Functional Programs into a Time \\& Space\n  Reasonable Model",
        "Neural Radiance Fields for the Real World: A Survey",
        "Israel-Hamas war through Telegram, Reddit and Twitter",
        "Roles of the $N(1535)$ and $a_0(980)$ in the process $\\Lambda_c^+ \\to\n  \\pi^+\\eta n$",
        "Resurgence of the Tilted Cusp Anomalous Dimension",
        "New Frontiers in Fighting Misinformation"
      ],
      "abstract":[
        "Energy is a fundamental component of modern life, driving nearly all aspects\nof daily activities. As such, the inability to access energy when needed is a\nsignificant issue that requires innovative solutions. In this paper, we propose\nED-DAO, a novel fully transparent and community-driven decentralized autonomous\norganization (DAO) designed to facilitate energy donations. We analyze the\nenergy donation process by exploring various approaches and categorizing them\nbased on both the source of donated energy and funding origins. We propose a\nnovel Hybrid Energy Donation (HED) algorithm, which enables contributions from\nboth external and internal donors. External donations are payments sourced from\nentities such as charities and organizations, where energy is sourced from the\nutility grid and prosumers. Internal donations, on the other hand, come from\npeer contributors with surplus energy. HED prioritizes donations in the\nfollowing sequence: peer-sourced energy (P2D), utilitygrid-sourced energy\n(UG2D), and direct energy donations by peers (P2PD). By merging these donation\napproaches, the HED algorithm increases the volume of donated energy, providing\na more effective means to address energy poverty. Experiments were conducted on\na dataset to evaluate the effectiveness of the proposed method. The results\nshowed that HED increased the total donated energy by at least 0.43% (64\nmegawatts) compared to the other algorithms (UG2D, P2D, and P2PD).",
        "For a high-performance blockchain like Supra's Layer 1, minimizing latencies\nacross key components is crucial-such as data dissemination, consensus (or\nordering), and transaction execution. While through significant innovations we\nhave improved the first two, transaction execution remains an area for further\noptimization. Software Transactional Memory (STM) is a widely used technique\nfor parallel execution, with Aptos' BlockSTM pioneering its application of\nefficient blockchain transaction processing on multi-core validator nodes.\nSubsequently, PEVM [13] adapted BlockSTM for EVM transaction execution.\nHowever, we identified a gap in existing STM techniques-while access\nspecifications have been used in industry (e.g., Solana's user-provided\nread-write sets), they have not been leveraged to enhance STM efficiency. Our\nexperimental analysis demonstrates that specification-aware STMs outperform\ntheir plain counterparts on both EVM and MoveVM. To maximize these benefits, we\nhave designed specification-aware SupraSTM (saSupraSTM), a novel algorithm that\nfully utilizes access specifications. Through extensive testing, saSupraSTM\noutperforms both our specification-aware adaptation of Aptos' BlockSTM and\nspecification-aware PEVM, setting a new benchmark for transaction execution\nefficiency in the context of blockchain networks.",
        "In the evolving landscape of cloud computing, optimizing energy efficiency\nacross the edge-cloud continuum is crucial for sustainability and\ncost-effectiveness. We introduce GMB-ECC, a framework for measuring and\nbenchmarking energy consumption across the software and hardware layers of the\nedge-cloud continuum. GMB-ECC enables energy assessments in diverse\nenvironments and introduces a precision parameter to adjust measurement\ncomplexity, accommodating system heterogeneity. We demonstrate GMB-ECC's\napplicability in an autonomous intra-logistic use case, highlighting its\nadaptability and capability in optimizing energy efficiency without\ncompromising performance. Thus, this framework not only assists in accurate\nenergy assessments but also guides strategic optimizations, cultivating\nsustainable and cost-effective operations.",
        "Large language models (LLMs) are widely used but expensive to run, especially\nas inference workloads grow. To lower costs, maximizing the request batch size\nby managing GPU memory efficiently is crucial. While PagedAttention has\nrecently been proposed to improve the efficiency of memory management, we find\nthat the growing heterogeneity in the embeddings dimensions, attention, and\naccess patterns of modern LLM architectures introduces new challenges for\nmemory allocation.\n  In this paper, we present Jenga, a novel memory allocation framework for\nheterogeneous embeddings in LLMs. Jenga tackles two key challenges: (1)\nminimizing memory fragmentation when managing embeddings of different sizes,\nand (2) enabling flexible caching and eviction policies tailored to the\nspecific token-dependency patterns of various layers. Jenga employs a two-level\nmemory allocator, leveraging the least common multiple (LCM) of embedding sizes\nto optimize memory usage and providing APIs to express layer-specific caching\nlogic to enhance memory reuse.\n  We implemente Jenga on vLLM, a state-of-the-art LLM inference engine, and\nevaluate it with diverse LLMs, datasets, and GPU configurations. Evaluations\nshow that Jenga improves GPU memory utilization by up to 79.6%, and increases\nserving throughput by up to 4.92x (1.80x on average).",
        "In this work, we formalize a novel shared memory model inspired by the\npopular GPU architecture. Within this model, we develop algorithmic solutions\nto the Byzantine Consensus problem.",
        "This work describes the design, implementation and performance analysis of a\ndistributed two-tiered storage software. The first tier functions as a\ndistributed software cache implemented using solid-state devices~(NVMes) and\nthe second tier consists of multiple hard disks~(HDDs). We describe an online\nlearning algorithm that manages data movement between the tiers. The software\nis hybrid, i.e. both distributed and multi-threaded. The end-to-end performance\nmodel of the two-tier system was developed using queuing networks and\nbehavioral models of storage devices. We identified significant parameters that\naffect the performance of storage devices and created behavioral models for\neach device. The performance of the software was evaluated on a many-core\ncluster using non-trivial read\/write workloads. The paper provides examples to\nillustrate the use of these models.",
        "Virtualization has become widespread across all computing environments, from\nedge devices to cloud systems. Its main advantages are resource management\nthrough abstraction and improved isolation of platform resources and processes.\nHowever, there are still some important tradeoffs as it requires significant\nsupport from the existing hardware infrastructure and negatively impacts\nperformance. Additionally, the current approaches to resource virtualization\nare inflexible, using a model that doesn't allow for dynamic adjustments during\noperation. This research introduces Light Virtualization (LightV), a new\nvirtualization method for commercial platforms. LightV uses programmable\nhardware to direct cache coherence traffic, enabling precise and seamless\ncontrol over which resources are virtualized. The paper explains the core\nprinciples of LightV, explores its capabilities, and shares initial findings\nfrom a basic proof-of-concept module tested on commercial hardware.",
        "Deep learning demonstrates effectiveness across a wide range of tasks.\nHowever, the dense and over-parameterized nature of these models results in\nsignificant resource consumption during deployment. In response to this issue,\nweight pruning, particularly through N:M sparsity matrix multiplication, offers\nan efficient solution by transforming dense operations into semi-sparse ones.\nN:M sparsity provides an option for balancing performance and model accuracy,\nbut introduces more complex programming and optimization challenges. To address\nthese issues, we design a systematic top-down performance analysis model for\nN:M sparsity. Meanwhile, NM-SpMM is proposed as an efficient general N:M\nsparsity implementation. Based on our performance analysis, NM-SpMM employs a\nhierarchical blocking mechanism as a general optimization to enhance data\nlocality, while memory access optimization and pipeline design are introduced\nas sparsity-aware optimization, allowing it to achieve close-to-theoretical\npeak performance across different sparsity levels. Experimental results show\nthat NM-SpMM is 2.1x faster than nmSPARSE (the state-of-the-art for general N:M\nsparsity) and 1.4x to 6.3x faster than cuBLAS's dense GEMM operations, closely\napproaching the theoretical maximum speedup resulting from the reduction in\ncomputation due to sparsity. NM-SpMM is open source and publicly available at\nhttps:\/\/github.com\/M-H482\/NM-SpMM.",
        "With the development of Ethereum, numerous blockchains compatible with\nEthereum's execution environment (i.e., Ethereum Virtual Machine, EVM) have\nemerged. Developers can leverage smart contracts to run various complex\ndecentralized applications on top of blockchains. However, the increasing\nnumber of EVM-compatible blockchains has introduced significant challenges in\ncross-chain interoperability, particularly in ensuring efficiency and atomicity\nfor the whole cross-chain application. Existing solutions are either limited in\nguaranteeing overall atomicity for the cross-chain application, or inefficient\ndue to the need for multiple rounds of cross-chain smart contract execution. To\naddress this gap, we propose IntegrateX, an efficient cross-chain\ninteroperability system that ensures the overall atomicity of cross-chain smart\ncontract invocations. The core idea is to deploy the logic required for\ncross-chain execution onto a single blockchain, where it can be executed in an\nintegrated manner. This allows cross-chain applications to perform all\ncross-chain logic efficiently within the same blockchain. IntegrateX consists\nof a cross-chain smart contract deployment protocol and a cross-chain smart\ncontract integrated execution protocol. The former achieves efficient and\nsecure cross-chain deployment by decoupling smart contract logic from state,\nand employing an off-chain cross-chain deployment mechanism combined with\non-chain cross-chain verification. The latter ensures atomicity of cross-chain\ninvocations through a 2PC-based mechanism, and enhances performance through\ntransaction aggregation and fine-grained state lock. We implement a prototype\nof IntegrateX. Extensive experiments demonstrate that it reduces up to 61.2%\nlatency compared to the state-of-the-art baseline while maintaining low gas\nconsumption.",
        "This paper introduces MonadBFT, a novel Byzantine Fault Tolerant (BFT)\nconsensus protocol designed to significantly enhance both performance and\nscalability. MonadBFT achieves linear message and authenticator complexity on\nthe happy path, enabling it to improve decentralization. It achieves\nspeculative finality within a single round and is optimistically responsive.\nThe speculative mechanism is refined such that only block equivocation can\nrevert speculative execution, enabling the protocol to ensure accountability\nfor malicious behavior. A notable innovation of MonadBFT is its built-in\nresistance to a specific form of Maximal Extractable Value (MEV) vulnerability\nknown as tail-forking. Tail-forking occurs when a malicious leader forks away\nfrom its predecessor's block, causing that block to be abandoned and depriving\nthe predecessor of rewards. This allows the malicious leader to reorder, steal,\nor exploit transactions, thereby exacerbating MEV exploitation. MonadBFT\neffectively mitigates such vulnerabilities, ensuring fairness and integrity in\ntransaction processing. To our knowledge, no other pipelined leader-based BFT\nconsensus protocol combines all these features.",
        "Asynchronous Many-Task (AMT) systems offer a potential solution for\nefficiently programming complicated scientific applications on extreme-scale\nheterogeneous architectures. However, they exhibit different communication\nneeds from traditional bulk-synchronous parallel (BSP) applications, posing new\nchallenges for underlying communication libraries. This work systematically\nstudies the communication needs of AMTs and explores how communication\nlibraries can be structured to better satisfy them through a case study of a\nreal-world AMT system, HPX. We first examine its communication stack layout and\nformalize the communication abstraction that underlying communication libraries\nneed to support. We then analyze its current MPI backend (parcelport) and\nidentify four categories of needs that are not typical in the BSP model and are\nnot well covered by the MPI standard. To bridge these gaps, we design from the\nnative network layer and incorporate various techniques, including one-sided\ncommunication, queue-based completion notification, explicit progressing, and\ndifferent ways of resource contention mitigation, in a new parcelport with an\nexperimental communication library, LCI. Overall, the resulting LCI parcelport\noutperforms the existing MPI parcelport with up to 50x in microbenchmarks and\n2x in a real-world application. Using it as a testbed, we design LCI parcelport\nvariants to quantify the performance contributions of each technique. This work\ncombines conceptual analysis and experiment results to offer a practical\nguideline for the future development of communication libraries and AMT\ncommunication layers.",
        "A previous case study measured performance vs source-code complexity across\nmultiple languages. The case study identified Chapel and HPX provide similar\nperformance and code complexity. This paper is the result of initial steps\ntoward closing the source-code complexity gap between Chapel and HPX by using a\nsource-to-source compiler. The investigation assesses the single-machine\nperformance of both Chapel and Chplx applications across Arm and x86.",
        "Larger model sizes and longer sequence lengths have empowered the Large\nLanguage Model (LLM) to achieve outstanding performance across various domains.\nHowever, this progress brings significant storage capacity challenges for LLM\npretraining. High Bandwidth Memory (HBM) is expensive and requires more\nadvanced packaging technologies for capacity expansion, creating an urgent need\nfor memory-efficient scheduling strategies. Yet, prior pipeline parallelism\nschedules have primarily focused on reducing bubble overhead, often neglecting\nmemory efficiency and lacking compatibility with other memory-efficient\nstrategies. Consequently, these methods struggle to meet the storage demands of\nstorage capacity for next-generation LLM. This work presents ChronosPipe, a\nChronos-aware pipeline parallelism for memory-efficient LLM pretraining. The\ncore insight of ChronosPipe is to treat HBM as a fast but small 'cache,'\noptimizing and exploiting temporal locality within LLM pretraining to enhance\nHBM utilization. ChronosPipe introduces a pipeline scheduling strategy,\nChronos-Pipe, to reduce the extrinsic overhead that disrupts the temporal\nlocality of activations. Additionally, it leverages Chronos-Recomp and\nChronos-Offload to efficiently harness the intrinsic temporal locality of\nactivations and weights in Deep Neural Networks. Experiment results show that\nChronosPipe can expand the trainable model size by 2.4x while maintaining\ncomparable throughput, achieving 1.5x better than the 1F1B strategy combined\nwith recomputation.",
        "Superradiance can lead to the formation of a black hole (BH) condensate\nsystem. We thoroughly investigate the accretion effect on the evolution of this\nsystem, and the gravitational wave signals it emits in the presence of multiple\nsuperradiance modes. Assuming the multiplication of the BH mass and scalar mass\nas a small number, we obtain the analytical approximations of all important\nquantities, which can be directly applied to phenomenological studies. In\naddition, we confirm that accretion could significantly enhance the\ngravitational wave (GW) emission and reduce its duration, and show that the GW\nbeat signature is similarly modified.",
        "We discuss scaling properties of the elastic $pp$ cross-section both at the\nISR and the LHC. We observe that the ratio of bump to dip positions of the\ndifferential cross-section $d\\sigma_{\\rm el}\/dt$ is constant over a wide energy\nrange. We next study the consequences of this property, including geometric\nscaling at the ISR and new scaling laws at the LHC.",
        "Here we analyse ways to achieve deep subthreshold parametric squeezing of\nfluctuations beyond the $-6$~dB limit of single degree-of-freedom parametric\nresonators. One way of accomplishing this is via a lock-in amplifier feedback\nloop. Initially, we calculate the phase-dependent parametric amplification with\nfeedback of an added ac signal. In one approach, we use the averaging method to\nobtain the amplification gain, while in the second approach, we obtain the ac\nresponse of the parametric amplifier with feedback using the harmonic balance\nmethod. In this latter approach, the feedback is proportional to an integral\nterm that emulates the cosine quadrature output of a lock-in amplifier\nmultiplied by a sine at the same tone of the lock-in. We find that the gain\nobtained via these two methods are the same whenever the integration time span\nof the integral is a multiple of the tone period. When this is not the case, we\ncan obtain considerable deamplification. Finally, we analyse the response of\nthe parametric resonator with feedback, described by this integro-differential\nmodel, to an added white noise in the frequency domain. Using this model we\nwere able to calculate, in addition to squeezing, the noise spectral density in\nthis resonator with feedback. Very strong squeezing or cooling can be obtained.",
        "Ising superconductivity allows in-plane upper critical magnetic fields to\nvastly surpass Pauli limit by locking the antiparallel electron spins of Cooper\npairs in the out-of-plane direction. It was first explicitly demonstrated in\nfully two-dimensional monolayers of transition metal dichalcogenides with large\nspin-orbit coupling and broken inversion symmetry. Since then, several studies\nhave shown that it can be present in layered bulk materials, too. In our\nprevious study, we have clarified the underlying microscopic mechanism of Ising\nsuperconductivity in bulk, based on a reduced electronic coupling between\nsuperconducting layers due to intercalation by insulating layers and restricted\ninversion symmetry. But earlier studies suggest that in some transition metal\ndichalcogenide polytypes Pauli paramagnetic limit is violated even without\nintercalation. Here, using heat capacity measurements we unambiguously\ndemonstrate, that the pristine noncentrosymmetric bulk 4Ha-NbSe2 polytype\nsignificantly violates the Pauli limit. The band structure parameters obtained\nfrom ab initio calculations using the experimentally determined crystal\nstructure are used in the theoretical model which provides the microscopic\nmechanism of the Ising protection based solely on broken inversion symmetry.",
        "Text-to-image diffusion models consistently fail at generating\ncounter-stereotypical action relationships (e.g., \"mouse chasing cat\"),\ndefaulting to frequent stereotypes even when explicitly prompted otherwise.\nThrough systematic investigation, we discover this limitation stems from\ndistributional biases rather than inherent model constraints. Our key insight\nreveals that while models fail on rare compositions when their inversions are\ncommon, they can successfully generate similar intermediate compositions (e.g.,\n\"mouse chasing boy\"). To test this hypothesis, we develop a Role-Bridging\nDecomposition framework that leverages these intermediates to gradually teach\nrare relationships without architectural modifications. We introduce\nActionBench, a comprehensive benchmark specifically designed to evaluate\naction-based relationship generation across stereotypical and\ncounter-stereotypical configurations. Our experiments validate that\nintermediate compositions indeed facilitate counter-stereotypical generation,\nwith both automatic metrics and human evaluations showing significant\nimprovements over existing approaches. This work not only identifies\nfundamental biases in current text-to-image systems but demonstrates a\npromising direction for addressing them through compositional reasoning.",
        "Our approach to training 3D vision-language understanding models is to train\na feedforward model that makes predictions in 3D, but never requires 3D labels\nand is supervised only in 2D, using 2D losses and differentiable rendering. The\napproach is new for vision-language understanding. By treating the\nreconstruction as a ``latent variable'', we can render the outputs without\nplacing unnecessary constraints on the network architecture (e.g. can be used\nwith decoder-only models). For training, only need images and camera pose, and\n2D labels. We show that we can even remove the need for 2D labels by using\npseudo-labels from pretrained 2D models. We demonstrate this to pretrain a\nnetwork, and we finetune it for 3D vision-language understanding tasks. We show\nthis approach outperforms baselines\/sota for 3D vision-language grounding, and\nalso outperforms other 3D pretraining techniques. Project page:\nhttps:\/\/liftgs.github.io.",
        "Galaxy clusters selected based on overdensities of galaxies in photometric\nsurveys provide the largest cluster samples. Yet modeling the selection\nfunction of such samples is complicated by non-cluster members projected along\nthe line of sight (projection effects) and the potential detection of\nunvirialized objects (contamination). We empirically constrain the magnitude of\nthese effects by cross-matching galaxy clusters selected in the Dark Energy\nsurvey data with the \\rdmpr$\\,$ algorithm with significant detections in three\nSouth Pole Telescope surveys (SZ, pol-ECS, pol-500d). For matched clusters, we\naugment the \\rdmpr$\\,$catalog by the SPT detection significance. For unmatched\nobjects we use the SPT detection threshold as an upper limit on the SZe\nsignature. Using a Bayesian population model applied to the collected\nmulti-wavelength data, we explore various physically motivated models to\ndescribe the relationship between observed richness and halo mass. Our analysis\nreveals the limitations of a simple lognormal scatter model in describing the\ndata. We rule out significant contamination by unvirialized objects at the\nhigh-richness end of the sample. While dedicated simulations offer a\nwell-fitting calibration of projection effects, our findings suggest the\npresence of redshift-dependent trends that these simulations may not have\ncaptured. Our findings highlight that modeling the selection function of\noptically detected clusters remains a complicated challenge, requiring a\ncombination of simulation and data-driven approaches.",
        "We show the 2-Local Stoquastic Hamiltonian problem on a 2D square lattice is\nStoqMA-complete. We achieve this by extending the spatially sparse circuit\nconstruction of Oliveira and Terhal, as well as the perturbative gadgets of\nBravyi, DiVincenzo, Oliveira, and Terhal. Our main contributions demonstrate\nStoqMA circuits can be made spatially sparse and that geometrical,\nstoquastic-preserving, perturbative gadgets can be constructed.",
        "Brain-Computer Interfaces (BCIs) are systems traditionally used in medicine\nand designed to interact with the brain to record or stimulate neurons. Despite\ntheir benefits, the literature has demonstrated that invasive BCIs focused on\nneurostimulation present vulnerabilities allowing attackers to gain control. In\nthis context, neural cyberattacks emerged as threats able to disrupt\nspontaneous neural activity by performing neural overstimulation or inhibition.\nPrevious work validated these attacks in small-scale simulations with a reduced\nnumber of neurons, lacking real-world complexity. Thus, this work tackles this\nlimitation by analyzing the impact of two existing neural attacks, Neuronal\nFlooding (FLO) and Neuronal Jamming (JAM), on a complex neuronal topology of\nthe primary visual cortex of mice consisting of approximately 230,000 neurons,\ntested on three realistic visual stimuli: flash effect, movie, and drifting\ngratings. Each attack was evaluated over three relevant events per stimulus,\nalso testing the impact of attacking 25% and 50% of the neurons. The results,\nbased on the number of spikes and shift percentages metrics, showed that the\nattacks caused the greatest impact on the movie, while dark and fixed events\nare the most robust. Although both attacks can significantly affect neural\nactivity, JAM was generally more damaging, producing longer temporal delays,\nand had a larger prevalence. Finally, JAM did not require to alter many neurons\nto significantly affect neural activity, while the impact in FLO increased with\nthe number of neurons attacked.",
        "We present a semi-automated framework to construct and reason about programs\nin a deeply-embedded while-language. The while-language we consider is a simple\ncomputation model that can simulate (and be simulated by) Turing machines with\na linear time and constant space blow-up. Our framework derives while-programs\nfrom functional programs written in a subset of Isabelle\/HOL, namely\ntail-recursive functions with first-order arguments and algebraic datatypes. As\nfar as we are aware, it is the first framework targeting a computation model\nthat is reasonable in time and space from a complexity-theoretic perspective.",
        "Neural Radiance Fields (NeRFs) have remodeled 3D scene representation since\nrelease. NeRFs can effectively reconstruct complex 3D scenes from 2D images,\nadvancing different fields and applications such as scene understanding, 3D\ncontent generation, and robotics. Despite significant research progress, a\nthorough review of recent innovations, applications, and challenges is lacking.\nThis survey compiles key theoretical advancements and alternative\nrepresentations and investigates emerging challenges. It further explores\napplications on reconstruction, highlights NeRFs' impact on computer vision and\nrobotics, and reviews essential datasets and toolkits. By identifying gaps in\nthe literature, this survey discusses open challenges and offers directions for\nfuture research.",
        "The Israeli-Palestinian conflict started on 7 October 2023, have resulted\nthus far to over 48,000 people killed including more than 17,000 children with\na majority from Gaza, more than 30,000 people injured, over 10,000 missing, and\nover 1 million people displaced, fleeing conflict zones. The infrastructure\ndamage includes the 87\\% of housing units, 80\\% of public buildings and 60\\% of\ncropland 17 out of 36 hospitals, 68\\% of road networks and 87\\% of school\nbuildings damaged. This conflict has as well launched an online discussion\nacross various social media platforms. Telegram was no exception due to its\nencrypted communication and highly involved audience. The current study will\ncover an analysis of the related discussion in relation to different\nparticipants of the conflict and sentiment represented in those discussion. To\nthis end, we prepared a dataset of 125K messages shared on channels in Telegram\nspanning from 23 October 2025 until today. Additionally, we apply the same\nanalysis in two publicly available datasets from Twitter containing 2001 tweets\nand from Reddit containing 2M opinions. We apply a volume analysis across the\nthree datasets, entity extraction and then proceed to BERT topic analysis in\norder to extract common themes or topics. Next, we apply sentiment analysis to\nanalyze the emotional tone of the discussions. Our findings hint at polarized\nnarratives as the hallmark of how political factions and outsiders mold public\nopinion. We also analyze the sentiment-topic prevalence relationship, detailing\nthe trends that may show manipulation and attempts of propaganda by the\ninvolved parties. This will give a better understanding of the online discourse\non the Israel-Palestine conflict and contribute to the knowledge on the\ndynamics of social media communication during geopolitical crises.",
        "We have investigated the process $\\Lambda_c^+ \\to \\pi^+\\eta n$ by taking into\naccount the contributions from the nucleon resonance $N(1535)$ and the scalar\nmeson $a_0(980)$, which could be dynamically generated by the interaction of\nthe $S$-wave pseudosalar meson-octet baryon and the $S$-wave pseudosalar\nmeson-pseudosalar meson, respectively. Our results show that, in $\\eta n$\ninvariant mass distribution, there is a significant near-threshold enhancement\nstructure, which could be associated with $N(1535)$. On the other hand, one can\nfind a clear cusp structure of $a_0(980)$ in $\\pi^+\\eta$ invariant mass\ndistribution. We further estimate the ratio $R$ = $\\mathcal{B}(\\Lambda_c^+ \\to\na_0(980)^+ n)\/\\mathcal{B}(\\Lambda_c^+ \\to \\pi^+\\eta n)\\approx 0.313$. Our\nresults can be tested by BESIII, Belle~II, and the proposed Super Tau-Charm\nFacility experiments in the future.",
        "We use resurgent extrapolation and continuation methods to extract detailed\nanalytic information about the tilted cusp anomalous dimension solely from its\nweak coupling and strong coupling expansions. This enables accurate and smooth\ninterpolation between the weak and strong coupling limits, and identifies the\nrelevant singularities governing the finite radius of convergence of the weak\ncoupling expansion and the asymptotic nature of the strong coupling expansion.\nThe input data is purely perturbative, generated from the BES equations, and\nthese resurgent methods extract accurate non-perturbative information which\nmatches the underlying physical structure.",
        "Despite extensive research and development of tools and technologies for\nmisinformation tracking and detection, we often find ourselves largely on the\nlosing side of the battle against misinformation. In an era where\nmisinformation poses a substantial threat to public discourse, trust in\ninformation sources, and societal and political stability, it is imperative\nthat we regularly revisit and reorient our work strategies. While we have made\nsignificant strides in understanding how and why misinformation spreads, we\nmust now broaden our focus and explore how technology can help realise new\napproaches to address this complex challenge more efficiently."
      ]
    }
  },
  {
    "id":2411.04992,
    "research_type":"applied",
    "start_id":"b16",
    "start_title":"Transfer Entropy Bottleneck: Learning Sequence to Sequence Information Transfer",
    "start_abstract":"When presented with a data stream of two statistically dependent variables, predicting the future one variables (the target stream) can benefit from information about both its history and other variable source stream). For example, fluctuations in temperature at weather station be predicted using temperatures barometric readings. However, challenge when modelling such is that it easy for neural network to rely on greatest joint correlations within stream, which may ignore crucial but small transfer stream. As well, there are often situations where have previously been modelled independently would useful use model inform new model. Here, we develop an bottleneck approach conditional learning streams data. Our method, call Transfer Entropy Bottleneck (TEB), allows learn bottlenecks directed transferred variable, while quantifying this such, TEB provides order make predictions them.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b5"
      ],
      "title":[
        "Symbolic Transfer Entropy"
      ],
      "abstract":[
        "We propose to estimate transfer entropy using a technique of symbolization. demonstrate numerically that symbolic is robust and computationally fast method quantify the dominating direction information flow between time series from structurally identical nonidentical coupled systems. Analyzing multiday, multichannel electroencephalographic recordings 15 epilepsy patients our approach allowed us reliably identify hemisphere containing epileptic focus without observing actual seizure activity."
      ],
      "categories":[
        "physics.data-an"
      ]
    },
    "list":{
      "title":[
        "Dynamic Metadata Schemes in the Neutron and Photon Science Communities:\n  A Case Study of X-Ray Photon Correlation Spectroscopy",
        "Disentangling sources of multifractality in time series",
        "Ordinal language of antipersistent binary walks",
        "Sparse identification of evolution equations via Bayesian model\n  selection",
        "Halo spin and orientation in Interacting Dark Matter Dark Energy\n  Cosmology",
        "Signatures of extreme events in the cumulative entropic spectrum",
        "High-Performance Data Format for Scientific Data Storage and Analysis",
        "Time-resolved Hubble Space Telescope UV observations of an X-ray\n  quasi-periodic eruption source",
        "Gravitational Wave Scattering via the Born Series: Scalar Tidal Matching\n  to $\\mathcal{O}(G^7)$ and Beyond",
        "Geometrically Templated Dynamic Wrinkling from Suspended Poly(vinyl\n  alcohol) Soap Films",
        "An Accessible Formulation for Defining the SI Second Based on Multiple\n  Atomic Transitions",
        "A skeletonization based image segmentation algorithm to isolate slender\n  regions in 3D microstructures",
        "Maximum likelihood estimation of burst-merging kernels for bursty time\n  series",
        "A domain decomposition strategy for natural imposition of mixed boundary\n  conditions in port-Hamiltonian systems",
        "Non-reciprocal interactions drive emergent chiral crystallites",
        "Reduced Basis Model for Compressible Flow",
        "Double Momentum and Error Feedback for Clipping with Fast Rates and\n  Differential Privacy",
        "Application of resolved low-J multi-CO line modeling with RADEX to\n  constrain the molecular gas properties in the starburst M82",
        "A regional implementation of a mixed finite-element, semi-implicit\n  dynamical core",
        "1\/f and Random Telegraph Noise of Single-Layer Graphene Devices with\n  Interdigitated Electrodes",
        "Multivariate Distribution-Free Nonparametric Testing: Generalizing\n  Wilcoxon's Tests via Optimal Transport",
        "Spectrum management and the EVN",
        "Multipole generalization of the Witten effect in Mie-resonant photonics",
        "Congruence properties of prime sums and Bernoulli polynomials",
        "Determination of unscaled blood input for human dynamic FDG brain PET",
        "Harnessing Hybrid Frequency-Entangled Qudits through Quantum\n  Interference",
        "$^{18}$F-FDG brain PET hypometabolism in post-SARS-CoV-2 infection:\n  substrate for persistent\/delayed disorders?",
        "Quantum Maslov classes"
      ],
      "abstract":[
        "Metadata is one of the most important aspects for advancing data management\npractices within all research communities. Definitions and schemes of metadata\nare inter alia of particular significance in the domain of neutron and photon\nscattering experiments covering a broad area of different scientific\ndisciplines. The demand of describing continuously evolving highly\nnonstandardized experiments, including the resulting processed and published\ndata, constitutes a considerable challenge for a static definition of metadata.\nHere, we present the concept of dynamic metadata for the neutron and photon\nscientific community, which enriches a static set of defined basic metadata. We\nexplore the idea of dynamic metadata with the help of the use case of X-ray\nPhoton Correlation Spectroscopy (XPCS), which is a synchrotron-based scattering\ntechnique that allows the investigation of nanoscale dynamic processes. It\nserves here as a demonstrator of how dynamic metadata can improve data\nacquisition, sharing, and analysis workflows. Our approach enables researchers\nto tailor metadata definitions dynamically and adapt them to the evolving\ndemands of describing data and results from a diverse set of experiments. We\ndemonstrate that dynamic metadata standards yield advantages that enhance data\nreproducibility, interoperability, and the dissemination of knowledge.",
        "This contribution addresses the question commonly asked in scientific\nliterature about the sources of multifractality in time series. Two primary\nsources are typically considered. These are temporal correlations and heavy\ntails in the distribution of fluctuations. Most often, they are treated as two\nindependent components, while true multifractality cannot occur without\ntemporal correlations. The distributions of fluctuations affect the span of the\nmultifractal spectrum only when correlations are present. These issues are\nillustrated here using series generated by several model mathematical cascades,\nwhich by design build correlations into these series. The thickness of the\ntails of fluctuations in such series is then governed by an appropriate\nprocedure of adjusting them to $q$-Gaussian distributions, and $q$ is treated\nas a variable parameter that, while preserving correlations, allows to tune\nthese distributions to the desired functional form. Multifractal detrended\nfluctuation analysis (MFDFA), as the most commonly used practical method for\nquantifying multifractality, is then used to identify the influence of the\nthickness of the fluctuation tails in the presence of temporal correlations on\nthe width of multifractal spectra. The obtained results point to the Gaussian\ndistribution, so $q=1$, as the appropriate reference distribution to evaluate\nthe contribution of fatter tails to the width of multifractal spectra. An\nappropriate procedure is presented to make such estimates.",
        "This paper explores the effectiveness of using ordinal pattern probabilities\nto evaluate antipersistency in the sign decomposition of long-range\nanti-correlated Gaussian fluctuations. It is numerically shown that ordinal\npatterns are able to effectively measure both persistent and antipersistent\ndynamics by analyzing the sign decomposition derived from fractional Gaussian\nnoise. These findings are crucial given that traditional methods such as\nDetrended Fluctuation Analysis are unsuccessful in detecting anti-correlations\nin such sequences. The numerical results are supported by physiological and\nenvironmental data, illustrating its applicability in real-world situations.",
        "The quantitative formulation of evolution equations is the backbone for\nprediction, control, and understanding of dynamical systems across diverse\nscientific fields. Besides deriving differential equations for dynamical\nsystems based on basic scientific reasoning or prior knowledge in recent times\na growing interest emerged to infer these equations purely from data. In this\narticle, we introduce a novel method for the sparse identification of nonlinear\ndynamical systems from observational data, based on the observation how the key\nchallenges of the quality of time derivatives and sampling rates influence this\nproblem. Our approach combines system identification based on thresholded least\nsquares minimization with additional error measures that account for both the\ndeviation between the model and the time derivative of the data, and the\nintegrated performance of the model in forecasting dynamics. Specifically, we\nintegrate a least squares error as well as the Wasserstein metric for estimated\nmodels and combine them within a Bayesian optimization framework to efficiently\ndetermine optimal hyperparameters for thresholding and weighting of the\ndifferent error norms. Additionally, we employ distinct regularization\nparameters for each differential equation in the system, enhancing the method's\nprecision and flexibility. We demonstrate the capabilities of our approach\nthrough applications to dynamical fMRI data and the prototypical example of a\nwake flow behind a cylinder. In the wake flow problem, our method identifies a\nsparse, accurate model that correctly captures transient dynamics, oscillation\nperiods, and phase information, outperforming existing methods. In the fMRI\nexample, we show how our approach extracts insights from a trained recurrent\nneural network, offering a novel avenue for explainable AI by inferring\ndifferential equations that capture potentially causal relationships.",
        "In recent years, the interaction between dark matter (DM) and dark energy has\nbecome a topic of interest in cosmology. Interacting dark matter-dark energy\n(IDE) models have a substantial impact on the formation of cosmological\nlarge-scale structures, which serve as the background for DM halo evolution.\nThis impact can be examined through the shape and spin orientation of halos in\nnumerical simulations incorporating IDE effects. In our work, we use the N-body\nsimulation pipeline ME-GADGET to simulate and study the halo spin and\norientation in IDE models. We found that in models where DM transfers into DE\n(IDE I), the alignment of halo shapes with the surrounding tidal field is\nenhanced, while the alignment of halo spins with the tidal field is decreased\ncompared to {\\Lambda}CDM. Conversely, in models where DE transfers into DM (IDE\nII), the opposite occurs. We have provided fitted functions to describe these\nalignment signals. Our study provides the foundation for more accurate modeling\nof observations in the future such as China Space Station Telescope.",
        "In this study, the cumulative effect of the empirical probability\ndistribution of a random variable is identified as a factor that amplifies the\noccurrence of extreme events in datasets. To quantify this observation, a\ncorresponding information measure is introduced, drawing upon Shannon entropy\nfor joint probabilities. The proposed approach is validated using selected\nmarket data as case studies, encompassing various instances of extreme events.\nIn particular, the results indicate that the introduced cumulative measure\nexhibits distinctive signatures of such events, even when the data is\nrelatively noisy. These findings highlight the potential of the discussed\nconcept for developing a new class of related indicators or classifiers.",
        "In this article, we present the High-Performance Output (HiPO) data format\ndeveloped at Jefferson Laboratory for storing and analyzing data from Nuclear\nPhysics experiments. The format was designed to efficiently store large amounts\nof experimental data, utilizing modern fast compression algorithms. The purpose\nof this development was to provide organized data in the output, facilitating\naccess to relevant information within the large data files. The HiPO data\nformat has features that are suited for storing raw detector data,\nreconstruction data, and the final physics analysis data efficiently,\neliminating the need to do data conversions through the lifecycle of\nexperimental data. The HiPO data format is implemented in C++ and JAVA, and\nprovides bindings to FORTRAN, Python, and Julia, providing users with the\nchoice of data analysis frameworks to use. In this paper, we will present the\ngeneral design and functionalities of the HiPO library and compare the\nperformance of the library with more established data formats used in data\nanalysis in High Energy and Nuclear Physics (such as ROOT and Parquete).",
        "X-ray quasi-periodic eruptions (QPEs) are a novel mode of variability in\nnearby galactic nuclei whose origin remains unknown. Their multi-wavelength\nproperties are poorly constrained, as studies have focused almost entirely on\nthe X-ray band. Here we report on time-resolved, coordinated Hubble Space\nTelescope far ultraviolet and XMM-Newton X-ray observations of the shortest\nperiod X-ray QPE source currently known, eRO-QPE2. We detect a bright UV point\nsource ($L_{\\rm FUV} \\approx {\\rm few} \\times 10^{41}$ erg s$^{-1}$) that does\nnot show statistically significant variability between the X-ray eruption and\nquiescent phases. This emission is unlikely to be powered by a young stellar\npopulation in a nuclear stellar cluster. The X-ray-to-UV spectral energy\ndistribution can be described by a compact accretion disk ($R_{\\rm out} =\n343^{+202}_{-138} \\ R_{\\rm g}$). Such compact disks are incompatible with\ntypical disks in active galactic nuclei, but form naturally following the tidal\ndisruption of a star. Our results rule out models (for eRO-QPE2) invoking i) a\nclassic AGN accretion disk and ii) no accretion disk at all. For orbiter\nmodels, the expected radius derived from the timing properties would naturally\nlead to disk-orbiter interactions for both quasi-spherical and eccentric\ntrajectories. We infer a black hole mass of log$_{10}(M_{\\rm BH}) = 5.9 \\pm\n0.3$ M$_{\\odot}$ and Eddington ratio of 0.13$^{+0.18}_{-0.07}$; in combination\nwith the compact outer radius this is inconsistent with existing disk\ninstability models. After accounting for the quiescent disk emission, we\nconstrain the ratio of X-ray to FUV luminosity of the eruption component to be\n$L_{\\rm X} \/ L_{\\rm FUV} > 16-85$ (depending on the intrinsic extinction).",
        "We introduce a novel method to compute gravitational wave amplitudes within\nthe framework of effective field theory. By reinterpreting the Feynman diagram\nexpansion as a Born series, our method offers several key advantages. It\ndirectly yields partial wave amplitudes, streamlining the matching with black\nhole perturbation theory. Long-distance gravitational interactions are\nunambiguously factorized from short-distance tidal effects, including\ndissipation, which are systematically incorporated via an in-in worldline\neffective action. Crucially, at every order in perturbation theory, integrals\nare expressed in terms of harmonic polylogarithms, enabling an end-to-end\ncomputation scalable to arbitrary orders. We illustrate the method with new\npredictions for scalar black hole Love numbers and their Renormalization Group\nequations to $\\mathcal{O}(G^7)$.",
        "Wrinkling is commonly observed as mechanical instability when a stiff thin\nfilm bound on a compliant thick substrate undergoes in-plane compression\nexceeding a threshold. Despite significant efforts to create a broad range of\nsurface patterns via wrinkling, little has been studied about a dynamic and\ntransient wrinkling process, where a suspended polymer thin film undergoes\nliquid-to-solid phase transitions. Here, a spontaneous wrinkling process is\nreported, when drying poly(vinyl alcohol) (PVA) soap films suspended on 3D\nprinted wireframes with near zero or negative Gaussian curvatures. As water\nevaporates, a thickness gradient across the sample is developed, leading to\nnon-uniform drying rates, and a concentration gradient between the inner and\nouter sides (exposed to air) of the suspended PVA soap film induces a\ndifferential osmotic pressure. Together, these effects contribute to an\nin-plane compressive stress, leading to the formation of surface wrinkles,\nwhose growth is guided by the geometry of the frame. Importantly, the wrinkles\nevolve dynamically: the wavelength and number of the wrinkles can be tuned by\naltering the concentration of the PVA aqueous solutions, the initial mass, the\nrelative humidity of the drying environment; the patterns of the resulting\nwrinkles can be programmed by the geometry of the wireframe.",
        "This work presents a novel formulation for a redefinition of the second based\non the weighted arithmetic mean of multiple normalized frequencies. We\ndemonstrate that it is mathematically equivalent to the previously discussed\nimplementation employing a geometric mean. In our reformulation, the\nnormalization of frequencies provides the defining constants with immediate\nphysical meaning, while maintaining the decoupling of assigned weights from the\nfrequencies of the reference transitions. We believe that a definition based on\nthis formulation would be significantly more accessible to both experts and\nnon-specialists, enhancing understanding and facilitating broader acceptance.\nWe hope that this approach will help overcome barriers to the adoption of a\nredefinition that effectively values all state-of-the-art atomic clocks.",
        "The work proposes an image segmentation algorithm that isolates slender\nregions in three-dimensional microstructures. Characterizing slender regions in\nmaterial microstructures is an extremely important aspect in material science\nbecause these regions govern the macroscopic behavior of materials for many\napplications like energy absorption, activation of metamaterials, stability of\nhigh temperature filters, etc. This work utilizes skeletonization method to\ncalculate centerline of the microstructure geometry followed by a novel pruning\nstrategy based on cross-sectional area to identify slender regions in the\nmicrostructure. 3D images of such microstructures obtained from micro-CT often\nsuffer from low image resolution resulting in high surface noise. The skeleton\nof such an image has many spurious skeletal branches that do not represent the\nactual microstructure geometry. The proposed pruning method of cross-sectional\narea is insensitive to surface noise and hence is a reliable method of\nidentifying skeletal branches that represent the slender regions in the\nmicrostructure. The proposed algorithm is implemented on a test case to\nshowcase its effectiveness. Further it is implemented on a 3D microstructure of\nceramic foam to identify the slender regions present in it. It is shown that\nthe method can be used to segment slender regions of varying dimensions and to\nstudy their geometric properties.",
        "Various time series in natural and social processes have been found to be\nbursty. Events in the time series rapidly occur within short time periods,\nforming bursts, which are alternated with long inactive periods. As the\ntimescale defining bursts increases, individual events are sequentially merged\nto become small bursts and then bigger ones, eventually leading to the single\nburst containing all events. Such a merging pattern has been depicted by a tree\nthat fully reveals the hierarchical structure of bursts, thus called a burst\ntree. The burst-tree structure can be simply characterized by a burst-merging\nkernel that dictates which bursts are merged together as the timescale\nincreases. In this work, we develop the maximum likelihood estimation method of\nthe burst-merging kernel from time series, which is successfully tested against\nthe time series generated using several model kernels. We also apply our method\nto some empirical time series from various backgrounds. Our method provides a\nuseful tool to precisely characterize the time series data, hence enabling to\nstudy their underlying mechanisms more accurately.",
        "In this contribution, a finite element scheme to impose mixed boundary\nconditions without introducing Lagrange multipliers is presented for wave\npropagation phenomena described as port-Hamiltonian systems. The strategy\nrelies on finite element exterior calculus and a domain decomposition to\ninterconnect two systems with different causalities. The spatial domain is\nsplit into two parts by introducing an arbitrary interface. Each subdomain is\ndiscretized with a mixed finite element formulation that introduces a uniform\nboundary condition in a natural way as the input. In each subdomain the spaces\nare selected from a finite element subcomplex to obtain a stable\ndiscretization. The two systems are then interconnected together by making use\nof a feedback interconnection. This is achieved by discretizing the boundary\ninputs using appropriate spaces that couple the two formulations. The final\nsystems includes all boundary conditions explicitly and does not contain any\nLagrange multiplier. Each subdomain is integrated using an implicit midpoint\nscheme in an uncoupled way from the other by means of a leapfrog scheme. The\nproposed strategy is tested on three different examples: the Euler-Bernoulli\nbeam, the wave equation and the Maxwell equations. Numerical tests assess the\nconservation properties of the scheme and the effectiveness of the methodology.",
        "We study a new type of 2D active material that exhibits macroscopic phases\nwith two emergent broken symmetries: self-propelled achiral particles that form\ndense hexatic clusters, which spontaneously rotate. We experimentally realise\nactive colloids that self-organise into both polar and hexatic crystallites,\nexhibiting exotic emergent phenomena. This is accompanied by a field theory of\ncoupled order parameters formulated on symmetry principles, including\nnon-reciprocity, to capture the non-equilibrium dynamics. We find that the\npresence of two interacting broken symmetry fields leads to the emergence of\nnovel chiral phases built from (2D) achiral active colloids (here Quincke\nrollers). These phases are characterised by the presence of both clockwise and\ncounterclockwise rotating clusters. We thus show that spontaneous rotation can\nemerge in non-equilibrium systems, even when the building blocks are achiral,\ndue to non-reciprocally coupled broken symmetries. This interplay leads to\nself-organized stirring through counter-rotating vortices in confined colloidal\nsystems, with cluster size controlled by external electric fields.",
        "Numerical simulations are a valuable research and layout tool for fluid flow\nproblems, yet repeated evaluations of parametrized problems, necessary to solve\noptimization problems, can be very costly. One option to speed up this process\nis to replace the costly CFD model with a cheaper one. These surrogate models\ncan be either data-driven or they can also rely on reduced basis (RB) methods\nto speed up the calculations. In contrast to data-driven surrogate models, the\nlatter are not based on regression techniques but are still aimed at explicitly\nsolving the conservation equations. Their speed-up comes from a strong\nreduction of the solution space, which results in much smaller algebraic\nsystems that need to be solved. Within this work, an RB model, suited for\nslightly compressible flow, is presented and tested on different flow\nconfigurations. The model is stabilized using a Petrov-Galerkin method with\ntrial and test function spaces of different dimensionality to generate stable\nresults for a wide range of Reynolds numbers. The presented model applies to\ngeometrically and physically parametrized flow problems. Finally, a data-driven\napproach was used to extend it to turbulent flows.",
        "Strong Differential Privacy (DP) and Optimization guarantees are two\ndesirable properties for a method in Federated Learning (FL). However, existing\nalgorithms do not achieve both properties at once: they either have optimal DP\nguarantees but rely on restrictive assumptions such as bounded\ngradients\/bounded data heterogeneity, or they ensure strong optimization\nperformance but lack DP guarantees. To address this gap in the literature, we\npropose and analyze a new method called Clip21-SGD2M based on a novel\ncombination of clipping, heavy-ball momentum, and Error Feedback. In\nparticular, for non-convex smooth distributed problems with clients having\narbitrarily heterogeneous data, we prove that Clip21-SGD2M has optimal\nconvergence rate and also near optimal (local-)DP neighborhood. Our numerical\nexperiments on non-convex logistic regression and training of neural networks\nhighlight the superiority of Clip21-SGD2M over baselines in terms of the\noptimization performance for a given DP-budget.",
        "The distribution and physical conditions of molecular gas are closely linked\nto star formation and the subsequent evolution of galaxies. Emission from\ncarbon monoxide (CO) and its isotopologues traces the bulk of molecular gas and\nprovides constraints on the physical conditions through their line ratios.\nHowever, comprehensive understanding on how the particular choice of line\nmodeling approach impacts derived molecular properties remain incomplete. Here,\nwe study the nearby starburst galaxy M82, known for its intense star formation\nand molecular emission, using the large set of available multi-CO line\nobservations. We present high-resolution (${\\sim}85$ pc) emission of seven CO\nisotopologue lines, including $^{12}$CO, $^{13}$CO, and C$^{18}$O from the $J =\n1-0$, $2-1$ and $3-2$ transitions. Using \\texttt{RADEX} for radiative transfer\nmodeling, we analyze M82\\textsc{\\char39}s molecular properties with (i) a\none-zone model and (ii) a variable density model, comparing observed and\nsimulated emissions via a minimum $\\chi^2$ analysis. We find that inferred gas\nconditions -- kinetic temperature and density -- are consistent across models,\nwith minimal statistical differences. However, due to their low critical\ndensities (${<}10^{4}$ cm$^{-3}$), low-$J$ CO isotopologue lines do not\neffectively probe higher density gas prevalent in starburst environments like\nthat of M82. Our results further imply that this limitation extends to\nhigh-redshift ($z{\\gtrapprox}1$) galaxies with similar conditions, where\nlow-$J$ CO lines are inadequate for density constraints. Future studies of\nextreme star-forming regions like M82 will require higher-$J$ CO lines or\nalternative molecular tracers with higher critical densities.",
        "This paper explores how to adapt a new dynamical core to enable its use in\none-way nested regional weather and climate models, where lateral boundary\nconditions (LBCs) are provided by a lower-resolution driving model. The\ndynamical core has recently been developed by the Met Office and uses an\niterated-semi-implicit time discretisation and mixed finite-element spatial\ndiscretisation.\n  The essential part of the adaptation is the addition of the LBCs to the\nright-hand-side of the linear system which solves for pressure and momentum\nsimultaneously. The impacts on the associated Helmholtz preconditioner and\nmultigrid techniques are also described.\n  The regional version of the dynamical core is validated through big-brother\nexperiments based on idealised dynamical core tests. These experiments\ndemonstrate that the subdomain results are consistent with those from the full\ndomain, confirming the correct application of LBCs. Inconsistencies arise in\ncases where the LBCs are not perfect, but it is shown that the application of\nblending can be used to overcome these problems.",
        "Single-layer Graphene (SLG) is a promising material for sensing applications.\nHigh performance graphene sensors can be achieved when Interdigitated\nElectrodes (IDE) are used. In this research work, we fabricated SLG\nmicro-ribbon (GMR) devices with IDE having different geometric parameters. 1\/f\nnoise behavior was observed in all of the examined devices, and in some cases\nrandom telegraph noise (RTN) signals suggesting that carrier\ntrapping\/de-trapping is taking place. Our experimental results indicate that\nthe geometrical characteristics can have a crucial impact on device\nperformance, due to the direct area dependence of the noise level.",
        "This paper reviews recent advancements in the application of optimal\ntransport (OT) to multivariate distribution-free nonparametric testing.\nInspired by classical rank-based methods, such as Wilcoxon's rank-sum and\nsigned-rank tests, we explore how OT-based ranks and signs generalize these\nconcepts to multivariate settings, while preserving key properties, including\ndistribution-freeness, robustness, and efficiency. Using the framework of\nasymptotic relative efficiency (ARE), we compare the power of the proposed\n(generalized Wilcoxon) tests against the Hotelling's $T^2$ test. The ARE lower\nbounds reveal the Hodges-Lehmann and Chernoff-Savage phenomena in the context\nof multivariate location testing, underscoring the high power and efficiency of\nthe proposed methods. We also demonstrate how OT-based ranks and signs can be\nseamlessly integrated with more modern techniques, such as kernel methods, to\ndevelop universally consistent, distribution-free tests. Additionally, we\npresent novel results on the construction of consistent and distribution-free\nkernel-based tests for multivariate symmetry, leveraging OT-based ranks and\nsigns.",
        "In recent years, the utilisation of the radio spectrum has dramatically\nincreased. Digital telecommunication applications, be it terrestrial cell-phone\nnetworks or new-space low-earth orbit satellite constellations, have not only\nacquired unprecedented amounts of spectrum but also use their frequencies\neverywhere on Earth. The consequences for radio astronomy and other scientific\nradio services are severe. A single cell-phone tower within hundreds of\nkilometers around a radio telescope can blind us and there is no place on Earth\nto escape the ubiquitous transmissions of satellite megaconstellations.\n  Since 1988, the Committee on Radio Astronomy Frequencies (CRAF) is advocating\nfor astronomers' rights to use the spectrum. CRAF does this by participation in\nthe national and international regulatory frameworks. Hundreds if not thousands\nof documents need to be processed every year. CRAF not only contributes to\nregulatory texts, but even more importantly, performs spectrum compatibility\ncalculations. In this contribution, CRAF's latest activities are summarized\nwith a focus on matters relevant to EVN operations.",
        "We present a generalization of the Witten effect on the case of oscillating\nmultipole sources exciting nonreciprocal sphere with effective axion response.\nWe find that the fields outside of the sphere are presented as a superposition\nof electric and magnetic multipoles. In addition to appearance of\ncross-polarized component in the radiation, Mie resonances of the system\nhybridize with each other, exhibiting characteristic double peaks in Mie\nspectra observed especially clearly for higher-order multipole resonances. This\ncharacteristic feature may provide a sensitive probe of axion-type\nnonreciprocal responses in Mie-resonant photonics.",
        "In this article, we derive a congruence property of particular sum rules\ninvolving prime numbers. The resulting expression involves Bernoulli numbers\nand polynomials, for which we obtain, as a consequence, a general congruence\nrelation as well.",
        "Objectives: Many existing techniques for the non-invasive quantification of\nthe blood input function in dynamic FDG-PET imaging require strong historical\ninformation or user input. The technique proposed in this work utilizes the\nassumption that a dynamic PET scan can be modeled by the Patlak plot to\ndetermine an unscaled blood input function. Materials and Methods: The time\nactivity curve (TAC) for each voxel in a dynamic image can be considered as an\nn-dimensional vector. In this context, a TAC follows the Patlak plot if and\nonly if the TAC is a linear combination of the blood input function and the\nintegral of the blood input function. Given a set of TACs which follow the\nPatlak plot, we can thus use PCA to determine a basis which spans the same\nvector space as the blood input function and the integral of the blood input\nfunction. We then seek to find two TACs in this vector space which best satisfy\nthat the estimated anti-derivative of one of the TACs is close to the other\nTAC; such TACs are candidates for the blood input function and the integral of\nthe blood input function. We were able to construct a low (2) dimensional\noptimization problem to find such TACs. Results: We applied our results to\nobtain predicted blood input functions and Ki maps for twelve normal subjects.\nScaling the predicted blood input function to best match the ground truth, we\nachieved an average SSE of $0.042 \\pm 0.032$ and an average DTW distance of\n$0.141 \\pm 0.053$. Matching the means of the predicted and ground truth Ki\nmaps, we achieved an average MAPE of $2.539 \\pm 0.928$ and an average SSIM of\n$0.991 \\pm 0.005$. Conclusion: While not often viewed as such, the assumption\nthat some dynamic data follows a kinetic model gives strong prior information.\nIn the case of the Patlak plot, we can use this assumption to estimate an\nunscaled blood input function and unscaled Ki map.",
        "High-dimensional (HD) quantum entanglement expands the Hilbert space,\noffering a robust framework for quantum information processing with enhanced\ncapacity and error resilience. In this work, we present a novel HD\nfrequency-domain entangled state, the hybrid frequency-entangled qudit (HFEQ),\ngenerated via Hong-Ou-Mandel (HOM) interference, exhibiting both\ndiscrete-variable (DV) and continuous-variable (CV) characteristics. By tuning\nHOM interference, we generate and control HFEQs with dimensions $D=5,7,9,11$,\nconfirming their DV nature. Franson interferometry confirms the global\nfrequency correlations with visibility exceeding 98% and verifies the CV\nentanglement within individual frequency modes with visibility greater than\n95%. Our findings provide deeper insight into the physical nature of\nfrequency-entangled qudits generated by quantum interference and introduce a\nnovel resource for HD time-frequency quantum information processing.",
        "Purpose: Several brain complications of SARS-CoV-2 infection have been\nreported. It has been moreover speculated that this neurotropism could\npotentially cause a delayed outbreak of neuropsychiatric and neurodegenerative\ndiseases of neuroinflammatory origin. A propagation mechanism has been proposed\nacross the cribriform plate of the ethmoid bone, from the nose to the olfactory\nepithelium, and possibly afterward to other limbic structures, and deeper parts\nof the brain including the brainstem. Methods: Review of clinical examination,\nand whole-brain voxel-based analysis of $^{18}$F-FDG PET metabolism in\ncomparison with healthy subjects (p voxel<0.001, p-cluster<0.05, uncorrected),\nof two patients with confirmed diagnosis of SARS-CoV-2 explored at the\npost-viral stage of the disease. Results: Hypometabolism of the\nolfactory\/rectus gyrus was found on the two patients, especially one with\n4-week prolonged anosmia. Additional hypometabolisms were found within\namygdala, hippocampus, parahippocampus, cingulate cortex, pre-\/post-central\ngyrus, thalamus\/hypothalamus, cerebellum, pons, and medulla in the other\npatient who complained of delayed onset of a painful syndrome. Conclusion:\nThese preliminary findings reinforce the hypotheses of SARS-CoV-2 neurotropism\nthrough the olfactory bulb and the possible extension of this impairment to\nother brain structures. $^{18}$F-FDG PET hypometabolism could constitute a\ncerebral quantitative biomarker of this involvement. Post-viral cohort studies\nare required to specify the exact relationship between such hypometabolisms and\nthe possible persistent disorders, especially involving cognitive or emotion\ndisturbances, residual respiratory symptoms, or painful complaints.",
        "We give a construction of ``quantum Maslov characteristic classes'',\ngeneralizing to higher dimensional cycles the Hu-Lalonde-Seidel morphism. We\nalso state a conjecture extending this to an $A _{\\infty}$ functor from the\nexact path category of the space of monotone Lagrangian branes to the Fukaya\ncategory. Quantum Maslov classes are used here for the study of Hofer geometry\nof Lagrangian equators in $S ^{2}$, giving a rigidity phenomenon for the Hofer\nmetric 2-systole, which stands in contrast to the flexibility phenomenon of the\nclosely related Hofer metric girth studied by Rauch ~\\cite{cite_Itamar}, in the\nsame context of Lagrangian equators of $S ^{2}$. More applications appear in\n~\\cite{cite_SavelyevGlobalFukayacategoryII}."
      ]
    }
  },
  {
    "id":2411.04992,
    "research_type":"applied",
    "start_id":"b5",
    "start_title":"Symbolic Transfer Entropy",
    "start_abstract":"We propose to estimate transfer entropy using a technique of symbolization. demonstrate numerically that symbolic is robust and computationally fast method quantify the dominating direction information flow between time series from structurally identical nonidentical coupled systems. Analyzing multiday, multichannel electroencephalographic recordings 15 epilepsy patients our approach allowed us reliably identify hemisphere containing epileptic focus without observing actual seizure activity.",
    "start_categories":[
      "physics.data-an"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b16"
      ],
      "title":[
        "Transfer Entropy Bottleneck: Learning Sequence to Sequence Information Transfer"
      ],
      "abstract":[
        "When presented with a data stream of two statistically dependent variables, predicting the future one variables (the target stream) can benefit from information about both its history and other variable source stream). For example, fluctuations in temperature at weather station be predicted using temperatures barometric readings. However, challenge when modelling such is that it easy for neural network to rely on greatest joint correlations within stream, which may ignore crucial but small transfer stream. As well, there are often situations where have previously been modelled independently would useful use model inform new model. Here, we develop an bottleneck approach conditional learning streams data. Our method, call Transfer Entropy Bottleneck (TEB), allows learn bottlenecks directed transferred variable, while quantifying this such, TEB provides order make predictions them."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "FedPCA: Noise-Robust Fair Federated Learning via Performance-Capacity\n  Analysis",
        "Technical Report: Aggregation on Learnable Manifolds for Asynchronous\n  Federated Optimization",
        "When do neural networks learn world models?",
        "mPOLICE: Provable Enforcement of Multi-Region Affine Constraints in Deep\n  Neural Networks",
        "A Survey of Direct Preference Optimization",
        "Probabilistic neural operators for functional uncertainty quantification",
        "Techniques for Enhancing Memory Capacity of Reservoir Computing",
        "Temporal Distribution Shift in Real-World Pharmaceutical Data:\n  Implications for Uncertainty Quantification in QSAR Models",
        "FedRand: Enhancing Privacy in Federated Learning with Randomized LoRA\n  Subparameter Updates",
        "Unraveling Zeroth-Order Optimization through the Lens of Low-Dimensional\n  Structured Perturbations",
        "Modeling Attention during Dimensional Shifts with Counterfactual and\n  Delayed Feedback",
        "Multi-Source Urban Traffic Flow Forecasting with Drone and Loop Detector\n  Data",
        "TeZO: Empowering the Low-Rankness on the Temporal Dimension in the\n  Zeroth-Order Optimization for Fine-tuning LLMs",
        "Collective Reasoning Among LLMs A Framework for Answer Validation\n  Without Ground Truth",
        "Predictive Target-to-User Association in Complex Scenarios via\n  Hybrid-Field ISAC Signaling",
        "States of Disarray: Cleaning Data for Gerrymandering Analysis",
        "Structure and Context of Retweet Coordination in the 2022 U.S. Midterm\n  Elections",
        "AccessFixer: Enhancing GUI Accessibility for Low Vision Users With R-GCN\n  Model",
        "Ro-To-Go! Robust Reactive Control with Signal Temporal Logic",
        "Inverse Intersections for Boolean Satisfiability Problems",
        "Electromagnetic Side-Channel Analysis of PRESENT Lightweight Cipher",
        "Local well-posedness for nonlinear Schr\\\"odinger equations on compact\n  product manifolds",
        "Spectrum of L\\'evy-Ornstein-Uhlenbeck semigroups on $\\mathbb{R}^d$",
        "Spike-and-Slab Posterior Sampling in High Dimensions",
        "Tight Bounds for some Classical Problems Parameterized by Cutwidth",
        "Large Language Models For Text Classification: Case Study And\n  Comprehensive Review",
        "CARE: Confidence-Aware Regression Estimation of building density\n  fine-tuning EO Foundation Models",
        "Complexity of approximate conflict-free, linearly-ordered, and\n  nonmonochromatic hypergraph colourings"
      ],
      "abstract":[
        "Training a model that effectively handles both common and rare data-i.e.,\nachieving performance fairness-is crucial in federated learning (FL). While\nexisting fair FL methods have shown effectiveness, they remain vulnerable to\nmislabeled data. Ensuring robustness in fair FL is therefore essential.\nHowever, fairness and robustness inherently compete, which causes robust\nstrategies to hinder fairness. In this paper, we attribute this competition to\nthe homogeneity in loss patterns exhibited by rare and mislabeled data clients,\npreventing existing loss-based fair and robust FL methods from effectively\ndistinguishing and handling these two distinct client types. To address this,\nwe propose performance-capacity analysis, which jointly considers model\nperformance on each client and its capacity to handle the dataset, measured by\nloss and a newly introduced feature dispersion score. This allows mislabeled\nclients to be identified by their significantly deviated performance relative\nto capacity while preserving rare data clients. Building on this, we introduce\nFedPCA, an FL method that robustly achieves fairness. FedPCA first identifies\nmislabeled clients via a Gaussian Mixture Model on loss-dispersion pairs, then\napplies fairness and robustness strategies in global aggregation and local\ntraining by adjusting client weights and selectively using reliable data.\nExtensive experiments on three datasets demonstrate FedPCA's effectiveness in\ntackling this complex challenge. Code will be publicly available upon\nacceptance.",
        "In Federated Learning (FL), a primary challenge to the server-side\naggregation of client models is device heterogeneity in both loss landscape\ngeometry and computational capacity. This issue can be particularly pronounced\nin clinical contexts where variations in data distribution (aggravated by class\nimbalance), infrastructure requirements, and sample sizes are common. We\npropose AsyncManifold, a novel asynchronous FL framework to address these\nissues by taking advantage of underlying solution space geometry at each of the\nlocal training, delay-correction, and aggregation stages. Our proposal is\naccompanied by a convergence proof in a general form and, motivated through\nexploratory studies of local behaviour, a proof-of-concept algorithm which\nperforms aggregation along non-linear mode connections and hence avoids\nbarriers to convergence that techniques based on linear interpolation will\nencounter.",
        "Humans develop world models that capture the underlying generation process of\ndata. Whether neural networks can learn similar world models remains an open\nproblem. In this work, we provide the first theoretical results for this\nproblem, showing that in a multi-task setting, models with a low-degree bias\nprovably recover latent data-generating variables under mild assumptions --\neven if proxy tasks involve complex, non-linear functions of the latents.\nHowever, such recovery is also sensitive to model architecture. Our analysis\nleverages Boolean models of task solutions via the Fourier-Walsh transform and\nintroduces new techniques for analyzing invertible Boolean transforms, which\nmay be of independent interest. We illustrate the algorithmic implications of\nour results and connect them to related research areas, including\nself-supervised learning, out-of-distribution generalization, and the linear\nrepresentation hypothesis in large language models.",
        "Deep neural networks are increasingly employed in fields such as climate\nmodeling, robotics, and industrial control, where strict output constraints\nmust be upheld. Although prior methods like the POLICE algorithm can enforce\naffine constraints in a single convex region by adjusting network parameters,\nthey struggle with multiple disjoint regions, often leading to conflicts or\nunintended affine extensions. We present mPOLICE, a new method that extends\nPOLICE to handle constraints imposed on multiple regions. mPOLICE assigns a\ndistinct activation pattern to each constrained region, preserving exact affine\nbehavior locally while avoiding overreach into other parts of the input domain.\nWe formulate a layer-wise optimization problem that adjusts both the weights\nand biases to assign unique activation patterns to each convex region, ensuring\nthat constraints are met without conflicts, while maintaining the continuity\nand smoothness of the learned function. Our experiments show the enforcement of\nmulti-region constraints for multiple scenarios, including regression and\nclassification, function approximation, and non-convex regions through\napproximation. Notably, mPOLICE adds zero inference overhead and minimal\ntraining overhead.",
        "Large Language Models (LLMs) have demonstrated unprecedented generative\ncapabilities, yet their alignment with human values remains critical for\nensuring helpful and harmless deployments. While Reinforcement Learning from\nHuman Feedback (RLHF) has emerged as a powerful paradigm for aligning LLMs with\nhuman preferences, its reliance on complex reward modeling introduces inherent\ntrade-offs in computational efficiency and training stability. In this context,\nDirect Preference Optimization (DPO) has recently gained prominence as a\nstreamlined alternative that directly optimizes LLMs using human preferences,\nthereby circumventing the need for explicit reward modeling. Owing to its\ntheoretical elegance and computational efficiency, DPO has rapidly attracted\nsubstantial research efforts exploring its various implementations and\napplications. However, this field currently lacks systematic organization and\ncomparative analysis. In this survey, we conduct a comprehensive overview of\nDPO and introduce a novel taxonomy, categorizing previous works into four key\ndimensions: data strategy, learning framework, constraint mechanism, and model\nproperty. We further present a rigorous empirical analysis of DPO variants\nacross standardized benchmarks. Additionally, we discuss real-world\napplications, open challenges, and future directions for DPO. This work\ndelivers both a conceptual framework for understanding DPO and practical\nguidance for practitioners, aiming to advance robust and generalizable\nalignment paradigms. All collected resources are available and will be\ncontinuously updated at\nhttps:\/\/github.com\/liushunyu\/awesome-direct-preference-optimization.",
        "Neural operators aim to approximate the solution operator of a system of\ndifferential equations purely from data. They have shown immense success in\nmodeling complex dynamical systems across various domains. However, the\noccurrence of uncertainties inherent in both model and data has so far rarely\nbeen taken into account\\textemdash{}a critical limitation in complex, chaotic\nsystems such as weather forecasting. In this paper, we introduce the\nprobabilistic neural operator (PNO), a framework for learning probability\ndistributions over the output function space of neural operators. PNO extends\nneural operators with generative modeling based on strictly proper scoring\nrules, integrating uncertainty information directly into the training process.\nWe provide a theoretical justification for the approach and demonstrate\nimproved performance in quantifying uncertainty across different domains and\nwith respect to different baselines. Furthermore, PNO requires minimal\nadjustment to existing architectures, shows improved performance for most\nprobabilistic prediction tasks, and leads to well-calibrated predictive\ndistributions and adequate uncertainty representations even for long dynamical\ntrajectories. Implementing our approach into large-scale models for physical\napplications can lead to improvements in corresponding uncertainty\nquantification and extreme event identification, ultimately leading to a deeper\nunderstanding of the prediction of such surrogate models.",
        "Reservoir Computing (RC) is a bio-inspired machine learning framework, and\nvarious models have been proposed. RC is a well-suited model for time series\ndata processing, but there is a trade-off between memory capacity and\nnonlinearity. In this study, we propose methods to improve the memory capacity\nof reservoir models by modifying their network configuration except for the\ninside of reservoirs. The Delay method retains past inputs by adding delay node\nchains to the input layer with the specified number of delay steps. To suppress\nthe effect of input value increase due to the Delay method, we divide the input\nweights by the number of added delay steps. The Pass through method feeds input\nvalues directly to the output layer. The Clustering method divides the input\nand reservoir nodes into multiple parts and integrates them at the output\nlayer. We applied these methods to an echo state network (ESN), a typical RC\nmodel, and the chaotic Boltzmann machine (CBM)-RC, which can be efficiently\nimplemented in integrated circuits. We evaluated their performance on the NARMA\ntask, and measured information processing capacity (IPC) to evaluate the\ntrade-off between memory capacity and nonlinearity.",
        "The estimation of uncertainties associated with predictions from quantitative\nstructure-activity relationship (QSAR) models can accelerate the drug discovery\nprocess by identifying promising experiments and allowing an efficient\nallocation of resources. Several computational tools exist that estimate the\npredictive uncertainty in machine learning models. However, deviations from the\ni.i.d. setting have been shown to impair the performance of these uncertainty\nquantification methods. We use a real-world pharmaceutical dataset to address\nthe pressing need for a comprehensive, large-scale evaluation of uncertainty\nestimation methods in the context of realistic distribution shifts over time.\nWe investigate the performance of several uncertainty estimation methods,\nincluding ensemble-based and Bayesian approaches. Furthermore, we use this\nreal-world setting to systematically assess the distribution shifts in label\nand descriptor space and their impact on the capability of the uncertainty\nestimation methods. Our study reveals significant shifts over time in both\nlabel and descriptor space and a clear connection between the magnitude of the\nshift and the nature of the assay. Moreover, we show that pronounced\ndistribution shifts impair the performance of popular uncertainty estimation\nmethods used in QSAR models. This work highlights the challenges of identifying\nuncertainty quantification methods that remain reliable under distribution\nshifts introduced by real-world data.",
        "Federated Learning (FL) is a widely used framework for training models in a\ndecentralized manner, ensuring that the central server does not have direct\naccess to data from local clients. However, this approach may still fail to\nfully preserve data privacy, as models from local clients are exposed to the\ncentral server during the aggregation process. This issue becomes even more\ncritical when training vision-language models (VLMs) with FL, as VLMs can\neasily memorize training data instances, making them vulnerable to membership\ninference attacks (MIAs). To address this challenge, we propose the FedRand\nframework, which avoids disclosing the full set of client parameters. In this\nframework, each client randomly selects subparameters of Low-Rank Adaptation\n(LoRA) from the server and keeps the remaining counterparts of the LoRA weights\nas private parameters. After training both parameters on the client's private\ndataset, only the non-private client parameters are sent back to the server for\naggregation. This approach mitigates the risk of exposing client-side VLM\nparameters, thereby enhancing data privacy. We empirically validate that\nFedRand improves robustness against MIAs compared to relevant baselines while\nachieving accuracy comparable to methods that communicate full LoRA parameters\nacross several benchmark datasets.",
        "Zeroth-order (ZO) optimization has emerged as a promising alternative to\ngradient-based backpropagation methods, particularly for black-box optimization\nand large language model (LLM) fine-tuning. However, ZO methods suffer from\nslow convergence due to high-variance stochastic gradient estimators. While\nstructured perturbations, such as sparsity and low-rank constraints, have been\nexplored to mitigate these issues, their effectiveness remains highly\nunder-explored. In this work, we develop a unified theoretical framework that\nanalyzes both the convergence and generalization properties of ZO optimization\nunder structured perturbations. We show that high dimensionality is the primary\nbottleneck and introduce the notions of \\textit{stable rank} and\n\\textit{effective overlap} to explain how structured perturbations reduce\ngradient noise and accelerate convergence. Using the uniform stability under\nour framework, we then provide the first theoretical justification for why\nthese perturbations enhance generalization. Additionally, through empirical\nanalysis, we identify that \\textbf{block coordinate descent} (BCD) to be an\neffective structured perturbation method. Extensive experiments show that,\ncompared to existing alternatives, memory-efficient ZO (MeZO) with BCD\n(\\textit{MeZO-BCD}) can provide improved converge with a faster wall-clock\ntime\/iteration by up to $\\times\\textbf{2.09}$ while yielding similar or better\naccuracy.",
        "Attention can be used to inform choice selection in contextual bandit tasks\neven when context features have not been previously experienced. One example of\nthis is in dimensional shifts, where additional feature values are introduced\nand the relationship between features and outcomes can either be static or\nvariable. Attentional mechanisms have been extensively studied in contextual\nbandit tasks where the feedback of choices is provided immediately, but less\nresearch has been done on tasks where feedback is delayed or in counterfactual\nfeedback cases. Some methods have successfully modeled human attention with\nimmediate feedback based on reward prediction errors (RPEs), though recent\nresearch raises questions of the applicability of RPEs onto more general\nattentional mechanisms. Alternative models suggest that information theoretic\nmetrics can be used to model human attention, with broader applications to\nnovel stimuli. In this paper, we compare two different methods for modeling how\nhumans attend to specific features of decision making tasks, one that is based\non calculating an information theoretic metric using a memory of past\nexperiences, and another that is based on iteratively updating attention from\nreward prediction errors. We compare these models using simulations in a\ncontextual bandit task with both intradimensional and extradimensional domain\nshifts, as well as immediate, delayed, and counterfactual feedback. We find\nthat calculating an information theoretic metric over a history of experiences\nis best able to account for human-like behavior in tasks that shift dimensions\nand alter feedback presentation. These results indicate that information\ntheoretic metrics of attentional mechanisms may be better suited than RPEs to\npredict human attention in decision making, though further studies of human\nbehavior are necessary to support these results.",
        "Traffic forecasting is a fundamental task in transportation research, however\nthe scope of current research has mainly focused on a single data modality of\nloop detectors. Recently, the advances in Artificial Intelligence and drone\ntechnologies have made possible novel solutions for efficient, accurate and\nflexible aerial observations of urban traffic. As a promising traffic\nmonitoring approach, drone-captured data can create an accurate multi-sensor\nmobility observatory for large-scale urban networks, when combined with\nexisting infrastructure. Therefore, this paper investigates the problem of\nmulti-source traffic speed prediction, simultaneously using drone and loop\ndetector data. A simple yet effective graph-based model HiMSNet is proposed to\nintegrate multiple data modalities and learn spatio-temporal correlations.\nDetailed analysis shows that predicting accurate segment-level speed is more\nchallenging than the regional speed, especially under high-demand scenarios\nwith heavier congestions and varying traffic dynamics. Utilizing both drone and\nloop detector data, the prediction accuracy can be improved compared to\nsingle-modality cases, when the sensors have lower coverages and are subject to\nnoise. Our simulation study based on vehicle trajectories in a real urban road\nnetwork has highlighted the added value of integrating drones in traffic\nforecasting and monitoring.",
        "Zeroth-order optimization (ZO) has demonstrated remarkable promise in\nefficient fine-tuning tasks for Large Language Models (LLMs). In particular,\nrecent advances incorporate the low-rankness of gradients, introducing low-rank\nZO estimators to further reduce GPU memory consumption. However, most existing\nworks focus solely on the low-rankness of each individual gradient, overlooking\na broader property shared by all gradients throughout the training, i.e., all\ngradients approximately reside within a similar subspace. In this paper, we\nconsider two factors together and propose a novel low-rank ZO estimator, TeZO,\nwhich captures the low-rankness across both the model and temporal dimension.\nSpecifically, we represent ZO perturbations along the temporal dimension as a\n3D tensor and employ Canonical Polyadic Decomposition (CPD) to extract each\nlow-rank 2D matrix, significantly reducing the training cost. TeZO can also be\neasily extended to the Adam variant while consuming less memory than MeZO-SGD,\nand requiring about only 35% memory of MeZO-Adam. Both comprehensive\ntheoretical analysis and extensive experimental research have validated its\nefficiency, achieving SOTA-comparable results with lower overhead of time and\nmemory.",
        "We present a collaborative framework where multiple large language models,\nnamely GPT-4-0125-preview, Meta-LLaMA-3-70B-Instruct, Claude-3-Opus, and\nGemini-1.5-Flash, work together to generate and respond to complex PhD-level\nprobability questions in the absence of definitive ground truth. This study\nexplores how inter-model consensus enhances response reliability and serves as\na proxy for assessing the quality of generated questions. To quantify agreement\nand consistency, we employ statistical methods including chi-square tests,\nFleiss' Kappa, and confidence interval analysis, measuring both response\nprecision and question clarity. Our findings highlight that Claude and Gemini\ngenerate well-structured and less ambiguous questions, leading to higher\ninter-model agreement. This is reflected in their narrower confidence intervals\nand stronger alignment with answering models. Conversely, LLaMA demonstrates\nincreased variability and lower reliability in question formulation, as\nindicated by broader confidence intervals and reduced consensus rates. These\nresults suggest that multi-model collaboration not only enhances the\nreliability of responses but also provides a valuable framework for assessing\nand improving question quality in the absence of explicit ground truth. This\nresearch offers meaningful insights into optimizing AI-driven reasoning through\ncollaborative large-language model interactions.",
        "This paper presents a novel and robust target-to-user (T2U) association\nframework to support reliable vehicle-to-infrastructure (V2I) networks that\npotentially operate within the hybrid field (near-field and far-field). To\naddress the challenges posed by complex vehicle maneuvers and user association\nambiguity, an interacting multiple-model filtering scheme is developed, which\ncombines coordinated turn and constant velocity models for predictive\nbeamforming. Building upon this foundation, a lightweight association scheme\nleverages user-specific integrated sensing and communication (ISAC) signaling\nwhile employing probabilistic data association to manage clutter measurements\nin dense traffic. Numerical results validate that the proposed framework\nsignificantly outperforms conventional methods in terms of both tracking\naccuracy and association reliability.",
        "The mathematics of redistricting is an area of study that has exploded in\nrecent years. In particular, many different research groups and expert\nwitnesses in court cases have used outlier analysis to argue that a proposed\nmap is a gerrymander. This outlier analysis relies on having an ensemble of\npotential redistricting maps against which the proposed map is compared.\nArguably the most widely-accepted method of creating such an ensemble is to use\na Markov Chain Monte Carlo (MCMC) process. This process requires that various\npieces of data be gathered, cleaned, and coalesced into a single file that can\nbe used as the seed of the MCMC process.\n  In this article, we describe how we have begun this cleaning process for each\nstate, and made the resulting data available for the public at\nhttps:\/\/github.com\/eveomett-states . At the time of submission, we have data\nfor 22 states available for researchers, students, and the general public to\neasily access and analyze. We will continue the data cleaning process for each\nstate, and we hope that the availability of these datasets will both further\nresearch in this area, and increase the public's interest in and understanding\nof modern techniques to detect gerrymandering.",
        "The ability to detect coordinated activity in communication networks is an\nongoing challenge. Prior approaches emphasize considering any activity\nexceeding a specific threshold of similarity to be coordinated. However,\nidentifying such a threshold is often arbitrary and can be difficult to\ndistinguish from grassroots organized behavior. In this paper, we investigate a\nset of Twitter retweeting data collected around the 2022 US midterm elections,\nusing a latent sharing-space model, in which we identify the main components of\nan association network, thresholded with a k-nearest neighbor criterion. This\napproach identifies a distribution of association values with different roles\nin the network at different ranges, where the shape of the distribution\nsuggests a natural place to threshold for coordinated user candidates. We find\ncoordination candidates belonging to two broad categories, one involving music\nawards and promotion of Korean pop or Taylor Swift, the other being users\nengaged in political mobilization. In addition, the latent space suggests\ncommon motivations for different coordinated groups otherwise fragmented by\nusing an appropriately high threshold criterion for coordination.",
        "The Graphical User Interface (GUI) plays a critical role in the interaction\nbetween users and mobile applications (apps), aiming at facilitating the\noperation process. However, due to the variety of functions and\nnon-standardized design, GUIs might have many accessibility issues, like the\nsize of components being too small or their intervals being narrow. These\nissues would hinder the operation of low vision users, preventing them from\nobtaining information accurately and conveniently. Although several\ntechnologies and methods have been proposed to address these issues, they are\ntypically confined to issue identification, leaving the resolution in the hands\nof developers. Moreover, it can be challenging to ensure that the color, size,\nand interval of the fixed GUIs are appropriately compared to the original ones.\nIn this work, we propose a novel approach named AccessFixer, which utilizes the\nRelational-Graph Convolutional Neural Network (R-GCN) to simultaneously fix\nthree kinds of accessibility issues, including small sizes, narrow intervals,\nand low color contrast in GUIs. With AccessFixer, the fixed GUIs would have a\nconsistent color palette, uniform intervals, and adequate size changes achieved\nthrough coordinated adjustments to the attributes of related components. Our\nexperiments demonstrate the effectiveness and usefulness of AccessFixer in\nfixing GUI accessibility issues. After fixing 30 real-world apps, our approach\nsolves an average of 81.2% of their accessibility issues. Also, we apply\nAccessFixer to 10 open-source apps by submitting the fixed results with pull\nrequests (PRs) on GitHub. The results demonstrate that developers approve of\nour submitted fixed GUIs, with 8 PRs being merged or under fixing. A user study\nexamines that low vision users host a positive attitude toward the GUIs fixed\nby our method.",
        "Signal Temporal Logic (STL) robustness is a common objective for optimal\nrobot control, but its dependence on history limits the robot's decision-making\ncapabilities when used in Model Predictive Control (MPC) approaches. In this\nwork, we introduce Signal Temporal Logic robustness-to-go (Ro-To-Go), a new\nquantitative semantics for the logic that isolates the contributions of suffix\ntrajectories. We prove its relationship to formula progression for Metric\nTemporal Logic, and show that the robustness-to-go depends only on the suffix\ntrajectory and progressed formula. We implement robustness-to-go as the\nobjective in an MPC algorithm and use formula progression to efficiently\nevaluate it online. We test the algorithm in simulation and compare it to MPC\nusing traditional STL robustness. Our experiments show that using\nrobustness-to-go results in a higher success rate.",
        "Boolean Satisfiability (SAT) problems are expressed as mathematical formulas.\nThis paper presents an alternative matrix representation for any type of these\nSAT problems. It shows how to use this matrix representation to get the full\nset of valid assignments. It proves that this is the full set of answers for\nthe given problem, and it shows that this is exponential in size, relative to\nthe matrix. It then presents an algorithm that utilizes the inverses of the\nclauses in this matrix for faster searching through this set of answers. It\nshows that this algorithm is both correct and polynomial.",
        "Side-channel vulnerabilities pose an increasing threat to cryptographically\nprotected devices. Consequently, it is crucial to observe information leakages\nthrough physical parameters such as power consumption and electromagnetic (EM)\nradiation to reduce susceptibility during interactions with cryptographic\nfunctions. EM side-channel attacks are becoming more prevalent. PRESENT is a\npromising lightweight cryptographic algorithm expected to be incorporated into\nInternet-of-Things (IoT) devices in the future. This research investigates the\nEM side-channel robustness of PRESENT using a correlation attack model. This\nwork extends our previous Correlation EM Analysis (CEMA) of PRESENT with\nimproved results. The attack targets the Substitution box (S-box) and can\nretrieve 8 bytes of the 10-byte encryption key with a minimum of 256 EM\nwaveforms. This paper presents the process of EM attack modelling, encompassing\nboth simple and correlation attacks, followed by a critical analysis.",
        "We prove new local well-posedness results for nonlinear Schr\\\"odinger\nequations posed on a general product of spheres and tori, by the standard\napproach of multi-linear Strichartz estimates. To prove these estimates, we\nestablish and utilize multi-linear bounds for the joint spectral projector\nassociated to the Laplace--Beltrami operators on the individual sphere factors\nof the product manifold. To treat the particular case of the cubic NLS on a\nproduct of two spheres at critical regularity, we prove a sharp\n$L^\\infty_xL^p_t$ estimate of the solution to the linear Schr\\\"odinger equation\non the two-torus.",
        "We investigate the spectral properties of Markov semigroups associated with\nOrnstein-Uhlenbeck (OU) processes driven by L\\'evy processes. These semigroups\nare generated by non-local, non-self-adjoint operators. In the special case\nwhere the driving L\\'evy process is Brownian motion, one recovers the classical\ndiffusion OU semigroup, whose spectral properties have been extensively studied\nover past few decades. Our main results establish that, under suitable\nconditions on the L\\'evy process, the spectrum of the L\\'evy-OU semigroup in\nthe $L^p$-space weighted with the invariant distribution coincides with that of\nthe diffusion OU semigroup. Furthermore, when the drift matrix $B$ is\ndiagonalizable with real eigenvalues, we derive explicit formulas for\neigenfunctions and co-eigenfunctions--an observation that, to the best of our\nknowledge, has not appeared in the literature. We also show that the\nmultiplicities of the eigenvalues remain independent of the choice of the\nL\\'evy process. A key ingredient in our approach is intertwining relationship:\nwe prove that every L\\'evy-OU semigroup is intertwined with a diffusion OU\nsemigroup. Additionally, we examine the compactness properties of these\nsemigroups and provide examples of non-compact L\\'evy-OU semigroups.",
        "Posterior sampling with the spike-and-slab prior [MB88], a popular multimodal\ndistribution used to model uncertainty in variable selection, is considered the\ntheoretical gold standard method for Bayesian sparse linear regression [CPS09,\nRoc18]. However, designing provable algorithms for performing this sampling\ntask is notoriously challenging. Existing posterior samplers for Bayesian\nsparse variable selection tasks either require strong assumptions about the\nsignal-to-noise ratio (SNR) [YWJ16], only work when the measurement count grows\nat least linearly in the dimension [MW24], or rely on heuristic approximations\nto the posterior. We give the first provable algorithms for spike-and-slab\nposterior sampling that apply for any SNR, and use a measurement count\nsublinear in the problem dimension. Concretely, assume we are given a\nmeasurement matrix $\\mathbf{X} \\in \\mathbb{R}^{n\\times d}$ and noisy\nobservations $\\mathbf{y} = \\mathbf{X}\\mathbf{\\theta}^\\star + \\mathbf{\\xi}$ of a\nsignal $\\mathbf{\\theta}^\\star$ drawn from a spike-and-slab prior $\\pi$ with a\nGaussian diffuse density and expected sparsity k, where $\\mathbf{\\xi} \\sim\n\\mathcal{N}(\\mathbb{0}_n, \\sigma^2\\mathbf{I}_n)$. We give a polynomial-time\nhigh-accuracy sampler for the posterior $\\pi(\\cdot \\mid \\mathbf{X},\n\\mathbf{y})$, for any SNR $\\sigma^{-1}$ > 0, as long as $n \\geq k^3 \\cdot\n\\text{polylog}(d)$ and $X$ is drawn from a matrix ensemble satisfying the\nrestricted isometry property. We further give a sampler that runs in\nnear-linear time $\\approx nd$ in the same setting, as long as $n \\geq k^5 \\cdot\n\\text{polylog}(d)$. To demonstrate the flexibility of our framework, we extend\nour result to spike-and-slab posterior sampling with Laplace diffuse densities,\nachieving similar guarantees when $\\sigma = O(\\frac{1}{k})$ is bounded.",
        "Cutwidth is a widely studied parameter that quantifies how well a graph can\nbe decomposed along small edge-cuts. It complements pathwidth, which captures\ndecomposition by small vertex separators, and it is well-known that cutwidth\nupper-bounds pathwidth. The SETH-tight parameterized complexity of problems on\ngraphs of bounded pathwidth (and treewidth) has been actively studied over the\npast decade while for cutwidth the complexity of many classical problems\nremained open.\n  For Hamiltonian Cycle, it is known that a $(2+\\sqrt{2})^{\\operatorname{pw}}\nn^{O(1)}$ algorithm is optimal for pathwidth under SETH~[Cygan et al.\\ JACM\n2022]. Van Geffen et al.~[J.\\ Graph Algorithms Appl.\\ 2020] and Bojikian et\nal.~[STACS 2023] asked which running time is optimal for this problem\nparameterized by cutwidth. We answer this question with\n$(1+\\sqrt{2})^{\\operatorname{ctw}} n^{O(1)}$ by providing matching upper and\nlower bounds. Second, as our main technical contribution, we close the gap left\nby van Heck~[2018] for Partition Into Triangles (and Triangle Packing) by\nimproving both upper and lower bound and getting a tight bound of\n$\\sqrt[3]{3}^{\\operatorname{ctw}} n^{O(1)}$, which to our knowledge exhibits\nthe only known tight non-integral basis apart from Hamiltonian Cycle. We show\nthat cuts inducing a disjoint union of paths of length three (unions of\nso-called $Z$-cuts) lie at the core of the complexity of the problem -- usually\nlower-bound constructions use simpler cuts inducing either a matching or a\ndisjoint union of bicliques. Finally, we determine the optimal running times\nfor Max Cut ($2^{\\operatorname{ctw}} n^{O(1)}$) and Induced Matching\n($3^{\\operatorname{ctw}} n^{O(1)}$) by providing matching lower bounds for the\nexisting algorithms -- the latter result also answers an open question for\ntreewidth by Chaudhary and Zehavi~[WG 2023].",
        "Unlocking the potential of Large Language Models (LLMs) in data\nclassification represents a promising frontier in natural language processing.\nIn this work, we evaluate the performance of different LLMs in comparison with\nstate-of-the-art deep-learning and machine-learning models, in two different\nclassification scenarios: i) the classification of employees' working locations\nbased on job reviews posted online (multiclass classification), and 2) the\nclassification of news articles as fake or not (binary classification). Our\nanalysis encompasses a diverse range of language models differentiating in\nsize, quantization, and architecture. We explore the impact of alternative\nprompting techniques and evaluate the models based on the weighted F1-score.\nAlso, we examine the trade-off between performance (F1-score) and time\n(inference response time) for each language model to provide a more nuanced\nunderstanding of each model's practical applicability. Our work reveals\nsignificant variations in model responses based on the prompting strategies. We\nfind that LLMs, particularly Llama3 and GPT-4, can outperform traditional\nmethods in complex classification tasks, such as multiclass classification,\nthough at the cost of longer inference times. In contrast, simpler ML models\noffer better performance-to-time trade-offs in simpler binary classification\ntasks.",
        "Performing accurate confidence quantification and assessment is important for\ndeep neural networks to predict their failures, improve their performance and\nenhance their capabilities in real-world applications, for their practical\ndeployment in real life. For pixel-wise regression tasks, confidence\nquantification and assessment has not been well addressed in the literature, in\ncontrast to classification tasks like semantic segmentation. The softmax output\nlayer is not used in deep neural networks that solve pixel-wise regression\nproblems. In this paper, to address these problems, we develop, train and\nevaluate the proposed model Confidence-Aware Regression Estimation (CARE). Our\nmodel CARE computes and assigns confidence to regression output results. We\nfocus on solving regression problems as downstream tasks of an AI Foundation\nModel for Earth Observation (EO). We evaluate the proposed model CARE and\nexperimental results on data from the Copernicus Sentinel-2 satellite\nconstellation for estimating the density of buildings show that the proposed\nmethod can be successfully applied to regression problems. We also show that\nour approach outperforms other methods.",
        "Using the algebraic approach to promise constraint satisfaction problems, we\nestablish complexity classifications of three natural variants of hypergraph\ncolourings: standard nonmonochromatic colourings, conflict-free colourings, and\nlinearly-ordered colourings.\n  Firstly, we show that finding an $\\ell$-colouring of a $k$-colourable\n$r$-uniform hypergraph is NP-hard for all constant $2\\leq k\\leq \\ell$ and\n$r\\geq 3$. This provides a shorter proof of a celebrated result by Dinur et al.\n[FOCS'02\/Combinatorica'05].\n  Secondly, we show that finding an $\\ell$-conflict-free colouring of an\n$r$-uniform hypergraph that admits a $k$-conflict-free colouring is NP-hard for\nall constant $3\\leq k\\leq\\ell$ and $r\\geq 4$, except for $r=4$ and $k=2$ (and\nany $\\ell$); this case is solvable in polynomial time. The case of $r=3$ is the\nstandard nonmonochromatic colouring, and the case of $r=2$ is the notoriously\ndifficult open problem of approximate graph colouring.\n  Thirdly, we show that finding an $\\ell$-linearly-ordered colouring of an\n$r$-uniform hypergraph that admits a $k$-linearly-ordered colouring is NP-hard\nfor all constant $3\\leq k\\leq\\ell$ and $r\\geq 4$, thus improving on the results\nof Nakajima and \\v{Z}ivn\\'y~[ICALP'22\/ACM TocT'23]."
      ]
    }
  },
  {
    "id":2411.06447,
    "research_type":"applied",
    "start_id":"b10",
    "start_title":"A deep learning approach for magnetization transfer contrast MR fingerprinting and chemical exchange saturation transfer imaging",
    "start_abstract":"Semisolid magnetization transfer contrast (MTC) and chemical exchange saturation (CEST) MRI based on MT phenomenon have shown potential to evaluate brain development, neurological, psychiatric, neurodegenerative diseases. However, a qualitative ratio (MTR) metric commonly used in conventional MTC imaging is limited the assessment of quantitative semisolid macromolecular proton rates concentrations. In addition, CEST signals measured by MTR asymmetry analysis are unavoidably contaminated upfield nuclear Overhauser enhancement (NOE) mobile macromolecules. To address these issues, we developed an MTC-MR fingerprinting (MTC-MRF) technique quantify tissue parameters, which further allows estimation accurate at certain frequency offset. A pseudorandomized RF scheme was generate unique signal evolutions for different tissues supervised deep neural network designed extract properties from MTC-MRF signals. Through detailed Bloch equation-based digital phantom vivo studies, demonstrated that can characteristics with high accuracy computational efficiency, compared equation fitting approach, provide baseline reference NOE imaging. For validation, images were synthesized using parameters estimated deep-learning method experimentally acquired as standard. The proposed framework 3D MTC, CEST, human within clinically acceptable scan time.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Magnetic resonance fingerprinting"
      ],
      "abstract":[
        "Magnetic Resonance Fingerprinting (MRF) is a new approach to quantitative magnetic resonance imaging that allows simultaneous measurement of multiple tissue properties in a single, time-efficient acquisition. The ability to reproducibly and quantitatively measure tissue properties could enable more objective tissue diagnosis, comparisons of scans acquired at different locations and time points, longitudinal follow-up of individual patients and development of imaging biomarkers. This review provides a general overview of MRF technology, current preclinical and clinical applications and potential future directions. MRF has been initially evaluated in brain, prostate, liver, cardiac, musculoskeletal imaging, and measurement of perfusion and microvascular properties through MR vascular fingerprinting."
      ],
      "categories":[
        "nucl-th"
      ]
    },
    "list":{
      "title":[
        "The 3$\\alpha$ correlations of ground and excited $0^+$ states of\n  $^{12}\\mathrm{C}$ within the microscopic cluster model",
        "Low-energy spectra of nobelium isotopes: Skyrme\n  random-phase-approximation analysis",
        "Nuclear cross sections from low-energy interactions",
        "Shear Viscosity of Collider-Produced QCD Matter II: Comparing a\n  Multi-Component Chapman-Enskog Framework with AMPT in Full Equilibrium",
        "Estimating nuclear equation of state parameters away from saturation\n  density",
        "Fourier shape parametrization in covariant density functional theory for\n  nuclear fission",
        "$\\Lambda$ and $\\Sigma$ potentials in neutron stars, hypernuclei, and\n  heavy-ion collisions",
        "Particle number projection on a spatial domain",
        "Time Evolution of Prompt Gamma-Ray Emission in $^{252}$Cf(sf) and\n  $^{233,235}$U($n$,f) Reactions",
        "Pauli Blocking effects in Nilsson states of weakly bound exotic nuclei",
        "In-medium nucleon-nucleon cross sections from relativistic ab initio\n  calculations",
        "Towards shell model interactions with credible uncertainties",
        "Emulators for scarce and noisy data II: Application to auxiliary-field\n  diffusion Monte Carlo for neutron matter",
        "Integrating UX Design in Astronomical Software Development: A Case Study",
        "Bibliometric Analysis of Scientific Production on the COVID-19 Effect in\n  Information Sciences",
        "Spatiotemporal Deep Learning Network for Photon-Level Block Compressed\n  Sensing Imaging",
        "Hybrid Channel- and Coding-Based Challenge-Response Physical-Layer\n  Authentication",
        "Thermoelectric properties of magic angle twisted bilayer\n  graphene-superconductor hetero-junction: effect of valley polarization and\n  trigonal warping",
        "Low-Eddington ratio, changing-look active galactic nuclei: the case of\n  NGC 4614",
        "Impacto del Enfoque Matematicas en Tres Actos en la Educacion Matematica",
        "Molecular Weight-Dependent Evaporation Dynamics and Morphology of PEG\n  Sessile Drops on Hydrophobic Substrates",
        "Quantized crystalline-electromagnetic responses in insulators",
        "WyckoffDiff -- A Generative Diffusion Model for Crystal Symmetry",
        "Interfacial spin-orbit coupling in superconducting hybrid systems",
        "Evolution of SMBHs in light of PTA measurements: implications for growth\n  by mergers and accretion",
        "Optimizing Portfolios with Pakistan-Exposed ETFs: Risk and Performance\n  Insight",
        "Quantum locally recoverable code with intersecting recovery sets",
        "How many unseen species are in multiple areas?"
      ],
      "abstract":[
        "The cluster structures of the $0^+$ states in $^{12}\\mathrm{C}$, including\nthe ground state, the Hoyle state, and the recently identified $0_3^+$ and\n$0_4^+$ states, are analyzed to explore the cluster configurations and\n$3\\alpha$ correlations without assuming the existence of $^8\\mathrm{Be}$. In\nparticular, the key quantity -- two-cluster overlap amplitudes -- is calculated\nfor the $3\\alpha$ clustering channels to reveal the essential features of these\n$0^+$ states. The results clearly show the distinction between the compact\nstructure of the ground state and the gas-like structures of the excited $0^+$\nstates. The Hoyle state exhibits the expected gas-like dominant ($0S$)\nconfiguration, while the $0_3^+$ state shows a more extended $3\\alpha$\nclustering structure, which can be viewed as a breathing-like excitation of the\nHoyle state, with an additional nodal structure. The $0_4^+$ state is found to\nhave a mixed configuration, featuring a bent-arm-like structure in the\n$S\\otimes S$ channel and an enhanced $2\\alpha$ correlation in the $D\\otimes D$\nchannel.",
        "Low-energy spectra in the isotopic chain $^{250-262}$No are systematically\ninvestigated within the fully self-consistent Quasiparticle\nRandom-Phase-Approximation (QRPA) using Skyrme forces SLy6, SkM* and SVbas.\nQRPA states of multipolarity $\\lambda\\mu$=20, 22, 30, 31, 32, 33, 43, 44 and 98\nare considered. The main attention is paid to isotopes $^{252}$No and\n$^{254}$No, for which the most extensive experimental spectroscopic information\nis available. In these two nuclei, a reasonable description of $K^{\\pi}=8^-,\n2^-$and $3^+$ isomers is obtained with the force SLy6. The disputed $8^-$\nisomer in $^{254}$No is assigned as neutron two-quasiparticle configuration\n$nn[734\\uparrow,613\\uparrow]$. At the energies 1.2 - 1.4 MeV, the 2qp K-isomers\n$4^-, 7^-$ in $^{252}$No and $4^-, 6^-, 7^-$ in $^{254}$No are predicted. In\n$^{254}$No, the $K^{\\pi}=3^+$ isomer should be accompanied by the nearby\n$K^{\\pi}=4^+$ counterpart. It is shown that, in the chain $^{250-262}$No, some\nfeatures of $^{252}$No and $^{254}$No exhibit essential irregularities caused\nby a shell gap in the neutron single-particle spectrum and corresponding\nbreakdown of the neutron pairing. In particular, low-energy pairing-vibrational\n$K^{\\pi}=0^+$ states in $^{252,254}$No are predicted.",
        "We present a method to calculate neutron scattering cross sections for\ndeformed nuclei using many--body wavefunctions described with multiple\nreference states. Nuclear states are calculated with the generator coordinate\nmethod using a low energy effective Hamiltonian. Using these states, a\nnon--local and energy dependent optical potential is consistently constructed,\nallowing to directly investigate the role of nuclear structure properties in\nnuclear scattering. The case of neutron scattering on $^{24}$Mg is presented.\nThe results are compared to experiment and to phenomenological optical\npotentials at energies below 13 MeV, demonstrating the importance of\nlow--energy collectivity in elastic and non--elastic scattering.",
        "Transport properties of the quark-gluon plasma are instrumental to testing\nperturbative quantum chromodynamics and understanding the extreme conditions of\nrelativistic heavy-ion collisions. This study presents an analytical\ninvestigation of the shear viscosity $\\eta$ and the shear viscosity-to-entropy\ndensity ratio $\\eta\/s$ of the QGP using a novel multi-component Chapman-Enskog\nframework assuming full thermalization. The approach incorporates\nspecies-specific contributions from gluons and (anti-)quarks into the plasma\nshear viscosity, temperature-dependent running parameters for the Debye mass\nand strong coupling, and a time-dependent cooling model. Our findings show that\nboth $\\eta$ and $\\eta\/s$ are enhanced by the inclusion of (anti-)quarks with\ngluons, and the parameters decrease over time due to the cooling and expansion\nof the QGP. These results align with perturbative QCD predictions, offering a\nmore optimistic representation of QGP transport properties under dynamic\nconditions. This multi-component framework is compared with a multi-phase\ntransport model that treats the QGP as a gluon gas with (anti-)quark\naugmentation.",
        "We explore the density variation of the correlation coefficient of the key\nparameters of the nuclear equation of state (EoS) with the bulk and crustal\nproperties of neutron stars. The analysis was performed using two diverse sets\nof nuclear effective interaction theories based on nonrelativistic\nSkyrme-Hartree Fock model and relativistic mean field model. We find that the\ncommonly studied EoS parameters, namely the isoscalar incompressibility of\nsymmetric nuclear matter $K(\\rho)$ and the isovector slope of symmetry energy\n$L(\\rho)$, reveal consistently maximum correlation with the radius, tidal\ndeformability, and moment of inertia all around twice the saturation density.\nWe find even more tighter and robust correlations beyond the saturation density\nfor constructed parameter $\\eta = [KL^2]^{1\/3}$ allowing the possibility to\nimpose stringent constraint on high-density $K(\\rho)$ and $L(\\rho)$. Extensive\ncorrelation analysis of the EoS parameters with the radius and tidal\ndeformability bounds from gravitational wave GW190814 event allows us to\nprovide reliable constraints on the central values of $K(\\rho_0) \\approx 248$\nMeV and $L(\\rho_0) \\approx 65$ MeV at the saturation density and $K(1.6\\rho_0)\n\\approx 391-517$ MeV and $L(1.6\\rho_0) \\approx 153-169$ MeV at 1.6 times the\nsaturation density. The crust-core transition density and the crustal fraction\nof moment of inertia is shown to correlate more strongly with $L(\\rho)$ and\n$\\eta(\\rho)$ near the subsaturation density.",
        "We implement the Fourier shape parametrization within the point-coupling\ncovariant density functional theory to construct the collective space,\npotential energy surface (PES), and mass tensor, which serve as inputs for the\ntime-dependent generator coordinate method to simulate the fission dynamics.\nTaking \\(^{226}\\)Th as a benchmark, we demonstrate the superiority of Fourier\nshape parametrization over conventional spherical harmonic parametrization: it\nsignificantly enhances the convergence of higher-order collective shape\nparameters by efficiently characterizing extreme nuclear deformations.\nConsequently, the new framework generates more reasonable elongated\nconfigurations, particularly for the scission configurations, and significantly\nimproves the description of charge distribution near the symmetric fission\npeak. Moreover, the Fourier shape parametrization provides a smooth and\nwell-defined three-dimensional (3D) PES with minimal correlations between\ndegrees of freedom, enabling high-precision 3D dynamical simulations of\nfission.",
        "With an appropriate $YNN$ force, the $\\Lambda$ single-particle potential\n($\\Lambda$ potential) can be made strongly repulsive at high density, and one\ncan solve the hyperon puzzle of neutron stars. We investigate the consistency\nof such a $\\Lambda$ potential, evaluated recently from $YN$ and $YNN$ forces\nbased on chiral effective field theory, with hypernuclear data and heavy-ion\ncollision data. It is found that model calculations with such a $\\Lambda$\npotential can reproduce the data of the $\\Lambda$ hypernuclear spectroscopy and\nthe $\\Lambda$ directed flow in heavy-ion collisions. Also, we evaluate the\n$\\Sigma$ potential, which can be calculated by using the same hyperon forces as\nfor the $\\Lambda$ potential. Specifically, we show that the low-energy\nconstants characterizing the strength of the $YNN$ force can be chosen to\nsuppress the appearance of the $\\Lambda$'s in neutron stars while at the same\ntime the empirical value of the $\\Sigma$ potential is reproduced.",
        "The formalism of particle number on a spatial domain for mean field wave\nfunctions with pairing is revisited to account for the case where finite\ndimensional basis are used. The formulas differ from the ones previously used\nin the literature. It is shown that the present formalism has the right limit\nin the well known case of zero pairing whereas the other formalism do not\nsatisfy this basic requirement. By using a simple one-dimensional model we\nillustrate the differences in the results for particle number distribution\nprobability obtained with the two methods.",
        "We investigate the time evolution of prompt fission $\\gamma$ emission due to\nthe presence of ns to ms isomers in the fragments produced in the\nneutron-induced fission of $^{233,235}$U and in the spontaneous fission of\n$^{252}$Cf. Calculations performed with the CGMF fission event generator are\ncompared with recent experimental data on $^{252}$Cf(sf) and $^{235}$U($n$,f)\nobtained with the DANCE+NEUANCE setup at the Los Alamos Neutron Science Center.\nOf particular interest are the average $\\gamma$-ray energy spectrum as a\nfunction of time since scission, $\\phi(\\epsilon_\\gamma,t)$, the increase of the\naverage $\\gamma$-ray multiplicity over time, $N_\\gamma(t)$, and its evolution\nin time as a function of the fission fragment mass, $N_\\gamma(A,t)$. The time\nevolution of isomeric ratios in post-neutron emission fission fragments can be\ndefined and used to test and reveal some deficiencies in our knowledge of the\nlow-lying levels of neutron-rich nuclei produced in fission reactions.",
        "The description of weakly bound nuclei using deformed few-body models has\nproven to be crucial in the study of reactions involving certain exotic nuclei.\nHowever, these core+valence models face the challenge of applying the Pauli\nexclusion principle, since the factorisation of the system does not allow\ncomplete antisymmetrization. Therefore, states occupied by core nucleons should\nbe blocked for the valence nucleons. We aim to study $^{17}$C and $^{19}$C,\nwhich are good examples of weakly bound exotic nuclei with significant\ndeformation where the valence shell is partially filled. The structure of\n$^{17}$C and $^{19}$C is described with deformed two-body models where a\nNilsson Hamiltonian is constructed using Antisymmetrized Molecular Dynamic\ncalculations of the cores. Different methods of blocking occupied Nilsson\nstates are considered using the Bardeen$-$Cooper$-$Schrieffer formalism:\nwithout blocking, total blocking and partial blocking. The latter also takes\ninto account pair correlations to some extent. These models are later used to\nstudy $^{16}$C$(d,p)^{17}$C, $^{17}$C$(p,d)^{16}$C and $^{18}$C$(d,p)^{19}$C\ntransfer reactions within the Adiabatic Distorted Wave Approximation. In the\nfirst case, the results are compared with experimental data. A good\nreproduction of the structure of $^{17}$C is found, significantly improving the\nagreement in the $^{16}$C$(d,p)^{17}$C reaction including blocking effects. The\n$^{19}$C spectrum is better reproduced considering blocking, in particular, the\npartial blocking method that considers the pairing interaction provides the\nbest description. Promising results are shown for the study of transfer\nreactions involving weakly bound exotic nuclei, by highlighting the effect of\nblocking occupied Nilsson states. We envision to extend the models to the study\nof breakup reactions and to newly discovered halo nuclei.",
        "The in-medium nucleon-nucleon scattering cross section is a pivotal quantity\nfor studying the medium effects of strong interaction, and its precise\nknowledge is critical for understanding the equation of state for dense matter,\nintermediate-energy heavy-ion collision dynamics, and related phenomena. In\nthis work, we perform a microscopic investigation of in-medium nucleon-nucleon\nscattering cross sections, by utilizing the relativistic Brueckner-Hartree-Fock\n(RBHF) theory with the Bonn potential. The fully incorporation of both\npositive- and negative-energy states in the RBHF solutions allows us to\ndetermine the single-particle potentials, the effective G matrix, and the\nscattering cross section uniquely. The momentum, density, and isospin\ndependence of the cross section for pp, nn, and np scattering are studied in\ndetail. Our results provide a solid foundation for future parameterization\nstudies of multi-parameter dependency of total scattering cross sections.",
        "Background: The nuclear shell model offers realistic predictions of nuclear\nstructure starting from (quasi-) proton and neutron degrees of freedom, but\nrelies on coupling constants (interaction matrix elements) that must be fit to\nexperiment. To extend the shell model's applicability across the nuclear chart,\nand specifically toward the driplines, we must first be able to efficiently\ntest new interaction matrix elements and assign credible uncertainties.\n  Purpose: We develop and test a framework to efficiently fit new shell model\ninteractions and obtain credible uncertainties. We further demonstrate its use\nby validating the uncertainty estimates of the known \\textit{sd}-shell\neffective interactions.\n  Methods: We use eigenvector continuation to emulate solutions to the exact\nshell model. First, we use the emulator to replicate earlier results using a\nwell-known linear-combination chi-squared minimization algorithm. Then, we\nemploy a modern Markov Chain Monte Carlo method to test for nonlinearities in\nthe observable posterior distributions, which previous sensitivity analyses\nprecluded.\n  Results: The emulator reproduces the USDB interaction within a small margin\nof error, allowing for the quantification of the matrix element uncertainty.\nHowever, we find that to obtain credible predictive intervals the model defect\nof the shell model itself, rather than experimental or emulator\nuncertainty\/error, must be taken into account.\n  Conclusions: Eigenvector continuation can be used to accelerate fitting shell\nmodel interactions. We confirm that the linear approximation used to develop\ninteractions in the past is indeed sufficient. However, we find that typical\nassumptions about the likelihood function must be modified in order to obtain a\ncredible uncertainty-quantified interaction.",
        "Understanding the equation of state (EOS) of pure neutron matter is necessary\nfor interpreting multi-messenger observations of neutron stars. Reliable data\nanalyses of these observations require well-quantified uncertainties for the\nEOS input, ideally propagating uncertainties from nuclear interactions directly\nto the EOS. This, however, requires calculations of the EOS for a prohibitively\nlarger number of nuclear Hamiltonians, solving the nuclear many-body problem\nfor each one. Quantum Monte Carlo methods, such as Auxiliary field diffusion\nMonte Carlo (AFDMC), provide precise and accurate results for the neutron\nmatter EOS but they are very computationally expensive, making them unsuitable\nfor the fast evaluations necessary for uncertainty propagation. Here, we employ\nparametric matrix models to develop fast emulators for AFDMC calculations of\nneutron matter, and use them to directly propagate uncertainties of coupling\nconstants in the Hamiltonian to the EOS. As these uncertainties include\nestimates of the EFT truncation uncertainty, this approach provides robust\nuncertainty estimates for use in astrophysical data analyses. This work will\nenable novel applications such as using astrophysical observations to put\nconstraints on coupling constants for nuclear interactions.",
        "In 2023, ASTRON took the step of incorporating a dedicated User Experience\n(UX) designer into its software development process. This decision aimed to\nenhance the accessibility and usability of services providing access to the\ndata holdings from the telescopes we are developing.\n  The field of astronomical software development has historically under\nemphasized UX design. ASTRON's initiative not only improves our own tools, but\ncan also be used to demonstrate to the broader community the value of\nintegrating UX expertise into development teams.\n  We discuss how we integrate the UX designer at the start of our software\ndevelopment lifecycle. We end with providing some considerations on how other\nprojects could make use of UX knowledge in their development process.",
        "This paper analyzes the scientific production on the COVID-19 effect in the\narea of Information Sciences from a bibliometric perspective. The objectives\nfocused on: 1) determining the most productive authors, countries, institutions\nand journals; 2) identifying the sources that constitute the core of scientific\nproduction; 3) examining the manuscripts with the greatest impact; and 4)\nvisualizing the thematic and conceptual structure of the scientific domain\nanalyzed. Bibliometric indicators and factor analysis techniques were used for\ndata analysis. A total of 1,175 publications indexed in the Web of Science\n(WoS) core collection from 2020 to 2022 were retrieved. The results showed that\nthe most relevant countries were the United States, United Kingdom, China and\nSpain. The core of the scientific production was formed by the publications:\nJournal of the American Medical Informatics Association, Information\nProfessional, Scientometrics and Journal of Health Communication. The papers\nwith the greatest impact were concentrated in those dedicated to the analysis\nof the role of telemedicine in medical care. The conceptual structure showed\nthe main research fronts, such as the role of telehealth, academic libraries\nand digital literacy in the fight against the pandemic, the role of social\nnetworks in the health crisis, as well as the problem of misinformation and\nfake news",
        "In this paper, we propose a spatiotemporal deep learning network for\nphoton-level Block Compressed Sensing Imaging, aimed to address challenges such\nas signal loss, artifacts, and noise interference in large-pixel dynamic\nimaging and tracking at the photon level. This approach combines information in\nthe time and frequency domains with a U-Net-LSTM deep learning model,\nsignificantly improving the restoration quality of dynamic target images at\nhigh frame rates. The experimental results demonstrate that dynamic target\nimaging with an average photon number of less than 10 per pixel can be achieved\nusing 16-channel parallel detection, where the pixel size is 256*256 and the\nframe rate is 200 fps.. Compared to conventional Block Compressed Sensing\nImaging, this method increases the peak signal-to-noise ratio to 38.66 dB and\nimproves the structural similarity index to 0.96. In the presence of a dynamic\nscattering medium and a static complex background, we successfully achieved\nimaging and tracking of two targets undergoing complex motion. Even in\nscenarios where the targets overlap or obstruct each other, we can still\nreconstruct clear images of each individual target separately.. This method\nprovides an effective solution for large-pixel dynamic target recognition,\ntracking, and real-time imaging in complex environments, offering promising\napplications in remote sensing, military reconnaissance, and beyond.",
        "This letter proposes a new physical layer authentication mechanism operating\nat the physical layer of a communication system where the receiver has partial\ncontrol of the channel conditions (e.g., using an intelligent reflecting\nsurface). We aim to exploit both instantaneous channel state information (CSI)\nand a secret shared key for authentication. This is achieved by both\ntransmitting an identifying key by wiretap coding (to conceal the key from the\nattacker) and checking that the instantaneous CSI corresponds to the channel\nconfiguration randomly selected by the receiver. We investigate the trade-off\nbetween the pilot signals used for CSI estimation and the coding rate (or key\nlength) to improve the overall security of the authentication procedure.",
        "We theoretically investigate the thermoelectric properties (electronic\ncontribution) of a normal-superconductor (NS) hybrid junction, where the normal\nregion consists of magic-angle twisted bilayer graphene (MATBG). The\nsuperconducting region is characterized by a common $s$-wave superconductor\nclosely proximitized to the MATBG. We compute various thermoelectric\ncoefficients, including thermal conductance, thermopower, and the figure of\nmerit ($zT$), using the scattering matrix formalism. These results are further\nsupported by calculations based on a lattice-regularized version of the\neffective Hamiltonian. Additionally, we explore the impact of trigonal warping\nand valley polarization on the thermoelectric coefficients. Notably, we find a\nsignificant variation in $zT$ as a function of these parameters, reaching\nvalues as high as 2.5. Interestingly, we observe a violation of the\nWiedemann-Franz law near the charge neutrality point with the superconducting\ncorrelation, indicating that MATBG electrons behave as slow Dirac fermions in\nthis regime. This observation is further confirmed by the damped oscillatory\nbehavior of the thermal conductance as a function of the barrier strength when\nan insulating barrier is modelled at the interface of the NS junction. Beyond\ntheoretical insights, our findings suggest new possibilities for thermoelectric\napplications using MATBG based NS junctions.",
        "Active galactic nuclei (AGN) are known to be variable sources across the\nentire electromagnetic spectrum, in particular at optical\/ultraviolet and X-ray\nenergies. Over the past decades, a growing number of AGN have displayed type\ntransitions: from type 1 to type 2 or viceversa within a few years or even\nseveral months. These galaxies have been commonly referred to as changing-look\nAGN (CLAGN). Here we report on a new CLAGN, NGC 4614, which transitioned from a\ntype 1.9 to a type 2 state. NGC 4614 is a nearly face-on barred galaxy at\nredshift $z = 0.016$, classified as a low-luminosity AGN. Its central black\nhole has a mass of about $1.6\\times 10^7 M_\\odot$ and an Eddington ratio around\n1 percent. We recently acquired optical spectra of NGC 4614 at the Telescopio\nNazionale Galileo and the data clearly suggest that the broad H$\\alpha$\ncomponent has strongly dimmed, if not disappeared. A very recent Swift\nobservation confirmed our current optical data, with the AGN weakened by almost\na factor of 10 with respect to previous X-ray observations. Indeed, NGC 4614\nhad been also observed by Swift\/XRT 6 times in 2011, when the source was\nclearly detected in all observations. By fitting the stack of the 2011 Swift\nobservations we obtain a photon index of $\\Gamma=1.3\\pm0.3$ and an equivalent\nhydrogen column density of $N_{\\rm H}$=$1.2\\pm0.3$ $\\times$10$^{22}$ cm$^{-2}$,\nindicating that NGC 4614 can be moderately absorbed in the X-rays. Although a\nsignificant change in the foreground gas absorption that may have obscured the\nbroad line region cannot be entirely ruled out, the most likely explanation for\nour optical and X-ray data is that NGC 4614 is experiencing a change in the\naccretion state that reduces the radiative efficiency of the X-ray corona.",
        "The \"Mathematics in Three Acts\" approach, proposed by Dan Meyer, aims to\ntransform the teaching of mathematics through a model that encourages active\nstudent participation, fostering creativity, problem-solving, and\nmetacognition. This study explores the implementation of this approach in a\nmathematics contest for secondary school students, evaluating its impact on\nvarious key competencies. Aspects such as mathematical creativity,\nproblem-solving skills, metacognitive abilities, and students' perceptions of\nmathematics are examined. The results show that the approach contributes to the\ndevelopment of creative skills, improves understanding and problem-solving\nabilities, and increases student motivation and confidence. However, areas for\nimprovement are also identified, particularly in the justification of\nprocedures and cognitive flexibility. This study highlights the effectiveness\nof the \"Mathematics in Three Acts\" approach as an innovative methodology that\nfosters more meaningful, reflective, and autonomous learning, suggesting its\npotential to transform mathematics teaching in diverse educational contexts.",
        "The evaporation dynamics of sessile drops are crucial for material deposition\nin applications like inkjet printing and pharmaceutical development. However,\nthe evaporation behavior of high molecular weight polymer solutions and their\nimpact on deposit morphology and flow fields are not well understood. This\nstudy investigates the evaporation dynamics and deposit morphology of\npolyethylene glycol (PEG) solution drops on hydrophobic substrates, with\nmolecular weights ranging from 200 to 1000k g\/mol, covering five orders of\nmagnitude. The results show that vapor diffusion dominates the evaporation\nprocess across all PEG molecular weights. Using image analysis and\nmicro-particle image velocimetry ($\\mu$-PIV), we reveal that molecular weight\naffects contact line dynamics and internal flow, leading to diverse deposit\nmorphologies, including spherical caps, pillars, pool-shaped disks, and flat\ndisks. Transient divergence and P\\'eclet number calculations further confirm\nthe role of hydrodynamics in deposit formation. These findings provide insights\ninto the hydrodynamic and thermodynamic factors governing evaporation in\npolymeric sessile drops, with implications for material fabrication and the\ndevelopment of inkjet printing and coating techniques.",
        "We introduce new classes of gapped topological phases characterized by\nquantized crystalline-electromagnetic responses, termed \"multipolar Chern\ninsulators\". These systems are characterized by nonsymmorphic momentum-space\nsymmetries and mirror symmetries, leading to quantization of momentum-weighted\nBerry curvature multipole moments. We construct lattice models for such phases\nand confirm their quantized responses through numerical calculations. These\nsystems exhibit bound charge and momentum densities at lattice and magnetic\ndefects, and currents induced by electric or time-varying strain fields. Our\nwork extends the classification of topological matter by uncovering novel\nsymmetry-protected topological phases with quantized responses.",
        "Crystalline materials often exhibit a high level of symmetry. However, most\ngenerative models do not account for symmetry, but rather model each atom\nwithout any constraints on its position or element. We propose a generative\nmodel, Wyckoff Diffusion (WyckoffDiff), which generates symmetry-based\ndescriptions of crystals. This is enabled by considering a crystal structure\nrepresentation that encodes all symmetry, and we design a novel neural network\narchitecture which enables using this representation inside a discrete\ngenerative model framework. In addition to respecting symmetry by construction,\nthe discrete nature of our model enables fast generation. We additionally\npresent a new metric, Fr\\'echet Wrenformer Distance, which captures the\nsymmetry aspects of the materials generated, and we benchmark WyckoffDiff\nagainst recently proposed generative models for crystal generation.",
        "We investigate the effects of interfacial spin-orbit coupling (ISOC) on\nsuperconductors, focusing on its impact on electronic transport and spin-charge\nconversion. Using a symmetry-based nonlinear sigma model, we derive effective\nboundary conditions for the Usadel and Maxwell equations that account for\nspin-galvanic effect, spin relaxation, and spin precession. This approach\nallows for the analysis of various interfaces without relying on specific\nmicroscopic models. We apply these boundary conditions to derive ISOC-induced\nterms in the Ginzburg-Landau functional, which is then used to compute the\ncritical temperature of superconducting films with ISOC subjected to an\nexternal magnetic field. Our findings show that, contrary to a recent\nprediction, the critical temperature of a film cannot be enhanced by an\nexternal magnetic field. Additionally, we demonstrate that the combination of\nISOC and an external magnetic field leads to a superconducting diode effect.\nIts efficiency strongly depends on the interplay between the spin-galvanic and\nthe spin relaxation terms. Our results provide a framework for understanding\nISOC in superconducting systems and highlight the potential for optimizing\ndiode efficiency through careful interface engineering",
        "We study the growth of supermassive black holes accounting for both accretion\nand mergers. The former is informed by observations of the quasar luminosity\nfunction (QLF) and the latter by the gravitational wave-background (GWB)\nrecently detected by PTAs, while estimates of the present-day black hole mass\nfunction provide a boundary condition. The GWB is dominated by the most massive\nblack holes ($\\gtrsim10^{9}M_{\\odot}$). We show that their evolution can be\nsimplified into a two-step process: mergers dominate at $z\\leq1$, while\naccretion peaks at $1.4\\leq z\\leq2$. The large amplitude of the observed GWB\nsuggests a significant number of mergers. We show that this generically implies\na higher average Eddington ratio for quasars relative to a scenario in which\nmergers are negligible. In the absence of mergers, matching local estimates of\nBH abundance to the QLF implies a radiative efficiency $\\epsilon_r=0.12$ and\nEddington ratio $\\lambda=0.2$. With mergers, a progenitor of mass $M_i$ is\nboosted to a final total mass $M_f$ and there is a direct relation between the\nmass gained in mergers and the average Eddington ratio of the quasar\npopulation, given by $M_f\/M_i\\sim\\lambda\/0.2$. There is thus a tension between\nthe observed GWB, quasar properties, and the BH mass function: estimates of the\nmass function consistent with Eddington ratios inferred in quasars and\n$\\epsilon_r\\sim0.1$ underpredict the GWB; multiple\/equal mass mergers can boost\nthe GWB, but lead to a high Eddington ratio. If the local mass function is on\nthe high end of current estimates, the GWB is more readily explained, but\nrequires low efficiencies $\\epsilon_r\\sim10^{-2}$ not expected in standard\nluminous accretion models. The significant merger rate implied by the GWB also\nstrongly suggests that the most massive BHs in the local universe have\nsignificant spin due to the orbital angular momentum from mergers, perhaps\n$a\\sim0.5$.",
        "This study examines the investment landscape of Pakistan as an emerging and\nfrontier market, focusing on implications for international investors,\nparticularly those in the United States, through exchange-traded funds (ETFs)\nwith exposure to Pakistan. The analysis encompasses 30 ETFs with varying\ndegrees of exposure to Pakistan, covering the period from January 1, 2016, to\nFebruary 2024. This research highlights the potential benefits and risks\nassociated with investing in these ETFs, emphasizing the importance of thorough\nrisk assessments and portfolio performance comparisons. By providing\ndescriptive statistics and performance metrics based on historical\noptimization, this paper aims to equip investors with the necessary insights to\nmake informed decisions when optimizing their portfolios with Pakistan-exposed\nETFs. The second part of the paper introduces and assesses dynamic optimization\nmethodologies. This section is designed to explore the adaptability and\nperformance metrics of dynamic optimization techniques in comparison with\nconventional historical optimization methods. By integrating dynamic\noptimization into the investigation, this research aims to offer insights into\nthe efficacy of these contrasting methodologies in the context of\nPakistan-exposed ETFs. The findings underscore the significance of Pakistan's\nmarket dynamics within the broader context of emerging markets, offering a\npathway for diversification and potential growth in investment strategies.",
        "We introduce the concept of quantum locally recoverable codes (qLRCs) with\nintersecting recovery sets. We derive a singleton-like bound for these codes by\nleveraging the additional information provided by the intersecting recovery\nsets. Furthermore, we provide a construction for qLRCs with intersecting\nrecovery sets by introducing a variation of the hypergraph product. Finally, we\napply our qLRC methods to obtain improved results for classical LRCs. These\nresults may provide new insights into the locality of quantum error correction\ncode.",
        "In ecology, the description of species composition and biodiversity calls for\nstatistical methods that involve estimating features of interest in unobserved\nsamples based on an observed one. In the last decade, the Bayesian\nnonparametrics literature has thoroughly investigated the case where data arise\nfrom a homogeneous population. In this work, we propose a novel framework to\naddress heterogeneous populations, specifically dealing with scenarios where\ndata arise from two areas. This setting significantly increases the\nmathematical complexity of the problem and, as a consequence, it received\nlimited attention in the literature. While early approaches leverage on\ncomputational methods, we provide a distributional theory for the in-sample\nanalysis of any observed sample and we enable out-of-sample prediction for the\nnumber of unseen distinct and shared species in additional samples of arbitrary\nsizes. The latter also extends the frequentist estimators which solely deal\nwith the one-step ahead prediction. Furthermore, our results can be applied to\naddress the sample size determination in sampling problems aimed at detecting\nshared species. Our results are illustrated in a real-world dataset concerning\na population of ants in the city of Trieste."
      ]
    }
  },
  {
    "id":2411.06447,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"Magnetic resonance fingerprinting",
    "start_abstract":"Magnetic Resonance Fingerprinting (MRF) is a new approach to quantitative magnetic resonance imaging that allows simultaneous measurement of multiple tissue properties in a single, time-efficient acquisition. The ability to reproducibly and quantitatively measure tissue properties could enable more objective tissue diagnosis, comparisons of scans acquired at different locations and time points, longitudinal follow-up of individual patients and development of imaging biomarkers. This review provides a general overview of MRF technology, current preclinical and clinical applications and potential future directions. MRF has been initially evaluated in brain, prostate, liver, cardiac, musculoskeletal imaging, and measurement of perfusion and microvascular properties through MR vascular fingerprinting.",
    "start_categories":[
      "nucl-th"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b10"
      ],
      "title":[
        "A deep learning approach for magnetization transfer contrast MR fingerprinting and chemical exchange saturation transfer imaging"
      ],
      "abstract":[
        "Semisolid magnetization transfer contrast (MTC) and chemical exchange saturation (CEST) MRI based on MT phenomenon have shown potential to evaluate brain development, neurological, psychiatric, neurodegenerative diseases. However, a qualitative ratio (MTR) metric commonly used in conventional MTC imaging is limited the assessment of quantitative semisolid macromolecular proton rates concentrations. In addition, CEST signals measured by MTR asymmetry analysis are unavoidably contaminated upfield nuclear Overhauser enhancement (NOE) mobile macromolecules. To address these issues, we developed an MTC-MR fingerprinting (MTC-MRF) technique quantify tissue parameters, which further allows estimation accurate at certain frequency offset. A pseudorandomized RF scheme was generate unique signal evolutions for different tissues supervised deep neural network designed extract properties from MTC-MRF signals. Through detailed Bloch equation-based digital phantom vivo studies, demonstrated that can characteristics with high accuracy computational efficiency, compared equation fitting approach, provide baseline reference NOE imaging. For validation, images were synthesized using parameters estimated deep-learning method experimentally acquired as standard. The proposed framework 3D MTC, CEST, human within clinically acceptable scan time."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "Imitation Learning from a Single Temporally Misaligned Video",
        "HyperArm Bandit Optimization: A Novel approach to Hyperparameter\n  Optimization and an Analysis of Bandit Algorithms in Stochastic and\n  Adversarial Settings",
        "A method for classification of data with uncertainty using hypothesis\n  testing",
        "A2Perf: Real-World Autonomous Agents Benchmark",
        "Model Monitoring in the Absence of Labeled Data via Feature Attributions\n  Distributions",
        "Covering Multiple Objectives with a Small Set of Solutions Using\n  Bayesian Optimization",
        "A Partial Initialization Strategy to Mitigate the Overfitting Problem in\n  CATE Estimation with Hidden Confounding",
        "Evaluating Time Series Foundation Models on Noisy Periodic Time Series",
        "Online-BLS: An Accurate and Efficient Online Broad Learning System for\n  Data Stream Classification",
        "Sepsyn-OLCP: An Online Learning-based Framework for Early Sepsis\n  Prediction with Uncertainty Quantification using Conformal Prediction",
        "KEA: Keeping Exploration Alive by Proactively Coordinating Exploration\n  Strategies",
        "How Your Location Relates to Health: Variable Importance and\n  Interpretable Machine Learning for Environmental and Sociodemographic Data",
        "Deep Learning and Foundation Models for Weather Prediction: A Survey",
        "Eigenvalue conditions implying edge-disjoint spanning trees and a forest\n  with constraints",
        "The Journey Matters: Average Parameter Count over Pre-training Unifies\n  Sparse and Dense Scaling Laws",
        "BaxBench: Can LLMs Generate Correct and Secure Backends?",
        "Personalized Interpolation: An Efficient Method to Tame Flexible\n  Optimization Window Estimation",
        "Logarithmic Width Suffices for Robust Memorization",
        "On the stress transit function",
        "Time Series Language Model for Descriptive Caption Generation",
        "Learning Closed-Loop Parametric Nash Equilibria of Multi-Agent\n  Collaborative Field Coverage",
        "TULIP: Towards Unified Language-Image Pretraining",
        "Towards Generalizable Trajectory Prediction Using Dual-Level\n  Representation Learning And Adaptive Prompting",
        "A note on improved bounds for hypergraph rainbow matching problems",
        "The Ball-Proximal (=\"Broximal\") Point Method: a New Algorithm,\n  Convergence Theory, and Applications",
        "Further Results for the Capacity Statistic Distribution on Compositions\n  of 1's and 2's",
        "3D Point Cloud Generation via Autoregressive Up-sampling",
        "Rerailing Automata"
      ],
      "abstract":[
        "We examine the problem of learning sequential tasks from a single visual\ndemonstration. A key challenge arises when demonstrations are temporally\nmisaligned due to variations in timing, differences in embodiment, or\ninconsistencies in execution. Existing approaches treat imitation as a\ndistribution-matching problem, aligning individual frames between the agent and\nthe demonstration. However, we show that such frame-level matching fails to\nenforce temporal ordering or ensure consistent progress. Our key insight is\nthat matching should instead be defined at the level of sequences. We propose\nthat perfect matching occurs when one sequence successfully covers all the\nsubgoals in the same order as the other sequence. We present ORCA (ORdered\nCoverage Alignment), a dense per-timestep reward function that measures the\nprobability of the agent covering demonstration frames in the correct order. On\ntemporally misaligned demonstrations, we show that agents trained with the ORCA\nreward achieve $4.5$x improvement ($0.11 \\rightarrow 0.50$ average normalized\nreturns) for Meta-world tasks and $6.6$x improvement ($6.55 \\rightarrow 43.3$\naverage returns) for Humanoid-v4 tasks compared to the best frame-level\nmatching algorithms. We also provide empirical analysis showing that ORCA is\nrobust to varying levels of temporal misalignment. Our code is available at\nhttps:\/\/github.com\/portal-cornell\/orca\/",
        "This paper explores the application of bandit algorithms in both stochastic\nand adversarial settings, with a focus on theoretical analysis and practical\napplications. The study begins by introducing bandit problems, distinguishing\nbetween stochastic and adversarial variants, and examining key algorithms such\nas Explore-Then-Commit (ETC), Upper Confidence Bound (UCB), and\nExponential-Weight Algorithm for Exploration and Exploitation (EXP3).\nTheoretical regret bounds are analyzed to compare the performance of these\nalgorithms. The paper then introduces a novel framework, HyperArm Bandit\nOptimization (HABO), which applies EXP3 to hyperparameter tuning in machine\nlearning models. Unlike traditional methods that treat entire configurations as\narms, HABO treats individual hyperparameters as super-arms, and its potential\nconfigurations as sub-arms, enabling dynamic resource allocation and efficient\nexploration. Experimental results demonstrate HABO's effectiveness in\nclassification and regression tasks, outperforming Bayesian Optimization in\nterms of computational efficiency and accuracy. The paper concludes with\ninsights into the convergence guarantees of HABO and its potential for scalable\nand robust hyperparameter optimization.",
        "Binary classification is a task that involves the classification of data into\none of two distinct classes. It is widely utilized in various fields. However,\nconventional classifiers tend to make overconfident predictions for data that\nbelong to overlapping regions of the two class distributions or for data\noutside the distributions (out-of-distribution data). Therefore, conventional\nclassifiers should not be applied in high-risk fields where classification\nresults can have significant consequences. In order to address this issue, it\nis necessary to quantify uncertainty and adopt decision-making approaches that\ntake it into account. Many methods have been proposed for this purpose;\nhowever, implementing these methods often requires performing resampling,\nimproving the structure or performance of models, and optimizing the thresholds\nof classifiers. We propose a new decision-making approach using two types of\nhypothesis testing. This method is capable of detecting ambiguous data that\nbelong to the overlapping regions of two class distributions, as well as\nout-of-distribution data that are not included in the training data\ndistribution. In addition, we quantify uncertainty using the empirical\ndistribution of feature values derived from the training data obtained through\nthe trained model. The classification threshold is determined by the\n$\\alpha$-quantile and ($1-\\alpha$)-quantile, where the significance level\n$\\alpha$ is set according to each specific situation.",
        "Autonomous agents and systems cover a number of application areas, from\nrobotics and digital assistants to combinatorial optimization, all sharing\ncommon, unresolved research challenges. It is not sufficient for agents to\nmerely solve a given task; they must generalize to out-of-distribution tasks,\nperform reliably, and use hardware resources efficiently during training and\ninference, among other requirements. Several methods, such as reinforcement\nlearning and imitation learning, are commonly used to tackle these problems,\neach with different trade-offs. However, there is a lack of benchmarking suites\nthat define the environments, datasets, and metrics which can be used to\nprovide a meaningful way for the community to compare progress on applying\nthese methods to real-world problems. We introduce A2Perf--a benchmark with\nthree environments that closely resemble real-world domains: computer chip\nfloorplanning, web navigation, and quadruped locomotion. A2Perf provides\nmetrics that track task performance, generalization, system resource\nefficiency, and reliability, which are all critical to real-world applications.\nUsing A2Perf, we demonstrate that web navigation agents can achieve latencies\ncomparable to human reaction times on consumer hardware, reveal reliability\ntrade-offs between algorithms for quadruped locomotion, and quantify the energy\ncosts of different learning approaches for computer chip-design. In addition,\nwe propose a data cost metric to account for the cost incurred acquiring\noffline data for imitation learning and hybrid algorithms, which allows us to\nbetter compare these approaches. A2Perf also contains several standard\nbaselines, enabling apples-to-apples comparisons across methods and\nfacilitating progress in real-world autonomy. As an open-source benchmark,\nA2Perf is designed to remain accessible, up-to-date, and useful to the research\ncommunity over the long term.",
        "Model monitoring involves analyzing AI algorithms once they have been\ndeployed and detecting changes in their behaviour. This thesis explores machine\nlearning model monitoring ML before the predictions impact real-world decisions\nor users. This step is characterized by one particular condition: the absence\nof labelled data at test time, which makes it challenging, even often\nimpossible, to calculate performance metrics.\n  The thesis is structured around two main themes: (i) AI alignment, measuring\nif AI models behave in a manner consistent with human values and (ii)\nperformance monitoring, measuring if the models achieve specific accuracy goals\nor desires.\n  The thesis uses a common methodology that unifies all its sections. It\nexplores feature attribution distributions for both monitoring dimensions.\nUsing these feature attribution explanations, we can exploit their theoretical\nproperties to derive and establish certain guarantees and insights into model\nmonitoring.",
        "In multi-objective black-box optimization, the goal is typically to find\nsolutions that optimize a set of T black-box objective functions, $f_1$, ...,\n$f_T$, simultaneously. Traditional approaches often seek a single\nPareto-optimal set that balances trade-offs among all objectives. In this work,\nwe introduce a novel problem setting that departs from this paradigm: finding a\nsmaller set of K solutions, where K < T, that collectively \"covers\" the T\nobjectives. A set of solutions is defined as \"covering\" if, for each objective\n$f_1$, ..., $f_T$, there is at least one good solution. A motivating example\nfor this problem setting occurs in drug design. For example, we may have T\npathogens and aim to identify a set of K < T antibiotics such that at least one\nantibiotic can be used to treat each pathogen. To address this problem, we\npropose Multi-Objective Coverage Bayesian Optimization (MOCOBO), a principled\nalgorithm designed to efficiently find a covering set. We validate our approach\nthrough extensive experiments on challenging high-dimensional tasks, including\napplications in peptide and molecular design. Experiments demonstrate MOCOBO's\nability to find high-performing covering sets of solutions. Additionally, we\nshow that the small sets of K < T solutions found by MOCOBO can match or nearly\nmatch the performance of T individually optimized solutions for the same\nobjectives. Our results highlight MOCOBO's potential to tackle complex\nmulti-objective problems in domains where finding at least one high-performing\nsolution for each objective is critical.",
        "Estimating the conditional average treatment effect (CATE) from observational\ndata plays a crucial role in areas such as e-commerce, healthcare, and\neconomics. Existing studies mainly rely on the strong ignorability assumption\nthat there are no hidden confounders, whose existence cannot be tested from\nobservational data and can invalidate any causal conclusion. In contrast, data\ncollected from randomized controlled trials (RCT) do not suffer from\nconfounding but are usually limited by a small sample size. To avoid\noverfitting caused by the small-scale RCT data, we propose a novel two-stage\npretraining-finetuning (TSPF) framework with a partial parameter initialization\nstrategy to estimate the CATE in the presence of hidden confounding. In the\nfirst stage, a foundational representation of covariates is trained to estimate\ncounterfactual outcomes through large-scale observational data. In the second\nstage, we propose to train an augmented representation of the covariates, which\nis concatenated with the foundational representation obtained in the first\nstage to adjust for the hidden confounding. Rather than training a separate\nnetwork from scratch, part of the prediction heads are initialized from the\nfirst stage. The superiority of our approach is validated on two datasets with\nextensive experiments.",
        "While recent advancements in foundation models have significantly impacted\nmachine learning, rigorous tests on the performance of time series foundation\nmodels (TSFMs) remain largely underexplored. This paper presents an empirical\nstudy evaluating the zero-shot, long-horizon forecasting abilities of several\nleading TSFMs over two synthetic datasets constituting noisy periodic time\nseries. We assess model efficacy across different noise levels, underlying\nfrequencies, and sampling rates. As benchmarks for comparison, we choose two\nstatistical techniques: a Fourier transform (FFT)-based approach and a linear\nautoregressive (AR) model. Our findings demonstrate that while for time series\nwith bounded periods and higher sampling rates, TSFMs can match or outperform\nthe statistical approaches, their forecasting abilities deteriorate with longer\nperiods, higher noise levels, lower sampling rates and more complex shapes of\nthe time series.",
        "The state-of-the-art online learning models generally conduct a single online\ngradient descent when a new sample arrives and thus suffer from suboptimal\nmodel weights. To this end, we introduce an online broad learning system\nframework with closed-form solutions for each online update. Different from\nemploying existing incremental broad learning algorithms for online learning\ntasks, which tend to incur degraded accuracy and expensive online update\noverhead, we design an effective weight estimation algorithm and an efficient\nonline updating strategy to remedy the above two deficiencies, respectively.\nSpecifically, an effective weight estimation algorithm is first developed by\nreplacing notorious matrix inverse operations with Cholesky decomposition and\nforward-backward substitution to improve model accuracy. Second, we devise an\nefficient online updating strategy that dramatically reduces online update\ntime. Theoretical analysis exhibits the splendid error bound and low time\ncomplexity of our model. The most popular test-then-training evaluation\nexperiments on various real-world datasets prove its superiority and\nefficiency. Furthermore, our framework is naturally extended to data stream\nscenarios with concept drift and exceeds state-of-the-art baselines.",
        "Sepsis is a life-threatening syndrome with high morbidity and mortality in\nhospitals. Early prediction of sepsis plays a crucial role in facilitating\nearly interventions for septic patients. However, early sepsis prediction\nsystems with uncertainty quantification and adaptive learning are scarce. This\npaper proposes Sepsyn-OLCP, a novel online learning algorithm for early sepsis\nprediction by integrating conformal prediction for uncertainty quantification\nand Bayesian bandits for adaptive decision-making. By combining the robustness\nof Bayesian models with the statistical uncertainty guarantees of conformal\nprediction methodologies, this algorithm delivers accurate and trustworthy\npredictions, addressing the critical need for reliable and adaptive systems in\nhigh-stakes healthcare applications such as early sepsis prediction. We\nevaluate the performance of Sepsyn-OLCP in terms of regret in stochastic bandit\nsetting, the area under the receiver operating characteristic curve (AUROC),\nand F-measure. Our results show that Sepsyn-OLCP outperforms existing\nindividual models, increasing AUROC of a neural network from 0.64 to 0.73\nwithout retraining and high computational costs. And the model selection policy\nconverges to the optimal strategy in the long run. We propose a novel\nreinforcement learning-based framework integrated with conformal prediction\ntechniques to provide uncertainty quantification for early sepsis prediction.\nThe proposed methodology delivers accurate and trustworthy predictions,\naddressing a critical need in high-stakes healthcare applications like early\nsepsis prediction.",
        "Soft Actor-Critic (SAC) has achieved notable success in continuous control\ntasks but struggles in sparse reward settings, where infrequent rewards make\nefficient exploration challenging. While novelty-based exploration methods\naddress this issue by encouraging the agent to explore novel states, they are\nnot trivial to apply to SAC. In particular, managing the interaction between\nnovelty-based exploration and SAC's stochastic policy can lead to inefficient\nexploration and redundant sample collection. In this paper, we propose KEA\n(Keeping Exploration Alive) which tackles the inefficiencies in balancing\nexploration strategies when combining SAC with novelty-based exploration. KEA\nintroduces an additional co-behavior agent that works alongside SAC and a\nswitching mechanism to facilitate proactive coordination between exploration\nstrategies from novelty-based exploration and stochastic policy. This\ncoordination allows the agent to maintain stochasticity in high-novelty\nregions, enhancing exploration efficiency and reducing repeated sample\ncollection. We first analyze this potential issue in a 2D navigation task and\nthen evaluate KEA on sparse reward control tasks from the DeepMind Control\nSuite. Compared to state-of-the-art novelty-based exploration baselines, our\nexperiments show that KEA significantly improves learning efficiency and\nrobustness in sparse reward setups.",
        "Health outcomes depend on complex environmental and sociodemographic factors\nwhose effects change over location and time. Only recently has fine-grained\nspatial and temporal data become available to study these effects, namely the\nMEDSAT dataset of English health, environmental, and sociodemographic\ninformation. Leveraging this new resource, we use a variety of variable\nimportance techniques to robustly identify the most informative predictors\nacross multiple health outcomes. We then develop an interpretable machine\nlearning framework based on Generalized Additive Models (GAMs) and Multiscale\nGeographically Weighted Regression (MGWR) to analyze both local and global\nspatial dependencies of each variable on various health outcomes. Our findings\nidentify NO2 as a global predictor for asthma, hypertension, and anxiety,\nalongside other outcome-specific predictors related to occupation, marriage,\nand vegetation. Regional analyses reveal local variations with air pollution\nand solar radiation, with notable shifts during COVID. This comprehensive\napproach provides actionable insights for addressing health disparities, and\nadvocates for the integration of interpretable machine learning in public\nhealth.",
        "Physics-based numerical models have been the bedrock of atmospheric sciences\nfor decades, offering robust solutions but often at the cost of significant\ncomputational resources. Deep learning (DL) models have emerged as powerful\ntools in meteorology, capable of analyzing complex weather and climate data by\nlearning intricate dependencies and providing rapid predictions once trained.\nWhile these models demonstrate promising performance in weather prediction,\noften surpassing traditional physics-based methods, they still face critical\nchallenges. This paper presents a comprehensive survey of recent deep learning\nand foundation models for weather prediction. We propose a taxonomy to classify\nexisting models based on their training paradigms: deterministic predictive\nlearning, probabilistic generative learning, and pre-training and fine-tuning.\nFor each paradigm, we delve into the underlying model architectures, address\nmajor challenges, offer key insights, and propose targeted directions for\nfuture research. Furthermore, we explore real-world applications of these\nmethods and provide a curated summary of open-source code repositories and\nwidely used datasets, aiming to bridge research advancements with practical\nimplementations while fostering open and trustworthy scientific practices in\nadopting cutting-edge artificial intelligence for weather prediction. The\nrelated sources are available at https:\/\/github.com\/JimengShi\/\nDL-Foundation-Models-Weather.",
        "Let $G$ be a nontrivial graph with minimum degree $\\delta$ and $k$ an integer\nwith $k\\ge 2$. In the literature, there are eigenvalue conditions that imply\n$G$ contains $k$ edge-disjoint spanning trees. We give eigenvalue conditions\nthat imply $G$ contains $k$ edge-disjoint spanning trees and another forest $F$\nwith $|E(F)|>\\frac{\\delta-1}{\\delta}(|V(G)|-1)$, and if $F$ is not a spanning\ntree, then $F$ has a component with at least $\\delta$ edges.",
        "Pruning eliminates unnecessary parameters in neural networks; it offers a\npromising solution to the growing computational demands of large language\nmodels (LLMs). While many focus on post-training pruning, sparse\npre-training--which combines pruning and pre-training into a single\nphase--provides a simpler alternative. In this work, we present the first\nsystematic exploration of optimal sparse pre-training configurations for LLMs\nthrough an examination of 80 unique pruning schedules across different sparsity\nlevels and training durations. We find that initiating pruning at 25% of total\ntraining compute and concluding at 75% achieves near-optimal final evaluation\nloss. These findings provide valuable insights for efficient and effective\nsparse pre-training of LLMs. Furthermore, we propose a new scaling law that\nmodifies the Chinchilla scaling law to use the average parameter count over\npre-training. Through empirical and theoretical validation, we demonstrate that\nthis modified scaling law accurately models evaluation loss for both sparsely\nand densely pre-trained LLMs, unifying scaling laws across pre-training\nparadigms. Our findings indicate that while sparse pre-training achieves the\nsame final model quality as dense pre-training for equivalent compute budgets,\nit provides substantial benefits through reduced model size, enabling\nsignificant potential computational savings during inference.",
        "The automatic generation of programs has long been a fundamental challenge in\ncomputer science. Recent benchmarks have shown that large language models\n(LLMs) can effectively generate code at the function level, make code edits,\nand solve algorithmic coding tasks. However, to achieve full automation, LLMs\nshould be able to generate production-quality, self-contained application\nmodules. To evaluate the capabilities of LLMs in solving this challenge, we\nintroduce BaxBench, a novel evaluation benchmark consisting of 392 tasks for\nthe generation of backend applications. We focus on backends for three critical\nreasons: (i) they are practically relevant, building the core components of\nmost modern web and cloud software, (ii) they are difficult to get right,\nrequiring multiple functions and files to achieve the desired functionality,\nand (iii) they are security-critical, as they are exposed to untrusted\nthird-parties, making secure solutions that prevent deployment-time attacks an\nimperative. BaxBench validates the functionality of the generated applications\nwith comprehensive test cases, and assesses their security exposure by\nexecuting end-to-end exploits. Our experiments reveal key limitations of\ncurrent LLMs in both functionality and security: (i) even the best model,\nOpenAI o1, achieves a mere 60% on code correctness; (ii) on average, we could\nsuccessfully execute security exploits on more than half of the correct\nprograms generated by each LLM; and (iii) in less popular backend frameworks,\nmodels further struggle to generate correct and secure applications. Progress\non BaxBench signifies important steps towards autonomous and secure software\ndevelopment with LLMs.",
        "In the realm of online advertising, optimizing conversions is crucial for\ndelivering relevant products to users and enhancing business outcomes.\nPredicting conversion events is challenging due to variable delays between user\ninteractions, such as impressions or clicks, and the actual conversions. These\ndelays differ significantly across various advertisers and products,\nnecessitating distinct optimization time windows for targeted conversions. To\naddress this, we introduce a novel approach named the \\textit{Personalized\nInterpolation} method, which innovatively builds upon existing fixed conversion\nwindow models to estimate flexible conversion windows. This method allows for\nthe accurate estimation of conversions across a variety of delay ranges, thus\nmeeting the diverse needs of advertisers without increasing system complexity.\nTo validate the efficacy of our proposed method, we conducted comprehensive\nexperiments using ads conversion model. Our experiments demonstrate that this\nmethod not only achieves high prediction accuracy but also does so more\nefficiently than other existing solutions. This validation underscores the\npotential of our Personalized Interpolation method to significantly enhance\nconversion optimization in real-world online advertising systems, promising\nimproved targeting and effectiveness in advertising strategies.",
        "The memorization capacity of neural networks with a given architecture has\nbeen thoroughly studied in many works. Specifically, it is well-known that\nmemorizing $N$ samples can be done using a network of constant width,\nindependent of $N$. However, the required constructions are often quite\ndelicate. In this paper, we consider the natural question of how well\nfeedforward ReLU neural networks can memorize robustly, namely while being able\nto withstand adversarial perturbations of a given radius. We establish both\nupper and lower bounds on the possible radius for general $l_p$ norms, implying\n(among other things) that width logarithmic in the number of input samples is\nnecessary and sufficient to achieve robust memorization (with robustness radius\nindependent of $N$).",
        "The stress interval $S(u,v)$ between $u,v\\in V(G)$ is the set of all vertices\nin a graph $G$ that lie on every shortest $u,v$-path. A set $U \\subseteq V(G)$\nis stress convex if $S(u,v) \\subseteq U$ for any $u,v\\in U$. A vertex $v \\in\nV(G)$ is s-extreme if $V(G)-v$ is a stress convex set in $G$. The stress number\n$sn(G)$ of $G$ is the minimum cardinality of a set $U$ where $\\bigcup_{u,v \\in\nU}S(u,v)=V(G)$. The stress hull number $sh(G)$ of $G$ is the minimum\ncardinality of a set whose stress convex hull is $V(G)$. In this paper, we\npresent many basic properties of stress intervals. We characterize s-extreme\nvertices of a graph $G$ and construct graphs $G$ with arbitrarily large\ndifference between the number of s-extreme vertices, $sh(G)$ and $sn(G)$. Then\nwe study these three invariants for some special graph families, such as graph\nproducts, split graphs, and block graphs. We show that in any split graph $G$,\n$sh(G)=sn(G)=|Ext_s(G)|$, where $Ext_s(G)$ is the set of s-extreme vertices of\n$G$. Finally, we show that for $k \\in \\mathbb{N}$, deciding whether $sn(G) \\leq\nk$ is NP-complete problem, even when restricted to bipartite graphs.",
        "The automatic generation of representative natural language descriptions for\nobservable patterns in time series data enhances interpretability, simplifies\nanalysis and increases cross-domain utility of temporal data. While pre-trained\nfoundation models have made considerable progress in natural language\nprocessing (NLP) and computer vision (CV), their application to time series\nanalysis has been hindered by data scarcity. Although several large language\nmodel (LLM)-based methods have been proposed for time series forecasting, time\nseries captioning is under-explored in the context of LLMs. In this paper, we\nintroduce TSLM, a novel time series language model designed specifically for\ntime series captioning. TSLM operates as an encoder-decoder model, leveraging\nboth text prompts and time series data representations to capture subtle\ntemporal patterns across multiple phases and generate precise textual\ndescriptions of time series inputs. TSLM addresses the data scarcity problem in\ntime series captioning by first leveraging an in-context prompting synthetic\ndata generation, and second denoising the generated data via a novel\ncross-modal dense retrieval scoring applied to time series-caption pairs.\nExperimental findings on various time series captioning datasets demonstrate\nthat TSLM outperforms existing state-of-the-art approaches from multiple data\nmodalities by a significant margin.",
        "Multi-agent reinforcement learning is a challenging and active field of\nresearch due to the inherent nonstationary property and coupling between\nagents. A popular approach to modeling the multi-agent interactions underlying\nthe multi-agent RL problem is the Markov Game. There is a special type of\nMarkov Game, termed Markov Potential Game, which allows us to reduce the Markov\nGame to a single-objective optimal control problem where the objective function\nis a potential function. In this work, we prove that a multi-agent\ncollaborative field coverage problem, which is found in many engineering\napplications, can be formulated as a Markov Potential Game, and we can learn a\nparameterized closed-loop Nash Equilibrium by solving an equivalent\nsingle-objective optimal control problem. As a result, our algorithm is 10x\nfaster during training compared to a game-theoretic baseline and converges\nfaster during policy execution.",
        "Despite the recent success of image-text contrastive models like CLIP and\nSigLIP, these models often struggle with vision-centric tasks that demand\nhigh-fidelity image understanding, such as counting, depth estimation, and\nfine-grained object recognition. These models, by performing language\nalignment, tend to prioritize high-level semantics over visual understanding,\nweakening their image understanding. On the other hand, vision-focused models\nare great at processing visual information but struggle to understand language,\nlimiting their flexibility for language-driven tasks. In this work, we\nintroduce TULIP, an open-source, drop-in replacement for existing CLIP-like\nmodels. Our method leverages generative data augmentation, enhanced image-image\nand text-text contrastive learning, and image\/text reconstruction\nregularization to learn fine-grained visual features while preserving global\nsemantic alignment. Our approach, scaling to over 1B parameters, outperforms\nexisting state-of-the-art (SOTA) models across multiple benchmarks,\nestablishing a new SOTA zero-shot performance on ImageNet-1K, delivering up to\na $2\\times$ enhancement over SigLIP on RxRx1 in linear probing for few-shot\nclassification, and improving vision-language models, achieving over $3\\times$\nhigher scores than SigLIP on MMVP. Our code\/checkpoints are available at\nhttps:\/\/tulip-berkeley.github.io",
        "Existing vehicle trajectory prediction models struggle with generalizability,\nprediction uncertainties, and handling complex interactions. It is often due to\nlimitations like complex architectures customized for a specific dataset and\ninefficient multimodal handling. We propose Perceiver with Register queries\n(PerReg+), a novel trajectory prediction framework that introduces: (1)\nDual-Level Representation Learning via Self-Distillation (SD) and Masked\nReconstruction (MR), capturing global context and fine-grained details.\nAdditionally, our approach of reconstructing segmentlevel trajectories and lane\nsegments from masked inputs with query drop, enables effective use of\ncontextual information and improves generalization; (2) Enhanced Multimodality\nusing register-based queries and pretraining, eliminating the need for\nclustering and suppression; and (3) Adaptive Prompt Tuning during fine-tuning,\nfreezing the main architecture and optimizing a small number of prompts for\nefficient adaptation. PerReg+ sets a new state-of-the-art performance on\nnuScenes [1], Argoverse 2 [2], and Waymo Open Motion Dataset (WOMD) [3].\nRemarkable, our pretrained model reduces the error by 6.8% on smaller datasets,\nand multi-dataset training enhances generalization. In cross-domain tests,\nPerReg+ reduces B-FDE by 11.8% compared to its non-pretrained variant.",
        "A natural question, inspired by the famous Ryser-Brualdi-Stein Conjecture, is\nto determine the largest positive integer $g(r,n)$ such that every collection\nof $n$ matchings, each of size $n$, in an $r$-partite $r$-uniform hypergraph\ncontains a rainbow matching of size $g(r,n)$. The parameter $g'(r,n)$ is\ndefined identically with the exception that the host hypergraph is not required\nto be $r$-partite.\n  In this note, we improve the best known lower bounds on $g'(r,n)$ for all $r\n\\geq 4$ and the upper bounds on $g(r,n)$ for all $r \\geq 3$, provided $n$ is\nsufficiently large. More precisely, we show that if $r\\ge3$ then\n$$\\frac{2n}{r+1}-\\Theta_r(1)\\le g'(r,n)\\le g(r,n)\\le\nn-\\Theta_r(n^{1-\\frac{1}{r}}).$$ Interestingly, while it has been conjectured\nthat $g(2,n)=g'(2,n)=n-1$, our results show that if $r\\ge3$ then $g(r,n)$ and\n$g'(r,n)$ are bounded away from $n$ by a function which grows in $n$.\n  We also prove analogous bounds for the related problem where we are\ninterested in the smallest size $s$ for which any collection of $n$ matchings\nof size $s$ in an ($r$-partite) $r$-uniform hypergraph contains a rainbow\nmatching of size $n$.",
        "Non-smooth and non-convex global optimization poses significant challenges\nacross various applications, where standard gradient-based methods often\nstruggle. We propose the Ball-Proximal Point Method, Broximal Point Method, or\nBall Point Method (BPM) for short - a novel algorithmic framework inspired by\nthe classical Proximal Point Method (PPM) (Rockafellar, 1976), which, as we\nshow, sheds new light on several foundational optimization paradigms and\nphenomena, including non-convex and non-smooth optimization, acceleration,\nsmoothing, adaptive stepsize selection, and trust-region methods. At the core\nof BPM lies the ball-proximal (\"broximal\") operator, which arises from the\nclassical proximal operator by replacing the quadratic distance penalty by a\nball constraint. Surprisingly, and in sharp contrast with the sublinear rate of\nPPM in the nonsmooth convex regime, we prove that BPM converges linearly and in\na finite number of steps in the same regime. Furthermore, by introducing the\nconcept of ball-convexity, we prove that BPM retains the same global\nconvergence guarantees under weaker assumptions, making it a powerful tool for\na broader class of potentially non-convex optimization problems. Just like PPM\nplays the role of a conceptual method inspiring the development of practically\nefficient algorithms and algorithmic elements, e.g., gradient descent, adaptive\nstep sizes, acceleration (Ahn & Sra, 2020), and \"W\" in AdamW (Zhuang et al.,\n2022), we believe that BPM should be understood in the same manner: as a\nblueprint and inspiration for further development.",
        "In this paper, we study additional aspects of the capacity distribution on\nthe set $\\mathcal{B}_n$ of compositions of $n$ consisting of $1$'s and $2$'s.\nAmong our results are further recurrences for this distribution as well as\nformulas for the total capacity and sign balance on $\\mathcal{B}_n$. We provide\nalgebraic and combinatorial proofs of our results. We also give combinatorial\nexplanations of some prior results where such a proof was requested. Finally,\nthe joint distribution of the capacity statistic with two further parameters on\n$\\mathcal{B}_n$ is briefly considered.",
        "We introduce a pioneering autoregressive generative model for 3D point cloud\ngeneration. Inspired by visual autoregressive modeling (VAR), we conceptualize\npoint cloud generation as an autoregressive up-sampling process. This leads to\nour novel model, PointARU, which progressively refines 3D point clouds from\ncoarse to fine scales. PointARU follows a two-stage training paradigm: first,\nit learns multi-scale discrete representations of point clouds, and then it\ntrains an autoregressive transformer for next-scale prediction. To address the\ninherent unordered and irregular structure of point clouds, we incorporate\nspecialized point-based up-sampling network modules in both stages and\nintegrate 3D absolute positional encoding based on the decoded point cloud at\neach scale during the second stage. Our model surpasses state-of-the-art (SoTA)\ndiffusion-based approaches in both generation quality and parameter efficiency\nacross diverse experimental settings, marking a new milestone for\nautoregressive methods in 3D point cloud generation. Furthermore, PointARU\ndemonstrates exceptional performance in completing partial 3D shapes and\nup-sampling sparse point clouds, outperforming existing generative models in\nthese tasks.",
        "In this paper, we introduce rerailing automata for $\\omega$-regular\nlanguages. They generalize both deterministic parity (DPW) and minimized\nhistory-deterministic co-B\\\"uchi automata (with transition based acceptance,\nHdTbcBW) while combining their favorable properties. In particular, rerailing\nautomata can represent arbitrary $\\omega$-regular languages while allowing for\npolynomial-time minimization, just as HdTbcBW do. Since DPW are a special case\nof rerailing automata, a minimized rerailing automaton is never larger than the\nsmallest deterministic parity automaton for the same language. We also show\nthat rerailing automata can be used as a replacement for deterministic parity\nautomata for the realizability check of open systems.\n  The price to be paid to obtain the useful properties of rerailing automata is\nthat the acceptance condition in such automata refers to the dominating colors\nalong all runs for a given word, where just as in parity automata, the\ndominating color along a run is the lowest one occurring infinitely often along\nit. A rerailing automaton accepts those words for which the greatest of the\ndominating colors along the runs is even. Additionally, rerailing automata\nguarantee that every prefix of a run for a word can be extended to eventually\nreach a point from which all runs for the word extending the prefix have the\nsame dominating color, and it is even if and only if the word is in the\nlanguage of the automaton. We show that these properties together allow\ncharacterizing the role of each state in such an automaton in a way that\nrelates it to state combinations in a sequence of co-B\\\"uchi automata for the\nrepresented language. This characterization forms the basis of the\npolynomial-time minimization approach in this paper."
      ]
    }
  },
  {
    "id":2411.10822,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"Additive manufacturing and sustainability: an exploratory study of the advantages and challenges",
    "start_abstract":"The emergence of advanced manufacturing technologies, coupled with consumer demands for more customised products and services, are causing shifts in the scale distribution manufacturing. In this paper, consideration is given to role one such process technology: additive consequences adopting novel production technology on industrial sustainability not well understood exploratory study draws publically available data provide insights into impacts sustainability. Benefits found exist across product material life cycles through redesign, improvements input processing, make-to-order component manufacturing, closing loop. As an immature technology, there substantial challenges these benefits being realised at each stage cycle. This paper summarises advantages challenges, discusses implications terms sources innovation, business models, configuration value chains.",
    "start_categories":[
      "cond-mat.mtrl-sci"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b5"
      ],
      "title":[
        "Machine learning in additive manufacturing: State-of-the-art and perspectives"
      ],
      "abstract":[
        "Additive manufacturing (AM) has emerged as a disruptive digital manufacturing technology. However, its broad adoption in industry is still hindered by high entry barriers of design for additive manufacturing (DfAM), limited materials library, various processing defects, and inconsistent product quality. In recent years, machine learning (ML) has gained increasing attention in AM due to its unprecedented performance in data tasks such as classification, regression and clustering. This article provides a comprehensive review on the state-of-the-art of ML applications in a variety of AM domains. In the DfAM, ML can be leveraged to output new high-performance metamaterials and optimized topological designs. In AM processing, contemporary ML algorithms can help to optimize process parameters, and conduct examination of powder spreading and in-process defect monitoring. On the production of AM, ML is able to assist practitioners in pre-manufacturing planning, and product quality assessment and control. Moreover, there has been an increasing concern about data security in AM as data breaches could occur with the aid of ML techniques. Lastly, it concludes with a section summarizing the main findings from the literature and providing perspectives on some selected interesting applications of ML in research and development of AM."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "No-Regret Linear Bandits under Gap-Adjusted Misspecification",
        "Continually Evolved Multimodal Foundation Models for Cancer Prognosis",
        "Mechanistic PDE Networks for Discovery of Governing Equations",
        "Fewer May Be Better: Enhancing Offline Reinforcement Learning with\n  Reduced Dataset",
        "Can KAN CANs? Input-convex Kolmogorov-Arnold Networks (KANs) as\n  hyperelastic constitutive artificial neural networks (CANs)",
        "Comparative Study of Deep Learning Architectures for Textual Damage\n  Level Classification",
        "Inorganic Catalyst Efficiency Prediction Based on EAPCR Model: A Deep\n  Learning Solution for Multi-Source Heterogeneous Data",
        "KNN and K-means in Gini Prametric Spaces",
        "Value Gradient Sampler: Sampling as Sequential Decision Making",
        "BOPO: Neural Combinatorial Optimization via Best-anchored and\n  Objective-guided Preference Optimization",
        "Model Monitoring in the Absence of Labeled Data via Feature Attributions\n  Distributions",
        "Preconditioned Inexact Stochastic ADMM for Deep Model",
        "Graph Augmentation for Cross Graph Domain Generalization",
        "RF Desense significance and its impact on the EVM at Signal Near the\n  Noise Floor",
        "Concentration phenomena for a mixed local\/nonlocal Schr\\\"{o}dinger\n  equation with Dirichlet datum",
        "Toward a Flexible Framework for Linear Representation Hypothesis Using\n  Maximum Likelihood Estimation",
        "Rethinking Oversmoothing in Graph Neural Networks: A Rank-Based\n  Perspective",
        "Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision\n  Language Model Inference",
        "Real Time Control of Tandem-Wing Experimental Platform Using Concerto\n  Reinforcement Learning",
        "Adaptive Teaming in Multi-Drone Pursuit: Simulation, Training, and\n  Deployment",
        "Transformer-based Wireless Symbol Detection Over Fading Channels",
        "Structured Context Recomposition for Large Language Models Using\n  Probabilistic Layer Realignment",
        "Increasing Batch Size Improves Convergence of Stochastic Gradient\n  Descent with Momentum",
        "Disharmony: Forensics using Reverse Lighting Harmonization",
        "On the surjectivity of $\\mathfrak{p}$-adic Galois representations\n  attached to Drinfeld modules of rank $2$",
        "Modified Dai-Liao Spectral Conjugate Gradient Method with Application to\n  Signal Processing",
        "Robust Data Watermarking in Language Models by Injecting Fictitious\n  Knowledge",
        "Large language models streamline automated systematic review: A\n  preliminary study"
      ],
      "abstract":[
        "This work studies linear bandits under a new notion of gap-adjusted\nmisspecification and is an extension of Liu et al. (2023). When the underlying\nreward function is not linear, existing linear bandits work usually relies on a\nuniform misspecification parameter $\\epsilon$ that measures the sup-norm error\nof the best linear approximation. This results in an unavoidable linear regret\nwhenever $\\epsilon > 0$. We propose a more natural model of misspecification\nwhich only requires the approximation error at each input $x$ to be\nproportional to the suboptimality gap at $x$. It captures the intuition that,\nfor optimization problems, near-optimal regions should matter more and we can\ntolerate larger approximation errors in suboptimal regions.\n  Quite surprisingly, we show that the classical LinUCB algorithm -- designed\nfor the realizable case -- is automatically robust against such\n$\\rho$-gap-adjusted misspecification with parameter $\\rho$ diminishing at\n$O(1\/(d \\sqrt{\\log T}))$. It achieves a near-optimal $O(\\sqrt{T})$ regret for\nproblems that the best-known regret is almost linear in time horizon $T$. We\nfurther advance this frontier by presenting a novel phased elimination-based\nalgorithm whose gap-adjusted misspecification parameter $\\rho = O(1\/\\sqrt{d})$\ndoes not scale with $T$. This algorithm attains optimal $O(\\sqrt{T})$ regret\nand is deployment-efficient, requiring only $\\log T$ batches of exploration. It\nalso enjoys an adaptive $O(\\log T)$ regret when a constant suboptimality gap\nexists. Technically, our proof relies on a novel self-bounding argument that\nbounds the part of the regret due to misspecification by the regret itself, and\na new inductive lemma that limits the misspecification error within the\nsuboptimality gap for all valid actions in each batch selected by G-optimal\ndesign.",
        "Cancer prognosis is a critical task that involves predicting patient outcomes\nand survival rates. To enhance prediction accuracy, previous studies have\nintegrated diverse data modalities, such as clinical notes, medical images, and\ngenomic data, leveraging their complementary information. However, existing\napproaches face two major limitations. First, they struggle to incorporate\nnewly arrived data with varying distributions into training, such as patient\nrecords from different hospitals, thus rendering sub-optimal generalizability\nand limited utility in real-world applications. Second, most multimodal\nintegration methods rely on simplistic concatenation or task-specific\npipelines, which fail to capture the complex interdependencies across\nmodalities. To address these, we propose a continually evolving multi-modal\nfoundation model. Extensive experiments on the TCGA dataset demonstrate the\neffectiveness of our approach, highlighting its potential to advance cancer\nprognosis by enabling robust and adaptive multimodal integration.",
        "We present Mechanistic PDE Networks -- a model for discovery of governing\npartial differential equations from data. Mechanistic PDE Networks represent\nspatiotemporal data as space-time dependent linear partial differential\nequations in neural network hidden representations. The represented PDEs are\nthen solved and decoded for specific tasks. The learned PDE representations\nnaturally express the spatiotemporal dynamics in data in neural network hidden\nspace, enabling increased power for dynamical modeling. Solving the PDE\nrepresentations in a compute and memory-efficient way, however, is a\nsignificant challenge. We develop a native, GPU-capable, parallel, sparse, and\ndifferentiable multigrid solver specialized for linear partial differential\nequations that acts as a module in Mechanistic PDE Networks. Leveraging the PDE\nsolver, we propose a discovery architecture that can discover nonlinear PDEs in\ncomplex settings while also being robust to noise. We validate PDE discovery on\na number of PDEs, including reaction-diffusion and Navier-Stokes equations.",
        "Offline reinforcement learning (RL) represents a significant shift in RL\nresearch, allowing agents to learn from pre-collected datasets without further\ninteraction with the environment. A key, yet underexplored, challenge in\noffline RL is selecting an optimal subset of the offline dataset that enhances\nboth algorithm performance and training efficiency. Reducing dataset size can\nalso reveal the minimal data requirements necessary for solving similar\nproblems. In response to this challenge, we introduce ReDOR (Reduced Datasets\nfor Offline RL), a method that frames dataset selection as a gradient\napproximation optimization problem. We demonstrate that the widely used\nactor-critic framework in RL can be reformulated as a submodular optimization\nobjective, enabling efficient subset selection. To achieve this, we adapt\northogonal matching pursuit (OMP), incorporating several novel modifications\ntailored for offline RL. Our experimental results show that the data subsets\nidentified by ReDOR not only boost algorithm performance but also do so with\nsignificantly lower computational complexity.",
        "Traditional constitutive models rely on hand-crafted parametric forms with\nlimited expressivity and generalizability, while neural network-based models\ncan capture complex material behavior but often lack interpretability. To\nbalance these trade-offs, we present Input-Convex Kolmogorov-Arnold Networks\n(ICKANs) for learning polyconvex hyperelastic constitutive laws. ICKANs\nleverage the Kolmogorov-Arnold representation, decomposing the model into\ncompositions of trainable univariate spline-based activation functions for rich\nexpressivity. We introduce trainable input-convex splines within the KAN\narchitecture, ensuring physically admissible polyconvex hyperelastic models.\nThe resulting models are both compact and interpretable, enabling explicit\nextraction of analytical constitutive relationships through an input-convex\nsymbolic regression techinque. Through unsupervised training on full-field\nstrain data and limited global force measurements, ICKANs accurately capture\nnonlinear stress-strain behavior across diverse strain states. Finite element\nsimulations of unseen geometries with trained ICKAN hyperelastic constitutive\nmodels confirm the framework's robustness and generalization capability.",
        "Given the paramount importance of safety in the aviation industry, even minor\noperational anomalies can have significant consequences. Comprehensive\ndocumentation of incidents and accidents serves to identify root causes and\npropose safety measures. However, the unstructured nature of incident event\nnarratives poses a challenge for computer systems to interpret. Our study aimed\nto leverage Natural Language Processing (NLP) and deep learning models to\nanalyze these narratives and classify the aircraft damage level incurred during\nsafety occurrences. Through the implementation of LSTM, BLSTM, GRU, and sRNN\ndeep learning models, our research yielded promising results, with all models\nshowcasing competitive performance, achieving an accuracy of over 88%\nsignificantly surpassing the 25% random guess threshold for a four-class\nclassification problem. Notably, the sRNN model emerged as the top performer in\nterms of recall and accuracy, boasting a remarkable 89%. These findings\nunderscore the potential of NLP and deep learning models in extracting\nactionable insights from unstructured text narratives, particularly in\nevaluating the extent of aircraft damage within the realm of aviation safety\noccurrences.",
        "The design of inorganic catalysts and the prediction of their catalytic\nefficiency are fundamental challenges in chemistry and materials science.\nTraditional catalyst evaluation methods primarily rely on machine learning\ntechniques; however, these methods often struggle to process multi-source\nheterogeneous data, limiting both predictive accuracy and generalization. To\naddress these limitations, this study introduces the\nEmbedding-Attention-Permutated CNN-Residual (EAPCR) deep learning model. EAPCR\nconstructs a feature association matrix using embedding and attention\nmechanisms and enhances predictive performance through permutated CNN\narchitectures and residual connections. This approach enables the model to\naccurately capture complex feature interactions across various catalytic\nconditions, leading to precise efficiency predictions. EAPCR serves as a\npowerful tool for computational researchers while also assisting domain experts\nin optimizing catalyst design, effectively bridging the gap between data-driven\nmodeling and experimental applications. We evaluate EAPCR on datasets from TiO2\nphotocatalysis, thermal catalysis, and electrocatalysis, demonstrating its\nsuperiority over traditional machine learning methods (e.g., linear regression,\nrandom forest) as well as conventional deep learning models (e.g., ANN, NNs).\nAcross multiple evaluation metrics (MAE, MSE, R2, and RMSE), EAPCR consistently\noutperforms existing approaches. These findings highlight the strong potential\nof EAPCR in inorganic catalytic efficiency prediction. As a versatile deep\nlearning framework, EAPCR not only improves predictive accuracy but also\nestablishes a solid foundation for future large-scale model development in\ninorganic catalysis.",
        "This paper introduces innovative enhancements to the K-means and K-nearest\nneighbors (KNN) algorithms based on the concept of Gini prametric spaces.\nUnlike traditional distance metrics, Gini-based measures incorporate both\nvalue-based and rank-based information, improving robustness to noise and\noutliers. The main contributions of this work include: proposing a Gini-based\nmeasure that captures both rank information and value distances; presenting a\nGini K-means algorithm that is proven to converge and demonstrates resilience\nto noisy data; and introducing a Gini KNN method that performs competitively\nwith state-of-the-art approaches such as Hassanat's distance in noisy\nenvironments. Experimental evaluations on 14 datasets from the UCI repository\ndemonstrate the superior performance and efficiency of Gini-based algorithms in\nclustering and classification tasks. This work opens new avenues for leveraging\nrank-based measures in machine learning and statistical analysis.",
        "We propose the Value Gradient Sampler (VGS), a trainable sampler based on the\ninterpretation of sampling as discrete-time sequential decision-making. VGS\ngenerates samples from a given unnormalized density (i.e., energy) by drifting\nand diffusing randomly initialized particles. In VGS, finding the optimal drift\nis equivalent to solving an optimal control problem where the cost is the upper\nbound of the KL divergence between the target density and the samples. We\nemploy value-based dynamic programming to solve this optimal control problem,\nwhich gives the gradient of the value function as the optimal drift vector. The\nconnection to sequential decision making allows VGS to leverage extensively\nstudied techniques in reinforcement learning, making VGS a fast, adaptive, and\naccurate sampler that achieves competitive results in various sampling\nbenchmarks. Furthermore, VGS can replace MCMC in contrastive divergence\ntraining of energy-based models. We demonstrate the effectiveness of VGS in\ntraining accurate energy-based models in industrial anomaly detection\napplications.",
        "Neural Combinatorial Optimization (NCO) has emerged as a promising approach\nfor NP-hard problems. However, prevailing RL-based methods suffer from low\nsample efficiency due to sparse rewards and underused solutions. We propose\nPreference Optimization for Combinatorial Optimization (POCO), a training\nparadigm that leverages solution preferences via objective values. It\nintroduces: (1) an efficient preference pair construction for better explore\nand exploit solutions, and (2) a novel loss function that adaptively scales\ngradients via objective differences, removing reliance on reward models or\nreference policies. Experiments on Job-Shop Scheduling (JSP), Traveling\nSalesman (TSP), and Flexible Job-Shop Scheduling (FJSP) show POCO outperforms\nstate-of-the-art neural methods, reducing optimality gaps impressively with\nefficient inference. POCO is architecture-agnostic, enabling seamless\nintegration with existing NCO models, and establishes preference optimization\nas a principled framework for combinatorial optimization.",
        "Model monitoring involves analyzing AI algorithms once they have been\ndeployed and detecting changes in their behaviour. This thesis explores machine\nlearning model monitoring ML before the predictions impact real-world decisions\nor users. This step is characterized by one particular condition: the absence\nof labelled data at test time, which makes it challenging, even often\nimpossible, to calculate performance metrics.\n  The thesis is structured around two main themes: (i) AI alignment, measuring\nif AI models behave in a manner consistent with human values and (ii)\nperformance monitoring, measuring if the models achieve specific accuracy goals\nor desires.\n  The thesis uses a common methodology that unifies all its sections. It\nexplores feature attribution distributions for both monitoring dimensions.\nUsing these feature attribution explanations, we can exploit their theoretical\nproperties to derive and establish certain guarantees and insights into model\nmonitoring.",
        "The recent advancement of foundation models (FMs) has brought about a\nparadigm shift, revolutionizing various sectors worldwide. The popular\noptimizers used to train these models are stochastic gradient descent-based\nalgorithms, which face inherent limitations, such as slow convergence and\nstringent assumptions for convergence. In particular, data heterogeneity\narising from distributed settings poses significant challenges to their\ntheoretical and numerical performance. This paper develops an algorithm, PISA\n({P}reconditioned {I}nexact {S}tochastic {A}lternating Direction Method of\nMultipliers), which enables scalable parallel computing and supports various\nsecond-moment schemes. Grounded in rigorous theoretical guarantees, the\nalgorithm converges under the sole assumption of Lipschitz continuity of the\ngradient, thereby removing the need for other conditions commonly imposed by\nstochastic methods. This capability enables PISA to tackle the challenge of\ndata heterogeneity effectively. Comprehensive experimental evaluations for\ntraining or fine-tuning diverse FMs, including vision models, large language\nmodels, reinforcement learning models, generative adversarial networks, and\nrecurrent neural networks, demonstrate its superior numerical performance\ncompared to various state-of-the-art optimizers.",
        "Cross-graph node classification, utilizing the abundant labeled nodes from\none graph to help classify unlabeled nodes in another graph, can be viewed as a\ndomain generalization problem of graph neural networks (GNNs) due to the\nstructure shift commonly appearing among various graphs. Nevertheless, current\nendeavors for cross-graph node classification mainly focus on model training.\nData augmentation approaches, a simple and easy-to-implement domain\ngeneralization technique, remain under-explored. In this paper, we develop a\nnew graph structure augmentation for the crossgraph domain generalization\nproblem. Specifically, low-weight edgedropping is applied to remove potential\nnoise edges that may hinder the generalization ability of GNNs, stimulating the\nGNNs to capture the essential invariant information underlying different\nstructures. Meanwhile, clustering-based edge-adding is proposed to generate\ninvariant structures based on the node features from the same distribution.\nConsequently, with these augmentation techniques, the GNNs can maintain the\ndomain invariant structure information that can improve the generalization\nability. The experiments on out-ofdistribution citation network datasets verify\nour method achieves state-of-the-art performance among conventional\naugmentations.",
        "Hardware impairments and system non-linearities impacting communication\nsignal is one of key aspect for having harmonics and RF desense which overall\ncausing the lower quality and integrity of the modulated signal, resulting in\nI\/Q imbalance, further bit error and spectral efficiency degradation. This\npresentation outlines the RF Desense results, EVM Measurement and it impact at\nthe almost noise floor with step size by 1 dB in QPSK at LTE Bands, Note for\nmmW 3GPP 38.521-2 clause 6.4.2.1 indicates single polarization.",
        "We consider the mixed local\/nonlocal semilinear equation\n  \\begin{equation*}\n  -\\epsilon^{2}\\Delta u +\\epsilon^{2s}(-\\Delta)^s u +u=u^p\\qquad \\text{in }\n\\Omega\n  \\end{equation*} with zero Dirichlet datum, where $\\epsilon>0$ is a small\nparameter, $s\\in(0,1)$, $p\\in(1,\\frac{n+2}{n-2})$ and $\\Omega$ is a smooth,\nbounded domain. We construct a family of solutions that concentrate, as\n$\\epsilon\\rightarrow 0$, at an interior point of $\\Omega$ having uniform\ndistance to $\\partial\\Omega$ (this point can also be characterized as a local\nminimum of a nonlocal functional).\n  In spite of the presence of the Laplace operator, the leading order of the\nrelevant reduced energy functional in the Lyapunov-Schmidt procedure is\npolynomial rather than exponential in the distance to the boundary, in light of\nthe nonlocal effect at infinity. A delicate analysis is required to establish\nsome uniform estimates with respect to $\\epsilon$, due to the difficulty caused\nby the different scales coming from the mixed operator.",
        "Linear representation hypothesis posits that high-level concepts are encoded\nas linear directions in the representation spaces of LLMs. Park et al. (2024)\nformalize this notion by unifying multiple interpretations of linear\nrepresentation, such as 1-dimensional subspace representation and\ninterventions, using a causal inner product. However, their framework relies on\nsingle-token counterfactual pairs and cannot handle ambiguous contrasting\npairs, limiting its applicability to complex or context-dependent concepts. We\nintroduce a new notion of binary concepts as unit vectors in a canonical\nrepresentation space, and utilize LLMs' (neural) activation differences along\nwith maximum likelihood estimation (MLE) to compute concept directions (i.e.,\nsteering vectors). Our method, Sum of Activation-base Normalized Difference\n(SAND), formalizes the use of activation differences modeled as samples from a\nvon Mises-Fisher (vMF) distribution, providing a principled approach to derive\nconcept directions. We extend the applicability of Park et al. (2024) by\neliminating the dependency on unembedding representations and single-token\npairs. Through experiments with LLaMA models across diverse concepts and\nbenchmarks, we demonstrate that our lightweight approach offers greater\nflexibility, superior performance in activation engineering tasks like\nmonitoring and manipulation.",
        "Oversmoothing is a fundamental challenge in graph neural networks (GNNs): as\nthe number of layers increases, node embeddings become increasingly similar,\nand model performance drops sharply. Traditionally, oversmoothing has been\nquantified using metrics that measure the similarity of neighbouring node\nfeatures, such as the Dirichlet energy. While these metrics are related to\noversmoothing, we argue they have critical limitations and fail to reliably\ncapture oversmoothing in realistic scenarios. For instance, they provide\nmeaningful insights only for very deep networks and under somewhat strict\nconditions on the norm of network weights and feature representations. As an\nalternative, we propose measuring oversmoothing by examining the numerical or\neffective rank of the feature representations. We provide theoretical support\nfor this approach, demonstrating that the numerical rank of feature\nrepresentations converges to one for a broad family of nonlinear activation\nfunctions under the assumption of nonnegative trained weights. To the best of\nour knowledge, this is the first result that proves the occurrence of\noversmoothing without assumptions on the boundedness of the weight matrices.\nAlong with the theoretical findings, we provide extensive numerical evaluation\nacross diverse graph architectures. Our results show that rank-based metrics\nconsistently capture oversmoothing, whereas energy-based metrics often fail.\nNotably, we reveal that a significant drop in the rank aligns closely with\nperformance degradation, even in scenarios where energy metrics remain\nunchanged.",
        "Vision Language Models (VLMs) have achieved remarkable success in a wide\nrange of vision applications of increasing complexity and scales, yet choosing\nthe right VLM model size involves a trade-off between response quality and\ncost. While smaller VLMs are cheaper to run, they typically produce responses\nonly marginally better than random guessing on benchmarks such as MMMU.\n  In this paper, we propose Cache of Thought (CoT), a master apprentice\nframework for collaborative inference between large and small VLMs. CoT manages\nhigh quality query results from large VLMs (master) in a cache, which are then\nselected via a novel multi modal retrieval and in-context learning to aid the\nperformance of small VLMs (apprentice). We extensively evaluate CoT on various\nwidely recognized and challenging general VQA benchmarks, and show that CoT\nincreases overall VQA performance by up to 7.7% under the same budget, and\nspecifically boosts the performance of apprentice VLMs by up to 36.6%.",
        "This paper introduces the CRL2RT algorithm, an advanced reinforcement\nlearning method aimed at improving the real-time control performance of the\nDirect-Drive Tandem-Wing Experimental Platform (DDTWEP). Inspired by dragonfly\nflight, DDTWEP's tandem wing structure causes nonlinear and unsteady\naerodynamic interactions, leading to complex load behaviors during pitch, roll,\nand yaw maneuvers. These complexities challenge stable motion control at high\nfrequencies (2000 Hz). To overcome these issues, we developed the CRL2RT\nalgorithm, which combines classical control elements with reinforcement\nlearning-based controllers using a time-interleaved architecture and a\nrule-based policy composer. This integration ensures finite-time convergence\nand single-life adaptability. Experimental results under various conditions,\nincluding different flapping frequencies and yaw disturbances, show that CRL2RT\nachieves a control frequency surpassing 2500 Hz on standard CPUs. Additionally,\nwhen integrated with classical controllers like PID, Adaptive PID, and Model\nReference Adaptive Control (MRAC), CRL2RT enhances tracking performance by\n18.3% to 60.7%. These findings demonstrate CRL2RT's broad applicability and\nsuperior performance in complex real-time control scenarios, validating its\neffectiveness in overcoming existing control strategy limitations and advancing\nrobust, efficient real-time control for biomimetic aerial vehicles.",
        "Adaptive teaming, the ability to collaborate with unseen teammates without\nprior coordination, remains an underexplored challenge in multi-robot\ncollaboration. This paper focuses on adaptive teaming in multi-drone\ncooperative pursuit, a critical task with real-world applications such as\nborder surveillance, search-and-rescue, and counter-terrorism. We first define\nand formalize the \\textbf{A}daptive Teaming in \\textbf{M}ulti-\\textbf{D}rone\n\\textbf{P}ursuit (AT-MDP) problem and introduce AT-MDP framework, a\ncomprehensive framework that integrates simulation, algorithm training and\nreal-world deployment. AT-MDP framework provides a flexible experiment\nconfigurator and interface for simulation, a distributed training framework\nwith an extensive algorithm zoo (including two newly proposed baseline methods)\nand an unseen drone zoo for evaluating adaptive teaming, as well as a\nreal-world deployment system that utilizes edge computing and Crazyflie drones.\nTo the best of our knowledge, AT-MDP framework is the first adaptive framework\nfor continuous-action decision-making in complex real-world drone tasks,\nenabling multiple drones to coordinate effectively with unseen teammates.\nExtensive experiments in four multi-drone pursuit environments of increasing\ndifficulty confirm the effectiveness of AT-MDP framework, while real-world\ndeployments further validate its feasibility in physical systems. Videos and\ncode are available at https:\/\/sites.google.com\/view\/at-mdp.",
        "Pre-trained Transformers, through in-context learning (ICL), have\ndemonstrated exceptional capabilities to adapt to new tasks using example\nprompts without model update. Transformer-based wireless receivers, where\nprompts consist of the pilot data in the form of transmitted and received\nsignal pairs, have shown high detection accuracy when pilot data are abundant.\nHowever, pilot information is often costly and limited in practice. In this\nwork, we propose the DEcision Feedback INcontExt Detection (DEFINED) solution\nas a new wireless receiver design, which bypasses channel estimation and\ndirectly performs symbol detection using the (sometimes extremely) limited\npilot data. The key innovation in DEFINED is the proposed decision feedback\nmechanism in ICL, where we sequentially incorporate the detected symbols into\nthe prompts as pseudo-labels to improve the detection for subsequent symbols.\nFurthermore, we proposed another detection method where we combine ICL with\nSemi-Supervised Learning (SSL) to extract information from both labeled and\nunlabeled data during inference, thus avoiding the errors propagated during the\ndecision feedback process of the original DEFINED. Extensive experiments across\na broad range of wireless communication settings demonstrate that a small\nTransformer trained with DEFINED or IC-SSL achieves significant performance\nimprovements over conventional methods, in some cases only needing a single\npilot pair to achieve similar performance of the latter with more than 4 pilot\npairs.",
        "Extended sequence generation often leads to degradation in contextual\nconsistency due to the inability of conventional self-attention mechanisms to\neffectively retain long-range dependencies. Existing approaches, including\nmemory compression and retrieval-augmented conditioning, introduce\ncomputational trade-offs that either increase inference latency or impose\nadditional storage overhead. Structured Context Recomposition (SCR) introduces\na probabilistic layer realignment strategy that dynamically adjusts learned\nrepresentations within transformer layers, ensuring that semantically relevant\nembeddings persist throughout extended transformations. The proposed method\nenhances coherence retention through a recursive weighting function that\nredistributes representational emphasis based on inferred contextual relevance\nrather than relying on fixed token-level attention scores. Empirical results\nindicate that probabilistic realignment mitigates abrupt topic shifts and\nlogical inconsistencies, particularly in scenarios where sequences exceed\nstandard attention window constraints. Sequence-level entropy analysis further\nreveals that SCR moderates representational variability without introducing\nexcessive output regularization, allowing models to sustain generative\ndiversity while preserving contextual alignment. Attention head deviation\nmeasurements confirm that hierarchical reweighting contributes to smoother\ntoken dependency transitions across transformer layers, reinforcing the\nstability of multi-turn interactions and document-level reasoning.\nComputational resource assessments show that while SCR incurs a moderate\nincrease in processing time, memory overhead remains within feasible limits,\nmaking it suitable for practical deployment in autoregressive generative\napplications.",
        "Stochastic gradient descent with momentum (SGDM), which is defined by adding\na momentum term to SGD, has been well studied in both theory and practice.\nTheoretically investigated results showed that the settings of the learning\nrate and momentum weight affect the convergence of SGDM. Meanwhile, practical\nresults showed that the setting of batch size strongly depends on the\nperformance of SGDM. In this paper, we focus on mini-batch SGDM with constant\nlearning rate and constant momentum weight, which is frequently used to train\ndeep neural networks in practice. The contribution of this paper is showing\ntheoretically that using a constant batch size does not always minimize the\nexpectation of the full gradient norm of the empirical loss in training a deep\nneural network, whereas using an increasing batch size definitely minimizes it,\nthat is, increasing batch size improves convergence of mini-batch SGDM. We also\nprovide numerical results supporting our analyses, indicating specifically that\nmini-batch SGDM with an increasing batch size converges to stationary points\nfaster than with a constant batch size. Python implementations of the\noptimizers used in the numerical experiments are available at\nhttps:\/\/anonymous.4open.science\/r\/momentum-increasing-batch-size-888C\/.",
        "Content generation and manipulation approaches based on deep learning methods\nhave seen significant advancements, leading to an increased need for techniques\nto detect whether an image has been generated or edited. Another area of\nresearch focuses on the insertion and harmonization of objects within images.\nIn this study, we explore the potential of using harmonization data in\nconjunction with a segmentation model to enhance the detection of edited image\nregions. These edits can be either manually crafted or generated using deep\nlearning methods. Our findings demonstrate that this approach can effectively\nidentify such edits. Existing forensic models often overlook the detection of\nharmonized objects in relation to the background, but our proposed Disharmony\nNetwork addresses this gap. By utilizing an aggregated dataset of harmonization\ntechniques, our model outperforms existing forensic networks in identifying\nharmonized objects integrated into their backgrounds, and shows potential for\ndetecting various forms of edits, including virtual try-on tasks.",
        "Let $\\mathbb{F}_{q}$ be the finite field with $q\\geq 5$ elements and\n$A:=\\mathbb{F}_{q}[T]$. For a class of $\\mathfrak{p} \\in \\mathrm{Spec}(A)\n\\setminus \\{(0)\\}$, but fixed, we produce infinitely many Drinfeld $A$-modules\nof rank $2$, for which the associated $\\mathfrak{p}$-adic Galois representation\nis surjective. This result is a variant of the work of~[Ray24] for\n$\\mathfrak{p}=(T)$. We also show that for a class of $\\mathfrak{l}=(l) \\in\n\\mathrm{Spec}(A)$, where $l$ is a monic polynomial, the $\\mathfrak{p}$-adic\nGalois representation, attached to the Drinfeld $A$-module\n$\\varphi_{T}=T+g_{1}\\tau-l^{q-1}\\tau^2$ with $g_{1} \\in A \\setminus\n\\mathfrak{l}$, is surjective for all $\\mathfrak{p} \\in\n\\mathrm{Spec}(A)\\setminus\\{(0)\\}$. This result generalizes the work of [Zyw11]\nfrom $\\mathfrak{l}=(T), g_1=1$.",
        "In this article, we present a modified variant of the Dai-Liao spectral\nconjugate gradient method, developed through an analysis of eigenvalues and\ninspired by a modified secant condition. We show that the proposed method is\nglobally convergent for general nonlinear functions under standard assumptions.\nBy incorporating the new secant condition and a quasi-Newton direction, we\nintroduce updated spectral parameters. These changes ensure that the resulting\nsearch direction satisfies the sufficient descent property without relying on\nany line search. Numerical experiments show that the proposed algorithm\nperforms better than several existing methods in terms of convergence speed and\ncomputational efficiency. Its effectiveness is further demonstrated through an\napplication in signal processing.",
        "Data watermarking in language models injects traceable signals, such as\nspecific token sequences or stylistic patterns, into copyrighted text, allowing\ncopyright holders to track and verify training data ownership. Previous data\nwatermarking techniques primarily focus on effective memorization after\npretraining, while overlooking challenges that arise in other stages of the LLM\npipeline, such as the risk of watermark filtering during data preprocessing, or\npotential forgetting through post-training, or verification difficulties due to\nAPI-only access. We propose a novel data watermarking approach that injects\ncoherent and plausible yet fictitious knowledge into training data using\ngenerated passages describing a fictitious entity and its associated\nattributes. Our watermarks are designed to be memorized by the LLM through\nseamlessly integrating in its training data, making them harder to detect\nlexically during preprocessing. We demonstrate that our watermarks can be\neffectively memorized by LLMs, and that increasing our watermarks' density,\nlength, and diversity of attributes strengthens their memorization. We further\nshow that our watermarks remain robust throughout LLM development, maintaining\ntheir effectiveness after continual pretraining and supervised finetuning.\nFinally, we show that our data watermarks can be evaluated even under API-only\naccess via question answering.",
        "Large Language Models (LLMs) have shown promise in natural language\nprocessing tasks, with the potential to automate systematic reviews. This study\nevaluates the performance of three state-of-the-art LLMs in conducting\nsystematic review tasks. We assessed GPT-4, Claude-3, and Mistral 8x7B across\nfour systematic review tasks: study design formulation, search strategy\ndevelopment, literature screening, and data extraction. Sourced from a\npreviously published systematic review, we provided reference standard\nincluding standard PICO (Population, Intervention, Comparison, Outcome) design,\nstandard eligibility criteria, and data from 20 reference literature. Three\ninvestigators evaluated the quality of study design and eligibility criteria\nusing 5-point Liker Scale in terms of accuracy, integrity, relevance,\nconsistency and overall performance. For other tasks, the output is defined as\naccurate if it is the same as the reference standard. Search strategy\nperformance was evaluated through accuracy and retrieval efficacy. Screening\naccuracy was assessed for both abstracts screening and full texts screening.\nData extraction accuracy was evaluated across 1,120 data points comprising\n3,360 individual fields. Claude-3 demonstrated superior overall performance in\nPICO design. In search strategy formulation, GPT-4 and Claude-3 achieved\ncomparable accuracy, outperforming Mistral. For abstract screening, GPT-4\nachieved the highest accuracy, followed by Mistral and Claude-3. In data\nextraction, GPT-4 significantly outperformed other models. LLMs demonstrate\npotential for automating systematic review tasks, with GPT-4 showing superior\nperformance in search strategy formulation, literature screening and data\nextraction. These capabilities make them promising assistive tools for\nresearchers and warrant further development and validation in this field."
      ]
    }
  },
  {
    "id":2411.10822,
    "research_type":"applied",
    "start_id":"b5",
    "start_title":"Machine learning in additive manufacturing: State-of-the-art and perspectives",
    "start_abstract":"Additive manufacturing (AM) has emerged as a disruptive digital manufacturing technology. However, its broad adoption in industry is still hindered by high entry barriers of design for additive manufacturing (DfAM), limited materials library, various processing defects, and inconsistent product quality. In recent years, machine learning (ML) has gained increasing attention in AM due to its unprecedented performance in data tasks such as classification, regression and clustering. This article provides a comprehensive review on the state-of-the-art of ML applications in a variety of AM domains. In the DfAM, ML can be leveraged to output new high-performance metamaterials and optimized topological designs. In AM processing, contemporary ML algorithms can help to optimize process parameters, and conduct examination of powder spreading and in-process defect monitoring. On the production of AM, ML is able to assist practitioners in pre-manufacturing planning, and product quality assessment and control. Moreover, there has been an increasing concern about data security in AM as data breaches could occur with the aid of ML techniques. Lastly, it concludes with a section summarizing the main findings from the literature and providing perspectives on some selected interesting applications of ML in research and development of AM.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "Additive manufacturing and sustainability: an exploratory study of the advantages and challenges"
      ],
      "abstract":[
        "The emergence of advanced manufacturing technologies, coupled with consumer demands for more customised products and services, are causing shifts in the scale distribution manufacturing. In this paper, consideration is given to role one such process technology: additive consequences adopting novel production technology on industrial sustainability not well understood exploratory study draws publically available data provide insights into impacts sustainability. Benefits found exist across product material life cycles through redesign, improvements input processing, make-to-order component manufacturing, closing loop. As an immature technology, there substantial challenges these benefits being realised at each stage cycle. This paper summarises advantages challenges, discusses implications terms sources innovation, business models, configuration value chains."
      ],
      "categories":[
        "cond-mat.mtrl-sci"
      ]
    },
    "list":{
      "title":[
        "Fine-tuning foundation models of materials interatomic potentials with\n  frozen transfer learning",
        "Dislocation correlations in GaN epitaxial films revealed by EBSD and XRD",
        "Chiral Altermagnon in MnTe",
        "From High-Entropy Alloys to Alloys with High Entropy: A New Paradigm in\n  Materials Science and Engineering for Advancing Sustainable Metallurgy",
        "Microscopic origin of magnetoferroelectricity in monolayer NiBr$_{2}$\n  and NiI$_{2}$",
        "In situ growth and magnetic characterization of Cr Chloride monolayers",
        "Goldstone-mediated polar instability in hexagonal barium titanate",
        "Compositionally Grading Alloy Stacking Fault Energy using Autonomous\n  Path Planning and Additive Manufacturing with Elemental Powders",
        "Intercalated structures formed by platinum on epitaxial graphene on\n  SiC(0001)",
        "Photo-induced spall failure of (111) twist grain boundaries in Ni\n  bicrystals",
        "Floquet optical selection rules in black phosphorus",
        "Magnon-phonon interactions from first principles",
        "A full breakthrough in vacuum ultraviolet nonlinear optical performance\n  of NH4B4O6F",
        "Modified FOX Optimizer for Solving optimization problems",
        "On the minimum cut-sets of the power graph of a finite cyclic group, II",
        "Inferring collective synchrony observing spiking of one or several\n  neurons",
        "Bridging statistical mechanics and thermodynamics away from equilibrium:\n  a data-driven approach for learning internal variables and their dynamics",
        "Scalar probability density function mixing models need not comply with\n  the linearity and independence hypothesis",
        "The entropy profiles of a definable set over finite fields",
        "Optimal Functional $2^{s-1}$-Batch Codes: Exploring New Sufficient\n  Conditions",
        "On the ascent of almost and quasi-atomicity to monoid semidomains",
        "Refined curve counting with descendants and quantum mirrors",
        "Dual Control for Interactive Autonomous Merging with Model Predictive\n  Diffusion",
        "Nonrelativistic spin-splitting multiferroic antiferromagnet and\n  compensated ferrimagnet with zero net magnetization",
        "The influence of missing data mechanisms and simple missing data\n  handling techniques on fairness",
        "From Mutation to Degradation: Predicting Nonsense-Mediated Decay with\n  NMDEP",
        "Optimal Follow-Up of Gravitational-Wave Events with the UltraViolet\n  EXplorer (UVEX)",
        "Zubarev response approach to polarization phenomena in local equilibrium"
      ],
      "abstract":[
        "Machine-learned interatomic potentials are revolutionising atomistic\nmaterials simulations by providing accurate and scalable predictions within the\nscope covered by the training data. However, generation of an accurate and\nrobust training data set remains a challenge, often requiring thousands of\nfirst-principles calculations to achieve high accuracy. Foundation models have\nstarted to emerge with the ambition to create universally applicable potentials\nacross a wide range of materials. While foundation models can be robust and\ntransferable, they do not yet achieve the accuracy required to predict reaction\nbarriers, phase transitions, and material stability. This work demonstrates\nthat foundation model potentials can reach chemical accuracy when fine-tuned\nusing transfer learning with partially frozen weights and biases. For two\nchallenging datasets on reactive chemistry at surfaces and stability and\nelastic properties of tertiary alloys, we show that frozen transfer learning\nwith 10-20% of the data (hundreds of datapoints) achieves similar accuracies to\nmodels trained from scratch (on thousands of datapoints). Moreover, we show\nthat an equally accurate, but significantly more efficient surrogate model can\nbe built using the transfer learned potential as the ground truth. In\ncombination, we present a simulation workflow for machine learning potentials\nthat improves data efficiency and computational efficiency.",
        "Correlations between dislocations in crystals reduce the elastic energy via\nscreening of the strain by the surrounding dislocations. We study the\ncorrelations of threading dislocations in GaN epitaxial films with dislocation\ndensities of 5x10^8 cm^-2 and 1.8x10^10 cm^-2 by X-ray diffraction (XRD) in\nreciprocal space and by high-resolution electron backscatter diffraction (EBSD)\nin real space, where the strain is derived from a cross-correlation analysis of\nthe Kikuchi patterns. The measured XRD curves and EBSD strain and rotation maps\nare compared with Monte Carlo simulations within one and the same model for the\ndislocation distributions. The screening of the dislocation strains is provided\nby creating pairs of dislocations with opposite Burgers vectors, with the mean\ndistance between dislocations in a pair equal to the screening distance. The\npairs overlap and cannot be distinguished as separate dipoles. The\nEBSD-measured autocorrelation functions of the strain and rotation components\nfollow the expected logarithmic law for distances smaller than the screening\ndistances and become zero for larger distances, which is confirmed by the Monte\nCarlo simulations. Screening distances of 2 \\textmu m and 0.3 \\textmu m are\nobtained for the samples with low and high dislocation densities, respectively.\nThe dislocation strain is thus screened by only 4 neighboring dislocations.\nHigh-resolution EBSD allows for a more precise determination of the screening\ndistances than from fits of the XRD curves. In addition, an anisotropic\nresolution of the EBSD measurements is observed and quantified.",
        "Altermagnetism has surfaced as a novel magnetic phase, bridging the\nproperties of ferro- and anti-ferromagnetism. The momentum-dependent\nspin-splitting observed in these materials reflects their unique symmetry\ncharacteristics which also establish the conditions for chiral magnons to\nemerge. Here we provide the first direct experimental evidence for a chiral\nmagnon in the altermagnetic candidate MnTe, revealed by circular-dichroism\nresonant inelastic X-ray scattering (CD-RIXS). This mode which we term chiral\naltermagnon exhibits a distinct momentum dependence consistent with the\nproposed altermagnetic $g-$wave symmetry of MnTe. Our results reveal a new\nclass of magnetic excitations, demonstrating how altermagnetic order shapes\nspin dynamics and paves the way for advances in spintronic and quantum\ntechnologies.",
        "The development of high-entropy alloys (HEAs) has marked a paradigm shift in\nalloy design, moving away from traditional methods that prioritize a dominant\nbase metal enhanced by minor elements. HEAs instead incorporate multiple\nalloying elements with no single dominant component, broadening the scope of\nalloy design. This shift has led to the creation of diverse alloys with high\nentropy (AHEs) families, including high-entropy steels, superalloys, and\nintermetallics, each highlighting the need to consider additional factors such\nas stacking fault energy (SFE), lattice misfit, and anti-phase boundary energy\n(APBE) due to their significant influence on microstructure and performance.\nLeveraging multiple elements in alloying opens up promising possibilities for\ndeveloping new alloys from multi-component scrap and electronic waste, reducing\nreliance on critical metals and emphasizing the need for advanced data\ngeneration techniques. With the vast possibilities offered by these\nmulti-component feedstocks, modelling and Artificial Intelligence based tools\nare essential to efficiently explore and optimize new alloys, supporting\nsustainable progress in metallurgy. These advancements call for a reimagined\nalloy design framework, emphasizing robust data acquisition, alternative design\nparameters, and advanced computational tools over traditional\ncomposition-focused methodologies.",
        "We investigate the magnetoelectric properties of the monolayer NiX$_{2}$ (X =\nBr, I) through first-principles calculations. Our calculations predict that the\nNiBr$_{2}$ monolayer exhibits a cycloidal magnetic ground state. For the\nNiI$_{2}$ monolayer, a proper-screw helical magnetic ground state with\nmodulation vector \\(\\boldsymbol{Q} = (q, 0, 0)\\) is adopted, approximated based\non experimental observations. The electric polarization in NiBr$_{2}$ shows a\nlinear dependence on the spin-orbit coupling strength \\(\\lambda_{\\text{SOC}}\\),\nwhich can be adequately described by the generalized Katsura-Nagaosa-Balatsky\n(gKNB) model, considering contributions from up to the third nearest-neighbor\nspin pairs. In contrast, the electric polarization in NiI$_{2}$ exhibits a\ndistinct dependence on \\(q\\) and \\(\\lambda_{\\text{SOC}}\\), which cannot be\nfully explained by the gKNB mechanism alone. To address this, the \\(p\\)-\\(d\\)\nhybridization mechanism is extended to NiI$_{2}$ to explain the observed\nbehavior. The respective contributions from the \\(p\\)-\\(d\\) hybridization and\nthe gKNB mechanism in NiI$_{2}$ are then quantitatively evaluated. Overall, our\nwork elucidates the microscopic mechanisms underlying multiferroicity in\nNiBr$_{2}$ and NiI$_{2}$ monolayers, with the conclusions readily applicable to\ntheir bulk forms.",
        "Monolayer Chromium Dihalides and Trihalides materials can be grown on a\nvariety of substrates by molecular beam epitaxy regardless of the lattice\nmismatch thanks to the van der Waals epitaxy. In this work, we studied the\nmagnetic nature of Cr Chloride monolayers grown on Au(111), Ni(111) and\ngraphene-passivated Ni(111) from the evaporation in ultra-high vacuum of the\nsame halide precursor. Structural, morphological and magnetic characterizations\nwere conducted in situ by low energy electron diffraction (LEED), scanning\ntunneling microscopy (STM) and X-ray magnetic circular dichroism (XMCD). Owing\nto opposite chemical behaviour, Au(111) and Ni(111) promote the formation of\ntwo different valence compounds, i.e. CrCl$_3$ and CrCl$_2$, showing distinct\nmagnetic properties at 4 K. When graphene is used to passivate the Ni(111)\nsurface, the formation of CrCl$_3$ becomes allowed also on this substrate. The\ncoexistence of CrCl$_3$ and CrCl$_2$, both showing few nm lateral size and\nsuper-paramagnetic properties, is demonstrated by XMCD spectra displaying two\ndichroic peaks at the characteristic Cr$^{3+}$ and Cr$^{2+}$ energies.\nSite-selective magnetization measurements performed with the photon energy\ntuned on the two absorption edges show reversed magnetization of some of the\nCrCl$_2$ islands with respect to the CrCl3 domains, which is interpreted in\nterms of magnetic frustration.",
        "We discover a rare structural manifestation of the Goldstone paradigm in a\nhexagonal polytype of the archetypal ferroelectric BaTiO3. First-principles\ncalculations confirm the Goldstone character of the order parameter, and\nhigh-resolution diffraction measurements link this to a quasi-continuous domain\ntexture in the vicinity of the low-temperature phase transitions. Our findings\nhighlight how changes in structural topology may be exploited to realize rich\npolar topologies in bulk ferroelectric perovskites.",
        "Compositionally graded alloys (CGAs) are often proposed for use in structural\ncomponents where the combination of two or more alloys within a single part can\nyield substantial enhancement in performance and functionality. For these\napplications, numerous design methodologies have been developed, one of the\nmost sophisticated being the application of path planning algorithms originally\ndesigned for robotics to solve CGA design problems. In addition to the\ntraditional application to structural components, this work proposes and\ndemonstrates the application of this CGA design framework to rapid alloy\ndesign, synthesis, and characterization. A composition gradient in the CoCrFeNi\nalloy space was planned between the maximum and minimum stacking fault energy\n(SFE) as predicted by a previously developed model in a face-centered cubic\n(FCC) high entropy alloy (HEA) space. The path was designed to be monotonic in\nSFE and avoid regions that did not meet FCC phase fraction and solidification\nrange constraints predicted by CALculation of PHAse Diagrams (CALPHAD).\nCompositions from the path were selected to produce a linear gradient in SFE,\nand the CGA was built using laser directed energy deposition (L-DED). The\nresulting gradient was characterized for microstructure and mechanical\nproperties, including hardness, elastic modulus, and strain rate sensitivity.\nDespite being predicted to contain a single FCC phase throughout the gradient,\npart of the CGA underwent a martensitic transformation, thereby demonstrating a\nlimitation of using equilibrium CALPHAD calculations for phase stability\npredictions. More broadly, this demonstrates the ability of the methods\nemployed to bring attention to blind spots in alloy models.",
        "Graphene on SiC intercalated with two-dimensional metal layers, such as Pt,\noffers a versatile platform for applications in spintronics, catalysis, and\nbeyond. Recent studies have demonstrated that Pt atoms can intercalate at the\nheterointerface between SiC(0001) and the C-rich\n$(6\\sqrt{3}\\times6\\sqrt{3})$R30{\\deg} reconstructed surface (hereafter referred\nas the buffer layer). However, key aspects such as intercalated phase structure\nand intercalation mechanisms remain unclear. In this work, we investigate\nchanges in morphology, chemistry, and electronic structure for both buffer\nlayer and monolayer graphene grown on SiC(0001) following Pt deposition and\nannealing cycles, which eventually led to Pt intercalation at temperatures\nabove 500{\\deg}C. Atomic-resolution imaging of the buffer layer reveals a\nsingle intercalated Pt layer that removes the periodic corrugation of the\nbuffer layer, arising from partial bonding of C-atoms with Si-atoms of the\nsubstrate. In monolayer graphene, the Pt-intercalated regions exhibit a\ntwo-level structure: the first level corresponds to a Pt layer intercalated\nbelow the buffer layer, while the second level contains a second Pt layer,\ngiving rise to a $(12\\times12)$ superstructure relative to graphene. Upon\nintercalation, Pt atoms appear as silicides, indicating a reaction with Si\natoms from the substrate. Additionally, charge neutral $\\pi$-bands\ncorresponding to quasi-free-standing monolayer and bilayer graphene emerge.\nAnalysis of multiple samples, coupled with a temperature-dependent study of the\nintercalation rate, demonstrates the pivotal role of buffer layer regions in\nfacilitating the Pt intercalation in monolayer graphene. These findings provide\nvaluable insight into Pt intercalation, advancing the potential for\napplications.",
        "Spall failure, a complex failure mechanism driven by tensile stress wave\ninteractions, has been extensively studied in single-crystal FCC metals,\nrevealing a precursor stage involving dislocation emission along closed-packed\ndirections. Here we investigate the photo-induced spall failure of Ni\nbicrystals under a two-pulse laser configuration, exploring various\nmisorientation angles through two-temperature molecular dynamics (MD)\nsimulations including electronic effects to simulate light-matter interaction.\nOur findings demonstrate that light-matter interactions can induce spall\nfailure at the sample center, similar to conventional plate-impact methods,\nwhen two laser-pulses are applied to the front and back surfaces of the sample.\nThe study reveals the significant influence of misorientation angles on\ndislocation activity and spall behavior, where grain boundaries (GBs) play\npivotal roles, either promoting or impeding dislocation interactions.\nFurthermore, our work highlights the potential for enhancing spall resistance\nby tailoring materials through misorientation angle variation.",
        "The optical selection rules endorsed by symmetry are crucial for\nunderstanding the optical properties of quantum materials and the associated\nultrafast spectral phenomena. Herein, we introduce momentum-resolved Floquet\noptical selection rules using the group theory to elucidate the pump-probe\nphotoemission spectral distributions of monolayer black phosphorus (BP), which\nare governed by the symmetries of both the material and the lasers. Using\ntime-dependent density functional theory (TDDFT), we further investigate the\ndynamical evolution of Floquet(-Volkov) states in the photoemission spectra of\nmonolayer BP, revealing their spectral weights at specific momenta for each\nsideband. These observations are comprehensively explained by the proposed\nFloquet optical selection rules. Our framework not only clarifies experimental\nphotoemission spectra but also uncovers novel characteristics under different\npump-probe configurations. Our results are expected to deepen the understanding\nof light-induced ultrafast spectra in BP and can be further extended to other\nFloquet systems.",
        "Modeling spin-wave (magnon) dynamics in novel materials is important to\nadvance spintronics and spin-based quantum technologies. The interactions\nbetween magnons and lattice vibrations (phonons) limit the length scale for\nmagnon transport. However, quantifying these interactions remains challenging.\nHere we show many-body calculations of magnon-phonon (mag-ph) coupling based on\nthe ab initio Bethe-Salpeter equation. We derive expressions for mag-ph\ncoupling matrices and compute them in 2D ferromagnets, focusing on hydrogenated\ngraphene and monolayer CrI3. Our analysis shows that electron-phonon (e-ph) and\nmag-ph interactions differ significantly, where modes with weak e-ph coupling\ncan exhibit strong mag-ph coupling (and vice versa), and reveals which phonon\nmodes couple more strongly with magnons. In both materials studied here, the\ninelastic magnon relaxation time is found to decrease abruptly above the\nthreshold for emission of strongly coupled phonons, thereby defining a\nlow-energy window for efficient magnon transport. By averaging in this window,\nwe compute the temperature-dependent magnon mean-free path, a key figure of\nmerit for spintronics, entirely from first principles. The theory and\ncomputational tools shown in this work enable studies of magnon interactions,\nscattering, and dynamics in generic materials, advancing the design of magnetic\nsystems and magnon- and spin-based devices.",
        "The lack of suitable vacuum ultraviolet (VUV) nonlinear optical (NLO)\ncrystals has hindered the development of compact, high-power VUV sources via\nsecond harmonic generation (SHG). Here, we report on the development of the\nfluorooxoborate crystal NH4B4O6F (ABF) as a promising material for VUV light\ngeneration. For the first time, devices with specific phase-matching angles\nwere constructed, achieving a record 158.9 nm VUV light through phase-matching\nSHG and a maximum nanosecond pulse energy of 4.8 mJ at 177.3 nm with a\nconversion efficiency of 5.9 %. The enhanced NLO performance is attributed to\noptimized arrangements of fluorine-based units creating asymmetric sublattices.\nThis work marks a significant milestone in the field of NLO materials,\nfacilitating the future applications of compact, high-power VUV lasers\nutilizing ABF.",
        "The FOX optimizer, inspired by red fox hunting behavior, is a powerful\nalgorithm for solving real-world and engineering problems. However, despite\nbalancing exploration and exploitation, it can prematurely converge to local\noptima, as agent positions are updated solely based on the current best-known\nposition, causing all agents to converge on one location. This study proposes\nthe modified FOX optimizer (mFOX) to enhance exploration and balance\nexploration and exploitation in three steps. First, the Oppositional-Based\nLearning (OBL) strategy is used to improve the initial population. Second,\ncontrol parameters are refined to achieve a better balance between exploration\nand exploitation. Third, a new update equation is introduced, allowing agents\nto adjust their positions relative to one another rather than relying solely on\nthe best-known position. This approach improves exploration efficiency without\nadding complexity. The mFOX algorithm's performance is evaluated against 12\nwell-known algorithms on 23 classical benchmark functions, 10 CEC2019\nfunctions, and 12 CEC2022 functions. It outperforms competitors in 74% of the\nclassical benchmarks, 60% of the CEC2019 benchmarks, and 58% of the CEC2022\nbenchmarks. Additionally, mFOX effectively addresses four engineering problems.\nThese results demonstrate mFOX's strong competitiveness in solving complex\noptimization tasks, including unimodal, constrained, and high-dimensional\nproblems.",
        "The power graph $\\mathcal{P}(G)$ of a finite group $G$ is the simple graph\nwith vertex set $G$ and two distinct vertices are adjacent if one of them is a\npower of the other. Let $n=p_1^{n_1}p_2^{n_2}\\cdots p_r^{n_r},$ where\n$p_1,p_2,\\ldots,p_r$ are primes with $p_1<p_2<\\cdots <p_r$ and $n_1,n_2,\\ldots,\nn_r$ are positive integers. For the cyclic group $C_n$ of order $n$, the\nminimum cut-sets of $\\mathcal{P}(C_n)$ are characterized in \\cite{cps} for\n$r\\leq 3$. Recently, in \\cite{MPS}, certain cut-sets of $\\mathcal{P}(C_n)$ are\nidentified such that any minimum cut-set of $\\mathcal{P}(C_n)$ must be one of\nthem. In this paper, for $r\\geq 4$, we explicitly determine the minimum\ncut-sets, in particular, the vertex connectivity of $\\mathcal{P}(C_n)$ when:\n(i) $n_r\\geq 2$, (ii) $r=4$ and $n_r=1$, and (iii) $r=5$, $n_r=1$, $p_1\\geq 3$.",
        "We tackle a quantification of synchrony in a large ensemble of interacting\nneurons from the observation of spiking events. In a simulation study, we\nefficiently infer the synchrony level in a neuronal population from a point\nprocess reflecting spiking of a small number of units and even from a single\nneuron. We introduce a synchrony measure (order parameter) based on the\nBartlett covariance density; this quantity can be easily computed from the\nrecorded point process. This measure is robust concerning missed spikes and, if\ncomputed from observing several neurons, does not require spike sorting. We\nillustrate the approach by modeling populations of spiking or bursting neurons,\nincluding the case of sparse synchrony.",
        "Thermodynamics with internal variables is a common approach in continuum\nmechanics to model inelastic (i.e., non-equilibrium) material behavior. While\nthis approach is computationally and theoretically attractive, it currently\nlacks a well-established statistical mechanics foundation. As a result,\ninternal variables are typically chosen phenomenologically and lack a direct\nlink to the underlying physics which hinders the predictability of the theory.\nTo address these challenges, we propose a machine learning approach that is\nconsistent with the principles of statistical mechanics and thermodynamics. The\nproposed approach leverages the following techniques (i) the information\nbottleneck (IB) method to ensure that the learned internal variables are\nfunctions of the microstates and are capable of capturing the salient feature\nof the microscopic distribution; (ii) conditional normalizing flows to\nrepresent arbitrary probability distributions of the microscopic states as\nfunctions of the state variables; and (iii) Variational Onsager Neural Networks\n(VONNs) to guarantee thermodynamic consistency and Markovianity of the learned\nevolution equations. The resulting framework, called IB-VONNs, is tested on two\nproblems of colloidal systems, governed at the microscale by overdamped\nLangevin dynamics. The first one is a prototypical model for a colloidal\nparticle in an optical trap, which can be solved analytically, and thus ideal\nto verify the framework. The second problem is a one-dimensional\nphase-transforming system, whose macroscopic description still lacks a\nstatistical mechanics foundation under general conditions. The results in both\ncases indicate that the proposed machine learning strategy can indeed bridge\nstatistical mechanics and thermodynamics with internal variables away from\nequilibrium.",
        "In a mixture of scalar fields undergoing diffusive processes governed by\nFick's law, the concentration at each point evolves linearly in the\nconcentrations at all points and independently from the other concentrations,\nwhen one considers a finite differences integration of their evolution\nequations. However, these properties must not necessarily be enforced in\nprobability density function models, since they are relaxed when conditional\nexpected values are taken.",
        "A definable set $X$ in the first-order language of rings defines a family of\nrandom vectors: for each finite field $\\mathbb{F}_q$, let the distribution be\nsupported and uniform on the $\\mathbb{F}_q$-rational points of $X$. We employ\nresults from the model theory of finite fields to show that their entropy\nprofiles settle into one of finitely many stable asymptotic behaviors as $q$\ngrows. The attainable asymptotic entropy profiles and their dominant terms as\nfunctions of $q$ are computable. This generalizes a construction of Mat\\'u\\v{s}\nwhich gives an information-theoretic interpretation to algebraic matroids.",
        "A functional $k$-batch code of dimension $s$ consists of $n$ servers storing\nlinear combinations of $s$ linearly independent information bits. These codes\nare designed to recover any multiset of $k$ requests, each being a linear\ncombination of the information bits, by $k$ disjoint subsets of servers. A\nrecent conjecture suggests that for any set of $k = 2^{s-1}$ requests, the\noptimal solution requires $2^s-1$ servers. This paper shows that the problem of\nfunctional $k$-batch codes is equivalent to several other problems. Using these\nequivalences, we derive sufficient conditions that improve understanding of the\nproblem and enhance the ability to find the optimal solution.",
        "A commutative monoid is atomic if every non-invertible element factors into\nirreducibles (also called atoms), while an integral (semi)domain is atomic if\nits multiplicative monoid is atomic. Notions weaker than atomicity have been\nintroduced and studied during the past decade, including almost atomicity and\nquasi-atomicity, which were coined and first investigated by Boynton and\nCoykendall in their study of graphs of divisibility of integral domains. The\nascent of atomicity to polynomial extensions was settled by Roitman back in\n1993 while the ascent of atomicity to monoid domains was settled by Coykendall\nand the second author in 2019 (in both cases the answer was negative). The main\npurpose of this paper is to study the ascent of almost atomicity and\nquasi-atomicity to polynomial extensions and monoid domains. Under certain\nreasonable conditions, we establish the ascent of both properties to polynomial\nextensions (over semidomains). Then we construct an explicit example\nillustrating that, with no extra conditions, quasi-atomicity does not ascend to\npolynomial extensions. Finally, we show that, in general, neither almost\natomicity nor quasi-atomicity ascend to monoid domains, improving upon a\nconstruction first provided by Coykendall and the second author for the\nnon-ascent of atomicity.",
        "We establish a formula for structure constants of the quantum mirror to a log\nCalabi Yau surface $(Y,D)$ in terms of descendent logarithmic Gromov--Witten\ninvariants of $(Y,D)$. Our result generalises the weak Frobenius structure\nconjecture for surfaces to the $q$-refined setting, and is proved by relating\nthese invariants to counts of quantum broken lines in the associated quantum\nscattering diagram.",
        "Interactive decision-making is essential in applications such as autonomous\ndriving, where the agent must infer the behavior of nearby human drivers while\nplanning in real-time. Traditional predict-then-act frameworks are often\ninsufficient or inefficient because accurate inference of human behavior\nrequires a continuous interaction rather than isolated prediction. To address\nthis, we propose an active learning framework in which we rigorously derive\npredicted belief distributions. Additionally, we introduce a novel model-based\ndiffusion solver tailored for online receding horizon control problems,\ndemonstrated through a complex, non-convex highway merging scenario. Our\napproach extends previous high-fidelity dual control simulations to hardware\nexperiments, which may be viewed at https:\/\/youtu.be\/Q_JdZuopGL4, and verifies\nbehavior inference in human-driven traffic scenarios, moving beyond idealized\nmodels. The results show improvements in adaptive planning under uncertainty,\nadvancing the field of interactive decision-making for real-world applications.",
        "Spin-splitting antiferromagnets with spin-polarized band structures in\nmomentum space have garnered intensive research attention due to their zero net\nmagnetic moments, ultras fast spin dynamics as conventional antiferromagnets,\nand spin-polarized transport properties akin to ferromagnets, making them\npromising candidates for antiferromagnetic spintronics. However, unlike\nspin-torque switching of ferromagnets by electric current, efficient electric\ncontrol of spin-splitting antiferromagnetic order remains challenges. In this\nwork, we identify prototypes of multiferroic spin-splitting antiferromagnets,\nincluding BiFeO3, Fe2Mo3O8 and compensated ferrimagnet GaFeO3 with\nferroelectric polarization as well as spin-polarized electronic structures. We\nestablish design principles for the spin-splitting multiferroic\nantiferromagnets and compensated ferrimagnets, elucidating the band symmetry\nfeatures in Brillouin zone. We demonstrate that the spin polarization in\nspin-splitting magnets, despite of zero net magnetic moment, can be switched by\nferroelectric polarization, providing an efficient means of controlling the\nantiferromagnetic order. Our work may inspire future development of novel\nmultiferroic functional magnets with zero magnetic moments and pave the way for\ntheir applications in magnetoelectric spintronic devices.",
        "Fairness of machine learning algorithms is receiving increasing attention, as\nsuch algorithms permeate the day-to-day aspects of our lives. One way in which\nbias can manifest in a dataset is through missing values. If data are missing,\nthese data are often assumed to be missing completely randomly; in reality the\npropensity of data being missing is often tied to the demographic\ncharacteristics of individuals. There is limited research into how missing\nvalues and the handling thereof can impact the fairness of an algorithm. Most\nresearchers either apply listwise deletion or tend to use the simpler methods\nof imputation (e.g. mean or mode) compared to the more advanced ones (e.g.\nmultiple imputation); we therefore study the impact of the simpler methods on\nthe fairness of algorithms. The starting point of the study is the mechanism of\nmissingness, leading into how the missing data are processed and finally how\nthis impacts fairness. Three popular datasets in the field of fairness are\namputed in a simulation study. The results show that under certain scenarios\nthe impact on fairness can be pronounced when the missingness mechanism is\nmissing at random. Furthermore, elementary missing data handling techniques\nlike listwise deletion and mode imputation can lead to higher fairness compared\nto more complex imputation methods like k-nearest neighbour imputation, albeit\noften at the cost of lower accuracy.",
        "Nonsense-mediated mRNA decay (NMD) is a critical post-transcriptional\nsurveillance mechanism that degrades transcripts with premature termination\ncodons, safeguarding transcriptome integrity and shaping disease phenotypes.\nHowever, accurately predicting NMD efficiency remains challenging, as existing\nmodels often rely on simplistic rule-based heuristics or limited feature sets,\nconstraining their accuracy and generalizability. Using paired DNA and RNA data\nfrom The Cancer Genome Atlas, we benchmark embedding-only models and\ndemonstrate that they underperform compared to a simple rule-based approach. To\naddress this, we develop NMDEP (NMD Efficiency Predictor), an integrative\nframework that combines optimized rule-based methods, sequence embeddings, and\ncurated biological features, achieving state-of-the-art predictive performance.\nThrough explainable AI, we identify key NMD determinants, reaffirming\nestablished factors such as variant position while uncovering novel\ncontributors like ribosome loading. Applied to over 2.9 million simulated\nstop-gain variants, NMDEP facilitates large-scale mRNA degradation assessments,\nadvancing variant interpretation and disease research.",
        "The UltraViolet EXplorer (UVEX) is a wide-field ultraviolet space telescope\nselected as a NASA Medium-Class Explorer (MIDEX) mission for launch in 2030.\nUVEX will undertake deep, cadenced surveys of the entire sky to probe low mass\ngalaxies and explore the ultraviolet (UV) time-domain sky, and it will carry\nthe first rapidly deployable UV spectroscopic capability for a broad range of\nscience applications. One of UVEX's prime objectives is to follow up\ngravitational wave (GW) binary neutron star mergers as targets of opportunity\n(ToOs), rapidly scanning across their localization regions to search for their\nkilonova (KN) counterparts. Early-time multiband ultraviolet light curves of\nKNe are key to explaining the interplay between jet and ejecta in binary\nneutron star mergers. Owing to high Galactic extinction in the ultraviolet and\nthe variation of GW distance estimates over the sky, the sensitivity to\nkilonovae can vary significantly across the GW localization and even across the\nfootprint of a single image given UVEX's large field of view. Good ToO\nobserving strategies to trade off between area and depth are neither simple nor\nobvious. We present an optimal strategy for GW follow-up with UVEX in which\nexposure time is adjusted dynamically for each field individually to maximize\nthe overall probability of detection. We model the scheduling problem using the\nexpressive and powerful mathematical framework of mixed integer linear\nprogramming (MILP), and employ a state-of-the-art MILP solver to automatically\ngenerate observing plan timelines that achieve high probabilities of kilonova\ndetection. We have implemented this strategy in an open-source astronomical\nscheduling software package called the Multi-Mission Multi-Messenger\nObservation Planning Toolkit (M4OPT), on GitHub at\nhttps:\/\/github.com\/m4opt\/m4opt.",
        "Using the expansion of Zubarev's density operator, we develop a linear\nresponse approach to study various spin physics in a locally equilibrated\nmedium, particularly focusing on various polarization phenomena in heavy-ion\ncollisions. Specifically, we connect familiar correlation functions and\ndiagrammatic methods to the Zubarev formalism, enabling the use of established\ntechniques like the Matsubara\/imaginary time formalism to facilitate\ncalculations. For a spin-1\/2 particle, we re-derive its vector polarization\nusing this Zubarev response approach, which exactly reproduces with our\nprevious results based on Luttinger's method. For a spin-1 particle, we\ncalculate the vector polarization and find the expected contributions from\nvorticity, temperature gradients, and shear, which are identical to those for\nspin-1\/2 particles except for a factor of 4\/3 as expected. For the tensor\npolarization and spin alignment of a spin-1 boson, we explicitly prove that the\nnon-dissipative contribution is zero at leading order in gradients, and briefly\nreiterate our previous findings for the dissipative contribution with further\ndiscussions on several concerns. Additionally, we discuss several relevant\nsubtleties and questions, including an alternative derivation for Zubarev\nresponse approach, the covariance issues of different spin density matrix\ndefinitions, a further explanation of slow and fast modes, the mode selection\nscheme, etc. We also discuss skeleton expansions, higher-order contributions,\nand non-perturbative methods, particularly their potential connection to\nlattice field theory. In summary, this work discusses the foundations and\nsubtleties of Zubarev response approach, with specific examples from spin\nphysics in heavy-ion collisions."
      ]
    }
  },
  {
    "id":2411.04323,
    "research_type":"applied",
    "start_id":"b2",
    "start_title":"Flow Network based Generative Models for Non-Iterative Diverse Candidate Generation",
    "start_abstract":"This paper is about the problem of learning a stochastic policy for generating an object (like molecular graph) from sequence actions, such that probability proportional to given positive reward object. Whereas standard return maximization tends converge single return-maximizing sequence, there are cases where we would like sample diverse set high-return solutions. These arise, example, in black-box function optimization when few rounds possible, each with large batches queries, should be diverse, e.g., design new molecules. One can also see this as approximately converting energy generative distribution. While MCMC methods achieve that, they expensive and generally only perform local exploration. Instead, training amortizes cost search during yields fast generation. Using insights Temporal Difference learning, propose GFlowNet, based on view process flow network, making it possible handle tricky case different trajectories yield same final state, many ways sequentially add atoms generate some graph. We cast convert consistency equations into objective, akin casting Bellman methods. prove any global minimum proposed objectives which samples desired distribution, demonstrate improved performance diversity GFlowNet simple domain modes function, molecule synthesis task.",
    "start_categories":[
      "cs.LG"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b5"
      ],
      "title":[
        "Graph Networks as a Universal Machine Learning Framework for Molecules and Crystals"
      ],
      "abstract":[
        "Graph networks are a new machine learning (ML) paradigm that supports both relational reasoning and combinatorial generalization. Here, we develop universal MatErials Network (MEGNet) models for accurate property prediction in molecules crystals. We demonstrate the MEGNet outperform prior ML such as SchNet 11 out of 13 properties QM9 molecule data set. Similarly, show trained on \u223c60 000 crystals Materials Project substantially formation energies, band gaps, elastic moduli crystals, achieving better than density functional theory accuracy over much larger present two strategies to address limitations common materials science chemistry. First, physically intuitive approach unify four separate molecular internal energy at 0 K room temperature, enthalpy, Gibbs free into single model by incorporating pressure, entropy global state inputs. Second, learned element embeddings encode periodic chemical trends can be transfer-learned from set (formation energies) improve with smaller amounts (band gaps moduli)."
      ],
      "categories":[
        "cond-mat.mtrl-sci"
      ]
    },
    "list":{
      "title":[
        "Functionalized Cr$_2$C MXenes: Novel Magnetic Semiconductors",
        "Uncertainty Quantification for Misspecified Machine Learned Interatomic\n  Potentials",
        "Ferromagnetism in LaFeO3\/LaNiO3 Superlattices with High Curie\n  Temperature",
        "Harnessing Layer-Controlled Two-dimensional Semiconductors for\n  Photoelectrochemical Energy Storage via Quantum Capacitance and Band Nesting",
        "Magnetotransport evidence of a potential low-lying Dirac node in\n  NbAl$_3$",
        "Stability of the long-range corrected exchange-correlation functional in\n  time-dependent density-functional theory",
        "Effect of Accelerated Thermal Degradation of Poly(Vinyl Chloride): The\n  Case of Unplasticized PVC",
        "Structural Modulation and Enhanced Magnetic Ordering in Incommensurate\n  K$_{1-{x}}$CrSe$_2$ Crystals",
        "Double-Crucible Vertical Bridgman Technique for Stoichiometry-Controlled\n  Chalcogenide Crystal Growth",
        "Curie temperature study of the Y(Fe$_{1-x}$Co$_x$)$_2$ and\n  Zr(Fe$_{1-x}$Co$_x$)$_2$ systems using mean-field theory and Monte Carlo\n  method",
        "Spin-reorientation driven topological Hall effect in Fe4GeTe2",
        "A full breakthrough in vacuum ultraviolet nonlinear optical performance\n  of NH4B4O6F",
        "Tuning topologically nontrivial states in the BHT-Ni metal organic\n  framework",
        "Anomalous Dynamics of a Liquid Corner Film",
        "HST Observations within the Sphere of Influence of the Powerful\n  Supermassive Black Hole in PKS0745-191",
        "Global bifurcations of nodal solutions for coupled elliptic equations",
        "A new convection scheme for GCMs of temperate sub-Neptunes",
        "Singularity of compound stationary measures",
        "The Stability and Accuracy of The Adams-Bashforth-type Integrator",
        "Characterizing Continuous Gravitational Waves from Supermassive Black\n  Hole Binaries in Realistic Pulsar Timing Array Data",
        "Electromagnetic System Conceptual Design for a Negative Triangularity\n  Tokamak",
        "Theory of quantum-geometric charge and spin Josephson diode effects in\n  strongly spin-polarized hybrid structures with noncoplanar spin textures",
        "QZO: A Catalog of 5 Million Quasars from the Zwicky Transient Facility",
        "Quantum dot-based device for high-performance magnetic microscopy and\n  spin filtering in the Kondo regime",
        "Indirect reciprocity as a dynamics for weak balance",
        "Simple games with minimum",
        "Fully viable DHOST bounce with extra scalar",
        "Thermodynamic properties and Joule-Thomson expansion of AdS black hole\n  with Gaussian distribution in non-commutative geometry"
      ],
      "abstract":[
        "We report an \\textit{ab initio} investigation of functionalized and\n3$d$-electrons doped Cr$_2$C MXenes. Upon functionalization, the Cr$_2$C\nbecomes chemically, dynamically, and mechanically stable, and it exhibits\nmagnetic semiconducting behavior. Cr$_2$CF$_2$ stands out as a wide band gap\nsemiconductor, possessing super exchange interaction mediated by F atoms within\nthe layer, however, the applied strain transforms it from an indirect to a\ndirect band gap semiconductor. Strong spin-phonon coupling found in\nCr$_2$CH$_2$ is supported by the distorted Cr spin density due to hydrogen\nenvironment. Two magnon branches, associated with two sub-lattice spins, are\nfound in the ferromagnetic Cr$_2$CO$_2$ and antiferromagnetic Cr$_2$CF$_2$.\nDepending on the types of 3$d$-electron dopants and functionalization, Cr$_2$C\nMXenes (except for Cr$_2$CO$_2$) change from the indirect band gap magnetic\nsemiconductor to different states of electronic and magnetic matter including\nexotic direct band gap magnetic bipolar semiconductor. In addition, we reveal a\nband inversion between the two highest valence bands in the Fe-doped\nCr$_2$CCl$_2$.",
        "The use of high-dimensional regression techniques from machine learning has\nsignificantly improved the quantitative accuracy of interatomic potentials.\nAtomic simulations can now plausibly target quantitative predictions in a\nvariety of settings, which has brought renewed interest in robust means to\nquantify uncertainties on simulation results. In many practical settings,\nencompassing both classical and a large class of machine learning potentials,\nthe dominant form of uncertainty is currently not due to lack of training data\nbut to misspecification, namely the inability of any one choice of model\nparameters to exactly match all ab initio training data. However, Bayesian\ninference, the most common formal tool used to quantify uncertainty, is known\nto ignore misspecification and thus significantly underestimates parameter\nuncertainties. Here, we employ a recent misspecification-aware regression\ntechnique to quantify parameter uncertainties, which is then propagated to a\nbroad range of phase and defect properties in tungsten via brute force\nresampling or implicit differentiation. The propagated misspecification\nuncertainties robustly envelope errors to direct \\textit{ab initio} calculation\nof material properties outside of the training dataset, an essential\nrequirement for any quantitative multi-scale modeling scheme. Finally, we\ndemonstrate application to recent foundational machine learning interatomic\npotentials, accurately predicting and bounding errors in MACE-MPA-0 energy\npredictions across the diverse materials project database. Perspectives for the\napproach in multiscale simulation workflows are discussed.",
        "Interfacing complex oxides in atomically engineered layered structures can\ngive rise to a wealth of exceptional electronic and magnetic properties that\nsurpass those of the individual building blocks. Herein, we demonstrate a\nferromagnetic spin order with a high Curie temperature of 608 K in\nsuperlattices consisting of otherwise paramagnetic perovskite LaNiO3 (LNO) and\nantiferromagnetic LaFeO3 (LFO). The extraordinary ferromagnetism likely results\nfrom the covalent exchange due to interfacial charge transfer from Fe to Ni\ncations. By deliberately controlling the thickness of the LNO sublayers thus\nthe amount of charge transfer, a robust ferromagnetism of 4 uB is realized for\na stacking periodicity consisting of one single unit cell of both LNO and LFO,\nan emergent double perovskite phase of La2FeNiO6 with B-site layered ordering\nconfigurations. The ferromagnetic LFO\/LNO superlattices offer great potential\nfor the search of emergent magnetodielectric and\/or multiferroic properties as\nwell as applications in spintronics and electrocatalysts.",
        "Two-dimensional (2D) transition metal dichalcogenides like molybdenum\ndiselenide (MoSe$_2$) have shown great potential in optoelectronics and energy\nstorage due to their layer-dependent bandgap. However, producing high-quality\n2D MoSe$_2$ layers in a scalable and controlled manner remains challenging.\nTraditional methods, such as hydrothermal and liquid-phase exfoliation, lack\nprecision and understanding at the nanoscale, limiting further applications.\nAtmospheric pressure chemical vapor deposition (APCVD) offers a scalable\nsolution for growing high-quality, large-area, layer-controlled 2D MoSe$_2$.\nDespite this, the photoelectrochemical performance of APCVD-grown 2D MoSe$_2$,\nparticularly in energy storage, has not been extensively explored. This study\naddresses this by examining MoSe$_2$'s layer-dependent quantum capacitance and\nphoto-induced charge storage properties. Using a three-electrode setup in 0.5M\nH$_2$SO$_4$, we observed a layer-dependent increase in areal capacitance under\nboth dark and illuminated conditions. A six-layer MoSe$_2$ film exhibited the\nhighest capacitance, reaching $96 \\mu\\mathrm{F\/cm^2}$ in the dark and $115\n\\mu\\mathrm{F\/cm^2}$ under illumination at a current density of $5\n\\mu\\mathrm{A\/cm^2}$. Density Functional Theory (DFT) and Many-Body Perturbation\nTheory calculations reveal that Van Hove singularities and band nesting\nsignificantly enhance optical absorption and quantum capacitance. These results\nhighlight APCVD-grown 2D MoSe$_2$'s potential as light-responsive,\nhigh-performance energy storage electrodes, paving the way for innovative\nenergy storage systems.",
        "NbAl$_3$ is a novel semimetal with a type-II Dirac node ~230 meV above the\nFermi energy. We have performed both out-of-plane ($B\\parallel c$) and in-plane\nmagnetotransport measurements ($B\\perp c$) on single-crystalline NbAl$_3$. In\nour out-of-plane data, we observe an interesting linear component in the\ntransverse magnetoresistance, and the mobility spectrum analysis of the\nout-of-plane data reveals an emergence of high-mobility electrons at low\ntemperatures. Near $B\\parallel c$, Shubnikov-de Haas oscillations are discerned\nin the magnetoresistance. The oscillation frequencies agree with the density\nfunctional theory calculation, the same theory that shows that the Dirac node\nis far above the Fermi energy. Therefore, the out-of-plane results cannot be\nattributed to the type-II Dirac node but suggest NbAl$_3$ has additional Dirac\nor Weyl nodes close to the Fermi energy. To support this, we examine the\nin-plane data obtained with the magnetic field perpendicular to the tilting\ndirection of the type-II Dirac cone. Such field direction excludes the\npossibility of chiral anomaly from the predicted type-II Dirac node.\nRemarkably, we observe the planar Hall effect, anisotropic magnetoresistance,\nand negative longitudinal magnetoresistance. These in-plane results are a\nstrong indication of chiral anomaly unrelated to the previously established\ntype-II Dirac node, pointing to the presence of additional Dirac or Weyl nodes\nnear the Fermi energy. Our new density functional theory calculation reveals a\ntype-I Dirac node ~50 meV below the Fermi energy that has previously been\noverlooked. We argue that the exotic transport phenomena observed in NbAl$_3$\ncan be attributed to the newly identified type-I Dirac node.",
        "Excitonic effects in the optical absorption spectra of solids can be\ndescribed with time-dependent density-functional theory (TDDFT) in the\nlinear-response regime, using a simple class of approximate, long-range\ncorrected (LRC) exchange-correlation functionals. It was recently demonstrated\nthat the LRC approximation can also be employed in real-time TDDFT to describe\nexciton dynamics. Here, we investigate the numerical stability of the\ntime-dependent LRC approach using a two-dimensional model solid. It is found\nthat the time-dependent Kohn-Sham equation with an LRC vector potential becomes\nmore and more prone to instabilities for increasing exciton binding energies.\nThe origin of these instabilities is traced back to time-averaged violations of\nthe zero-force theorem, which leads to a simple and robust numerical\nstabilization scheme. This explains and justifies a recently proposed method by\nDewhurst et al., arXiv:2401.16140.",
        "The thermal degradation of unplasticized poly(vinyl chloride), PVC, was\ncomprehensively investigated through the application of spectroscopic\ntechniques, as well as contact angle measurements (CA), dynamic mechanical\nanalysis (DMA), and size-exclusion chromatography (SEC). To study the effect of\nrelative humidity (RH) on the deterioration of unplasticized PVC, two regimes\nof accelerated degradation experiments were selected: low RH (max. 30% RH) and\nhigh RH = 60% levels, which corresponds to usually the highest RH in heritage\ninstitutions equipped with an HVAC system. Nuclear magnetic resonance (NMR) and\ninfrared spectroscopy (FTIR) did not reveal any significant changes in the\nmaterial during its degradation up to 20 weeks at temperatures ranging from\n60{\\deg}C to 80{\\deg}C. Notable changes were observed in the Raman and UV-Vis\nspectra, indicative of the formation of conjugated carbon-carbon double bonds.\nThe formation of polyenes was responsible for the yellowing of samples.\nNotwithstanding, the aforementioned changes did not lead to a notable decline\nin the mechanical properties, as evidenced by DMA and SEC measurements. EPR\nmeasurements demonstrated the formation of 2 radicals at 60{\\deg}C, and in the\nsample degraded at 80{\\deg}C the presence of radicals was evident. This\nindicates that a radical degradation mechanism cannot be excluded even at such\nlow temperatures.",
        "Layered delafossite-type compounds and related transition metal\ndichalcogenides, characterized by their triangular net structures, serve as\nprototypical systems for exploring the intricate interplay between crystal\nstructure and magnetic behavior. Herein, we report on the discovery of the\ncompound K$_{1-x}$CrSe$_2$ ($x \\approx$ 0.13), an incommensurately modulated\nphase. Single crystals of this compound were grown for the first time using a\nK\/Se self-flux. We find a monoclinic crystal structure with incommensurate\nmodulation, that can be rationalized by a 3+1 dimensional model. This\nmodulation compensates for the under-stoichiometry of K cations, creating\npronounced undulations in the CrSe$_2$ layers. Our anisotropic magnetization\nmeasurements reveal that K$_{1-x}$CrSe$_2$ undergoes a transition to a\nlong-range magnetically ordered state below $T_{\\mathrm N}$ = 133 K, a\ntemperature 1.6 to 3.3 times higher than in earlier reported KCrSe$_2$\ncompounds. Our findings open new avenues for tuning the magnetic properties of\nthese layered materials through structural modulation.",
        "Precise stoichiometry control in single-crystal growth is essential for both\ntechnological applications and fundamental research. However, conventional\ngrowth methods often face challenges such as non-stoichiometry, compositional\ngradients, and phase impurities, particularly in non-congruent melting systems.\nEven in congruent melting systems like Bi2Se3, deviations from the ideal\nstoichiometric composition can lead to significant property degradation, such\nas excessive bulk conductivity, which limits its topological applications. In\nthis study, we introduce the double-crucible vertical Bridgman (DCVB) method, a\nnovel approach that enhances stoichiometry control through the combined use of\ncontinuous source material feeding, traveling-solvent growth, and liquid\nencapsulation, which suppresses volatile element loss under high pressure.\nUsing Bi2Se3 as a model system, we demonstrate that crystals grown via DCVB\nexhibit enhanced stoichiometric control, significantly reducing defect density\nand achieving much lower carrier concentrations compared to those produced by\nconventional Bridgman techniques. Moreover, the continuous feeding of source\nmaterial enables the growth of large crystals. This approach presents a\npromising strategy for synthesizing high-quality, large-scale crystals,\nparticularly for metal chalcogenides and pnictides that exhibit challenging\nnon-congruent melting behaviors.",
        "The cubic Laves phases including YFe$_2$, YCo$_2$, ZrFe$_2$, and ZrCo$_2$ are\nconsidered as promising candidates for application in hydrogen storage and\nmagnetic refrigeration. While YFe$_2$ and ZrFe$_2$ are ferromagnets, alloying\nwith Co decreases magnetic moments and Curie temperatures ($T_\\mathrm{C}$) of\npseudobinary Zr(Fe$_{1-x}$Co$_x$)$_2$ and Y(Fe$_{1-x}$Co$_x$)$_2$ systems,\nleading to the paramagnetic states of YCo$_2$ and ZrCo$_2$. The following study\nfocus on the investigation of Curie temperature of the Y(Fe$_{1-x}$Co$_x$)$_2$\nand Zr(Fe$_{1-x}$Co$_x$)$_2$ system from first principles. To do it, the Monte\nCarlo (MC) simulations and the mean field theory (MFT) based on the disordered\nlocal moments (DLM) calculations are used. The DLM-MFT results agree\nqualitatively with the experiment and preserve the characteristic features of\n$T_\\mathrm{C}(x)$ dependencies for both Y(Fe$_{1-x}$Co$_x$)$_2$ and\nZr(Fe$_{1-x}$Co$_x$)$_2$. However, we have encountered complications in the\nCo-rich regions due to failure of the local density approximation (LDA) in\ndescribing the Co magnetic moment in the DLM state. The analysis of Fe-Fe\nexchange couplings for YFe$_2$ and ZrFe$_2$ phases indicates that the\nnearest-neighbor interactions play the main role in the formation of\n$T_{\\mathrm{C}}$.",
        "Iron-based van der Waals (vdW) ferromagnets with relatively high ordering\ntemperatures are a current research focus due to their significance in\nfundamental physics and potential applications in spintronics. Competing\nmagnetic interactions and anisotropies can give rise to nontrivial spin\ntextures in these materials, resulting in novel topological features. Fe4GeTe2\n(F4GT) is a nearly room-temperature vdW ferromagnet, well known for hosting a\nspin-reorientation transition (SRT) arising from the interplay of perpendicular\nmagnetic anisotropy (PMA) and shape anisotropy. In this work, we investigate\nthe angle-dependent magneto-transport properties of F4GT single crystals. We\nreport a large topological Hall effect (THE) in a multi-layer F4GT originating\nfrom the SRT-driven non-coplanar spin textures. The THE appears at the in-plane\norientation of the external magnetic field and persists over a wide range of\ntemperatures around SRT. Additionally, we find a thickness-sensitive THE signal\nfor the c axis orientation of the magnetic field at a low-temperature regime\nwhich is associated with a reentrant Lifshitz transition.",
        "The lack of suitable vacuum ultraviolet (VUV) nonlinear optical (NLO)\ncrystals has hindered the development of compact, high-power VUV sources via\nsecond harmonic generation (SHG). Here, we report on the development of the\nfluorooxoborate crystal NH4B4O6F (ABF) as a promising material for VUV light\ngeneration. For the first time, devices with specific phase-matching angles\nwere constructed, achieving a record 158.9 nm VUV light through phase-matching\nSHG and a maximum nanosecond pulse energy of 4.8 mJ at 177.3 nm with a\nconversion efficiency of 5.9 %. The enhanced NLO performance is attributed to\noptimized arrangements of fluorine-based units creating asymmetric sublattices.\nThis work marks a significant milestone in the field of NLO materials,\nfacilitating the future applications of compact, high-power VUV lasers\nutilizing ABF.",
        "Using first principles calculations, we have demonstrated the creation of\nmultiple quantum states, in the experimentally accessible metal organic\nframework BHT-Ni. Specifically, quantum spin Hall and quantum anomalous Hall\nstates are induced by two and four electron doping, respectively. The\ngeometrical symmetry breaking, is also investigated. For a low electron doping\nconcentration of two electrons per unit cell, the Fermi energy shifts to a\nnontrivial band gap, between Dirac bands and a quantized spin Hall conductivity\nis predicted. Subsequently in a high electron doping concentration, Anomalous\nHall conductivity with a quantized value was observed. In addition, for\ncentrosymmetric (trans-like) and non-centrosymmetric (cis-like) structures, we\nfound that the trans-like structure preserves quantum spin Hall and quantized\nspin Hall conductivity. In contrast, in the cis-like structure, space inversion\nsymmetry breaking leads to the appearance of valley Hall effect and the\ndisappearance of spin Hall conductivity.",
        "Measuring the rheology of liquids typically requires precise control over\nshear rates and stresses. However, we demonstrate that the features of a\npower-law fluid can be predicted by simply observing the capillary spreading\ndynamics of viscous droplets within a wedge-shaped geometry. By considering the\ninfluence of capillary and viscous forces within this geometry, we show that\nthe spreading dynamics can be described by a nonlinear diffusion equation.\nAnalytical predictions indicate subdiffusive behavior, establishing a direct\nrelationship between the diffusion exponent and the rheological exponent, which\nis also corroborated by experimental results. Since this relationship is\nindependent of flow details, it provides robust predictions for the rheological\nproperties of power-law fluids.",
        "We present Space Telescope Imaging Spectrograph observations from the Hubble\nSpace Telescope of the supermassive black hole (SMBH) at the center of\nPKS0745-191, a brightest cluster galaxy (BCG) undergoing powerful radio-mode\nAGN feedback ($P_{\\rm cav}\\sim5\\times10^{45}$ erg s$^{-1}$). These\nhigh-resolution data offer the first spatially resolved map of gas dynamics\nwithin a SMBHs sphere of influence under such powerful feedback. Our results\nreveal the presence of highly chaotic, non-rotational ionized gas flows on\nsub-kpc scales, in contrast to the more coherent flows observed on larger\nscales. While radio-mode feedback effectively thermalizes hot gas in galaxy\nclusters on kiloparsec scales, within the core, the hot gas flow may decouple,\nleading to a reduction in angular momentum and supplying ionized gas through\ncooling, which could enhance accretion onto the SMBH. This process could, in\nturn, lead to a self-regulating feedback loop. Compared to other BCGs with\nweaker radio-mode feedback, where rotation is more stable, intense feedback may\nlead to more chaotic flows, indicating a stronger coupling between jet activity\nand gas dynamics. Additionally, we observe a sharp increase in velocity\ndispersion near the nucleus, consistent with a very massive $M_{\\rm\nBH}\\sim1.5\\times10^{10} M_\\odot$ SMBH. The density profile of the ionized gas\nis also notably flat, paralleling the profiles observed in X-ray gas around\ngalaxies where the Bondi radius is resolved. These results provide valuable\ninsights into the complex mechanisms driving galaxy evolution, highlighting the\nintricate relationship between SMBH fueling and AGN feedback within the host\ngalaxy.",
        "We investigate the global bifurcation structure of the radial nodal solutions\nto the coupled elliptic equations \\begin{equation}\n  \\left\\{\n  \\begin{array}{lr}\n  -{\\Delta}u+u=u^3+\\beta uv^2\\mbox{ in }B_1 ,\\nonumber\n  -{\\Delta}v+v=v^3+\\beta u^2v\\mbox{ in }B_1 ,\\nonumber\n  u,v\\in H_{0,r}^1(B_1).\\nonumber\n  \\end{array}\n  \\right. \\end{equation} Here $B_1$ is a unit ball in $\\mathbb{R}^3$ and\n$\\beta\\in\\mathbb{R}$ the coupling constant is used as bifurcation parameter.\nFor each $k$, the unique pair of nodal solutions $\\pm w_k$ with exactly $k-1$\nzeroes to the scalar field equation $-\\Delta w + w=w^3$ generate exactly four\nsynchronized solution curves and exactly four semi-trivial solution curves to\nthe above system. We obtain a fairly complete global bifurcation structure of\nall bifurcating branches emanating from these eight solution curves of the\nsystem, and show that for different $k$ these bifurcation structures are\ndisjoint. We obtain exact and distinct nodal information for each of the\nbifurcating branches, thus providing a fairly complete characterization of\nnodal solutions of the system in terms of the coupling.",
        "Atmospheric characterisation of temperate sub-Neptunes is the new frontier of\nexoplanetary science with recent JWST observations of possible Hycean world\nK2-18b. Accurate modelling of atmospheric processes is essential to\ninterpreting high-precision spectroscopic data given the wide range of possible\nconditions in the sub-Neptune regime, including on potentially habitable\nplanets. Notably, convection is an important process which can operate in\ndifferent modes across sub-Neptune conditions. Convection can act very\ndifferently in atmospheres with a high condensible mass fraction (non-dilute\natmospheres) or with a lighter background gas, e.g. water convection in a\nH$_2$-rich atmosphere, and can be much weaker or even shut down entirely in the\nlatter case. We present a new mass-flux scheme which can capture these\nvariations and simulate convection over a wide range of parameter space for use\nin 3D general circulation models (GCMs). We validate our scheme for two\nrepresentative cases, a terrestrial-like atmosphere and a mini-Neptune\natmosphere. In the terrestrial case, considering TRAPPIST-1e with an Earth-like\natmosphere, the model performs near-identically to Earth-tuned models in an\nEarth-like convection case. In the mini-Neptune case, considering the bulk\nproperties of K2-18b and assuming a deep H$_2$-rich atmosphere, we demonstrate\nthe capability of the scheme to reproduce non-condensing convection. We find\nconvection occurring at pressures greater than 0.3 bar and the dynamical\nstructure shows high-latitude prograde jets. Our convection scheme will aid in\nthe 3D climate modelling of a wide range of exoplanet atmospheres, and enable\nfurther exploration of temperate sub-Neptune atmospheres.",
        "We show that the product or convex combination of two Markov operators with\nequivalent stationary measures need not have a stationary measure from the same\nmeasure class. More specifically, we exhibit examples of a hitherto undescribed\nphenomenon: maximal entropy random walks for which the resulting compound\nrandom walks no longer have maximal entropy. The underlying group in these\nexamples is $PSL(2,\\mathbb Z)\\cong{{\\mathbb Z}_2}*{{\\mathbb Z}_3}$, and the\nassociated harmonic measures belong to the canonical Minkowski and Denjoy\nmeasure classes on the boundary. These examples also demonstrate that a number\nof other natural families of random walks are not closed under convolutions or\nconvex combinations of step distributions.",
        "This paper presents stability and accuracy analysis of a high-order explicit\ntime stepping scheme introduced by \\cite[Section 2.2]{Buvoli2019}, which\nexhibits superior stability compared to classical Adams-Bashforth. A conjecture\nthat is supported by several numerical phenomena in \\cite[Figure\n2.5]{Buvoli2018}, the method appears to remain stable when the accuracy\napproaches infinity, although it is not yet proven. It is regrettable that this\nhypothesis has been refuted from a fundamental perspective in harmonic\nanalysis. Notwithstanding the aforementioned, this method displays considerably\nenhanced stability in comparison to conventional explicit schemes. Furthermore,\nwe present a criterion for ascertaining the maximum permissible accuracy for a\ngiven specific parabolic stability radius. Conversely, the original method will\nlose one order associated with the expected accuracy, which can be recovered\nwith a slight modification. Consequently, a unified analysis strategy for the\n\\( L^2 \\)-stability will be presented for extensional PDEs under the CFL\ncondition. Finally, a selection of representative numerical examples will be\nshown in order to substantiate the theoretical analysis.",
        "Pulsar timing arrays recently found evidence for a gravitational wave\nbackground (GWB), likely the stochastic overlap of GWs from many supermassive\nblack hole binaries. Anticipating a continuous gravitational wave (CW)\ndetection from a single binary soon to follow, we examine how well current\nBayesian methods can detect CWs and characterize their binary properties by\nmodeling the response of the NANOGrav 15-year pulsar timing array to simulated\nbinary populations. We run Markov Chain Monte Carlo searches for CWs in these\ndatasets and compare them to quicker detection statistics including the optimal\nsignal-to-noise ratio, matched filter detection statistic, and reduced\nlog-likelihood ratio between the signal and noise models calculated at the\ninjected parameters. The latter is the best proxy for Bayesian detection\nfractions, corresponding to a 50% detection fraction (by Bayes factors >10\nfavoring a CW detection over noise-only model) at a signal-to-noise ratio of\n4.6. Source confusion between the GWB and a CW, or between multiple CWs, can\ncause false detections and unexpected dismissals. 53% of realistic binary\npopulations consistent with the recently observed GWB have successful CW\ndetections. 82% of these CWs are in the 4th or 5th frequency bin of the 16.03\nyr dataset (6.9 nHz and 10.8 nHz), with 95 percentile regions spanning\n4nHz-12nHz frequencies, $7-20\\times10^9 M_\\odot$ chirp masses, 60Mpc-8Gpc\nluminosity distances, and 18-13,000 sq. deg 68% confidence localization areas.\nThese successful detections often poorly recover the chirp mass, with only 29%\nidentifying the chirp mass accurately to within 1 dex with a 68% posterior\nwidth also narrower than 1 dex.",
        "Negative triangularity (NT) tokamak configurations have several key benefits\nincluding sufficient core confinement, improved power handling, and reduced\nedge pressure gradients that allow for edge-localized mode (ELM) free\noperation. We present the design of a compact NT device for testing\nsophisticated simulation and control software, with the aim of demonstrating NT\ncontrollability and informing power plant operation. The TokaMaker code is used\nto develop the basic electromagnetic system of the $R_0$ = 1 m, $a$ = 0.27 m,\n$B_t$ = 3 T, $I_p$ = 0.75 MA tokamak. The proposed design utilizes eight\npoloidal field coils with maximum currents of 1 MA to achieve a wide range of\nplasma geometries with $-0.7 < \\delta < -0.3$ and $1.5 < \\kappa < 1.9$.\nScenarios with strong negative triangularity and high elongation are\nparticularly susceptible to vertical instability, necessitating the inclusion\nof high-field side and\/or low-field side passive stabilizing plates which\ntogether reduce vertical instability growth rates by $\\approx$75%. Upper limits\nfor the forces on poloidal and toroidal field coils are predicted and\nmechanical loads on passive structures during current quench events are\nassessed. The 3 T on-axis toroidal field is achieved with 16 demountable copper\ntoroidal field coils, allowing for easy maintenance of the vacuum vessel and\npoloidal field coils. This pre-conceptual design study demonstrates that the\nkey capabilities required of a dedicated NT tokamak experiment can be realized\nwith existing copper magnet technologies.",
        "We present a systematic study of the spin-resolved Josephson diode effect\n(JDE) in strongly spin-polarized ferromagnets (sFM) coupled to singlet\nsuperconductors (SC) via ferromagnetic insulating interfaces (FI). All metallic\nparts are described in the framework of the quasiclassical Usadel Green's\nfunction theory applicable to diffusive systems. The interfaces are\ncharacterized by an S-matrix obtained for a model potential with exchange\nvectors pointing in an arbitrary direction with respect to the magnetization in\nthe sFM. Our theory predicts a large charge Josephson diode effect with an\nefficiency exceeding $33\\%$ and a perfect spin diode effect with $100\\%$\nefficiency. To achieve these the following conditions are necessary: (i) a\nnoncoplanar profile of the three magnetization vectors in the system and (ii)\ndifferent densities of states of spin-$\\uparrow$ and spin-$\\downarrow$ bands in\nthe sFM achieved by a strong spin polarization. The former gives rise to the\nquantum-geometric phase, $\\Delta\\varphi$, that enters the theory in a very\nsimilar manner as the superconducting phase difference across the junction,\n$\\Delta\\chi$. We perform a harmonic analysis of the Josephson current in both\nvariables and find symmetries between Fourier coefficients allowing an\ninterpretation in terms of transfer processes of multiple equal-spin Cooper\npairs across the two ferromagnetic spin bands. We point out the importance of\ncrossed pair transmission processes. Finally, we study a spin-switching effect\nof an equal-spin supercurrent by reversing the magnetic flux in a SQUID device\nincorporating the mentioned junction and propose a way for measuring it.",
        "Machine learning methods are well established in the classification of\nquasars (QSOs). However, the advent of light curve observations adds a great\namount of complexity to the problem. Our goal is to use the Zwicky Transient\nFacility (ZTF) to create a catalog of QSOs. We process the ZTF DR20 light\ncurves with a transformer artificial neural network and combine the Pan-STARRS\n(PS), AllWISE, and Gaia surveys with extreme gradient boosting. Using ZTF\ng-band data with at least 100 observational epochs per light curve, we obtain\n97% F1 score for QSOs. We find that with 3 day median cadence, a survey time\nspan of at least 900 days is required to achieve 90% QSO F1 score. However, one\ncan obtain the same score with a survey time span of 1800 days and the median\ncadence prolonged to 12 days. We find that ZTF classification is superior to\nthe PS static bands, and on par with WISE and Gaia measurements. Additionally,\nwe find that the light curves provide the most important features for QSO\nclassification in the ZTF dataset. We robustly classify objects fainter than\nthe $5\\sigma$ SNR limit at $g=20.8$ by requiring $g < \\mathrm{n_{obs}} \/ 80 +\n20.375$. For this sample, we run inference with added WISE observations, and\nfind 4,849,574 objects classified as QSOs. For 33% of QZO objects, with\navailable WISE data, we publish redshifts with estimated error $\\Delta z\/(1 +\nz) = 0.14$.",
        "We propose a nanoscale device consisting of a double quantum dot with a full\nexchange and pair hopping interaction. In this design, the current can only\nflow through the upper dot, but is sensitive to the spin state of the lower\ndot. The system is immersed in a highly inhomogeneous magnetic field, and only\nthe bottom dot feels a substantial magnetic field, while the top dot\nexperiences only a residual one.\n  We show that our device exhibits very interesting magnetic field-dependent\ntransport properties at low temperatures. The Kondo effect partially survives\nthe presence of the magnetic field and allows to obtain conductances that\ndiffer by several orders of magnitude for the two spin types across the top\ndot.\n  Interestingly, as a function of the magnetic field, our two-dot device\nchanges from a spin singlet state to a spin triplet state, in which the\namplitudes of the spin-dependent conductances are reversed.\n  Our device is able to discriminate between positive and negative magnetic\nfields with a high sensitivity and is therefore particularly interesting for\nimaging the surface of anti-ferromagnetic (AF) insulating materials with\nalternated surface magnetic field, as well as for spin filtering applications.",
        "A social network is often divided into many factions. People are friends\nwithin each faction, while they are enemies of the other factions, and even my\nenemy's enemy is not necessarily my friend. This configuration can be described\nin terms of a weak form of structural balance. Although weak balance explains a\nnumber of real social networks, which dynamical rule achieves it has remained\nunexplored. In this work, we show that the answer can be found in the field of\nindirect reciprocity, which assumes that people assess each other's behavior\nand choose how to behave to others based on the assessment according to a\nsocial norm. We begin by showing that weak structural balance is equivalent to\nstationarity when the rule is given by a norm called `judging'. By analyzing\nits cluster dynamics of merging, division, and migration induced by assessment\nerror in complete graphs, we obtain the cluster size distribution in a steady\nstate, which shows the coexistence of a giant cluster and smaller ones. We\ncompare this shape with the distributions of seats among the parties in the\nparliaments of Germany, the United Kingdom, and Spain. This study suggests that\nindirect reciprocity can provide insight into the interplay between a norm that\nindividuals abide by and the macroscopic group structure in society.",
        "Every simple game is a monotone Boolean function. For the other direction we\njust have to exclude the two constant functions. The enumeration of monotone\nBoolean functions with distinguishable variables is also known as the\nDedekind's problem. The corresponding number for nine variables was determined\njust recently by two disjoint research groups. Considering permutations of the\nvariables as symmetries we can also speak about non-equivalent monotone Boolean\nfunctions (or simple games). Here we consider simple games with minimum, i.e.,\nsimple games with a unique minimal winning vector. A closed formula for the\nnumber of such games is found as well as its dimension in terms of the number\nof players and equivalence classes of players.",
        "In this paper we construct a class of Degenerate Higher-Order Scalar-Tensor\n(DHOST) theories with an extra scalar field, which admits viable solutions of\nbouncing universe satisfying the following requirements: (i) absence of\nBelinski-Khalatnikov-Lifshitz (BKL) instability, ghost and gradient\ninstability, (ii) absence of superluminality, (iii) generation of nearly\nscale-invariant curvature perturbations and very small tensor-to-scalar ratio,\nand (iv) conventional asymptotics in the distant past and future, where gravity\nsector is described by General Relativity and the DHOST scalar has a canonical\nform of Lagrangian. We also expect our models to have sufficiently small\nnon-Gaussianities of primordial curvature perturbations to be compatible with\nobservations. As such, this work exemplifies for the first time the fully\nviable two-field DHOST bouncing cosmology, which is free of instability and\nsuperluminality problems as well as compatible with observations.",
        "The thermodynamics and Joule-Thomson expansion of anti-de Sitter black hole\n(AdS BH) with Gaussian distribution in non-commutative geometry is\nsystematically studied. The metric of Gaussian-distributed BH is obtained,\nshowing a dS geometry at the core of BH. The research indicates that the BH\ncharacterized by a Gaussian distribution exhibit thermodynamic properties that\nare remarkably similar to those of BH with a Lorentzian distribution in\nnon-commutative geometry. This similarity is specifically manifested in the\nsmall BH-large BH phase transition, the corrected first law of thermodynamics,\nthe criticality, the heat capacity, the zeroth-order phase transition and the\nJoule-Thomson process. Notably, the critical ratio of Gaussian-distributed BH\n(0.46531) is significantly larger than those observed in Van der Waals fluids\n(0.375), and indeed, it is also substantially exceed those of\nLorentzian-distributed BH (0.36671). Moreover, compared to the case of\nLorentzian source, the zeroth-order phase transition effect in\nGaussian-distributed BH is exceedingly subtle (accompanied by a relative\nincrease in the Gibbs free energy on the order of $10^{-3}\\!\\sim\\!\\!10^{-2}$)\nand is difficult to detect distinctly."
      ]
    }
  },
  {
    "id":2411.04323,
    "research_type":"applied",
    "start_id":"b5",
    "start_title":"Graph Networks as a Universal Machine Learning Framework for Molecules and Crystals",
    "start_abstract":"Graph networks are a new machine learning (ML) paradigm that supports both relational reasoning and combinatorial generalization. Here, we develop universal MatErials Network (MEGNet) models for accurate property prediction in molecules crystals. We demonstrate the MEGNet outperform prior ML such as SchNet 11 out of 13 properties QM9 molecule data set. Similarly, show trained on \u223c60 000 crystals Materials Project substantially formation energies, band gaps, elastic moduli crystals, achieving better than density functional theory accuracy over much larger present two strategies to address limitations common materials science chemistry. First, physically intuitive approach unify four separate molecular internal energy at 0 K room temperature, enthalpy, Gibbs free into single model by incorporating pressure, entropy global state inputs. Second, learned element embeddings encode periodic chemical trends can be transfer-learned from set (formation energies) improve with smaller amounts (band gaps moduli).",
    "start_categories":[
      "cond-mat.mtrl-sci"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b2"
      ],
      "title":[
        "Flow Network based Generative Models for Non-Iterative Diverse Candidate Generation"
      ],
      "abstract":[
        "This paper is about the problem of learning a stochastic policy for generating an object (like molecular graph) from sequence actions, such that probability proportional to given positive reward object. Whereas standard return maximization tends converge single return-maximizing sequence, there are cases where we would like sample diverse set high-return solutions. These arise, example, in black-box function optimization when few rounds possible, each with large batches queries, should be diverse, e.g., design new molecules. One can also see this as approximately converting energy generative distribution. While MCMC methods achieve that, they expensive and generally only perform local exploration. Instead, training amortizes cost search during yields fast generation. Using insights Temporal Difference learning, propose GFlowNet, based on view process flow network, making it possible handle tricky case different trajectories yield same final state, many ways sequentially add atoms generate some graph. We cast convert consistency equations into objective, akin casting Bellman methods. prove any global minimum proposed objectives which samples desired distribution, demonstrate improved performance diversity GFlowNet simple domain modes function, molecule synthesis task."
      ],
      "categories":[
        "cs.LG"
      ]
    },
    "list":{
      "title":[
        "Models That Are Interpretable But Not Transparent",
        "Dissecting the Impact of Model Misspecification in Data-driven\n  Optimization",
        "ScalingNoise: Scaling Inference-Time Search for Generating Infinite\n  Videos",
        "Lifelong Learning with Task-Specific Adaptation: Addressing the\n  Stability-Plasticity Dilemma",
        "SeisMoLLM: Advancing Seismic Monitoring via Cross-modal Transfer with\n  Pre-trained Large Language Model",
        "Saliency Maps are Ambiguous: Analysis of Logical Relations on First and\n  Second Order Attributions",
        "General Time-series Model for Universal Knowledge Representation of\n  Multivariate Time-Series data",
        "Learning from Reward-Free Offline Data: A Case for Planning with Latent\n  Dynamics Models",
        "The Relationship Between Head Injury and Alzheimer's Disease: A Causal\n  Analysis with Bayesian Networks",
        "Fixed-sized clusters $k$-Means",
        "Robust and Efficient Writer-Independent IMU-Based Handwriting\n  Recognization",
        "Preconditioned Inexact Stochastic ADMM for Deep Model",
        "Evaluating Inter-Column Logical Relationships in Synthetic Tabular Data\n  Generation",
        "Bridging Classical and Modern Approaches to Thales' Theorem",
        "S2TX: Cross-Attention Multi-Scale State-Space Transformer for Time\n  Series Forecasting",
        "Deep End-to-End Posterior ENergy (DEEPEN) for image recovery",
        "Family-wise Error Rate Control with E-values",
        "GRNFormer: A Biologically-Guided Framework for Integrating Gene\n  Regulatory Networks into RNA Foundation Models",
        "Fast-Locking and High-Resolution Mixed-Mode DLL with Binary Search and\n  Clock Failure Detection for Wide Frequency Ranges in 3-nm FinFET CMOS",
        "PFSD: A Multi-Modal Pedestrian-Focus Scene Dataset for Rich Tasks in\n  Semi-Structured Environments",
        "The H\\\"{o}lder regularity of div-curl system with anisotropic\n  coefficients",
        "A Semi-Orthogonal Decomposition Theorem for Weighted Blowups",
        "Perceived Confidence Scoring for Data Annotation with Zero-Shot LLMs",
        "Randomized block-Krylov subspace methods for low-rank approximation of\n  matrix functions",
        "Algorithmic Data Minimization for Machine Learning over\n  Internet-of-Things Data Streams",
        "Graph-Dependent Regret Bounds in Multi-Armed Bandits with Interference",
        "Medial Axis in Pseudo-Euclidean Spaces",
        "Unified Uncertainty-Aware Diffusion for Multi-Agent Trajectory Modeling"
      ],
      "abstract":[
        "Faithful explanations are essential for machine learning models in\nhigh-stakes applications. Inherently interpretable models are well-suited for\nthese applications because they naturally provide faithful explanations by\nrevealing their decision logic. However, model designers often need to keep\nthese models proprietary to maintain their value. This creates a tension: we\nneed models that are interpretable--allowing human decision-makers to\nunderstand and justify predictions, but not transparent, so that the model's\ndecision boundary is not easily replicated by attackers. Shielding the model's\ndecision boundary is particularly challenging alongside the requirement of\ncompletely faithful explanations, since such explanations reveal the true logic\nof the model for an entire subspace around each query point. This work provides\nan approach, FaithfulDefense, that creates model explanations for logical\nmodels that are completely faithful, yet reveal as little as possible about the\ndecision boundary. FaithfulDefense is based on a maximum set cover formulation,\nand we provide multiple formulations for it, taking advantage of submodularity.",
        "Data-driven optimization aims to translate a machine learning model into\ndecision-making by optimizing decisions on estimated costs. Such a pipeline can\nbe conducted by fitting a distributional model which is then plugged into the\ntarget optimization problem. While this fitting can utilize traditional methods\nsuch as maximum likelihood, a more recent approach uses estimation-optimization\nintegration that minimizes decision error instead of estimation error. Although\nintuitive, the statistical benefit of the latter approach is not well\nunderstood yet is important to guide the prescriptive usage of machine\nlearning. In this paper, we dissect the performance comparisons between these\napproaches in terms of the amount of model misspecification. In particular, we\nshow how the integrated approach offers a ``universal double benefit'' on the\ntop two dominating terms of regret when the underlying model is misspecified,\nwhile the traditional approach can be advantageous when the model is nearly\nwell-specified. Our comparison is powered by finite-sample tail regret bounds\nthat are derived via new higher-order expansions of regrets and the leveraging\nof a recent Berry-Esseen theorem.",
        "Video diffusion models (VDMs) facilitate the generation of high-quality\nvideos, with current research predominantly concentrated on scaling efforts\nduring training through improvements in data quality, computational resources,\nand model complexity. However, inference-time scaling has received less\nattention, with most approaches restricting models to a single generation\nattempt. Recent studies have uncovered the existence of \"golden noises\" that\ncan enhance video quality during generation. Building on this, we find that\nguiding the scaling inference-time search of VDMs to identify better noise\ncandidates not only evaluates the quality of the frames generated in the\ncurrent step but also preserves the high-level object features by referencing\nthe anchor frame from previous multi-chunks, thereby delivering long-term\nvalue. Our analysis reveals that diffusion models inherently possess flexible\nadjustments of computation by varying denoising steps, and even a one-step\ndenoising approach, when guided by a reward signal, yields significant\nlong-term benefits. Based on the observation, we proposeScalingNoise, a\nplug-and-play inference-time search strategy that identifies golden initial\nnoises for the diffusion sampling process to improve global content consistency\nand visual diversity. Specifically, we perform one-step denoising to convert\ninitial noises into a clip and subsequently evaluate its long-term value,\nleveraging a reward model anchored by previously generated content. Moreover,\nto preserve diversity, we sample candidates from a tilted noise distribution\nthat up-weights promising noises. In this way, ScalingNoise significantly\nreduces noise-induced errors, ensuring more coherent and spatiotemporally\nconsistent video generation. Extensive experiments on benchmark datasets\ndemonstrate that the proposed ScalingNoise effectively improves long video\ngeneration.",
        "Lifelong learning (LL) aims to continuously acquire new knowledge while\nretaining previously learned knowledge. A central challenge in LL is the\nstability-plasticity dilemma, which requires models to balance the preservation\nof previous knowledge (stability) with the ability to learn new tasks\n(plasticity). While parameter-efficient fine-tuning (PEFT) has been widely\nadopted in large language models, its application to lifelong learning remains\nunderexplored. To bridge this gap, this paper proposes AdaLL, an adapter-based\nframework designed to address the dilemma through a simple, universal, and\neffective strategy. AdaLL co-trains the backbone network and adapters under\nregularization constraints, enabling the backbone to capture task-invariant\nfeatures while allowing the adapters to specialize in task-specific\ninformation. Unlike methods that freeze the backbone network, AdaLL\nincrementally enhances the backbone's capabilities across tasks while\nminimizing interference through backbone regularization. This architectural\ndesign significantly improves both stability and plasticity, effectively\neliminating the stability-plasticity dilemma. Extensive experiments demonstrate\nthat AdaLL consistently outperforms existing methods across various\nconfigurations, including dataset choices, task sequences, and task scales.",
        "Recent advances in deep learning have revolutionized seismic monitoring, yet\ndeveloping a foundation model that performs well across multiple complex tasks\nremains challenging, particularly when dealing with degraded signals or data\nscarcity. This work presents SeisMoLLM, the first foundation model that\nutilizes cross-modal transfer for seismic monitoring, to unleash the power of\nlarge-scale pre-training from a large language model without requiring direct\npre-training on seismic datasets. Through elaborate waveform tokenization and\nfine-tuning of pre-trained GPT-2 model, SeisMoLLM achieves state-of-the-art\nperformance on the DiTing and STEAD datasets across five critical tasks:\nback-azimuth estimation, epicentral distance estimation, magnitude estimation,\nphase picking, and first-motion polarity classification. It attains 36 best\nresults out of 43 task metrics and 12 top scores out of 16 few-shot\ngeneralization metrics, with many relative improvements ranging from 10% to\n50%. In addition to its superior performance, SeisMoLLM maintains efficiency\ncomparable to or even better than lightweight models in both training and\ninference. These findings establish SeisMoLLM as a promising foundation model\nfor practical seismic monitoring and highlight cross-modal transfer as an\nexciting new direction for earthquake studies, showcasing the potential of\nadvanced deep learning techniques to propel seismology research forward.",
        "Recent work uncovered potential flaws in \\eg attribution or heatmap based\nsaliency methods. A typical flaw is a confirmations bias, where the scores are\ncompared to human expectation. Since measuring the quality of saliency methods\nis hard due to missing ground truth model reasoning, finding general\nlimitations is also hard. This is further complicated, because masking-based\nevaluation on complex data can easily introduce a bias, as most methods cannot\nfully ignore inputs. In this work, we extend our previous analysis on the\nlogical dataset framework ANDOR, where we showed that all analysed saliency\nmethods fail to grasp all needed classification information for all possible\nscenarios. Specifically, this paper extends our previous work using analysis on\nmore datasets, in order to better understand in which scenarios the saliency\nmethods fail. Further, we apply the Global Coherence Representation as an\nadditional evaluation method in order to enable actual input omission.",
        "Universal knowledge representation is a central problem for multivariate time\nseries(MTS) foundation models and yet remains open. This paper investigates\nthis problem from the first principle and it makes four folds of contributions.\nFirst, a new empirical finding is revealed: time series with different time\ngranularities (or corresponding frequency resolutions) exhibit distinct joint\ndistributions in the frequency domain. This implies a crucial aspect of\nlearning universal knowledge, one that has been overlooked by previous studies.\nSecond, a novel Fourier knowledge attention mechanism is proposed to enable\nlearning time granularity-aware representations from both the temporal and\nfrequency domains. Third, an autoregressive blank infilling pre-training\nframework is incorporated to time series analysis for the first time, leading\nto a generative tasks agnostic pre-training strategy. To this end, we develop\nthe General Time-series Model (GTM), a unified MTS foundation model that\naddresses the limitation of contemporary time series models, which often\nrequire token, pre-training, or model-level customizations for downstream tasks\nadaption. Fourth, extensive experiments show that GTM outperforms\nstate-of-the-art (SOTA) methods across all generative tasks, including\nlong-term forecasting, anomaly detection, and imputation.",
        "A long-standing goal in AI is to build agents that can solve a variety of\ntasks across different environments, including previously unseen ones. Two\ndominant approaches tackle this challenge: (i) reinforcement learning (RL),\nwhich learns policies through trial and error, and (ii) optimal control, which\nplans actions using a learned or known dynamics model. However, their relative\nstrengths and weaknesses remain underexplored in the setting where agents must\nlearn from offline trajectories without reward annotations. In this work, we\nsystematically analyze the performance of different RL and control-based\nmethods under datasets of varying quality. On the RL side, we consider\ngoal-conditioned and zero-shot approaches. On the control side, we train a\nlatent dynamics model using the Joint Embedding Predictive Architecture (JEPA)\nand use it for planning. We study how dataset properties-such as data\ndiversity, trajectory quality, and environment variability-affect the\nperformance of these approaches. Our results show that model-free RL excels\nwhen abundant, high-quality data is available, while model-based planning\nexcels in generalization to novel environment layouts, trajectory stitching,\nand data-efficiency. Notably, planning with a latent dynamics model emerges as\na promising approach for zero-shot generalization from suboptimal data.",
        "This study examines the potential causal relationship between head injury and\nthe risk of developing Alzheimer's disease (AD) using Bayesian networks and\nregression models. Using a dataset of 2,149 patients, we analyze key medical\nhistory variables, including head injury history, memory complaints,\ncardiovascular disease, and diabetes. Logistic regression results suggest an\nodds ratio of 0.88 for head injury, indicating a potential but statistically\ninsignificant protective effect against AD. In contrast, memory complaints\nexhibit a strong association with AD, with an odds ratio of 4.59. Linear\nregression analysis further confirms the lack of statistical significance for\nhead injury (coefficient: -0.0245, p = 0.469) while reinforcing the predictive\nimportance of memory complaints. These findings highlight the complex interplay\nof medical history factors in AD risk assessment and underscore the need for\nfurther research utilizing larger datasets and advanced causal modeling\ntechniques.",
        "We present a $k$-means-based clustering algorithm, which optimizes the mean\nsquare error, for given cluster sizes. A straightforward application is\nbalanced clustering, where the sizes of each cluster are equal. In the\n$k$-means assignment phase, the algorithm solves an assignment problem using\nthe Hungarian algorithm. This makes the assignment phase time complexity\n$O(n^3)$. This enables clustering of datasets of size more than 5000 points.",
        "Online handwriting recognition (HWR) using data from inertial measurement\nunits (IMUs) remains challenging due to variations in writing styles and the\nlimited availability of high-quality annotated datasets. Traditional models\noften struggle to recognize handwriting from unseen writers, making\nwriter-independent (WI) recognition a crucial but difficult problem. This paper\npresents an HWR model with an encoder-decoder structure for IMU data, featuring\na CNN-based encoder for feature extraction and a BiLSTM decoder for sequence\nmodeling, which supports inputs of varying lengths. Our approach demonstrates\nstrong robustness and data efficiency, outperforming existing methods on WI\ndatasets, including the WI split of the OnHW dataset and our own dataset.\nExtensive evaluations show that our model maintains high accuracy across\ndifferent age groups and writing conditions while effectively learning from\nlimited data. Through comprehensive ablation studies, we analyze key design\nchoices, achieving a balance between accuracy and efficiency. These findings\ncontribute to the development of more adaptable and scalable HWR systems for\nreal-world applications.",
        "The recent advancement of foundation models (FMs) has brought about a\nparadigm shift, revolutionizing various sectors worldwide. The popular\noptimizers used to train these models are stochastic gradient descent-based\nalgorithms, which face inherent limitations, such as slow convergence and\nstringent assumptions for convergence. In particular, data heterogeneity\narising from distributed settings poses significant challenges to their\ntheoretical and numerical performance. This paper develops an algorithm, PISA\n({P}reconditioned {I}nexact {S}tochastic {A}lternating Direction Method of\nMultipliers), which enables scalable parallel computing and supports various\nsecond-moment schemes. Grounded in rigorous theoretical guarantees, the\nalgorithm converges under the sole assumption of Lipschitz continuity of the\ngradient, thereby removing the need for other conditions commonly imposed by\nstochastic methods. This capability enables PISA to tackle the challenge of\ndata heterogeneity effectively. Comprehensive experimental evaluations for\ntraining or fine-tuning diverse FMs, including vision models, large language\nmodels, reinforcement learning models, generative adversarial networks, and\nrecurrent neural networks, demonstrate its superior numerical performance\ncompared to various state-of-the-art optimizers.",
        "Current evaluations of synthetic tabular data mainly focus on how well joint\ndistributions are modeled, often overlooking the assessment of their\neffectiveness in preserving realistic event sequences and coherent entity\nrelationships across columns.This paper proposes three evaluation metrics\ndesigned to assess the preservation of logical relationships among columns in\nsynthetic tabular data. We validate these metrics by assessing the performance\nof both classical and state-of-the-art generation methods on a real-world\nindustrial dataset.Experimental results reveal that existing methods often fail\nto rigorously maintain logical consistency (e.g., hierarchical relationships in\ngeography or organization) and dependencies (e.g., temporal sequences or\nmathematical relationships), which are crucial for preserving the fine-grained\nrealism of real-world tabular data. Building on these insights, this study also\ndiscusses possible pathways to better capture logical relationships while\nmodeling the distribution of synthetic tabular data.",
        "In this paper, we reconstruct Euclid's theory of similar triangles, as\ndeveloped in Book VI of the \\textit{Elements}, along with its 20th-century\ncounterparts, formulated within the systems of Hilbert, Birkhoff, Borsuk and\nSzmielew, Millman and Parker, as well as Hartshorne. In the final sections, we\npresent recent developments concerning non-Archimedean fields and mechanized\nproofs. Thales' theorem (VI.2) serves as the reference point in our\ncomparisons. It forms the basis of Euclid's system and follows from VI.1 the\nonly proposition within the theory of similar triangles that explicitly applies\nthe definition of proportion. Instead of the ancient proportion, modern systems\nadopt the arithmetic of line segments or real numbers. Accordingly, they adopt\nother propositions from Euclid's Book VI, such as VI.4, VI.6, or VI.9, as a\nbasis. In {\\S}\\,10, we present a system that, while meeting modern criteria of\nrigor, reconstructs Euclid's theory and mimics its deductive structure,\nbeginning with VI.1. This system extends to automated proofs of Euclid's\npropositions from Book VI. Systems relying on real numbers provide the\nfoundation for trigonometry as applied in modern mathematics. In {\\S}\\,9, we\nprove Thales' theorem in geometry over the hyperreal numbers. Just as Hilbert\nmanaged to prove Thales' theorem without referencing the Archimedean axiom, so\ndo we by applying the arithmetic of the non-Archimedean field of hyperreal\nnumbers.",
        "Time series forecasting has recently achieved significant progress with\nmulti-scale models to address the heterogeneity between long and short range\npatterns. Despite their state-of-the-art performance, we identify two potential\nareas for improvement. First, the variates of the multivariate time series are\nprocessed independently. Moreover, the multi-scale (long and short range)\nrepresentations are learned separately by two independent models without\ncommunication. In light of these concerns, we propose State Space Transformer\nwith cross-attention (S2TX). S2TX employs a cross-attention mechanism to\nintegrate a Mamba model for extracting long-range cross-variate context and a\nTransformer model with local window attention to capture short-range\nrepresentations. By cross-attending to the global context, the Transformer\nmodel further facilitates variate-level interactions as well as local\/global\ncommunications. Comprehensive experiments on seven classic long-short range\ntime-series forecasting benchmark datasets demonstrate that S2TX can achieve\nhighly robust SOTA results while maintaining a low memory footprint.",
        "Current end-to-end (E2E) and plug-and-play (PnP) image reconstruction\nalgorithms approximate the maximum a posteriori (MAP) estimate but cannot offer\nsampling from the posterior distribution, like diffusion models. By contrast,\nit is challenging for diffusion models to be trained in an E2E fashion. This\npaper introduces a Deep End-to-End Posterior ENergy (DEEPEN) framework, which\nenables MAP estimation as well as sampling. We learn the parameters of the\nposterior, which is the sum of the data consistency error and the negative\nlog-prior distribution, using maximum likelihood optimization in an E2E\nfashion. The proposed approach does not require algorithm unrolling, and hence\nhas a smaller computational and memory footprint than current E2E methods,\nwhile it does not require contraction constraints typically needed by current\nPnP methods. Our results demonstrate that DEEPEN offers improved performance\nthan current E2E and PnP models in the MAP setting, while it also offers faster\nsampling compared to diffusion models. In addition, the learned energy-based\nmodel is observed to be more robust to changes in image acquisition settings.",
        "The closure principle is a standard tool for achieving family-wise error rate\n(FWER) control in multiple testing problems. In general, the computational cost\nfor closed testing can be exponential in the number of hypotheses. The\ncelebrated graphical approach of FWER control overcomes the computational\nhurdle by using weighted Bonferroni local tests on p-values with appropriately\nchosen weights. In this study, we extend the graphical approach to e-values.\nWith valid e-values -- common in settings of sequential hypothesis testing or\nuniversal inference for irregular parametric models -- we can derive strictly\nmore powerful local tests based on weighted averages of e-values. Consequently,\nthis e-value-based closed test is more powerful than the corresponding\ngraphical approach with inverse e-values as p-values. Although the\ncomputational shortcuts for the p-value-based graphical approach are not\napplicable, we develop efficient polynomial-time algorithms using dynamic\nprogramming for e-value-based graphical approaches with any directed acyclic\ngraph. For special graphs, such as those used in the Holm's procedure and\nfallback procedure, we develop tailored algorithms with computation cost linear\nin the number of hypotheses, up to logarithmic factors.",
        "Foundation models for single-cell RNA sequencing (scRNA-seq) have shown\npromising capabilities in capturing gene expression patterns. However, current\napproaches face critical limitations: they ignore biological prior knowledge\nencoded in gene regulatory relationships and fail to leverage multi-omics\nsignals that could provide complementary regulatory insights. In this paper, we\npropose GRNFormer, a new framework that systematically integrates multi-scale\nGene Regulatory Networks (GRNs) inferred from multi-omics data into RNA\nfoundation model training. Our framework introduces two key innovations. First,\nwe introduce a pipeline for constructing hierarchical GRNs that capture\nregulatory relationships at both cell-type-specific and cell-specific\nresolutions. Second, we design a structure-aware integration framework that\naddresses the information asymmetry in GRNs through two technical advances: (1)\nA graph topological adapter using multi-head cross-attention to weight\nregulatory relationships dynamically, and (2) a novel edge perturbation\nstrategy that perturb GRNs with biologically-informed co-expression links to\naugment graph neural network training. Comprehensive experiments have been\nconducted on three representative downstream tasks across multiple model\narchitectures to demonstrate the effectiveness of GRNFormer. It achieves\nconsistent improvements over state-of-the-art (SoTA) baselines: $3.6\\%$\nincrease in drug response prediction correlation, $9.6\\%$ improvement in\nsingle-cell drug classification AUC, and $1.1\\%$ average gain in gene\nperturbation prediction accuracy.",
        "This paper presents a mixed-mode delay-locked loop (MM-DLL) with binary\nsearch (BS) locking, designed to cover a broad frequency range from 533 MHz to\n4.26 GHz. The BS locking scheme optimizes the locking time, reducing it from a\nlinear to a logarithmic function, completing in B+1 cycles, where B represents\nthe digital-to-analog (DAC) resolution controlling the voltage-controlled delay\nline (VCDL). At the start of the BS process, large step sizes can cause\nsignificant bias overshoots, potentially leading to clock failure conditions\n(i.e., clocks fail to propagate through the VCDL). To address this issue, a\ntoggle detector is introduced to monitor clock activity and adjust the binary\nsearch controller. Upon detecting a stalled clock, the controller reverts the\nDAC code to the previous working code and resumes the BS with a reduced step\nsize. Fabricated in a 3-nm FinFET CMOS process, the proposed MM-DLL achieves a\nlocking time of under 10.5 ns while consuming 5.4 mW from a 0.75 V supply at\n4.26 GHz. The measured performance includes a high resolution of 0.73 ps, with\na static phase error of 0.73 ps, RMS jitter of 1.2 ps, and peak-to-peak jitter\nof 4.9 ps. The proposed MM-DLL achieves state-of-the-art power figure of merit\n(FoM) of 0.82 pJ and DLL locking FoM of 0.01 $pJ\\cdot ns^2$.",
        "Recent advancements in autonomous driving perception have revealed\nexceptional capabilities within structured environments dominated by vehicular\ntraffic. However, current perception models exhibit significant limitations in\nsemi-structured environments, where dynamic pedestrians with more diverse\nirregular movement and occlusion prevail. We attribute this shortcoming to the\nscarcity of high-quality datasets in semi-structured scenes, particularly\nconcerning pedestrian perception and prediction. In this work, we present the\nmulti-modal Pedestrian-Focused Scene Dataset(PFSD), rigorously annotated in\nsemi-structured scenes with the format of nuScenes. PFSD provides comprehensive\nmulti-modal data annotations with point cloud segmentation, detection, and\nobject IDs for tracking. It encompasses over 130,000 pedestrian instances\ncaptured across various scenarios with varying densities, movement patterns,\nand occlusions. Furthermore, to demonstrate the importance of addressing the\nchallenges posed by more diverse and complex semi-structured environments, we\npropose a novel Hybrid Multi-Scale Fusion Network (HMFN). Specifically, to\ndetect pedestrians in densely populated and occluded scenarios, our method\neffectively captures and fuses multi-scale features using a meticulously\ndesigned hybrid framework that integrates sparse and vanilla convolutions.\nExtensive experiments on PFSD demonstrate that HMFN attains improvement in mean\nAverage Precision (mAP) over existing methods, thereby underscoring its\nefficacy in addressing the challenges of 3D pedestrian detection in complex\nsemi-structured environments. Coding and benchmark are available.",
        "This research examines the regularity of weak solutions to the Div-Curl\nsystem with low regularity anisotropic coefficients. The H\\\"older regularity of\nthe Div-Curl system with one anisotropic coefficient was an unresolved problem\nraised by Yin in 2016. We have addressed the open problem, and the findings\nextend to the scenario involving two anisotropic coefficients. We establish the\nH\\\"{o}lder regularity of the solution when the coefficients is H\\\"{o}lder\ncontinuous. Moreover, the degree of H\\\"{o}lder regularity of the solution can\nbe improved if the coefficient has a greater degree of H\\\"{o}lder regularity.",
        "We establish a semi-orthogonal decomposition for the weighted blowup of an\nalgebraic stack along a Koszul-regular weighted centre, generalising the\nclassic result of Orlov. Our approach is based on the work of Bergh-Schn\\\"urer.",
        "Zero-shot LLMs are now also used for textual classification tasks, e.g.,\nsentiment\/emotion detection of a given input as a sentence\/article. However,\ntheir performance can be suboptimal in such data annotation tasks. We introduce\na novel technique Perceived Confidence Scoring (PCS) that evaluates LLM's\nconfidence for its classification of an input by leveraging Metamorphic\nRelations (MRs). The MRs generate semantically equivalent yet textually mutated\nversions of the input. Following the principles of Metamorphic Testing (MT),\nthe mutated versions are expected to have annotation labels similar to the\ninput. By analyzing the consistency of LLM responses across these variations,\nPCS computes a confidence score based on the frequency of predicted labels. PCS\ncan be used both for single LLM and multiple LLM settings (e.g., majority\nvoting). We introduce an algorithm Perceived Differential Evolution (PDE) that\ndetermines the optimal weights assigned to the MRs and the LLMs for a\nclassification task. Empirical evaluation shows PCS significantly improves\nzero-shot accuracy for Llama-3-8B-Instruct (4.96%) and Mistral-7B-Instruct-v0.3\n(10.52%), with Gemma-2-9b-it showing a 9.39% gain. When combining all three\nmodels, PCS significantly outperforms majority voting by 7.75%.",
        "The randomized SVD is a method to compute an inexpensive, yet accurate,\nlow-rank approximation of a matrix. The algorithm assumes access to the matrix\nthrough matrix-vector products (matvecs). Therefore, when we would like to\napply the randomized SVD to a matrix function, $f(A)$, one needs to approximate\nmatvecs with $f(A)$ using some other algorithm, which is typically treated as a\nblack-box. Chen and Hallman (SIMAX 2023) argued that, in the common setting\nwhere matvecs with $f(A)$ are approximated using Krylov subspace methods\n(KSMs), more efficient low-rank approximation is possible if we open this\nblack-box. They present an alternative approach that significantly outperforms\nthe naive combination of KSMs with the randomized SVD, although the method\nlacked theoretical justification. In this work, we take a closer look at the\nmethod, and provide strong and intuitive error bounds that justify its\nexcellent performance for low-rank approximation of matrix functions.",
        "Machine learning can analyze vast amounts of data generated by IoT devices to\nidentify patterns, make predictions, and enable real-time decision-making. By\nprocessing sensor data, machine learning models can optimize processes, improve\nefficiency, and enhance personalized user experiences in smart systems.\nHowever, IoT systems are often deployed in sensitive environments such as\nhouseholds and offices, where they may inadvertently expose identifiable\ninformation, including location, habits, and personal identifiers. This raises\nsignificant privacy concerns, necessitating the application of data\nminimization -- a foundational principle in emerging data regulations, which\nmandates that service providers only collect data that is directly relevant and\nnecessary for a specified purpose. Despite its importance, data minimization\nlacks a precise technical definition in the context of sensor data, where\ncollections of weak signals make it challenging to apply a binary \"relevant and\nnecessary\" rule. This paper provides a technical interpretation of data\nminimization in the context of sensor streams, explores practical methods for\nimplementation, and addresses the challenges involved. Through our approach, we\ndemonstrate that our framework can reduce user identifiability by up to 16.7%\nwhile maintaining accuracy loss below 1%, offering a viable path toward\nprivacy-preserving IoT data processing.",
        "Multi-armed bandits (MABs) are frequently used for online sequential\ndecision-making in applications ranging from recommending personalized content\nto assigning treatments to patients. A recurring challenge in the applicability\nof the classic MAB framework to real-world settings is ignoring\n\\textit{interference}, where a unit's outcome depends on treatment assigned to\nothers. This leads to an exponentially growing action space, rendering standard\napproaches computationally impractical. We study the MAB problem under network\ninterference, where each unit's reward depends on its own treatment and those\nof its neighbors in a given interference graph. We propose a novel algorithm\nthat uses the local structure of the interference graph to minimize regret. We\nderive a graph-dependent upper bound on cumulative regret showing that it\nimproves over prior work. Additionally, we provide the first lower bounds for\nbandits with arbitrary network interference, where each bound involves a\ndistinct structural property of the interference graph. These bounds\ndemonstrate that when the graph is either dense or sparse, our algorithm is\nnearly optimal, with upper and lower bounds that match up to logarithmic\nfactors. We complement our theoretical results with numerical experiments,\nwhich show that our approach outperforms baseline methods.",
        "We investigate the notion of the medial axis for pseudo-Euclidean spaces. For\nmost of the article, we follow the path of Birbrair and Denkowski's article\n\"Medial Axis and Singularities\", checking its feasibility in the new context.",
        "Multi-agent trajectory modeling has primarily focused on forecasting future\nstates, often overlooking broader tasks like trajectory completion, which are\ncrucial for real-world applications such as correcting tracking data. Existing\nmethods also generally predict agents' states without offering any state-wise\nmeasure of uncertainty. Moreover, popular multi-modal sampling methods lack any\nerror probability estimates for each generated scene under the same prior\nobservations, making it difficult to rank the predictions during inference\ntime. We introduce U2Diff, a \\textbf{unified} diffusion model designed to\nhandle trajectory completion while providing state-wise \\textbf{uncertainty}\nestimates jointly. This uncertainty estimation is achieved by augmenting the\nsimple denoising loss with the negative log-likelihood of the predicted noise\nand propagating latent space uncertainty to the real state space. Additionally,\nwe incorporate a Rank Neural Network in post-processing to enable \\textbf{error\nprobability} estimation for each generated mode, demonstrating a strong\ncorrelation with the error relative to ground truth. Our method outperforms the\nstate-of-the-art solutions in trajectory completion and forecasting across four\nchallenging sports datasets (NBA, Basketball-U, Football-U, Soccer-U),\nhighlighting the effectiveness of uncertainty and error probability estimation.\nVideo at https:\/\/youtu.be\/ngw4D4eJToE"
      ]
    }
  },
  {
    "id":2411.16896,
    "research_type":"applied",
    "start_id":"b16",
    "start_title":"Characterization of fluorescence lifetime of organic fluorophores for molecular imaging in the shortwave infrared window",
    "start_abstract":"SignificanceFluorescence lifetime imaging in the shortwave infrared (SWIR) is expected to enable high-resolution multiplexed molecular highly scattering tissue.AimTo characterize brightness and fluorescence of commercially available organic SWIR fluorophores benchmark them against tail emission conventional NIR-excited probes.ApproachCharacterization was performed through our established time-domain mesoscopic tomography system integrated around a time-correlated single-photon counting-single-photon avalanche diode array. Brightness were measured for NIR probes >1000 nm. Simultaneous probe then assess their potential studies.ResultsThe outperformed while mean lifetimes extremely short. The phantom study demonstrated feasibility multiplexing window with both probes.ConclusionsLong-tail Fluorescence readily detectable window, where showed shorter compared probes. We demonstrate which paves way vivo studies intact tissues at improved resolution.",
    "start_categories":[
      "q-bio.BM"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b27"
      ],
      "title":[
        "Fast fit-free analysis of fluorescence lifetime imaging via deep learning"
      ],
      "abstract":[
        "Fluorescence lifetime imaging (FLI) provides unique quantitative information in biomedical and molecular biology studies but relies on complex data-fitting techniques to derive the quantities of interest. Herein, we propose a fit-free approach FLI image formation that is based deep learning (DL) quantify fluorescence decays simultaneously over whole at fast speeds. We report neural network (DNN) architecture, named (FLI-Net) designed trained for different classes experiments, including visible near-infrared (NIR) microscopy (FLIM) NIR gated macroscopy (MFLI). FLI-Net outputs quantitatively spatially resolved lifetime-based parameters are typically employed field. validate utility framework by performing microscopic preclinical across spectra, as well 2 main data acquisition technologies. These results demonstrate suited accurately lifetimes cells and, real time, intact animals without any parameter settings. Hence, paves way reproducible unprecedented speeds, improved dissemination impact many important applications ranging from fundamental discoveries cellular clinical translation."
      ],
      "categories":[
        "cs.NE"
      ]
    },
    "list":{
      "title":[
        "Adding numbers with spiking neural circuits on neuromorphic hardware",
        "Extract-QD Framework: A Generic Approach for Quality-Diversity in Noisy,\n  Stochastic or Uncertain Domains",
        "Fig Tree-Wasp Symbiotic Coevolutionary Optimization Algorithm",
        "Evolving Form and Function: Dual-Objective Optimization in Neural\n  Symbolic Regression Networks",
        "Hybrid Firefly Algorithm and Sperm Swarm Optimization Algorithm using\n  Newton-Raphson Method (HFASSON) and its application in CR-VANET",
        "Hardware-In-The-Loop Training of a 4f Optical Correlator with\n  Logarithmic Complexity Reduction for CNNs",
        "The working principles of model-based GAs fall within the PAC framework:\n  A mathematical theory of problem decomposition",
        "Continuous signal sparse encoding using analog neuromorphic variability",
        "Domain-Agnostic Co-Evolution of Generalizable Parallel Algorithm\n  Portfolios",
        "Spiking World Model with Multi-Compartment Neurons for Model-based\n  Reinforcement Learning",
        "$SpikePack$: Enhanced Information Flow in Spiking Neural Networks with\n  High Hardware Compatibility",
        "Neuromorphic Digital-Twin-based Controller for Indoor Multi-UAV Systems\n  Deployment",
        "Multiple-gain Estimation for Running Time of Evolutionary Combinatorial\n  Optimization",
        "Testing the limits of ITkPixV2: the ATLAS inner tracker pixel detector\n  readout chip",
        "Analysis of pitchfork bifurcations and symmetry breaking in the elliptic\n  restricted three-body problem",
        "Range-Only Localization System for Small-Scale Flapping-Wing Robots",
        "Point-LN: A Lightweight Framework for Efficient Point Cloud\n  Classification Using Non-Parametric Positional Encoding",
        "A Novel Interpretation of the Radon Transform's Ray- and Pixel-Driven\n  Discretizations under Balanced Resolutions",
        "ADAPT: An Autonomous Forklift for Construction Site Operation",
        "Perimeter length of the convex hull of Brownian motion in the hyperbolic\n  plane",
        "Post-disaster building indoor damage and survivor detection using\n  autonomous path planning and deep learning with unmanned aerial vehicles",
        "On families of strongly divisible modules of rank 2",
        "On-demand storage and retrieval of single photons from a semiconductor\n  quantum dot in a room-temperature atomic vapor memory",
        "High Energy Jet Emission from GRS 1758-258 & 1E 1740.7-2942 with\n  INTEGRAL?",
        "Emotional Multifaceted Feedback on AI Tool Use in EFL Learning\n  Initiation: Chain-Mediated Effects of Motivation and Metacognitive Strategies\n  in an Optimized TAM Model",
        "Derivation of the Planck Units Based in a Membranes Model",
        "Nonparametric Smoothing of Directional and Axial Data",
        "Carbonic anhydrase II simulated with a universal neural network\n  potential"
      ],
      "abstract":[
        "Progress in neuromorphic computing requires efficient implementation of\nstandard computational problems, like adding numbers. Here we implement one\nsequential and two parallel binary adders in the Lava software framework, and\ndeploy them to the neuromorphic chip Loihi 2. We describe the time complexity,\nneuron and synaptic resources, as well as constraints on the bit width of the\nnumbers that can be added with the current implementations. Further, we measure\nthe time required for the addition operation on-chip. Importantly, we encounter\ntrade-offs in terms of time complexity and required chip resources for the\nthree considered adders. While sequential adders have linear time complexity\n$\\bf\\mathcal{O}(n)$ and require a linearly increasing number of neurons and\nsynapses with number of bits $n$, the parallel adders have constant time\ncomplexity $\\bf\\mathcal{O}(1)$ and also require a linearly increasing number of\nneurons, but nonlinearly increasing synaptic resources (scaling with $\\bf n^2$\nor $\\bf n \\sqrt{n}$). This trade-off between compute time and chip resources\nmay inform decisions in application development, and the implementations we\nprovide may serve as a building block for further progress towards efficient\nneuromorphic algorithms.",
        "Quality-Diversity (QD) has demonstrated potential in discovering collections\nof diverse solutions to optimisation problems. Originally designed for\ndeterministic environments, QD has been extended to noisy, stochastic, or\nuncertain domains through various Uncertain-QD (UQD) methods. However, the\nlarge number of UQD methods, each with unique constraints, makes selecting the\nmost suitable one challenging. To remedy this situation, we present two\ncontributions: first, the Extract-QD Framework (EQD Framework), and second,\nExtract-ME (EME), a new method derived from it. The EQD Framework unifies\nexisting approaches within a modular view, and facilitates developing novel\nmethods by interchanging modules. We use it to derive EME, a novel method that\nconsistently outperforms or matches the best existing methods on standard\nbenchmarks, while previous methods show varying performance. In a second\nexperiment, we show how our EQD Framework can be used to augment existing QD\nalgorithms and in particular the well-established\nPolicy-Gradient-Assisted-MAP-Elites method, and demonstrate improved\nperformance in uncertain domains at no additional evaluation cost. For any new\nuncertain task, our contributions now provide EME as a reliable \"first guess\"\nmethod, and the EQD Framework as a tool for developing task-specific\napproaches. Together, these contributions aim to lower the cost of adopting UQD\ninsights in QD applications.",
        "The nature inspired algorithms are becoming popular due to their simplicity\nand wider applicability. In the recent past several such algorithms have been\ndeveloped. They are mainly bio-inspired, swarm based, physics based and\nsocio-inspired; however, the domain based on symbiotic relation between\ncreatures is still to be explored. A novel metaheuristic optimization algorithm\nreferred to as Fig Tree-Wasp Symbiotic Coevolutionary (FWSC) algorithm is\nproposed. It models the symbiotic coevolutionary relationship between fig trees\nand wasps. More specifically, the mating of wasps, pollinating the figs,\nsearching for new trees for pollination and wind effect drifting of wasps are\nmodeled in the algorithm. These phenomena help in balancing the two important\naspects of exploring the search space efficiently as well as exploit the\npromising regions. The algorithm is successfully tested on a variety of test\nproblems. The results are compared with existing methods and algorithms. The\nWilcoxon Signed Rank Test and Friedman Test are applied for the statistical\nvalidation of the algorithm performance. The algorithm is also further applied\nto solve the real-world engineering problems. The performance of the FWSC\nunderscored that the algorithm can be applied to wider variety of real-world\nproblems.",
        "Data increasingly abounds, but distilling their underlying relationships down\nto something interpretable remains challenging. One approach is genetic\nprogramming, which `symbolically regresses' a data set down into an equation.\n  However, symbolic regression (SR) faces the issue of requiring training from\nscratch for each new dataset. To generalize across all datasets, deep learning\ntechniques have been applied to SR.\n  These networks, however, are only able to be trained using a symbolic\nobjective: NN-generated and target equations are symbolically compared. But\nthis does not consider the predictive power of these equations, which could be\nmeasured by a behavioral objective that compares the generated equation's\npredictions to actual data.\n  Here we introduce a method that combines gradient descent and evolutionary\ncomputation to yield neural networks that minimize the symbolic and behavioral\nerrors of the equations they generate from data.\n  As a result, these evolved networks are shown to generate more symbolically\nand behaviorally accurate equations than those generated by networks trained by\nstate-of-the-art gradient based neural symbolic regression methods.\n  We hope this method suggests that evolutionary algorithms, combined with\ngradient descent, can improve SR results by yielding equations with more\naccurate form and function.",
        "This paper proposes a new hybrid algorithm, combining FA, SSO, and the N-R\nmethod to accelerate convergence towards global optima, named the Hybrid\nFirefly Algorithm and Sperm Swarm Optimization with Newton-Raphson (HFASSON).\nThe performance of HFASSON is evaluated using 23 benchmark functions from the\nCEC 2017 suite, tested in 30, 50, and 100 dimensions. A statistical comparison\nis performed to assess the effectiveness of HFASSON against FA, SSO, HFASSO,\nand five hybrid algorithms: Water Cycle Moth Flame Optimization (WCMFO), Hybrid\nParticle Swarm Optimization and Genetic Algorithm (HPSOGA), Hybrid Sperm Swarm\nOptimization and Gravitational Search Algorithm (HSSOGSA), Grey Wolf and Cuckoo\nSearch Algorithm (GWOCS), and Hybrid Firefly Genetic Algorithm (FAGA). Results\nfrom the Friedman rank test show the superior performance of HFASSON.\nAdditionally, HFASSON is applied to Cognitive Radio Vehicular Ad-hoc Networks\n(CR-VANET), outperforming basic CR-VANET in spectrum utilization. These\nfindings demonstrate HFASSON's efficiency in wireless network applications.",
        "This work evaluates a forward-only learning algorithm on the MNIST dataset\nwith hardware-in-the-loop training of a 4f optical correlator, achieving 87.6%\naccuracy with O(n2) complexity, compared to backpropagation, which achieves\n88.8% accuracy with O(n2 log n) complexity.",
        "The concepts of linkage, building blocks, and problem decomposition have long\nexisted in the genetic algorithm (GA) field and have guided the development of\nmodel-based GAs for decades. However, their definitions are usually vague,\nmaking it difficult to develop theoretical support. This paper provides an\nalgorithm-independent definition to describe the concept of linkage. With this\ndefinition, the paper proves that any problems with a bounded degree of linkage\nare decomposable and that proper problem decomposition is possible via linkage\nlearning. The way of decomposition given in this paper also offers a new\nperspective on nearly decomposable problems with bounded difficulty and\nbuilding blocks from the theoretical aspect. Finally, this paper relates\nproblem decomposition to PAC learning and proves that the global optima of\nthese problems and the minimum decomposition blocks are PAC learnable under\ncertain conditions.",
        "Achieving fast and reliable temporal signal encoding is crucial for\nlow-power, always-on systems. While current spike-based encoding algorithms\nrely on complex networks or precise timing references, simple and robust\nencoding models can be obtained by leveraging the intrinsic properties of\nanalog hardware substrates. We propose an encoding framework inspired by\nbiological principles that leverages intrinsic neuronal variability to robustly\nencode continuous stimuli into spatio-temporal patterns, using at most one\nspike per neuron. The encoder has low model complexity, relying on a shallow\nnetwork of heterogeneous neurons. It relies on an internal time reference,\nallowing for continuous processing. Moreover, stimulus parameters can be\nlinearly decoded from the spiking patterns, granting fast information\nretrieval. Our approach, validated on both analog neuromorphic hardware and\nsimulation, demonstrates high robustness to noise, spike jitter, and reduced\nheterogeneity. Consistently with biological observations, we observed the\nspontaneous emergence of patterns with stereotyped spiking order. The proposed\nencoding scheme facilitates fast, robust and continuous information processing,\nmaking it well-suited for low-power, low-latency processing of temporal data on\nanalog neuromorphic substrates.",
        "Generalization is the core objective when training optimizers from data.\nHowever, limited training instances often constrain the generalization\ncapability of the trained optimizers. Co-evolutionary approaches address this\nchallenge by simultaneously evolving a parallel algorithm portfolio (PAP) and\nan instance population to eventually obtain PAPs with good generalization. Yet,\nwhen applied to a specific problem class, these approaches have a major\nlimitation. They require practitioners to provide instance generators specially\ntailored to the problem class, which is often non-trivial to design. This work\nproposes a general-purpose, off-the-shelf PAP construction approach, named\ndomain-agnostic co-evolution of parameterized search (DACE), for binary\noptimization problems where decision variables take values of 0 or 1. The key\ninnovation of DACE lies in its neural network-based domain-agnostic instance\nrepresentation and generation mechanism that delimitates the need for\ndomain-specific instance generators. The strong generality of DACE is validated\nacross three real-world binary optimization problems: the complementary\ninfluence maximization problem (CIMP), the compiler arguments optimization\nproblem (CAOP), and the contamination control problem (CCP). Given only a small\nset of training instances from these classes, DACE, without requiring any\ndomain knowledge, constructs PAPs with better generalization performance than\nexisting approaches on all three classes, despite their use of domain-specific\ninstance generators.",
        "Brain-inspired spiking neural networks (SNNs) have garnered significant\nresearch attention in algorithm design and perception applications. However,\ntheir potential in the decision-making domain, particularly in model-based\nreinforcement learning, remains underexplored. The difficulty lies in the need\nfor spiking neurons with long-term temporal memory capabilities, as well as\nnetwork optimization that can integrate and learn information for accurate\npredictions. The dynamic dendritic information integration mechanism of\nbiological neurons brings us valuable insights for addressing these challenges.\nIn this study, we propose a multi-compartment neuron model capable of\nnonlinearly integrating information from multiple dendritic sources to\ndynamically process long sequential inputs. Based on this model, we construct a\nSpiking World Model (Spiking-WM), to enable model-based deep reinforcement\nlearning (DRL) with SNNs. We evaluated our model using the DeepMind Control\nSuite, demonstrating that Spiking-WM outperforms existing SNN-based models and\nachieves performance comparable to artificial neural network (ANN)-based world\nmodels employing Gated Recurrent Units (GRUs). Furthermore, we assess the\nlong-term memory capabilities of the proposed model in speech datasets,\nincluding SHD, TIMIT, and LibriSpeech 100h, showing that our multi-compartment\nneuron model surpasses other SNN-based architectures in processing long\nsequences. Our findings underscore the critical role of dendritic information\nintegration in shaping neuronal function, emphasizing the importance of\ncooperative dendritic processing in enhancing neural computation.",
        "Spiking Neural Networks (SNNs) hold promise for energy-efficient,\nbiologically inspired computing. We identify substantial informatio loss during\nspike transmission, linked to temporal dependencies in traditional Leaky\nIntegrate-and-Fire (LIF) neuron-a key factor potentially limiting SNN\nperformance. Existing SNN architectures also underutilize modern GPUs,\nconstrained by single-bit spike storage and isolated weight-spike operations\nthat restrict computational efficiency. We introduce ${SpikePack}$, a neuron\nmodel designed to reduce transmission loss while preserving essential features\nlike membrane potential reset and leaky integration. ${SpikePack}$ achieves\nconstant $\\mathcal{O}(1)$ time and space complexity, enabling efficient\nparallel processing on GPUs and also supporting serial inference on existing\nSNN hardware accelerators. Compatible with standard Artificial Neural Network\n(ANN) architectures, ${SpikePack}$ facilitates near-lossless ANN-to-SNN\nconversion across various networks. Experimental results on tasks such as image\nclassification, detection, and segmentation show ${SpikePack}$ achieves\nsignificant gains in accuracy and efficiency for both directly trained and\nconverted SNNs over state-of-the-art models. Tests on FPGA-based platforms\nfurther confirm cross-platform flexibility, delivering high performance and\nenhanced sparsity. By enhancing information flow and rethinking SNN-ANN\nintegration, ${SpikePack}$ advances efficient SNN deployment across diverse\nhardware platforms.",
        "Presented study introduces a novel distributed cloud-edge framework for\nautonomous multi-UAV systems that combines the computational efficiency of\nneuromorphic computing with nature-inspired control strategies. The proposed\narchitecture equips each UAV with an individual Spiking Neural Network (SNN)\nthat learns to reproduce optimal control signals generated by a cloud-based\ncontroller, enabling robust operation even during communication interruptions.\nBy integrating spike coding with nature-inspired control principles inspired by\nTilapia fish territorial behavior, our system achieves sophisticated formation\ncontrol and obstacle avoidance in complex urban environments. The distributed\narchitecture leverages cloud computing for complex calculations while\nmaintaining local autonomy through edge-based SNNs, significantly reducing\nenergy consumption and computational overhead compared to traditional\ncentralized approaches. Our framework addresses critical limitations of\nconventional methods, including the dependency on pre-modeled environments,\ncomputational intensity of traditional methods, and local minima issues in\npotential field approaches. Simulation results demonstrate the system's\neffectiveness across two different scenarios. First, the indoor deployment of a\nmulti-UAV system made-up of 15 UAVs. Then the collision-free formation control\nof a moving UAV flock including 6 UAVs considering the obstacle avoidance.\nOwing to the sparsity of spiking patterns, and the event-based nature of SNNs\nin average for the whole group of UAVs, the framework achieves almost 90%\nreduction in computational burden compared to traditional von Neumann\narchitectures implementing traditional artificial neural networks.",
        "The running-time analysis of evolutionary combinatorial optimization is a\nfundamental topic in evolutionary computation. Its current research mainly\nfocuses on specific algorithms for simplified problems due to the challenge\nposed by fluctuating fitness values. This paper proposes a multiple-gain model\nto estimate the fitness trend of population during iterations. The proposed\nmodel is an improved version of the average gain model, which is the approach\nto estimate the running time of evolutionary algorithms for numerical\noptimization. The improvement yields novel results of evolutionary\ncombinatorial optimization, including a briefer proof for the time complexity\nupper bound in the case of (1+1) EA for the Onemax problem, two tighter time\ncomplexity upper bounds than the known results in the case of (1+$\\lambda$) EA\nfor the knapsack problem with favorably correlated weights and a closed-form\nexpression of time complexity upper bound in the case of (1+$\\lambda$) EA for\ngeneral $k$-MAX-SAT problems. The results indicate that the practical running\ntime aligns with the theoretical results, verifying that the multiple-gain\nmodel is more general for running-time analysis of evolutionary combinatorial\noptimization than state-of-the-art methods.",
        "The ITkPixV2 chip is the final production readout chip for the ATLAS Phase 2\nInner Tracker (ITk) upgrade at the upcoming High-Luminosity LHC (HL-LHC). Due\nto the extraordinarily high peak luminosity at the HL-LHC of $5 \\times 10^{34}$\ncm$^{-1}$s$^{-1}$, ITkPixV2 must meet significant increases in nearly all\ndesign requirements, including a 10x increase in trigger rate, a 7.5x increase\nin hit rate, a 3x increase in radiation tolerance, and a 12.5x decrease in\npixel current draw per unit area, all while maintaining a similar power per\nunit area as present pixel detectors. Here we present the first measurements of\nthe ITkPixV2 chip operated at the limits of the full chip design requirements,\nincluding in particular a measurement of the activity-induced current of the\nchip as a function of increasing hit rate.",
        "A unified framework is proposed to quantitatively characterize pitchfork\nbifurcations and associated symmetry breaking in the elliptic restricted\nthree-body problem (ERTBP). It is known that planar\/vertical Lyapunov orbits\nand Lissajous orbits near the collinear libration points undergo pitchfork\nbifurcations with varying orbital energy. These bifurcations induce symmetry\nbreaking, generating bifurcated families including halo\/quasi-halo orbits,\naxial\/quasi-axial orbits, and their corresponding invariant manifolds.\nTraditional semi-analytical methods for constructing halo orbits, based on\nresonant bifurcation mechanisms, have obstacles in fully exploiting the\nintrinsic symmetry breaking characteristics in pitchfork bifurcations. In this\npaper, we propose a unified trigonometric series-based framework to analyze\nthese bifurcated families from the perspective of coupling-induced bifurcation\nmechanisms. By introducing a coupling coefficient and various bifurcation\nequations into the ERTBP, different symmetry breaking is achieved when the\ncoupling coefficient is non-zero. This unified semi-analytical framework\ncaptures bifurcations of both periodic\/quasi-periodic and transit\/non-transit\norbits. Furthermore, it reveals that pitchfork bifurcation solutions in the\nERTBP fundamentally depend solely on the orbital eccentricity and three\namplitude parameters of the system's degrees of freedom, governing both the\nelliptic direction and the hyperbolic one.",
        "The design of localization systems for small-scale flapping-wing aerial\nrobots faces relevant challenges caused by the limited payload and onboard\ncomputational resources. This paper presents an ultra-wideband localization\nsystem particularly designed for small-scale flapping-wing robots. The solution\nrelies on custom 5 grams ultra-wideband sensors and provides robust, very\nefficient (in terms of both computation and energy consumption), and accurate\n(mean error of 0.28 meters) 3D position estimation. We validate our system\nusing a Flapper Nimble+ flapping-wing robot.",
        "We introduce Point-LN, a novel lightweight framework engineered for efficient\n3D point cloud classification. Point-LN integrates essential non-parametric\ncomponents-such as Farthest Point Sampling (FPS), k-Nearest Neighbors (k-NN),\nand non-learnable positional encoding-with a streamlined learnable classifier\nthat significantly enhances classification accuracy while maintaining a minimal\nparameter footprint. This hybrid architecture ensures low computational costs\nand rapid inference speeds, making Point-LN ideal for real-time and\nresource-constrained applications. Comprehensive evaluations on benchmark\ndatasets, including ModelNet40 and ScanObjectNN, demonstrate that Point-LN\nachieves competitive performance compared to state-of-the-art methods, all\nwhile offering exceptional efficiency. These results establish Point-LN as a\nrobust and scalable solution for diverse point cloud classification tasks,\nhighlighting its potential for widespread adoption in various computer vision\napplications.",
        "Tomographic investigations are a central tool in medical applications,\nallowing doctors to image the interior of patients. The corresponding\nmeasurement process is commonly modeled by the Radon transform. In practice,\nthe solution of the tomographic problem requires discretization of the Radon\ntransform and its adjoint (called the backprojection). There are various\ndiscretization schemes; often structured around three discretization\nparameters: spatial-, detector-, and angular resolutions. The most widespread\napproach uses the ray-driven Radon transform and the pixel-driven\nbackprojection in a balanced resolution setting, i.e., the spatial resolution\nroughly equals the detector resolution. The use of these particular\ndiscretization approaches is based on anecdotal reports of their approximation\nperformance, but there is little rigorous analysis of these methods'\napproximation errors. This paper presents a novel interpretation of ray-driven\nand pixel-driven methods as convolutional discretizations, illustrating that\nfrom an abstract perspective these methods are similar. Moreover, we announce\nstatements concerning the convergence of the ray-driven Radon transform and the\npixel-driven backprojection under balanced resolutions. Our considerations are\nsupported by numerical experiments highlighting aspects of the discussed\nmethods.",
        "Efficient material logistics play a critical role in controlling costs and\nschedules in the construction industry. However, manual material handling\nremains prone to inefficiencies, delays, and safety risks. Autonomous forklifts\noffer a promising solution to streamline on-site logistics, reducing reliance\non human operators and mitigating labor shortages. This paper presents the\ndevelopment and evaluation of the Autonomous Dynamic All-terrain Pallet\nTransporter (ADAPT), a fully autonomous off-road forklift designed for\nconstruction environments. Unlike structured warehouse settings, construction\nsites pose significant challenges, including dynamic obstacles, unstructured\nterrain, and varying weather conditions. To address these challenges, our\nsystem integrates AI-driven perception techniques with traditional approaches\nfor decision making, planning, and control, enabling reliable operation in\ncomplex environments. We validate the system through extensive real-world\ntesting, comparing its long-term performance against an experienced human\noperator across various weather conditions. We also provide a comprehensive\nanalysis of challenges and key lessons learned, contributing to the advancement\nof autonomous heavy machinery. Our findings demonstrate that autonomous outdoor\nforklifts can operate near human-level performance, offering a viable path\ntoward safer and more efficient construction logistics.",
        "We relate the expected hyperbolic length of the perimeter of the convex hull\nof the trajectory of Brownian motion in the hyperbolic plane to an expectation\nof a certain exponential functional of a one-dimensional real-valued Brownian\nmotion, and hence derive small- and large-time asymptotics for the expected\nhyperbolic perimeter. In contrast to the case of Euclidean Brownian motion with\nnon-zero drift, the large-time asymptotics are a factor of two greater than the\nlower bound implied by the fact that the convex hull includes the hyperbolic\nline segment from the origin to the endpoint of the hyperbolic Brownian motion.\nWe also obtain an exact expression for the expected perimeter length after an\nindependent exponential random time.",
        "Rapid response to natural disasters such as earthquakes is a crucial element\nin ensuring the safety of civil infrastructures and minimizing casualties.\nTraditional manual inspection is labour-intensive, time-consuming, and can be\ndangerous for inspectors and rescue workers. This paper proposed an autonomous\ninspection approach for structural damage inspection and survivor detection in\nthe post-disaster building indoor scenario, which incorporates an autonomous\nnavigation method, deep learning-based damage and survivor detection method,\nand a customized low-cost micro aerial vehicle (MAV) with onboard sensors.\nExperimental studies in a pseudo-post-disaster office building have shown the\nproposed methodology can achieve high accuracy in structural damage inspection\nand survivor detection. Overall, the proposed inspection approach shows great\npotential to improve the efficiency of existing manual post-disaster building\ninspection.",
        "Let $p$ be an odd prime, and $\\mathbf{Q}_{p^f}$ the unramified extension of\n$\\mathbf{Q}_p$ of degree $f$. In this paper, we reduce the problem of\nconstructing strongly divisible modules for $2$-dimensional semi-stable\nnon-crystalline representations of\n$\\mathrm{Gal}(\\overline{\\mathbf{Q}}_p\/\\mathbf{Q}_{p^f})$ with Hodge--Tate\nweights in the Fontaine--Laffaille range to solving systems of linear equations\nand inequalities. We also determine the Breuil modules corresponding to the\nmod-$p$ reduction of the strongly divisible modules. We expect our method to\nproduce at least one Galois-stable lattice in each such representation for\ngeneral $f$. Moreover, when the mod-$p$ reduction is an extension of distinct\ncharacters, we further expect our method to provide the two non-homothetic\nlattices. As applications, we show that our approach recovers previously known\nresults for $f=1$ and determine the mod-$p$ reduction of the semi-stable\nrepresentations with some small Hodge--Tate weights when $f=2$.",
        "Interfacing light from solid-state single-photon sources with scalable and\nrobust room-temperature quantum memories has been a long-standing challenge in\nphotonic quantum information technologies due to inherent noise processes and\ntime-scale mismatches between the operating conditions of solid-state and\natomic systems. Here, we demonstrate on-demand storage and retrieval of single\nphotons from a semiconductor quantum dot device in a room-temperature atomic\nvapor memory. A deterministically fabricated InGaAs quantum dot light source\nemits single photons at the wavelength of the cesium D1 line at 895\\,nm which\nexhibit an inhomogeneously broadened linewidth of 5.1(7)\\,GHz and are\nsubsequently stored in a low-noise ladder-type cesium vapor memory. We show\ncontrol over the interaction between the single photons and the atomic vapor,\nallowing for variable retrieval times of up to 19.8(3)\\,ns at an internal\nefficiency of $\\eta_\\mathrm{int}=0.6(1)\\%$. Our results significantly expand\nthe application space of both room-temperature vapor memories and semiconductor\nquantum dots in future quantum network architectures.",
        "GRS 1758-258 and 1E 1740.7-2942 are two long-known persistent black hole\nbinaries in the Galactic Center region. Using INTEGRAL's extensive monitoring\nof the Galactic Center and Bulge, we studied their temporal and spectral\nevolutions in the 30-610 keV energy range from March 2003 through April 2022\nwith the IBIS\/ISGRI gamma-ray telescope. Our analyses found that the sources\ntypically had Comptonized spectra, though not always with the same parameters.\nThe spectral states with more than 8 Ms of observation time show deviations\nfrom a Comptonized spectrum above ~200 keV or a \"hard tail\" that extends up to\nat least 600 keV. The origin of this component remains debated with the most\npopular scenarios being synchrotron emission from the jet or Comptonization in\na hybrid thermal\/non-thermal plasma. Anyway, the GRS 1758-258 and 1E\n1740.7-2942 spectra are acceptably described by CompTT+po (jet) and Eqpair\n(hybrid Comptonization) scenarios. To differentiate between the two scenarios,\nwe calculated the Spearman correlation coefficient comparing 30-50 keV count\nrates with those in higher energy bands (50-100, 100-300, and 300-600 keV). The\ncount rates below 300 keV are strongly correlated, indicating those photons\narise from the same physical process. Above 300 keV the count rates are either\nanti-correlated or not correlated with the 30-50 keV count rates for GRS\n1758-258, which suggests that the photons originate from a different physical\nprocess. For 1E 1740.7-2942, the level of correlation is unclear due to scatter\nin the data points. However, the 300-600 keV count rates are consistent with a\nconstant value. This disfavors the hybrid Comptonization scenario for both\nsources.",
        "This study specifically investigates the initiation phase of EFL learners'\nengagement with AI tools, focusing on how technology acceptance constructs\nperceived usefulness (PU), perceived ease of use (PEOU), and perceived\nself-efficacy (PSE) influence learning resilience. Drawing on an optimized\nTechnology Acceptance Model (TAM) and integrating constructs from positive\npsychology, the study examines the chain-mediated effects of learning\nmotivation (LM) and metacognitive strategies (MS) on resilience outcomes,\noperationalized through optimism (OP), psychological resilience (PR), and\ngrowth mindset (GM). A survey of first-year English majors (N = 730) was\nconducted, and structural equation modeling was employed to analyze the data.\nThe findings indicate that favorable perceptions of AI tools are significantly\nassociated with enhanced LM and MS, which in turn positively impact resilience\nmeasures. These results suggest that the interplay between technology\nacceptance and internal regulatory processes is vital in shaping EFL learners'\nearly experiences with AI-assisted learning. Practical implications for\neducators and researchers are discussed, with an emphasis on promoting\nuser-friendly and effective AI environments to support the development of\nadaptive learning behaviors.",
        "In this study, the Planck units (mass, time and length) have only been\nderived, explained and attributed a physical meaning when they were deduced\nbased on the concept of interacting membranes (membranes instead of strings of\nstring theory). For this purpose, a set of five assumptions were proposed: (a)\nthe existence of the interacting membranes; (b) the curvatures of the membranes\noscillate according to the classical wave equation; (c) the spatial period of\nthe wave that arise when the membranes oscillate is given by $\\lambda =\n{\\xi}{\\pi}\/k$; (d) the membranes oscillate with wavelength given by de Broglie\nrelation and (e) $x=ct$ holds. The parameter $\\xi$ determines the period of\noscillation of the given membranes. In deriving the Planck units in this work,\n$\\xi$ must take the value 2 and determines a period 2$\\pi$, closely to minimum\nvalue 1 or to fundamental period $\\pi$, respectively. In this context, Planck\nunits must be fundamental. Moreover, the parameter $\\xi$ was reported as a\nunification parameter between the formulas for the Coulomb$^{\\prime}$s law and\nNewton$^{\\prime}$s law of universal gravitation linking the forces of\nmicroworld and macroworld. Depending on the value $\\xi$ takes, one force or\nanother will be had. It is also shown that the potential $V = hc\/{\\xi}{\\pi}x$\ndeduced from the above assumptions and which contributes to deduce the Planck\nunits, can be derived from Yukawa$^{\\prime}$s equation. Hence, the present work\nwould be contributing to theoretical physics, since at the Planck scale\npredictions of some theories like Standard Model, quantum field theory and\ngeneral relativity are not expected to be valid.",
        "We discuss generalized linear models for directional data where the\nconditional distribution of the response is a von Mises-Fisher distribution in\narbitrary dimension or a Bingham distribution on the unit circle. To do this\nproperly, we parametrize von Mises-Fisher distributions by Euclidean parameters\nand investigate computational aspects of this parametrization. Then we modify\nthis approach for local polynomial regression as a means of nonparametric\nsmoothing of distributional data. The methods are illustrated with simulated\ndata and a data set from planetary sciences involving covariate vectors on a\nsphere with axial response.",
        "The carbonic anhydrase II enzyme (CA II) is one of the most significant\nenzymes in nature, reversibly converting CO$_2$ to bicarbonate at a remarkable\nrate. The precise mechanism it uses to achieve this rapid turnover remains\nunclear due to our inability to directly observe or simulate the full process\ndynamically. Here, we use a recently developed universal neural network\npotential (Orb) to simulate the active site of CA II. We reproduce several\nknown features of the reaction mechanism, including the proton relay that\nconducts protons out of the active site to the His64 residue. Additionally, we\nobserve a new reaction pathway where CO$_2$ reacts with a water molecule in the\nactive site, which donates a proton to the zinc-bound hydroxide. This differs\nfrom the established mechanism where CO$_2$ directly reacts with hydroxide.\nExisting experimental data and independent quantum chemistry calculations are\nused to support the plausibility of this new mechanism. This demonstrates the\npotential of Orb to efficiently generate novel insights into important\nmolecular scale processes that can potentially be harnessed to improve CO$_2$\ncapture technologies and drug design."
      ]
    }
  },
  {
    "id":2411.16896,
    "research_type":"applied",
    "start_id":"b27",
    "start_title":"Fast fit-free analysis of fluorescence lifetime imaging via deep learning",
    "start_abstract":"Fluorescence lifetime imaging (FLI) provides unique quantitative information in biomedical and molecular biology studies but relies on complex data-fitting techniques to derive the quantities of interest. Herein, we propose a fit-free approach FLI image formation that is based deep learning (DL) quantify fluorescence decays simultaneously over whole at fast speeds. We report neural network (DNN) architecture, named (FLI-Net) designed trained for different classes experiments, including visible near-infrared (NIR) microscopy (FLIM) NIR gated macroscopy (MFLI). FLI-Net outputs quantitatively spatially resolved lifetime-based parameters are typically employed field. validate utility framework by performing microscopic preclinical across spectra, as well 2 main data acquisition technologies. These results demonstrate suited accurately lifetimes cells and, real time, intact animals without any parameter settings. Hence, paves way reproducible unprecedented speeds, improved dissemination impact many important applications ranging from fundamental discoveries cellular clinical translation.",
    "start_categories":[
      "cs.NE"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b16"
      ],
      "title":[
        "Characterization of fluorescence lifetime of organic fluorophores for molecular imaging in the shortwave infrared window"
      ],
      "abstract":[
        "SignificanceFluorescence lifetime imaging in the shortwave infrared (SWIR) is expected to enable high-resolution multiplexed molecular highly scattering tissue.AimTo characterize brightness and fluorescence of commercially available organic SWIR fluorophores benchmark them against tail emission conventional NIR-excited probes.ApproachCharacterization was performed through our established time-domain mesoscopic tomography system integrated around a time-correlated single-photon counting-single-photon avalanche diode array. Brightness were measured for NIR probes >1000 nm. Simultaneous probe then assess their potential studies.ResultsThe outperformed while mean lifetimes extremely short. The phantom study demonstrated feasibility multiplexing window with both probes.ConclusionsLong-tail Fluorescence readily detectable window, where showed shorter compared probes. We demonstrate which paves way vivo studies intact tissues at improved resolution."
      ],
      "categories":[
        "q-bio.BM"
      ]
    },
    "list":{
      "title":[
        "Engineered Zwitterion-Infused Clay Composites with Antibacterial and\n  Antifungal Efficacy",
        "Applying computational protein design to therapeutic antibody discovery\n  -- current state and perspectives",
        "Non-Markovain Quantum State Diffusion for the Tunneling in SARS-COVID-19\n  virus",
        "Progress of the anti-obesity of Berberine",
        "Inverse problems with experiment-guided AlphaFold",
        "DeepSilencer: A Novel Deep Learning Model for Predicting siRNA Knockdown\n  Efficiency",
        "Deep Learning of Proteins with Local and Global Regions of Disorder",
        "COLOR: A compositional linear operation-based representation of protein\n  sequences for identification of monomer contributions to properties",
        "Prediction of Binding Affinity for ErbB Inhibitors Using Deep Neural\n  Network Model with Morgan Fingerprints as Features",
        "How Large is the Universe of RNA-Like Motifs? A Clustering Analysis of\n  RNA Graph Motifs Using Topological Descriptors",
        "Remodeling Peptide-MHC-TCR Triad Binding as Sequence Fusion for\n  Immunogenicity Prediction",
        "Mechanism of Electricacupuncture Treating Detrusor Bladder Neck\n  Dyscoordination After Suprasacral Spinal Cord Injury by Proteomics",
        "Advances in RNA secondary structure prediction and RNA modifications:\n  Methods, data, and applications",
        "Short Paths in the Planar Graph Product Structure Theorem",
        "Measuring $A_\\text{FB}^b$ and $R_b$ with exclusive $b$-hadron decays at\n  the FCC-ee",
        "Monotonicity and convergence of two-relaxation-times lattice Boltzmann\n  schemes for a non-linear conservation law",
        "Coherent dynamics of flavor mode entangled neutrinos",
        "On the Use of WGANs for Super Resolution in Dark-Matter Simulations",
        "Relationship between the $\\gamma-$ray variability and the pc-scale jet\n  in the blazar 3C 454.3",
        "Gas Perturbations in Hot Smooth Atmospheres X-ray Surface Brightness\n  Fluctuations in Smooth Galaxy Cluster Atmospheres",
        "Causal Inference on Outcomes Learned from Text",
        "Propagation of extreme events in multiplex neuronal networks",
        "The Energy Cascade Rate in Supersonic Magnetohydrodynamic Turbulence",
        "Self-organized institutions in evolutionary dynamical-systems game",
        "Identifying Flare Locations Through Exoplanet Transit Occultations",
        "Effect of 3d Transition Metal Doping (Mn, Fe, Co, Ni) on the Electronic\n  and Magnetic Properties of Pd Alloys at Low Impurity Concentrations: An Ab\n  initio Study",
        "Resonant current from singlet-triplet state mixing in coupled quantum\n  dots",
        "Exchange Rate Sensitivity in Free Zone Trade: An Empirical Study of the\n  Istanbul Ataturk Airport Free Zone"
      ],
      "abstract":[
        "Microbes and pathogens play a detrimental role in healing wounds, causing\ninfections like impetigo through bodily fluids and skin and entering the\nbloodstream through the wounds, thereby hindering the healing process and\ntissue regeneration. Clay, known for its long history of natural therapeutic\nuse, has emerged as one of the most promising candidates for biomedical\napplications due to its non-toxic nature, porosity, high surface area,\nubiquity, and excellent cation exchange capacity. This study demonstrates an\ninnovative approach to engineering an organo-functionalized,\ninfection-resistant, easy-to-use bandage material from clay, an environmentally\nbenign and sustainable material. The hybrid membranes have been developed using\nclays, zwitterions, silver ions, and terbinafine hydrochloride (TBH) to impart\nantibacterial and antifungal efficacy. A critical aspect of this study is\nembedding organic molecules and metal ions with the clays and releasing them to\nresist the growth and kill the pathogens. The antimicrobial efficacy of the\nmembranes has been tested using a zone of inhibition study against the most\ncommon microbes in skin wounds, viz. S. aureus, E. coli, and C. albicans.\nResults from our studies not only demonstrate the potential of these hybrid\nclay membranes as a cost-effective, scalable, and effective solution for\ntreating microbial infections but also instill newer avenues for point-of-care\nwound-healing treatments, offering hope for improved patient outcomes.",
        "Machine learning applications in protein sciences have ushered in a new era\nfor designing molecules in silico. Antibodies, which currently form the largest\ngroup of biologics in clinical use, stand to benefit greatly from this shift.\nDespite the proliferation of these protein design tools, their direct\napplication to antibodies is often limited by the unique structural biology of\nthese molecules. Here, we review the current computational methods for antibody\ndesign, highlighting their role in advancing computational drug discovery.",
        "In the context of biology, unlike the comprehensively established Standard\nModel in physics, many biological processes lack a complete theoretical\nframework and are often described phenomenologically. A pertinent example is\nolfaction -- the process through which humans and animals distinguish various\nodors. The conventional biological explanation for olfaction relies on the lock\nand key model, which, while useful, does not fully account for all observed\nphenomena. As an alternative or complement to this model, vibration-assisted\nelectron tunneling has been proposed. Drawing inspiration from the\nvibration-assisted electron tunneling model for olfaction, we have developed a\ntheoretical model for electron tunneling in SARS-CoV-2 virus infection within a\nnon-Markovian framework. We approach this by solving the non-Markovian quantum\nstochastic Schrodinger equation. In our model, the spike protein and the GPCR\nreceptor are conceptualized as a dimer, utilizing the spin-Boson model to\nfacilitate the description of electron tunneling. Our analysis demonstrates\nthat electron tunneling in this context exhibits inherently non-Markovian\ncharacteristics, extending into the intermediate and strong coupling regimes\nbetween the dimer components. This behavior stands in stark contrast to\npredictions from Markovian models, which fail to accurately describe electron\ntunneling in the strong coupling limit. Notably, Markovian approximations often\nlead to unphysical negative probabilities in this regime, underscoring their\nlimitations and highlighting the necessity of incorporating non-Markovian\ndynamics for a more realistic description of biological quantum processes. This\napproach not only broadens our understanding of viral infection mechanisms but\nalso enhances the biological accuracy and relevance of our theoretical\nframework in describing complex biological interactions.",
        "Obesity is defined as the excessive accumulation or abnormal distribution of\nbody fat. According to data from World Obesity Atlas 2024, the increase in\nprevalence of obesity has become a major worldwide health problem in adults as\nwell as among children and adolescents. Although an increasing number of drugs\nhave been approved for the treatment of obesity in recent years, many of these\ndrugs have inevitable side effects which have increased the demand for new\nsafe, accessible and effective drugs for obesity and prompt interest in natural\nproducts. Berberine (BBR) and its metabolites, known for their multiple\npharmacological effects. Recent studies have emphatically highlighted the\nanti-obesity benefits of BBR and the underlying mechanisms have been gradually\nelucidated. However, its clinical application is limited by poor oral\nabsorption and low bioavailability. Based on this, this review summarizes\ncurrent research on the anti-obesity effects of BBR and its metabolites,\nincluding advancements in clinical trail results, understanding potential\nmolecular mechanisms and absorption and bioavailability. As a natural compound\nderived from plants, BBR holds potential as an alternative approach for\nmanaging obesity.",
        "Proteins exist as a dynamic ensemble of multiple conformations, and these\nmotions are often crucial for their functions. However, current structure\nprediction methods predominantly yield a single conformation, overlooking the\nconformational heterogeneity revealed by diverse experimental modalities. Here,\nwe present a framework for building experiment-grounded protein structure\ngenerative models that infer conformational ensembles consistent with measured\nexperimental data. The key idea is to treat state-of-the-art protein structure\npredictors (e.g., AlphaFold3) as sequence-conditioned structural priors, and\ncast ensemble modeling as posterior inference of protein structures given\nexperimental measurements. Through extensive real-data experiments, we\ndemonstrate the generality of our method to incorporate a variety of\nexperimental measurements. In particular, our framework uncovers previously\nunmodeled conformational heterogeneity from crystallographic densities, and\ngenerates high-accuracy NMR ensembles orders of magnitude faster than the\nstatus quo. Notably, we demonstrate that our ensembles outperform AlphaFold3\nand sometimes better fit experimental data than publicly deposited structures\nto the Protein Data Bank (PDB). We believe that this approach will unlock\nbuilding predictive models that fully embrace experimentally observed\nconformational diversity.",
        "Background: Small interfering RNA (siRNA) is a promising therapeutic agent\ndue to its ability to silence disease-related genes via RNA interference. While\ntraditional machine learning and early deep learning methods have made progress\nin predicting siRNA efficacy, there remains significant room for improvement.\nAdvanced deep learning techniques can enhance prediction accuracy, reducing the\nreliance on extensive wet-lab experiments and accelerating the identification\nof effective siRNA sequences. This approach also provides deeper insights into\nthe mechanisms of siRNA efficacy, facilitating more targeted and efficient\ntherapeutic strategies.\n  Methods: We introduce DeepSilencer, an innovative deep learning model\ndesigned to predict siRNA knockdown efficiency. DeepSilencer utilizes advanced\nneural network architectures to capture the complex features of siRNA\nsequences. Our key contributions include a specially designed deep learning\nmodel, an innovative online data sampling method, and an improved loss function\ntailored for siRNA prediction. These enhancements collectively boost the\nmodel's prediction accuracy and robustness.\n  Results: Extensive evaluations on multiple test sets demonstrate that\nDeepSilencer achieves state-of-the-art performance using only siRNA sequences\nand basic physicochemical properties. Our model surpasses several other methods\nand shows superior predictive performance, particularly when incorporating\nthermodynamic parameters.\n  Conclusion: The advancements in data sampling, model design, and loss\nfunction significantly enhance the predictive capabilities of DeepSilencer.\nThese improvements underscore its potential to advance RNAi therapeutic design\nand development, offering a powerful tool for researchers and clinicians.",
        "Although machine learning has transformed protein structure prediction of\nfolded protein ground states with remarkable accuracy, intrinsically disordered\nproteins and regions (IDPs\/IDRs) are defined by diverse and dynamical\nstructural ensembles that are predicted with low confidence by algorithms such\nas AlphaFold. We present a new machine learning method, IDPForge (Intrinsically\nDisordered Protein, FOlded and disordered Region GEnerator), that exploits a\ntransformer protein language diffusion model to create all-atom IDP ensembles\nand IDR disordered ensembles that maintains the folded domains. IDPForge does\nnot require sequence-specific training, back transformations from\ncoarse-grained representations, nor ensemble reweighting, as in general the\ncreated IDP\/IDR conformational ensembles show good agreement with solution\nexperimental data, and options for biasing with experimental restraints are\nprovided if desired. We envision that IDPForge with these diverse capabilities\nwill facilitate integrative and structural studies for proteins that contain\nintrinsic disorder.",
        "The properties of biological materials like proteins and nucleic acids are\nlargely determined by their primary sequence. While certain segments in the\nsequence strongly influence specific functions, identifying these segments, or\nso-called motifs, is challenging due to the complexity of sequential data.\nWhile deep learning (DL) models can accurately capture sequence-property\nrelationships, the degree of nonlinearity in these models limits the assessment\nof monomer contributions to a property - a critical step in identifying key\nmotifs. Recent advances in explainable AI (XAI) offer attention and\ngradient-based methods for estimating monomeric contributions. However, these\nmethods are primarily applied to classification tasks, such as binding site\nidentification, where they achieve limited accuracy (40-45%) and rely on\nqualitative evaluations. To address these limitations, we introduce a DL model\nwith interpretable steps, enabling direct tracing of monomeric contributions.\nWe also propose a metric ($\\mathcal{I}$), inspired by the masking technique in\nthe field of image analysis and natural language processing, for quantitative\nanalysis on datasets mainly containing distinct properties of anti-cancer\npeptides (ACP), antimicrobial peptides (AMP), and collagen. Our model exhibits\n22% higher explainability, pinpoints critical motifs (RRR, RRI, and RSS) that\nsignificantly destabilize ACPs, and identifies motifs in AMPs that are 50% more\neffective in converting non-AMPs to AMPs. These findings highlight the\npotential of our model in guiding mutation strategies for designing\nprotein-based biomaterials.",
        "The ErbB receptor family, including EGFR and HER2, plays a crucial role in\ncell growth and survival and is associated with the progression of various\ncancers such as breast and lung cancer. In this study, we developed a deep\nlearning model to predict the binding affinity of ErbB inhibitors using\nmolecular fingerprints derived from SMILES representations. The SMILES\nrepresentations for each ErbB inhibitor were obtained from the ChEMBL database.\nWe first generated Morgan fingerprints from the SMILES strings and applied\nAutoDock Vina docking to calculate the binding affinity values. After filtering\nthe dataset based on binding affinity, we trained a deep neural network (DNN)\nmodel to predict binding affinity values from the molecular fingerprints. The\nmodel achieved significant performance, with a Mean Squared Error (MSE) of\n0.2591, Mean Absolute Error (MAE) of 0.3658, and an R-squared value of 0.9389\non the training set. Although performance decreased slightly on the test set (R\nsquared = 0.7731), the model still demonstrated robust generalization\ncapabilities. These results indicate that the deep learning approach is highly\neffective for predicting the binding affinity of ErbB inhibitors, offering a\nvaluable tool for virtual screening and drug discovery.",
        "We introduce a computational topology-based approach with unsupervised\nmachine-learning algorithms to estimate the database size and content of\nRNA-like graph topologies. Specifically, we apply graph theory enumeration to\ngenerate all 110,667 possible 2D dual graphs for vertex numbers ranging from 2\nto 9. Among them, only 0.11% graphs correspond to approximately 200,000 known\nRNA atomic fragments (collected in 2021) using the RNA-as-Graphs (RAG) mapping\nmethod. The remaining 99.89% of the dual graphs may be RNA-like or\nnon-RNA-like. To determine which dual graphs in the 99.89% hypothetical set are\nmore likely to be associated with RNA structures, we apply computational\ntopology descriptors using the Persistent Spectral Graphs (PSG) method to\ncharacterize each graph using 19 PSG-based features and use clustering\nalgorithms that partition all possible dual graphs into two clusters, RNA-like\ncluster and non-RNA-like cluster. The distance of each dual graph to the center\nof the RNA-like cluster represents the likelihood of it belonging to RNA\nstructures. From validation, our PSG-based RNA-like cluster includes 97.3% of\nthe 121 known RNA dual graphs, suggesting good performance. Furthermore,\n46.017% of the hypothetical RNAs are predicted to be RNA-like. Significantly,\nwe observe that all the top 15 RNA-like dual graphs can be separated into\nmultiple subgraphs, whereas the top 15 non-RNA-like dual graphs tend not to\nhave any subgraphs. Moreover, a significant topological difference between top\nRNA-like and non-RNA-like graphs is evident when comparing their topological\nfeatures. These findings provide valuable insights into the size of the RNA\nmotif universe and RNA design strategies, offering a novel framework for\npredicting RNA graph topologies and guiding the discovery of novel RNA motifs,\nperhaps anti-viral therapeutics by subgraph assembly.",
        "The complex nature of tripartite peptide-MHC-TCR interactions is a critical\nyet underexplored area in immunogenicity prediction. Traditional studies on\nTCR-antigen binding have not fully addressed the complex dependencies in triad\nbinding. In this paper, we propose new modeling approaches for these tripartite\ninteractions, utilizing sequence information from MHCs, peptides, and TCRs. Our\nmethods adhere to native sequence forms and align with biological processes to\nenhance prediction accuracy. By incorporating representation learning\ntechniques, we introduce a fusion mechanism to integrate the three sequences\neffectively. Empirical experiments show that our models outperform traditional\nmethods, achieving a 2.8 to 13.3 percent improvement in prediction accuracy\nacross existing benchmarks. We further validate our approach with extensive\nablation studies, demonstrating the effectiveness of the proposed model\ncomponents. The model implementation, code, and supplementary materials,\nincluding a manuscript with colored hyperlinks and a technical appendix for\ndigital viewing, will be open-sourced upon publication.",
        "Objectives This study aimed to elucidate the potential mechanisms of\nelectroacupuncture (EA) in restoring detrusor-bladder neck dyssynergesia (DBND)\nfollowing suprasacral spinal cord injury.\n  Methods A total of 52 adult female Sprague-Dawley rats were randomly assigned\nto either a sham group (n=12) or a spinal cord injury model group (n=40). In\nthe model group, DBND was induced in 40 rats through Hassan Shaker spinal cord\ntransection, with 24 rats surviving spinal shock and subsequently randomized\ninto two groups: a model-only group (DBND, n=12) and an EA intervention group\n(DBND+EA, n=12). DBND+EA was administered at Ciliao (BL32), Zhongji (RN3), and\nSanyinjiao (SP6) acupoints, for 20 minutes per session, once daily for 10\nconsecutive days. On day 29 post-injury, all rats underwent urodynamic\nassessments, followed by hematoxylin and eosin (HE) staining, tandem mass tag\n(TMT) proteomics, and Western blot (WB) analysis of the detrusor and bladder\nneck tissues.\n  Results Urodynamic evaluation demonstrated that EA intervention enhanced\nbladder function in DBND rats. HE staining indicated reduced fibroplasia in the\ndetrusor muscle and alleviated inflammation in the bladder neck following EA.\nTMT proteomic analysis revealed 30 differentially expressed proteins (DEPs) in\nthe detrusor and 59 DEPs in the bladder neck post-EA treatment. WB results\ncorroborated these TMT findings.\n  Conclusion EA effectively promotes synergy between the detrusor muscle and\nbladder neck in DBND, likely by enhancing detrusor contractility and\nfacilitating bladder neck relaxation during urination. This study provides\nmechanistic insights into the therapeutic role of EA in managing DBND.",
        "Due to the hierarchical organization of RNA structures and their pivotal\nroles in fulfilling RNA functions, the formation of RNA secondary structure\ncritically influences many biological processes and has thus been a crucial\nresearch topic. This review sets out to explore the computational prediction of\nRNA secondary structure and its connections to RNA modifications, which have\nemerged as an active domain in recent years. We first examine the progression\nof RNA secondary structure prediction methodology, focusing on a set of\nrepresentative works categorized into thermodynamic, comparative, machine\nlearning, and hybrid approaches. Next, we survey the advances in RNA\nmodifications and computational methods for identifying RNA modifications,\nfocusing on the prominent modification types. Subsequently, we highlight the\ninterplay between RNA modifications and secondary structures, emphasizing how\nmodifications such as m6A dynamically affect RNA folding and vice versa. In\naddition, we also review relevant data sources and provide a discussion of\ncurrent challenges and opportunities in the field. Ultimately, we hope our\nreview will be able to serve as a cornerstone to aid in the development of\ninnovative methods for this emerging topic and foster therapeutic applications\nin the future.",
        "The Planar Graph Product Structure Theorem of Dujmovi\\'c et al. [J. ACM '20]\nsays that every planar graph $G$ is contained in $H\\boxtimes P\\boxtimes K_3$\nfor some planar graph $H$ with treewidth at most 3 and some path $P$. This\nresult has been the key to solving several old open problems. Several people\nhave asked whether the Planar Graph Product Structure Theorem can be proved\nwith good upper bounds on the length of $P$. No $o(n)$ upper bound was\npreviously known for $n$-vertex planar graphs. We answer this question in the\naffirmative, by proving that for any $\\epsilon\\in (0,1)$ every $n$-vertex\nplanar graph is contained in $H\\boxtimes P\\boxtimes K_{O(1\/\\epsilon)}$, for\nsome planar graph $H$ with treewidth 3 and for some path $P$ of length\n$O(\\frac{1}{\\epsilon}n^{(1+\\epsilon)\/2})$. This bound is almost tight since\nthere is a lower bound of $\\Omega(n^{1\/2})$ for certain $n$-vertex planar\ngraphs. In fact, we prove a stronger result with $P$ of length\n$O(\\frac{1}{\\epsilon}\\,\\textrm{tw}(G)\\,n^{\\epsilon})$, which is tight up to the\n$O(\\frac{1}{\\epsilon}\\,n^{\\epsilon})$ factor for every $n$-vertex planar graph\n$G$. Finally, taking $\\epsilon=\\frac{1}{\\log n}$, we show that every $n$-vertex\nplanar graph $G$ is contained in $H\\boxtimes P\\boxtimes K_{O(\\log n)}$ for some\nplanar graph $H$ with treewidth at most 3 and some path $P$ of length\n$O(\\textrm{tw}(G)\\,\\log n)$. This result is particularly attractive since the\ntreewidth of the product $H\\boxtimes P\\boxtimes K_{O(\\log n)}$ is within a\n$O(\\log^2n)$ factor of the treewidth of $G$.",
        "This paper presents a novel tagging technique to measure the beauty-quark\npartial decay-width ratio $R_b$ and its forward-backward asymmetry\n$A_\\text{FB}^b$ at the FCC-ee, using $\\mathcal{O}(10^{12})$ $Z$-boson decays.\nThe method is based on the exclusive reconstruction of a selected list of\n$b$-hadron decay modes in $Z\\to b\\bar{b}$ events at the $Z$ pole, which can\nprovide the flavour and possibly the charge of the hemisphere. This approach\neffectively eliminates the contamination from light-quark physics events and\nreduces the leading systematic uncertainties arising from background\ncontamination, tagging-efficiency correlations, and gluon-radiation corrections\nby exploiting the geometric and kinematic properties of beauty hadrons. This\nresults in a total relative uncertainty of the order of $0.01\\,\\%$ for both\nobservables. Furthermore, this precision allows to obtain a commensurate\nprecision on the weak mixing angle $\\sin^2(\\theta_W^\\text{eff})$ compared to\nthe muon forward-backward asymmetry on the order of $0.002\\,\\%$.",
        "We address the convergence analysis of lattice Boltzmann methods for scalar\nnon-linear conservation laws, focusing on two-relaxation-times (TRT) schemes.\nUnlike Finite Difference\/Finite Volume methods, lattice Boltzmann schemes offer\nexceptional computational efficiency and parallelization capabilities. However,\ntheir monotonicity and $L^{\\infty}$-stability remain underexplored. Extending\nexisting results on simpler BGK schemes, we derive conditions ensuring that TRT\nschemes are monotone and stable by leveraging their unique relaxation\nstructure. Our analysis culminates in proving convergence of the numerical\nsolution to the weak entropy solution of the conservation law. Compared to BGK\nschemes, TRT schemes achieve reduced numerical diffusion while retaining\nprovable convergence. Numerical experiments validate and illustrate the\ntheoretical findings.",
        "As the lynchpin of all quantum correlations, quantum coherence is fundamental\nfor distinguishing quantum systems from classical ones and is essential for\nrealizing quantum advantages in areas such as computation, communication, and\nmetrology. In this study, we investigate the relationship between quantum\ncoherence and neutrino oscillations by mapping the neutrino state as a\nmulti-mode quantum system into qubit and qutrit frameworks. Our analysis\nextends beyond the commonly used $l_1$-norm and relative entropy of coherence\nto include all relevant measures of coherence such as robustness of coherence,\ncoherence concurrence, trace-norm distance measure of coherence, coherence of\nformation, Schatten-$p$-norm-based functionals, geometric coherence and\nlogarithmic coherence rank, each offering unique insights into the quantum\ncorrelations in these systems. Notably, while the $l_1$-norm and relative\nentropy-based measures apply to general quantum states, the other measures are\nparticularly relevant for entangled systems, highlighting the critical role of\nentanglement in neutrino oscillations. We present a detailed methodology for\ncalculating coherence measures in both two-flavor and three-flavor mixing\nscenarios, contributing to a deeper understanding of how quantum coherence\nmanifests and evolves in mode-entangled neutrino systems. Our findings\nemphasize the potential of these systems as robust candidates for quantum\ninformation tasks, facilitated by the weak interaction nature of neutrinos.",
        "Super-resolution techniques have the potential to reduce the computational\ncost of cosmological and astrophysical simulations. This can be achieved by\nenabling traditional simulation methods to run at lower resolution and then\nefficiently computing high-resolution data corresponding to the simulated\nlow-resolution data. In this work, we investigate the application of a\nWasserstein Generative Adversarial Network (WGAN) to increase the particle\nresolution of dark-matter-only simulations, reproducing and building on prior\nresults. Our WGAN models successfully generate high-resolution data with\nsummary statistics, including the power spectrum and halo mass function, that\nclosely match those of true high-resolution simulations. We also identify a\nlimitation of the WGAN model in the form of smeared features in the generated\nhigh-resolution snapshots, particularly in the shapes of dark-matter halos.",
        "3C 454.3 is a flat spectrum radio quasar (FSRQ) known for its high\nvariability across the electromagnetic spectrum, showing structural and flux\nvariability in its pc-scale jet, and correlated variability among frequency\nbands. This study aims to identify the structure, dynamics, and radiative\nprocesses common to the innermost regions of the blazar 3C 454.3. We\ninvestigate whether any jet component can be associated with $\\gamma-$ray\nemission and variability. We analyze the relationship between the variable\n$\\gamma-$ray emission and pc-scale jet properties in 3C 454.3 by combining\n$\\gamma-$ray data spanning twelve years with contemporaneous VLBA multi-epoch\nimages at 15 and 43 GHz. Spearman rank correlation tests are conducted to\ndetermine if the flux variability of any jet component is associated with\n$\\gamma-$ray variability. Core emission at 43 and 15 GHz strongly correlates\nwith $\\gamma-$ray emission. The 43 GHz core (Q0) contributes around 37$\\%$ of\nthe observed $\\gamma-$ray variability, while the 15 GHz core (K0) accounts for\n30$\\%$. A quasi-stationary component at 43 GHz, at a projected distance of 4.6\npc, correlates with the $\\gamma-$ray flux, accounting for 20$\\%$ of its\nemission between 2016 and 2021. We found a mobile component (Q3 between 2010.18\nand 2011.16) at 43 GHz with a projected distance between 0.8 and 2.3 pc and\napparent velocity of $\\beta_{app} = 9.9 \\pm 1.1$ c, accounting for\napproximately 28% of the $\\gamma-$ray emission. The observed simultaneous\nvariability in emission regions beyond the central parsec strongly suggests\nsynchrotron self-Compton (SSC) as the primary mechanism for $\\gamma-$ray\nproduction in these regions. Our findings demonstrate the existence of multiple\n$\\gamma-$ray emission regions within the blazar jet but also suggest that some\nof these regions are non-stationary over time.",
        "We measure surface brightness fluctuations in Chandra X-ray images of the\ncores of the galaxy clusters Abell 2029, Abell 2151, Abell 2107, RBS0533, and\nRBS0540. Their relatively structureless X-ray atmospheres exhibit the\nthermodynamic properties of cool cores including short central cooling times\nand low entropy. However, unlike typical cool-core clusters, molecular gas,\nstar formation, and bubbles associated with radio jets are faint or absent near\ntheir central galaxies. Four clusters show typical gas density fluctuation\namplitudes of $\\sim$ 10 per cent on the scales probed, apart from RBS0540,\nwhich exhibits lower amplitudes, suggesting that its gas is mildly disturbed.\nUnder the assumption that gas density fluctuations are indicative of random gas\nvelocities, we estimate scale-dependent velocity amplitudes of gas motions\nacross all studied clusters, which range from 100 km\/s to 200 km\/s in Abell\n2029, Abell 2151, and Abell 2107. These velocity estimates are comparable to\nthe atmospheric velocity dispersion in the Perseus cluster measured by the\nHitomi X-ray Observatory. The turbulent heating rates implied by our\nmeasurements are of the same order as the radiative cooling rates. Our results\nsuggest that atmospheric sloshing and perhaps turbulent motion may aid radio\njets in stabilizing atmospheric cooling.",
        "We propose a machine-learning tool that yields causal inference on text in\nrandomized trials. Based on a simple econometric framework in which text may\ncapture outcomes of interest, our procedure addresses three questions: First,\nis the text affected by the treatment? Second, which outcomes is the effect on?\nAnd third, how complete is our description of causal effects? To answer all\nthree questions, our approach uses large language models (LLMs) that suggest\nsystematic differences across two groups of text documents and then provides\nvalid inference based on costly validation. Specifically, we highlight the need\nfor sample splitting to allow for statistical validation of LLM outputs, as\nwell as the need for human labeling to validate substantive claims about how\ndocuments differ across groups. We illustrate the tool in a proof-of-concept\napplication using abstracts of academic manuscripts.",
        "In previous studies, the propagation of extreme events across nodes in\nmonolayer networks has been extensively studied. In this work, we extend this\ninvestigation to explore the propagation of extreme events between two distinct\nlayers in a multiplex network. We consider a two-layer network, where one layer\nis globally coupled and exhibits extreme events, while the second layer remains\nuncoupled. The interlayer connections between the layers are either\nunidirectional or bidirectional. We find that unidirectional coupling between\nthe layers can induce extreme events in the uncoupled layer, whereas\nbidirectional coupling tends to mitigate extreme events in the globally coupled\nlayer. To characterize extreme and non-extreme states, we use probability plots\nto identify distinct regions in the parameter space. Additionally, we study the\nrobustness of extreme events emergence by examining various network topologies\nin the uncoupled layer. The mechanism behind the occurrence of extreme events\nis explored, with a particular focus on the transition from asynchronous states\nto a fully synchronized excitable state. For numerical simulations, we use\nnonidentical FitzHugh-Nagumo neurons at each node, which captures the dynamical\nbehavior of both coupled and uncoupled layers. Our findings suggest that\nextreme events in the uncoupled layer emerge through the gradual disappearance\nof disorder, accompanied by occasional bursts of synchronized activity. Results\nobtained in this work will serve a starting point in understanding the dynamics\nbehind the propagation of extreme events in real-world networks.",
        "Three-dimensional direct numerical simulations (DNS) are implemented to\ninvestigate the energy cascade rate in compressible isothermal\nmagnetohydrodynamic (MHD) turbulence. Utilizing an exact law derived from the\nK\\'arm\\'an-Howarth equation, we examine the contributions of flux and non-flux\nterms to the cascade rate across a broad range of sonic and Alfv\\'enic Mach\nnumbers, from subsonic to supersonic regimes and varying mean magnetic fields.\nCascade rates are computed using on-grid 3-D decomposition and two plasma\nincrement approaches: signed and absolute values. Anisotropy induced by strong\nmagnetic fields is analyzed through angular-dependent scaling of the cascade\nterms. Moreover, the increment calculation method significantly influences the\nrelative contributions of flux and non-flux terms, with absolute methods\ntending to overestimate the latter. These findings extend current studies of\ncompressible turbulence and offer critical insights into energy transfer\nmechanisms relevant to many astrophysical phenomena.",
        "Social institutions are systems of shared norms and rules that regulate\npeople's behaviors, often emerging without external enforcement. They provide\ncriteria to distinguish cooperation from defection and establish rules to\nsustain cooperation, shaped through long-term trial and error. While principles\nfor successful institutions have been proposed, the mechanisms underlying their\nemergence remain poorly understood. Here, we introduce the evolutionary\ndynamical-systems game, a framework that couples game actions with\nenvironmental dynamics and explores the evolution of cognitive frameworks for\ndecision-making. We analyze a minimal model of common-pool resource management,\nwhere resources grow naturally and are harvested. Players use decision-making\nfunctions to determine whether to harvest at each step, based on environmental\nand peer monitoring. As these functions evolve, players detect selfish\nharvesting and punish it by degrading the environment through harvesting. This\nprocess leads to the self-organization of norms that classify harvesting\nactions as cooperative, defective, or punitive. The emergent norms for\n``cooperativeness'' and rules of punishment serve as institutions. The\nenvironmental and players' states converge to distinct modes characterized by\nlimit-cycles, representing temporal regularities in socio-ecological systems.\nThese modes remain stable despite slight variations in decision-making,\nillustrating the stability of institutions. The evolutionary robustness of\ndecision-making functions serves as a measure of the evolutionary favorability\nof institutions, highlighting the role of plasticity in responding to diverse\nopponents. This work introduces foundational concepts in evolutionary\ndynamical-systems games and elucidates the mechanisms underlying the\nself-organization of institutions by modeling the interplay between ecological\ndynamics and human decision-making.",
        "M dwarfs are the most common stars in the galaxy, with long lifespans, a high\noccurrence rate of rocky planets, and close-in habitable zones. However, high\nstellar activity in the form of frequent flaring and any associated coronal\nmass ejections may drive atmospheric escape with the bombardment of radiation\nand high-energy particles, drastically impacting the habitability of these\nsystems. The stellar latitude where flares and coronal mass ejections occur\ndetermines the space weather that exoplanets are subject to, with high-energy\nparticle events associated with equatorial flares producing significant\natmospheric erosion. However, the flaring latitudes for M dwarfs remain largely\nunconstrained. To aid in the effort to locate these flaring regions we explore\nthe applicability of flare occultations using optical photometry to identify\nthe latitudes of flares. As a planet transits in front of an ongoing flare the\ntiming and geometry of the transit can be used to constrain the latitude and\nlongitude of the flare. We predict the probability of detecting an occultation\nfor known transiting planets and eclipsing binaries. From this, we estimate\n3-22 detectable occultations exist within the TESS primary mission photometry,\nwith the majority occurring in eclipsing binary observations. To demonstrate\nthis technique, we analyze a candidate flare occultation event for the\neclipsing binary CM Draconis.",
        "The nature of low-impurity ferromagnetism remains a challenging problem in\nthe solid-state community. Despite initial experiments dating back to the\nmid-20th century, a comprehensive theoretical explanation and reliable ab\ninitio evaluations have remained elusive. The present research aims to bridge\nthis gap by refining first-principle calculations by elucidating the magnetic\nand electronic behavior of Pd1-xMx alloys (where M = Mn, Fe, Co, Ni). Our study\nincludes calculations of magnetic properties throughout the range of impurity\nconcentrations, from 1 to 100 atomic percent (at.%), where we estimate critical\nconcentrations and perform a comparative analysis for the listed alloys.\nFurthermore, electronic structure was analyzed, including the calculations of\natomic, spin, and orbital-resolved states density, and exploration of the\nspatial formation of magnetic clusters containing ferromagnetic impurities\nacross all concentration ranges.",
        "Electrically driven spin resonances in double quantum dots can lift the spin\nblockade and give rise to a resonant current. This current can probe the\nproperties of coupled two-spin states for different quantum dot configurations.\nUsing a Floquet-Markov quantum transport model we compute the resonant current\nfor different driving amplitudes and ac field frequencies in spin-orbit coupled\nquantum dots. We show that the resonant current has a very rich interference\npattern which can give valuable insight into the singlet-triplet state mixing.",
        "This study as part of an ongoing research effort, empirically examines the\nrelationship between foreign trade in the Istanbul Ataturk Airport Free Zone\nand exchange rate movements. Monthly data from 2003 to 2016 were analyzed\nthrough stationarity tests (Unit Root), followed by the Vector Autoregressive\n(VAR) model, Cointegration Analysis, and the Toda-Yamamoto Causality Test. The\nfindings indicate that the exchange rate does not significantly affect imports\nand exports in the free zone. This result suggests that free zones, due to\ntheir structural characteristics and operational framework, may be relatively\ninsulated from exchange rate fluctuations. The study contributes to the\nliterature by providing a focused analysis of a specific free zone in Turkiye,\nhighlighting the potential independence of free zone trade from exchange rate\nvolatility."
      ]
    }
  },
  {
    "id":2411.16961,
    "research_type":"applied",
    "start_id":"b7",
    "start_title":"Pathology image analysis using segmentation deep learning algorithms",
    "start_abstract":"With the rapid development of image scanning techniques and visualization software, whole slide imaging (WSI) is becoming a routine diagnostic method. Accelerating clinical diagnosis from pathology images and automating image analysis efficiently and accurately remain significant challenges. Recently, deep learning algorithms have shown great promise in pathology image analysis, such as in tumor region identification, metastasis detection, and patient prognosis. Many machine learning algorithms, including convolutional neural networks, have been proposed to automatically segment pathology images. Among these algorithms, segmentation deep learning algorithms such as fully convolutional networks stand out for their accuracy, computational efficiency, and generalizability. Thus, deep learning\u2013based pathology image segmentation has become an important tool in WSI analysis. In this review, the pathology image segmentation process using deep learning algorithms is described in detail. The goals are to provide quick guidance for implementing deep learning into pathology image analysis and to provide some potential ways of further improving segmentation performance. Although there have been previous reviews on using machine learning methods in digital pathology image analysis, this is the first in-depth review of the applications of deep learning algorithms for segmentation in WSI analysis.",
    "start_categories":[
      "cs.NE"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b9"
      ],
      "title":[
        "Comparative gene expression profiles of intestinal transporters in mice, rats and humans"
      ],
      "abstract":[
        "We have studied gene expression profiles of intestinal transporters in model animals and humans. Total RNA was isolated from duodenum and the mRNA expression was measured using Affymetrix GeneChip oligonucleotide arrays. Detected genes from the intestine of mice, rats, and humans were about 60% of 22,690 sequences, 40% of 8739, and 47% of 12,559, respectively. A total of 86 genes involving transporters expressed in mice, 50 genes in rats, and 61 genes in humans were detected. Mice exhibited abundant mRNA expressions for peptide transporter HPT1, amino acid transporters CSNU3, CT1 and ASC1, nucleoside transporter CNT2, organic cation transporter SFXN1, organic anion transporter NBC3, glucose transporter SGLT1, and fatty acid transporters FABP1 and FABP2. Rats showed high expression profiles of peptide transporter PEPT1, amino acid transporters CSNU1 and 4F2HC, nucleoside transporter CNT2, organic cation transporter OCT5, organic anion transporter SDCT1, glucose transporter GLUT2 and GLUT5, and folate carrier FOLT. In humans, the highly expressed genes were peptide transporter HPT1, amino acid transporters LAT3, 4F2HC and PROT, nucleoside transporter CNT2, organic cation transporter OCTN2, organic anion transporters NADC1, NBC1 and SBC2, glucose transporters SGLT1 and GLUT5, multidrug resistance-associated protein RHO12, fatty acid transporters FABP1 and FABP2, and phosphate carrier PHC. Overall these data reveal diverse transcriptomic profiles for intestinal transporters among these species. Therefore, this transcriptional data may lead to more effective use of the laboratory animals as a model for oral drug development."
      ],
      "categories":[
        "q-bio.TO"
      ]
    },
    "list":{
      "title":[
        "Asynchronous Hebbian\/anti-Hebbian networks",
        "Effect of a new type of healthy and live food supplement on osteoporosis\n  blood parameters and induced rheumatoid arthritis in Wistar rats",
        "Tumor microenvironment (Part I): Tissue integrity in a rat model of\n  peripheral neural cancer",
        "Simplified model of immunotherapy for glioblastoma multiforme: cancer\n  stem cells hypothesis perspective",
        "Advanced 3D-Printed Multiphasic Scaffold with Optimal PRP Dosage for\n  Chondrogenesis of BM-MSCs in Osteochondral Tissue Engineering",
        "Integrating anatomy and electrophysiology in the healthy human heart:\n  Insights from biventricular statistical shape analysis using universal\n  coordinates",
        "Practical parameter identifiability of respiratory mechanics in the\n  extremely preterm infant",
        "Geometric immunosuppression in CAR-T cell treatment: Insights from\n  mathematical modeling",
        "Unveiling sex dimorphism in the healthy cardiac anatomy: fundamental\n  differences between male and female heart shapes",
        "A tumor-immune model of chronic myeloid leukemia with optimal\n  immunotherapeutic protocols",
        "Photodynamic, UV-curable and fibre-forming polyvinyl alcohol derivative\n  with broad processability and staining-free antibacterial capability",
        "MorphoITH: A Framework for Deconvolving Intra-Tumor Heterogeneity Using\n  Tissue Morphology",
        "Bridging high resolution sub-cellular imaging with physiologically\n  relevant engineered tissues",
        "Thermodynamic properties of fcc lead: A scalar and fully relativistic\n  first principle study",
        "QUOKKA-based understanding of outflows (QED) -- III. Outflow loading and\n  phase structure as a function of galactic environment",
        "Viscoelastic tensor and hydrodynamics of altermagnets",
        "Star Formation Rates, Metallicities, and Stellar Masses on kpc-scales in\n  TNG50",
        "Euclid Quick Data Release (Q1). Combined Euclid and Spitzer galaxy\n  density catalogues at $z>$ 1.3 and detection of significant Euclid passive\n  galaxy overdensities in Spitzer overdense regions",
        "Vacuum axisymmetric gravitational collapse revisited: preliminary\n  investigation",
        "Asymptotic integrability and its consequences",
        "Bidirectional controlled quantum state preparation in high-dimensional\n  quantum system",
        "Multiple orthogonal polynomial ensembles of derivative type",
        "Bayesian Hierarchical Emulators for Multi-Level Models: BayHEm",
        "Scaffold-Assisted Window Junctions for Superconducting Qubit Fabrication",
        "A benchmark analysis of saliency-based explainable deep learning methods\n  for the morphological classification of radio galaxies",
        "Simulations of Magnetic Monopole Collisions",
        "First-principles study of dielectric properties of ferroelectric\n  perovskite oxides with on-site and inter-site Hubbard interactions",
        "When Less is More: Evolutionary Dynamics of Deception in a\n  Sender-Receiver Game"
      ],
      "abstract":[
        "Lateral inhibition models coupled with Hebbian plasticity have been shown to\nlearn factorised causal representations of input stimuli, for instance,\noriented edges are learned from natural images. Currently, these models require\nthe recurrent dynamics to settle into a stable state before weight changes can\nbe applied, which is not only biologically implausible, but also impractical\nfor real-time learning systems. Here, we propose a new Hebbian learning rule\nwhich is implemented using plausible biological mechanisms that have been\nobserved experimentally. We find that this rule allows for efficient,\ntime-continuous learning of factorised representations, very similar to the\nclassic noncontinuous Hebbian\/anti-Hebbian learning. Furthermore, we show that\nthis rule naturally prevents catastrophic forgetting when stimuli from\ndifferent distributions are shown sequentially.",
        "Summary Osteoporosis is a skeletal disorder, characterized by a decrease in\nbone strength and puts the individual at risk for fracture. On the other hand,\nrheumatoid arthritis is a systemic disease of unknown etiology that causes\ninflammation of the joints of the organs. Purpose Due to the destructive\neffects of these diseases and its increasing prevalence and lack of appropriate\nmedication for treatment, the present study aimed to evaluate the therapeutic\neffect of a new type of healthy and live food supplement on rheumatoid\narthritis and induced osteoporosis in rats. Methods In this research, healthy\nand live food powder were synthesized by a new and green route. This organic\nbiomaterial was named NBS. The NBS food supplement had various vitamins, macro\nand micro molecules, and ingredients. The new healthy and nutritious diet\nshowed that the use of this supplement led to the return of the parameters to\nnormal levels. Results The concentration of 12.5 mg\/ kg showed the least\ntherapeutic effect and 50 mg\/ kg had the highest therapeutic effect for\nosteoporosis. The results of blood parameters involved in inflammation in both\nhealthy and patient groups showed that the use of complete adjuvant induction\ncauses joint inflammation. In the study of the interaction of the\nconcentrations, it was observed that the concentration of 50 mg\/ kg had the\nhighest therapeutic effect against the disease in the studied mice. Conclusion\nThe results showed that the new healthy and viable supplement restores the\nblood osteoporotic and rheumatoid factors of the mice to normal.",
        "ICAM-1 (intercellular adhesion molecule 1) and MPZ (myelin protein zero) are\nthought to be a factor in the integrity of nerve tissues. In this report, we\nattempted to trace the expression of ICAM-1, responsible for cell-to-cell\nadhesion, and of MPZ, the main constituent of myelin sheath, in malignant\ntissues of the sciatic nerve (SN) in inbred male Copenhagen rats. AT-1 Cells\n(anaplastic tumor 1) were injected in the perineurial sheath, and tissues of\nthe SNs were collected after 7, 14 and 21 days and compared to a sham-operated\ngroup of rats (n = 6 each). Tissues were sectioned and histologically examined,\nunder light microscope, and stained for measuring the immunoreactivity of\nICAM-1 and MPZ under laser scanning microscope. The cancer model was\nestablished, and the tumor growth was confirmed. ICAM-1 showed severe\ndecreases, proportional to the growing anaplastic cells, as compared to the\nsham group. MPZ revealed, however, a distinct defensive pattern before\nsubstantially decreasing in a comparison with sham. These results support the\nnotion that malignancies damage peripheral nerves and cause severe axonal\ninjury and loss of neuronal integrity, and clearly define the role of ICAM-1\nand MPZ in safeguarding the nerve tissues.",
        "Despite ongoing efforts in cancer research, a fully effective treatment for\nglioblastoma multiforme (GBM) is still unknown. Since adoptive cell transfer\nimmunotherapy is one of the potential cure candidates, efforts have been made\nto assess its effectiveness using mathematical modeling. In this paper, we\nconsider a model of GBM immunotherapy proposed by Abernathy and Burke (2016),\nwhich also takes into account the dynamics of cancer stem cells, i.e., the type\nof cancer cells that are hypothesized to be largely responsible for cancer\nrecurrence. We modify the initial ODE system by applying simplifying\nassumptions and analyze the existence and stability of steady states of the\nobtained simplified model depending on the treatment levels.",
        "In osteochondral tissue engineering (OCTE), simultaneously regenerating\nsubchondral bone and cartilage tissue presents a significant challenge.\nMultiphasic scaffolds were created and manufactured using 3D printing to\naddress this issue. Excellent interfacial mechanical properties and\nbiocompatibility enhance the growth and chondrogenic differentiation of bone\nmarrow mesenchymal stem cells (BM-MSCs). The subchondral bone bottom layer is\nmimicked by incorporating varying concentrations of graphene oxide (GO) (0%,\n1%, and 2% w\/v) into a bioink composed of alginate (Alg) and gelatin (Gel).\nBased on evaluations of mechanical and biocompatibility properties, 1% GO is\nselected for further studies. Subsequently, the GO concentration is kept\nconstant while varying the platelet-rich plasma (PRP) dosage in the multiphasic\nscaffolds. Different PRP dosages (0%, 1%, 2%, and 3% w\/v) are integrated into\nthe Alg-Gel bioink to simulate cartilage tissues. Results indicate that\n3D-printed scaffolds containing 1% or 2% PRP exhibit favorable biomechanical\nproperties, with no significant differences observed. However, BM-MSCs exposed\nto 2% PRP demonstrate enhanced adhesion, growth, and viability. Additionally,\nreal-time PCR and Alcian blue staining confirm increased chondrogenic\nexpression and glycosaminoglycans (GAGs) synthesis. This work highlights the\npromising potential of 3D-printed multiphasic frameworks in the development of\nOCTE.",
        "A cardiac digital twin is a virtual replica of a patient-specific heart,\nmimicking its anatomy and physiology. A crucial step of building a cardiac\ndigital twin is anatomical twinning, where the computational mesh of the\ndigital twin is tailored to the patient-specific cardiac anatomy. In a number\nof studies, the effect of anatomical variation on clinically relevant\nfunctional measurements like electrocardiograms (ECGs) is investigated, using\ncomputational simulations. While such a simulation environment provides\nresearchers with a carefully controlled ground truth, the impact of anatomical\ndifferences on functional measurements in real-world patients remains\nunderstudied. In this study, we develop a biventricular statistical shape model\nand use it to quantify the effect of biventricular anatomy on ECG-derived and\ndemographic features, providing novel insights for the development of digital\ntwins of cardiac electrophysiology. To this end, a dataset comprising\nhigh-resolution cardiac CT scans from 271 healthy individuals, including\nathletes, is utilized. Furthermore, a novel, universal, ventricular\ncoordinate-based method is developed to establish lightweight shape\ncorrespondence. The performance of the shape model is rigorously established,\nfocusing on its dimensionality reduction capabilities and the training data\nrequirements. Additionally, a comprehensive synthetic cohort is made available,\nfeaturing ready-to-use biventricular meshes with fiber structures and\nanatomical region annotations. These meshes are well-suited for\nelectrophysiological simulations.",
        "The complexity of mathematical models describing respiratory mechanics has\ngrown in recent years, however, parameter identifiability of such models has\nonly been studied in the last decade in the context of observable data. This\nstudy investigates parameter identifiability of a nonlinear respiratory\nmechanics model tuned to the physiology of an extremely preterm infant, using\nglobal Morris screening, local deterministic sensitivity analysis, and singular\nvalue decomposition-based subset selection. The model predicts airflow and\ndynamic pulmonary volumes and pressures under varying levels of continuous\npositive airway pressure, and a range of parameters characterizing both\nsurfactant-treated and surfactant-deficient lung. Sensitivity analyses\nindicated eleven parameters influence model outputs over the range of\ncontinuous positive airway pressure and lung health scenarios. The model was\nadapted to data from a spontaneously breathing 1 kg infant using gradient-based\noptimization to estimate the parameter subset characterizing the patient's\nstate of health.",
        "Chimeric antigen receptor T (CAR-T) cell therapy has emerged as a promising\ntreatment for hematological malignancies, offering a targeted approach to\ncancer treatment. Understanding the complexities of CAR-T cell therapy within\nsolid tumors poses challenges due to the intricate interactions within the\ntumor microenvironment. Mathematical modeling may serve as a valuable tool to\nunravel the dynamics of CAR-T cell therapy and improve its effectiveness in\nsolid tumors. This study aimed to investigate the impact of spatial aspects in\nCAR-T therapy of solid tumors, utilizing cellular automata for modeling\npurposes. Our main objective was to deepen our understanding of treatment\neffects by analyzing scenarios with different spatial distributions and varying\nthe initial quantities of tumor and CAR-T cells. Tumor geometry significantly\ninfluenced treatment efficacy in-silico, with notable differences observed\nbetween tumors with block-like arrangements and those with sparse cell\ndistributions, leading to the concept of immune suppression due to geometrical\neffects. This research delves into the intricate relationship between spatial\ndynamics and the effectiveness of CAR-T therapy in solid tumors, highlighting\nthe relevance of tumor geometry in the outcome of cellular immunotherapy\ntreatments. Our results provide a basis for improving the efficacy of CAR-T\ncell treatments by combining them with other ones reducing the density of\ncompact tumor areas and thus opening access ways for tumor killing T-cells.",
        "Sex-based differences in cardiovascular disease are well documented, yet the\nprecise nature and extent of these discrepancies in cardiac anatomy remain\nincompletely understood. Traditional scaling models often fail to capture the\ninterplay of age, blood pressure, and body size, prompting a more nuanced\ninvestigation. Here, we employ statistical shape modeling in a healthy subset\n(n=456) of the UK Biobank to explore sex-specific variations in biventricular\nanatomy. We reconstruct 3D meshes and perform multivariate analyses of shape\ncoefficients, controlling for age, blood pressure, and various body size\nmetrics. Our findings reveal that sex alone explains at least 25 percent of\nmorphological variability, with strong discrimination between men and women\n(AUC=0.96-0.71) persisting even after correction for confounders. Notably, the\nmost discriminative modes highlight pronounced differences in cardiac chamber\nvolumes, the anterior-posterior width of the right ventricle, and the relative\npositioning of the cardiac chambers. These results underscore that sex has a\nfundamental influence on cardiac morphology, which may have important clinical\nimplications for differing cardiac structural assessments in men and women.\nFuture work should investigate how these anatomical differences manifest in\nvarious cardiovascular conditions, ultimately paving the way for more precise\nrisk stratification and personalized therapeutic strategies for both men and\nwomen.",
        "The interactions between tumor cells and the immune system play a crucial\nrole in cancer evolution. In this study, we explore how these interactions\ninfluence cancer progression by modeling the relationships among naive T cells,\neffector T cells, and chronic myeloid leukemia cells. We examine the existence\nof equilibria, the asymptotic stability of the positive steady state, and the\nglobal stability of the tumor-free equilibrium. Additionally, we develop a\npartial differential equation to describe the conditions under which the\nconcentration of cancer cells reaches a level that allows for effective control\nof cancer evolution. Finally, we apply our proposed model to investigate\noptimal treatment strategies that aim to minimize both the concentration of\ncancer cells at the end of treatment and the accumulation of tumor burden, as\nwell as the cost associated with treatment during the intervention period. Our\nstudy reveals an optimal therapeutic protocol using optimal control theory. We\nperform numerical simulations to illustrate our theoretical results and to\nexplore the dynamic behavior of the system and optimal therapeutic protocols.\nThe simulations indicate that the optimal treatment strategy can be more\neffective than a constant treatment approach, even when applying the same\ntreatment interval and total drug input.",
        "Antimicrobial photodynamic therapy (APDT) is a promising antibiotic-free\nstrategy for broad-spectrum infection control in chronic wounds, minimising\nbacterial resistance risks. However, rapid photosensitiser diffusion, tissue\nstaining, side toxicity, and short-lived antimicrobial effects present\nsignificant clinical limitations for integrating APDT into wound dressings. To\naddress these challenges, we present the design of a bespoke polyvinyl alcohol\n(PVA) derivative conjugated with both phenothiazine and methacrylate\nfunctionalities, enabling staining-free antibacterial photodynamic effects,\ncellular tolerability and processability into various wound dressing formats,\nincluding films, textile fibres and nanoscale coatings. Tosylation of PVA is\nleveraged for the covalent coupling of toluidine blue, as confirmed by UV-Vis\nspectroscopy and the minimal release of TB in vitro. UV-induced network\nformation is exploited to accomplish cast films and nanoscale integrated wound\ndressing coatings. UV curing is also successfully coupled with an in-house wet\nspinning process to realise individual, water-insoluble fibres as the building\nblocks of fibrous wound dressings. A fluorometric assay supports the generation\nof reactive oxygen species when the UV-cured samples are exposed to work, but\nnot UV, light, yielding a mean log10 reduction of up to 2.13 in S. aureus, and\nthe complete eradication of P. aeruginosa. Direct and extract cytotoxicity\ntests with UV-cured films and fibres demonstrate the viability of L929\nfibroblasts following 60-min light irradiation and 72-hour cell culture. The\nbespoke molecular architecture, broad processability and cellular tolerability\nof this PVA derivative are highly attractive aiming to integrate durable\nstaining-free photodynamic capability in a wide range of healthcare\ntechnologies, from chronic wound dressings up to minimally invasive localised\ntherapy.",
        "The ability of tumors to evolve and adapt by developing subclones in\ndifferent genetic and epigenetic states is a major challenge in oncology.\nTraditional tools like multi-regional sequencing used to study tumor evolution\nand the resultant intra-tumor heterogeneity (ITH) are often impractical because\nof their resource-intensiveness and limited scalability. Here, we present\nMorphoITH, a novel framework that leverages histopathology slides to deconvolve\nmolecular ITH through tissue morphology. MorphoITH integrates a self-supervised\ndeep learning similarity measure to capture phenotypic variation across\nmultiple dimensions (cytology, architecture, and microenvironment) with\nrigorous methods to eliminate spurious sources of variation. Using a prototype\nof ITH, clear cell renal cell carcinoma (ccRCC), we show that MorphoITH\ncaptures clinically-significant biological features, such as vascular\narchitecture and nuclear grades. Furthermore, we find that MorphoITH recognizes\ndifferential biological states corresponding to subclonal changes in key driver\ngenes (BAP1\/PBRM1\/SETD2). Finally, by applying MorphoITH to a multi-regional\nsequencing experiment, we postulate evolutionary trajectories that largely\nrecapitulate genetic evolution. In summary, MorphoITH provides a scalable\nphenotypic lens that bridges the gap between histopathology and genomics,\nadvancing precision oncology.",
        "While high-resolution microscopic techniques are crucial for studying\ncellular structures in cell biology, obtaining such images from thick 3D\nengineered tissues remains challenging. In this review, we explore advancements\nin fluorescence microscopy, alongside the use of various fluorescent probes and\nmaterial processing techniques to address these challenges. We navigate through\nthe diverse array of imaging options available in tissue engineering field,\nfrom wide field to super-resolution microscopy, so researchers can make more\ninformed decisions based on the specific tissue and cellular structures of\ninterest. Finally, we provide some recent examples of how traditional\nlimitations on obtaining high-resolution images on sub-cellular architecture\nwithin 3D tissues have been overcome by combining imaging advancements with\ninnovative tissue engineering approaches.",
        "This study investigates the thermodynamic properties of face-centered cubic\nlead (fcc-Pb) using ab-initio methods within the quasi-harmonic approximation\n(QHA), examining the influence of spin-orbit coupling (SOC) and the\nexchange-correlation functionals. Two types of ultrasoft pseudopotential\n(US-PP) are considered: one that excludes (scalar relativistic PP) and one that\nincludes the SOC effects (fully relativistic PP). Further, for each PP, we test\nthe performance of three popular exchange-correlation functionals:\nPerdew-Burke-Ernzerhof generalized gradient approximation (PBE) (Perdew et al.\nPhys. Rev. Lett. 77, 3865 (1996)), PBE modified for dense solids (PBEsol)\n(Perdew et al. Phys. Rev. Lett. 100, 136406 (2008)), and local density\napproximation (LDA) (Perdew et al. Phys. Rev. B 23, 5048 (1981)). We calculate\nthe Helmholtz free energy, incorporating lattice vibrations (phonons) and\nelectronic excitation contributions. The estimated equation of state (at 4 K\nand 301 K), phonon dispersions (at 100 K and 300 K), mode-Gr\\\"uneisen\nparameters ({\\gamma}q{\\eta}) (at 100 K), volume thermal expansion coefficient\n(\\b{eta}), isobaric heat capacity (CP), bulk modulus (BS), and thermodynamic\naverage Gr\\\"uneisen parameter ({\\gamma}) are compared with the available\nexperimental and theoretical studies. Moreover, the 0 K pressure-dependent\nelastic constant-coefficient (Cij) of fcc lead and Pugh ratio, Debye\ntemperature, and longitudinal and transverse sound velocities for\npolycrystalline lead are presented. The contributions of electronic excitations\nin all the thermodynamic properties are found to be negligible. With increasing\npressure, the role of spin-orbit effects decreases but does not vanish. Our\nfindings demonstrate that SOC leads to results distinct from the SR approach,\nbut agreement with the experiment is not consistently improved by including\nSOC.",
        "We present results from a suite of 3D high-resolution hydrodynamic\nsimulations of supernova-driven outflows from galactic disc regions with a\nrange of gas surface density, metallicity, and supernova scale height. We use\nthis suite to quantify how outflow properties -- particularly the loading\nfactors for mass, metallicity, and energy -- vary with these parameters. We\nfind that the winds fall into three broad categories: steady and hot,\nmultiphase and moderately bursty, and cool and highly bursty. The first of\nthese is characterised by efficient metal and energy loading but weak mass\nloading, the second by moderate loading of mass, metals, and energy, and the\nthird by negligible metal and energy loading but substantial mass loading. The\nmost important factor in determining the kind of wind a galaxy will produce is\nthe ratio of supernova to gas gas scale heights, with the latter set by a\ncombination of supernova rate, metallicity-dependent cooling rate, and the\ngravitational potential. These often combine in counterintuitive ways -- for\nexample increased cooling causes cold clouds to sink into the galactic midplane\nmore rapidly, lowering the volume-filling factor of dense gas and making the\nenvironment more favourable for strong winds. Our findings suggest that the\nnature of galactic winds is likely highly sensitive to phenomena such as\nrunaway stars occuring at a large height and dense gas and are poorly captured\nin most simulations, and that metal loading factors for type Ia supernovae may\nbe substantially larger than those for type II, with important implications for\ngalactic chemical evolution.",
        "We calculate the viscoelasticity tensor for altermagnets and formulate the\ncorresponding hydrodynamic equations. The anisotropy of altermagnetic Fermi\nsurfaces allows for additional terms in the viscoelasticity tensor and is\nmanifested in transport properties including electron and spin flows in a\nchannel and nonlocal responses. In the channel geometry, the altermagnetic spin\nsplitting leads to nontrivial spin density and spin current. Like the electric\ncurrent, the spin current acquires a Poiseuille profile for no-slip boundary\nconditions. In nonlocal responses, the altermagnetic anisotropy affects current\nstreamlines and electric potential distributions in the viscous regime. Our\nresults provide signatures of the hydrodynamic transport regime in altermagnets\npotentially facilitating its experimental studies and discovery.",
        "Integral field units (IFU) have extended our knowledge of galactic properties\nto kpc (or, sometimes, even smaller) patches of galaxies. These scales are\nwhere the physics driving galaxy evolution (feedback, chemical enrichment,\netc.) take place. Quantifying the spatially-resolved properties of galaxies,\nboth observationally and theoretically, is therefore critical to our\nunderstanding of galaxy evolution. To this end, we investigate\nspatially-resolved scaling relations within central galaxies\n($M_\\star>10^{9.0}$) at $z=0$ in IllustrisTNG. We examine both the resolved\nstar-forming main sequence (rSFMS) and the resolved mass-metallicity relation\n(rMZR) using $1~{\\rm kpc}\\times1~{\\rm kpc}$ maps of galaxies. We find that the\nrSFMS in IllustrisTNG is well-described by a power-law, but has some dependence\non the host galaxy's mass. Conversely, the rMZR for IllustrisTNG can be\ndescribed by a single power-law at low stellar mass surface density that\nflattens at high surface densities and is independent of host galaxy mass. We\nfind quantitative agreement in both the rSFMS and rMZR with recent IFU\nobservational campaigns. Furthermore, we argue that the rSFMS is an indirect\nresult of the Schmidt-Kennicutt (SK) law and local gas fraction relation, which\nare both independent of host galaxy properties. Finally, we expand upon a\nlocalized leaky-box model to study the evolution of idealized spaxels and find\nthat it provides a good description of these resolved relations. The degree of\nagreement, however, between idealized spaxels and simulated spaxels depends on\nthe `net' outflow rate for the spaxel, and the observed scaling relations\nindicate a preference for a low net outflow rate.",
        "Euclid will detect tens of thousands of clusters and protoclusters at\n$z$>1.3. With a total coverage of 63.1deg$^2$, the Euclid Quick Data Release 1\n(Q1) is large enough to detect tens of clusters and hundreds of protoclusters\nat these early epochs. The Q1 photometric redshift catalogue enables us to\ndetect clusters out to $z$ < 1.5; however, infrared imaging from Spitzer\nextends this limit to higher redshifts by using high local projected densities\nof Spitzer-selected galaxies as signposts for cluster and protocluster\ncandidates. We use Spitzer imaging of the Euclid Deep Fields (EDFs) to derive\ndensities for a sample of Spitzer-selected galaxies at redshifts $z$ > 1.3,\nbuilding Spitzer IRAC1 and IRAC2 photometric catalogues that are 95% complete\nat a magnitude limit of IRAC2=22.2, 22.6, and 22.8 for the EDF-S, EDF-F, and\nEDF-N, respectively. We apply two complementary methods to calculate galaxy\ndensities: (1) aperture and surface density; and (2) the Nth-nearest-neighbour\nmethod. When considering a sample selected at a magnitude limit of IRAC2 <\n22.2, at which all three EDFs are 95% complete, our surface density\ndistributions are consistent among the three EDFs and with the SpUDS blank\nfield survey. We also considered a deeper sample (IRAC2 < 22.8), finding that\n2% and 3% of the surface densities in the North and Fornax fields are 3$\\sigma$\nhigher than the average field distribution and similar to densities found in\nthe CARLA cluster survey. Our surface densities are also consistent with\npredictions from the GAEA semi-analytical model. Using combined Euclid and\nground-based i-band photometry we show that our highest Spitzer-selected galaxy\noverdense regions, found at $z$~1.5, also host high densities of passive\ngalaxies. This means that we measure densities consistent with those found in\nclusters and protoclusters at $z$>1.3.",
        "Validating the results of [A.M. Abrahams and C.R. Evans, Phys. Rev. Lett. 70,\n2980] poses a numerical challenge and has been inspiring a lot of research. We\njoin these efforts and present our first steps to achieve this goal: we discuss\na formulation of Einstein equations for a vacuum axisymmetric spacetime with\nvanishing twist in spherical-polar coordinates, its linearised approximation,\nand identify some problems in achieving numerically stable evolution at the\nthreshold of a black hole formation.",
        "We give a brief review of the concept of asymptotic integrability, which\nmeans that the Hamilton equations for the propagation of short-wavelength\npackets along a smooth, large-scale background wave have an integral\nindependent of the initial conditions. The existence of such an integral leads\nto a number of important consequences, which include, besides the direct\napplication to the packets propagation problems, Hamiltonian theory of narrow\nsolitons motion and generalized Bohr-Sommerfeld rule for parameters of solitons\nproduced from an intensive initial pulse. We show that in the case of systems\nwith two wave variables and exact fulfillment of the asymptotic integrability\ncondition, the `quantization' of mechanical systems, associated with the\nadditional integrals, yields the Lax pairs for a number of typical completely\nintegrable equations, and this sheds new light on the origin of the complete\nintegrability in nonlinear wave physics.",
        "High-dimensional quantum system exhibits unique advantages over the qubit\nsystem in some quantum information processing tasks. We present a program for\nimplementing deterministic bidirectional controlled remote quantum state\npreparation (BCRSP) in arbitrary $N$-dimensional (quNit) system. By introducing\ntwo generalized Greenberger-Horne-Zeilinger (GHZ) states as quantum channels,\ntwo communication parties can simultaneously prepare a single-particle\nhigh-dimensional state at each other's site under the control of Charlie.\nCompared with the previous counterparts, the significant advantage of our\nscheme is that the high-dimensional CNOT operations are not required. Moreover,\nthe performance our scheme are evaluated. The evaluation of the performance\nshows that if the quNit is encoded in the spatial mode of single photons, our\nscheme can be accomplished solely using only linear optical elements.",
        "We characterize the biorthogonal ensembles that are both a multiple\northogonal polynomial ensemble and a polynomial ensemble of derivative type\n(also called a P\\'olya ensemble). We focus on the two notions of derivative\ntype that typically appear in connection with the squared singular values of\nproducts of invertible random matrices and the eigenvalues of sums of Hermitian\nrandom matrices. Essential in the characterization is the use of the Mellin and\nLaplace transform: we show that the derivative type structure, which is a\npriori analytic in nature, becomes algebraic after applying the appropriate\ntransform. Afterwards, we explain how these notions of derivative type can be\nused to provide a partial solution to an open problem related to orthogonality\nof the finite finite free multiplicative and additive convolution from finite\nfree probability. In particular, we obtain families of multiple orthogonal\npolynomials that (de)compose naturally using these convolutions.",
        "Decision making often uses complex computer codes run at the exa-scale (10e18\nflops). Such computer codes or models are often run in a hierarchy of different\nlevels of fidelity ranging from the basic to the very sophisticated. The top\nlevels in this hierarchy are expensive to run, limiting the number of possible\nruns. To make use of runs over all levels, and crucially improve emulation at\nthe top level, we use multi-level Gaussian process emulators (GPs). We will\npresent a new method of building GP emulators from hierarchies of models. In\norder to share information across the different levels, l=1,...,L, we define\nthe form of the prior of the l+1th level to be the posterior of the lth level,\nhence building a Bayesian hierarchical structure for the top Lth level. This\nenables us to not only learn about the GP hyperparameters as we move up the\nmulti-level hierarchy, but also allows us to limit the total number of\nparameters in the full model, whilst maintaining accuracy.",
        "The superconducting qubit is one of the promising directions in realizing\nfault-tolerant quantum computing (FTQC), which requires many high-quality\nqubits. To achieve this, it is desirable to leverage modern semiconductor\nindustry technology to ensure quality, uniformity, and reproducibility.\nHowever, conventional Josephson junction fabrication relies mainly on\nresist-assistant double-angle evaporation, posing integration challenges. Here,\nwe demonstrate a lift-off-free qubit fabrication that integrates seamlessly\nwith existing industrial technologies. This method employs a silicon oxide\n(SiO$_2$) scaffold to define an etched window with a well-controlled size to\nform a Josephson junction. The SiO$_2$, which has a large dielectric loss, is\netched away in the final step using vapor HF leaving little residue. This\nWindow junction (WJ) process mitigates the degradation of qubit quality during\nfabrication and allows clean removal of the scaffold. The WJ process is\nvalidated by inspection and Josephson junction measurement. The scaffold\nremoval process is verified by measuring the quality factor of the resonators.\nFurthermore, compared to scaffolds fabricated by plasma-enhanced chemical vapor\ndeposition (PECVD), qubits made by WJ through physical vapor deposition (PVD)\nachieve relaxation time up to $57\\,\\mu\\text{s}$. Our results pave the way for a\nlift-off-free qubit fabrication process, designed to be compatible with modern\nfoundry tools and capable of minimizing damage to the substrate and material\nsurfaces.",
        "This work proposes a saliency-based attribution framework to evaluate and\ncompare 10 state-of-the-art explainability methods for deep learning models in\nastronomy, focusing on the classification of radio galaxy images. While\nprevious work has primarily emphasized classification accuracy, we prioritize\nmodel interpretability. Qualitative assessments reveal that Score-CAM,\nGrad-CAM, and Grad-CAM++ consistently produce meaningful attribution maps,\nhighlighting the brightest regions of FRI and FRII galaxies in alignment with\nknown astrophysical features. In contrast, other methods often emphasize\nirrelevant or noisy areas, reducing their effectiveness.",
        "In this paper, we investigate the scattering of BPS magnetic monopoles\nthrough numerical simulations. We present an ansatz for various multi-monopole\nconfigurations suitable for analyzing monopole scattering processes. Our study\nincludes planar scattering scenarios involving two, three, and four monopoles,\nas well as non-planar processes where three and four monopoles form\nintermediate tetrahedral and cubic states, respectively. Our observations align\nwith the theoretical predictions of the moduli space approximation.\nFurthermore, we extend our analysis to relativistic velocities and explore\nparameters beyond the BPS limit.",
        "We study the atomic and electronic structures of ferroelectric perovskite\noxides, BaTiO$_3$, LiNbO$_3$, and PbTiO$_3$ using ab initio extended Hubbard\nfunctionals in which the on-site and inter-site Hubbard interactions are\ndetermined self-consistently, adapted from the pseudohybrid density functional\nproposed by Agapito-Curtarolo-Buongiorno Nardelli. Band structures,\nferroelectric distortions, polarization, Born effective charges, and switching\nbarriers are calculated with extended Hubbard functionals, that are compared\nwith those using local density approximation (LDA), generalized gradient\napproximation (GGA), and Hybrid (HSE06) functionals. The properties of all\nthree compounds calculated by extended Hubbard functionals are in good\nagreement with experimental data. We find a substantial increase in band gaps\ndue to the inter-site Coulomb interactions, which show better agreement with\n$GW$ results compared to those from LDA and GGA functionals. The crucial role\nof the inter-site Coulomb interactions in restoring the suppressed polar\ninstability, which is computed when only the on-site Hubbard interactions are\nconsidered, is also highlighted. Overall, we find that the properties\ncalculated using our extended Hubbard functionals exhibit trends similar to\nthose obtained with the HSE06 functional, while reducing computational costs by\nover an order of magnitude. Thus, we propose that the current method is\nwell-suited for high-throughput calculations for perovskite oxides, offering\nsignificantly improved accuracy in computing band gap and other related\nphysical properties such as the shift current photovoltaic effect and band\nalignments in ferroelectric heterostructures.",
        "The spread of disinformation poses a significant threat to societal\nwell-being. We analyze this phenomenon using an evolutionary game theory model\nof the sender-receiver game, where senders aim to mislead receivers and\nreceivers aim to discern the truth. Using a combination of replicator\nequations, finite-size scaling analysis, and extensive Monte Carlo simulations,\nwe investigate the long-term evolutionary dynamics of this game. Our central\nfinding is a counterintuitive threshold phenomenon: the role (sender or\nreceiver) with the larger difference in payoffs between successful and\nunsuccessful interactions is surprisingly more likely to lose in the long run.\nWe show that this effect is robust across different parameter values and arises\nfrom the interplay between the relative speeds of evolution of the two roles\nand the ability of the slower evolving role to exploit the fixed strategy of\nthe faster evolving role. Moreover, for finite populations we find that the\ninitially less frequent strategy of the slower role is more likely to fixate in\nthe population. The initially rarer strategy in the less-rewarded role is,\nparadoxically, more likely to prevail."
      ]
    }
  },
  {
    "id":2411.16961,
    "research_type":"applied",
    "start_id":"b9",
    "start_title":"Comparative gene expression profiles of intestinal transporters in mice, rats and humans",
    "start_abstract":"We have studied gene expression profiles of intestinal transporters in model animals and humans. Total RNA was isolated from duodenum and the mRNA expression was measured using Affymetrix GeneChip oligonucleotide arrays. Detected genes from the intestine of mice, rats, and humans were about 60% of 22,690 sequences, 40% of 8739, and 47% of 12,559, respectively. A total of 86 genes involving transporters expressed in mice, 50 genes in rats, and 61 genes in humans were detected. Mice exhibited abundant mRNA expressions for peptide transporter HPT1, amino acid transporters CSNU3, CT1 and ASC1, nucleoside transporter CNT2, organic cation transporter SFXN1, organic anion transporter NBC3, glucose transporter SGLT1, and fatty acid transporters FABP1 and FABP2. Rats showed high expression profiles of peptide transporter PEPT1, amino acid transporters CSNU1 and 4F2HC, nucleoside transporter CNT2, organic cation transporter OCT5, organic anion transporter SDCT1, glucose transporter GLUT2 and GLUT5, and folate carrier FOLT. In humans, the highly expressed genes were peptide transporter HPT1, amino acid transporters LAT3, 4F2HC and PROT, nucleoside transporter CNT2, organic cation transporter OCTN2, organic anion transporters NADC1, NBC1 and SBC2, glucose transporters SGLT1 and GLUT5, multidrug resistance-associated protein RHO12, fatty acid transporters FABP1 and FABP2, and phosphate carrier PHC. Overall these data reveal diverse transcriptomic profiles for intestinal transporters among these species. Therefore, this transcriptional data may lead to more effective use of the laboratory animals as a model for oral drug development.",
    "start_categories":[
      "q-bio.TO"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b7"
      ],
      "title":[
        "Pathology image analysis using segmentation deep learning algorithms"
      ],
      "abstract":[
        "With the rapid development of image scanning techniques and visualization software, whole slide imaging (WSI) is becoming a routine diagnostic method. Accelerating clinical diagnosis from pathology images and automating image analysis efficiently and accurately remain significant challenges. Recently, deep learning algorithms have shown great promise in pathology image analysis, such as in tumor region identification, metastasis detection, and patient prognosis. Many machine learning algorithms, including convolutional neural networks, have been proposed to automatically segment pathology images. Among these algorithms, segmentation deep learning algorithms such as fully convolutional networks stand out for their accuracy, computational efficiency, and generalizability. Thus, deep learning\u2013based pathology image segmentation has become an important tool in WSI analysis. In this review, the pathology image segmentation process using deep learning algorithms is described in detail. The goals are to provide quick guidance for implementing deep learning into pathology image analysis and to provide some potential ways of further improving segmentation performance. Although there have been previous reviews on using machine learning methods in digital pathology image analysis, this is the first in-depth review of the applications of deep learning algorithms for segmentation in WSI analysis."
      ],
      "categories":[
        "cs.NE"
      ]
    },
    "list":{
      "title":[
        "Backpropagation through Soft Body: Investigating Information Processing\n  in Brain-Body Coupling Systems",
        "A Weight Adaptation Trigger Mechanism in Decomposition-based\n  Evolutionary Multi-Objective Optimisation",
        "Abnormal Mutations: Evolution Strategies Don't Require Gaussianity",
        "Pareto Optimization with Robust Evaluation for Noisy Subset Selection",
        "Channel-wise Parallelizable Spiking Neuron with Multiplication-free\n  Dynamics and Large Temporal Receptive Fields",
        "Was Tournament Selection All We Ever Needed? A Critical Reflection on\n  Lexicase Selection",
        "Fig Tree-Wasp Symbiotic Coevolutionary Optimization Algorithm",
        "Quantum Simplicial Neural Networks",
        "Sustainable AI: Mathematical Foundations of Spiking Neural Networks",
        "Aerial Reliable Collaborative Communications for Terrestrial Mobile\n  Users via Evolutionary Multi-Objective Deep Reinforcement Learning",
        "Range and Angle Estimation with Spiking Neural Resonators for FMCW Radar",
        "UAV-assisted Joint Mobile Edge Computing and Data Collection via\n  Matching-enabled Deep Reinforcement Learning",
        "Hybrid Firefly Algorithm and Sperm Swarm Optimization Algorithm using\n  Newton-Raphson Method (HFASSON) and its application in CR-VANET",
        "Hotter and Colder: A New Approach to Annotating Sentiment, Emotions, and\n  Bias in Icelandic Blog Comments",
        "Global Hall-magnetohydrodynamic simulations of transition disks",
        "Vertical Federated Learning in Practice: The Good, the Bad, and the Ugly",
        "MIGHTEE: exploring the relationship between spectral index, redshift and\n  radio luminosity",
        "Storing quantum coherence in a quantum dot nuclear spin ensemble for\n  over 100 milliseconds",
        "Intrinsic charm and $D^+D^-$ asymmetry produced in proton-proton\n  collisions",
        "A note on partial polynomial functions, in memory of Marek Jarnicki",
        "Gaussian Universality of Products Over Split Reductive Groups and the\n  Satake Isomorphism",
        "Random Dynamical Systems on the circle without a finite orbit",
        "First photometric investigation of V517 Cam combined with ground-based\n  and TESS data",
        "Enhancing the charging performance of an atomic quantum battery",
        "BEARCUBS: A benchmark for computer-using web agents",
        "Interfacial spin-orbit coupling in superconducting hybrid systems",
        "Practical Spoofing Attacks on Galileo Open Service Navigation Message\n  Authentication",
        "Intelligent Gradient Boosting Algorithms for Estimating Strength of\n  Modified Subgrade Soil"
      ],
      "abstract":[
        "Animals achieve sophisticated behavioral control through dynamic coupling of\nthe brain, body, and environment. Accordingly, the co-design approach, in which\nboth the controllers and the physical properties are optimized simultaneously,\nhas been suggested for generating refined agents without designing each\ncomponent separately. In this study, we aim to reveal how the function of the\ninformation processing is distributed between brains and bodies while applying\nthe co-design approach. Using a framework called ``backpropagation through soft\nbody,\" we developed agents to perform specified tasks and analyzed their\nmechanisms. The tasks included classification and corresponding behavioral\nassociation, nonlinear dynamical system emulation, and autonomous behavioral\ngeneration. In each case, our analyses revealed reciprocal relationships\nbetween the brains and bodies. In addition, we show that optimized brain\nfunctionalities can be embedded into bodies using physical reservoir computing\ntechniques. Our results pave the way for efficient designs of brain--body\ncoupling systems.",
        "Decomposition-based multi-objective evolutionary algorithms (MOEAs) are\nwidely used for solving multi-objective optimisation problems. However, their\neffectiveness depends on the consistency between the problems Pareto front\nshape and the weight distribution. Decomposition-based MOEAs, with uniformly\ndistributed weights (in a simplex), perform well on problems with a regular\n(simplex-like) Pareto front, but not on those with an irregular Pareto front.\nPrevious studies have focused on adapting the weights to approximate the\nirregular Pareto front during the evolutionary process. However, these\nadaptations can actually harm the performance on the regular Pareto front via\nchanging the weights during the search process that are eventually the best fit\nfor the Pareto front. In this paper, we propose an algorithm called the weight\nadaptation trigger mechanism for decomposition-based MOEAs (ATM-MOEA\/D) to\ntackle this issue. ATM-MOEA\/D uses an archive to gradually approximate the\nshape of the Pareto front during the search. When the algorithm detects\nevolution stagnation (meaning the population no longer improves significantly),\nit compares the distribution of the population with that of the archive to\ndistinguish between regular and irregular Pareto fronts. Only when an irregular\nPareto front is identified, the weights are adapted. Our experimental results\nshow that the proposed algorithm not only performs generally better than seven\nstate-of-the-art weight-adapting methods on irregular Pareto fronts but also is\nable to achieve the same results as fixed-weight methods like MOEA\/D on regular\nPareto fronts.",
        "The mutation process in evolution strategies has been interlinked with the\nnormal distribution since its inception. Many lines of reasoning have been\ngiven for this strong dependency, ranging from maximum entropy arguments to the\nneed for isotropy. However, some theoretical results suggest that other\ndistributions might lead to similar local convergence properties. This paper\nempirically shows that a wide range of evolutionary strategies, from the\n(1+1)-ES to CMA-ES, show comparable optimization performance when using a\nmutation distribution other than the standard Gaussian. Replacing it with,\ne.g., uniformly distributed mutations, does not deteriorate the performance of\nES, when using the default adaptation mechanism for the strategy parameters. We\nobserve that these results hold not only for the sphere model but also for a\nwider range of benchmark problems.",
        "Subset selection is a fundamental problem in combinatorial optimization,\nwhich has a wide range of applications such as influence maximization and\nsparse regression. The goal is to select a subset of limited size from a ground\nset in order to maximize a given objective function. However, the evaluation of\nthe objective function in real-world scenarios is often noisy. Previous\nalgorithms, including the greedy algorithm and multi-objective evolutionary\nalgorithms POSS and PONSS, either struggle in noisy environments or consume\nexcessive computational resources. In this paper, we focus on the noisy subset\nselection problem with a cardinality constraint, where the evaluation of a\nsubset is noisy. We propose a novel approach based on Pareto Optimization with\nRobust Evaluation for noisy subset selection (PORE), which maximizes a robust\nevaluation function and minimizes the subset size simultaneously. PORE can\nefficiently identify well-structured solutions and handle computational\nresources, addressing the limitations observed in PONSS. Our experiments,\nconducted on real-world datasets for influence maximization and sparse\nregression, demonstrate that PORE significantly outperforms previous methods,\nincluding the classical greedy algorithm, POSS, and PONSS. Further validation\nthrough ablation studies confirms the effectiveness of our robust evaluation\nfunction.",
        "Spiking Neural Networks (SNNs) are distinguished from Artificial Neural\nNetworks (ANNs) for their sophisticated neuronal dynamics and sparse binary\nactivations (spikes) inspired by the biological neural system. Traditional\nneuron models use iterative step-by-step dynamics, resulting in serial\ncomputation and slow training speed of SNNs. Recently, parallelizable spiking\nneuron models have been proposed to fully utilize the massive parallel\ncomputing ability of graphics processing units to accelerate the training of\nSNNs. However, existing parallelizable spiking neuron models involve dense\nfloating operations and can only achieve high long-term dependencies learning\nability with a large order at the cost of huge computational and memory costs.\nTo solve the dilemma of performance and costs, we propose the mul-free\nchannel-wise Parallel Spiking Neuron, which is hardware-friendly and suitable\nfor SNNs' resource-restricted application scenarios. The proposed neuron\nimports the channel-wise convolution to enhance the learning ability, induces\nthe sawtooth dilations to reduce the neuron order, and employs the bit shift\noperation to avoid multiplications. The algorithm for design and implementation\nof acceleration methods is discussed meticulously. Our methods are validated in\nneuromorphic Spiking Heidelberg Digits voices, sequential CIFAR images, and\nneuromorphic DVS-Lip vision datasets, achieving the best accuracy among SNNs.\nTraining speed results demonstrate the effectiveness of our acceleration\nmethods, providing a practical reference for future research.",
        "The success of lexicase selection has led to various extensions, including\nits combination with down-sampling, which further increased performance.\nHowever, recent work found that down-sampling also leads to significant\nimprovements in the performance of tournament selection. This raises the\nquestion of whether tournament selection combined with down-sampling is the\nbetter choice, given its faster running times. To address this question, we run\na set of experiments comparing epsilon-lexicase and tournament selection with\ndifferent down-sampling techniques on synthetic problems of varying noise\nlevels and problem sizes as well as real-world symbolic regression problems.\nOverall, we find that down-sampling improves generalization and performance\neven when compared over the same number of generations. This means that\ndown-sampling is beneficial even with way fewer fitness evaluations.\nAdditionally, down-sampling successfully reduces code growth. We observe that\npopulation diversity increases for tournament selection when combined with\ndown-sampling. Further, we find that tournament selection and epsilon-lexicase\nselection with down-sampling perform similar, while tournament selection is\nsignificantly faster. We conclude that tournament selection should be further\nanalyzed and improved in future work instead of only focusing on the\nimprovement of lexicase variants.",
        "The nature inspired algorithms are becoming popular due to their simplicity\nand wider applicability. In the recent past several such algorithms have been\ndeveloped. They are mainly bio-inspired, swarm based, physics based and\nsocio-inspired; however, the domain based on symbiotic relation between\ncreatures is still to be explored. A novel metaheuristic optimization algorithm\nreferred to as Fig Tree-Wasp Symbiotic Coevolutionary (FWSC) algorithm is\nproposed. It models the symbiotic coevolutionary relationship between fig trees\nand wasps. More specifically, the mating of wasps, pollinating the figs,\nsearching for new trees for pollination and wind effect drifting of wasps are\nmodeled in the algorithm. These phenomena help in balancing the two important\naspects of exploring the search space efficiently as well as exploit the\npromising regions. The algorithm is successfully tested on a variety of test\nproblems. The results are compared with existing methods and algorithms. The\nWilcoxon Signed Rank Test and Friedman Test are applied for the statistical\nvalidation of the algorithm performance. The algorithm is also further applied\nto solve the real-world engineering problems. The performance of the FWSC\nunderscored that the algorithm can be applied to wider variety of real-world\nproblems.",
        "Graph Neural Networks (GNNs) excel at learning from graph-structured data but\nare limited to modeling pairwise interactions, insufficient for capturing\nhigher-order relationships present in many real-world systems. Topological Deep\nLearning (TDL) has allowed for systematic modeling of hierarchical higher-order\ninteractions by relying on combinatorial topological spaces such as simplicial\ncomplexes. In parallel, Quantum Neural Networks (QNNs) have been introduced to\nleverage quantum mechanics for enhanced computational and learning power. In\nthis work, we present the first Quantum Topological Deep Learning Model:\nQuantum Simplicial Networks (QSNs), being QNNs operating on simplicial\ncomplexes. QSNs are a stack of Quantum Simplicial Layers, which are inspired by\nthe Ising model to encode higher-order structures into quantum states.\nExperiments on synthetic classification tasks show that QSNs can outperform\nclassical simplicial TDL models in accuracy and efficiency, demonstrating the\npotential of combining quantum computing with TDL for processing data on\ncombinatorial topological spaces.",
        "Deep learning's success comes with growing energy demands, raising concerns\nabout the long-term sustainability of the field. Spiking neural networks,\ninspired by biological neurons, offer a promising alternative with potential\ncomputational and energy-efficiency gains. This article examines the\ncomputational properties of spiking networks through the lens of learning\ntheory, focusing on expressivity, training, and generalization, as well as\nenergy-efficient implementations while comparing them to artificial neural\nnetworks. By categorizing spiking models based on time representation and\ninformation encoding, we highlight their strengths, challenges, and potential\nas an alternative computational paradigm.",
        "Unmanned aerial vehicles (UAVs) have emerged as the potential aerial base\nstations (BSs) to improve terrestrial communications. However, the limited\nonboard energy and antenna power of a UAV restrict its communication range and\ntransmission capability. To address these limitations, this work employs\ncollaborative beamforming through a UAV-enabled virtual antenna array to\nimprove transmission performance from the UAV to terrestrial mobile users,\nunder interference from non-associated BSs and dynamic channel conditions.\nSpecifically, we introduce a memory-based random walk model to more accurately\ndepict the mobility patterns of terrestrial mobile users. Following this, we\nformulate a multi-objective optimization problem (MOP) focused on maximizing\nthe transmission rate while minimizing the flight energy consumption of the UAV\nswarm. Given the NP-hard nature of the formulated MOP and the highly dynamic\nenvironment, we transform this problem into a multi-objective Markov decision\nprocess and propose an improved evolutionary multi-objective reinforcement\nlearning algorithm. Specifically, this algorithm introduces an evolutionary\nlearning approach to obtain the approximate Pareto set for the formulated MOP.\nMoreover, the algorithm incorporates a long short-term memory network and\nhyper-sphere-based task selection method to discern the movement patterns of\nterrestrial mobile users and improve the diversity of the obtained Pareto set.\nSimulation results demonstrate that the proposed method effectively generates a\ndiverse range of non-dominated policies and outperforms existing methods.\nAdditional simulations demonstrate the scalability and robustness of the\nproposed CB-based method under different system parameters and various\nunexpected circumstances.",
        "Automotive radar systems face the challenge of managing high sampling rates\nand large data bandwidth while complying with stringent real-time and energy\nefficiency requirements. The growing complexity of autonomous vehicles further\nintensifies these requirements. Neuromorphic computing offers promising\nsolutions because of its inherent energy efficiency and parallel processing\ncapacity. This research presents a novel spiking neuron model for signal\nprocessing of frequency-modulated continuous wave (FMCW) radars that\noutperforms the state-of-the-art spectrum analysis algorithms in latency and\ndata bandwidth. These spiking neural resonators are based on the\nresonate-and-fire neuron model and optimized to dynamically process raw radar\ndata while simultaneously emitting an output in the form of spikes. We designed\nthe first neuromorphic neural network consisting of these spiking neural\nresonators that estimates range and angle from FMCW radar data. We evaluated\nthe range-angle maps on simulated datasets covering multiple scenarios and\ncompared the results with a state-of-the-art pipeline for radar processing. The\nproposed neuron model significantly reduces the processing latency compared to\ntraditional frequency analysis algorithms, such as the Fourier transformation\n(FT), which needs to sample and store entire data frames before processing. The\nevaluations demonstrate that these spiking neural resonators achieve\nstate-of-the-art detection accuracy while emitting spikes simultaneously to\nprocessing and transmitting only 0.02 % of the data compared to a float-32 FT.\nThe results showcase the potential for neuromorphic signal processing for FMCW\nradar systems and pave the way for designing neuromorphic radar sensors.",
        "Unmanned aerial vehicle (UAV)-assisted mobile edge computing (MEC) and data\ncollection (DC) have been popular research issues. Different from existing\nworks that consider MEC and DC scenarios separately, this paper investigates a\nmulti-UAV-assisted joint MEC-DC system. Specifically, we formulate a joint\noptimization problem to minimize the MEC latency and maximize the collected\ndata volume. This problem can be classified as a non-convex mixed integer\nprogramming problem that exhibits long-term optimization and dynamics. Thus, we\npropose a deep reinforcement learning-based approach that jointly optimizes the\nUAV movement, user transmit power, and user association in real time to solve\nthe problem efficiently. Specifically, we reformulate the optimization problem\ninto an action space-reduced Markov decision process (MDP) and optimize the\nuser association by using a two-phase matching-based association (TMA)\nstrategy. Subsequently, we propose a soft actor-critic (SAC)-based approach\nthat integrates the proposed TMA strategy (SAC-TMA) to solve the formulated\njoint optimization problem collaboratively. Simulation results demonstrate that\nthe proposed SAC-TMA is able to coordinate the two subsystems and can\neffectively reduce the system latency and improve the data collection volume\ncompared with other benchmark algorithms.",
        "This paper proposes a new hybrid algorithm, combining FA, SSO, and the N-R\nmethod to accelerate convergence towards global optima, named the Hybrid\nFirefly Algorithm and Sperm Swarm Optimization with Newton-Raphson (HFASSON).\nThe performance of HFASSON is evaluated using 23 benchmark functions from the\nCEC 2017 suite, tested in 30, 50, and 100 dimensions. A statistical comparison\nis performed to assess the effectiveness of HFASSON against FA, SSO, HFASSO,\nand five hybrid algorithms: Water Cycle Moth Flame Optimization (WCMFO), Hybrid\nParticle Swarm Optimization and Genetic Algorithm (HPSOGA), Hybrid Sperm Swarm\nOptimization and Gravitational Search Algorithm (HSSOGSA), Grey Wolf and Cuckoo\nSearch Algorithm (GWOCS), and Hybrid Firefly Genetic Algorithm (FAGA). Results\nfrom the Friedman rank test show the superior performance of HFASSON.\nAdditionally, HFASSON is applied to Cognitive Radio Vehicular Ad-hoc Networks\n(CR-VANET), outperforming basic CR-VANET in spectrum utilization. These\nfindings demonstrate HFASSON's efficiency in wireless network applications.",
        "This paper presents Hotter and Colder, a dataset designed to analyze various\ntypes of online behavior in Icelandic blog comments. Building on previous work,\nwe used GPT-4o mini to annotate approximately 800,000 comments for 25 tasks,\nincluding sentiment analysis, emotion detection, hate speech, and group\ngeneralizations. Each comment was automatically labeled on a 5-point Likert\nscale. In a second annotation stage, comments with high or low probabilities of\ncontaining each examined behavior were subjected to manual revision. By\nleveraging crowdworkers to refine these automatically labeled comments, we\nensure the quality and accuracy of our dataset resulting in 12,232 uniquely\nannotated comments and 19,301 annotations. Hotter and Colder provides an\nessential resource for advancing research in content moderation and\nautomatically detectiong harmful online behaviors in Icelandic.",
        "Context. Transition disks (TDs) are a type of protoplanetary disk\ncharacterized by a central dust and gas cavity. The processes behind how these\ncavities are formed and maintained, along with their observed high accretion\nrates of $10^{-8} -10^{-7} \\, M_{\\odot} \\, \\mathrm{yr}^{-1}$, continue to be\nsubjects of active research. Aims. This work aims to investigate how the\ninclusion of the Hall effect (HE) alongside Ohmic resistivity (OR) and\nambipolar diffusion (AD) affects the structure of the TD. Of key interest is\nthe dynamical evolution of the cavity and whether it can indeed produce\ntransonic accretion, as predicted by theoretical models in order to account for\nthe observed high accretion rates despite the inner disk's low density.\nMethods. We present our results of 2D axisymmetric global radiation\nmagnetohydrodynamic (MHD) simulations of TDs for which all three non-ideal MHD\neffects are accounted. We used the NIRVANA-III fluid code and initialized our\nmodel with a disk cavity reaching up to $R=8~\\mathrm{au}$ with a density\ncontrast of $10^5$. We performed three runs, one with only OR and AD, and one\nfor each of the two configurations that arise when additionally including the\nHE, that is, with the field aligned (anti-aligned) with respect to the rotation\naxis. Results. For all three runs, our models maintain an intact inner cavity\nand an outer standard disk. MHD winds are launched both from the cavity and\nfrom the disk. Notably, when the HE is included, ring-like structures develop\nwithin the cavity. We moreover obtain accretion rates of $3 - 8 \\times 10^{-8}\n\\, M_{\\odot} \\, \\mathrm{yr}^{-1}$, comparable to typical values seen in full\ndisks. Importantly, we clearly observe transonic accretion ($v_{\\mathrm{acc}}\n\\gtrsim c_{s}$) in the cavity. Additionally, outward magnetic flux transport\noccurs in all three runs.",
        "Vertical Federated Learning (VFL) is a privacy-preserving collaborative\nlearning paradigm that enables multiple parties with distinct feature sets to\njointly train machine learning models without sharing their raw data. Despite\nits potential to facilitate cross-organizational collaborations, the deployment\nof VFL systems in real-world applications remains limited. To investigate the\ngap between existing VFL research and practical deployment, this survey\nanalyzes the real-world data distributions in potential VFL applications and\nidentifies four key findings that highlight this gap. We propose a novel\ndata-oriented taxonomy of VFL algorithms based on real VFL data distributions.\nOur comprehensive review of existing VFL algorithms reveals that some common\npractical VFL scenarios have few or no viable solutions. Based on these\nobservations, we outline key research directions aimed at bridging the gap\nbetween current VFL research and real-world applications.",
        "It has been known for many years that there is an apparent trend for the\nspectral index ({\\alpha}) of radio sources to steepen with redshift z, which\nhas led to attempts to select high-redshift objects by searching for radio\nsources with steep spectra. In this study we use data from the MeerKAT, LOFAR,\nGMRT, and uGMRT telescopes, particularly using the MIGHTEE and superMIGHTEE\nsurveys, to select compact sources over a wide range of redshifts and\nluminosities. We investigate the relationship between spectral index,\nluminosity and redshift and compare our results to those of previous studies.\nAlthough there is a correlation between {\\alpha} and z in our sample for some\ncombinations of frequency where good data are available, there is a clear\noffset between the {\\alpha}-z relations in our sample and those derived\npreviously from samples of more luminous objects; in other words, the\n{\\alpha}-z relation is different for low and high luminosity sources. The\nrelationships between {\\alpha} and luminosity are also weak in our sample but\nin general the most luminous sources are steeper-spectrum and this trend is\nextended by samples from previous studies. In detail, we argue that both a\n{\\alpha}-luminosity relation and an {\\alpha}-z relation can be found in the\ndata, but it is the former that drives the apparent {\\alpha}-z relation\nobserved in earlier work, which only appears because of the strong\nredshift-luminosity relation in bright, flux density-limited samples.\nSteep-spectrum selection should be applied with caution in searching for high-z\nsources in future deep surveys.",
        "States with long coherence are a crucial requirement for qubits and quantum\nmemories. Nuclear spins in epitaxial quantum dots are a great candidate,\noffering excellent isolation from external environments and on-demand coupling\nto optical flying qubits. However, coherence times are limited to $\\lesssim1$\nms by the dipole-dipole interactions between the nuclei and their quadrupolar\ncoupling to inhomogeneous crystal strain. Here, we combine strain engineering\nof the nuclear spin ensemble and tailored dynamical decoupling sequences to\nachieve nuclear spin coherence times exceeding 100 ms. Recently, a reversible\ntransfer of quantum information into nuclear spin ensembles has been\ndemonstrated in quantum dots. Our results provide a path to develop this\nconcept into a functioning solid-state quantum memory suitable for quantum\nrepeaters in optical quantum communication networks.",
        "We investigate the contribution of the charm-anticharrm ($c{\\bar c}$)\nasymmetry of the proton eigenstate obtained from QCD lattice gauge to the\nasymmetry of $D^+D^-$ and $D^0{\\bar D}^0$ mesons produced in $pp$ collisions at\nlarge Feynman variables $x$. It is shown that an important tool for the\nestablishing the intrinsic charm (IC) content of the proton is the charm\nhadron-antihadron asymmetry formed in $pp$ collisions. Predictions for the\nasymmetry as function of $x$ for different IC probabilities are presented. We\nshow that the interference of the intrinsic $|uud c{\\bar c}>$ Fock state with\nthe standard contribution from the PQCD evolution leads to a large $D^+D^-$\nasymmetry at large Feynman $x$.",
        "We present an extension theorem for a separately holomorphic function which\nis polynomial\/rational in some variables.",
        "We establish that the singular numbers (arising from Cartan decomposition)\nand corners (emerging from Iwasawa decomposition) in split reductive groups\nover non-archimedean fields are fundamentally determined by Hall-Littlewood\npolynomials. Through applications of the Satake isomorphism, we extend Van\nPeski's results (arXiv:2011.09356, Theorem 1.3) to encompass arbitrary root\nsystems. Leveraging this theoretical foundation, we further develop Shen's work\n(arXiv:2411.01104, Theorem 1.1) to demonstrate that both singular numbers and\ncorners of such products exhibit minimal separation. This characterization\nenables the derivation of asymptotic properties for singular numbers in matrix\nproducts, particularly establishing the strong law of large numbers and central\nlimit theorem for these quantities. Our results provide a unified framework\nconnecting algebraic decomposition structures with probabilistic limit theorems\nin non-archimedean settings.",
        "In this paper, we study Random Dynamical Systems (RDSs) of homeomorphisms on\nthe circle without a finite orbit. We characterize the topological dynamics of\nthe associated semigroup by identifying the existence of invariant sets which\nare finite unions of intervals. We describe the accumulation points of the\naverage orbit of the transfer operator. For each ergodic stationary measure, we\ndemonstrate interesting properties of its weight function on the circle.\nRelationships between the minimal sets of an RDS and its inverse RDS are also\nestablished.",
        "The observations of eclipsing binary systems are of great importance in\nastrophysics, as they allow direct measurements of fundamental stellar\nparameters. By analysing high-quality space-based observations with\nground-based photometric data, it becomes possible to detect these fundamental\nparameters with greater precision using multicolour photometry. Here, we report\nthe first photometric analysis results of the V517 Cam eclipsing binary system\nby combining the Transiting Exoplanet Survey Satellite (TESS) light curve and\nnew CCD observations in BVRI filters, obtained with a 60 cm robotic telescope\n(T60) at the T\\\"UB\\.ITAK National Observatory. By means of photometric\nanalyses, the masses and radii of the primary and secondary stars were\ncarefully determined to be $M_{1}= 1.47\\pm 0.06\\,M_\\odot$, $M_{2}=\n0.79\\pm0.05\\,M_\\odot$, and $R_{1}=1.43\\pm 0.03\\,R_\\odot$, $R_{2}= 0.75\\pm\n0.04\\,R_\\odot$, respectively. Furthermore, the distance to V517 Cam was\ncalculated to be $284\\pm20$ pc. The overall age of the system is estimated to\nbe around $63\\pm15$ Myr. At this age, the primary component stands near the\nonset of its main-sequence evolution, near the ZAMS, whereas the secondary\ncomponent remains in the pre-main-sequence evolutionary phase. To better\nunderstand the evolutionary status and nature of V517 Cam, the mass ratio and\ntemperature values, obtained with relatively low sensitivity by photometric\nmeasurements, need to be confirmed by spectral analysis.",
        "We study a quantum battery (QB) model composed of two atoms, where the\ncharger and battery elements are coupled to a multimode vacuum field that\nserves as a mediator for energy transfer. Different figures of merit such as\nergotropy, charging time, and charging efficiency are analyzed, putting\nemphasis on the role of various control parameters on the charging performance.\nIt is found that there is a range of angle between the transition dipole\nmoments and interatomic axis in which the QB can be charged. The optimal\ncharging performance is achieved if the atomic dipole moments are perpendicular\nor parallel to the interatomic axis. The charging performance also improves\nwith the decrease of the interatomic distance. Besides, the charged ergotropy\ncan be enhanced by increasing the initial ergotropy of the charger and it is\nbeneficial to charge the QB starting from a passive state.",
        "Modern web agents possess computer use abilities that allow them to interact\nwith webpages by sending commands to a virtual keyboard and mouse. While such\nagents have considerable potential to assist human users with complex tasks,\nevaluating their capabilities in real-world settings poses a major challenge.\nTo this end, we introduce BEARCUBS, a \"small but mighty\" benchmark of 111\ninformation-seeking questions designed to evaluate a web agent's ability to\nsearch, browse, and identify factual information from the web. Unlike prior web\nagent benchmarks, solving BEARCUBS requires (1) accessing live web content\nrather than synthetic or simulated pages, which captures the unpredictability\nof real-world web interactions; and (2) performing a broad range of multimodal\ninteractions (e.g., video understanding, 3D navigation) that cannot be bypassed\nvia text-based workarounds. Each question in BEARCUBS has a corresponding\nshort, unambiguous answer and a human-validated browsing trajectory, allowing\nfor transparent evaluation of agent performance and strategies. A human study\nconfirms that BEARCUBS questions are solvable but non-trivial (84.7% human\naccuracy), revealing search inefficiencies and domain knowledge gaps as common\nfailure points. By contrast, state-of-the-art computer-using agents\nunderperform, with the best-scoring system (OpenAI's Operator) reaching only\n24.3% accuracy. These results highlight critical areas for improvement,\nincluding reliable source selection and more powerful multimodal capabilities.\nTo facilitate future research, BEARCUBS will be updated periodically to replace\ninvalid or contaminated questions, keeping the benchmark fresh for future\ngenerations of web agents.",
        "We investigate the effects of interfacial spin-orbit coupling (ISOC) on\nsuperconductors, focusing on its impact on electronic transport and spin-charge\nconversion. Using a symmetry-based nonlinear sigma model, we derive effective\nboundary conditions for the Usadel and Maxwell equations that account for\nspin-galvanic effect, spin relaxation, and spin precession. This approach\nallows for the analysis of various interfaces without relying on specific\nmicroscopic models. We apply these boundary conditions to derive ISOC-induced\nterms in the Ginzburg-Landau functional, which is then used to compute the\ncritical temperature of superconducting films with ISOC subjected to an\nexternal magnetic field. Our findings show that, contrary to a recent\nprediction, the critical temperature of a film cannot be enhanced by an\nexternal magnetic field. Additionally, we demonstrate that the combination of\nISOC and an external magnetic field leads to a superconducting diode effect.\nIts efficiency strongly depends on the interplay between the spin-galvanic and\nthe spin relaxation terms. Our results provide a framework for understanding\nISOC in superconducting systems and highlight the potential for optimizing\ndiode efficiency through careful interface engineering",
        "This paper examines the Galileo Open Service Navigation Message\nAuthentication (OSNMA) and, for the first time, discovers two critical\nvulnerabilities, namely artificially-manipulated time synchronization (ATS) and\ninterruptible message authentication (IMA). ATS allows attackers falsify a\nreceiver's signals and\/or local reference time (LRT) while still fulfilling the\ntime synchronization (TS) requirement. IMA allows temporary interruption of the\nnavigation data authentication process due to the reception of a broken message\n(probably caused by spoofing attacks) and restores the authentication later. By\nexploiting the ATS vulnerability, we propose a TS-comply replay (TSR) attack\nwith two variants (real-time and non-real-time), where attackers replay signals\nto a victim receiver while strictly complying with the TS rule. We further\npropose a TS-comply forgery (TSF) attack, where attackers first use a\npreviously-disclosed key to forge a message based on the OSNMA protocol, then\ntamper with the vitcim receiver's LRT correspondingly to comply with the TS\nrule and finally transmit the forged message to the receiver. Finally, we\npropose a concatenating replay (CR) attack based on the IMA vulnerability,\nwhere attackers concatenate replayed signals to the victim receiver's signals\nin a way that still enables correct verification of the navigation data in the\nreplayed signals. To validate the effectiveness of the proposed attacks, we\nconduct real-world experiments with a commercial Galileo receiver manufactured\nby Septentrio, two software-defined radio (SDR) devices, open-source\nGalileo-SDR-SIM and OSNMAlib software. The results showed that all the attacks\ncan successfully pass the OSNMA scheme and the TSF attack can spoof receivers\nto arbitrary locations.",
        "The performance of pavement under loading depends on the strength of the\nsubgrade. However, experimental estimation of properties of pavement strengths\nsuch as California bearing ratio (CBR), unconfined compressive strength (UCS)\nand resistance value (R) are often tedious, time-consuming and costly, thereby\ninspiring a growing interest in machine learning based tools which are simple,\ncheap and fast alternatives. Thus, the potential application of two boosting\ntechniques; categorical boosting (CatBoost) and extreme gradient boosting\n(XGBoost) and support vector regression (SVR), is similarly explored in this\nstudy for estimation of properties of subgrade soil modified with hydrated lime\nactivated rice husk ash (HARSH). Using 121 experimental data samples of varying\nproportions of HARSH, plastic limit, liquid limit, plasticity index, clay\nactivity, optimum moisture content, and maximum dry density as input for CBR,\nUCS and R estimation, four evaluation metrics namely coefficient of\ndetermination (R2), root mean squared error (RMSE), mean absolute error (MAE)\nand mean absolute percentage error (MAPE) are used to evaluate the models'\nperformance. The results indicate that XGBoost outperformed CatBoost and SVR in\nestimating these properties, yielding R2 of 0.9994, 0.9995 and 0.9999 in\nestimating the CBR, UCS and R respectively. Also, SVR outperformed CatBoost in\nestimating the CBR and R with R2 of 0.9997 respectively. On the other hand,\nCatBoost outperformed SVR in estimating the UCS with R2 of 0.9994. Feature\nsensitivity analysis shows that the three machine learning techniques are\nunanimous that increasing HARSH proportion lead to values of the estimated\nproperties respectively. A comparison with previous results also shows\nsuperiority of XGBoost in estimating subgrade properties."
      ]
    }
  },
  {
    "id":2411.0675,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"3-D ultrasound imaging: a review",
    "start_abstract":"The development of 3-D ultrasound imaging is a way to address the disadvantages conventional imaging. In this article authors review approaches that have been attempted in such as B-mode, color Doppler, and power Doppler systems. Acquisition, reconstruction, rendering techniques for are discussed, well applications limitations.",
    "start_categories":[
      "cs.CV",
      "eess.IV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b13"
      ],
      "title":[
        "Symmetric diffeomorphic image registration with crosscorrelation: evaluating automated labeling of elderly and neurodegenerative brain"
      ],
      "abstract":[
        "One of the most challenging problems in modern neuroimaging is detailed characterization of neurodegeneration. Quantifying spatial and longitudinal atrophy patterns is an important component of this process. These spatiotemporal signals will aid in discriminating between related diseases, such as frontotemporal dementia (FTD) and Alzheimer's disease (AD), which manifest themselves in the same at-risk population. Here, we develop a novel symmetric image normalization method (SyN) for maximizing the cross-correlation within the space of diffeomorphic maps and provide the Euler-Lagrange equations necessary for this optimization. We then turn to a careful evaluation of our method. Our evaluation uses gold standard, human cortical segmentation to contrast SyN's performance with a related elastic method and with the standard ITK implementation of Thirion's Demons algorithm. The new method compares favorably with both approaches, in particular when the distance between the template brain and the target brain is large. We then report the correlation of volumes gained by algorithmic cortical labelings of FTD and control subjects with those gained by the manual rater. This comparison shows that, of the three methods tested, SyN's volume measurements are the most strongly correlated with volume measurements gained by expert labeling. This study indicates that SyN, with cross-correlation, is a reliable method for normalizing and making anatomical measurements in volumetric MRI of patients and at-risk elderly individuals."
      ],
      "categories":[
        "q-bio.NC"
      ]
    },
    "list":{
      "title":[
        "Neural encoding with affine feature response transforms",
        "Evolution of diverse (and advanced) cognitive abilities through adaptive\n  fine-tuning of learning and chunking mechanisms",
        "Fluctuation-response relations and response-response relations for\n  membrane voltage and spike train of stochastic integrate-and-fire neurons",
        "Using economic value signals from primate prefrontal cortex in\n  neuro-engineering applications",
        "Automatic target validation based on neuroscientific literature mining\n  for tractography",
        "The Role of Affective States in Computational Psychiatry",
        "Vagus nerve stimulation as a modulator of feedforward and feedback\n  neural transmission",
        "Multi-Site rs-fMRI Domain Alignment for Autism Spectrum Disorder\n  Auxiliary Diagnosis Based on Hyperbolic Space",
        "From Thought to Action: How a Hierarchy of Neural Dynamics Supports\n  Language Production",
        "Structuring the Environment Nudges Participants Toward Hierarchical Over\n  Shortest Path Planning",
        "The Neural Basis of Groove Sensations: Implications for Music-Based\n  Interventions and Dance Therapy in Parkinson's Disease",
        "Role of connectivity anisotropies in the dynamics of cultured neuronal\n  networks",
        "On Questions of Predictability and Control of an Intelligent System\n  Using Probabilistic State-Transitions",
        "Super-resolution Live-cell Fluorescence Lifetime Imaging",
        "A New Circle Theorem for Two Dimensional Ising Spin Glasses",
        "Learning Conditional Average Treatment Effects in Regression\n  Discontinuity Designs using Bayesian Additive Regression Trees",
        "Reducing Frequency Bias of Fourier Neural Operators in 3D Seismic\n  Wavefield Simulations Through Multi-Stage Training",
        "Left Jacobson Rings",
        "Safe exploration in reproducing kernel Hilbert spaces",
        "Orthogonal Delay-Doppler Division Multiplexing Modulation with\n  Hierarchical Mode-Based Index Modulation",
        "Periodic Response Solutions to Multi-Dimensional Nonlinear Schr\\\"odinger\n  equation with unbounded perturbation",
        "DESI DR2 Results II: Measurements of Baryon Acoustic Oscillations and\n  Cosmological Constraints",
        "Spectral modelling of Cygnus A between 110 and 250 MHz. Impact on the\n  LOFAR 21-cm signal power spectrum",
        "Calculating the Hawking Temperature of Black Holes in $f(Q)$ Gravity\n  Using the RVB Method: A Residue-Based Approach",
        "Slow Light Waveguides based on Bound States in the Continuum",
        "Multivariable $p$-adic Hodge theory for products of Galois groups",
        "Convex Split Lemma without Inequalities",
        "FGM Modeling of Thermo-Diffusive Unstable Lean Premixed Hydrogen-Air\n  Flames"
      ],
      "abstract":[
        "Current linearizing encoding models that predict neural responses to sensory\ninput typically neglect neuroscience-inspired constraints that could enhance\nmodel efficiency and interpretability. To address this, we propose a new method\ncalled affine feature response transform (AFRT), which exploits the brain's\nretinotopic organization. Applying AFRT to encode multi-unit activity in areas\nV1, V4, and IT of the macaque brain, we demonstrate that AFRT reduces redundant\ncomputations and enhances the performance of current linearizing encoding\nmodels by segmenting each neuron's receptive field into an affine retinal\ntransform, followed by a localized feature response. Remarkably, by factorizing\nreceptive fields into a sequential affine component with three interpretable\nparameters (for shifting and scaling) and response components with a small\nnumber of feature weights per response, AFRT achieves encoding with orders of\nmagnitude fewer parameters compared to unstructured models. We show that the\nretinal transform of each neuron's encoding agrees well with the brain's\nreceptive field. Together, these findings suggest that this new subset within\nspatial transformer network can be instrumental in neural encoding models of\nnaturalistic stimuli.",
        "The evolution of cognition is frequently discussed as the evolution of\ncognitive abilities or the evolution of some neuronal structures in the brain.\nHowever, since such traits or abilities are often highly complex, understanding\ntheir evolution requires explaining how they could have gradually evolved\nthrough selection acting on heritable variations in simpler cognitive\nmechanisms. With this in mind, making use of a previously proposed theory, here\nwe show how the evolution of cognitive abilities can be captured by the\nfine-tuning of basic learning mechanisms and, in particular, chunking\nmechanisms. We use the term chunking broadly for all types of non-elemental\nlearning, claiming that the process by which elements are combined into chunks\nand associated with other chunks, or elements, is critical for what the brain\ncan do, and that it must be fine-tuned to ecological conditions. We discuss the\nrelevance of this approach to studies in animal cognition, using examples from\nanimal foraging and decision-making, problem solving, and cognitive\nflexibility. Finally, we explain how even the apparent human-animal gap in\nsequence learning ability can be explained in terms of different fine-tunings\nof a similar chunking process.",
        "Neurons display spontaneous spiking (in the absence of stimulus signals) as\nwell as a characteristic response to time-dependent external stimuli. In a\nsimple but important class of stochastic neuron models, the integrate-and-fire\nmodel with Gaussian current noise, both aspects can mathematically be related\nvia fluctuation-response relations (FRRs) as has been shown recently. Here we\nextend the class of FRRs to include the susceptibilities of the membrane\nvoltage and subthreshold voltage nonlinearity as well as the power spectrum of\nthe membrane voltage. For a simple but often considered IF model, the leaky IF\nmodel with white Gaussian noise, we exploit the FRRs and derive explicit\nexpressions for the power spectrum and susceptibility of the subthreshold\nmembrane voltage. We also put forward a relation between the response functions\nof the spike train and membrane voltage, a response-response relation (RRR)\nthat holds true for a more general setting than considered in most parts of the\npaper. For the generalized IF model with an adaptation current and colored\nGaussian noise we derive an FRR and an RRR. We briefly discuss useful\napplications of the derived FRRs and RRRs.",
        "Neural signals related to movement can be measured from intracranial\nrecordings and used in brain-machine interface devices (BMI) to restore\nphysical function in impaired patients. In this study, we explore the use of\nmore abstract neural signals related to economic value in a BMI context. Using\ndata collected from the orbitofrontal cortex in non-human primates, we develop\ndeep learning-based neural decoders that can predict the monkey's choice in a\nvalue-based decision-making task. Out-of-sample performance was improved by\naugmenting the training set with synthesized data, showing the feasibility of\nusing limited training data. We further demonstrate that we can predict the\nmonkey's choice sooner using a neural forecasting module that is equipped with\ntask-related information. These findings support the feasibility of user\npreference-informed neuroengineering devices that leverage abstract cognitive\nsignals.",
        "Target identification for tractography studies requires solid anatomical\nknowledge validated by an extensive literature review across species for each\nseed structure to be studied. Manual literature review to identify targets for\na given seed region is tedious and potentially subjective. Therefore,\ncomplementary approaches would be useful. We propose to use text-mining models\nto automatically suggest potential targets from the neuroscientific literature,\nfull-text articles and abstracts, so that they can be used for anatomical\nconnection studies and more specifically for tractography. We applied\ntext-mining models to three structures: two well-studied structures, since\nvalidated deep brain stimulation targets, the internal globus pallidus and the\nsubthalamic nucleus and, the nucleus accumbens, an exploratory target for\ntreating psychiatric disorders. We performed a systematic review of the\nliterature to document the projections of the three selected structures and\ncompared it with the targets proposed by text-mining models, both in rat and\nprimate (including human). We ran probabilistic tractography on the nucleus\naccumbens and compared the output with the results of the text-mining models\nand literature review. Overall, text-mining the literature could find three\ntimes as many targets as two man-weeks of curation could. The overall\nefficiency of the text-mining against literature review in our study was 98%\nrecall (at 36% precision), meaning that over all the targets for the three\nselected seeds, only one target has been missed by text-mining. We demonstrate\nthat connectivity for a structure of interest can be extracted from a very\nlarge amount of publications and abstracts. We believe this tool will be useful\nin helping the neuroscience community to facilitate connectivity studies of\nparticular brain regions. The text mining tools used for the study are part of\nthe HBP Neuroinformatics Platform, publicly available at\nhttp:\/\/connectivity-brainer.rhcloud.com",
        "Studying psychiatric illness has often been limited by difficulties in\nconnecting symptoms and behavior to neurobiology. Computational psychiatry\napproaches promise to bridge this gap by providing formal accounts of the\nlatent information processing changes that underlie the development and\nmaintenance of psychiatric phenomena. Models based on these theories generate\nindividual-level parameter estimates which can then be tested for relationships\nto neurobiology. In this review, we explore computational modelling approaches\nto one key aspect of health and illness: affect. We discuss strengths and\nlimitations of key approaches to modelling affect, with a focus on\nreinforcement learning, active inference, the hierarchical gaussian filter, and\ndrift-diffusion models. We find that, in this literature, affect is an\nimportant source of modulation in decision making, and has a bidirectional\ninfluence on how individuals infer both internal and external states.\nHighlighting the potential role of affect in information processing changes\nunderlying symptom development, we extend an existing model of psychosis, where\naffective changes are influenced by increasing cortical noise and consequent\nincreases in either perceived environmental instability or expected noise in\nsensory input, becoming part of a self-reinforcing process generating\nnegatively valenced, over-weighted priors underlying positive symptom\ndevelopment. We then provide testable predictions from this model at\ncomputational, neurobiological, and phenomenological levels of description.",
        "Vagus nerve stimulation (VNS) has emerged as a promising therapeutic\nintervention across various neurological and psychiatric conditions, including\nepilepsy, depression, and stroke rehabilitation; however, its mechanisms of\naction on neural circuits remain incompletely understood. Here, we present a\nnovel theoretical framework based on predictive coding that conceptualizes VNS\neffects through differential modulation of feedforward and feedback neural\ncircuits. Based on recent evidence, we propose that VNS shifts the balance\nbetween feedforward and feedback processing through multiple neuromodulatory\nsystems, resulting in enhanced feedforward signal transmission. This framework\nintegrates anatomical pathways, receptor distributions, and physiological\nresponses to explain the influence of the VNS on neural dynamics across\ndifferent spatial and temporal scales. VNS may facilitate neural plasticity and\nadaptive behavior through acetylcholine and noradrenaline (norepinephrine),\nwhich differentially modulate feedforward and feedback signaling. This\nmechanistic understanding serves as a basis for interpreting the cognitive and\ntherapeutic outcomes across different clinical conditions. Our perspective\nprovides a unified theoretical framework for understanding circuit-specific VNS\neffects and suggests new directions for investigating their therapeutic\nmechanisms.",
        "In the medical field, most resting-state fMRI (rs-fMRI) data are collected\nfrom multiple hospital sites. Multi-site rs-fMRI data can increase the volume\nof training data, enabling auxiliary diagnostic algorithms for brain diseases\nto learn more accurate and stable models. However, due to the significant\nheterogeneity and domain shift in rs-fMRI data across different sites, the\naccuracy of auxiliary diagnosis remains unsatisfactory. Moreover, there has\nbeen limited exploration of multi-source domain adaptation algorithms, and the\ninterpretability of models is often poor. To address these challenges, we\nproposed a domain-adaptive algorithm based on hyperbolic space embedding.\nHyperbolic space is naturally suited for representing the topology of complex\nnetworks such as brain functional networks. Therefore, we embedded the brain\nfunctional network into hyperbolic space and constructed the corresponding\nhyperbolic space community network to effectively extract brain network\nrepresentations. To address the heterogeneity of data across different sites\nand the issue of domain shift, we introduce a constraint loss function, HMMD\n(Hyperbolic Maximum Mean Discrepancy), to align the marginal distributions in\nthe hyperbolic space. Additionally, we employ class prototype alignment to\nalign the conditional distributions. This significantly improves the quality of\nbrain representations and enhances diagnostic classification accuracy for\nAutism Spectrum Disorder (ASD). Experimental results demonstrated that the\nproposed algorithm is robust to multi-site heterogeneity and shows promising\npotential for brain network mechanism analysis.",
        "Humans effortlessly communicate their thoughts through intricate sequences of\nmotor actions. Yet, the neural processes that coordinate language production\nremain largely unknown, in part because speech artifacts limit the use of\nneuroimaging. To elucidate the unfolding of language production in the brain,\nwe investigate with magnetoencephalography (MEG) and electroencephalography\n(EEG) the neurophysiological activity of 35 skilled typists, while they typed\nsentences on a keyboard. This approach confirms the hierarchical predictions of\nlinguistic theories: the neural activity preceding the production of each word\nis marked by the sequential rise and fall of context-, word-, syllable-, and\nletter-level representations. Remarkably, each of these neural representations\nis maintained over long time periods within each level of the language\nhierarchy. This phenomenon results in a superposition of successive\nrepresentations that is supported by a hierarchy of dynamic neural codes.\nOverall, these findings provide a precise computational breakdown of the neural\ndynamics that coordinate the production of language in the human brain.",
        "Effective planning is crucial for navigating complex environments and\nachieving goals efficiently. In this study, we investigated how environmental\nstructure influences the selection of planning strategies. Participants\nnavigated a space station to collect colored spheres, with environments either\nstructured (spheres grouped by color) or unstructured (spheres scattered\nrandomly). We tested three types of plans: hierarchical (grouping spheres by\ncolor), shortest path (minimizing travel distance), and neutral (none of the\nabove). By manipulating environmental structure, we were able to nudge\nparticipants toward a preference for hierarchical planning in structured\nenvironments, while shortest path plans were favored in unstructured\nenvironments. A mismatch between self-reported preferences and actual choices\nindicated that participants often adopted implicit strategies, unaware of their\ndecision-making processes. These findings highlight the powerful effect of\nenvironmental cues on planning and suggest that even subtle changes in\nstructure can guide the selection of planning strategies.",
        "Groove sensations arise from rhythmic structures that evoke an urge to move\nin response to music. While syncopation has been extensively studied in groove\nperception, the neural mechanisms underlying low-frequency groove remain\nunderexplored. This fMRI study examines the role of the mirror neuron system\nand associated brain regions in processing low-frequency groove.\nRegion-of-interest analysis revealed that amplifying drum and bass components\nin K-pop songs significantly increased activity in the right posterior inferior\nfrontal gyrus, right inferior\/superior parietal lobules, left dorsolateral\nprefrontal cortex, and bilateral posterior middle\/inferior temporal gyrus.\nThese findings suggest that low-frequency grooves engage sensorimotor,\nexecutive, and rhythm semantics networks, reinforcing their role in\naction-related processing. Building on these insights, we propose an enhanced\nrhythmic auditory stimulation paradigm for Parkinson's disease, incorporating\namplified low-frequency rhythmic cues to improve gait synchronization.",
        "Laboratory-grown, engineered living neuronal networks in vitro have emerged\nin the last years as an experimental technique to understand the collective\nbehavior of neuronal assemblies in relation to their underlying connectivity.\nAn inherent obstacle in the design of such engineered systems is the difficulty\nto predict the dynamic repertoire of the emerging network and its dependence on\nexperimental variables. To fill this gap, and inspired on recent experimental\nstudies, here we present a numerical model that aims at, first, replicating the\nanisotropies in connectivity imprinted through engineering, to next realize the\ncollective behavior of the neuronal network and make predictions. We use\nexperimentally measured, biologically-realistic data combined with the\nIzhikevich model to quantify the dynamics of the neuronal network in relation\nto tunable structural and dynamical parameters. These parameters include the\nsynaptic noise, strength of the imprinted anisotropies, and average axon\nlengths. The latter are involved in the simulation of the development of\nneurons in vitro. We show that the model captures the behavior of engineered\nneuronal cultures, in which a rich repertoire of activity patterns emerge but\nwhose details are strongly dependent on connectivity details and noise. Results\nalso show that the presence of connectivity anisotropies substantially improves\nthe capacity of reconstructing structural connectivity from activity data, an\naspect that is important in the quest for understanding the\nstructure-to-function relationship in neuronal networks. Our work provides the\nin silico basis to assist experimentalists in the design of laboratory in vitro\nnetworks and anticipate their outcome, an aspect that is particularly important\nin the effort to conceive reliable brain-on-a-chip circuits and explore key\naspects such as input-output relationships or information coding.",
        "One of the central aims of neuroscience is to reliably predict the behavioral\nresponse of an organism using its neural activity. If possible, this implies we\ncan causally manipulate the neural response and design brain-computer-interface\nsystems to alter behavior, and vice-versa. Hence, predictions play an important\nrole in both fundamental neuroscience and its applications. Can we predict the\nneural and behavioral states of an organism at any given time? Can we predict\nbehavioral states using neural states, and vice-versa, and is there a\nmemory-component required to reliably predict such states? Are the predictions\ncomputable within a given timescale to meaningfully stimulate and make the\nsystem reach the desired states? Through a series of mathematical treatments,\nsuch conjectures and questions are discussed. Answering them might be key for\nfuture developments in understanding intelligence and designing\nbrain-computer-interfaces.",
        "Super-resolution Structured Illumination Microscopy (SR-SIM) enables\nfluorescence microscopy beyond the diffraction limit at high frame rates.\nCompared to other super-resolution microscopy techniques, the low photon\nfluence used in SR-SIM makes it readily compatible with live-cell imaging.\nHere, we combine SR-SIM with electro-optic fluorescence lifetime imaging\n(EOFLIM), adding the capability of monitoring physicochemical parameters with\n156 nm spatial resolution at high frame rate for live-cell imaging. We\ndemonstrate that our new SIMFLIM technique enables super-resolved multiplexed\nimaging of spectrally overlapping fluorophores, environmental sensing, and\nlive-cell imaging.",
        "The Lee-Yang circle theorem revolutionized our understanding of phase\ntransitions in ferromagnetic systems by showing that the complex zeros of\npartition functions lie on the unit circle, with criticality arising as these\nzeros approach the real axis in the thermodynamic limit. However, in frustrated\nsystems such as antiferromagnets and spin glasses, the zeros deviate from this\nstructure, making it challenging to extend the Lee-Yang theory to disordered\nsystems. In this work, we establish a new circle theorem for two-dimensional\nIsing spin glasses, proving that the square of the partition function exhibits\nzeros densely packed along the unit circle. Numerical simulations on the square\nlattice confirm our theoretical predictions, demonstrating the validity of the\ncircle law for quenched disorder. Furthermore, our results uncover a\nfinite-temperature crossover in $\\pm J$ spin glasses, characterized by the\nemergence of a spectral gap in the angular distribution of zeros. This result\nextends the Lee-Yang framework to disordered systems, offering new insights\ninto spin-glass criticality.",
        "BART (Bayesian additive regression trees) has been established as a leading\nsupervised learning method, particularly in the field of causal inference. This\npaper explores the use of BART models for learning conditional average\ntreatment effects (CATE) from regression discontinuity designs, where treatment\nassignment is based on whether an observed covariate (called the running\nvariable) exceeds a pre-specified threshold. A purpose-built version of BART\nthat uses linear regression leaf models (of the running variable and treatment\nassignment dummy) is shown to out-perform off-the-shelf BART implementations as\nwell as a local polynomial regression approach and a CART-based approach. The\nnew method is evaluated in thorough simulation studies as well as an empirical\napplication looking at the effect of academic probation on student performance.",
        "The recent development of Neural Operator (NeurOp) learning for solutions to\nthe elastic wave equation shows promising results and provides the basis for\nfast large-scale simulations for different seismological applications. In this\npaper, we use the Fourier Neural Operator (FNO) model to directly solve the 3D\nHelmholtz wave equation for fast seismic ground motion simulations on different\nfrequencies, and show the frequency bias of the FNO model, i.e. it learns the\nlower frequencies better comparing to the higher frequencies. To reduce the\nfrequency bias, we adopt the multi-stage FNO training, i.e., after training a\n1st stage FNO model for estimating the ground motion, we use a second FNO model\nas the 2nd stage to learn from the residual, which greatly reduced the errors\non the higher frequencies. By adopting this multi-stage training, the FNO\nmodels show reduced biases on higher frequencies, which enhanced the overall\nresults of the ground motion simulations. Thus the multi-stage training FNO\nimproves the accuracy and realism of the ground motion simulations.",
        "We say that a ring is strongly (resp. weakly) left Jacobson if every\nsemiprime (resp. prime) left ideal is an intersection of maximal left ideals.\nThere exist Jacobson rings that are not weakly left Jacobson, e.g. the Weyl\nalgebra. Our main result is the following one-sided noncommutative\nNullstellensatz: For any finite-dimensional F-algebra A the ring\nA[$x_1$,...,$x_n$] of polynomials with coefficients in A is strongly left\nJacobson and every maximal left ideal of A[$x_1$,...,$x_n$] has a special form.\nWe also prove that an algebra that is a finitely generated module over its\ncenter is weakly left Jacobson iff it is Jacobson, and that an Azumaya algebra\nis strongly left Jacobson iff its center is Jacobson.",
        "Popular safe Bayesian optimization (BO) algorithms learn control policies for\nsafety-critical systems in unknown environments. However, most algorithms make\na smoothness assumption, which is encoded by a known bounded norm in a\nreproducing kernel Hilbert space (RKHS). The RKHS is a potentially\ninfinite-dimensional space, and it remains unclear how to reliably obtain the\nRKHS norm of an unknown function. In this work, we propose a safe BO algorithm\ncapable of estimating the RKHS norm from data. We provide statistical\nguarantees on the RKHS norm estimation, integrate the estimated RKHS norm into\nexisting confidence intervals and show that we retain theoretical guarantees,\nand prove safety of the resulting safe BO algorithm. We apply our algorithm to\nsafely optimize reinforcement learning policies on physics simulators and on a\nreal inverted pendulum, demonstrating improved performance, safety, and\nscalability compared to the state-of-the-art.",
        "The orthogonal time frequency space with index modulation (OTFS-IM) offers\nflexible tradeoffs between spectral efficiency (SE) and bit error rate (BER) in\ndoubly selective fading channels. While OTFS-IM schemes demonstrated such\npotential, a persistent challenge lies in the detection complexity. To address\nthis problem, we propose the hierarchical mode-based index modulation (HMIM).\nHMIM introduces a novel approach to modulate information bits by IM patterns,\nsignificantly simplifying the complexity of maximum a posteriori (MAP)\nestimation with Gaussian noise. Further, we incorporate HMIM with the recently\nproposed orthogonal delay-Doppler division multiplexing (ODDM) modulation,\nnamely ODDM-HMIM, to exploit the full diversity of the delay-Doppler (DD)\nchannel. The BER performance of ODDM-HMIM is analyzed considering a maximum\nlikelihood (ML) detector. Our numerical results reveal that, with the same SE,\nHMIM can outperform conventional IM in terms of both BER and computational\ncomplexity. In addition, we propose a successive interference\ncancellation-based minimum mean square error (SIC-MMSE) detector for ODDM-HMIM,\nwhich enables low-complexity detection with large frame sizes.",
        "By applying the Craig-Wayne-Bourgain (CWB) method, we establish the existence\nof periodic response solutions to multi-dimensional nonlinear Schr\\\"{o}dinger\nequations (NLS) with unbounded perturbation.",
        "We present baryon acoustic oscillation (BAO) measurements from more than 14\nmillion galaxies and quasars drawn from the Dark Energy Spectroscopic\nInstrument (DESI) Data Release 2 (DR2), based on three years of operation. For\ncosmology inference, these galaxy measurements are combined with DESI\nLyman-$\\alpha$ forest BAO results presented in a companion paper. The DR2 BAO\nresults are consistent with DESI DR1 and SDSS, and their distance-redshift\nrelationship matches those from recent compilations of supernovae (SNe) over\nthe same redshift range. The results are well described by a flat $\\Lambda$CDM\nmodel, but the parameters preferred by BAO are in mild, $2.3\\sigma$ tension\nwith those determined from the cosmic microwave background (CMB), although the\nDESI results are consistent with the acoustic angular scale $\\theta_*$ that is\nwell-measured by Planck. This tension is alleviated by dark energy with a\ntime-evolving equation of state parametrized by $w_0$ and $w_a$, which provides\na better fit to the data, with a favored solution in the quadrant with $w_0>-1$\nand $w_a<0$. This solution is preferred over $\\Lambda$CDM at $3.1\\sigma$ for\nthe combination of DESI BAO and CMB data. When also including SNe, the\npreference for a dynamical dark energy model over $\\Lambda$CDM ranges from\n$2.8-4.2\\sigma$ depending on which SNe sample is used. We present evidence from\nother data combinations which also favor the same behavior at high\nsignificance. From the combination of DESI and CMB we derive 95% upper limits\non the sum of neutrino masses, finding $\\sum m_\\nu<0.064$ eV assuming\n$\\Lambda$CDM and $\\sum m_\\nu<0.16$ eV in the $w_0w_a$ model. Unless there is an\nunknown systematic error associated with one or more datasets, it is clear that\n$\\Lambda$CDM is being challenged by the combination of DESI BAO with other\nmeasurements and that dynamical dark energy offers a possible solution.",
        "Studying the redshifted 21-cm signal from the the neutral hydrogen during the\nEpoch of Reionization and Cosmic Dawn is fundamental for understanding the\nphysics of the early universe. One of the challenges that 21-cm experiments\nface is the contamination by bright foreground sources, such as Cygnus A, for\nwhich accurate spatial and spectral models are needed to minimise the residual\ncontamination after their removal. In this work, we develop a new,\nhigh-resolution model of Cygnus A using Low Frequency Array (LOFAR)\nobservations in the $110{-}250$ MHz range, improving upon previous models by\nincorporating physical spectral information through the forced-spectrum method\nduring multi-frequency deconvolution. This approach addresses the limitations\nof earlier models by providing a more accurate representation of the complex\nstructure and spectral behaviour of Cygnus A, including the spectral turnover\nin its brightest hotspots. The impact of this new model on the LOFAR 21-cm\nsignal power spectrum is assessed by comparing it with both simulated and\nobserved North Celestial Pole data sets. Significant improvements are observed\nin the cylindrical power spectrum along the Cygnus A direction, highlighting\nthe importance of having spectrally accurate models of the brightest foreground\nsources. However, this improvement is washed out in the spherical power\nspectrum, where we measure differences of a few hundred mK at\n$k<0.63\\,h\\,\\text{cMpc}^{-1}$, but not statistically significant. The results\nsuggest that other systematic effects must be mitigated before a substantial\nimpact on 21-cm power spectrum can be achieved.",
        "This paper investigates the computation of Hawking temperatures for black\nholes within various $f(Q)$ gravity models using the RVB method. This\ntopological approach uncovers an additional term in the temperature\ncalculation, which we propose originates from the residue of a specific contour\nintegral related to the metric or curvature. By examining several specific\n$f(Q)$ models including quadratic, logarithmic, square root, and power law\nmodifications as well as well known black hole solutions such as RN, Kerr, and\nKN black holes, we demonstrate that the correction term can consistently be\ninterpreted as this residue. Our findings provide new insights into black hole\nthermodynamics within modified gravity frameworks.",
        "The concept of bound states in the continuum (BIC) has been advancing light\nconfinement technology in leaky environments. In this letter, we propose and\nnumerically demonstrate a slow light waveguide based on a BIC mode. We\nconsidered a waveguide with a polymer core loaded on a plane slab, which\nsupports a leaky guided mode coupled to the radiation continuum in the slab. We\nfound that periodic modulation of the polymer core along the propagation\ndirection can result in a high group index mode with a low propagation loss due\nto BIC confinement. The introduction of one-dimensional photonic crystals into\nthe BIC waveguides will largely expand its functionality and applications in\nintegrated photonics.",
        "In this paper we explain how to attach to a family of $p$-adic\nrepresentations of a product of Galois groups an overconvergent family of\nmultivariable $(\\varphi,\\Gamma)$-modules, generalizing results from Pal-Zabradi\nand Carter-Kedlaya-Zabradi, using Colmez-Sen-Tate descent. We also define rings\nof multivariable crystalline and semistable periods, and explain how to recover\nthis multivariable $p$-adic theory attached to a family of representations from\nits multivariable $(\\varphi,\\Gamma)$-module. We also explain how our framework\nallows us to recover the main results of Brinon-Chiarellotto-Mazzari on\nmultivariable $p$-adic Galois representations.",
        "We introduce a refinement to the convex split lemma by replacing the max\nmutual information with the collision mutual information, transforming the\ninequality into an equality. This refinement yields tighter achievability\nbounds for quantum source coding tasks, including state merging and state\nsplitting. Furthermore, we derive a universal upper bound on the smoothed max\nmutual information, where \"universal\" signifies that the bound depends\nexclusively on R\\'enyi entropies and is independent of the system's dimensions.\nThis result has significant implications for quantum information processing,\nparticularly in applications such as the reverse quantum Shannon theorem.",
        "Ultra-lean premixed hydrogen combustion is a possible solution to decarbonize\nindustry, while limiting flame temperatures and thus nitrous oxide emissions.\nThese lean hydrogen\/air flames experience strong preferential diffusion\neffects, which result in thermo-diffusive (TD) instabilities. To efficiently\nand accurately model lean premixed hydrogen flames, it is crucial to\nincorporate these preferential diffusion effects into flamelet tabulated\nchemistry frameworks, such as the Flamelet-Generated Manifold (FGM) method.\nThis is challenging because the preferential diffusion terms in the control\nvariable transport equations contain diffusion fluxes of all species in the\nmechanism. In this work, a new implementation is presented; the full term is\nreduced by only considering the most contributing species. When carefully\nselecting this set of major species, preferential diffusion fluxes along the\nflame front, i.e., cross-diffusion, can be captured. This is particularly\nimportant for manifolds that include heat loss effects, where enthalpy is one\nof the control variables. The diffusion of the H-radical has a significant\ncontribution to the enthalpy transport equation, and cross-diffusion of the\nH-radical is non-negligible. Two manifolds, without and with heat loss effects,\nand the set of major species are analyzed in an a-priori and a-posteriori\nmanner. Simulations of TD unstable hydrogen-air flames with detailed chemistry\nand several FGM models show that accurately capturing cross-diffusion of\nenthalpy is important for correctly predicting the flame shape and dynamics."
      ]
    }
  },
  {
    "id":2411.0675,
    "research_type":"applied",
    "start_id":"b21",
    "start_title":"Generative AI for Medical Imaging: extending the MONAI Framework",
    "start_abstract":"Recent advances in generative AI have brought incredible breakthroughs several areas, including medical imaging. These models tremendous potential not only to help safely share data via synthetic datasets but also perform an array of diverse applications, such as anomaly detection, image-to-image translation, denoising, and MRI reconstruction. However, due the complexity these models, their implementation reproducibility can be difficult. This hinder progress, act a use barrier, dissuade comparison new methods with existing works. In this study, we present MONAI Generative Models, freely available open-source platform that allows researchers developers easily train, evaluate, deploy related applications. Our reproduces state-of-art studies standardised way involving different architectures (such diffusion autoregressive transformers, GANs), provides pre-trained for community. We implemented generalisable fashion, illustrating results extended 2D or 3D scenarios, images modalities (like CT, MRI, X-Ray data) from anatomical areas. Finally, adopt modular extensible approach, ensuring long-term maintainability extension current applications future features.",
    "start_categories":[
      "cs.CV",
      "eess.IV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b13"
      ],
      "title":[
        "Symmetric diffeomorphic image registration with crosscorrelation: evaluating automated labeling of elderly and neurodegenerative brain"
      ],
      "abstract":[
        "One of the most challenging problems in modern neuroimaging is detailed characterization of neurodegeneration. Quantifying spatial and longitudinal atrophy patterns is an important component of this process. These spatiotemporal signals will aid in discriminating between related diseases, such as frontotemporal dementia (FTD) and Alzheimer's disease (AD), which manifest themselves in the same at-risk population. Here, we develop a novel symmetric image normalization method (SyN) for maximizing the cross-correlation within the space of diffeomorphic maps and provide the Euler-Lagrange equations necessary for this optimization. We then turn to a careful evaluation of our method. Our evaluation uses gold standard, human cortical segmentation to contrast SyN's performance with a related elastic method and with the standard ITK implementation of Thirion's Demons algorithm. The new method compares favorably with both approaches, in particular when the distance between the template brain and the target brain is large. We then report the correlation of volumes gained by algorithmic cortical labelings of FTD and control subjects with those gained by the manual rater. This comparison shows that, of the three methods tested, SyN's volume measurements are the most strongly correlated with volume measurements gained by expert labeling. This study indicates that SyN, with cross-correlation, is a reliable method for normalizing and making anatomical measurements in volumetric MRI of patients and at-risk elderly individuals."
      ],
      "categories":[
        "q-bio.NC"
      ]
    },
    "list":{
      "title":[
        "Neural encoding with affine feature response transforms",
        "Evolution of diverse (and advanced) cognitive abilities through adaptive\n  fine-tuning of learning and chunking mechanisms",
        "Fluctuation-response relations and response-response relations for\n  membrane voltage and spike train of stochastic integrate-and-fire neurons",
        "Using economic value signals from primate prefrontal cortex in\n  neuro-engineering applications",
        "Automatic target validation based on neuroscientific literature mining\n  for tractography",
        "The Role of Affective States in Computational Psychiatry",
        "Vagus nerve stimulation as a modulator of feedforward and feedback\n  neural transmission",
        "Multi-Site rs-fMRI Domain Alignment for Autism Spectrum Disorder\n  Auxiliary Diagnosis Based on Hyperbolic Space",
        "From Thought to Action: How a Hierarchy of Neural Dynamics Supports\n  Language Production",
        "Structuring the Environment Nudges Participants Toward Hierarchical Over\n  Shortest Path Planning",
        "The Neural Basis of Groove Sensations: Implications for Music-Based\n  Interventions and Dance Therapy in Parkinson's Disease",
        "Role of connectivity anisotropies in the dynamics of cultured neuronal\n  networks",
        "On Questions of Predictability and Control of an Intelligent System\n  Using Probabilistic State-Transitions",
        "Super-resolution Live-cell Fluorescence Lifetime Imaging",
        "A New Circle Theorem for Two Dimensional Ising Spin Glasses",
        "Learning Conditional Average Treatment Effects in Regression\n  Discontinuity Designs using Bayesian Additive Regression Trees",
        "Reducing Frequency Bias of Fourier Neural Operators in 3D Seismic\n  Wavefield Simulations Through Multi-Stage Training",
        "Left Jacobson Rings",
        "Safe exploration in reproducing kernel Hilbert spaces",
        "Orthogonal Delay-Doppler Division Multiplexing Modulation with\n  Hierarchical Mode-Based Index Modulation",
        "Periodic Response Solutions to Multi-Dimensional Nonlinear Schr\\\"odinger\n  equation with unbounded perturbation",
        "DESI DR2 Results II: Measurements of Baryon Acoustic Oscillations and\n  Cosmological Constraints",
        "Spectral modelling of Cygnus A between 110 and 250 MHz. Impact on the\n  LOFAR 21-cm signal power spectrum",
        "Calculating the Hawking Temperature of Black Holes in $f(Q)$ Gravity\n  Using the RVB Method: A Residue-Based Approach",
        "Slow Light Waveguides based on Bound States in the Continuum",
        "Multivariable $p$-adic Hodge theory for products of Galois groups",
        "Convex Split Lemma without Inequalities",
        "FGM Modeling of Thermo-Diffusive Unstable Lean Premixed Hydrogen-Air\n  Flames"
      ],
      "abstract":[
        "Current linearizing encoding models that predict neural responses to sensory\ninput typically neglect neuroscience-inspired constraints that could enhance\nmodel efficiency and interpretability. To address this, we propose a new method\ncalled affine feature response transform (AFRT), which exploits the brain's\nretinotopic organization. Applying AFRT to encode multi-unit activity in areas\nV1, V4, and IT of the macaque brain, we demonstrate that AFRT reduces redundant\ncomputations and enhances the performance of current linearizing encoding\nmodels by segmenting each neuron's receptive field into an affine retinal\ntransform, followed by a localized feature response. Remarkably, by factorizing\nreceptive fields into a sequential affine component with three interpretable\nparameters (for shifting and scaling) and response components with a small\nnumber of feature weights per response, AFRT achieves encoding with orders of\nmagnitude fewer parameters compared to unstructured models. We show that the\nretinal transform of each neuron's encoding agrees well with the brain's\nreceptive field. Together, these findings suggest that this new subset within\nspatial transformer network can be instrumental in neural encoding models of\nnaturalistic stimuli.",
        "The evolution of cognition is frequently discussed as the evolution of\ncognitive abilities or the evolution of some neuronal structures in the brain.\nHowever, since such traits or abilities are often highly complex, understanding\ntheir evolution requires explaining how they could have gradually evolved\nthrough selection acting on heritable variations in simpler cognitive\nmechanisms. With this in mind, making use of a previously proposed theory, here\nwe show how the evolution of cognitive abilities can be captured by the\nfine-tuning of basic learning mechanisms and, in particular, chunking\nmechanisms. We use the term chunking broadly for all types of non-elemental\nlearning, claiming that the process by which elements are combined into chunks\nand associated with other chunks, or elements, is critical for what the brain\ncan do, and that it must be fine-tuned to ecological conditions. We discuss the\nrelevance of this approach to studies in animal cognition, using examples from\nanimal foraging and decision-making, problem solving, and cognitive\nflexibility. Finally, we explain how even the apparent human-animal gap in\nsequence learning ability can be explained in terms of different fine-tunings\nof a similar chunking process.",
        "Neurons display spontaneous spiking (in the absence of stimulus signals) as\nwell as a characteristic response to time-dependent external stimuli. In a\nsimple but important class of stochastic neuron models, the integrate-and-fire\nmodel with Gaussian current noise, both aspects can mathematically be related\nvia fluctuation-response relations (FRRs) as has been shown recently. Here we\nextend the class of FRRs to include the susceptibilities of the membrane\nvoltage and subthreshold voltage nonlinearity as well as the power spectrum of\nthe membrane voltage. For a simple but often considered IF model, the leaky IF\nmodel with white Gaussian noise, we exploit the FRRs and derive explicit\nexpressions for the power spectrum and susceptibility of the subthreshold\nmembrane voltage. We also put forward a relation between the response functions\nof the spike train and membrane voltage, a response-response relation (RRR)\nthat holds true for a more general setting than considered in most parts of the\npaper. For the generalized IF model with an adaptation current and colored\nGaussian noise we derive an FRR and an RRR. We briefly discuss useful\napplications of the derived FRRs and RRRs.",
        "Neural signals related to movement can be measured from intracranial\nrecordings and used in brain-machine interface devices (BMI) to restore\nphysical function in impaired patients. In this study, we explore the use of\nmore abstract neural signals related to economic value in a BMI context. Using\ndata collected from the orbitofrontal cortex in non-human primates, we develop\ndeep learning-based neural decoders that can predict the monkey's choice in a\nvalue-based decision-making task. Out-of-sample performance was improved by\naugmenting the training set with synthesized data, showing the feasibility of\nusing limited training data. We further demonstrate that we can predict the\nmonkey's choice sooner using a neural forecasting module that is equipped with\ntask-related information. These findings support the feasibility of user\npreference-informed neuroengineering devices that leverage abstract cognitive\nsignals.",
        "Target identification for tractography studies requires solid anatomical\nknowledge validated by an extensive literature review across species for each\nseed structure to be studied. Manual literature review to identify targets for\na given seed region is tedious and potentially subjective. Therefore,\ncomplementary approaches would be useful. We propose to use text-mining models\nto automatically suggest potential targets from the neuroscientific literature,\nfull-text articles and abstracts, so that they can be used for anatomical\nconnection studies and more specifically for tractography. We applied\ntext-mining models to three structures: two well-studied structures, since\nvalidated deep brain stimulation targets, the internal globus pallidus and the\nsubthalamic nucleus and, the nucleus accumbens, an exploratory target for\ntreating psychiatric disorders. We performed a systematic review of the\nliterature to document the projections of the three selected structures and\ncompared it with the targets proposed by text-mining models, both in rat and\nprimate (including human). We ran probabilistic tractography on the nucleus\naccumbens and compared the output with the results of the text-mining models\nand literature review. Overall, text-mining the literature could find three\ntimes as many targets as two man-weeks of curation could. The overall\nefficiency of the text-mining against literature review in our study was 98%\nrecall (at 36% precision), meaning that over all the targets for the three\nselected seeds, only one target has been missed by text-mining. We demonstrate\nthat connectivity for a structure of interest can be extracted from a very\nlarge amount of publications and abstracts. We believe this tool will be useful\nin helping the neuroscience community to facilitate connectivity studies of\nparticular brain regions. The text mining tools used for the study are part of\nthe HBP Neuroinformatics Platform, publicly available at\nhttp:\/\/connectivity-brainer.rhcloud.com",
        "Studying psychiatric illness has often been limited by difficulties in\nconnecting symptoms and behavior to neurobiology. Computational psychiatry\napproaches promise to bridge this gap by providing formal accounts of the\nlatent information processing changes that underlie the development and\nmaintenance of psychiatric phenomena. Models based on these theories generate\nindividual-level parameter estimates which can then be tested for relationships\nto neurobiology. In this review, we explore computational modelling approaches\nto one key aspect of health and illness: affect. We discuss strengths and\nlimitations of key approaches to modelling affect, with a focus on\nreinforcement learning, active inference, the hierarchical gaussian filter, and\ndrift-diffusion models. We find that, in this literature, affect is an\nimportant source of modulation in decision making, and has a bidirectional\ninfluence on how individuals infer both internal and external states.\nHighlighting the potential role of affect in information processing changes\nunderlying symptom development, we extend an existing model of psychosis, where\naffective changes are influenced by increasing cortical noise and consequent\nincreases in either perceived environmental instability or expected noise in\nsensory input, becoming part of a self-reinforcing process generating\nnegatively valenced, over-weighted priors underlying positive symptom\ndevelopment. We then provide testable predictions from this model at\ncomputational, neurobiological, and phenomenological levels of description.",
        "Vagus nerve stimulation (VNS) has emerged as a promising therapeutic\nintervention across various neurological and psychiatric conditions, including\nepilepsy, depression, and stroke rehabilitation; however, its mechanisms of\naction on neural circuits remain incompletely understood. Here, we present a\nnovel theoretical framework based on predictive coding that conceptualizes VNS\neffects through differential modulation of feedforward and feedback neural\ncircuits. Based on recent evidence, we propose that VNS shifts the balance\nbetween feedforward and feedback processing through multiple neuromodulatory\nsystems, resulting in enhanced feedforward signal transmission. This framework\nintegrates anatomical pathways, receptor distributions, and physiological\nresponses to explain the influence of the VNS on neural dynamics across\ndifferent spatial and temporal scales. VNS may facilitate neural plasticity and\nadaptive behavior through acetylcholine and noradrenaline (norepinephrine),\nwhich differentially modulate feedforward and feedback signaling. This\nmechanistic understanding serves as a basis for interpreting the cognitive and\ntherapeutic outcomes across different clinical conditions. Our perspective\nprovides a unified theoretical framework for understanding circuit-specific VNS\neffects and suggests new directions for investigating their therapeutic\nmechanisms.",
        "In the medical field, most resting-state fMRI (rs-fMRI) data are collected\nfrom multiple hospital sites. Multi-site rs-fMRI data can increase the volume\nof training data, enabling auxiliary diagnostic algorithms for brain diseases\nto learn more accurate and stable models. However, due to the significant\nheterogeneity and domain shift in rs-fMRI data across different sites, the\naccuracy of auxiliary diagnosis remains unsatisfactory. Moreover, there has\nbeen limited exploration of multi-source domain adaptation algorithms, and the\ninterpretability of models is often poor. To address these challenges, we\nproposed a domain-adaptive algorithm based on hyperbolic space embedding.\nHyperbolic space is naturally suited for representing the topology of complex\nnetworks such as brain functional networks. Therefore, we embedded the brain\nfunctional network into hyperbolic space and constructed the corresponding\nhyperbolic space community network to effectively extract brain network\nrepresentations. To address the heterogeneity of data across different sites\nand the issue of domain shift, we introduce a constraint loss function, HMMD\n(Hyperbolic Maximum Mean Discrepancy), to align the marginal distributions in\nthe hyperbolic space. Additionally, we employ class prototype alignment to\nalign the conditional distributions. This significantly improves the quality of\nbrain representations and enhances diagnostic classification accuracy for\nAutism Spectrum Disorder (ASD). Experimental results demonstrated that the\nproposed algorithm is robust to multi-site heterogeneity and shows promising\npotential for brain network mechanism analysis.",
        "Humans effortlessly communicate their thoughts through intricate sequences of\nmotor actions. Yet, the neural processes that coordinate language production\nremain largely unknown, in part because speech artifacts limit the use of\nneuroimaging. To elucidate the unfolding of language production in the brain,\nwe investigate with magnetoencephalography (MEG) and electroencephalography\n(EEG) the neurophysiological activity of 35 skilled typists, while they typed\nsentences on a keyboard. This approach confirms the hierarchical predictions of\nlinguistic theories: the neural activity preceding the production of each word\nis marked by the sequential rise and fall of context-, word-, syllable-, and\nletter-level representations. Remarkably, each of these neural representations\nis maintained over long time periods within each level of the language\nhierarchy. This phenomenon results in a superposition of successive\nrepresentations that is supported by a hierarchy of dynamic neural codes.\nOverall, these findings provide a precise computational breakdown of the neural\ndynamics that coordinate the production of language in the human brain.",
        "Effective planning is crucial for navigating complex environments and\nachieving goals efficiently. In this study, we investigated how environmental\nstructure influences the selection of planning strategies. Participants\nnavigated a space station to collect colored spheres, with environments either\nstructured (spheres grouped by color) or unstructured (spheres scattered\nrandomly). We tested three types of plans: hierarchical (grouping spheres by\ncolor), shortest path (minimizing travel distance), and neutral (none of the\nabove). By manipulating environmental structure, we were able to nudge\nparticipants toward a preference for hierarchical planning in structured\nenvironments, while shortest path plans were favored in unstructured\nenvironments. A mismatch between self-reported preferences and actual choices\nindicated that participants often adopted implicit strategies, unaware of their\ndecision-making processes. These findings highlight the powerful effect of\nenvironmental cues on planning and suggest that even subtle changes in\nstructure can guide the selection of planning strategies.",
        "Groove sensations arise from rhythmic structures that evoke an urge to move\nin response to music. While syncopation has been extensively studied in groove\nperception, the neural mechanisms underlying low-frequency groove remain\nunderexplored. This fMRI study examines the role of the mirror neuron system\nand associated brain regions in processing low-frequency groove.\nRegion-of-interest analysis revealed that amplifying drum and bass components\nin K-pop songs significantly increased activity in the right posterior inferior\nfrontal gyrus, right inferior\/superior parietal lobules, left dorsolateral\nprefrontal cortex, and bilateral posterior middle\/inferior temporal gyrus.\nThese findings suggest that low-frequency grooves engage sensorimotor,\nexecutive, and rhythm semantics networks, reinforcing their role in\naction-related processing. Building on these insights, we propose an enhanced\nrhythmic auditory stimulation paradigm for Parkinson's disease, incorporating\namplified low-frequency rhythmic cues to improve gait synchronization.",
        "Laboratory-grown, engineered living neuronal networks in vitro have emerged\nin the last years as an experimental technique to understand the collective\nbehavior of neuronal assemblies in relation to their underlying connectivity.\nAn inherent obstacle in the design of such engineered systems is the difficulty\nto predict the dynamic repertoire of the emerging network and its dependence on\nexperimental variables. To fill this gap, and inspired on recent experimental\nstudies, here we present a numerical model that aims at, first, replicating the\nanisotropies in connectivity imprinted through engineering, to next realize the\ncollective behavior of the neuronal network and make predictions. We use\nexperimentally measured, biologically-realistic data combined with the\nIzhikevich model to quantify the dynamics of the neuronal network in relation\nto tunable structural and dynamical parameters. These parameters include the\nsynaptic noise, strength of the imprinted anisotropies, and average axon\nlengths. The latter are involved in the simulation of the development of\nneurons in vitro. We show that the model captures the behavior of engineered\nneuronal cultures, in which a rich repertoire of activity patterns emerge but\nwhose details are strongly dependent on connectivity details and noise. Results\nalso show that the presence of connectivity anisotropies substantially improves\nthe capacity of reconstructing structural connectivity from activity data, an\naspect that is important in the quest for understanding the\nstructure-to-function relationship in neuronal networks. Our work provides the\nin silico basis to assist experimentalists in the design of laboratory in vitro\nnetworks and anticipate their outcome, an aspect that is particularly important\nin the effort to conceive reliable brain-on-a-chip circuits and explore key\naspects such as input-output relationships or information coding.",
        "One of the central aims of neuroscience is to reliably predict the behavioral\nresponse of an organism using its neural activity. If possible, this implies we\ncan causally manipulate the neural response and design brain-computer-interface\nsystems to alter behavior, and vice-versa. Hence, predictions play an important\nrole in both fundamental neuroscience and its applications. Can we predict the\nneural and behavioral states of an organism at any given time? Can we predict\nbehavioral states using neural states, and vice-versa, and is there a\nmemory-component required to reliably predict such states? Are the predictions\ncomputable within a given timescale to meaningfully stimulate and make the\nsystem reach the desired states? Through a series of mathematical treatments,\nsuch conjectures and questions are discussed. Answering them might be key for\nfuture developments in understanding intelligence and designing\nbrain-computer-interfaces.",
        "Super-resolution Structured Illumination Microscopy (SR-SIM) enables\nfluorescence microscopy beyond the diffraction limit at high frame rates.\nCompared to other super-resolution microscopy techniques, the low photon\nfluence used in SR-SIM makes it readily compatible with live-cell imaging.\nHere, we combine SR-SIM with electro-optic fluorescence lifetime imaging\n(EOFLIM), adding the capability of monitoring physicochemical parameters with\n156 nm spatial resolution at high frame rate for live-cell imaging. We\ndemonstrate that our new SIMFLIM technique enables super-resolved multiplexed\nimaging of spectrally overlapping fluorophores, environmental sensing, and\nlive-cell imaging.",
        "The Lee-Yang circle theorem revolutionized our understanding of phase\ntransitions in ferromagnetic systems by showing that the complex zeros of\npartition functions lie on the unit circle, with criticality arising as these\nzeros approach the real axis in the thermodynamic limit. However, in frustrated\nsystems such as antiferromagnets and spin glasses, the zeros deviate from this\nstructure, making it challenging to extend the Lee-Yang theory to disordered\nsystems. In this work, we establish a new circle theorem for two-dimensional\nIsing spin glasses, proving that the square of the partition function exhibits\nzeros densely packed along the unit circle. Numerical simulations on the square\nlattice confirm our theoretical predictions, demonstrating the validity of the\ncircle law for quenched disorder. Furthermore, our results uncover a\nfinite-temperature crossover in $\\pm J$ spin glasses, characterized by the\nemergence of a spectral gap in the angular distribution of zeros. This result\nextends the Lee-Yang framework to disordered systems, offering new insights\ninto spin-glass criticality.",
        "BART (Bayesian additive regression trees) has been established as a leading\nsupervised learning method, particularly in the field of causal inference. This\npaper explores the use of BART models for learning conditional average\ntreatment effects (CATE) from regression discontinuity designs, where treatment\nassignment is based on whether an observed covariate (called the running\nvariable) exceeds a pre-specified threshold. A purpose-built version of BART\nthat uses linear regression leaf models (of the running variable and treatment\nassignment dummy) is shown to out-perform off-the-shelf BART implementations as\nwell as a local polynomial regression approach and a CART-based approach. The\nnew method is evaluated in thorough simulation studies as well as an empirical\napplication looking at the effect of academic probation on student performance.",
        "The recent development of Neural Operator (NeurOp) learning for solutions to\nthe elastic wave equation shows promising results and provides the basis for\nfast large-scale simulations for different seismological applications. In this\npaper, we use the Fourier Neural Operator (FNO) model to directly solve the 3D\nHelmholtz wave equation for fast seismic ground motion simulations on different\nfrequencies, and show the frequency bias of the FNO model, i.e. it learns the\nlower frequencies better comparing to the higher frequencies. To reduce the\nfrequency bias, we adopt the multi-stage FNO training, i.e., after training a\n1st stage FNO model for estimating the ground motion, we use a second FNO model\nas the 2nd stage to learn from the residual, which greatly reduced the errors\non the higher frequencies. By adopting this multi-stage training, the FNO\nmodels show reduced biases on higher frequencies, which enhanced the overall\nresults of the ground motion simulations. Thus the multi-stage training FNO\nimproves the accuracy and realism of the ground motion simulations.",
        "We say that a ring is strongly (resp. weakly) left Jacobson if every\nsemiprime (resp. prime) left ideal is an intersection of maximal left ideals.\nThere exist Jacobson rings that are not weakly left Jacobson, e.g. the Weyl\nalgebra. Our main result is the following one-sided noncommutative\nNullstellensatz: For any finite-dimensional F-algebra A the ring\nA[$x_1$,...,$x_n$] of polynomials with coefficients in A is strongly left\nJacobson and every maximal left ideal of A[$x_1$,...,$x_n$] has a special form.\nWe also prove that an algebra that is a finitely generated module over its\ncenter is weakly left Jacobson iff it is Jacobson, and that an Azumaya algebra\nis strongly left Jacobson iff its center is Jacobson.",
        "Popular safe Bayesian optimization (BO) algorithms learn control policies for\nsafety-critical systems in unknown environments. However, most algorithms make\na smoothness assumption, which is encoded by a known bounded norm in a\nreproducing kernel Hilbert space (RKHS). The RKHS is a potentially\ninfinite-dimensional space, and it remains unclear how to reliably obtain the\nRKHS norm of an unknown function. In this work, we propose a safe BO algorithm\ncapable of estimating the RKHS norm from data. We provide statistical\nguarantees on the RKHS norm estimation, integrate the estimated RKHS norm into\nexisting confidence intervals and show that we retain theoretical guarantees,\nand prove safety of the resulting safe BO algorithm. We apply our algorithm to\nsafely optimize reinforcement learning policies on physics simulators and on a\nreal inverted pendulum, demonstrating improved performance, safety, and\nscalability compared to the state-of-the-art.",
        "The orthogonal time frequency space with index modulation (OTFS-IM) offers\nflexible tradeoffs between spectral efficiency (SE) and bit error rate (BER) in\ndoubly selective fading channels. While OTFS-IM schemes demonstrated such\npotential, a persistent challenge lies in the detection complexity. To address\nthis problem, we propose the hierarchical mode-based index modulation (HMIM).\nHMIM introduces a novel approach to modulate information bits by IM patterns,\nsignificantly simplifying the complexity of maximum a posteriori (MAP)\nestimation with Gaussian noise. Further, we incorporate HMIM with the recently\nproposed orthogonal delay-Doppler division multiplexing (ODDM) modulation,\nnamely ODDM-HMIM, to exploit the full diversity of the delay-Doppler (DD)\nchannel. The BER performance of ODDM-HMIM is analyzed considering a maximum\nlikelihood (ML) detector. Our numerical results reveal that, with the same SE,\nHMIM can outperform conventional IM in terms of both BER and computational\ncomplexity. In addition, we propose a successive interference\ncancellation-based minimum mean square error (SIC-MMSE) detector for ODDM-HMIM,\nwhich enables low-complexity detection with large frame sizes.",
        "By applying the Craig-Wayne-Bourgain (CWB) method, we establish the existence\nof periodic response solutions to multi-dimensional nonlinear Schr\\\"{o}dinger\nequations (NLS) with unbounded perturbation.",
        "We present baryon acoustic oscillation (BAO) measurements from more than 14\nmillion galaxies and quasars drawn from the Dark Energy Spectroscopic\nInstrument (DESI) Data Release 2 (DR2), based on three years of operation. For\ncosmology inference, these galaxy measurements are combined with DESI\nLyman-$\\alpha$ forest BAO results presented in a companion paper. The DR2 BAO\nresults are consistent with DESI DR1 and SDSS, and their distance-redshift\nrelationship matches those from recent compilations of supernovae (SNe) over\nthe same redshift range. The results are well described by a flat $\\Lambda$CDM\nmodel, but the parameters preferred by BAO are in mild, $2.3\\sigma$ tension\nwith those determined from the cosmic microwave background (CMB), although the\nDESI results are consistent with the acoustic angular scale $\\theta_*$ that is\nwell-measured by Planck. This tension is alleviated by dark energy with a\ntime-evolving equation of state parametrized by $w_0$ and $w_a$, which provides\na better fit to the data, with a favored solution in the quadrant with $w_0>-1$\nand $w_a<0$. This solution is preferred over $\\Lambda$CDM at $3.1\\sigma$ for\nthe combination of DESI BAO and CMB data. When also including SNe, the\npreference for a dynamical dark energy model over $\\Lambda$CDM ranges from\n$2.8-4.2\\sigma$ depending on which SNe sample is used. We present evidence from\nother data combinations which also favor the same behavior at high\nsignificance. From the combination of DESI and CMB we derive 95% upper limits\non the sum of neutrino masses, finding $\\sum m_\\nu<0.064$ eV assuming\n$\\Lambda$CDM and $\\sum m_\\nu<0.16$ eV in the $w_0w_a$ model. Unless there is an\nunknown systematic error associated with one or more datasets, it is clear that\n$\\Lambda$CDM is being challenged by the combination of DESI BAO with other\nmeasurements and that dynamical dark energy offers a possible solution.",
        "Studying the redshifted 21-cm signal from the the neutral hydrogen during the\nEpoch of Reionization and Cosmic Dawn is fundamental for understanding the\nphysics of the early universe. One of the challenges that 21-cm experiments\nface is the contamination by bright foreground sources, such as Cygnus A, for\nwhich accurate spatial and spectral models are needed to minimise the residual\ncontamination after their removal. In this work, we develop a new,\nhigh-resolution model of Cygnus A using Low Frequency Array (LOFAR)\nobservations in the $110{-}250$ MHz range, improving upon previous models by\nincorporating physical spectral information through the forced-spectrum method\nduring multi-frequency deconvolution. This approach addresses the limitations\nof earlier models by providing a more accurate representation of the complex\nstructure and spectral behaviour of Cygnus A, including the spectral turnover\nin its brightest hotspots. The impact of this new model on the LOFAR 21-cm\nsignal power spectrum is assessed by comparing it with both simulated and\nobserved North Celestial Pole data sets. Significant improvements are observed\nin the cylindrical power spectrum along the Cygnus A direction, highlighting\nthe importance of having spectrally accurate models of the brightest foreground\nsources. However, this improvement is washed out in the spherical power\nspectrum, where we measure differences of a few hundred mK at\n$k<0.63\\,h\\,\\text{cMpc}^{-1}$, but not statistically significant. The results\nsuggest that other systematic effects must be mitigated before a substantial\nimpact on 21-cm power spectrum can be achieved.",
        "This paper investigates the computation of Hawking temperatures for black\nholes within various $f(Q)$ gravity models using the RVB method. This\ntopological approach uncovers an additional term in the temperature\ncalculation, which we propose originates from the residue of a specific contour\nintegral related to the metric or curvature. By examining several specific\n$f(Q)$ models including quadratic, logarithmic, square root, and power law\nmodifications as well as well known black hole solutions such as RN, Kerr, and\nKN black holes, we demonstrate that the correction term can consistently be\ninterpreted as this residue. Our findings provide new insights into black hole\nthermodynamics within modified gravity frameworks.",
        "The concept of bound states in the continuum (BIC) has been advancing light\nconfinement technology in leaky environments. In this letter, we propose and\nnumerically demonstrate a slow light waveguide based on a BIC mode. We\nconsidered a waveguide with a polymer core loaded on a plane slab, which\nsupports a leaky guided mode coupled to the radiation continuum in the slab. We\nfound that periodic modulation of the polymer core along the propagation\ndirection can result in a high group index mode with a low propagation loss due\nto BIC confinement. The introduction of one-dimensional photonic crystals into\nthe BIC waveguides will largely expand its functionality and applications in\nintegrated photonics.",
        "In this paper we explain how to attach to a family of $p$-adic\nrepresentations of a product of Galois groups an overconvergent family of\nmultivariable $(\\varphi,\\Gamma)$-modules, generalizing results from Pal-Zabradi\nand Carter-Kedlaya-Zabradi, using Colmez-Sen-Tate descent. We also define rings\nof multivariable crystalline and semistable periods, and explain how to recover\nthis multivariable $p$-adic theory attached to a family of representations from\nits multivariable $(\\varphi,\\Gamma)$-module. We also explain how our framework\nallows us to recover the main results of Brinon-Chiarellotto-Mazzari on\nmultivariable $p$-adic Galois representations.",
        "We introduce a refinement to the convex split lemma by replacing the max\nmutual information with the collision mutual information, transforming the\ninequality into an equality. This refinement yields tighter achievability\nbounds for quantum source coding tasks, including state merging and state\nsplitting. Furthermore, we derive a universal upper bound on the smoothed max\nmutual information, where \"universal\" signifies that the bound depends\nexclusively on R\\'enyi entropies and is independent of the system's dimensions.\nThis result has significant implications for quantum information processing,\nparticularly in applications such as the reverse quantum Shannon theorem.",
        "Ultra-lean premixed hydrogen combustion is a possible solution to decarbonize\nindustry, while limiting flame temperatures and thus nitrous oxide emissions.\nThese lean hydrogen\/air flames experience strong preferential diffusion\neffects, which result in thermo-diffusive (TD) instabilities. To efficiently\nand accurately model lean premixed hydrogen flames, it is crucial to\nincorporate these preferential diffusion effects into flamelet tabulated\nchemistry frameworks, such as the Flamelet-Generated Manifold (FGM) method.\nThis is challenging because the preferential diffusion terms in the control\nvariable transport equations contain diffusion fluxes of all species in the\nmechanism. In this work, a new implementation is presented; the full term is\nreduced by only considering the most contributing species. When carefully\nselecting this set of major species, preferential diffusion fluxes along the\nflame front, i.e., cross-diffusion, can be captured. This is particularly\nimportant for manifolds that include heat loss effects, where enthalpy is one\nof the control variables. The diffusion of the H-radical has a significant\ncontribution to the enthalpy transport equation, and cross-diffusion of the\nH-radical is non-negligible. Two manifolds, without and with heat loss effects,\nand the set of major species are analyzed in an a-priori and a-posteriori\nmanner. Simulations of TD unstable hydrogen-air flames with detailed chemistry\nand several FGM models show that accurately capturing cross-diffusion of\nenthalpy is important for correctly predicting the flame shape and dynamics."
      ]
    }
  },
  {
    "id":2411.0675,
    "research_type":"applied",
    "start_id":"b13",
    "start_title":"Symmetric diffeomorphic image registration with crosscorrelation: evaluating automated labeling of elderly and neurodegenerative brain",
    "start_abstract":"One of the most challenging problems in modern neuroimaging is detailed characterization of neurodegeneration. Quantifying spatial and longitudinal atrophy patterns is an important component of this process. These spatiotemporal signals will aid in discriminating between related diseases, such as frontotemporal dementia (FTD) and Alzheimer's disease (AD), which manifest themselves in the same at-risk population. Here, we develop a novel symmetric image normalization method (SyN) for maximizing the cross-correlation within the space of diffeomorphic maps and provide the Euler-Lagrange equations necessary for this optimization. We then turn to a careful evaluation of our method. Our evaluation uses gold standard, human cortical segmentation to contrast SyN's performance with a related elastic method and with the standard ITK implementation of Thirion's Demons algorithm. The new method compares favorably with both approaches, in particular when the distance between the template brain and the target brain is large. We then report the correlation of volumes gained by algorithmic cortical labelings of FTD and control subjects with those gained by the manual rater. This comparison shows that, of the three methods tested, SyN's volume measurements are the most strongly correlated with volume measurements gained by expert labeling. This study indicates that SyN, with cross-correlation, is a reliable method for normalizing and making anatomical measurements in volumetric MRI of patients and at-risk elderly individuals.",
    "start_categories":[
      "q-bio.NC"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b3",
        "b21"
      ],
      "title":[
        "3-D ultrasound imaging: a review",
        "Generative AI for Medical Imaging: extending the MONAI Framework"
      ],
      "abstract":[
        "The development of 3-D ultrasound imaging is a way to address the disadvantages conventional imaging. In this article authors review approaches that have been attempted in such as B-mode, color Doppler, and power Doppler systems. Acquisition, reconstruction, rendering techniques for are discussed, well applications limitations.",
        "Recent advances in generative AI have brought incredible breakthroughs several areas, including medical imaging. These models tremendous potential not only to help safely share data via synthetic datasets but also perform an array of diverse applications, such as anomaly detection, image-to-image translation, denoising, and MRI reconstruction. However, due the complexity these models, their implementation reproducibility can be difficult. This hinder progress, act a use barrier, dissuade comparison new methods with existing works. In this study, we present MONAI Generative Models, freely available open-source platform that allows researchers developers easily train, evaluate, deploy related applications. Our reproduces state-of-art studies standardised way involving different architectures (such diffusion autoregressive transformers, GANs), provides pre-trained for community. We implemented generalisable fashion, illustrating results extended 2D or 3D scenarios, images modalities (like CT, MRI, X-Ray data) from anatomical areas. Finally, adopt modular extensible approach, ensuring long-term maintainability extension current applications future features."
      ],
      "categories":[
        "cs.CV",
        "eess.IV"
      ]
    },
    "list":{
      "title":[
        "Revealing Microscopic Objects in Fluorescence Live Imaging by\n  Video-to-video Translation Based on A Spatial-temporal Generative Adversarial\n  Network",
        "GC-ConsFlow: Leveraging Optical Flow Residuals and Global Context for\n  Robust Deepfake Detection",
        "Training-Free Motion-Guided Video Generation with Enhanced Temporal\n  Consistency Using Motion Consistency Loss",
        "Task-Agnostic Guided Feature Expansion for Class-Incremental Learning",
        "$\\mathbf{\\Phi}$-GAN: Physics-Inspired GAN for Generating SAR Images\n  Under Limited Data",
        "DocTTT: Test-Time Training for Handwritten Document Recognition Using\n  Meta-Auxiliary Learning",
        "GCE-Pose: Global Context Enhancement for Category-level Object Pose\n  Estimation",
        "Light4GS: Lightweight Compact 4D Gaussian Splatting Generation via\n  Context Model",
        "Multi-Layer Visual Feature Fusion in Multimodal LLMs: Methods, Analysis,\n  and Best Practices",
        "Multi-dataset synergistic in supervised learning to pre-label structural\n  components in point clouds from shell construction scenes",
        "Adversarial Robustness of Discriminative Self-Supervised Learning in\n  Vision",
        "Probabilistic Prompt Distribution Learning for Animal Pose Estimation",
        "General mean-field stochastic linear quadratic control problem driven by\n  L\\'evy processes with random coefficients",
        "Slowly decaying strain solitons in nonlinear viscoelastic waveguides",
        "AI-Invented Tonal Languages: Preventing a Machine Lingua Franca Beyond\n  Human Understanding",
        "Bounded Synthesis of Synchronized Distributed Models from Lightweight\n  Specifications",
        "EEG-CLIP : Learning EEG representations from natural language\n  descriptions",
        "Please, do tell",
        "Hot-carrier thermal breakdown and S-type current-voltage characteristics\n  in perforated graphene structures",
        "Goal-oriented Transmission Scheduling: Structure-guided DRL with a\n  Unified Dual On-policy and Off-policy Approach",
        "Optically Detected Magnetic Resonance Imaging and Sensing Within\n  Functionalized Additively Manufactured Microporous Structures",
        "Craw4LLM: Efficient Web Crawling for LLM Pretraining",
        "High-Accuracy ECG Image Interpretation using Parameter-Efficient LoRA\n  Fine-Tuning with Multimodal LLaMA 3.2",
        "Modeling of Rumor Propagation in Large Populations with Network via\n  Graphon Games",
        "On the Chermak-Delgado lattice of a finite group",
        "OfficeMate: Pilot Evaluation of an Office Assistant Robot",
        "MetaOpenFOAM 2.0: Large Language Model Driven Chain of Thought for\n  Automating CFD Simulation and Post-Processing"
      ],
      "abstract":[
        "In spite of being a valuable tool to simultaneously visualize multiple types\nof subcellular structures using spectrally distinct fluorescent labels, a\nstandard fluoresce microscope is only able to identify a few microscopic\nobjects; such a limit is largely imposed by the number of fluorescent labels\navailable to the sample. In order to simultaneously visualize more objects, in\nthis paper, we propose to use video-to-video translation that mimics the\ndevelopment process of microscopic objects. In essence, we use a microscopy\nvideo-to-video translation framework namely Spatial-temporal Generative\nAdversarial Network (STGAN) to reveal the spatial and temporal relationships\nbetween the microscopic objects, after which a microscopy video of one object\ncan be translated to another object in a different domain. The experimental\nresults confirm that the proposed STGAN is effective in microscopy\nvideo-to-video translation that mitigates the spectral conflicts caused by the\nlimited fluorescent labels, allowing multiple microscopic objects be\nsimultaneously visualized.",
        "The rapid development of Deepfake technology has enabled the generation of\nhighly realistic manipulated videos, posing severe social and ethical\nchallenges. Existing Deepfake detection methods primarily focused on either\nspatial or temporal inconsistencies, often neglecting the interplay between the\ntwo or suffering from interference caused by natural facial motions. To address\nthese challenges, we propose the global context consistency flow (GC-ConsFlow),\na novel dual-stream framework that effectively integrates spatial and temporal\nfeatures for robust Deepfake detection. The global grouped context aggregation\nmodule (GGCA), integrated into the global context-aware frame flow stream\n(GCAF), enhances spatial feature extraction by aggregating grouped global\ncontext information, enabling the detection of subtle, spatial artifacts within\nframes. The flow-gradient temporal consistency stream (FGTC), rather than\ndirectly modeling the residuals, it is used to improve the robustness of\ntemporal feature extraction against the inconsistency introduced by unnatural\nfacial motion using optical flow residuals and gradient-based features. By\ncombining these two streams, GC-ConsFlow demonstrates the effectiveness and\nrobustness in capturing complementary spatiotemporal forgery traces. Extensive\nexperiments show that GC-ConsFlow outperforms existing state-of-the-art methods\nin detecting Deepfake videos under various compression scenarios.",
        "In this paper, we address the challenge of generating temporally consistent\nvideos with motion guidance. While many existing methods depend on additional\ncontrol modules or inference-time fine-tuning, recent studies suggest that\neffective motion guidance is achievable without altering the model architecture\nor requiring extra training. Such approaches offer promising compatibility with\nvarious video generation foundation models. However, existing training-free\nmethods often struggle to maintain consistent temporal coherence across frames\nor to follow guided motion accurately. In this work, we propose a simple yet\neffective solution that combines an initial-noise-based approach with a novel\nmotion consistency loss, the latter being our key innovation. Specifically, we\ncapture the inter-frame feature correlation patterns of intermediate features\nfrom a video diffusion model to represent the motion pattern of the reference\nvideo. We then design a motion consistency loss to maintain similar feature\ncorrelation patterns in the generated video, using the gradient of this loss in\nthe latent space to guide the generation process for precise motion control.\nThis approach improves temporal consistency across various motion control tasks\nwhile preserving the benefits of a training-free setup. Extensive experiments\nshow that our method sets a new standard for efficient, temporally coherent\nvideo generation.",
        "The ability to learn new concepts while preserve the learned knowledge is\ndesirable for learning systems in Class-Incremental Learning (CIL). Recently,\nfeature expansion of the model become a prevalent solution for CIL, where the\nold features are fixed during the training of the new task while new features\nare expanded for the new tasks. However, such task-specific features learned\nfrom the new task may collide with the old features, leading to\nmisclassification between tasks. Therefore, the expanded model is often\nencouraged to capture diverse features from the new task, aiming to avoid such\ncollision. However, the existing solution is largely restricted to the samples\nfrom the current task, because of the poor accessibility to previous samples.\nTo promote the learning and transferring of diverse features across tasks, we\npropose a framework called Task-Agnostic Guided Feature Expansion (TagFex).\nFirstly, it captures task-agnostic features continually with a separate model,\nproviding extra task-agnostic features for subsequent tasks. Secondly, to\nobtain useful features from the task-agnostic model for the current task, it\naggregates the task-agnostic features with the task-specific feature using a\nmerge attention. Then the aggregated feature is transferred back into the\ntask-specific feature for inference, helping the task-specific model capture\ndiverse features. Extensive experiments show the effectiveness and superiority\nof TagFex on various CIL settings. Code is available at\nhttps:\/\/github.com\/bwnzheng\/TagFex_CVPR2025.",
        "Approaches for improving generative adversarial networks (GANs) training\nunder a few samples have been explored for natural images. However, these\nmethods have limited effectiveness for synthetic aperture radar (SAR) images,\nas they do not account for the unique electromagnetic scattering properties of\nSAR. To remedy this, we propose a physics-inspired regularization method dubbed\n$\\Phi$-GAN, which incorporates the ideal point scattering center (PSC) model of\nSAR with two physical consistency losses. The PSC model approximates SAR\ntargets using physical parameters, ensuring that $\\Phi$-GAN generates SAR\nimages consistent with real physical properties while preventing discriminator\noverfitting by focusing on PSC-based decision cues. To embed the PSC model into\nGANs for end-to-end training, we introduce a physics-inspired neural module\ncapable of estimating the physical parameters of SAR targets efficiently. This\nmodule retains the interpretability of the physical model and can be trained\nwith limited data. We propose two physical loss functions: one for the\ngenerator, guiding it to produce SAR images with physical parameters consistent\nwith real ones, and one for the discriminator, enhancing its robustness by\nbasing decisions on PSC attributes. We evaluate $\\Phi$-GAN across several\nconditional GAN (cGAN) models, demonstrating state-of-the-art performance in\ndata-scarce scenarios on three SAR image datasets.",
        "Despite recent significant advancements in Handwritten Document Recognition\n(HDR), the efficient and accurate recognition of text against complex\nbackgrounds, diverse handwriting styles, and varying document layouts remains a\npractical challenge. Moreover, this issue is seldom addressed in academic\nresearch, particularly in scenarios with minimal annotated data available. In\nthis paper, we introduce the DocTTT framework to address these challenges. The\nkey innovation of our approach is that it uses test-time training to adapt the\nmodel to each specific input during testing. We propose a novel Meta-Auxiliary\nlearning approach that combines Meta-learning and self-supervised Masked\nAutoencoder~(MAE). During testing, we adapt the visual representation\nparameters using a self-supervised MAE loss. During training, we learn the\nmodel parameters using a meta-learning framework, so that the model parameters\nare learned to adapt to a new input effectively. Experimental results show that\nour proposed method significantly outperforms existing state-of-the-art\napproaches on benchmark datasets.",
        "A key challenge in model-free category-level pose estimation is the\nextraction of contextual object features that generalize across varying\ninstances within a specific category. Recent approaches leverage foundational\nfeatures to capture semantic and geometry cues from data. However, these\napproaches fail under partial visibility. We overcome this with a\nfirst-complete-then-aggregate strategy for feature extraction utilizing class\npriors. In this paper, we present GCE-Pose, a method that enhances pose\nestimation for novel instances by integrating category-level global context\nprior. GCE-Pose performs semantic shape reconstruction with a proposed Semantic\nShape Reconstruction (SSR) module. Given an unseen partial RGB-D object\ninstance, our SSR module reconstructs the instance's global geometry and\nsemantics by deforming category-specific 3D semantic prototypes through a\nlearned deep Linear Shape Model. We further introduce a Global Context Enhanced\n(GCE) feature fusion module that effectively fuses features from partial RGB-D\nobservations and the reconstructed global context. Extensive experiments\nvalidate the impact of our global context prior and the effectiveness of the\nGCE fusion module, demonstrating that GCE-Pose significantly outperforms\nexisting methods on challenging real-world datasets HouseCat6D and\nNOCS-REAL275. Our project page is available at\nhttps:\/\/colin-de.github.io\/GCE-Pose\/.",
        "3D Gaussian Splatting (3DGS) has emerged as an efficient and high-fidelity\nparadigm for novel view synthesis. To adapt 3DGS for dynamic content,\ndeformable 3DGS incorporates temporally deformable primitives with learnable\nlatent embeddings to capture complex motions. Despite its impressive\nperformance, the high-dimensional embeddings and vast number of primitives lead\nto substantial storage requirements. In this paper, we introduce a\n\\textbf{Light}weight \\textbf{4}D\\textbf{GS} framework, called Light4GS, that\nemploys significance pruning with a deep context model to provide a lightweight\nstorage-efficient dynamic 3DGS representation. The proposed Light4GS is based\non 4DGS that is a typical representation of deformable 3DGS. Specifically, our\nframework is built upon two core components: (1) a spatio-temporal significance\npruning strategy that eliminates over 64\\% of the deformable primitives,\nfollowed by an entropy-constrained spherical harmonics compression applied to\nthe remainder; and (2) a deep context model that integrates intra- and\ninter-prediction with hyperprior into a coarse-to-fine context structure to\nenable efficient multiscale latent embedding compression. Our approach achieves\nover 120x compression and increases rendering FPS up to 20\\% compared to the\nbaseline 4DGS, and also superior to frame-wise state-of-the-art 3DGS\ncompression methods, revealing the effectiveness of our Light4GS in terms of\nboth intra- and inter-prediction methods without sacrificing rendering quality.",
        "Multimodal Large Language Models (MLLMs) have made significant advancements\nin recent years, with visual features playing an increasingly critical role in\nenhancing model performance. However, the integration of multi-layer visual\nfeatures in MLLMs remains underexplored, particularly with regard to optimal\nlayer selection and fusion strategies. Existing methods often rely on arbitrary\ndesign choices, leading to suboptimal outcomes. In this paper, we\nsystematically investigate two core aspects of multi-layer visual feature\nfusion: (1) selecting the most effective visual layers and (2) identifying the\nbest fusion approach with the language model. Our experiments reveal that while\ncombining visual features from multiple stages improves generalization,\nincorporating additional features from the same stage typically leads to\ndiminished performance. Furthermore, we find that direct fusion of multi-layer\nvisual features at the input stage consistently yields superior and more stable\nperformance across various configurations. We make all our code publicly\navailable: https:\/\/github.com\/EIT-NLP\/Layer_Select_Fuse_for_MLLM.",
        "The significant effort required to annotate data for new training datasets\nhinders computer vision research and machine learning in the construction\nindustry. This work explores adapting standard datasets and the latest\ntransformer model architectures for point cloud semantic segmentation in the\ncontext of shell construction sites. Unlike common approaches focused on object\nsegmentation of building interiors and furniture, this study addressed the\nchallenges of segmenting complex structural components in Architecture,\nEngineering, and Construction (AEC). We establish a baseline through supervised\ntraining and a custom validation dataset, evaluate the cross-domain inference\nwith large-scale indoor datasets, and utilize transfer learning to maximize\nsegmentation performance with minimal new data. The findings indicate that with\nminimal fine-tuning, pre-trained transformer architectures offer an effective\nstrategy for building component segmentation. Our results are promising for\nautomating the annotation of new, previously unseen data when creating larger\ntraining resources and for the segmentation of frequently recurring objects.",
        "Self-supervised learning (SSL) has advanced significantly in visual\nrepresentation learning, yet comprehensive evaluations of its adversarial\nrobustness remain limited. In this study, we evaluate the adversarial\nrobustness of seven discriminative self-supervised models and one supervised\nmodel across diverse tasks, including ImageNet classification, transfer\nlearning, segmentation, and detection. Our findings suggest that discriminative\nSSL models generally exhibit better robustness to adversarial attacks compared\nto their supervised counterpart on ImageNet, with this advantage extending to\ntransfer learning when using linear evaluation. However, when fine-tuning is\napplied, the robustness gap between SSL and supervised models narrows\nconsiderably. Similarly, this robustness advantage diminishes in segmentation\nand detection tasks. We also investigate how various factors might influence\nadversarial robustness, including architectural choices, training duration,\ndata augmentations, and batch sizes. Our analysis contributes to the ongoing\nexploration of adversarial robustness in visual self-supervised representation\nsystems.",
        "Multi-species animal pose estimation has emerged as a challenging yet\ncritical task, hindered by substantial visual diversity and uncertainty. This\npaper challenges the problem by efficient prompt learning for Vision-Language\nPretrained (VLP) models, \\textit{e.g.} CLIP, aiming to resolve the\ncross-species generalization problem. At the core of the solution lies in the\nprompt designing, probabilistic prompt modeling and cross-modal adaptation,\nthereby enabling prompts to compensate for cross-modal information and\neffectively overcome large data variances under unbalanced data distribution.\nTo this end, we propose a novel probabilistic prompting approach to fully\nexplore textual descriptions, which could alleviate the diversity issues caused\nby long-tail property and increase the adaptability of prompts on unseen\ncategory instance. Specifically, we first introduce a set of learnable prompts\nand propose a diversity loss to maintain distinctiveness among prompts, thus\nrepresenting diverse image attributes. Diverse textual probabilistic\nrepresentations are sampled and used as the guidance for the pose estimation.\nSubsequently, we explore three different cross-modal fusion strategies at\nspatial level to alleviate the adverse impacts of visual uncertainty. Extensive\nexperiments on multi-species animal pose benchmarks show that our method\nachieves the state-of-the-art performance under both supervised and zero-shot\nsettings. The code is available at https:\/\/github.com\/Raojiyong\/PPAP.",
        "This paper studies a stochastic mean-field linear-quadratic optimal control\nproblem with random coefficients. The state equation is a general linear\nstochastic differential equation with mean-field terms $\\EE X(t)$ and $\\EE\nu(t)$ of the state and the control processes and is driven by a Brownian motion\nand a Poisson random measure. By the coupled system of Riccati equations, an\nexplicit expressions for the optimal state feedback control is obtained. As a\nby-product, the non-homogeneous stochastic linear-quadratic control problem\nwith random coefficients and L\\'evy driving noises is also studied.",
        "This paper is devoted to the modeling of longitudinal strain waves in a rod\ncomposed of a nonlinear viscoelastic material characterized by\nfrequency-dependent second- and third-order elastic constants. We demonstrate\nthat long waves in such a material can be effectively described by a damped\nBoussinesq-type equation for the longitudinal strain, incorporating dissipation\nthrough retarded operators. Using the existing theory of solitary wave\nsolutions in nearly integrable systems, we derive a slowly-decaying strain\nsoliton solution to this equation. The derived soliton characteristics are\nshown to be in a good agreement with results from full 3D simulations. We\ndemonstrate the importance of taking into account the frequency dependence of\nthird-order elastic constants for the description of strain solitons.",
        "This paper investigates the potential for large language models (LLMs) to\ndevelop private tonal languages for machine-to-machine (M2M) communication.\nInspired by cryptophasia in human twins (affecting up to 50% of twin births)\nand natural tonal languages like Mandarin and Vietnamese, we implement a\nprecise character-to-frequency mapping system that encodes the full ASCII\ncharacter set (32-126) using musical semitones. Each character is assigned a\nunique frequency, creating a logarithmic progression beginning with space (220\nHz) and ending with tilde (50,175.42 Hz). This spans approximately 7.9 octaves,\nwith higher characters deliberately mapped to ultrasonic frequencies beyond\nhuman perception (>20 kHz). Our implemented software prototype demonstrates\nthis encoding through visualization, auditory playback, and ABC musical\nnotation, allowing for analysis of information density and transmission speed.\nTesting reveals that tonal encoding can achieve information rates exceeding\nhuman speech while operating partially outside human perceptual boundaries.\nThis work responds directly to concerns about AI systems catastrophically\ndeveloping private languages within the next five years, providing a concrete\nprototype software example of how such communication might function and the\ntechnical foundation required for its emergence, detection, and governance.",
        "We present an approach to automatically synthesize synchronized models from\nlightweight formal specifications. Our approach takes as input a specification\nof a distributed system along with a global linear time constraint, which must\nbe fulfilled by the interaction of the system's components. It produces\nexecutable models for the component specifications (in the style of Promela\nlanguage) whose concurrent execution satisfies the global constraint. The\ncomponent specifications consist of a collection of actions described by means\nof pre and post conditions together with first-order relational formulas\nprescribing their behavior. We use the Alloy Analyzer to encode the component\nspecifications and enumerate their potential implementations up to some bound,\nwhose concurrent composition is model checked against the global property. Even\nthough this approach is sound and complete up to the selected bound, it is\nimpractical as the number of candidate implementations grows exponentially. To\naddress this, we propose an algorithm that uses batches of counterexamples to\nprune the solution space, it has two main phases: exploration, the algorithm\ncollects a batch of counterexamples, and exploitation, where this knowledge is\nused to speed up the search. The approach is sound, while its completeness\ndepends on the batches used. We present a prototype tool, describe some\nexperiments, and compare it with related approaches.",
        "Deep networks for electroencephalogram (EEG) decoding are currently often\ntrained to only solve a specific task like pathology or gender decoding. A more\ngeneral approach leveraging the medical reports of clinical EEG recordings is\nto learn mappings between medical reports and EEG recordings. This approach was\npioneered in the computer vision domain matching images and their text captions\nand subsequently allowed to do successful zero-shot decoding using textual\nclass prompts. In this work, we follow this approach and develop a contrastive\nlearning framework EEG-CLIP that aligns EEG time series and their corresponding\nclinical text descriptions in a shared embedding space. We investigate its\npotential for versatile EEG decoding, assessing performance on a range of\nfew-shot and zero-shot settings. Overall, results show that EEG-CLIP manages to\nnontrivially align text and EEG representations. Our work presents a promising\napproach to learn general EEG representations, which could enable easier\nanalyses of diverse decoding questions through zero shot decoding or training\ntask-specific models from fewer training examples. The code for reproducing our\nresults is available at https:\/\/github.com\/tidiane-camaret\/EEGClip.",
        "\"Math is not a spectator sport.\" \"Lecturing is educational malpractice.\"\nSlogans like these rally some mathematicians to teach classes that feature\n\"active learning\", where lecturing is eschewed for student participation. Yet\nas much as I believe that students must do math to learn math, I also find\nblanket statements to be more about bandwagons than considered reflection on\nteaching. In this column, published in the Fall 2021 AWM Newsletter, I urge us\nto think through the math we offer students and how we set up students to\nlearn. Although I draw primarily from my experiences teaching proofs in\nabstract algebra and real analysis, the scenarios extend to other topics in\nfirst year undergraduate education and beyond.",
        "We investigate the carrier transport characteristics of perforated graphene\nlayer (PGL) composed of arrays of interdigital coplanar graphene microribbons\n(GMRs) connected by graphene nanoribbon (GNR) bridges. We analyze their\noperation at room-temperature. Under an applied bias voltage, two-dimensional\nelectron and hole systems (2DES and 2DHS) form in adjacent GMRs. The terminal\ncurrent in these PGL structures is primarily governed by thermionic transport\nacross the GNR bridges. As electrons and holes traverse the GNRs, they induce\nheating in the 2DES and 2DHS, creating a positive feedback loop between carrier\nheating and thermionic emission. This phenomenon, characterized as hot-carrier\nthermal breakdown, can give rise to S-shaped inter-GMR current-voltage\ncharacteristics. These unique transport properties make PGLs promising\ncandidates for fast, voltage-controlled room-temperature switches and\nelectromagnetic radiation detectors.",
        "Goal-oriented communications prioritize application-driven objectives over\ndata accuracy, enabling intelligent next-generation wireless systems. Efficient\nscheduling in multi-device, multi-channel systems poses significant challenges\ndue to high-dimensional state and action spaces. We address these challenges by\nderiving key structural properties of the optimal solution to the goal-oriented\nscheduling problem, incorporating Age of Information (AoI) and channel states.\nSpecifically, we establish the monotonicity of the optimal state value function\n(a measure of long-term system performance) w.r.t. channel states and prove its\nasymptotic convexity w.r.t. AoI states. Additionally, we derive the\nmonotonicity of the optimal policy w.r.t. channel states, advancing the\ntheoretical framework for optimal scheduling. Leveraging these insights, we\npropose the structure-guided unified dual on-off policy DRL (SUDO-DRL), a\nhybrid algorithm that combines the stability of on-policy training with the\nsample efficiency of off-policy methods. Through a novel structural property\nevaluation framework, SUDO-DRL enables effective and scalable training,\naddressing the complexities of large-scale systems. Numerical results show\nSUDO-DRL improves system performance by up to 45% and reduces convergence time\nby 40% compared to state-of-the-art methods. It also effectively handles\nscheduling in much larger systems, where off-policy DRL fails and on-policy\nbenchmarks exhibit significant performance loss, demonstrating its scalability\nand efficacy in goal-oriented communications.",
        "Quantum sensing with nitrogen-vacancy centers in diamond has emerged as a\npowerful tool for measuring diverse physical parameters, yet the versatility of\nthese measurement approaches is often limited by the achievable layout and\ndimensionality of bulk-crystal platforms. Here, we demonstrate a versatile\napproach to creating designer quantum sensors by surface-functionalizing\nmultiphoton lithography microstructures with NV-containing nanodiamonds. We\nshowcase this capability by fabricating a 150 $\\mu$m x 150 $\\mu$m x 150 $\\mu$m\ntriply periodic minimal surface gyroid structure with millions of attached\nnanodiamonds. We demonstrate a means to volumetrically image these structures\nusing a refractive index matching confocal imaging technique, and extract ODMR\nspectra from 1.86 $\\mu$m x 1.86 $\\mu$m areas of highly concentrated\nnanodiamonds across a cross section of the gyroid. Furthermore, the high\ndensity of sensing elements enables ensemble temperature measurements with\nsensitivity of 0.548 {\\deg}K\/$\\sqrt{Hz}$ at 5 mW excitation power. This\napproach to creating quantum-enabled microarchitectures opens new possibilities\nfor multimodal sensing in complex three-dimensional environments.",
        "Web crawl is a main source of large language models' (LLMs) pretraining data,\nbut the majority of crawled web pages are discarded in pretraining due to low\ndata quality. This paper presents Craw4LLM, an efficient web crawling method\nthat explores the web graph based on the preference of LLM pretraining.\nSpecifically, it leverages the influence of a webpage in LLM pretraining as the\npriority score of the web crawler's scheduler, replacing the standard graph\nconnectivity based priority. Our experiments on a web graph containing 900\nmillion webpages from a commercial search engine's index demonstrate the\nefficiency of Craw4LLM in obtaining high-quality pretraining data. With just\n21% URLs crawled, LLMs pretrained on Craw4LLM data reach the same downstream\nperformances of previous crawls, significantly reducing the crawling waste and\nalleviating the burdens on websites. Our code is publicly available at\nhttps:\/\/github.com\/cxcscmu\/Craw4LLM.",
        "Electrocardiogram (ECG) interpretation is a cornerstone of cardiac\ndiagnostics. This paper explores a practical approach to enhance ECG image\ninterpretation using the multimodal LLaMA 3.2 model. We used a\nparameter-efficient fine-tuning strategy, Low-Rank Adaptation (LoRA),\nspecifically designed to boost the model's ability to understand ECG images and\nachieve better outcomes across a wide range of cardiac conditions. Our method\nis tailored for ECG analysis and leverages ECGInstruct, a large-scale\ninstruction dataset with 1 Million samples. This dataset is a rich collection\nof synthesized ECG images, generated from raw ECG data from trusted open-source\nrepositories like MIMIC-IV ECG and PTB-XL. Each ECG image in ECGInstruct comes\nwith expert-written questions and detailed answers, covering diverse ECG\ninterpretation scenarios, including complex cardiac conditions like Myocardial\nInfarction and Conduction Disturbances. Our fine-tuning approach efficiently\nadapts the LLaMA 3.2 model (built upon LLaMA 3) by integrating low-rank\nadaptation techniques, focusing on efficiency by updating only a small set of\nparameters, specifically ignoring the `lm_head` and `embed_tokens` layers. This\npaper details the model setup, our efficient fine-tuning method, and\nimplementation specifics. We provide a thorough evaluation through extensive\nexperiments, demonstrating the effectiveness of our method across various ECG\ninterpretation tasks. The results convincingly show that our\nparameter-efficient LoRA fine-tuning achieves excellent performance in ECG\nimage interpretation, significantly outperforming baseline models and reaching\naccuracy comparable to or exceeding traditional CNN-based methods in\nidentifying a wide range of cardiac abnormalities, including over 70 conditions\nfrom the PTB-XL dataset.",
        "In this paper, we propose a graphon game model to understand how rumor (such\nas fake news) propagates in large populations that are interacting on a network\nand how different policies affect the spread. We extend the SKIR model that is\nused to model rumor propagation and implement individual controls and weighted\ninteractions with other agents to have controlled dynamics. The agents aim to\nminimize their own expected costs non-cooperatively. We give the finite player\ngame model and the limiting graphon game model to approximate the Nash\nequilibrium in the population. We give the graphon game Nash equilibrium as a\nsolution to a continuum of ordinary differential equations (ODEs) and give\nexistence results. Finally, we give a numerical approach and analyze examples\nwhere we use piecewise constant graphon.",
        "By imposing conditions upon the index of a self-centralizing subgroup of a\ngroup, and upon the index of the center of the group, we are able to classify\nthe Chermak-Delgado lattice of the group. This is our main result. We use this\nresult to classify the Chermak-Delgado lattices of dicyclic groups and of\nmetabelian $p$-groups of maximal class.",
        "Office Assistant Robots (OARs) offer a promising solution to proactively\nprovide in-situ support to enhance employee well-being and productivity in\noffice spaces. We introduce OfficeMate, a social OAR designed to assist with\npractical tasks, foster social interaction, and promote health and well-being.\nThrough a pilot evaluation with seven participants in an office environment, we\nfound that users see potential in OARs for reducing stress and promoting\nhealthy habits and value the robot's ability to provide companionship and\nphysical activity reminders in the office space. However, concerns regarding\nprivacy, communication, and the robot's interaction timing were also raised.\nThe feedback highlights the need to carefully consider the robot's appearance\nand behaviour to ensure it enhances user experience and aligns with office\nsocial norms. We believe these insights will better inform the development of\nadaptive, intelligent OAR systems for future office space integration.",
        "Computational Fluid Dynamics (CFD) is widely used in aerospace, energy, and\nbiology to model fluid flow, heat transfer, and chemical reactions. While Large\nLanguage Models (LLMs) have transformed various domains, their application in\nCFD remains limited, particularly for complex tasks like post-processing. To\nbridge this gap, we introduce MetaOpenFOAM 2.0, which leverages Chain of\nThought (COT) decomposition and iterative verification to enhance accessibility\nfor non-expert users through natural language inputs. Tested on a new benchmark\ncovering simulation (fluid flow, heat transfer, combustion) and post-processing\n(extraction, visualization), MetaOpenFOAM 2.0 achieved an Executability score\nof 6.3\/7 and a pass rate of 86.9%, significantly outperforming MetaOpenFOAM 1.0\n(2.1\/7, 0%). Additionally, it proved cost-efficient, averaging $0.15 per case.\nAn ablation study confirmed that COT-driven decomposition and iterative\nrefinement substantially improved task performance. Furthermore, scaling laws\nshowed that increasing COT steps enhanced accuracy while raising token usage,\naligning with LLM post-training scaling trends. These results highlight the\ntransformative potential of LLMs in automating CFD workflows for industrial and\nresearch applications. Code is available at\nhttps:\/\/github.com\/Terry-cyx\/MetaOpenFOAM"
      ]
    }
  },
  {
    "id":2411.1742,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"Deep learning-based classification of healthy aging controls, mild cognitive impairment and alzheimer's disease using fusion of mri-pet imaging",
    "start_abstract":"Automated detection of dementia stage using multimodal imaging modalities will be helpful for improving the clinical diagnosis. In this study, we develop the Inception-ResNet wrapper model in differentiating the healthy controls (HC), mild cognitive impairment (MCI), and Alzheimer\u2019s disease (AD) using conjoint magnetic resonance imaging (MRI) and positron emission tomography (PET) scans. We use T1-weighted MR and PET images of individuals aged between 42 and 95 years, including HC, MCI and AD patients. We first perform 3D tissue segmentation of MR images after skull striping. The atlas-based segmented MR image tissue is fused with PET image. Then we transform PET images from RGB to HSI color space and apply fusion of MRI with PET images using two-dimensional Fourier and discrete wavelet transform (DWT) and then reconstruct the MR-PET fused image using inverse Fourier and DWT methods. After the fusion of MRI and PET imaging modalities, we used 60 % training, 20 % for validation and the remaining 20 % as a test set using various convolutional neural networks. We found the proposed model as the best classifier with an accuracy of 95.5 %, 94.1 % and 95.9 % in classifying HC vs MCI, MCI vs AD and AD vs HC respectively when compared to the existing methods. We conclude that the proposed deep learning model has potential in automated classification of healthy and dementia stages using combined MRI and PET modalities with good performance.",
    "start_categories":[
      "cs.NE",
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b19"
      ],
      "title":[
        "Bidirectional Mapping of Brain MRI and PET With 3D Reversible GAN for the Diagnosis of Alzheimer\u2019s Disease"
      ],
      "abstract":[
        "<jats:p>Combining multi-modality data for brain disease diagnosis such as Alzheimer\u2019s disease (AD) commonly leads to improved performance than those using a single modality. However, it is still challenging to train a multi-modality model since it is difficult in clinical practice to obtain complete data that includes all modality data. Generally speaking, it is difficult to obtain both magnetic resonance images (MRI) and positron emission tomography (PET) images of a single patient. PET is expensive and requires the injection of radioactive substances into the patient\u2019s body, while MR images are cheaper, safer, and more widely used in practice. Discarding samples without PET data is a common method in previous studies, but the reduction in the number of samples will result in a decrease in model performance. To take advantage of multi-modal complementary information, we first adopt the Reversible Generative Adversarial Network (RevGAN) model to reconstruct the missing data. After that, a 3D convolutional neural network (CNN) classification model with multi-modality input was proposed to perform AD diagnosis. We have evaluated our method on the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) database, and compared the performance of the proposed method with those using state-of-the-art methods. The experimental results show that the structural and functional information of brain tissue can be mapped well and that the image synthesized by our method is close to the real image. In addition, the use of synthetic data is beneficial for the diagnosis and prediction of Alzheimer\u2019s disease, demonstrating the effectiveness of the proposed framework.<\/jats:p>"
      ],
      "categories":[
        "q-bio.NC"
      ]
    },
    "list":{
      "title":[
        "A 7T fMRI dataset of synthetic images for out-of-distribution modeling\n  of vision",
        "Personal Danger Signals Reprocessing: New Online Group Intervention for\n  Chronic Pain",
        "From Thought to Action: How a Hierarchy of Neural Dynamics Supports\n  Language Production",
        "Dynamical phases of short-term memory mechanisms in RNNs",
        "Stiff-sloppy analysis of brain networks to reveal individual differences\n  in task performance",
        "Neuro-oscillatory models of cortical speech processing",
        "Neural Constraints on Cognitive Experience and Mental Health",
        "Subthreshold moment analysis of neuronal populations driven by\n  synchronous synaptic inputs",
        "From Bedside to Desktop: A Data Protocol for Normative Intracranial EEG\n  and Abnormality Mapping",
        "Towards an integrative approach to the study of brain-environment\n  interactions in human and non-human primate",
        "Phase Alignment Enhances Oscillatory Power in Neural Mass Models\n  Optimized for Class Encoding",
        "How constraints on editing affects cultural evolution",
        "Spaces and sequences in the hippocampus: a homological perspective",
        "2D Surface Brightness Modelling of Large 2MASS Galaxies II: The Role of\n  Classical Bulges and Pseudobulges on Galaxy Scaling Relations and its\n  implication for Supermassive Black Hole Formation",
        "Feasibility of short blocklength Reed-Muller codes for physical layer\n  security in real environment",
        "Compact superconducting vacuum-gap capacitors with low microwave loss\n  and high mechanical coherence for scalable quantum circuits",
        "Probabilistic Shielding for Safe Reinforcement Learning",
        "Bayesian mixture modeling using a mixture of finite mixtures with\n  normalized inverse Gaussian weights",
        "Fitting multivariate Hawkes processes to interval count data with an\n  application to terrorist activity modelling -- a particle Markov chain Monte\n  Carlo approach",
        "Tuning topologically nontrivial states in the BHT-Ni metal organic\n  framework",
        "The R Package WMAP: Tools for Causal Meta-Analysis by Integrating\n  Multiple Observational Studies",
        "Operator-isomorphism pairs and Zak transform methods for the study of\n  Gabor systems",
        "Change Point Detection for Random Objects with Possibly Periodic\n  Behavior",
        "Enhancing energy transport utilising permanent molecular dipoles",
        "Continuum-wise hyperbolicity and periodic points",
        "Limits of nonlinear and dispersive fiber propagation for photonic\n  extreme learning",
        "Enhancement of Superconductivity in WP via Oxide-Assisted Chemical Vapor\n  Transport",
        "Structure factors and quantum geometry in multiband BCS superconductors"
      ],
      "abstract":[
        "Large-scale visual neural datasets such as the Natural Scenes Dataset (NSD)\nare boosting NeuroAI research by enabling computational models of the brain\nwith performances beyond what was possible just a decade ago. However, these\ndatasets lack out-of-distribution (OOD) components, which are crucial for the\ndevelopment of more robust models. Here, we address this limitation by\nreleasing NSD-synthetic, a dataset consisting of 7T fMRI responses from the\neight NSD subjects for 284 carefully controlled synthetic images. We show that\nNSD-synthetic's fMRI responses reliably encode stimulus-related information and\nare OOD with respect to NSD. Furthermore, OOD generalization tests on\nNSD-synthetic reveal differences between models of the brain that are not\ndetected with NSD - specifically, self-supervised deep neural networks better\nexplain neural responses than their task-supervised counterparts. These results\nshowcase how NSD-synthetic enables OOD generalization tests that facilitate the\ndevelopment of more robust models of visual processing, and the formulation of\nmore accurate theories of human vision.",
        "Chronic pain is a significant global health issue, with many patients\nexperiencing persistent pain despite no identifiable organic cause, classified\nas nociplastic pain. Increasing evidence highlights the role of danger signal\nprocessing in the maintenance of chronic pain. In response, we developed\nPersonal Danger Signals Reprocessing (PDSR), an online, group-based\nintervention designed to modify these mechanisms using coaching techniques to\nenhance accessibility and affordability.\n  This study evaluated the efficacy of PDSR in reducing pain and mental health\ncomorbidities. A cohort of women (N=19, mean age 43) participated in an 8-week\nonline program, receiving weekly sessions on chronic pain mechanisms within a\nsystemic framework. Outcomes were assessed at three time points:\npre-intervention, mid-intervention, and post-intervention. A waiting list group\n(N=20, mean age 43.5) completed assessments at the same intervals.\n  Participants in the PDSR group showed significant pain reduction (p < .001),\nwith moderate to large effects observed at mid-intervention (Cohen's D = 0.7)\nand post-intervention (Cohen's D = 1.5) compared to controls. Pain interference\nsignificantly decreased (p < .01), with large reductions in the PDSR group\n(Cohen's D = -1.7, p < .0001). Well-being also improved substantially (p <\n.001, Cohen's D = 1.7-1.8). Secondary outcomes, including pain catastrophizing,\nsleep interference, anxiety, and depressive symptoms, consistently improved\n(all p-values < .01).\n  Findings suggest PDSR is an effective, scalable intervention for reducing\npain, improving function, and enhancing well-being in individuals with chronic\npain.",
        "Humans effortlessly communicate their thoughts through intricate sequences of\nmotor actions. Yet, the neural processes that coordinate language production\nremain largely unknown, in part because speech artifacts limit the use of\nneuroimaging. To elucidate the unfolding of language production in the brain,\nwe investigate with magnetoencephalography (MEG) and electroencephalography\n(EEG) the neurophysiological activity of 35 skilled typists, while they typed\nsentences on a keyboard. This approach confirms the hierarchical predictions of\nlinguistic theories: the neural activity preceding the production of each word\nis marked by the sequential rise and fall of context-, word-, syllable-, and\nletter-level representations. Remarkably, each of these neural representations\nis maintained over long time periods within each level of the language\nhierarchy. This phenomenon results in a superposition of successive\nrepresentations that is supported by a hierarchy of dynamic neural codes.\nOverall, these findings provide a precise computational breakdown of the neural\ndynamics that coordinate the production of language in the human brain.",
        "Short-term memory is essential for cognitive processing, yet our\nunderstanding of its neural mechanisms remains unclear. A key focus in\nneuroscience has been the study of sequential activity patterns, where neurons\nfire one after another within large networks, to explain how information is\nmaintained. While recurrent connections were shown to drive sequential\ndynamics, a mechanistic understanding of this process still remains unknown. In\nthis work, we first introduce two unique mechanisms that can subserve\nshort-term memory: slow-point manifolds generating direct sequences or limit\ncycles providing temporally localized approximations. Then, through analytical\nmodels, we identify fundamental properties that govern the selection of these\nmechanisms, \\textit{i.e.}, we derive theoretical scaling laws for critical\nlearning rates as a function of the delay period length, beyond which no\nlearning is possible. We empirically verify these observations by training and\nevaluating more than 35,000 recurrent neural networks (RNNs) that we will\npublicly release upon publication. Overall, our work provides new insights into\nshort-term memory mechanisms and proposes experimentally testable predictions\nfor systems neuroscience.",
        "Understanding how brain networks recruit resources during cognitive tasks is\nkey to explaining individual differences in task performance. Brain network\nparameters-including activity levels of regions and their connectivity-reflect\nthe integration and segregation of functional subnetworks underlying task\nprocessing. However, the complexity and high dimensionality of these parameters\npose a significant barrier to identifying functionally relevant individual\ndifferences. Here, we introduce stiff-sloppy analysis as a framework for\nuncovering the stiff parameter combinations that critically influence\ntask-state brain dynamics, exemplified by working memory. Using the pairwise\nmaximum entropy model (PMEM) calibrated to fMRI data and Fisher Information\nMatrix (FIM) analysis, we reveal that the stiff dimensions of the model\nparameters capture the most relevant integration and segregation processes of\nthe default mode network and the working memory network. Individual differences\nalong these stiff neural dimensions consistently correlate with working memory\nperformance. Notably, stiff parameters robustly predicted working memory\nperformance, even when the less sensitive (\"sloppy\") parameters were excluded.\nThis study establishes stiff-sloppy analysis as a powerful approach to identify\ncognition-related brain networks, bridging neural dynamics and behavior and\noffering new avenues for personalized neuroscience including therapeutic\ninnovation.",
        "In this review, we examine computational models that explore the role of\nneural oscillations in speech perception, spanning from early auditory\nprocessing to higher cognitive stages. We focus on models that use rhythmic\nbrain activities, such as gamma, theta, and delta oscillations, to encode\nphonemes, segment speech into syllables and words, and integrate linguistic\nelements to infer meaning. We analyze the mechanisms underlying these models,\ntheir biological plausibility, and their potential applications in processing\nand understanding speech in real time, a computational feature that is achieved\nby the human brain but not yet implemented in speech recognition models.\nReal-time processing enables dynamic adaptation to incoming speech, allowing\nsystems to handle the rapid and continuous flow of auditory information\nrequired for effective communication, interactive applications, and accurate\nspeech recognition in a variety of real-world settings. While significant\nprogress has been made in modeling the neural basis of speech perception,\nchallenges remain, particularly in accounting for the complexity of semantic\nprocessing and the integration of contextual influences. Moreover, the high\ncomputational demands of biologically realistic models pose practical\ndifficulties for their implementation and analysis. Despite these limitations,\nthese models provide valuable insights into the neural mechanisms of speech\nperception. We conclude by identifying current limitations, proposing future\nresearch directions, and suggesting how these models can be further developed\nto achieve a more comprehensive understanding of speech processing in the human\nbrain.",
        "Understanding how neural dynamics shape cognitive experiences remains a\ncentral challenge in neuroscience and psychiatry. Here, we present a novel\nframework leveraging state-to-output controllability from dynamical systems\ntheory to model the interplay between cognitive perturbations, neural activity,\nand subjective experience. We demonstrate that large-scale fMRI signals are\nconstrained to low-dimensional manifolds, where affective and cognitive states\nare naturally organized. Furthermore, we provide a theoretically robust method\nto estimate the controllability Gramian from steady-state neural responses,\noffering a direct measure of the energy required to steer cognitive outcomes.\nIn five healthy participants viewing 2,185 emotionally evocative short videos,\nour analyses reveal a strong alignment between neural activations and affective\nratings, with an average correlation of $r \\approx 0.7$. In a clinical cohort\nof 255 patients with major depressive disorder, biweekly Hamilton Rating Scale\ntrajectories over 11 weeks significantly mapped onto these manifolds,\nexplaining approximately 20% more variance than chance ($p < 10^{-10}$,\nnumerically better than chance in 93% reaching statistical significance in\none-third of subjects). Our work bridges dynamical systems theory and clinical\nneuroscience, providing a principled approach to optimize mental health\ntreatments by targeting the most efficient neural pathways for cognitive\nchange.",
        "Even when driven by the same stimulus, neuronal responses are well-known to\nexhibit a striking level of spiking variability. In-vivo electrophysiological\nrecordings also reveal a surprisingly large degree of variability at the\nsubthreshold level. In prior work, we considered biophysically relevant\nneuronal models to account for the observed magnitude of membrane voltage\nfluctuations. We found that accounting for these fluctuations requires weak but\nnonzero synchrony in the spiking activity, in amount that are consistent with\nexperimentally measured spiking correlations. Here we investigate whether such\nsynchrony can explain additional statistical features of the measured neural\nactivity, including neuronal voltage covariability and voltage skewness.\nAddressing this question involves conducting a generalized moment analysis of\nconductance-based neurons in response to input drives modeled as correlated\njump processes. Technically, we perform such an analysis using fixed-point\ntechniques from queuing theory that are applicable in the stationary regime of\nactivity. We found that weak but nonzero synchrony can consistently explain the\nexperimentally reported voltage covariance and skewness. This confirms the role\nof synchrony as a primary driver of cortical variability and supports that\nphysiological neural activity emerges as a population-level phenomenon,\nespecially in the spontaneous regime.",
        "Normative mapping is a framework used to map population-level features of\nhealth-related variables. It is widely used in neuroscience research, but the\nliterature lacks established protocols in modalities that do not support\nhealthy control measurements, such as intracranial EEG (icEEG). An icEEG\nnormative map would allow researchers to learn about population-level brain\nactivity and enable comparison of individual data against these norms to\nidentify abnormalities. Currently, no standardised guide exists for\ntransforming clinical data into a normative, regional icEEG map. Papers often\ncite different software and numerous articles to summarise the lengthy method,\nmaking it laborious for other researchers to understand or apply the process.\nOur protocol seeks to remedy this gap by providing a dataflow guide and key\ndecision points that summarise existing methods. This protocol is used heavily\nin published works from our own lab (twelve peer-reviewed journal\npublications). Briefly, we take as input, icEEG recordings and neuroimaging\ndata from people with epilepsy who are undergoing evaluation for resective\nsurgery. As final outputs, we obtain a normative icEEG map, comprising signal\nproperties localised to brain regions. Optionally, we can also process new\nsubjects through the same pipeline and obtain their z-scores (or centiles) in\neach brain region, for abnormality detection and localisation. To date, a\nsingle, cohesive, dataflow pipeline for generating normative icEEG maps, along\nwith abnormality mapping, has not been created. We envisage that this dataflow\nguide will not only increase understanding and application of normative mapping\nmethods, but will also improve the consistency and quality of studies in the\nfield.",
        "By retracing my scientific journey that began 20 years ago, I highlight in\nthis thesis the need to consider the organization of the brain, which is\ncertainly globally hierarchical, but also highly distributed and mixed in the\nneuronal response of its different areas (by focusing on the sensorimotor\ncortex-basal ganglia network). I also emphasize the importance of adopting\nbehavioral paradigms that reflect as much as possible the characteristics of\nthe scenarios encountered in the real life of animals. And finally, I mention\nthe importance of \"disintegrating\" the way data analysis is traditionally\ncarried out, and of taking into account the dynamic nature of behavior, for\nexample by favoring the study of behavioral variables and so-called \"latent\"\nneuronal activities. I conclude this thesis by presenting the vision of my\nideal laboratory in a perspective of 5 to 10 years from today. This laboratory\nwould have two experimental devices, a first, classic, for carrying out tests\nin a constrained and therefore unnatural environment, the data of which would\nbe compared to those from a second device called \"naturalistic\" in which the\nanimals could potentially express their entire behavioral repertoire. The tasks\ntested would be characterized by strong ecological principles, such as in those\nsimulating the properties of foraging, and the data would make it possible to\ntest hypotheses based on the progressive addition of ecological components in\nthese tasks, all this in order to maintain control over the interpretability of\nthese data which are complex by nature.",
        "Neural encoding of objects and cognitive states remains an elusive yet\ncrucial aspect of brain function. While traditional feed-forward machine\nlearning neural networks have enormous potential to encode information, modern\narchitectures provide little insight into the brain's mechanisms. In this work,\na Jansen and Rit neural mass model was constructed to encode different sets of\ninputs, aiming to understand how simple neural circuits can represent\ninformation. A genetic algorithm was used to optimize parameters that maximized\nthe differences in responses to particular inputs. These differences in\nresponses manifested as phase-shifted oscillations across the set of inputs. By\ndelivering impulses of excitation synchronized with a particular phase-shifted\noscillation, we demonstrated that the encoded phase could be decoded by\nmeasuring oscillatory power. These findings demonstrate the capability of\nneural dynamical circuits to encode and decode information through phase.",
        "When is it beneficial to constrain creativity? Creativity thrives with\nfreedom, but when people collaborate to create artifacts, there is tension\nbetween giving individuals freedom to revise, and protecting prior\nachievements. To test how imposing constraints may affect collective\ncreativity, we performed cultural evolution experiments where participants\ncollaborated to create melodies and images in chains. With melodies, we found\nthat limiting step size (number of musical notes that can be changed) improved\npleasantness ratings for created tunes. Similar results were observed in\ncohorts of musicians, and with different selection regimes. In contrast,\nlimiting step size in creating images consistently reduced pleasantness. These\nconflicting findings suggest that in domains such as music, where artifacts can\nbe easily damaged, and where evolutionary outcomes are hard to foresee,\ncollective creativity may benefit from imposing small step sizes. We discuss\nparallels with search algorithms and the evolution of conservative birdsong\ncultures.",
        "Topological techniques have become a popular tool for studying information\nflows in neural networks. In particular, simplicial homology theory is used to\nanalyze how cognitive representations of space emerge from large conglomerates\nof independent neuronal contributions. Meanwhile, a growing number of studies\nsuggest that many cognitive functions are sustained by serial patterns of\nactivity. Here, we investigate stashes of such patterns using path homology\ntheory -- an impartial, universal approach that does not require a priori\nassumptions about the sequences' nature, functionality, underlying mechanisms,\nor other contexts. We focus on the hippocampus -- a key enabler of learning and\nmemory in mammalian brains -- and quantify the ordinal arrangement of its\nactivity similarly to how its topology has previously been studied in terms of\nsimplicial homologies. The results reveal that the vast majority of sequences\nproduced during spatial navigation are structurally equivalent to one another.\nOnly a few classes of distinct sequences form an ordinal schema of serial\nactivity that remains stable as the pool of sequences consolidates.\nImportantly, the structure of both maps is upheld by combinations of short\nsequences, suggesting that brief activity motifs dominate physiological\ncomputations. This ordinal organization emerges and stabilizes on timescales\ncharacteristic of spatial learning, displaying similar dynamics. Yet, the\nordinal maps generally do not reflect topological affinities -- spatial and\nsequential analyses address qualitatively different aspects of spike flows,\nrepresenting two complementary formats of information processing.",
        "We have generated 2D-multicomponent surface brightness (SB) modelling for 100\ngalaxies in the Large Galaxy Atlas (LGA) together with 19 nearby cD galaxies\nusing the near-infrared (NIR) images from 2MASS (J, H and Ks ). Our final\nsample of 119 galaxies includes cD galaxies, Virgo cluster galaxies, group\ngalaxies, and field galaxies. We revisited known scaling relations (SRs)\ninvolving structural parameters, as well as those involving supermassive black\nholes (SMBHs) and ultramassive black holes (UMBHs). Refining the SRs, we also\nrevisited the bulge classification and considered the Fundamental Plane (FP)\nand its projections, as well as other SRs, such as the colour-magnitude\nrelation (CMR), Tully-Fisher relation (TFR) and luminosity concentration\nrelation (LCR). Classical bulges follow the same relations as elliptical\ngalaxies, while pseudobulges are usually outliers. The NIR colours of classical\nbulges and pseudobulges indicate that their ages are not radically different\ndespite their spread in luminosity, but we noticed that classical bulges are\nmore luminous than pseudobulges, therefore, this property provides a\ncomplementary bulge classification criterion. We included pseudobulges from\nother studies to strengthen the tendencies seen for pseudobulges in our sample.\nFrom the SRs for BHs, we found that pseudobulges do not follow SRs for\nearly-type galaxies and classical bulges. Additionally, the lack of correlation\nbetween BHs and discs may indicate these structures have not coevolved. From\nthe revision of SRs, we present a sample of galaxies likely to host SMBHs or\nUMBHs, which are suitable for dynamical BH mass determination from the ground.",
        "In this paper, we investigate the application of Reed-Muller (RM) codes for\nPhysical-layer security in a real world wiretap channel scenario. Utilizing\nsoftware-defined radios (SDRs) in a real indoor environment, we implement a\ncoset coding scheme that leverages the hierarchical structure of RM codes to\nsecure data transmission. The generator matrix of the RM code is used to\npartition codewords into cosets in the usual way, where each message\ncorresponds to a unique coset, and auxiliary bits select specific codewords\nwithin each coset. This approach enables the legitimate receiver (Bob) can\ndecode the transmitted message with minimal information leakage to eavesdropper\n(Eve) thus protecting the confidentiality of the communication with the help of\ncoset structure. Mutual information neural estimation (MINE) is used to\nquantify information leakage and validate the effectiveness of the scheme.\nExperimental results indicate that RM codes can achieve robust security even in\npractical environments affected by real-world channel impairments. These\nfindings demonstrate the potential of RM codes as an efficient solution for\nphysical-layer security, particularly for applications that require low latency\nand short blocklengths.",
        "Vacuum gap capacitors have recently gained considerable attention in\nsuperconducting circuit platforms due to their compact design and low\ndielectric losses in the microwave regime. Their ability to support mechanical\nvibrational modes makes them ideal candidates for circuit optomechanics.\nHowever, precise control of gap size and achieving high coherence in mechanical\nmodes remain long-standing challenges. Here, we present a detailed fabrication\nprocess for scalable vacuum gap capacitors that support ultra-high-coherence\nmechanical motion, exhibit low microwave loss, and maintain a small footprint\ncompared to planar geometries. We fabricate arrays of up to 24 LC resonators,\nwith capacitors featuring nanometer-scale gap size variations. We demonstrate\nthat the mechanical quality factors can reach up to $40 \\times 10^6$, a\n100-fold improvement over other platforms, with microwave quality factors\n$\\mathcal{O}(10^5)$ at low photon number levels. This platform also achieves a\nsizable single-photon optomechanical coupling rate of approximately 20 Hz.\nUsing this, we cooled the mechanical oscillator to its ground state (0.07\nquanta) and squeezed its motion below the vacuum level by 2.7 dB. We further\ndemonstrate the scalability of this platform by implementing large-scale\noptomechanical arrays, a strained graphene model, and observing quantum\ncollective phenomena in a mechanical hexamer. These vacuum gap capacitors are\npromising candidates for coupling superconducting qubits with mechanical\nsystems, serving as storage elements in quantum computing, and exploring\ngravitational effects on quantum mechanics.",
        "In real-life scenarios, a Reinforcement Learning (RL) agent aiming to\nmaximise their reward, must often also behave in a safe manner, including at\ntraining time. Thus, much attention in recent years has been given to Safe RL,\nwhere an agent aims to learn an optimal policy among all policies that satisfy\na given safety constraint. However, strict safety guarantees are often provided\nthrough approaches based on linear programming, and thus have limited scaling.\nIn this paper we present a new, scalable method, which enjoys strict formal\nguarantees for Safe RL, in the case where the safety dynamics of the Markov\nDecision Process (MDP) are known, and safety is defined as an undiscounted\nprobabilistic avoidance property. Our approach is based on state-augmentation\nof the MDP, and on the design of a shield that restricts the actions available\nto the agent. We show that our approach provides a strict formal safety\nguarantee that the agent stays safe at training and test time. Furthermore, we\ndemonstrate that our approach is viable in practice through experimental\nevaluation.",
        "In Bayesian inference for mixture models with an unknown number of\ncomponents, a finite mixture model is usually employed that assumes prior\ndistributions for mixing weights and the number of components. This model is\ncalled a mixture of finite mixtures (MFM). As a prior distribution for the\nweights, a (symmetric) Dirichlet distribution is widely used for conjugacy and\ncomputational simplicity, while the selection of the concentration parameter\ninfluences the estimate of the number of components. In this paper, we focus on\nestimating the number of components. As a robust alternative Dirichlet weights,\nwe present a method based on a mixture of finite mixtures with normalized\ninverse Gaussian weights. The motivation is similar to the use of normalized\ninverse Gaussian processes instead of Dirichlet processes for infinite mixture\nmodeling. Introducing latent variables, the posterior computation is carried\nout using block Gibbs sampling without using the reversible jump algorithm. The\nperformance of the proposed method is illustrated through some numerical\nexperiments and real data examples, including clustering, density estimation,\nand community detection.",
        "Terrorist activities often exhibit temporal and spatial clustering, making\nthe multivariate Hawkes process (MHP) a useful statistical model for analysing\nterrorism across different geographic regions. However, terror attack data from\nthe Global Terrorism Database is reported as total event counts in disjoint\nobservation periods, with precise event times unknown. When the MHP is only\nobserved discretely, the likelihood function becomes intractable, hindering\nlikelihood-based inference. To address this, we design an unbiased estimate of\nthe intractable likelihood function using sequential Monte Carlo (SMC) based on\na representation of the unobserved event times as latent variables in a\nstate-space model. The unbiasedness of the SMC estimate allows for its use in\nplace of the true likelihood in a Metropolis-Hastings algorithm, from which we\nconstruct a Markov Chain Monte Carlo sample of the distribution over the\nparameters of the MHP. Using simulated data, we assess the performance of our\nmethod and demonstrate that it outperforms an alternative method in the\nliterature based on mean squared error. Terrorist activity in Afghanistan and\nPakistan from 2018 to 2021 is analysed based on daily count data to examine the\nself- and cross-excitation effects of terrorism events.",
        "Using first principles calculations, we have demonstrated the creation of\nmultiple quantum states, in the experimentally accessible metal organic\nframework BHT-Ni. Specifically, quantum spin Hall and quantum anomalous Hall\nstates are induced by two and four electron doping, respectively. The\ngeometrical symmetry breaking, is also investigated. For a low electron doping\nconcentration of two electrons per unit cell, the Fermi energy shifts to a\nnontrivial band gap, between Dirac bands and a quantized spin Hall conductivity\nis predicted. Subsequently in a high electron doping concentration, Anomalous\nHall conductivity with a quantized value was observed. In addition, for\ncentrosymmetric (trans-like) and non-centrosymmetric (cis-like) structures, we\nfound that the trans-like structure preserves quantum spin Hall and quantized\nspin Hall conductivity. In contrast, in the cis-like structure, space inversion\nsymmetry breaking leads to the appearance of valley Hall effect and the\ndisappearance of spin Hall conductivity.",
        "Integrating multiple observational studies for meta-analysis has sparked much\ninterest. The presented R package WMAP (Weighted Meta-Analysis with\nPseudo-Population) addresses a critical gap in the implementation of\nintegrative weighting approaches for multiple observational studies and causal\ninferences about various groups of subjects, such as disease subtypes. The\npackage features three weighting approaches, each representing a special case\nof the unified weighting framework introduced by Guha and Li (2024), which\nincludes an extension of inverse probability weights for data integration\nsettings. It performs meta-analysis on user-inputted datasets as follows: (i)\nit first estimates the propensity scores for study-group combinations,\ncalculates subject balancing weights, and determines the effective sample size\n(ESS) for a user-specified weighting method; and (ii) it then estimates various\nfeatures of multiple counterfactual group outcomes, such as group medians and\ndifferences in group means for the mRNA expression of eight genes.\nAdditionally, bootstrap variability estimates are provided. Among the\nimplemented weighting methods, we highlight the FLEXible, Optimized, and\nRealistic (FLEXOR) method, which is specifically designed to maximize the ESS\nwithin the unified framework. The use of the software is illustrated by\nsimulations as well as a multi-site breast cancer study conducted in seven\nmedical centers.",
        "We collect and summarize results on the unitary equivalence of Gabor systems\nby pairs of unitary operators and global isometries. The methods are then used\nto study Gabor systems with Hermite functions. We provide new proofs of some\nknown results and an outlook on double over-sampling.",
        "Time-varying random objects have been increasingly encountered in modern data\nanalysis. Moreover, in a substantial number of these applications, periodic\nbehavior of the random objects has been observed. We introduce a new, powerful\nscan statistic and corresponding test for the precise identification and\nlocalization of abrupt changes in the distribution of non-Euclidean random\nobjects with possibly periodic behavior. Our approach is nonparametric and\neffectively captures the entire distribution of these random objects.\nRemarkably, it operates with minimal tuning parameters, requiring only the\nspecification of cut-off intervals near endpoints, where change points are\nassumed not to occur. Our theoretical contributions include deriving the\nasymptotic distribution of the test statistic under the null hypothesis of no\nchange points, establishing the consistency of the test in the presence of\nchange points under contiguous alternatives and providing rigorous guarantees\non the near-optimal consistency in estimating the number and locations of\nchange points, whether dealing with a single change point or multiple ones. We\ndemonstrate that the most competitive method currently in the literature for\nchange point detection in random objects is degraded by periodic behavior, as\nperiodicity leads to blurring of the changes that this procedure aims to\ndiscover. Through comprehensive simulation studies, we demonstrate the superior\npower and accuracy of our approach in both detecting change points and\npinpointing their locations, across scenarios involving both periodic and\nnonperiodic random objects. Our main application is to weighted networks,\nrepresented through graph Laplacians. The proposed method delivers highly\ninterpretable results, as evidenced by the identification of meaningful change\npoints in the New York City Citi Bike sharing system that align with\nsignificant historical events.",
        "We study exciton quantum transfer along a molecular chain whilst accounting\nfor the effects of permanent dipoles that are induced by charge displacements\nin the molecular orbitals. These effects are typically neglected as they do not\narise in atomic quantum optics; however, they can play an important role in\nmolecular systems. We also consider novel collective photon-assisted transport\nand compare it against the scaling of phonon-assisted transport in chains\nfeaturing permanent dipoles, and determine a linear scaling with the number of\ndipoles, akin to single-excitation superradiance. We further demonstrate how\npermanent dipoles, dipoles can preferentially arrange energy eigenstates to\nsupport excitation transport. Finally, we show how permanent dipoles can\nenhance the ability of the molecular chain to support excitation transport\ncompared to that of systems that do not possess permanent dipoles across a\nrange of environmental and system configurations.",
        "We prove that cw-hyperbolic homeomorphisms with jointly continuous\nstable\/unstable holonomies satisfy the periodic shadowing property and, if they\nare topologically mixing, the periodic specification property. We discuss\ndifficulties to adapt Bowen's techniques to obtain a measure of maximal entropy\nfor cw-hyperbolic homeomorphisms, exhibit the unique measure of maximal entropy\nfor Walter's pseudo-Anosov diffeomorphism of $\\mathbb{S}^2$, and prove it can\nbe obtained, as in the expansive case, as the weak* limit of an average of\nDirac measures on periodic orbits. As an application, we exhibit the unique\nmeasure of maximal entropy for the homeomorphism on the Sierpi\\'nski Carpet\ndefined in [12], which does not satisfy the specification property.",
        "We report a generalized nonlinear Schr\\\"odinger equation simulation model of\nan extreme learning machine (ELM) based on optical fiber propagation. Using\nhandwritten digit classification as a benchmark, we study how accuracy depends\non propagation dynamics, as well as parameters governing spectral encoding,\nreadout, and noise. Test accuracies of over 91% and 93% are found for\npropagation in the anomalous and normal dispersion regimes respectively. Our\nsimulation results also suggest that quantum noise on the input pulses\nintroduces an intrinsic penalty to ELM performance.",
        "Tungsten monophosphide (WP) has been reported to superconduct below 0.8 K,\nand theoretical work has predicted an unconventional Cooper pairing mechanism.\nHere we present data for WP single crystals grown by means of chemical vapor\ntransport (CVT) of WO3, P, and I2. In comparison to synthesis using WP powder\nas a starting material, this technique results in samples with substantially\ndecreased low-temperature scattering and favors a more three dimensional\nmorphology. We also find that the resistive superconducting transitions in\nthese samples begin above 1 K. Variation in Tc is often found in strongly\ncorrelated superconductors, and its presence in WP could be the result of\ninfluence from a competing order and\/or a non s-wave gap.",
        "We consider multiband BCS superconductors that exhibit time-reversal symmetry\nand uniform pairing, and analyze their dynamic density and spin structure\nfactors using linear-response theory within the mean-field BCS-BEC crossover\nframework at zero temperature. Our results for the multi-orbital Hubbard model\nsatisfy the associated f-sum rules in several limits. In particular, in the\nstrong-coupling limit, they coincide with those of a weakly-interacting Bose\ngas of Cooper pairs, where the low-energy collective Goldstone modes serve as\nBogoliubov phonons. We further reveal that the quantum-geometric origin of the\nlow-energy structure factors, along with related observables such as the\nsuperfluid-weight tensor and the effective-mass tensor of Cooper pairs, can be\ntraced all the way back to the effective-mass theorem for Bloch bands in this\nlimit. As an illustration, we investigate the pyrochlore-Hubbard model\nnumerically and demonstrate that the Goldstone modes are the only relevant\ncollective degrees of freedom in the flat-band regime."
      ]
    }
  },
  {
    "id":2411.1742,
    "research_type":"applied",
    "start_id":"b17",
    "start_title":"Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks",
    "start_abstract":"Image-to-image translation is a class of vision and graphics problems where the goal to learn mapping between an input image output using training set aligned pairs. However, for many tasks, paired data will not be available. We present approach learning translate from source domain X target Y in absence examples. Our G : \u2192 such that distribution images G(X) indistinguishable adversarial loss. Because this highly under-constrained, we couple it with inverse F introduce cycle consistency loss push F(G(X)) \u2248 (and vice versa). Qualitative results are presented on several tasks does exist, including collection style transfer, object transfiguration, season photo enhancement, etc. Quantitative comparisons against prior methods demonstrate superiority our approach.",
    "start_categories":[
      "cs.NE",
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b19"
      ],
      "title":[
        "Bidirectional Mapping of Brain MRI and PET With 3D Reversible GAN for the Diagnosis of Alzheimer\u2019s Disease"
      ],
      "abstract":[
        "<jats:p>Combining multi-modality data for brain disease diagnosis such as Alzheimer\u2019s disease (AD) commonly leads to improved performance than those using a single modality. However, it is still challenging to train a multi-modality model since it is difficult in clinical practice to obtain complete data that includes all modality data. Generally speaking, it is difficult to obtain both magnetic resonance images (MRI) and positron emission tomography (PET) images of a single patient. PET is expensive and requires the injection of radioactive substances into the patient\u2019s body, while MR images are cheaper, safer, and more widely used in practice. Discarding samples without PET data is a common method in previous studies, but the reduction in the number of samples will result in a decrease in model performance. To take advantage of multi-modal complementary information, we first adopt the Reversible Generative Adversarial Network (RevGAN) model to reconstruct the missing data. After that, a 3D convolutional neural network (CNN) classification model with multi-modality input was proposed to perform AD diagnosis. We have evaluated our method on the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) database, and compared the performance of the proposed method with those using state-of-the-art methods. The experimental results show that the structural and functional information of brain tissue can be mapped well and that the image synthesized by our method is close to the real image. In addition, the use of synthetic data is beneficial for the diagnosis and prediction of Alzheimer\u2019s disease, demonstrating the effectiveness of the proposed framework.<\/jats:p>"
      ],
      "categories":[
        "q-bio.NC"
      ]
    },
    "list":{
      "title":[
        "A 7T fMRI dataset of synthetic images for out-of-distribution modeling\n  of vision",
        "Personal Danger Signals Reprocessing: New Online Group Intervention for\n  Chronic Pain",
        "From Thought to Action: How a Hierarchy of Neural Dynamics Supports\n  Language Production",
        "Dynamical phases of short-term memory mechanisms in RNNs",
        "Stiff-sloppy analysis of brain networks to reveal individual differences\n  in task performance",
        "Neuro-oscillatory models of cortical speech processing",
        "Neural Constraints on Cognitive Experience and Mental Health",
        "Subthreshold moment analysis of neuronal populations driven by\n  synchronous synaptic inputs",
        "From Bedside to Desktop: A Data Protocol for Normative Intracranial EEG\n  and Abnormality Mapping",
        "Towards an integrative approach to the study of brain-environment\n  interactions in human and non-human primate",
        "Phase Alignment Enhances Oscillatory Power in Neural Mass Models\n  Optimized for Class Encoding",
        "How constraints on editing affects cultural evolution",
        "Spaces and sequences in the hippocampus: a homological perspective",
        "2D Surface Brightness Modelling of Large 2MASS Galaxies II: The Role of\n  Classical Bulges and Pseudobulges on Galaxy Scaling Relations and its\n  implication for Supermassive Black Hole Formation",
        "Feasibility of short blocklength Reed-Muller codes for physical layer\n  security in real environment",
        "Compact superconducting vacuum-gap capacitors with low microwave loss\n  and high mechanical coherence for scalable quantum circuits",
        "Probabilistic Shielding for Safe Reinforcement Learning",
        "Bayesian mixture modeling using a mixture of finite mixtures with\n  normalized inverse Gaussian weights",
        "Fitting multivariate Hawkes processes to interval count data with an\n  application to terrorist activity modelling -- a particle Markov chain Monte\n  Carlo approach",
        "Tuning topologically nontrivial states in the BHT-Ni metal organic\n  framework",
        "The R Package WMAP: Tools for Causal Meta-Analysis by Integrating\n  Multiple Observational Studies",
        "Operator-isomorphism pairs and Zak transform methods for the study of\n  Gabor systems",
        "Change Point Detection for Random Objects with Possibly Periodic\n  Behavior",
        "Enhancing energy transport utilising permanent molecular dipoles",
        "Continuum-wise hyperbolicity and periodic points",
        "Limits of nonlinear and dispersive fiber propagation for photonic\n  extreme learning",
        "Enhancement of Superconductivity in WP via Oxide-Assisted Chemical Vapor\n  Transport",
        "Structure factors and quantum geometry in multiband BCS superconductors"
      ],
      "abstract":[
        "Large-scale visual neural datasets such as the Natural Scenes Dataset (NSD)\nare boosting NeuroAI research by enabling computational models of the brain\nwith performances beyond what was possible just a decade ago. However, these\ndatasets lack out-of-distribution (OOD) components, which are crucial for the\ndevelopment of more robust models. Here, we address this limitation by\nreleasing NSD-synthetic, a dataset consisting of 7T fMRI responses from the\neight NSD subjects for 284 carefully controlled synthetic images. We show that\nNSD-synthetic's fMRI responses reliably encode stimulus-related information and\nare OOD with respect to NSD. Furthermore, OOD generalization tests on\nNSD-synthetic reveal differences between models of the brain that are not\ndetected with NSD - specifically, self-supervised deep neural networks better\nexplain neural responses than their task-supervised counterparts. These results\nshowcase how NSD-synthetic enables OOD generalization tests that facilitate the\ndevelopment of more robust models of visual processing, and the formulation of\nmore accurate theories of human vision.",
        "Chronic pain is a significant global health issue, with many patients\nexperiencing persistent pain despite no identifiable organic cause, classified\nas nociplastic pain. Increasing evidence highlights the role of danger signal\nprocessing in the maintenance of chronic pain. In response, we developed\nPersonal Danger Signals Reprocessing (PDSR), an online, group-based\nintervention designed to modify these mechanisms using coaching techniques to\nenhance accessibility and affordability.\n  This study evaluated the efficacy of PDSR in reducing pain and mental health\ncomorbidities. A cohort of women (N=19, mean age 43) participated in an 8-week\nonline program, receiving weekly sessions on chronic pain mechanisms within a\nsystemic framework. Outcomes were assessed at three time points:\npre-intervention, mid-intervention, and post-intervention. A waiting list group\n(N=20, mean age 43.5) completed assessments at the same intervals.\n  Participants in the PDSR group showed significant pain reduction (p < .001),\nwith moderate to large effects observed at mid-intervention (Cohen's D = 0.7)\nand post-intervention (Cohen's D = 1.5) compared to controls. Pain interference\nsignificantly decreased (p < .01), with large reductions in the PDSR group\n(Cohen's D = -1.7, p < .0001). Well-being also improved substantially (p <\n.001, Cohen's D = 1.7-1.8). Secondary outcomes, including pain catastrophizing,\nsleep interference, anxiety, and depressive symptoms, consistently improved\n(all p-values < .01).\n  Findings suggest PDSR is an effective, scalable intervention for reducing\npain, improving function, and enhancing well-being in individuals with chronic\npain.",
        "Humans effortlessly communicate their thoughts through intricate sequences of\nmotor actions. Yet, the neural processes that coordinate language production\nremain largely unknown, in part because speech artifacts limit the use of\nneuroimaging. To elucidate the unfolding of language production in the brain,\nwe investigate with magnetoencephalography (MEG) and electroencephalography\n(EEG) the neurophysiological activity of 35 skilled typists, while they typed\nsentences on a keyboard. This approach confirms the hierarchical predictions of\nlinguistic theories: the neural activity preceding the production of each word\nis marked by the sequential rise and fall of context-, word-, syllable-, and\nletter-level representations. Remarkably, each of these neural representations\nis maintained over long time periods within each level of the language\nhierarchy. This phenomenon results in a superposition of successive\nrepresentations that is supported by a hierarchy of dynamic neural codes.\nOverall, these findings provide a precise computational breakdown of the neural\ndynamics that coordinate the production of language in the human brain.",
        "Short-term memory is essential for cognitive processing, yet our\nunderstanding of its neural mechanisms remains unclear. A key focus in\nneuroscience has been the study of sequential activity patterns, where neurons\nfire one after another within large networks, to explain how information is\nmaintained. While recurrent connections were shown to drive sequential\ndynamics, a mechanistic understanding of this process still remains unknown. In\nthis work, we first introduce two unique mechanisms that can subserve\nshort-term memory: slow-point manifolds generating direct sequences or limit\ncycles providing temporally localized approximations. Then, through analytical\nmodels, we identify fundamental properties that govern the selection of these\nmechanisms, \\textit{i.e.}, we derive theoretical scaling laws for critical\nlearning rates as a function of the delay period length, beyond which no\nlearning is possible. We empirically verify these observations by training and\nevaluating more than 35,000 recurrent neural networks (RNNs) that we will\npublicly release upon publication. Overall, our work provides new insights into\nshort-term memory mechanisms and proposes experimentally testable predictions\nfor systems neuroscience.",
        "Understanding how brain networks recruit resources during cognitive tasks is\nkey to explaining individual differences in task performance. Brain network\nparameters-including activity levels of regions and their connectivity-reflect\nthe integration and segregation of functional subnetworks underlying task\nprocessing. However, the complexity and high dimensionality of these parameters\npose a significant barrier to identifying functionally relevant individual\ndifferences. Here, we introduce stiff-sloppy analysis as a framework for\nuncovering the stiff parameter combinations that critically influence\ntask-state brain dynamics, exemplified by working memory. Using the pairwise\nmaximum entropy model (PMEM) calibrated to fMRI data and Fisher Information\nMatrix (FIM) analysis, we reveal that the stiff dimensions of the model\nparameters capture the most relevant integration and segregation processes of\nthe default mode network and the working memory network. Individual differences\nalong these stiff neural dimensions consistently correlate with working memory\nperformance. Notably, stiff parameters robustly predicted working memory\nperformance, even when the less sensitive (\"sloppy\") parameters were excluded.\nThis study establishes stiff-sloppy analysis as a powerful approach to identify\ncognition-related brain networks, bridging neural dynamics and behavior and\noffering new avenues for personalized neuroscience including therapeutic\ninnovation.",
        "In this review, we examine computational models that explore the role of\nneural oscillations in speech perception, spanning from early auditory\nprocessing to higher cognitive stages. We focus on models that use rhythmic\nbrain activities, such as gamma, theta, and delta oscillations, to encode\nphonemes, segment speech into syllables and words, and integrate linguistic\nelements to infer meaning. We analyze the mechanisms underlying these models,\ntheir biological plausibility, and their potential applications in processing\nand understanding speech in real time, a computational feature that is achieved\nby the human brain but not yet implemented in speech recognition models.\nReal-time processing enables dynamic adaptation to incoming speech, allowing\nsystems to handle the rapid and continuous flow of auditory information\nrequired for effective communication, interactive applications, and accurate\nspeech recognition in a variety of real-world settings. While significant\nprogress has been made in modeling the neural basis of speech perception,\nchallenges remain, particularly in accounting for the complexity of semantic\nprocessing and the integration of contextual influences. Moreover, the high\ncomputational demands of biologically realistic models pose practical\ndifficulties for their implementation and analysis. Despite these limitations,\nthese models provide valuable insights into the neural mechanisms of speech\nperception. We conclude by identifying current limitations, proposing future\nresearch directions, and suggesting how these models can be further developed\nto achieve a more comprehensive understanding of speech processing in the human\nbrain.",
        "Understanding how neural dynamics shape cognitive experiences remains a\ncentral challenge in neuroscience and psychiatry. Here, we present a novel\nframework leveraging state-to-output controllability from dynamical systems\ntheory to model the interplay between cognitive perturbations, neural activity,\nand subjective experience. We demonstrate that large-scale fMRI signals are\nconstrained to low-dimensional manifolds, where affective and cognitive states\nare naturally organized. Furthermore, we provide a theoretically robust method\nto estimate the controllability Gramian from steady-state neural responses,\noffering a direct measure of the energy required to steer cognitive outcomes.\nIn five healthy participants viewing 2,185 emotionally evocative short videos,\nour analyses reveal a strong alignment between neural activations and affective\nratings, with an average correlation of $r \\approx 0.7$. In a clinical cohort\nof 255 patients with major depressive disorder, biweekly Hamilton Rating Scale\ntrajectories over 11 weeks significantly mapped onto these manifolds,\nexplaining approximately 20% more variance than chance ($p < 10^{-10}$,\nnumerically better than chance in 93% reaching statistical significance in\none-third of subjects). Our work bridges dynamical systems theory and clinical\nneuroscience, providing a principled approach to optimize mental health\ntreatments by targeting the most efficient neural pathways for cognitive\nchange.",
        "Even when driven by the same stimulus, neuronal responses are well-known to\nexhibit a striking level of spiking variability. In-vivo electrophysiological\nrecordings also reveal a surprisingly large degree of variability at the\nsubthreshold level. In prior work, we considered biophysically relevant\nneuronal models to account for the observed magnitude of membrane voltage\nfluctuations. We found that accounting for these fluctuations requires weak but\nnonzero synchrony in the spiking activity, in amount that are consistent with\nexperimentally measured spiking correlations. Here we investigate whether such\nsynchrony can explain additional statistical features of the measured neural\nactivity, including neuronal voltage covariability and voltage skewness.\nAddressing this question involves conducting a generalized moment analysis of\nconductance-based neurons in response to input drives modeled as correlated\njump processes. Technically, we perform such an analysis using fixed-point\ntechniques from queuing theory that are applicable in the stationary regime of\nactivity. We found that weak but nonzero synchrony can consistently explain the\nexperimentally reported voltage covariance and skewness. This confirms the role\nof synchrony as a primary driver of cortical variability and supports that\nphysiological neural activity emerges as a population-level phenomenon,\nespecially in the spontaneous regime.",
        "Normative mapping is a framework used to map population-level features of\nhealth-related variables. It is widely used in neuroscience research, but the\nliterature lacks established protocols in modalities that do not support\nhealthy control measurements, such as intracranial EEG (icEEG). An icEEG\nnormative map would allow researchers to learn about population-level brain\nactivity and enable comparison of individual data against these norms to\nidentify abnormalities. Currently, no standardised guide exists for\ntransforming clinical data into a normative, regional icEEG map. Papers often\ncite different software and numerous articles to summarise the lengthy method,\nmaking it laborious for other researchers to understand or apply the process.\nOur protocol seeks to remedy this gap by providing a dataflow guide and key\ndecision points that summarise existing methods. This protocol is used heavily\nin published works from our own lab (twelve peer-reviewed journal\npublications). Briefly, we take as input, icEEG recordings and neuroimaging\ndata from people with epilepsy who are undergoing evaluation for resective\nsurgery. As final outputs, we obtain a normative icEEG map, comprising signal\nproperties localised to brain regions. Optionally, we can also process new\nsubjects through the same pipeline and obtain their z-scores (or centiles) in\neach brain region, for abnormality detection and localisation. To date, a\nsingle, cohesive, dataflow pipeline for generating normative icEEG maps, along\nwith abnormality mapping, has not been created. We envisage that this dataflow\nguide will not only increase understanding and application of normative mapping\nmethods, but will also improve the consistency and quality of studies in the\nfield.",
        "By retracing my scientific journey that began 20 years ago, I highlight in\nthis thesis the need to consider the organization of the brain, which is\ncertainly globally hierarchical, but also highly distributed and mixed in the\nneuronal response of its different areas (by focusing on the sensorimotor\ncortex-basal ganglia network). I also emphasize the importance of adopting\nbehavioral paradigms that reflect as much as possible the characteristics of\nthe scenarios encountered in the real life of animals. And finally, I mention\nthe importance of \"disintegrating\" the way data analysis is traditionally\ncarried out, and of taking into account the dynamic nature of behavior, for\nexample by favoring the study of behavioral variables and so-called \"latent\"\nneuronal activities. I conclude this thesis by presenting the vision of my\nideal laboratory in a perspective of 5 to 10 years from today. This laboratory\nwould have two experimental devices, a first, classic, for carrying out tests\nin a constrained and therefore unnatural environment, the data of which would\nbe compared to those from a second device called \"naturalistic\" in which the\nanimals could potentially express their entire behavioral repertoire. The tasks\ntested would be characterized by strong ecological principles, such as in those\nsimulating the properties of foraging, and the data would make it possible to\ntest hypotheses based on the progressive addition of ecological components in\nthese tasks, all this in order to maintain control over the interpretability of\nthese data which are complex by nature.",
        "Neural encoding of objects and cognitive states remains an elusive yet\ncrucial aspect of brain function. While traditional feed-forward machine\nlearning neural networks have enormous potential to encode information, modern\narchitectures provide little insight into the brain's mechanisms. In this work,\na Jansen and Rit neural mass model was constructed to encode different sets of\ninputs, aiming to understand how simple neural circuits can represent\ninformation. A genetic algorithm was used to optimize parameters that maximized\nthe differences in responses to particular inputs. These differences in\nresponses manifested as phase-shifted oscillations across the set of inputs. By\ndelivering impulses of excitation synchronized with a particular phase-shifted\noscillation, we demonstrated that the encoded phase could be decoded by\nmeasuring oscillatory power. These findings demonstrate the capability of\nneural dynamical circuits to encode and decode information through phase.",
        "When is it beneficial to constrain creativity? Creativity thrives with\nfreedom, but when people collaborate to create artifacts, there is tension\nbetween giving individuals freedom to revise, and protecting prior\nachievements. To test how imposing constraints may affect collective\ncreativity, we performed cultural evolution experiments where participants\ncollaborated to create melodies and images in chains. With melodies, we found\nthat limiting step size (number of musical notes that can be changed) improved\npleasantness ratings for created tunes. Similar results were observed in\ncohorts of musicians, and with different selection regimes. In contrast,\nlimiting step size in creating images consistently reduced pleasantness. These\nconflicting findings suggest that in domains such as music, where artifacts can\nbe easily damaged, and where evolutionary outcomes are hard to foresee,\ncollective creativity may benefit from imposing small step sizes. We discuss\nparallels with search algorithms and the evolution of conservative birdsong\ncultures.",
        "Topological techniques have become a popular tool for studying information\nflows in neural networks. In particular, simplicial homology theory is used to\nanalyze how cognitive representations of space emerge from large conglomerates\nof independent neuronal contributions. Meanwhile, a growing number of studies\nsuggest that many cognitive functions are sustained by serial patterns of\nactivity. Here, we investigate stashes of such patterns using path homology\ntheory -- an impartial, universal approach that does not require a priori\nassumptions about the sequences' nature, functionality, underlying mechanisms,\nor other contexts. We focus on the hippocampus -- a key enabler of learning and\nmemory in mammalian brains -- and quantify the ordinal arrangement of its\nactivity similarly to how its topology has previously been studied in terms of\nsimplicial homologies. The results reveal that the vast majority of sequences\nproduced during spatial navigation are structurally equivalent to one another.\nOnly a few classes of distinct sequences form an ordinal schema of serial\nactivity that remains stable as the pool of sequences consolidates.\nImportantly, the structure of both maps is upheld by combinations of short\nsequences, suggesting that brief activity motifs dominate physiological\ncomputations. This ordinal organization emerges and stabilizes on timescales\ncharacteristic of spatial learning, displaying similar dynamics. Yet, the\nordinal maps generally do not reflect topological affinities -- spatial and\nsequential analyses address qualitatively different aspects of spike flows,\nrepresenting two complementary formats of information processing.",
        "We have generated 2D-multicomponent surface brightness (SB) modelling for 100\ngalaxies in the Large Galaxy Atlas (LGA) together with 19 nearby cD galaxies\nusing the near-infrared (NIR) images from 2MASS (J, H and Ks ). Our final\nsample of 119 galaxies includes cD galaxies, Virgo cluster galaxies, group\ngalaxies, and field galaxies. We revisited known scaling relations (SRs)\ninvolving structural parameters, as well as those involving supermassive black\nholes (SMBHs) and ultramassive black holes (UMBHs). Refining the SRs, we also\nrevisited the bulge classification and considered the Fundamental Plane (FP)\nand its projections, as well as other SRs, such as the colour-magnitude\nrelation (CMR), Tully-Fisher relation (TFR) and luminosity concentration\nrelation (LCR). Classical bulges follow the same relations as elliptical\ngalaxies, while pseudobulges are usually outliers. The NIR colours of classical\nbulges and pseudobulges indicate that their ages are not radically different\ndespite their spread in luminosity, but we noticed that classical bulges are\nmore luminous than pseudobulges, therefore, this property provides a\ncomplementary bulge classification criterion. We included pseudobulges from\nother studies to strengthen the tendencies seen for pseudobulges in our sample.\nFrom the SRs for BHs, we found that pseudobulges do not follow SRs for\nearly-type galaxies and classical bulges. Additionally, the lack of correlation\nbetween BHs and discs may indicate these structures have not coevolved. From\nthe revision of SRs, we present a sample of galaxies likely to host SMBHs or\nUMBHs, which are suitable for dynamical BH mass determination from the ground.",
        "In this paper, we investigate the application of Reed-Muller (RM) codes for\nPhysical-layer security in a real world wiretap channel scenario. Utilizing\nsoftware-defined radios (SDRs) in a real indoor environment, we implement a\ncoset coding scheme that leverages the hierarchical structure of RM codes to\nsecure data transmission. The generator matrix of the RM code is used to\npartition codewords into cosets in the usual way, where each message\ncorresponds to a unique coset, and auxiliary bits select specific codewords\nwithin each coset. This approach enables the legitimate receiver (Bob) can\ndecode the transmitted message with minimal information leakage to eavesdropper\n(Eve) thus protecting the confidentiality of the communication with the help of\ncoset structure. Mutual information neural estimation (MINE) is used to\nquantify information leakage and validate the effectiveness of the scheme.\nExperimental results indicate that RM codes can achieve robust security even in\npractical environments affected by real-world channel impairments. These\nfindings demonstrate the potential of RM codes as an efficient solution for\nphysical-layer security, particularly for applications that require low latency\nand short blocklengths.",
        "Vacuum gap capacitors have recently gained considerable attention in\nsuperconducting circuit platforms due to their compact design and low\ndielectric losses in the microwave regime. Their ability to support mechanical\nvibrational modes makes them ideal candidates for circuit optomechanics.\nHowever, precise control of gap size and achieving high coherence in mechanical\nmodes remain long-standing challenges. Here, we present a detailed fabrication\nprocess for scalable vacuum gap capacitors that support ultra-high-coherence\nmechanical motion, exhibit low microwave loss, and maintain a small footprint\ncompared to planar geometries. We fabricate arrays of up to 24 LC resonators,\nwith capacitors featuring nanometer-scale gap size variations. We demonstrate\nthat the mechanical quality factors can reach up to $40 \\times 10^6$, a\n100-fold improvement over other platforms, with microwave quality factors\n$\\mathcal{O}(10^5)$ at low photon number levels. This platform also achieves a\nsizable single-photon optomechanical coupling rate of approximately 20 Hz.\nUsing this, we cooled the mechanical oscillator to its ground state (0.07\nquanta) and squeezed its motion below the vacuum level by 2.7 dB. We further\ndemonstrate the scalability of this platform by implementing large-scale\noptomechanical arrays, a strained graphene model, and observing quantum\ncollective phenomena in a mechanical hexamer. These vacuum gap capacitors are\npromising candidates for coupling superconducting qubits with mechanical\nsystems, serving as storage elements in quantum computing, and exploring\ngravitational effects on quantum mechanics.",
        "In real-life scenarios, a Reinforcement Learning (RL) agent aiming to\nmaximise their reward, must often also behave in a safe manner, including at\ntraining time. Thus, much attention in recent years has been given to Safe RL,\nwhere an agent aims to learn an optimal policy among all policies that satisfy\na given safety constraint. However, strict safety guarantees are often provided\nthrough approaches based on linear programming, and thus have limited scaling.\nIn this paper we present a new, scalable method, which enjoys strict formal\nguarantees for Safe RL, in the case where the safety dynamics of the Markov\nDecision Process (MDP) are known, and safety is defined as an undiscounted\nprobabilistic avoidance property. Our approach is based on state-augmentation\nof the MDP, and on the design of a shield that restricts the actions available\nto the agent. We show that our approach provides a strict formal safety\nguarantee that the agent stays safe at training and test time. Furthermore, we\ndemonstrate that our approach is viable in practice through experimental\nevaluation.",
        "In Bayesian inference for mixture models with an unknown number of\ncomponents, a finite mixture model is usually employed that assumes prior\ndistributions for mixing weights and the number of components. This model is\ncalled a mixture of finite mixtures (MFM). As a prior distribution for the\nweights, a (symmetric) Dirichlet distribution is widely used for conjugacy and\ncomputational simplicity, while the selection of the concentration parameter\ninfluences the estimate of the number of components. In this paper, we focus on\nestimating the number of components. As a robust alternative Dirichlet weights,\nwe present a method based on a mixture of finite mixtures with normalized\ninverse Gaussian weights. The motivation is similar to the use of normalized\ninverse Gaussian processes instead of Dirichlet processes for infinite mixture\nmodeling. Introducing latent variables, the posterior computation is carried\nout using block Gibbs sampling without using the reversible jump algorithm. The\nperformance of the proposed method is illustrated through some numerical\nexperiments and real data examples, including clustering, density estimation,\nand community detection.",
        "Terrorist activities often exhibit temporal and spatial clustering, making\nthe multivariate Hawkes process (MHP) a useful statistical model for analysing\nterrorism across different geographic regions. However, terror attack data from\nthe Global Terrorism Database is reported as total event counts in disjoint\nobservation periods, with precise event times unknown. When the MHP is only\nobserved discretely, the likelihood function becomes intractable, hindering\nlikelihood-based inference. To address this, we design an unbiased estimate of\nthe intractable likelihood function using sequential Monte Carlo (SMC) based on\na representation of the unobserved event times as latent variables in a\nstate-space model. The unbiasedness of the SMC estimate allows for its use in\nplace of the true likelihood in a Metropolis-Hastings algorithm, from which we\nconstruct a Markov Chain Monte Carlo sample of the distribution over the\nparameters of the MHP. Using simulated data, we assess the performance of our\nmethod and demonstrate that it outperforms an alternative method in the\nliterature based on mean squared error. Terrorist activity in Afghanistan and\nPakistan from 2018 to 2021 is analysed based on daily count data to examine the\nself- and cross-excitation effects of terrorism events.",
        "Using first principles calculations, we have demonstrated the creation of\nmultiple quantum states, in the experimentally accessible metal organic\nframework BHT-Ni. Specifically, quantum spin Hall and quantum anomalous Hall\nstates are induced by two and four electron doping, respectively. The\ngeometrical symmetry breaking, is also investigated. For a low electron doping\nconcentration of two electrons per unit cell, the Fermi energy shifts to a\nnontrivial band gap, between Dirac bands and a quantized spin Hall conductivity\nis predicted. Subsequently in a high electron doping concentration, Anomalous\nHall conductivity with a quantized value was observed. In addition, for\ncentrosymmetric (trans-like) and non-centrosymmetric (cis-like) structures, we\nfound that the trans-like structure preserves quantum spin Hall and quantized\nspin Hall conductivity. In contrast, in the cis-like structure, space inversion\nsymmetry breaking leads to the appearance of valley Hall effect and the\ndisappearance of spin Hall conductivity.",
        "Integrating multiple observational studies for meta-analysis has sparked much\ninterest. The presented R package WMAP (Weighted Meta-Analysis with\nPseudo-Population) addresses a critical gap in the implementation of\nintegrative weighting approaches for multiple observational studies and causal\ninferences about various groups of subjects, such as disease subtypes. The\npackage features three weighting approaches, each representing a special case\nof the unified weighting framework introduced by Guha and Li (2024), which\nincludes an extension of inverse probability weights for data integration\nsettings. It performs meta-analysis on user-inputted datasets as follows: (i)\nit first estimates the propensity scores for study-group combinations,\ncalculates subject balancing weights, and determines the effective sample size\n(ESS) for a user-specified weighting method; and (ii) it then estimates various\nfeatures of multiple counterfactual group outcomes, such as group medians and\ndifferences in group means for the mRNA expression of eight genes.\nAdditionally, bootstrap variability estimates are provided. Among the\nimplemented weighting methods, we highlight the FLEXible, Optimized, and\nRealistic (FLEXOR) method, which is specifically designed to maximize the ESS\nwithin the unified framework. The use of the software is illustrated by\nsimulations as well as a multi-site breast cancer study conducted in seven\nmedical centers.",
        "We collect and summarize results on the unitary equivalence of Gabor systems\nby pairs of unitary operators and global isometries. The methods are then used\nto study Gabor systems with Hermite functions. We provide new proofs of some\nknown results and an outlook on double over-sampling.",
        "Time-varying random objects have been increasingly encountered in modern data\nanalysis. Moreover, in a substantial number of these applications, periodic\nbehavior of the random objects has been observed. We introduce a new, powerful\nscan statistic and corresponding test for the precise identification and\nlocalization of abrupt changes in the distribution of non-Euclidean random\nobjects with possibly periodic behavior. Our approach is nonparametric and\neffectively captures the entire distribution of these random objects.\nRemarkably, it operates with minimal tuning parameters, requiring only the\nspecification of cut-off intervals near endpoints, where change points are\nassumed not to occur. Our theoretical contributions include deriving the\nasymptotic distribution of the test statistic under the null hypothesis of no\nchange points, establishing the consistency of the test in the presence of\nchange points under contiguous alternatives and providing rigorous guarantees\non the near-optimal consistency in estimating the number and locations of\nchange points, whether dealing with a single change point or multiple ones. We\ndemonstrate that the most competitive method currently in the literature for\nchange point detection in random objects is degraded by periodic behavior, as\nperiodicity leads to blurring of the changes that this procedure aims to\ndiscover. Through comprehensive simulation studies, we demonstrate the superior\npower and accuracy of our approach in both detecting change points and\npinpointing their locations, across scenarios involving both periodic and\nnonperiodic random objects. Our main application is to weighted networks,\nrepresented through graph Laplacians. The proposed method delivers highly\ninterpretable results, as evidenced by the identification of meaningful change\npoints in the New York City Citi Bike sharing system that align with\nsignificant historical events.",
        "We study exciton quantum transfer along a molecular chain whilst accounting\nfor the effects of permanent dipoles that are induced by charge displacements\nin the molecular orbitals. These effects are typically neglected as they do not\narise in atomic quantum optics; however, they can play an important role in\nmolecular systems. We also consider novel collective photon-assisted transport\nand compare it against the scaling of phonon-assisted transport in chains\nfeaturing permanent dipoles, and determine a linear scaling with the number of\ndipoles, akin to single-excitation superradiance. We further demonstrate how\npermanent dipoles, dipoles can preferentially arrange energy eigenstates to\nsupport excitation transport. Finally, we show how permanent dipoles can\nenhance the ability of the molecular chain to support excitation transport\ncompared to that of systems that do not possess permanent dipoles across a\nrange of environmental and system configurations.",
        "We prove that cw-hyperbolic homeomorphisms with jointly continuous\nstable\/unstable holonomies satisfy the periodic shadowing property and, if they\nare topologically mixing, the periodic specification property. We discuss\ndifficulties to adapt Bowen's techniques to obtain a measure of maximal entropy\nfor cw-hyperbolic homeomorphisms, exhibit the unique measure of maximal entropy\nfor Walter's pseudo-Anosov diffeomorphism of $\\mathbb{S}^2$, and prove it can\nbe obtained, as in the expansive case, as the weak* limit of an average of\nDirac measures on periodic orbits. As an application, we exhibit the unique\nmeasure of maximal entropy for the homeomorphism on the Sierpi\\'nski Carpet\ndefined in [12], which does not satisfy the specification property.",
        "We report a generalized nonlinear Schr\\\"odinger equation simulation model of\nan extreme learning machine (ELM) based on optical fiber propagation. Using\nhandwritten digit classification as a benchmark, we study how accuracy depends\non propagation dynamics, as well as parameters governing spectral encoding,\nreadout, and noise. Test accuracies of over 91% and 93% are found for\npropagation in the anomalous and normal dispersion regimes respectively. Our\nsimulation results also suggest that quantum noise on the input pulses\nintroduces an intrinsic penalty to ELM performance.",
        "Tungsten monophosphide (WP) has been reported to superconduct below 0.8 K,\nand theoretical work has predicted an unconventional Cooper pairing mechanism.\nHere we present data for WP single crystals grown by means of chemical vapor\ntransport (CVT) of WO3, P, and I2. In comparison to synthesis using WP powder\nas a starting material, this technique results in samples with substantially\ndecreased low-temperature scattering and favors a more three dimensional\nmorphology. We also find that the resistive superconducting transitions in\nthese samples begin above 1 K. Variation in Tc is often found in strongly\ncorrelated superconductors, and its presence in WP could be the result of\ninfluence from a competing order and\/or a non s-wave gap.",
        "We consider multiband BCS superconductors that exhibit time-reversal symmetry\nand uniform pairing, and analyze their dynamic density and spin structure\nfactors using linear-response theory within the mean-field BCS-BEC crossover\nframework at zero temperature. Our results for the multi-orbital Hubbard model\nsatisfy the associated f-sum rules in several limits. In particular, in the\nstrong-coupling limit, they coincide with those of a weakly-interacting Bose\ngas of Cooper pairs, where the low-energy collective Goldstone modes serve as\nBogoliubov phonons. We further reveal that the quantum-geometric origin of the\nlow-energy structure factors, along with related observables such as the\nsuperfluid-weight tensor and the effective-mass tensor of Cooper pairs, can be\ntraced all the way back to the effective-mass theorem for Bloch bands in this\nlimit. As an illustration, we investigate the pyrochlore-Hubbard model\nnumerically and demonstrate that the Goldstone modes are the only relevant\ncollective degrees of freedom in the flat-band regime."
      ]
    }
  },
  {
    "id":2411.1742,
    "research_type":"applied",
    "start_id":"b19",
    "start_title":"Bidirectional Mapping of Brain MRI and PET With 3D Reversible GAN for the Diagnosis of Alzheimer\u2019s Disease",
    "start_abstract":"<jats:p>Combining multi-modality data for brain disease diagnosis such as Alzheimer\u2019s disease (AD) commonly leads to improved performance than those using a single modality. However, it is still challenging to train a multi-modality model since it is difficult in clinical practice to obtain complete data that includes all modality data. Generally speaking, it is difficult to obtain both magnetic resonance images (MRI) and positron emission tomography (PET) images of a single patient. PET is expensive and requires the injection of radioactive substances into the patient\u2019s body, while MR images are cheaper, safer, and more widely used in practice. Discarding samples without PET data is a common method in previous studies, but the reduction in the number of samples will result in a decrease in model performance. To take advantage of multi-modal complementary information, we first adopt the Reversible Generative Adversarial Network (RevGAN) model to reconstruct the missing data. After that, a 3D convolutional neural network (CNN) classification model with multi-modality input was proposed to perform AD diagnosis. We have evaluated our method on the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) database, and compared the performance of the proposed method with those using state-of-the-art methods. The experimental results show that the structural and functional information of brain tissue can be mapped well and that the image synthesized by our method is close to the real image. In addition, the use of synthetic data is beneficial for the diagnosis and prediction of Alzheimer\u2019s disease, demonstrating the effectiveness of the proposed framework.<\/jats:p>",
    "start_categories":[
      "q-bio.NC"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b3",
        "b17"
      ],
      "title":[
        "Deep learning-based classification of healthy aging controls, mild cognitive impairment and alzheimer's disease using fusion of mri-pet imaging",
        "Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks"
      ],
      "abstract":[
        "Automated detection of dementia stage using multimodal imaging modalities will be helpful for improving the clinical diagnosis. In this study, we develop the Inception-ResNet wrapper model in differentiating the healthy controls (HC), mild cognitive impairment (MCI), and Alzheimer\u2019s disease (AD) using conjoint magnetic resonance imaging (MRI) and positron emission tomography (PET) scans. We use T1-weighted MR and PET images of individuals aged between 42 and 95 years, including HC, MCI and AD patients. We first perform 3D tissue segmentation of MR images after skull striping. The atlas-based segmented MR image tissue is fused with PET image. Then we transform PET images from RGB to HSI color space and apply fusion of MRI with PET images using two-dimensional Fourier and discrete wavelet transform (DWT) and then reconstruct the MR-PET fused image using inverse Fourier and DWT methods. After the fusion of MRI and PET imaging modalities, we used 60 % training, 20 % for validation and the remaining 20 % as a test set using various convolutional neural networks. We found the proposed model as the best classifier with an accuracy of 95.5 %, 94.1 % and 95.9 % in classifying HC vs MCI, MCI vs AD and AD vs HC respectively when compared to the existing methods. We conclude that the proposed deep learning model has potential in automated classification of healthy and dementia stages using combined MRI and PET modalities with good performance.",
        "Image-to-image translation is a class of vision and graphics problems where the goal to learn mapping between an input image output using training set aligned pairs. However, for many tasks, paired data will not be available. We present approach learning translate from source domain X target Y in absence examples. Our G : \u2192 such that distribution images G(X) indistinguishable adversarial loss. Because this highly under-constrained, we couple it with inverse F introduce cycle consistency loss push F(G(X)) \u2248 (and vice versa). Qualitative results are presented on several tasks does exist, including collection style transfer, object transfiguration, season photo enhancement, etc. Quantitative comparisons against prior methods demonstrate superiority our approach."
      ],
      "categories":[
        "cs.NE",
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "Chameleon: On the Scene Diversity and Domain Variety of AI-Generated\n  Videos Detection",
        "LXLv2: Enhanced LiDAR Excluded Lean 3D Object Detection with Fusion of\n  4D Radar and Camera",
        "SAFER: Sharpness Aware layer-selective Finetuning for Enhanced\n  Robustness in vision transformers",
        "DroneSplat: 3D Gaussian Splatting for Robust 3D Reconstruction from\n  In-the-Wild Drone Imagery",
        "MExD: An Expert-Infused Diffusion Model for Whole-Slide Image\n  Classification",
        "Test-time Loss Landscape Adaptation for Zero-Shot Generalization in\n  Vision-Language Models",
        "Towards Intelligent Design: A Self-driven Framework for Collocated\n  Clothing Synthesis Leveraging Fashion Styles and Textures",
        "Go-with-the-Flow: Motion-Controllable Video Diffusion Models Using\n  Real-Time Warped Noise",
        "Fine-Grained Image-Text Correspondence with Cost Aggregation for\n  Open-Vocabulary Part Segmentation",
        "Visual Autoregressive Modeling for Image Super-Resolution",
        "Green Video Camouflaged Object Detection",
        "Attention Reallocation: Towards Zero-cost and Controllable Hallucination\n  Mitigation of MLLMs",
        "A mathematical perspective on the paradox that chemotherapy sometimes\n  works backwards",
        "An Efficient Quantum Approximate Optimization Algorithm with Fixed\n  Linear Ramp Schedule for Truss Structure Optimization",
        "The COSMOS-Web ring: Spectroscopic confirmation of the background source\n  at z = 5.1",
        "Financial Fraud Detection with Entropy Computing",
        "Survey on Question Answering over Visually Rich Documents: Methods,\n  Challenges, and Trends",
        "Cracking Vector Search Indexes",
        "Fineness and smoothness of a KSBA moduli of marked cubic surfaces",
        "Aspects of Complexity in Quantum Evolutions on the Bloch Sphere",
        "Fluctuation-driven topological Hall effect in room-temperature itinerant\n  helimagnet Fe3Ga4",
        "Invariant and non-invariant almost complex structures on compact\n  quotients of Lie groups",
        "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search",
        "Self-supervised conformal prediction for uncertainty quantification in\n  Poisson imaging problems",
        "Dynamic Manipulation of Multiphase Fluid in Microgravity Using\n  Photoresponsive Surfactant",
        "InfiGUIAgent: A Multimodal Generalist GUI Agent with Native Reasoning\n  and Reflection",
        "EXALT: EXplainable ALgorithmic Tools for Optimization Problems"
      ],
      "abstract":[
        "Artificial intelligence generated content (AIGC), known as DeepFakes, has\nemerged as a growing concern because it is being utilized as a tool for\nspreading disinformation. While much research exists on identifying\nAI-generated text and images, research on detecting AI-generated videos is\nlimited. Existing datasets for AI-generated videos detection exhibit\nlimitations in terms of diversity, complexity, and realism. To address these\nissues, this paper focuses on AI-generated videos detection and constructs a\ndiverse dataset named Chameleon. We generate videos through multiple generation\ntools and various real video sources. At the same time, we preserve the videos'\nreal-world complexity, including scene switches and dynamic perspective\nchanges, and expand beyond face-centered detection to include human actions and\nenvironment generation. Our work bridges the gap between AI-generated dataset\nconstruction and real-world forensic needs, offering a valuable benchmark to\ncounteract the evolving threats of AI-generated content.",
        "As the previous state-of-the-art 4D radar-camera fusion-based 3D object\ndetection method, LXL utilizes the predicted image depth distribution maps and\nradar 3D occupancy grids to assist the sampling-based image view\ntransformation. However, the depth prediction lacks accuracy and consistency,\nand the concatenation-based fusion in LXL impedes the model robustness. In this\nwork, we propose LXLv2, where modifications are made to overcome the\nlimitations and improve the performance. Specifically, considering the position\nerror in radar measurements, we devise a one-to-many depth supervision strategy\nvia radar points, where the radar cross section (RCS) value is further\nexploited to adjust the supervision area for object-level depth consistency.\nAdditionally, a channel and spatial attention-based fusion module named\nCSAFusion is introduced to improve feature adaptiveness. Experimental results\non the View-of-Delft and TJ4DRadSet datasets show that the proposed LXLv2 can\noutperform LXL in detection accuracy, inference speed and robustness,\ndemonstrating the effectiveness of the model.",
        "Vision transformers (ViTs) have become essential backbones in advanced\ncomputer vision applications and multi-modal foundation models. Despite their\nstrengths, ViTs remain vulnerable to adversarial perturbations, comparable to\nor even exceeding the vulnerability of convolutional neural networks (CNNs).\nFurthermore, the large parameter count and complex architecture of ViTs make\nthem particularly prone to adversarial overfitting, often compromising both\nclean and adversarial accuracy.\n  This paper mitigates adversarial overfitting in ViTs through a novel,\nlayer-selective fine-tuning approach: SAFER. Instead of optimizing the entire\nmodel, we identify and selectively fine-tune a small subset of layers most\nsusceptible to overfitting, applying sharpness-aware minimization to these\nlayers while freezing the rest of the model. Our method consistently enhances\nboth clean and adversarial accuracy over baseline approaches. Typical\nimprovements are around 5%, with some cases achieving gains as high as 20%\nacross various ViT architectures and datasets.",
        "Drones have become essential tools for reconstructing wild scenes due to\ntheir outstanding maneuverability. Recent advances in radiance field methods\nhave achieved remarkable rendering quality, providing a new avenue for 3D\nreconstruction from drone imagery. However, dynamic distractors in wild\nenvironments challenge the static scene assumption in radiance fields, while\nlimited view constraints hinder the accurate capture of underlying scene\ngeometry. To address these challenges, we introduce DroneSplat, a novel\nframework designed for robust 3D reconstruction from in-the-wild drone imagery.\nOur method adaptively adjusts masking thresholds by integrating local-global\nsegmentation heuristics with statistical approaches, enabling precise\nidentification and elimination of dynamic distractors in static scenes. We\nenhance 3D Gaussian Splatting with multi-view stereo predictions and a\nvoxel-guided optimization strategy, supporting high-quality rendering under\nlimited view constraints. For comprehensive evaluation, we provide a\ndrone-captured 3D reconstruction dataset encompassing both dynamic and static\nscenes. Extensive experiments demonstrate that DroneSplat outperforms both 3DGS\nand NeRF baselines in handling in-the-wild drone imagery.",
        "Whole Slide Image (WSI) classification poses unique challenges due to the\nvast image size and numerous non-informative regions, which introduce noise and\ncause data imbalance during feature aggregation. To address these issues, we\npropose MExD, an Expert-Infused Diffusion Model that combines the strengths of\na Mixture-of-Experts (MoE) mechanism with a diffusion model for enhanced\nclassification. MExD balances patch feature distribution through a novel\nMoE-based aggregator that selectively emphasizes relevant information,\neffectively filtering noise, addressing data imbalance, and extracting\nessential features. These features are then integrated via a diffusion-based\ngenerative process to directly yield the class distribution for the WSI. Moving\nbeyond conventional discriminative approaches, MExD represents the first\ngenerative strategy in WSI classification, capturing fine-grained details for\nrobust and precise results. Our MExD is validated on three widely-used\nbenchmarks-Camelyon16, TCGA-NSCLC, and BRACS consistently achieving\nstate-of-the-art performance in both binary and multi-class tasks.",
        "Test-time adaptation of pre-trained vision-language models has emerged as a\ntechnique for tackling distribution shifts during the test time. Although\nexisting methods, especially those based on Test-time Prompt Tuning (TPT), have\nshown promising results, their high computational cost associated with\nparameter optimization presents challenges for scalability and practical\napplication. This paper unveils the unnecessary nature of backpropagation in\nexisting methods from a loss landscape perspective. Building on this insight,\nthis paper proposes a simple yet effective framework called Test-time Loss\nLandscape Adaptation (TLLA). TLLA leverages the relative position between the\ntraining minimum and test loss landscapes to guide the adaptation process,\navoiding the update of model parameters at test time. Specifically, it mainly\nconsists of two main stages: In the prompt tuning stage, a Sharpness-Aware\nPrompt Tuning (SAPT) method is introduced to identify the training flat\nminimum, setting the foundation for the subsequent test-time adaptation; In the\ntest stage, a Sharpness-based Test Sample Selection (STSS) approach is utilized\nto ensure the alignment of flat minima within the training loss landscape and\neach augmented test sample's loss landscape. Extensive experiments on both\ndomain generalization and cross-dataset benchmarks demonstrate that TLLA\nachieves state-of-the-art performances while significantly reducing\ncomputational overhead. Notably, TLLA surpasses TPT by an average of 5.32\\% and\n6.98\\% on four ImageNet variant datasets when employing ResNet50 and ViT-B\/16\nimage encoders, respectively. The code will be available soon.",
        "Collocated clothing synthesis (CCS) has emerged as a pivotal topic in fashion\ntechnology, primarily concerned with the generation of a clothing item that\nharmoniously matches a given item. However, previous investigations have relied\non using paired outfits, such as a pair of matching upper and lower clothing,\nto train a generative model for achieving this task. This reliance on the\nexpertise of fashion professionals in the construction of such paired outfits\nhas engendered a laborious and time-intensive process. In this paper, we\nintroduce a new self-driven framework, named style- and texture-guided\ngenerative network (ST-Net), to synthesize collocated clothing without the\nnecessity for paired outfits, leveraging self-supervised learning. ST-Net is\ndesigned to extrapolate fashion compatibility rules from the style and texture\nattributes of clothing, using a generative adversarial network. To facilitate\nthe training and evaluation of our model, we have constructed a large-scale\ndataset specifically tailored for unsupervised CCS. Extensive experiments\nsubstantiate that our proposed method outperforms the state-of-the-art\nbaselines in terms of both visual authenticity and fashion compatibility.",
        "Generative modeling aims to transform random noise into structured outputs.\nIn this work, we enhance video diffusion models by allowing motion control via\nstructured latent noise sampling. This is achieved by just a change in data: we\npre-process training videos to yield structured noise. Consequently, our method\nis agnostic to diffusion model design, requiring no changes to model\narchitectures or training pipelines. Specifically, we propose a novel noise\nwarping algorithm, fast enough to run in real time, that replaces random\ntemporal Gaussianity with correlated warped noise derived from optical flow\nfields, while preserving the spatial Gaussianity. The efficiency of our\nalgorithm enables us to fine-tune modern video diffusion base models using\nwarped noise with minimal overhead, and provide a one-stop solution for a wide\nrange of user-friendly motion control: local object motion control, global\ncamera movement control, and motion transfer. The harmonization between\ntemporal coherence and spatial Gaussianity in our warped noise leads to\neffective motion control while maintaining per-frame pixel quality. Extensive\nexperiments and user studies demonstrate the advantages of our method, making\nit a robust and scalable approach for controlling motion in video diffusion\nmodels. Video results are available on our webpage:\nhttps:\/\/eyeline-research.github.io\/Go-with-the-Flow. Source code and model\ncheckpoints are available on GitHub:\nhttps:\/\/github.com\/Eyeline-Research\/Go-with-the-Flow.",
        "Open-Vocabulary Part Segmentation (OVPS) is an emerging field for recognizing\nfine-grained parts in unseen categories. We identify two primary challenges in\nOVPS: (1) the difficulty in aligning part-level image-text correspondence, and\n(2) the lack of structural understanding in segmenting object parts. To address\nthese issues, we propose PartCATSeg, a novel framework that integrates\nobject-aware part-level cost aggregation, compositional loss, and structural\nguidance from DINO. Our approach employs a disentangled cost aggregation\nstrategy that handles object and part-level costs separately, enhancing the\nprecision of part-level segmentation. We also introduce a compositional loss to\nbetter capture part-object relationships, compensating for the limited part\nannotations. Additionally, structural guidance from DINO features improves\nboundary delineation and inter-part understanding. Extensive experiments on\nPascal-Part-116, ADE20K-Part-234, and PartImageNet datasets demonstrate that\nour method significantly outperforms state-of-the-art approaches, setting a new\nbaseline for robust generalization to unseen part categories.",
        "Image Super-Resolution (ISR) has seen significant progress with the\nintroduction of remarkable generative models. However, challenges such as the\ntrade-off issues between fidelity and realism, as well as computational\ncomplexity, have also posed limitations on their application. Building upon the\ntremendous success of autoregressive models in the language domain, we propose\n\\textbf{VARSR}, a novel visual autoregressive modeling for ISR framework with\nthe form of next-scale prediction. To effectively integrate and preserve\nsemantic information in low-resolution images, we propose using prefix tokens\nto incorporate the condition. Scale-aligned Rotary Positional Encodings are\nintroduced to capture spatial structures and the diffusion refiner is utilized\nfor modeling quantization residual loss to achieve pixel-level fidelity.\nImage-based Classifier-free Guidance is proposed to guide the generation of\nmore realistic images. Furthermore, we collect large-scale data and design a\ntraining process to obtain robust generative priors. Quantitative and\nqualitative results show that VARSR is capable of generating high-fidelity and\nhigh-realism images with more efficiency than diffusion-based methods. Our\ncodes will be released at https:\/\/github.com\/qyp2000\/VARSR.",
        "Camouflaged object detection (COD) aims to distinguish hidden objects\nembedded in an environment highly similar to the object. Conventional\nvideo-based COD (VCOD) methods explicitly extract motion cues or employ complex\ndeep learning networks to handle the temporal information, which is limited by\nhigh complexity and unstable performance. In this work, we propose a green VCOD\nmethod named GreenVCOD. Built upon a green ICOD method, GreenVCOD uses long-\nand short-term temporal neighborhoods (TN) to capture joint spatial\/temporal\ncontext information for decision refinement. Experimental results show that\nGreenVCOD offers competitive performance compared to state-of-the-art VCOD\nbenchmarks.",
        "Multi-Modal Large Language Models (MLLMs) stand out in various tasks but\nstill struggle with hallucinations. While recent training-free mitigation\nmethods mostly introduce additional inference overhead via retrospection\nstrategy and contrastive decoding, we propose attention reallocation (AttnReal)\nto mitigate hallucinations with nearly zero extra cost. Our approach is\nmotivated by the key observations that, MLLM's unreasonable attention\ndistribution causes features to be dominated by historical output tokens, which\nfurther contributes to hallucinated responses because of the distribution gap\nbetween different token types. Based on the observations, AttnReal recycles\nexcessive attention from output tokens and reallocates it to visual tokens,\nwhich reduces MLLM's reliance on language priors and ensures the decoding\nprocess depends more on the visual inputs. More interestingly, we find that, by\ncontrolling the intensity of AttnReal, we can achieve a wide-range trade-off\nbetween the response faithfulness and overall performance. Comprehensive\nresults from different benchmarks validate the effectiveness of AttnReal across\nsix open-source MLLMs and three decoding strategies.",
        "Doctors are well aware that sometimes cancer treatments not only fail, but\neven work backwards, i.e. they make the treated tumor grow. In this work we\npresent a mathematical perspective on this paradox in the case of chemotherapy,\nby studying a minimally parameterized mathematical model for the system\ncomposed of the tumor and the surrounding vasculature. To this end, we will use\na system of two well-established nonlinear ordinary differential equations,\nwhich incorporates the cytotoxic (via the Norton-Simon hypothesis) and\nantiangiogenic effects of chemotherapy. Finally, we provide two theoretical\nways to avoid these anomalies.",
        "This study proposes a novel structural optimization framework based on\nquantum variational circuits, in which the multiplier acting on the\ncross-sectional area of each rod in a truss structure as an updater is used as\na design variable. Specifically, we employ a classical processor for structural\nanalysis with the finite element method, and the Quantum Approximate\nOptimization Algorithm (QAOA) is subsequently performed to update the\ncross-sectional area so that the compliance is minimized. The advantages of\nthis framework can be seen in three key aspects. First, by defining design\nvariables as multipliers, rather than simply reducing the design variable to a\nbinary candidate of inclusion or exclusion (corresponding to qubit states, ``0\"\nand ``1\"), it provides greater flexibility in adjusting the cross-sectional\narea of the rod at each iteration of the optimization process. Second, the\nmultipliers acting on rods are encoded with on-off encoding, eliminating\nadditional constraints in the convergence judgement. As a result, the objective\nfunction is in a simple format, enabling efficient optimization using\nQAOA.Third, a fixed linear ramp schedule (FLRS) for variational parameter\nsetting bypasses the classical optimization process, thereby improving the\noperational efficiency of the framework. In the two structural cases\ninvestigated in this study, the proposed approach highlights the feasibility\nand applicability potential of quantum computing in advancing engineering\ndesign and optimization. Numerical experiments have demonstrated the\neffectiveness of this framework, providing a firm foundation for future\nresearch on quantum-assisted optimization methods in engineering fields.",
        "We report the spectroscopic confirmation of the background source of the most\ndistant Einstein ring known to date, the COSMOS-Web ring. This system consists\nof a complete Einstein ring at $z=5.1$, lensed by a massive early-type galaxy\nat $z\\sim2$. The redshift $z=5.1043\\pm0.0004$ is unambiguously identified with\nour NOEMA and Keck\/MOSFIRE spectroscopy, where the NOEMA observations reveal\nthe CO(4-3) and CO(5-4) lines at $>8\\,\\sigma$, and the MOSFIRE data detect\n[O\\textsc{ii}] at $\\sim 6\\,\\sigma$. Using multi-wavelength photometry spanning\nnear-infrared to radio bands, we find that the lensed galaxy is a dust-obscured\nstarburst ($M_{\\star} \\sim 1.8\\times10^{10}\\,{\\rm M_{\\odot}}$, ${\\rm\nSFR_{IR}\\sim 60\\,{\\rm M_{\\odot}} ~yr^{-1}}$) with high star-formation\nefficiency (gas depletion time $\\tau_{\\rm dep}<100~$Myr) as indicated by the\n[C\\textsc{i}](1-0) non-detection. The redshift confirmation revalidates that\nthe total lens mass budget within the Einstein radius is fully accounted for by\nthe stellar and dark matter components, without the need of modifying the\ninitial mass function or dark matter distribution profile. This work paves the\nway for detailed studies and future follow-ups of this unique lensing system,\nproviding an ideal laboratory for studying mass distribution at $z\\sim2$ and\nphysical conditions of star formation at $z\\sim5$.",
        "We introduce CVQBoost, a novel classification algorithm that leverages early\nhardware implementing Quantum Computing Inc's Entropy Quantum Computing (EQC)\nparadigm, Dirac-3 [Nguyen et. al. arXiv:2407.04512]. We apply CVQBoost to a\nfraud detection test case and benchmark its performance against XGBoost, a\nwidely utilized ML method. Running on Dirac-3, CVQBoost demonstrates a\nsignificant runtime advantage over XGBoost, which we evaluate on\nhigh-performance hardware comprising up to 48 CPUs and four NVIDIA L4 GPUs\nusing the RAPIDS AI framework. Our results show that CVQBoost maintains\ncompetitive accuracy (measured by AUC) while significantly reducing training\ntime, particularly as dataset size and feature complexity increase. To assess\nscalability, we extend our study to large synthetic datasets ranging from 1M to\n70M samples, demonstrating that CVQBoost on Dirac-3 is well-suited for\nlarge-scale classification tasks. These findings position CVQBoost as a\npromising alternative to gradient boosting methods, offering superior\nscalability and efficiency for high-dimensional ML applications such as fraud\ndetection.",
        "The field of visually-rich document understanding, which involves interacting\nwith visually-rich documents (whether scanned or born-digital), is rapidly\nevolving and still lacks consensus on several key aspects of the processing\npipeline. In this work, we provide a comprehensive overview of state-of-the-art\napproaches, emphasizing their strengths and limitations, pointing out the main\nchallenges in the field, and proposing promising research directions.",
        "Retrieval Augmented Generation (RAG) uses vector databases to expand the\nexpertise of an LLM model without having to retrain it. This idea can be\napplied over data lakes, leading to the notion of embeddings data lakes, i.e.,\na pool of vector databases ready to be used by RAGs. The key component in these\nsystems is the indexes enabling Approximated Nearest Neighbor Search (ANNS).\nHowever, in data lakes, one cannot realistically expect to build indexes for\nevery possible dataset. In this paper, we propose an adaptive, partition-based\nindex, CrackIVF, that performs much better than up-front index building.\nCrackIVF starts answering queries by near brute force search and only expands\nas it sees enough queries. It does so by progressively adapting the index to\nthe query workload. That way, queries can be answered right away without having\nto build a full index first. After seeing enough queries, CrackIVF will produce\nan index comparable to the best of those built using conventional techniques.\nAs the experimental evaluation shows, CrackIVF can often answer more than 1\nmillion queries before other approaches have even built the index and can start\nanswering queries immediately, achieving 10-1000x faster initialization times.\nThis makes it ideal when working with cold data or infrequently used data or as\na way to bootstrap access to unseen datasets.",
        "By work of Gallardo-Kerr-Schaffler, it is known that Naruki's\ncompactification of the moduli space of marked cubic surfaces is isomorphic to\nthe normalization of the Koll\\'ar, Shepherd-Barron, and Alexeev\ncompactification parametrizing pairs\n$\\left(S,\\left(\\frac{1}{9}+\\epsilon\\right)D\\right)$, with $D$ the sum of the\n$27$ marked lines on $S$, and their stable degenerations. In the current paper,\nwe show that the normalization assumption is not necessary as we prove that\nthis KSBA compactification is smooth. Additionally, we show it is a fine moduli\nspace. This is done by studying the automorphisms and the\n$\\mathbb{Q}$-Gorenstein obstructions of the stable pairs parametrized by it.",
        "We enhance our quantitative comprehension of the complexity associated with\nboth time-optimal and time sub-optimal quantum Hamiltonian evolutions that\nconnect arbitrary source and target states on the Bloch sphere, as recently\npresented in Nucl. Phys. B1010, 116755 (2025). Initially, we examine each\nunitary Schrodinger quantum evolution selected through various metrics, such as\npath length, geodesic efficiency, speed efficiency, and the curvature\ncoefficient of the corresponding quantum-mechanical trajectory that connects\nthe source state to the target state on the Bloch sphere. Subsequently, we\nevaluate the selected evolutions using our proposed measure of complexity, as\nwell as in relation to the concept of complexity length scale. The choice of\nboth time-optimal and time sub-optimal evolutions, along with the selection of\nsource and target states, enables us to conduct pertinent sanity checks that\nseek to validate the physical relevance of the framework supporting our\nproposed complexity measure. Our research suggests that, in general, efficient\nquantum evolutions possess a lower complexity than their inefficient\ncounterparts. However, it is important to recognize that complexity is not\nsolely determined by length; in fact, longer trajectories that are adequately\ncurved may exhibit a complexity that is less than or equal to that of shorter\ntrajectories with a lower curvature coefficient.",
        "The topological Hall effect (THE) is a hallmark of a non-trivial geometric\nspin arrangement in a magnetic metal, originating from a finite scalar spin\nchirality (SSC). The associated Berry phase is often a consequence of\nnon-coplanar magnetic structures identified by multiple k-vectors. For single-k\nmagnetic structures however with zero SSC, the emergence of a finite\ntopological Hall signal presents a conceptual challenge. Here, we report that a\nfluctuation-driven mechanism involving chiral magnons is responsible for the\nobserved THE in a low-symmetry compound, monoclinic Fe3Ga4. Through neutron\nscattering experiments, we discovered several nontrivial magnetic phases in\nthis system. In our focus is the helical spiral phase at room temperature,\nwhich transforms into a transverse conical state in applied magnetic field,\nsupporting a significant THE signal up to and above room temperature. Our work\noffers a fresh perspective in the search for novel materials with intertwined\ntopological magnetic and transport properties.",
        "In this paper we briefly survey the classical problem of understanding which\nLie algebras admit a complex structure, put in the broader perspective of\nalmost complex structures with special properties. We focus on the different\nbehavior of invariant and non-invariant structures, with a special attention to\ntheir canonical bundle and Kodaira dimension. We provide new examples of\ncomputations of Kodaira dimension of invariant and non-invariant structures.",
        "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index.\n  This paper introduces VSAG, an open-source framework that aims to enhance the\nin production performance of graph-based ANNS algorithms. VSAG has been\ndeployed at scale in the services of Ant Group, and it incorporates three key\noptimizations: (i) efficient memory access: it reduces L3 cache misses with\npre-fetching and cache-friendly vector organization; (ii) automated parameter\ntuning: it automatically selects performance-optimal parameters without\nrequiring index rebuilding; (iii) efficient distance computation: it leverages\nmodern hardware, scalar quantization, and smartly switches to low-precision\nrepresentation to dramatically reduce the distance computation costs. We\nevaluate VSAG on real-world datasets. The experimental results show that VSAG\nachieves the state-of-the-art performance and provides up to 4x speedup over\nHNSWlib (an industry-standard library) while ensuring the same accuracy.",
        "Image restoration problems are often ill-posed, leading to significant\nuncertainty in reconstructed images. Accurately quantifying this uncertainty is\nessential for the reliable interpretation of reconstructed images. However,\nimage restoration methods often lack uncertainty quantification capabilities.\nConformal prediction offers a rigorous framework to augment image restoration\nmethods with accurate uncertainty quantification estimates, but it typically\nrequires abundant ground truth data for calibration. This paper presents a\nself-supervised conformal prediction method for Poisson imaging problems which\nleverages Poisson Unbiased Risk Estimator to eliminate the need for ground\ntruth data. The resulting self-calibrating conformal prediction approach is\napplicable to any Poisson linear imaging problem that is ill-conditioned, and\nis particularly effective when combined with modern self-supervised image\nrestoration techniques trained directly on measurement data. The proposed\nmethod is demonstrated through numerical experiments on image denoising and\ndeblurring; its performance are comparable to supervised conformal prediction\nmethods relying on ground truth data.",
        "Control of bubble motion is essential for improving efficiency and creating\nnew functionalities in electrochemistry, heat transfer, and biomedical systems.\nPhotoresponsive surfactants enable bubble manipulation by creating surface\ntension gradients, inducing a photo-Marangoni flow under illumination, without\nneeding any engineered substrates, by leveraging a reversible switch in\nmolecular conformation. Although previous studies have demonstrated bubble\nmanipulation using photo-responsive surfactants, a comprehensive understanding\nof how fluid behavior is affected by critical parameters, such as bubble size,\nillumination, photo-switching kinetics, concentration, and adsorption\ndesorption kinetics, remains elusive. Advances have been limited by the complex\nmultiphysics processed involved, and by the fact that earth-bound experiments\ncannot study bubble photo-Marangoni dynamics without interference from bubble\nbuoyancy and photo-thermal convection. We elucidate the factors enabling fast\nphoto-Marangoni-driven bubble motion, by performing microgravity experiments,\nenabled by a bespoke photo-surfactant, complemented by a detailed modeling\nframework. We identify an optimal bubble size for migration, since smaller and\nlarger bubbles incur weaker photo-Marangoni stresses and larger drag,\nrespectively. Surfactants that switch rapidly under illumination drive fast\nmigration, provided their reverse switch (in darkness) is much slower, yet not\nnegligible. These foundational results enable the synthesis of next-generation\nphoto-surfactants and photo-Marangoni manipulation across multiphase fluid\nsystems.",
        "Graphical User Interface (GUI) Agents, powered by multimodal large language\nmodels (MLLMs), have shown great potential for task automation on computing\ndevices such as computers and mobile phones. However, existing agents face\nchallenges in multi-step reasoning and reliance on textual annotations,\nlimiting their effectiveness. We introduce \\textit{InfiGUIAgent}, an MLLM-based\nGUI Agent trained with a two-stage supervised fine-tuning pipeline. Stage 1\nenhances fundamental skills such as GUI understanding and grounding, while\nStage 2 integrates hierarchical reasoning and expectation-reflection reasoning\nskills using synthesized data to enable native reasoning abilities of the\nagents. \\textit{InfiGUIAgent} achieves competitive performance on several GUI\nbenchmarks, highlighting the impact of native reasoning skills in enhancing GUI\ninteraction for automation tasks. Resources are available at\n\\url{https:\/\/github.com\/Reallm-Labs\/InfiGUIAgent}.",
        "Algorithmic solutions have significant potential to improve decision-making\nacross various domains, from healthcare to e-commerce. However, the widespread\nadoption of these solutions is hindered by a critical challenge: the lack of\nhuman-interpretable explanations. Current approaches to Explainable AI (XAI)\npredominantly focus on complex machine learning models, often producing brittle\nand non-intuitive explanations. This project proposes a novel approach to\ndeveloping explainable algorithms by starting with optimization problems,\nspecifically the assignment problem. The developed software library enriches\nbasic algorithms with human-understandable explanations through four key\nmethodologies: generating meaningful alternative solutions, creating robust\nsolutions through input perturbation, generating concise decision trees and\nproviding reports with comprehensive explanation of the results. Currently\ndeveloped tools are often designed with specific clustering algorithms in mind,\nwhich limits their adaptability and flexibility to incorporate alternative\ntechniques. Additionally, many of these tools fail to integrate expert\nknowledge, which could enhance the clustering process by providing valuable\ninsights and context. This lack of adaptability and integration can hinder the\neffectiveness and robustness of the clustering outcomes in various\napplications. The represents a step towards making algorithmic solutions more\ntransparent, trustworthy, and accessible. By collaborating with industry\npartners in sectors such as sales, we demonstrate the practical relevance and\ntransformative potential of our approach."
      ]
    }
  },
  {
    "id":2412.16425,
    "research_type":"applied",
    "start_id":"b40",
    "start_title":"Whole Slide Imaging Versus Microscopy for Primary Diagnosis in Surgical Pathology: A Multicenter Blinded Randomized Noninferiority Study of 1992 Cases (Pivotal Study)",
    "start_abstract":"Most prior studies of primary diagnosis in surgical pathology using whole slide imaging (WSI) versus microscopy have focused on specific organ systems or included relatively few cases. The objective this study was to demonstrate that WSI is noninferior for pathology. A blinded randomized noninferiority conducted across the entire range cases (biopsies and resections, including hematoxylin eosin, immunohistochemistry, special stains) from 4 institutions original sign-out (baseline diagnosis) as reference standard. Cases were scanned, converted randomized. Sixteen pathologists interpreted by WSI, followed a wash-out period \u22654 weeks, after which read same observers other modality. Major discordances identified an adjudication panel, differences between major discordance rates both (against standard) calculated. total 1992 included, resulting 15,925 reads. rate with standard 4.9% 4.6% microscopy. difference 0.4% (95% confidence interval, -0.30% 1.01%). highest endocrine (1.8%), neoplastic kidney (1.5%), urinary bladder (1.3%), gynecologic (1.2%). Detailed analysis these revealed no instances where interpretation consistently inaccurate compared multiple observers. We conclude pathology, biopsies resections stained immunohistochemistry stains. This conclusion valid wide variety specimen types.",
    "start_categories":[
      "q-bio.CB"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b15"
      ],
      "title":[
        "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      ],
      "abstract":[
        "While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "VideoPure: Diffusion-based Adversarial Purification for Video\n  Recognition",
        "Motion Anything: Any to Motion Generation",
        "Embed Any NeRF: Graph Meta-Networks for Neural Tasks on Arbitrary NeRF\n  Architectures",
        "Multi-modal Fusion and Query Refinement Network for Video Moment\n  Retrieval and Highlight Detection",
        "DNRSelect: Active Best View Selection for Deferred Neural Rendering",
        "MAST-Pro: Dynamic Mixture-of-Experts for Adaptive Segmentation of\n  Pan-Tumors with Knowledge-Driven Prompts",
        "SimLabel: Consistency-Guided OOD Detection with Pretrained\n  Vision-Language Models",
        "Leveraging Textual Anatomical Knowledge for Class-Imbalanced\n  Semi-Supervised Multi-Organ Segmentation",
        "Project-Probe-Aggregate: Efficient Fine-Tuning for Group Robustness",
        "Parameter Efficient Merging for Multimodal Large Language Models with\n  Complementary Parameter Adaptation",
        "EchoVideo: Identity-Preserving Human Video Generation by Multimodal\n  Feature Fusion",
        "EdgeRegNet: Edge Feature-based Multimodal Registration Network between\n  Images and LiDAR Point Clouds",
        "SliceOcc: Indoor 3D Semantic Occupancy Prediction with Vertical Slice\n  Representation",
        "CFT-RAG: An Entity Tree Based Retrieval Augmented Generation Algorithm\n  With Cuckoo Filter",
        "Polyregular Model Checking",
        "Quasinormal modes of nonthermal fixed points",
        "Constrained multi-fidelity Bayesian optimization with automatic stop\n  condition",
        "A Probabilistic WxChallenge Proposal",
        "LIFT-GS: Cross-Scene Render-Supervised Distillation for 3D Language\n  Grounding",
        "AdaptiveLog: An Adaptive Log Analysis Framework with the Collaboration\n  of Large and Small Language Model",
        "MBTSAD: Mitigating Backdoors in Language Models Based on Token Splitting\n  and Attention Distillation",
        "Movable Antenna Enhanced DF and AF Relaying Systems: Performance\n  Analysis and Optimization",
        "Connection between planetary He I $\\lambda$10830 \\AA\\ absorption and\n  extreme-ultraviolet emission of planet-host stars",
        "Low-Complexity Event Detection and Identification in Coherent\n  Correlation OTDR Measurements",
        "Assortment optimization given basket shopping behavior using the Ising\n  model",
        "NavG: Risk-Aware Navigation in Crowded Environments Based on\n  Reinforcement Learning with Guidance Points",
        "Leveraging Randomness in Model and Data Partitioning for Privacy\n  Amplification",
        "Adaptive Retrieval Without Self-Knowledge? Bringing Uncertainty Back\n  Home"
      ],
      "abstract":[
        "Recent work indicates that video recognition models are vulnerable to\nadversarial examples, posing a serious security risk to downstream\napplications. However, current research has primarily focused on adversarial\nattacks, with limited work exploring defense mechanisms. Furthermore, due to\nthe spatial-temporal complexity of videos, existing video defense methods face\nissues of high cost, overfitting, and limited defense performance. Recently,\ndiffusion-based adversarial purification methods have achieved robust defense\nperformance in the image domain. However, due to the additional temporal\ndimension in videos, directly applying these diffusion-based adversarial\npurification methods to the video domain suffers performance and efficiency\ndegradation. To achieve an efficient and effective video adversarial defense\nmethod, we propose the first diffusion-based video purification framework to\nimprove video recognition models' adversarial robustness: VideoPure. Given an\nadversarial example, we first employ temporal DDIM inversion to transform the\ninput distribution into a temporally consistent and trajectory-defined\ndistribution, covering adversarial noise while preserving more video structure.\nThen, during DDIM denoising, we leverage intermediate results at each denoising\nstep and conduct guided spatial-temporal optimization, removing adversarial\nnoise while maintaining temporal consistency. Finally, we input the list of\noptimized intermediate results into the video recognition model for multi-step\nvoting to obtain the predicted class. We investigate the defense performance of\nour method against black-box, gray-box, and adaptive attacks on benchmark\ndatasets and models. Compared with other adversarial purification methods, our\nmethod overall demonstrates better defense performance against different\nattacks. Our code is available at https:\/\/github.com\/deep-kaixun\/VideoPure.",
        "Conditional motion generation has been extensively studied in computer\nvision, yet two critical challenges remain. First, while masked autoregressive\nmethods have recently outperformed diffusion-based approaches, existing masking\nmodels lack a mechanism to prioritize dynamic frames and body parts based on\ngiven conditions. Second, existing methods for different conditioning\nmodalities often fail to integrate multiple modalities effectively, limiting\ncontrol and coherence in generated motion. To address these challenges, we\npropose Motion Anything, a multimodal motion generation framework that\nintroduces an Attention-based Mask Modeling approach, enabling fine-grained\nspatial and temporal control over key frames and actions. Our model adaptively\nencodes multimodal conditions, including text and music, improving\ncontrollability. Additionally, we introduce Text-Music-Dance (TMD), a new\nmotion dataset consisting of 2,153 pairs of text, music, and dance, making it\ntwice the size of AIST++, thereby filling a critical gap in the community.\nExtensive experiments demonstrate that Motion Anything surpasses\nstate-of-the-art methods across multiple benchmarks, achieving a 15%\nimprovement in FID on HumanML3D and showing consistent performance gains on\nAIST++ and TMD. See our project website\nhttps:\/\/steve-zeyu-zhang.github.io\/MotionAnything",
        "Neural Radiance Fields (NeRFs) have emerged as a groundbreaking paradigm for\nrepresenting 3D objects and scenes by encoding shape and appearance information\ninto the weights of a neural network. Recent works have shown how such weights\ncan be used as input to frameworks processing them to solve deep learning\ntasks. Yet, these frameworks can only process NeRFs with a specific, predefined\narchitecture. In this paper, we present the first framework that can ingest\nNeRFs with multiple architectures and perform inference on architectures unseen\nat training time. We achieve this goal by training a Graph Meta-Network in a\nrepresentation learning framework. Moreover, we show how a contrastive\nobjective is conducive to obtaining an architecture-agnostic latent space. In\nexperiments on both MLP-based and tri-planar NeRFs, our approach demonstrates\nrobust performance in classification and retrieval tasks that either matches or\nexceeds that of existing frameworks constrained to single architectures, thus\nproviding the first architecture-agnostic method to perform tasks on NeRFs by\nprocessing their weights.",
        "Given a video and a linguistic query, video moment retrieval and highlight\ndetection (MR&HD) aim to locate all the relevant spans while simultaneously\npredicting saliency scores. Most existing methods utilize RGB images as input,\noverlooking the inherent multi-modal visual signals like optical flow and\ndepth. In this paper, we propose a Multi-modal Fusion and Query Refinement\nNetwork (MRNet) to learn complementary information from multi-modal cues.\nSpecifically, we design a multi-modal fusion module to dynamically combine RGB,\noptical flow, and depth map. Furthermore, to simulate human understanding of\nsentences, we introduce a query refinement module that merges text at different\ngranularities, containing word-, phrase-, and sentence-wise levels.\nComprehensive experiments on QVHighlights and Charades datasets indicate that\nMRNet outperforms current state-of-the-art methods, achieving notable\nimprovements in MR-mAP@Avg (+3.41) and HD-HIT@1 (+3.46) on QVHighlights.",
        "Deferred neural rendering (DNR) is an emerging computer graphics pipeline\ndesigned for high-fidelity rendering and robotic perception. However, DNR\nheavily relies on datasets composed of numerous ray-traced images and demands\nsubstantial computational resources. It remains under-explored how to reduce\nthe reliance on high-quality ray-traced images while maintaining the rendering\nfidelity. In this paper, we propose DNRSelect, which integrates a reinforcement\nlearning-based view selector and a 3D texture aggregator for deferred neural\nrendering. We first propose a novel view selector for deferred neural rendering\nbased on reinforcement learning, which is trained on easily obtained rasterized\nimages to identify the optimal views. By acquiring only a few ray-traced images\nfor these selected views, the selector enables DNR to achieve high-quality\nrendering. To further enhance spatial awareness and geometric consistency in\nDNR, we introduce a 3D texture aggregator that fuses pyramid features from\ndepth maps and normal maps with UV maps. Given that acquiring ray-traced images\nis more time-consuming than generating rasterized images, DNRSelect minimizes\nthe need for ray-traced data by using only a few selected views while still\nachieving high-fidelity rendering results. We conduct detailed experiments and\nablation studies on the NeRF-Synthetic dataset to demonstrate the effectiveness\nof DNRSelect. The code will be released.",
        "Accurate tumor segmentation is crucial for cancer diagnosis and treatment.\nWhile foundation models have advanced general-purpose segmentation, existing\nmethods still struggle with: (1) limited incorporation of medical priors, (2)\nimbalance between generic and tumor-specific features, and (3) high\ncomputational costs for clinical adaptation. To address these challenges, we\npropose MAST-Pro (Mixture-of-experts for Adaptive Segmentation of pan-Tumors\nwith knowledge-driven Prompts), a novel framework that integrates dynamic\nMixture-of-Experts (D-MoE) and knowledge-driven prompts for pan-tumor\nsegmentation. Specifically, text and anatomical prompts provide domain-specific\npriors, guiding tumor representation learning, while D-MoE dynamically selects\nexperts to balance generic and tumor-specific feature learning, improving\nsegmentation accuracy across diverse tumor types. To enhance efficiency, we\nemploy Parameter-Efficient Fine-Tuning (PEFT), optimizing MAST-Pro with\nsignificantly reduced computational overhead. Experiments on multi-anatomical\ntumor datasets demonstrate that MAST-Pro outperforms state-of-the-art\napproaches, achieving up to a 5.20% improvement in average DSC while reducing\ntrainable parameters by 91.04%, without compromising accuracy.",
        "Detecting out-of-distribution (OOD) data is crucial in real-world machine\nlearning applications, particularly in safety-critical domains. Existing\nmethods often leverage language information from vision-language models (VLMs)\nto enhance OOD detection by improving confidence estimation through rich\nclass-wise text information. However, when building OOD detection score upon on\nin-distribution (ID) text-image affinity, existing works either focus on each\nID class or whole ID label sets, overlooking inherent ID classes' connection.\nWe find that the semantic information across different ID classes is beneficial\nfor effective OOD detection. We thus investigate the ability of image-text\ncomprehension among different semantic-related ID labels in VLMs and propose a\nnovel post-hoc strategy called SimLabel. SimLabel enhances the separability\nbetween ID and OOD samples by establishing a more robust image-class similarity\nmetric that considers consistency over a set of similar class labels. Extensive\nexperiments demonstrate the superior performance of SimLabel on various\nzero-shot OOD detection benchmarks. The proposed model is also extended to\nvarious VLM-backbones, demonstrating its good generalization ability. Our\ndemonstration and implementation codes are available at:\nhttps:\/\/github.com\/ShuZou-1\/SimLabel.",
        "Annotating 3D medical images demands substantial time and expertise, driving\nthe adoption of semi-supervised learning (SSL) for segmentation tasks. However,\nthe complex anatomical structures of organs often lead to significant class\nimbalances, posing major challenges for deploying SSL in real-world scenarios.\nDespite the availability of valuable prior information, such as inter-organ\nrelative positions and organ shape priors, existing SSL methods have yet to\nfully leverage these insights. To address this gap, we propose a novel approach\nthat integrates textual anatomical knowledge (TAK) into the segmentation model.\nSpecifically, we use GPT-4o to generate textual descriptions of anatomical\npriors, which are then encoded using a CLIP-based model. These encoded priors\nare injected into the segmentation model as parameters of the segmentation\nhead. Additionally, contrastive learning is employed to enhance the alignment\nbetween textual priors and visual features. Extensive experiments demonstrate\nthe superior performance of our method, significantly surpassing\nstate-of-the-art approaches. The source code will be available at:\nhttps:\/\/github.com\/Lunn88\/TAK-Semi.",
        "While image-text foundation models have succeeded across diverse downstream\ntasks, they still face challenges in the presence of spurious correlations\nbetween the input and label. To address this issue, we propose a simple\nthree-step approach,Project-Probe-Aggregate (PPA), that enables\nparameter-efficient fine-tuning for foundation models without relying on group\nannotations. Building upon the failure-based debiasing scheme, our method, PPA,\nimproves its two key components: minority samples identification and the robust\ntraining algorithm. Specifically, we first train biased classifiers by\nprojecting image features onto the nullspace of class proxies from text\nencoders. Next, we infer group labels using the biased classifier and probe\ngroup targets with prior correction. Finally, we aggregate group weights of\neach class to produce the debiased classifier. Our theoretical analysis shows\nthat our PPA enhances minority group identification and is Bayes optimal for\nminimizing the balanced group error, mitigating spurious correlations.\nExtensive experimental results confirm the effectiveness of our PPA: it\noutperforms the state-of-the-art by an average worst-group accuracy while\nrequiring less than 0.01% tunable parameters without training group labels.",
        "Fine-tuning pre-trained models with custom data leads to numerous expert\nmodels on specific tasks. Merging models into one universal model to empower\nmulti-task ability refraining from data leakage has gained popularity. With the\nexpansion in data and model size, parameter efficient tuning becomes the common\npractice for obtaining task-specific models efficiently. However, we observe\nthat existing methods designed for full fine-tuning merging fail under\nefficient tuning. To address the issues, we analyze from low-rank decomposition\nand reveal that maintaining direction and compensating for gap between singular\nvalues are crucial for efficient model merging. Consequently, we propose\nCoPA-Merging, a training-free parameter efficient merging method with\ncomplementary parameter adaptation. Specifically, we (1) prune parameters and\nconstruct scaling coefficients from inter-parameter relation to compensate for\nperformance drop from task interference and (2) perform cross-task\nnormalization to enhance unseen task generalization. We establish a benchmark\nconsisting of diverse multimodal tasks, on which we conduct experiments to\ncertificate the outstanding performance and generalizability of our method.\nAdditional study and extensive analyses further showcase the effectiveness.",
        "Recent advancements in video generation have significantly impacted various\ndownstream applications, particularly in identity-preserving video generation\n(IPT2V). However, existing methods struggle with \"copy-paste\" artifacts and low\nsimilarity issues, primarily due to their reliance on low-level facial image\ninformation. This dependence can result in rigid facial appearances and\nartifacts reflecting irrelevant details. To address these challenges, we\npropose EchoVideo, which employs two key strategies: (1) an Identity Image-Text\nFusion Module (IITF) that integrates high-level semantic features from text,\ncapturing clean facial identity representations while discarding occlusions,\nposes, and lighting variations to avoid the introduction of artifacts; (2) a\ntwo-stage training strategy, incorporating a stochastic method in the second\nphase to randomly utilize shallow facial information. The objective is to\nbalance the enhancements in fidelity provided by shallow features while\nmitigating excessive reliance on them. This strategy encourages the model to\nutilize high-level features during training, ultimately fostering a more robust\nrepresentation of facial identities. EchoVideo effectively preserves facial\nidentities and maintains full-body integrity. Extensive experiments demonstrate\nthat it achieves excellent results in generating high-quality, controllability\nand fidelity videos.",
        "Cross-modal data registration has long been a critical task in computer\nvision, with extensive applications in autonomous driving and robotics.\nAccurate and robust registration methods are essential for aligning data from\ndifferent modalities, forming the foundation for multimodal sensor data fusion\nand enhancing perception systems' accuracy and reliability. The registration\ntask between 2D images captured by cameras and 3D point clouds captured by\nLight Detection and Ranging (LiDAR) sensors is usually treated as a visual pose\nestimation problem. High-dimensional feature similarities from different\nmodalities are leveraged to identify pixel-point correspondences, followed by\npose estimation techniques using least squares methods. However, existing\napproaches often resort to downsampling the original point cloud and image data\ndue to computational constraints, inevitably leading to a loss in precision.\nAdditionally, high-dimensional features extracted using different feature\nextractors from various modalities require specific techniques to mitigate\ncross-modal differences for effective matching. To address these challenges, we\npropose a method that uses edge information from the original point clouds and\nimages for cross-modal registration. We retain crucial information from the\noriginal data by extracting edge points and pixels, enhancing registration\naccuracy while maintaining computational efficiency. The use of edge points and\nedge pixels allows us to introduce an attention-based feature exchange block to\neliminate cross-modal disparities. Furthermore, we incorporate an optimal\nmatching layer to improve correspondence identification. We validate the\naccuracy of our method on the KITTI and nuScenes datasets, demonstrating its\nstate-of-the-art performance.",
        "3D semantic occupancy prediction is a crucial task in visual perception, as\nit requires the simultaneous comprehension of both scene geometry and\nsemantics. It plays a crucial role in understanding 3D scenes and has great\npotential for various applications, such as robotic vision perception and\nautonomous driving. Many existing works utilize planar-based representations\nsuch as Bird's Eye View (BEV) and Tri-Perspective View (TPV). These\nrepresentations aim to simplify the complexity of 3D scenes while preserving\nessential object information, thereby facilitating efficient scene\nrepresentation. However, in dense indoor environments with prevalent\nocclusions, directly applying these planar-based methods often leads to\ndifficulties in capturing global semantic occupancy, ultimately degrading model\nperformance. In this paper, we present a new vertical slice representation that\ndivides the scene along the vertical axis and projects spatial point features\nonto the nearest pair of parallel planes. To utilize these slice features, we\npropose SliceOcc, an RGB camera-based model specifically tailored for indoor 3D\nsemantic occupancy prediction. SliceOcc utilizes pairs of slice queries and\ncross-attention mechanisms to extract planar features from input images. These\nlocal planar features are then fused to form a global scene representation,\nwhich is employed for indoor occupancy prediction. Experimental results on the\nEmbodiedScan dataset demonstrate that SliceOcc achieves a mIoU of 15.45% across\n81 indoor categories, setting a new state-of-the-art performance among RGB\ncamera-based models for indoor 3D semantic occupancy prediction. Code is\navailable at https:\/\/github.com\/NorthSummer\/SliceOcc.",
        "Although retrieval-augmented generation(RAG) significantly improves\ngeneration quality by retrieving external knowledge bases and integrating\ngenerated content, it faces computational efficiency bottlenecks, particularly\nin knowledge retrieval tasks involving hierarchical structures for Tree-RAG.\nThis paper proposes a Tree-RAG acceleration method based on the improved Cuckoo\nFilter, which optimizes entity localization during the retrieval process to\nachieve significant performance improvements. Tree-RAG effectively organizes\nentities through the introduction of a hierarchical tree structure, while the\nCuckoo Filter serves as an efficient data structure that supports rapid\nmembership queries and dynamic updates. The experiment results demonstrate that\nour method is much faster than naive Tree-RAG while maintaining high levels of\ngenerative quality. When the number of trees is large, our method is hundreds\nof times faster than naive Tree-RAG. Our work is available at\nhttps:\/\/github.com\/TUPYP7180\/CFT-RAG-2025.",
        "We reduce the model checking problem for a subset of Python to the\nsatisfiability of a first-order formula over finite words, which is known to be\ndecidable. The reduction is based on the theory of polyregular functions, a\nrecently developed generalization of regular languages to polynomial output\nstring-to-string functions. We implemented this reduction in a verification\ntool called PolyCheck, that can use both automata-based solvers and classical\nSMT solvers as backends.",
        "Quasinormal modes play a prominent role in relaxation of diverse physical\nsystems to equilibria, ranging from astrophysical black holes to tiny droplets\nof quark-gluon plasma at RHIC and LHC accelerators. We propose that a novel\nkind of quasinormal modes govern the direct approach to self-similar time\nevolution of nonthermal fixed points, whose relevance ranges from high energy\nphysics to cold atom gases. We utilize black hole perturbation theory\ntechniques to compute the spectrum of these far from equilibrium quasinormal\nmodes for a kinetic theory with a Focker-Planck collision kernel in isotropic\nand homogeneous states. Our conclusion is that quasinormal modes of nonthermal\nfixed points give rise to a tower of progressively more decaying power-law\ncontributions. A byproduct of our analysis is a precise determination and\nimproved understanding of the distribution function characterizing nonthermal\nfixed points.",
        "Bayesian optimization (BO) is increasingly employed in critical applications\nto find the optimal design with minimal cost. While BO is known for its sample\nefficiency, relying solely on costly high-fidelity data can still result in\nhigh costs. This is especially the case in constrained search spaces where BO\nmust not only optimize but also ensure feasibility. A related issue in the BO\nliterature is the lack of a systematic stopping criterion. To solve these\nchallenges, we develop a constrained cost-aware multi-fidelity BO (CMFBO)\nframework whose goal is to minimize overall sampling costs by utilizing\ninexpensive low-fidelity sources while ensuring feasibility. In our case, the\nconstraints can change across the data sources and may be even black-box\nfunctions. We also introduce a systematic stopping criterion that addresses the\nlong-lasting issue associated with BO's convergence assessment. Our framework\nis publicly available on GitHub through the GP+ Python package and herein we\nvalidate it's efficacy on multiple benchmark problems.",
        "The national forecasting competition WxChallenge, brainchild of Brad Illston\nat the University of Oklahoma in 2005, has become a cherished institution\nplayed across the United States each year. Participants include students,\nfaculty, alumni, and industry professionals. However, forecasts are given as\nscalar values without expression of uncertainty, probabilities being a keystone\nof meteorological forecasting today, and previous attempts to add probabilistic\nelements to WxChallenge have failed partly due to challenges in making\nprobability forecasting accessible to all, and inability to combine scores with\ndifferent units while also appropriately rewarding forecasts using proper\nscoring rules. Much of the competition's maintenance relies on dedicated\nvolunteers, highlighting need for more automation. Hence I propose three new\nfeatures: (1) automated forecast problems based on morning ensemble guidance,\nforming prediction baselines, thresholds over which the players demonstrate\nskill in their later forecast; (2) a spread betting game, where the players\nallocate 100 confidence credits to the over-under for exceeding a percentile\n(e.g., 50pc) threshold of a variable (e.g., maximum temperature) derived from\nthe ensemble baseline; and (3) a game where players distribute 100 confidence\ncredits across bins of a continuous variable (e.g., accumulated precipitation)\napproximating a probability mass function. Forecasts are evaluated using\nShannon information gained over the baseline forecast, yielding additive units\nof bits that allow score combinations of different variables and units.\nInformation gain parallels the Brier Score and is likewise a sound measure of\nskill due its punishment of hedging. This proposal objective is to augment\nWxChallenge with two new probabilistic games that are accessible,\nscientifically sound, enjoyable, and optional.",
        "Our approach to training 3D vision-language understanding models is to train\na feedforward model that makes predictions in 3D, but never requires 3D labels\nand is supervised only in 2D, using 2D losses and differentiable rendering. The\napproach is new for vision-language understanding. By treating the\nreconstruction as a ``latent variable'', we can render the outputs without\nplacing unnecessary constraints on the network architecture (e.g. can be used\nwith decoder-only models). For training, only need images and camera pose, and\n2D labels. We show that we can even remove the need for 2D labels by using\npseudo-labels from pretrained 2D models. We demonstrate this to pretrain a\nnetwork, and we finetune it for 3D vision-language understanding tasks. We show\nthis approach outperforms baselines\/sota for 3D vision-language grounding, and\nalso outperforms other 3D pretraining techniques. Project page:\nhttps:\/\/liftgs.github.io.",
        "Automated log analysis is crucial to ensure high availability and reliability\nof complex systems. The advent of LLMs in NLP has ushered in a new era of\nlanguage model-driven automated log analysis, garnering significant interest.\nWithin this field, two primary paradigms based on language models for log\nanalysis have become prominent. Small Language Models (SLMs) follow the\npre-train and fine-tune paradigm, focusing on the specific log analysis task\nthrough fine-tuning on supervised datasets. On the other hand, LLMs following\nthe in-context learning paradigm, analyze logs by providing a few examples in\nprompt contexts without updating parameters. Despite their respective\nstrengths, we notice that SLMs are more cost-effective but less powerful,\nwhereas LLMs with large parameters are highly powerful but expensive and\ninefficient. To trade-off between the performance and inference costs of both\nmodels in automated log analysis, this paper introduces an adaptive log\nanalysis framework known as AdaptiveLog, which effectively reduces the costs\nassociated with LLM while ensuring superior results. This framework\ncollaborates an LLM and a small language model, strategically allocating the\nLLM to tackle complex logs while delegating simpler logs to the SLM.\nSpecifically, to efficiently query the LLM, we propose an adaptive selection\nstrategy based on the uncertainty estimation of the SLM, where the LLM is\ninvoked only when the SLM is uncertain. In addition, to enhance the reasoning\nability of the LLM in log analysis tasks, we propose a novel prompt strategy by\nretrieving similar error-prone cases as the reference, enabling the model to\nleverage past error experiences and learn solutions from these cases. Extensive\nexperiments demonstrate that AdaptiveLog achieves state-of-the-art results\nacross different tasks, elevating the overall accuracy of log analysis while\nmaintaining cost efficiency.",
        "In recent years, attention-based models have excelled across various domains\nbut remain vulnerable to backdoor attacks, often from downloading or\nfine-tuning on poisoned datasets. Many current methods to mitigate backdoors in\nNLP models rely on the pre-trained (unfine-tuned) weights, but these methods\nfail in scenarios where the pre-trained weights are not available. In this\nwork, we propose MBTSAD, which can mitigate backdoors in the language model by\nutilizing only a small subset of clean data and does not require pre-trained\nweights. Specifically, MBTSAD retrains the backdoored model on a dataset\ngenerated by token splitting. Then MBTSAD leverages attention distillation, the\nretrained model is the teacher model, and the original backdoored model is the\nstudent model. Experimental results demonstrate that MBTSAD achieves comparable\nbackdoor mitigation performance as the methods based on pre-trained weights\nwhile maintaining the performance on clean data. MBTSAD does not rely on\npre-trained weights, enhancing its utility in scenarios where pre-trained\nweights are inaccessible. In addition, we simplify the min-max problem of\nadversarial training and visualize text representations to discover that the\ntoken splitting method in MBTSAD's first step generates Out-of-Distribution\n(OOD) data, leading the model to learn more generalized features and eliminate\nbackdoor patterns.",
        "Movable antenna (MA) has been deemed as a promising technology to flexibly\nreconfigure wireless channels by adjusting the antenna positions in a given\nlocal region. In this paper, we investigate the application of the MA\ntechnology in both decode-and-forward (DF) and amplify-and-forward (AF)\nrelaying systems, where a relay is equipped with multiple MAs to assist in the\ndata transmission between two single-antenna nodes. For the DF relaying system,\nour objective is to maximize the achievable rate at the destination by jointly\noptimizing the positions of the MAs in two stages for receiving signals from\nthe source and transmitting signals to the destination, respectively. To drive\nessential insights, we first derive a closed-form upper bound on the maximum\nachievable rate of the DF relaying system. Then, a low-complexity algorithm\nbased on projected gradient ascent (PGA) and alternating optimization (AO) is\nproposed to solve the antenna position optimization problem. For the AF\nrelaying system, our objective is to maximize the achievable rate by jointly\noptimizing the two-stage MA positions as well as the AF beamforming matrix at\nthe relay, which results in a more challenging optimization problem due to the\nintricate coupling variables. To tackle this challenge, we first reveal the\nhidden separability among the antenna position optimization in the two stages\nand the beamforming optimization. Based on such separability, we derive a\nclosed-form upper bound on the maximum achievable rate of the AF relaying\nsystem and propose a low-complexity algorithm to obtain a high-quality\nsuboptimal solution to the considered problem. Simulation results validate the\nefficacy of our theoretical analysis and demonstrate the superiority of the\nMA-enhanced relaying systems to the conventional relaying systems with\nfixed-position antennas (FPAs) and other benchmark schemes.",
        "Context. The detection of the He I 10830 A triplet in exoplanet atmospheres\nhas opened a new window for probing planetary properties, including atmospheric\nescape. Unlike Lyman alpha, the triplet is less affected by ISM absorption.\nSufficient XUV stellar irradiation may trigger the formation of the He I\ntriplet via photoionization and posterior recombination processes in the planet\natmospheres. Only a weak trend between stellar XUV and the planetary He I\nstrength has been observed so far. Aims. We aim to confirm this mechanism for\nproducing the He I absorption in exoplanetary atmospheres by examining a sample\nof planetary systems. Methods. We obtained homogeneous measurements of the\nplanetary He I line EW and consistently computed the stellar XUV ionizing\nirradiation. We first derived new coronal models for the planet-host stars. We\nused updated data from the X-exoplanets database, archival X-ray spectra of\nM-type stars (including AU Mic and Proxima Cen), and new XMM-Newton X-ray data\nobtained for the CARMENES project. These data were complemented at longer\nwavelengths with publicly available HST, FUSE, and EUVE spectra. A total of 75\nstars are carefully analyzed to obtain a new calibration between X-ray and EUV\nemission. Results. Two distinct relationships between stellar X-ray emission\n(5-100 A) and EUV_H (100-920 A) or EUV_He (100-504 A) radiation are obtained to\nscale the emission from late-type stellar coronae. A total of 48 systems with\nreported planetary He I 10830 A studies, exhibit a robust relationship between\nthe planetary He I feature and the ionizing XUV_He received by the planet,\ncorrected by stellar and planetary radii, and the planet's gravitational\npotential. Some outliers could be explained by a different atmospheric\ncomposition or the lack of planetary gaseous atmospheres. This relation may be\nused to predict the He I 10830 A absorption in exoplanet atmospheres.",
        "Pairing coherent correlation OTDR with low-complexity analysis methods, we\ninvestigate the detection of fast temperature changes and vibrations in optical\nfibers. A localization accuracy of ~2 m and extraction of vibration amplitudes\nand frequencies is demonstrated.",
        "In markets where customers tend to purchase baskets of products rather than\nsingle products, assortment optimization is a major challenge for retailers.\nRemoving a product from a retailer's assortment can result in a severe drop in\naggregate demand if this product is a complement to other products. Therefore,\naccounting for the complementarity effect is essential when making assortment\ndecisions. In this paper, we develop a modeling framework designed to address\nthis problem. We model customers' choices using a Markov random field -- in\nparticular, the Ising model -- which captures pairwise demand dependencies as\nwell as the individual attractiveness of each product. Using the Ising model\nallows us to leverage existing methodologies for various purposes including\nparameter estimation and efficient simulation of customer choices. We formulate\nthe assortment optimization problem under this model and show that its decision\nversion is NP-hard. We also provide multiple theoretical insights into the\nstructure of the optimal assortments based on the graphical representation of\nthe Ising model, and propose several heuristic algorithms that can be used to\nobtain high-quality solutions to the assortment optimization problem. Our\nnumerical analysis demonstrates that the developed simulated annealing\nprocedure leads to an expected profit gain of 15% compared to offering an\nunoptimized assortment (where all products are included) and around 5% compared\nto using a revenue-ordered heuristic algorithm.",
        "Motion planning in navigation systems is highly susceptible to upstream\nperceptual errors, particularly in human detection and tracking. To mitigate\nthis issue, the concept of guidance points--a novel directional cue within a\nreinforcement learning-based framework--is introduced. A structured method for\nidentifying guidance points is developed, consisting of obstacle boundary\nextraction, potential guidance point detection, and redundancy elimination. To\nintegrate guidance points into the navigation pipeline, a\nperception-to-planning mapping strategy is proposed, unifying guidance points\nwith other perceptual inputs and enabling the RL agent to effectively leverage\nthe complementary relationships among raw laser data, human detection and\ntracking, and guidance points. Qualitative and quantitative simulations\ndemonstrate that the proposed approach achieves the highest success rate and\nnear-optimal travel times, greatly improving both safety and efficiency.\nFurthermore, real-world experiments in dynamic corridors and lobbies validate\nthe robot's ability to confidently navigate around obstacles and robustly avoid\npedestrians.",
        "We study how inherent randomness in the training process -- where each sample\n(or client in federated learning) contributes only to a randomly selected\nportion of training -- can be leveraged for privacy amplification. This\nincludes (1) data partitioning, where a sample participates in only a subset of\ntraining iterations, and (2) model partitioning, where a sample updates only a\nsubset of the model parameters. We apply our framework to model parallelism in\nfederated learning, where each client updates a randomly selected subnetwork to\nreduce memory and computational overhead, and show that existing methods, e.g.\nmodel splitting or dropout, provide a significant privacy amplification gain\nnot captured by previous privacy analysis techniques. Additionally, we\nintroduce Balanced Iteration Subsampling, a new data partitioning method where\neach sample (or client) participates in a fixed number of training iterations.\nWe show that this method yields stronger privacy amplification than Poisson\n(i.i.d.) sampling of data (or clients). Our results demonstrate that randomness\nin the training process, which is structured rather than i.i.d. and interacts\nwith data in complex ways, can be systematically leveraged for significant\nprivacy amplification.",
        "Retrieval Augmented Generation (RAG) improves correctness of Question\nAnswering (QA) and addresses hallucinations in Large Language Models (LLMs),\nyet greatly increase computational costs. Besides, RAG is not always needed as\nmay introduce irrelevant information. Recent adaptive retrieval methods\nintegrate LLMs' intrinsic knowledge with external information appealing to LLM\nself-knowledge, but they often neglect efficiency evaluations and comparisons\nwith uncertainty estimation techniques. We bridge this gap by conducting a\ncomprehensive analysis of 35 adaptive retrieval methods, including 8 recent\napproaches and 27 uncertainty estimation techniques, across 6 datasets using 10\nmetrics for QA performance, self-knowledge, and efficiency. Our findings show\nthat uncertainty estimation techniques often outperform complex pipelines in\nterms of efficiency and self-knowledge, while maintaining comparable QA\nperformance."
      ]
    }
  },
  {
    "id":2412.16425,
    "research_type":"applied",
    "start_id":"b15",
    "start_title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
    "start_abstract":"While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b40"
      ],
      "title":[
        "Whole Slide Imaging Versus Microscopy for Primary Diagnosis in Surgical Pathology: A Multicenter Blinded Randomized Noninferiority Study of 1992 Cases (Pivotal Study)"
      ],
      "abstract":[
        "Most prior studies of primary diagnosis in surgical pathology using whole slide imaging (WSI) versus microscopy have focused on specific organ systems or included relatively few cases. The objective this study was to demonstrate that WSI is noninferior for pathology. A blinded randomized noninferiority conducted across the entire range cases (biopsies and resections, including hematoxylin eosin, immunohistochemistry, special stains) from 4 institutions original sign-out (baseline diagnosis) as reference standard. Cases were scanned, converted randomized. Sixteen pathologists interpreted by WSI, followed a wash-out period \u22654 weeks, after which read same observers other modality. Major discordances identified an adjudication panel, differences between major discordance rates both (against standard) calculated. total 1992 included, resulting 15,925 reads. rate with standard 4.9% 4.6% microscopy. difference 0.4% (95% confidence interval, -0.30% 1.01%). highest endocrine (1.8%), neoplastic kidney (1.5%), urinary bladder (1.3%), gynecologic (1.2%). Detailed analysis these revealed no instances where interpretation consistently inaccurate compared multiple observers. We conclude pathology, biopsies resections stained immunohistochemistry stains. This conclusion valid wide variety specimen types."
      ],
      "categories":[
        "q-bio.CB"
      ]
    },
    "list":{
      "title":[
        "Intercellular contact is sufficient to drive Fibroblast to Myofibroblast\n  transitions",
        "Nonsuppressible viremia during HIV-1 therapy meets molecular virology",
        "Tumor-associated CD19$^+$ macrophages induce immunosuppressive\n  microenvironment in hepatocellular carcinoma",
        "Multilayer Networks in Neuroimaging",
        "Trait-structured chemotaxis: Exploring ligand-receptor dynamics and\n  travelling wave properties in a Keller-Segel model",
        "Origin of $\\alpha$-satellite repeat arrays from mitochondrial molecular\n  fossils -- sequential insertion, expansion, and evolution in the nuclear\n  genome",
        "Integrating protein sequence embeddings with structure via graph-based\n  deep learning for the prediction of single-residue properties",
        "Multicellular self-organization in Escherichia coli",
        "A spatially resolved and lipid-structured model for macrophage\n  populations in early human atherosclerotic lesions",
        "SPM 25: open source neuroimaging analysis software",
        "Optimal Inference of Asynchronous Boolean Network Models",
        "Tumor-associated CD19$^+$ macrophages induce immunosuppressive\n  microenvironment in hepatocellular carcinoma",
        "Homeostatic Kinematic Growth Model for Arteries -- Residual Stresses and\n  Active Response",
        "Modular Forms and Certain ${}_2F_1(1)$ Hypergeometric Series",
        "Signal amplification in a solid-state quantum sensor via asymmetric\n  time-reversal of many-body dynamics",
        "Flavor dependence of Energy-energy correlators",
        "Integral Ricci Curvature for Graphs",
        "Optimal Low degree hardness for Broadcasting on Trees",
        "Geodesics for Discrete manifolds",
        "Controlling spin currents with magnon interference in a canted\n  antiferromagnet",
        "Unique continuation for locally uniformly distributed measures",
        "Isospin sum rules for bottom-baryon weak decays",
        "Sampling Binary Data by Denoising through Score Functions",
        "Consonance in music -- the Pythagorean approach revisited",
        "Optimal lower Lipschitz bounds for ReLU layers, saturation, and phase\n  retrieval",
        "The Andreadakis Problem for the McCool groups",
        "Intrinsic superconducting diode effect and nonreciprocal\n  superconductivity in rhombohedral graphene multilayers",
        "On cyclotomic nearly-doubly-regular tournaments"
      ],
      "abstract":[
        "Fibroblast cells play a key role in maintaining the extracellular matrix.\nDuring wound healing, fibroblasts differentiate into highly contractile\nmyofibroblasts, which secrete extracellular matrix proteins like collagen to\nfacilitate tissue repair. Under normal conditions, myofibroblasts undergo\nprogrammed cell death after healing to prevent excessive scar formation.\nHowever, in diseases like fibrosis, the myofibroblasts remain active even after\nthe wound is closed, resulting in excessive collagen buildup and a stiff,\nfibrotic matrix. The reasons for the persistence of myofibroblasts in fibrosis\nare not well understood. Here we show the existence of a mechanism where direct\nphysical contact between a fibroblast and a myofibroblast is sufficient for\nfibroblasts to transition into myofibroblasts. We show that\nfibroblast-myofibroblast transition can occur even in the absence of known\nbiochemical cues such as growth factor activation or mechanical cues from a\nstiff, fibrotic matrix. Further, we show that contact-based\nfibroblast-myofibroblast activation can be blocked by G{\\alpha}q\/11\/14\ninhibitor FR9003591, which inhibits the formation of myofibroblasts. These\nfindings offer new insights into the persistence of fibrosis despite\ntherapeutic interventions and suggest a potential strategy to target\nfibroblast-to-myofibroblast transition in fibrosis.",
        "HIV-1 replication can be suppressed with antiretroviral therapy (ART), but\nindividuals who stop taking ART soon become viremic again. Some people\nexperience extended times of detectable viremia despite optimal adherence to\nART. In the issue of the JCI, White, Wu, and coauthors elucidate a source of\nnonsuppressible viremia (NSV) in treatment-adherent patients clonally expanded\nT cells harboring HIV-1 proviruses with small deletions or mutations in the\n5'-leader, the UTR that includes the major splice donor site of viral RNA.\nThese mutations altered viral RNA-splicing efficiency and RNA dimerization and\npackaging, yet still allowed production of detectable levels of noninfectious\nvirus particles. These particles lacked the HIV-1 Env surface protein required\nfor cell entry and failed to form the mature capsid cone required for\ninfectivity. These studies improve our understanding of NSV and the regulation\nof viral functions in the 5'-leader with implications for rationalized care in\nindividuals with NSV.",
        "Tumor-associated macrophages are a key component that contributes to the\nimmunosuppressive microenvironment in human cancers. However, therapeutic\ntargeting of macrophages has been a challenge in clinic due to the limited\nunderstanding of their heterogeneous subpopulations and distinct functions.\nHere, we identify a unique and clinically relevant CD19$^+$ subpopulation of\nmacrophages that is enriched in many types of cancer, particularly in\nhepatocellular carcinoma (HCC). The CD19$^+$ macrophages exhibit increased\nlevels of PD-L1 and CD73, enhanced mitochondrial oxidation, and compromised\nphagocytosis, indicating their immunosuppressive functions. Targeting CD19$^+$\nmacrophages with anti-CD19 chimeric antigen receptor T (CAR-T) cells inhibited\nHCC tumor growth. We identify PAX5 as a primary driver of up-regulated\nmitochondrial biogenesis in CD19$^+$ macrophages, which depletes cytoplasmic\nCa$^{2+}$, leading to lysosomal deficiency and consequent accumulation of CD73\nand PD-L1. Inhibiting CD73 or mitochondrial oxidation enhanced the efficacy of\nimmune checkpoint blockade therapy in treating HCC, suggesting great promise\nfor CD19$^+$ macrophage-targeting therapeutics.",
        "Recent advances in network science, applied to \\textit{in vivo} brain\nrecordings, have paved the way for better understanding of the structure and\nfunction of the brain. However, despite its obvious usefulness in neuroscience,\ntraditional network science lacks tools for -- so important -- simultaneous\ninvestigation of the inter-relationship between the two domains. In this\nchapter, I explore the increasing role of multilayer networks in building brain\ngenerative models and abilities of such models to uncover the full information\nabout the brain complex spatiotemporal interactions that span across multiple\nscales and modalities. First, I begin with the theoretical foundation of brain\nnetworks accompanied by a brief overview of traditional networks and their role\nin constructing multilayer network models. Then, I delve into the applications\nof multilayer networks in neuroscience, particularly in deciphering\nstructure-function relationship, modelling diseases, and integrating\nmulti-scale and multi-modal data. Finally, I demonstrate how incorporating the\nmultilayer framework into network neuroscience has brought to light previously\nhidden features of brain networks and, how multilayer networks can provide new\ninsights and a description of the structure and function of the brain.",
        "A novel trait-structured Keller-Segel model that explores the dynamics of a\nmigrating cell population guided by chemotaxis in response to an external\nligand concentration is derived and analysed. Unlike traditional Keller-Segel\nmodels, this framework introduces an explicit representation of ligand-receptor\nbindings on the cell membrane, where the percentage of occupied receptors\nconstitutes the trait that influences cellular phenotype. The model posits that\nthe cell's phenotypic state directly modulates its capacity for chemotaxis and\nproliferation, governed by a trade-off due to a finite energy budget: cells\nhighly proficient in chemotaxis exhibit lower proliferation rates, while more\nproliferative cells show diminished chemotactic abilities. The model is derived\nfrom the principles of a biased random walk, resulting in a system of two\nnon-local partial differential equations, describing the densities of both\ncells and ligands. Using a Hopf-Cole transformation, we derive an equation that\ncharacterises the distribution of cellular traits within travelling wave\nsolutions for the total cell density, allowing us to uncover the monotonicity\nproperties of these waves. Numerical investigations are conducted to examine\nthe model's behaviour across various biological scenarios, providing insights\ninto the complex interplay between chemotaxis, proliferation, and phenotypic\ndiversity in migrating cell populations.",
        "Alpha satellite DNA is large tandem arrays of 150-400 bp units, and its\norigin remains an evolutionary mystery. In this research, we identified 1,545\nalpha-satellite-like (SatL) repeat units in the nuclear genome of jewel wasp\nNasonia vitripennis. Among them, thirty-nine copies of SatL were organized in\ntwo palindromic arrays in mitochondria, resulting in a 50% increase in the\ngenome size. Strikingly, genomic neighborhood analyses of 1,516 nuclear SatL\nrepeats revealed that they are located in NuMT (nuclear mitochondrial DNA)\nregions, and SatL phylogeny matched perfectly with mitochondrial genes and NuMT\npseudogenes. These results support that SatL arrays originated from ten\nindependent mitochondria insertion events into the nuclear genome within the\nlast 500,000 years, after divergence from its sister species N. giraulti.\nDramatic repeat GC-percent elevation (from 33.9% to 50.4%) is a hallmark of\nrapid SatL sequence evolution in mitochondria due to GC-biased gene conversion\nfacilitated by the palindromic sequence pairing of the two mitochondrial SatL\narrays. The nuclear SatL repeat arrays underwent substantial copy number\nexpansion, from 12-15 (SatL1) to over 400 copies (SatL4). The oldest SatL4B\narray consists of four types of repeat units derived from deletions in the\nAT-rich region of ancestral repeats, and complex high-order structures have\nevolved through duplications. We also discovered similar repeat insertions into\nthe nuclear genome of Muscidifurax, suggesting this mechanism can be common in\ninsects. This is the first report of the mitochondrial origin of nuclear\nsatellite sequences, and our findings shed new light on the origin and\nevolution of satellite DNA.",
        "Understanding the intertwined contributions of amino acid sequence and\nspatial structure is essential to explain protein behaviour. Here, we introduce\nINFUSSE (Integrated Network Framework Unifying Structure and Sequence\nEmbeddings), a Deep Learning framework that combines sequence embeddings,\ngenerated by a Large Language Model (LLM), with graph-based representations of\nprotein structures, integrated through a diffusive Graph Convolutional Network\n(diff-GCN), to predict single-residue properties within proteins. Our approach\nfollows two steps. First, we fine-tune LLM sequence embeddings obtained from\nbidirectional transformers to make predictions from protein sequence alone.\nSecond, we combine these enriched sequence representations with a geometric\ngraph Laplacian within diff-GCN to refine the initial predictions. This\napproach leads to improved predictions while allowing us to systematically\ndisentangle the contribution of sequence and structure. We illustrate our\nframework by applying it to the prediction of local residue flexibility\n(B-factors) of antibody-antigen complexes, and show that it provides improved\nperformance compared to current Machine Learning (ML) approaches. The addition\nof structural information via geometric graphs is shown to enhance predictions\nespecially for intrinsically disordered regions, protein-protein interaction\nsites, and highly variable amino acid positions.",
        "Escherichia coli has long been a trusty companion, maintaining health in our\nguts and advancing biological knowledge in the laboratory. In light of recent\nfindings, we discuss multicellular self-organization in E. coli and develop\ngeneral ideas for multicellularity, including the necessity for multicellular\ndynamics and interpretation by dynamic graphs, applicable to both unicellular\nand multicellular organisms. In this context, we next discuss the documented\nbehaviors of E. coli self-organization (rosette formation, multicellular\nextension, and attached dormancy) and two potential behaviors (internal\ncommunication and mating). Finally, by comparing the dynamic graphs for\ndifferent communities, we develop principles relevant to the theory of\nmulticellularity.",
        "Atherosclerosis is a chronic inflammatory disease of the artery wall. The\nearly stages of atherosclerosis are driven by interactions between lipids and\nmonocyte-derived-macrophages (MDMs). The mechanisms that govern the spatial\ndistribution of lipids and MDMs in the lesion remain poorly understood. In this\npaper, we develop a spatially-resolved and lipid-structured model for early\natherosclerosis. The model development and analysis are guided by images of\nhuman coronary lesions by Nakashima et al. 2007. Consistent with their\nfindings, the model predicts that lipid initially accumulates deep in the\nintima due to a spatially non-uniform LDL retention capacity. The model also\nqualitatively reproduces the global internal maxima in the Nakashima images\nonly when the MDM mobility is sufficiently sensitive to lipid content, and MDM\nlifespan sufficiently insensitive. Introducing lipid content-dependence to MDM\nmobility and mean lifespan produced minimal impact on model behaviour at early\ntimes, but strongly impacted lesion composition at steady state. Increases to\nthe sensitivity of MDM lifespan to lipid content yield lesions with fewer MDMs,\nless total lesion lipid content and reduced mean MDM infiltration depth.\nIncreases to the sensitivity of MDM mobility to lipid content also reduces the\nMDM infiltration depth, but increases the proportion of lipid-laden MDMs. We\nfind that MDM lipid content increases with spatial depth, regardless of blood\nLDL and HDL content. These results shed light on the mechanisms that drive\nspatial variation in the composition of early atherosclerotic lesions, and the\nrole of macrophage lipid content in disease progression.",
        "Statistical Parametric Mapping (SPM) is an integrated set of methods for\ntesting hypotheses about the brain's structure and function, using data from\nimaging devices. These methods are implemented in an open source software\npackage, SPM, which has been in continuous development for more than 30 years\nby an international community of developers. This paper reports the release of\nSPM 25.01, a major new version of the software that incorporates novel analysis\nmethods, optimisations of existing methods, as well as improved practices for\nopen science and software development.",
        "Associations between phenotype and genomic and epigenomic markers are often\nderived by correlation. Systems Biology aims to make more robust connections\nand uncover broader insights by modeling the cellular mechanisms that produce a\nphenotype. The question of choosing the modeling methodology is of central\nimportance. A model that does not capture biological reality closely enough\nwill not explain the system's behavior. At the same time, highly detailed\nmodels suffer from computational limitations and are likely to overfit the\ndata. Boolean networks strike a balance between complexity and descriptiveness\nand thus have received increasing interest. We previously described an\nalgorithm for fitting Boolean networks to high-throughout experimental data\nthat finds the optimal network with respect to the information in a given\ndataset. In this work, we describe a simple extension that enables the modeling\nof asynchronous dynamics, i.e. different reaction times for different network\nnodes. In addition, we present a new method for pseudo-time assignment for\nsingle-cell RNA sequencing data that is derived from the modeling procedure.\nOur approach greatly simplifies the construction of Boolean network models for\ntime-series datasets, where asynchronicity often occurs. We demonstrate our\nmethodology by integrating real data from transcriptomics experiments. These\nresults significantly expand the applicability of the Boolean network model to\nexperimental data.",
        "Tumor-associated macrophages are a key component that contributes to the\nimmunosuppressive microenvironment in human cancers. However, therapeutic\ntargeting of macrophages has been a challenge in clinic due to the limited\nunderstanding of their heterogeneous subpopulations and distinct functions.\nHere, we identify a unique and clinically relevant CD19$^+$ subpopulation of\nmacrophages that is enriched in many types of cancer, particularly in\nhepatocellular carcinoma (HCC). The CD19$^+$ macrophages exhibit increased\nlevels of PD-L1 and CD73, enhanced mitochondrial oxidation, and compromised\nphagocytosis, indicating their immunosuppressive functions. Targeting CD19$^+$\nmacrophages with anti-CD19 chimeric antigen receptor T (CAR-T) cells inhibited\nHCC tumor growth. We identify PAX5 as a primary driver of up-regulated\nmitochondrial biogenesis in CD19$^+$ macrophages, which depletes cytoplasmic\nCa$^{2+}$, leading to lysosomal deficiency and consequent accumulation of CD73\nand PD-L1. Inhibiting CD73 or mitochondrial oxidation enhanced the efficacy of\nimmune checkpoint blockade therapy in treating HCC, suggesting great promise\nfor CD19$^+$ macrophage-targeting therapeutics.",
        "A simple kinematic growth model for muscular arteries is presented which\nallows the incorporation of residual stresses such that a homeostatic in-vivo\nstress state under physiological loading is obtained. To this end, new\nevolution equations for growth are proposed, which avoid issues with\ninstability of final growth states known from other kinematric growth models.\nThese evolution equations are connected to a new set of growth driving forces.\nBy introducing a formulation using the principle Cauchy stresses, reasonable in\nvivo stress distributions can be obtained while ensuring realistic geometries\nof arteries after growth. To incorporate realistic Cauchy stresses for muscular\narteries under varying loading conditions, which appear in vivo, e.g., due to\nphysical activity, the growth model is combined with a newly proposed\nstretch-dependent model for smooth muscle activation. To account for the\nchanges of geometry and mechanical behavior during the growth process, an\noptimization procedure is described which leads to a more accurate\nrepresentation of an arterial ring and its mechanical behavior after the\ngrowth-related homogenization of the stresses is reached. Based thereon,\nparameters are adjusted to experimental data of a healthy middle cerebral\nartery of a rat showing that the proposed model accurately describes real\nmaterial behavior. The successful combination of growth and active response\nindicates that the new growth model can be used without problems for modeling\nactive tissues under various conditions.",
        "Using the framework relating hypergeometric motives to modular forms, we\ndefine an explicit family of weight 2 Hecke eigenforms with complex\nmultiplication. We use the theory of ${}_2F_1(1)$ hypergeometric series and\nRamanujan's theory of alternative bases to compute the exact central $L$-value\nof these Hecke eigenforms in terms of special beta values. We also show the\nintegral Fourier coefficients can be written in terms of Jacobi sums,\nreflecting a motivic relation between the hypergeometric series and the modular\nforms.",
        "Electronic spins of nitrogen vacancy (NV) centers in diamond constitute a\npromising system for micro- and nano-scale magnetic sensing, due to their\noperation under ambient conditions, ease of placement in close proximity to\nsensing targets, and biological compatibility. At high densities, the\nelectronic spins interact through dipolar coupling, which typically limits but\ncan also potentially enhance sensing performance. Here we report the\nexperimental demonstration of many-body signal amplification in a solid-state,\nroom temperature quantum sensor. Our approach utilizes time-reversed\ntwo-axis-twisting interactions, engineered through dynamical control of the\nquantization axis and Floquet engineering in a two-dimensional ensemble of NV\ncenters. Strikingly, we observe that the optimal amplification occurs when the\nbackward evolution time equals twice the forward evolution time, in sharp\ncontrast to the conventional Loschmidt echo. These observations can be\nunderstood as resulting from an underlying time-reversed mirror symmetry of the\nmicroscopic dynamics, providing key insights into signal amplification and\nopening the door towards entanglement-enhanced practical quantum sensing.",
        "Energy-energy correlators (EECs) within high energy jets serve as a key\nexperimentally accessible quantity to probe the scale and structure of the\nquark-gluon plasma (QGP) in relativistic heavy-ion collisions. The CMS\nCollaboration's first measurement of the modification to the EEC within single\ninclusive jets in Pb+Pb collisions relative to p+p collisions reveals a\nsignificant enhancement at small angles, which may arise from jet transverse\nmomentum $p_T$ selection biases due to jet energy loss. We investigate the\ndependence of jet EECs on the flavor of the initiating parton. The EEC\ndistribution of a gluon jet is broader and the peak of transition from\nperturbative to non-perturbative regime occurs at a larger angle than a quark\njet. Such flavor dependence leads to the different EECs for $\\gamma$-jets and\nsingle inclusive jets due to their different flavor composition. It is also\nresponsible for a colliding energy dependence of EECs of single inclusive jets\nat fixed jet energy. We also investigate the impact of flavor composition\nvariation on the $p_T$ dependence of the jet EEC. We further propose that a\nchange in the gluon jet fraction in A+A collisions compared to p+p can also\ncontribute to a non-negligible enhancement of the medium modified EEC at small\nangles. Using the \\textsc{Jewel} model, we predict the reduction of the gluon\njet fraction in A+A collisions and estimate its impact on the EEC.",
        "We introduce the notion of integral Ricci curvature $I_{\\kappa_0}$ for\ngraphs, which measures the amount of Ricci curvature below a given threshold\n$\\kappa_0$. We focus our attention on the Lin-Lu-Yau Ricci curvature. As\napplications, we prove a Bonnet-Myers-type diameter estimate, a Moore-type\nestimate on the number of vertices of a graph in terms of the maximum degree\n$d_M$ and diameter $D$, and a Lichnerowicz-type estimate for the first\neigenvalue $\\lambda_1$ of the Graph Laplacian, generalizing the results\nobtained by Lin, Lu, and Yau. All estimates are uniform, depending only on\ngeometric parameters like $\\kappa_0$, $I_{\\kappa_0}$, $d_M$, or $D$, and do not\nrequire the graphs to be positively curved.",
        "Broadcasting on trees is a fundamental model from statistical physics that\nplays an important role in information theory, noisy computation and\nphylogenetic reconstruction within computational biology and linguistics. While\nthis model permits efficient linear-time algorithms for the inference of the\nroot from the leaves, recent work suggests that non-trivial computational\ncomplexity may be required for inference.\n  The inference of the root state can be performed using the celebrated Belief\nPropagation (BP) algorithm, which achieves Bayes-optimal performance. Although\nBP runs in linear time using real arithmetic operations, recent research\nindicates that it requires non-trivial computational complexity using more\nrefined complexity measures.\n  Moitra, Mossel, and Sandon demonstrated such complexity by constructing a\nMarkov chain for which estimating the root better than random guessing (for\ntypical inputs) is $NC^1$-complete. Kohler and Mossel constructed chains where,\nfor trees with $N$ leaves, achieving better-than-random root recovery requires\npolynomials of degree $N^{\\Omega(1)}$. The papers above raised the question of\nwhether such complexity bounds hold generally below the celebrated\nKesten-Stigum bound.\n  In a recent work, Huang and Mossel established a general degree lower bound\nof $\\Omega(\\log N)$ below the Kesten-Stigum bound. Specifically, they proved\nthat any function expressed as a linear combination of functions of at most\n$O(log N)$ leaves has vanishing correlation with the root. In this work, we get\nan exponential improvement of this lower bound by establishing an\n$N^{\\Omega(1)}$ degree lower bound, for any broadcast process in the whole\nregime below the Kesten-Stigum bound.",
        "The geodesic flow on a finite discrete q-manifold with or without boundary is\ndefined as as a permutation of its ordered q-simplices. This allows to define\ngeodesic sheets and a notion of sectional curvature.",
        "Controlling spin current lies at the heart of spintronics and its\napplications. The sign of spin currents is monotonous in ferromagnets once the\ncurrent direction is determined. Spin currents in antiferromagnets can possess\nopposite polarization, but requires enormous magnetic fields to lift the\ndegeneracy. Controlling spin currents with different polarization is urgently\ndemanded but remains hitherto elusive. Here, we demonstrate the control of spin\ncurrents at room temperature by magnon interference in a canted\nantiferromagnet, hematite recently also classified as an altermagnet.\nMagneto-optical characterization by Brillouin light scattering revealed that\nthe spatial periodicity of the beating patterns was tunable via the microwave\nfrequency. The inverse spin-Hall voltage changed sign as the frequency was\nscanned, i.e., a frequency-controlled switching of polarization in pure spin\ncurrents was obtained. Our work marks the use of antiferromagnetic magnon\ninterference to control spin currents, which substantially extends the horizon\nfor the emerging field of coherent antiferromagnetic spintronics.",
        "In this note we show that the support of a locally $k$-uniform measure in\n$\\mathbb R^{n+1}$ satisfies a kind of unique continuation property. As a\nconsequence, we show that locally uniformly distributed measures satisfy a\nweaker unique continuation property. This continues work of Kirchheim and\nPreiss (Math. Scand. 2002) and David, Kenig and Toro (Comm. Pure Appl. Math.\n2001) and lends additional evidence to the conjecture proposed by Kowalski and\nPreiss (J. Reine Angew. Math. 1987) that each connected component of the\nsupport of a locally $n$-uniform measure in $\\mathbb R^{n+1}$ is contained in\nthe zero set of a quadratic polynomial.",
        "Isospin symmetry, as the most precise flavor symmetry, can be used to extract\ninformation about hadronic dynamics. The effective Hamiltonian operators of\nbottom quark weak decays are zero under a series of isospin lowering operators\n$I_-^n$, which permits us to generate isospin sum rules without the\nWigner-Eckhart invariants. In this work, we derive hundreds of isospin sum\nrules for the two- and three-body non-leptonic decays of bottom baryons. They\nprovide hints for new decay modes and the isospin partners of pentaquark\nstates.",
        "Gaussian smoothing combined with a probabilistic framework for denoising via\nthe empirical Bayes formalism, i.e., the Tweedie-Miyasawa formula (TMF), are\nthe two key ingredients in the success of score-based generative models in\nEuclidean spaces. Smoothing holds the key for easing the problem of learning\nand sampling in high dimensions, denoising is needed for recovering the\noriginal signal, and TMF ties these together via the score function of noisy\ndata. In this work, we extend this paradigm to the problem of learning and\nsampling the distribution of binary data on the Boolean hypercube by adopting\nBernoulli noise, instead of Gaussian noise, as a smoothing device. We first\nderive a TMF-like expression for the optimal denoiser for the Hamming loss,\nwhere a score function naturally appears. Sampling noisy binary data is then\nachieved using a Langevin-like sampler which we theoretically analyze for\ndifferent noise levels. At high Bernoulli noise levels sampling becomes easy,\nakin to log-concave sampling in Euclidean spaces. In addition, we extend the\nsequential multi-measurement sampling of Saremi et al. (2024) to the binary\nsetting where we can bring the \"effective noise\" down by sampling multiple\nnoisy measurements at a fixed noise level, without the need for continuous-time\nstochastic processes. We validate our formalism and theoretical findings by\nexperiments on synthetic data and binarized images.",
        "The Pythagorean school attributed consonance in music to simplicity of\nfrequency ratios between musical tones. In the last two centuries, the\nconsonance curves developed by Helmholtz, Plompt and Levelt shifted focus to\npsycho-acoustic considerations in perceiving consonances. The appearance of\npeaks of these curves at the ratios considered by the Pythagorean school, and\nwhich were a consequence of an attempt to understand the world by nice\nmathematical proportions, remained a curiosity. This paper addresses this\ncuriosity, by describing a mathematical model of musical sound, along with a\nmathematical definition of consonance. First, we define pure, complex and mixed\ntones as mathematical models of musical sound. By a sequence of numerical\nexperiments and analytic calculations, we show that continuous cosine\nsimilarity, abbreviated as cosim, applied to these models quantifies the\nelusive concept of consonance as a frequency ratio which gives a local maximum\nof the cosim function. We prove that these maxima occur at the ratios\nconsidered as consonant in classical music theory. Moreover, we provide a\nsimple explanation why the number of musical intervals considered as consonant\nby musicians is finite, but has been increasing over the centuries.\nSpecifically, our formulas show that the number of consonant intervals changes\nwith the depth of the tone (the number of harmonics present).",
        "The injectivity of ReLU layers in neural networks, the recovery of vectors\nfrom clipped or saturated measurements, and (real) phase retrieval in\n$\\mathbb{R}^n$ allow for a similar problem formulation and characterization\nusing frame theory. In this paper, we revisit all three problems with a unified\nperspective and derive lower Lipschitz bounds for ReLU layers and clipping\nwhich are analogous to the previously known result for phase retrieval and are\noptimal up to a constant factor.",
        "In this short paper, we show that the McCool group does not satisfy the\nAndreadakis equality from degree $7$, and we give a lower bound for the size of\nthe difference between the two relevant filtrations. As a consequence, we see\nthat the Andreadakis problem for the McCool group does not stabilize.",
        "Rhombohedral tetralayer graphene has recently emerged as an exciting platform\nfor a possible chiral superconducting state. Here, we theoretically demonstrate\nand study the emergence of nonreciprocal superconductivity and an intrinsic\nsuperconducting diode effect in this system. Our results are based on a fully\nself-consistent framework for determining the superconducting order parameter\nfrom a Kohn-Luttinger mechanism to superconductivity and show that large diode\nefficiencies, $\\sim$ 60%, are achievable and highly tunable by an external\ndisplacement field. Moreover, we also find that the diodicity shows a\ncharacteristic angular dependence with multiple enhanced lobes, which depend on\nthe Fermi surface structure of the underlying normal state. Hence, our results\nsuggest that the intrinsic superconducting diode effect could provide insights\ninto the type of Fermi surface topology from which superconductivity arises.",
        "Nearly-doubly-regular tournaments have played significant roles in extremal\ngraph theory. In this note, we construct new cyclotomic nearly-doubly-regular\ntournaments and determine their spectrum by establishing a new connection\nbetween cyclotomic nearly-doubly-regular tournaments and almost difference sets\nfrom combinatorial design theory. Furthermore, under the celebrated\nHardy-Littlewood conjecture F in analytic number theory, our results confirm\nthe conjecture due to Sergey Savchenko (J. Graph Theory {\\bf 83} (2016),\n44--77) on the existence of infinitely many nearly-doubly-regular tournaments\nwith the canonical spectrum."
      ]
    }
  }
]