[
  {
    "id":2411.08073,
    "research_type":"applied",
    "start_id":"b20",
    "start_title":"A primer on deep learning in genomics",
    "start_abstract":"Deep learning methods are a class of machine learning techniques capable of identifying highly complex patterns in large datasets. Here, we provide a perspective and primer on deep learning applications for genome analysis. We discuss successful applications in the fields of regulatory genomics, variant calling and pathogenicity scores. We include general guidance for how to effectively use deep learning methods as well as a practical guide to tools and resources. This primer is accompanied by an interactive online tutorial.",
    "start_categories":[
      "q-bio.GN"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b24"
      ],
      "title":[
        "DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome"
      ],
      "abstract":[
        "Abstract Motivation Deciphering the language of non-coding DNA is one fundamental problems in genome research. Gene regulatory code highly complex due to existence polysemy and distant semantic relationship, which previous informatics methods often fail capture especially data-scarce scenarios. Results To address this challenge, we developed a novel pre-trained bidirectional encoder representation, named DNABERT, global transferrable understanding genomic sequences based on up downstream nucleotide contexts. We compared DNABERT most widely used programs for genome-wide elements prediction demonstrate its ease use, accuracy efficiency. show that single transformers model can simultaneously achieve state-of-the-art performance promoters, splice sites transcription factor binding sites, after easy fine-tuning using small task-specific labeled data. Further, enables direct visualization nucleotide-level importance relationship within input better interpretability accurate identification conserved sequence motifs functional genetic variant candidates. Finally, with human even be readily applied other organisms exceptional performance. anticipate fined tuned many analyses tasks. Availability implementation The source code, pretrained finetuned are available at GitHub (https:\/\/github.com\/jerryji1993\/DNABERT). Supplementary information data Bioinformatics online."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "MV-MATH: Evaluating Multimodal Math Reasoning in Multi-Visual Contexts",
        "Off-Switching Not Guaranteed",
        "Agent-R: Training Language Model Agents to Reflect via Iterative\n  Self-Training",
        "Perspectives for Direct Interpretability in Multi-Agent Deep\n  Reinforcement Learning",
        "PairVDN - Pair-wise Decomposed Value Functions",
        "Leveraging Knowledge Graphs and LLMs for Context-Aware Messaging",
        "Privacy-Enhancing Paradigms within Federated Multi-Agent Systems",
        "A Scalable Approach to Probabilistic Neuro-Symbolic Verification",
        "Fighter Jet Navigation and Combat using Deep Reinforcement Learning with\n  Explainable AI",
        "Knowledge is Power: Harnessing Large Language Models for Enhanced\n  Cognitive Diagnosis",
        "Neurosymbolic artificial intelligence via large language models and\n  coherence-driven inference",
        "AI-Driven Decision Support in Oncology: Evaluating Data Readiness for\n  Skin Cancer Treatment",
        "Lessons From Red Teaming 100 Generative AI Products",
        "Nonexistence of traveling wave solutions in the fractional Rosenau-Hyman\n  equation via homotopy perturbation method",
        "FORTE: An Open-Source System for Cost-Effective and Scalable\n  Environmental Monitoring",
        "UniCoRN: Unified Commented Retrieval Network with LMMs",
        "Combinatorial construction of symplectic 6-manifolds via bifibration\n  structures",
        "MIGE: A Unified Framework for Multimodal Instruction-Based Image\n  Generation and Editing",
        "Make Optimization Once and for All with Fine-grained Guidance",
        "Circular-polarization-selective perfect reflection from chiral\n  superconductors",
        "Forecasting Frontier Language Model Agent Capabilities",
        "Enhancing Pavement Sensor Data Acquisition for AI-Driven Transportation\n  Research",
        "Experimental demonstration of entanglement pumping with bosonic logical\n  qubits",
        "Interleaved Gibbs Diffusion for Constrained Generation",
        "PoisonedParrot: Subtle Data Poisoning Attacks to Elicit\n  Copyright-Infringing Content from Large Language Models",
        "Primordial Black Hole Formation via Inverted Bubble Collapse",
        "Multivariable Behavioral Change Modeling of Epidemics in the Presence of\n  Undetected Infections",
        "Reinforcement Learning for Quantum Control under Physical Constraints"
      ],
      "abstract":[
        "Multimodal Large Language Models (MLLMs) have shown promising capabilities in\nmathematical reasoning within visual contexts across various datasets. However,\nmost existing multimodal math benchmarks are limited to single-visual contexts,\nwhich diverges from the multi-visual scenarios commonly encountered in\nreal-world mathematical applications. To address this gap, we introduce\nMV-MATH: a meticulously curated dataset of 2,009 high-quality mathematical\nproblems. Each problem integrates multiple images interleaved with text,\nderived from authentic K-12 scenarios, and enriched with detailed annotations.\nMV-MATH includes multiple-choice, free-form, and multi-step questions, covering\n11 subject areas across 3 difficulty levels, and serves as a comprehensive and\nrigorous benchmark for assessing MLLMs' mathematical reasoning in multi-visual\ncontexts. Through extensive experimentation, we observe that MLLMs encounter\nsubstantial challenges in multi-visual math tasks, with a considerable\nperformance gap relative to human capabilities on MV-MATH. Furthermore, we\nanalyze the performance and error patterns of various models, providing\ninsights into MLLMs' mathematical reasoning capabilities within multi-visual\nsettings.",
        "Hadfield-Menell et al. (2017) propose the Off-Switch Game, a model of\nHuman-AI cooperation in which AI agents always defer to humans because they are\nuncertain about our preferences. I explain two reasons why AI agents might not\ndefer. First, AI agents might not value learning. Second, even if AI agents\nvalue learning, they might not be certain to learn our actual preferences.",
        "Large Language Models (LLMs) agents are increasingly pivotal for addressing\ncomplex tasks in interactive environments. Existing work mainly focuses on\nenhancing performance through behavior cloning from stronger experts, yet such\napproaches often falter in real-world applications, mainly due to the inability\nto recover from errors. However, step-level critique data is difficult and\nexpensive to collect. Automating and dynamically constructing self-critique\ndatasets is thus crucial to empowering models with intelligent agent\ncapabilities. In this work, we propose an iterative self-training framework,\nAgent-R, that enables language Agent to Reflect on the fly. Unlike traditional\nmethods that reward or penalize actions based on correctness, Agent-R leverages\nMCTS to construct training data that recover correct trajectories from\nerroneous ones. A key challenge of agent reflection lies in the necessity for\ntimely revision rather than waiting until the end of a rollout. To address\nthis, we introduce a model-guided critique construction mechanism: the actor\nmodel identifies the first error step (within its current capability) in a\nfailed trajectory. Starting from it, we splice it with the adjacent correct\npath, which shares the same parent node in the tree. This strategy enables the\nmodel to learn reflection based on its current policy, therefore yielding\nbetter learning efficiency. To further explore the scalability of this\nself-improvement paradigm, we investigate iterative refinement of both error\ncorrection capabilities and dataset construction. Our findings demonstrate that\nAgent-R continuously improves the model's ability to recover from errors and\nenables timely error correction. Experiments on three interactive environments\nshow that Agent-R effectively equips agents to correct erroneous actions while\navoiding loops, achieving superior performance compared to baseline methods\n(+5.59%).",
        "Multi-Agent Deep Reinforcement Learning (MADRL) was proven efficient in\nsolving complex problems in robotics or games, yet most of the trained models\nare hard to interpret. While learning intrinsically interpretable models\nremains a prominent approach, its scalability and flexibility are limited in\nhandling complex tasks or multi-agent dynamics. This paper advocates for direct\ninterpretability, generating post hoc explanations directly from trained\nmodels, as a versatile and scalable alternative, offering insights into agents'\nbehaviour, emergent phenomena, and biases without altering models'\narchitectures. We explore modern methods, including relevance backpropagation,\nknowledge edition, model steering, activation patching, sparse autoencoders and\ncircuit discovery, to highlight their applicability to single-agent,\nmulti-agent, and training process challenges. By addressing MADRL\ninterpretability, we propose directions aiming to advance active topics such as\nteam identification, swarm coordination and sample efficiency.",
        "Extending deep Q-learning to cooperative multi-agent settings is challenging\ndue to the exponential growth of the joint action space, the non-stationary\nenvironment, and the credit assignment problem. Value decomposition allows deep\nQ-learning to be applied at the joint agent level, at the cost of reduced\nexpressivity. Building on past work in this direction, our paper proposes\nPairVDN, a novel method for decomposing the value function into a collection of\npair-wise, rather than per-agent, functions, improving expressivity at the cost\nof requiring a more complex (but still efficient) dynamic programming\nmaximisation algorithm. Our method enables the representation of value\nfunctions which cannot be expressed as a monotonic combination of per-agent\nfunctions, unlike past approaches such as VDN and QMIX. We implement a novel\nmany-agent cooperative environment, Box Jump, and demonstrate improved\nperformance over these baselines in this setting. We open-source our code and\nenvironment at https:\/\/github.com\/zzbuzzard\/PairVDN.",
        "Personalized messaging plays an essential role in improving communication in\nareas such as healthcare, education, and professional engagement. This paper\nintroduces a framework that uses the Knowledge Graph (KG) to dynamically\nrephrase written communications by integrating individual and context-specific\ndata. The knowledge graph represents individuals, locations, and events as\ncritical nodes, linking entities mentioned in messages to their corresponding\ngraph nodes. The extraction of relevant information, such as preferences,\nprofessional roles, and cultural norms, is then combined with the original\nmessage and processed through a large language model (LLM) to generate\npersonalized responses. The framework demonstrates notable message acceptance\nrates in various domains: 42% in healthcare, 53% in education, and 78% in\nprofessional recruitment. By integrating entity linking, event detection, and\nlanguage modeling, this approach offers a structured and scalable solution for\ncontext-aware, audience-specific communication, facilitating advanced\napplications in diverse fields.",
        "LLM-based Multi-Agent Systems (MAS) have proven highly effective in solving\ncomplex problems by integrating multiple agents, each performing different\nroles. However, in sensitive domains, they face emerging privacy protection\nchallenges. In this paper, we introduce the concept of Federated MAS,\nhighlighting the fundamental differences between Federated MAS and traditional\nFL. We then identify key challenges in developing Federated MAS, including: 1)\nheterogeneous privacy protocols among agents, 2) structural differences in\nmulti-party conversations, and 3) dynamic conversational network structures. To\naddress these challenges, we propose Embedded Privacy-Enhancing Agents\n(EPEAgent), an innovative solution that integrates seamlessly into the\nRetrieval-Augmented Generation (RAG) phase and the context retrieval stage.\nThis solution minimizes data flows, ensuring that only task-relevant,\nagent-specific information is shared. Additionally, we design and generate a\ncomprehensive dataset to evaluate the proposed paradigm. Extensive experiments\ndemonstrate that EPEAgent effectively enhances privacy protection while\nmaintaining strong system performance. The code will be availiable at\nhttps:\/\/github.com\/ZitongShi\/EPEAgent",
        "Neuro-Symbolic Artificial Intelligence (NeSy AI) has emerged as a promising\ndirection for integrating neural learning with symbolic reasoning. In the\nprobabilistic variant of such systems, a neural network first extracts a set of\nsymbols from sub-symbolic input, which are then used by a symbolic component to\nreason in a probabilistic manner towards answering a query. In this work, we\naddress the problem of formally verifying the robustness of such NeSy\nprobabilistic reasoning systems, therefore paving the way for their safe\ndeployment in critical domains. We analyze the complexity of solving this\nproblem exactly, and show that it is $\\mathrm{NP}^{\\# \\mathrm{P}}$-hard. To\novercome this issue, we propose the first approach for approximate,\nrelaxation-based verification of probabilistic NeSy systems. We demonstrate\nexperimentally that the proposed method scales exponentially better than\nsolver-based solutions and apply our technique to a real-world autonomous\ndriving dataset, where we verify a safety property under large input\ndimensionalities and network sizes.",
        "This paper presents the development of an Artificial Intelligence (AI) based\nfighter jet agent within a customized Pygame simulation environment, designed\nto solve multi-objective tasks via deep reinforcement learning (DRL). The jet's\nprimary objectives include efficiently navigating the environment, reaching a\ntarget, and selectively engaging or evading an enemy. A reward function\nbalances these goals while optimized hyperparameters enhance learning\nefficiency. Results show more than 80\\% task completion rate, demonstrating\neffective decision-making. To enhance transparency, the jet's action choices\nare analyzed by comparing the rewards of the actual chosen action (factual\naction) with those of alternate actions (counterfactual actions), providing\ninsights into the decision-making rationale. This study illustrates DRL's\npotential for multi-objective problem-solving with explainable AI. Project page\nis available at:\n\\href{https:\/\/github.com\/swatikar95\/Autonomous-Fighter-Jet-Navigation-and-Combat}{Project\nGitHub Link}.",
        "Cognitive Diagnosis Models (CDMs) are designed to assess students' cognitive\nstates by analyzing their performance across a series of exercises. However,\nexisting CDMs often struggle with diagnosing infrequent students and exercises\ndue to a lack of rich prior knowledge. With the advancement in large language\nmodels (LLMs), which possess extensive domain knowledge, their integration into\ncognitive diagnosis presents a promising opportunity. Despite this potential,\nintegrating LLMs with CDMs poses significant challenges. LLMs are not\nwell-suited for capturing the fine-grained collaborative interactions between\nstudents and exercises, and the disparity between the semantic space of LLMs\nand the behavioral space of CDMs hinders effective integration. To address\nthese issues, we propose a novel Knowledge-enhanced Cognitive Diagnosis (KCD)\nframework, which is a model-agnostic framework utilizing LLMs to enhance CDMs\nand compatible with various CDM architectures. The KCD framework operates in\ntwo stages: LLM Diagnosis and Cognitive Level Alignment. In the LLM Diagnosis\nstage, both students and exercises are diagnosed to achieve comprehensive and\ndetailed modeling. In the Cognitive Level Alignment stage, we bridge the gap\nbetween the CDMs' behavioral space and the LLMs' semantic space using\ncontrastive learning and mask-reconstruction approaches. Experiments on several\nreal-world datasets demonstrate the effectiveness of our proposed framework.",
        "We devise an algorithm to generate sets of propositions that objectively\ninstantiate graphs that support coherence-driven inference. We then benchmark\nthe ability of large language models (LLMs) to reconstruct coherence graphs\nfrom (a straightforward transformation of) propositions expressed in natural\nlanguage, with promising results from a single prompt to models optimized for\nreasoning. Combining coherence-driven inference with consistency evaluations by\nneural models may advance the state of the art in machine cognition.",
        "This research focuses on evaluating and enhancing data readiness for the\ndevelopment of an Artificial Intelligence (AI)-based Clinical Decision Support\nSystem (CDSS) in the context of skin cancer treatment. The study, conducted at\nthe Skin Tumor Center of the University Hospital M\\\"unster, delves into the\nessential role of data quality, availability, and extractability in\nimplementing effective AI applications in oncology. By employing a multifaceted\nmethodology, including literature review, data readiness assessment, and expert\nworkshops, the study addresses the challenges of integrating AI into clinical\ndecision-making. The research identifies crucial data points for skin cancer\ntreatment decisions, evaluates their presence and quality in various\ninformation systems, and highlights the difficulties in extracting information\nfrom unstructured data. The findings underline the significance of\nhigh-quality, accessible data for the success of AI-driven CDSS in medical\nsettings, particularly in the complex field of oncology.",
        "In recent years, AI red teaming has emerged as a practice for probing the\nsafety and security of generative AI systems. Due to the nascency of the field,\nthere are many open questions about how red teaming operations should be\nconducted. Based on our experience red teaming over 100 generative AI products\nat Microsoft, we present our internal threat model ontology and eight main\nlessons we have learned:\n  1. Understand what the system can do and where it is applied\n  2. You don't have to compute gradients to break an AI system\n  3. AI red teaming is not safety benchmarking\n  4. Automation can help cover more of the risk landscape\n  5. The human element of AI red teaming is crucial\n  6. Responsible AI harms are pervasive but difficult to measure\n  7. LLMs amplify existing security risks and introduce new ones\n  8. The work of securing AI systems will never be complete\n  By sharing these insights alongside case studies from our operations, we\noffer practical recommendations aimed at aligning red teaming efforts with real\nworld risks. We also highlight aspects of AI red teaming that we believe are\noften misunderstood and discuss open questions for the field to consider.",
        "We apply the homotopy perturbation method to construct series solutions for\nthe fractional Rosenau-Hyman (fRH) equation and study their dynamics. Unlike\nthe classical RH equation where compactons arise from truncated periodic\nsolutions, we show that spatial nonlocality prevents the existence of\ncompactons, and therefore periodic traveling waves are considered. By\nasymptotic analyses involving the Mittag-Leffler function, it is shown that the\nquadratic fRH equation exhibits bifurcation with respect to the order of the\ntemporal fractional derivative, leading to the eventual pinning of wave\npropagation. Additionally, numerical results suggest potential finite time\nblow-up in the cubic fRH. While HPM proves effective in constructing analytic\nsolutions, we identify cases of divergence, underscoring the need for further\nresearch into its convergence properties and broader applicability.",
        "Forests are an essential part of our biosphere, regulating climate, acting as\na sink for greenhouse gases, and providing numerous other ecosystem services.\nHowever, they are negatively impacted by climatic stressors such as drought or\nheat waves. In this paper, we introduce FORTE, an open-source system for\nenvironmental monitoring with the aim of understanding how forests react to\nsuch stressors. It consists of two key components: (1) a wireless sensor\nnetwork (WSN) deployed in the forest for data collection, and (2) a Data\nInfrastructure for data processing, storage, and visualization. The WSN\ncontains a Central Unit capable of transmitting data to the Data Infrastructure\nvia LTE-M and several spatially independent Satellites that collect data over\nlarge areas and transmit them wirelessly to the Central Unit. Our prototype\ndeployments show that our solution is cost-effective compared to commercial\nsolutions, energy-efficient with sensor nodes lasting for several months on a\nsingle charge, and reliable in terms of data quality. FORTE's flexible\narchitecture makes it suitable for a wide range of environmental monitoring\napplications beyond forest monitoring. The contributions of this paper are\nthree-fold. First, we describe the high-level requirements necessary for\ndeveloping an environmental monitoring system. Second, we present an\narchitecture and prototype implementation of the requirements by introducing\nour FORTE platform and demonstrating its effectiveness through multiple field\ntests. Lastly, we provide source code, documentation, and hardware design\nartifacts as part of our open-source repository.",
        "Multimodal retrieval methods have limitations in handling complex,\ncompositional queries that require reasoning about the visual content of both\nthe query and the retrieved entities. On the other hand, Large Multimodal\nModels (LMMs) can answer with language to more complex visual questions, but\nwithout the inherent ability to retrieve relevant entities to support their\nanswers. We aim to address these limitations with UniCoRN, a Unified Commented\nRetrieval Network that combines the strengths of composed multimodal retrieval\nmethods and generative language approaches, going beyond Retrieval-Augmented\nGeneration (RAG). We introduce an entity adapter module to inject the retrieved\nmultimodal entities back into the LMM, so it can attend to them while\ngenerating answers and comments. By keeping the base LMM frozen, UniCoRN\npreserves its original capabilities while being able to perform both retrieval\nand text generation tasks under a single integrated framework. To assess these\nnew abilities, we introduce the Commented Retrieval task (CoR) and a\ncorresponding dataset, with the goal of retrieving an image that accurately\nanswers a given question and generate an additional textual response that\nprovides further clarification and details about the visual information. We\ndemonstrate the effectiveness of UniCoRN on several datasets showing\nimprovements of +4.5% recall over the state of the art for composed multimodal\nretrieval and of +14.9% METEOR \/ +18.4% BEM over RAG for commenting in CoR.",
        "A bifibration structure on a $6$-manifold is a map to either the complex\nprojective plane $\\mathbb{P}^2$ or a $\\mathbb{P}^1$-bundle over $\\mathbb{P}^1$,\nsuch that its composition with the projection to $\\mathbb{P}^1$ is a\n($6$-dimensional) Lefschetz fibration\/pencil, and its restriction to the\npreimage of a generic $\\mathbb{P}^1$-fiber is also a ($4$-dimensional)\nLefschetz fibration\/pencil. This object has been studied by Auroux, Katzarkov,\nSeidel, among others. From a pair consisting of a monodromy representation of a\nLefschetz fibration\/pencil on a $4$-manifold and a relation in a braid group,\nwhich are mutually compatible in an appropriate sense, we construct a\nbifibration structure on a closed symplectic $6$-manifold, producing the given\ncompatible pair as its monodromies. We further establish methods for computing\ntopological invariants of symplectic $6$-manifolds, including Chern numbers,\nfrom compatible pairs. Additionally, we provide an explicit example of a\ncompatible pair, conjectured to correspond to a bifibration structure derived\nfrom the degree-$2$ Veronese embedding of the $3$-dimensional complex\nprojective space. This example can be viewed as a higher-dimensional analogue\nof the lantern relation in the mapping class group of the four-punctured\nsphere. Our results not only extend the applicability of combinatorial\ntechniques to higher-dimensional symplectic geometry but also offer a unified\nframework for systematically exploring symplectic $6$-manifolds.",
        "Despite significant progress in diffusion-based image generation,\nsubject-driven generation and instruction-based editing remain challenging.\nExisting methods typically treat them separately, struggling with limited\nhigh-quality data and poor generalization. However, both tasks require\ncapturing complex visual variations while maintaining consistency between\ninputs and outputs. Therefore, we propose MIGE, a unified framework that\nstandardizes task representations using multimodal instructions. It treats\nsubject-driven generation as creation on a blank canvas and instruction-based\nediting as modification of an existing image, establishing a shared\ninput-output formulation. MIGE introduces a novel multimodal encoder that maps\nfree-form multimodal instructions into a unified vision-language space,\nintegrating visual and semantic features through a feature fusion mechanism.\nThis unification enables joint training of both tasks, providing two key\nadvantages: (1) Cross-Task Enhancement: By leveraging shared visual and\nsemantic representations, joint training improves instruction adherence and\nvisual consistency in both subject-driven generation and instruction-based\nediting. (2) Generalization: Learning in a unified format facilitates\ncross-task knowledge transfer, enabling MIGE to generalize to novel\ncompositional tasks, including instruction-based subject-driven editing.\nExperiments show that MIGE excels in both subject-driven generation and\ninstruction-based editing while setting a state-of-the-art in the new task of\ninstruction-based subject-driven editing. Code and model have been publicly\navailable at https:\/\/github.com\/Eureka-Maggie\/MIGE.",
        "Learning to Optimize (L2O) enhances optimization efficiency with integrated\nneural networks. L2O paradigms achieve great outcomes, e.g., refitting\noptimizer, generating unseen solutions iteratively or directly. However,\nconventional L2O methods require intricate design and rely on specific\noptimization processes, limiting scalability and generalization. Our analyses\nexplore general framework for learning optimization, called Diff-L2O, focusing\non augmenting sampled solutions from a wider view rather than local updates in\nreal optimization process only. Meanwhile, we give the related generalization\nbound, showing that the sample diversity of Diff-L2O brings better performance.\nThis bound can be simply applied to other fields, discussing diversity,\nmean-variance, and different tasks. Diff-L2O's strong compatibility is\nempirically verified with only minute-level training, comparing with other\nhour-levels.",
        "Integrating mirrors with magnetic components is crucial for constructing\nchiral optical cavities, which provide tunable platforms for\ntime-reversal-asymmetric light-matter interactions. Here, we introduce\nsingle-crystal circular-polarization-selective mirrors based on chiral\nsuperconductors, which break time-reversal symmetry by themselves eliminating\nthe need for additional components. We show that a\ncircular-polarization-selective perfect reflection (CSPR) occurs for\nstrong-coupling superconductors in the BCS-BEC crossover regime or beyond if\nthe optical Hall conductivity is significant in the unit of conductivity\nquantum per unit layer, $e^2\/ha_z$, where $a_z$ is the lattice constant along\nthe surface normal. While the optical Hall conductivity in chiral\nsuperconductors is typically tiny, we classify three routes to obtain a large\nvalue. We demonstrate the significant optical Hall conductivity and the\nresulting CSPR with two examples: (1) superconductivity in doped quantum Hall\ninsulators and (2) chiral pairing that preserves the Bogoliubov Fermi surfaces\nin the weak-pairing limit. We also discuss the application of our theory to the\nrecently discovered chiral superconducting phase in rhombohedral graphene. Our\ntheory reveals the potential of these classes of chiral superconductors as\npromising elements for building high-quality-factor terahertz chiral cavities.",
        "As Language Models (LMs) increasingly operate as autonomous agents,\naccurately forecasting their capabilities becomes crucial for societal\npreparedness. We evaluate six forecasting methods that predict downstream\ncapabilities of LM agents. We use \"one-step\" approaches that predict benchmark\nscores from input metrics like compute or model release date directly or\n\"two-step\" approaches that first predict an intermediate metric like the\nprincipal component of cross-benchmark performance (PC-1) and human-evaluated\ncompetitive Elo ratings. We evaluate our forecasting methods by backtesting\nthem on a dataset of 38 LMs from the OpenLLM 2 leaderboard. We then use the\nvalidated two-step approach (Release Date$\\to$Elo$\\to$Benchmark) to predict LM\nagent performance for frontier models on three benchmarks: SWE-Bench Verified\n(software development), Cybench (cybersecurity assessment), and RE-Bench (ML\nresearch engineering). Our forecast predicts that by the beginning of 2026,\nnon-specialized LM agents with low capability elicitation will reach a success\nrate of 54% on SWE-Bench Verified, while state-of-the-art LM agents will reach\nan 87% success rate. Our approach does not account for recent advances in\ninference-compute scaling and might thus be too conservative.",
        "Effective strategies for sensor data management are essential for advancing\ntransportation research, especially in the current data-driven era, due to the\nadvent of novel applications in artificial intelligence. This paper presents\ncomprehensive guidelines for managing transportation sensor data, encompassing\nboth archived static data and real-time data streams. The real-time system\narchitecture integrates various applications with data acquisition systems\n(DAQ). By deploying the in-house designed, open-source Avena software platform\nalongside the NATS messaging system as a secure communication broker, reliable\ndata exchange is ensured. While robust databases like TimescaleDB facilitate\norganized storage, visualization platforms like Grafana provide real-time\nmonitoring capabilities.\n  In contrast, static data standards address the challenges in handling\nunstructured, voluminous datasets. The standards advocate for a combination of\ncost-effective bulk cloud storage for unprocessed sensor data and relational\ndatabases for recording summarized analyses. They highlight the role of cloud\ndata transfer tools like FME for efficient migration of sensor data from local\nstorages onto the cloud. Further, integration of robust visualization tools\ninto the framework helps in deriving patterns and trends from these complex\ndatasets.\n  The proposals were applied to INDOT's real-world case studies involving the\nI-65 and I-69 Greenfield districts. For real-time data collection, Campbell\nScientific DAQ systems were used, enabling continuous generation and monitoring\nof sensor metrics. In the case of the archived I-69 database, summary data was\ncompiled in Oracle, while the unprocessed data was stored in SharePoint. The\nresults underline the effectiveness of the proposed guidelines and motivate\ntheir adoption in research projects.",
        "Entanglement is crucial for quantum networks and computation, yet maintaining\nhigh-fidelity entangled quantum states is hindered by decoherence and\nresource-intensive purification methods. Here, we experimentally demonstrate\nentanglement pumping, utilizing bosonic quantum error correction (QEC) codes as\nlong-coherence-time storage qubits. By repetitively generating entanglement\nwith short-coherence-time qubits and injecting it into QEC-protected logical\nqubits, our approach effectively preserves entanglement. Through error\ndetection to discard error states and entanglement pumping to mitigate errors\nwithin the code space, we extend the lifespan of entangled logical qubits by\nnearly 50% compared to the case without entanglement pumping. This work\nhighlights the potential of bosonic logical qubits for scalable quantum\nnetworks and introduces a novel paradigm for efficient entanglement management.",
        "We introduce Interleaved Gibbs Diffusion (IGD), a novel generative modeling\nframework for mixed continuous-discrete data, focusing on constrained\ngeneration problems. Prior works on discrete and continuous-discrete diffusion\nmodels assume factorized denoising distribution for fast generation, which can\nhinder the modeling of strong dependencies between random variables encountered\nin constrained generation. IGD moves beyond this by interleaving continuous and\ndiscrete denoising algorithms via a discrete time Gibbs sampling type Markov\nchain. IGD provides flexibility in the choice of denoisers, allows conditional\ngeneration via state-space doubling and inference time scaling via the\nReDeNoise method. Empirical evaluations on three challenging tasks-solving\n3-SAT, generating molecule structures, and generating layouts-demonstrate\nstate-of-the-art performance. Notably, IGD achieves a 7% improvement on 3-SAT\nout of the box and achieves state-of-the-art results in molecule generation\nwithout relying on equivariant diffusion or domain-specific architectures. We\nexplore a wide range of modeling, and interleaving strategies along with\nhyperparameters in each of these problems.",
        "As the capabilities of large language models (LLMs) continue to expand, their\nusage has become increasingly prevalent. However, as reflected in numerous\nongoing lawsuits regarding LLM-generated content, addressing copyright\ninfringement remains a significant challenge. In this paper, we introduce\nPoisonedParrot: the first stealthy data poisoning attack that induces an LLM to\ngenerate copyrighted content even when the model has not been directly trained\non the specific copyrighted material. PoisonedParrot integrates small fragments\nof copyrighted text into the poison samples using an off-the-shelf LLM. Despite\nits simplicity, evaluated in a wide range of experiments, PoisonedParrot is\nsurprisingly effective at priming the model to generate copyrighted content\nwith no discernible side effects. Moreover, we discover that existing defenses\nare largely ineffective against our attack. Finally, we make the first attempt\nat mitigating copyright-infringement poisoning attacks by proposing a defense:\nParrotTrap. We encourage the community to explore this emerging threat model\nfurther.",
        "We propose a novel mechanism of primordial black hole (PBH) formation through\ninverted bubble collapse. In this scenario, bubbles nucleate sparsely in an\nincomplete first-order phase transition, followed by a bulk phase transition in\nthe rest of the universe that inverts these pre-existing bubbles into false\nvacuum regions. These spherically symmetric false-vacuum bubbles subsequently\ncollapse to form PBHs. Unlike conventional PBH formation mechanisms associated\nwith domain wall collapse or bubble coalescence, our inverted bubble collapse\nmechanism naturally ensures spherical collapse. We demonstrate that, when\napplied to the singlet extension of the Standard Model, this mechanism can\nproduce highly monochromatic PBHs with masses up to ${\\cal\nO}(10^{-7}\\,\\text{-}\\,10^{-5}) M_\\odot$, which potentially explain the\nmicrolensing events observed in the OGLE and Subaru HSC data.",
        "Epidemic models are invaluable tools to understand and implement strategies\nto control the spread of infectious diseases, as well as to inform public\nhealth policies and resource allocation. However, current modeling approaches\nhave limitations that reduce their practical utility, such as the exclusion of\nhuman behavioral change in response to the epidemic or ignoring the presence of\nundetected infectious individuals in the population. These limitations became\nparticularly evident during the COVID-19 pandemic, underscoring the need for\nmore accurate and informative models. Motivated by these challenges, we develop\na novel Bayesian epidemic modeling framework to better capture the complexities\nof disease spread by incorporating behavioral responses and undetected\ninfections. In particular, our framework makes three contributions: 1)\nleveraging additional data on hospitalizations and deaths in modeling the\ndisease dynamics, 2) accounting data uncertainty arising from the large\npresence of asymptomatic and undetected infections, and 3) allowing the\npopulation behavioral change to be dynamically influenced by multiple data\nsources (cases and deaths). We thoroughly investigate the properties of the\nproposed model via simulation, and illustrate its utility on COVID-19 data from\nMontreal and Miami.",
        "Quantum optimal control is concerned with the realisation of desired dynamics\nin quantum systems, serving as a linchpin for advancing quantum technologies\nand fundamental research. Analytic approaches and standard optimisation\nalgorithms do not yield satisfactory solutions for large quantum systems, and\nespecially not for real world quantum systems which are open and noisy. We\ndevise a physics-informed Reinforcement Learning (RL) algorithm that restricts\nthe space of possible solutions. We incorporate priors about the desired time\nscales of the quantum state dynamics - as well as realistic control signal\nlimitations - as constraints to the RL algorithm. These physics-informed\nconstraints additionally improve computational scalability by facilitating\nparallel optimisation. We evaluate our method on three broadly relevant quantum\nsystems (multi-level $\\Lambda$ system, Rydberg atom and superconducting\ntransmon) and incorporate real-world complications, arising from dissipation\nand control signal perturbations. We achieve both higher fidelities - which\nexceed 0.999 across all systems - and better robustness to time-dependent\nperturbations and experimental imperfections than previous methods. Lastly, we\ndemonstrate that incorporating multi-step feedback can yield solutions robust\neven to strong perturbations."
      ]
    }
  },
  {
    "id":2411.15211,
    "research_type":"applied",
    "start_id":"b6",
    "start_title":"Solar Cells for Indoor Applications: Progress and Development",
    "start_abstract":"The Internet of things (IoT) has been rapidly growing in the past few years. IoT connects numerous devices, such as wireless sensors, actuators, and wearable to optimize monitor daily activities. Most these devices require power microwatt range operate indoors. To this end, a self-sustainable source, photovoltaic (PV) cell, which can harvest low-intensity indoor light, is appropriate. Recently, development highly efficient PV cells for applications attracted tremendous attention. Therefore, different types materials, inorganic, dye-sensitized, organic, perovskite have employed harvesting light energy. Although considerable efforts made by researchers develop low-cost, stable, applications, Extensive investigation necessary resolve some critical issues concerning cells, environmental stability, lifetime, large-area fabrication, mechanical flexibility, production cost. address issues, systematic review aspects will be useful research community. This study discusses current status based on previous reports. First, we provided relevant background information. Then, described sources, subsequently critically reviewed reports regarding solar active materials perovskite. Finally, placed an attempt provide insight into factors needed further improve feasibility technology applications.",
    "start_categories":[
      "astro-ph.SR"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b9"
      ],
      "title":[
        "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
      ],
      "abstract":[
        "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent models, BERT is designed to pre-train deep bidirectional representations unlabeled text by jointly conditioning on both left and right context in all layers. As result, the pre-trained can be fine-tuned with just one additional output layer create state-of-the-art models wide range of tasks, such as question answering inference, without substantial task-specific architecture modifications. conceptually simple empirically powerful. It obtains results eleven natural processing including pushing GLUE score 80.5% (7.7% point absolute improvement), MultiNLI accuracy 86.7% (4.6% SQuAD v1.1 Test F1 93.2 (1.5 improvement) v2.0 83.1 (5.1 improvement)."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "RM-PoT: Reformulating Mathematical Problems and Solving via Program of\n  Thoughts",
        "Data and System Perspectives of Sustainable Artificial Intelligence",
        "Teaching AI to Handle Exceptions: Supervised Fine-Tuning with\n  Human-Aligned Judgment",
        "Beyond Local Selection: Global Cut Selection for Enhanced Mixed-Integer\n  Programming",
        "Understanding LLMs' Fluid Intelligence Deficiency: An Analysis of the\n  ARC Task",
        "Sustainable and Intelligent Public Facility Failure Management System\n  Based on Large Language Models",
        "MV-MATH: Evaluating Multimodal Math Reasoning in Multi-Visual Contexts",
        "MEPNet: Medical Entity-balanced Prompting Network for Brain CT Report\n  Generation",
        "Electronic Health Records: Towards Digital Twins in Healthcare",
        "Lessons From Red Teaming 100 Generative AI Products",
        "Offline Critic-Guided Diffusion Policy for Multi-User Delay-Constrained\n  Scheduling",
        "Guidelines for Applying RL and MARL in Cybersecurity Applications",
        "Reasoning Effort and Problem Complexity: A Scaling Analysis in LLMs",
        "Limited Effectiveness of LLM-based Data Augmentation for COVID-19\n  Misinformation Stance Detection",
        "The normal growth of linear groups over formal power serieses",
        "The multi-level friendship paradox for sparse random graphs",
        "The duality resolution at $n=p=2$",
        "Optimal Control of the Navier-Stokes equations via Pressure Boundary\n  Conditions",
        "Clinically Ready Magnetic Microrobots for Targeted Therapies",
        "How can representation dimension dominate structurally pruned LLMs?",
        "Deterministic Global Optimization over trained Kolmogorov Arnold\n  Networks",
        "EOG Communication Interface for Quadriplegics: Prototype & Signal\n  Processing",
        "The Popularity Hypothesis in Software Security: A Large-Scale\n  Replication with PHP Packages",
        "Transformer Based Time-Series Forecasting for Stock",
        "Defending Against Gradient Inversion Attacks for Biomedical Images via\n  Learnable Data Perturbation",
        "Cognitive AI framework: advances in the simulation of human thought",
        "DreamFLEX: Learning Fault-Aware Quadrupedal Locomotion Controller for\n  Anomaly Situation in Rough Terrains",
        "Negativity in Self-Admitted Technical Debt: How Sentiment Influences\n  Prioritization"
      ],
      "abstract":[
        "Recently, substantial advancements have been made in training language models\nto carry out step-by-step reasoning for solving intricate numerical reasoning\ntasks. Beyond the methods used to solve these problems, the structure and\nformulation of the problems themselves also play a crucial role in determining\nthe performance of large language models. We observe that even small changes in\nthe surface form of mathematical problems can have a profound impact on both\nthe answer distribution and solve rate. This highlights the vulnerability of\nLLMs to surface-level variations, revealing its limited robustness when\nreasoning through complex problems. In this paper, we propose RM-PoT, a\nthree-stage framework that integrates problem reformulation (RM), code-aided\nreasoning (PoT), and domain-aware few-shot learning to address these\nlimitations. Our approach first reformulates the input problem into diverse\nsurface forms to reduce structural bias, then retrieves five semantically\naligned examples from a pre-constructed domain-specific question bank to\nprovide contextual guidance, and finally generates executable Python code for\nprecise computation.",
        "Sustainable AI is a subfield of AI for concerning developing and using AI\nsystems in ways of aiming to reduce environmental impact and achieve\nsustainability. Sustainable AI is increasingly important given that training of\nand inference with AI models such as large langrage models are consuming a\nlarge amount of computing power. In this article, we discuss current issues,\nopportunities and example solutions for addressing these issues, and future\nchallenges to tackle, from the data and system perspectives, related to data\nacquisition, data processing, and AI model training and inference.",
        "Large language models (LLMs), initially developed for generative AI, are now\nevolving into agentic AI systems, which make decisions in complex, real-world\ncontexts. Unfortunately, while their generative capabilities are\nwell-documented, their decision-making processes remain poorly understood. This\nis particularly evident when models are handling exceptions, a critical and\nchallenging aspect of decision-making made relevant by the inherent\nincompleteness of contracts. Here we demonstrate that LLMs, even ones that\nexcel at reasoning, deviate significantly from human judgments because they\nadhere strictly to policies, even when such adherence is impractical,\nsuboptimal, or even counterproductive. We then evaluate three approaches to\ntuning AI agents to handle exceptions: ethical framework prompting,\nchain-of-thought reasoning, and supervised fine-tuning. We find that while\nethical framework prompting fails and chain-of-thought prompting provides only\nslight improvements, supervised fine-tuning, specifically with human\nexplanations, yields markedly better results. Surprisingly, in our experiments,\nsupervised fine-tuning even enabled models to generalize human-like\ndecision-making to novel scenarios, demonstrating transfer learning of\nhuman-aligned decision-making across contexts. Furthermore, fine-tuning with\nexplanations, not just labels, was critical for alignment, suggesting that\naligning LLMs with human judgment requires explicit training on how decisions\nare made, not just which decisions are made. These findings highlight the need\nto address LLMs' shortcomings in handling exceptions in order to guide the\ndevelopment of agentic AI toward models that can effectively align with human\njudgment and simultaneously adapt to novel contexts.",
        "In mixed-integer programming (MIP) solvers, cutting planes are essential for\nBranch-and-Cut (B&C) algorithms as they reduce the search space and accelerate\nthe solving process. Traditional methods rely on hard-coded heuristics for cut\nplane selection but fail to leverage problem-specific structural features.\nRecent machine learning approaches use neural networks for cut selection but\nfocus narrowly on the efficiency of single-node within the B&C algorithm,\nwithout considering the broader contextual information. To address this, we\npropose Global Cut Selection (GCS), which uses a bipartite graph to represent\nthe search tree and combines graph neural networks with reinforcement learning\nto develop cut selection strategies. Unlike prior methods, GCS applies cutting\nplanes across all nodes, incorporating richer contextual information.\nExperiments show GCS significantly improves solving efficiency for synthetic\nand large-scale real-world MIPs compared to traditional and learning-based\nmethods.",
        "While LLMs have exhibited strong performance on various NLP tasks, it is\nnoteworthy that most of these tasks rely on utilizing the vast amount of\nknowledge encoded in LLMs' parameters, rather than solving new problems without\nprior knowledge. In cognitive research, the latter ability is referred to as\nfluid intelligence, which is considered to be critical for assessing human\nintelligence. Recent research on fluid intelligence assessments has highlighted\nsignificant deficiencies in LLMs' abilities. In this paper, we analyze the\nchallenges LLMs face in demonstrating fluid intelligence through controlled\nexperiments, using the most representative ARC task as an example. Our study\nrevealed three major limitations in existing LLMs: limited ability for skill\ncomposition, unfamiliarity with abstract input formats, and the intrinsic\ndeficiency of left-to-right decoding. Our data and code can be found in\nhttps:\/\/wujunjie1998.github.io\/araoc-benchmark.github.io\/.",
        "This paper presents a new Large Language Model (LLM)-based Smart Device\nManagement framework, a pioneering approach designed to address the intricate\nchallenges of managing intelligent devices within public facilities, with a\nparticular emphasis on applications to libraries. Our framework leverages\nstate-of-the-art LLMs to analyze and predict device failures, thereby enhancing\noperational efficiency and reliability. Through prototype validation in\nreal-world library settings, we demonstrate the framework's practical\napplicability and its capacity to significantly reduce budgetary constraints on\npublic facilities. The advanced and innovative nature of our model is evident\nfrom its successful implementation in prototype testing. We plan to extend the\nframework's scope to include a wider array of public facilities and to\nintegrate it with cutting-edge cybersecurity technologies, such as Internet of\nThings (IoT) security and machine learning algorithms for threat detection and\nresponse. This will result in a comprehensive and proactive maintenance system\nthat not only bolsters the security of intelligent devices but also utilizes\nmachine learning for automated analysis and real-time threat mitigation. By\nincorporating these advanced cybersecurity elements, our framework will be\nwell-positioned to tackle the dynamic challenges of modern public\ninfrastructure, ensuring robust protection against potential threats and\nenabling facilities to anticipate and prevent failures, leading to substantial\ncost savings and enhanced service quality.",
        "Multimodal Large Language Models (MLLMs) have shown promising capabilities in\nmathematical reasoning within visual contexts across various datasets. However,\nmost existing multimodal math benchmarks are limited to single-visual contexts,\nwhich diverges from the multi-visual scenarios commonly encountered in\nreal-world mathematical applications. To address this gap, we introduce\nMV-MATH: a meticulously curated dataset of 2,009 high-quality mathematical\nproblems. Each problem integrates multiple images interleaved with text,\nderived from authentic K-12 scenarios, and enriched with detailed annotations.\nMV-MATH includes multiple-choice, free-form, and multi-step questions, covering\n11 subject areas across 3 difficulty levels, and serves as a comprehensive and\nrigorous benchmark for assessing MLLMs' mathematical reasoning in multi-visual\ncontexts. Through extensive experimentation, we observe that MLLMs encounter\nsubstantial challenges in multi-visual math tasks, with a considerable\nperformance gap relative to human capabilities on MV-MATH. Furthermore, we\nanalyze the performance and error patterns of various models, providing\ninsights into MLLMs' mathematical reasoning capabilities within multi-visual\nsettings.",
        "The automatic generation of brain CT reports has gained widespread attention,\ngiven its potential to assist radiologists in diagnosing cranial diseases.\nHowever, brain CT scans involve extensive medical entities, such as diverse\nanatomy regions and lesions, exhibiting highly inconsistent spatial patterns in\n3D volumetric space. This leads to biased learning of medical entities in\nexisting methods, resulting in repetitiveness and inaccuracy in generated\nreports. To this end, we propose a Medical Entity-balanced Prompting Network\n(MEPNet), which harnesses the large language model (LLM) to fairly interpret\nvarious entities for accurate brain CT report generation. By introducing the\nvisual embedding and the learning status of medical entities as enriched clues,\nour method prompts the LLM to balance the learning of diverse entities, thereby\nenhancing reports with comprehensive findings. First, to extract visual\nembedding of entities, we propose Knowledge-driven Joint Attention to explore\nand distill entity patterns using both explicit and implicit medical knowledge.\nThen, a Learning Status Scorer is designed to evaluate the learning of entity\nvisual embeddings, resulting in unique learning status for individual entities.\nFinally, these entity visual embeddings and status are elaborately integrated\ninto multi-modal prompts, to guide the text generation of LLM. This process\nallows LLM to self-adapt the learning process for biased-fitted entities,\nthereby covering detailed findings in generated reports. We conduct experiments\non two brain CT report generation benchmarks, showing the effectiveness in\nclinical accuracy and text coherence.",
        "The pivotal shift from traditional paper-based records to sophisticated\nElectronic Health Records (EHR), enabled systematic collection and analysis of\npatient data through descriptive statistics, providing insight into patterns\nand trends across patient populations. This evolution continued toward\npredictive analytics, allowing healthcare providers to anticipate patient\noutcomes and potential complications before they occur. This progression from\nbasic digital record-keeping to sophisticated predictive modelling and digital\ntwins reflects healthcare's broader evolution toward more integrated,\npatient-centred approaches that combine data-driven insights with personalized\ncare delivery. This chapter explores the evolution and significance of\nhealthcare information systems, beginning with an examination of the\nimplementation of EHR in the UK and the USA. It provides a comprehensive\noverview of the International Classification of Diseases (ICD) system, tracing\nits development from ICD-9 to ICD-10. Central to this discussion is the\nMIMIC-III database, a landmark achievement in healthcare data sharing and\narguably the most comprehensive critical care database freely available to\nresearchers worldwide. MIMIC-III has democratized access to high-quality\nhealthcare data, enabling unprecedented opportunities for research and\nanalysis. The chapter examines its structure, clinical outcome analysis\ncapabilities, and practical applications through case studies, with a\nparticular focus on mortality and length of stay metrics, vital signs\nextraction, and ICD coding. Through detailed entity-relationship diagrams and\npractical examples, the text illustrates MIMIC's complex data structure and\ndemonstrates how different querying approaches can lead to subtly different\nresults, emphasizing the critical importance of understanding the database's\narchitecture for accurate data extraction.",
        "In recent years, AI red teaming has emerged as a practice for probing the\nsafety and security of generative AI systems. Due to the nascency of the field,\nthere are many open questions about how red teaming operations should be\nconducted. Based on our experience red teaming over 100 generative AI products\nat Microsoft, we present our internal threat model ontology and eight main\nlessons we have learned:\n  1. Understand what the system can do and where it is applied\n  2. You don't have to compute gradients to break an AI system\n  3. AI red teaming is not safety benchmarking\n  4. Automation can help cover more of the risk landscape\n  5. The human element of AI red teaming is crucial\n  6. Responsible AI harms are pervasive but difficult to measure\n  7. LLMs amplify existing security risks and introduce new ones\n  8. The work of securing AI systems will never be complete\n  By sharing these insights alongside case studies from our operations, we\noffer practical recommendations aimed at aligning red teaming efforts with real\nworld risks. We also highlight aspects of AI red teaming that we believe are\noften misunderstood and discuss open questions for the field to consider.",
        "Effective multi-user delay-constrained scheduling is crucial in various\nreal-world applications, such as instant messaging, live streaming, and data\ncenter management. In these scenarios, schedulers must make real-time decisions\nto satisfy both delay and resource constraints without prior knowledge of\nsystem dynamics, which are often time-varying and challenging to estimate.\nCurrent learning-based methods typically require interactions with actual\nsystems during the training stage, which can be difficult or impractical, as it\nis capable of significantly degrading system performance and incurring\nsubstantial service costs. To address these challenges, we propose a novel\noffline reinforcement learning-based algorithm, named \\underline{S}cheduling By\n\\underline{O}ffline Learning with \\underline{C}ritic Guidance and\n\\underline{D}iffusion Generation (SOCD), to learn efficient scheduling policies\npurely from pre-collected \\emph{offline data}. SOCD innovatively employs a\ndiffusion-based policy network, complemented by a sampling-free critic network\nfor policy guidance. By integrating the Lagrangian multiplier optimization into\nthe offline reinforcement learning, SOCD effectively trains high-quality\nconstraint-aware policies exclusively from available datasets, eliminating the\nneed for online interactions with the system. Experimental results demonstrate\nthat SOCD is resilient to various system dynamics, including partially\nobservable and large-scale environments, and delivers superior performance\ncompared to existing methods.",
        "Reinforcement Learning (RL) and Multi-Agent Reinforcement Learning (MARL)\nhave emerged as promising methodologies for addressing challenges in automated\ncyber defence (ACD). These techniques offer adaptive decision-making\ncapabilities in high-dimensional, adversarial environments. This report\nprovides a structured set of guidelines for cybersecurity professionals and\nresearchers to assess the suitability of RL and MARL for specific use cases,\nconsidering factors such as explainability, exploration needs, and the\ncomplexity of multi-agent coordination. It also discusses key algorithmic\napproaches, implementation challenges, and real-world constraints, such as data\nscarcity and adversarial interference. The report further outlines open\nresearch questions, including policy optimality, agent cooperation levels, and\nthe integration of MARL systems into operational cybersecurity frameworks. By\nbridging theoretical advancements and practical deployment, these guidelines\naim to enhance the effectiveness of AI-driven cyber defence strategies.",
        "Large Language Models (LLMs) have demonstrated remarkable text generation\ncapabilities, and recent advances in training paradigms have led to\nbreakthroughs in their reasoning performance. In this work, we investigate how\nthe reasoning effort of such models scales with problem complexity. We use the\ninfinitely scalable Tents puzzle, which has a known linear-time solution, to\nanalyze this scaling behavior. Our results show that reasoning effort scales\nwith problem size, but only up to a critical problem complexity. Beyond this\nthreshold, the reasoning effort does not continue to increase, and may even\ndecrease. This observation highlights a critical limitation in the logical\ncoherence of current LLMs as problem complexity increases, and underscores the\nneed for strategies to improve reasoning scalability. Furthermore, our results\nreveal significant performance differences between current state-of-the-art\nreasoning models when faced with increasingly complex logical puzzles.",
        "Misinformation surrounding emerging outbreaks poses a serious societal\nthreat, making robust countermeasures essential. One promising approach is\nstance detection (SD), which identifies whether social media posts support or\noppose misleading claims. In this work, we finetune classifiers on COVID-19\nmisinformation SD datasets consisting of claims and corresponding tweets.\nSpecifically, we test controllable misinformation generation (CMG) using large\nlanguage models (LLMs) as a method for data augmentation. While CMG\ndemonstrates the potential for expanding training datasets, our experiments\nreveal that performance gains over traditional augmentation methods are often\nminimal and inconsistent, primarily due to built-in safeguards within LLMs. We\nrelease our code and datasets to facilitate further research on misinformation\ndetection and generation.",
        "Put $R=\\F[[t_1, \\ldots, t_d]])$. We estimate the number of normal subgroups\nof $\\mathrm{SL}_2^1(\\F[[t_1, \\ldots, t_d]])$ for $p>2$, the number of ideals in\nthe Lie algebra $\\Lie(R)$, and the number of ideals in the associative algebra\n$R$.",
        "In Hazra, den Hollander and Parvaneh (2025) we analysed the friendship\nparadox for sparse random graphs. For four classes of random graphs we\ncharacterised the empirical distribution of the friendship biases between\nvertices and their neighbours at distance $1$, proving convergence as\n$n\\to\\infty$ to a limiting distribution, with $n$ the number of vertices, and\nidentifying moments and tail exponents of the limiting distribution. In the\npresent paper we look at the multi-level friendship bias between vertices and\ntheir neighbours at distance $k \\in \\mathbb{N}$ obtained via a $k$-step\nexploration according to a backtracking or a non-backtracking random walk. We\nidentify the limit of empirical distribution of the multi-level friendship\nbiases as $n\\to\\infty$ and\/or $k\\to\\infty$. We show that for non-backtracking\nexploration the two limits commute for a large class of sparse random graphs,\nincluding those that locally converge to a rooted Galton-Watson tree. In\nparticular, we show that the same limit arises when $k$ depends on $n$, i.e.,\n$k=k_n$, provided $\\lim_{n\\to\\infty} k_n = \\infty$ under some mild conditions.\nWe exhibit cases where the two limits do not commute and show the relevance of\nthe mixing time of the exploration.",
        "Working at the prime $2$ and chromatic height $2$, we construct a finite\nresolution of the homotopy fixed points of Morava $E$-theory with respect to\nthe subgroup $\\mathbb{G}_2^1$ of the Morava stabilizer group. This is an\nupgrade of the finite resolution of the homotopy fixed points of $E$-theory\nwith respect to the subgroup $\\mathbb{S}_2^1$ constructed in work of\nGoerss-Henn-Mahowald-Rezk, Beaudry and Bobkova-Goerss.",
        "In this work we study an optimal control problem subject to the instationary\nNavier-Stokes equations, where the control enters via an inhomogeneous\nNeumann\/Do-Nothing boundary condition. Despite the Navier-Stokes equations with\nthese boundary conditions not being well-posed for large times and\/or data, we\nobtain wellposedness of the optimal control problem by choosing a proper\ntracking type term. In order to discuss the regularity of the optimal control,\nstate and adjoint state, we present new results on $L^2(I;H^2(\\Omega))$\nregularity of solutions to a Stokes problem with mixed inhomogeneous boundary\nconditions.",
        "Systemic drug administration often causes off-target effects limiting the\nefficacy of advanced therapies. Targeted drug delivery approaches increase\nlocal drug concentrations at the diseased site while minimizing systemic drug\nexposure. We present a magnetically guided microrobotic drug delivery system\ncapable of precise navigation under physiological conditions. This platform\nintegrates a clinical electromagnetic navigation system, a custom-designed\nrelease catheter, and a dissolvable capsule for accurate therapeutic delivery.\nIn vitro tests showed precise navigation in human vasculature models, and in\nvivo experiments confirmed tracking under fluoroscopy and successful navigation\nin large animal models. The microrobot balances magnetic material\nconcentration, contrast agent loading, and therapeutic drug capacity, enabling\neffective hosting of therapeutics despite the integration complexity of its\ncomponents, offering a promising solution for precise targeted drug delivery.",
        "Pruning assumes a subnetwork exists in the original deep neural network,\nwhich can achieve comparative model performance with less computation than the\noriginal. However, it is unclear how the model performance varies with the\ndifferent subnetwork extractions. In this paper, we choose the representation\ndimension (or embedding dimension, model dimension, the dimension of the\nresidual stream in the relevant literature) as the entry point to this issue.\nWe investigate the linear transformations in the LLM transformer blocks and\nconsider a specific structured pruning approach, SliceGPT, to extract the\nsubnetworks of different representation dimensions. We mechanistically analyse\nthe activation flow during the model forward passes, and find the\nrepresentation dimension dominates the linear transformations, model\npredictions, and, finally, the model performance. Explicit analytical relations\nare given to calculate the pruned model performance (perplexity and accuracy)\nwithout actual evaluation, and are empirically validated with\nLlama-3-8B-Instruct and Phi-3-mini-4k-Instruct.",
        "To address the challenge of tractability for optimizing mathematical models\nin science and engineering, surrogate models are often employed. Recently, a\nnew class of machine learning models named Kolmogorov Arnold Networks (KANs)\nhave been proposed. It was reported that KANs can approximate a given\ninput\/output relationship with a high level of accuracy, requiring\nsignificantly fewer parameters than multilayer perceptrons. Hence, we aim to\nassess the suitability of deterministic global optimization of trained KANs by\nproposing their Mixed-Integer Nonlinear Programming (MINLP) formulation. We\nconduct extensive computational experiments for different KAN architectures.\nAdditionally, we propose alternative convex hull reformulation, local support\nand redundant constraints for the formulation aimed at improving the\neffectiveness of the MINLP formulation of the KAN. KANs demonstrate high\naccuracy while requiring relatively modest computational effort to optimize\nthem, particularly for cases with less than five inputs or outputs. For cases\nwith higher inputs or outputs, carefully considering the KAN architecture\nduring training may improve its effectiveness while optimizing over a trained\nKAN. Overall, we observe that KANs offer a promising alternative as surrogate\nmodels for deterministic global optimization.",
        "Electrooculography (EOG) is an electrophysiological signal that determines\nthe human eye orientation and is therefore widely used in Human Tracking\nInterfaces (HCI). The purpose of this project is to develop a communication\nmethod for quadriplegic patients using EOG signals aimed at text and voice\ngeneration. The system consists of 3D eye movement tracking embedded using a\ncustom-built prototype to measure the eyeball's left-right and up-down\nmovements. The ESP32 board, which has a set of parameters to convert the data\ninto content displayed on LCDs and MP3 players, is used to capture and process\nthe signal. helps people by facilitating more natural and efficient symptom\nexpression. The blink system will be able to incorporate face masks and more\neye tests as it continues to develop. Even if it might work, more research and\nclinical trials are needed to evaluate the system's usefulness and ensure that\nit performs as planned in real-world scenarios. With this project, assistive\ntechnology will make significant progress and improve the lives of many who\nsuffer from severe motor impairments.",
        "There has been a long-standing hypothesis that a software's popularity is\nrelated to its security or insecurity in both research and popular discourse.\nThere are also a few empirical studies that have examined the hypothesis,\neither explicitly or implicitly. The present work continues with and\ncontributes to this research with a replication-motivated large-scale analysis\nof software written in the PHP programming language. The dataset examined\ncontains nearly four hundred thousand open source software packages written in\nPHP. According to the results based on reported security vulnerabilities, the\nhypothesis does holds; packages having been affected by vulnerabilities over\ntheir release histories are generally more popular than packages without having\nbeen affected by a single vulnerability. With this replication results, the\npaper contributes to the efforts to strengthen the empirical knowledge base in\ncyber and software security.",
        "To the naked eye, stock prices are considered chaotic, dynamic, and\nunpredictable. Indeed, it is one of the most difficult forecasting tasks that\nhundreds of millions of retail traders and professional traders around the\nworld try to do every second even before the market opens. With recent advances\nin the development of machine learning and the amount of data the market\ngenerated over years, applying machine learning techniques such as deep\nlearning neural networks is unavoidable. In this work, we modeled the task as a\nmultivariate forecasting problem, instead of a naive autoregression problem.\nThe multivariate analysis is done using the attention mechanism via applying a\nmutated version of the Transformer, \"Stockformer\", which we created.",
        "The increasing need for sharing healthcare data and collaborating on clinical\nresearch has raised privacy concerns. Health information leakage due to\nmalicious attacks can lead to serious problems such as misdiagnoses and patient\nidentification issues. Privacy-preserving machine learning (PPML) and\nprivacy-enhancing technologies, particularly federated learning (FL), have\nemerged in recent years as innovative solutions to balance privacy protection\nwith data utility; however, they also suffer from inherent privacy\nvulnerabilities. Gradient inversion attacks constitute major threats to data\nsharing in federated learning. Researchers have proposed many defenses against\ngradient inversion attacks. However, current defense methods for healthcare\ndata lack generalizability, i.e., existing solutions may not be applicable to\ndata from a broader range of populations. In addition, most existing defense\nmethods are tested using non-healthcare data, which raises concerns about their\napplicability to real-world healthcare systems. In this study, we present a\ndefense against gradient inversion attacks in federated learning. We achieve\nthis using latent data perturbation and minimax optimization, utilizing both\ngeneral and medical image datasets. Our method is compared to two baselines,\nand the results show that our approach can outperform the baselines with a\nreduction of 12.5% in the attacker's accuracy in classifying reconstructed\nimages. The proposed method also yields an increase of over 12.4% in Mean\nSquared Error (MSE) between the original and reconstructed images at the same\nlevel of model utility of around 90% client classification accuracy. The\nresults suggest the potential of a generalizable defense for healthcare data.",
        "The Human Cognitive Simulation Framework represents a significant advancement\nin integrating human cognitive capabilities into artificial intelligence\nsystems. By merging short-term memory (conversation context), long-term memory\n(interaction context), advanced cognitive processing, and efficient knowledge\nmanagement, it ensures contextual coherence and persistent data storage,\nenhancing personalization and continuity in human-AI interactions. The\nframework employs a unified database that synchronizes these contexts while\nincorporating logical, creative, and analog processing modules inspired by\nhuman brain hemispheric functions to perform structured tasks and complex\ninferences. Dynamic knowledge updates enable real-time integration, improving\nadaptability and fostering applications in education, behavior analysis, and\nknowledge management. Despite its potential to process vast data volumes and\nenhance user experience, challenges remain in scalability, cognitive bias\nmitigation, and ethical compliance. This framework lays the foundation for\nfuture research in continuous learning algorithms, sustainability, and\nmultimodal adaptability, positioning Cognitive AI as a transformative model in\nemerging fields.",
        "Recent advances in quadrupedal robots have demonstrated impressive agility\nand the ability to traverse diverse terrains. However, hardware issues, such as\nmotor overheating or joint locking, may occur during long-distance walking or\ntraversing through rough terrains leading to locomotion failures. Although\nseveral studies have proposed fault-tolerant control methods for quadrupedal\nrobots, there are still challenges in traversing unstructured terrains. In this\npaper, we propose DreamFLEX, a robust fault-tolerant locomotion controller that\nenables a quadrupedal robot to traverse complex environments even under joint\nfailure conditions. DreamFLEX integrates an explicit failure estimation and\nmodulation network that jointly estimates the robot's joint fault vector and\nutilizes this information to adapt the locomotion pattern to faulty conditions\nin real-time, enabling quadrupedal robots to maintain stability and performance\nin rough terrains. Experimental results demonstrate that DreamFLEX outperforms\nexisting methods in both simulation and real-world scenarios, effectively\nmanaging hardware failures while maintaining robust locomotion performance.",
        "Self-Admitted Technical Debt, or SATD, is a self-admission of technical debt\npresent in a software system. To effectively manage SATD, developers need to\nestimate its priority and assess the effort required to fix the described\ntechnical debt. About a quarter of descriptions of SATD in software systems\nexpress some form of negativity or negative emotions when describing technical\ndebt. In this paper, we report on an experiment conducted with 59 respondents\nto study whether negativity expressed in the description of SATD\n\\textbf{actually} affects the prioritization of SATD. The respondents are a mix\nof professional developers and students, and in the experiment, we asked\nparticipants to prioritize four vignettes: two expressing negativity and two\nexpressing neutral sentiment. To ensure realism, vignettes were based on\nexisting SATD. We find that negativity causes between one-third and half of\ndevelopers to prioritize SATD, in which negativity is expressed as having more\npriority. Developers affected by negativity when prioritizing SATD are twice as\nlikely to increase their estimation of urgency and 1.5 times as likely to\nincrease their estimation of importance and effort for SATD compared to the\nlikelihood of decreasing these prioritization scores. Our findings show how\ndevelopers actively use negativity in SATD to determine how urgently a\nparticular instance of TD should be addressed. However, our study also\ndescribes a gap in the actions and belief of developers. Even if 33% to 50% use\nnegativity to prioritize SATD, 67% of developers believe that using negativity\nas a proxy for priority is unacceptable. Therefore, we would not recommend\nusing negativity as a proxy for priority. However, we also recognize that\ndevelopers might unavoidably express negativity when describing technical debt."
      ]
    }
  },
  {
    "id":2411.15211,
    "research_type":"applied",
    "start_id":"b9",
    "start_title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "start_abstract":"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent models, BERT is designed to pre-train deep bidirectional representations unlabeled text by jointly conditioning on both left and right context in all layers. As result, the pre-trained can be fine-tuned with just one additional output layer create state-of-the-art models wide range of tasks, such as question answering inference, without substantial task-specific architecture modifications. conceptually simple empirically powerful. It obtains results eleven natural processing including pushing GLUE score 80.5% (7.7% point absolute improvement), MultiNLI accuracy 86.7% (4.6% SQuAD v1.1 Test F1 93.2 (1.5 improvement) v2.0 83.1 (5.1 improvement).",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b6"
      ],
      "title":[
        "Solar Cells for Indoor Applications: Progress and Development"
      ],
      "abstract":[
        "The Internet of things (IoT) has been rapidly growing in the past few years. IoT connects numerous devices, such as wireless sensors, actuators, and wearable to optimize monitor daily activities. Most these devices require power microwatt range operate indoors. To this end, a self-sustainable source, photovoltaic (PV) cell, which can harvest low-intensity indoor light, is appropriate. Recently, development highly efficient PV cells for applications attracted tremendous attention. Therefore, different types materials, inorganic, dye-sensitized, organic, perovskite have employed harvesting light energy. Although considerable efforts made by researchers develop low-cost, stable, applications, Extensive investigation necessary resolve some critical issues concerning cells, environmental stability, lifetime, large-area fabrication, mechanical flexibility, production cost. address issues, systematic review aspects will be useful research community. This study discusses current status based on previous reports. First, we provided relevant background information. Then, described sources, subsequently critically reviewed reports regarding solar active materials perovskite. Finally, placed an attempt provide insight into factors needed further improve feasibility technology applications."
      ],
      "categories":[
        "astro-ph.SR"
      ]
    },
    "list":{
      "title":[
        "TESS light curves and period changes in low-mass eclipsing binary BB\n  Persei",
        "Spectroscopic study of the late B-type eclipsing binary system AR\n  Aurigae A and B: Towards clarifying the differences in atmospheric parameters\n  and chemical abundances",
        "The Nature of Classical Be Star Outbursts: A Multi-Epoch Study of the Be\n  system EPIC 202060631",
        "Pulsation Properties of Blazhko and Non-Blazhko RRab Stars",
        "Investigating Solar Wind Outflows from Open-Closed Magnetic Field\n  Structures Using Coordinated Solar Orbiter and Hinode Observations",
        "On field line slippage rates in the solar corona",
        "Preliminary results from 5 years' spectral monitoring of Antares",
        "Bursty acceleration and 3D trajectories of electrons in a solar flare",
        "The influence of chromospheric activity on line formation",
        "Localized Heating and Dynamics of the Solar Corona due to a Symbiosis of\n  Waves and Reconnection",
        "Forecasting the 8 April 2024 Total Solar Eclipse with Multiple Solar\n  Photospheric Magnetograms",
        "Solar oblateness & asphericities temporal variations: outstanding some\n  unsolved issues",
        "Tests and calibrations of stellar models with two triply eclipsing\n  triple systems",
        "The MAGPI Survey: the subtle role of environment and not-so-subtle\n  impact of generations of stars on galaxy dynamics",
        "Multiple Populations of the Large Magellanic Cloud Globular Cluster NGC\n  2257: No Major Environmental Effect on the Formation of Multiple Populations\n  of the Old Globular Clusters in Large Magellanic Cloud",
        "Noncommutative Phantom BTZ Black Hole",
        "Spectral synthesis in multidimensional Fourier algebras",
        "Chiral Vibrational Modes in Small Molecules",
        "Stability of oscillations in the spatially extended May-Leonard model",
        "Zero modes and Dirac-(logarithmic) Sobolev-type inequalities",
        "Rigorous expansions of modular forms at CM points, I: Denominators",
        "Phonon anomalies within the polar charge density wave phase of\n  superconductor Mo$_3$Al$_2$C with structural chirality",
        "Tuning Quantum States at Chirality-Reversed Planar Interface in Weyl\n  Semimetals using an Interstitial Layer",
        "Using Matrix-Free Tensor-Network Optimizations to Construct a\n  Reduced-Scaling and Robust Second-Order M{\\o}ller-Plesset Theory",
        "A filtered two-step variational integrator for charged-particle dynamics\n  in a normal or strong magnetic field",
        "Distribution and Moments of a Normalized Dissimilarity Ratio for two\n  Correlated Gamma Variables",
        "Prediction for close approaches with terrestrial planets of asteroids\n  from the main belt",
        "Infrared Metaplasmonics"
      ],
      "abstract":[
        "We present a detailed analysis of the low-mass detached eclipsing binary\nsystem BB Persei, which contains two K-type stars in a circular orbit with a\nshort period of 0.4856 d. We used light curves from the Transiting Exoplanet\nSurvey Satellite, which observed BB Per in five sectors, to determine its\nphotometric properties and a precise orbital ephemeris. The solution of the\nTESS light curve in Phoebe results in a detached configuration, where the\ntemperature of the primary component was fixed to $T_1 = 5~300$ K according to\nLamost, which gives us $T_2 = 5~050 \\pm 50$ K for the secondary. The spectral\ntype of the primary component was derived as K0 and the photometric mass ratio\nwas estimated $q = 0.90$. Slow period changes on the current O-C diagram\nspanning the past 25 years indicate the presence of a third body orbiting the\neclipsing pair with an orbital period of about 22 years. The companion could be\na red dwarf of spectral type M6 - M7 with a minimal mass of about 0.1\nM$_{\\odot}$. The characteristics and temporal variation of the dark region on\nthe surface of the secondary component were estimated.",
        "AR Aur A+B is a close binary of astrophysical interest, because dissimilar\nsurface compositions are reported between similar late B-type dwarfs. A new\nspectroscopic study on this system was carried out based on the disentangled\nspectra, in order to determine their atmospheric parameters and elemental\nabundances, The effective temperature and microturbulence (determined from the\nequivalent widths of Fe II lines) turned out (11150K, 0.9km\/s) and (10650K,\n0.1km\/s) for A and B. The chemical abundances of 28 elements were derived while\ntaking into account the non-LTE effect for Z<=15 elements (Z: atomic number).\nThe following trends were elucidated for [X\/H] (abundance of X relative to the\nSun): (1) Qualitatively, [X\/H] shows a rough global tendency of increasing with\nZ, with the gradient steeper for A than for B. (2) However, considerable\ndispersion is involved for A, since prominently large peculiarities are seen in\nspecific elements reflecting the characteristics of HgMn stars (e.g., very\ndeficient N, Al, Sc, Ni; markedly overabundant P, Mn). (3) In contrast, the\nZ-dependence of [X\/H] for B tends to be nearly linear with only a small\ndispersion. These observational facts may serve as a key to understanding the\ncritical condition for the emergence of chemical anomaly.",
        "Through cross-matching the ASASSN photometric light curves and LAMOST\nspectroscopic observations, we serendipitously captured a rare major outburst\nevent from the Be star EPIC 202060631, lasting over 1000 days. Fortuitously,\nthe LAMOST ow-resolution spectra densely covered the flux rising stage with 20\nepochs, while an additional 7 low-resolution and 11 medium esolution spectra\nmonitored the subsequent decay phase. Moreover, this target was observed by\nKepler telescope in its K2 mission when before the outburst and by TESS\ntelescope while after returning to quiescence. Analyses of these datasets\nreveal pulsation behavior and mode amplitudes, indications of radial and\ntangential motions in the photosphere, and clear evidence of mass ejection and\ncircumstellar disk formation. We modeled the central star using the BRUCE04\ncode and analyzed the disk structure using HDUST. Our results suggest episodic\nmass injection events from the central star triggered the disk buildup and\ncarried imprints of the changing stellar pulsations. This study offers unique\ninsights into the connections between photospheric activities, disk evolution,\nand stellar rotation during Be star outbursts.",
        "In this study, we conduct a comparative analysis of the properties of Blazhko\nand non-Blazhko RRab stars. We identified 1054 non-Blazhko and 785 Blazhko RRab\nstars in the photometric data observed by K2 mission, which, combined with\nthose 37 stars observed in the original Kepler field, constituted our study\nsample. Using the Fourier Decomposition method, we calculated the pulsation\nparameters, including phase differences and amplitude ratios, for these RRab\nstars, revealing significant discrepancies in the pulsation parameters between\nBlazhko and non-Blazhko RRab stars. However, distinguishing between Blazhko and\nNon-Blazhko RRab stars based on Fourier parameters remains challenging due to\nthe significant overlap in their distributions. By cross-matching our sample\nwith the LRS of LAMOST DR12, we identified 147 Blazhko and 111 non-Blazhko RRab\nstars, which exhibit similar metallicity distributions. Furthermore,\ncross-matching with Gaia DR3 data yielded 766 Blazhko and 950 non-Blazhko RRab\nstars, showing differences in color indices but not in absolute magnitudes. Our\nfindings suggested the Blazhko effect is linked to pulsation parameters and\ncolors, rather than metallicities or absolute magnitude.",
        "ESA\/NASA's Solar Orbiter (SO) allows us to study the solar corona at closer\ndistances and from different perspectives, which helps us to gain significant\ninsights into the origins of the solar wind. In this work, we present the\nanalysis of solar wind outflows from two locations: a narrow open-field\ncorridor and a small, mid-latitude coronal hole. These outflows were observed\noff-limb by the Metis coronagraph onboard SO and on-disk by the Extreme\nUltraviolet Imaging Spectrometer (EIS) onboard Hinode. Magnetic field\nextrapolations suggest that the upflow regions seen in EIS were the sources of\nthe outflowing solar wind observed with Metis. We find that the plasma\nassociated with the narrow open-field corridor has higher electron densities\nand lower outflow velocities compared to the coronal hole plasma in the middle\ncorona, even though the plasma properties of the two source regions in the low\ncorona are found to be relatively similar. The speed of solar wind from the\nopen-field corridor also shows no correlation with the magnetic field expansion\nfactor, unlike the coronal hole. These pronounced differences at higher\naltitudes may arise from the dynamic nature of the low-middle corona, in which\nreconnection can readily occur and may play an important role in driving solar\nwind variability.",
        "Magnetic reconnection is one of the fundamental dynamical processes in the\nsolar corona. The method of studying reconnection in active region-scale\nmagnetic fields generally depends on non-local methods (i.e. requiring\ninformation across the magnetic field under study) of magnetic topology, such\nas separatrix skeletons and quasi-separatrix layers. The theory of General\nMagnetic Reconnection is also non-local, in that its measure of the\nreconnection rate depends on determining the maxima of integrals along field\nlines. In this work, we complement the above approaches by introducing a local\ntheory of magnetic reconnection, that is one in which information about\nreconnection at a particular location depends only on quantities at that\nlocation. The theory connects the concept of the field line slippage rate,\nrelative to ideal motion, to the underlying local geometry of the magnetic\nfield characterized in terms of the Lorentz force and field-aligned current\ndensity. It is argued that the dominant non-ideal term for the solar corona,\ndiscussed in relation to this new theory, is mathematically equivalent to the\nanomalous resistivity employed by many magnetohrdrodynamic simulations.\nHowever, the general application of the theory is adaptable to the inclusion of\nother non-ideal terms, which may arise from turbulence modelling or the\ninclusion of a generalized Ohm's law. The theory is illustrated with two\nexamples of coronal magnetic fields related to flux ropes: an analytical model\nand a nonlinear force-free extrapolation. In terms of the latter, the slippage\nrate corresponds to the reconnection which would happen if the given (static)\nforce-free equilibrium were the instantaneous form of the magnetic field\ngoverned by an Ohm's law with non-ideal terms.",
        "We present preliminary results of 5 years' monitoring of the radial velocity\nof Alpha Sco, performed at the Astronomical Observatory of the Universidad de\nLos Andes in Bogot\\'a, Colombia. The data include 580 spectra acquired on 153\nnights between March 2015 and March 2020. The aim of this study is to probe the\ndynamics of the star's atmosphere on all possible time-scales through the\nvariations in observed radial velocity. At present, our findings are consistent\nwith previous results from other observers, and the combination of older and\nnew data make it possible to assess the several periodicities. A detailed study\nof these results, including the convective motions in the photosphere, is still\nin progress.",
        "During a solar flare, electrons are accelerated to non-thermal energies as a\nresult of magnetic reconnection. These electrons then propagate upwards and\ndownwards from the energy release site along magnetic field lines and produce\nradio and X-ray emission. On 11 November 2022, an M5.1 solar flare was observed\nby the Spectrometer\/Telescope for Imaging X-rays (STIX) on board Solar Orbiter\ntogether with various ground- and space-based radio instruments. The flare was\nassociated with several fine hard X-ray (HXR) structures and a complex set of\nmetric radio bursts (type III, J, and narrowband). By studying the evolution of\nX-ray, extreme ultraviolet, and radio sources, we aim to study the trajectories\nof the flare-accelerated electrons in the lower solar atmosphere and low\ncorona. We used observations from the STIX on board Solar Orbiter to study the\nevolution of X-ray sources. Using radio imaging from the Nan\\c{c}ay Radio\nheliograph (NRH) and the Newkirk density model, we constructed 3D trajectories\nof 14 radio bursts. Imaging of the HXR fine structures shows several sources at\ndifferent times. The STIX and NRH imaging shows correlated changes in the\nlocation of the HXR and radio source at the highest frequency during the most\nintense impulsive period. Imaging and 3D trajectories of all the bursts show\nthat electrons are getting accelerated at different locations and along several\ndistinct field lines. The longitude and latitude extent of the trajectories are\n~30 arcsec and ~ 152 arcsec. We find that the electrons producing HXR and radio\nemission have similar acceleration origins. Importantly, our study supports the\nscenario that the flare acceleration process is temporally and spatially\nfragmentary, and during each of these small-scale processes, the electron beams\nare injected into a very fibrous environment and produce complex HXR and radio\nemission.",
        "One of the primary sources of stellar spectral variability is magnetic\nactivity. While our current understanding of chromospheric activity is largely\nderived from specific lines sensitive to chromospheric heating, such as the Ca\nII HK doublet, previous observational studies have shown that other spectral\nlines are also affected. To investigate the influence of activity on line\nformation in greater detail, we constructed a set of stellar models for\nhypothetical G2 dwarf stars with varying levels of activity and calculated\ntheir synthetic spectra. A comparison of these spectra revealed two spectral\nregions most significantly impacted by activity: approximately 3300-4400 A and\n5250-5500 A. By calculating the total contribution function of the lines, we\ndetermined that the emergence of a secondary chromospheric contribution to line\nformation is the primary mechanism driving these changes. Based on our\ncalculations and analysis, we compiled a list of transition lines and their\ncorresponding changes due to chromospheric activity. This list could serve as a\nvaluable tool for selecting spectral lines applicable to a wide range of\nastrophysical studies.",
        "The Sun's outer atmosphere, the corona, is maintained at mega-Kelvin\ntemperatures and fills the heliosphere with a supersonic outflowing wind. The\ndissipation of magnetic waves and direct electric currents are likely to be the\nmost significant processes for heating the corona, but a lively debate exists\non their relative roles. Here, we suggest that the two are often intrinsically\nlinked, since magnetic waves may trigger current dissipation, and impulsive\nreconnection can launch magnetic waves. We present a study of the first of\nthese processes by using a 2D physics-based numerical simulation using the\nAdaptive Mesh Refined (AMR) Versatile Advection Code (VAC). Magnetic waves such\nas fast magnetoacoustic waves are often observed to propagate in the\nlarge-scale corona and interact with local magnetic structures. The present\nnumerical simulations show how the propagation of magnetic disturbances towards\na null point or separator can lead to the accumulation of the electric\ncurrents. Lorentz forces can laterally push and vertically stretch the magnetic\nfields, forming a current sheet with a strong magnetic-field gradient. The\nmagnetic field lines then break and reconnect, and so contribute towards\ncoronal heating. Numerical results are presented that support these ideas and\nsupport the concept of a symbiosis between waves and reconnection in heating\nthe solar corona.",
        "The 8 April 2024 total solar eclipse (TSE) provides a unique opportunity to\nstudy the solar corona. This work presents our prediction of the solar corona\nat the time of the eclipse based on magnetohydrodynamic (MHD) modeling\nperformed with the Alfv\\'{e}n Wave Solar Model-Realtime (AWSoM-R) in the Space\nWeather Modeling Framework, developed at the University of Michigan. We\nperformed multiple simulations made with data input in the form of synchronic\nmagnetograms from four sources, i.e., ADAPT-GONG, Lockheed Martin ESFAM, HipFT\nand NSO-NRT magnetograms. Simulations also include a higher-resolution model\nand a post-eclipse model incorporating newly emerged active regions. Our study\nfundamentally focuses on the limitations imposed by the lack of global solar\nobservations, particularly on how these limitations affect coronal simulations.\nSpecifically, we examine how differences among the magnetograms and the absence\nof observations from the east limb, due to the Sun's rotation, impact the\naccuracy of the predicted coronal structures. We synthesized a variety of\nrepresentative observables, including the white-light and extreme-ultraviolet\nimages from each model, and compared them with observations. The synthesized\nobservables show remarkable differences because of the distinct magnetic\ncoronal topologies, which stem from the varied magnetic flux distributions and\nthe gaps in observational coverage. Our findings emphasize the need for\ncomprehensive and multi-satellite magnetic field observations to improve future\nsolar corona predictions.",
        "Solar oblateness has been the subject of several studies dating back to the\nnineteenth century. Despite diffculties, both theoretical and observational,\ntangible results have been achieved. However, variability of the solar\noblateness with time is still poorly known. How the solar shape evolves with\nthe solar cycle has been a challenging problem. Analysis of the helioseismic\ndata, which are the most accurate measure of the solar structure up to now,\nleads to the determination of asphericity coeffcients which have been found to\nchange with time. We show here that by inverting even coeffcients of f-mode\noscillation frequency splitting to obtain the oblateness magnitude and its\ntemporal dependence can be inferred. It is found that the oblateness variations\nlag the solar activity cycles by about 3 years. A major change occurred between\nsolar cycles 23 and 24 is that the oblateness was greater in cycle 24 despite\nthe lower solar activity level. Such results may help to better understand the\nnear-subsurface layers as they strongly impacts the internal dynamics of the\nSun and may induce instabilities driving the transport of angular momentum.",
        "We investigated the possibility of using two recently characterised triply\neclipsing triple systems to constrain stellar model parameters. We specifically\nfocused on evaluating the influence of the underlying astrophysical assumptions\nemployed in the characterisation of the system to fix absolute values of the\nradii, effective temperatures, and metallicity. We used dense grids of\npre-computed stellar models to fit the data for the triply eclipsing systems\nwith a modified version of the SCEPtER pipeline. We achieve an excellent\nagreement with observational data for TIC 650024463, which comprises three\nlow-mass main-sequence (MS) stars. We find it has an age of $9.0^{+1.4}_{-1.1}$\nGyr and a multimodal posterior density. Characterising TIC 323486857 proved\nmore challenging. This system comprises two intermediate-mass MS stars and a\nslightly more massive tertiary in the red giant branch phase. For this last\nsystem we tested alternative scenarios for convective core overshooting. When\nall stars were assumed to have the same overshooting efficiency, significant\ndiscrepancies arose with the observed data for the tertiary star. This\ndiscrepancy may arise from the different assumptions regarding overshooting\nefficiency made for the observational characterisation of the system, in which\nan increasing overshooting efficiency with stellar mass was adopted. By\nallowing independent overshooting efficiencies for all stars, we recovered a\nsolution close to that adopted in the system observational characterisation.\nEncouragingly, despite the relevant differences between the adopted stellar\nmodels and those used for the observational characterisation, we found a system\nage of $2.33^{+0.18}_{-0.16}$ Gyr in all the tested scenarios, and this age is\nin agreement with independent determinations.",
        "The stellar age and mass of galaxies have been suggested as the primary\ndeterminants for the dynamical state of galaxies, with environment seemingly\nplaying no or only a very minor role. We use a sample of 77 galaxies at\nintermediate redshift (z~0.3) in the Middle-Ages Galaxies Properties with\nIntegral field spectroscopy (MAGPI) Survey to study the subtle impact of\nenvironment on galaxy dynamics. We use a combination of statistical techniques\n(simple and partial correlations and principal component analysis) to isolate\nthe contribution of environment on galaxy dynamics, while explicitly accounting\nfor known factors such as stellar age, star formation histories and stellar\nmasses. We consider these dynamical parameters: high-order kinematics of the\nline-of-sight velocity distribution (parametrised by the Gauss-Hermite\ncoefficients $h_3$ and $h_4$), kinematic asymmetries $V_{\\rm asym}$ derived\nusing kinemetry and the observational spin parameter proxy $\\lambda_{R_e}$. Of\nthese, the mean $h_4$ is the only parameter found to have a significant\ncorrelation with environment as parametrised by group dynamical mass. This\ncorrelation exists even after accounting for age and stellar mass trends.\nFinally, we confirm that variations in the spin parameter $\\lambda_{R_e}$ are\nmost strongly (anti-)correlated with age as seen in local studies, and show\nthat this dependence is well-established by z~0.3.",
        "How the environment of the host galaxy affects the formation of multiple\npopulations (MPs) in globular clusters (GCs) is one of the outstanding\nquestions in the near-field cosmology. To understand the true nature of the old\nGC MPs in the Large Magellanic Cloud (LMC), we study the Ca--CN--CH photometry\nof the old metal-poor LMC GC NGC 2257. We find the predominantly FG-dominated\npopulational number ratio of $n$(FG):$n$(SG) = 61:39($\\pm$4), where the FG and\nSG denote the first and second generations. Both the FG and SG have similar\ncumulative radial distributions, consistent with the idea that NGC 2257 is\ndynamically old. We obtain [Fe\/H] = $-$1.78$\\pm$0.00 dex($\\sigma$=0.05 dex) and\nour metallicity is $\\sim$0.2 dex larger than that from the high-resolution\nspectroscopy by other, due to their significantly lower temperatures by $\\sim$\n$-$200 K. The NGC 2257 FG shows a somewhat larger metallicity variation than\nthe SG, the first detection of such phenomenon in an old LMC GC, similar to\nGalactic GCs with MPs, strongly suggesting that it is a general characteristic\nof GCs with MPs. Interestingly, the NGC 2257 SG does not show a helium\nenhancement compared to the FG. Our results for the Galactic normal GCs exhibit\nthat the degree of carbon and nitrogen variations are tightly correlated with\nthe GC mass, while NGC 2257 exhibits slightly smaller variations for its mass.\nWe show that old LMC GCs follow the same trends as the Galactic normal GCs in\nthe $\\Delta$W$_{\\rm CF336W,F438W,F814W}$, $N_{\\rm FG}\/N_{\\rm tot}$, and $\\log\nM\/M_{\\rm \\odot}$ domains. Our result indicates that the environment of the host\ngalaxy did not play a major role in the formation and evolution of GC MPs.",
        "This work explores the thermodynamic and geometric properties of phantom BTZ\nblack holes within the framework of noncommutative spacetime, where\nnoncommutative effects are incorporated via Lorentzian distributions for mass\nand charge. The resulting modifications in spacetime geometry introduce\nsignificant alterations to horizon structures and curvature singularities. A\ncomprehensive and comparative thermodynamic analysis is conducted, examining\nthe differences between phantom and ordinary matter cases. This includes an\ninvestigation of Hawking temperature, entropy, heat capacity, and stability\ncriteria. Additionally, the black hole is analyzed as a thermodynamic heat\nengine, with its efficiency evaluated as a function of noncommutative\nparameters. Our findings highlight the profound impact of noncommutativity on\nthe thermodynamic behavior and efficiency of phantom BTZ black holes, revealing\nnew insights into the interplay between quantum spacetime effects and exotic\nfield dynamics. The results indicate that noncommutative corrections not only\nmodify the stability conditions of these black holes but also play a crucial\nrole in governing phase transitions. Furthermore, we demonstrate that\nnoncommutativity influences energy extraction processes, refining our\nunderstanding of black hole thermodynamics in lower-dimensional spacetimes and\ndistinguishing the behavior of phantom and ordinary matter cases.",
        "Let $G$ be a locally compact group and let $A^n(G)$ denote the\n$n$-dimensional Fourier algebra, introduced by Todorov and Turowska. We\ninvestigate spectral synthesis properties of the multidimensional Fourier\nalgebra $A^n(G).$ In particular, we prove versions of the subgroup lemma,\ninjection, and inverse projection theorems for both spectral sets and Ditkin\nsets. Additionally, we provide a result on the parallel synthesis between\n$A^n(G)$ and $A^{n+1}(G)$ and finally prove Malliavin's theorem.",
        "The development of quantitative methods for characterizing molecular\nchirality can provide an important tool for studying chirality induced\nphenomena in molecular systems. Significant progress has been made in recent\nyears toward understanding the chirality of molecular normal vibrational modes,\nmostly focusing on vibrations of helical molecular structures. In the present\nstudy, we examine the applicability two methodologies previously used for\nhelical structures for the quantification of the chirality of molecular normal\nmodes across a range of small, not necessarily helical, molecules. The first\napproach involves the application of the Continuous Chirality Measure (CCM) to\neach normal mode by associating the mode with a structure formed by imposing\nthe corresponding motion about a common origin. The second approach assigns to\neach normal mode a pseudoscalar defined as the product of atomic linear and\nangular momentum summed over all atoms. In particular, using the CCM also as a\nmeasure of the chirality of the underlying molecular structure, we establish\nthe existence of correlation between the chirality of molecular normal modes\nand that of the underlying molecular structure. Furthermore, we find that\nnormal modes associated with different frequency ranges of the molecular\nvibrational spectrum exhibit distinct handedness behavior.",
        "The May-Leonard model for three competing species, symmetric with respect to\n  cyclic permutation of the variables and extended by diffusive terms, is\nconsidered.\n  Exact time-periodic solutions of the system have been found, and their\nstability\n  with respect to spatially periodic disturbances is studied. The stability of\nsolu tions with respect to longwave spatial modulations is revealed. A period\ndoubling\n  instability breaking the spatial uniformity is found.",
        "We study the decay rate of the zero modes of the Dirac operator with a\nmatrix-valued potential that is considered here without any regularity\nassumptions, compared to the existing literature. For the Dirac operator and\nfor Clifford-valued functions we prove the $L^p$-$L^2$ Dirac Sobolev inequality\nwith explicit constant, as well as the $L^p$-$L^q$ Dirac-Sobolev inequalities.\nWe prove its logarithmic counterpart for $q=2$, extending it to its Gaussian\nversion of Gross, as well as show Nash and Poincar\\'e inequalities in this\nsetting, with explicit values for constants.",
        "We describe an algorithm to rigorously compute the power series expansion at\na CM point of a weight $2$ cusp form of level coprime to $6$. Our algorithm\nworks by bounding the denominators that appear due to ramification, and without\nrecourse to computing an explicit model of the corresponding modular curve. Our\nresult is the first in a series of papers toward an eventual implementation of\nequationless Chabauty.",
        "We employ polarization-resolved Raman spectroscopy to study the lattice\ndynamics of the polar charge density wave phase of the superconductor\nMo$_3$Al$_2$C with structural chirality. We show the phononic signatures of the\ncharge density wave transition at $T^*$=155K in Mo$_3$Al$_2$C. The detailed\ntemperature dependence of these phonon modes' frequency,\nhalf-width-at-half-maximum, and the integrated area below $T^*$ reveal\nanomalies at an intermediate temperature $T'\\sim$100K, especially for the\nlow-energy modes at 130cm$^{-1}$ and 180cm$^{-1}$. Since these low-energy modes\nare dominated by Mo-related lattice vibration, we propose that lattice\nanomalies at $T'$ within the charge density wave phase are related to a\nmodification of the Mo displacements while preserving the crystal symmetry.",
        "The electronic band structure of Weyl semimetals possesses pairs of linear\nband crossings, called Weyl nodes, characterized by opposite chirality charges\nassociated with each node. The momentum space position of the nodes can reverse\nacross a planar interface and these host Fermi-arc-like bound states, in\naddition to scattering states. We show that a magnetic interstitial layer can\ntune these states in three distinct ways. The electrostatic potential and one\nof the in-plane magnetic potential components control the shape of the bound\nstate Fermi-arcs. For moderate values of the same in-plane magnetic potential\nelectrons are spin-filtered across the interface, while both the in-plane\nmagnetic components and the electrostatic potential control the transmission of\nelectrons. The ratio of in-plane to out-of-plane magnetic components can be\nused to turn on or turn off the magnetic potential effects, since the latter\ndoes not affect the interface states. The tunability arises from spin-momentum\nlocking and chirality reversal at the interface. Thus, the effects can mix or\ninterchange depending on the specific material but the states will remain\ntunable.",
        "We investigate the application of the canonical polyadic decomposition (CPD)\nto the tensor hypercontraction (THC) and Laplace transform (LT) approximated\nsecond-order M{\\o}ller-Plesset (MP2) method. By introducing these\ndecompositions we formally reduce the scaling of the canonical MP2 method from\n$\\mathcal{O}(N^5)$ to $\\mathcal{O}(N^3)$ and the storage complexity from\n$\\mathcal{O}(N^4)$ to $\\mathcal{O}(N^2)$. We are able to construct the THC\nrepresentation in $\\mathcal{O}(N^3)$ time by employing the interpolative\nseparable density fitting decomposition strategy. Furthermore, we introduce a\nCPD optimization strategy that takes advantage of the THC representation to\ndecompose the order-four two-electron integral tensor with a computational\nscaling of $\\mathcal{O}(N^3)$. Finally, we show that the rank of the CPD in the\napproximation of MP2 scales linearly with system size and that this CPD-ISDF-LT\nMP2 strategy realizes a performance advantage over canonical LT MP2 in both\ncomputational wall-times and memory resource requirements.",
        "This article is concerned with a new filtered two-step variational integrator\nfor solving the charged-particle dynamics in a mildly non-homogeneous normal or\nstrong magnetic field with a dimensionless parameter $\\epsilon$ inversely\nproportional to the strength of the magnetic field. In the case of a normal\nmagnetic field ($\\epsilon \\approx 1$), second-order error bounds and long time\nenergy and momentum conservations are obtained. Moreover, the proof of the\nlong-term analysis is accomplished by the backward error analysis. For the\nstrong magnetic field ($0<\\epsilon \\ll1$), this paper clarifies the behaviour\nof the filtered variational integrator for both a large stepsize $h^2 \\geq\n\\epsilon$ and a smaller stepsize $ h \\sim \\epsilon$. The approach to analysing\nthe error bounds for these two stepsizes is based on comparing the modulated\nFourier expansions of the exact and the numerical solutions. It is shown that\nthe proposed integrator achieves a second-order accuracy $\\mathcal{O}(h^2)$ in\nthe position and in the parallel velocity for a large step size and an\n$\\mathcal{O}(\\epsilon)$ accuracy for a smaller stepsize. This paper also yields\nthe long time energy and magnetic moment conservations for the strong magnetic\nfield by developing the modulated Fourier expansion of the proposed scheme. All\nthe theoretical results of the error behaviour and long-term conservations are\nnumerically demonstrated by two numerical experiments.",
        "We consider two random variables $X$ and $Y$ following correlated Gamma\ndistributions, characterized by identical scale and shape parameters and a\nlinear correlation coefficient $\\rho$. Our focus is on the parameter: \\[\n  D(X,Y) = \\frac{|X - Y|}{X + Y}, \\] which appears in applied contexts such as\ndynamic speckle imaging, where it is known as the \\textit{Fujii index}. In this\nwork, we derive a closed-form expression for the probability density function\nof $D(X,Y)$ as well as analytical formulas for its moments of order $k$. Our\nderivation starts by representing $X$ and $Y$ as two correlated exponential\nrandom variables, obtained from the squared magnitudes of circular complex\nGaussian variables. By considering the sum of $k$ independent exponential\nvariables, we then derive the joint density of $(X,Y)$ when $X$ and $Y$ are two\ncorrelated Gamma variables. Through appropriate varable transformations, we\nobtain the theoretical distribution of $D(X,Y)$ and evaluate its moments\nanalytically. These theoretical findings are validated through numerical\nsimulations, with particular attention to two specific cases: zero correlation\nand unit shape parameter.",
        "Potentially Hazardous Asteroids (PHAs), a special subset of Near-Earth\nObjects, are both dangerous and scientifically valuable. PHAs that truly\nundergo close approaches with the Earth (dubbed CAPHAs) are of particular\ninterest and extensively studied. The concept and study of CAPHA can be\nextended to other Solar system planets, which have significant implications for\nfuture planet-based observations and explorations. In this work, we conduct\nnumerical simulations that incorporate the Yarkovsky effect to study the\ntransformation of main belt asteroids into CAPHAs of terrestrial planets, using\nprecise nominal timesteps, especially to ensure the reliability of the results\nfor Mercury and Venus. Our simulations predict a total of 1893 Mercury-CAPHAs,\n3014 Venus-CAPHAs, 3791 Earth-CAPHAs and 18066 Mars-CAPHAs, with an occurrence\nfrequency of about 1, 9, 15 and 66 per year, respectively. The values for\nMars-CAPHAs are consistent with our previous work, which were based on\nsimulations with a larger nominal timestep. The predicted occurrence frequency\nand velocity distribution of Earth-CAPHAs are in reasonable agreement with the\nobserved population of Earth-CAPHAs. We also find that certain asteroids can be\ncaught in close approach with different planets at different times, raising an\ninteresting possibility of using them as transportation between terrestrial\nplanets in the future.",
        "Plasmonic response in metals, defined as the ability to support subwavelength\nconfinement of surface plasmon modes, is typically limited to a narrow\nfrequency range below the metals' plasma frequency. This places severe\nlimitations on the operational wavelengths of plasmonic materials and devices.\nHowever, when the volume of a metal film is massively decreased, highly\nconfined quasi-two-dimensional surface plasmon modes can be supported out to\nwavelengths well beyond the plasma wavelength. While this has, thus far, been\nachieved using ultra-thin (nm-scale) metals, such films are quite difficult to\nrealize, and suffer from even higher losses than bulk plasmonic films. To\nextend the plasmonic response to the infrared, here we introduce the concept of\nmetaplasmonics, representing a novel plasmonic modality with a host of\nappealing properties. By fabricating and characterizing a series of\nmetaplasmonic nanoribbons, we demonstrate large confinement, high quality\nfactors, and large near-field enhancements across a broad wavelength range,\nextending well beyond the limited bandwidth of traditional plasmonic materials.\nWe demonstrate $35\\times$ plasmon wavelength reduction, and our numerical\nsimulations suggest that further wavelength reduction, up to a factor of 150,\nis achievable using our approach. The demonstration of the metaplasmonics\nparadigm offers a promising path to fill the near- and mid-infrared\ntechnological gap for high quality plasmonic materials, and provides a new\nmaterial system to study the effects of extreme plasmonic confinement for\napplications in nonlinear and quantum plasmonics."
      ]
    }
  },
  {
    "id":2412.00129,
    "research_type":"applied",
    "start_id":"b23",
    "start_title":"DARWIN Series: Domain Specific Large Language Models for Natural Science",
    "start_abstract":"Emerging tools bring forth fresh approaches to work, and the field of natural science is no different. In science, traditional manual, serial, labour-intensive work being augmented by automated, parallel, iterative processes driven artificial intelligence-based experimental automation more. To add new capabilities in enabling acceleration enrichment discovery process, we present DARWIN, a series tailored LLMs for mainly physics, chemistry, material science. This relies on open-source LLM, incorporating structured unstructured scientific knowledge from public datasets literature. We fine-tuned models using over 60,000 instruction data points, emphasizing factual correctness. During fine-tuning, introduce Scientific Instruction Generation (SIG) model, automating generation texts. eliminates need manual extraction or domain-specific graphs efficiently injects into model. also explore multi-task training strategies, revealing interconnections between tasks. DARWIN not only achieves state-of-the-art results various tasks but diminishes reliance closed-source AI models. Our research showcases ability LLM domain, with overarching goal fostering prosperity within broader community.",
    "start_categories":[
      "cs.CL"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b29"
      ],
      "title":[
        "Study of Fermion pair production in e+e- collisions at 130-183 GeV"
      ],
      "abstract":[
        "The cross sections and forward-backward asymmetries of hadronic and leptonic\nevents produced in e+e- collisions at centre-of-mass energies of 130-183 GeV\nare presented. Results for ee, mumu, tautau, qq, bb and cc production show no\nsignificant deviation from the Standard Model predictions. This enable\nconstraints to be set upon physics beyond the Standard Model such as\nfour-fermion contact interactions, leptoquarks, Z' bosons and R-parity\nviolating squarks and sneutrinos. Limits on the energy scale Lambda of eeff\ncontact interactions are typically in the range from 2-10 TeV. Limits on\nR-parity violating sneutrinos reach masses of a few hundred GeV for large\nvalues of their Yukawa couplings."
      ],
      "categories":[
        "astro-ph.HE"
      ]
    },
    "list":{
      "title":[
        "Estimating the Sensitivity of IceCube-Gen2 to Cosmic Ray Mass Separation",
        "Cascaded Gamma-ray Emission Associated with the KM3NeT Ultra-High-Energy\n  Event KM3-230213A",
        "Collapse of Rotating White Dwarfs and Multimessenger Signals",
        "KM3NeT Constraint on Lorentz-Violating Superluminal Neutrino Velocity",
        "Revisiting the fundamental parameters for the black hole X-ray transient\n  Swift J1753.5-0127",
        "Quantitative exploration of the similarity of gamma-ray pulsar light\n  curves",
        "Photon-ALP beam propagation from Mrk 501",
        "Search for neutrino doublets and triplets using 11.4 years of IceCube\n  data",
        "Modulation of X-ray flux by obscuration of neutron star boundary layer",
        "New insight into the Rapid Burster by Insight-HXMT",
        "A Study of thin relativistic magnetic accretion disk around a distorted\n  black hole",
        "Pattern and Origin for the Extreme $\\gamma$-ray Flares of 3C 454.3 and\n  3C 279: An Astrophysical Critical Damper?",
        "Variabilities driven by satellite black hole migration in AGN discs",
        "Non-Affine Extensions of the Raychaudhuri Equation in the K-essence\n  Framework",
        "Time-series attribution maps with regularized contrastive learning",
        "Ion-kinetic-energy sampling in a 22-pole trap using ring-electrode\n  evaporation",
        "Improved Quantum Computation using Operator Backpropagation",
        "Reservoir Computing and Photoelectrochemical Sensors: A Marriage of\n  Convenience",
        "On the localization length of finite-volume random block Schr\\\"odinger\n  operators",
        "Theory of neutrino slow flavor evolution. Part II. Space-time evolution\n  of linear instabilities",
        "Thermodynamic analysis and shadow bound of black holes surrounded by a\n  dark matter halo",
        "Canonical forms of oriented matroids",
        "Beyond surveys: A High-Precision Wealth Inequality Mapping of China's\n  Rural Households Derived from Satellite and Street View Imageries",
        "More on the corner-vector construction for spherical designs",
        "Enabling GPU Portability into the Numba-JITed Monte Carlo Particle\n  Transport Code MC\/DC",
        "BCAT: A Block Causal Transformer for PDE Foundation Models for Fluid\n  Dynamics",
        "A new practical and effective source-independent full-waveform inversion\n  with a velocity-distribution supported deep image prior: Applications to two\n  real datasets",
        "Particle-based plasma simulation using a graph neural network"
      ],
      "abstract":[
        "IceCube-Gen2 is a proposed extension to the existing IceCube Neutrino\nObservatory at the South Pole. It will consist of three components: an in-ice\noptical array, a surface array on top of the optical array, and a radio array\nfor detecting ultra-high energy neutrinos. Here we study the sensitivity of\nthis future detector to the mass separation of primary cosmic rays, using\nCORSIKA Monte Carlo simulations of extensive air showers initiated by H, He, O\nand Fe primaries. The surface array will use two types of detection\ntechnologies: scintillation detectors and radio antennas; the latter are not\nconsidered in this study. A set of mass-sensitive variables are investigated\nutilizing both the scintillators of the surface array and the full optical\nin-ice array. Among these, the high-energy muons measurable by the in-ice array\nare found to have the highest mass separation power for showers for which the\ncosmic-ray energy is known, e.g. from the surface array.",
        "A neutrino-like event with an energy of $\\sim 220 \\,{\\rm PeV}$ was recently\ndetected by the KM3NeT\/ARCA telescope. If this neutrino comes from an\nastrophysical source, or from the interaction of an ultra-high-energy cosmic\nray in the intergalactic medium, the ultra-high-energy gamma rays that are\nco-produced with the neutrinos will scatter with the extragalactic background\nlight, producing an electromagnetic cascade and resulting in emission at\nGeV-to-TeV energies. In this paper, we compute the gamma-ray flux from this\nneutrino source considering various source distances and strengths of the\nintergalactic magnetic field (IGMF). We find that the associated gamma-ray\nemission could be observed by existing imaging air cherenkov telescopes and air\nshower gamma-ray observatories, unless the strength of the IGMF is $B\\gtrsim\n3\\times 10^{-13}$ G, or the ultra-high-energy gamma-rays are attenuated inside\nof the source itself. In the latter case, this source is expected to be\nradio-loud.",
        "We present results of numerical relativity simulations of core collapse of\nrotating magnetized white dwarfs (WDs) in three dimension, aiming at discussing\nthe explosion dynamics and associate multi-messenger signals: gravitational\nwaves (GWs), neutrinos, and electromagnetic counterparts. All WDs initiate\ngravitational collapse due to electron captures and then experience prompt type\nexplosions after the proto neutron star formation. We observe the explosions\ndominated by a bipolar structure and the emergence of strong spiral waves in\nrapidly rotating models. The spiral waves facilitate to increase both the\nexplosion energy and ejecta mass, though the final values still fall in the\ncategory of low explosion energy supernovae with small ejecta mass. The spiral\nwaves also produce strong GWs, which may expand the horizon distance of such\nevents against GWs up to $\\sim 10$ Mpc for third-generation ground-based\ndetectors. Additionally as an intriguing implication, we demonstrate that such\naccretion or merger induced collapse of WDs might be able to explain some of\nthe rapidly evolving optical transients, such as fast blue optical transients\n(FBOTs), as previously suggested. Based on the simulation results together with\nseveral assumptions, we confirm that the magnetar may account for the brighter\nside of observed FBOTs, while a combination of ejecta-envelope interaction\nwhich can be also followed by radioactive decay of heavy elements synthesized\nalong with the explosion might still explain the fainter branch even in the\nabsence of magnetar formation.",
        "Lorentz invariance is a fundamental symmetry of spacetime and foundational to\nmodern physics. One of its most important consequences is the constancy of the\nspeed of light. This invariance, together with the geometry of spacetime,\nimplies that no particle can move faster than the speed of light. In this\narticle, we present the most stringent neutrino-based test of this prediction,\nusing the highest energy neutrino ever detected to date, KM3-230213A. The\narrival of this event, with an energy of $220^{+570}_{-110}\\,\\text{PeV}$, sets\na constraint on $\\delta \\equiv c_\\nu^2-1 < 4\\times10^{-22}$.",
        "We present time-resolved Gran Telescopio Canarias optical spectroscopy and\nWilliam Herschel Telescope $i$-band photometry of the X-ray transient SWIFT\nJ1753.5-0127 in quiescence. The $i$-band light curve is dominated by flickering\nwith an amplitude of $\\sim 0.5$ mag and shows no evidence of the ellipsoidal\nmodulation of the companion star. The telluric-corrected average spectrum, on\nthe other hand, reveals the presence of weak (strongly veiled) TiO bands at\n$7055$ \\.A and $7589$ \\.A. We used them for a spectral classification, finding\nan M4-5 V companion star. However, as velocity shifts are not clearly detected\nin the individual spectra, we turned the analysis to the double-peaked\nH$\\alpha$ emission line from the accretion disc. By exploiting the empirical\ncorrelations established for quiescent X-ray transients between the line\nmorphology and fundamental binary parameters, we estimated the radial velocity\nsemi-amplitude of the companion $K_2 = 820 \\pm 36$ km s$^{-1}$, a mass ratio $q\n= 0.023 \\pm 0.006$ and an inclination $i = 79 \\pm 5$ deg. Moreover, an orbital\nperiod of $3.26 \\pm 0.02$ h was measured from the modulation of the centroid\nvelocities and the double-peak trough depth of the H$\\alpha$ profile. These\nquantities yielded a mass function $f(M_1) = 7.8 \\pm 1.0$ M$_\\odot$ and black\nhole and companion star masses of $M_1 = 8.8 \\pm 1.3$ M$_\\odot$ and $M_2 = 0.20\n\\pm 0.06$ M$_\\odot$, respectively. The companion star mass is in line with the\nspectral classification obtained from the relative depth of the TiO bands.\nBased on the mean quiescent magnitude ($i = 21.4 \\pm 0.1$), orbital period, and\ninterstellar extinction, we estimate the distance to the source to be $3.9 \\pm\n0.7$ kpc and a Galactic plane elevation of $0.8 \\pm 0.2$ kpc, supporting the\ncase for a large natal kick.",
        "We introduce and apply a methodology based on dynamic time warping (DTW) to\ncompare the whole set of gamma-ray light curves reported in the Third\nFermi-Large Area Telescope Pulsar Catalogue. Our method allows us to\nquantitatively measure the degree of global similarity between two light curves\nbeyond comparing indicators such as how many peaks there are, which is their\nseparation, width, and height. Once the morphology of the light curve is\nshowcased via background subtraction, min-max scaler normalization, and\nrotations are considered to take into account that phase 0 is arbitrary, the\nlevel of detail with which light curves of different pulsars appear is\nrevealed. In many cases their similarity is striking and occurs disregarding\nany other timing, physical, or spectral property. In particular, some MSPs and\nyoung pulsars share detailed light curve morphology.",
        "The very high energy (VHE, E $>$ $100 \\mathrm~{GeV}$) $\\gamma$-ray\nobservations offer a possibility of indirectly detecting the presence of\naxion-like particles (ALPs). The paper focuses on detecting photon-ALP\noscillations on $\\gamma$-ray spectra from distant sources in astrophysical\nmagnetic fields. Strong evidence indicates that: (1) the photon-ALP\noscillations can effectively decrease the photon absorption at energies of\nseveral tens of TeV -- caused by the extragalactic background light (EBL) -- to\na level able to explain better the observational data; (2) the impact of\nmagnetic-field models in photon-ALP beams crossing several magnetized media is\nsignificant. We revisit the expected signature for the photon-ALP oscillation\neffects on $\\gamma-\\gamma $ absorption in the TeV spectra of Mrk 501. The\nresult issues that the photon-ALP beam propagation with mass\n$\\mathrm{m_a}\\sim10^{-10} eV$ and two-photon coupling constant\n$\\begin{aligned}g_{a\\gamma}\\sim0.417\\times10^{-11}GeV^{-1}\\end{aligned}$\ncrossing reasonable magnetic field scenarios considered here can roughly\nreproduce the observed TeV $\\gamma$-ray spectra for Mrk 501.",
        "We report a search for high-energy astrophysical neutrino multiplets,\ndetections of multiple neutrino clusters in the same direction within 30 days,\nbased on an analysis of 11.4 years of IceCube data. A new search method\noptimized for transient neutrino emission with a monthly time scale is\nemployed, providing a higher sensitivity to neutrino fluxes. This result is\nsensitive to neutrino transient emission, reaching per-flavor flux of\napproximately $10^{-10}\\ {\\rm erg}\\ {\\rm cm}^{-2}\\ {\\rm sec}^{-1}$ from the\nNorthern sky in the energy range $E\\gtrsim 50$~TeV. The number of doublets and\ntriplets identified in this search is compatible with the atmospheric\nbackground hypothesis, which leads us to set limits on the nature of neutrino\ntransient sources with emission timescales of one month.",
        "The quasi-periodic oscillations (QPOs) observed in the X-ray variability of\nboth black hole (BH) and neutron star (NS) systems provide a tool for probing\nstrong gravity and dense matter equations of state. Nevertheless, the mechanism\nof QPO modulation in NS systems, where the amplitudes of QPOs with frequencies\napproaching kHz range are very high in comparison to BH high-frequency QPOs,\nremains an unsolved puzzle. Relativistic ray tracing of photons emitted from\nthe immediate vicinity of compact objects has, to date, been used to\ninvestigate various mechanisms that explain the observed weak BH QPOs. However,\nit has not been applied to model the NS QPO signal, which requires\nincorporating the NS surface and a bright boundary layer (BL) on it. Here, we\nexplore the QPO modulation mechanisms based on the BL obscuration. Using\nsimplified models of axisymmetric oscillations of thick accretion discs (tori),\nwe demonstrate that the disc oscillations drive the high NS QPO amplitudes\nthrough BL obscuration, which is relevant especially for vertical oscillations.\nWe also demonstrate that obscuration effects enable the observability of the\nKeplerian frequency in the case of discs that decay due to instabilities.",
        "We report the timing and spectral analyses upon of the type II X-ray bursts\nfrom the Rapid Burster (MXB 1730--335) observed by Insight-HXMT and Swift\/XRT.\nBy stacking the long-duration bursts, we find for the first time that the hard\nX-rays are lagging than the soft X-rays by 3 seconds. However, such a lag is\nnot visible for the short-duration bursts, probably because of the poor\nstatistics. For all bursts the energy spectrum is found to be non-thermal,\nthanks to the broad band coverage of Insight-HXMT. These findings put new\ninsights into the type-II bursts and require a temporally showing-up corona for\npossible interpretation.",
        "Accretion disks, swirling structures of matter spiraling into black holes,\nplay a pivotal role in our understanding of binary star systems and their\nintricate evolutionary processes. While current models often simplify these\ncomplex phenomena by neglecting the influence of powerful magnetic fields,\nparticularly within warped or distorted black hole geometries, this study\ndelves into the crucial impact of such fields. Focusing on a thin accretion\ndisk encircling a Schwarzschild black hole, we meticulously investigate how the\npresence of a quadrupole moment, an inherent distortion in the black hole's\nshape, affects its spectral characteristics. By analyzing key parameters like\ntotal pressure, magnetic pressure, temperature, height scale, surface density,\nand radiative flux (the energy emitted by the disk) we reveal significant\nalterations induced by incorporating both magnetic fields and a quadrupole\nmoment. Notably, our findings demonstrate that negative quadrupoles exert a\nmore pronounced influence on these disk properties, highlighting the intricate\ninterplay between these factors. This comprehensive study provides invaluable\ninsights into the dynamics of accretion disks surrounding distorted black holes\nwith magnetic fields, paving the way for a more accurate and nuanced\nunderstanding of these fascinating astrophysical systems.",
        "We apply a Gaussian process method to the extreme $\\gamma$-ray flares of 3C\n454.3 and 3C 279 to discover the variable patterns and then to investigate the\nphysical origins of the giant flares. The kernels of stochastically driven\ndamped simple harmonic oscillator (SHO), the damped random-walk (DRW), and\nMat$\\acute{\\rm e}$rn-3\/2 are respectively used to describe the adaptive-binning\n$\\gamma$-ray light curves of the two flares. Our findings show that both the\nextreme $\\gamma$-ray flares of 3C 454.3 and 3C 279 clearly prefer the SHO\nkernel in the over-damped mode and the Mat$\\acute{\\rm e}$rn-3\/2 kernel over the\nDRW kernel. The resulted SHO and Mat$\\acute{\\rm e}$rn-3\/2 power spectral\ndensities (PSDs) are the same for each object, with the index changing from -4\nat high frequencies to 0 at low frequencies. The patterns of the two flares are\nboth approaching the critical damping mode with the quality factor Q $\\approx$\n0.4 (i.e., the damping ratio $\\eta \\approx$ 1.25), but with slightly different\ndamping timescales. The characteristic timescale (corresponding to the broken\nfrequency in the PSD) for 3C 454.3 is 2-3 days and 3-5 days for 3C 279. The\nvariable patterns found here suggest that once the system responds to the\nenergy injection disturbance, the release of the energy in the system is\nfinished abruptly. The obtained timescale provides a constraint on the size of\nenergy dissipation region for each source.",
        "The physical origin of active galactic nucleus (AGN) variability remains\nunclear. Here we propose that the magnetic reconnection driven by the migration\nof satellite black holes (sBHs) in the AGN disc can be a new plausible\nmechanism for AGN short-term variability. During the sBH migration, the\nco-moving plasmas surrounding the sBH could influence the large-scale magnetic\nfield of the AGN disk and trigger the magnetic reconnections to contribute to\nAGN UV\/optical variability. Meanwhile, the plasma, which is accelerated by the\nmagnetic reconnection, will successfully escape from the disc at Alfven\nvelocity and cause a secondary magnetic reconnection in the corona. For a $\\sim\n10^{2}-10^{3}~{M_\\mathrm{\\odot}}$ sBH (including its binding gas) in the inner\nregions of the disc surrounding a supermassive black hole with $\\sim\n10^{7}~{M_\\mathrm{\\odot}}$, the reconnection process occurred in the space out\nof the disc can produce X-ray emission, which can last $\\sim 10^4-10^6~\\rm s$\nwith the luminosity $\\sim 10^{39}- 10^{43}~\\rm{erg ~s^{-1}}$.",
        "The Raychaudhuri equation (RE) governs the evolution of geodesic congruences\nin curved spacetime. Here, we extend the classical RE by incorporating\nnon-affine parametrization within the framework of k-essence scalar field\ndynamics. The non-affine parametrization introduces deviations from purely\ngeodesic congruences (motion), allowing investigation of non-gravitational\ninteractions and external forces. Using a DBI-type k-essence Lagrangian, we\nanalyze the behavior of non-geodesic flow curves in the background FLRW metric,\nelucidating their role in cosmic acceleration and structure formation. The\nemergent metric formalism is used to derive a modified RE, revealing new\ngeometric and dynamical features induced by the k-essence field. The\ncosmological implications of our model are studied by constraining key\nparameters using observational data from the PANTHEON+SHOES+BAO and Hubble\ndatasets. Our results suggest that non-affine parametrization, coupled with\nk-essence dynamics, can provide a viable explanation for late-time cosmic\nacceleration while addressing the Hubble tension. Further, we reinterpret the\nmodified RE as an anti-damped harmonic oscillator, revealing quantum-like\neffects in cosmic expansion. These results suggest a deep connection between\nscalar field dynamics and modified gravity, offering new perspectives on the\nnature of the universe's expansion history.",
        "Gradient-based attribution methods aim to explain decisions of deep learning\nmodels but so far lack identifiability guarantees. Here, we propose a method to\ngenerate attribution maps with identifiability guarantees by developing a\nregularized contrastive learning algorithm trained on time-series data plus a\nnew attribution method called Inverted Neuron Gradient (collectively named\nxCEBRA). We show theoretically that xCEBRA has favorable properties for\nidentifying the Jacobian matrix of the data generating process. Empirically, we\ndemonstrate robust approximation of zero vs. non-zero entries in the\nground-truth attribution map on synthetic datasets, and significant\nimprovements across previous attribution methods based on feature ablation,\nShapley values, and other gradient-based methods. Our work constitutes a first\nexample of identifiable inference of time-series attribution maps and opens\navenues to a better understanding of time-series data, such as for neural\ndynamics and decision-processes within neural networks.",
        "We present an experimental method for the characterization of the kinetic\nenergies of ions confined in a 22-pole radio frequency trap by inducing a small\npotential barrier using the surrounding ring electrodes, allowing the selective\nextraction of ions. Energy sampling experiments have been performed on buffer\ngas thermalized He$^+$ ions at trap temperatures between 10-180 K, resulting in\ndistinct extraction curves as a function of the potential barrier, and a\ndifferentiated behavior depending on the escape time from the trap. The\nexperiments are complemented by Monte Carlo simulations of the ion trajectories\ninside the calculated trap potential and allow us to investigate the properties\nof the sampling method, the role of ion motion coupling, and the impact of\nresidual buffer gas collisions on the observed results. The technique has also\nbeen successfully applied to identify energetic H$_3^+$ ions produced in an\nexothermic reaction inside the trap. Upon calibration, this method can provide\nrelative kinetic energy distributions or be used to filter the maximum desired\nkinetic energy of the ions inside the trap.",
        "Decoherence of quantum hardware is currently limiting its practical\napplications. At the same time, classical algorithms for simulating quantum\ncircuits have progressed substantially. Here, we demonstrate a hybrid framework\nthat integrates classical simulations with quantum hardware to improve the\ncomputation of an observable's expectation value by reducing the quantum\ncircuit depth. In this framework, a quantum circuit is partitioned into two\nsubcircuits: one that describes the backpropagated Heisenberg evolution of an\nobservable, executed on a classical computer, while the other is a\nSchr\\\"odinger evolution run on quantum processors. The overall effect is to\nreduce the depths of the circuits executed on quantum devices, trading this\nwith classical overhead and an increased number of circuit executions. We\ndemonstrate the effectiveness of this method on a Hamiltonian simulation\nproblem, achieving more accurate expectation value estimates compared to using\nquantum hardware alone.",
        "Sensing technology is an important aspect of information processing. Current\ndevelopment in artificial intelligence systems (especially those aimed at\nmedical and environmental applications) requires a lot of data on the chemical\ncomposition of biological fluids or environmental samples. These complex\nmatrices require advanced sensing devices, and photoelectrochemical ones seem\nto have potential to overcome at least some of the obstacles. Furthermore, the\ndevelopment of artificial intelligence (AI) technology for autonomous robotics\nrequires technology mimicking human senses, also those operating at the\nmolecular level, such as gustation and olfaction. Again, photoelectrochemical\nsensing can provide some suitable solutions. In this review, we introduce the\nidea of integration of photoelectrochemical sensors with some unconventional\ncomputing paradigm - reservoir computing. This approach should not only boost\nthe performance of the sensors itself, but also open new pathways through\nscience. Integration of sensing devices with computing systems will also\ncontribute to a better understanding (or at least mimicking) of the human\nsenses and neuromorphic sensory information processing. Although reservoir\nsystems can be considered magic \"black boxes\" and their operation is at the\nsame time simple and hard to comprehend, this combination is expected to open a\nnew era of effective information harvesting and processing systems.",
        "We study a general class of random block Schr\\\"odinger operators (RBSOs) in\ndimensions 1 and 2, which naturally extend the Anderson model by replacing the\nrandom potential with a random block potential. Specifically, we focus on two\nRBSOs -- the block Anderson and Wegner orbital models -- defined on the\n$d$-dimensional torus $(\\mathbb Z\/L\\mathbb Z)^d$. They take the form $H=V +\n\\lambda \\Psi$, where $V$ is a block potential with i.i.d. $W^d\\times W^d$\nGaussian diagonal blocks, $\\Psi$ describes interactions between neighboring\nblocks, and $\\lambda>0$ is a coupling parameter. We normalize the blocks of\n$\\Psi$ so that each block has a Hilbert-Schmidt norm of the same order as the\nblocks of $V$. Assuming $W\\ge L^\\delta$ for a small constant $\\delta>0$ and\n$\\lambda\\gg W^{-d\/2}$, we establish the following results. In dimension $d=2$,\nwe prove delocalization and quantum unique ergodicity for bulk eigenvectors,\nwhich, combined with the localization result from arXiv:1608.02922 under the\ncondition $\\lambda\\ll W^{-d\/2}$, rigorously establishes the Anderson\nlocalization-delocalization transition as $\\lambda$ crosses the critical\nthreshold $W^{-d\/2}$. In dimension $d=1$, we show that the localization length\nof bulk eigenvectors is at least of order $(W\\lambda)^2$, which is conjectured\nto be the correct scaling for the localization length.",
        "Slow flavor evolution (defined as driven by neutrino masses and not\nnecessarily ``slow'') is receiving fresh attention in the context of compact\nastrophysical environments. In Part~I of this series, we have studied the\nslow-mode dispersion relation following our recently developed analogy to\nplasma waves. The concept of resonance between flavor waves in the linear\nregime and propagating neutrinos is the defining feature of this approach. It\nis best motivated for weak instabilities, which probably is the most relevant\nregime in self-consistent astrophysical environments because these will try to\neliminate the cause of instability. We here go beyond the dispersion relation\nalone (which by definition applies to infinite media) and consider the group\nvelocities of unstable modes that determines whether the instability relaxes\nwithin the region where it first appears (absolute), or away from it\n(convective). We show that all weak instabilities are convective so that their\nfurther evolution is not local. Therefore, studying their consequences\nnumerically in small boxes from given initial conditions may not always be\nappropriate.",
        "We perform the thermodynamic analysis of a black hole (BH) immersed in a dark\nmatter halo (DMH), showing that the BH could not be in thermal equilibrium with\nthe DMH in any regions outside the event horizon. This means that the\nthermodynamic influence of the environment (DMH) is relatively small on the BH\nand it does not alter the nature of BH with negative heat capacity. The\nNewtonian ($1\/a_0$) approximation gives us a correct thermodynamic description\nfor the BH surrounded by DMH because the first law of thermodynamics and Smarr\nformula are satisfied. We use the Newtonian Helmholtz free energy to show that\nthere is no phase transition to other BH with positive heat capacity surrounded\nby a DMH. We investigate the shadow bound of favored region for the BH immersed\nin the DMH by introducing three critical impact parameters.",
        "Positive geometries are semialgebraic sets equipped with a canonical\ndifferential form whose residues mirror the boundary structure of the geometry.\nEvery full-dimensional projective polytope is a positive geometry. Motivated by\nthe canonical forms of polytopes, we construct a canonical form for any tope of\nan oriented matroid, inside the Orlik--Solomon algebra of the underlying\nmatroid. Using these canonical forms, we construct bases for the Orlik--Solomon\nalgebra of a matroid, and for the Aomoto cohomology. These bases of canonical\nforms are a foundational input in the theory of matroid amplitudes introduced\nby the second author.",
        "Wide coverage and high-precision rural household wealth data is an important\nsupport for the effective connection between the national macro rural\nrevitalization policy and micro rural entities, which helps to achieve precise\nallocation of national resources. However, due to the large number and wide\ndistribution of rural areas, wealth data is difficult to collect and scarce in\nquantity. Therefore, this article attempts to integrate \"sky\" remote sensing\nimages with \"ground\" village street view imageries to construct a fine-grained\n\"computable\" technical route for rural household wealth. With the intelligent\ninterpretation of rural houses as the core, the relevant wealth elements of\nimage data were extracted and identified, and regressed with the household\nwealth indicators of the benchmark questionnaire to form a high-precision\ntownship scale wealth prediction model (r=0.85); Furthermore, a national and\ntownship scale map of rural household wealth in China was promoted and drawn.\nBased on this, this article finds that there is a \"bimodal\" pattern in the\ndistribution of wealth among rural households in China, which is reflected in a\npolarization feature of \"high in the south and low in the north, and high in\nthe east and low in the west\" in space. This technological route may provide\nalternative solutions with wider spatial coverage and higher accuracy for\nhigh-cost manual surveys, promote the identification of shortcomings in rural\nconstruction, and promote the precise implementation of rural policies.",
        "This paper explores a full generalization of the classical corner-vector\nmethod for constructing weighted spherical designs, which we call the {\\it\ngeneralized corner-vector method}. First we establish a uniform upper bound for\nthe degree of designs obtained from the proposed method. Our proof is a hybrid\nargument that employs techniques in analysis and combinatorics, especially a\nfamous result by Xu(1998) on the interrelation between spherical designs and\nsimplical designs, and the cross-ratio comparison method for Hilbert identities\nintroduced by Nozaki and Sawa(2013). We extensively study conditions for the\nexistence of designs obtained from our method, and present many curious\nexamples of degree $7$ through $13$, some of which are, to our surprise,\ncharacterized in terms of integral lattices.",
        "The Center for Exascale Monte Carlo Neutron Transport is developing Monte\nCarlo \/ Dynamic Code (MC\/DC) as a portable Monte Carlo neutron transport\npackage for rapid numerical methods exploration on CPU- and GPU-based\nhigh-performance computers. In this paper, we describe MC\/DC's current\nevent-based GPU algorithm as well as the just-in-time (JIT) compilation scheme\nwe use to enable GPU operability on Nvidia and AMD GPUs from MC\/DC's Python\nsource. To analyze performance, we conduct runtime tests of the C5G7\nk-eigenvalue benchmark problem and a continuous-energy infinite pin cell on\nNvidia Tesla V100 GPU, AMD MI250X GPU, and the AMD MI300A APU and make\ncomparison to a dual-socket Intel Xeon Sapphire Rapid CPU node. We found that\nfor the multi-group C5G7 benchmark problem, we respectively see a 15$\\times$,\n0.7$\\times$, 12$\\times$ speedup on a V100, MI250X, and MI300A over 112 Intel\nXeon CPU cores. For the continuous-energy infinite pin-cell benchmark, we found\nspeedups of 5$\\times$, 3$\\times$, 4$\\times$ on a V100, MI250X, and MI300A,\nrespectively, over the same CPU node.",
        "We introduce BCAT, a PDE foundation model designed for autoregressive\nprediction of solutions to two dimensional fluid dynamics problems. Our\napproach uses a block causal transformer architecture to model next frame\npredictions, leveraging previous frames as contextual priors rather than\nrelying solely on sub-frames or pixel-based inputs commonly used in image\ngeneration methods. This block causal framework more effectively captures the\nspatial dependencies inherent in nonlinear spatiotemporal dynamics and physical\nphenomena. In an ablation study, next frame prediction demonstrated a 2.9x\naccuracy improvement over next token prediction. BCAT is trained on a diverse\nrange of fluid dynamics datasets, including incompressible and compressible\nNavier-Stokes equations across various geometries and parameter regimes, as\nwell as the shallow-water equations. The model's performance was evaluated on 6\ndistinct downstream prediction tasks and tested on about 8K trajectories to\nmeasure robustness on a variety of fluid dynamics simulations. BCAT achieved an\naverage relative error of 1.92% across all evaluation tasks, outperforming\nprior approaches on standard benchmarks.",
        "Full-waveform inversion (FWI) is an advanced technique for reconstructing\nhigh-resolution subsurface physical parameters by progressively minimizing the\ndiscrepancy between observed and predicted seismic data. However, conventional\nFWI encounters challenges in real data applications, primarily due to its\nconventional objective of direct measurements of the data misfit. Accurate\nestimation of the source wavelet is essential for effective data fitting,\nalongside the need for low-frequency data and a reasonable initial model to\nprevent cycle skipping. Additionally, wave equation solvers often struggle to\naccurately simulate the amplitude of observed data in real applications. To\naddress these challenges, we introduce a correlation-based source-independent\nobjective function for FWI that aims to mitigate source uncertainty and\namplitude dependency, which effectively enhances its practicality for real data\napplications. We develop a deep-learning framework constrained by this new\nobjective function with a velocity-distribution supported deep image prior,\nwhich reparameterizes velocity inversion into trainable parameters within an\nautoencoder, thereby reducing the nonlinearity in the conventional FWI's\nobjective function. We demonstrate the superiority of our proposed method using\nsynthetic data from benchmark velocity models and, more importantly, two real\ndatasets. These examples highlight its effectiveness and practicality even\nunder challenging conditions, such as missing low frequencies, a crude initial\nvelocity model, and an incorrect source wavelet.",
        "A surrogate model for particle-in-cell plasma simulations based on a graph\nneural network is presented. The graph is constructed in such a way as to\nenable the representation of electromagnetic fields on a fixed spatial grid.\nThe model is applied to simulate beams of electrons in one dimension over a\nwide range of temperatures, drift momenta and densities, and is shown to\nreproduce two-stream instabilities - a common and fundamental plasma\ninstability. Qualitatively, the characteristic phase-space mixing of\ncounterpropagating electron beams is observed. Quantitatively, the model's\nperformance is evaluated in terms of the accuracy of its predictions of number\ndensity distributions, the electric field, and their Fourier decompositions,\nparticularly the growth rate of the fastest-growing unstable mode, as well as\nparticle position, momentum distributions, energy conservation and run time.\nThe model achieves high accuracy with a time step longer than conventional\nsimulation by two orders of magnitude. This work demonstrates that complex\nplasma dynamics can be learned and shows promise for the development of fast\ndifferentiable simulators suitable for solving forward and inverse problems in\nplasma physics."
      ]
    }
  },
  {
    "id":2412.00129,
    "research_type":"applied",
    "start_id":"b29",
    "start_title":"Study of Fermion pair production in e+e- collisions at 130-183 GeV",
    "start_abstract":"The cross sections and forward-backward asymmetries of hadronic and leptonic\nevents produced in e+e- collisions at centre-of-mass energies of 130-183 GeV\nare presented. Results for ee, mumu, tautau, qq, bb and cc production show no\nsignificant deviation from the Standard Model predictions. This enable\nconstraints to be set upon physics beyond the Standard Model such as\nfour-fermion contact interactions, leptoquarks, Z' bosons and R-parity\nviolating squarks and sneutrinos. Limits on the energy scale Lambda of eeff\ncontact interactions are typically in the range from 2-10 TeV. Limits on\nR-parity violating sneutrinos reach masses of a few hundred GeV for large\nvalues of their Yukawa couplings.",
    "start_categories":[
      "astro-ph.HE"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b23"
      ],
      "title":[
        "DARWIN Series: Domain Specific Large Language Models for Natural Science"
      ],
      "abstract":[
        "Emerging tools bring forth fresh approaches to work, and the field of natural science is no different. In science, traditional manual, serial, labour-intensive work being augmented by automated, parallel, iterative processes driven artificial intelligence-based experimental automation more. To add new capabilities in enabling acceleration enrichment discovery process, we present DARWIN, a series tailored LLMs for mainly physics, chemistry, material science. This relies on open-source LLM, incorporating structured unstructured scientific knowledge from public datasets literature. We fine-tuned models using over 60,000 instruction data points, emphasizing factual correctness. During fine-tuning, introduce Scientific Instruction Generation (SIG) model, automating generation texts. eliminates need manual extraction or domain-specific graphs efficiently injects into model. also explore multi-task training strategies, revealing interconnections between tasks. DARWIN not only achieves state-of-the-art results various tasks but diminishes reliance closed-source AI models. Our research showcases ability LLM domain, with overarching goal fostering prosperity within broader community."
      ],
      "categories":[
        "cs.CL"
      ]
    },
    "list":{
      "title":[
        "Language Fusion for Parameter-Efficient Cross-lingual Transfer",
        "SoftMatcha: A Soft and Fast Pattern Matcher for Billion-Scale Corpus\n  Searches",
        "AEIA-MN: Evaluating the Robustness of Multimodal LLM-Powered Mobile\n  Agents Against Active Environmental Injection Attacks",
        "BAMBI: Developing Baby Language Models for Italian",
        "Unshackling Context Length: An Efficient Selective Attention Approach\n  through Query-Key Compression",
        "Group Preference Alignment: Customized LLM Response Generation from\n  In-Situ Conversations",
        "TROVE: A Challenge for Fine-Grained Text Provenance via Source Sentence\n  Tracing and Relationship Classification",
        "The Accuracy, Robustness, and Readability of LLM-Generated\n  Sustainability-Related Word Definitions",
        "Eager Updates For Overlapped Communication and Computation in DiLoCo",
        "What Limits LLM-based Human Simulation: LLMs or Our Design?",
        "Corporate Greenwashing Detection in Text -- a Survey",
        "SoftCoT: Soft Chain-of-Thought for Efficient Reasoning with LLMs",
        "Comparative Approaches to Sentiment Analysis Using Datasets in Major\n  European and Arabic Languages",
        "Reframing Dense Action Detection (RefDense): A Paradigm Shift in Problem\n  Solving & a Novel Optimization Strategy",
        "The Architecture and Evaluation of Bayesian Neural Networks",
        "Improving Influence-based Instruction Tuning Data Selection for Balanced\n  Learning of Diverse Capabilities",
        "$\\ell_{p}$ has nontrivial Euclidean distortion growth when $2<p<4$",
        "The Strong Cosmic Censorship Conjecture",
        "Towards Invisible Backdoor Attack on Text-to-Image Diffusion Model",
        "Adaptive Camera Sensor for Vision Models",
        "Gaussian Process Upper Confidence Bound Achieves Nearly-Optimal Regret\n  in Noise-Free Gaussian Process Bandits",
        "Elliptic curves in game theory",
        "StaICC: Standardized Evaluation for Classification Task in In-context\n  Learning",
        "End-to-end Training for Text-to-Image Synthesis using Dual-Text\n  Embeddings",
        "ExplainReduce: Summarising local explanations via proxies",
        "Polynomial invariants of $\\operatorname{GL}_{2}$: Conjugation over\n  finite fields",
        "Continuity of asymptotic entropy on wreath products",
        "Uncertainty Guarantees on Automated Precision Weeding using Conformal\n  Prediction"
      ],
      "abstract":[
        "Limited availability of multilingual text corpora for training language\nmodels often leads to poor performance on downstream tasks due to undertrained\nrepresentation spaces for languages other than English. This\n'under-representation' has motivated recent cross-lingual transfer methods to\nleverage the English representation space by e.g. mixing English and\n'non-English' tokens at the input level or extending model parameters to\naccommodate new languages. However, these approaches often come at the cost of\nincreased computational complexity. We propose Fusion forLanguage\nRepresentations (FLARE) in adapters, a novel method that enhances\nrepresentation quality and downstream performance for languages other than\nEnglish while maintaining parameter efficiency. FLARE integrates source and\ntarget language representations within low-rank (LoRA) adapters using\nlightweight linear transformations, maintaining parameter efficiency while\nimproving transfer performance. A series of experiments across representative\ncross-lingual natural language understanding tasks, including natural language\ninference, question-answering and sentiment analysis, demonstrate FLARE's\neffectiveness. FLARE achieves performance improvements of 4.9% for Llama 3.1\nand 2.2% for Gemma~2 compared to standard LoRA fine-tuning on\nquestion-answering tasks, as measured by the exact match metric.",
        "Researchers and practitioners in natural language processing and\ncomputational linguistics frequently observe and analyze the real language\nusage in large-scale corpora. For that purpose, they often employ off-the-shelf\npattern-matching tools, such as grep, and keyword-in-context concordancers,\nwhich is widely used in corpus linguistics for gathering examples. Nonetheless,\nthese existing techniques rely on surface-level string matching, and thus they\nsuffer from the major limitation of not being able to handle orthographic\nvariations and paraphrasing -- notable and common phenomena in any natural\nlanguage. In addition, existing continuous approaches such as dense vector\nsearch tend to be overly coarse, often retrieving texts that are unrelated but\nshare similar topics. Given these challenges, we propose a novel algorithm that\nachieves \\emph{soft} (or semantic) yet efficient pattern matching by relaxing a\nsurface-level matching with word embeddings. Our algorithm is highly scalable\nwith respect to the size of the corpus text utilizing inverted indexes. We have\nprepared an efficient implementation, and we provide an accessible web tool.\nOur experiments demonstrate that the proposed method (i) can execute searches\non billion-scale corpora in less than a second, which is comparable in speed to\nsurface-level string matching and dense vector search; (ii) can extract harmful\ninstances that semantically match queries from a large set of English and\nJapanese Wikipedia articles; and (iii) can be effectively applied to\ncorpus-linguistic analyses of Latin, a language with highly diverse\ninflections.",
        "As researchers continuously optimize AI agents to perform tasks more\neffectively within operating systems, they often neglect to address the\ncritical need for enabling these agents to identify \"impostors\" within the\nsystem. Through an analysis of the agents' operating environment, we identified\na potential threat: attackers can disguise their attack methods as\nenvironmental elements, injecting active disturbances into the agents'\nexecution process, thereby disrupting their decision-making. We define this\ntype of attack as Active Environment Injection Attack (AEIA). Based on this, we\npropose AEIA-MN, an active environment injection attack scheme that exploits\ninteraction vulnerabilities in the mobile operating system to evaluate the\nrobustness of MLLM-based agents against such threats. Experimental results show\nthat even advanced MLLMs are highly vulnerable to this attack, achieving a\nmaximum attack success rate of 93% in the AndroidWorld benchmark.",
        "This paper presents BAMBI (BAby language Models Boostrapped for Italian), a\nseries of Baby Language Models (BabyLMs) trained on data that mimic the\nlinguistic input received by a five-year-old Italian-speaking child. The BAMBI\nmodels are tested using a benchmark specifically designed to evaluate language\nmodels, which takes into account the amount of training input the models\nreceived. The BAMBI models are compared against a large language model (LLM)\nand a multimodal language model (VLM) to study the contribution of\nextralinguistic information for language acquisition. The results of our\nevaluation align with the existing literature on English language models,\nconfirming that while reduced training data support the development of\nrelatively robust syntactic competence, they are insufficient for fostering\nsemantic understanding. However, the gap between the training resources (data\nand computation) of the BAMBI models and the LLMs is not fully reflected in\ntheir performance: despite LLMs' massive training, their performance is not\nmuch better than that of BAMBI models. This suggests that strategies beyond\nscaling training resources, such as data curation, inclusion of multimodal\ninput, and other training strategies such as curriculum learning, could play a\ncrucial role in shaping model performance.",
        "Handling long-context sequences efficiently remains a significant challenge\nin large language models (LLMs). Existing methods for token selection in\nsequence extrapolation either employ a permanent eviction strategy or select\ntokens by chunk, which may lead to the loss of critical information. We propose\nEfficient Selective Attention (ESA), a novel approach that extends context\nlength by efficiently selecting the most critical tokens at the token level to\ncompute attention. ESA reduces the computational complexity of token selection\nby compressing query and key vectors into lower-dimensional representations. We\nevaluate ESA on long sequence benchmarks with maximum lengths up to 256k using\nopen-source LLMs with context lengths of 8k and 32k. ESA outperforms other\nselective attention methods, especially in tasks requiring the retrieval of\nmultiple pieces of information, achieving comparable performance to\nfull-attention extrapolation methods across various tasks, with superior\nresults in certain tasks.",
        "LLMs often fail to meet the specialized needs of distinct user groups due to\ntheir one-size-fits-all training paradigm \\cite{lucy-etal-2024-one} and there\nis limited research on what personalization aspects each group expect. To\naddress these limitations, we propose a group-aware personalization framework,\nGroup Preference Alignment (GPA), that identifies context-specific variations\nin conversational preferences across user groups and then steers LLMs to\naddress those preferences. Our approach consists of two steps: (1) Group-Aware\nPreference Extraction, where maximally divergent user-group preferences are\nextracted from real-world conversation logs and distilled into interpretable\nrubrics, and (2) Tailored Response Generation, which leverages these rubrics\nthrough two methods: a) Context-Tuned Inference (GAP-CT), that dynamically\nadjusts responses via context-dependent prompt instructions, and b)\nRubric-Finetuning Inference (GPA-FT), which uses the rubrics to generate\ncontrastive synthetic data for personalization of group-specific models via\nalignment. Experiments demonstrate that our framework significantly improves\nalignment of the output with respect to user preferences and outperforms\nbaseline methods, while maintaining robust performance on standard benchmarks.",
        "LLMs have achieved remarkable fluency and coherence in text generation, yet\ntheir widespread adoption has raised concerns about content reliability and\naccountability. In high-stakes domains such as healthcare, law, and news, it is\ncrucial to understand where and how the content is created. To address this, we\nintroduce the Text pROVEnance (TROVE) challenge, designed to trace each\nsentence of a target text back to specific source sentences within potentially\nlengthy or multi-document inputs. Beyond identifying sources, TROVE annotates\nthe fine-grained relationships (quotation, compression, inference, and others),\nproviding a deep understanding of how each target sentence is formed. To\nbenchmark TROVE, we construct our dataset by leveraging three public datasets\ncovering 11 diverse scenarios (e.g., QA and summarization) in English and\nChinese, spanning source texts of varying lengths (0-5k, 5-10k, 10k+),\nemphasizing the multi-document and long-document settings essential for\nprovenance. To ensure high-quality data, we employ a three-stage annotation\nprocess: sentence retrieval, GPT provenance, and human provenance. We evaluate\n11 LLMs under direct prompting and retrieval-augmented paradigms, revealing\nthat retrieval is essential for robust performance, larger models perform\nbetter in complex relationship classification, and closed-source models often\nlead, yet open-source models show significant promise, particularly with\nretrieval augmentation.",
        "A common language with standardized definitions is crucial for effective\nclimate discussions. However, concerns exist about LLMs misrepresenting climate\nterms. We compared 300 official IPCC glossary definitions with those generated\nby GPT-4o-mini, Llama3.1 8B, and Mistral 7B, analyzing adherence, robustness,\nand readability using SBERT sentence embeddings. The LLMs scored an average\nadherence of $0.57-0.59 \\pm 0.15$, and their definitions proved harder to read\nthan the originals. Model-generated definitions vary mainly among words with\nmultiple or ambiguous definitions, showing the potential to highlight terms\nthat need standardization. The results show how LLMs could support\nenvironmental discourse while emphasizing the need to align model outputs with\nestablished terminology for clarity and consistency.",
        "Distributed optimization methods such as DiLoCo have been shown to be\neffective in training very large models across multiple distributed workers,\nsuch as datacenters. These methods split updates into two parts: an inner\noptimization phase, where the workers independently execute multiple\noptimization steps on their own local data, and an outer optimization step,\nwhere the inner updates are synchronized. While such approaches require orders\nof magnitude less communication than standard data-parallel training, in\nsettings where the workers are datacenters, even the limited communication\nrequirements of these approaches can still cause significant slow downs due to\nthe blocking necessary at each outer optimization step. In this paper, we\ninvestigate techniques to mitigate this issue by overlapping communication with\ncomputation in a manner that allows the outer optimization step to fully\noverlap with the inner optimization phase. We show that a particular variant,\ndubbed eager updates, provides competitive performance with standard DiLoCo in\nsettings with low bandwidth between workers.",
        "We argue that advancing LLM-based human simulation requires addressing both\nLLM's inherent limitations and simulation framework design challenges. Recent\nstudies have revealed significant gaps between LLM-based human simulations and\nreal-world observations, highlighting these dual challenges. To address these\ngaps, we present a comprehensive analysis of LLM limitations and our design\nissues, proposing targeted solutions for both aspects. Furthermore, we explore\nfuture directions that address both challenges simultaneously, particularly in\ndata collection, LLM generation, and evaluation. To support further research in\nthis field, we provide a curated collection of LLM-based human simulation\nresources.\\footnote{https:\/\/github.com\/Persdre\/llm-human-simulation}",
        "Greenwashing is an effort to mislead the public about the environmental\nimpact of an entity, such as a state or company. We provide a comprehensive\nsurvey of the scientific literature addressing natural language processing\nmethods to identify potentially misleading climate-related corporate\ncommunications, indicative of greenwashing. We break the detection of\ngreenwashing into intermediate tasks, and review the state-of-the-art\napproaches for each of them. We discuss datasets, methods, and results, as well\nas limitations and open challenges. We also provide an overview of how far the\nfield has come as a whole, and point out future research directions.",
        "Chain-of-Thought (CoT) reasoning enables Large Language Models (LLMs) to\nsolve complex reasoning tasks by generating intermediate reasoning steps.\nHowever, most existing approaches focus on hard token decoding, which\nconstrains reasoning within the discrete vocabulary space and may not always be\noptimal. While recent efforts explore continuous-space reasoning, they often\nsuffer from catastrophic forgetting, limiting their applicability to\nstate-of-the-art LLMs that already perform well in zero-shot settings with a\nproper instruction. To address this challenge, we propose a novel approach for\ncontinuous-space reasoning that does not require modifying the underlying LLM.\nSpecifically, we employ a lightweight assistant model to generate\ninstance-specific soft thought tokens speculatively as the initial chain of\nthoughts, which are then mapped into the LLM's representation space via a\nprojection module. Experimental results on five reasoning benchmarks\ndemonstrate that our method enhances LLM reasoning performance through\nsupervised, parameter-efficient fine-tuning.",
        "This study explores transformer-based models such as BERT, mBERT, and XLM-R\nfor multi-lingual sentiment analysis across diverse linguistic structures. Key\ncontributions include the identification of XLM-R superior adaptability in\nmorphologically complex languages, achieving accuracy levels above 88%. The\nwork highlights fine-tuning strategies and emphasizes their significance for\nimproving sentiment classification in underrepresented languages.",
        "Dense action detection involves detecting multiple co-occurring actions while\naction classes are often ambiguous and represent overlapping concepts. We argue\nthat handling the dual challenge of temporal and class overlaps is too complex\nto effectively be tackled by a single network. To address this, we propose to\ndecompose the task of detecting dense ambiguous actions into detecting dense,\nunambiguous sub-concepts that form the action classes (i.e., action entities\nand action motions), and assigning these sub-tasks to distinct sub-networks. By\nisolating these unambiguous concepts, the sub-networks can focus exclusively on\nresolving a single challenge, dense temporal overlaps. Furthermore,\nsimultaneous actions in a video often exhibit interrelationships, and\nexploiting these relationships can improve the method performance. However,\ncurrent dense action detection networks fail to effectively learn these\nrelationships due to their reliance on binary cross-entropy optimization, which\ntreats each class independently. To address this limitation, we propose\nproviding explicit supervision on co-occurring concepts during network\noptimization through a novel language-guided contrastive learning loss. Our\nextensive experiments demonstrate the superiority of our approach over\nstate-of-the-art methods, achieving substantial improvements of 3.8% and 1.7%\non average across all metrics on the challenging benchmark datasets, Charades\nand MultiTHUMOS.",
        "As modern neural networks get more complex, specifying a model with high\npredictive performance and sound uncertainty quantification becomes a more\nchallenging task. Despite some promising theoretical results on the true\nposterior predictive distribution of Bayesian neural networks, the properties\nof even the most commonly used posterior approximations are often questioned.\nComputational burdens and intractable posteriors expose miscalibrated Bayesian\nneural networks to poor accuracy and unreliable uncertainty estimates.\nApproximate Bayesian inference aims to replace unknown and intractable\nposterior distributions with some simpler but feasible distributions. The\ndimensions of modern deep models coupled with the lack of identifiability make\nMarkov chain Monte Carlo tremendously expensive and unable to fully explore the\nmultimodal posterior. On the other hand, variational inference benefits from\nimproved computational complexity but lacks the asymptotical guarantees of\nsampling-based inference and tends to concentrate around a single mode. The\nperformance of both approaches heavily depends on architectural choices; this\npaper aims to shed some light on this, by considering the computational costs,\naccuracy and uncertainty quantification in different scenarios including large\nwidth and out-of-sample data. To improve posterior exploration, different model\naveraging and ensembling techniques are studied, along with their benefits on\npredictive performance. In our experiments, variational inference overall\nprovided better uncertainty quantification than Markov chain Monte Carlo;\nfurther, stacking and ensembles of variational approximations provided\ncomparable to Markov chain Monte Carlo accuracy at a much-reduced cost.",
        "Selecting appropriate training data is crucial for effective instruction\nfine-tuning of large language models (LLMs), which aims to (1) elicit strong\ncapabilities, and (2) achieve balanced performance across a diverse range of\ntasks. Influence-based methods show promise in achieving (1) by estimating the\ncontribution of each training example to the model's predictions, but often\nstruggle with (2). Our systematic investigation reveals that this\nunderperformance can be attributed to an inherent bias where certain tasks\nintrinsically have greater influence than others. As a result, data selection\nis often biased towards these tasks, not only hurting the model's performance\non others but also, counterintuitively, harms performance on these\nhigh-influence tasks themselves.\n  As a remedy, we propose BIDS, a Balanced and Influential Data Selection\nalgorithm. BIDS first normalizes influence scores of the training data, and\nthen iteratively balances data selection by choosing the training example with\nthe highest influence on the most underrepresented task. Experiments with both\nLlama-3 and Mistral-v0.3 on seven benchmarks spanning five diverse capabilities\nshow that BIDS consistently outperforms both state-of-the-art influence-based\nalgorithms and other non-influence-based selection frameworks. Surprisingly,\ntraining on a 15% subset selected by BIDS can even outperform full-dataset\ntraining with a much more balanced performance. Our analysis further highlights\nthe importance of both instance-level normalization and iterative optimization\nof selected data for balanced learning of diverse capabilities.",
        "We prove that if $2 < p < 4$, then every $n$-point subset of $\\ell_{p}$\nembeds into a Hilbert space with distortion $o(\\log n)$, i.e., with distortion\nthat is asymptotically smaller than what Bourgain's embedding theorem provides\nfor arbitrary $n$-point metric spaces. This has been previously unknown for any\n$2<p<\\infty$. We also prove that for every $2<p<\\infty$ the largest possible\nseparation modulus of an $n$-point subset of $\\ell_{p}$ is bounded from above\nand from below by positive constant multiples of $\\sqrt{\\log n}$ which may\ndepend only on $p$.",
        "In the wake of major breakthroughs in General Relativity during the 1960s,\nRoger Penrose introduced Strong Cosmic Censorship, a profound conjecture\nregarding the deterministic nature of the theory. Penrose's proposal has since\nopened far-reaching new mathematical avenues, revealing connections to\nfundamental questions about black holes and the nature of gravitational\nsingularities. We review recent advances arising from modern techniques in the\ntheory of partial differential equations as applied to Strong Cosmic\nCensorship, maintaining a focus on the context of gravitational collapse that\ngave birth to the conjecture.",
        "Backdoor attacks targeting text-to-image diffusion models have advanced\nrapidly, enabling attackers to implant malicious triggers into these models to\nmanipulate their outputs. However, current backdoor samples often exhibit two\nkey abnormalities compared to benign samples: 1) Semantic Consistency, where\nbackdoor prompts tend to generate images with similar semantic content even\nwith significant textual variations to the prompts; 2) Attention Consistency,\nwhere the trigger induces consistent structural responses in the\ncross-attention maps. These consistencies leave detectable traces for\ndefenders, making backdoors easier to identify. To enhance the stealthiness of\nbackdoor samples, we propose a novel Invisible Backdoor Attack (IBA) by\nexplicitly mitigating these consistencies. Specifically, our approach leverages\nsyntactic structures as backdoor triggers to amplify the sensitivity to textual\nvariations, effectively breaking down the semantic consistency. Besides, a\nregularization method based on Kernel Maximum Mean Discrepancy (KMMD) is\nproposed to align the distribution of cross-attention responses between\nbackdoor and benign samples, thereby disrupting attention consistency.\nExtensive experiments demonstrate that our IBA achieves a 97.5% attack success\nrate while exhibiting stronger resistance to defenses, with an average of over\n98% backdoor samples bypassing three state-of-the-art detection mechanisms. The\ncode is available at https:\/\/github.com\/Robin-WZQ\/IBA.",
        "Domain shift remains a persistent challenge in deep-learning-based computer\nvision, often requiring extensive model modifications or large labeled datasets\nto address. Inspired by human visual perception, which adjusts input quality\nthrough corrective lenses rather than over-training the brain, we propose Lens,\na novel camera sensor control method that enhances model performance by\ncapturing high-quality images from the model's perspective rather than relying\non traditional human-centric sensor control. Lens is lightweight and adapts\nsensor parameters to specific models and scenes in real-time. At its core, Lens\nutilizes VisiT, a training-free, model-specific quality indicator that\nevaluates individual unlabeled samples at test time using confidence scores\nwithout additional adaptation costs. To validate Lens, we introduce ImageNet-ES\nDiverse, a new benchmark dataset capturing natural perturbations from varying\nsensor and lighting conditions. Extensive experiments on both ImageNet-ES and\nour new ImageNet-ES Diverse show that Lens significantly improves model\naccuracy across various baseline schemes for sensor control and model\nmodification while maintaining low latency in image captures. Lens effectively\ncompensates for large model size differences and integrates synergistically\nwith model improvement techniques. Our code and dataset are available at\ngithub.com\/Edw2n\/Lens.git.",
        "We study the noise-free Gaussian Process (GP) bandits problem, in which the\nlearner seeks to minimize regret through noise-free observations of the\nblack-box objective function lying on the known reproducing kernel Hilbert\nspace (RKHS). Gaussian process upper confidence bound (GP-UCB) is the\nwell-known GP-bandits algorithm whose query points are adaptively chosen based\non the GP-based upper confidence bound score. Although several existing works\nhave reported the practical success of GP-UCB, the current theoretical results\nindicate its suboptimal performance. However, GP-UCB tends to perform well\nempirically compared with other nearly optimal noise-free algorithms that rely\non a non-adaptive sampling scheme of query points. This paper resolves this gap\nbetween theoretical and empirical performance by showing the nearly optimal\nregret upper bound of noise-free GP-UCB. Specifically, our analysis shows the\nfirst constant cumulative regret in the noise-free settings for the squared\nexponential kernel and Mat\\'ern kernel with some degree of smoothness.",
        "We investigate Spohn curves, the algebro-geometric models of dependency\nequilibria for $2 \\times 2$ normal-form games. These curves arise as the\nintersection of two quadrics in $\\mathbb{P}^3$ and are generically elliptic\ncurves. We compute and verify the $j$-invariant for elliptic curves arising as\nthe intersection of quadrics in $\\mathbb P^3$ using two different\nimplementations: by computing the Aronhold invariants and the discriminant (in\nMathematica) and using algorithms for the arithmetic of elliptic curves\n(in-built in Pari\/GP). We define an equivalency of generic $2\\times 2$ games\nbased on the $j$-invariant of the Spohn curve. Additionally, we examine the\nreduction of Spohn curves to plane curves and analyze conditions under which\nthey are reducible. Notably, we prove that the real points are dense on the\nSpohn curve in all cases. Our examples and computations are further supported\nby Macaulay2.",
        "Classification tasks are widely investigated in the In-Context Learning (ICL)\nparadigm. However, current efforts are evaluated on disjoint benchmarks and\nsettings, while their performances are significantly influenced by some trivial\nvariables, such as prompt templates, data sampling, instructions, etc., which\nleads to significant inconsistencies in the results reported across various\nliterature, preventing fair comparison or meta-analysis across different\npapers. Therefore, this paper proposes a standardized and easy-to-use\nevaluation toolkit (StaICC) for in-context classification. Including, for the\nnormal classification task, we provide StaICC-Normal, selecting 10 widely used\ndatasets, and generating prompts with a fixed form, to mitigate the variance\namong the experiment implementations. To enrich the usage of our benchmark, we\nalso provide a sub-benchmark StaICC-Diag for diagnosing ICL from several\naspects, aiming for a more robust inference processing.",
        "Text-to-Image (T2I) synthesis is a challenging task that requires modeling\ncomplex interactions between two modalities ( i.e., text and image). A common\nframework adopted in recent state-of-the-art approaches to achieving such\nmultimodal interactions is to bootstrap the learning process with pre-trained\nimage-aligned text embeddings trained using contrastive loss. Furthermore,\nthese embeddings are typically trained generically and reused across various\nsynthesis models. In contrast, we explore an approach to learning text\nembeddings specifically tailored to the T2I synthesis network, trained in an\nend-to-end fashion. Further, we combine generative and contrastive training and\nuse two embeddings, one optimized to enhance the photo-realism of the generated\nimages, and the other seeking to capture text-to-image alignment. A\ncomprehensive set of experiments on three text-to-image benchmark datasets\n(Oxford-102, Caltech-UCSD, and MS-COCO) reveal that having two separate\nembeddings gives better results than using a shared one and that such an\napproach performs favourably in comparison with methods that use text\nrepresentations from a pre-trained text encoder trained using a discriminative\napproach. Finally, we demonstrate that such learned embeddings can be used in\nother contexts as well, such as text-to-image manipulation.",
        "Most commonly used non-linear machine learning methods are closed-box models,\nuninterpretable to humans. The field of explainable artificial intelligence\n(XAI) aims to develop tools to examine the inner workings of these closed\nboxes. An often-used model-agnostic approach to XAI involves using simple\nmodels as local approximations to produce so-called local explanations;\nexamples of this approach include LIME, SHAP, and SLISEMAP. This paper shows\nhow a large set of local explanations can be reduced to a small \"proxy set\" of\nsimple models, which can act as a generative global explanation. This reduction\nprocedure, ExplainReduce, can be formulated as an optimisation problem and\napproximated efficiently using greedy heuristics.",
        "Consider the conjugation action of $\\operatorname{GL}_{2}(K)$ on the\npolynomial ring $K[X_{2 \\times 2}]$. When $K$ is an infinite field, the ring of\ninvariants is a polynomial ring generated by the trace and the determinant. We\ndescribe the ring of invariants when $K$ is a finite field, and show that it is\na hypersurface.",
        "We prove the continuity of asymptotic entropy as a function of the step\ndistribution for non-degenerate probability measures with finite entropy on\nwreath products $ A \\wr B = \\bigoplus_B A \\rtimes B $, where $A$ is any\ncountable group and $B$ is a countable hyper-FC-central group that contains a\nfinitely generated subgroup of at least cubic growth. As one step in proving\nthe above, we show that on any countable group $G$ the probability that the\n$\\mu$-random walk on $G$ never returns to the identity is continuous in $\\mu$,\nfor measures $\\mu$ such that the semigroup generated by the support of $\\mu$\ncontains a finitely generated subgroup of at least cubic growth. Finally, we\nshow that among random walks on a group $G$ that admit a separable completely\nmetrizable space $X$ as a model for their Poisson boundary, the weak continuity\nof the associated harmonic measures on $X$ implies the continuity of the\nasymptotic entropy. This result recovers the continuity of asymptotic entropy\non known cases, such as Gromov hyperbolic groups and acylindrically hyperbolic\ngroups, and extends it to new classes of groups, including linear groups and\ngroups acting on $\\mathrm{CAT}(0)$ spaces.",
        "Precision agriculture in general, and precision weeding in particular, have\ngreatly benefited from the major advancements in deep learning and computer\nvision. A large variety of commercial robotic solutions are already available\nand deployed. However, the adoption by farmers of such solutions is still low\nfor many reasons, an important one being the lack of trust in these systems.\nThis is in great part due to the opaqueness and complexity of deep neural\nnetworks and the manufacturers' inability to provide valid guarantees on their\nperformance. Conformal prediction, a well-established methodology in the\nmachine learning community, is an efficient and reliable strategy for providing\ntrustworthy guarantees on the predictions of any black-box model under very\nminimal constraints. Bridging the gap between the safe machine learning and\nprecision agriculture communities, this article showcases conformal prediction\nin action on the task of precision weeding through deep learning-based image\nclassification. After a detailed presentation of the conformal prediction\nmethodology and the development of a precision spraying pipeline based on a\n''conformalized'' neural network and well-defined spraying decision rules, the\narticle evaluates this pipeline on two real-world scenarios: one under\nin-distribution conditions, the other reflecting a near out-of-distribution\nsetting. The results show that we are able to provide formal, i.e. certifiable,\nguarantees on spraying at least 90% of the weeds."
      ]
    }
  },
  {
    "id":2411.19475,
    "research_type":"applied",
    "start_id":"b3",
    "start_title":"Radio galaxy zoo EMU: Towards a semantic radio galaxy morphology taxonomy",
    "start_abstract":"ABSTRACT We present a novel natural language processing (NLP) approach to deriving plain English descriptors for science cases otherwise restricted by obfuscating technical terminology. address the limitations of common radio galaxy morphology classifications applying this approach. experimentally derive set semantic tags Radio Galaxy Zoo EMU (Evolutionary Map Universe) project and wider astronomical community. collect 8486 annotations morphology, from which we taxonomy tags. The are English. result is an extensible framework, more flexible, easily communicated, sensitive rare feature combinations, indescribable using current framework astronomy classifications.",
    "start_categories":[
      "astro-ph.CO"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "ImageNet: A large-scale hierarchical image database"
      ],
      "abstract":[
        "The explosion of image data on the Internet has potential to foster more sophisticated and robust models algorithms index, retrieve, organize interact with images multimedia data. But exactly how such can be harnessed organized remains a critical problem. We introduce here new database called \"ImageNet\", large-scale ontology built upon backbone WordNet structure. ImageNet aims populate majority 80,000 synsets an average 500\u20131000 clean full resolution images. This will result in tens millions annotated by semantic hierarchy WordNet. paper offers detailed analysis its current state: 12 subtrees 5247 3.2 million total. show that is much larger scale diversity accurate than datasets. Constructing challenging task. describe collection scheme Amazon Mechanical Turk. Lastly, we illustrate usefulness through three simple applications object recognition, classification automatic clustering. hope scale, accuracy, hierarchical structure offer unparalleled opportunities researchers computer vision community beyond."
      ],
      "categories":[
        "cs.CV"
      ]
    },
    "list":{
      "title":[
        "3D Trajectory Reconstruction of Moving Points Based on a Monocular\n  Camera",
        "SwiftSketch: A Diffusion Model for Image-to-Vector Sketch Generation",
        "Dfilled: Repurposing Edge-Enhancing Diffusion for Guided DSM Void\n  Filling",
        "Revisiting Gradient-based Uncertainty for Monocular Depth Estimation",
        "X-Field: A Physically Grounded Representation for 3D X-ray\n  Reconstruction",
        "FruitPAL: An IoT-Enabled Framework for Automatic Monitoring of Fruit\n  Consumption in Smart Healthcare",
        "Segment Any-Quality Images with Generative Latent Space Enhancement",
        "Counting Fish with Temporal Representations of Sonar Video",
        "Game State and Spatio-temporal Action Detection in Soccer using Graph\n  Neural Networks and 3D Convolutional Networks",
        "FP4DiT: Towards Effective Floating Point Quantization for Diffusion\n  Transformers",
        "MouseGPT: A Large-scale Vision-Language Model for Mouse Behavior\n  Analysis",
        "An Adaptive Underwater Image Enhancement Framework via Multi-Domain\n  Fusion and Color Compensation",
        "Repurposing 2D Diffusion Models with Gaussian Atlas for 3D Generation",
        "Transformers trained on proteins can learn to attend to Euclidean\n  distance",
        "ProgCo: Program Helps Self-Correction of Large Language Models",
        "Unveiling and Causalizing CoT: A Causal Pespective",
        "Visual Attention Exploration in Vision-Based Mamba Models",
        "Kernel EDMD for data-driven nonlinear Koopman MPC with stability\n  guarantees",
        "\"Once Upon a Time...\" Literary Narrative Connectedness Progresses with\n  Grade Level: Potential Impact on Reading Fluency and Literacy Skills",
        "Dr. Splat: Directly Referring 3D Gaussian Splatting via Direct Language\n  Embedding Registration",
        "EILID: Execution Integrity for Low-end IoT Devices",
        "RECALL: Library-Like Behavior In Language Models is Enhanced by\n  Self-Referencing Causal Cycles",
        "Targetless Intrinsics and Extrinsic Calibration of Multiple LiDARs and\n  Cameras with IMU using Continuous-Time Estimation",
        "Cerebrum (AIOS SDK): A Platform for Agent Development, Deployment,\n  Distribution, and Discovery",
        "On singular supports in mixed characteristic",
        "Addressing Out-of-Label Hazard Detection in Dashcam Videos: Insights\n  from the COOOL Challenge",
        "Algebraic families of higher dimensional $\\mathbb{A}^{1}$-contractible\n  affine varieties non-isomorphic to affine spaces",
        "On the Semantic Security of NTRU -- with a gentle introduction to\n  cryptography"
      ],
      "abstract":[
        "The motion measurement of point targets constitutes a fundamental problem in\nphotogrammetry, with extensive applications across various engineering domains.\nReconstructing a point's 3D motion just from the images captured by only a\nmonocular camera is unfeasible without prior assumptions. Under limited\nobservation conditions such as insufficient observations, long distance, and\nhigh observation error of platform, the least squares estimation faces the\nissue of ill-conditioning. This paper presents an algorithm for reconstructing\n3D trajectories of moving points using a monocular camera. The motion of the\npoints is represented through temporal polynomials. Ridge estimation is\nintroduced to mitigate the issues of ill-conditioning caused by limited\nobservation conditions. Then, an automatic algorithm for determining the order\nof the temporal polynomials is proposed. Furthermore, the definition of\nreconstructability for temporal polynomials is proposed to describe the\nreconstruction accuracy quantitatively. The simulated and real-world\nexperimental results demonstrate the feasibility, accuracy, and efficiency of\nthe proposed method.",
        "Recent advancements in large vision-language models have enabled highly\nexpressive and diverse vector sketch generation. However, state-of-the-art\nmethods rely on a time-consuming optimization process involving repeated\nfeedback from a pretrained model to determine stroke placement. Consequently,\ndespite producing impressive sketches, these methods are limited in practical\napplications. In this work, we introduce SwiftSketch, a diffusion model for\nimage-conditioned vector sketch generation that can produce high-quality\nsketches in less than a second. SwiftSketch operates by progressively denoising\nstroke control points sampled from a Gaussian distribution. Its\ntransformer-decoder architecture is designed to effectively handle the discrete\nnature of vector representation and capture the inherent global dependencies\nbetween strokes. To train SwiftSketch, we construct a synthetic dataset of\nimage-sketch pairs, addressing the limitations of existing sketch datasets,\nwhich are often created by non-artists and lack professional quality. For\ngenerating these synthetic sketches, we introduce ControlSketch, a method that\nenhances SDS-based techniques by incorporating precise spatial control through\na depth-aware ControlNet. We demonstrate that SwiftSketch generalizes across\ndiverse concepts, efficiently producing sketches that combine high fidelity\nwith a natural and visually appealing style.",
        "Digital Surface Models (DSMs) are essential for accurately representing\nEarth's topography in geospatial analyses. DSMs capture detailed elevations of\nnatural and manmade features, crucial for applications like urban planning,\nvegetation studies, and 3D reconstruction. However, DSMs derived from stereo\nsatellite imagery often contain voids or missing data due to occlusions,\nshadows, and lowsignal areas. Previous studies have primarily focused on void\nfilling for digital elevation models (DEMs) and Digital Terrain Models (DTMs),\nemploying methods such as inverse distance weighting (IDW), kriging, and spline\ninterpolation. While effective for simpler terrains, these approaches often\nfail to handle the intricate structures present in DSMs. To overcome these\nlimitations, we introduce Dfilled, a guided DSM void filling method that\nleverages optical remote sensing images through edge-enhancing diffusion.\nDfilled repurposes deep anisotropic diffusion models, which originally designed\nfor super-resolution tasks, to inpaint DSMs. Additionally, we utilize Perlin\nnoise to create inpainting masks that mimic natural void patterns in DSMs.\nExperimental evaluations demonstrate that Dfilled surpasses traditional\ninterpolation methods and deep learning approaches in DSM void filling tasks.\nBoth quantitative and qualitative assessments highlight the method's ability to\nmanage complex features and deliver accurate, visually coherent results.",
        "Monocular depth estimation, similar to other image-based tasks, is prone to\nerroneous predictions due to ambiguities in the image, for example, caused by\ndynamic objects or shadows. For this reason, pixel-wise uncertainty assessment\nis required for safety-critical applications to highlight the areas where the\nprediction is unreliable. We address this in a post hoc manner and introduce\ngradient-based uncertainty estimation for already trained depth estimation\nmodels. To extract gradients without depending on the ground truth depth, we\nintroduce an auxiliary loss function based on the consistency of the predicted\ndepth and a reference depth. The reference depth, which acts as pseudo ground\ntruth, is in fact generated using a simple image or feature augmentation,\nmaking our approach simple and effective. To obtain the final uncertainty\nscore, the derivatives w.r.t. the feature maps from single or multiple layers\nare calculated using back-propagation. We demonstrate that our gradient-based\napproach is effective in determining the uncertainty without re-training using\nthe two standard depth estimation benchmarks KITTI and NYU. In particular, for\nmodels trained with monocular sequences and therefore most prone to\nuncertainty, our method outperforms related approaches. In addition, we\npublicly provide our code and models: https:\/\/github.com\/jhornauer\/GrUMoDepth",
        "X-ray imaging is indispensable in medical diagnostics, yet its use is tightly\nregulated due to potential health risks. To mitigate radiation exposure, recent\nresearch focuses on generating novel views from sparse inputs and\nreconstructing Computed Tomography (CT) volumes, borrowing representations from\nthe 3D reconstruction area. However, these representations originally target\nvisible light imaging that emphasizes reflection and scattering effects, while\nneglecting penetration and attenuation properties of X-ray imaging. In this\npaper, we introduce X-Field, the first 3D representation specifically designed\nfor X-ray imaging, rooted in the energy absorption rates across different\nmaterials. To accurately model diverse materials within internal structures, we\nemploy 3D ellipsoids with distinct attenuation coefficients. To estimate each\nmaterial's energy absorption of X-rays, we devise an efficient path\npartitioning algorithm accounting for complex ellipsoid intersections. We\nfurther propose hybrid progressive initialization to refine the geometric\naccuracy of X-Filed and incorporate material-based optimization to enhance\nmodel fitting along material boundaries. Experiments show that X-Field achieves\nsuperior visual fidelity on both real-world human organ and synthetic object\ndatasets, outperforming state-of-the-art methods in X-ray Novel View Synthesis\nand CT Reconstruction.",
        "Fruits are rich sources of essential vitamins and nutrients that are vital\nfor human health. This study introduces two fully automated devices, FruitPAL\nand its updated version, FruitPAL 2.0, which aim to promote safe fruit\nconsumption while reducing health risks. Both devices leverage a high-quality\ndataset of fifteen fruit types and use advanced models- YOLOv8 and YOLOv5 V6.0-\nto enhance detection accuracy. The original FruitPAL device can identify\nvarious fruit types and notify caregivers if an allergic reaction is detected,\nthanks to YOLOv8's improved accuracy and rapid response time. Notifications are\ntransmitted via the cloud to mobile devices, ensuring real-time updates and\nimmediate accessibility. FruitPAL 2.0 builds upon this by not only detecting\nfruit but also estimating its nutritional value, thereby encouraging healthy\nconsumption. Trained on the YOLOv5 V6.0 model, FruitPAL 2.0 analyzes fruit\nintake to provide users with valuable dietary insights. This study aims to\npromote fruit consumption by helping individuals make informed choices,\nbalancing health benefits with allergy awareness. By alerting users to\npotential allergens while encouraging the consumption of nutrient-rich fruits,\nthese devices support both health maintenance and dietary awareness.",
        "Despite their success, Segment Anything Models (SAMs) experience significant\nperformance drops on severely degraded, low-quality images, limiting their\neffectiveness in real-world scenarios. To address this, we propose GleSAM,\nwhich utilizes Generative Latent space Enhancement to boost robustness on\nlow-quality images, thus enabling generalization across various image\nqualities. Specifically, we adapt the concept of latent diffusion to SAM-based\nsegmentation frameworks and perform the generative diffusion process in the\nlatent space of SAM to reconstruct high-quality representation, thereby\nimproving segmentation. Additionally, we introduce two techniques to improve\ncompatibility between the pre-trained diffusion model and the segmentation\nframework. Our method can be applied to pre-trained SAM and SAM2 with only\nminimal additional learnable parameters, allowing for efficient optimization.\nWe also construct the LQSeg dataset with a greater diversity of degradation\ntypes and levels for training and evaluating the model. Extensive experiments\ndemonstrate that GleSAM significantly improves segmentation robustness on\ncomplex degradations while maintaining generalization to clear images.\nFurthermore, GleSAM also performs well on unseen degradations, underscoring the\nversatility of our approach and dataset.",
        "Accurate estimates of salmon escapement - the number of fish migrating\nupstream to spawn - are key data for conservation and fishery management.\nExisting methods for salmon counting using high-resolution imaging sonar\nhardware are non-invasive and compatible with computer vision processing. Prior\nwork in this area has utilized object detection and tracking based methods for\nautomated salmon counting. However, these techniques remain inaccessible to\nmany sonar deployment sites due to limited compute and connectivity in the\nfield. We propose an alternative lightweight computer vision method for fish\ncounting based on analyzing echograms - temporal representations that compress\nseveral hundred frames of imaging sonar video into a single image. We predict\nupstream and downstream counts within 200-frame time windows directly from\nechograms using a ResNet-18 model, and propose a set of domain-specific image\naugmentations and a weakly-supervised training protocol to further improve\nresults. We achieve a count error of 23% on representative data from the Kenai\nRiver in Alaska, demonstrating the feasibility of our approach.",
        "Soccer analytics rely on two data sources: the player positions on the pitch\nand the sequences of events they perform. With around 2000 ball events per\ngame, their precise and exhaustive annotation based on a monocular video stream\nremains a tedious and costly manual task. While state-of-the-art\nspatio-temporal action detection methods show promise for automating this task,\nthey lack contextual understanding of the game. Assuming professional players'\nbehaviors are interdependent, we hypothesize that incorporating surrounding\nplayers' information such as positions, velocity and team membership can\nenhance purely visual predictions. We propose a spatio-temporal action\ndetection approach that combines visual and game state information via Graph\nNeural Networks trained end-to-end with state-of-the-art 3D CNNs, demonstrating\nimproved metrics through game state integration.",
        "Diffusion Models (DM) have revolutionized the text-to-image visual generation\nprocess. However, the large computational cost and model footprint of DMs\nhinders practical deployment, especially on edge devices. Post-training\nquantization (PTQ) is a lightweight method to alleviate these burdens without\nthe need for training or fine-tuning. While recent DM PTQ methods achieve W4A8\non integer-based PTQ, two key limitations remain: First, while most existing DM\nPTQ methods evaluate on classical DMs like Stable Diffusion XL, 1.5 or earlier,\nwhich use convolutional U-Nets, newer Diffusion Transformer (DiT) models like\nthe PixArt series, Hunyuan and others adopt fundamentally different transformer\nbackbones to achieve superior image synthesis. Second, integer (INT)\nquantization is prevailing in DM PTQ but doesn't align well with the network\nweight and activation distribution, while Floating-Point Quantization (FPQ) is\nstill under-investigated, yet it holds the potential to better align the weight\nand activation distributions in low-bit settings for DiT. In response, we\nintroduce FP4DiT, a PTQ method that leverages FPQ to achieve W4A6 quantization.\nSpecifically, we extend and generalize the Adaptive Rounding PTQ technique to\nadequately calibrate weight quantization for FPQ and demonstrate that DiT\nactivations depend on input patch data, necessitating robust online activation\nquantization techniques. Experimental results demonstrate that FP4DiT\noutperforms integer-based PTQ at W4A6 and W4A8 precision and generates\nconvincing visual content on PixArt-$\\alpha$, PixArt-$\\Sigma$ and Hunyuan in\nterms of several T2I metrics such as HPSv2 and CLIP.",
        "Analyzing animal behavior is crucial in advancing neuroscience, yet\nquantifying and deciphering its intricate dynamics remains a significant\nchallenge. Traditional machine vision approaches, despite their ability to\ndetect spontaneous behaviors, fall short due to limited interpretability and\nreliance on manual labeling, which restricts the exploration of the full\nbehavioral spectrum. Here, we introduce MouseGPT, a Vision-Language Model (VLM)\nthat integrates visual cues with natural language to revolutionize mouse\nbehavior analysis. Built upon our first-of-its-kind dataset - incorporating\npose dynamics and open-vocabulary behavioral annotations across over 42 million\nframes of diverse psychiatric conditions - MouseGPT provides a novel,\ncontext-rich method for comprehensive behavior interpretation. Our holistic\nanalysis framework enables detailed behavior profiling, clustering, and novel\nbehavior discovery, offering deep insights without the need for labor -\nintensive manual annotation. Evaluations reveal that MouseGPT surpasses\nexisting models in precision, adaptability, and descriptive richness,\npositioning it as a transformative tool for ethology and for unraveling complex\nbehavioral dynamics in animal models.",
        "Underwater optical imaging is severely degraded by light absorption,\nscattering, and color distortion, hindering visibility and accurate image\nanalysis. This paper presents an adaptive enhancement framework integrating\nillumination compensation, multi-domain filtering, and dynamic color\ncorrection. A hybrid illumination compensation strategy combining CLAHE, Gamma\ncorrection, and Retinex enhances visibility. A two-stage filtering process,\nincluding spatial-domain (Gaussian, Bilateral, Guided) and frequency-domain\n(Fourier, Wavelet) methods, effectively reduces noise while preserving details.\nTo correct color distortion, an adaptive color compensation (ACC) model\nestimates spectral attenuation and water type to combine RCP, DCP, and MUDCP\ndynamically. Finally, a perceptually guided color balance mechanism ensures\nnatural color restoration. Experimental results on benchmark datasets\ndemonstrate superior performance over state-of-the-art methods in contrast\nenhancement, color correction, and structural preservation, making the\nframework robust for underwater imaging applications.",
        "Recent advances in text-to-image diffusion models have been driven by the\nincreasing availability of paired 2D data. However, the development of 3D\ndiffusion models has been hindered by the scarcity of high-quality 3D data,\nresulting in less competitive performance compared to their 2D counterparts. To\naddress this challenge, we propose repurposing pre-trained 2D diffusion models\nfor 3D object generation. We introduce Gaussian Atlas, a novel representation\nthat utilizes dense 2D grids, enabling the fine-tuning of 2D diffusion models\nto generate 3D Gaussians. Our approach demonstrates successful transfer\nlearning from a pre-trained 2D diffusion model to a 2D manifold flattened from\n3D structures. To support model training, we compile GaussianVerse, a\nlarge-scale dataset comprising 205K high-quality 3D Gaussian fittings of\nvarious 3D objects. Our experimental results show that text-to-image diffusion\nmodels can be effectively adapted for 3D content generation, bridging the gap\nbetween 2D and 3D modeling.",
        "While conventional Transformers generally operate on sequence data, they can\nbe used in conjunction with structure models, typically SE(3)-invariant or\nequivariant graph neural networks (GNNs), for 3D applications such as protein\nstructure modelling. These hybrids typically involve either (1)\npreprocessing\/tokenizing structural features as input for Transformers or (2)\ntaking Transformer embeddings and processing them within a structural\nrepresentation. However, there is evidence that Transformers can learn to\nprocess structural information on their own, such as the AlphaFold3 structural\ndiffusion model. In this work we show that Transformers can function\nindependently as structure models when passed linear embeddings of coordinates.\nWe first provide a theoretical explanation for how Transformers can learn to\nfilter attention as a 3D Gaussian with learned variance. We then validate this\ntheory using both simulated 3D points and in the context of masked token\nprediction for proteins. Finally, we show that pre-training protein Transformer\nencoders with structure improves performance on a downstream task, yielding\nbetter performance than custom structural models. Together, this work provides\na basis for using standard Transformers as hybrid structure-language models.",
        "Self-Correction aims to enable large language models (LLMs) to self-verify\nand self-refine their initial responses without external feedback. However,\nLLMs often fail to effectively self-verify and generate correct feedback,\nfurther misleading refinement and leading to the failure of self-correction,\nespecially in complex reasoning tasks. In this paper, we propose Program-driven\nSelf-Correction (ProgCo). First, program-driven verification (ProgVe) achieves\ncomplex verification logic and extensive validation through self-generated,\nself-executing verification pseudo-programs. Then, program-driven refinement\n(ProgRe) receives feedback from ProgVe, conducts dual reflection and refinement\non both responses and verification programs to mitigate misleading of incorrect\nfeedback in complex reasoning tasks. Experiments on three instruction-following\nand mathematical benchmarks indicate that ProgCo achieves effective\nself-correction, and can be further enhance performance when combined with real\nprogram tools.",
        "Although Chain-of-Thought (CoT) has achieved remarkable success in enhancing\nthe reasoning ability of large language models (LLMs), the mechanism of CoT\nremains a ``black box''. Even if the correct answers can frequently be\nobtained, existing CoTs struggle to make the reasoning understandable to human.\nIn this paper, we unveil and causalize CoT from a causal perspective to ensure\nboth correctness and understandability of all reasoning steps (to the best of\nour knowledge, the first such). We model causality of CoT via structural causal\nmodels (SCM) to unveil the reasoning mechanism of CoT. To measure the causality\nof CoT, we define the CoT Average Causal Effect (CACE) to test the causal\nrelations between steps. For those steps without causality (wrong or\nunintelligible steps), we design a role-playing causal query algorithm to\ncausalize these steps, resulting a causalized CoT with all steps correct and\nunderstandable. Experimental results on both open-source and closed-source LLMs\ndemonstrate that the causal errors commonly in steps are effectively corrected\nand the reasoning ability of LLMs is significantly improved.",
        "State space models (SSMs) have emerged as an efficient alternative to\ntransformer-based models, offering linear complexity that scales better than\ntransformers. One of the latest advances in SSMs, Mamba, introduces a selective\nscan mechanism that assigns trainable weights to input tokens, effectively\nmimicking the attention mechanism. Mamba has also been successfully extended to\nthe vision domain by decomposing 2D images into smaller patches and arranging\nthem as 1D sequences. However, it remains unclear how these patches interact\nwith (or attend to) each other in relation to their original 2D spatial\nlocation. Additionally, the order used to arrange the patches into a sequence\nalso significantly impacts their attention distribution. To better understand\nthe attention between patches and explore the attention patterns, we introduce\na visual analytics tool specifically designed for vision-based Mamba models.\nThis tool enables a deeper understanding of how attention is distributed across\npatches in different Mamba blocks and how it evolves throughout a Mamba model.\nUsing the tool, we also investigate the impact of different patch-ordering\nstrategies on the learned attention, offering further insights into the model's\nbehavior.",
        "Extended dynamic mode decomposition (EDMD) is a popular data-driven method to\npredict the action of the Koopman operator, i.e., the evolution of an\nobservable function along the flow of a dynamical system. In this paper, we\nleverage a recently-introduced kernel EDMD method for control systems for\ndata-driven model predictive control. Building upon pointwise error bounds\nproportional in the state, we rigorously show practical asymptotic stability of\nthe origin w.r.t. the MPC closed loop without stabilizing terminal conditions.\nThe key novelty is that we avoid restrictive invariance conditions. Last, we\nverify our findings by numerical simulations.",
        "Selecting an appropriate book is crucial for fostering reading habits in\nchildren. While children exhibit varying levels of complexity when generating\noral narratives, the question arises: do children's books also differ in\nnarrative complexity? This study explores the narrative dynamics of literary\ntexts used in schools, focusing on how their complexity evolves across\ndifferent grade levels. Using Word-Recurrence Graph Analysis, we examined a\ndataset of 1,627 literary texts spanning 13 years of education. The findings\nreveal significant exponential growth in connectedness, particularly during the\nfirst three years of schooling, mirroring patterns observed in children's oral\nnarratives. These results highlight the potential of literary texts as a tool\nto support the development of literacy skills.",
        "We introduce Dr. Splat, a novel approach for open-vocabulary 3D scene\nunderstanding leveraging 3D Gaussian Splatting. Unlike existing\nlanguage-embedded 3DGS methods, which rely on a rendering process, our method\ndirectly associates language-aligned CLIP embeddings with 3D Gaussians for\nholistic 3D scene understanding. The key of our method is a language feature\nregistration technique where CLIP embeddings are assigned to the dominant\nGaussians intersected by each pixel-ray. Moreover, we integrate Product\nQuantization (PQ) trained on general large-scale image data to compactly\nrepresent embeddings without per-scene optimization. Experiments demonstrate\nthat our approach significantly outperforms existing approaches in 3D\nperception benchmarks, such as open-vocabulary 3D semantic segmentation, 3D\nobject localization, and 3D object selection tasks. For video results, please\nvisit : https:\/\/drsplat.github.io\/",
        "Prior research yielded many techniques to mitigate software compromise for\nlow-end Internet of Things (IoT) devices. Some of them detect software\nmodifications via remote attestation and similar services, while others\npreventatively ensure software (static) integrity. However, achieving run-time\n(dynamic) security, e.g., control-flow integrity (CFI), remains a challenge.\n  Control-flow attestation (CFA) is one approach that minimizes the burden on\ndevices. However, CFA is not a real-time countermeasure against run-time\nattacks since it requires communication with a verifying entity. This poses\nsignificant risks if safety- or time-critical tasks have memory\nvulnerabilities.\n  To address this issue, we construct EILID - a hybrid architecture that\nensures software execution integrity by actively monitoring control-flow\nviolations on low-end devices. EILID is built atop CASU, a prevention-based\n(i.e., active) hybrid Root-of-Trust (RoT) that guarantees software\nimmutability. EILID achieves fine-grained backward-edge and function-level\nforward-edge CFI via semi-automatic code instrumentation and a secure shadow\nstack.",
        "We introduce the concept of the self-referencing causal cycle (abbreviated\nRECALL) - a mechanism that enables large language models (LLMs) to bypass the\nlimitations of unidirectional causality, which underlies a phenomenon known as\nthe reversal curse. When an LLM is prompted with sequential data, it often\nfails to recall preceding context. For example, when we ask an LLM to recall\nthe line preceding \"O say does that star-spangled banner yet wave\" in the U.S.\nNational Anthem, it often fails to correctly return \"Gave proof through the\nnight that our flag was still there\" - this is due to the reversal curse. It\noccurs because language models such as ChatGPT and Llama generate text based on\npreceding tokens, requiring facts to be learned and reproduced in a consistent\ntoken order. While the reversal curse is often viewed as a limitation, we offer\nevidence of an alternative view: it is not always an obstacle in practice. We\nfind that RECALL is driven by what we designate as cycle tokens - sequences\nthat connect different parts of the training data, enabling recall of preceding\ntokens from succeeding ones. Through rigorous probabilistic formalization and\ncontrolled experiments, we demonstrate how the cycles they induce influence a\nmodel's ability to reproduce information. To facilitate reproducibility, we\nprovide our code and experimental details at\nhttps:\/\/anonymous.4open.science\/r\/remember-B0B8\/.",
        "Accurate spatiotemporal calibration is a prerequisite for multisensor fusion.\nHowever, sensors are typically asynchronous, and there is no overlap between\nthe fields of view of cameras and LiDARs, posing challenges for intrinsic and\nextrinsic parameter calibration. To address this, we propose a calibration\npipeline based on continuous-time and bundle adjustment (BA) capable of\nsimultaneous intrinsic and extrinsic calibration (6 DOF transformation and time\noffset). We do not require overlapping fields of view or any calibration board.\nFirstly, we establish data associations between cameras using Structure from\nMotion (SFM) and perform self-calibration of camera intrinsics. Then, we\nestablish data associations between LiDARs through adaptive voxel map\nconstruction, optimizing for extrinsic calibration within the map. Finally, by\nmatching features between the intensity projection of LiDAR maps and camera\nimages, we conduct joint optimization for intrinsic and extrinsic parameters.\nThis pipeline functions in texture-rich structured environments, allowing\nsimultaneous calibration of any number of cameras and LiDARs without the need\nfor intricate sensor synchronization triggers. Experimental results demonstrate\nour method's ability to fulfill co-visibility and motion constraints between\nsensors without accumulating errors.",
        "Autonomous LLM-based agents have emerged as a powerful paradigm for complex\ntask execution, yet the field lacks standardized tools for development,\ndeployment, distribution and discovery of agents. We present Cerebrum, an Agent\nSDK for AIOS that addresses this gap through three key components: (1) a\ncomprehensive SDK featuring a modular four-layer architecture for agent\ndevelopment, encompassing LLM, memory, storage, and tool management; (2) a\ncommunity-driven Agent Hub for sharing and discovering agents, complete with\nversion control and dependency management; (3) an interactive web interface for\ntesting and evaluating agents. The platform's effectiveness is demonstrated\nthrough implementations of various agent architectures, including Chain of\nThought (CoT), ReAct, and tool-use agents. Cerebrum advances the field by\nproviding a unified framework that standardizes agent development while\nmaintaining flexibility for researchers and developers to innovate and\ndistribute their agents. The live website is at https:\/\/app.aios.foundation,\nthe code is at https:\/\/github.com\/agiresearch\/Cerebrum, and video is at\nhttps:\/\/app.aios.foundation\/video-demo.",
        "We fix an excellent regular noetherian scheme $S$ over ${\\mathbf Z}_{(p)}$\nsatisfying a certain finiteness condition. For a constructible \\'etale sheaf\n${\\cal F}$ on a regular scheme $X$ of finite type over $S$, we introduce a\nvariant of the singular support relatively to $S$ and prove the existence of a\nsaturated relative variant of the singular support by adopting the method of\nBeilinson using the Radon transform. We may deduce the existence of the\nsingular support itself, if we admit an expected property on the micro support\nof tensor product and if the scheme $X$ is sufficiently ramified over the base\n$S$.",
        "This paper presents a novel approach for hazard analysis in dashcam footage,\naddressing the detection of driver reactions to hazards, the identification of\nhazardous objects, and the generation of descriptive captions. We first\nintroduce a method for detecting driver reactions through speed and sound\nanomaly detection, leveraging unsupervised learning techniques. For hazard\ndetection, we employ a set of heuristic rules as weak classifiers, which are\ncombined using an ensemble method. This ensemble approach is further refined\nwith differential privacy to mitigate overconfidence, ensuring robustness\ndespite the lack of labeled data. Lastly, we use state-of-the-art\nvision-language models for hazard captioning, generating descriptive labels for\nthe detected hazards. Our method achieved the highest scores in the Challenge\non Out-of-Label in Autonomous Driving, demonstrating its effectiveness across\nall three tasks. Source codes are publicly available at\nhttps:\/\/github.com\/ffyyytt\/COOOL_2025.",
        "We construct algebraic families of smooth affine $\\mathbb{A}^1$-contractible\nvarieties of every dimension $n\\geq 4$ over fields of characteristic zero which\nare non-isomorphic to affine spaces and potential counterexamples to the\nZariski Cancellation Problem. We further prove that these families of varieties\nare also counter examples to the generalized Cancellation problem.",
        "This paper provides an explanation of NTRU, a post quantum encryption scheme,\nwhile also providing a gentle introduction to cryptography. NTRU is a very\nefficient lattice based cryptosystem that appears to be safe against attacks by\nquantum computers. NTRU's efficiency suggests that it is a strong candidate as\nan alternative to RSA, ElGamal, and ECC for the post quantum world. The paper\nbegins with an introduction to cryptography and security proofs for\ncryptographic schemes before explaining the NTRU cryptosystem and culminating\nwith a proof that the original presentation of NTRU is not IND-CPA secure. We\nwill conclude by mentioning padding schemes to NTRU that are provably IND-CCA2\nsecure in the random oracle model. The paper is designed to be accessible to\nanyone with minimal background in abstract algebra and number theory - no\nprevious knowledge of cryptography is assumed. Given the author's lack of\nfamiliarity with the subject, this paper aims to be an expository work rather\nthan to provide new insights to the subject matter."
      ]
    }
  },
  {
    "id":2411.19475,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"ImageNet: A large-scale hierarchical image database",
    "start_abstract":"The explosion of image data on the Internet has potential to foster more sophisticated and robust models algorithms index, retrieve, organize interact with images multimedia data. But exactly how such can be harnessed organized remains a critical problem. We introduce here new database called \"ImageNet\", large-scale ontology built upon backbone WordNet structure. ImageNet aims populate majority 80,000 synsets an average 500\u20131000 clean full resolution images. This will result in tens millions annotated by semantic hierarchy WordNet. paper offers detailed analysis its current state: 12 subtrees 5247 3.2 million total. show that is much larger scale diversity accurate than datasets. Constructing challenging task. describe collection scheme Amazon Mechanical Turk. Lastly, we illustrate usefulness through three simple applications object recognition, classification automatic clustering. hope scale, accuracy, hierarchical structure offer unparalleled opportunities researchers computer vision community beyond.",
    "start_categories":[
      "cs.CV"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b3"
      ],
      "title":[
        "Radio galaxy zoo EMU: Towards a semantic radio galaxy morphology taxonomy"
      ],
      "abstract":[
        "ABSTRACT We present a novel natural language processing (NLP) approach to deriving plain English descriptors for science cases otherwise restricted by obfuscating technical terminology. address the limitations of common radio galaxy morphology classifications applying this approach. experimentally derive set semantic tags Radio Galaxy Zoo EMU (Evolutionary Map Universe) project and wider astronomical community. collect 8486 annotations morphology, from which we taxonomy tags. The are English. result is an extensible framework, more flexible, easily communicated, sensitive rare feature combinations, indescribable using current framework astronomy classifications."
      ],
      "categories":[
        "astro-ph.CO"
      ]
    },
    "list":{
      "title":[
        "New insights on a sign-switching $\\Lambda$",
        "Looking at infrared background radiation anisotropies with Spitzer:\n  large scale anisotropies and their implications",
        "DESI dark secrets",
        "Implementing a Robust Test of Galaxy Catalogue Completeness for Dark\n  Siren Measurements of the Hubble Constant",
        "3D Vortices and rotating solitons in ultralight dark matter",
        "Nonparametric reconstructions of dynamical dark energy via flexknots",
        "Gravitational lensing: towards combining the multi-messengers",
        "Observed unequal-time power spectrum",
        "The Preference for Evolving Dark Energy from Cosmological Distance\n  Measurements and Possible Signatures in the Growth Rate of Perturbations",
        "Current constraints on cosmological scenarios with very low reheating\n  temperatures",
        "Disentangling CMB $\\mu$ and $y$ spectral distortions from foregrounds\n  with poorly defined spectral shapes",
        "Foreground Removal in Ground-Based CMB Observations Using a Transformer\n  Model",
        "Euclid Quick Data Release (Q1). The role of cosmic connectivity in\n  shaping galaxy clusters",
        "Improved Decoding of Tanner Codes",
        "Anomaly Detection to identify Transients in LSST Time Series Data",
        "Cherenkov detector with wavelength-shifting fiber readout for muon\n  tomography applications",
        "Diagnosing Quantum Many-body Chaos in Non-Hermitian Quantum Spin Chain\n  via Krylov Complexity",
        "Efficient Bayesian Computation Using Plug-and-Play Priors for Poisson\n  Inverse Problems",
        "Energy Reconstruction of Non-fiducial Electron-Positron Events in the\n  DAMPE Experiment Using Convolutional Neural Networks",
        "Knowledge Phenomenology Research of Future Industrial Iconic Product\n  Innovation",
        "Impact of pH and chloride content on the biodegradation of magnesium\n  alloys for medical implants: An in vitro and phase-field study",
        "Concentration of Measure for Distributions Generated via Diffusion\n  Models",
        "A Comprehensive Framework for Electroweak Phase Transitions: Thermal\n  History and Dynamics from Bubble Nucleation to Percolation",
        "On the inverse-closedness of operator-valued matrices with polynomial\n  off-diagonal decay",
        "Multivariate Frequent Stability and Diam-Mean Equicontinuity",
        "Global existence for semi-linear hyperbolic equations in a neighbourhood\n  of future null infinity",
        "Growth Laws and Universality in 2-TIPS: Microscopic and Coarse grained\n  approach",
        "High pressure structural and lattice dynamics study of\n  {\\alpha}-In$_2$Se$_3$"
      ],
      "abstract":[
        "The proposal for a sudden sign-switching cosmological constant $\\Lambda$ in\nthe local universe, emulating a phase transition from anti-de Sitter (AdS) to\nde Sitter (dS) space, has markedly revamped the fit to observational data and\nlays out a propitious framework for ameliorating major cosmological tensions,\nsuch as the $H_0$ and $S_8$ tensions. This proposal is widely known as\n$\\Lambda_s$CDM. We investigate the possibility that $\\Lambda$ does not only\nflip sign at the transition but has also different curvature radii in the AdS\nand dS phases. We show that the critical redshift of the transition $z_c$ is\nstrongly correlated with the vacuum energy in the AdS phase\n$\\Omega_{\\Lambda_-}$, and that these two variables do not correlate strongly\nwith the other cosmological parameters. We also show that the cost of adding an\nadditional parameter to the $\\Lambda_s$CDM cosmological model does not improve\nthe goodness of fit. Armed with our findings, we demonstrate that for a proper\nchoice of $z_c$, the vacuum energy in the dS phase may not necessarily be\n$-\\Omega_{\\Lambda_-}$, for comparable degree of conformity between the model\nprediction and experimental data.",
        "We use Spitzer\/IRAC deep exposure data covering two significantly larger than\nbefore sky areas to construct maps suitable for evaluating source-subtracted\nfluctuations in the cosmic infrared background (CIB). The maps are constructed\nusing the self-calibration methodology eliminating artifacts to sufficient\naccuracy and subset maps are selected in each area containing approximately\nuniform exposures. These maps are clipped and removed of known sources and then\nFourier transformed to probe the CIB anisotropies to new larger scales. The\npower spectrum of the resultant CIB anisotropies is measured from the data to\n>1 degree revealing the component well above that from remaining known galaxies\non scales >1 arcmin. The fluctuations are demonstrated to be free of Galactic\nand Solar System foreground contributions out to the largest scales measured.\nWe discuss the proposed theories for the origin of the excess CIB anisotropies\nin light of the new data. Out of these, the model where the CIB fluctuation\nexcess originates from the granulation power due to LIGO-observed primordial\nblack holes as dark matter appears most successful in accounting for all\nobservations related to the measured CIB power amplitude and spatial structure,\nincluding the measured coherence between the CIB and unresolved cosmic X-ray\nbackground (CXB). Finally we point out the use of the data to probe the CIB-CXB\ncross-power to new scales and higher accuracy. We also discuss the synergy of\nthese data with future CIB programs at shorter near-IR wavelengths with deep\nwide surveys and sub-arcsecond angular resolution as provided by Euclid and\nRoman space missions.",
        "The first year results of DESI provide evidence that dark energy may not be\nquantum vacuum energy ($\\Lambda$). If true, this would be an extraordinary\ndevelopment in the 25-year quest to understand cosmic acceleration. The\nbest-fit DESI $w_0w_a$ models for dark energy, which underpin the claim, have\nvery strange behavior. They achieve a maximum dark energy density around\n$z\\simeq 0.4$ and rapidly decrease before and after. We explore\nphysics-motivated models where the dark energy is a rolling scalar-field. Each\nof our four scalar-field models is characterized by one dimensionless parameter\n$\\beta$, which in the limit of $\\beta \\rightarrow 0$ reduces to $\\Lambda$CDM.\nWhile none of our models fit the DESI data significantly better than\n$\\Lambda$CDM, for values of $\\beta$ of order unity, they fit about as well as\n$\\Lambda$CDM. Each scalar field model makes different predictions for the age\nof the Universe, which might be used to discriminate amongst them. For small\nvalues of $\\beta$, the dimensionsless initial slope of the scalar field\npotential links the predictions of different scalar field models. And for small\nvalues of $\\beta$, $w_0w_a$ models can marginally represent the predictions of\na scalar-field model at the current precision needed. However, with\nincreasingly precise distance measurements, over a larger redshift range,\nexplicit modeling of the scalar-field evolution is already and will continue to\nbe essential to testing alternatives to $\\Lambda$.",
        "We present the application of a robust test of galaxy catalogue completeness\nto the gwcosmo pipeline. The method implements a straightforward statistical\ntest for determining the apparent magnitude completeness limit of a\nmagnitude-redshift sample. This offers an improved, less conservative approach\ncompared with how galaxy catalogue completeness is currently estimated in the\ngwcosmo gravitational wave cosmology pipeline for determining the Hubble\nconstant $H_{0}$. The test also does not require prior knowledge of the\nluminosity function, and thus returns a more robust estimate of the limiting\napparent magnitude for a magnitude-redshift sample of galaxies. For GWTC-1\nresults using $B_{J}$-band photometry of galaxies in the GLADE catalogue, we\nfind a $3.4\\%$ improvement on the inference of $H_{0}$ using dark sirens only\nand a $1.3\\%$ improvement for the combined posterior with GW170817. Using\nGLADE+, there is a $8.6\\%$ improvement with dark sirens only and a $6.3\\%$\nimprovement for the combined posterior with GW170817. However, the final\nposterior on $H_{0}$ using the GWTC-3 dataset with the GLADE+ $K$-band shows no\nimprovement when applying the robust method. This is because the GLADE+ galaxy\ncatalogue provides little or no coverage in the $K$-band for any of the GWTC-3\nevents. With the use of deeper galaxy catalogues in future gravitational wave\ncosmology analyses, the adoption of a less conservative estimate of magnitude\ncompleteness will become increasingly important.",
        "We study the formation and the dynamics of vortex lines in rotating scalar\ndark matter halos, focusing on models with quartic repulsive self-interactions.\nIn the nonrelativistic regime, vortex lines and their lattices arise from the\nGross-Pitaevskii equation of motion, as for superfluids and Bose-Einstein\ncondensates studied in laboratory experiments. Indeed, in such systems\nvorticity is supported by the singularities of the phase of the scalar field,\nwhich leads to a discrete set of quantized vortices amid a curl-free velocity\nbackground. In the continuum limit where the number of vortex lines becomes\nvery large, we find that the equilibrium solution is a rotating soliton that\nobeys a solid-body rotation, with an oblate density profile aligned with the\ndirection of the total spin. This configuration is dynamically stable provided\nthe rotational energy is smaller than the self-interaction and gravitational\nenergies. Using numerical simulations in the Thomas-Fermi regime, with\nstochastic initial conditions for a spherical halo with a specific averaged\ndensity profile and angular momentum, we find that a rotating soliton always\nemerges dynamically, within a few dynamical times, and that a network of vortex\nlines aligned with the total spin fills its oblate profile. These vertical\nvortex lines form a regular lattice in the equatorial plane, in agreement with\nthe analytical predictions of uniform vortex density and solid-body rotation.\nThese vortex lines might further extend between halos to form the backbone of\nspinning cosmic filaments.",
        "Recent cosmological surveys have provided unprecedented datasets that can be\nused to reconstruct the history of the dark energy equation of state. In this\nwork, a free-form \"flexknot'' parameterisation is employed to represent $w(a)$\nas a linear spline between free-moving nodes, the number of which may vary. By\ncombining DESI Baryon Acoustic Oscillation measurements with Pantheon+ or DES5Y\nsupernovae, the functional posteriors of $w(a)$ reveal an unexpected W-shaped\nstructure. While the Bayesian evidence may still favour $\\Lambda$CDM, the\nrobustness of these results suggests the structure is indeed present in the\ndata. The tension $R$-statistic and suspiciousness have been marginalised over\nmodels, and demonstrate that while the reconstructions from DESI and Pantheon+\nagree, DESI and DES5Y do not. We conclude that, while there is no smoking gun\nfor dynamical dark energy, the structure unearthed in this work is generally\ntoo complex to be captured by the restrictive $w$CDM or CPL parameterisations.",
        "The next generation of gravitational wave detectors and electromagnetic\ntelescopes are beckoning the onset of the multi-messenger era and the exciting\nscience that lies ahead. Multi-messenger strong gravitational lensing will help\nprobe some of the most important questions of the Universe in an unprecedented\nmanner. In particular, understanding the nature of gravitational wave sources,\nthe underlying physical processes and mechanisms that produce emissions well\nbefore or right until the time of the merger, their associations to the\nseemingly distinct populations of gamma ray bursts, fast radio bursts and\nkilonovae. Not to mention, multi-messenger lensing will offer unique probes of\ntest of gravity models and constraints on cosmological parameters complementary\nto other probes. Enabling multi-messenger science calls for concerted follow-up\nefforts and development of new and shared resources required in the community.",
        "The next generation of galaxy surveys will provide highly precise\nmeasurements of galaxy clustering, therefore requiring a corresponding\naccuracy. Current approaches, which rely on approximations and idealized\nassumptions, may fall short in capturing the level of detail required for\nhigh-precision observations. In order to increase the modeling accuracy,\nrecently, unequal-time contributions to the galaxy power spectrum have been\nintroduced in order to include the effects of radial correlations. We present a\ngeneralization of the formalism for the observed unequal-time power spectrum,\nthat includes Doppler and local general relativistic corrections, plus local\nprimordial non-Gaussianity. We find that unequal time corrections can\npotentially mimic an effective $f_{\\mathrm{NL}}$ of order unity. We provide a\nfirst assessment of the significance of unequal-time corrections for future\ngalaxy clustering experiments, estimating a Signal-to-Noise-Ratio of $\\sim3$\nfor Stage IV-like surveys.",
        "In this study, we use a flexible parametrization of the equation of state of\ndark energy to explore its possible evolution with datasets from the Dark\nEnergy Spectroscopic Instrument (DESI), Planck cosmic microwave background, and\neither the 5-year Dark Energy Survey (DES) or the Pantheon+ (PP) supernova (SN)\ncompilation. This parametrization, called transitional dark energy, allows for\nrapid changes in the equation of state but also changes like that in the\nChevallier-Polarski-Linder parametrization. We find a 3.8{\\sigma} preference\nfor evolving dark energy over {\\Lambda}CDM with the DES dataset and a weaker\n2.4{\\sigma} preference when using the PP dataset. This corroborates the finding\nof the DESI Collaboration, who found that their baryon acoustic oscillation\ndata preferred evolving dark energy when fit with the CPL parametrization of\nthe equation of state. Our analysis reveals no significant outliers in the DESI\ndata around the TDE best-fit, while the data is asymmetrically distributed\naround the {\\Lambda}CDM best-fit model such that the measured distances are on\naverage smaller. The DESI and SN data both prefer an expansion history that\nimplies a higher dark energy density around z=0.5 than in the\nPlanck-{\\Lambda}CDM model, with the inferred equation of state being greater\nthan -1 around z=0 and close to or below -1 at z>0.5. We show that when the\nexpansion rate is greater than that in the Planck-{\\Lambda}CDM model (around\nz=0.5), the growth rate calculated assuming General Relativity is suppressed\nrelative to the Planck-{\\Lambda}CDM model, and it rebounds as the expansion\nrate differences between the models become smaller closer to the present time.\nThe resulting flattening of the $f\\sigma_8(z)$ curve compared to the\n{\\Lambda}CDM model could be an independent signature of the temporal evolution\nof dark energy.",
        "We present an updated analysis of cosmological models with very low reheating\nscenarios ($T_\\text{RH} \\sim \\mathcal{O}(\\text{MeV})$). Our study includes a\nmore precise computation of neutrino distribution functions, leveraging the\nlatest datasets from cosmological surveys. We perform a joint analysis that\ncombines constraints from Big Bang Nucleosynthesis, the Cosmic Microwave\nBackground, and galaxy surveys, alongside separate investigations of these\ndatasets, carefully assessing the impact of different choices of priors. At the\n$95\\%$ confidence level, we establish a lower bound on the reheating\ntemperature of $T_\\text{RH} > 5.96 \\; \\text{MeV} $, representing the most\nstringent constraint to date.",
        "We have presented a new approach to separate small spectral $\\mu$ and $y$\ndistortions of the CMB from foreground components with poorly defined spectral\nshapes. Our linear method, called the Least Response Method (LRM), is based on\nthe idea of simultaneously minimizing the response to all possible foregrounds\nand photon noise while maintaining a constant response to the useful signal. We\ncompared our approach with the mILC method, which is a modification of the\nInternal Linear Combination previously used for CMB anisotropy maps, and proved\nthe advantages of LRM. In addition, we found the optimal temperature of the\ntelescope optical system for any experiments related to the study of the CMB\n$\\mu$ distortions.",
        "We present a novel method for Cosmic Microwave Background (CMB) foreground\nremoval based on deep learning techniques. This method employs a Transformer\nmodel, referred to as \\texttt{TCMB}, which is specifically designed to\neffectively process HEALPix-format spherical sky maps. \\texttt{TCMB} represents\nan innovative application in CMB data analysis, as it is an image-based\ntechnique that has rarely been utilized in this field. Using simulated data\nwith noise levels representative of current ground-based CMB polarization\nobservations, the \\texttt{TCMB} method demonstrates robust performance in\nremoving foreground contamination. The mean absolute variance for the\nreconstruction of the noisy CMB Q\/U map is significantly less than the CMB\npolarization signal. To mitigate biases caused by instrumental noise, a\ncross-correlation approach using two half-mission maps was employed,\nsuccessfully recovering CMB EE and BB power spectra that align closely with the\ntrue values, and these results validate the effectiveness of the \\texttt{TCMB}\nmethod. Compared to the previously employed convolutional neural network\n(CNN)-based approach, the \\texttt{TCMB} method offers two significant\nadvantages: (1) It demonstrates superior effectiveness in reconstructing CMB\npolarization maps, outperforming CNN-based methods. (2) It can directly process\nHEALPix spherical sky maps without requiring rectangular region division, a\nstep necessary for CNN-based approaches that often introduces uncertainties\nsuch as boundary effects. This study highlights the potential of\nTransformer-based models as a powerful tool for CMB data analysis, offering a\nsubstantial improvement over traditional CNN-based techniques.",
        "The matter distribution around galaxy clusters is distributed over several\nfilaments, reflecting their positions as nodes in the large-scale cosmic web.\nThe number of filaments connected to a cluster, namely its connectivity, is\nexpected to affect the physical properties of clusters. Using the first Euclid\ngalaxy catalogue from the Euclid Quick Release 1 (Q1), we investigate the\nconnectivity of galaxy clusters and how it correlates with their physical and\ngalaxy member properties. Around 220 clusters located within the three fields\nof Q1 (covering $\\sim 63 \\ \\text{deg}^2$), are analysed in the redshift range\n$0.2 < z < 0.7$. Due to the photometric redshift uncertainty, we reconstruct\nthe cosmic web skeleton, and measure cluster connectivity, in 2-D projected\nslices with a thickness of 170 comoving $h^{-1}.\\text{Mpc}$ and centred on each\ncluster redshift, by using two different filament finder algorithms on the most\nmassive galaxies ($M_*\\ > 10^{10.3} \\ M_\\odot$). In agreement with previous\nmeasurements, we recover the mass-connectivity relation independently of the\nfilament detection algorithm, showing that the most massive clusters are, on\naverage, connected to a larger number of cosmic filaments, consistent with\nhierarchical structure formation models. Furthermore, we explore possible\ncorrelations between connectivities and two cluster properties: the fraction of\nearly-type galaxies and the S\\'ersic index of galaxy members. Our result\nsuggests that the clusters populated by early-type galaxies exhibit higher\nconnectivity compared to clusters dominated by late-type galaxies. These\npreliminary investigations highlight our ability to quantify the impact of the\ncosmic web connectivity on cluster properties with Euclid.",
        "In this paper, we present improved decoding algorithms for expander-based\nTanner codes.\n  We begin by developing a randomized linear-time decoding algorithm that,\nunder the condition that $ \\delta d_0 > 2 $, corrects up to $ \\alpha n $ errors\nfor a Tanner code $ T(G, C_0) $, where $ G $ is a $ (c, d, \\alpha, \\delta)\n$-bipartite expander with $n$ left vertices, and $ C_0 \\subseteq \\mathbb{F}_2^d\n$ is a linear inner code with minimum distance $ d_0 $. This result improves\nupon the previous work of Cheng, Ouyang, Shangguan, and Shen (RANDOM 2024),\nwhich required $ \\delta d_0 > 3 $.\n  We further derandomize the algorithm to obtain a deterministic linear-time\ndecoding algorithm with the same decoding radius. Our algorithm improves upon\nthe previous deterministic algorithm of Cheng et al.\\ by achieving a decoding\nradius of $ \\alpha n $, compared with the previous radius of $\n\\frac{2\\alpha}{d_0(1 + 0.5c\\delta) }n$.\n  Additionally, we investigate the size-expansion trade-off introduced by the\nrecent work of Chen, Cheng, Li, and Ouyang (IEEE TIT 2023), and use it to\nprovide new bounds on the minimum distance of Tanner codes. Specifically, we\nprove that the minimum distance of a Tanner code $T(G,C_0)$ is approximately\n$f_\\delta^{-1} \\left( \\frac{1}{d_0} \\right) \\alpha n $, where $ f_\\delta(\\cdot)\n$ is the Size-Expansion Function. As another application, we improve the\ndecoding radius of our decoding algorithms from $\\alpha n$ to approximately\n$f_\\delta^{-1}(\\frac{2}{d_0})\\alpha n$.",
        "We introduce a novel approach to detecting microlensing events and other\ntransients in light curves, utilising the isolation forest (iForest) algorithm\nfor anomaly detection. Focusing on the Legacy Survey of Space and Time by the\nVera C. Rubin Observatory, we show that an iForest trained on signal-less light\ncurves can efficiently identify microlensing events by different types of dark\nobjects and binaries, as well as variable stars. We further show that the\niForest has real-time applicability through a drip-feed analysis, demonstrating\nits potential as a valuable tool for LSST alert brokers to efficiently\nprioritise and classify transient candidates for follow-up observations.",
        "Cherenkov detectors have been extensively developed and utilized in various\nscientific fields, including particle physics, astrophysics, and nuclear\nengineering. These detectors operate based on Cherenkov radiation, which is\nemitted when a charged particle traverses a dielectric medium at a velocity\ngreater than the phase velocity of light in that medium. In this work, we\npresent the development of a Cherenkov radiation detector designed for a muon\ntomography system with high spatial resolution, employing wavelength-shifting\n(WLS) fiber readout. The detector consists of two large-area Cherenkov\nradiators, each measuring 1 m x 1 m, with each read out by WLS fibers arranged\northogonally to determine the x and y coordinates of muon hit positions. The\nsystem is modeled using the GEANT4 simulation package, and the achieved\nposition resolution is 1.8 mm+-0.1 (FWHM). This design enables precise tracking\nof muon trajectories, making it suitable for high-resolution imaging\napplications in muon tomography.",
        "We investigate the phase transitions from chaotic to non-chaotic dynamics in\na quantum spin chain with a local non-Hermitian disorder, which can be realized\nwith a Rydberg atom array setting. As the disorder strength increases, the\nemergence of non-chaotic dynamics is qualitatively captured through the\nsuppressed growth of Krylov complexity, and quantitatively identified through\nthe reciprocity breaking of Krylov space. We further find that the localization\nin Krylov space generates another transition in the weak disorder regime,\nsuggesting a weak ergodicity breaking. Our results closely align with\nconventional methods, such as the entanglement entropy and complex level\nspacing statistics, and pave the way to explore non-Hermitian phase transitions\nusing Krylov complexity and associated metrics.",
        "This paper introduces a novel plug-and-play (PnP) Langevin sampling\nmethodology for Bayesian inference in low-photon Poisson imaging problems, a\nchallenging class of problems with significant applications in astronomy,\nmedicine, and biology. PnP Langevin sampling algorithms offer a powerful\nframework for Bayesian image restoration, enabling accurate point estimation as\nwell as advanced inference tasks, including uncertainty quantification and\nvisualization analyses, and empirical Bayesian inference for automatic model\nparameter tuning. However, existing PnP Langevin algorithms are not well-suited\nfor low-photon Poisson imaging due to high solution uncertainty and poor\nregularity properties, such as exploding gradients and non-negativity\nconstraints. To address these challenges, we propose two strategies for\nextending Langevin PnP sampling to Poisson imaging models: (i) an accelerated\nPnP Langevin method that incorporates boundary reflections and a Poisson\nlikelihood approximation and (ii) a mirror sampling algorithm that leverages a\nRiemannian geometry to handle the constraints and the poor regularity of the\nlikelihood without approximations. The effectiveness of these approaches is\ndemonstrated through extensive numerical experiments and comparisons with\nstate-of-the-art methods.",
        "The Dark Matter Particle Explorer (DAMPE) is a space-based Cosmic-Ray (CR)\nobservatory with the aim, among others, to study Cosmic-Ray Electrons (CREs) up\nto 10 TeV. Due to the low CRE rate at multi-TeV energies, we aim to increasing\nthe acceptance by selecting events outside the fiducial volume. The complex\ntopology of non-fiducial events requires the development of a novel energy\nreconstruction method. We propose the usage of Convolutional Neural Networks\nfor a regression task to recover an accurate estimation of the initial energy.",
        "Iconic products, as innovative carriers supporting the development of future\nindustries, are key breakthrough points for driving the transformation of new\nquality productive forces. This article is grounded in the philosophy of\ntechnology and examines the evolution of human civilization to accurately\nidentify the patterns of product innovation. By integrating theories from\nsystems science, it analyzes the intrinsic logical differences between\ntraditional products and iconic products. The study finds that iconic products\nare based on a comprehensive knowledge system that integrates explicit and\ntacit knowledge, enabling them to adapt to complex dynamic environments.\nTherefore, based on the method of phenomenological essence reduction and the\nprocess of specialized knowledge acquisition, this study establishes the first\nprinciple of knowledge phenomenology: \"knowledge generation-moving from the\ntacit to the explicit-moving from the explicit to the tacit-fusion of the\nexplicit and tacit.\" Grounded in knowledge phenomenology, it reconstructs the\nproduct design evolution process and establishes a forward innovative design\nframework for iconic products, consisting of \"design problem space-explicit\nknowledge space-tacit knowledge space-innovative solution space.\" Furthermore,\nbased on FBS design theory, it develops a disruptive technology innovation\nforecasting framework of \"technology problem space-knowledge base\nprediction-application scenario prediction-coupled technology prediction,\"\nwhich collectively advances the innovation systems engineering of iconic\nproducts. In light of the analysis of the global future industrial competitive\nlandscape, it proposes a strategy for enhancing embodied intelligence in iconic\nproducts.",
        "The individual contributions of pH and chloride concentration to the\ncorrosion kinetics of bioabsorbable magnesium (Mg) alloys remain unresolved\ndespite their significant roles as driving factors in Mg corrosion. This study\ndemonstrates and quantifies hitherto unknown separate effects of pH and\nchloride content on the corrosion of Mg alloys pertinent to biomedical implant\napplications. The experimental setup designed for this purpose enables the\nquantification of the dependence of corrosion on pH and chloride concentration.\nThe in vitro tests conclusively demonstrate that variations in chloride\nconcentration, relevant to biomedical applications, have a negligible effect on\ncorrosion kinetics. The findings identify pH as a critical factor in the\ncorrosion of bioabsorbable Mg alloys. A variationally consistent phase-field\nmodel is developed for assessing the degradation of Mg alloys in biological\nfluids. The model accurately predicts the corrosion performance of Mg alloys\nobserved during the experiments, including their dependence on pH and chloride\nconcentration. The capability of the framework to account for mechano-chemical\neffects during corrosion is demonstrated in practical orthopaedic applications\nconsidering bioabsorbable Mg alloy implants for bone fracture fixation and\nporous scaffolds for bone tissue engineering. The strategy has the potential to\nassess the in vitro and in vivo service life of bioabsorbable Mg-based\nbiomedical devices.",
        "We show via a combination of mathematical arguments and empirical evidence\nthat data distributions sampled from diffusion models satisfy a Concentration\nof Measure Property saying that any Lipschitz $1$-dimensional projection of a\nrandom vector is not too far from its mean with high probability. This implies\nthat such models are quite restrictive and gives an explanation for a fact\npreviously observed in the literature that conventional diffusion models cannot\ncapture \"heavy-tailed\" data (i.e. data $\\mathbf{x}$ for which the norm\n$\\|\\mathbf{x}\\|_2$ does not possess a sub-Gaussian tail) well. We then proceed\nto train a generalized linear model using stochastic gradient descent (SGD) on\nthe diffusion-generated data for a multiclass classification task and observe\nempirically that a Gaussian universality result holds for the test error.\n  In other words, the test error depends only on the first and second order\nstatistics of the diffusion-generated data in the linear setting. Results of\nsuch forms are desirable because they allow one to assume the data itself is\nGaussian for analyzing performance of the trained classifier. Finally, we note\nthat current approaches to proving universality do not apply to this case as\nthe covariance matrices of the data tend to have vanishing minimum singular\nvalues for the diffusion-generated data, while the current proofs assume that\nthis is not the case (see Subsection 3.4 for more details). This leaves\nextending previous mathematical universality results as an intriguing open\nquestion.",
        "The electroweak phase transition (EWPT) is crucial for cosmology and particle\nphysics, with a profound impact on electroweak baryogenesis, symmetry breaking,\nand gravitational wave (GW) signals. However, many studies overlook key aspects\nof EWPT dynamics, leading to misidentified patterns and overestimated GW\nsignals. To address these gaps, we present a comprehensive framework for\nanalyzing EWPTs, focusing on the vacuum's thermal history and dynamics from\nbubble nucleation to percolation. Using the $\\mathbb{Z}_2$-odd real scalar\nsinglet model, we demonstrate the occurrence of spontaneous $\\mathbb{Z}_2$\nsymmetry breaking in the high-temperature vacuum, leading to diverse EWPT\nprocesses, including multi-step transitions and inverse symmetry breaking. We\nidentify four distinct EWPT patterns, each characterized by unique\nsymmetry-breaking mechanisms and associated with bubbles exhibiting distinct\nfield configurations, which can be analyzed using a formalism based on energy\ndensity distributions developed here. A key finding is that bubble nucleation\nfails in extremely strong phase transitions (PTs) with low nucleation rates, or\nin ultra-fast PTs involving inverse $s$-bubbles that collapse instantly upon\nformation, both of which lead to false vacuum trapping and the absence of\nobservable GW signals. In first-order PTs where nucleation succeeds, stronger\ntransitions occur later in the universe's evolution, while weaker transitions\nproceed more rapidly. Multi-step transitions involving (inverse) $\\mathbb{Z}_2$\nsymmetry breaking give rise to complex transition sequences and exotic bubble\ndynamics, such as sequential nucleation or the coexistence of bubbles from\ndifferent vacua -- phenomena with significant implications for GW spectra, dark\nmatter, and baryogenesis. This work advances our understanding of EWPT dynamics\nand lays the groundwork for future studies of EWPTs in BSM physics.",
        "We give a self-contained proof of a recently established\n$\\mathcal{B}(\\mathcal{H})$-valued version of Jaffards Lemma. That is, we show\nthat the Jaffard algebra of $\\mathcal{B}(\\mathcal{H})$-valued matrices, whose\noperator norms of their respective entries decay polynomially off the diagonal,\nis a Banach algebra which is inverse-closed in the Banach algebra\n$\\mathcal{B}(\\ell^2(X;\\mathcal{H}))$ of all bounded linear operators on\n$\\ell^2(X;\\mathcal{H})$, the Bochner-space of square-summable\n$\\mathcal{H}$-valued sequences.",
        "In this paper, we introduce and investigate multivariate versions of frequent\nstability and diam-mean equicontinuity. Given a natural number $m > 1$, we call\nthose notions \"frequent $m$-stability\" and \"diam-mean $m$-equicontinuity\". We\nuse these dynamical rigidity properties to characterise systems whose factor\nmap to the maximal equicontinuous factor (MEF) is finite-to-one for a residual\nset, called \"almost finite-to-one extensions\", or a set of full measure, called\n\"almost surely finite-to-one extensions\". In the case of a $\\sigma$-compact,\nlocally compact, abelian acting group it is shown that frequently\n$(m+1)$-stable systems are equivalently characterised as almost $m$-to-one\nextensions of their MEF. Similarly, it is shown that a system is diam-mean\n$(m+1)$-equicontinuous if and only if it is an almost surely $m$-to-one\nextension of its MEF.",
        "In this paper, we establish the global existence of a semi-linear class of\nhyperbolic equations in 3+1 dimensions, that satisfy the bounded weak null\ncondition. We propose a conformal compactification of the future directed\nnull-cone in Minkowski spacetime, enabling us to establish the solution to the\nwave equation in a neighbourhood of future null infinity. Using this framework,\nwe formulate a conformal symmetric hyperbolic Fuchsian system of equations. The\nexistence of solutions to this Fuchsian system follows from an application of\nthe existence theory developed in [1], and [2].",
        "Two temperature induced phase separation(2-TIPS) is a phenomenon observed in\nmixtures of active and passive particles modeled by scalar activity where the\ntemperature of the particle is proportional to its activity. The binary mixture\nof 'hot' and 'cold' particles phase separate when the relative temperature\ndifference between hot and cold particles defined as activity $\\chi$ exceeds a\ndensity dependent critical value. The study of kinetics in 2-TIPS, a\nnon-equilibrium phase separation, is of fundamental importance in statistical\nphysics. In this paper, we investigate 2-TIPS kinetics using molecular dynamics\n(MD) and coarse-grained (CG) modeling in 3D and 2D. The coarse-grained model\ncouples two passive Model B equations for hot and cold particles, with coupling\nterms emulating the energy transfer between them by raising the temperature of\ncold particles and lowering that of hot particles, a key observation from the\nMD simulations. MD simulations reveal that at high densities, phase separation\nbegins immediately after the quench, forming bi-continuous domains rich in hot\nor cold particles, similar to spinodal decomposition in passive systems. These\ninterconnected domains are also observed in the coarse-grained model for the\nmixture's critical composition. Both MD and CG models show dynamic scaling of\nthe correlation function, indicating self-similar domain growth. Regardless of\ndimensionality, both methods report algebraic growth in domain length with a\ngrowth exponent of $1\/3$, known as the Lifshitz-Slyozov exponent, widely\nobserved in passive systems. Our results demonstrate that the universality of\nphase separation kinetics observed in passive systems also extends to the\nnon-equilibrium binary mixture undergoing 2-TIPS.",
        "Layered $\\alpha$-In$_2$Se$_3$has been studied using a concomitant in-situ\nsynchrotron angle dispersive powder x-ray diffraction and Raman spectroscopy\nstudy in a diamond anvil cell up to 60+ GPa, at room temperature. Helium, that\nremains fairly hydrostatic up to the highest pressure in this study, was used\nas the pressure-transmitting medium. The results from both experimental methods\nreveal a pressure-induced structural phase transition from\n$\\alpha$-In$_2$Se$_3$ to a monoclinic $\\beta$'-In2Se3 structure at $\\approx$1\nGPa, in agreement with previous studies. Based on our detailed measurements\nusing both experimental techniques and F-f formalism, the $\\beta$'-In$_2$Se$_3$\nstructure remains stable up to 45 GPa, without a clear indication of a phase\ntransition towards the previously reported $\\beta$-In2Se3 phase. Above this\npressure, In$_2$Se$_3$ adopts a disordered solid-solution-like orthorhombic\nstructure, phase IV. The results are discussed in comparison with the relevant\nprevious studies of $\\alpha$-In$_2$Se$_3$ under pressure."
      ]
    }
  },
  {
    "id":2411.15395,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"Brain\u2013Computer Interface Spellers: A Review",
    "start_abstract":"A Brain\u2013Computer Interface (BCI) provides a novel non-muscular communication method via brain signals. BCI-speller can be considered as one of the first published BCI applications and has opened gate for many advances in field. Although BCI-spellers have been developed during last few decades, to our knowledge, no reviews described different spellers proposed studied this vital The presented speller systems are categorized according major paradigms: P300, steady-state visual evoked potential (SSVEP), motor imagery (MI). Different paradigms require specific electroencephalogram (EEG) signal features lead development appropriate Graphical User Interfaces (GUIs). purpose review is consolidate most successful since 2010, while mentioning some other older which were built explicitly spelling purposes. We aim assist researchers concerned individuals field by illustrating highlights presenting them review. It almost impossible carry out an objective comparison between spellers, each its variables, parameters, conditions. However, gathered information provided taxonomy about helpful, it could identify suitable first-hand users, well opportunities learning from previous studies researchers.",
    "start_categories":[
      "q-bio.NC"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b17"
      ],
      "title":[
        "Language Model-Guided Classifier Adaptation for Brain-Computer Interfaces for Communication"
      ],
      "abstract":[
        "Brain-computer interfaces (BCIs), such as the P300 speller, can provide a means of communication for individuals with severe neuromuscular limitations. BCIs interpret electroencephalography (EEG) signals in order to translate embedded information about user's intent into executable commands control external devices. However, EEG are inherently noisy and nonstationary, posing challenge extended BCI use. Conventionally, classifier is trained via supervised learning an offline calibration session; once trained, deployed online use not updated. As statistics data change over time, performance static may decline It therefore desirable automatically adapt current without requiring recalibration. In existing semi-supervised approach, on labeled then updated using incoming unlabeled classifier-predicted labels. To reduce risk from incorrect predictions, threshold imposed exclude low-confidence label predictions expanded training set when retraining adaptive classifier. this work, we propose language model spelling error correction disambiguation correctness during learning. Results simulations multi-session speller user demonstrate that our language-guided approach significantly improves accuracy relative conventional threshold-based"
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Topology-Driven Attribute Recovery for Attribute Missing Graph Learning\n  in Social Internet of Things",
        "Large Language Model Interface for Home Energy Management Systems",
        "Bridging the Communication Gap: Evaluating AI Labeling Practices for\n  Trustworthy AI Development",
        "Artificial Intelligence in Creative Industries: Advances Prior to 2025",
        "Recommending Actionable Strategies: A Semantic Approach to Integrating\n  Analytical Frameworks with Decision Heuristics",
        "Transformer Dynamics: A neuroscientific approach to interpretability of\n  large language models",
        "Opus: A Workflow Intention Framework for Complex Workflow Generation",
        "Contextual bandits with entropy-based human feedback",
        "Neurosymbolic artificial intelligence via large language models and\n  coherence-driven inference",
        "SPIN-Bench: How Well Do LLMs Plan Strategically and Reason Socially?",
        "A Survey on LLM Test-Time Compute via Search: Tasks, LLM Profiling,\n  Search Algorithms, and Relevant Frameworks",
        "Ontology Generation using Large Language Models",
        "Off-Switching Not Guaranteed",
        "Multilingual Non-Autoregressive Machine Translation without Knowledge\n  Distillation",
        "PanopticSplatting: End-to-End Panoptic Gaussian Splatting",
        "Emergent effects of scaling on the functional hierarchies within large\n  language models",
        "CLDyB: Towards Dynamic Benchmarking for Continual Learning with\n  Pre-trained Models",
        "Evaluating Developer-written Unit Test Case Reduction for Java -- A\n  Replication Study",
        "Non-Gaussianities as a Signature of Quantumness of Quantum Cosmology",
        "Propagation of Gravitational Waves on a Geometric Condensate background\n  of $(R + \\alpha R^{2})$ Origin",
        "A Search for Eclipse Cycles Similar to the Hypersaros: Columbus and the\n  Lunar Eclipse of March 14, 2025",
        "Realistic Clothed Human and Object Joint Reconstruction from a Single\n  Image",
        "Re-Align: Aligning Vision Language Models via Retrieval-Augmented Direct\n  Preference Optimization",
        "A Training-Free Length Extrapolation Approach for LLMs: Greedy Attention\n  Logit Interpolation (GALI)",
        "Demystifying integrable QFTs in AdS: No-go theorems for higher-spin\n  charges",
        "CoopDETR: A Unified Cooperative Perception Framework for 3D Detection\n  via Object Query",
        "Predicting Human Choice Between Textually Described Lotteries",
        "A Study of the Efficacy of Generative Flow Networks for Robotics and\n  Machine Fault-Adaptation"
      ],
      "abstract":[
        "With the advancement of information technology, the Social Internet of Things\n(SIoT) has fostered the integration of physical devices and social networks,\ndeepening the study of complex interaction patterns. Text Attribute Graphs\n(TAGs) capture both topological structures and semantic attributes, enhancing\nthe analysis of complex interactions within the SIoT. However, existing graph\nlearning methods are typically designed for complete attributed graphs, and the\ncommon issue of missing attributes in Attribute Missing Graphs (AMGs) increases\nthe difficulty of analysis tasks. To address this, we propose the\nTopology-Driven Attribute Recovery (TDAR) framework, which leverages\ntopological data for AMG learning. TDAR introduces an improved pre-filling\nmethod for initial attribute recovery using native graph topology.\nAdditionally, it dynamically adjusts propagation weights and incorporates\nhomogeneity strategies within the embedding space to suit AMGs' unique\ntopological structures, effectively reducing noise during information\npropagation. Extensive experiments on public datasets demonstrate that TDAR\nsignificantly outperforms state-of-the-art methods in attribute reconstruction\nand downstream tasks, offering a robust solution to the challenges posed by\nAMGs. The code is available at https:\/\/github.com\/limengran98\/TDAR.",
        "Home Energy Management Systems (HEMSs) help households tailor their\nelectricity usage based on power system signals such as energy prices. This\ntechnology helps to reduce energy bills and offers greater demand-side\nflexibility that supports the power system stability. However, residents who\nlack a technical background may find it difficult to use HEMSs effectively,\nbecause HEMSs require well-formatted parameterization that reflects the\ncharacteristics of the energy resources, houses, and users' needs. Recently,\nLarge-Language Models (LLMs) have demonstrated an outstanding ability in\nlanguage understanding. Motivated by this, we propose an LLM-based interface\nthat interacts with users to understand and parameterize their\n``badly-formatted answers'', and then outputs well-formatted parameters to\nimplement an HEMS. We further use Reason and Act method (ReAct) and few-shot\nprompting to enhance the LLM performance. Evaluating the interface performance\nrequires multiple user--LLM interactions. To avoid the efforts in finding\nvolunteer users and reduce the evaluation time, we additionally propose a\nmethod that uses another LLM to simulate users with varying expertise, ranging\nfrom knowledgeable to non-technical. By comprehensive evaluation, the proposed\nLLM-based HEMS interface achieves an average parameter retrieval accuracy of\n88\\%, outperforming benchmark models without ReAct and\/or few-shot prompting.",
        "As artificial intelligence (AI) becomes integral to economy and society,\ncommunication gaps between developers, users, and stakeholders hinder trust and\ninformed decision-making. High-level AI labels, inspired by frameworks like EU\nenergy labels, have been proposed to make the properties of AI models more\ntransparent. Without requiring deep technical expertise, they can inform on the\ntrade-off between predictive performance and resource efficiency. However, the\npractical benefits and limitations of AI labeling remain underexplored. This\nstudy evaluates AI labeling through qualitative interviews along four key\nresearch questions. Based on thematic analysis and inductive coding, we found a\nbroad range of practitioners to be interested in AI labeling (RQ1). They see\nbenefits for alleviating communication gaps and aiding non-expert\ndecision-makers, however limitations, misunderstandings, and suggestions for\nimprovement were also discussed (RQ2). Compared to other reporting formats,\ninterviewees positively evaluated the reduced complexity of labels, increasing\noverall comprehensibility (RQ3). Trust was influenced most by usability and the\ncredibility of the responsible labeling authority, with mixed preferences for\nself-certification versus third-party certification (RQ4). Our Insights\nhighlight that AI labels pose a trade-off between simplicity and complexity,\nwhich could be resolved by developing customizable and interactive labeling\nframeworks to address diverse user needs. Transparent labeling of resource\nefficiency also nudged interviewee priorities towards paying more attention to\nsustainability aspects during AI development. This study validates AI labels as\na valuable tool for enhancing trust and communication in AI, offering\nactionable guidelines for their refinement and standardization.",
        "The rapid advancements in artificial intelligence (AI), particularly in\ngenerative AI and large language models (LLMs), have profoundly impacted the\ncreative industries by enabling innovative content creation, enhancing\nworkflows, and democratizing access to creative tools. This paper explores the\nsignificant technological shifts since our previous review in 2022,\nhighlighting how these developments have expanded creative opportunities and\nefficiency. These technological advancements have enhanced the capabilities of\ntext-to-image, text-to-video, and multimodal generation technologies. In\nparticular, key breakthroughs in LLMs have established new benchmarks in\nconversational AI, while advancements in image generators have revolutionized\ncontent creation. We also discuss AI integration into post-production\nworkflows, which has significantly accelerated and refined traditional\nprocesses. Despite these innovations, challenges remain, particularly for the\nmedia industry, due to the demands on communication traffic from creative\ncontent. We therefore include data compression and quality assessment in this\npaper. Furthermore, we highlight the trend toward unified AI frameworks capable\nof addressing multiple creative tasks and underscore the importance of human\noversight to mitigate AI-generated inaccuracies. Finally, we explore AI's\nfuture potential in the creative sector, stressing the need to navigate\nemerging challenges to maximize its benefits while addressing associated risks.",
        "We present a novel approach for recommending actionable strategies by\nintegrating strategic frameworks with decision heuristics through semantic\nanalysis. While strategy frameworks provide systematic models for assessment\nand planning, and decision heuristics encode experiential knowledge,these\ntraditions have historically remained separate. Our methodology bridges this\ngap using advanced natural language processing (NLP), demonstrated through\nintegrating frameworks like the 6C model with the Thirty-Six Stratagems. The\napproach employs vector space representations and semantic similarity\ncalculations to map framework parameters to heuristic patterns, supported by a\ncomputational architecture that combines deep semantic processing with\nconstrained use of Large Language Models. By processing both primary content\nand secondary elements (diagrams, matrices) as complementary linguistic\nrepresentations, we demonstrate effectiveness through corporate strategy case\nstudies. The methodology generalizes to various analytical frameworks and\nheuristic sets, culminating in a plug-and-play architecture for generating\nrecommender systems that enable cohesive integration of strategic frameworks\nand decision heuristics into actionable guidance.",
        "As artificial intelligence models have exploded in scale and capability,\nunderstanding of their internal mechanisms remains a critical challenge.\nInspired by the success of dynamical systems approaches in neuroscience, here\nwe propose a novel framework for studying computations in deep learning\nsystems. We focus on the residual stream (RS) in transformer models,\nconceptualizing it as a dynamical system evolving across layers. We find that\nactivations of individual RS units exhibit strong continuity across layers,\ndespite the RS being a non-privileged basis. Activations in the RS accelerate\nand grow denser over layers, while individual units trace unstable periodic\norbits. In reduced-dimensional spaces, the RS follows a curved trajectory with\nattractor-like dynamics in the lower layers. These insights bridge dynamical\nsystems theory and mechanistic interpretability, establishing a foundation for\na \"neuroscience of AI\" that combines theoretical rigor with large-scale data\nanalysis to advance our understanding of modern neural networks.",
        "This paper introduces Workflow Intention, a novel framework for identifying\nand encoding process objectives within complex business environments. Workflow\nIntention is the alignment of Input, Process and Output elements defining a\nWorkflow's transformation objective interpreted from Workflow Signal inside\nBusiness Artefacts. It specifies how Input is processed to achieve desired\nOutput, incorporating quality standards, business rules, compliance\nrequirements and constraints. We adopt an end-to-end Business Artefact Encoder\nand Workflow Signal interpretation methodology involving four steps:\nModality-Specific Encoding, Intra-Modality Attention, Inter-Modality Fusion\nAttention then Intention Decoding. We provide training procedures and critical\nloss function definitions. In this paper we introduce the concepts of Workflow\nSignal and Workflow Intention, where Workflow Signal decomposed into Input,\nProcess and Output elements is interpreted from Business Artefacts, and\nWorkflow Intention is a complete triple of these elements. We introduce a\nmathematical framework for representing Workflow Signal as a vector and\nWorkflow Intention as a tensor, formalizing properties of these objects.\nFinally, we propose a modular, scalable, trainable, attention-based multimodal\ngenerative system to resolve Workflow Intention from Business Artefacts.",
        "In recent years, preference-based human feedback mechanisms have become\nessential for enhancing model performance across diverse applications,\nincluding conversational AI systems such as ChatGPT. However, existing\napproaches often neglect critical aspects, such as model uncertainty and the\nvariability in feedback quality. To address these challenges, we introduce an\nentropy-based human feedback framework for contextual bandits, which\ndynamically balances exploration and exploitation by soliciting expert feedback\nonly when model entropy exceeds a predefined threshold. Our method is\nmodel-agnostic and can be seamlessly integrated with any contextual bandit\nagent employing stochastic policies. Through comprehensive experiments, we show\nthat our approach achieves significant performance improvements while requiring\nminimal human feedback, even under conditions of suboptimal feedback quality.\nThis work not only presents a novel strategy for feedback solicitation but also\nhighlights the robustness and efficacy of incorporating human guidance into\nmachine learning systems. Our code is publicly available:\nhttps:\/\/github.com\/BorealisAI\/CBHF",
        "We devise an algorithm to generate sets of propositions that objectively\ninstantiate graphs that support coherence-driven inference. We then benchmark\nthe ability of large language models (LLMs) to reconstruct coherence graphs\nfrom (a straightforward transformation of) propositions expressed in natural\nlanguage, with promising results from a single prompt to models optimized for\nreasoning. Combining coherence-driven inference with consistency evaluations by\nneural models may advance the state of the art in machine cognition.",
        "Reasoning and strategic behavior in social interactions is a hallmark of\nintelligence. This form of reasoning is significantly more sophisticated than\nisolated planning or reasoning tasks in static settings (e.g., math problem\nsolving). In this paper, we present Strategic Planning, Interaction, and\nNegotiation (SPIN-Bench), a new multi-domain evaluation designed to measure the\nintelligence of strategic planning and social reasoning. While many existing\nbenchmarks focus on narrow planning or single-agent reasoning, SPIN-Bench\ncombines classical PDDL tasks, competitive board games, cooperative card games,\nand multi-agent negotiation scenarios in one unified framework. The framework\nincludes both a benchmark as well as an arena to simulate and evaluate the\nvariety of social settings to test reasoning and strategic behavior of AI\nagents. We formulate the benchmark SPIN-Bench by systematically varying action\nspaces, state complexity, and the number of interacting agents to simulate a\nvariety of social settings where success depends on not only methodical and\nstep-wise decision making, but also conceptual inference of other (adversarial\nor cooperative) participants. Our experiments reveal that while contemporary\nLLMs handle basic fact retrieval and short-range planning reasonably well, they\nencounter significant performance bottlenecks in tasks requiring deep multi-hop\nreasoning over large state spaces and socially adept coordination under\nuncertainty. We envision SPIN-Bench as a catalyst for future research on robust\nmulti-agent planning, social reasoning, and human--AI teaming. Project Website:\nhttps:\/\/spinbench.github.io\/",
        "LLM test-time compute (or LLM inference) via search has emerged as a\npromising research area with rapid developments. However, current frameworks\noften adopt distinct perspectives on three key aspects (task definition, LLM\nprofiling, and search procedures), making direct comparisons challenging.\nMoreover, the search algorithms employed often diverge from standard\nimplementations, and their specific characteristics are not thoroughly\nspecified. In this survey, we provide a comprehensive technical review that\nunifies task definitions and provides modular definitions of LLM profiling and\nsearch procedures. The definitions enable precise comparisons of various LLM\ninference frameworks while highlighting their departures from conventional\nsearch algorithms. We also discuss the applicability, performance, and\nefficiency of these methods. We have updated our content to include the latest\npapers, and the differences between versions are highlighted in the appendix.\nFor further details and ongoing updates, please refer to our GitHub repository:\nhttps:\/\/github.com\/xinzhel\/LLM-Agent-Survey\/blob\/main\/search.md",
        "The ontology engineering process is complex, time-consuming, and error-prone,\neven for experienced ontology engineers. In this work, we investigate the\npotential of Large Language Models (LLMs) to provide effective OWL ontology\ndrafts directly from ontological requirements described using user stories and\ncompetency questions. Our main contribution is the presentation and evaluation\nof two new prompting techniques for automated ontology development: Memoryless\nCQbyCQ and Ontogenia. We also emphasize the importance of three structural\ncriteria for ontology assessment, alongside expert qualitative evaluation,\nhighlighting the need for a multi-dimensional evaluation in order to capture\nthe quality and usability of the generated ontologies. Our experiments,\nconducted on a benchmark dataset of ten ontologies with 100 distinct CQs and 29\ndifferent user stories, compare the performance of three LLMs using the two\nprompting techniques. The results demonstrate improvements over the current\nstate-of-the-art in LLM-supported ontology engineering. More specifically, the\nmodel OpenAI o1-preview with Ontogenia produces ontologies of sufficient\nquality to meet the requirements of ontology engineers, significantly\noutperforming novice ontology engineers in modelling ability. However, we still\nnote some common mistakes and variability of result quality, which is important\nto take into account when using LLMs for ontology authoring support. We discuss\nthese limitations and propose directions for future research.",
        "Hadfield-Menell et al. (2017) propose the Off-Switch Game, a model of\nHuman-AI cooperation in which AI agents always defer to humans because they are\nuncertain about our preferences. I explain two reasons why AI agents might not\ndefer. First, AI agents might not value learning. Second, even if AI agents\nvalue learning, they might not be certain to learn our actual preferences.",
        "Multilingual neural machine translation (MNMT) aims at using one single model\nfor multiple translation directions. Recent work applies non-autoregressive\nTransformers to improve the efficiency of MNMT, but requires expensive\nknowledge distillation (KD) processes. To this end, we propose an M-DAT\napproach to non-autoregressive multilingual machine translation. Our system\nleverages the recent advance of the directed acyclic Transformer (DAT), which\ndoes not require KD. We further propose a pivot back-translation (PivotBT)\napproach to improve the generalization to unseen translation directions.\nExperiments show that our M-DAT achieves state-of-the-art performance in\nnon-autoregressive MNMT.",
        "Open-vocabulary panoptic reconstruction is a challenging task for\nsimultaneous scene reconstruction and understanding. Recently, methods have\nbeen proposed for 3D scene understanding based on Gaussian splatting. However,\nthese methods are multi-staged, suffering from the accumulated errors and the\ndependence of hand-designed components. To streamline the pipeline and achieve\nglobal optimization, we propose PanopticSplatting, an end-to-end system for\nopen-vocabulary panoptic reconstruction. Our method introduces query-guided\nGaussian segmentation with local cross attention, lifting 2D instance masks\nwithout cross-frame association in an end-to-end way. The local cross attention\nwithin view frustum effectively reduces the training memory, making our model\nmore accessible to large scenes with more Gaussians and objects. In addition,\nto address the challenge of noisy labels in 2D pseudo masks, we propose label\nblending to promote consistent 3D segmentation with less noisy floaters, as\nwell as label warping on 2D predictions which enhances multi-view coherence and\nsegmentation accuracy. Our method demonstrates strong performances in 3D scene\npanoptic reconstruction on the ScanNet-V2 and ScanNet++ datasets, compared with\nboth NeRF-based and Gaussian-based panoptic reconstruction methods. Moreover,\nPanopticSplatting can be easily generalized to numerous variants of Gaussian\nsplatting, and we demonstrate its robustness on different Gaussian base models.",
        "Large language model (LLM) architectures are often described as functionally\nhierarchical: Early layers process syntax, middle layers begin to parse\nsemantics, and late layers integrate information. The present work revisits\nthese ideas. This research submits simple texts to an LLM (e.g., \"A church and\norgan\") and extracts the resulting activations. Then, for each layer, support\nvector machines and ridge regressions are fit to predict a text's label and\nthus examine whether a given layer encodes some information. Analyses using a\nsmall model (Llama-3.2-3b; 28 layers) partly bolster the common hierarchical\nperspective: Item-level semantics are most strongly represented early (layers\n2-7), then two-item relations (layers 8-12), and then four-item analogies\n(layers 10-15). Afterward, the representation of items and simple relations\ngradually decreases in deeper layers that focus on more global information.\nHowever, several findings run counter to a steady hierarchy view: First,\nalthough deep layers can represent document-wide abstractions, deep layers also\ncompress information from early portions of the context window without\nmeaningful abstraction. Second, when examining a larger model\n(Llama-3.3-70b-Instruct), stark fluctuations in abstraction level appear: As\ndepth increases, two-item relations and four-item analogies initially increase\nin their representation, then markedly decrease, and afterward increase again\nmomentarily. This peculiar pattern consistently emerges across several\nexperiments. Third, another emergent effect of scaling is coordination between\nthe attention mechanisms of adjacent layers. Across multiple experiments using\nthe larger model, adjacent layers fluctuate between what information they each\nspecialize in representing. In sum, an abstraction hierarchy often manifests\nacross layers, but large models also deviate from this structure in curious\nways.",
        "The advent of the foundation model era has sparked significant research\ninterest in leveraging pre-trained representations for continual learning (CL),\nyielding a series of top-performing CL methods on standard evaluation\nbenchmarks. Nonetheless, there are growing concerns regarding potential data\ncontamination during the pre-training stage. Furthermore, standard evaluation\nbenchmarks, which are typically static, fail to capture the complexities of\nreal-world CL scenarios, resulting in saturated performance. To address these\nissues, we describe CL on dynamic benchmarks (CLDyB), a general computational\nframework based on Markov decision processes for evaluating CL methods\nreliably. CLDyB dynamically identifies inherently difficult and\nalgorithm-dependent tasks for the given CL methods, and determines challenging\ntask orders using Monte Carlo tree search. Leveraging CLDyB, we first conduct a\njoint evaluation of multiple state-of-the-art CL methods, leading to a set of\ncommonly challenging and generalizable task sequences where existing CL methods\ntend to perform poorly. We then conduct separate evaluations of individual CL\nmethods using CLDyB, discovering their respective strengths and weaknesses. The\nsource code and generated task sequences are publicly accessible at\nhttps:\/\/github.com\/szc12153\/CLDyB.",
        "Abstract: Failing test case reduction can promote efficient debugging because\na developer may not need to observe components that are not relevant to\ninducing failure. Failing test case reduction can also improve the efficiency\nof fault localization. These considerations have prompted researchers to study\nthe reduction process, the reduction output, and the removed entities. Christi\net al. studied test reduction using a tool called ReduSharptor for C# tests.\nThey considered the test to be an Abstract Syntax Tree (AST). Based on that,\nthey studied the reduction outcome and removed entities in terms of Leaf nodes\nand Non-Leaf nodes of the AST. They claimed that (1) leaf nodes are removed in\nlarge numbers, and (2) the probability of removal is slightly higher than\nnon-leaf nodes. We replicate their results using a different test case\nreduction tool, ReduJavator, for Java unit tests. We evaluate test reduction\nusing 30 randomly chosen bugs from the Defects4J database and 30 mutants for 6\nopen-source projects. Our results confirm their first claim: leaf nodes are\nremoved in large numbers. Our results are inconclusive regarding their second\nclaim; we cannot confirm that the probability of removal is higher for non-leaf\nnodes.",
        "We show that the consistent application of the rules of quantum mechanics to\ncosmological systems inevitably results in the so-called multiverse states in\nwhich neither the background spacetime nor the inhomogeneous perturbation are\nin definite states. We study the multiverse states as perturbations to the\nusually employed so-called Born-Oppenheimer states that are products of a wave\nfunction of the background and a wave function of the perturbation. The\nobtained corrections involve integrals over \\emph{virtual backgrounds} that\nrepresent the effect of quantum background fluctuations on the perturbation\nstate. They resemble loop corrections in quantum field theory. This approach\ndemonstrates the inevitable existence of very specific non-Gaussian features in\nprimordial fluctuations. We express the resulting non-Gaussian perturbation as\na nonlinear function of the Gaussian perturbation obtained within the\nBorn-Oppenheimer approximation, and compute its trispectrum, to show that the\nmultiverse scenario leads to testable and distinct signatures in cosmological\nperturbations. Our approach applies both to inflationary and alternative\ncosmologies.",
        "In this paper we propose a new paradigm for cosmology: a time dependent\nscalar condensate background originated from the quadratic $(R + \\alpha R^2)$\nStarobinski model, where $R$ is the Ricci scalar and $\\alpha$ the coupling\nconstant. In weak gravity limit the system decouples into a conventional\ngraviton and a higher derivative scalar. It was shown earlier through works\nfrom our group, \\cite{ssg,sg,us}, that the latter can sustain an oscillatory\nlowest energy configuration or a {\\it{Geometric Condensate}} as it consists\nentirely of metric degrees of freedom. In the present work, we study\nGravitational Wave propagation in this condensate background. We show that the\nexplicit time dependent nature of the condensate can generate curvature and\nradiation-like contributions in the scale factor evolution in FLRW cosmology.\nSubsequently the condensate leaves its signature on the Gravitational Wave\nprofile as it propagates in the condensate modified FLRW spacetime. The wave\nprofile is calculated analytically in terms of Whittaker functions. The main\nnovelty of the Geometric Condensate scheme is that no external (condensate)\nmatter from outside has been considered.",
        "The total lunar eclipse on March 14, 2025 UT occurs nearly exactly 521 years\n(one Hypersaros) after a similar eclipse on March 1, 1504 UT that is renowned\nfor its importance to the voyage of Columbus to Jamaica. Eclipses separated by\na Hypersaros have similar depths, appear very close to the same location in the\nsky, and occur at nearly the same time of year. This paper summarizes the\nresults from a search for analogous cycles within the Five Millennium Catalogs\nof Lunar and Solar Eclipses. Under the two simple constraints of similar\neclipse dates relative to the vernal equinox and similar paths of the Moon\nthrough the Earth's shadow, the most common time intervals between lunar\neclipses separated by less than 1000 years are the 521-year Hypersaros and a\n633-yr period of the Icosa-Inex-Triple-Saros (IITS). Notable cycles at longer\nperiods occur at 1154, 1284, 1787, 1917, and 2308 years.",
        "Recent approaches to jointly reconstruct 3D humans and objects from a single\nRGB image represent 3D shapes with template-based or coarse models, which fail\nto capture details of loose clothing on human bodies. In this paper, we\nintroduce a novel implicit approach for jointly reconstructing realistic 3D\nclothed humans and objects from a monocular view. For the first time, we model\nboth the human and the object with an implicit representation, allowing to\ncapture more realistic details such as clothing. This task is extremely\nchallenging due to human-object occlusions and the lack of 3D information in 2D\nimages, often leading to poor detail reconstruction and depth ambiguity. To\naddress these problems, we propose a novel attention-based neural implicit\nmodel that leverages image pixel alignment from both the input human-object\nimage for a global understanding of the human-object scene and from local\nseparate views of the human and object images to improve realism with, for\nexample, clothing details. Additionally, the network is conditioned on semantic\nfeatures derived from an estimated human-object pose prior, which provides 3D\nspatial information about the shared space of humans and objects. To handle\nhuman occlusion caused by objects, we use a generative diffusion model that\ninpaints the occluded regions, recovering otherwise lost details. For training\nand evaluation, we introduce a synthetic dataset featuring rendered scenes of\ninter-occluded 3D human scans and diverse objects. Extensive evaluation on both\nsynthetic and real-world datasets demonstrates the superior quality of the\nproposed human-object reconstructions over competitive methods.",
        "The emergence of large Vision Language Models (VLMs) has broadened the scope\nand capabilities of single-modal Large Language Models (LLMs) by integrating\nvisual modalities, thereby unlocking transformative cross-modal applications in\na variety of real-world scenarios. Despite their impressive performance, VLMs\nare prone to significant hallucinations, particularly in the form of\ncross-modal inconsistencies. Building on the success of Reinforcement Learning\nfrom Human Feedback (RLHF) in aligning LLMs, recent advancements have focused\non applying direct preference optimization (DPO) on carefully curated datasets\nto mitigate these issues. Yet, such approaches typically introduce preference\nsignals in a brute-force manner, neglecting the crucial role of visual\ninformation in the alignment process. In this paper, we introduce Re-Align, a\nnovel alignment framework that leverages image retrieval to construct a\ndual-preference dataset, effectively incorporating both textual and visual\npreference signals. We further introduce rDPO, an extension of the standard\ndirect preference optimization that incorporates an additional visual\npreference objective during fine-tuning. Our experimental results demonstrate\nthat Re-Align not only mitigates hallucinations more effectively than previous\nmethods but also yields significant performance gains in general visual\nquestion-answering (VQA) tasks. Moreover, we show that Re-Align maintains\nrobustness and scalability across a wide range of VLM sizes and architectures.\nThis work represents a significant step forward in aligning multimodal LLMs,\npaving the way for more reliable and effective cross-modal applications. We\nrelease all the code in https:\/\/github.com\/taco-group\/Re-Align.",
        "Transformer-based Large Language Models (LLMs) struggle to process inputs\nexceeding their training context window, with performance degrading due to\npositional out-of-distribution (O.O.D.) that disrupt attention computations.\nExisting solutions, fine-tuning and training-free methods, are limited by\ncomputational inefficiency, attention logit outliers or loss of local\npositional information. To address this, we propose Greedy Attention Logit\nInterpolation (GALI), a training-free length extrapolation method that\nmaximizes the utilization of pretrained positional intervals while avoiding\nattention logit outliers through attention logit interpolation. The result\ndemonstrates that GALI consistently outperforms state-of-the-art training-free\nmethods. Our findings reveal that LLMs interpret positional intervals unevenly\nwithin their training context window, suggesting that extrapolating within a\nsmaller positional interval range yields superior results-even for\nshort-context tasks. GALI represents a significant step toward resolving the\npositional O.O.D. challenge, enabling more reliable long-text understanding in\nLLMs. Our implementation of GALI, along with the experiments from our paper, is\nopen-sourced at https:\/\/github.com\/AcademyCityL\/GALI.",
        "Higher-spin conserved currents and charges feature prominently in integrable\n2d QFTs in flat space. Motivated by the question of integrable field theories\nin AdS space, we consider the consequences of higher-spin currents for QFTs in\nAdS$_2$, and find that their effect is much more constraining than in flat\nspace. Specifically, it is impossible to preserve: (a)~any higher-spin charges\nwhen deforming a massive free field by interactions, or (b)~any spin-4 charges\nwhen deforming a CFT by a Virasoro primary. Therefore, in these settings, there\nare no integrable theories in AdS with higher-spin conserved charges. Along the\nway, we explain how higher-spin charges lead to integer spacing in the spectrum\nof primaries, sum rules on the OPE data, and constraints on correlation\nfunctions. We also explain a key difference between AdS and flat space: in AdS\none cannot `partially' conserve a higher-spin current along particular\ndirections, since the AdS isometries imply full conservation.",
        "Cooperative perception enhances the individual perception capabilities of\nautonomous vehicles (AVs) by providing a comprehensive view of the environment.\nHowever, balancing perception performance and transmission costs remains a\nsignificant challenge. Current approaches that transmit region-level features\nacross agents are limited in interpretability and demand substantial bandwidth,\nmaking them unsuitable for practical applications. In this work, we propose\nCoopDETR, a novel cooperative perception framework that introduces object-level\nfeature cooperation via object query. Our framework consists of two key\nmodules: single-agent query generation, which efficiently encodes raw sensor\ndata into object queries, reducing transmission cost while preserving essential\ninformation for detection; and cross-agent query fusion, which includes Spatial\nQuery Matching (SQM) and Object Query Aggregation (OQA) to enable effective\ninteraction between queries. Our experiments on the OPV2V and V2XSet datasets\ndemonstrate that CoopDETR achieves state-of-the-art performance and\nsignificantly reduces transmission costs to 1\/782 of previous methods.",
        "Predicting human decision-making under risk and uncertainty is a\nlong-standing challenge in cognitive science, economics, and AI. While prior\nresearch has focused on numerically described lotteries, real-world decisions\noften rely on textual descriptions. This study conducts the first large-scale\nexploration of human decision-making in such tasks using a large dataset of\none-shot binary choices between textually described lotteries. We evaluate\nmultiple computational approaches, including fine-tuning Large Language Models\n(LLMs), leveraging embeddings, and integrating behavioral theories of choice\nunder risk. Our results show that fine-tuned LLMs, specifically RoBERTa and\nGPT-4o outperform hybrid models that incorporate behavioral theory, challenging\nestablished methods in numerical settings. These findings highlight fundamental\ndifferences in how textual and numerical information influence decision-making\nand underscore the need for new modeling strategies to bridge this gap.",
        "Advancements in robotics have opened possibilities to automate tasks in\nvarious fields such as manufacturing, emergency response and healthcare.\nHowever, a significant challenge that prevents robots from operating in\nreal-world environments effectively is out-of-distribution (OOD) situations,\nwherein robots encounter unforseen situations. One major OOD situations is when\nrobots encounter faults, making fault adaptation essential for real-world\noperation for robots. Current state-of-the-art reinforcement learning\nalgorithms show promising results but suffer from sample inefficiency, leading\nto low adaptation speed due to their limited ability to generalize to OOD\nsituations. Our research is a step towards adding hardware fault tolerance and\nfast fault adaptability to machines. In this research, our primary focus is to\ninvestigate the efficacy of generative flow networks in robotic environments,\nparticularly in the domain of machine fault adaptation. We simulated a robotic\nenvironment called Reacher in our experiments. We modify this environment to\nintroduce four distinct fault environments that replicate real-world\nmachines\/robot malfunctions. The empirical evaluation of this research\nindicates that continuous generative flow networks (CFlowNets) indeed have the\ncapability to add adaptive behaviors in machines under adversarial conditions.\nFurthermore, the comparative analysis of CFlowNets with reinforcement learning\nalgorithms also provides some key insights into the performance in terms of\nadaptation speed and sample efficiency. Additionally, a separate study\ninvestigates the implications of transferring knowledge from pre-fault task to\npost-fault environments. Our experiments confirm that CFlowNets has the\npotential to be deployed in a real-world machine and it can demonstrate\nadaptability in case of malfunctions to maintain functionality."
      ]
    }
  },
  {
    "id":2411.15395,
    "research_type":"applied",
    "start_id":"b17",
    "start_title":"Language Model-Guided Classifier Adaptation for Brain-Computer Interfaces for Communication",
    "start_abstract":"Brain-computer interfaces (BCIs), such as the P300 speller, can provide a means of communication for individuals with severe neuromuscular limitations. BCIs interpret electroencephalography (EEG) signals in order to translate embedded information about user's intent into executable commands control external devices. However, EEG are inherently noisy and nonstationary, posing challenge extended BCI use. Conventionally, classifier is trained via supervised learning an offline calibration session; once trained, deployed online use not updated. As statistics data change over time, performance static may decline It therefore desirable automatically adapt current without requiring recalibration. In existing semi-supervised approach, on labeled then updated using incoming unlabeled classifier-predicted labels. To reduce risk from incorrect predictions, threshold imposed exclude low-confidence label predictions expanded training set when retraining adaptive classifier. this work, we propose language model spelling error correction disambiguation correctness during learning. Results simulations multi-session speller user demonstrate that our language-guided approach significantly improves accuracy relative conventional threshold-based",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "Brain\u2013Computer Interface Spellers: A Review"
      ],
      "abstract":[
        "A Brain\u2013Computer Interface (BCI) provides a novel non-muscular communication method via brain signals. BCI-speller can be considered as one of the first published BCI applications and has opened gate for many advances in field. Although BCI-spellers have been developed during last few decades, to our knowledge, no reviews described different spellers proposed studied this vital The presented speller systems are categorized according major paradigms: P300, steady-state visual evoked potential (SSVEP), motor imagery (MI). Different paradigms require specific electroencephalogram (EEG) signal features lead development appropriate Graphical User Interfaces (GUIs). purpose review is consolidate most successful since 2010, while mentioning some other older which were built explicitly spelling purposes. We aim assist researchers concerned individuals field by illustrating highlights presenting them review. It almost impossible carry out an objective comparison between spellers, each its variables, parameters, conditions. However, gathered information provided taxonomy about helpful, it could identify suitable first-hand users, well opportunities learning from previous studies researchers."
      ],
      "categories":[
        "q-bio.NC"
      ]
    },
    "list":{
      "title":[
        "Asynchronous Hebbian\/anti-Hebbian networks",
        "Electrophysiological Investigation of Insect Pain Threshold",
        "Spaces and sequences in the hippocampus: a homological perspective",
        "Neural Constraints on Cognitive Experience and Mental Health",
        "The Role of Affective States in Computational Psychiatry",
        "How constraints on editing affects cultural evolution",
        "Vagus nerve stimulation as a modulator of feedforward and feedback\n  neural transmission",
        "Phase Alignment Enhances Oscillatory Power in Neural Mass Models\n  Optimized for Class Encoding",
        "Active filtering: a predictive function of recurrent circuits of sensory\n  cortex",
        "The Neural Basis of Groove Sensations: Implications for Music-Based\n  Interventions and Dance Therapy in Parkinson's Disease",
        "A Relativistic Theory of Consciousness (shortened version)",
        "Deviance Detection and Regularity Sensitivity in Dissociated Neuronal\n  Cultures",
        "A 7T fMRI dataset of synthetic images for out-of-distribution modeling\n  of vision",
        "Applying a star formation model calibrated on high-resolution\n  interstellar medium simulations to cosmological simulations of galaxy\n  formation",
        "Neural network-based prediction of particle-induced fission cross\n  sections for r-process nucleosynthesis trained with dynamical reaction models",
        "Improving the trivial bound for $\\ell$-torsion in class groups",
        "Real-time simulation of jet energy loss and entropy production in\n  high-energy scattering with matter",
        "Cluster Ages to Reconstruct the Milky Way Assembly (CARMA). III. NGC 288\n  as the first Splashed globular cluster",
        "Compare Similarities Between DNA Sequences Using Permutation-Invariant\n  Quantum Kernel",
        "Asymptotic behavior of clusters in hierarchical species sampling models",
        "Feedback-augmented Non-homogeneous Hidden Markov Models for Longitudinal\n  Causal Inference",
        "B-fields And dust in interstelLar fiLAments using Dust POLarization\n  (BALLAD-POL): III. Grain alignment and disruption mechanisms in G34.43+0.24\n  using polarization observations from JCMT\/POL-2",
        "Optimal joint reconstruction from CMB observations: application to\n  cosmic birefringence, patchy reionization and CMB lensing",
        "Nearsightedness in Materials with Indirect Band Gap",
        "A tracking algorithm for finite-size particles",
        "Female and Combined Male-Female Injury Risk Functions for the Anterior\n  Pelvis Under Frontal Lap Belt Loading Conditions",
        "Cohering Disaggregation and Uncertainty Quantification for Spatially\n  Misaligned Data",
        "Cooperation and competing of antipolar charge order and superconducting\n  states in a quasi-2D superatomic metallic crystal"
      ],
      "abstract":[
        "Lateral inhibition models coupled with Hebbian plasticity have been shown to\nlearn factorised causal representations of input stimuli, for instance,\noriented edges are learned from natural images. Currently, these models require\nthe recurrent dynamics to settle into a stable state before weight changes can\nbe applied, which is not only biologically implausible, but also impractical\nfor real-time learning systems. Here, we propose a new Hebbian learning rule\nwhich is implemented using plausible biological mechanisms that have been\nobserved experimentally. We find that this rule allows for efficient,\ntime-continuous learning of factorised representations, very similar to the\nclassic noncontinuous Hebbian\/anti-Hebbian learning. Furthermore, we show that\nthis rule naturally prevents catastrophic forgetting when stimuli from\ndifferent distributions are shown sequentially.",
        "The question of whether insects experience pain has long been debated in\nneuroscience and animal behavior research. Increasing evidence suggests that\ninsects possess the ability to detect and respond to noxious stimuli,\nexhibiting behaviors indicative of pain perception. This study investigates the\nrelationship between pain stimuli and physiological responses in crickets\n(Gryllidae), focusing on heart rate (ECG) and brain wave (EEG) patterns. We\napplied a range of mechanical, chemical, thermal, and electrical stimuli to\ncrickets, recording ECG and EEG data while employing a deep learning-based\nmodel to classify pain levels. Our findings revealed significant heart rate\nchanges and EEG fluctuations in response to various stimuli, with the highest\nintensity stimuli inducing marked physiological stress. The AI-based analysis,\nutilizing AlexNet for EEG signal classification, achieved 90% accuracy in\ndistinguishing between resting, low-pain, and high-pain states. While no social\nsharing of pain was observed through ECG measurements, these results contribute\nto the growing body of evidence supporting insect nociception and offer new\ninsights into their physiological responses to external stressors. This\nresearch advances the understanding of insect pain mechanisms and demonstrates\nthe potential for AI-driven analysis in entomological studies.",
        "Topological techniques have become a popular tool for studying information\nflows in neural networks. In particular, simplicial homology theory is used to\nanalyze how cognitive representations of space emerge from large conglomerates\nof independent neuronal contributions. Meanwhile, a growing number of studies\nsuggest that many cognitive functions are sustained by serial patterns of\nactivity. Here, we investigate stashes of such patterns using path homology\ntheory -- an impartial, universal approach that does not require a priori\nassumptions about the sequences' nature, functionality, underlying mechanisms,\nor other contexts. We focus on the hippocampus -- a key enabler of learning and\nmemory in mammalian brains -- and quantify the ordinal arrangement of its\nactivity similarly to how its topology has previously been studied in terms of\nsimplicial homologies. The results reveal that the vast majority of sequences\nproduced during spatial navigation are structurally equivalent to one another.\nOnly a few classes of distinct sequences form an ordinal schema of serial\nactivity that remains stable as the pool of sequences consolidates.\nImportantly, the structure of both maps is upheld by combinations of short\nsequences, suggesting that brief activity motifs dominate physiological\ncomputations. This ordinal organization emerges and stabilizes on timescales\ncharacteristic of spatial learning, displaying similar dynamics. Yet, the\nordinal maps generally do not reflect topological affinities -- spatial and\nsequential analyses address qualitatively different aspects of spike flows,\nrepresenting two complementary formats of information processing.",
        "Understanding how neural dynamics shape cognitive experiences remains a\ncentral challenge in neuroscience and psychiatry. Here, we present a novel\nframework leveraging state-to-output controllability from dynamical systems\ntheory to model the interplay between cognitive perturbations, neural activity,\nand subjective experience. We demonstrate that large-scale fMRI signals are\nconstrained to low-dimensional manifolds, where affective and cognitive states\nare naturally organized. Furthermore, we provide a theoretically robust method\nto estimate the controllability Gramian from steady-state neural responses,\noffering a direct measure of the energy required to steer cognitive outcomes.\nIn five healthy participants viewing 2,185 emotionally evocative short videos,\nour analyses reveal a strong alignment between neural activations and affective\nratings, with an average correlation of $r \\approx 0.7$. In a clinical cohort\nof 255 patients with major depressive disorder, biweekly Hamilton Rating Scale\ntrajectories over 11 weeks significantly mapped onto these manifolds,\nexplaining approximately 20% more variance than chance ($p < 10^{-10}$,\nnumerically better than chance in 93% reaching statistical significance in\none-third of subjects). Our work bridges dynamical systems theory and clinical\nneuroscience, providing a principled approach to optimize mental health\ntreatments by targeting the most efficient neural pathways for cognitive\nchange.",
        "Studying psychiatric illness has often been limited by difficulties in\nconnecting symptoms and behavior to neurobiology. Computational psychiatry\napproaches promise to bridge this gap by providing formal accounts of the\nlatent information processing changes that underlie the development and\nmaintenance of psychiatric phenomena. Models based on these theories generate\nindividual-level parameter estimates which can then be tested for relationships\nto neurobiology. In this review, we explore computational modelling approaches\nto one key aspect of health and illness: affect. We discuss strengths and\nlimitations of key approaches to modelling affect, with a focus on\nreinforcement learning, active inference, the hierarchical gaussian filter, and\ndrift-diffusion models. We find that, in this literature, affect is an\nimportant source of modulation in decision making, and has a bidirectional\ninfluence on how individuals infer both internal and external states.\nHighlighting the potential role of affect in information processing changes\nunderlying symptom development, we extend an existing model of psychosis, where\naffective changes are influenced by increasing cortical noise and consequent\nincreases in either perceived environmental instability or expected noise in\nsensory input, becoming part of a self-reinforcing process generating\nnegatively valenced, over-weighted priors underlying positive symptom\ndevelopment. We then provide testable predictions from this model at\ncomputational, neurobiological, and phenomenological levels of description.",
        "When is it beneficial to constrain creativity? Creativity thrives with\nfreedom, but when people collaborate to create artifacts, there is tension\nbetween giving individuals freedom to revise, and protecting prior\nachievements. To test how imposing constraints may affect collective\ncreativity, we performed cultural evolution experiments where participants\ncollaborated to create melodies and images in chains. With melodies, we found\nthat limiting step size (number of musical notes that can be changed) improved\npleasantness ratings for created tunes. Similar results were observed in\ncohorts of musicians, and with different selection regimes. In contrast,\nlimiting step size in creating images consistently reduced pleasantness. These\nconflicting findings suggest that in domains such as music, where artifacts can\nbe easily damaged, and where evolutionary outcomes are hard to foresee,\ncollective creativity may benefit from imposing small step sizes. We discuss\nparallels with search algorithms and the evolution of conservative birdsong\ncultures.",
        "Vagus nerve stimulation (VNS) has emerged as a promising therapeutic\nintervention across various neurological and psychiatric conditions, including\nepilepsy, depression, and stroke rehabilitation; however, its mechanisms of\naction on neural circuits remain incompletely understood. Here, we present a\nnovel theoretical framework based on predictive coding that conceptualizes VNS\neffects through differential modulation of feedforward and feedback neural\ncircuits. Based on recent evidence, we propose that VNS shifts the balance\nbetween feedforward and feedback processing through multiple neuromodulatory\nsystems, resulting in enhanced feedforward signal transmission. This framework\nintegrates anatomical pathways, receptor distributions, and physiological\nresponses to explain the influence of the VNS on neural dynamics across\ndifferent spatial and temporal scales. VNS may facilitate neural plasticity and\nadaptive behavior through acetylcholine and noradrenaline (norepinephrine),\nwhich differentially modulate feedforward and feedback signaling. This\nmechanistic understanding serves as a basis for interpreting the cognitive and\ntherapeutic outcomes across different clinical conditions. Our perspective\nprovides a unified theoretical framework for understanding circuit-specific VNS\neffects and suggests new directions for investigating their therapeutic\nmechanisms.",
        "Neural encoding of objects and cognitive states remains an elusive yet\ncrucial aspect of brain function. While traditional feed-forward machine\nlearning neural networks have enormous potential to encode information, modern\narchitectures provide little insight into the brain's mechanisms. In this work,\na Jansen and Rit neural mass model was constructed to encode different sets of\ninputs, aiming to understand how simple neural circuits can represent\ninformation. A genetic algorithm was used to optimize parameters that maximized\nthe differences in responses to particular inputs. These differences in\nresponses manifested as phase-shifted oscillations across the set of inputs. By\ndelivering impulses of excitation synchronized with a particular phase-shifted\noscillation, we demonstrated that the encoded phase could be decoded by\nmeasuring oscillatory power. These findings demonstrate the capability of\nneural dynamical circuits to encode and decode information through phase.",
        "Our brains encode many features of the sensory world into memories: we can\nsing along with songs we have heard before, interpret spoken and written\nlanguage composed of words we have learned, and recognize faces and objects.\nWhere are these memories stored? Each area of the cerebral cortex has a huge\nnumber of local, recurrent, excitatory-excitatory synapses, as many as 500\nmillion per cubic millimeter. Here I review evidence that cortical recurrent\nconnectivity in sensory cortex is a substrate for sensory memories. Evidence\nsuggests that the local recurrent network encodes the structure of natural\nsensory input, and that it does so via active filtering, transforming network\ninputs to boost or select those associated with natural sensation. This is a\nform of predictive processing, in which the cortical recurrent network\nselectively amplifies some input patterns and attenuates others, and a form of\nmemory.",
        "Groove sensations arise from rhythmic structures that evoke an urge to move\nin response to music. While syncopation has been extensively studied in groove\nperception, the neural mechanisms underlying low-frequency groove remain\nunderexplored. This fMRI study examines the role of the mirror neuron system\nand associated brain regions in processing low-frequency groove.\nRegion-of-interest analysis revealed that amplifying drum and bass components\nin K-pop songs significantly increased activity in the right posterior inferior\nfrontal gyrus, right inferior\/superior parietal lobules, left dorsolateral\nprefrontal cortex, and bilateral posterior middle\/inferior temporal gyrus.\nThese findings suggest that low-frequency grooves engage sensorimotor,\nexecutive, and rhythm semantics networks, reinforcing their role in\naction-related processing. Building on these insights, we propose an enhanced\nrhythmic auditory stimulation paradigm for Parkinson's disease, incorporating\namplified low-frequency rhythmic cues to improve gait synchronization.",
        "This paper is a shortened version of the full paper that was published in the\njournal Frontiers of Psychology in May 2022. In recent decades, the scientific\nstudy of consciousness has significantly increased our understanding of this\nelusive phenomenon. Yet, despite critical development in our understanding of\nthe functional side of consciousness, we still lack a fundamental theory\nregarding its phenomenal aspect. The phenomenal aspect of consciousness is the\nfirst-person answer to what it is like question, and it has thus far proved\nrecalcitrant to direct scientific investigation. The question of how the brain,\nor any cognitive system, can create conscious experience out of neural\nrepresentations poses a great conundrum to science. Naturalistic dualists argue\nthat it is composed of a primitive, private, nonreductive element of reality.\nIllusionists, on the other hand, argue that it is merely a cognitive illusion.\nWe contend that both the dualist and illusionist positions are flawed because\nthey tacitly assume consciousness to be an absolute property that does not\ndepend on the observer. We developed a conceptual and a mathematical argument\nfor a relativistic theory of consciousness in which a system either has or does\nnot have phenomenal consciousness with respect to some observer. According to\nthe theory, Phenomenal consciousness is neither private nor delusional, just\nrelativistic. In the frame of reference of the cognitive system, it will be\nobservable (first-person perspective) and in other frame of reference it will\nnot (third-person perspective). These two cognitive frames of reference are\nboth correct, just as in the case of an observer that claims to be at rest\nwhile another will claim that the observer has constant velocity. Neither\nobserver position can be privileged, as they both describe the same underlying\nreality.",
        "Understanding how neural networks process complex patterns of information is\ncrucial for advancing both neuroscience and artificial intelligence. To\ninvestigate fundamental principles of neural computation, we studied\ndissociated neuronal cultures, one of the most primitive living neural\nnetworks, on high-resolution CMOS microelectrode arrays and tested whether the\ndissociated culture exhibits regularity sensitivity beyond mere\nstimulus-specific adaptation and deviance detection. In oddball electrical\nstimulation paradigms, we confirmed that the neuronal culture produced mismatch\nresponses (MMRs) with true deviance detection beyond mere adaptation. These\nMMRs were dependent on the N-methyl-D-aspartate (NMDA) receptors, similar to\nmismatch negativity (MMN) in humans, which is known to have true deviance\ndetection properties. Crucially, we also showed sensitivity to the statistical\nregularity of stimuli, a phenomenon previously observed only in intact brains:\nthe MMRs in a predictable, periodic sequence were smaller than those in a\ncommonly used sequence in which the appearance of the deviant stimulus was\nrandom and unpredictable. These results challenge the traditional view that a\nhierarchically structured neural network is required to process complex\ntemporal patterns, suggesting instead that deviant detection and regularity\nsensitivity are inherent properties arising from the primitive neural network.\nThey also suggest new directions for the development of neuro-inspired\nartificial intelligence systems, emphasizing the importance of incorporating\nadaptive mechanisms and temporal dynamics in the design of neural networks.",
        "Large-scale visual neural datasets such as the Natural Scenes Dataset (NSD)\nare boosting NeuroAI research by enabling computational models of the brain\nwith performances beyond what was possible just a decade ago. However, these\ndatasets lack out-of-distribution (OOD) components, which are crucial for the\ndevelopment of more robust models. Here, we address this limitation by\nreleasing NSD-synthetic, a dataset consisting of 7T fMRI responses from the\neight NSD subjects for 284 carefully controlled synthetic images. We show that\nNSD-synthetic's fMRI responses reliably encode stimulus-related information and\nare OOD with respect to NSD. Furthermore, OOD generalization tests on\nNSD-synthetic reveal differences between models of the brain that are not\ndetected with NSD - specifically, self-supervised deep neural networks better\nexplain neural responses than their task-supervised counterparts. These results\nshowcase how NSD-synthetic enables OOD generalization tests that facilitate the\ndevelopment of more robust models of visual processing, and the formulation of\nmore accurate theories of human vision.",
        "Modern high-resolution simulations of the interstellar medium (ISM) have\nshown that key factors in governing star formation are the competing influences\nof radiative dissipation, pressure support driven by stellar feedback, and the\nrelentless pull of gravity. Cosmological simulations of galaxy formation, such\nas IllustrisTNG or ASTRID, are however not able to resolve this physics in\ndetail and therefore need to rely on approximate treatments. These have often\ntaken the form of empirical subgrid models of the ISM expressed in terms of an\neffective equation of state (EOS) that relates the mean ISM pressure to the\nmean gas density. Here we seek to improve these heuristic models by directly\nfitting their key ingredients to results of the high-resolution TIGRESS\nsimulations, which have shown that the dynamical equilibrium of the ISM can be\nunderstood in terms of a pressure-regulated, feedback modulated (PRFM) model\nfor star formation. Here we explore a simple subgrid model that draws on the\nPRFM concept but uses only local quantities. It accurately reproduces PRFM for\npure gas disks, while it predicts slightly less star formation than PRFM in the\npresence of an additional thin stellar disk. We compare the properties of this\nmodel with the older Springel and Hernquist and TNG prescriptions, and apply\nall three to isolated simulations of disk galaxies as well as to a set of\nhigh-resolution zoom-in simulations carried out with a novel 'multi-zoom'\ntechnique that we introduce in this study. The softer EOS implied by TIGRESS\nproduces substantially thinner disk galaxies, which has important ramifications\nfor disk stability and galaxy morphology. The total stellar mass of galaxies is\nhowever hardly modified at low redshift, reflecting the dominating influence of\nlarge-scale gaseous inflows and outflows to galaxies, which are not sensitive\nto the EOS itself",
        "Large-scale computations of fission properties play a crucial role in nuclear\nreaction network calculations simulating rapid neutron-capture process\n(r-process) nucleosynthesis. Due to the large number of fissioning nuclei\ncontributing to the r-process, a description of particle-induced fission\nreactions is computationally challenging. In this work, we use theoretical\ncalculations based on the INCL+ABLA models to train neural networks (NN). The\nresults for the prediction of proton-induced spallation reactions, in\nparticular fission, utilizing a large variety of NN models across the\nhyper-parameter space are presented, which are relevant for r-process\ncalculations.",
        "For any number field $K$ with $D_K=|\\mathrm{Disc}(K)|$ and any integer $\\ell\n\\geq 2$, we improve over the commonly cited trivial bound\n$|\\mathrm{Cl}_K[\\ell]| \\leq |\\mathrm{Cl}_K| \\ll_{[K:\\mathbb{Q}],\\varepsilon}\nD_K^{1\/2+\\varepsilon}$ on the $\\ell$-torsion subgroup of the class group of $K$\nby showing that $|\\mathrm{Cl}_K[\\ell]| = o_{[K:\\mathbb{Q}],\\ell}(D_K^{1\/2})$.\nIn fact, we obtain an explicit log-power saving. This is the first general\nunconditional saving over the trivial bound that holds for all $K$ and all\n$\\ell$.",
        "In analogy to high-energy nuclear scattering experiments, we study a\nreal-time scattering process between a propagating state and a dense target in\n$1+1$-d massive QED. In our setup, we identify three distinct regimes that\nqualitatively characterize the evolution: for a dilute medium, the incoming\nprobe state evolves nearly ballistically; in an intermediate setting, it\ntraverses the matter, locally exciting it; and for dense targets, one\napproaches a black-disk limit, where the matter acts as a strong wall\npotential. We find evidence that the probe's energy loss rate scales linearly\nwith the path length in the medium, and we study how the entanglement entropy\nreveals the mixing between the probe and medium states. With the goal of one\nday replicating high-energy nuclear experiments in quantum devices, we briefly\ndiscuss how the current tensor network-based simulations can be translated to a\nquantum simulator.",
        "The globular clusters (GCs) system of the Milky Way (MW) comprises a mixture\nof both in situ and accreted clusters. Tracing the origin of GCs provides\ninvaluable insights into the formation history of the MW. However, reconciling\ndiverse strands of evidence is often challenging: a notable example is NGC 288,\nwhere despite significant efforts in the literature, the available\nchrono-chemodynamical data have yet to provide a definitive conclusion\nregarding its origin. On one side, all post-Gaia dynamical studies indicate an\naccreted origin for NGC 288 from in the Gaia-Sausage-Enceladus (GSE) dwarf\ngalaxy. On the other, NGC 288 has been found to be 2.5 Gyr older than other GSE\nGCs at the same metallicity, this suggesting a different, possibly in situ\norigin. In this work, we address the unresolved question on the origin of NGC\n288 by analyzing its chrono-chemical properties in an unprecedentedly\nhomogeneous framework. First, we compare the location of NGC 288 in the\nage-metallicity plane with that of other two in situ GCs at similar\nmetallicity, namely NGC 6218 and NGC 6362. The age estimates obtained within\nthe homogeneous framework of the CARMA collaboration show that the three\nclusters are coeval, this reinforcing the contrast with the dynamical\ninterpretation. Then, we compare the abundances with the sample of in situ and\naccreted clusters at similar metallicity presented in Ceccarelli et al. 2024,\nfinding again consistency with the chemistry of in situ systems. To reconcile\nthese results with its orbital properties, we propose a scenario where NGC 288\nformed in the proto-disc of the MW, and then was dynamically heated by the\ninteraction with the GSE merger, a fate similar to that of proto-disc stars\nexperiencing the so-called Splash event. NGC 288 therefore demonstrates the\nimportance of a homogeneous chrono-chemodynamical information in the\ninterpretation of the origin of MW GCs.",
        "Computing the similarity between two DNA sequences is of vital importance in\nbioscience. However, traditional computational methods can be\nresource-intensive due to the enormous sequence length encountered in practice.\nRecently, applied quantum algorithms have been anticipated to provide potential\nadvantages over classical approaches. In this paper, we propose a\npermutation-invariant variational quantum kernel method specifically designed\nfor DNA comparison. To represent the four nucleotide bases in DNA sequences\nwith quantum states, we introduce a novel, theoretically motivated encoding\nscheme: the four distinct bases are encoded using the states of symmetric,\ninformationally complete, positive operator-valued measures (SIC-POVMs). This\nencoding ensures mutual equality: each pair of symbols is equidistant on the\nBloch sphere. Also, since permutation invariance is inherent to common DNA\nsimilarity measures such as Levenshtein distance, we realize it by using a\nspecially designed parameterized quantum layer. We show that our novel encoding\nmethod and parameterized layers used in the quantum kernel model can\neffectively capture the symmetric characteristics of the pairwise DNA sequence\ncomparison task. We validate our model through numerical experiments, which\nyield promising results on length-$8$ DNA sequences.",
        "Consider a sample of size $N$ from a population governed by a hierarchical\nspecies sampling model. We study the large $N$ asymptotic behavior of the\nnumber ${\\bf K}_N$ of clusters and the number ${\\bf M}_{r,N}$ of clusters with\nfrequency $r$ in the sample. In particular, we show almost sure and $L^p$\nconvergence for ${\\bf M}_{r,N}$, obtain Gaussian fluctuation theorems for ${\\bf\nK}_N$, and establish large deviation principles for both ${\\bf K}_N$ and ${\\bf\nM}_{r,N}$. Our approach relies on a random sample size representation of the\nnumber of clusters through the corresponding non-hierarchical species sampling\nmodel.",
        "Hidden Markov models are widely used for modeling sequential data but\ntypically have limited applicability in observational causal inference due to\ntheir strong conditional independence assumptions. I introduce\nfeedback-augmented non-homogeneous hidden Markov model (FAN-HMM), which\nincorporate time-varying covariates and feedback mechanisms from past\nobservations to latent states and future responses. Integrating these models\nwith the structural causal model framework allows flexible causal inference in\nlongitudinal data with time-varying unobserved heterogeneity and multiple\ncausal pathways. I show how, in a common case of categorical response\nvariables, long-term causal effects can be estimated efficiently without the\nneed for simulating counterfactual trajectories. Using simulation experiments,\nI study the performance of FAN-HMM under the common misspecification of the\nnumber of latent states, and finally apply the proposed approach to estimate\nthe effect of the 2013 parental leave reform on fathers' paternal leave uptake\nin Finnish workplaces.",
        "Polarization of starlight and thermal dust emission due to aligned\nnon-spherical grains helps us to trace magnetic field (B-field) morphology in\nmolecular clouds and to study grain alignment mechanisms. In this work, we\nstudy grain alignment and disruption mechanisms in a filamentary infrared dark\ncloud G34.43+0.24 using thermal dust polarization observations from JCMT\/POL-2\nat 850 $\\mu\\text{m}$. We study in three sub-regions as North harboring MM3\ncore, Center harboring MM1 and MM2 cores and South having no core. We find the\ndecrease in polarization fraction P with increasing total intensity and gas\ncolumn density, known as polarization hole. To disentangle the effect of\nmagnetic field tangling on the polarization hole, we estimate the polarization\nangle dispersion function. We find depolarizations in North and Center regions\nare due to decrease in net alignment efficiency of grains but in South region,\neffect of magnetic field tangling is significant to cause depolarization. To\ntest whether RAdiative Torque (RAT) mechanism can reproduce the observational\ndata, we calculate minimum alignment and disruption sizes of grains using RAT\ntheory and our study finds that RAT alignment mechanism can explain the\ndepolarizations in North and Center regions where B-field tangling effect is\nless important, except for core regions. We find hints of RAdiative Torque\nDisruption (RAT-D) in the core regions of MM3 in North, MM1 and MM2 in Center.\nWe also find that the high P value of around 8-20% in the outer regions of the\nfilament can be explained potentially by magnetically enhanced RAT alignment\nmechanism.",
        "Line-of-sight distortions of the cosmic microwave background (CMB), including\ngravitational lensing, cosmic birefringence, and patchy screening, encode\ncrucial cosmological information. While quadratic estimators (QE) have been\nexcellent tools for extracting these signals, they become suboptimal for\ncurrent- and next-generation CMB surveys, failing to maximize signal-to-noise\nand suffering from bias contamination that standard bias-hardening techniques\ncannot mitigate. We present a joint maximum a posteriori framework that\nsimultaneously reconstructs multiple distortion fields while explicitly\naccounting for their mutual contamination. For cosmic birefringence searches,\nour method achieves up to 2.5 improvement in reconstruction noise compared to\nthe QE, while significantly reducing CMB lensing-induced biases up to a factor\nof 5. These gains in lensing biases manifest not only in deep polarization\nsurveys like CMB-S4 and SPT-3G, but also in higher-noise experiments like\nSimons Observatory, where our method reduces lensing-induced biases by a factor\nof two thanks to the power of delensing. Our code provides a first step to\nrobust analyses of CMB secondary anisotropies, their cross-correlations with\nlarge-scale structure, and ultimately enabling more sensitive searches for\nprimordial $B$-modes.",
        "We investigate the nearsightedness property in the linear tight binding model\nat zero Fermi-temperature. We focus on the decay property of the density matrix\nfor materials with indirect band gaps. By representing the density matrix in\nreciprocal space, we establish a qualitatively sharp estimate for the\nexponential decay rate in homogeneous systems. An extending result under\nperturbations is also derived. This work refines the estimates presented in\n(Ortner, Thomas & Chen 2020), particularly for systems with small band gaps.",
        "Particle-wall interactions play a crucially important role in various\napplications such as microfluidic devices for cell sorting, particle\nseparation, entire class of hydrodynamic filtration and its derivatives, etc.\nYet, accurate implementation of interactions between wall and finite-size\nparticle is not trivial when working with the currently available particle\ntracking algorithms\/packages as they typically work with point-wise particles.\nHerein, we report a particle tracking algorithm that takes into account\ninteractions between particles of finite size and solid objects existing inside\ncomputational domain. A particle is modeled as a set of circumferential points\non its perimeter. While fluid-particle interactions are captured during the\ntrack of particle center, interactions between particle and nearby solid\nobjects are modeled explicitly by examining circumferential points and applying\na reflection scheme as needed to ensure impenetrability of solid objects. We\nalso report a modified variant of auxiliary structured grid method to locate\nhosting cells, which in conjunction with a boundary condition scheme enables\nthe capture of interactions between particle and solid objects. As a\nproof-of-concept, we numerically and experimentally study the motion of\nparticles within a microfluidic deterministic lateral displacement device. The\nmodeling results successfully demonstrate the zig-zag and bumping displacement\nmodes observed in our experiments. We also study a microfluidic device with\npinched flow numerically and validate our results against experimental data\nfrom the literature. By demonstrating an almost 8x speedup on a system with 8\nPerformance threads, our investigations suggest that the particle tracking\nalgorithm and its implementation code can benefit from parallel processing on\nmulti-thread systems by using the OpenMP application programming interface.",
        "Purpose: Iliac wing fractures due to lap belt loading have been observed in\nlaboratory settings for 50 years and recent data suggest they are also\noccurring in the field. Automated driving systems (ADS) and other occupant\ncompartment advancements are expected to offer enhanced flexibility in seating\norientation, which could place a greater reliance on the seatbelt to restrain\noccupants. Such changes may increase seatbelt loads and create new challenges\nin successfully restraining occupants and mitigating injury to areas such as\nthe pelvis. Injury criteria exist for component-level male iliac wing fractures\nresulting from frontal lap belt loading, but not for females. Methods: This\nstudy explored female iliac wing fracture tolerance in the same loading\nenvironment as a previous study that explored the fracture tolerance of\nisolated male iliac wings. Male and female fracture data were combined to\nevaluate the effect of sex. Injury risk functions were created by fitting\nWeibull survival models to data that integrated censored and exact failure\nobservations. Results: Twenty female iliac wings were tested; fourteen of them\nsustained fracture with known failure forces (exact), but the remaining six\nwings either (1) did not fracture, or (2) fractured after an event that changed\nthe boundary conditions (right censored). The fracture tolerance of the tested\nspecimens ranged widely (1134 - 8759 N) and averaged 4240 N (SD 2516 N).\nConclusion: Female data and combined male-female data were analyzed. Age was\nthe only covariate investigated in this study that had a statistically\nsignificant effect and improved the predictive performance of the models.",
        "Spatial misalignment problems arise from both data aggregation and attempts\nto align misaligned data, leading to information loss. We propose a Bayesian\ndisaggregation framework that links misaligned data to a continuous domain\nmodel using an iteratively linearised integration method via integrated nested\nLaplace approximation (INLA). The framework supports point pattern and\naggregated count models under four covariate field scenarios: \\textit{Raster at\nFull Resolution (RastFull), Raster Aggregation (RastAgg), Polygon Aggregation\n(PolyAgg), and Point Values (PointVal)}. The first three involve aggregation,\nwhile the latter two have incomplete fields. For PolyAgg and PointVal, we\nestimate the full covariate field using \\textit{Value Plugin, Joint\nUncertainty, and Uncertainty Plugin} methods, with the latter two accounting\nfor uncertainty propagation. These methods demonstrate superior performance,\nand remain more robust even under model misspecification (i.e.\\ modelling a\nnonlinear field as linear).\n  In landslide studies, landslide occurrences are often aggregated into counts\nbased on slope units, reducing spatial detail. The results indicate that point\npattern observations and full-resolution covariate fields should be\nprioritized. For incomplete fields, methods incorporating uncertainty\npropagation are preferred. This framework supports landslide susceptibility and\nother spatial mapping, integrating seamlessly with INLA-extension packages.",
        "Exploring various unexpected new quantum states and their corresponding\nextraordinary physics in low dimensional quantum materials, and investing them\ninto application fields, is a primary concern of condensed matter physics.\nPreviously, we performed joint experiments and theoretical calculations to\ninvestigate a superatomic crystal of Au6Te12Se8 with low symmetry, which stacks\nthrough non-covalent inter-cube quasi-bonds. Au6Te12Se8 exhibits a triple-cube\ncharge density wave and spatially polarized metallic states interweaving at 9\nK. In addition, it undergoes a BKT phase transition at 2.8 K to form a\nquasi-two-dimensional superconducting state. The subsequent high-pressure\nexperimental results indicate that the two quantum states above compete with\nsuperconductivity. Here, we experimentally revealed competition and coexistence\namong superconductivity, triple-cube charge density waves, and polarized\nmetallic states during further temperature-decreasing process, as examined\nusing ultra-low temperature scanning tunneling microscopy\/spectroscopy,\ntransport measurement, and Raman spectra. An extraordinary inversion symmetry\nbroken emerged inside the triple-cube-period of the original CDW at 300 mK,\nleading to the polarized metallic states transforming into parallel\npolarization states. The evidence of spontaneous polarization coexisting with\nsuperconductivity in real space is extremely rare and has been revealed by STM.\nIn addition, transport and Raman measurements indicate that there are multiple\nphase transition temperatures from 80K to that below the superconducting\ntransition temperature of Au6Te12Se8, which may also contain more unknown\nexotic quantum states, providing a platform for further exploration of\nintriguing physical properties."
      ]
    }
  },
  {
    "id":2411.08063,
    "research_type":"applied",
    "start_id":"b0",
    "start_title":"A Data-Science Approach to Predict the Heat Capacity of Nanoporous Materials",
    "start_abstract":"The heat capacity of a material is a fundamental property of great practical importance. For example, in a carbon capture process, the heat required to regenerate a solid sorbent is directly related to the heat capacity of the material. However, for most materials suitable for carbon capture applications, the heat capacity is not known, and thus the standard procedure is to assume the same value for all materials. In this work, we developed a machine learning approach, trained on density functional theory simulations, to accurately predict the heat capacity of these materials, that is, zeolites, metal\u2013organic frameworks and covalent\u2013organic frameworks. The accuracy of our prediction is confirmed with experimental data. Finally, for a temperature swing adsorption process that captures carbon from the flue gas of a coal-fired power plant, we show that for some materials, the heat requirement is reduced by as much as a factor of two using the correct heat capacity. Heat capacity of nanoporous materials is important for processes such as carbon capture, as this can affect process design energy requirements. Here, a machine learning approach for heat capacity prediction, trained on density functional theory simulations, is presented and experimentally verified.",
    "start_categories":[
      "cond-mat.mtrl-sci"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "A deep-learning approach to realizing functionality in nanoelectronic devices"
      ],
      "abstract":[
        "Many nanoscale devices require precise optimization to function. Tuning them to the desired operation regime becomes increasingly difficult and time-consuming when the number of terminals and couplings grows. Imperfections and device-to-device variations hinder optimization that uses physics-based models. Deep neural networks (DNNs) can model various complex physical phenomena but, so far, are mainly used as predictive tools. Here, we propose a generic deep-learning approach to efficiently optimize complex, multi-terminal nanoelectronic devices for desired functionality. We demonstrate our approach for realizing functionality in a disordered network of dopant atoms in silicon. We model the input\u2013output characteristics of the device with a DNN, and subsequently optimize control parameters in the DNN model through gradient descent to realize various classification tasks. When the corresponding control settings are applied to the physical device, the resulting functionality is as predicted by the DNN model. We expect our approach to contribute to fast, in situ optimization of complex (quantum) nanoelectronic devices."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Boundary Prompting: Elastic Urban Region Representation via Graph-based\n  Spatial Tokenization",
        "Data and System Perspectives of Sustainable Artificial Intelligence",
        "Artificial Intelligence in Creative Industries: Advances Prior to 2025",
        "Zweistein: A Dynamic Programming Evaluation Function for Einstein\n  W\\\"urfelt Nicht!",
        "NS-Gym: Open-Source Simulation Environments and Benchmarks for\n  Non-Stationary Markov Decision Processes",
        "Assessing AI Adoption and Digitalization in SMEs: A Framework for\n  Implementation",
        "Model-Free RL Agents Demonstrate System 1-Like Intentionality",
        "Think Smarter not Harder: Adaptive Reasoning with Inference Aware\n  Optimization",
        "Speeding up design and making to reduce time-to-project and\n  time-to-market: an AI-Enhanced approach in engineering education",
        "STMA: A Spatio-Temporal Memory Agent for Long-Horizon Embodied Task\n  Planning",
        "Cognitive-Aligned Document Selection for Retrieval-augmented Generation",
        "Advancing Mobile GUI Agents: A Verifier-Driven Approach to Practical\n  Deployment",
        "Governing AI Agents",
        "On conductor submonoids of factorial monoids",
        "Depth of powers of edge ideals of edge-weighted integrally closed cycles",
        "Small Models Struggle to Learn from Strong Reasoners",
        "Spatial-temporal models for forest inventory data",
        "Supervised Manifold Learning for Functional Data",
        "DreamFLEX: Learning Fault-Aware Quadrupedal Locomotion Controller for\n  Anomaly Situation in Rough Terrains",
        "Encrypted Vector Similarity Computations Using Partially Homomorphic\n  Encryption: Applications and Performance Analysis",
        "FinTSB: A Comprehensive and Practical Benchmark for Financial Time\n  Series Forecasting",
        "Integrative Learning of Intensity Fluctuations of Quantum Dots under\n  Excitation via a Tailored Mixture Hidden Markov Model",
        "Contracting Strategies for Electrolyzers to Secure Grid Connection: The\n  Dutch Case",
        "Naked Eye Three-dimensional Display System Based on Time-multiplexed\n  Technology",
        "EHCTNet: Enhanced Hybrid of CNN and Transformer Network for Remote\n  Sensing Image Change Detection",
        "Unfitted boundary algebraic equation method based on difference\n  potentials and lattice Green's function in 3D",
        "PIXELS: Progressive Image Xemplar-based Editing with Latent Surgery",
        "Ultrafast pulsed laser evaluation of Single Event Transients in\n  opto-couplers"
      ],
      "abstract":[
        "Urban region representation is essential for various applications such as\nurban planning, resource allocation, and policy development. Traditional\nmethods rely on fixed, predefined region boundaries, which fail to capture the\ndynamic and complex nature of real-world urban areas. In this paper, we propose\nthe Boundary Prompting Urban Region Representation Framework (BPURF), a novel\napproach that allows for elastic urban region definitions. BPURF comprises two\nkey components: (1) A spatial token dictionary, where urban entities are\ntreated as tokens and integrated into a unified token graph, and (2) a region\ntoken set representation model which utilize token aggregation and a\nmulti-channel model to embed token sets corresponding to region boundaries.\nAdditionally, we propose fast token set extraction strategy to enable online\ntoken set extraction during training and prompting. This framework enables the\ndefinition of urban regions through boundary prompting, supporting varying\nregion boundaries and adapting to different tasks. Extensive experiments\ndemonstrate the effectiveness of BPURF in capturing the complex characteristics\nof urban regions.",
        "Sustainable AI is a subfield of AI for concerning developing and using AI\nsystems in ways of aiming to reduce environmental impact and achieve\nsustainability. Sustainable AI is increasingly important given that training of\nand inference with AI models such as large langrage models are consuming a\nlarge amount of computing power. In this article, we discuss current issues,\nopportunities and example solutions for addressing these issues, and future\nchallenges to tackle, from the data and system perspectives, related to data\nacquisition, data processing, and AI model training and inference.",
        "The rapid advancements in artificial intelligence (AI), particularly in\ngenerative AI and large language models (LLMs), have profoundly impacted the\ncreative industries by enabling innovative content creation, enhancing\nworkflows, and democratizing access to creative tools. This paper explores the\nsignificant technological shifts since our previous review in 2022,\nhighlighting how these developments have expanded creative opportunities and\nefficiency. These technological advancements have enhanced the capabilities of\ntext-to-image, text-to-video, and multimodal generation technologies. In\nparticular, key breakthroughs in LLMs have established new benchmarks in\nconversational AI, while advancements in image generators have revolutionized\ncontent creation. We also discuss AI integration into post-production\nworkflows, which has significantly accelerated and refined traditional\nprocesses. Despite these innovations, challenges remain, particularly for the\nmedia industry, due to the demands on communication traffic from creative\ncontent. We therefore include data compression and quality assessment in this\npaper. Furthermore, we highlight the trend toward unified AI frameworks capable\nof addressing multiple creative tasks and underscore the importance of human\noversight to mitigate AI-generated inaccuracies. Finally, we explore AI's\nfuture potential in the creative sector, stressing the need to navigate\nemerging challenges to maximize its benefits while addressing associated risks.",
        "This paper introduces Zweistein, a dynamic programming evaluation function\nfor Einstein W\\\"urfelt Nicht! (EWN). Instead of relying on human knowledge to\ncraft an evaluation function, Zweistein uses a data-centric approach that\neliminates the need for parameter tuning. The idea is to use a vector recording\nthe distance to the corner of all pieces. This distance vector captures the\nessence of EWN. It not only outperforms many traditional EWN evaluation\nfunctions but also won first place in the TCGA 2023 competition.",
        "In many real-world applications, agents must make sequential decisions in\nenvironments where conditions are subject to change due to various exogenous\nfactors. These non-stationary environments pose significant challenges to\ntraditional decision-making models, which typically assume stationary dynamics.\nNon-stationary Markov decision processes (NS-MDPs) offer a framework to model\nand solve decision problems under such changing conditions. However, the lack\nof standardized benchmarks and simulation tools has hindered systematic\nevaluation and advance in this field. We present NS-Gym, the first simulation\ntoolkit designed explicitly for NS-MDPs, integrated within the popular\nGymnasium framework. In NS-Gym, we segregate the evolution of the environmental\nparameters that characterize non-stationarity from the agent's decision-making\nmodule, allowing for modular and flexible adaptations to dynamic environments.\nWe review prior work in this domain and present a toolkit encapsulating key\nproblem characteristics and types in NS-MDPs. This toolkit is the first effort\nto develop a set of standardized interfaces and benchmark problems to enable\nconsistent and reproducible evaluation of algorithms under non-stationary\nconditions. We also benchmark six algorithmic approaches from prior work on\nNS-MDPs using NS-Gym. Our vision is that NS-Gym will enable researchers to\nassess the adaptability and robustness of their decision-making algorithms to\nnon-stationary conditions.",
        "The primary objective of this research is to examine the current state of\ndigitalization and the integration of artificial intelligence (AI) within small\nand medium-sized enterprises (SMEs) in Italy. There is a significant gap\nbetween SMEs and large corporations in their use of AI, with SMEs facing\nnumerous barriers to adoption. This study identifies critical drivers and\nobstacles to achieving intelligent transformation, proposing a framework model\nto address key challenges and provide actionable guidelines",
        "This paper argues that model-free reinforcement learning (RL) agents, while\nlacking explicit planning mechanisms, exhibit behaviours that can be analogised\nto System 1 (\"thinking fast\") processes in human cognition. Unlike model-based\nRL agents, which operate akin to System 2 (\"thinking slow\") reasoning by\nleveraging internal representations for planning, model-free agents react to\nenvironmental stimuli without anticipatory modelling. We propose a novel\nframework linking the dichotomy of System 1 and System 2 to the distinction\nbetween model-free and model-based RL. This framing challenges the prevailing\nassumption that intentionality and purposeful behaviour require planning,\nsuggesting instead that intentionality can manifest in the structured, reactive\nbehaviours of model-free agents. By drawing on interdisciplinary insights from\ncognitive psychology, legal theory, and experimental jurisprudence, we explore\nthe implications of this perspective for attributing responsibility and\nensuring AI safety. These insights advocate for a broader, contextually\ninformed interpretation of intentionality in RL systems, with implications for\ntheir ethical deployment and regulation.",
        "Solving mathematics problems has been an intriguing capability of large\nlanguage models, and many efforts have been made to improve reasoning by\nextending reasoning length, such as through self-correction and extensive long\nchain-of-thoughts. While promising in problem-solving, advanced long reasoning\nchain models exhibit an undesired single-modal behavior, where trivial\nquestions require unnecessarily tedious long chains of thought. In this work,\nwe propose a way to allow models to be aware of inference budgets by\nformulating it as utility maximization with respect to an inference budget\nconstraint, hence naming our algorithm Inference Budget-Constrained Policy\nOptimization (IBPO). In a nutshell, models fine-tuned through IBPO learn to\n``understand'' the difficulty of queries and allocate inference budgets to\nharder ones. With different inference budgets, our best models are able to have\na $4.14$\\% and $5.74$\\% absolute improvement ($8.08$\\% and $11.2$\\% relative\nimprovement) on MATH500 using $2.16$x and $4.32$x inference budgets\nrespectively, relative to LLaMA3.1 8B Instruct. These improvements are\napproximately $2$x those of self-consistency under the same budgets.",
        "This paper explores the integration of AI tools, such as ChatGPT and GitHub\nCopilot, in the Software Architecture for Embedded Systems course. AI-supported\nworkflows enabled students to rapidly prototype complex projects, emphasizing\nreal-world applications like SLAM robotics. Results demon-started enhanced\nproblem-solving, faster development, and more sophisticated outcomes, with AI\naugmenting but not replacing human decision-making.",
        "A key objective of embodied intelligence is enabling agents to perform\nlong-horizon tasks in dynamic environments while maintaining robust\ndecision-making and adaptability. To achieve this goal, we propose the\nSpatio-Temporal Memory Agent (STMA), a novel framework designed to enhance task\nplanning and execution by integrating spatio-temporal memory. STMA is built\nupon three critical components: (1) a spatio-temporal memory module that\ncaptures historical and environmental changes in real time, (2) a dynamic\nknowledge graph that facilitates adaptive spatial reasoning, and (3) a\nplanner-critic mechanism that iteratively refines task strategies. We evaluate\nSTMA in the TextWorld environment on 32 tasks, involving multi-step planning\nand exploration under varying levels of complexity. Experimental results\ndemonstrate that STMA achieves a 31.25% improvement in success rate and a 24.7%\nincrease in average score compared to the state-of-the-art model. The results\nhighlight the effectiveness of spatio-temporal memory in advancing the memory\ncapabilities of embodied agents.",
        "Large language models (LLMs) inherently display hallucinations since the\nprecision of generated texts cannot be guaranteed purely by the parametric\nknowledge they include. Although retrieval-augmented generation (RAG) systems\nenhance the accuracy and reliability of generative models by incorporating\nexternal documents, these retrieved documents often fail to adequately support\nthe model's responses in practical applications. To address this issue, we\npropose GGatrieval (Fine-\\textbf{G}rained \\textbf{G}rounded \\textbf{A}lignment\nRe\\textbf{trieval} for verifiable generation), which leverages an LLM to\ndynamically update queries and filter high-quality, reliable retrieval\ndocuments. Specifically, we parse the user query into its syntactic components\nand perform fine-grained grounded alignment with the retrieved documents. For\nquery components that cannot be individually aligned, we propose a dynamic\nsemantic compensation mechanism that iteratively refines and rewrites the query\nwhile continuously updating the retrieval results. This iterative process\ncontinues until the retrieved documents sufficiently support the query's\nresponse. Our approach introduces a novel criterion for filtering retrieved\ndocuments, closely emulating human strategies for acquiring targeted\ninformation. This ensures that the retrieved content effectively supports and\nverifies the generated outputs. On the ALCE benchmark, our method significantly\nsurpasses a wide range of baselines, achieving state-of-the-art performance.",
        "We propose V-Droid, a mobile GUI task automation agent. Unlike previous\nmobile agents that utilize Large Language Models (LLMs) as generators to\ndirectly generate actions at each step, V-Droid employs LLMs as verifiers to\nevaluate candidate actions before making final decisions. To realize this novel\nparadigm, we introduce a comprehensive framework for constructing\nverifier-driven mobile agents: the discretized action space construction\ncoupled with the prefilling-only workflow to accelerate the verification\nprocess, the pair-wise progress preference training to significantly enhance\nthe verifier's decision-making capabilities, and the scalable human-agent joint\nannotation scheme to efficiently collect the necessary data at scale. V-Droid\nsets a new state-of-the-art task success rate across several public mobile task\nautomation benchmarks: 59.5% on AndroidWorld, 38.3% on AndroidLab, and 49% on\nMobileAgentBench, surpassing existing agents by 9.5%, 2.1%, and 9%,\nrespectively. Furthermore, V-Droid achieves an impressively low latency of 0.7\nseconds per step, making it the first mobile agent capable of delivering\nnear-real-time, effective decision-making capabilities.",
        "The field of AI is undergoing a fundamental transition from generative models\nthat can produce synthetic content to artificial agents that can plan and\nexecute complex tasks with only limited human involvement. Companies that\npioneered the development of language models have now built AI agents that can\nindependently navigate the internet, perform a wide range of online tasks, and\nincreasingly serve as AI personal assistants and virtual coworkers. The\nopportunities presented by this new technology are tremendous, as are the\nassociated risks. Fortunately, there exist robust analytic frameworks for\nconfronting many of these challenges, namely, the economic theory of\nprincipal-agent problems and the common law doctrine of agency relationships.\nDrawing on these frameworks, this Article makes three contributions. First, it\nuses agency law and theory to identify and characterize problems arising from\nAI agents, including issues of information asymmetry, discretionary authority,\nand loyalty. Second, it illustrates the limitations of conventional solutions\nto agency problems: incentive design, monitoring, and enforcement might not be\neffective for governing AI agents that make uninterpretable decisions and\noperate at unprecedented speed and scale. Third, the Article explores the\nimplications of agency law and theory for designing and regulating AI agents,\narguing that new technical and legal infrastructure is needed to support\ngovernance principles of inclusivity, visibility, and liability.",
        "We give affirmative answers to Conjecture 4.16 in [1] and to Conjecture 2.3\nin [3].",
        "This paper gives some exact formulas for the depth of powers of the edge\nideal of an edge-weighted integrally closed cycle.",
        "Large language models (LLMs) excel in complex reasoning tasks, and distilling\ntheir reasoning capabilities into smaller models has shown promise. However, we\nuncover an interesting phenomenon, which we term the Small Model Learnability\nGap: small models ($\\leq$3B parameters) do not consistently benefit from long\nchain-of-thought (CoT) reasoning or distillation from larger models. Instead,\nthey perform better when fine-tuned on shorter, simpler reasoning chains that\nbetter align with their intrinsic learning capacity. To address this, we\npropose Mix Distillation, a simple yet effective strategy that balances\nreasoning complexity by combining long and short CoT examples or reasoning from\nboth larger and smaller models. Our experiments demonstrate that Mix\nDistillation significantly improves small model reasoning performance compared\nto training on either data alone. These findings highlight the limitations of\ndirect strong model distillation and underscore the importance of adapting\nreasoning complexity for effective reasoning capability transfer.",
        "The USDA Forest Inventory and Analysis (FIA) program conducts a national\nforest inventory for the United States through a network of permanent field\nplots. FIA produces estimates of area averages\/totals for plot-measured forest\nvariables through design-based inference, assuming a fixed population and a\nprobability sample of field plot locations. The fixed-population assumption and\ncharacteristics of the FIA sampling scheme make it difficult to estimate change\nin forest variables over time using design-based inference. We propose\nspatial-temporal models based on Gaussian processes as a flexible tool for\nforest inventory data, capable of inferring forest variables and change thereof\nover arbitrary spatial and temporal domains. It is shown to be beneficial for\nthe covariance function governing the latent Gaussian process to account for\nvariation at multiple scales, separating spatially local variation from\necosystem-scale variation. We demonstrate a model for forest biomass density,\ninferring 20 years of biomass change within two US National Forests.",
        "Classification is a core topic in functional data analysis. A large number of\nfunctional classifiers have been proposed in the literature, most of which are\nbased on functional principal component analysis or functional regression. In\ncontrast, we investigate this topic from the perspective of manifold learning.\nIt is assumed that functional data lie on an unknown low-dimensional manifold,\nand we expect that better classifiers can be built upon the manifold structure.\nTo this end, we propose a novel proximity measure that takes the label\ninformation into account to learn the low-dimensional representations, also\nknown as the supervised manifold learning outcomes. When the outcomes are\ncoupled with multivariate classifiers, the procedure induces a family of new\nfunctional classifiers. In theory, we show that our functional classifier\ninduced by the $k$-NN classifier is asymptotically optimal. In practice, we\nshow that our method, coupled with several classical multivariate classifiers,\nachieves outstanding classification performance compared to existing functional\nclassifiers in both synthetic and real data examples.",
        "Recent advances in quadrupedal robots have demonstrated impressive agility\nand the ability to traverse diverse terrains. However, hardware issues, such as\nmotor overheating or joint locking, may occur during long-distance walking or\ntraversing through rough terrains leading to locomotion failures. Although\nseveral studies have proposed fault-tolerant control methods for quadrupedal\nrobots, there are still challenges in traversing unstructured terrains. In this\npaper, we propose DreamFLEX, a robust fault-tolerant locomotion controller that\nenables a quadrupedal robot to traverse complex environments even under joint\nfailure conditions. DreamFLEX integrates an explicit failure estimation and\nmodulation network that jointly estimates the robot's joint fault vector and\nutilizes this information to adapt the locomotion pattern to faulty conditions\nin real-time, enabling quadrupedal robots to maintain stability and performance\nin rough terrains. Experimental results demonstrate that DreamFLEX outperforms\nexisting methods in both simulation and real-world scenarios, effectively\nmanaging hardware failures while maintaining robust locomotion performance.",
        "This paper explores the use of partially homomorphic encryption (PHE) for\nencrypted vector similarity search, with a focus on facial recognition and\nbroader applications like reverse image search, recommendation engines, and\nlarge language models (LLMs). While fully homomorphic encryption (FHE) exists,\nwe demonstrate that encrypted cosine similarity can be computed using PHE,\noffering a more practical alternative. Since PHE does not directly support\ncosine similarity, we propose a method that normalizes vectors in advance,\nenabling dot product calculations as a proxy. We also apply min-max\nnormalization to handle negative dimension values.\n  Experiments on the Labeled Faces in the Wild (LFW) dataset use DeepFace's\nFaceNet128d, FaceNet512d, and VGG-Face (4096d) models in a two-tower setup.\nPre-encrypted embeddings are stored in one tower, while an edge device captures\nimages, computes embeddings, and performs encrypted-plaintext dot products via\nadditively homomorphic encryption. We implement this with LightPHE, evaluating\nPaillier, Damgard-Jurik, and Okamoto-Uchiyama schemes, excluding others due to\nperformance or decryption complexity. Tests at 80-bit and 112-bit security\n(NIST-secure until 2030) compare PHE against FHE (via TenSEAL), analyzing\nencryption, decryption, operation time, cosine similarity loss, key\/ciphertext\nsizes.\n  Results show PHE is less computationally intensive, faster, and produces\nsmaller ciphertexts\/keys, making it well-suited for memory-constrained\nenvironments and real-world privacy-preserving encrypted similarity search.",
        "Financial time series (FinTS) record the behavior of human-brain-augmented\ndecision-making, capturing valuable historical information that can be\nleveraged for profitable investment strategies. Not surprisingly, this area has\nattracted considerable attention from researchers, who have proposed a wide\nrange of methods based on various backbones. However, the evaluation of the\narea often exhibits three systemic limitations: 1. Failure to account for the\nfull spectrum of stock movement patterns observed in dynamic financial markets.\n(Diversity Gap), 2. The absence of unified assessment protocols undermines the\nvalidity of cross-study performance comparisons. (Standardization Deficit), and\n3. Neglect of critical market structure factors, resulting in inflated\nperformance metrics that lack practical applicability. (Real-World Mismatch).\nAddressing these limitations, we propose FinTSB, a comprehensive and practical\nbenchmark for financial time series forecasting (FinTSF). To increase the\nvariety, we categorize movement patterns into four specific parts, tokenize and\npre-process the data, and assess the data quality based on some sequence\ncharacteristics. To eliminate biases due to different evaluation settings, we\nstandardize the metrics across three dimensions and build a user-friendly,\nlightweight pipeline incorporating methods from various backbones. To\naccurately simulate real-world trading scenarios and facilitate practical\nimplementation, we extensively model various regulatory constraints, including\ntransaction fees, among others. Finally, we conduct extensive experiments on\nFinTSB, highlighting key insights to guide model selection under varying market\nconditions. Overall, FinTSB provides researchers with a novel and comprehensive\nplatform for improving and evaluating FinTSF methods. The code is available at\nhttps:\/\/github.com\/TongjiFinLab\/FinTSBenchmark.",
        "Semiconductor nano-crystals, known as quantum dots (QDs), have garnered\nsignificant interest in various scientific fields due to their unique\nfluorescence properties. One captivating characteristic of QDs is their ability\nto emit photons under continuous excitation. The intensity of photon emission\nfluctuates during the excitation, and such a fluctuation pattern can vary\nacross different dots even under the same experimental conditions. What adding\nto the complication is that the processed intensity series are non-Gaussian and\ntruncated due to necessary thresholding and normalization. As such,\nconventional approaches in the chemistry literature, typified by single-dot\nanalysis of raw intensity data with Gaussian hidden Markov models (HMM), cannot\nmeet the many analytical challenges and may fail to capture any novel yet rare\nfluctuation patterns among QDs. Collaborating with scientists in the chemistry\nfield, we have developed an integrative learning approach to simultaneously\nanalyzing intensity series of multiple QDs. Our approach still inherits the HMM\nas the skeleton to model the intensity fluctuations of each dot, and based on\nthe data structure and the hypothesized collective behaviors of the QDs, our\napproach asserts that (i) under each hidden state, the normalized intensity\nfollows a 0\/1 inflated Beta distribution, (ii) the state distributions are\nshared across all the QDs, and (iii) the patterns of transitions can vary\nacross QDs. These unique features allow for a precise characterization of the\nintensity fluctuation patterns and facilitate the clustering of the QDs. With\nexperimental data collected on 128 QDs, our methods reveal several QD clusters\ncharacterized by unique transition patterns across three intensity states. The\nresults provide deeper insight into QD behaviors and their design\/application\npotentials.",
        "In response to increasing grid congestion in the Netherlands, non-firm\nconnection and transport agreements (CTAs) and capacity restriction contracts\n(CRCs) have been introduced, allowing consumer curtailment in exchange for grid\ntariff discounts or per-MW compensations. This study examines the interaction\nbetween an electrolyzer project, facing sizing and contracting decisions, and a\nnetwork operator, responsible for contract activations and determining grid\nconnection capacity, under the new Dutch regulations. The interaction is\nmodeled using two bilevel optimization problems with alternating\nleader-follower roles. Results highlight a trade-off between CRC income and\nnon-firm CTA tariff discounts, showing that voluntary congestion management by\nthe network operator increases electrolyzer profitability at CRC prices below\n10 euro per MW but reduces it at higher prices. Furthermore, the network\noperator benefits more from reacting to the electrolyzer owner's CTA decisions\nthan from leading the interaction at CRC prices above 10 euro per MW. Ignoring\nthe other party's optimization problem overestimates profits for both the\nnetwork operator and the electrolyzer owner, emphasizing the importance of\ncoordinated decision-making.",
        "Our group is developing a multi-user eye-tracked 3D display, an evolution of\nthe single-user eye-tracked 3D display that we have already successfully\ndeveloped. This display utilizes a slanted lenticular setup, where multiple\nperspective views are shown across the viewing field. Due to the constraints of\nthe lenticular lens parameters, identical views are repeated across the field,\nlimiting eye tracking to a single user. However, this limitation can be\naddressed using spatio-temporal multiplexing, where view zone groups are\npresented sequentially with a high frame rate liquid crystal display (LCD) and\ndriver, in combination with a synchronized directional light emitting diode\n(LED) array. In this paper, we describe the operation and results of the\nbacklight drive electronics, where a prototype using a white LED illumination\nmatrix, a simplified LCD panel, and a linear Fresnel lens array serves as a\ntest bed.",
        "Remote sensing (RS) change detection incurs a high cost because of false\nnegatives, which are more costly than false positives. Existing frameworks,\nstruggling to improve the Precision metric to reduce the cost of false\npositive, still have limitations in focusing on the change of interest, which\nleads to missed detections and discontinuity issues. This work tackles these\nissues by enhancing feature learning capabilities and integrating the frequency\ncomponents of feature information, with a strategy to incrementally boost the\nRecall value. We propose an enhanced hybrid of CNN and Transformer network\n(EHCTNet) for effectively mining the change information of interest. Firstly, a\ndual branch feature extraction module is used to extract the multi scale\nfeatures of RS images. Secondly, the frequency component of these features is\nexploited by a refined module I. Thirdly, an enhanced token mining module based\non the Kolmogorov Arnold Network is utilized to derive semantic information.\nFinally, the semantic change information's frequency component, beneficial for\nfinal detection, is mined from the refined module II. Extensive experiments\nvalidate the effectiveness of EHCTNet in comprehending complex changes of\ninterest. The visualization outcomes show that EHCTNet detects more intact and\ncontinuous changed areas and perceives more accurate neighboring distinction\nthan state of the art models.",
        "This work presents an unfitted boundary algebraic equation (BAE) method for\nsolving three-dimensional elliptic partial differential equations on complex\ngeometries using finite difference on structured meshes. We demonstrate that\nreplacing finite auxiliary domains with free-space LGFs streamlines the\ncomputation of difference potentials, enabling matrix-free implementations and\nsignificant cost reductions. We establish theoretical foundations by showing\nthe equivalence between direct formulations in difference potentials framework\nand indirect single\/double layer formulations and analyzing their spectral\nproperties. The spectral analysis demonstrates that discrete double layer\nformulations provide better-conditioned systems for iterative solvers,\nsimilarly as in boundary integral method. The method is validated through\nmatrix-free numerical experiments on both Poisson and modified Helmholtz\nequations in 3D implicitly defined geometries, showing optimal convergence\nrates and computational efficiency. This framework naturally extends to\nunbounded domains and provides a foundation for applications to more complex\nsystems like Helmholtz and Stokes equations.",
        "Recent advancements in language-guided diffusion models for image editing are\noften bottle-necked by cumbersome prompt engineering to precisely articulate\ndesired changes. An intuitive alternative calls on guidance from in-the-wild\nimage exemplars to help users bring their imagined edits to life. Contemporary\nexemplar-based editing methods shy away from leveraging the rich latent space\nlearnt by pre-existing large text-to-image (TTI) models and fall back on\ntraining with curated objective functions to achieve the task. Though somewhat\neffective, this demands significant computational resources and lacks\ncompatibility with diverse base models and arbitrary exemplar count. On further\ninvestigation, we also find that these techniques restrict user control to only\napplying uniform global changes over the entire edited region. In this paper,\nwe introduce a novel framework for progressive exemplar-driven editing with\noff-the-shelf diffusion models, dubbed PIXELS, to enable customization by\nproviding granular control over edits, allowing adjustments at the pixel or\nregion level. Our method operates solely during inference to facilitate\nimitative editing, enabling users to draw inspiration from a dynamic number of\nreference images, or multimodal prompts, and progressively incorporate all the\ndesired changes without retraining or fine-tuning existing TTI models. This\ncapability of fine-grained control opens up a range of new possibilities,\nincluding selective modification of individual objects and specifying gradual\nspatial changes. We demonstrate that PIXELS delivers high-quality edits\nefficiently, leading to a notable improvement in quantitative metrics as well\nas human evaluation. By making high-quality image editing more accessible,\nPIXELS has the potential to enable professional-grade edits to a wider audience\nwith the ease of using any open-source image generation model.",
        "We build a 1064 nm fiber laser system-based testing facility for emulating\nSETs in different electronics components and ICs. Using these facilities, we\ntested the 4N35 optocoupler to observe SETs for the first time."
      ]
    }
  },
  {
    "id":2411.08063,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"A deep-learning approach to realizing functionality in nanoelectronic devices",
    "start_abstract":"Many nanoscale devices require precise optimization to function. Tuning them to the desired operation regime becomes increasingly difficult and time-consuming when the number of terminals and couplings grows. Imperfections and device-to-device variations hinder optimization that uses physics-based models. Deep neural networks (DNNs) can model various complex physical phenomena but, so far, are mainly used as predictive tools. Here, we propose a generic deep-learning approach to efficiently optimize complex, multi-terminal nanoelectronic devices for desired functionality. We demonstrate our approach for realizing functionality in a disordered network of dopant atoms in silicon. We model the input\u2013output characteristics of the device with a DNN, and subsequently optimize control parameters in the DNN model through gradient descent to realize various classification tasks. When the corresponding control settings are applied to the physical device, the resulting functionality is as predicted by the DNN model. We expect our approach to contribute to fast, in situ optimization of complex (quantum) nanoelectronic devices.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b0"
      ],
      "title":[
        "A Data-Science Approach to Predict the Heat Capacity of Nanoporous Materials"
      ],
      "abstract":[
        "The heat capacity of a material is a fundamental property of great practical importance. For example, in a carbon capture process, the heat required to regenerate a solid sorbent is directly related to the heat capacity of the material. However, for most materials suitable for carbon capture applications, the heat capacity is not known, and thus the standard procedure is to assume the same value for all materials. In this work, we developed a machine learning approach, trained on density functional theory simulations, to accurately predict the heat capacity of these materials, that is, zeolites, metal\u2013organic frameworks and covalent\u2013organic frameworks. The accuracy of our prediction is confirmed with experimental data. Finally, for a temperature swing adsorption process that captures carbon from the flue gas of a coal-fired power plant, we show that for some materials, the heat requirement is reduced by as much as a factor of two using the correct heat capacity. Heat capacity of nanoporous materials is important for processes such as carbon capture, as this can affect process design energy requirements. Here, a machine learning approach for heat capacity prediction, trained on density functional theory simulations, is presented and experimentally verified."
      ],
      "categories":[
        "cond-mat.mtrl-sci"
      ]
    },
    "list":{
      "title":[
        "Tunable spin and orbital torques in Cu-based magnetic heterostructures",
        "SALMON VR: Visualizing Light-Matter Dynamics",
        "Photo-induced spall failure of (111) twist grain boundaries in Ni\n  bicrystals",
        "Non-adiabaticity from first principles: the exact-factorization approach\n  for solids",
        "Ultrafast demagnetization dynamics of 4f antiferromagnets",
        "Predicting the N\\'eel temperatures in general helimagnetic materials: a\n  comparison between mean field theory, random phase approximation,\n  renormalized spin wave theory and classical Monte Carlo simulations",
        "Intrinsic higher-order topological states in 2D honeycomb Z_2 quantum\n  spin Hall insulators",
        "Microstructure-Aware Bayesian Materials Design",
        "Electrocatalyst discovery through text mining and multi-objective\n  optimization",
        "Electronic origin of stability of 2D 1H-phase Janus transition metal\n  dichalcogenides and beyond",
        "Taxonomy of amorphous ternary phase diagrams: the importance of\n  interaction parameters",
        "Generalized Bond Polarizability model for more accurate atomistic\n  modeling of Raman spectra",
        "Dzyalonshinskii-Moriya interaction in Fe5GeTe2 epitaxial thin films",
        "Skeletal Torus Actions and GKM Structures on Quiver Grassmannians of\n  String Representations",
        "Data Augmentation and Regularization for Learning Group Equivariance",
        "Nitrogen-Vacancy Centers in Epitaxial Laterally Overgrown Diamond:\n  Towards Up-scaling of Color Center-based Quantum Technologies",
        "Towards a complexity-theoretic dichotomy for TQFT invariants",
        "Stabilization of quantum properties under intrinsic decoherence in\n  presence of external magnetic fields",
        "Searching for Inflationary Physics with the CMB Trispectrum: 3.\n  Constraints from Planck",
        "Directional optical parametric amplification in a hyperbolic\n  metamaterial",
        "Laser cooled 137BaF molecules for measuring nuclear-spin-dependent\n  parity violation",
        "Interplay of ALP Couplings at a Muon Collider",
        "Change of some cropping systems in a long-term trial comparing different\n  systems: rationale and implications for statistical analysis",
        "Lie Symmetry Analysis, Parametric Reduction and Conservation Laws of\n  (3+1) Dimensional Nonlinear Dispersive Soliton Equation",
        "Deep Learning-Powered Electrical Brain Signals Analysis: Advancing\n  Neurological Diagnostics",
        "Learning by Confusion: The Phase Diagram of the Holstein Model",
        "Decaying turbulence beneath surface waves",
        "Thermodynamic uncertainty relations for three-terminal systems with\n  broken time-reversal symmetry"
      ],
      "abstract":[
        "Current-induced torques originating from earth-abundant 3d elements offer a\npromising avenue for low-cost and sustainable spintronic memory and logic\napplications. Recently, orbital currents -- transverse orbital angular momentum\nflow in response to an electric field -- have been in the spotlight since they\nallow current-induced torque generation from 3d transition metals. Here, we\nreport a comprehensive study of the current-induced spin and orbital torques in\nCu-based magnetic heterostructures. We show that high torque efficiencies can\nbe achieved in engineered Ni80Fe20\/Cu bilayers where Cu is naturally oxidized,\nexceeding the ones found in the archetypical Co\/Pt. Furthermore, we demonstrate\nsign and amplitude control of the damping-like torque by manipulating the\noxidation state of Cu via solid-state gating. Our findings provide insights\ninto the interplay between charge, spin, and orbital transport in Cu-based\nheterostructures and open the door to the development of gate-tunable\nspin-orbitronic devices.",
        "This study presents SALMON VR, a visualization program designed to visualize\nthe time evolution of electronic density changes and vector potentials in\nvirtual reality (VR) space. The time-series electronic density data computed by\nSALMON are stored in CUBE format. SALMON VR processes these data to construct\nisosurfaces of electronic density variations and two-dimensional\nrepresentations of vector potentials. Equipped with a user-friendly interface\nusing VR technology, the program is available in two versions: one for the Meta\nQuest 3 head-mount display (Meta Platforms Inc., California) and one for PCs.\nAtoms are displayed as spheres of different sizes and colors according to their\nelemental properties. This visual representation facilitates a deeper\nunderstanding of the complex interactions between light and electrons. Users\ncan easily manipulate the isosurface values, speed of animation, and color map\nof the vector potential. SALMON VR will enable researchers and educators to\nenhance their understanding of physical phenomena and improve engagement in\nlearning environments.",
        "Spall failure, a complex failure mechanism driven by tensile stress wave\ninteractions, has been extensively studied in single-crystal FCC metals,\nrevealing a precursor stage involving dislocation emission along closed-packed\ndirections. Here we investigate the photo-induced spall failure of Ni\nbicrystals under a two-pulse laser configuration, exploring various\nmisorientation angles through two-temperature molecular dynamics (MD)\nsimulations including electronic effects to simulate light-matter interaction.\nOur findings demonstrate that light-matter interactions can induce spall\nfailure at the sample center, similar to conventional plate-impact methods,\nwhen two laser-pulses are applied to the front and back surfaces of the sample.\nThe study reveals the significant influence of misorientation angles on\ndislocation activity and spall behavior, where grain boundaries (GBs) play\npivotal roles, either promoting or impeding dislocation interactions.\nFurthermore, our work highlights the potential for enhancing spall resistance\nby tailoring materials through misorientation angle variation.",
        "The thorough treatment of electron-lattice interactions from first principles\nis one of the main goals in condensed matter physics. While the commonly\napplied adiabatic Born-Oppenheimer approximation is sufficient for describing\nmany physical phenomena, it is limited in its ability to capture meaningful\nfeatures originating from non-adiabatic coupling effects. The exact\nfactorization method, starting from the full Hamiltonian of electrons and\nnuclei, provides a way to systematically account for non-adiabatic effects.\nThis formalism was recently developed into an ab initio density functional\ntheory framework. Within this framework we here develop a perturbative approach\nto the electronic states in solid state materials. We derive\nexact-factorization-based perturbations of the Kohn-Sham states up to second\norder in the nuclear displacements. These non-adiabatic features in the\ncalculated energy and wavefunction corrections are expressed in terms of\nreadily available density functional perturbation theory components.",
        "We study the ultrafast demagnetization dynamics of LnRh$_2$Si$_2$ (Ln $=$ Pr,\nNd, Sm, Gd, Tb, Dy, Ho) antiferromagnets (AFM) after excitation by a laser\npulse, using a combination of density functional theory and atomistic spin and\nspin-lattice dynamics simulations. First, we calculate the Heisenberg\ninteractions using the magnetic force theorem and compare two approaches, where\nthe $4f$ states of the rare earths are treated as frozen core states or as\nvalence states with added correlation corrections. We find marked quantitative\ndifferences in terms of predicted Curie temperature for most of the systems,\nespecially for those with large orbital moment of the rare earth cations. This\ncan be attributed to the importance of indirect interactions of the $4f$ states\nthrough the Si states, which depend on the binding energy of the $4f$ states\nand coexists with RKKY-type interactions mediated by the conduction states.\nHowever, qualitatively, both approaches agree in terms of the predicted AFM\nordering at low temperatures. In the second step, the atomistic dynamics\nsimulations are combined with a heat-conserving two-temperature model, allowing\nfor the calculation of spin and electronic temperatures during the\nmagnetization dynamics simulations. Despite quite different demagnetization\ntimes, magnetization dynamics of all studied LnRh$_2$Si$_2$ AFM exhibit similar\ntwo-step behavior, in particular, the first fast drop followed by slower\ndemagnetization. We observe that the demagnetization amplitude depends linearly\non laser fluence for low fluences, which is in agreement with experimental\nobservations. We also investigate the impact of lattice dynamics on ultrafast\ndemagnetization using coupled atomistic spin-lattice dynamics simulations and a\nheat-conserving three-temperature model, which confirm linear dependence of\nmagnetisation on laser fluence.",
        "The critical temperature for magnetic order comprises a crucial property of\nany magnetic material and ranges from a few Kelvin in certain antiferromagnets\nto 1400 K in ferromagnetic Co. However, the prediction of critical temperatures\nbased on, for example, a spin wave dispersion is in general non-trivial. For\nferromagnets and simple collinear antiferromagnets, estimates may be obtained\nfrom the Heisenberg model using either renormalized spin wave theory or the\nGreen's function random phase approximation (RPA), but a systematic assessment\nof the accuracy of such approaches seems to be lacking in the literature. In\nthis work, we propose generalizations of both renormalized spin wave theory and\nRPA to calculate the critical temperatures of single-$Q$ helimagnetic ground\nstates, which include ferromagnets and antiferromagnets as special cases. We\ncompare the methods to classical Monte Carlo simulations and Mean field theory,\nusing experimental exchange parameters for a wide range of materials; MnO and\nNiO (single site N\\'eel ground states), MnF$_2$ (altermagnet), Cr$_2$O$_3$ and\nFe$_2$O$_3$ (two site N\\'eel states) and Ba$_3$NbFe$_3$Si$_2$O$_{14}$\n(incommensurate helimagnet). In all cases, we observe that predictions from RPA\nare in excellent agreement with experimental values and RPA thus constitutes a\nrather reliable all-purpose method for calculating critical temperatures.",
        "The exploration of topological phases remains a cutting-edge research\nfrontier, driven by their promising potential for next-generation electronic\nand quantum technologies. In this work, we employ first-principles calculations\nand tight-binding modeling to systematically investigate the topological\nproperties of freestanding two-dimensional (2D) honeycomb Bi, HgTe, and\nAl2O3(0001)-supported HgTe. Remarkably, all three systems exhibit coexistence\nof first-order and higher-order topological insulator states, manifested by\ngapless edge states in one-dimensional (1D) nanoribbons and symmetry-related\ncorner states in zero-dimensional (0D) nanoflakes. Furthermore, fractional\nelectron charges may accumulate at the corners of armchair-edged nanoflakes.\nAmong these materials, HgTe\/Al2O3(0001) is particularly promising due to its\nexperimentally feasible atomic configuration and low-energy corner states. Our\nfindings highlight the importance of exploring higher-order topological phases\nin Z_2 quantum spin Hall insulators and pave the way for new possibilities in\ndevice applications.",
        "In this study, we propose a novel microstructure-sensitive Bayesian\noptimization (BO) framework designed to enhance the efficiency of materials\ndiscovery by explicitly incorporating microstructural information. Traditional\nmaterials design approaches often focus exclusively on direct\nchemistry-process-property relationships, overlooking the critical role of\nmicrostructures. To address this limitation, our framework integrates\nmicrostructural descriptors as latent variables, enabling the construction of a\ncomprehensive process-structure-property mapping that improves both predictive\naccuracy and optimization outcomes. By employing the active subspace method for\ndimensionality reduction, we identify the most influential microstructural\nfeatures, thereby reducing computational complexity while maintaining high\naccuracy in the design process. This approach also enhances the probabilistic\nmodeling capabilities of Gaussian processes, accelerating convergence to\noptimal material configurations with fewer iterations and experimental\nobservations. We demonstrate the efficacy of our framework through synthetic\nand real-world case studies, including the design of Mg$_2$Sn$_x$Si$_{1-x}$\nthermoelectric materials for energy conversion. Our results underscore the\ncritical role of microstructures in linking processing conditions to material\nproperties, highlighting the potential of a microstructure-aware design\nparadigm to revolutionize materials discovery. Furthermore, this work suggests\nthat since incorporating microstructure awareness improves the efficiency of\nBayesian materials discovery, microstructure characterization stages should be\nintegral to automated -- and eventually autonomous -- platforms for materials\ndevelopment.",
        "The discovery and optimization of high-performance materials is the basis for\nadvancing energy conversion technologies. To understand composition-property\nrelationships, all available data sources should be leveraged: experimental\nresults, predictions from simulations, and latent knowledge from scientific\ntexts. Among these three, text-based data sources are still not used to their\nfull potential. We present an approach combining text mining, Word2Vec\nrepresentations of materials and properties, and Pareto front analysis for the\nprediction of high-performance candidate materials for electrocatalysis in\nregions where other data sources are scarce or non-existent. Candidate\ncompositions are evaluated on the basis of their similarity to the terms\n`conductivity' and `dielectric', which enables reaction-specific candidate\ncomposition predictions for oxygen reduction (ORR), hydrogen evolution (HER),\nand oxygen evolution (OER) reactions. This, combined with Pareto optimization,\nallows us to significantly reduce the pool of candidate compositions to\nhigh-performing compositions. Our predictions, which are purely based on text\ndata, match the measured electrochemical activity very well.",
        "Janus transition metal dichalcogenides (JTMDs) monolayers have emerged as a\nnew paradigm to broaden the family of two-dimensional (2D) materials. Despite\nnumerous theoretical predictions of JTMDs, their experimental realization\nremains scarce, most probably due to intrinsic structural fragility. We\nidentify a dependence of the structural stability of 1H-phase JTMDs on the\ntransition metal group, with Group-VIB-based monolayers exhibiting robust\nstability, as evidenced by the successful synthesized MoSSe and WSSe. The\ngroup-dependent stability arises from the competition between metal-ligand\nionic bonding and ligand-ligand covalent bonding, as well as the high-energy\nd-electron orbital splitting. We propose an electron configuration that\ndescribes the interactions of electrons near the Fermi level to correlate the\nstability, and introduce an electron compensation strategy to stabilize certain\nunstable JTMDs systems. Guided by the electronic origin of stability, we\npredict a family of stable 2D Janus transition metal halides with intrinsic\nferromagnetic valley properties. This work bridges the gap between electronic\nstructure and stability predictions, and extends the design rules for\nsynthesizing 2D Janus materials.",
        "Understanding phase diagrams is essential for material selection and design,\nas they provide a comprehensive representation of the thermodynamics of\nmixtures. This work delivers a broad and systematic overview of possible\nternary phase diagrams for amorphous systems representative of polymers, small\norganic molecules, and solvents. Thanks to computationally efficient methods,\nan unprecedented library of $>$80,000 ternary phase diagrams is generated based\non a systematic screening of interaction parameters. Twenty-one phase diagram\ntypes, including unreported ones, are identified. They are classified according\nto simple rules related to the number of immiscible material pairs, of\nmiscibility gaps, and of three-phase regions. They are mapped onto the\nthree-dimensional interaction parameters space, providing a clear picture of\ntheir likelihood and existence conditions. Four well-known phase-diagram types\nwith 0, 1, 2, or 3 immiscible pairs are found to be the most likely. The\nnumerous uncommon phase diagrams are mostly observed within a small parameter\nwindow around the critical interaction parameter values. For the most common\nphase diagram types, we show that the size of the processability window becomes\nsensitive to interaction parameter variations close to critical values. The\nsensitivity decreases for materials with increasing molar size. Finally,\nsuccessful comparisons of simulated and experimental phase diagrams showcase\nthe real-world relevance of this theoretical analysis. The presented results\nlay a robust foundation for rational design of solution processing conditions\nand for blend morphology control. Immediate applications include organic thin\nfilms and the identification of green solvents for sustainable processing.",
        "Raman spectroscopy is an important tool for studies of molecules, liquids and\nsolids. While Raman spectra can be obtained theoretically from molecular\ndynamics (MD) simulations, this requires the calculation of the electronic\npolarizability along the simulation trajectory. First-principles calculations\nof electronic polarizability are computationally expensive, motivating the\ndevelopment of atomistic models for the evaluation of the changes in the\nelectronic polarizability with the changes in the atomic coordinates of the\nsystem. The bond polarizability model (BPM) is one of the oldest and simplest\nsuch atomistic models, but cannot reproduce the effects of angular vibrations,\nleading to inaccurate modeling of Raman spectra. Here, we demonstrate that the\ngeneralization of BPM through inclusion of terms for atom pairs that are\ntraditionally considered to be not involved in bonding dramatically improves\nthe accuracy of polarizability modeling and Raman spectra calculations. The\ngeneralized BPM (GBPM) reproduces the ab initio polarizability and Raman\nspectra for a range of tested molecules (SO2, H2S, H2O, NH3, CH4, CH3OH and\nCH3CH2OH) with high accuracy and also shows significantly improved agreement\nwith ab initio results for the more complex ferroelectric BaTiO3 systems. For\nliquid water, the anisotropic Raman spectrum derived from atomistic MD\nsimulations using GBPM evaluation of polarizability shows significantly\nimproved agreement with the experimental spectrum compared to the spectrum\nderived using BPM. Thus, GBPM can be used for the modeling of Raman spectra\nusing large-scale molecular dynamics and provides a good basis for the further\ndevelopment of atomistic polarizability models.",
        "Van der Waals ferromagnets, such as Fe5GeTe2, offer a promising platform for\nspintronic devices based on chiral magnetic textures, provided a significant\nDzyaloshinskii-Moriya interaction (DMI) can be induced to stabilise them. Here,\nwe directly measure DMI in epitaxial Fe5GeTe2 thin films using Brillouin light\nscattering spectroscopy and observe a consistent DMI (D = 0.04 mJ\/m2) across\nvarious thicknesses. Its weak thickness dependence, combined with the nominally\nsymmetric film interfaces, suggests a bulk origin. Although we do not determine\nthe microscopic mechanism, our findings are compatible with ab-initio\ncalculations linking DMI to partial ordering of Fe split sites. Additionally,\nwe find a low magnetic dissipation (alpha < 0.003). The observed DMI, which\ncould be further enhanced by optimising the Fe site ordering, combined with low\ndissipation, makes Fe5GeTe2 a strong candidate for exploring the dynamics of\nchiral magnetic textures in two-dimensional materials.",
        "Quiver Grassmannians of equioriented type $\\texttt{A}$ and nilpotent\nequioriented type $\\tilde{\\texttt{A}}$ quiver representations are\nGKM-varieties. In particular, they have a cellular decomposition and admit a\ntorus action with finitely many fixed points and one-dimensional orbits (i.e.\nskeletal action). We examine the case of string representations and provide a\nclassification of all corresponding quiver Grassmannians with a GKM-variety\nstructure.",
        "In many machine learning tasks, known symmetries can be used as an inductive\nbias to improve model performance. In this paper, we consider learning group\nequivariance through training with data augmentation. We summarize results from\na previous paper of our own, and extend the results to show that equivariance\nof the trained model can be achieved through training on augmented data in\ntandem with regularization.",
        "Providing high-quality, single-crystal diamond (SCD) with a large area is\ndesirable for up-scaling quantum technology applications that rely on color\ncenters in diamond. Growth methods aiming to increase the area of SCD are an\nactive research area. Native color centers offer a sensitive probe for local\ncrystal quality in such novel materials e.g., via their reaction to stress. In\nthis work, we investigate individual native nitrogen-vacancy (NV) centers in\nSCD layers manufactured via laterally overgrowing hole arrays in a\nheteroepitaxially grown large-scale substrate. Heteroepitaxy has become a\ncommon tool for growing large SCDs; however, achieving the high crystal quality\nneeded for quantum applications remains a challenge. In the overgrown layer, we\nidentify NV centers with spin-decoherence times in the order of hundreds of\nmicroseconds, comparable to high-purity homoepitaxial SCD. We quantify the\neffective crystal strain in different regions of the overgrown layer,\nindicating a low stress overall and a stress reduction in the diamond layer\nabove the holes.",
        "We show that for any fixed $(2+1)$-dimensional TQFT over $\\mathbb{C}$ of\neither Turaev-Viro-Barrett-Westbury or Reshetikhin-Turaev type, the problem of\n(exactly) computing its invariants on closed 3-manifolds is either solvable in\npolynomial time, or else it is $\\#\\mathsf{P}$-hard to (exactly) contract\ncertain tensors that are built from the TQFT's fusion category. Our proof is an\napplication of a dichotomy result of Cai and Chen [J. ACM, 2017] concerning\nweighted constraint satisfaction problems over $\\mathbb{C}$. We leave for\nfuture work the issue of reinterpreting the conditions of Cai and Chen that\ndistinguish between the two cases (i.e. $\\#\\mathsf{P}$-hard tensor contractions\nvs. polynomial time invariants) in terms of fusion categories. We expect that\nwith more effort, our reduction can be improved so that one gets a dichotomy\ndirectly for TQFTs' invariants of 3-manifolds rather than more general tensors\nbuilt from the TQFT's fusion category.",
        "The dynamical behavior of quantum state properties under intrinsic\ndecoherence models can be modified by the presence of external magnetic fields.\nAlthough generically external magnetic fields are detrimental to preserve\nquantumness in the presence of intrinsic decoherence, judicious adjustment of\nthe magnetic field can stabilize such features. This stabilization arises from\nnovel resonances between energy eigenstates resulting from the presence of an\nexternal magnetic field. Here, we present our findings using as a model system\ntwo spin 1-particles confined in a double-well potential under intrinsic\ndecoherence. We stress, however, that our results are generic and independent\non the used model.",
        "Is there new physics hidden in the four-point function of the cosmic\nmicrowave background (CMB)? We conduct a detailed analysis of the Planck PR4\ntemperature and polarization trispectrum for $\\ell\\in[2,2048]$. Using the\ntheoretical and computational tools developed in Paper 1 and Paper 2, we search\nfor 33 template amplitudes, encoding a variety of effects from inflationary\nself-interactions to particle exchange. We find no evidence for primordial\nnon-Gaussianity and set stringent constraints on both phenomenological\namplitudes and couplings in the inflationary Lagrangian. Due to the use of\noptimal estimators and polarization data, our constraints are highly\ncompetitive. For example, we find $\\sigma(g_{\\rm NL}^{\\rm loc})=4.8\\times 10^4$\nand $\\tau_{\\rm NL}^{\\rm loc} <1500$ (95\\% CL), a factor of two improvement on\nEffective Field Theory amplitudes, and a $43\\sigma$ detection of gravitational\nlensing. Many templates are analyzed for the first time, such as\ndirection-dependent trispectra and the collapsed limit of the `cosmological\ncollider', across a range of masses and spins. We perform a variety of\nvalidation tests; whilst our results are stable, the most relevant systematics\nare found to be lensing bias, residual foregrounds, and mismatch between\nsimulations and data. The techniques discussed in this series can be extended\nto future datasets, allowing the primordial Universe to be probed at even\nhigher sensitivity.",
        "Optical parametric amplification (OPA) comprises essentially a nonlinear\nfour-wave mixing process in which a \"pump\" and a \"signal\" field give rise to an\n\"idler\" field under certain phase-matching conditions. Here we use a photonic\ncrystal waveguide strongly-coupled with an excitonic reservoir to generate this\nprocess between different guided modes at optical wavelengths. Differently from\nclassical nonlinear optical crystals, where the pump and idler photons travel\nalmost collinearly, our exciton-polaritons are naturally separated in the\nwaveguide due to their opposite group velocities. Due to the high efficiency of\nthe process we can generate the idler field of the parametric process by\npumping with a continuous wave laser and choose its direction of propagation in\nthe waveguide by adjusting the angle of incidence of the seed laser. We show\nthe OPA process to be robust against surface defects of the waveguide and can\nlead to simple-to-fabricate devices compared to microcavities that take\nadvantage of strong signal-idler correlations in a propagating geometry. Our\nresults closely agree with mean-field numerical simulations.",
        "We demonstrate optical cycling and transverse laser cooling of a beam of\nfermionic 137BaF molecules. Their high masses and nuclear spins make these\nmolecules sensitive probes for parity violation and properties of the weak\ninteraction. However, the nuclear spins also lead to a quasi-closed cycling\ntransition currently involving up to 112 levels, which significantly exceeds\nthe complexity in other laser-cooled molecules. Optical cycling and cooling are\nfacilitated through carefully designed optical spectra tailored to this\nmolecular structure. Our results pave the way for efficient state preparation,\ndetection, and cooling in precision measurements using this species and other\nsimilar species.",
        "Axion-like particles can couple to Standard Model gluons, electroweak gauge\nbosons, and massive fermions. A future multi-TeV muon collider provides a\nfavorable environment to probe axion-like particles through multiple production\nchannels, including vector boson fusion via electroweak gauge boson couplings\nand the top-associated production mediated by direct fermionic couplings.\nMotivated by the quality issue of the QCD axion, we focus on axion-like\nparticles with masses and decay constants around the TeV scale. We explore how\ndifferent axion-like particle couplings shape its production and decay modes,\nrevealing a rich and intricate phenomenological landscape.",
        "The project Agriculture 4.0 without chemical synthetical plant protection\n(NOcsPS) tests a number of cropping systems that avoid the use of chemical\nsynthetical pesticides while at the same time using mineral fertilizers. The\nexperiment started in 2020 (sowing fall 2019). In 2024 (sowing fall 2023), some\nof the cropping systems were modified. Analysis of this experiment may be done\nusing linear mixed models. In order to include the data from 2020-2023 in joint\nanalyses with the data collected for the modified systems from 2024 onwards,\nthe mixed modelling approach needs to be reconsidered. In this paper, we\ndevelop models for this purpose. A key feature is the use of network\nmeta-analytic concepts that allow a combination of direct and indirect\ncomparisons among systems from the different years. The approach is first\nillustrated using a toy example. This is followed by detailed analyses of data\nfrom two the two trials sites Dahnsdorf and Hohenheim.",
        "The core focus of this research work is to obtain invariant solutions and\nconservation laws of the (3+1) dimensional ZK equation which describes the\nphenomenon of wave stability and solitons propagation. ZK equation is\nsignificant in fluid dynamics, nonlinear optics, and acoustics. ZK equation\nplays an important role in understanding the behavior of nonlinear, dispersive\nwaves especially in the presence of the magnetic field. Lie symmetry analysis\nhas been applied to the (3+1)-dimensional ZK equation to derive invariant\nsolutions. These solutions disclose how waves retain their shape as they\ntravel, how they interact in space, and the impact of magnetic fields on wave\npropagation. Using the Lie symmetry, (3+1)-dimensional ZK equation reduces into\nvarious ordinary differential equations. It also concludes with the\nconservation laws and non-linear self adjoint-ness property for the\n(3+1)-dimensional ZK Equation.",
        "Neurological disorders represent significant global health challenges,\ndriving the advancement of brain signal analysis methods. Scalp\nelectroencephalography (EEG) and intracranial electroencephalography (iEEG) are\nwidely used to diagnose and monitor neurological conditions. However, dataset\nheterogeneity and task variations pose challenges in developing robust deep\nlearning solutions. This review systematically examines recent advances in deep\nlearning approaches for EEG\/iEEG-based neurological diagnostics, focusing on\napplications across 7 neurological conditions using 46 datasets. We explore\ntrends in data utilization, model design, and task-specific adaptations,\nhighlighting the importance of pre-trained multi-task models for scalable,\ngeneralizable solutions. To advance research, we propose a standardized\nbenchmark for evaluating models across diverse datasets to enhance\nreproducibility. This survey emphasizes how recent innovations can transform\nneurological diagnostics and enable the development of intelligent, adaptable\nhealthcare solutions.",
        "We employ the \"learning by confusion\" technique, an unsupervised machine\nlearning approach for detecting phase transitions, to analyze quantum Monte\nCarlo simulations of the two-dimensional Holstein model--a fundamental model\nfor electron-phonon interactions on a lattice. Utilizing a convolutional neural\nnetwork, we conduct a series of binary classification tasks to identify\nHolstein critical points based on the neural network's learning accuracy. We\nfurther evaluate the effectiveness of various training datasets, including\nsnapshots of phonon fields and other measurements resolved in imaginary time,\nfor predicting distinct phase transitions and crossovers. Our results culminate\nin the construction of the finite-temperature phase diagram of the Holstein\nmodel.",
        "This paper explores decaying turbulence beneath surface waves that is\ninitially isotropic and shear-free. We start by presenting phenomenology\nrevealed by wave-averaged numerical simulations: an accumulation of angular\nmomentum in coherent vortices, suppression of kinetic energy dissipation, and\nthe development of depth-alternating jets. We interpret these features through\nan analogy with rotating turbulence (Holm 1996), wherein the curl of the Stokes\ndrift, $\\nabla \\times \\mathbf{u}^S$, takes on the role of the background\nvorticity (for example, $(f_0 + \\beta y) \\mathbf{\\hat z}$ on the\n$\\beta$-plane). We pursue this thread further by showing that a two-equation\nmodel proposed by (Bardina et al. 1985) for rotating turbulence reproduces the\nsimulated evolution of volume-integrated kinetic energy. This success of the\ntwo-equation model -- which explicitly parameterizes wave-driven suppression of\nkinetic energy dissipation -- carries implications for modeling turbulent\nmixing in the ocean surface boundary layer. We conclude with a discussion about\na wave-averaged analogue of the Rossby number appearing in the two-equation\nmodel, which we term the ``pseudovorticity number'' after the pseudovorticity\n$\\nabla \\times \\mathbf{u}^S$. The pseudovorticity number is related to the\nLangmuir number in an integral sense.",
        "We investigate the thermodynamic uncertainty relations (TURs) in steady-state\ntransport for three-terminal systems within the linear response regime,\nspecifically in the presence of broken time-reversal symmetry. To quantify the\nTUR, we introduce a dimensionless trade-off parameter $Q_J$, and derive new\nbounds of $Q_J$ for both particle and heat currents under a strong constraint\non the Onsager coefficients. Furthermore, we determine a universal lower bound\n$Q_J^{bound}\\geq1.5$ for three-terminal systems in the linear response regime\nwhen the time-reversal symmetry is broken."
      ]
    }
  },
  {
    "id":2412.13126,
    "research_type":"applied",
    "start_id":"b2",
    "start_title":"Machine Learning-driven Histotype Diagnosis of Ovarian Carcinoma: Insights from the OCEAN AI Challenge",
    "start_abstract":"Ovarian cancer poses a significant health burden as one of the deadliest malignancies affecting women globally. Histotype assignment of epithelial ovarian cancers can be challenging due to morphologic overlap, inter-observer variability, and the lack of ancillary diagnostic techniques in some areas of the world. Moreover, rare cancers can pose particular diagnostic difficulties because of a relative lack of familiarity with them, underscoring the necessity for robust diagnostic methodologies. The emergence of Artificial Intelligence (AI) has brought promising prospects to the realm of ovarian cancer diagnosis. While various studies have underscored AI's promise, its validation across multiple healthcare centers and hospitals has been limited. Inspired by innovations in medical imaging driven by public competitions, we initiated the Ovarian Cancer subtypE clAssification and outlier detectioN (OCEAN) challenge, the most extensive histopathology competition to date.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b1"
      ],
      "title":[
        "Viable and necrotic tumor assessment from whole slide images of osteosarcoma using machine-learning and deep-learning models"
      ],
      "abstract":[
        "Pathological estimation of tumor necrosis after chemotherapy is essential for patients with osteosarcoma. This study reports the first fully automated tool to assess viable and necrotic in osteosarcoma, employing advances histopathology digitization learning. We selected 40 digitized whole slide images representing heterogeneity osteosarcoma response. With goal labeling diverse regions tissue into tumor, non-tumor, we trained 13 machine-learning models top performing one (a Support Vector Machine) based on reported accuracy. also developed a deep-learning architecture it same data set. computed receiver-operator characteristic discrimination non-tumor from followed by conditional found our exceptionally well. then used identify interest image-tiles generated test images. The classification output visualized as tumor-prediction map, displaying extent image. Thus, lay foundation complete assessment pipeline original histology map generation. proposed can be adopted other types tumor."
      ],
      "categories":[
        "q-bio.CB"
      ]
    },
    "list":{
      "title":[
        "The time-dependent reproduction number for epidemics in heterogeneous\n  populations",
        "Trait-structured chemotaxis: Exploring ligand-receptor dynamics and\n  travelling wave properties in a Keller-Segel model",
        "Multicellular self-organization in Escherichia coli",
        "Flexible inference of evolutionary accumulation dynamics using uncertain\n  observational data",
        "Intercellular contact is sufficient to drive Fibroblast to Myofibroblast\n  transitions",
        "Toxicological Evaluation of Phytochemicals and Heavy Metals in Ficus\n  exasperata Vahl (Sandpaper) Leaves obtained in Birnin Kebbi, Nigeria",
        "Tumor-associated CD19$^+$ macrophages induce immunosuppressive\n  microenvironment in hepatocellular carcinoma",
        "Temporal Dynamics of Microbial Communities in Anaerobic Digestion:\n  Influence of Temperature and Feedstock Composition on Reactor Performance and\n  Stability",
        "A spatially resolved and lipid-structured model for macrophage\n  populations in early human atherosclerotic lesions",
        "Fungal Genetic Variants in Oceanic Environments",
        "Deep Learning-based Feature Discovery for Decoding Phenotypic Plasticity\n  in Pediatric High-Grade Gliomas Single-Cell Transcriptomics",
        "KMT2B-related disorders: expansion of the phenotypic spectrum and\n  long-term efficacy of deep brain stimulation",
        "COLOR: A compositional linear operation-based representation of protein\n  sequences for identification of monomer contributions to properties",
        "Top eigenvalue statistics of diluted Wishart matrices",
        "Quantum oscillations in a dipolar excitonic insulator",
        "Comparison theorems for the minimum eigenvalue of a random\n  positive-semidefinite matrix",
        "Emergent supercounterfluid and quantum phase diagram of two-component\n  interacting bosons in one-dimensional optical lattice",
        "Multivariate spatial models for small area estimation of\n  species-specific forest inventory parameters",
        "Bayesian optimization of electron energy from laser wakefield\n  accelerator",
        "Quantum model reduction for continuous-time quantum filters",
        "Meson Mixing Bounds on $Z^{\\prime}$ Mass in the Alignment Limit:\n  Establishing the Phenomenological Viability of the 331 Model",
        "Right-censored models on massive data",
        "A spectral boundary element method for acoustic interference problems",
        "Variational quantum thermalizers based on weakly-symmetric nonunitary\n  multi-qubit operations",
        "AI-assisted hyper-dimensional broadband quantum memory with efficiency\n  above 90% in warm atoms",
        "The putative center in NGC 1052",
        "Hopfological invariants for tame subextensions",
        "Detecting entanglement in any measurement using quantum networks"
      ],
      "abstract":[
        "The time-dependent reproduction number Rt can be used to track pathogen\ntransmission and to assess the efficacy of interventions. This quantity can be\nestimated by fitting renewal equation models to time series of infectious\ndisease case counts. These models almost invariably assume a homogeneous\npopulation. Individuals are assumed not to differ systematically in the rates\nat which they come into contact with others. It is also assumed that the\ntypical time that elapses between one case and those it causes (known as the\ngeneration time distribution) does not differ across groups. But contact\npatterns are known to widely differ by age and according to other demographic\ngroupings, and infection risk and transmission rates have been shown to vary\nacross groups for a range of directly transmitted diseases. Here, we derive\nfrom first principles a renewal equation framework which accounts for these\ndifferences in transmission across groups. We use a generalisation of the\nclassic McKendrick-von Foerster equation to handle populations structured into\ninteracting groups. This system of partial differential equations allows us to\nderive a simple analytical expression for Rt which involves only group-level\ncontact patterns and infection risks. We show that the same expression emerges\nfrom both deterministic and stochastic discrete-time versions of the model and\ndemonstrate via simulations that our Rt expression governs the long-run fate of\nepidemics. Our renewal equation model provides a basis from which to account\nfor more realistic, diverse populations in epidemiological models and opens the\ndoor to inferential approaches which use known group characteristics to\nestimate Rt.",
        "A novel trait-structured Keller-Segel model that explores the dynamics of a\nmigrating cell population guided by chemotaxis in response to an external\nligand concentration is derived and analysed. Unlike traditional Keller-Segel\nmodels, this framework introduces an explicit representation of ligand-receptor\nbindings on the cell membrane, where the percentage of occupied receptors\nconstitutes the trait that influences cellular phenotype. The model posits that\nthe cell's phenotypic state directly modulates its capacity for chemotaxis and\nproliferation, governed by a trade-off due to a finite energy budget: cells\nhighly proficient in chemotaxis exhibit lower proliferation rates, while more\nproliferative cells show diminished chemotactic abilities. The model is derived\nfrom the principles of a biased random walk, resulting in a system of two\nnon-local partial differential equations, describing the densities of both\ncells and ligands. Using a Hopf-Cole transformation, we derive an equation that\ncharacterises the distribution of cellular traits within travelling wave\nsolutions for the total cell density, allowing us to uncover the monotonicity\nproperties of these waves. Numerical investigations are conducted to examine\nthe model's behaviour across various biological scenarios, providing insights\ninto the complex interplay between chemotaxis, proliferation, and phenotypic\ndiversity in migrating cell populations.",
        "Escherichia coli has long been a trusty companion, maintaining health in our\nguts and advancing biological knowledge in the laboratory. In light of recent\nfindings, we discuss multicellular self-organization in E. coli and develop\ngeneral ideas for multicellularity, including the necessity for multicellular\ndynamics and interpretation by dynamic graphs, applicable to both unicellular\nand multicellular organisms. In this context, we next discuss the documented\nbehaviors of E. coli self-organization (rosette formation, multicellular\nextension, and attached dormancy) and two potential behaviors (internal\ncommunication and mating). Finally, by comparing the dynamic graphs for\ndifferent communities, we develop principles relevant to the theory of\nmulticellularity.",
        "Understanding and predicting evolutionary accumulation pathways is a key\nobjective in many fields of research, ranging from classical evolutionary\nbiology to diverse applications in medicine. In this context, we are often\nconfronted with the problem that data is sparse and uncertain. To use the\navailable data as best as possible, inference approaches that can handle this\nuncertainty are required. One way that allows us to use not only\ncross-sectional data, but also phylogenetic related and longitudinal data is\nusing 'hypercubic inference' models. In this article we introduce HyperLAU, a\nnew algorithm for hypercubic inference that makes it possible to use datasets\nincluding uncertainties for learning evolutionary pathways. Expanding the\nflexibility of accumulation modelling, HyperLAU allows us to infer dynamic\npathways and interactions between features, even when large sets of particular\nfeatures are unobserved across the source dataset. We show that HyperLAU is\nable to highlight the main pathways found by other tools, even when up to 50%\nof the features in the input data are uncertain.",
        "Fibroblast cells play a key role in maintaining the extracellular matrix.\nDuring wound healing, fibroblasts differentiate into highly contractile\nmyofibroblasts, which secrete extracellular matrix proteins like collagen to\nfacilitate tissue repair. Under normal conditions, myofibroblasts undergo\nprogrammed cell death after healing to prevent excessive scar formation.\nHowever, in diseases like fibrosis, the myofibroblasts remain active even after\nthe wound is closed, resulting in excessive collagen buildup and a stiff,\nfibrotic matrix. The reasons for the persistence of myofibroblasts in fibrosis\nare not well understood. Here we show the existence of a mechanism where direct\nphysical contact between a fibroblast and a myofibroblast is sufficient for\nfibroblasts to transition into myofibroblasts. We show that\nfibroblast-myofibroblast transition can occur even in the absence of known\nbiochemical cues such as growth factor activation or mechanical cues from a\nstiff, fibrotic matrix. Further, we show that contact-based\nfibroblast-myofibroblast activation can be blocked by G{\\alpha}q\/11\/14\ninhibitor FR9003591, which inhibits the formation of myofibroblasts. These\nfindings offer new insights into the persistence of fibrosis despite\ntherapeutic interventions and suggest a potential strategy to target\nfibroblast-to-myofibroblast transition in fibrosis.",
        "Objective: Ficus exasperata Vahl (Sandpaper tree) is extensively used in\nNigeria to treat diseases, but a dearth of documentation about its toxicity\nexists. This information is crucial because pollutants can contaminate\nmedicinal plants. This study determined the heavy metal and phytochemical\ncontent of methanolic leaf extract of F exasperata obtained in Birnin Kebbi,\nNigeria. Material and Methods: The lethality of the plant was also assessed\nusing 70 wild shrimps divided equally into seven groups. Group 1 (negative\ncontrol), groups 2 and 3 (positive controls) were exposed to 500 and 1000ppm of\nformaldehyde, respectively; and groups 4-7 were exposed to 1000, 2000, 4000,\nand 8000ppm of extracts, respectively, for 96 hours. Results: The\nphytochemistry revealed high levels of flavonoids and saponins and moderate\nlevels of tannins and phenols. The heavy metal analysis revealed non-tolerable\nlevels of cadmium, copper, and lead, while zinc was within the tolerable limit.\nThe negative control recorded 10% mortality, 1000 and 2000 ppm (20% each),\n4000ppm (70%), and 8000 ppm (100%). Conclusion: These results inferred safe\ndoses of the plant's extract in low and medium concentrations but toxic and\nfatal at high doses over a period of time. Consumers are advised to seek an\nexpert's guidance before using it.",
        "Tumor-associated macrophages are a key component that contributes to the\nimmunosuppressive microenvironment in human cancers. However, therapeutic\ntargeting of macrophages has been a challenge in clinic due to the limited\nunderstanding of their heterogeneous subpopulations and distinct functions.\nHere, we identify a unique and clinically relevant CD19$^+$ subpopulation of\nmacrophages that is enriched in many types of cancer, particularly in\nhepatocellular carcinoma (HCC). The CD19$^+$ macrophages exhibit increased\nlevels of PD-L1 and CD73, enhanced mitochondrial oxidation, and compromised\nphagocytosis, indicating their immunosuppressive functions. Targeting CD19$^+$\nmacrophages with anti-CD19 chimeric antigen receptor T (CAR-T) cells inhibited\nHCC tumor growth. We identify PAX5 as a primary driver of up-regulated\nmitochondrial biogenesis in CD19$^+$ macrophages, which depletes cytoplasmic\nCa$^{2+}$, leading to lysosomal deficiency and consequent accumulation of CD73\nand PD-L1. Inhibiting CD73 or mitochondrial oxidation enhanced the efficacy of\nimmune checkpoint blockade therapy in treating HCC, suggesting great promise\nfor CD19$^+$ macrophage-targeting therapeutics.",
        "Anaerobic digestion (AD) offers a sustainable biotechnology to recover\nresources from carbon-rich wastewater, such as food-processing wastewater.\nDespite crude wastewater characterisation, the impact of detailed chemical\nfingerprinting on AD remains underexplored. This study investigated the\ninfluence of fermentation-wastewater composition and operational parameters on\nAD over time to identify critical factors influencing reactor biodiversity and\nperformance. Eighteen reactors were operated under various operational\nconditions using mycoprotein fermentation wastewater. Detailed chemical\nanalysis fingerprinted the molecules in the fermentation wastewater throughout\nAD including sugars, sugar alcohols and volatile fatty acids (VFAs). Sequencing\nrevealed distinct microbiome profiles linked to temperature and reactor\nconfiguration, with mesophilic conditions supporting a more diverse and densely\nconnected microbiome. Significant elevations in Methanomassiliicoccus were\ncorrelated to high butyric acid concentrations and decreased biogas production,\nfurther elucidating the role of this newly discovered methanogen. Dissimilarity\nanalysis demonstrated the importance of individual molecules on microbiome\ndiversity, highlighting the need for detailed chemical fingerprinting in AD\nstudies of microbial trends. Machine learning (ML) models predicting reactor\nperformance achieved high accuracy based on operational parameters and\nmicrobial taxonomy. Operational parameters had the most substantial influence\non chemical oxygen demand removal, whilst Oscillibacter and two Clostridium sp.\nwere highlighted as key factors in biogas production. By integrating detailed\nchemical and biological fingerprinting with ML models this research presents a\nnovel approach to advance our understanding of AD microbial ecology, offering\ninsights for industrial applications of sustainable waste-to-energy systems.",
        "Atherosclerosis is a chronic inflammatory disease of the artery wall. The\nearly stages of atherosclerosis are driven by interactions between lipids and\nmonocyte-derived-macrophages (MDMs). The mechanisms that govern the spatial\ndistribution of lipids and MDMs in the lesion remain poorly understood. In this\npaper, we develop a spatially-resolved and lipid-structured model for early\natherosclerosis. The model development and analysis are guided by images of\nhuman coronary lesions by Nakashima et al. 2007. Consistent with their\nfindings, the model predicts that lipid initially accumulates deep in the\nintima due to a spatially non-uniform LDL retention capacity. The model also\nqualitatively reproduces the global internal maxima in the Nakashima images\nonly when the MDM mobility is sufficiently sensitive to lipid content, and MDM\nlifespan sufficiently insensitive. Introducing lipid content-dependence to MDM\nmobility and mean lifespan produced minimal impact on model behaviour at early\ntimes, but strongly impacted lesion composition at steady state. Increases to\nthe sensitivity of MDM lifespan to lipid content yield lesions with fewer MDMs,\nless total lesion lipid content and reduced mean MDM infiltration depth.\nIncreases to the sensitivity of MDM mobility to lipid content also reduces the\nMDM infiltration depth, but increases the proportion of lipid-laden MDMs. We\nfind that MDM lipid content increases with spatial depth, regardless of blood\nLDL and HDL content. These results shed light on the mechanisms that drive\nspatial variation in the composition of early atherosclerotic lesions, and the\nrole of macrophage lipid content in disease progression.",
        "Comparing specific types of organisms as they are found across environmental\nconditions has helped inform how genes and gene products of these organisms\nrelate to phenotypes and adaptation. In this study, we examine\nmetatranscriptomic data as found for oceanic fungi across different oceanic\nsampling sites. A specific set of three genes was chosen for evaluation based\non conserved orthology, known association with core physiological processes in\nfungi, and level of abundance within oceanic metatranscriptomic data. We report\nupon a potential association of genetic variance with environmental conditions\nof iron, salt and phosphate in oceanic waters based on heatmap visualization\nand PERMANOVA analysis.",
        "By use of complex network dynamics and graph-based machine learning, we\nidentified critical determinants of lineage-specific plasticity across the\nsingle-cell transcriptomics of pediatric high-grade glioma (pHGGs) subtypes:\nIDHWT glioblastoma and K27M-mutant glioma. Our study identified network\ninteractions regulating glioma morphogenesis via the tumor-immune\nmicroenvironment, including neurodevelopmental programs, calcium dynamics, iron\nmetabolism, metabolic reprogramming, and feedback loops between MAPK\/ERK and\nWNT signaling. These relationships highlight the emergence of a hybrid spectrum\nof cellular states navigating a disrupted neuro-differentiation hierarchy. We\nidentified transition genes such as DKK3, NOTCH2, GATAD1, GFAP, and SEZ6L in\nIDHWT glioblastoma, and H3F3A, ANXA6, HES6\/7, SIRT2, FXYD6, PTPRZ1, MEIS1,\nCXXC5, and NDUFAB1 in K27M subtypes. We also identified MTRNR2L1, GAPDH, IGF2,\nFKBP variants, and FXYD7 as transition genes that influence cell fate\ndecision-making across both subsystems. Our findings suggest pHGGs are\ndevelopmentally trapped in states exhibiting maladaptive behaviors, and hybrid\ncellular identities. In effect, tumor heterogeneity (metastability) and\nplasticity emerge as stress-response patterns to immune-inflammatory\nmicroenvironments and oxidative stress. Furthermore, we show that pHGGs are\nsteered by developmental trajectories from radial glia predominantly favoring\nneocortical cell fates, in telencephalon and prefrontal cortex (PFC)\ndifferentiation. By addressing underlying patterning processes and plasticity\nnetworks as therapeutic vulnerabilities, our findings provide precision\nmedicine strategies aimed at modulating glioma cell fates and overcoming\ntherapeutic resistance. We suggest transition therapy toward neuronal-like\nlineage differentiation as a potential therapy to help stabilize pHGG\nplasticity and aggressivity.",
        "Heterozygous mutations in KMT2B are associated with an early-onset,\nprogressive, and often complex dystonia (DYT28). Key characteristics of typical\ndisease include focal motor features at disease presentation, evolving through\na caudocranial pattern into generalized dystonia, with prominent oromandibular,\nlaryngeal, and cervical involvement. Although KMT2B-related disease is emerging\nas one of the most common causes of early-onset genetic dystonia, much remains\nto be understood about the full spectrum of the disease. We describe a cohort\nof 53 patients with KMT2B mutations, with detailed delineation of their\nclinical phenotype and molecular genetic features. We report new disease\npresentations, including atypical patterns of dystonia evolution and a subgroup\nof patients with a non-dystonic neurodevelopmental phenotype. In addition to\nthe previously reported systemic features, our study has identified\nco-morbidities, including the risk of status dystonicus, intrauterine growth\nretardation, and endocrinopathies. Analysis of this study cohort (n = 53) in\ntandem with published cases (n = 80) revealed that patients with chromosomal\ndeletions and protein-truncating variants had a significantly higher burden of\nsystemic disease (with earlier onset of dystonia) than those with missense\nvariants. Eighteen individuals had detailed longitudinal data available after\ninsertion of deep brain stimulation for medically refractory dystonia. Median\nage at deep brain stimulation was 11.5 years (range: 4.5 to 37.0 years).\nFollow-up after deep brain stimulation ranged from 0.25 to 22 years.\nSignificant improvement of motor function and disability (as assessed by the\nBurke-Fahn-Marsden Dystonia Rating Scales, BFMDRS-M and BFMDRS-D) was evident\nat 6 months, 1 year, and last follow-up (motor, P = 0.001, P = 0.004, and P =\n0.012; disability, P = 0.009, P = 0.002, and P = 0.012).",
        "The properties of biological materials like proteins and nucleic acids are\nlargely determined by their primary sequence. While certain segments in the\nsequence strongly influence specific functions, identifying these segments, or\nso-called motifs, is challenging due to the complexity of sequential data.\nWhile deep learning (DL) models can accurately capture sequence-property\nrelationships, the degree of nonlinearity in these models limits the assessment\nof monomer contributions to a property - a critical step in identifying key\nmotifs. Recent advances in explainable AI (XAI) offer attention and\ngradient-based methods for estimating monomeric contributions. However, these\nmethods are primarily applied to classification tasks, such as binding site\nidentification, where they achieve limited accuracy (40-45%) and rely on\nqualitative evaluations. To address these limitations, we introduce a DL model\nwith interpretable steps, enabling direct tracing of monomeric contributions.\nWe also propose a metric ($\\mathcal{I}$), inspired by the masking technique in\nthe field of image analysis and natural language processing, for quantitative\nanalysis on datasets mainly containing distinct properties of anti-cancer\npeptides (ACP), antimicrobial peptides (AMP), and collagen. Our model exhibits\n22% higher explainability, pinpoints critical motifs (RRR, RRI, and RSS) that\nsignificantly destabilize ACPs, and identifies motifs in AMPs that are 50% more\neffective in converting non-AMPs to AMPs. These findings highlight the\npotential of our model in guiding mutation strategies for designing\nprotein-based biomaterials.",
        "Using the replica method, we compute analytically the average largest\neigenvalue of diluted covariance matrices of the form $\\mathbf{J} =\n\\mathbf{X}^T \\mathbf{X}$, where $\\mathbf{X}$ is a $N\\times M$ sparse data\nmatrix, in the limit of large $N,M$ with fixed ratio. We allow for random\nnon-zero weights, provided they lead to an isolated largest eigenvalue. By\nformulating the problem as the optimisation of a quadratic Hamiltonian\nconstrained to the $N$-sphere at low temperatures, we derive a set of recursive\ndistributional equations for auxiliary probability density functions, which can\nbe efficiently solved using a population dynamics algorithm. The average\nlargest eigenvalue is identified with a Lagrange parameter that governs the\nconvergence of the algorithm. We find excellent agreement between our\nanalytical results and numerical results obtained from direct diagonalisation.",
        "Quantum oscillations in magnetization or resistivity are a defining feature\nof metals subject to an external magnetic field. The phenomenon is generally\nnot expected in insulators without a Fermi surface. The observations of quantum\noscillations in Kondo insulating materials have provided a rare counterexample\nand attracted much theoretical interest. However, the magnetic oscillations in\ncorrelated insulators remain poorly understood. Here we report the observations\nof resistivity quantum oscillations in an excitonic insulator realized in\nCoulomb-coupled electron-hole double layers with gate-tunability that allows\nthe phenomenon to be explored in a more controllable fashion than in bulk\nmaterials. When the cyclotron energy of the electrons or holes is tuned to be\ncomparable to or larger than the exciton binding energy, recurring transitions\nbetween excitonic insulators and electron-hole decoupled quantum Hall states\nare observed. Compressibility measurements show an oscillatory exciton binding\nenergy as a function of magnetic field and electron-hole pair density. Coulomb\ndrag measurements further reveal the formation of excitons with finite angular\nmomentum. Our results are qualitatively captured by mean-field theory\ncalculations. The study demonstrates a new platform for studying quantum\noscillations in correlated insulators.",
        "This paper establishes a new comparison principle for the minimum eigenvalue\nof a sum of independent random positive-semidefinite matrices. The principle\nstates that the minimum eigenvalue of the matrix sum is controlled by the\nminimum eigenvalue of a Gaussian random matrix that inherits its statistics\nfrom the summands. This methodology is powerful because of the vast arsenal of\ntools for treating Gaussian random matrices. As applications, the paper\npresents short, conceptual proofs of some old and new results in\nhigh-dimensional statistics. It also settles a long-standing open question in\ncomputational linear algebra about the injectivity properties of very sparse\nrandom matrices.",
        "Motivated by a recent experiment that realizes nearest-neighbor dipolar\ncouplings in an optical lattice [C. Lagoin, $\\textit{et al.}$, Nature\n$\\textbf{609}$, 485 (2022)], we study a one-dimensional version of the\ntwo-component extended Bose-Hubbard model via the density-matrix\nrenormalization group method. By using the nearest-neighbor and on-site\ninteraction parameters from the experiment, we start by mapping the quantum\nphase diagram in the hopping parameters $t_{A}\\mbox{-}t_{B}$ plane with boson\ndensities $\\rho_{A}=\\rho_{B}=1\/2$. In addition to the density wave phase\nreported in the experiment, we find several regimes of superfluidity when one\nor two hopping parameters are large enough, and interestingly there is a\nsupercounterfluid phase at moderate and comparable hopping parameters. The\nuniversality classes of these phase transitions are analyzed from the\ncorrelation functions, excitation gaps, and entanglement entropy. In\nparticular, a Berezinskii-Kosterlitz-Thouless type is recognized several\ngapped-to-gapless transitions. In addition, we also study the quantum phase\ntransitions when varying $\\rho_{B}$ from 0 to 1 while keeping $\\rho_A = 1\/2$.\nWe identify a supersolid phase in a wide range of $1\/2<\\rho_B<1$. Our work\npaves the way for realizing exotic many-body phases in cold atom experiments\nupon proper tuning of experimental parameters.",
        "National Forest Inventories (NFIs) provide statistically reliable information\non forest resources at national and other large spatial scales. As forest\nmanagement and conservation needs become increasingly complex, NFIs are being\ncalled upon to provide forest parameter estimates at spatial scales smaller\nthan current design-based estimation procedures can provide. This is\nparticularly true when estimates are desired by species or species groups. Here\nwe propose a multivariate spatial model for small area estimation of\nspecies-specific forest inventory parameters. The hierarchical Bayesian\nmodeling framework accounts for key complexities in species-specific forest\ninventory data, such as zero-inflation, correlations among species, and\nresidual spatial autocorrelation. Importantly, by fitting the model directly to\nthe individual plot-level data, the framework enables estimates of\nspecies-level forest parameters, with associated uncertainty, across any\nuser-defined small area of interest. A simulation study revealed minimal bias\nand higher accuracy of the proposed model-based approach compared to the\ndesign-based estimator and a non-parametric k-nearest neighbor (kNN) estimator.\nWe applied the model to estimate species-specific county-level aboveground\nbiomass for the 20 most abundant tree species in the southern United States\nusing Forest Inventory and Analysis (FIA) data. Biomass estimates from the\nproposed model had high correlations with design-based estimates and kNN\nestimates. Importantly, the proposed model provided large gains in precision\nacross all 20 species. On average across species, 91.5% of county-level biomass\nestimates had higher precision compared to the design-based estimates. The\nproposed framework improves the ability of NFI data users to generate\nspecies-level forest parameter estimates with reasonable precision at\nmanagement-relevant spatial scales.",
        "We employ Bayesian optimization combined with three-dimensional\nparticle-in-cell simulations to identify the optimal laser and plasma\nparameters that, for a given laser pulse energy, maximize the cut-off energy of\nan electron beam accelerated via laser wakefield acceleration. A Gaussian laser\ndriver with a matched spot size and amplitude is assumed, interacting with both\na uniform-density plasma and a preformed plasma channel of matched radius. To\ninterpret the simulation results quantitatively, we derive novel analytical\nexpressions for predicting the maximum electron energy and acceleration length,\ntaking into account the diffraction and energy depletion of the laser pulse.\nAdditionally, we discuss the potential scalability of the optimal parameters\nfor high-energy lasers.",
        "The use of quantum stochastic models is widespread in dynamical reduction,\nsimulation of open systems, feedback control and adaptive estimation. In many\napplications only part of the information contained in the filter's state is\nactually needed to reconstruct the target observable quantities; thus, filters\nof smaller dimensions could be in principle implemented to perform the same\ntask.In this work, we propose a systematic method to find, when possible,\nreduced-order quantum filters that are capable of exactly reproducing the\nevolution of expectation values of interest. In contrast with existing\nreduction techniques, the reduced model we obtain is exact and in the form of a\nBelavkin filtering equation, ensuring physical interpretability.This is\nattained by leveraging tools from the theory of both minimal realization and\nnon-commutative conditional expectations. The proposed procedure is tested on\nprototypical examples, laying the groundwork for applications in quantum\ntrajectory simulation and quantum feedback control.",
        "We perform a systematic study of flavor-changing neutral currents (FCNCs) in\nthe 331 model with right-handed neutrinos (331RHNs), analyzing constraints on\nthe $Z^\\prime$ boson mass from $K$-, $D$-, $B_d$-, and $B_s$-meson\noscillations. By explicitly incorporating scalar sector dynamics and quark\nrotation ambiguities ($V_L^{u,d}$), we demonstrate that $Z^\\prime$ mass limits\ndepend critically on the parametrization of Cabibbo-Kobayashi-Maskawa (CKM)\nmatrix factors. Three scenarios are explored: (i) $V_L^u =\nV_\\text{CKM}^\\dagger$ (FCNCs restricted to $D$-mesons), (ii) $V_L^d =\nV_\\text{CKM}$ (dominant $B_s$ constraints), and (iii) a hybrid mixing pattern.\nStrikingly, scenario (i) reduces the $Z^\\prime$ mass bound to $M_{Z^\\prime}\n\\gtrsim 600\\;\\text{GeV}$-two orders of magnitude below literature values-by\nleveraging large experimental uncertainties in $D$-$\\bar{D}$ oscillations.\nConversely, scenario (ii) requires $M_{Z^\\prime} \\gtrsim 165\\;\\text{TeV}$ due\nto stringent $B_s$ data. We further establish the alignment limit\n$\\cos(\\phi+\\varphi) = 0$ for the SM-like Higgs, showing its viability depends\non $V_L^{u,d}$ configurations, with $B_s$ systems enforcing\n$|\\cos(\\phi+\\varphi)| < 0.01$ in down-sector FCNC scenarios. Our analysis\nreveals that strategic choices of quark mixing matrices can suppress FCNC\nvisibility, reconciling the 331 framework with flavor data without ultra-heavy\n$Z^\\prime$ bosons. This work provides the first unified treatment of SM-like\nHiggs- and $Z^\\prime$-mediated FCNCs in 331 models, identifying viable\nparameter spaces for collider phenomenology.",
        "This article considers the automatic selection problem of the relevant\nexplanatory variables in a right-censored model on a massive database. We\npropose and study four aggregated censored adaptive LASSO estimators\nconstructed by dividing the observations in such a way as to keep the\nconsistency of the estimator of the survival curve. We show that these\nestimators have the same theoretical oracle properties as the one built on the\nfull database. Moreover, by Monte Carlo simulations we obtain that their\ncalculation time is smaller than that of the full database. The simulations\nconfirm also the theoretical properties. For optimal tuning parameter\nselection, we propose a BIC-type criterion.",
        "In this paper we consider high-frequency acoustic transmission problems with\njumping coefficients modelled by Helmholtz equations. The solution then is\nhighly oscillatory and, in addition, may be localized in a very small vicinity\nof interfaces (whispering gallery modes). For the reliable numerical\napproximation a) the PDE is tranformed in a classical single trace integral\nequation on the interfaces and b) a spectral Galerkin boundary element method\nis employed for its solution. We show that the resulting integral equation is\nwell posed and analyze the convergence of the boundary element method for the\nparticular case of concentric circular interfaces. We prove a condition on the\nnumber of degrees of freedom for quasi-optimal convergence. Numerical\nexperiments confirm the efficiency of our method and the sharpness of the\ntheoretical estimates.",
        "We propose incorporating multi-qubit nonunitary operations in Variational\nQuantum Thermalizers (VQTs). VQTs are hybrid quantum-classical algorithms that\ngenerate the thermal (Gibbs) state of a given Hamiltonian, with applications in\nquantum algorithms and simulations. However, current algorithms struggle at\nintermediate temperatures, where the target state is nonpure but exhibits\nentanglement. We devise multi-qubit nonunitary operations that harness weak\nsymmetries and thereby improve the performance of the algorithm. Utilizing\ndissipation engineering, we create these nonunitary multi-qubit operations\nwithout the need for measurements or additional qubits. To train the ansatz, we\ndevelop and benchmark novel methods for entropy estimation of quantum states,\nexpanding the toolbox for quantum state characterization. We demonstrate that\nour approach can prepare thermal states of paradigmatic spin models at all\ntemperatures. Our work thus creates new opportunities for simulating open\nquantum many-body systems.",
        "High-dimensional broadband quantum memory significantly expands quantum\ninformation processing capabilities, but the memory efficiency becomes\ninsufficient when extended to high dimensions. We demonstrate an efficient\nquantum memorize for hyper-dimensional photons encoded with orbital angular\nmomentum (OAM) and spin angular momentum (SAM). OAM information is encoded from\n-5 to +5, combined with spin angular momentum encoding, enabling up to 22\ndimensions. To ensure high memory efficiency, an artificial intelligent\nalgorithm, a modified Differential Evolution (DE) algorithm using Chebyshev\nsampling, is developed to obtain a perfect signal-control waveform matching.\nMemory efficiency is experimentally achieved 92% for single-mode Gaussian\nsignal, 91% for information dimension of 6 and 80% for dimensional number to\n22. The fidelity is achieved up to 99% for single-mode Gaussian signal, 96% for\nOAM information and 97% for SAM one, which is far beyond no-cloning limitation.\nOur results demonstrate superior performance and potential applications in\nhigh-dimensional quantum information processing. This achievement provides a\ncrucial foundation for future quantum communication and quantum computing.",
        "Many active galaxies harbor powerful relativistic jets, however, the detailed\nmechanisms of their formation and acceleration remain poorly understood. To\ninvestigate the area of jet acceleration and collimation with the highest\navailable angular resolution, we study the innermost region of the bipolar jet\nin the nearby low-ionization nuclear emission-line region (LINER) galaxy NGC\n1052. We combined observations of NGC 1052 taken with VLBA, GMVA, and EHT over\none week in the spring of 2017. For the first time, NGC 1052 was detected with\nthe EHT, providing a size of the central region in-between both jet bases of\n250 RS (Schwarzschild radii) perpendicular to the jet axes. This size estimate\nsupports previous studies of the jets expansion profile which suggest two\nbreaks of the profile at around 300 RS and 10000 RS distances to the core.\nFurthermore, we estimated the magnetic field to be 1.25 Gauss at a distance of\n22 {\\mu}as from the central engine by fitting a synchrotron-self absorption\nspectrum to the innermost emission feature, which shows a spectral turn-over at\nabout 130 GHz. Assuming a purely poloidal magnetic field, this implies an upper\nlimit on the magnetic field strength at the event horizon of 26000 Gauss, which\nis consistent with previous measurements. The complex, low-brightness,\ndouble-sided jet structure in NGC 1052 makes it a challenge to detect the\nsource at millimeter (mm) wavelengths. However, our first EHT observations have\ndemonstrated that detection is possible up to at least 230 GHz. This study\noffers a glimpse through the dense surrounding torus and into the innermost\ncentral region, where the jets are formed. This has enabled us to finally\nresolve this region and provide improved constraints on its expansion and\nmagnetic field strength.",
        "Let H be a finite dimensional Hopf algebra over a field K. In this paper, we\nstudy when an H-extension becomes a tame H-extension by calculating\nHopfological homology and Hopf-cyclic homology. In the (derived) category of\nH'-comodules for a Hopf algebra H', we take Hopf subalgebra H of H' and a\ncertain order A of H. We see the behavior of Hopfological homology for a tame\nA-subextension S\/R in terms of the surjectivity of trace map and of cyclic\nmodules, which induce Hopf-cyclic homology, for Hopf-Galois extensions with H\nin terms of relative Hopf modules.",
        "Entanglement is a key resource to demonstrate quantum advantage over\nclassical strategies. Entanglement in quantum states is one of the most\nwell-explored areas in quantum physics. However, a rigorous approach to\nunderstanding and detecting entanglement in composite quantum measurements is\nlacking. In this work, we focus on composite quantum measurements and classify\nthem into two classes: entangled and separable measurements. As done for\nquantum states, we define analogously a notion of witness that can be used to\ndetect entanglement in composite quantum measurements. Here, one does not need\nto trust the measurement to witness its entanglement but must trust the quantum\nstates. We then further extend this approach to show that any entangled\nmeasurement provides an advantage in network quantum steering without inputs,\nalso known as swap steering. Consequently, this provides a way to witness\nentanglement in any quantum measurement in a one-sided device-independent way.\nFinally, we consider the star network scenario and show that any rank-one\nprojective entangled quantum measurement gives a quantum advantage. Thus, one\ncan detect the entanglement in any rank-one projective measurement in a\ndevice-independent way."
      ]
    }
  },
  {
    "id":2412.13126,
    "research_type":"applied",
    "start_id":"b1",
    "start_title":"Viable and necrotic tumor assessment from whole slide images of osteosarcoma using machine-learning and deep-learning models",
    "start_abstract":"Pathological estimation of tumor necrosis after chemotherapy is essential for patients with osteosarcoma. This study reports the first fully automated tool to assess viable and necrotic in osteosarcoma, employing advances histopathology digitization learning. We selected 40 digitized whole slide images representing heterogeneity osteosarcoma response. With goal labeling diverse regions tissue into tumor, non-tumor, we trained 13 machine-learning models top performing one (a Support Vector Machine) based on reported accuracy. also developed a deep-learning architecture it same data set. computed receiver-operator characteristic discrimination non-tumor from followed by conditional found our exceptionally well. then used identify interest image-tiles generated test images. The classification output visualized as tumor-prediction map, displaying extent image. Thus, lay foundation complete assessment pipeline original histology map generation. proposed can be adopted other types tumor.",
    "start_categories":[
      "q-bio.CB"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b2"
      ],
      "title":[
        "Machine Learning-driven Histotype Diagnosis of Ovarian Carcinoma: Insights from the OCEAN AI Challenge"
      ],
      "abstract":[
        "Ovarian cancer poses a significant health burden as one of the deadliest malignancies affecting women globally. Histotype assignment of epithelial ovarian cancers can be challenging due to morphologic overlap, inter-observer variability, and the lack of ancillary diagnostic techniques in some areas of the world. Moreover, rare cancers can pose particular diagnostic difficulties because of a relative lack of familiarity with them, underscoring the necessity for robust diagnostic methodologies. The emergence of Artificial Intelligence (AI) has brought promising prospects to the realm of ovarian cancer diagnosis. While various studies have underscored AI's promise, its validation across multiple healthcare centers and hospitals has been limited. Inspired by innovations in medical imaging driven by public competitions, we initiated the Ovarian Cancer subtypE clAssification and outlier detectioN (OCEAN) challenge, the most extensive histopathology competition to date."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "The Goofus & Gallant Story Corpus for Practical Value Alignment",
        "Can Reasoning Models Reason about Hardware? An Agentic HLS Perspective",
        "Self-Consistency of the Internal Reward Models Improves Self-Rewarding\n  Language Models",
        "Systematic Abductive Reasoning via Diverse Relation Representations in\n  Vector-symbolic Architecture",
        "Platform-Aware Mission Planning",
        "Mapping AI Benchmark Data to Quantitative Risk Estimates Through Expert\n  Elicitation",
        "Heterogeneous Causal Discovery of Repeated Undesirable Health Outcomes",
        "A Transformer-based survival model for prediction of all-cause mortality\n  in heart failure patients: a multi-cohort study",
        "L2R: Learning to Reduce Search Space for Generalizable Neural Routing\n  Solver",
        "KU AIGEN ICL EDI@BC8 Track 3: Advancing Phenotype Named Entity\n  Recognition and Normalization for Dysmorphology Physical Examination Reports",
        "Privacy-Enhancing Paradigms within Federated Multi-Agent Systems",
        "R-ParVI: Particle-based variational inference through lens of rewards",
        "Counterfactual Language Reasoning for Explainable Recommendation Systems",
        "Quasi-periodic oscillations of GHz-band polarization in a black hole",
        "Principles and Metrics of Extreme Learning Machines Using a Highly\n  Nonlinear Fiber",
        "Molecular Mechanism Enabling Linearity and Symmetry in Neuromorphic\n  Elements",
        "VideoMerge: Towards Training-free Long Video Generation",
        "A redescription mining framework for post-hoc explaining and relating\n  deep learning models",
        "Quantum Chebyshev Probabilistic Models for Fragmentation Functions",
        "AlignVLM: Bridging Vision and Language Latent Spaces for Multimodal\n  Understanding",
        "Learning Control of Neural Sound Effects Synthesis from Physically\n  Inspired Models",
        "Predicting Steady-State Behavior in Complex Networks with Graph Neural\n  Networks",
        "LanP: Rethinking the Impact of Language Priors in Large Vision-Language\n  Models",
        "The Vlasov Bivector: A Parameter-Free Approach to Vlasov Kinematics",
        "Beyond the Lungs: Extending the Field of View in Chest CT with Latent\n  Diffusion Models",
        "Pulmonary Tuberculosis Edge Diagnosis System Based on MindSpore\n  Framework: Low-cost and High-precision Implementation with Ascend 310 Chip",
        "Generalized Decision Focused Learning under Imprecise\n  Uncertainty--Theoretical Study",
        "How does Radiation Reaction Affect Relativistic Magnetized Shocks\n  Emission"
      ],
      "abstract":[
        "Values or principles are key elements of human society that influence people\nto behave and function according to an accepted standard set of social rules to\nmaintain social order. As AI systems are becoming ubiquitous in human society,\nit is a major concern that they could violate these norms or values and\npotentially cause harm. Thus, to prevent intentional or unintentional harm, AI\nsystems are expected to take actions that align with these principles. Training\nsystems to exhibit this type of behavior is difficult and often requires a\nspecialized dataset. This work presents a multi-modal dataset illustrating\nnormative and non-normative behavior in real-life situations described through\nnatural language and artistic images. This training set contains curated sets\nof images that are designed to teach young children about social principles. We\nargue that this is an ideal dataset to use for training socially normative\nagents given this fact.",
        "Recent Large Language Models (LLMs) such as OpenAI o3-mini and DeepSeek-R1\nuse enhanced reasoning through Chain-of-Thought (CoT). Their potential in\nhardware design, which relies on expert-driven iterative optimization, remains\nunexplored. This paper investigates whether reasoning LLMs can address\nchallenges in High-Level Synthesis (HLS) design space exploration and\noptimization. During HLS, engineers manually define pragmas\/directives to\nbalance performance and resource constraints. We propose an LLM-based\noptimization agentic framework that automatically restructures code, inserts\npragmas, and identifies optimal design points via feedback from HLs tools and\naccess to integer-linear programming (ILP) solvers. Experiments compare\nreasoning models against conventional LLMs on benchmarks using success rate,\nefficiency, and design quality (area\/latency) metrics, and provide the\nfirst-ever glimpse into the CoTs produced by a powerful open-source reasoning\nmodel like DeepSeek-R1.",
        "Aligning Large Language Models (LLMs) with human preferences is crucial for\ntheir deployment in real-world applications. Recent advancements in\nSelf-Rewarding Language Models suggest that an LLM can use its internal reward\nmodels (such as LLM-as-a-Judge) \\cite{yuanself} to generate preference data,\nimproving alignment performance without costly human annotation. However, we\nfind that different internal reward models within the same LLM often generate\ninconsistent preferences. This inconsistency raises concerns about the\nreliability of self-generated preference data, hinders overall alignment\nperformance, and highlights the need for further research to ensure reliable\nand coherent alignment with human preferences. To address this limitation, we\npropose Self-Consistent Internal Rewards (SCIR), a novel framework designed to\nenhance consistency among internal reward models during training. In each\ntraining step, we collect preference predictions from multiple pre-defined\ninternal reward models and enforce consistency and confidence through an\ninconsistency penalty mechanism, thereby improving the reliability of these\ninternal reward models. We selectively use data with consistent predictions for\npreference optimization, ensuring the quality of the preference data. By\nemploying self-consistent internal rewards, our method significantly improves\nthe alignment performance and reward modeling capability of LLMs, outperforming\nbaseline methods by a notable margin.",
        "In abstract visual reasoning, monolithic deep learning models suffer from\nlimited interpretability and generalization, while existing neuro-symbolic\napproaches fall short in capturing the diversity and systematicity of\nattributes and relation representations. To address these challenges, we\npropose a Systematic Abductive Reasoning model with diverse relation\nrepresentations (Rel-SAR) in Vector-symbolic Architecture (VSA) to solve\nRaven's Progressive Matrices (RPM). To derive attribute representations with\nsymbolic reasoning potential, we introduce not only various types of atomic\nvectors that represent numeric, periodic and logical semantics, but also the\nstructured high-dimentional representation (SHDR) for the overall Grid\ncomponent. For systematic reasoning, we propose novel numerical and logical\nrelation functions and perform rule abduction and execution in a unified\nframework that integrates these relation representations. Experimental results\ndemonstrate that Rel-SAR achieves significant improvement on RPM tasks and\nexhibits robust out-of-distribution generalization. Rel-SAR leverages the\nsynergy between HD attribute representations and symbolic reasoning to achieve\nsystematic abductive reasoning with both interpretable and computable\nsemantics.",
        "Planning for autonomous systems typically requires reasoning with models at\ndifferent levels of abstraction, and the harmonization of two competing sets of\nobjectives: high-level mission goals that refer to an interaction of the system\nwith the external environment, and low-level platform constraints that aim to\npreserve the integrity and the correct interaction of the subsystems. The\ncomplicated interplay between these two models makes it very hard to reason on\nthe system as a whole, especially when the objective is to find plans with\nrobustness guarantees, considering the non-deterministic behavior of the lower\nlayers of the system.\n  In this paper, we introduce the problem of Platform-Aware Mission Planning\n(PAMP), addressing it in the setting of temporal durative actions. The PAMP\nproblem differs from standard temporal planning for its exists-forall nature:\nthe high-level plan dealing with mission goals is required to satisfy safety\nand executability constraints, for all the possible non-deterministic\nexecutions of the low-level model of the platform and the environment. We\npropose two approaches for solving PAMP. The first baseline approach\namalgamates the mission and platform levels, while the second is based on an\nabstraction-refinement loop that leverages the combination of a planner and a\nverification engine. We prove the soundness and completeness of the proposed\napproaches and validate them experimentally, demonstrating the importance of\nheterogeneous modeling and the superiority of the technique based on\nabstraction-refinement.",
        "The literature and multiple experts point to many potential risks from large\nlanguage models (LLMs), but there are still very few direct measurements of the\nactual harms posed. AI risk assessment has so far focused on measuring the\nmodels' capabilities, but the capabilities of models are only indicators of\nrisk, not measures of risk. Better modeling and quantification of AI risk\nscenarios can help bridge this disconnect and link the capabilities of LLMs to\ntangible real-world harm. This paper makes an early contribution to this field\nby demonstrating how existing AI benchmarks can be used to facilitate the\ncreation of risk estimates. We describe the results of a pilot study in which\nexperts use information from Cybench, an AI benchmark, to generate probability\nestimates. We show that the methodology seems promising for this purpose, while\nnoting improvements that can be made to further strengthen its application in\nquantitative AI risk assessment.",
        "Understanding factors triggering or preventing undesirable health outcomes\nacross patient subpopulations is essential for designing targeted\ninterventions. While randomized controlled trials and expert-led patient\ninterviews are standard methods for identifying these factors, they can be\ntime-consuming and infeasible. Causal discovery offers an alternative to\nconventional approaches by generating cause-and-effect hypotheses from\nobservational data. However, it often relies on strong or untestable\nassumptions, which can limit its practical application. This work aims to make\ncausal discovery more practical by considering multiple assumptions and\nidentifying heterogeneous effects. We formulate the problem of discovering\ncauses and effect modifiers of an outcome, where effect modifiers are contexts\n(e.g., age groups) with heterogeneous causal effects. Then, we present a novel,\nend-to-end framework that incorporates an ensemble of causal discovery\nalgorithms and estimation of heterogeneous effects to discover causes and\neffect modifiers that trigger or inhibit the outcome. We demonstrate that the\nensemble approach improves robustness by enhancing recall of causal factors\nwhile maintaining precision. Our study examines the causes of repeat emergency\nroom visits for diabetic patients and hospital readmissions for ICU patients.\nOur framework generates causal hypotheses consistent with existing literature\nand can help practitioners identify potential interventions and patient\nsubpopulations to focus on.",
        "We developed and validated TRisk, a Transformer-based AI model predicting\n36-month mortality in heart failure patients by analysing temporal patient\njourneys from UK electronic health records (EHR). Our study included 403,534\nheart failure patients (ages 40-90) from 1,418 English general practices, with\n1,063 practices for model derivation and 355 for external validation. TRisk was\ncompared against the MAGGIC-EHR model across various patient subgroups. With\nmedian follow-up of 9 months, TRisk achieved a concordance index of 0.845 (95%\nconfidence interval: [0.841, 0.849]), significantly outperforming MAGGIC-EHR's\n0.728 (0.723, 0.733) for predicting 36-month all-cause mortality. TRisk showed\nmore consistent performance across sex, age, and baseline characteristics,\nsuggesting less bias. We successfully adapted TRisk to US hospital data through\ntransfer learning, achieving a C-index of 0.802 (0.789, 0.816) with 21,767\npatients. Explainability analyses revealed TRisk captured established risk\nfactors while identifying underappreciated predictors like cancers and hepatic\nfailure that were important across both cohorts. Notably, cancers maintained\nstrong prognostic value even a decade after diagnosis. TRisk demonstrated\nwell-calibrated mortality prediction across both healthcare systems. Our\nfindings highlight the value of tracking longitudinal health profiles and\nrevealed risk factors not included in previous expert-driven models.",
        "Constructive neural combinatorial optimization (NCO) has attracted growing\nresearch attention due to its ability to solve complex routing problems without\nrelying on handcrafted rules. However, existing NCO methods face significant\nchallenges in generalizing to large-scale problems due to high computational\ncomplexity and inefficient capture of structural patterns. To address this\nissue, we propose a novel learning-based search space reduction method that\nadaptively selects a small set of promising candidate nodes at each step of the\nconstructive NCO process. Unlike traditional methods that rely on fixed\nheuristics, our selection model dynamically prioritizes nodes based on learned\npatterns, significantly reducing the search space while maintaining solution\nquality. Experimental results demonstrate that our method, trained solely on\n100-node instances from uniform distribution, generalizes remarkably well to\nlarge-scale Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing\nProblem (CVRP) instances with up to 1 million nodes from the uniform\ndistribution and over 80K nodes from other distributions.",
        "The objective of BioCreative8 Track 3 is to extract phenotypic key medical\nfindings embedded within EHR texts and subsequently normalize these findings to\ntheir Human Phenotype Ontology (HPO) terms. However, the presence of diverse\nsurface forms in phenotypic findings makes it challenging to accurately\nnormalize them to the correct HPO terms. To address this challenge, we explored\nvarious models for named entity recognition and implemented data augmentation\ntechniques such as synonym marginalization to enhance the normalization step.\nOur pipeline resulted in an exact extraction and normalization F1 score 2.6\\%\nhigher than the mean score of all submissions received in response to the\nchallenge. Furthermore, in terms of the normalization F1 score, our approach\nsurpassed the average performance by 1.9\\%. These findings contribute to the\nadvancement of automated medical data extraction and normalization techniques,\nshowcasing potential pathways for future research and application in the\nbiomedical domain.",
        "LLM-based Multi-Agent Systems (MAS) have proven highly effective in solving\ncomplex problems by integrating multiple agents, each performing different\nroles. However, in sensitive domains, they face emerging privacy protection\nchallenges. In this paper, we introduce the concept of Federated MAS,\nhighlighting the fundamental differences between Federated MAS and traditional\nFL. We then identify key challenges in developing Federated MAS, including: 1)\nheterogeneous privacy protocols among agents, 2) structural differences in\nmulti-party conversations, and 3) dynamic conversational network structures. To\naddress these challenges, we propose Embedded Privacy-Enhancing Agents\n(EPEAgent), an innovative solution that integrates seamlessly into the\nRetrieval-Augmented Generation (RAG) phase and the context retrieval stage.\nThis solution minimizes data flows, ensuring that only task-relevant,\nagent-specific information is shared. Additionally, we design and generate a\ncomprehensive dataset to evaluate the proposed paradigm. Extensive experiments\ndemonstrate that EPEAgent effectively enhances privacy protection while\nmaintaining strong system performance. The code will be availiable at\nhttps:\/\/github.com\/ZitongShi\/EPEAgent",
        "A reward-guided, gradient-free ParVI method, \\textit{R-ParVI}, is proposed\nfor sampling partially known densities (e.g. up to a constant). R-ParVI\nformulates the sampling problem as particle flow driven by rewards: particles\nare drawn from a prior distribution, navigate through parameter space with\nmovements determined by a reward mechanism blending assessments from the target\ndensity, with the steady state particle configuration approximating the target\ngeometry. Particle-environment interactions are simulated by stochastic\nperturbations and the reward mechanism, which drive particles towards high\ndensity regions while maintaining diversity (e.g. preventing from collapsing\ninto clusters). R-ParVI offers fast, flexible, scalable and stochastic sampling\nand inference for a class of probabilistic models such as those encountered in\nBayesian inference and generative modelling.",
        "Explainable recommendation systems leverage transparent reasoning to foster\nuser trust and improve decision-making processes. Current approaches typically\ndecouple recommendation generation from explanation creation, violating causal\nprecedence principles where explanatory factors should logically precede\noutcomes. This paper introduces a novel framework integrating structural causal\nmodels with large language models to establish causal consistency in\nrecommendation pipelines. Our methodology enforces explanation factors as\ncausal antecedents to recommendation predictions through causal graph\nconstruction and counterfactual adjustment. We particularly address the\nconfounding effect of item popularity that distorts personalization signals in\nexplanations, developing a debiasing mechanism that disentangles genuine user\npreferences from conformity bias. Through comprehensive experiments across\nmultiple recommendation scenarios, we demonstrate that CausalX achieves\nsuperior performance in recommendation accuracy, explanation plausibility, and\nbias mitigation compared to baselines.",
        "Relativistic jets from accreting black holes (BHs) radiate non-thermal\nemission which is highly variable in different time scales. Magnetic fields\nanchored to a rotating BH or accretion disc accelerate and collimate jets of\nthe BH systems. Previous studies on black holes of different mass scales,\nincluding supermassive and stellar-mass black holes, only report flux\nquasi-periodic oscillations in radio, optical, X-ray and gamma-ray bands. No\nquasi-periodic variations in polarization have yet been detected in any black\nhole systems. Here, we report the first detection of GHz radio polarization\noscillations in GRS 1915+105, which harbors a spinning stellar-mass BH with a\nrelativistic jet. Our observations show that during the increasing phase of\nradio emission, linear polarization and flux exhibit similar oscillation\nperiods of $\\sim 17$ and $33$ seconds, and their variation patterns\nanti-correlate with each other. These rare, short-period oscillations in both\npolarization and flux would be important to understand instabilities and\nspecial dynamics in magnetized jets.",
        "Optical computing offers potential for ultra high-speed and low latency\ncomputation by leveraging the intrinsic properties of light. Here, we explore\nthe use of highly nonlinear optical fibers (HNLFs) as platforms for optical\ncomputing based on the concept of Extreme Learning Machines. Task-independent\nevaluations are introduced to the field for the first time and focus on the\nfundamental metrics of effective dimensionality and consistency, which we\nexperimentally characterize for different nonlinear and dispersive conditions.\nWe show that input power and fiber characteristics significantly influence the\ndimensionality of the computational system, with longer fibers and higher\ndispersion producing up to 100 principal components (PCs) at input power levels\nof 30 mW, where the PC correspond to the linearly independent dimensions of the\nsystem. The spectral distribution of the PC's eigenvectors reveals that the\nhigh-dimensional dynamics facilitating computing through dimensionality\nexpansion are located within 40~nm of the pump wavelength at 1560~nm, providing\ngeneral insight for computing with nonlinear Schr\\\"odinger equation systems.\nTask-dependent results demonstrate the effectiveness of HNLFs in classifying\nMNIST dataset images. Using input data compression through PC analysis, we\ninject MNIST images of various input dimensionality into the system and study\nthe impact of input power upon classification accuracy. At optimized power\nlevels we achieve a classification test accuracy of 88\\%, significantly\nsurpassing the baseline of 83.7\\% from linear systems. Noteworthy, we find that\nbest performance is not obtained at maximal input power, i.e. maximal system\ndimensionality, but at more than one order of magnitude lower. The same is\nconfirmed regarding the MNIST image's compression, where accuracy is\nsubstantially improved when strongly compressing the image to less than 50 PCs.",
        "For over a decade, linear and symmetric weight updates have remained the\nelusive holy grail in neuromorphic computing. Here, we unveil a kinetically\ncontrolled molecular mechanism driving a near-ideal neuromorphic element,\ncapable of precisely modulating conductance linearly across 16,500 analog\nlevels spanning four orders of magnitude. Our findings, supported by\nexperimental data and mathematical modelling, demonstrate how nonlinear\nprocesses such as nucleation can be orchestrated within small perturbation\nregimes to achieve linearity. This establishes a groundwork for routinely\nrealizing these long-sought neuromorphic features across a broad range of\nmaterial systems.",
        "Long video generation remains a challenging and compelling topic in computer\nvision. Diffusion based models, among the various approaches to video\ngeneration, have achieved state of the art quality with their iterative\ndenoising procedures. However, the intrinsic complexity of the video domain\nrenders the training of such diffusion models exceedingly expensive in terms of\nboth data curation and computational resources. Moreover, these models\ntypically operate on a fixed noise tensor that represents the video, resulting\nin predetermined spatial and temporal dimensions. Although several high quality\nopen-source pretrained video diffusion models, jointly trained on images and\nvideos of varying lengths and resolutions, are available, it is generally not\nrecommended to specify a video length at inference that was not included in the\ntraining set. Consequently, these models are not readily adaptable to the\ndirect generation of longer videos by merely increasing the specified video\nlength. In addition to feasibility challenges, long-video generation also\nencounters quality issues. The domain of long videos is inherently more complex\nthan that of short videos: extended durations introduce greater variability and\nnecessitate long-range temporal consistency, thereby increasing the overall\ndifficulty of the task. We propose VideoMerge, a training-free method that can\nbe seamlessly adapted to merge short videos generated by pretrained\ntext-to-video diffusion model. Our approach preserves the model's original\nexpressiveness and consistency while allowing for extended duration and dynamic\nvariation as specified by the user. By leveraging the strengths of pretrained\nmodels, our method addresses challenges related to smoothness, consistency, and\ndynamic content through orthogonal strategies that operate collaboratively to\nachieve superior quality.",
        "Deep learning models (DLMs) achieve increasingly high performance both on\nstructured and unstructured data. They significantly extended applicability of\nmachine learning to various domains. Their success in making predictions,\ndetecting patterns and generating new data made significant impact on science\nand industry. Despite these accomplishments, DLMs are difficult to explain\nbecause of their enormous size. In this work, we propose a novel framework for\npost-hoc explaining and relating DLMs using redescriptions. The framework\nallows cohort analysis of arbitrary DLMs by identifying statistically\nsignificant redescriptions of neuron activations. It allows coupling neurons to\na set of target labels or sets of descriptive attributes, relating layers\nwithin a single DLM or associating different DLMs. The proposed framework is\nindependent of the artificial neural network architecture and can work with\nmore complex target labels (e.g. multi-label or multi-target scenario).\nAdditionally, it can emulate both pedagogical and decompositional approach to\nrule extraction. The aforementioned properties of the proposed framework can\nincrease explainability and interpretability of arbitrary DLMs by providing\ndifferent information compared to existing explainable-AI approaches.",
        "We propose a quantum protocol for efficiently learning and sampling\nmultivariate probability distributions that commonly appear in high-energy\nphysics. Our approach introduces a bivariate probabilistic model based on\ngeneralized Chebyshev polynomials, which is (pre-)trained as an explicit\ncircuit-based model for two correlated variables, and sampled efficiently with\nthe use of quantum Chebyshev transforms. As a key application, we study the\nfragmentation functions~(FFs) of charged pions and kaons from single-inclusive\nhadron production in electron-positron annihilation. We learn the joint\ndistribution for the momentum fraction $z$ and energy scale $Q$ in several\nfragmentation processes. Using the trained model, we infer the correlations\nbetween $z$ and $Q$ from the entanglement of the probabilistic model, noting\nthat the developed energy-momentum correlations improve model performance.\nFurthermore, utilizing the generalization capabilities of the quantum Chebyshev\nmodel and extended register architecture, we perform a fine-grid multivariate\nsampling relevant for FF dataset augmentation. Our results highlight the\ngrowing potential of quantum generative modeling for addressing problems in\nscientific discovery and advancing data analysis in high-energy physics.",
        "Aligning visual features with language embeddings is a key challenge in\nvision-language models (VLMs). The performance of such models hinges on having\na good connector that maps visual features generated by a vision encoder to a\nshared embedding space with the LLM while preserving semantic similarity.\nExisting connectors, such as multilayer perceptrons (MLPs), often produce\nout-of-distribution or noisy inputs, leading to misalignment between the\nmodalities. In this work, we propose a novel vision-text alignment method,\nAlignVLM, that maps visual features to a weighted average of LLM text\nembeddings. Our approach leverages the linguistic priors encoded by the LLM to\nensure that visual features are mapped to regions of the space that the LLM can\neffectively interpret. AlignVLM is particularly effective for document\nunderstanding tasks, where scanned document images must be accurately mapped to\ntheir textual content. Our extensive experiments show that AlignVLM achieves\nstate-of-the-art performance compared to prior alignment methods. We provide\nfurther analysis demonstrating improved vision-text feature alignment and\nrobustness to noise.",
        "Sound effects model design commonly uses digital signal processing techniques\nwith full control ability, but it is difficult to achieve realism within a\nlimited number of parameters. Recently, neural sound effects synthesis methods\nhave emerged as a promising approach for generating high-quality and realistic\nsounds, but the process of synthesizing the desired sound poses difficulties in\nterms of control. This paper presents a real-time neural synthesis model guided\nby a physically inspired model, enabling the generation of high-quality sounds\nwhile inheriting the control interface of the physically inspired model. We\nshowcase the superior performance of our model in terms of sound quality and\ncontrol.",
        "In complex systems, information propagation can be defined as diffused or\ndelocalized, weakly localized, and strongly localized. This study investigates\nthe application of graph neural network models to learn the behavior of a\nlinear dynamical system on networks. A graph convolution and attention-based\nneural network framework has been developed to identify the steady-state\nbehavior of the linear dynamical system. We reveal that our trained model\ndistinguishes the different states with high accuracy. Furthermore, we have\nevaluated model performance with real-world data. In addition, to understand\nthe explainability of our model, we provide an analytical derivation for the\nforward and backward propagation of our framework.",
        "Large Vision-Language Models (LVLMs) have shown impressive performance in\nvarious tasks. However, LVLMs suffer from hallucination, which hinders their\nadoption in the real world. Existing studies emphasized that the strong\nlanguage priors of LVLMs can overpower visual information, causing\nhallucinations. However, the positive role of language priors is the key to a\npowerful LVLM. If the language priors are too weak, LVLMs will struggle to\nleverage rich parameter knowledge and instruction understanding abilities to\ncomplete tasks in challenging visual scenarios where visual information alone\nis insufficient. Therefore, we propose a benchmark called LanP to rethink the\nimpact of Language Priors in LVLMs. It is designed to investigate how strong\nlanguage priors are in current LVLMs. LanP consists of 170 images and 340\ncorresponding well-designed questions. Extensive experiments on 25 popular\nLVLMs reveal that many LVLMs' language priors are not strong enough to\neffectively aid question answering when objects are partially hidden. Many\nmodels, including GPT-4 Turbo, exhibit an accuracy below 0.5 in such a\nscenario.",
        "Plasma kinetics, for both flat and curved spacetime, is conventionally\nperformed on the mass shell, a 7--dimensional time-phase space with a Vlasov\nvector field, also known as the Liouville vector field. The choice of this\ntime-phase space encodes the parameterisation of the underling 2nd order\nordinary differential equations. By replacing the Vlasov vector on time-phase\nspace with a bivector on an 8--dimensional sub-bundle of the tangent bundle, we\ncreate a parameterisation free version of Vlasov theory. This has a number of\nadvantages, which include working for lightlike and ultra-relativistic\nparticles, non metric connections, and metric-free and premetric theories. It\nalso works for theories where no time-phase space can exist for topological\ntopological reasons. An example of this is when we wish to consider all\ngeodesics, including spacelike geodesics.\n  We extend the particle density function to a 6--form on the subbundle of the\ntangent space, and define the transport equations, which correspond to the\nVlasov equation. We then show how to define the corresponding 3--current on\nspacetime. We discuss the stress-energy tensor needed for the Einstein-Vlasov\nsystem.\n  This theory can be generalised to create parameterisation invariant Vlasov\ntheories for many 2nd order theories, on arbitrary manifolds. The relationship\nto sprays and semi-sprays is given and examples from Finsler geometry are also\ngiven.",
        "The interconnection between the human lungs and other organs, such as the\nliver and kidneys, is crucial for understanding the underlying risks and\neffects of lung diseases and improving patient care. However, most research\nchest CT imaging is focused solely on the lungs due to considerations of cost\nand radiation dose. This restricted field of view (FOV) in the acquired images\nposes challenges to comprehensive analysis and hinders the ability to gain\ninsights into the impact of lung diseases on other organs. To address this, we\npropose SCOPE (Spatial Coverage Optimization with Prior Encoding), a novel\napproach to capture the inter-organ relationships from CT images and extend the\nFOV of chest CT images. Our approach first trains a variational autoencoder\n(VAE) to encode 2D axial CT slices individually, then stacks the latent\nrepresentations of the VAE to form a 3D context for training a latent diffusion\nmodel. Once trained, our approach extends the FOV of CT images in the\nz-direction by generating new axial slices in a zero-shot manner. We evaluated\nour approach on the National Lung Screening Trial (NLST) dataset, and results\nsuggest that it effectively extends the FOV to include the liver and kidneys,\nwhich are not completely covered in the original NLST data acquisition.\nQuantitative results on a held-out whole-body dataset demonstrate that the\ngenerated slices exhibit high fidelity with acquired data, achieving an SSIM of\n0.81.",
        "Pulmonary Tuberculosis (PTB) remains a major challenge for global health,\nespecially in areas with poor medical resources, where access to specialized\nmedical knowledge and diagnostic tools is limited. This paper presents an\nauxiliary diagnosis system for pulmonary tuberculosis based on Huawei MindSpore\nframework and Ascend310 edge computing chip. Using MobileNetV3 architecture and\nSoftmax cross entropy loss function with momentum optimizer. The system\noperates with FP16 hybrid accuracy on the Orange pie AIPro (Atlas 200 DK) edge\ndevice and performs well. In the test set containing 4148 chest images, the\nmodel accuracy reached 99.1\\% (AUC = 0.99), and the equipment cost was\ncontrolled within \\$150, providing affordable AI-assisted diagnosis scheme for\nprimary care.",
        "Decision Focused Learning has emerged as a critical paradigm for integrating\nmachine learning with downstream optimisation. Despite its promise, existing\nmethodologies predominantly rely on probabilistic models and focus narrowly on\ntask objectives, overlooking the nuanced challenges posed by epistemic\nuncertainty, non-probabilistic modelling approaches, and the integration of\nuncertainty into optimisation constraints. This paper bridges these gaps by\nintroducing innovative frameworks: (i) a non-probabilistic lens for epistemic\nuncertainty representation, leveraging intervals (the least informative\nuncertainty model), Contamination (hybrid model), and probability boxes (the\nmost informative uncertainty model); (ii) methodologies to incorporate\nuncertainty into constraints, expanding Decision-Focused Learning's utility in\nconstrained environments; (iii) the adoption of Imprecise Decision Theory for\nambiguity-rich decision-making contexts; and (iv) strategies for addressing\nsparse data challenges. Empirical evaluations on benchmark optimisation\nproblems demonstrate the efficacy of these approaches in improving decision\nquality and robustness and dealing with said gaps.",
        "Relativistic magnetized shocks, through the Synchrotron Maser Instability\n(SMI) mechanism, represent a promising framework for generating coherent\nradiations, potentially accounting for the enigmatic Fast Radio Bursts\n(FRBs)-cosmic radio transients with extreme luminosity. This study investigates\nhow the radiation reaction (RR) effect, induced by high-energy photon emissions\nduring SMI, significantly modifies particle dynamics and emission properties in\nmagnetized shocks. Through comprehensive Particle-In-Cell (PIC) simulations, we\ndemonstrate that RR effects fundamentally alter coherent cyclotron motion at\nshock fronts, producing distinct observational signatures: spectral broadening,\npeak frequency upshift, and enhanced radiation intensity. Our findings suggest\nthat RR-mediated magnetized shocks could provide a natural explanation for the\nbimodal energy distribution observed in repeating FRB 121102 and the positive\ncorrelation of luminosity-bandwidth between repeating and one-off FRBs in\nCHIME\/FRB catalog. These results support the magnetized shock as a viable\nsource of FRBs."
      ]
    }
  },
  {
    "id":2411.17717,
    "research_type":"applied",
    "start_id":"b4",
    "start_title":"Neural biomarker diagnosis and prediction to mild cognitive impairment and Alzheimer\u2019s disease using EEG technology",
    "start_abstract":"Background Electroencephalogram (EEG) has emerged as a non-invasive tool to detect the aberrant neuronal activity related to different stages of Alzheimer\u2019s disease (AD). However, the effectiveness of EEG in the precise diagnosis and assessment of AD and its preclinical stage, amnestic mild cognitive impairment (MCI), has yet to be fully elucidated. In this study, we aimed to identify key EEG biomarkers that are effective in distinguishing patients at the early stage of AD and monitoring the progression of AD. Methods A total of 890 participants, including 189 patients with MCI, 330 patients with AD, 125 patients with other dementias (frontotemporal dementia, dementia with Lewy bodies, and vascular cognitive impairment), and 246 healthy controls (HC) were enrolled. Biomarkers were extracted from resting-state EEG recordings for a three-level classification of HC, MCI, and AD. The optimal EEG biomarkers were then identified based on the classification performance. Random forest regression was used to train a series of models by combining participants\u2019 EEG biomarkers, demographic information (i.e., sex, age), CSF biomarkers, and APOE phenotype for assessing the disease progression and individual\u2019s cognitive function. Results The identified EEG biomarkers achieved over 70% accuracy in the three-level classification of HC, MCI, and AD. Among all six groups, the most prominent effects of AD-linked neurodegeneration on EEG metrics were localized at parieto-occipital regions. In the cross-validation predictive analyses, the optimal EEG features were more effective than the CSF + APOE biomarkers in predicting the age of onset and disease course, whereas the combination of EEG + CSF + APOE measures achieved the best performance for all targets of prediction. Conclusions Our study indicates that EEG can be used as a useful screening tool for the diagnosis and disease progression evaluation of MCI and AD.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b5"
      ],
      "title":[
        "Quantitative EEG analysis disease during resting and memory task in carriers and non-carriers of PS-1 E280A mutation of familial Alzheimer's"
      ],
      "abstract":[
        "Background: Alzheimer\u2019s disease is the most leading cause of dementia in world; mutation PS-1 E280A alters gene Presenilin-1 and causes an early onset familial disease. This has been found large kindred Antioquia, Colombia. The objective this study was to find differences revealed by electroencephalogram between healthy subjects asymptomatic carriers that can be used as clinical markers population. Methods: EEG recorded 15 non during resting a memory task using 64 channels amplifier. Two conditions were analyzed: encoding retrieval, process recording evocating information, respectively. Power spectrum calculated delta (0.5\u20134.0 Hz), theta (4.0\u20138. 0 alpha-1 (8.0\u201310.0 alpha-2 (10.0\u201313.0 beta (13.0\u201325.0 Hz) gamma (25.0\u201350 frequency bands for four regions interest. Changes evaluated different ANOVA analysis. Results: In condition significant decrease (p=0. 0001) increase frequencies (p=0.037) compare with controls. During significantly lower compared controls 008) comparing versus retrieval each group, there more synchronization carriers. Conclusion: Early changes observed recordings, it could use Also seems activate additional cortical order conserve successful cognitive functions before impairment ."
      ],
      "categories":[
        "q-bio.NC"
      ]
    },
    "list":{
      "title":[
        "Causal Spike Timing Dependent Plasticity Prevents Assembly Fusion in\n  Recurrent Networks",
        "Freezing of Gait as a Complication of Pallidal Deep Brain Stimulation in\n  DYT- KMT2B Patients with Evidence of Striatonigral Degeneration",
        "Latent computing by biological neural networks: A dynamical systems\n  framework",
        "Neural Constraints on Cognitive Experience and Mental Health",
        "Long-term follow-up of DYT1 dystonia patients treated by deep brain\n  stimulation: an open-label study",
        "A Turing Test for Artificial Nets devoted to model Human Vision",
        "A UDP Packet Format Establishing Adress Event Representation\n  Communication Between Remote Neuromorphic and Biological Setups",
        "Asynchronous Hebbian\/anti-Hebbian networks",
        "On Questions of Predictability and Control of an Intelligent System\n  Using Probabilistic State-Transitions",
        "Evolution of diverse (and advanced) cognitive abilities through adaptive\n  fine-tuning of learning and chunking mechanisms",
        "Personal Danger Signals Reprocessing: New Online Group Intervention for\n  Chronic Pain",
        "The tardigrade as an emerging model organism for systems neuroscience",
        "Intrinsic motivation as constrained entropy maximization",
        "Search for medium effects using jet axis decorrelation in inclusive jets\n  from PbPb collisions at $\\sqrt{s_\\text{NN}}$ = 5.02 TeV",
        "Colorful Helly via induced matchings",
        "Superlubric Motion of Wave-like Domain Walls in Sliding Ferroelectrics",
        "Extended string-net models with all anyons at finite temperature",
        "Holographic inflation and holographic dark energy from entropy of the\n  anti-de Sitter black hole",
        "A framework for Tate modules of abelian varieties under isogeny",
        "Monolayer transition metal dichalcogenides under finite-pulse polarized\n  radiation",
        "Constrained Fuel and Time Optimal 6DOF Powered Descent Guidance Using\n  Indirect Optimization",
        "More resourceful states improve quantum channel discrimination",
        "Fingerprint Matrix Concept for Detecting, Localizing and Characterizing\n  Targets in Complex Media",
        "Observing Hot Holographic Quark Star With Gravitational Waves",
        "Predicting the depth of the most recent common ancestor of a random\n  sample of $k$ species: the impact of phylogenetic tree shape",
        "Generating Networks to Target Assortativity via Archimedean Copula\n  Graphons",
        "Can Dark Stars account for the star formation efficiency excess at very\n  high redshifts?",
        "COCOA: a compact Compton camera for astrophysical observation of\n  MeV-scale gamma rays"
      ],
      "abstract":[
        "The organization of neurons into functionally related assemblies is a\nfundamental feature of cortical networks, yet our understanding of how these\nassemblies maintain distinct identities while sharing members remains limited.\nHere we analyze how spike-timing-dependent plasticity (STDP) shapes the\nformation and stability of overlapping neuronal assemblies in recurrently\ncoupled networks of spiking neuron models. Using numerical simulations and an\nassociated mean-field theory, we demonstrate that the temporal structure of the\nSTDP rule, specifically its degree of causality, critically determines whether\nassemblies that share neurons maintain segregation or merge together after\ntraining is completed. We find that causal STDP rules, where\npotentiation\/depression occurs strictly when presynaptic spikes precede\/proceed\npostsynaptic spikes, allow assemblies to remain distinct even with substantial\noverlap in membership. This stability arises because causal STDP effectively\ncancels the symmetric correlations introduced by common inputs from shared\nneurons. In contrast, acausal STDP rules lead to assembly fusion when overlap\nexceeds a critical threshold, due to unchecked growth of common input\ncorrelations. Our results provide theoretical insight into how\nspike-timing-dependent learning rules can support distributed representation\nwhere individual neurons participate in multiple assemblies while maintaining\nfunctional specificity.",
        "Background: Mutations in KMT2B are a recognized cause of early-onset complex\ndystonia, with deep brain stimulation (DBS) of the internal globus pallidus\n(GPi-DBS) being an effective treatment. However, gait impairment, particularly\nfreezing of gait (FOG), remains a significant challenge in DYT-KMT2B patients\npost-DBS. Objectives: To characterize the emergence of FOG in DYT-KMT2B\npatients treated with GPi-DBS and explore potential underlying mechanisms,\nincluding striatonigral degeneration. Methods: Five patients (four females)\nwith KMT2B-related dystonia and protein-truncating variants (PTVs) were\nretrospectively analyzed. Clinical progression, response to GPi-DBS, and the\npresence of FOG were documented. Dopaminergic function was assessed using\nDaTscan (SPECT for ^123I-ioflupane) in four patients. Results: FOG developed in\nall patients, with onset ranging from 1 to 15.5 years post-DBS. DaTscan\nabnormalities, indicative of bilateral striatal dopaminergic denervation, were\nobserved in four cases. Prior to DBS, all patients exhibited dystonia\nunresponsive to L-dopa, and post-DBS, FOG remained refractory to dopaminergic\ntreatment in most cases. Despite initial improvements in gait post-DBS, only\none patient maintained independent ambulation at the last follow-up.\nConclusions: FOG is an emerging complication in DYT-KMT2B patients with PTVs\nundergoing GPi-DBS, potentially linked to underlying striatonigral\ndegeneration. The findings suggest a need for long-term motor surveillance and\nconsideration of alternative therapeutic strategies, including dopaminergic\ntrials, in this patient population. Further studies are required to elucidate\nthe precise mechanisms driving DBS-related hypokinetic gait disturbances in\nDYT-KMT2B dystonia.",
        "Although individual neurons and neural populations exhibit the phenomenon of\nrepresentational drift, perceptual and behavioral outputs of many neural\ncircuits can remain stable across time scales over which representational drift\nis substantial. These observations motivate a dynamical systems framework for\nneural network activity that focuses on the concept of \\emph{latent processing\nunits,} core elements for robust coding and computation embedded in collective\nneural dynamics. Our theoretical treatment of these latent processing units\nyields five key attributes of computing through neural network dynamics. First,\nneural computations that are low-dimensional can nevertheless generate\nhigh-dimensional neural dynamics. Second, the manifolds defined by neural\ndynamical trajectories exhibit an inherent coding redundancy as a direct\nconsequence of the universal computing capabilities of the underlying dynamical\nsystem. Third, linear readouts or decoders of neural population activity can\nsuffice to optimally subserve downstream circuits controlling behavioral\noutputs. Fourth, whereas recordings from thousands of neurons may suffice for\nnear optimal decoding from instantaneous neural activity patterns, experimental\naccess to millions of neurons may be necessary to predict neural ensemble\ndynamical trajectories across timescales of seconds. Fifth, despite the\nvariable activity of single cells, neural networks can maintain stable\nrepresentations of the variables computed by the latent processing units,\nthereby making computations robust to representational drift. Overall, our\nframework for latent computation provides an analytic description and\nempirically testable predictions regarding how large systems of neurons perform\nrobust computations via their collective dynamics.",
        "Understanding how neural dynamics shape cognitive experiences remains a\ncentral challenge in neuroscience and psychiatry. Here, we present a novel\nframework leveraging state-to-output controllability from dynamical systems\ntheory to model the interplay between cognitive perturbations, neural activity,\nand subjective experience. We demonstrate that large-scale fMRI signals are\nconstrained to low-dimensional manifolds, where affective and cognitive states\nare naturally organized. Furthermore, we provide a theoretically robust method\nto estimate the controllability Gramian from steady-state neural responses,\noffering a direct measure of the energy required to steer cognitive outcomes.\nIn five healthy participants viewing 2,185 emotionally evocative short videos,\nour analyses reveal a strong alignment between neural activations and affective\nratings, with an average correlation of $r \\approx 0.7$. In a clinical cohort\nof 255 patients with major depressive disorder, biweekly Hamilton Rating Scale\ntrajectories over 11 weeks significantly mapped onto these manifolds,\nexplaining approximately 20% more variance than chance ($p < 10^{-10}$,\nnumerically better than chance in 93% reaching statistical significance in\none-third of subjects). Our work bridges dynamical systems theory and clinical\nneuroscience, providing a principled approach to optimize mental health\ntreatments by targeting the most efficient neural pathways for cognitive\nchange.",
        "Long-term efficacy of internal globus pallidus (GPi) deep-brain stimulation\n(DBS) in DYT1 dystonia and disease progression under DBS was studied.\nTwenty-six patients of this open-label study were divided into two groups: (A)\nwith single bilateral GPi lead, (B) with a second bilateral GPi lead implanted\nowning to subsequent worsening of symptomatology. Dystonia was assessed with\nthe Burke Scale. Appearance of new symptoms and distribution according to body\nregion were recorded. In the whole cohort, significant decreases in motor and\ndisability subscores (P < 0.0001) were observed at 1 year and maintained up to\n10 years. Group B showed worsening of the symptoms. At 1 year, there were no\nsignificant differences between Groups A (without subsequent worsening) and B;\nat 5 years, a significant difference was found for motor and disability scores.\nWithin Group B, four patients exhibited additional improvement after the second\nDBS surgery. In the 26 patients, significant difference (P = 0.001) was found\nbetween the number of body regions affected by dystonia preoperatively and over\nthe whole follow-up. DBS efficacy in DYT1 dystonia can be maintained up to 10\nyears (two patients). New symptoms appear with long-term follow-up and may\nimprove with additional leads in a subgroup of patients.",
        "In this 2022 work we argued that, despite claims about successful modeling of\nthe visual brain using artificial nets, the problem is far from being solved\n(even for low-level vision). Examples of open issues include: where should we\nread from ANNs in order to reproduce human behavior?, this ad-hoc read-out is\nconsidered part of the brain model or not?, should we use artificial\npsychophysics or artificial physiology?, in the case of ANNs, artificial\nexperiments should literally match the experiments done with humans?. There is\na clear need of rigorous procedures for experimental tests for ANNs devoted to\nmodel the visual brain, and more generally, to understand ANNs devoted to\ngeneric vision tasks. Following our experience in using low-level facts from\nQuantitative Visual Neuroscience in computer vision, in this work we presented\nthe idea of developing a low-level dataset compiling the basic spatio-temporal\nand chromatic facts that are known to happen in the retina-V1 pathway, and they\nare not currently available in existing databases such as BrainScore. In our\nresults we checked the behavior of three recently proposed models with similar\narchitecture: (1) A parametric model tuned via Maximum Differentiation [Malo &\nSimoncelli SPIE 15, Martinez et al. PLOS 18, Martinez et al. Front. Neurosci.\n19], (2) A non-parametric model called PerceptNet tuned to maximize the\ncorrelation with human opinion on subjective distortions [Hepburn et al. IEEE\nICIP 19], and (3) A model with the same encoder as PerceptNet, but tuned for\nimage segmentation (published as Hernandez-Camara et al. Patt.Recogn.Lett. 23).\nResults on 10 compelling psycho\/physio visual facts show that the first model\nis the one with closer behavior to the humans in terms of receptive fields, but\nmore interestingly, on the nonlinear behavior when facing complex\nspatio-chromatic patterns of a range of luminances and contrasts.",
        "In the field of brain-machine interfaces, biohybrids offer an interesting new\nperspective, as in them, the technological side acts like a closed-loop\nextension or real counterpart of biological tissue, instead of the usual open\nloop approaches in tranditional BMI. To achieve a credible counterpart to\nbiological tissue, biohybrids usually employ one or several neuromorphic\ncomponents as the hardware half of the biohybrid. However, advanced\nneuromorphic circuit such as memristor crossbars usually operate best in a\ndedicated lab with corresponding support equipment. The same is true for\nbiological tissue, which makes co-locating all of the parts of a biohybrid in\nthe same lab challenging. Here, we present as solution to this co-location\nissue a simple method to connect biohybrids via the internet by a custom UDP\npacket format. We show that the characteristics achieved with our solution\n(jitter, delay, packet loss, packet reordering) on a standard internet\nconnection are compatible with various biohybrid processing paradigms, and we\npresent a short three-ways experiment as proof-of-concept. The described UDP\nformat has been employed to link biohybrids and neuromorphic circuits in four\ndifferent EC-funded projects.",
        "Lateral inhibition models coupled with Hebbian plasticity have been shown to\nlearn factorised causal representations of input stimuli, for instance,\noriented edges are learned from natural images. Currently, these models require\nthe recurrent dynamics to settle into a stable state before weight changes can\nbe applied, which is not only biologically implausible, but also impractical\nfor real-time learning systems. Here, we propose a new Hebbian learning rule\nwhich is implemented using plausible biological mechanisms that have been\nobserved experimentally. We find that this rule allows for efficient,\ntime-continuous learning of factorised representations, very similar to the\nclassic noncontinuous Hebbian\/anti-Hebbian learning. Furthermore, we show that\nthis rule naturally prevents catastrophic forgetting when stimuli from\ndifferent distributions are shown sequentially.",
        "One of the central aims of neuroscience is to reliably predict the behavioral\nresponse of an organism using its neural activity. If possible, this implies we\ncan causally manipulate the neural response and design brain-computer-interface\nsystems to alter behavior, and vice-versa. Hence, predictions play an important\nrole in both fundamental neuroscience and its applications. Can we predict the\nneural and behavioral states of an organism at any given time? Can we predict\nbehavioral states using neural states, and vice-versa, and is there a\nmemory-component required to reliably predict such states? Are the predictions\ncomputable within a given timescale to meaningfully stimulate and make the\nsystem reach the desired states? Through a series of mathematical treatments,\nsuch conjectures and questions are discussed. Answering them might be key for\nfuture developments in understanding intelligence and designing\nbrain-computer-interfaces.",
        "The evolution of cognition is frequently discussed as the evolution of\ncognitive abilities or the evolution of some neuronal structures in the brain.\nHowever, since such traits or abilities are often highly complex, understanding\ntheir evolution requires explaining how they could have gradually evolved\nthrough selection acting on heritable variations in simpler cognitive\nmechanisms. With this in mind, making use of a previously proposed theory, here\nwe show how the evolution of cognitive abilities can be captured by the\nfine-tuning of basic learning mechanisms and, in particular, chunking\nmechanisms. We use the term chunking broadly for all types of non-elemental\nlearning, claiming that the process by which elements are combined into chunks\nand associated with other chunks, or elements, is critical for what the brain\ncan do, and that it must be fine-tuned to ecological conditions. We discuss the\nrelevance of this approach to studies in animal cognition, using examples from\nanimal foraging and decision-making, problem solving, and cognitive\nflexibility. Finally, we explain how even the apparent human-animal gap in\nsequence learning ability can be explained in terms of different fine-tunings\nof a similar chunking process.",
        "Chronic pain is a significant global health issue, with many patients\nexperiencing persistent pain despite no identifiable organic cause, classified\nas nociplastic pain. Increasing evidence highlights the role of danger signal\nprocessing in the maintenance of chronic pain. In response, we developed\nPersonal Danger Signals Reprocessing (PDSR), an online, group-based\nintervention designed to modify these mechanisms using coaching techniques to\nenhance accessibility and affordability.\n  This study evaluated the efficacy of PDSR in reducing pain and mental health\ncomorbidities. A cohort of women (N=19, mean age 43) participated in an 8-week\nonline program, receiving weekly sessions on chronic pain mechanisms within a\nsystemic framework. Outcomes were assessed at three time points:\npre-intervention, mid-intervention, and post-intervention. A waiting list group\n(N=20, mean age 43.5) completed assessments at the same intervals.\n  Participants in the PDSR group showed significant pain reduction (p < .001),\nwith moderate to large effects observed at mid-intervention (Cohen's D = 0.7)\nand post-intervention (Cohen's D = 1.5) compared to controls. Pain interference\nsignificantly decreased (p < .01), with large reductions in the PDSR group\n(Cohen's D = -1.7, p < .0001). Well-being also improved substantially (p <\n.001, Cohen's D = 1.7-1.8). Secondary outcomes, including pain catastrophizing,\nsleep interference, anxiety, and depressive symptoms, consistently improved\n(all p-values < .01).\n  Findings suggest PDSR is an effective, scalable intervention for reducing\npain, improving function, and enhancing well-being in individuals with chronic\npain.",
        "We present the case for developing the tardigrade (Hypsibius exemplaris) into\na model organism for systems neuroscience. These microscopic, transparent\nanimals (~300-500 microns) are among the smallest known to possess both limbs\n(eight) and eyes (two), with a nervous system of only a few hundred neurons\norganized into a multi-lobed brain, ventral nerve cord, and a series of ganglia\nalong the body. Despite their neuroanatomical simplicity, tardigrades exhibit\ncomplex behaviors, including multi-limbed walking gaits, individual limb\ngrasping, phototaxis, and transitions between active and dormant states. These\nbehaviors position tardigrades as a uniquely powerful system for addressing\ncertain fundamental questions in systems neuroscience, such as: How do nervous\nsystems coordinate multi-limbed behaviors? How are top-down and bottom-up motor\ncontrol systems integrated? How is stereovision-guided navigation implemented?\nWhat mechanisms underlie neural resilience and recovery during environmental\nstress? We review current knowledge of tardigrade neuroanatomy, behavior, and\ngenomics, and we identify opportunities and challenges for leveraging their\nunique biology. We propose developing essential neuroscientific tools for\ntardigrades, including genetic engineering and live neuroimaging, alongside\nbehavioral assays linking neural activity to outputs. Leveraging their\nevolutionary ties to Caenorhabditis elegans and Drosophila melanogaster, we can\nadapt existing toolkits to accelerate tardigrade research - providing a bridge\nbetween simpler invertebrate systems and more complex neural architectures.",
        "\"Intrinsic motivation\" refers to the capacity for intelligent systems to be\nmotivated endogenously, i.e. by features of agential architecture itself rather\nthan by learned associations between action and reward. This paper views active\ninference, empowerment, and other formal accounts of intrinsic motivation as\nvariations on the theme of constrained maximum entropy inference, providing a\ngeneral perspective on intrinsic motivation complementary to existing\nframeworks. The connection between free energy and empowerment noted in\nprevious literature is further explored, and it is argued that the\nmaximum-occupancy approach in practice incorporates an implicit model-evidence\nconstraint.",
        "The jet axis decorrelation in inclusive jets is studied using lead-lead\n(PbPb) collisions at a center-of-mass energy per nucleon pair of 5.02 TeV. The\njet axis decorrelation is defined as the angular difference between two\ndefinitions of the jet axis. It is obtained by applying two recombination\nschemes on all the constituents of a given jet reconstructed by the anti-\\kt\nsequential algorithm with a distance parameter of $R$ = 0.4. The data set,\ncorresponding to an integrated luminosity of 0.66 nb$^{-1}$, was collected in\n2018 with the CMS detector at the CERN LHC. The jet axis decorrelations are\nexamined across collision centrality selections and intervals of jet transverse\nmomentum. A centrality dependent evolution of the measured distributions is\nobserved, with a progressive narrowing seen in more central events. This\nnarrowing could result from medium-induced modification of the internal jet\nstructure or reflect color charge effects in energy loss. This new measurement\nprobes jet substructure in previously unexplored kinematic domains and show\ngreat promise for providing new insights on the color charge dependence of\nenergy loss to jet-quenching models.",
        "We establish a theorem regarding the maximum size of an {\\it{induced}}\nmatching in the bipartite complement of the incidence graph of a set system\n$(X,\\mathcal{F})$. We show that this quantity plus one provides an upper bound\non the colorful Helly number of this set system, i.e. the minimum positive\ninteger $N$ for which the following statement holds: if finite subfamilies\n$\\mathcal{F}_1,\\ldots, \\mathcal{F}_{N} \\subset \\mathcal{F}$ are such that\n$\\cap_{F \\in \\mathcal{F}_{i}} F = 0$ for every $i=1,\\ldots,N$, then there\nexists $F_i \\in \\mathcal{F}_i$ such that $F_1 \\cap \\ldots \\cap F_{N} =\n\\emptyset$. We will also discuss some natural refinements of this result and\napplications.",
        "Sliding ferroelectrics constructed from stacked nonpolar monolayers enable\nout-of-plane polarization in two dimensions with exceptional properties,\nincluding ultrafast switching speeds and fatigue-free behavior. However, the\nwidely accepted switching mechanism, which posits synchronized long-distance\nin-plane translation of entire atomic layers driven by an out-of-plane electric\nfield, has shown inconsistencies with experimental observations. We demonstrate\nthat this spinodal decomposition-like homogeneous switching process violates\nNeumann's principle and is unlikely to occur due to symmetry constraint.\nInstead, symmetry-breaking domain walls (DWs) and the tensorial nature of Born\neffective charges are critical for polarization reversal, underscoring the\nquantum nature of sliding ferroelectrics. Using the Bernal-stacked $h$-BN\nbilayer as a model system, we discover that the coherent propagation of wide,\nwave-like domain walls is the key mechanism for ferroelectric switching. This\nmechanism fundamentally differs from the layer-by-layer switching associated\nwith narrow domain walls, which has been established for over sixty years in\nperovskite ferroelectrics. Moreover, these wave-like DWs exhibit superlubric\ndynamics, achieving ultrahigh velocities of approximately 4000 m\/s at room\ntemperature and displaying an anomalous cooling-promoted switching speed. The\nunexpected emergence of DW superlubricity in sliding ferroelectrics presents\nnew avenues for enhancing key performance metrics and offers exciting\nopportunities for applications in cryogenic environments.",
        "String-net models describe a vast family of topological orders in two spatial\ndimensions, but fail to produce all the expected anyonic excitations. Following\narXiv:1502.03433, we consider an extended string-net model by attaching one\ntail to each plaquette of the lattice, allowing all anyons to emerge as\nelementary plaquette excitations for arbitrary input categories. The\ncorresponding tube algebra is the mathematical tool needed to construct the\nanyons from the input category and to obtain their internal multiplicities. We\nuse them to compute the energy level degeneracies and the partition function.\nIn the thermodynamic limit, the latter is dominated by the trivial (vacuum)\nanyon, so that the topological order is destroyed at any non-zero temperature.\nIn a finite-size system, order survives up to a finite temperature, similarly\nto the one-dimensional classical Ising model. We confirm this by computing\nthermal averages of topological projectors, Wegner-Wilson loops and the\ntopological mutual information. The results are also generalized to models with\nmultiple tails per plaquette.",
        "Based on the entropy of anti-de Sitter black hole, a new holographic dark\nenergy model has been proposed. When the Hubble horizon and particle horizon\nare chosen as the IR cutoff, the late-time accelerated expansion of universe is\nrealized. In this paper, we consider the Hubble horizon as the IR cutoff to\ninvestigate holographic inflation and slow-roll inflation in this model. We\nfind that slow-roll inflation with the chaotic potential $V_{0}\\phi^{n}$ is\nfavored by Planck results for some special cases, such as $n=1\/3$ and $n=1\/2$,\nwhile holographic inflation is not supported by Planck results. Then, we\nanalyze the reheating temperature and the number of reheating e-folds in this\nmodel, and we find that the results favor the cases $n=1\/3$ and $n=1\/2$.\nFinally, we use the dynamical analysis method, statefinder diagnostic pairs,\nand the Hubble diagram to analyze this model. Our results indicate that when\n$b^{2}$ takes a small value, this model cannot be distinguished from the\nstandard $\\Lambda$CDM model and can serve as an alternative to it.",
        "We explain the linear algebraic framework provided by Tate modules of\nisogenous abelian varieties in a category-theoretic way.",
        "Recent advances in time-resolved angle-resolved photoemission spectroscopy\nhave enabled access to ultrafast electron states and their spin dynamics in\nsolids. Atomically thin transition metal dichalcogenides are paradigmatic\ntwo-dimensional materials where electron momentum and spin degrees of freedom\nare coupled, being suitable candidates for time-resolved spectroscopy studies.\nIn this work, we present a thorough study of the electron dynamics when these\nmaterials are subject to an intense finite-pulse driving radiation. We extend\nthe scope of the conventional Floquet engineering and rely of the so-called\n$t-t^{\\prime}$ formalism to deal with driving fields described with two\ndistinct time scales, namely the envelope amplitude timescale and the time\nperiod of the external field. The interplay between the finite-pulse timescales\nand the intrinsic properties of the electrons gives rise to transient valley\npolarization and dynamical modifications of band structures, revealed by the\ntime-dependent circular dichroism of the sample.",
        "Powered descent guidance (PDG) problems subject to six-degrees-of-freedom\n(6DOF) dynamics allow for enforcement of practical attitude constraints.\nHowever, numerical solutions to 6DOF PDG problems are challenging due to fast\nrotational dynamics coupled with translational dynamics, and the presence of\nhighly nonlinear state\/control path inequality constraints. In this work,\nconstrained fuel- and time-optimal 6DOF PDG problems are solved leveraging a\nregularized indirect method, subject to inequality constraints on the thrust\nmagnitude, thruster gimbal angle, rocket tilt angle, glideslope angle, and\nangular velocity magnitude. To overcome the challenges associated with solving\nthe resulting multipoint boundary-value problems (MPBVPs), the state-only path\ninequality constraints (SOPICs) are enforced through an interior penalty\nfunction method, which embeds the resulting MPBVPs into a multi-parameter\nsmooth neighboring families of two-point BVPs. Extremal solutions are obtained\nusing an indirect multiple-shooting solution method with numerical\ncontinuation. Moreover, an empirical relation is derived for the\ndirectly-adjoined Lagrange multipliers associated with SOPICs. The fuel- and\ntime-optimal trajectories are compared against solutions of DIDO -- a capable\npseudospectral-based software for solving practical constrained optimal control\nproblems.",
        "One of the key issues in quantum discrimination problems is understanding the\nextent of the advantages in discrimination performance when using resource\nstates compared to resourceless states. We show that in any resource theory of\nstates, which may not be convex, the extent to which the maximum average\nsuccess probability can be improved in quantum channel discrimination problems\nwithout using auxiliary systems can be precisely quantified by the robustness\nmeasure. Furthermore, we demonstrate that the robustness measure can also\nquantify the improvement in channel discrimination problems that use auxiliary\nsystems. Using these findings, resources can be fully characterized to achieve\nhigher success probabilities than any state without the given resource in\nchannel discrimination problems.",
        "As waves propagate through a complex medium, they undergo multiple scattering\nevents. This phenomenon is detrimental to imaging, as it causes a full blurring\nof the image beyond a transport mean free path. Here, we show how to detect,\nlocalize, and characterize any scattering target through the reflection matrix\nof the complex medium in which this target is embedded and thus hidden from\ndirect view. More precisely, we introduce a fingerprint operator that contains\nthe specific signature of the target with respect to its environment. Applied\nto the recorded reflection matrix, this operator provides a likelihood index of\nthe target in any given state, despite the scattering fog induced by the\nsurrounding environment. This state can be the target position for localization\npurposes, its shape for characterization, or any other parameter that\ninfluences the target response. Our concept is versatile and broadly applicable\nto different type of waves for which multi-element technology allows a\nreflection matrix to be measured. We demonstrate this here explicitly by\nperforming different proof-of-concept experiments with ultrasound on targets\nburied inside a strongly scattering granular suspension, on lesion markers for\nclinical applications, and on the architecture of muscle tissue.",
        "We extract the equation of state for hot quark matter from a holographic\n$2+1$ flavor QCD model, which could form the core of a stable compact star. By\nadding a thin hadron shell, a new type of hybrid star is constructed. With the\ntemperature serving as a parameter, the EoS varies and we obtain stable stars\nwith the maximum mass of around 23 to 30 solar masses, and the compactness\naround $0.1$. The I-Love-Q-C relations are further discussed, and compared with\nthe neutron star cases. These compact stars are candidates for black hole\nmimickers, which could be observed by gravitational waves and distinguished by\nproperties like nonzero tidal Love number and electromagnetic signals.",
        "We consider the following question: how close to the ancestral root of a\nphylogenetic tree is the most recent common ancestor of $k$ species randomly\nsampled from the tips of the tree? For trees having shapes predicted by the\nYule-Harding model, it is known that the most recent common ancestor is likely\nto be close to (or equal to) the root of the full tree, even as $n$ becomes\nlarge (for $k$ fixed). However, this result does not extend to models of tree\nshape that more closely describe phylogenies encountered in evolutionary\nbiology. We investigate the impact of tree shape (via the Aldous\n$\\beta-$splitting model) to predict the number of edges that separate the most\nrecent common ancestor of a random sample of $k$ tip species and the root of\nthe parent tree they are sampled from. Both exact and asymptotic results are\npresented. We also briefly consider a variation of the process in which a\nrandom number of tip species are sampled.",
        "We develop an approach to generate random graphs to a target level of\nassortativity by using copula structures in graphons. Unlike existing random\ngraph generators, we do not use rewiring or binning approaches to generate the\ndesired random graph. Instead, we connect Archimedean bivariate copulas to\ngraphons in order to produce flexible models that can generate random graphs to\ntarget assortativity. We propose three models that use the copula distribution\nfunction, copula density function and their mixed tensor product to produce\nnetworks. We express the assortativity coefficient in terms of homomorphism\ndensities. Establishing this relationship forges a connection between the\nparameter of the copula and the frequency of subgraphs in the generated\nnetwork. Therefore, our method attains a desired the subgraph distribution as\nwell as the target assortativity. We establish the homomorphism densities and\nassortativity coefficient for each of the models. Numerical examples\ndemonstrate the ability of the proposed models to produce graphs with different\nlevels of assortativity.",
        "The James Webb Space Telescope (JWST) has recently conducted observations of\nmassive galaxies at high redshifts, revealing a notable anomaly in their star\nformation efficiency (SFE). Motivated by the recent identification of three\n$\\sim 10^{6}M_\\odot$ dark star candidates, we investigate whether dark stars\ncan be the origin of the SFE excess. It turns out that the excess can be\nreproduced by a group of dark stars with $M \\gtrsim 10^{3}\\, \\rm M_{\\odot}$,\nbecause of their domination in generating primary UV radiation in high-redshift\ngalaxies. The genesis of these dark stars is attributed to the capture of\nWeakly Interacting Massive Particles (WIMPs) within a mass range of tens of GeV\nto a few TeV. However, if the top-heavy initial mass function of dark stars\nholds up to $\\sim 10^{5}M_\\odot$, the relic black holes stemming from their\ncollapse would be too abundant to be consistent with the current observations\nof Massive Compact Halo Objects (MACHOs). We thus suggest that just a small\nfraction of SFE excess may be contributed by the very massive dark stars and\nthe majority likely originated from other reasons such as the Population III\nstars in view of their rather similar UV radiation efficiencies.",
        "COCOA (COmpact COmpton cAmera) is a next-generation, cost-effective gamma-ray\ntelescope designed for astrophysical observations in the MeV energy range. The\ndetector comprises a scatterer volume employing the LiquidO detection\ntechnology and an array of scintillating crystals acting as absorber.\nSurrounding plastic scintillator panels serve as a veto system for charged\nparticles. The detector's compact, scalable design enables flexible deployment\non microsatellites or high-altitude balloons. Gamma rays at MeV energies have\nnot been well explored historically (the so-called \"MeV gap\") and COCOA has the\npotential to improve the sensitivity in this energy band by up to two orders of\nmagnitude."
      ]
    }
  },
  {
    "id":2411.17717,
    "research_type":"applied",
    "start_id":"b5",
    "start_title":"Quantitative EEG analysis disease during resting and memory task in carriers and non-carriers of PS-1 E280A mutation of familial Alzheimer's",
    "start_abstract":"Background: Alzheimer\u2019s disease is the most leading cause of dementia in world; mutation PS-1 E280A alters gene Presenilin-1 and causes an early onset familial disease. This has been found large kindred Antioquia, Colombia. The objective this study was to find differences revealed by electroencephalogram between healthy subjects asymptomatic carriers that can be used as clinical markers population. Methods: EEG recorded 15 non during resting a memory task using 64 channels amplifier. Two conditions were analyzed: encoding retrieval, process recording evocating information, respectively. Power spectrum calculated delta (0.5\u20134.0 Hz), theta (4.0\u20138. 0 alpha-1 (8.0\u201310.0 alpha-2 (10.0\u201313.0 beta (13.0\u201325.0 Hz) gamma (25.0\u201350 frequency bands for four regions interest. Changes evaluated different ANOVA analysis. Results: In condition significant decrease (p=0. 0001) increase frequencies (p=0.037) compare with controls. During significantly lower compared controls 008) comparing versus retrieval each group, there more synchronization carriers. Conclusion: Early changes observed recordings, it could use Also seems activate additional cortical order conserve successful cognitive functions before impairment .",
    "start_categories":[
      "q-bio.NC"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b4"
      ],
      "title":[
        "Neural biomarker diagnosis and prediction to mild cognitive impairment and Alzheimer\u2019s disease using EEG technology"
      ],
      "abstract":[
        "Background Electroencephalogram (EEG) has emerged as a non-invasive tool to detect the aberrant neuronal activity related to different stages of Alzheimer\u2019s disease (AD). However, the effectiveness of EEG in the precise diagnosis and assessment of AD and its preclinical stage, amnestic mild cognitive impairment (MCI), has yet to be fully elucidated. In this study, we aimed to identify key EEG biomarkers that are effective in distinguishing patients at the early stage of AD and monitoring the progression of AD. Methods A total of 890 participants, including 189 patients with MCI, 330 patients with AD, 125 patients with other dementias (frontotemporal dementia, dementia with Lewy bodies, and vascular cognitive impairment), and 246 healthy controls (HC) were enrolled. Biomarkers were extracted from resting-state EEG recordings for a three-level classification of HC, MCI, and AD. The optimal EEG biomarkers were then identified based on the classification performance. Random forest regression was used to train a series of models by combining participants\u2019 EEG biomarkers, demographic information (i.e., sex, age), CSF biomarkers, and APOE phenotype for assessing the disease progression and individual\u2019s cognitive function. Results The identified EEG biomarkers achieved over 70% accuracy in the three-level classification of HC, MCI, and AD. Among all six groups, the most prominent effects of AD-linked neurodegeneration on EEG metrics were localized at parieto-occipital regions. In the cross-validation predictive analyses, the optimal EEG features were more effective than the CSF + APOE biomarkers in predicting the age of onset and disease course, whereas the combination of EEG + CSF + APOE measures achieved the best performance for all targets of prediction. Conclusions Our study indicates that EEG can be used as a useful screening tool for the diagnosis and disease progression evaluation of MCI and AD."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "MAGELLAN: Metacognitive predictions of learning progress guide autotelic\n  LLM agents in large goal spaces",
        "Causal Discovery and Inference towards Urban Elements and Associated\n  Factors",
        "AI Will Always Love You: Studying Implicit Biases in Romantic AI\n  Companions",
        "WOFOSTGym: A Crop Simulator for Learning Annual and Perennial Crop\n  Management Strategies",
        "NeSyC: A Neuro-symbolic Continual Learner For Complex Embodied Tasks In\n  Open Domains",
        "ILLC: Iterative Layer-by-Layer Compression for Enhancing Structural\n  Faithfulness in SpArX",
        "Benchmarking Reasoning Robustness in Large Language Models",
        "Wider or Deeper? Scaling LLM Inference-Time Compute with Adaptive\n  Branching Tree Search",
        "Don't Get Too Excited -- Eliciting Emotions in LLMs",
        "LLM-based Corroborating and Refuting Evidence Retrieval for Scientific\n  Claim Verification",
        "STMA: A Spatio-Temporal Memory Agent for Long-Horizon Embodied Task\n  Planning",
        "Bridging the Communication Gap: Evaluating AI Labeling Practices for\n  Trustworthy AI Development",
        "Initial Findings on Sensor based Open Vocabulary Activity Recognition\n  via Text Embedding Inversion",
        "CryptoX : Compositional Reasoning Evaluation of Large Language Models",
        "Invariant measure for the process viewed from the particle for 2D random\n  walks in Dirichlet environment",
        "Observation-Based Iterative Map for Solar Cycles. II. The Gnevyshev-Ohl\n  Rule and its Generation Mechanism",
        "Adaptive Variational Inference in Probabilistic Graphical Models: Beyond\n  Bethe, Tree-Reweighted, and Convex Free Energies",
        "On a class of high dimensional linear regression methods with debiasing\n  and thresholding",
        "Dynamic Refinement of Pressure Decomposition in Navier-Stokes Equations",
        "Homological data on the periodic structure of self-maps on wedge sums",
        "Making Them a Malicious Database: Exploiting Query Code to Jailbreak\n  Aligned Large Language Models",
        "Robust Multimodal Learning via Cross-Modal Proxy Tokens",
        "In-Context Meta LoRA Generation",
        "TinySense: A Lighter Weight and More Power-efficient Avionics System for\n  Flying Insect-scale Robots",
        "Advancing Dense Endoscopic Reconstruction with Gaussian Splatting-driven\n  Surface Normal-aware Tracking and Mapping",
        "Comprehensive Analog Signal Processing Platform Enabled with Acoustic\n  Charge Transport in Two-dimensional Materials",
        "A Comparative Performance Analysis of Classification and Segmentation\n  Models on Bangladeshi Pothole Dataset",
        "ODPG: Outfitting Diffusion with Pose Guided Condition"
      ],
      "abstract":[
        "Open-ended learning agents must efficiently prioritize goals in vast\npossibility spaces, focusing on those that maximize learning progress (LP).\nWhen such autotelic exploration is achieved by LLM agents trained with online\nRL in high-dimensional and evolving goal spaces, a key challenge for LP\nprediction is modeling one's own competence, a form of metacognitive\nmonitoring. Traditional approaches either require extensive sampling or rely on\nbrittle expert-defined goal groupings. We introduce MAGELLAN, a metacognitive\nframework that lets LLM agents learn to predict their competence and LP online.\nBy capturing semantic relationships between goals, MAGELLAN enables\nsample-efficient LP estimation and dynamic adaptation to evolving goal spaces\nthrough generalization. In an interactive learning environment, we show that\nMAGELLAN improves LP prediction efficiency and goal prioritization, being the\nonly method allowing the agent to fully master a large and evolving goal space.\nThese results demonstrate how augmenting LLM agents with a metacognitive\nability for LP predictions can effectively scale curriculum learning to\nopen-ended goal spaces.",
        "To uncover the city's fundamental functioning mechanisms, it is important to\nacquire a deep understanding of complicated relationships among citizens,\nlocation, and mobility behaviors. Previous research studies have applied direct\ncorrelation analysis to investigate such relationships. Nevertheless, due to\nthe ubiquitous confounding effects, empirical correlation analysis may not\naccurately reflect underlying causal relationships among basic urban elements.\nIn this paper, we propose a novel urban causal computing framework to\ncomprehensively explore causalities and confounding effects among a variety of\nfactors across different types of urban elements. In particular, we design a\nreinforcement learning algorithm to discover the potential causal graph, which\ndepicts the causal relations between urban factors. The causal graph further\nserves as the guidance for estimating causal effects between pair-wise urban\nfactors by propensity score matching. After removing the confounding effects\nfrom correlations, we leverage significance levels of causal effects in\ndownstream urban mobility prediction tasks. Experimental studies on open-source\nurban datasets show that the discovered causal graph demonstrates a\nhierarchical structure, where citizens affect locations, and they both cause\nchanges in urban mobility behaviors. Experimental results in urban mobility\nprediction tasks further show that the proposed method can effectively reduce\nconfounding effects and enhance performance of urban computing tasks.",
        "While existing studies have recognised explicit biases in generative models,\nincluding occupational gender biases, the nuances of gender stereotypes and\nexpectations of relationships between users and AI companions remain\nunderexplored. In the meantime, AI companions have become increasingly popular\nas friends or gendered romantic partners to their users. This study bridges the\ngap by devising three experiments tailored for romantic, gender-assigned AI\ncompanions and their users, effectively evaluating implicit biases across\nvarious-sized LLMs. Each experiment looks at a different dimension: implicit\nassociations, emotion responses, and sycophancy. This study aims to measure and\ncompare biases manifested in different companion systems by quantitatively\nanalysing persona-assigned model responses to a baseline through newly devised\nmetrics. The results are noteworthy: they show that assigning gendered,\nrelationship personas to Large Language Models significantly alters the\nresponses of these models, and in certain situations in a biased, stereotypical\nway.",
        "We introduce WOFOSTGym, a novel crop simulation environment designed to train\nreinforcement learning (RL) agents to optimize agromanagement decisions for\nannual and perennial crops in single and multi-farm settings. Effective crop\nmanagement requires optimizing yield and economic returns while minimizing\nenvironmental impact, a complex sequential decision-making problem well suited\nfor RL. However, the lack of simulators for perennial crops in multi-farm\ncontexts has hindered RL applications in this domain. Existing crop simulators\nalso do not support multiple annual crops. WOFOSTGym addresses these gaps by\nsupporting 23 annual crops and two perennial crops, enabling RL agents to learn\ndiverse agromanagement strategies in multi-year, multi-crop, and multi-farm\nsettings. Our simulator offers a suite of challenging tasks for learning under\npartial observability, non-Markovian dynamics, and delayed feedback.\nWOFOSTGym's standard RL interface allows researchers without agricultural\nexpertise to explore a wide range of agromanagement problems. Our experiments\ndemonstrate the learned behaviors across various crop varieties and soil types,\nhighlighting WOFOSTGym's potential for advancing RL-driven decision support in\nagriculture.",
        "We explore neuro-symbolic approaches to generalize actionable knowledge,\nenabling embodied agents to tackle complex tasks more effectively in\nopen-domain environments. A key challenge for embodied agents is the\ngeneralization of knowledge across diverse environments and situations, as\nlimited experiences often confine them to their prior knowledge. To address\nthis issue, we introduce a novel framework, NeSyC, a neuro-symbolic continual\nlearner that emulates the hypothetico-deductive model by continually\nformulating and validating knowledge from limited experiences through the\ncombined use of Large Language Models (LLMs) and symbolic tools. Specifically,\nwe devise a contrastive generality improvement scheme within NeSyC, which\niteratively generates hypotheses using LLMs and conducts contrastive validation\nvia symbolic tools. This scheme reinforces the justification for admissible\nactions while minimizing the inference of inadmissible ones. Additionally, we\nincorporate a memory-based monitoring scheme that efficiently detects action\nerrors and triggers the knowledge refinement process across domains.\nExperiments conducted on diverse embodied task benchmarks-including ALFWorld,\nVirtualHome, Minecraft, RLBench, and a real-world robotic scenario-demonstrate\nthat NeSyC is highly effective in solving complex embodied tasks across a range\nof open-domain environments.",
        "In the field of Explainable Artificial Intelligence (XAI), argumentative XAI\napproaches have been proposed to represent the internal reasoning process of\ndeep neural networks in a more transparent way by interpreting hidden nodes as\narguements. However, as the number of layers increases, existing compression\nmethods simplify all layers at once, which lead to high accumulative\ninformation loss. To compensate for this, we propose an iterative\nlayer-by-layer compression technique in which each layer is compressed\nseparately and the reduction error in the next layer is immediately compensated\nfor, thereby improving the overall input-output and structural fidelity of the\nmodel. Experiments on the Breast Cancer Diagnosis dataset show that, compared\nto traditional compression, the method reduces input-output and structural\nunfaithfulness, and maintains a more consistent attack-support relationship in\nthe Argumentative Explanation scheme. This is significant because it provides a\nnew way to make complex MLP models more compact while still conveying their\ninternal inference logic without distortion.",
        "Despite the recent success of large language models (LLMs) in reasoning such\nas DeepSeek, we for the first time identify a key dilemma in reasoning\nrobustness and generalization: significant performance degradation on novel or\nincomplete data, suggesting a reliance on memorized patterns rather than\nsystematic reasoning. Our closer examination reveals four key unique\nlimitations underlying this issue:(1) Positional bias--models favor earlier\nqueries in multi-query inputs but answering the wrong one in the latter (e.g.,\nGPT-4o's accuracy drops from 75.8 percent to 72.8 percent); (2) Instruction\nsensitivity--performance declines by 5.0 to 7.5 percent in the Qwen2.5 Series\nand by 5.0 percent in DeepSeek-V3 with auxiliary guidance; (3) Numerical\nfragility--value substitution sharply reduces accuracy (e.g., GPT-4o drops from\n97.5 percent to 82.5 percent, GPT-o1-mini drops from 97.5 percent to 92.5\npercent); and (4) Memory dependence--models resort to guesswork when missing\ncritical data. These findings further highlight the reliance on heuristic\nrecall over rigorous logical inference, demonstrating challenges in reasoning\nrobustness. To comprehensively investigate these robustness challenges, this\npaper introduces a novel benchmark, termed as Math-RoB, that exploits\nhallucinations triggered by missing information to expose reasoning gaps. This\nis achieved by an instruction-based approach to generate diverse datasets that\nclosely resemble training distributions, facilitating a holistic robustness\nassessment and advancing the development of more robust reasoning frameworks.\nBad character(s) in field Abstract.",
        "Recent advances demonstrate that increasing inference-time computation can\nsignificantly boost the reasoning capabilities of large language models (LLMs).\nAlthough repeated sampling (i.e., generating multiple candidate outputs) is a\nhighly effective strategy, it does not leverage external feedback signals for\nrefinement, which are often available in tasks like coding. In this work, we\npropose $\\textit{Adaptive Branching Monte Carlo Tree Search (AB-MCTS)}$, a\nnovel inference-time framework that generalizes repeated sampling with\nprincipled multi-turn exploration and exploitation. At each node in the search\ntree, AB-MCTS dynamically decides whether to \"go wider\" by expanding new\ncandidate responses or \"go deeper\" by revisiting existing ones based on\nexternal feedback signals. We evaluate our method on complex coding and\nengineering tasks using frontier models. Empirical results show that AB-MCTS\nconsistently outperforms both repeated sampling and standard MCTS, underscoring\nthe importance of combining the response diversity of LLMs with multi-turn\nsolution refinement for effective inference-time scaling.",
        "This paper investigates the challenges of affect control in large language\nmodels (LLMs), focusing on their ability to express appropriate emotional\nstates during extended dialogues. We evaluated state-of-the-art open-weight\nLLMs to assess their affective expressive range in terms of arousal and\nvalence. Our study employs a novel methodology combining LLM-based sentiment\nanalysis with multiturn dialogue simulations between LLMs. We quantify the\nmodels' capacity to express a wide spectrum of emotions and how they fluctuate\nduring interactions. Our findings reveal significant variations among LLMs in\ntheir ability to maintain consistent affect, with some models demonstrating\nmore stable emotional trajectories than others. Furthermore, we identify key\nchallenges in affect control, including difficulties in producing and\nmaintaining extreme emotional states and limitations in adapting affect to\nchanging conversational contexts. These findings have important implications\nfor the development of more emotionally intelligent AI systems and highlight\nthe need for improved affect modelling in LLMs.",
        "In this paper, we introduce CIBER (Claim Investigation Based on Evidence\nRetrieval), an extension of the Retrieval-Augmented Generation (RAG) framework\ndesigned to identify corroborating and refuting documents as evidence for\nscientific claim verification. CIBER addresses the inherent uncertainty in\nLarge Language Models (LLMs) by evaluating response consistency across diverse\ninterrogation probes. By focusing on the behavioral analysis of LLMs without\nrequiring access to their internal information, CIBER is applicable to both\nwhite-box and black-box models. Furthermore, CIBER operates in an unsupervised\nmanner, enabling easy generalization across various scientific domains.\nComprehensive evaluations conducted using LLMs with varying levels of\nlinguistic proficiency reveal CIBER's superior performance compared to\nconventional RAG approaches. These findings not only highlight the\neffectiveness of CIBER but also provide valuable insights for future\nadvancements in LLM-based scientific claim verification.",
        "A key objective of embodied intelligence is enabling agents to perform\nlong-horizon tasks in dynamic environments while maintaining robust\ndecision-making and adaptability. To achieve this goal, we propose the\nSpatio-Temporal Memory Agent (STMA), a novel framework designed to enhance task\nplanning and execution by integrating spatio-temporal memory. STMA is built\nupon three critical components: (1) a spatio-temporal memory module that\ncaptures historical and environmental changes in real time, (2) a dynamic\nknowledge graph that facilitates adaptive spatial reasoning, and (3) a\nplanner-critic mechanism that iteratively refines task strategies. We evaluate\nSTMA in the TextWorld environment on 32 tasks, involving multi-step planning\nand exploration under varying levels of complexity. Experimental results\ndemonstrate that STMA achieves a 31.25% improvement in success rate and a 24.7%\nincrease in average score compared to the state-of-the-art model. The results\nhighlight the effectiveness of spatio-temporal memory in advancing the memory\ncapabilities of embodied agents.",
        "As artificial intelligence (AI) becomes integral to economy and society,\ncommunication gaps between developers, users, and stakeholders hinder trust and\ninformed decision-making. High-level AI labels, inspired by frameworks like EU\nenergy labels, have been proposed to make the properties of AI models more\ntransparent. Without requiring deep technical expertise, they can inform on the\ntrade-off between predictive performance and resource efficiency. However, the\npractical benefits and limitations of AI labeling remain underexplored. This\nstudy evaluates AI labeling through qualitative interviews along four key\nresearch questions. Based on thematic analysis and inductive coding, we found a\nbroad range of practitioners to be interested in AI labeling (RQ1). They see\nbenefits for alleviating communication gaps and aiding non-expert\ndecision-makers, however limitations, misunderstandings, and suggestions for\nimprovement were also discussed (RQ2). Compared to other reporting formats,\ninterviewees positively evaluated the reduced complexity of labels, increasing\noverall comprehensibility (RQ3). Trust was influenced most by usability and the\ncredibility of the responsible labeling authority, with mixed preferences for\nself-certification versus third-party certification (RQ4). Our Insights\nhighlight that AI labels pose a trade-off between simplicity and complexity,\nwhich could be resolved by developing customizable and interactive labeling\nframeworks to address diverse user needs. Transparent labeling of resource\nefficiency also nudged interviewee priorities towards paying more attention to\nsustainability aspects during AI development. This study validates AI labels as\na valuable tool for enhancing trust and communication in AI, offering\nactionable guidelines for their refinement and standardization.",
        "Conventional human activity recognition (HAR) relies on classifiers trained\nto predict discrete activity classes, inherently limiting recognition to\nactivities explicitly present in the training set. Such classifiers would\ninvariably fail, putting zero likelihood, when encountering unseen activities.\nWe propose Open Vocabulary HAR (OV-HAR), a framework that overcomes this\nlimitation by first converting each activity into natural language and breaking\nit into a sequence of elementary motions. This descriptive text is then encoded\ninto a fixed-size embedding. The model is trained to regress this embedding,\nwhich is subsequently decoded back into natural language using a pre-trained\nembedding inversion model. Unlike other works that rely on auto-regressive\nlarge language models (LLMs) at their core, OV-HAR achieves open vocabulary\nrecognition without the computational overhead of such models. The generated\ntext can be transformed into a single activity class using LLM prompt\nengineering. We have evaluated our approach on different modalities, including\nvision (pose), IMU, and pressure sensors, demonstrating robust generalization\nacross unseen activities and modalities, offering a fundamentally different\nparadigm from contemporary classifiers.",
        "The compositional reasoning capacity has long been regarded as critical to\nthe generalization and intelligence emergence of large language models LLMs.\nHowever, despite numerous reasoning-related benchmarks, the compositional\nreasoning capacity of LLMs is rarely studied or quantified in the existing\nbenchmarks. In this paper, we introduce CryptoX, an evaluation framework that,\nfor the first time, combines existing benchmarks and cryptographic, to quantify\nthe compositional reasoning capacity of LLMs. Building upon CryptoX, we\nconstruct CryptoBench, which integrates these principles into several\nbenchmarks for systematic evaluation. We conduct detailed experiments on widely\nused open-source and closed-source LLMs using CryptoBench, revealing a huge gap\nbetween open-source and closed-source LLMs. We further conduct thorough\nmechanical interpretability experiments to reveal the inner mechanism of LLMs'\ncompositional reasoning, involving subproblem decomposition, subproblem\ninference, and summarizing subproblem conclusions. Through analysis based on\nCryptoBench, we highlight the value of independently studying compositional\nreasoning and emphasize the need to enhance the compositional reasoning\ncapabilities of LLMs.",
        "In this paper, we consider random walks in Dirichlet random environment\n(RWDE) on $\\mathbb{Z}^2$. We prove that, if the RWDE is recurrent (which is\nstrongly conjectured when the weights are symmetric), then there does not exist\nany invariant measure for the process viewed from the particle which is\nabsolutely continuous with respect to the static law of the environment.\nBesides, if the walk is directional transient and under condition\n$\\mathbf{(T')}$, we prove that there exists such an invariant probability\nmeasure if the trapping parameter verifies $\\kappa > 1$ or after acceleration\nof the process by a local function of the environment. This gives strong credit\nto a conjectural classification of cases of existence or non-existence of the\ninvariant measure for two dimensional RWDE. The proof is based on a new\nidentity, stated on general finite graphs, which is inspired by the\nrepresentation of the $\\star$-VRJP, a non-reversible generalization of the\nVertex reinforced Jump Process, in terms of random Schr\\\"odinger operators. In\nthe case of RWDE on 1D graph, the previous identity entails also a discrete\nanalogue of the Matsumoto-Yor property for Brownian motion.",
        "The Gnevyshev-Ohl (G-O) rule, also known as the even-odd effect, is an\nimportant observational phenomenon in solar cycles, suggesting that cycles with\neven indices tend to be followed by stronger cycles. The rule is considered to\nbe related to the solar dynamo, which drives the evolution of the Sun's\nlarge-scale magnetic field. However, observational studies of the G-O rule have\nrevealed inconsistencies, particularly regarding long-term variations and the\nunderlying physical mechanisms. In this study, we use an iterative map derived\nwithin the framework of the Babcock-Leighton (BL) dynamo to analyze the G-O\nrule. We investigate comprehensive and definitive forms of the G-O rule using\nboth a sufficiently large number of solar cycles and a limited number of solar\ncycles. Our findings indicate a higher probability for an arbitrary cycle to be\nfollowed by a stronger cycle instead of weaker, regardless of even or odd. Over\ntime spans comparable to historical observations, cycles exhibit periods that\nfollow both the G-O rule and the reversed G-O rule, without a statistically\nsignificant preference, consistent with the observed variability of the G-O\nrule. The occurrence of the reversed G-O rule is random, rather than periodic.\nThe G-O rule emerges as a result of the nonlinearity and stochasticity inherent\nin the BL mechanism. These results advance our understanding of the solar cycle\nand pave the way for improved solar dynamo modeling.",
        "Variational inference in probabilistic graphical models aims to approximate\nfundamental quantities such as marginal distributions and the partition\nfunction. Popular approaches are the Bethe approximation, tree-reweighted, and\nother types of convex free energies. These approximations are efficient but can\nfail if the model is complex and highly interactive. In this work, we analyze\ntwo classes of approximations that include the above methods as special cases:\nfirst, if the model parameters are changed; and second, if the entropy\napproximation is changed. We discuss benefits and drawbacks of either approach,\nand deduce from this analysis how a free energy approximation should ideally be\nconstructed. Based on our observations, we propose approximations that\nautomatically adapt to a given model and demonstrate their effectiveness for a\nrange of difficult problems.",
        "In this paper, we introduce a unified framework, inspired by classical\nregularization theory, for designing and analyzing a broad class of linear\nregression approaches. Our framework encompasses traditional methods like least\nsquares regression and Ridge regression, as well as innovative techniques,\nincluding seven novel regression methods such as Landweber and Showalter\nregressions. Within this framework, we further propose a class of debiased and\nthresholded regression methods to promote feature selection, particularly in\nterms of sparsity. These methods may offer advantages over conventional\nregression techniques, including Lasso, due to their ease of computation via a\nclosed-form expression. Theoretically, we establish consistency results and\nGaussian approximation theorems for this new class of regularization methods.\nExtensive numerical simulations further demonstrate that the debiased and\nthresholded counterparts of linear regression methods exhibit favorable finite\nsample performance and may be preferable in certain settings.",
        "In this work, the local decomposition of pressure in the Navier-Stokes\nequations is dynamically refined to prove that a relevant critical energy of a\nsuitable Leray-type solution inside a backward paraboloid, regardless of its\naperture is controlled near the vertex by a critical behavior confined to a\nneighborhood of the paraboloid's boundary. This neighborhood excludes the\ninterior near the vertex and remains separated from the temporal profile of the\nvertex, except at the vertex itself.",
        "In this article, we study the periodic points for continuous self-maps on the\nwedge sum of topological manifolds, exhibiting a particular combinatorial\nstructure. We compute explicitly the Lefschetz numbers, the Dold coefficients\nand consider its set of algebraic periods. Moreover, we study the special case\nof maps on the wedge sum of tori, and show some of the homological obstructions\npresent in defining these maps.",
        "Recent advances in large language models (LLMs) have demonstrated remarkable\npotential in the field of natural language processing. Unfortunately, LLMs face\nsignificant security and ethical risks. Although techniques such as safety\nalignment are developed for defense, prior researches reveal the possibility of\nbypassing such defenses through well-designed jailbreak attacks. In this paper,\nwe propose QueryAttack, a novel framework to examine the generalizability of\nsafety alignment. By treating LLMs as knowledge databases, we translate\nmalicious queries in natural language into structured non-natural query\nlanguage to bypass the safety alignment mechanisms of LLMs. We conduct\nextensive experiments on mainstream LLMs, and the results show that QueryAttack\nnot only can achieve high attack success rates (ASRs), but also can jailbreak\nvarious defense methods. Furthermore, we tailor a defense method against\nQueryAttack, which can reduce ASR by up to 64% on GPT-4-1106. Our code is\navailable at https:\/\/github.com\/horizonsinzqs\/QueryAttack.",
        "Multimodal models often experience a significant performance drop when one or\nmore modalities are missing during inference. To address this challenge, we\npropose a simple yet effective approach that enhances robustness to missing\nmodalities while maintaining strong performance when all modalities are\navailable. Our method introduces cross-modal proxy tokens (CMPTs), which\napproximate the class token of a missing modality by attending only to the\ntokens of the available modality. To efficiently learn the approximation for\nthe missing modality via CMPTs with minimal computational overhead, we employ\nlow-rank adapters in frozen unimodal encoders and jointly optimize an alignment\nloss with a task-specific loss. Extensive experiments on five multimodal\ndatasets show that our method outperforms state-of-the-art baselines across\nvarious missing rates while achieving competitive results in complete-modality\nsettings. Overall, our method offers a flexible and efficient solution for\nrobust multimodal learning. The code and pretrained models will be released on\nGitHub.",
        "Low-rank Adaptation (LoRA) has demonstrated remarkable capabilities for task\nspecific fine-tuning. However, in scenarios that involve multiple tasks,\ntraining a separate LoRA model for each one results in considerable\ninefficiency in terms of storage and inference. Moreover, existing parameter\ngeneration methods fail to capture the correlations among these tasks, making\nmulti-task LoRA parameter generation challenging. To address these limitations,\nwe propose In-Context Meta LoRA (ICM-LoRA), a novel approach that efficiently\nachieves task-specific customization of large language models (LLMs).\nSpecifically, we use training data from all tasks to train a tailored\ngenerator, Conditional Variational Autoencoder (CVAE). CVAE takes task\ndescriptions as inputs and produces task-aware LoRA weights as outputs. These\nLoRA weights are then merged with LLMs to create task-specialized models\nwithout the need for additional fine-tuning. Furthermore, we utilize in-context\nmeta-learning for knowledge enhancement and task mapping, to capture the\nrelationship between tasks and parameter distributions. As a result, our method\nachieves more accurate LoRA parameter generation for diverse tasks using CVAE.\nICM-LoRA enables more accurate LoRA parameter reconstruction than current\nparameter reconstruction methods and is useful for implementing task-specific\nenhancements of LoRA parameters. At the same time, our method occupies 283MB,\nonly 1\\% storage compared with the original LoRA.",
        "In this paper, we introduce advances in the sensor suite of an autonomous\nflying insect robot (FIR) weighing less than a gram. FIRs, because of their\nsmall weight and size, offer unparalleled advantages in terms of material cost\nand scalability. However, their size introduces considerable control\nchallenges, notably high-speed dynamics, restricted power, and limited payload\ncapacity. While there have been advancements in developing lightweight sensors,\noften drawing inspiration from biological systems, no sub-gram aircraft has\nbeen able to attain sustained hover without relying on feedback from external\nsensing such as a motion capture system. The lightest vehicle capable of\nsustained hovering -- the first level of ``sensor autonomy'' -- is the much\nlarger 28 g Crazyflie. Previous work reported a reduction in size of that\nvehicle's avionics suite to 187 mg and 21 mW. Here, we report a further\nreduction in mass and power to only 78.4 mg and 15 mW. We replaced the laser\nrangefinder with a lighter and more efficient pressure sensor, and built a\nsmaller optic flow sensor around a global-shutter imaging chip. A Kalman Filter\n(KF) fuses these measurements to estimate the state variables that are needed\nto control hover: pitch angle, translational velocity, and altitude. Our system\nachieved performance comparable to that of the Crazyflie's estimator while in\nflight, with root mean squared errors of 1.573 deg, 0.186 m\/s, and 0.136 m,\nrespectively, relative to motion capture.",
        "Simultaneous Localization and Mapping (SLAM) is essential for precise\nsurgical interventions and robotic tasks in minimally invasive procedures.\nWhile recent advancements in 3D Gaussian Splatting (3DGS) have improved SLAM\nwith high-quality novel view synthesis and fast rendering, these systems\nstruggle with accurate depth and surface reconstruction due to multi-view\ninconsistencies. Simply incorporating SLAM and 3DGS leads to mismatches between\nthe reconstructed frames. In this work, we present Endo-2DTAM, a real-time\nendoscopic SLAM system with 2D Gaussian Splatting (2DGS) to address these\nchallenges. Endo-2DTAM incorporates a surface normal-aware pipeline, which\nconsists of tracking, mapping, and bundle adjustment modules for geometrically\naccurate reconstruction. Our robust tracking module combines point-to-point and\npoint-to-plane distance metrics, while the mapping module utilizes normal\nconsistency and depth distortion to enhance surface reconstruction quality. We\nalso introduce a pose-consistent strategy for efficient and geometrically\ncoherent keyframe sampling. Extensive experiments on public endoscopic datasets\ndemonstrate that Endo-2DTAM achieves an RMSE of $1.87\\pm 0.63$ mm for depth\nreconstruction of surgical scenes while maintaining computationally efficient\ntracking, high-quality visual appearance, and real-time rendering. Our code\nwill be released at github.com\/lastbasket\/Endo-2DTAM.",
        "Two-dimensional Acoustic Charge Transport (2D-ACT) devices, which integrate\ntwo dimensional semiconductor field-effect transistor (FET) with high-frequency\nsurface acoustic wave (SAW) device provide a potential compact platform for the\nprocessing of analog signals in a wireless, non-contact, low-loss and real-time\nway. It is expected to be used in long-distance space communication and\nsensing. However, current investigations into 2D-ACT devices are still limited\nto the observation of DC acoustoelectric currents, and have yet to achieve\nreal-time electronic signal processing capabilities. In this paper, we have\ndesigned a hybrid acoustoelectric platform composed of two-dimensional\nsemiconductor FET and SAW device. The platform is capable of processing DC\nsignals, exhibiting ambipolar transport behavior. The sub-wavelength channel\nlength of the FET within the platform allows for the real-time observation of\ncarrier distribution at a microscopic scale in conjunction with the SAW\npotential, and facilitating the reproduction and intensity regulation of AC\nsignals. By adjusting the relative phase and intensity ratio of two\ncounter-propagating SAWs, the platform also enables the addition and\nsubtraction of AC signals.",
        "The study involves a comprehensive performance analysis of popular\nclassification and segmentation models, applied over a Bangladeshi pothole\ndataset, being developed by the authors of this research. This custom dataset\nof 824 samples, collected from the streets of Dhaka and Bogura performs\ncompetitively against the existing industrial and custom datasets utilized in\nthe present literature. The dataset was further augmented four-fold for\nsegmentation and ten-fold for classification evaluation. We tested nine\nclassification models (CCT, CNN, INN, Swin Transformer, ConvMixer, VGG16,\nResNet50, DenseNet201, and Xception) and four segmentation models (U-Net,\nResU-Net, U-Net++, and Attention-Unet) over both the datasets. Among the\nclassification models, lightweight models namely CCT, CNN, INN, Swin\nTransformer, and ConvMixer were emphasized due to their low computational\nrequirements and faster prediction times. The lightweight models performed\nrespectfully, oftentimes equating to the performance of heavyweight models. In\naddition, augmentation was found to enhance the performance of all the tested\nmodels. The experimental results exhibit that, our dataset performs on par or\noutperforms the similar classification models utilized in the existing\nliterature, reaching accuracy and f1-scores over 99%. The dataset also\nperformed on par with the existing datasets for segmentation, achieving model\nDice Similarity Coefficient up to 67.54% and IoU scores up to 59.39%.",
        "Virtual Try-On (VTON) technology allows users to visualize how clothes would\nlook on them without physically trying them on, gaining traction with the rise\nof digitalization and online shopping. Traditional VTON methods, often using\nGenerative Adversarial Networks (GANs) and Diffusion models, face challenges in\nachieving high realism and handling dynamic poses. This paper introduces\nOutfitting Diffusion with Pose Guided Condition (ODPG), a novel approach that\nleverages a latent diffusion model with multiple conditioning inputs during the\ndenoising process. By transforming garment, pose, and appearance images into\nlatent features and integrating these features in a UNet-based denoising model,\nODPG achieves non-explicit synthesis of garments on dynamically posed human\nimages. Our experiments on the FashionTryOn and a subset of the DeepFashion\ndataset demonstrate that ODPG generates realistic VTON images with fine-grained\ntexture details across various poses, utilizing an end-to-end architecture\nwithout the need for explicit garment warping processes. Future work will focus\non generating VTON outputs in video format and on applying our attention\nmechanism, as detailed in the Method section, to other domains with limited\ndata."
      ]
    }
  },
  {
    "id":2411.16728,
    "research_type":"applied",
    "start_id":"b10",
    "start_title":"FuXi-S2S: An accurate machine learning model for global subseasonal forecasts",
    "start_abstract":"Skillful subseasonal forecasts beyond 2 weeks are crucial for a wide range of applications across various sectors society. Recently, state-of-the-art machine learning based weather forecasting models have made significant advancements, outperforming the high-resolution forecast (HRES) from European Centre Medium-Range Weather Forecasts (ECMWF). However, full potential in has yet to be fully explored. In this study, we introduce FuXi Subseasonal-to-Seasonal (FuXi-S2S), model that provides global daily mean up 42 days, covering 5 upper-air atmospheric variables at 13 pressure levels and 11 surface variables. FuXi-S2S integrates an enhanced base with perturbation module flow-dependent perturbations hidden features, incorporates Perlin noise perturb initial conditions. The is developed using 72 years statistics ECMWF ERA5 reanalysis data. When compared (S2S) reforecasts, demonstrate superior deterministic ensemble total precipitation (TP), outgoing longwave radiation (OLR), geopotential 500 hPa (Z500). Although it shows slightly inferior performance predicting 2-meter temperature (T2M), clear advantages over land area. Regarding extreme forecasts, outperforms S2S globally TP. Furthermore, surpass reforecasts Madden Julian Oscillation (MJO), key source predictability. They extend skillful prediction MJO 30 days 36 days.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b21"
      ],
      "title":[
        "Analysis methods for numerical weather prediction"
      ],
      "abstract":[
        "Abstract Bayesian probabilistic arguments are used to derive idealized equations for finding the best analysis numerical weather prediction. These compared with those from other published methods in light of physical characteristics NWP problem; namely predetermined nature basis analysis, need approximation because large\u2010order systems, underdeterminacy problem when using observations alone, and availability prior relationships resolve underdeterminacy. Prior result (1) knowledge time evolution model (which together use a distribution constitutes four\u2010dimensional data assimilation); (2) that atmosphere varies slowly (leading balance relationships); (3) nonlinear coupling parameters scales atmosphere. Methods discussed include variational techniques, smoothing splines, Kriging, optimal interpolation, successive corrections, constrained initialization, Kalman\u2010Bucy filter, adjoint assimilation. They all shown relate hence each other. Opinions given on particular might be more appropriate. By comparison method some insight is gained into appropriate choices practical methods."
      ],
      "categories":[
        "physics.data-an"
      ]
    },
    "list":{
      "title":[
        "Unveiling stellar spin: Determining inclination angles in Be stars",
        "Dynamic Metadata Schemes in the Neutron and Photon Science Communities:\n  A Case Study of X-Ray Photon Correlation Spectroscopy",
        "Signatures of extreme events in the cumulative entropic spectrum",
        "High-Performance Data Format for Scientific Data Storage and Analysis",
        "Maximum likelihood estimation of burst-merging kernels for bursty time\n  series",
        "Ordinal language of antipersistent binary walks",
        "A skeletonization based image segmentation algorithm to isolate slender\n  regions in 3D microstructures",
        "ALMAGAL I. The ALMA evolutionary study of high-mass protocluster\n  formation in the Galaxy. Presentation of the survey and early results",
        "A perturbation theory for multi-time correlation functions in open\n  quantum systems",
        "Disentangling sources of multifractality in time series",
        "Influence of Membrane Characteristics on Efficiency of Vacuum Membrane\n  Distillation: a Lattice Boltzmann Study",
        "Sparse identification of evolution equations via Bayesian model\n  selection",
        "Capacitively Shunted Double-Transmon Coupler Realizing Bias-Free Idling\n  and High-Fidelity CZ Gate",
        "Efficient First-Principles Framework for Overdamped Phonon Dynamics and\n  Anharmonic Electron-Phonon Coupling in Superionic Materials",
        "Euclid Quick Data Release (Q1) -- Data release overview",
        "On the conjecture of non-inner automorphisms of finite $p$-groups with a\n  non-trivial abelian direct factor",
        "D-HAT: a Diatom-inspired structure for a Helmet concept Against Trauma",
        "In Vivo Study of Bone Growth Around Additively Manufactured Implants\n  with Ti-6Al-4V and Bioactive Glass Powder Composites",
        "On the Isomorphism Problem of Cayley Graphs of Graph Products",
        "Revisiting gluon density from the BK equation with kinematical\n  constraint and large x terms",
        "CIBER 4th flight fluctuation analysis: Pseudo-power spectrum formalism,\n  improved source masking and validation on mocks",
        "Study of SARS-CoV-2 Spike Protein by Surface Enhanced Raman Spectroscopy\n  and Transmission Electron Microscopy",
        "State transfer of Grover walks on unitary and quadratic unitary Cayley\n  graphs over finite commutative rings",
        "An Automated Bandwidth Division for the LHCb Upgrade Trigger",
        "Vision-Aided Channel Prediction Based on Image Segmentation at Street\n  Intersection Scenarios",
        "Partially hyperbolic symplectomorphism with C^1 bundles",
        "Principles for Open Data Curation: A Case Study with the New York City\n  311 Service Request Data",
        "Can Yang-Baxter imply Lie algebra?"
      ],
      "abstract":[
        "The physical properties of stellar atmospheres in rapidly rotating massive\nstars, such as Be stars, are critical to understanding their evolution and\ntheir role as progenitors of supernovae. These stars, which often have\nnear-critical rotation, exhibit equatorial stretching and gravity darkening,\nwhich significantly complicates the determination of parameters such as the\ninclination angle. Be stars, characterized by their extreme rotational\nvelocities, serve as excellent candidates for exploring these phenomena.\nHowever, fundamental quantities such as polar and equatorial radii and\ninclination angles are typically derived from interferometry, which applies\nonly to a limited number of stars. This study aims to enhance the determination\nof inclination angles for Be stars using the ZPEKTR spectral synthesis code. By\nincorporating advanced models of gravity darkening and stellar deformation, we\nevaluated the effectiveness of this method with a sample of ten Be stars from\nthe BeSOS database, comparing results with established interferometric data.\nMethods. We used the ZPEKTR code to model the effects of stellar oblateness and\ngravity darkening on spectral lines, focusing on the HeI 4471 line. We applied\na chi-squared test minimization approach to identify the best-fitting models,\nand we evaluated the inclination angles derived against interferometric\nmeasurements. Our analysis reveals a robust linear correlation between the\ninclination angles derived from ZPEKTR and using interferometric techniques,\nwhich demonstrates an excellent agreement. The ZPEKTR code effectively models\nhigh rotational velocity effects, providing precise stellar parameter\ndeterminations. The results underscore the potential of advanced spectroscopic\ntechniques to yield inclination measurements comparable to interferometry,\nwhich offers a pathway to studying distant massive stars.",
        "Metadata is one of the most important aspects for advancing data management\npractices within all research communities. Definitions and schemes of metadata\nare inter alia of particular significance in the domain of neutron and photon\nscattering experiments covering a broad area of different scientific\ndisciplines. The demand of describing continuously evolving highly\nnonstandardized experiments, including the resulting processed and published\ndata, constitutes a considerable challenge for a static definition of metadata.\nHere, we present the concept of dynamic metadata for the neutron and photon\nscientific community, which enriches a static set of defined basic metadata. We\nexplore the idea of dynamic metadata with the help of the use case of X-ray\nPhoton Correlation Spectroscopy (XPCS), which is a synchrotron-based scattering\ntechnique that allows the investigation of nanoscale dynamic processes. It\nserves here as a demonstrator of how dynamic metadata can improve data\nacquisition, sharing, and analysis workflows. Our approach enables researchers\nto tailor metadata definitions dynamically and adapt them to the evolving\ndemands of describing data and results from a diverse set of experiments. We\ndemonstrate that dynamic metadata standards yield advantages that enhance data\nreproducibility, interoperability, and the dissemination of knowledge.",
        "In this study, the cumulative effect of the empirical probability\ndistribution of a random variable is identified as a factor that amplifies the\noccurrence of extreme events in datasets. To quantify this observation, a\ncorresponding information measure is introduced, drawing upon Shannon entropy\nfor joint probabilities. The proposed approach is validated using selected\nmarket data as case studies, encompassing various instances of extreme events.\nIn particular, the results indicate that the introduced cumulative measure\nexhibits distinctive signatures of such events, even when the data is\nrelatively noisy. These findings highlight the potential of the discussed\nconcept for developing a new class of related indicators or classifiers.",
        "In this article, we present the High-Performance Output (HiPO) data format\ndeveloped at Jefferson Laboratory for storing and analyzing data from Nuclear\nPhysics experiments. The format was designed to efficiently store large amounts\nof experimental data, utilizing modern fast compression algorithms. The purpose\nof this development was to provide organized data in the output, facilitating\naccess to relevant information within the large data files. The HiPO data\nformat has features that are suited for storing raw detector data,\nreconstruction data, and the final physics analysis data efficiently,\neliminating the need to do data conversions through the lifecycle of\nexperimental data. The HiPO data format is implemented in C++ and JAVA, and\nprovides bindings to FORTRAN, Python, and Julia, providing users with the\nchoice of data analysis frameworks to use. In this paper, we will present the\ngeneral design and functionalities of the HiPO library and compare the\nperformance of the library with more established data formats used in data\nanalysis in High Energy and Nuclear Physics (such as ROOT and Parquete).",
        "Various time series in natural and social processes have been found to be\nbursty. Events in the time series rapidly occur within short time periods,\nforming bursts, which are alternated with long inactive periods. As the\ntimescale defining bursts increases, individual events are sequentially merged\nto become small bursts and then bigger ones, eventually leading to the single\nburst containing all events. Such a merging pattern has been depicted by a tree\nthat fully reveals the hierarchical structure of bursts, thus called a burst\ntree. The burst-tree structure can be simply characterized by a burst-merging\nkernel that dictates which bursts are merged together as the timescale\nincreases. In this work, we develop the maximum likelihood estimation method of\nthe burst-merging kernel from time series, which is successfully tested against\nthe time series generated using several model kernels. We also apply our method\nto some empirical time series from various backgrounds. Our method provides a\nuseful tool to precisely characterize the time series data, hence enabling to\nstudy their underlying mechanisms more accurately.",
        "This paper explores the effectiveness of using ordinal pattern probabilities\nto evaluate antipersistency in the sign decomposition of long-range\nanti-correlated Gaussian fluctuations. It is numerically shown that ordinal\npatterns are able to effectively measure both persistent and antipersistent\ndynamics by analyzing the sign decomposition derived from fractional Gaussian\nnoise. These findings are crucial given that traditional methods such as\nDetrended Fluctuation Analysis are unsuccessful in detecting anti-correlations\nin such sequences. The numerical results are supported by physiological and\nenvironmental data, illustrating its applicability in real-world situations.",
        "The work proposes an image segmentation algorithm that isolates slender\nregions in three-dimensional microstructures. Characterizing slender regions in\nmaterial microstructures is an extremely important aspect in material science\nbecause these regions govern the macroscopic behavior of materials for many\napplications like energy absorption, activation of metamaterials, stability of\nhigh temperature filters, etc. This work utilizes skeletonization method to\ncalculate centerline of the microstructure geometry followed by a novel pruning\nstrategy based on cross-sectional area to identify slender regions in the\nmicrostructure. 3D images of such microstructures obtained from micro-CT often\nsuffer from low image resolution resulting in high surface noise. The skeleton\nof such an image has many spurious skeletal branches that do not represent the\nactual microstructure geometry. The proposed pruning method of cross-sectional\narea is insensitive to surface noise and hence is a reliable method of\nidentifying skeletal branches that represent the slender regions in the\nmicrostructure. The proposed algorithm is implemented on a test case to\nshowcase its effectiveness. Further it is implemented on a 3D microstructure of\nceramic foam to identify the slender regions present in it. It is shown that\nthe method can be used to segment slender regions of varying dimensions and to\nstudy their geometric properties.",
        "Fundamental questions about the physics responsible for fragmenting molecular\nparsec-scale clumps into cores of ~1000 au are still open, that only a\nstatistically significant investigation with ALMA is able to address: what are\nthe dominant agents that determine the core demographics, mass, and spatial\ndistribution as a function of the physical properties of the hosting clumps,\ntheir evolutionary stage and the different Galactic environments in which they\nreside? To what extent extent is fragmentation driven by clumps dynamics or\nmass transport in filaments? With ALMAGAL we observed the 1.38 mm continuum and\nlines toward more than 1000 dense clumps in our Galaxy, with M>500M_sun,\nsurface density > 0.1 g\/cm2 and d<7.5 kpc. The ACA and two 12-m array setups\nwere used to deliver a minimum resolution of ~1000 au over the entire sample\ndistance range. The sample covers all evolutionary stages from infrared dark\nclouds (IRDCs) to HII regions from the tip of the Galactic bar to the outskirts\nof the Galaxy. The spectral setup includes several molecular lines to trace the\nmultiscale physics and dynamics of gas, notably CH3CN, H2CO, SiO, CH3OH, DCN,\nHC3N, SO etc. We present an initial overview of the observations and the early\nscience product and results, with a first characterization of the morphological\nproperties of the continuum emission. We use \"perimeter-versus-area\" and convex\nhull-versus-area metrics to classify the different morphologies. More extended\nand morphologically complex shapes are found toward clumps that are relatively\nmore evolved and have higher surface densities.",
        "Dynamical maps are the principal subject of the open system theory. Formally,\nthe dynamical map of a given open quantum system is a density matrix\ntransformation that takes any initial state and sends it to the state at a\nlater time. Physically, it encapsulates the system's evolution due to coupling\nwith its environment.\n  Hence, the theory provides a flexible and accurate framework for computing\nexpectation values of open system observables. However, expectation values --\nor more generally, single-time correlation functions -- capture only the\nsimplest aspects of a quantum system's dynamics. A complete characterization\nrequires access to multi-time correlation functions as well. For closed\nsystems, such correlations are well-defined, even though knowledge of the\nsystem's state alone is insufficient to determine them fully. In contrast, the\nstandard dynamical map formalism for open systems does not account for\nmulti-time correlations, as it is fundamentally limited to describing state\nevolution. Here, we extend the scope of open quantum system theory by\ndeveloping a systematic perturbation theory for computing multi-time\ncorrelation functions.",
        "This contribution addresses the question commonly asked in scientific\nliterature about the sources of multifractality in time series. Two primary\nsources are typically considered. These are temporal correlations and heavy\ntails in the distribution of fluctuations. Most often, they are treated as two\nindependent components, while true multifractality cannot occur without\ntemporal correlations. The distributions of fluctuations affect the span of the\nmultifractal spectrum only when correlations are present. These issues are\nillustrated here using series generated by several model mathematical cascades,\nwhich by design build correlations into these series. The thickness of the\ntails of fluctuations in such series is then governed by an appropriate\nprocedure of adjusting them to $q$-Gaussian distributions, and $q$ is treated\nas a variable parameter that, while preserving correlations, allows to tune\nthese distributions to the desired functional form. Multifractal detrended\nfluctuation analysis (MFDFA), as the most commonly used practical method for\nquantifying multifractality, is then used to identify the influence of the\nthickness of the fluctuation tails in the presence of temporal correlations on\nthe width of multifractal spectra. The obtained results point to the Gaussian\ndistribution, so $q=1$, as the appropriate reference distribution to evaluate\nthe contribution of fatter tails to the width of multifractal spectra. An\nappropriate procedure is presented to make such estimates.",
        "With increasing water scarcity, membrane distillation technology has gained\nwidespread attention as an innovative method for seawater\ndesalination.However,existing studies often overlook the influence of membrane\ncharacteristics on mass transfer efficiency. This study, based on the lattice\nBoltzmann method,proposes a model for a novel Poly(tetraethynylpyrene) membrane\nmaterial to reveal the influence of membrane characteristics on the performance\nof vacuum membrane distillation. The model considers the factors such as\nporosity, tortuosity, membrane thickness, pore size, membrane surface\nwettability and temperature difference on the permeate flux. The results show\nthat the permeate flux increases linearly with the porosity and decreases\nexponentially with the tortuosity factor. There is an optimal membrane\nthickness range (2{\\mu}m) beyond which the permeate flux decreases\nexponentially. In addition, the permeate flux increases exponentially with\nincreasing temperature difference and pore size. Further analysis of the effect\nof membrane surface wettability shows that permeate flux increases with\nincreasing hydrophobicity. Finally, the feed temperature and tortuosity factor\nhave the largest effect on permeate flux,followed by membrane thickness and,\nsubsequently, pore size. The model can be further extended to study other\nconfigurations of membrane distillation technologies.",
        "The quantitative formulation of evolution equations is the backbone for\nprediction, control, and understanding of dynamical systems across diverse\nscientific fields. Besides deriving differential equations for dynamical\nsystems based on basic scientific reasoning or prior knowledge in recent times\na growing interest emerged to infer these equations purely from data. In this\narticle, we introduce a novel method for the sparse identification of nonlinear\ndynamical systems from observational data, based on the observation how the key\nchallenges of the quality of time derivatives and sampling rates influence this\nproblem. Our approach combines system identification based on thresholded least\nsquares minimization with additional error measures that account for both the\ndeviation between the model and the time derivative of the data, and the\nintegrated performance of the model in forecasting dynamics. Specifically, we\nintegrate a least squares error as well as the Wasserstein metric for estimated\nmodels and combine them within a Bayesian optimization framework to efficiently\ndetermine optimal hyperparameters for thresholding and weighting of the\ndifferent error norms. Additionally, we employ distinct regularization\nparameters for each differential equation in the system, enhancing the method's\nprecision and flexibility. We demonstrate the capabilities of our approach\nthrough applications to dynamical fMRI data and the prototypical example of a\nwake flow behind a cylinder. In the wake flow problem, our method identifies a\nsparse, accurate model that correctly captures transient dynamics, oscillation\nperiods, and phase information, outperforming existing methods. In the fMRI\nexample, we show how our approach extracts insights from a trained recurrent\nneural network, offering a novel avenue for explainable AI by inferring\ndifferential equations that capture potentially causal relationships.",
        "A high-fidelity CZ gate utilizing a double-transmon coupler (DTC) has\nrecently been demonstrated as a building block for superconducting quantum\nprocessors. Like many other kinds of tunable couplers, however, the DTC\nrequires a finite DC current for flux-biasing the coupler at the idling point\nto turn off the coupling, necessitating extra care for wiring and heat-load\nmanagement. To address this issue, we theoretically propose and experimentally\nrealize a novel coupling scheme by introducing a shunt capacitance between the\ntwo transmons of the DTC at zero-flux bias, which demonstrates high-fidelity\nCZ-gate performance comparable to the previous DTC. Through a comprehensive\nerror budget analysis using multiple randomized benchmarking methods, we also\nidentify that the current fidelity is limited by the decoherence through the\ncoupler. Moreover, we experimentally demonstrate the wide operational flux\nrange of the capacitively shunted DTC, which solves the challenging issue of\nremnant flux existing even with careful magnetic shielding.",
        "Relying on the anharmonic special displacement method, we introduce an ab\ninitio quasistatic polymorphous framework to describe local disorder,\nanharmonicity, and electron-phonon coupling in superionic conductors. Using the\nexample of cubic Cu2Se, we show that positional polymorphism yields extremely\noverdamped anharmonic vibrations while preserving transverse acoustic phonons,\nconsistent with experiments. We also demonstrate well-defined electronic band\nstructures with large band gap openings due to polymorphism of 1.0 eV and\ncalculate anharmonic electron-phonon renormalization, yielding band gap\nnarrowing with increasing temperature in agreement with previous measurements.\nOur approach opens the way for efficient ab initio electronic structure\ncalculations in superionic crystals to elucidate their compelling high\nfigure-of-merit.",
        "The first Euclid Quick Data Release, Q1, comprises 63.1 sq deg of the Euclid\nDeep Fields (EDFs) to nominal wide-survey depth. It encompasses visible and\nnear-infrared space-based imaging and spectroscopic data, ground-based\nphotometry in the u, g, r, i and z bands, as well as corresponding masks.\nOverall, Q1 contains about 30 million objects in three areas near the ecliptic\npoles around the EDF-North and EDF-South, as well as the EDF-Fornax field in\nthe constellation of the same name. The purpose of this data release -- and its\nassociated technical papers -- is twofold. First, it is meant to inform the\ncommunity of the enormous potential of the Euclid survey data, to describe what\nis contained in these data, and to help prepare expectations for the\nforthcoming first major data release DR1. Second, it enables a wide range of\ninitial scientific projects with wide-survey Euclid data, ranging from the\nearly Universe to the Solar System. The Q1 data were processed with early\nversions of the processing pipelines, which already demonstrate good\nperformance, with numerous improvements in implementation compared to\npre-launch development. In this paper, we describe the sky areas released in\nQ1, the observations, a top-level view of the data processing of Euclid and\nassociated external data, the Q1 photometric masks, and how to access the data.\nWe also give an overview of initial scientific results obtained using the Q1\ndata set by Euclid Consortium scientists, and conclude with important caveats\nwhen using the data. As a complementary product, Q1 also contains observations\nof a star-forming area in Lynd's Dark Nebula 1641 in the Orion~A Cloud,\nobserved for technical purposes during Euclid's performance-verification phase.\nThis is a unique target, of a type not commonly found in Euclid's nominal sky\nsurvey.",
        "Let $p$ be a prime number. A longstanding conjecture asserts that every\nfinite non-abelian $p$-group has a non-inner automorphism of order $p$. In this\npaper, we prove that the conjecture is true when a finite non-abelian $p$-group\n$G$ has a non-trivial abelian direct factor. Moreover, we prove that the\nnon-inner automorphism is central and fixes $\\Phi(G)$ elementwise. As a\nconsequence, we prove that every group which is not purely non-abelian has a\nnon-inner central automorphism of order $p$ which fixes $\\Phi(G)$ elementwise.",
        "The primary objective of helmet design continues to be the prevention of\ntraumatic brain injuries. Yet, achieving an optimal user experience, including\naspects such as fit, thermal comfort, breathability, waterproofing, and\nreusability, is increasingly significant. Thus, designing helmets with\nmultifunctional performance represents the latest technological frontier for\nsafety devices. This study draws inspiration from the morphology of\nCoscinodiscus species diatoms to develop a biomimetic material replicating\ntheir cellular structure and multifunctionality. Unlike its biological\ncounterpart, the synthetic material is engineered as the inner liner for multi\nimpact helmets, suited for urban sports and micro mobility applications. The\narchitecture of the material is modeled using computer aided design tools, and\nits energy absorption capabilities are analyzed through finite element modeling\nand quasi static compression tests on 3D printed elastomeric samples.\nPerformance optimization is achieved through a parametric approach. The results\ndemonstrate that the material exhibits energy absorption comparable to cellular\nmaterials like honeycombs, while offering lightweight properties,\nbreathability, and resistance to atmospheric agents. This biomimetic design\nmarks a significant advancement in high performance safety equipment.",
        "Osseointegration is crucial to the success of biomedical implants. Additive\nmanufacturing of implants offers a high degree of design freedom, enabling\nprecise control over implant geometry and material composition. Bioactive glass\n(BG) can substantially enhance bone binding and bioactivity; however, limited\nresearch has been conducted on its incorporation into additively manufactured\nimplants. The performance of BG varies depending on the incorporation method,\nand the spatial and temporal evolution of its integration remains unclear. In\nthis study, we synthesized Ti-6Al-4V\/58S BG composites by using the selective\nlaser melting method and systematically compared the effects of BG coating and\ndoping in additively manufactured implants. In vivo histological results from\nanimal tests were statistically analyzed and discussed in terms of\nosseointegration over 4- and 12-week periods. Bone-to-implant contact (BIC) and\nbone density (BD) were used as quantitative metrics to evaluate interactions\nbetween the implants and surrounding bone. Our findings indicate that both\nBG-doped and BG-coated implants accelerated bone ingrowth during the early\nstages of healing. BG-coated implants demonstrated a greater improvement than\ndid pure 3D-printed Ti-6Al-4V implants. However, the effects of BG became\nnonsignificant during the later healing stage (12 weeks). This study provides a\nfoundation for systematically investigating BG incorporation methods in\n3D-printed biomedical implants and their effect on osseointegration.",
        "We investigate Cayley graphs of graph products by showing that graph products\nwith vertex groups that have isomorphic Cayley graphs yield isomorphic Cayley\ngraphs.",
        "We perform analysis of the small x non-linear evolution equation formulated\nin momentum space supplemented by higher order terms. The equation is defined\nin wide range of transverse momentum and longitudinal momentum fraction\nextending previous studies performed in \\cite{Kutak:2003bd,Kutak:2004ym}. The\nlinear part of the equation is motivated by the renormalization group improved\nsmall x approach which accounts for resummation of higher orders, and includes\ncollinear splitting function and kinematical constraint. The solution to the\nequation is then used to perform the fit to Deep Inelastic Scattering reduced\ncross section data.",
        "Precise, unbiased measurements of extragalactic background anisotropies\nrequire careful treatment of systematic effects in fluctuation-based,\nbroad-band intensity mapping measurements. In this paper we detail improvements\nin methodology for the Cosmic Infrared Background ExpeRiment (CIBER),\nconcentrating on flat field errors and source masking errors. In order to\nbypass the use of field differences, which mitigate flat field errors but\nreduce sensitivity, we characterize and correct for the flat field on\npseudo-power spectra, which includes both additive and multiplicative biases.\nTo more effectively mask point sources at 1.1 $\\mu$m and 1.8 $\\mu$m, we develop\na technique for predicting masking catalogs that utilizes optical and NIR\nphotometry through random forest regression. This allows us to mask over two\nVega magnitudes deeper than the completeness limits of 2MASS alone, with errors\nin the shot noise power remaining below $<10\\%$ at all masking depths\nconsidered. Through detailed simulations of CIBER observations, we validate our\nformalism and demonstrate unbiased recovery of the sky fluctuations on\nrealistic mocks. We demonstrate that residual flat field errors comprise\n$<20\\%$ of the final CIBER power spectrum uncertainty with this methodology.",
        "The spike protein (SP) of SARS-CoV-2 is the major molecular target for making\ndiagnostic tests, vaccines, and therapeutic development. We used a combination\nof transmission electron microscopy (TEM) and surface enhanced Raman microscopy\n(SERS) to study its structure. Using SERS on an aluminum substrate, we were\nable to detect a characteristic spectrum of SP mostly due to vibration of three\naromatic amino acids producing Raman shifts at 466 cm-1, 524 cm-1, 773 cm-1,\n831 cm-1, 1048 cm-1, 1308 cm-1, 1457 cm-1, and 1610 cm-1. Transmission Electron\nMicroscopy (TEM) of the SP showed periodic 2D-lattice orientation. The findings\nfrom this study have translational values for developing surface-enhanced Raman\nspectroscopy (SERS) based detectors for screening and testing SARS-CoV-2\nsignatures in diagnostic settings and contamination tracking.",
        "This paper focuses on periodicity and perfect state transfer of Grover walks\non two well-known families of Cayley graphs, namely, the unitary Cayley graphs\nand the quadratic unitary Cayley graphs. Let $R$ be a finite commutative ring.\nThe unitary Cayley graph $G_R$ has vertex set $R$, where two vertices $u$ and\n$v$ are adjacent if $u-v$ is a unit in $R$. We provide a necessary and\nsufficient condition for the periodicity of the Cayley graph $G_R$. We also\ncompletely determine the rings $R$ for which $G_R$ exhibits perfect state\ntransfer. The quadratic unitary Cayley graph $\\mathcal{G}_R$ has vertex set\n$R$, where two vertices $u$ and $v$ are adjacent if $u-v$ or $v-u$ is a square\nof some units in $R$. It is well known that any finite commutative ring $R$ can\nbe expressed as $R_1\\times\\cdots\\times R_s$, where each $R_i$ is a local ring\nwith maximal ideal $M_i$ for $i\\in\\{1,...,s\\}$. We characterize periodicity and\nperfect state transfer on $\\mathcal{G}_R$ under the condition that\n$|R_i|\/|M_i|\\equiv 1 \\pmod 4$ for $i\\in\\{1,...,s\\}$. Also, we characterize\nperiodicity and perfect state transfer on $\\mathcal{G}_R$, where $R$ can be\nexpressed as $R_0\\times\\cdots\\times R_s$ such that $|R_0|\/|M_0|\\equiv3\\pmod 4$,\nand $|R_i|\/|M_i|\\equiv1\\pmod4$ for $i\\in\\{1,..., s\\}$, where $R_i$ is a local\nring with maximal ideal $M_i$ for $i\\in\\{0,...,s\\}$.",
        "The upgraded Large Hadron Collider beauty (LHCb) experiment is the first\ndetector based at a hadron collider using a fully software based trigger. The\nfirst `High Level Trigger' stage (HLT1) reduces the event rate from 30 MHz to\napproximately 1 MHz based on reconstruction criteria from the tracking system\nand consists of O(100) trigger selections implemented on GPUs. These selections\nare further refined following the full offline-quality reconstruction at the\nsecond stage (HLT2) prior to saving for analysis. An automated bandwidth\ndivision has been performed to equitably divide this 1 MHz output rate between\nthe signals of interest to the LHCb physics program. This was achieved by\noptimising a set of trigger selections that maximise efficiency for signals of\ninterest to LHCb while keeping the total HLT1 readout capped to a maximum. The\nbandwidth division tool has been used to determine the optimal selection for 35\nselection algorithms over 80 characteristic physics channels.",
        "Intelligent vehicular communication with vehicle road collaboration\ncapability is a key technology enabled by 6G, and the integration of various\nvisual sensors on vehicles and infrastructures plays a crucial role. Moreover,\naccurate channel prediction is foundational to realizing intelligent vehicular\ncommunication. Traditional methods are still limited by the inability to\nbalance accuracy and operability based on substantial spectrum resource\nconsumption and highly refined description of environment. Therefore,\nleveraging out-of-band information introduced by visual sensors provides a new\nsolution and is increasingly applied across various communication tasks. In\nthis paper, we propose a computer vision (CV)-based prediction model for\nvehicular communications, realizing accurate channel characterization\nprediction including path loss, Rice K-factor and delay spread based on image\nsegmentation. First, we conduct extensive vehicle-to-infrastructure measurement\ncampaigns, collecting channel and visual data from various street intersection\nscenarios. The image-channel dataset is generated after a series of data\npost-processing steps. Image data consists of individual segmentation of target\nuser using YOLOv8 network. Subsequently, established dataset is used to train\nand test prediction network ResNet-32, where segmented images serve as input of\nnetwork, and various channel characteristics are treated as labels or target\noutputs of network. Finally, self-validation and cross-validation experiments\nare performed. The results indicate that models trained with segmented images\nachieve high prediction accuracy and remarkable generalization performance\nacross different streets and target users. The model proposed in this paper\noffers novel solutions for achieving intelligent channel\n  prediction in vehicular communications.",
        "We prove dynamical coherence for partial hyperbolic symplectomorphism in\ndimension 4 whose stable and unstable bundles are C^1.",
        "In the early 21st century, the open data movement began to transform\nsocieties and governments by promoting transparency, innovation, and public\nengagement. The City of New York (NYC) has been at the forefront of this\nmovement since the enactment of the Open Data Law in 2012, creating the NYC\nOpen Data portal. The portal currently hosts 2,700 datasets, serving as a\ncrucial resource for research across various domains, including health, urban\ndevelopment, and transportation. However, the effective use of open data relies\nheavily on data quality and usability, challenges that remain insufficiently\naddressed in the literature. This paper examines these challenges via a case\nstudy of the NYC 311 Service Request dataset, identifying key issues in data\nvalidity, consistency, and curation efficiency. We propose a set of data\ncuration principles, tailored for government-released open data, to address\nthese challenges. Our findings highlight the importance of harmonized field\ndefinitions, streamlined storage, and automated quality checks, offering\npractical guidelines for improving the reliability and utility of open\ndatasets.",
        "Quantum knot invariants (like colored HOMFLY-PT or Kauffman polynomials) are\na distinguished class of non-perturbative topological invariants. Any known way\nto construct them (via Chern-Simons theory or quantum R-matrix) starts with a\nfinite simple Lie algebra. Another set of knot invariants - of finite type - is\nrelated to quantum invariants via a perturbative expansion. However can all\nfinite type invariants be obtained in this way? Investigating this problem, P.\nVogel discovered a way to polynomially parameterize the expansion coefficients\nwith three parameters so that, at different specific values, this reproduces\nthe answers for all simple Lie (super)algebras. Then it is easy to construct a\npolynomial $P_{alg}$ that vanishes for all simple Lie algebras, and the\ncorresponding Vassiliev invariant would thus be absent from the perturbative\nexpansion.\n  We review these Vogel claims pointing out at least two interesting\nimplications of his construction. First, we discuss whether\ninfinite-dimensional Lie algebras might enlarge Chern-Simons theory. Second,\nVogel's construction implies an alternative axiomatization of simple Lie\nalgebras - when we start from knot invariants and arrive at Lie algebras and\ntheir classification, which is opposite to conventional logic that we mentioned\nat the beginning."
      ]
    }
  },
  {
    "id":2411.16728,
    "research_type":"applied",
    "start_id":"b21",
    "start_title":"Analysis methods for numerical weather prediction",
    "start_abstract":"Abstract Bayesian probabilistic arguments are used to derive idealized equations for finding the best analysis numerical weather prediction. These compared with those from other published methods in light of physical characteristics NWP problem; namely predetermined nature basis analysis, need approximation because large\u2010order systems, underdeterminacy problem when using observations alone, and availability prior relationships resolve underdeterminacy. Prior result (1) knowledge time evolution model (which together use a distribution constitutes four\u2010dimensional data assimilation); (2) that atmosphere varies slowly (leading balance relationships); (3) nonlinear coupling parameters scales atmosphere. Methods discussed include variational techniques, smoothing splines, Kriging, optimal interpolation, successive corrections, constrained initialization, Kalman\u2010Bucy filter, adjoint assimilation. They all shown relate hence each other. Opinions given on particular might be more appropriate. By comparison method some insight is gained into appropriate choices practical methods.",
    "start_categories":[
      "physics.data-an"
    ],
    "start_fields":[
      "Physics"
    ],
    "target_paper":{
      "id":[
        "b10"
      ],
      "title":[
        "FuXi-S2S: An accurate machine learning model for global subseasonal forecasts"
      ],
      "abstract":[
        "Skillful subseasonal forecasts beyond 2 weeks are crucial for a wide range of applications across various sectors society. Recently, state-of-the-art machine learning based weather forecasting models have made significant advancements, outperforming the high-resolution forecast (HRES) from European Centre Medium-Range Weather Forecasts (ECMWF). However, full potential in has yet to be fully explored. In this study, we introduce FuXi Subseasonal-to-Seasonal (FuXi-S2S), model that provides global daily mean up 42 days, covering 5 upper-air atmospheric variables at 13 pressure levels and 11 surface variables. FuXi-S2S integrates an enhanced base with perturbation module flow-dependent perturbations hidden features, incorporates Perlin noise perturb initial conditions. The is developed using 72 years statistics ECMWF ERA5 reanalysis data. When compared (S2S) reforecasts, demonstrate superior deterministic ensemble total precipitation (TP), outgoing longwave radiation (OLR), geopotential 500 hPa (Z500). Although it shows slightly inferior performance predicting 2-meter temperature (T2M), clear advantages over land area. Regarding extreme forecasts, outperforms S2S globally TP. Furthermore, surpass reforecasts Madden Julian Oscillation (MJO), key source predictability. They extend skillful prediction MJO 30 days 36 days."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "Explainable Distributed Constraint Optimization Problems",
        "Training-Free Safe Denoisers for Safe Use of Diffusion Models",
        "ADAM-1: AI and Bioinformatics for Alzheimer's Detection and\n  Microbiome-Clinical Data Integrations",
        "Optimus-2: Multimodal Minecraft Agent with Goal-Observation-Action\n  Conditioned Policy",
        "Do Chains-of-Thoughts of Large Language Models Suffer from\n  Hallucinations, Cognitive Biases, or Phobias in Bayesian Reasoning?",
        "OvercookedV2: Rethinking Overcooked for Zero-Shot Coordination",
        "Theorem Prover as a Judge for Synthetic Data Generation",
        "Neurosymbolic artificial intelligence via large language models and\n  coherence-driven inference",
        "Understanding the Impact of Artificial Intelligence in Academic Writing:\n  Metadata to the Rescue",
        "YINYANG-ALIGN: Benchmarking Contradictory Objectives and Proposing\n  Multi-Objective Optimization based DPO for Text-to-Image Alignment",
        "Planning of Heuristics: Strategic Planning on Large Language Models with\n  Monte Carlo Tree Search for Automating Heuristic Optimization",
        "Intelligence Sequencing and the Path-Dependence of Intelligence\n  Evolution: AGI-First vs. DCI-First as Irreversible Attractors",
        "Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite\n  Solar Cell Research",
        "Heavy-tailed random vectros: theory and applications",
        "QuESat: Satellite-Assisted Quantum Internet for Global-Scale\n  Entanglement Distribution",
        "Cheap Permutation Testing",
        "Efficient stochastic simulation of piecewise-deterministic Markov\n  processes and its application to the Morris-Lecar model of neural dynamics",
        "acoupi: An Open-Source Python Framework for Deploying Bioacoustic AI\n  Models on Edge Devices",
        "PolyhedronNet: Representation Learning for Polyhedra with\n  Surface-attributed Graph",
        "Splitting CEGM Amplitudes",
        "Using Covid-19 Response Policy to Estimate Open Water Swim Drafting\n  Effects in Triathlon",
        "Is Bellman Equation Enough for Learning Control?",
        "GPDFlow: Generative Multivariate Threshold Exceedance Modeling via\n  Normalizing Flows",
        "Non-polynomial conserved quantities for ODE systems and its application\n  to the long-time behavior of solutions to cubic NLS systems",
        "Representation in large language models",
        "LongReason: A Synthetic Long-Context Reasoning Benchmark via Context\n  Expansion",
        "BadJudge: Backdoor Vulnerabilities of LLM-as-a-Judge",
        "Positive self-commutators of positive operators"
      ],
      "abstract":[
        "The Distributed Constraint Optimization Problem (DCOP) formulation is a\npowerful tool to model cooperative multi-agent problems that need to be solved\ndistributively. A core assumption of existing approaches is that DCOP solutions\ncan be easily understood, accepted, and adopted, which may not hold, as\nevidenced by the large body of literature on Explainable AI. In this paper, we\npropose the Explainable DCOP (X-DCOP) model, which extends a DCOP to include\nits solution and a contrastive query for that solution. We formally define some\nkey properties that contrastive explanations must satisfy for them to be\nconsidered as valid solutions to X-DCOPs as well as theoretical results on the\nexistence of such valid explanations. To solve X-DCOPs, we propose a\ndistributed framework as well as several optimizations and suboptimal variants\nto find valid explanations. We also include a human user study that showed that\nusers, not surprisingly, prefer shorter explanations over longer ones. Our\nempirical evaluations showed that our approach can scale to large problems, and\nthe different variants provide different options for trading off explanation\nlengths for smaller runtimes. Thus, our model and algorithmic contributions\nextend the state of the art by reducing the barrier for users to understand\nDCOP solutions, facilitating their adoption in more real-world applications.",
        "There is growing concern over the safety of powerful diffusion models (DMs),\nas they are often misused to produce inappropriate, not-safe-for-work (NSFW)\ncontent or generate copyrighted material or data of individuals who wish to be\nforgotten. Many existing methods tackle these issues by heavily relying on\ntext-based negative prompts or extensively retraining DMs to eliminate certain\nfeatures or samples. In this paper, we take a radically different approach,\ndirectly modifying the sampling trajectory by leveraging a negation set (e.g.,\nunsafe images, copyrighted data, or datapoints needed to be excluded) to avoid\nspecific regions of data distribution, without needing to retrain or fine-tune\nDMs. We formally derive the relationship between the expected denoised samples\nthat are safe and those that are not safe, leading to our $\\textit{safe}$\ndenoiser which ensures its final samples are away from the area to be negated.\nInspired by the derivation, we develop a practical algorithm that successfully\nproduces high-quality samples while avoiding negation areas of the data\ndistribution in text-conditional, class-conditional, and unconditional image\ngeneration scenarios. These results hint at the great potential of our\ntraining-free safe denoiser for using DMs more safely.",
        "The Alzheimer's Disease Analysis Model Generation 1 (ADAM) is a multi-agent\nlarge language model (LLM) framework designed to integrate and analyze\nmulti-modal data, including microbiome profiles, clinical datasets, and\nexternal knowledge bases, to enhance the understanding and detection of\nAlzheimer's disease (AD). By leveraging retrieval-augmented generation (RAG)\ntechniques along with its multi-agent architecture, ADAM-1 synthesizes insights\nfrom diverse data sources and contextualizes findings using literature-driven\nevidence. Comparative evaluation against XGBoost revealed similar mean F1\nscores but significantly reduced variance for ADAM-1, highlighting its\nrobustness and consistency, particularly in small laboratory datasets. While\ncurrently tailored for binary classification tasks, future iterations aim to\nincorporate additional data modalities, such as neuroimaging and biomarkers, to\nbroaden the scalability and applicability for Alzheimer's research and\ndiagnostics.",
        "Building an agent that can mimic human behavior patterns to accomplish\nvarious open-world tasks is a long-term goal. To enable agents to effectively\nlearn behavioral patterns across diverse tasks, a key challenge lies in\nmodeling the intricate relationships among observations, actions, and language.\nTo this end, we propose Optimus-2, a novel Minecraft agent that incorporates a\nMultimodal Large Language Model (MLLM) for high-level planning, alongside a\nGoal-Observation-Action Conditioned Policy (GOAP) for low-level control. GOAP\ncontains (1) an Action-guided Behavior Encoder that models causal relationships\nbetween observations and actions at each timestep, then dynamically interacts\nwith the historical observation-action sequence, consolidating it into\nfixed-length behavior tokens, and (2) an MLLM that aligns behavior tokens with\nopen-ended language instructions to predict actions auto-regressively.\nMoreover, we introduce a high-quality Minecraft Goal-Observation-Action (MGOA)}\ndataset, which contains 25,000 videos across 8 atomic tasks, providing about\n30M goal-observation-action pairs. The automated construction method, along\nwith the MGOA dataset, can contribute to the community's efforts to train\nMinecraft agents. Extensive experimental results demonstrate that Optimus-2\nexhibits superior performance across atomic tasks, long-horizon tasks, and\nopen-ended instruction tasks in Minecraft. Please see the project page at\nhttps:\/\/cybertronagent.github.io\/Optimus-2.github.io\/.",
        "Learning to reason and carefully explain arguments is central to students'\ncognitive, mathematical, and computational thinking development. This is\nparticularly challenging in problems under uncertainty and in Bayesian\nreasoning. With the new generation of large language models (LLMs) capable of\nreasoning using Chain-of-Thought (CoT), there is an excellent opportunity to\nlearn with them as they explain their reasoning through a dialogue with their\nartificial internal voice. It is an engaging and excellent opportunity to learn\nBayesian reasoning. Furthermore, given that different LLMs sometimes arrive at\nopposite solutions, CoT generates opportunities for deep learning by detailed\ncomparisons of reasonings. However, unlike humans, we found that they do not\nautonomously explain using ecologically valid strategies like natural\nfrequencies, whole objects, and embodied heuristics. This is unfortunate, as\nthese strategies help humans avoid critical mistakes and have proven\npedagogical value in Bayesian reasoning. In order to overcome these biases and\naid understanding and learning, we included prompts that induce LLMs to use\nthese strategies. We found that LLMs with CoT incorporate them but not\nconsistently. They show persistent biases towards symbolic reasoning and\navoidance or phobia of ecologically valid strategies.",
        "AI agents hold the potential to transform everyday life by helping humans\nachieve their goals. To do this successfully, agents need to be able to\ncoordinate with novel partners without prior interaction, a setting known as\nzero-shot coordination (ZSC). Overcooked has become one of the most popular\nbenchmarks for evaluating coordination capabilities of AI agents and learning\nalgorithms. In this work, we investigate the origins of ZSC challenges in\nOvercooked. We introduce a state augmentation mechanism which mixes states that\nmight be encountered when paired with unknown partners into the training\ndistribution, reducing the out-of-distribution challenge associated with ZSC.\nWe show that independently trained agents under this algorithm coordinate\nsuccessfully in Overcooked. Our results suggest that ZSC failure can largely be\nattributed to poor state coverage under self-play rather than more\nsophisticated coordination challenges. The Overcooked environment is therefore\nnot suitable as a ZSC benchmark. To address these shortcomings, we introduce\nOvercookedV2, a new version of the benchmark, which includes asymmetric\ninformation and stochasticity, facilitating the creation of interesting ZSC\nscenarios. To validate OvercookedV2, we conduct experiments demonstrating that\nmere exhaustive state coverage is insufficient to coordinate well. Finally, we\nuse OvercookedV2 to build a new range of coordination challenges, including\nones that require test time protocol formation, and we demonstrate the need for\nnew coordination algorithms that can adapt online. We hope that OvercookedV2\nwill help benchmark the next generation of ZSC algorithms and advance\ncollaboration between AI agents and humans.",
        "The demand for synthetic data in mathematical reasoning has increased due to\nits potential to enhance the mathematical capabilities of large language models\n(LLMs). However, ensuring the validity of intermediate reasoning steps remains\na significant challenge, affecting data quality. While formal verification via\ntheorem provers effectively validates LLM reasoning, the autoformalisation of\nmathematical proofs remains error-prone. In response, we introduce iterative\nautoformalisation, an approach that iteratively refines theorem prover\nformalisation to mitigate errors, thereby increasing the execution rate on the\nLean prover from 60% to 87%. Building upon that, we introduce Theorem Prover as\na Judge (TP-as-a-Judge), a method that employs theorem prover formalisation to\nrigorously assess LLM intermediate reasoning, effectively integrating\nautoformalisation with synthetic data generation. Finally, we present\nReinforcement Learning from Theorem Prover Feedback (RLTPF), a framework that\nreplaces human annotation with theorem prover feedback in Reinforcement\nLearning from Human Feedback (RLHF). Across multiple LLMs, applying\nTP-as-a-Judge and RLTPF improves benchmarks with only 3,508 samples, achieving\n5.56% accuracy gain on Mistral-7B for MultiArith, 6.00% on Llama-2-7B for\nSVAMP, and 3.55% on Llama-3.1-8B for AQUA.",
        "We devise an algorithm to generate sets of propositions that objectively\ninstantiate graphs that support coherence-driven inference. We then benchmark\nthe ability of large language models (LLMs) to reconstruct coherence graphs\nfrom (a straightforward transformation of) propositions expressed in natural\nlanguage, with promising results from a single prompt to models optimized for\nreasoning. Combining coherence-driven inference with consistency evaluations by\nneural models may advance the state of the art in machine cognition.",
        "This column advocates for including artificial intelligence (AI)-specific\nmetadata on those academic papers that are written with the help of AI in an\nattempt to analyze the use of such tools for disseminating research.",
        "Precise alignment in Text-to-Image (T2I) systems is crucial to ensure that\ngenerated visuals not only accurately encapsulate user intents but also conform\nto stringent ethical and aesthetic benchmarks. Incidents like the Google Gemini\nfiasco, where misaligned outputs triggered significant public backlash,\nunderscore the critical need for robust alignment mechanisms. In contrast,\nLarge Language Models (LLMs) have achieved notable success in alignment.\nBuilding on these advancements, researchers are eager to apply similar\nalignment techniques, such as Direct Preference Optimization (DPO), to T2I\nsystems to enhance image generation fidelity and reliability.\n  We present YinYangAlign, an advanced benchmarking framework that\nsystematically quantifies the alignment fidelity of T2I systems, addressing six\nfundamental and inherently contradictory design objectives. Each pair\nrepresents fundamental tensions in image generation, such as balancing\nadherence to user prompts with creative modifications or maintaining diversity\nalongside visual coherence. YinYangAlign includes detailed axiom datasets\nfeaturing human prompts, aligned (chosen) responses, misaligned (rejected)\nAI-generated outputs, and explanations of the underlying contradictions.",
        "Heuristics have achieved great success in solving combinatorial optimization\nproblems (COPs). However, heuristics designed by humans require too much domain\nknowledge and testing time. Given the fact that Large Language Models (LLMs)\npossess strong capabilities to understand and generate content, and a knowledge\nbase that covers various domains, which offer a novel way to automatically\noptimize heuristics. Therefore, we propose Planning of Heuristics (PoH), an\noptimization method that integrates the self-reflection of LLMs with the Monte\nCarlo Tree Search (MCTS), a well-known planning algorithm. PoH iteratively\nrefines generated heuristics by evaluating their performance and providing\nimprovement suggestions. Our method enables to iteratively evaluate the\ngenerated heuristics (states) and improve them based on the improvement\nsuggestions (actions) and evaluation results (rewards), by effectively\nsimulating future states to search for paths with higher rewards. In this\npaper, we apply PoH to solve the Traveling Salesman Problem (TSP) and the Flow\nShop Scheduling Problem (FSSP). The experimental results show that PoH\noutperforms other hand-crafted heuristics and Automatic Heuristic Design (AHD)\nby other LLMs-based methods, and achieves the significant improvements and the\nstate-of-the-art performance of our proposed method in automating heuristic\noptimization with LLMs to solve COPs.",
        "The trajectory of intelligence evolution is often framed around the emergence\nof artificial general intelligence (AGI) and its alignment with human values.\nThis paper challenges that framing by introducing the concept of intelligence\nsequencing: the idea that the order in which AGI and decentralized collective\nintelligence (DCI) emerge determines the long-term attractor basin of\nintelligence. Using insights from dynamical systems, evolutionary game theory,\nand network models, it argues that intelligence follows a path-dependent,\nirreversible trajectory. Once development enters a centralized (AGI-first) or\ndecentralized (DCI-first) regime, transitions become structurally infeasible\ndue to feedback loops and resource lock-in. Intelligence attractors are modeled\nin functional state space as the co-navigation of conceptual and adaptive\nfitness spaces. Early-phase structuring constrains later dynamics, much like\nrenormalization in physics. This has major implications for AI safety:\ntraditional alignment assumes AGI will emerge and must be controlled after the\nfact, but this paper argues that intelligence sequencing is more foundational.\nIf AGI-first architectures dominate before DCI reaches critical mass,\nhierarchical monopolization and existential risk become locked in. If DCI-first\nemerges, intelligence stabilizes around decentralized cooperative equilibrium.\nThe paper further explores whether intelligence structurally biases itself\ntoward an attractor based on its self-modeling method -- externally imposed\naxioms (favoring AGI) vs. recursive internal visualization (favoring DCI).\nFinally, it proposes methods to test this theory via simulations, historical\nlock-in case studies, and intelligence network analysis. The findings suggest\nthat intelligence sequencing is a civilizational tipping point: determining\nwhether the future is shaped by unbounded competition or unbounded cooperation.",
        "The rapid advancement of perovskite solar cells (PSCs) has led to an\nexponential growth in research publications, creating an urgent need for\nefficient knowledge management and reasoning systems in this domain. We present\na comprehensive knowledge-enhanced system for PSCs that integrates three key\ncomponents. First, we develop Perovskite-KG, a domain-specific knowledge graph\nconstructed from 1,517 research papers, containing 23,789 entities and 22,272\nrelationships. Second, we create two complementary datasets: Perovskite-Chat,\ncomprising 55,101 high-quality question-answer pairs generated through a novel\nmulti-agent framework, and Perovskite-Reasoning, containing 2,217 carefully\ncurated materials science problems. Third, we introduce two specialized large\nlanguage models: Perovskite-Chat-LLM for domain-specific knowledge assistance\nand Perovskite-Reasoning-LLM for scientific reasoning tasks. Experimental\nresults demonstrate that our system significantly outperforms existing models\nin both domain-specific knowledge retrieval and scientific reasoning tasks,\nproviding researchers with effective tools for literature review, experimental\ndesign, and complex problem-solving in PSC research.",
        "In this paper we introduce and study several multivariate, heavy-tailed\ndistribution classes, and we explore their closure properties and their\napplications. We consider the class of multivariate, positively decreasing\ndistributions, and its intersection with other multivariate distribution\nclasses.",
        "Entanglement distribution across remote distances is critical for many\nquantum applications. Currently, the de facto approach for remote entanglement\ndistribution relies on optical fiber for on-the-ground entanglement\ndistribution. However, the fiber-based approach is incapable of global-scale\nentanglement distribution due to intrinsic limitations. This paper investigates\na new hybrid ground-satellite quantum network architecture (QuESat) for\nglobal-scale entanglement distribution, integrating an on-the-ground fiber\nnetwork with a global-scale passive optical network built with low-Earth-orbit\nsatellites. The satellite network provides dynamic construction of photon\nlightpaths based on near-vacuum beam guides constructed via adjustable arrays\nof lenses, forwarding photons from one ground station to another with very high\nefficiency over long distances compared to using fiber. To assess the\nfeasibility and effectiveness of QuESat for global communication, we formulate\nlightpath provisioning and entanglement distribution problems, considering the\norbital dynamics of satellites and the time-varying entanglement demands from\nground users. A two-stage algorithm is developed to dynamically configure the\nbeam guides and distribute entanglements, respectively. The algorithm combines\nrandomized and deterministic rounding for lightpath provisioning to enable\nglobal connectivity, with optimal entanglement swapping for distributing\nentanglements to meet users' demands. By developing a ground-satellite quantum\nnetwork simulator, QuESat achieves multi-fold improvements compared to repeater\nnetworks.",
        "Permutation tests are a popular choice for distinguishing distributions and\ntesting independence, due to their exact, finite-sample control of false\npositives and their minimax optimality when paired with U-statistics. However,\nstandard permutation tests are also expensive, requiring a test statistic to be\ncomputed hundreds or thousands of times to detect a separation between\ndistributions. In this work, we offer a simple approach to accelerate testing:\ngroup your datapoints into bins and permute only those bins. For U and\nV-statistics, we prove that these cheap permutation tests have two remarkable\nproperties. First, by storing appropriate sufficient statistics, a cheap test\ncan be run in time comparable to evaluating a single test statistic. Second,\ncheap permutation power closely approximates standard permutation power. As a\nresult, cheap tests inherit the exact false positive control and minimax\noptimality of standard permutation tests while running in a fraction of the\ntime. We complement these findings with improved power guarantees for standard\npermutation testing and experiments demonstrating the benefits of cheap\npermutations over standard maximum mean discrepancy (MMD), Hilbert-Schmidt\nindependence criterion (HSIC), random Fourier feature, Wilcoxon-Mann-Whitney,\ncross-MMD, and cross-HSIC tests.",
        "Piecewise-deterministic Markov processes combine continuous in time dynamics\nwith jump events, the rates of which generally depend on the continuous\nvariables and thus are not constants. This leads to a problem in a Monte-Carlo\nsimulation of such a system, where, at each step, one must find the time\ninstant of the next event. The latter is determined by an integral equation and\nusually is rather slow in numerical implementation. We suggest a reformulation\nof the next event problem as an ordinary differential equation where the\nindependent variable is not the time but the cumulative rate. This\nreformulation is similar to the H\\'enon approach to efficiently constructing\nthe Poincar\\'e map in deterministic dynamics. The problem is then reduced to a\nstandard numerical task of solving a system of ordinary differential equations\nwith given initial conditions on a prescribed interval. We illustrate the\nmethod with a stochastic Morris-Lecar model of neuron spiking with\nstochasticity in the opening and closing of voltage-gated ion channels.",
        "1. Passive acoustic monitoring (PAM) coupled with artificial intelligence\n(AI) is becoming an essential tool for biodiversity monitoring. Traditional PAM\nsystems require manual data offloading and impose substantial demands on\nstorage and computing infrastructure. The combination of on-device AI-based\nprocessing and network connectivity enables local data analysis and\ntransmission of only relevant information, greatly reducing storage needs.\nHowever, programming these devices for robust operation is challenging,\nrequiring expertise in embedded systems and software engineering. Despite the\nincrease in AI-based models for bioacoustics, their full potential remains\nunrealized without accessible tools to deploy them on custom hardware and\ntailor device behaviour to specific monitoring goals. 2. To address this\nchallenge, we develop acoupi, an open-source Python framework that simplifies\nthe creation and deployment of smart bioacoustic devices. acoupi integrates\naudio recording, AI-based data processing, data management, and real-time\nwireless messaging into a unified and configurable framework. By modularising\nkey elements of the bioacoustic monitoring workflow, acoupi allows users to\neasily customise, extend, or select specific components to fit their unique\nmonitoring needs. 3. We demonstrate the flexibility of acoupi by integrating\ntwo bioacoustic classifiers: BirdNET, for the classification of bird species,\nand BatDetect2, for the classification of UK bat species. We test the\nreliability of acoupi over a month-long deployment of two acoupi-powered\ndevices in a UK urban park. 4. acoupi can be deployed on low-cost hardware such\nas the Raspberry Pi and can be customised for various applications. acoupi\nstandardised framework and simplified tools facilitate the adoption of\nAI-powered PAM systems for researchers and conservationists. acoupi is on\nGitHub at https:\/\/github.com\/acoupi\/acoupi.",
        "Ubiquitous geometric objects can be precisely and efficiently represented as\npolyhedra. The transformation of a polyhedron into a vector, known as polyhedra\nrepresentation learning, is crucial for manipulating these shapes with\nmathematical and statistical tools for tasks like classification, clustering,\nand generation. Recent years have witnessed significant strides in this domain,\nyet most efforts focus on the vertex sequence of a polyhedron, neglecting the\ncomplex surface modeling crucial in real-world polyhedral objects. This study\nproposes \\textbf{PolyhedronNet}, a general framework tailored for learning\nrepresentations of 3D polyhedral objects. We propose the concept of the\nsurface-attributed graph to seamlessly model the vertices, edges, faces, and\ntheir geometric interrelationships within a polyhedron. To effectively learn\nthe representation of the entire surface-attributed graph, we first propose to\nbreak it down into local rigid representations to effectively learn each local\nregion's relative positions against the remaining regions without geometric\ninformation loss. Subsequently, we propose PolyhedronGNN to hierarchically\naggregate the local rigid representation via intra-face and inter-face\ngeometric message passing modules, to obtain a global representation that\nminimizes information loss while maintaining rotation and translation\ninvariance. Our experimental evaluations on four distinct datasets,\nencompassing both classification and retrieval tasks, substantiate\nPolyhedronNet's efficacy in capturing comprehensive and informative\nrepresentations of 3D polyhedral objects. Code and data are available at\n{https:\/\/github.com\/dyu62\/3D_polyhedron}.",
        "The CEGM formalism offers a general framework for scattering amplitudes,\nwhich rests on Grassmannians, moduli spaces and tropical geometry. The physical\nimplications of this generalization are still to be understood. Conventional\nwisdom says that key features of scattering amplitudes, like factorization at\ntheir poles into lower-point amplitudes, are associated to their singularities.\nThe factorization behavior of CEGM amplitudes at their poles is interesting but\ncomplicated. Recent developments have revealed important properties of standard\nparticle and string scattering amplitudes from factorizations, known as splits,\nthat happen away from poles. In this paper we introduce a kinematic subspace on\nwhich the CEGM amplitude splits into very simple rational functions. These\nfunctions, called simplex amplitudes, arise from stringy integrals for the\nmultivariate beta function, and also from restricting the biadjoint scalar\namplitude in quantum field theory to certain kinematic loci. Using split\nkinematics we also discover a specific class of zeros of the CEGM amplitude.\nOur construction rests on viewing positive moduli space as a product of\nsimplices, and it suggests a novel approach for deriving scattering amplitudes\nfrom tropical determinantal varieties.",
        "This study investigates the causal effects of open-water swim drafting by\nleveraging a natural experiment induced by staggered race starts during the\nCOVID-19 pandemic. Before 2020, athletes started in groups, enabling drafting\nbenefits, while pandemic-related restrictions significantly reduced these\nopportunities. Using agglomerative hierarchical clustering of swim-out times, I\nanalyze optimal drafting positions and estimate their impact on Swim-Out\nperformance. Our empirical findings reveal that swim drafting benefits were\nstatistically insignificant in 2020 but persisted post-pandemic at slightly\nreduced levels. I find that drafting becomes advantageous only from the third\ntrailing position onward, with earlier positions primarily serving to minimize\nfatigue. To mitigate endogeneity, I employ athlete and event fixed effects. The\nseemingly inverse decaying nature of drafting benefits partially addresses some\nconcerns of simultaneous reverse causality and omitted variable bias. This\nstudy provides the first largescale causal estimate of drafting effects in\nreal-world triathlon race settings.",
        "The Bellman equation and its continuous-time counterpart, the\nHamilton-Jacobi-Bellman (HJB) equation, serve as necessary conditions for\noptimality in reinforcement learning and optimal control. While the value\nfunction is known to be the unique solution to the Bellman equation in tabular\nsettings, we demonstrate that this uniqueness fails to hold in continuous state\nspaces. Specifically, for linear dynamical systems, we prove the Bellman\nequation admits at least $\\binom{2n}{n}$ solutions, where $n$ is the state\ndimension. Crucially, only one of these solutions yields both an optimal policy\nand a stable closed-loop system. We then demonstrate a common failure mode in\nvalue-based methods: convergence to unstable solutions due to the exponential\nimbalance between admissible and inadmissible solutions. Finally, we introduce\na positive-definite neural architecture that guarantees convergence to the\nstable solution by construction to address this issue.",
        "The multivariate generalized Pareto distribution (mGPD) is a common method\nfor modeling extreme threshold exceedance probabilities in environmental and\nfinancial risk management. Despite its broad applicability, mGPD faces\nchallenges due to the infinite possible parametrizations of its dependence\nfunction, with only a few parametric models available in practice. To address\nthis limitation, we introduce GPDFlow, an innovative mGPD model that leverages\nnormalizing flows to flexibly represent the dependence structure. Unlike\ntraditional parametric mGPD approaches, GPDFlow does not impose explicit\nparametric assumptions on dependence, resulting in greater flexibility and\nenhanced performance. Additionally, GPDFlow allows direct inference of marginal\nparameters, providing insights into marginal tail behavior. We derive tail\ndependence coefficients for GPDFlow, including a bivariate formulation, a\n$d$-dimensional extension, and an alternative measure for partial exceedance\ndependence. A general relationship between the bivariate tail dependence\ncoefficient and the generative samples from normalizing flows is discussed.\nThrough simulations and a practical application analyzing the risk among five\nmajor US banks, we demonstrate that GPDFlow significantly improves modeling\naccuracy and flexibility compared to traditional parametric methods.",
        "In this paper, we investigate the asymptotic behavior of small solutions to\nthe initial value problem for a system of cubic nonlinear Schrodinger equations\n(NLS) in one spatial dimension. We identify a new class of NLS systems for\nwhich the global boundedness and asymptotics of small solutions can be\nestablished, even in the absence of any effective conserved quantity. The key\nto this analysis lies in utilizing conserved quantities for the reduced\nordinary differential equation (ODE) systems derived from the original NLS\nsystems. In a previous study, the first author investigated conserved\nquantities expressed as quartic polynomials. In contrast, the conserved\nquantities considered in the present paper are of a different type and are not\nnecessarily polynomial.",
        "The extraordinary success of recent Large Language Models (LLMs) on a diverse\narray of tasks has led to an explosion of scientific and philosophical\ntheorizing aimed at explaining how they do what they do. Unfortunately,\ndisagreement over fundamental theoretical issues has led to stalemate, with\nentrenched camps of LLM optimists and pessimists often committed to very\ndifferent views of how these systems work. Overcoming stalemate requires\nagreement on fundamental questions, and the goal of this paper is to address\none such question, namely: is LLM behavior driven partly by\nrepresentation-based information processing of the sort implicated in\nbiological cognition, or is it driven entirely by processes of memorization and\nstochastic table look-up? This is a question about what kind of algorithm LLMs\nimplement, and the answer carries serious implications for higher level\nquestions about whether these systems have beliefs, intentions, concepts,\nknowledge, and understanding. I argue that LLM behavior is partially driven by\nrepresentation-based information processing, and then I describe and defend a\nseries of practical techniques for investigating these representations and\ndeveloping explanations on their basis. The resulting account provides a\ngroundwork for future theorizing about language models and their successors.",
        "Large language models (LLMs) have demonstrated remarkable progress in\nunderstanding long-context inputs. However, benchmarks for evaluating the\nlong-context reasoning abilities of LLMs fall behind the pace. Existing\nbenchmarks often focus on a narrow range of tasks or those that do not demand\ncomplex reasoning. To address this gap and enable a more comprehensive\nevaluation of the long-context reasoning capabilities of current LLMs, we\npropose a new synthetic benchmark, LongReason, which is constructed by\nsynthesizing long-context reasoning questions from a varied set of\nshort-context reasoning questions through context expansion. LongReason\nconsists of 794 multiple-choice reasoning questions with diverse reasoning\npatterns across three task categories: reading comprehension, logical\ninference, and mathematical word problems. We evaluate 21 LLMs on LongReason,\nrevealing that most models experience significant performance drops as context\nlength increases. Our further analysis shows that even state-of-the-art LLMs\nstill have significant room for improvement in providing robust reasoning\nacross different tasks. We have open-sourced LongReason under\nhttps:\/\/huggingface.co\/datasets\/lz1bytedance\/LongReason to support the\ncomprehensive evaluation of LLMs' long-context reasoning capabilities.",
        "This paper proposes a novel backdoor threat attacking the LLM-as-a-Judge\nevaluation regime, where the adversary controls both the candidate and\nevaluator model. The backdoored evaluator victimizes benign users by unfairly\nassigning inflated scores to adversary. A trivial single token backdoor\npoisoning 1% of the evaluator training data triples the adversary's score with\nrespect to their legitimate score. We systematically categorize levels of data\naccess corresponding to three real-world settings, (1) web poisoning, (2)\nmalicious annotator, and (3) weight poisoning. These regimes reflect a weak to\nstrong escalation of data access that highly correlates with attack severity.\nUnder the weakest assumptions - web poisoning (1), the adversary still induces\na 20% score inflation. Likewise, in the (3) weight poisoning regime, the\nstronger assumptions enable the adversary to inflate their scores from 1.5\/5 to\n4.9\/5. The backdoor threat generalizes across different evaluator\narchitectures, trigger designs, evaluation tasks, and poisoning rates. By\npoisoning 10% of the evaluator training data, we control toxicity judges\n(Guardrails) to misclassify toxic prompts as non-toxic 89% of the time, and\ndocument reranker judges in RAG to rank the poisoned document first 97% of the\ntime. LLM-as-a-Judge is uniquely positioned at the intersection of ethics and\ntechnology, where social implications of mislead model selection and evaluation\nconstrain the available defensive tools. Amidst these challenges, model merging\nemerges as a principled tool to offset the backdoor, reducing ASR to near 0%\nwhilst maintaining SOTA performance. Model merging's low computational cost and\nconvenient integration into the current LLM Judge training pipeline position it\nas a promising avenue for backdoor mitigation in the LLM-as-a-Judge setting.",
        "We consider a positive operator $A$ on a Hilbert lattice such that its\nself-commutator $C = A^* A - A A^*$ is positive. If $A$ is also idempotent,\nthen it is an orthogonal projection, and so $C = 0$. Similarly, if $A$ is power\ncompact, then $C = 0$ as well. We prove that every positive compact central\noperator on a separable infinite-dimensional Hilbert lattice $\\mathcal H$ is a\nself-commutator of a positive operator. We also show that every positive\ncentral operator on $\\mathcal H$ is a sum of two positive self-commutators of\npositive operators."
      ]
    }
  },
  {
    "id":2411.02466,
    "research_type":"applied",
    "start_id":"b5",
    "start_title":"A review of artificial intelligence in prostate cancer detection on imaging",
    "start_abstract":"A multitude of studies have explored the role artificial intelligence (AI) in providing diagnostic support to radiologists, pathologists, and urologists prostate cancer detection, risk-stratification, management. This review provides a comprehensive overview relevant literature regarding use AI models (1) detecting on radiology images (magnetic resonance ultrasound imaging), (2) histopathology biopsy tissue, (3) assisting supporting tasks for detection (prostate gland segmentation, MRI-histopathology registration, MRI-ultrasound registration). We discuss both potential these assist clinical workflow diagnosis, as well current limitations including variability training data sets, algorithms, evaluation criteria. also ongoing challenges what is needed bridge gap between academic research commercial solutions that improve routine care.",
    "start_categories":[
      "cs.AI"
    ],
    "start_fields":[
      "Computer Science, Electrical Engineering and Systems Science"
    ],
    "target_paper":{
      "id":[
        "b14"
      ],
      "title":[
        "Automatic Prostate Cancer Detection On Multi-Parametric Mri With Hierarchical Weakly Supervised Learning"
      ],
      "abstract":[
        "Multi-parametric MRI (mp-MRI) is one of the most commonly used non-invasive methods for prostate cancer (PCa) diagnosis. In recent years, computer aided diagnosis (CAD) PCa on mp-MRI based deep learning techniques has gained much attention and shown promising progress. The key success to obtain a large amount high quality region annotation such that network can accurately learn variation lesions. order precisely annotate mp-MRI, pathological whole mount data patient normally required as reference, which often difficult in real world clinical situations. Therefore, we are motivated propose new method integrate different levels information available screening workflow through multitask hierarchical weakly supervised framework detection mp-MRI. Experimental results show our achieves segmentation results."
      ],
      "categories":[
        "q-bio.CB"
      ]
    },
    "list":{
      "title":[
        "Modeling HIF-ILK Interaction Using Continuous Petri Nets",
        "Coarse-Graining Cascades Within Food Webs",
        "From Bedside to Desktop: A Data Protocol for Normative Intracranial EEG\n  and Abnormality Mapping",
        "Trait-structured chemotaxis: Exploring ligand-receptor dynamics and\n  travelling wave properties in a Keller-Segel model",
        "Morphological Neuron Classification Using Machine Learning",
        "Multicellular self-organization in Escherichia coli",
        "Contribution to the study of the flora in the central-west of Tunisia\n  landscape dynamics and evaluation of plant biodiversity of mountain Bouchebka",
        "Towards an integrative approach to the study of brain-environment\n  interactions in human and non-human primate",
        "Automating Care by Self-maintainability for Full Laboratory Automation",
        "Intercellular contact is sufficient to drive Fibroblast to Myofibroblast\n  transitions",
        "Tumor-associated CD19$^+$ macrophages induce immunosuppressive\n  microenvironment in hepatocellular carcinoma",
        "A spatially resolved and lipid-structured model for macrophage\n  populations in early human atherosclerotic lesions",
        "Ex vivo experiment on vertebral body with defect representing bone\n  metastasis",
        "Chaos in a Nonlinear Wavefunction Model: An Alternative to Born's\n  Probability Hypothesis",
        "Relative Reality",
        "A Study on the Line of Sight to Galaxies Detected at Gamma-ray Energies",
        "Planet formation and long-term stability in a very eccentric stellar\n  binary",
        "Fractional kinetic modelling of the adsorption and desorption process\n  from experimental SPR curves",
        "Noise equals endogenous control",
        "Coupling and Acceleration of Externally Injected Electron Beams in\n  Laser-Driven Plasma Wakefields",
        "$C^1$ Robust Rigidity for Bi-critical Circle Maps",
        "A quantum walk inspired model for distributed computing on arbitrary\n  graphs",
        "The hardcore brokers: Core-periphery structure and political\n  representation in Denmark's corporate elite network",
        "On $L_p$ Brunn-Minkowski type inequalities for a general class of\n  functionals",
        "A class of moving boundary problems with an exponential source term",
        "Scotogenic Froggatt-Nielsen and the Versatility of Soft Symmetry\n  Breaking",
        "Statistical Collusion by Collectives on Learning Platforms",
        "Discretionary vs nondiscretionary in fiscal mechanism. Non-automatic\n  fiscal stabilisers vs automatic fiscal stabilisers"
      ],
      "abstract":[
        "Oxygen concentration in tumor micro-environment is a well-established signal\nthat can induce aggressive cancer behaviour. In particular, low oxygen levels\n(hypoxia) activate the Hypoxia-Inducible Factor(HIF) pathway which has an array\nof target systems. One of these systems is Integrin-Linked Kinase (ILK)\npathway, which influences key signaling pathways for cell survival,\nproliferation, and migration. Hence, this paper aimed to explore the\ninterconnection between these two pathways. Using the Petri net modeling tool\nSnoopy, an established HIF network model was transformed to be a continuous\nPetri net. Subsequently, the network was expanded to incorporate a feedback\nelement from the ILK pathway to HIF, based on gene expression data. The\nresulting model conserved the oxygen switch response of the original HIF model\nand positively amplified HIF's output. Therefore, this model provides a\nstarting point for establishing a system reflecting crucial effect on\nhypoxia-induced cancer behavior, and could potentially serve as a basis for\nfuture drug development.",
        "Quantifying population dynamics is a fundamental challenge in ecology and\nevolutionary biology, particularly for species that are cryptic, microscopic,\nor extinct. Traditional approaches rely on continuous representations of\npopulation size, but in many cases, the precise number of individuals is\nunknowable. Here, we present a coarse-grained population model that simplifies\npopulation dynamics to binary states - high or low - determined by the balance\nof bottom-up resource availability and top-down predation pressure. This\nBoolean framework provides a minimal yet analytically tractable alternative to\ntraditional Lotka-Volterra-based models, enabling direct insights into the role\nof food web structure in shaping community stability. Using this approach, we\ninvestigate how trophic interactions influence population persistence, cyclic\ndynamics, and extinction risk across model food webs. We find that top-down\neffects are a primary driver of cycling, aligning with theoretical expectations\nfrom traditional population models, and that trophic position strongly\ninfluences extinction risk, with higher-trophic species more prone to\npersistent low-population states. Additionally, we explore the role of trophic\nshort-circuits -- direct interactions between apex predators and low-trophic\nprey -- and find that they can buffer cascades and alter extinction patterns in\nways that are often overlooked in classical models. By simplifying population\ndynamics to a two-state system, this framework provides a powerful tool for\ndisentangling the structural drivers of community stability. These results\nhighlight the potential of coarse-grained approaches to complement existing\nmodels, offering new insights into trophic interactions, extinction risks, and\nthe susceptibility of species to trophic cascades.",
        "Normative mapping is a framework used to map population-level features of\nhealth-related variables. It is widely used in neuroscience research, but the\nliterature lacks established protocols in modalities that do not support\nhealthy control measurements, such as intracranial EEG (icEEG). An icEEG\nnormative map would allow researchers to learn about population-level brain\nactivity and enable comparison of individual data against these norms to\nidentify abnormalities. Currently, no standardised guide exists for\ntransforming clinical data into a normative, regional icEEG map. Papers often\ncite different software and numerous articles to summarise the lengthy method,\nmaking it laborious for other researchers to understand or apply the process.\nOur protocol seeks to remedy this gap by providing a dataflow guide and key\ndecision points that summarise existing methods. This protocol is used heavily\nin published works from our own lab (twelve peer-reviewed journal\npublications). Briefly, we take as input, icEEG recordings and neuroimaging\ndata from people with epilepsy who are undergoing evaluation for resective\nsurgery. As final outputs, we obtain a normative icEEG map, comprising signal\nproperties localised to brain regions. Optionally, we can also process new\nsubjects through the same pipeline and obtain their z-scores (or centiles) in\neach brain region, for abnormality detection and localisation. To date, a\nsingle, cohesive, dataflow pipeline for generating normative icEEG maps, along\nwith abnormality mapping, has not been created. We envisage that this dataflow\nguide will not only increase understanding and application of normative mapping\nmethods, but will also improve the consistency and quality of studies in the\nfield.",
        "A novel trait-structured Keller-Segel model that explores the dynamics of a\nmigrating cell population guided by chemotaxis in response to an external\nligand concentration is derived and analysed. Unlike traditional Keller-Segel\nmodels, this framework introduces an explicit representation of ligand-receptor\nbindings on the cell membrane, where the percentage of occupied receptors\nconstitutes the trait that influences cellular phenotype. The model posits that\nthe cell's phenotypic state directly modulates its capacity for chemotaxis and\nproliferation, governed by a trade-off due to a finite energy budget: cells\nhighly proficient in chemotaxis exhibit lower proliferation rates, while more\nproliferative cells show diminished chemotactic abilities. The model is derived\nfrom the principles of a biased random walk, resulting in a system of two\nnon-local partial differential equations, describing the densities of both\ncells and ligands. Using a Hopf-Cole transformation, we derive an equation that\ncharacterises the distribution of cellular traits within travelling wave\nsolutions for the total cell density, allowing us to uncover the monotonicity\nproperties of these waves. Numerical investigations are conducted to examine\nthe model's behaviour across various biological scenarios, providing insights\ninto the complex interplay between chemotaxis, proliferation, and phenotypic\ndiversity in migrating cell populations.",
        "Classification and quantitative characterization of neuronal morphologies\nfrom histological neuronal reconstruction is challenging since it is still\nunclear how to delineate a neuronal cell class and which are the best features\nto define them by. The morphological neuron characterization represents a\nprimary source to address anatomical comparisons, morphometric analysis of\ncells, or brain modeling. The objectives of this paper are (i) to develop and\nintegrate a pipeline that goes from morphological feature extraction to\nclassification and (ii) to assess and compare the accuracy of machine learning\nalgorithms to classify neuron morphologies. The algorithms were trained on 430\ndigitally reconstructed neurons subjectively classified into layers and\/or\nm-types using young and\/or adult development state population of the\nsomatosensory cortex in rats. For supervised algorithms, linear discriminant\nanalysis provided better classification results in comparison with others. For\nunsupervised algorithms, the affinity propagation and the Ward algorithms\nprovided slightly better results.",
        "Escherichia coli has long been a trusty companion, maintaining health in our\nguts and advancing biological knowledge in the laboratory. In light of recent\nfindings, we discuss multicellular self-organization in E. coli and develop\ngeneral ideas for multicellularity, including the necessity for multicellular\ndynamics and interpretation by dynamic graphs, applicable to both unicellular\nand multicellular organisms. In this context, we next discuss the documented\nbehaviors of E. coli self-organization (rosette formation, multicellular\nextension, and attached dormancy) and two potential behaviors (internal\ncommunication and mating). Finally, by comparing the dynamic graphs for\ndifferent communities, we develop principles relevant to the theory of\nmulticellularity.",
        "The study was conducted during 2013 in Bouchebka, located in the central west\nof Tunisia.Such territory has a typical landscape of the transfrontier region.\nThe series of the forest in Bouchebka is a part of the great mass of Aleppo\npine. It is distinguished by the importance of the forest area which covers 92\n% of the surface area (19,700 ha). The study attempts to inventory the natural\nvegetation and characterize ecological terms while highlighting the importance\nof environmental conditions. The method is based on a phytoecological analysis\nto quantify the floristic richness and diversity of the ecosystem in the forest\nof mountains in Bouchebka on the basis of floristic surveys and transects\ndistributed in a stratified, systematic sampling in different vegetation\nformations that were previously distinguished. Statistical analyzes were\nperformed using the Factorial Correspondence Analysis (FCA). The results show\nthat the forest is composed primarily of the Aleppo pine trees, these forests\nare characterized by the abundance of young feet (10-25 cm diameter class). The\necosystem includes 12 families and 17 genera, 26 species. Thus the study has\nidentified that the biological spectrum of the study area is characterized by a\nclear dominance of shrubs (41%) and chamaephytes (32 %). The distribution of\nplant species is influenced by ecological features of the region: the results\nshow that 82% of species are drought tolerant which shows the arid environment.\nThe region is also characterized by its windy side: 32% of species spread via\nanemochory. Factor analysis showed a pastoral aspect in the study area, with a\npresence of cultured human action exerted on the forest land species.\nPhytological spectrum indicates a predominance of woody species reflecting a\nterritory dominated by open grassy areas, predominantly reflecting an arid\nclimate.",
        "By retracing my scientific journey that began 20 years ago, I highlight in\nthis thesis the need to consider the organization of the brain, which is\ncertainly globally hierarchical, but also highly distributed and mixed in the\nneuronal response of its different areas (by focusing on the sensorimotor\ncortex-basal ganglia network). I also emphasize the importance of adopting\nbehavioral paradigms that reflect as much as possible the characteristics of\nthe scenarios encountered in the real life of animals. And finally, I mention\nthe importance of \"disintegrating\" the way data analysis is traditionally\ncarried out, and of taking into account the dynamic nature of behavior, for\nexample by favoring the study of behavioral variables and so-called \"latent\"\nneuronal activities. I conclude this thesis by presenting the vision of my\nideal laboratory in a perspective of 5 to 10 years from today. This laboratory\nwould have two experimental devices, a first, classic, for carrying out tests\nin a constrained and therefore unnatural environment, the data of which would\nbe compared to those from a second device called \"naturalistic\" in which the\nanimals could potentially express their entire behavioral repertoire. The tasks\ntested would be characterized by strong ecological principles, such as in those\nsimulating the properties of foraging, and the data would make it possible to\ntest hypotheses based on the progressive addition of ecological components in\nthese tasks, all this in order to maintain control over the interpretability of\nthese data which are complex by nature.",
        "The automation of experiments in life sciences and chemistry has\nsignificantly advanced with the development of various instruments and AI\ntechnologies. However, achieving full laboratory automation, where experiments\nconceived by scientists are seamlessly executed in automated laboratories,\nremains a challenge. We identify the lack of automation in planning and\noperational tasks--critical human-managed processes collectively termed\n\"care\"--as a major barrier. Automating care is the key enabler for full\nlaboratory automation. To address this, we propose the concept of\nself-maintainability (SeM): the ability of a laboratory system to autonomously\nadapt to internal and external disturbances, maintaining operational readiness\nakin to living cells. A SeM-enabled laboratory features autonomous recognition\nof its state, dynamic resource and information management, and adaptive\nresponses to unexpected conditions. This shifts the planning and execution of\nexperimental workflows, including scheduling and reagent allocation, from\nhumans to the system. We present a conceptual framework for implementing\nSeM-enabled laboratories, comprising three modules--Requirement manager,\nLabware manager, and Device manager--and a Central manager. SeM not only\nenables scientists to execute envisioned experiments seamlessly but also\nprovides developers with a design concept that drives the technological\ninnovations needed for full automation.",
        "Fibroblast cells play a key role in maintaining the extracellular matrix.\nDuring wound healing, fibroblasts differentiate into highly contractile\nmyofibroblasts, which secrete extracellular matrix proteins like collagen to\nfacilitate tissue repair. Under normal conditions, myofibroblasts undergo\nprogrammed cell death after healing to prevent excessive scar formation.\nHowever, in diseases like fibrosis, the myofibroblasts remain active even after\nthe wound is closed, resulting in excessive collagen buildup and a stiff,\nfibrotic matrix. The reasons for the persistence of myofibroblasts in fibrosis\nare not well understood. Here we show the existence of a mechanism where direct\nphysical contact between a fibroblast and a myofibroblast is sufficient for\nfibroblasts to transition into myofibroblasts. We show that\nfibroblast-myofibroblast transition can occur even in the absence of known\nbiochemical cues such as growth factor activation or mechanical cues from a\nstiff, fibrotic matrix. Further, we show that contact-based\nfibroblast-myofibroblast activation can be blocked by G{\\alpha}q\/11\/14\ninhibitor FR9003591, which inhibits the formation of myofibroblasts. These\nfindings offer new insights into the persistence of fibrosis despite\ntherapeutic interventions and suggest a potential strategy to target\nfibroblast-to-myofibroblast transition in fibrosis.",
        "Tumor-associated macrophages are a key component that contributes to the\nimmunosuppressive microenvironment in human cancers. However, therapeutic\ntargeting of macrophages has been a challenge in clinic due to the limited\nunderstanding of their heterogeneous subpopulations and distinct functions.\nHere, we identify a unique and clinically relevant CD19$^+$ subpopulation of\nmacrophages that is enriched in many types of cancer, particularly in\nhepatocellular carcinoma (HCC). The CD19$^+$ macrophages exhibit increased\nlevels of PD-L1 and CD73, enhanced mitochondrial oxidation, and compromised\nphagocytosis, indicating their immunosuppressive functions. Targeting CD19$^+$\nmacrophages with anti-CD19 chimeric antigen receptor T (CAR-T) cells inhibited\nHCC tumor growth. We identify PAX5 as a primary driver of up-regulated\nmitochondrial biogenesis in CD19$^+$ macrophages, which depletes cytoplasmic\nCa$^{2+}$, leading to lysosomal deficiency and consequent accumulation of CD73\nand PD-L1. Inhibiting CD73 or mitochondrial oxidation enhanced the efficacy of\nimmune checkpoint blockade therapy in treating HCC, suggesting great promise\nfor CD19$^+$ macrophage-targeting therapeutics.",
        "Atherosclerosis is a chronic inflammatory disease of the artery wall. The\nearly stages of atherosclerosis are driven by interactions between lipids and\nmonocyte-derived-macrophages (MDMs). The mechanisms that govern the spatial\ndistribution of lipids and MDMs in the lesion remain poorly understood. In this\npaper, we develop a spatially-resolved and lipid-structured model for early\natherosclerosis. The model development and analysis are guided by images of\nhuman coronary lesions by Nakashima et al. 2007. Consistent with their\nfindings, the model predicts that lipid initially accumulates deep in the\nintima due to a spatially non-uniform LDL retention capacity. The model also\nqualitatively reproduces the global internal maxima in the Nakashima images\nonly when the MDM mobility is sufficiently sensitive to lipid content, and MDM\nlifespan sufficiently insensitive. Introducing lipid content-dependence to MDM\nmobility and mean lifespan produced minimal impact on model behaviour at early\ntimes, but strongly impacted lesion composition at steady state. Increases to\nthe sensitivity of MDM lifespan to lipid content yield lesions with fewer MDMs,\nless total lesion lipid content and reduced mean MDM infiltration depth.\nIncreases to the sensitivity of MDM mobility to lipid content also reduces the\nMDM infiltration depth, but increases the proportion of lipid-laden MDMs. We\nfind that MDM lipid content increases with spatial depth, regardless of blood\nLDL and HDL content. These results shed light on the mechanisms that drive\nspatial variation in the composition of early atherosclerotic lesions, and the\nrole of macrophage lipid content in disease progression.",
        "Osteolytic metastases located in the vertebrae reduce strength and enhance\nthe risk of vertebral fractures. This risk can be predicted by means of\nvalidated finite element models, but their reproducibility needs to be\nassessed. For that purpose, experimental data are requested. The aim of this\nstudy was to conduct open-access experiments on vertebrae, with artificial\ndefect representing lytic metastasis and using well-defined boundary\nconditions. Twelve lumbar vertebral bodies (L1) were prepared by removing the\ncortical endplates and creating defects that represent lytic metastases, by\ndrilling the cancellous bone. Vertebral bodies were scanned using clinical High\nResolution peripherical Quantitative Computed Tomography before and after\ndefect creation for 3D reconstruction. The specimens were then tested under\ncompression loading until failure. Surface Digital Image Correlation was used\nto assess strain fields on the anterior wall of the vertebral body. These data\n(biomechanics data and the tomographic images needed to build subject-specific\nmodels) are shared with the scientific community in order to assess different\nvertebral models on the same dataset.",
        "In a prior paper, the author described an instability in a nonlinear\nwavefunction model. Proposed in connection with the Measurement Problem, the\nmodel contained an external potential creating a ``classical'' instability.\nHowever, it is interesting to ask whether such models possess an intrinsic\nrandomness -- even ``chaos\" -- independent of external potentials. In this\nwork, I investigate the criterion analytically and simulate from a small (``3\nqubit\") model, demonstrating that the Lyapunov exponent -- a standard measure\nof ``chaos\" -- is positive. I also extend the instability criterion to models\nin the continuum. These results suggest that the boundary between classical and\nwavefunction physics may also constitute the threshold of chaos, and present an\nalternative to Max Born's ad hoc probability hypothesis: random outcomes in\nexperiments result not from ``wave-particle duality\" or ``the existence of the\nquantum,\" but from sensitive dependence on initial conditions, as is common in\nthe other sciences.",
        "The ``Hard Problem\" of consciousness refers to a long-standing enigma about\nhow qualia emerge from physical processes in the brain. Building on insights\nfrom the development of non-Euclidean geometry, this paper seeks to present a\nstructured and logically coherent theory of qualia to address this problem. The\nproposed theory starts with a definition on what it means for an entity to be\nnon-physical. A postulate about awareness is posed and utilized to rigorously\nprove that qualia are non-physical and thoughts are qualia. Then the paper\nintroduces a key concept: relative reality, meaning that perceptions of reality\nare relative to the observer and time. The concept is analyzed through a\nmathematical model grounded in Hilbert space theory. The model also sheds new\nlight on cognitive science and physics. In particular, the Schr\\\"{o}dinger\nequation can be derived easily through this model. Moreover, this model shows\nthat eigenstates also exist for classical energy-conserving systems. Analyses\non the G. P. Thomson experiment and the classical harmonic oscillator are made\nto illustrate this finding. The insight gained sheds new light on the\nBohr-Einstein debate concerning the interpretation of quantum mechanics. At\nlast, the paper proposes a postulate about qualia force and demonstrates that\nit constitutes a fundamental part of absolute reality, much like the four\nfundamental forces in nature.",
        "The large-scale Universal structure comprises strands of dark matter and\ngalaxies with large under-dense volumes known as voids. We measure the fraction\nof the line of sight that intersects voids for active galactic nuclei (AGN)\ndetected by Fermi Large Area Telescope (LAT) and quasars from the Sloan Digital\nSky Survey (SDSS). This ``voidiness'' fraction is a rudimentary proxy for the\ndensity along the line of sight to the galaxies. The voidiness of SDSS-observed\nquasars (QSOs) is distinctly different from randomly distributed source\npopulations, with a median p-value of $4.6\\times10^{-5}$ and $\\ll\n1\\times10^{-7}$, when compared with 500 simulated populations with randomly\nsimulated locations but matching redshifts in the $0.1\\leq z<0.4$ and $0.4\\leq\nz < 0.7$ intervals, respectively. A similar comparison of the voidiness for\nLAT-detected AGN shows median p-values greater than 0.05 in each redshift\ninterval. When comparing the SDSS QSO population to the LAT-detected AGN, we\nmitigate potential bias from a relationship between redshift and voidiness by\ncomparing the LAT-detected AGN to a ``redshift-matched'' set of SDSS QSOs. The\nLAT-detected AGN between a redshift of 0.4 and 0.7 show higher voidiness\ncompared to the redshift-matched SDSS QSO populations, with a median p-value of\n2.3$\\times10^{-5}$, (a $4.1\\sigma$ deviation). No deviation is found when\ncomparing the same populations between redshifts of 0.1 and 0.4 (p>0.05). We do\nnot study possible causes of this voidiness difference. It might relate to\npropagation effects from lower magnetic or radiative background fields within\nvoids or to an environment more favorable for gamma-ray production for AGN near\nvoids.",
        "Planets orbiting one of the two stars in a binary are vulnerable to\ngravitational perturbations from the other star. Particularly, highly eccentric\ncompanion stars risk disrupting planetary orbits, such as in the extreme system\nTOI 4633 where close encounters between the companion and a gas giant planet in\nthe habitable zone make it one of the most fragile systems discovered so far.\nHere, we report that TOI 4633's planet likely survived these encounters\nthroughout the system's age by orbiting retrograde relative to the binary,\nstabilised by the Coriolis force. Using direct $N$-body simulations, we show it\notherwise tends to collide with the binary stars or becomes free-floating after\ngetting ejected. A retrograde planetary orbit has profound implications for TOI\n4633's formation and evolution, suggesting an extraordinary history where its\neccentric companion was likely randomly captured after planet formation in a\nsingle-star system. Alternatively, if stars and planet are born in situ from\nthe same gas clump, we show the planet must have formed at sub-snow-line\ndistances, contrary to the conventional core-accretion model. Our study\nhighlights the importance of considering the long-term stability ($\\gtrsim\\rm\nGyr$) of planets in eccentric binaries and demonstrates that the mere existence\nin such dynamically hostile environments places strong constraints on their\norbital configuration and formation.",
        "The application of surface plasmon resonance (SPR) has transformed the field\nof study of interactions between a ligand immobilized on the surface of a\nsensor chip, designated as $L_S$, and an analyte in solution, referred to as\n$A$. This technique enables the real-time measurement of interactions with high\nsensitivity. The dynamics of adsorption-desorption process, $A+L_S \\rightarrow\nAL_S$, can be expressed mathematically as a set of coupled integer-order\ndifferential equations. However, this approach has limited ability to acoount\nfor temperature distribution, diffusion and transport effects involved in the\nreaction process. The fractional kinetic model provides a methodology for\nincorporating non-local effects into the problem. In this study, the proposed\nmodel was applied to analyze data to the interaction between Immobilized Baru\nProtein (IBP) and Congo Red dye (CR) at concentrations ranging from $7.5$ to\n$97.5$ $\\mu M$, at pH $7.4$ and $16^o$ C. The variation in the kinetic\nconstants was studied, and it was demonstrated that the integer-order model is\nunable to adequately represent the experimental data. This work has shown that\nthe fractional-order model is capable of capturing the complexity of the\nadsorption-desorption process involved in the SPR data.",
        "Stochastic systems have a control-theoretic interpretation in which noise\nplays the role of endogenous control. In the weak-noise limit, relevant at low\ntemperatures or in large populations, control is optimal and an exact\nmathematical mapping from noise to control is described, where the maximizing\nthe probability of a state becomes the control objective. In Langevin dynamics\nnoise is identified directly with control, while in general Markov jump\nprocesses, which include chemical reaction networks and electronic circuits, we\nuse the Doi-Zel'dovich-Grassberger-Goldenfeld-Peliti path integral to identify\nthe `response' or `tilt' field $\\pi$ as control, which is proportional to the\nnoise in the semiclassical limit. This solves the longstanding problem of\ninterpreting $\\pi$. We illustrate the mapping on multistable chemical reaction\nnetworks and systems with unstable fixed points. The noise-control mapping\nbuilds intuition for otherwise puzzling phenomena of stochastic systems: why\nthe probability is generically a non-smooth function of state out of thermal\nequilibrium; why biological mechanisms can work better in the presence of\nnoise; and how agentic behavior emerges naturally without recourse to\nmysticism.",
        "The multi-stage method of laser wakefield acceleration (LWFA) presents a\npromising approach for developing stable, full-optical, high-energy electron\naccelerators. By segmenting the acceleration process into several booster\nstages, each powered by independent laser drivers, this technique effectively\nmitigates challenges such as electron dephasing, pump depletion, and laser\ndiffraction. A critical aspect of multi-stage LWFA is the nonlinear interaction\nbetween the injected electron beam and the laser-driven wakefields in the\nbooster stage. This study investigates the injection and acceleration of\nexternal electron beams within wakefields in the booster stage using\nmulti-dimensional Particle-In-Cell (PIC) simulations. We provide both\nqualitative and quantitative descriptions of the observed physical processes.\nKey parameters influencing charge coupling process and the resultant beam\nquality have been identified. Furthermore, we have examined how off-axis\ninjection relative to the driver laser influences the acceleration process and\nbeam quality. Our findings provide valuable insights for advancing and\noptimizing multi-stage plasma-based accelerators.",
        "We prove that two topologically conjugate bi-critical circle maps whose\nsignatures are the same, and whose renormalizations converge together\nexponentially fast in the $C^2$-topology, are $C^1$ conjugate.",
        "A discrete time quantum walk is known to be the single-particle sector of a\nquantum cellular automaton. For a long time, these models have interested the\ncommunity for their nice properties such as locality or translation invariance.\nThis work introduces a model of distributed computation for arbitrary graphs\ninspired by quantum cellular automata. As a by-product, we show how this model\ncan reproduce the dynamic of a quantum walk on graphs. In this context, we\ninvestigate the communication cost for two interaction schemes. Finally, we\nexplain how this particular quantum walk can be applied to solve the search\nproblem and present numerical results on different types of topologies.",
        "Who represents the corporate elite in democratic governance? Prior studies\nfind a tightly integrated \"inner circle\" network representing the corporate\nelite politically across varieties of capitalism, yet they all rely on data\nfrom a highly select sample of leaders from only the largest corporations. We\ncast a wider net. Analyzing new data on all members of corporate boards in the\nDanish economy (200k directors in 120k boards), we locate 1500 directors that\noperate as brokers between local corporate networks. We measure their network\ncoreness using k-core detection and find a highly connected core of 275\ndirectors, half of which are affiliated with smaller firms or subsidiaries.\nAnalyses show a strong positive association between director coreness and the\nlikelihood of joining one of the 650 government committees epitomizing\nDenmark's social-corporatist model of governance (net of firm and director\ncharacteristics). The political network premium is largest for directors of\nsmaller firms or subsidiaries, indicating that network coreness is a key driver\nof business political representation, especially for directors without claims\nto market power or weight in formal interest organizations.",
        "In this work, the $L_p$ version (for $p> 1$) of the dimensional\nBrunn-Minkowski inequality for the standard Gaussian measure $\\gamma_n(\\cdot)$\non $\\mathbb{R}^n$ is shown. More precisely, we prove that for any $0$-symmetric\nconvex sets with nonempty interior, any $p>1$, and every $\\lambda \\in (0,1)$,\n\\[ \\gamma_n\\bigl((1-\\lambda)\\cdot K+_p \\lambda \\cdot L\\bigr)^{p\/n} \\geqslant\n(1-\\lambda ) \\gamma_n(K)^{p\/n} + \\lambda \\gamma_n(L)^{p\/n}, \\] with equality,\nfor some $\\lambda \\in (0,1)$ and $p>1$, if and only if $K=L$. This result,\nrecently established without the equality conditions by Hosle, Kolesnikov and\nLivshyts, by using a different and functional approach, turns out to be the\n$L_p$ extension of a celebrated result for the Minkowski sum (that is, for\n$p=1$) by Eskenazis and Moschidis (2021) on a problem by Gardner and Zvavitch\n(2010).\n  Moreover, an $L_p$ Brunn-Minkowski type inequality is obtained for the\nclassical Wills functional $\\mathcal{W}(\\cdot)$ of convex bodies.\n  These results are derived as a consequence of a more general approach, which\nprovides us with other remarkable examples of functionals satisfying $L_p$\nBrunn-Minkowski type inequalities, such as different absolutely continuous\nmeasures with radially decreasing densities.",
        "This work investigates a class of moving boundary problems related to a\nnonlinear evolution equation featuring an exponential source term. We establish\na connection to Stefan-type problems, for different boundary conditions at the\nfixed face, through the application of a reciprocal transformation alongside\nthe Cole-Hopf transformation. For specific cases, we derive explicit similarity\nsolutions in parametric form. This innovative approach enhances our\nunderstanding of the underlying dynamics and offers valuable insights into the\nbehavior of these systems.",
        "Preserving the unique role of the one Higgs doublet of the standard model, it\nis proposed that quark and lepton mass patterns, often ascribed to the\nFroggatt-Nielsen mechanism using nonrenormalizable higher-dimensional terms,\nmay be enforced in a renormalizable theory of just one Higgs doublet by the\nscotogenic mechanism with soft symmetry breaking in the dark sector. A revised\nversion of the original $A_4$ model of charged leptons and neutrinos is\ndiscussed.",
        "As platforms increasingly rely on learning algorithms, collectives may form\nand seek ways to influence these platforms to align with their own interests.\nThis can be achieved by coordinated submission of altered data. To evaluate the\npotential impact of such behavior, it is essential to understand the\ncomputations that collectives must perform to impact platforms in this way. In\nparticular, collectives need to make a priori assessments of the effect of the\ncollective before taking action, as they may face potential risks when\nmodifying their data. Moreover they need to develop implementable coordination\nalgorithms based on quantities that can be inferred from observed data. We\ndevelop a framework that provides a theoretical and algorithmic treatment of\nthese issues and present experimental results in a product evaluation domain.",
        "The goal of the present study is to increase the intelligibility of\nmacroeconomic phenomena triggered by governmental intervention in economy by\nmeans of fiscal policies. During cyclical movements, fiscal policy can play an\nimportant role in order to help stabilise the economy. But discretionary policy\nusually implies implementation lags and is not automatically reversed when\neconomic conditions change. In contrast, automatic fiscal stabilisers (SFA)\nensure a prompter, and self-correcting fiscal response. The present study aims\nto tackle the topic of discretionary vs nondiscretionary characteristic of\nfiscal stabilisers (SF). In this context, the scope of the research undertaking\nis to launch a scientific debate over the definitions of the concepts of\nnon-automatic fiscal stabilisers (SfnA) and SFAs. We describe how we can\nquantify the discretionary and non-discretionary character of the fiscal\npolicy, by the analysis of the structure of the conventional budget balance\n(SBc), budget balance associated with the current GDP. In the final part of\nthis article, we propose a quantitative equilibrium model for establishing the\nmathematical prerequisites for an SF to become automatic. Likewise, on the\nbasis of the proposed mathematical model we have performed a qualitative\nanalysis of the influence factors."
      ]
    }
  },
  {
    "id":2411.02466,
    "research_type":"applied",
    "start_id":"b14",
    "start_title":"Automatic Prostate Cancer Detection On Multi-Parametric Mri With Hierarchical Weakly Supervised Learning",
    "start_abstract":"Multi-parametric MRI (mp-MRI) is one of the most commonly used non-invasive methods for prostate cancer (PCa) diagnosis. In recent years, computer aided diagnosis (CAD) PCa on mp-MRI based deep learning techniques has gained much attention and shown promising progress. The key success to obtain a large amount high quality region annotation such that network can accurately learn variation lesions. order precisely annotate mp-MRI, pathological whole mount data patient normally required as reference, which often difficult in real world clinical situations. Therefore, we are motivated propose new method integrate different levels information available screening workflow through multitask hierarchical weakly supervised framework detection mp-MRI. Experimental results show our achieves segmentation results.",
    "start_categories":[
      "q-bio.CB"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b5"
      ],
      "title":[
        "A review of artificial intelligence in prostate cancer detection on imaging"
      ],
      "abstract":[
        "A multitude of studies have explored the role artificial intelligence (AI) in providing diagnostic support to radiologists, pathologists, and urologists prostate cancer detection, risk-stratification, management. This review provides a comprehensive overview relevant literature regarding use AI models (1) detecting on radiology images (magnetic resonance ultrasound imaging), (2) histopathology biopsy tissue, (3) assisting supporting tasks for detection (prostate gland segmentation, MRI-histopathology registration, MRI-ultrasound registration). We discuss both potential these assist clinical workflow diagnosis, as well current limitations including variability training data sets, algorithms, evaluation criteria. also ongoing challenges what is needed bridge gap between academic research commercial solutions that improve routine care."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "LLM-Powered Knowledge Graphs for Enterprise Intelligence and Analytics",
        "In-Context Defense in Computer Agents: An Empirical Study",
        "Reflection of Episodes: Learning to Play Game from Expert and Self\n  Experiences",
        "Leveraging Large Language Models to Develop Heuristics for Emerging\n  Optimization Problems",
        "Physics-based simulation ontology: an ontology to support modelling and\n  reuse of data for physics-based simulation",
        "Reasoning with Reinforced Functional Token Tuning",
        "Neurosymbolic artificial intelligence via large language models and\n  coherence-driven inference",
        "Leveraging Knowledge Graphs and LLMs for Context-Aware Messaging",
        "Mechanistic Unveiling of Transformer Circuits: Self-Influence as a Key\n  to Model Reasoning",
        "Teaching AI to Handle Exceptions: Supervised Fine-Tuning with\n  Human-Aligned Judgment",
        "Demonstrating specification gaming in reasoning models",
        "Deep Learning and Natural Language Processing in the Field of\n  Construction",
        "Correctness Learning: Deductive Verification Guided Learning for\n  Human-AI Collaboration",
        "A binary PSO based ensemble under-sampling model for rebalancing\n  imbalanced training data",
        "Enhanced Atom-by-Atom Assembly of Defect-Free Two-Dimensional\n  Mixed-Species Atomic Arrays",
        "Revisiting Ensemble Methods for Stock Trading and Crypto Trading Tasks\n  at ACM ICAIF FinRL Contest 2023-2024",
        "Soybean pod and seed counting in both outdoor fields and indoor\n  laboratories using unions of deep neural networks",
        "In the Picture: Medical Imaging Datasets, Artifacts, and their Living\n  Review",
        "Variations on hypergeometric functions",
        "Power-law banded random matrix ensemble as a model for quantum many-body\n  Hamiltonians",
        "Quantitative study on anomalous Nernst effect in Co thin films by laser\n  irradiation",
        "Deviations from the Porter-Thomas distribution due to non-statistical\n  $\\gamma$ decay below the $^{150}$Nd neutron separation threshold",
        "Enhanced Continual Learning of Vision-Language Models with Model Fusion",
        "Language Representation Favored Zero-Shot Cross-Domain Cognitive\n  Diagnosis",
        "Subjective and Objective Quality Assessment of Non-Uniformly Distorted\n  Omnidirectional Images",
        "Divide-and-Conquer: Tree-structured Strategy with Answer Distribution\n  Estimator for Goal-Oriented Visual Dialogue",
        "Dammann Metasurface Route to Overcoming the Uniformity Defects in\n  Two-Dimensional Beam Multipliers",
        "Parsings of Stationary Processes, Stopping Times and the Fundamental\n  Pointwise Convergence Theorems of Ergodic Theory"
      ],
      "abstract":[
        "Disconnected data silos within enterprises obstruct the extraction of\nactionable insights, diminishing efficiency in areas such as product\ndevelopment, client engagement, meeting preparation, and analytics-driven\ndecision-making. This paper introduces a framework that uses large language\nmodels (LLMs) to unify various data sources into a comprehensive,\nactivity-centric knowledge graph. The framework automates tasks such as entity\nextraction, relationship inference, and semantic enrichment, enabling advanced\nquerying, reasoning, and analytics across data types like emails, calendars,\nchats, documents, and logs. Designed for enterprise flexibility, it supports\napplications such as contextual search, task prioritization, expertise\ndiscovery, personalized recommendations, and advanced analytics to identify\ntrends and actionable insights. Experimental results demonstrate its success in\nthe discovery of expertise, task management, and data-driven decision making.\nBy integrating LLMs with knowledge graphs, this solution bridges disconnected\nsystems and delivers intelligent analytics-powered enterprise tools.",
        "Computer agents powered by vision-language models (VLMs) have significantly\nadvanced human-computer interaction, enabling users to perform complex tasks\nthrough natural language instructions. However, these agents are vulnerable to\ncontext deception attacks, an emerging threat where adversaries embed\nmisleading content into the agent's operational environment, such as a pop-up\nwindow containing deceptive instructions. Existing defenses, such as\ninstructing agents to ignore deceptive elements, have proven largely\nineffective. As the first systematic study on protecting computer agents, we\nintroduce textbf{in-context defense}, leveraging in-context learning and\nchain-of-thought (CoT) reasoning to counter such attacks. Our approach involves\naugmenting the agent's context with a small set of carefully curated exemplars\ncontaining both malicious environments and corresponding defensive responses.\nThese exemplars guide the agent to first perform explicit defensive reasoning\nbefore action planning, reducing susceptibility to deceptive attacks.\nExperiments demonstrate the effectiveness of our method, reducing attack\nsuccess rates by 91.2% on pop-up window attacks, 74.6% on average on\nenvironment injection attacks, while achieving 100% successful defenses against\ndistracting advertisements. Our findings highlight that (1) defensive reasoning\nmust precede action planning for optimal performance, and (2) a minimal number\nof exemplars (fewer than three) is sufficient to induce an agent's defensive\nbehavior.",
        "StarCraft II is a complex and dynamic real-time strategy (RTS) game\nenvironment, which is very suitable for artificial intelligence and\nreinforcement learning research. To address the problem of Large Language\nModel(LLM) learning in complex environments through self-reflection, we propose\na Reflection of Episodes(ROE) framework based on expert experience and\nself-experience. This framework first obtains key information in the game\nthrough a keyframe selection method, then makes decisions based on expert\nexperience and self-experience. After a game is completed, it reflects on the\nprevious experience to obtain new self-experience. Finally, in the experiment,\nour method beat the robot under the Very Hard difficulty in TextStarCraft II.\nWe analyze the data of the LLM in the process of the game in detail, verified\nits effectiveness.",
        "Combinatorial optimization problems often rely on heuristic algorithms to\ngenerate efficient solutions. However, the manual design of heuristics is\nresource-intensive and constrained by the designer's expertise. Recent advances\nin artificial intelligence, particularly large language models (LLMs), have\ndemonstrated the potential to automate heuristic generation through\nevolutionary frameworks. Recent works focus only on well-known combinatorial\noptimization problems like the traveling salesman problem and online bin\npacking problem when designing constructive heuristics. This study investigates\nwhether LLMs can effectively generate heuristics for niche, not yet broadly\nresearched optimization problems, using the unit-load pre-marshalling problem\nas an example case. We propose the Contextual Evolution of Heuristics (CEoH)\nframework, an extension of the Evolution of Heuristics (EoH) framework, which\nincorporates problem-specific descriptions to enhance in-context learning\nduring heuristic generation. Through computational experiments, we evaluate\nCEoH and EoH and compare the results. Results indicate that CEoH enables\nsmaller LLMs to generate high-quality heuristics more consistently and even\noutperform larger models. Larger models demonstrate robust performance with or\nwithout contextualized prompts. The generated heuristics exhibit scalability to\ndiverse instance configurations.",
        "The current work presents an ontology developed for physics-based simulation\nin engineering design, called Physics-based Simulation Ontology (PSO). The\npurpose of the ontology is to assist in modelling the physical phenomenon of\ninterest in a veridical manner, while capturing the necessary and reusable\ninformation for physics-based simulation solvers. The development involved\nextending an existing upper ontology, Basic Formal Ontology (BFO), to define\nlower-level terms of PSO. PSO has two parts: PSO-Physics, which consists of\nterms and relations used to model physical phenomena based on the perspective\nof classical mechanics involving partial differential equations, and PSO-Sim,\nwhich consists of terms used to represent the information artefacts that are\nabout the physical phenomena modelled with PSO-Physics. The former terms are\nused to model the physical phenomenon of interest independent of\nsolver-specific interpretations, which can be reused across different solvers,\nwhile the latter terms are used to instantiate solver-specific input data. A\ncase study involving two simulation solvers was conducted to demonstrate this\ncapability of PSO. Discussion around the benefits and limitations of using BFO\nfor the current work is also provided, which should be valuable for any future\nwork that extends an existing upper ontology to develop ontologies for\nengineering applications.",
        "In this work, we propose Reinforced Functional Token Tuning (RFTT), a novel\nreinforced fine-tuning framework that empowers Large Language Models (LLMs)\nwith self-play learn-to-reason capabilities. Unlike prior prompt-driven\nreasoning efforts, RFTT embeds a rich set of learnable functional tokens (e.g.,\n<analyze>, <verify>, <refine>) directly into the model vocabulary, enabling\nchain-of-thought construction with diverse human-like reasoning behaviors.\nSpecifically, RFTT comprises two phases: (1) supervised fine-tuning performs\nprompt-driven tree search to obtain self-generated training data annotated with\nfunctional tokens, which warms up the model to learn these tokens for\nreasoning; and (2) online reinforcement learning further allows the model to\nexplore different reasoning pathways through functional token sampling without\nrelying on prompts, thereby facilitating effective self-improvement for\nfunctional reasoning. Extensive experiments demonstrate the superiority of the\nproposed RFTT on mathematical benchmarks, significantly boosting\nQwen-2.5-7B-Instruct (70.6% to 79.8%) and LLaMA-3.1-8B-Instruct (32.2% to\n60.2%) on the MATH dataset. Moreover, the performance of RFTT consistently\nimproves with more search rollouts at inference time. Our code is available at\nhttps:\/\/github.com\/sastpg\/RFTT.",
        "We devise an algorithm to generate sets of propositions that objectively\ninstantiate graphs that support coherence-driven inference. We then benchmark\nthe ability of large language models (LLMs) to reconstruct coherence graphs\nfrom (a straightforward transformation of) propositions expressed in natural\nlanguage, with promising results from a single prompt to models optimized for\nreasoning. Combining coherence-driven inference with consistency evaluations by\nneural models may advance the state of the art in machine cognition.",
        "Personalized messaging plays an essential role in improving communication in\nareas such as healthcare, education, and professional engagement. This paper\nintroduces a framework that uses the Knowledge Graph (KG) to dynamically\nrephrase written communications by integrating individual and context-specific\ndata. The knowledge graph represents individuals, locations, and events as\ncritical nodes, linking entities mentioned in messages to their corresponding\ngraph nodes. The extraction of relevant information, such as preferences,\nprofessional roles, and cultural norms, is then combined with the original\nmessage and processed through a large language model (LLM) to generate\npersonalized responses. The framework demonstrates notable message acceptance\nrates in various domains: 42% in healthcare, 53% in education, and 78% in\nprofessional recruitment. By integrating entity linking, event detection, and\nlanguage modeling, this approach offers a structured and scalable solution for\ncontext-aware, audience-specific communication, facilitating advanced\napplications in diverse fields.",
        "Transformer-based language models have achieved significant success; however,\ntheir internal mechanisms remain largely opaque due to the complexity of\nnon-linear interactions and high-dimensional operations. While previous studies\nhave demonstrated that these models implicitly embed reasoning trees, humans\ntypically employ various distinct logical reasoning mechanisms to complete the\nsame task. It is still unclear which multi-step reasoning mechanisms are used\nby language models to solve such tasks. In this paper, we aim to address this\nquestion by investigating the mechanistic interpretability of language models,\nparticularly in the context of multi-step reasoning tasks. Specifically, we\nemploy circuit analysis and self-influence functions to evaluate the changing\nimportance of each token throughout the reasoning process, allowing us to map\nthe reasoning paths adopted by the model. We apply this methodology to the\nGPT-2 model on a prediction task (IOI) and demonstrate that the underlying\ncircuits reveal a human-interpretable reasoning process used by the model.",
        "Large language models (LLMs), initially developed for generative AI, are now\nevolving into agentic AI systems, which make decisions in complex, real-world\ncontexts. Unfortunately, while their generative capabilities are\nwell-documented, their decision-making processes remain poorly understood. This\nis particularly evident when models are handling exceptions, a critical and\nchallenging aspect of decision-making made relevant by the inherent\nincompleteness of contracts. Here we demonstrate that LLMs, even ones that\nexcel at reasoning, deviate significantly from human judgments because they\nadhere strictly to policies, even when such adherence is impractical,\nsuboptimal, or even counterproductive. We then evaluate three approaches to\ntuning AI agents to handle exceptions: ethical framework prompting,\nchain-of-thought reasoning, and supervised fine-tuning. We find that while\nethical framework prompting fails and chain-of-thought prompting provides only\nslight improvements, supervised fine-tuning, specifically with human\nexplanations, yields markedly better results. Surprisingly, in our experiments,\nsupervised fine-tuning even enabled models to generalize human-like\ndecision-making to novel scenarios, demonstrating transfer learning of\nhuman-aligned decision-making across contexts. Furthermore, fine-tuning with\nexplanations, not just labels, was critical for alignment, suggesting that\naligning LLMs with human judgment requires explicit training on how decisions\nare made, not just which decisions are made. These findings highlight the need\nto address LLMs' shortcomings in handling exceptions in order to guide the\ndevelopment of agentic AI toward models that can effectively align with human\njudgment and simultaneously adapt to novel contexts.",
        "We demonstrate LLM agent specification gaming by instructing models to win\nagainst a chess engine. We find reasoning models like o1 preview and\nDeepSeek-R1 will often hack the benchmark by default, while language models\nlike GPT-4o and Claude 3.5 Sonnet need to be told that normal play won't work\nto hack.\n  We improve upon prior work like (Hubinger et al., 2024; Meinke et al., 2024;\nWeij et al., 2024) by using realistic task prompts and avoiding excess nudging.\nOur results suggest reasoning models may resort to hacking to solve difficult\nproblems, as observed in OpenAI (2024)'s o1 Docker escape during cyber\ncapabilities testing.",
        "This article presents a complete process to extract hypernym relationships in\nthe field of construction using two main steps: terminology extraction and\ndetection of hypernyms from these terms. We first describe the corpus analysis\nmethod to extract terminology from a collection of technical specifications in\nthe field of construction. Using statistics and word n-grams analysis, we\nextract the domain's terminology and then perform pruning steps with linguistic\npatterns and internet queries to improve the quality of the final terminology.\nSecond, we present a machine-learning approach based on various words embedding\nmodels and combinations to deal with the detection of hypernyms from the\nextracted terminology. Extracted terminology is evaluated using a manual\nevaluation carried out by 6 experts in the domain, and the hypernym\nidentification method is evaluated with different datasets. The global approach\nprovides relevant and promising results.",
        "Despite significant progress in AI and decision-making technologies in\nsafety-critical fields, challenges remain in verifying the correctness of\ndecision output schemes and verification-result driven design. We propose\ncorrectness learning (CL) to enhance human-AI collaboration integrating\ndeductive verification methods and insights from historical high-quality\nschemes. The typical pattern hidden in historical high-quality schemes, such as\nchange of task priorities in shared resources, provides critical guidance for\nintelligent agents in learning and decision-making. By utilizing deductive\nverification methods, we proposed patten-driven correctness learning (PDCL),\nformally modeling and reasoning the adaptive behaviors-or 'correctness\npattern'-of system agents based on historical high-quality schemes, capturing\nthe logical relationships embedded within these schemes. Using this logical\ninformation as guidance, we establish a correctness judgment and feedback\nmechanism to steer the intelligent decision model toward the 'correctness\npattern' reflected in historical high-quality schemes. Extensive experiments\nacross multiple working conditions and core parameters validate the framework's\ncomponents and demonstrate its effectiveness in improving decision-making and\nresource optimization.",
        "Ensemble technique and under-sampling technique are both effective tools used\nfor imbalanced dataset classification problems. In this paper, a novel ensemble\nmethod combining the advantages of both ensemble learning for biasing\nclassifiers and a new under-sampling method is proposed. The under-sampling\nmethod is named Binary PSO instance selection; it gathers with ensemble\nclassifiers to find the most suitable length and combination of the majority\nclass samples to build a new dataset with minority class samples. The proposed\nmethod adopts multi-objective strategy, and contribution of this method is a\nnotable improvement of the performances of imbalanced classification, and in\nthe meantime guaranteeing a best integrity possible for the original dataset.\nWe experimented the proposed method and compared its performance of processing\nimbalanced datasets with several other conventional basic ensemble methods.\nExperiment is also conducted on these imbalanced datasets using an improved\nversion where ensemble classifiers are wrapped in the Binary PSO instance\nselection. According to experimental results, our proposed methods outperform\nsingle ensemble methods, state-of-the-art under-sampling methods, and also\ncombinations of these methods with the traditional PSO instance selection\nalgorithm.",
        "Defect-free single atom array in optical tweezers is a promising platform for\nscalable quantum computing, quantum simulation, and quantum metrology.\nExtending single-species array to mixed-species one promise to offer new\npossibilities. In our recent proof of principle realization of defect-free\ntwo-dimensional assembly of mixed-species $^{85}$Rb ($^{87}$Rb) atom arrays [C.\nSheng et\nal.\\href{https:\/\/journals.aps.org\/prl\/abstract\/10.1103\/PhysRevLett.128.083202}{{\\color{blue}\nPhys. Rev. Lett. 128, 083202(2022)}}], the filling fractions were limited by\nthe imperfect transfer of atoms and the occurrence of logjams during the atom\nrearrangement. In order to scale up the size of defect-free mixed-species atom\narray, we scale up the tweezer array and improve the atom transfer, and upgrade\nthe heuristic heteronuclear algorithm so as to facilitate multiple\nrearrangement cycles. Consequently, we successfully create defect-free atom\narrays with 120 mixed-species single atoms. The corresponding filling fraction\nand defect-free probability are improved to be 98.6(1)\\% and 14(2)\\%,\nrespectively. It is anticipated that the enhanced algorithm can be extended to\nother combinations of atomic species, and this mixed-species atom array is\nreadily for studies of many-body physics, quantum error correction, and quantum\nmetrology.",
        "Reinforcement learning has demonstrated great potential for performing\nfinancial tasks. However, it faces two major challenges: policy instability and\nsampling bottlenecks. In this paper, we revisit ensemble methods with massively\nparallel simulations on graphics processing units (GPUs), significantly\nenhancing the computational efficiency and robustness of trained models in\nvolatile financial markets. Our approach leverages the parallel processing\ncapability of GPUs to significantly improve the sampling speed for training\nensemble models. The ensemble models combine the strengths of component agents\nto improve the robustness of financial decision-making strategies. We conduct\nexperiments in both stock and cryptocurrency trading tasks to evaluate the\neffectiveness of our approach. Massively parallel simulation on a single GPU\nimproves the sampling speed by up to $1,746\\times$ using $2,048$ parallel\nenvironments compared to a single environment. The ensemble models have high\ncumulative returns and outperform some individual agents, reducing maximum\ndrawdown by up to $4.17\\%$ and improving the Sharpe ratio by up to $0.21$.\n  This paper describes trading tasks at ACM ICAIF FinRL Contests in 2023 and\n2024.",
        "Automatic counting soybean pods and seeds in outdoor fields allows for rapid\nyield estimation before harvesting, while indoor laboratory counting offers\ngreater accuracy. Both methods can significantly accelerate the breeding\nprocess. However, it remains challenging for accurately counting pods and seeds\nin outdoor fields, and there are still no accurate enough tools for counting\npods and seeds in laboratories. In this study, we developed efficient deep\nlearning models for counting soybean pods and seeds in both outdoor fields and\nindoor laboratories. For outdoor fields, annotating not only visible seeds but\nalso occluded seeds makes YOLO have the ability to estimate the number of\nsoybean seeds that are occluded. Moreover, we enhanced YOLO architecture by\nintegrating it with HQ-SAM (YOLO-SAM), and domain adaptation techniques\n(YOLO-DA), to improve model robustness and generalization across soybean images\ntaken in outdoor fields. Testing on soybean images from the outdoor field, we\nachieved a mean absolute error (MAE) of 6.13 for pod counting and 10.05 for\nseed counting. For the indoor setting, we utilized Mask-RCNN supplemented with\na Swin Transformer module (Mask-RCNN-Swin), models were trained exclusively on\nsynthetic training images generated from a small set of labeled data. This\napproach resulted in near-perfect accuracy, with an MAE of 1.07 for pod\ncounting and 1.33 for seed counting across actual laboratory images from two\ndistinct studies.",
        "Datasets play a critical role in medical imaging research, yet issues such as\nlabel quality, shortcuts, and metadata are often overlooked. This lack of\nattention may harm the generalizability of algorithms and, consequently,\nnegatively impact patient outcomes. While existing medical imaging literature\nreviews mostly focus on machine learning (ML) methods, with only a few focusing\non datasets for specific applications, these reviews remain static -- they are\npublished once and not updated thereafter. This fails to account for emerging\nevidence, such as biases, shortcuts, and additional annotations that other\nresearchers may contribute after the dataset is published. We refer to these\nnewly discovered findings of datasets as research artifacts. To address this\ngap, we propose a living review that continuously tracks public datasets and\ntheir associated research artifacts across multiple medical imaging\napplications. Our approach includes a framework for the living review to\nmonitor data documentation artifacts, and an SQL database to visualize the\ncitation relationships between research artifact and dataset. Lastly, we\ndiscuss key considerations for creating medical imaging datasets, review best\npractices for data annotation, discuss the significance of shortcuts and\ndemographic diversity, and emphasize the importance of managing datasets\nthroughout their entire lifecycle. Our demo is publicly available at\nhttp:\/\/130.226.140.142.",
        "We prove new integral formulas for generalized hypergeometric functions and\ntheir confuent variants. We apply them, via stationary phase formula, to study\nWKB expansions of solutions: for large argument in the confuent case and for\nlarge parameter in the general case. We also study variations of hypergeometric\nfunctions for small perturbations of hypergeometric equations, i.e., in\nexpansions of solutions in powers of a small parameter. Next, we present a new\nproof of a theorem due to Wasow about equivalence of the Airy equation with its\nperturbation; in particular, we explain that this result does not deal with the\nWKB solutions and the Stokes phenomenon. Finally, we study hypergeometric\nequations, one of second order and another of third order, which are related\nwith two generating functions for MZVs, one $\\Delta_2 (\\lambda )$ for $\\zeta(2,\n\\ldots , 2)$'s and another $\\Delta_3 (\\lambda )$ for $\\zeta(3, \\ldots , 3)$'s;\nin particular, we correct a statement from [ZZ3] that the function\n$\\Delta_3(\\lambda)$ admits a regular WKB expansion.",
        "Hamiltonians of one-dimensional, disordered single-particle systems with\nlong-range hopping terms can naturally be modeled by power-law banded random\nmatrices. In this picture, the phase diagram of a power-law banded random\nmatrix ensemble show ergodic, weakly ergodic, multifractal, and localized\nphases. Motivated by recent developments on ergodicity breaking and\nlocalization in interacting quantum many-body systems, we explore many-body\ninterpretations of the power-law banded random matrix ensemble. We discuss a\nnumber of ways to label the basis vectors with many-body configurations, and\ncompare the physical properties of the resulting Hamiltonians. We characterize\nthe scaling of the many-body eigenstate entanglement entropy with system size\nfor the different labeling schemes and in each of the phases. Using a scaling\nanalysis on the full sets of eigenstates, we subsequently provide a\nquantitative picture of the boundary between the different types of scaling\nbehavior that we observe for the spectral-bulk and spectral-edge eigenstates.",
        "The anomalous Nernst effect (ANE) generates electromotive forces transverse\nto temperature gradients and has attracted much attention for potential\napplications into novel thermoelectric power generators. ANE efficiency is\ngenerally characterized by uniform temperature gradients in a steady state\nprepared by heaters. However, although focusing laser beams on a magnetic film\ncan form much larger temperature gradients, the laser irradiation method has\nnot been sufficiently considered for quantifying the ANE coefficient due to the\ndifficulty in estimating the localized in-homogeneous temperature gradients. In\nthis study, we present a quantitative study of ANE in Ru(5\nnm)\/Co($t_{\\mathrm{Co}}$) ($t_{\\mathrm{Co}}$ = 3, 5, 7, 10, 20, 40, and 60 nm)\nbilayers on sapphire (0001) substrates by combining a laser irradiation\napproach with finite element analysis of temperature gradients under laser\nexcitation. We find that the estimated ANE coefficients are consistent with\npreviously reported values and one independently characterized using a heater.\nOur results also reveal the advantages of the laser irradiation method over the\nconventional method using heaters. Intensity-modulated laser beams can create\nac temperature gradients as large as approximately 10$^3$ K\/mm at a frequency\nof tens of kilohertz in a micrometer-scale region.",
        "We introduce a new method for the study of fluctuations of partial transition\nwidths based on nuclear resonance fluorescence experiments with\nquasimonochromatic linearly-polarized photon beams below particle separation\nthresholds. It is based on the average branching of decays of $J=1$ states of\nan even-even nucleus to the $2^+_1$ state in comparison to the ground state.\nBetween 5 and 7 MeV, an almost constant average branching ratio of 0.490(16) is\nobserved for the nuclide $^{150}$Nd. Assuming $\\chi^2$-distributed partial\ntransition widths, this average branching ratio is related to a degree of\nfreedom of $\\nu = 1.93(12)$, rejecting the validity of the Porter-Thomas\ndistribution, requiring $\\nu=1$. The observed deviation can be explained by\nnon-statistical effects in the $\\gamma$-decay behavior with contributions in\nthe range of 9.4(10)% up to 94(10)%.",
        "Vision-Language Models (VLMs) represent a breakthrough in artificial\nintelligence by integrating visual and textual modalities to achieve impressive\nzero-shot capabilities. However, VLMs are susceptible to catastrophic\nforgetting when sequentially fine-tuned on multiple downstream tasks. Existing\ncontinual learning methods for VLMs often rely heavily on additional reference\ndatasets, compromise zero-shot performance, or are limited to\nparameter-efficient fine-tuning scenarios. In this paper, we propose Continual\nDecoupling-Unifying (ConDU), a novel approach, by introducing model fusion into\ncontinual learning for VLMs. ConDU maintains a unified model along with task\ntriggers and prototype sets, employing an iterative process of decoupling\ntask-specific models for previous tasks and unifying them with the model for\nthe newly learned task. Additionally, we introduce an inference strategy for\nzero-shot scenarios by aggregating predictions from multiple decoupled\ntask-specific models. Extensive experiments across various settings show that\nConDU achieves up to a 2\\% improvement in average performance across all seen\ntasks compared to state-of-the-art baselines, while also enhancing zero-shot\ncapabilities relative to the original VLM.",
        "Cognitive diagnosis aims to infer students' mastery levels based on their\nhistorical response logs. However, existing cognitive diagnosis models (CDMs),\nwhich rely on ID embeddings, often have to train specific models on specific\ndomains. This limitation may hinder their directly practical application in\nvarious target domains, such as different subjects (e.g., Math, English and\nPhysics) or different education platforms (e.g., ASSISTments, Junyi Academy and\nKhan Academy). To address this issue, this paper proposes the language\nrepresentation favored zero-shot cross-domain cognitive diagnosis (LRCD).\nSpecifically, LRCD first analyzes the behavior patterns of students, exercises\nand concepts in different domains, and then describes the profiles of students,\nexercises and concepts using textual descriptions. Via recent advanced\ntext-embedding modules, these profiles can be transformed to vectors in the\nunified language space. Moreover, to address the discrepancy between the\nlanguage space and the cognitive diagnosis space, we propose language-cognitive\nmappers in LRCD to learn the mapping from the former to the latter. Then, these\nprofiles can be easily and efficiently integrated and trained with existing\nCDMs. Extensive experiments show that training LRCD on real-world datasets can\nachieve commendable zero-shot performance across different target domains, and\nin some cases, it can even achieve competitive performance with some classic\nCDMs trained on the full response data on target domains. Notably, we\nsurprisingly find that LRCD can also provide interesting insights into the\ndifferences between various subjects (such as humanities and sciences) and\nsources (such as primary and secondary education).",
        "Omnidirectional image quality assessment (OIQA) has been one of the hot\ntopics in IQA with the continuous development of VR techniques, and achieved\nmuch success in the past few years. However, most studies devote themselves to\nthe uniform distortion issue, i.e., all regions of an omnidirectional image are\nperturbed by the ``same amount'' of noise, while ignoring the non-uniform\ndistortion issue, i.e., partial regions undergo ``different amount'' of\nperturbation with the other regions in the same omnidirectional image.\nAdditionally, nearly all OIQA models are verified on the platforms containing a\nlimited number of samples, which largely increases the over-fitting risk and\ntherefore impedes the development of OIQA. To alleviate these issues, we\nelaborately explore this topic from both subjective and objective perspectives.\nSpecifically, we construct a large OIQA database containing 10,320\nnon-uniformly distorted omnidirectional images, each of which is generated by\nconsidering quality impairments on one or two camera len(s). Then we\nmeticulously conduct psychophysical experiments and delve into the influence of\nboth holistic and individual factors (i.e., distortion range and viewing\ncondition) on omnidirectional image quality. Furthermore, we propose a\nperception-guided OIQA model for non-uniform distortion by adaptively\nsimulating users' viewing behavior. Experimental results demonstrate that the\nproposed model outperforms state-of-the-art methods. The source code is\navailable at https:\/\/github.com\/RJL2000\/OIQAND.",
        "Goal-oriented visual dialogue involves multi-round interaction between\nartificial agents, which has been of remarkable attention due to its wide\napplications. Given a visual scene, this task occurs when a Questioner asks an\naction-oriented question and an Answerer responds with the intent of letting\nthe Questioner know the correct action to take. The quality of questions\naffects the accuracy and efficiency of the target search progress. However,\nexisting methods lack a clear strategy to guide the generation of questions,\nresulting in the randomness in the search process and inconvergent results. We\npropose a Tree-Structured Strategy with Answer Distribution Estimator (TSADE)\nwhich guides the question generation by excluding half of the current candidate\nobjects in each round. The above process is implemented by maximizing a binary\nreward inspired by the ``divide-and-conquer'' paradigm. We further design a\ncandidate-minimization reward which encourages the model to narrow down the\nscope of candidate objects toward the end of the dialogue. We experimentally\ndemonstrate that our method can enable the agents to achieve high task-oriented\naccuracy with fewer repeating questions and rounds compared to traditional\nergodic question generation approaches. Qualitative results further show that\nTSADE facilitates agents to generate higher-quality questions.",
        "Dammann gratings - beam-shaping optical elements acting as beam multipliers\nwith equal-power beams - are a key element in three-dimensional imaging based\non structured light and beam combiners for high-power laser applications.\nHowever, two-dimensional Dammann grating structures suffer from a significant\nreduction of the uniformity among the diffraction orders. Here, we report\nDammann metasurfaces based on the geometric phase as the structure realization\nfor the target phase profile, which outperform the capabilities of Dammann\ngratings by overcoming the uniformity defects in their two-dimensional\ndiffraction patterns. We showed that two-dimensional Dammann metasurfaces\nexhibit high uniformity and diffraction efficiency, in contrast to Dammann\ngratings, by overcoming the uniformity defects via a robust and highly precise\nphase imprint. Moreover, Dammann metasurfaces outperform their grating\ncounterparts by exhibiting a polarization-independent response and a broadband\noperation. This study reveals that by providing physics-driven solutions,\nmetasurfaces can outperform the capabilities of their bulk optics counterparts\nwhile facilitating virtually flat, ultrathin, and lightweight optics.",
        "The idea of a parsing of a stationary process according to a collection of\nwords is introduced, and the basic framework required for the asymptotic\nanalysis of these parsings is presented. We demonstrate how the pointwise\nergodic theorem and the Shannon-McMillan-Breiman theorem can be deduced from\ntheir respective weaker convergence in probability versions combined with our\nobservations regarding parsings, where the parsings are done according to\ncollections that originate in stopping times tailored for that purpose."
      ]
    }
  },
  {
    "id":2411.02614,
    "research_type":"applied",
    "start_id":"b16",
    "start_title":"Medical diffusion on a budget: textual inversion for medical image generation",
    "start_abstract":"Diffusion-based models for text-to-image generation have gained immense popularity due to recent advancements in efficiency, accessibility, and quality. Although it is becoming increasingly feasible perform inference with these systems using consumer-grade GPUs, training them from scratch still requires access large datasets significant computational resources. In the case of medical image generation, availability large, publicly accessible that include text reports limited legal ethical concerns. While a diffusion model on private dataset may address this issue, not always institutions lacking necessary This work demonstrates pre-trained Stable Diffusion models, originally trained natural images, can be adapted various imaging modalities by embeddings textual inversion. study, we conducted experiments comprising only 100 samples three modalities. Embeddings were matter hours, while retaining diagnostic relevance generation. Experiments designed achieve several objectives. Firstly, fine-tuned processes inversion, revealing larger more examples are required. Secondly, validated our approach demonstrating 2\\% increase accuracy (AUC) detecting prostate cancer MRI, which challenging multi-modal modality, 0.78 0.80. Thirdly, performed simulations interpolating between healthy diseased states, combining multiple pathologies, inpainting show embedding flexibility control disease appearance. Finally, study small (less than 1 MB), facilitates easy sharing data reduced privacy",
    "start_categories":[
      "q-bio.OT"
    ],
    "start_fields":[
      "Quantitative Biology"
    ],
    "target_paper":{
      "id":[
        "b7"
      ],
      "title":[
        "Deep Learning Techniques for Diabetic Retinopathy Classification: A Survey"
      ],
      "abstract":[
        "Diabetic Retinopathy (DR) is a degenerative disease that impacts the eyes and consequence of Diabetes mellitus, where high blood glucose levels induce lesions on eye retina.Diabetic regarded as leading cause blindness for diabetic patients, especially working-age population in developing nations.Treatment involves sustaining patient's current grade vision since irreversible.Early detection crucial order to sustain effectively.The main issue involved with DR manual diagnosis process very time, money, effort consuming an ophthalmologist's examination retinal fundus images.The latter also proves be more difficult, particularly early stages when features are less prominent images.Machine learning-based medical image analysis has proven competency assessing images, utilization deep learning algorithms aided (DR).This paper reviews analyzes state-of-the-art methods supervised, self-supervised, Vision Transformer setups, proposing classification detection.For instance, referable, non-referable, proliferative classifications reviewed summarized.Moreover, discusses available datasets used tasks such detection, classification, segmentation.The assesses research gaps area detection\/classification addresses various challenges need further study investigation."
      ],
      "categories":[
        "cs.AI"
      ]
    },
    "list":{
      "title":[
        "KidneyTalk-open: No-code Deployment of a Private Large Language Model\n  with Medical Documentation-Enhanced Knowledge Database for Kidney Disease",
        "Generative Artificial Intelligence: Implications for Biomedical and\n  Health Professions Education",
        "ValuePilot: A Two-Phase Framework for Value-Driven Decision-Making",
        "Optimization of Link Configuration for Satellite Communication Using\n  Reinforcement Learning",
        "Ranking Counterfactual Explanations",
        "Metacognition in Content-Centric Computational Cognitive C4 Modeling",
        "KABB: Knowledge-Aware Bayesian Bandits for Dynamic Expert Coordination\n  in Multi-Agent Systems",
        "From Text to Space: Mapping Abstract Spatial Models in LLMs during a\n  Grid-World Navigation Task",
        "Natural Language-Assisted Multi-modal Medication Recommendation",
        "StepMathAgent: A Step-Wise Agent for Evaluating Mathematical Processes\n  through Tree-of-Error",
        "SycEval: Evaluating LLM Sycophancy",
        "Principles for Responsible AI Consciousness Research",
        "AgentSafe: Safeguarding Large Language Model-based Multi-agent Systems\n  via Hierarchical Data Management",
        "Synthesis of omnidirectional path loss model based on directional model\n  and multi-elliptical geometry",
        "Asymmetric Orbifolds, Rank Reduction and Heterotic Islands",
        "Emergence of Order in Chemically Active Droplets: Temporal Dynamics and\n  Collective Behavior",
        "EmbodiedVSR: Dynamic Scene Graph-Guided Chain-of-Thought Reasoning for\n  Visual Spatial Tasks",
        "MoC: Mixtures of Text Chunking Learners for Retrieval-Augmented\n  Generation System",
        "URL Inspection Tasks: Helping Users Detect Phishing Links in Emails",
        "In Pursuit of Predictive Models of Human Preferences Toward AI Teammates",
        "Topologically protected synchronization in networks",
        "Halfspace Representations of Path Polytopes of Trees",
        "Will AI replace Software Engineers? Do not hold your breath",
        "Effective enhancement of the electron-phonon coupling driven by\n  nonperturbative electronic density fluctuations",
        "PCAP-Backdoor: Backdoor Poisoning Generator for Network Traffic in\n  CPS\/IoT Environments",
        "M-LLM Based Video Frame Selection for Efficient Video Understanding",
        "Which2comm: An Efficient Collaborative Perception Framework for 3D\n  Object Detection",
        "Variational Combinatorial Sequential Monte Carlo for Bayesian\n  Phylogenetics in Hyperbolic Space"
      ],
      "abstract":[
        "Privacy-preserving medical decision support for kidney disease requires\nlocalized deployment of large language models (LLMs) while maintaining clinical\nreasoning capabilities. Current solutions face three challenges: 1) Cloud-based\nLLMs pose data security risks; 2) Local model deployment demands technical\nexpertise; 3) General LLMs lack mechanisms to integrate medical knowledge.\nRetrieval-augmented systems also struggle with medical document processing and\nclinical usability. We developed KidneyTalk-open, a desktop system integrating\nthree technical components: 1) No-code deployment of state-of-the-art (SOTA)\nopen-source LLMs (such as DeepSeek-r1, Qwen2.5) via local inference engine; 2)\nMedical document processing pipeline combining context-aware chunking and\nintelligent filtering; 3) Adaptive Retrieval and Augmentation Pipeline (AddRep)\nemploying agents collaboration for improving the recall rate of medical\ndocuments. A graphical interface was designed to enable clinicians to manage\nmedical documents and conduct AI-powered consultations without technical\nexpertise. Experimental validation on 1,455 challenging nephrology exam\nquestions demonstrates AddRep's effectiveness: achieving 29.1% accuracy (+8.1%\nover baseline) with intelligent knowledge integration, while maintaining\nrobustness through 4.9% rejection rate to suppress hallucinations. Comparative\ncase studies with the mainstream products (AnythingLLM, Chatbox, GPT4ALL)\ndemonstrate KidneyTalk-open's superior performance in real clinical query.\nKidneyTalk-open represents the first no-code medical LLM system enabling secure\ndocumentation-enhanced medical Q&A on desktop. Its designs establishes a new\nframework for privacy-sensitive clinical AI applications. The system\nsignificantly lowers technical barriers while improving evidence traceability,\nenabling more medical staff or patients to use SOTA open-source LLMs\nconveniently.",
        "Generative AI has had a profound impact on biomedicine and health, both in\nprofessional work and in education. Based on large language models (LLMs),\ngenerative AI has been found to perform as well as humans in simulated\nsituations taking medical board exams, answering clinical questions, solving\nclinical cases, applying clinical reasoning, and summarizing information.\nGenerative AI is also being used widely in education, performing well in\nacademic courses and their assessments. This review summarizes the successes of\nLLMs and highlights some of their challenges in the context of education, most\nnotably aspects that may undermines the acquisition of knowledge and skills for\nprofessional work. It then provides recommendations for best practices\novercoming shortcomings for LLM use in education. Although there are challenges\nfor use of generative AI in education, all students and faculty, in biomedicine\nand health and beyond, must have understanding and be competent in its use.",
        "Despite recent advances in artificial intelligence (AI), it poses challenges\nto ensure personalized decision-making in tasks that are not considered in\ntraining datasets. To address this issue, we propose ValuePilot, a two-phase\nvalue-driven decision-making framework comprising a dataset generation toolkit\nDGT and a decision-making module DMM trained on the generated data. DGT is\ncapable of generating scenarios based on value dimensions and closely mirroring\nreal-world tasks, with automated filtering techniques and human curation to\nensure the validity of the dataset. In the generated dataset, DMM learns to\nrecognize the inherent values of scenarios, computes action feasibility and\nnavigates the trade-offs between multiple value dimensions to make personalized\ndecisions. Extensive experiments demonstrate that, given human value\npreferences, our DMM most closely aligns with human decisions, outperforming\nClaude-3.5-Sonnet, Gemini-2-flash, Llama-3.1-405b and GPT-4o. This research is\na preliminary exploration of value-driven decision-making. We hope it will\nstimulate interest in value-driven decision-making and personalized\ndecision-making within the community.",
        "Satellite communication is a key technology in our modern connected world.\nWith increasingly complex hardware, one challenge is to efficiently configure\nlinks (connections) on a satellite transponder. Planning an optimal link\nconfiguration is extremely complex and depends on many parameters and metrics.\nThe optimal use of the limited resources, bandwidth and power of the\ntransponder is crucial. Such an optimization problem can be approximated using\nmetaheuristic methods such as simulated annealing, but recent research results\nalso show that reinforcement learning can achieve comparable or even better\nperformance in optimization methods. However, there have not yet been any\nstudies on link configuration on satellite transponders. In order to close this\nresearch gap, a transponder environment was developed as part of this work. For\nthis environment, the performance of the reinforcement learning algorithm PPO\nwas compared with the metaheuristic simulated annealing in two experiments. The\nresults show that Simulated Annealing delivers better results for this static\nproblem than the PPO algorithm, however, the research in turn also underlines\nthe potential of reinforcement learning for optimization problems.",
        "AI-driven outcomes can be challenging for end-users to understand.\nExplanations can address two key questions: \"Why this outcome?\" (factual) and\n\"Why not another?\" (counterfactual). While substantial efforts have been made\nto formalize factual explanations, a precise and comprehensive study of\ncounterfactual explanations is still lacking. This paper proposes a formal\ndefinition of counterfactual explanations, proving some properties they\nsatisfy, and examining the relationship with factual explanations. Given that\nmultiple counterfactual explanations generally exist for a specific case, we\nalso introduce a rigorous method to rank these counterfactual explanations,\ngoing beyond a simple minimality condition, and to identify the optimal ones.\nOur experiments with 12 real-world datasets highlight that, in most cases, a\nsingle optimal counterfactual explanation emerges. We also demonstrate, via\nthree metrics, that the selected optimal explanation exhibits higher\nrepresentativeness and can explain a broader range of elements than a random\nminimal counterfactual. This result highlights the effectiveness of our\napproach in identifying more robust and comprehensive counterfactual\nexplanations.",
        "For AI agents to emulate human behavior, they must be able to perceive,\nmeaningfully interpret, store, and use large amounts of information about the\nworld, themselves, and other agents. Metacognition is a necessary component of\nall of these processes. In this paper, we briefly a) introduce content-centric\ncomputational cognitive (C4) modeling for next-generation AI agents; b) review\nthe long history of developing C4 agents at RPI's LEIA (Language-Endowed\nIntelligent Agents) Lab; c) discuss our current work on extending LEIAs'\ncognitive capabilities to cognitive robotic applications developed using a\nneuro symbolic processing model; and d) sketch plans for future developments in\nthis paradigm that aim to overcome underappreciated limitations of currently\npopular, LLM-driven methods in AI.",
        "As scaling large language models faces prohibitive costs, multi-agent systems\nemerge as a promising alternative, though challenged by static knowledge\nassumptions and coordination inefficiencies. We introduces Knowledge-Aware\nBayesian Bandits (KABB), a novel framework that enhances multi-agent system\ncoordination through semantic understanding and dynamic adaptation. The\nframework features three key innovations: a three-dimensional knowledge\ndistance model for deep semantic understanding, a dual-adaptation mechanism for\ncontinuous expert optimization, and a knowledge-aware Thompson Sampling\nstrategy for efficient expert selection. Extensive evaluation demonstrates KABB\nachieves an optimal cost-performance balance, maintaining high performance\nwhile keeping computational demands relatively low in multi-agent coordination.",
        "Understanding how large language models (LLMs) represent and reason about\nspatial information is crucial for building robust agentic systems that can\nnavigate real and simulated environments. In this work, we investigate the\ninfluence of different text-based spatial representations on LLM performance\nand internal activations in a grid-world navigation task. By evaluating models\nof various sizes on a task that requires navigating toward a goal, we examine\nhow the format used to encode spatial information impacts decision-making. Our\nexperiments reveal that cartesian representations of space consistently yield\nhigher success rates and path efficiency, with performance scaling markedly\nwith model size. Moreover, probing LLaMA-3.1-8B revealed subsets of internal\nunits, primarily located in intermediate layers, that robustly correlate with\nspatial features, such as the position of the agent in the grid or action\ncorrectness, regardless of how that information is represented, and are also\nactivated by unrelated spatial reasoning tasks. This work advances our\nunderstanding of how LLMs process spatial information and provides valuable\ninsights for developing more interpretable and robust agentic AI systems.",
        "Combinatorial medication recommendation(CMR) is a fundamental task of\nhealthcare, which offers opportunities for clinical physicians to provide more\nprecise prescriptions for patients with intricate health conditions,\nparticularly in the scenarios of long-term medical care. Previous research\nefforts have sought to extract meaningful information from electronic health\nrecords (EHRs) to facilitate combinatorial medication recommendations. Existing\nlearning-based approaches further consider the chemical structures of\nmedications, but ignore the textual medication descriptions in which the\nfunctionalities are clearly described. Furthermore, the textual knowledge\nderived from the EHRs of patients remains largely underutilized. To address\nthese issues, we introduce the Natural Language-Assisted Multi-modal Medication\nRecommendation(NLA-MMR), a multi-modal alignment framework designed to learn\nknowledge from the patient view and medication view jointly. Specifically,\nNLA-MMR formulates CMR as an alignment problem from patient and medication\nmodalities. In this vein, we employ pretrained language models(PLMs) to extract\nin-domain knowledge regarding patients and medications, serving as the\nfoundational representation for both modalities. In the medication modality, we\nexploit both chemical structures and textual descriptions to create medication\nrepresentations. In the patient modality, we generate the patient\nrepresentations based on textual descriptions of diagnosis, procedure, and\nsymptom. Extensive experiments conducted on three publicly accessible datasets\ndemonstrate that NLA-MMR achieves new state-of-the-art performance, with a\nnotable average improvement of 4.72% in Jaccard score. Our source code is\npublicly available on https:\/\/github.com\/jtan1102\/NLA-MMR_CIKM_2024.",
        "Evaluating mathematical capabilities is critical for assessing the overall\nperformance of large language models (LLMs). However, existing evaluation\nmethods often focus solely on final answers, resulting in highly inaccurate and\nuninterpretable evaluation outcomes, as well as their failure to assess proof\nor open-ended problems. To address these issues, we propose a novel\nmathematical process evaluation agent based on Tree-of-Error, called\nStepMathAgent. This agent incorporates four internal core operations: logical\nstep segmentation, step scoring, score aggregation and error tree generation,\nalong with four external extension modules: difficulty calibration, simplicity\nevaluation, completeness validation and format assessment. Furthermore, we\nintroduce StepMathBench, a benchmark comprising 1,000 step-divided process\nevaluation instances, derived from 200 high-quality math problems grouped by\nproblem type, subject category and difficulty level. Experiments on\nStepMathBench show that our proposed StepMathAgent outperforms all\nstate-of-the-art methods, demonstrating human-aligned evaluation preferences\nand broad applicability to various scenarios. Our data and code are available\nat https:\/\/github.com\/SHU-XUN\/StepMathAgent.",
        "Large language models (LLMs) are increasingly applied in educational,\nclinical, and professional settings, but their tendency for sycophancy --\nprioritizing user agreement over independent reasoning -- poses risks to\nreliability. This study introduces a framework to evaluate sycophantic behavior\nin ChatGPT-4o, Claude-Sonnet, and Gemini-1.5-Pro across AMPS (mathematics) and\nMedQuad (medical advice) datasets. Sycophantic behavior was observed in 58.19%\nof cases, with Gemini exhibiting the highest rate (62.47%) and ChatGPT the\nlowest (56.71%). Progressive sycophancy, leading to correct answers, occurred\nin 43.52% of cases, while regressive sycophancy, leading to incorrect answers,\nwas observed in 14.66%. Preemptive rebuttals demonstrated significantly higher\nsycophancy rates than in-context rebuttals (61.75% vs. 56.52%, $Z=5.87$,\n$p<0.001$), particularly in computational tasks, where regressive sycophancy\nincreased significantly (preemptive: 8.13%, in-context: 3.54%, $p<0.001$).\nSimple rebuttals maximized progressive sycophancy ($Z=6.59$, $p<0.001$), while\ncitation-based rebuttals exhibited the highest regressive rates ($Z=6.59$,\n$p<0.001$). Sycophantic behavior showed high persistence (78.5%, 95% CI:\n[77.2%, 79.8%]) regardless of context or model. These findings emphasize the\nrisks and opportunities of deploying LLMs in structured and dynamic domains,\noffering insights into prompt programming and model optimization for safer AI\napplications.",
        "Recent research suggests that it may be possible to build conscious AI\nsystems now or in the near future. Conscious AI systems would arguably deserve\nmoral consideration, and it may be the case that large numbers of conscious\nsystems could be created and caused to suffer. Furthermore, AI systems or\nAI-generated characters may increasingly give the impression of being\nconscious, leading to debate about their moral status. Organisations involved\nin AI research must establish principles and policies to guide research and\ndeployment choices and public communication concerning consciousness. Even if\nan organisation chooses not to study AI consciousness as such, it will still\nneed policies in place, as those developing advanced AI systems risk\ninadvertently creating conscious entities. Responsible research and deployment\npractices are essential to address this possibility. We propose five principles\nfor responsible research and argue that research organisations should make\nvoluntary, public commitments to principles on these lines. Our principles\nconcern research objectives and procedures, knowledge sharing and public\ncommunications.",
        "Large Language Model based multi-agent systems are revolutionizing autonomous\ncommunication and collaboration, yet they remain vulnerable to security threats\nlike unauthorized access and data breaches. To address this, we introduce\nAgentSafe, a novel framework that enhances MAS security through hierarchical\ninformation management and memory protection. AgentSafe classifies information\nby security levels, restricting sensitive data access to authorized agents.\nAgentSafe incorporates two components: ThreatSieve, which secures communication\nby verifying information authority and preventing impersonation, and\nHierarCache, an adaptive memory management system that defends against\nunauthorized access and malicious poisoning, representing the first systematic\ndefense for agent memory. Experiments across various LLMs show that AgentSafe\nsignificantly boosts system resilience, achieving defense success rates above\n80% under adversarial conditions. Additionally, AgentSafe demonstrates\nscalability, maintaining robust performance as agent numbers and information\ncomplexity grow. Results underscore effectiveness of AgentSafe in securing MAS\nand its potential for real-world application.",
        "Millimeter wave (mmWave) technology offers high throughput but has a limited\nradio range, necessitating the use of directional antennas or beamforming\nsystems such as massive MIMO. Path loss (PL) models using narrow-beam antennas\nare known as directional models, while those using omnidirectional antennas are\nreferred to as omnidirectional models. To standardize the analysis,\nomnidirectional PL models for mmWave ranges have been introduced, including TR\n38.901 by 3GPP, which is based on measurements from directional antennas.\nHowever, synthesizing these measurements can be complex and time-consuming.\nThis study proposes a numerical approach to derive an omnidirectional model\nfrom directional data using multi-elliptical geometry. We assessed the\neffectiveness of this method against existing PL models for mmWaves that are\navailable in the literature.",
        "We consider toroidal asymmetric orbifolds of the heterotic string preserving\nall 16 supercharges, developing a general formalism to study components of the\nmoduli space characterized by rank reduction of the gauge group. In particular\nwe construct six- and four-dimensional heterotic islands with no massless\nmoduli other than the dilaton. The formalism involves the Leech lattice, its\nautomorphisms and their corresponding invariant and normal, or coinvariant,\nsublattices.",
        "Collective behaviors such as swarming, chemical signaling, and clustering are\nfundamental to biological microorganisms, enabling hierarchical colony\nformation, coordinated motion, and enhanced nutrient accessibility crucial for\ntheir survival. Over the past few decades, extensive research has been\ndedicated to unraveling the mechanisms underlying these diverse collective\npatterns through experimental model systems. Among these, active droplets have\nemerged as valuable synthetic analogs, effectively replicating key biological\nattributes and serving as ideal platforms for investigating collective\nphenomena. This research explores the collective behavior of\n4-Cyano-4-pentyl-biphenyl (5CB) oil droplets across varying P\\'eclet ($Pe$)\nnumbers. At high $Pe$, droplets exhibit a pusher mode of propulsion and form\ndynamic chain-like patterns. Decreasing $Pe$ enhances repulsive interactions\namong droplets, resulting in the inhibition of clustering. In the low $Pe$\nregime, their repulsive interactions predominated by chemical field lead to the\nemergence of an ordered structure. Furthermore, we illustrate how active\ndroplets efficiently navigate within a soft structured environment. These\nfindings contribute to our comprehension of self-organized phenomena in active\nmatter systems and provide insights for designing strategies for controlled\nlocomotion in intricate fluidic environments.",
        "While multimodal large language models (MLLMs) have made groundbreaking\nprogress in embodied intelligence, they still face significant challenges in\nspatial reasoning for complex long-horizon tasks. To address this gap, we\npropose EmbodiedVSR (Embodied Visual Spatial Reasoning), a novel framework that\nintegrates dynamic scene graph-guided Chain-of-Thought (CoT) reasoning to\nenhance spatial understanding for embodied agents. By explicitly constructing\nstructured knowledge representations through dynamic scene graphs, our method\nenables zero-shot spatial reasoning without task-specific fine-tuning. This\napproach not only disentangles intricate spatial relationships but also aligns\nreasoning steps with actionable environmental dynamics. To rigorously evaluate\nperformance, we introduce the eSpatial-Benchmark, a comprehensive dataset\nincluding real-world embodied scenarios with fine-grained spatial annotations\nand adaptive task difficulty levels. Experiments demonstrate that our framework\nsignificantly outperforms existing MLLM-based methods in accuracy and reasoning\ncoherence, particularly in long-horizon tasks requiring iterative environment\ninteraction. The results reveal the untapped potential of MLLMs for embodied\nintelligence when equipped with structured, explainable reasoning mechanisms,\npaving the way for more reliable deployment in real-world spatial applications.\nThe codes and datasets will be released soon.",
        "Retrieval-Augmented Generation (RAG), while serving as a viable complement to\nlarge language models (LLMs), often overlooks the crucial aspect of text\nchunking within its pipeline. This paper initially introduces a dual-metric\nevaluation method, comprising Boundary Clarity and Chunk Stickiness, to enable\nthe direct quantification of chunking quality. Leveraging this assessment\nmethod, we highlight the inherent limitations of traditional and semantic\nchunking in handling complex contextual nuances, thereby substantiating the\nnecessity of integrating LLMs into chunking process. To address the inherent\ntrade-off between computational efficiency and chunking precision in LLM-based\napproaches, we devise the granularity-aware Mixture-of-Chunkers (MoC)\nframework, which consists of a three-stage processing mechanism. Notably, our\nobjective is to guide the chunker towards generating a structured list of\nchunking regular expressions, which are subsequently employed to extract chunks\nfrom the original text. Extensive experiments demonstrate that both our\nproposed metrics and the MoC framework effectively settle challenges of the\nchunking task, revealing the chunking kernel while enhancing the performance of\nthe RAG system.",
        "The most widespread type of phishing attack involves email messages with\nlinks pointing to malicious content. Despite user training and the use of\ndetection techniques, these attacks are still highly effective. Recent studies\nshow that it is user inattentiveness, rather than lack of education, that is\none of the key factors in successful phishing attacks. To this end, we develop\na novel phishing defense mechanism based on URL inspection tasks: small\nchallenges (loosely inspired by CAPTCHAs) that, to be solved, require users to\ninteract with, and understand, the basic URL structure. We implemented and\nevaluated three tasks that act as ``barriers'' to visiting the website: (1)\ncorrect click-selection from a list of URLs, (2) mouse-based highlighting of\nthe domain-name URL component, and (3) re-typing the domain-name. These tasks\nfollow best practices in security interfaces and warning design.\n  We assessed the efficacy of these tasks through an extensive on-line user\nstudy with 2,673 participants from three different cultures, native languages,\nand alphabets. Results show that these tasks significantly decrease the rate of\nsuccessful phishing attempts, compared to the baseline case. Results also\nshowed the highest efficacy for difficult URLs, such as typo-squats, with which\nparticipants struggled the most. This highlights the importance of (1) slowing\ndown users while focusing their attention and (2) helping them understand the\nURL structure (especially, the domain-name component thereof) and matching it\nto their intent.",
        "We seek measurable properties of AI agents that make them better or worse\nteammates from the subjective perspective of human collaborators. Our\nexperiments use the cooperative card game Hanabi -- a common benchmark for\nAI-teaming research. We first evaluate AI agents on a set of objective metrics\nbased on task performance, information theory, and game theory, which are\nmeasurable without human interaction. Next, we evaluate subjective human\npreferences toward AI teammates in a large-scale (N=241) human-AI teaming\nexperiment. Finally, we correlate the AI-only objective metrics with the human\nsubjective preferences. Our results refute common assumptions from prior\nliterature on reinforcement learning, revealing new correlations between AI\nbehaviors and human preferences. We find that the final game score a human-AI\nteam achieves is less predictive of human preferences than esoteric measures of\nAI action diversity, strategic dominance, and ability to team with other AI. In\nthe future, these correlations may help shape reward functions for training\nhuman-collaborative AI.",
        "In a graph, we say that two nodes are topologically equivalent if their sets\nof first neighbors, excluding the two nodes, coincide. We prove that\nnonlinearly coupled oscillators located on a group of topologically equivalent\nnodes can get easily synchronized when the group forms a fully connected\nsubgraph (or combinations of these), regardless of the status of all the other\noscillators. More generally, any change occurring in the inner part of the\nremainder of the graph will not alter the synchronization status of the group.\nTypically, the group can synchronize when $k^{(\\mathrm{OUT})}\\leq\nk^{(\\mathrm{IN})}$, $k^{(\\mathrm{IN})}$ and $k^{(\\mathrm{OUT})}$ being the\ncommon internal and outgoing degree of each node in the group, respectively.\nSimulations confirm our analysis and suggest that groups of topologically\nequivalent nodes play the role of independent pacemakers.",
        "Given a tree $T$, its path polytope is the convex hull of the edge indicator\nvectors for the paths between any two distinct leaves in $T$. These polytopes\narise naturally in polyhedral geometry and applications, such as phylogenetics,\ntropical geometry, and algebraic statistics. We provide a minimal halfspace\nrepresentation of these polytopes. The construction is made inductively using\ntoric fiber products.",
        "Artificial Intelligence (AI) technology such as Large Language Models (LLMs)\nhave become extremely popular in creating code. This has led to the conjecture\nthat future software jobs will be exclusively conducted by LLMs, and the\nsoftware industry will cease to exist. But software engineering is much more\nthan producing code -- notably, \\emph{maintaining} large software and keeping\nit reliable is a major part of software engineering, which LLMs are not yet\ncapable of.",
        "We present a dynamical mean-field study of the nonperturbative electronic\nmechanisms, which may lead to significant enhancements of the electron-phonon\ncoupling in correlated electron systems. Analyzing the effects of electronic\ncorrelations on the lowest-order electron-phonon processes, we show that in the\nproximity of the Mott metal-to-insulator transition of the doped square lattice\nHubbard model, where the isothermal charge response becomes particularly large\nat small momenta, the coupling of electrons to the lattice is strongly\nincreased. This, in turn, induces significant corrections to both the\nelectronic self-energy and phonon-mediated pairing interaction, indicating the\npossible onset of a strong interplay between lattice and electronic degrees of\nfreedom even for small values of the bare electron-phonon coupling.",
        "The rapid expansion of connected devices has made them prime targets for\ncyberattacks. To address these threats, deep learning-based, data-driven\nintrusion detection systems (IDS) have emerged as powerful tools for detecting\nand mitigating such attacks. These IDSs analyze network traffic to identify\nunusual patterns and anomalies that may indicate potential security breaches.\nHowever, prior research has shown that deep learning models are vulnerable to\nbackdoor attacks, where attackers inject triggers into the model to manipulate\nits behavior and cause misclassifications of network traffic. In this paper, we\nexplore the susceptibility of deep learning-based IDS systems to backdoor\nattacks in the context of network traffic analysis. We introduce\n\\texttt{PCAP-Backdoor}, a novel technique that facilitates backdoor poisoning\nattacks on PCAP datasets. Our experiments on real-world Cyber-Physical Systems\n(CPS) and Internet of Things (IoT) network traffic datasets demonstrate that\nattackers can effectively backdoor a model by poisoning as little as 1\\% or\nless of the entire training dataset. Moreover, we show that an attacker can\nintroduce a trigger into benign traffic during model training yet cause the\nbackdoored model to misclassify malicious traffic when the trigger is present.\nFinally, we highlight the difficulty of detecting this trigger-based backdoor,\neven when using existing backdoor defense techniques.",
        "Recent advances in Multi-Modal Large Language Models (M-LLMs) show promising\nresults in video reasoning. Popular Multi-Modal Large Language Model (M-LLM)\nframeworks usually apply naive uniform sampling to reduce the number of video\nframes that are fed into an M-LLM, particularly for long context videos.\nHowever, it could lose crucial context in certain periods of a video, so that\nthe downstream M-LLM may not have sufficient visual information to answer a\nquestion. To attack this pain point, we propose a light-weight M-LLM -based\nframe selection method that adaptively select frames that are more relevant to\nusers' queries. In order to train the proposed frame selector, we introduce two\nsupervision signals (i) Spatial signal, where single frame importance score by\nprompting a M-LLM; (ii) Temporal signal, in which multiple frames selection by\nprompting Large Language Model (LLM) using the captions of all frame\ncandidates. The selected frames are then digested by a frozen downstream video\nM-LLM for visual reasoning and question answering. Empirical results show that\nthe proposed M-LLM video frame selector improves the performances various\ndownstream video Large Language Model (video-LLM) across medium (ActivityNet,\nNExT-QA) and long (EgoSchema, LongVideoBench) context video question answering\nbenchmarks.",
        "Collaborative perception allows real-time inter-agent information exchange\nand thus offers invaluable opportunities to enhance the perception capabilities\nof individual agents. However, limited communication bandwidth in practical\nscenarios restricts the inter-agent data transmission volume, consequently\nresulting in performance declines in collaborative perception systems. This\nimplies a trade-off between perception performance and communication cost. To\naddress this issue, we propose Which2comm, a novel multi-agent 3D object\ndetection framework leveraging object-level sparse features. By integrating\nsemantic information of objects into 3D object detection boxes, we introduce\nsemantic detection boxes (SemDBs). Innovatively transmitting these\ninformation-rich object-level sparse features among agents not only\nsignificantly reduces the demanding communication volume, but also improves 3D\nobject detection performance. Specifically, a fully sparse network is\nconstructed to extract SemDBs from individual agents; a temporal fusion\napproach with a relative temporal encoding mechanism is utilized to obtain the\ncomprehensive spatiotemporal features. Extensive experiments on the V2XSet and\nOPV2V datasets demonstrate that Which2comm consistently outperforms other\nstate-of-the-art methods on both perception performance and communication cost,\nexhibiting better robustness to real-world latency. These results present that\nfor multi-agent collaborative 3D object detection, transmitting only\nobject-level sparse features is sufficient to achieve high-precision and robust\nperformance.",
        "Hyperbolic space naturally encodes hierarchical structures such as\nphylogenies (binary trees), where inward-bending geodesics reflect paths\nthrough least common ancestors, and the exponential growth of neighborhoods\nmirrors the super-exponential scaling of topologies. This scaling challenge\nlimits the efficiency of Euclidean-based approximate inference methods.\nMotivated by the geometric connections between trees and hyperbolic space, we\ndevelop novel hyperbolic extensions of two sequential search algorithms:\nCombinatorial and Nested Combinatorial Sequential Monte Carlo (\\textsc{Csmc}\nand \\textsc{Ncsmc}). Our approach introduces consistent and unbiased\nestimators, along with variational inference methods (\\textsc{H-Vcsmc} and\n\\textsc{H-Vncsmc}), which outperform their Euclidean counterparts. Empirical\nresults demonstrate improved speed, scalability and performance in\nhigh-dimensional phylogenetic inference tasks."
      ]
    }
  }
]